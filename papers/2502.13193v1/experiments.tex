


\section{Experiments}\label{sec:experiments}


\begin{table*}[t]
\centering
{\renewcommand{\arraystretch}{2}
\caption{Datasets, public vocabularies and pre-trained embedding models used in our experiments.} \label{tbl:datasets}
\begin{centering}
\small
\begin{tabular}{llcll}
\toprule
 \textbf{Dataset} & \textbf{Data type} & \textbf{\# Classes} & \textbf{Public vocabulary} ($\mathcal V$) & \textbf{Pre-trained emb.~model} ($\mathcal E$) \\
\midrule
MIMIC & medical records & 2 & UMLS medical glossary & BioBERT ($d=768$) \\
DBPedia-14 & Wikipedia summaries & 14 & GloVe English dictionary & SentenceBERT ($d=768$) \\
\bottomrule
\end{tabular}
\end{centering}}
\end{table*}

%\subsection{Datasets}
%\noindent\textbf{Datasets.}
We test \alg\ on two text classification tasks on public datasets from different domains (cf.~\Cref{tbl:datasets}):

\noindent\textbf{(i) MIMIC} \cite{goldberger2000physiobank}: MIMIC is a dataset of medical records. We consider the binary classification task of patients diagnosed with cardiac conditions versus those who weren't, from the description of the hospital course provided in their discharge summary. The groundtruth labels are determined by the ICD diagnostic codes\footnote{\url{https://www.who.int/standards/classifications/classification-of-diseases}} 
%\cite{ICDLink}
for which the patients were billed.
We chose 10K medical records of each class at random, and use 8K of each class as the private dataset $\mathcal D$, and the remaining records as the test set. 
As the public vocabulary $\mathcal V$ for this task, we use 400K medical concepts from the Unified Medical Language System (UMLS) Glossary.\footnote{\url{https://www.nlm.nih.gov/research/umls/index.html}}
%\cite{UMLSLink}
For the pre-trained embedding model $\mathcal E$ we use BioBERT \cite{lee2020biobert}. See \Cref{app:mimic} for additional details.

\noindent\textbf{(ii) DBPedia-14} \cite{NIPS2015_250cf8b5}: The dataset consists of short summaries of Wikipedia articles from 14 topic categories, and the task is topic classification. We use its given split to training and test sets, treating the training set as the private dataset $\mathcal D$. 
For the public vocabulary $\mathcal V$, we use GloVe 6B dictionary \cite{pennington2014glove},\footnote{\url{https://nlp.stanford.edu/projects/glove/}} which consists of 400K generic English language words. For the pre-trained embedding model $\mathcal E$ we use ``all-mpnet-base-v2'' from SentenceBERT \cite{reimers2019sentence}.


\noindent\textit{Inclusion in LLM pre-training data.} 
LLMs typically do not disclose the data they have been pre-trained on, which creates methodological difficulties in using them for research, particularly in the context of privacy, so we comment on this matter directly. In our case, the MIMIC legally binding dataset usage terms prohibit using the data in training LLMs,\footnote{\url{https://physionet.org/news/post/gpt-responsible-use}} 
and therefore we may assume with high certainty that the main LLM we use in experiments (Claude v2) has not seen the private data.\footnote{We have also verified this with Anthropic AI by contacting their Support Team.} 
On the other hand, it is rather likely to have seen DBPedia-14, and nearly certainly has seen the original Wikipedia articles on which the dataset is based. 
As the case would be similar with any standard benchmark dataset, we opt to include one for completeness while acknowledging this limitation. 

\noindent\textbf{Experimental setup.}
For both datasets, our downstream task is text classification. To apply \alg\ to this task, in the preprocessing step we create a mutual privatized dictionary $\widetilde{\mathcal{V}}$ for all classes, and then perform the sequence generation step for each class separately with its own DP KDE. Using each class DP KDE, we generate keyphrase sequences for seeding prompts, and the texts the LLM generates for them are used as synthetic training records labeled with the same class. 
The LLM prompts in the final step contain only the overall document type (``medical record'' for MIMIC and ``summary of a Wikipedia-style article'' for DBPedia-14) and the privatized sequence of phrases, without explicit class information (e.g., we do not prompt the LLM to generate medical records for patients with cardiac conditions, or Wikipedia articles about artists). 

\noindent\textbf{Method components.}
In the preprocessing step we use $S=10$ phrases of each dataset record to produce a private vocabulary $\widetilde{\mathcal{V}}$ of size $1000$ from the public vocabulary $\mathcal V$. In the sequence generation step, we generate $1500$ sequences of length $L=10$ for each MIMIC class, and $1000$ sequences of length $L=10$ for each DBPedia-14 class. The main LLM we use is Anthropic AI's Claude v2 \cite{ClaudeLink}.
%through its publicly available prompting API. 
%As a point of reference, and for compatibility with prior work, we also include experiments with OpenAI's GPT-2 \cite{radford2019language}, a smaller LLM whose parameters are publicly available. 
For the domain adaptation step, we use Deep CORAL \cite{sun2016deep}, see \Cref{sec:deep_coral_details} for details.

\noindent\textbf{Privacy parameters.}
To choose $\epsilon$ we followed the guidelines offered by \cite[Section 5]{ponomareva2023dp} for private ML. They review current DP-trained ML models and LLMs, as well as real-world deployments of DP, and report them using $\varepsilon$ between 5 to 15. They advocate for $\epsilon\leq10$ for real-world deployments.

We chose two values, one representing tighter privacy and one moderate privacy, for each of the two DP budget components in \alg: $\varepsilon_{\mathrm{voc}}\in\{1,5\}$ and $\varepsilon_{\mathrm{kde}}\in\{5,10\}$. 
We evaluate \alg\ with total privacy budgets equal to the resulting four combinations, $\varepsilon=\varepsilon_{\mathrm{voc}}+\varepsilon_{\mathrm{kde}}\in\{6,10,11,15\}$. 
They represent tighter ($6$), moderate ($10,11$) and looser ($15$) privacy settings. 
We limit our experiments to four $\epsilon$ values due to the high impact associated with large-scale experiments with a high-end LLM. 

%concrete examples, they mention that the US Census (likely the most famous and celebrated real-world deployment of DP) used $\varepsilon=12.2$, and that the Gboard Spanish LLM (the only publicly deployed example of a DP LLM they could find) used $\varepsilon=8.9$. Their review of the academic literature yields that state-of-the-art DP-trained LLMs (as of 2023) have used $\varepsilon$ between $5.36$ to $6.7$, which resulted in a performance drop of between 3% to 34% depending on the model and on work cited. The choices of $\varepsilon_{total}$ in our experiments are in line with these ranges.

\noindent\textbf{Comparison to AugPE.} 
AugPE \cite{api2} is another recent method for generating private synthetic texts by LLM prompts (see \Cref{sec:related}). It start by prompting the LLM for random texts, and then sequentially prompts it to ``evolve'' them toward the private texts while maintaining DP.  
As implemented and reported in \cite{api2}, the initial prompt includes the target class topic in the clear (e.g., ``write an article about an athlete''), and also includes random words from a list that the LLM had been prompted to generate for that class topic (e.g., generic words related to athletes). 
This already ``gives away'' much of the intended content to the LLM in the clear.
However, in our setting, the class topics are considered private and are unknown to the algorithm, and therefore cannot be included in prompts. Furthermore, including the class topic in the prompt may lead to methodological artifacts, as it is hard to discern how much of the downstream accuracy owes to the class topic, which was given to the algorithm free of privacy, versus meaningfully making use of the private dataset. 

Therefore, to directly compare AugPE to \alg\ in our setting, we do not include the class topic in prompts for neither method. Since also removing the random words from AugPE's prompts leads to substantially degraded performance (see results below), we replace them  with random words from the privatized dictionary $\widetilde{\mathcal{V}}$ that we create in our pre-processing step (with $\varepsilon_{\mathrm{voc}}=1$). 
This is a variant of AugPE compatible with the privacy constraints of our setting, and we denote it by AugPE+$\widetilde{\mathcal{V}}$.

\subsection{Results}
\begin{table*}[t]
{
% \renewcommand{\arraystretch}{2}

\small
\begin{center}
% TAL: Please don't move table captions, the NeurIPS style guide says they need to be ABOVE the table
\caption{Classification accuracy of \alg\ (ours), AugPE+$\widetilde{\mathcal{V}}$, and the original texts.}\label{tbl:mainres}
%\caption{\textcolor{red}{Classification accuracy of proposed \alg\ method vs. AugPE. 
%In this table, we fix the size of the generated content to 1K examples for all methods.  (In Figure~\ref{fig:result_augpe_epsall}, larger sample sizes for AugPE are considered.) The rows depict increasing privacy budgets. The columns compare the two methods on the MIMIC and DBPedia datasets.  The last row shows the accuracy of the model on the original data, and hence is as an accuracy upper bound. }}
%\begin{adjustwidth}{-0.15in}{-0.1in}
\begin{tabular}{ c | c c c | c c c }
\toprule
& \multicolumn{3}{c|}{MIMIC} & \multicolumn{3}{c}{DBPedia-14} \\
\cmidrule{2-7}
%\midrule
  \hfill Method: & \alg\ & AugPE+$\widetilde{\mathcal{V}}$  & AugPE+$\widetilde{\mathcal{V}}$ & \alg\ & AugPE+$\widetilde{\mathcal{V}}$  & AugPE+$\widetilde{\mathcal{V}}$ \\
 \hfill\textit{\footnotesize{(\#texts, \#prompts) / class:}} & \textit{\footnotesize{(1K, 1K)}} & \textit{\footnotesize{(100, 1K)}} &\textit{\footnotesize{(1K, 10K)}} &\textit{\footnotesize{(1K, 1K)}} &\textit{\footnotesize{(100, 1K)}} &\textit{\footnotesize{(1K, 10K)}} \\
\midrule
%\rule{0pt}{3ex}
\hspace{-5pt}$\varepsilon=6$\scriptsize{$\;\;(=1+5)\hfill$} & 71.6\% &50.0\% &63.8\% & 77.9\% &45.7\% &79.3\%  \\
\hspace{-5pt}$\varepsilon=10$\scriptsize{$\;\;(=5+5)\hfill$} & 71.7\% &50.0\% &60.8\% & 80.4\% &46.7\% &76.3\% \\
\hspace{-5pt}$\varepsilon=11$\scriptsize{$\;\;(=1+10)\hfill$} & 72.2\% &50.3\% &66.3\% & 82.4\% &38.7\% &79.4\% \\
\hspace{-5pt}$\varepsilon=15$\scriptsize{$\;\;(=5+10)\hfill$} & 72.2\% &50.0\% &72.3\% & 85.1\% &34.8\% &80.5\% \\
\midrule
Original training set 
& \multicolumn{3}{c|}{76.0\%} &  \multicolumn{3}{c}{97.0\%} \\
\bottomrule
\end{tabular}
%\end{adjustwidth}
\end{center}}
\end{table*}

\noindent\textbf{Downstream classification accuracy.}
\Cref{tbl:mainres} lists the accuracy obtained  by training a classifier over the synthetic corpora produced by \alg,
compared to AugPE+$\widetilde{\mathcal{V}}$, and to training on a similar number of (randomly sampled) texts from the original training set. 
For each corpus we train the classifier by finetuning the pre-trained embedding model (BioBERT for MIMIC and SentenceBERT for DBPedia-14), namely by training a 3-layer fully connected neural network composed over it. 

Note that while \alg\ prompts the LLM once per synthetic text record, AugPE makes 10 iterative prompts per text, due to its ``evolution'' procedure (see also \Cref{sec:augpe_details}). To compare the methods, for \alg\ we use 1K docs and 1K prompts per class, and for AugPE+$\widetilde{\mathcal{V}}$, we report in \Cref{tbl:mainres} results once with the same number of prompts (and less texts), and once with the same number of texts (and more prompts). In \Cref{fig:result_augpe_epsall} we further compare the accuracy of both methods under varying ``prompts budgets'' for AugPE+$\widetilde{\mathcal{V}}$, and also report results for AugPE (without $\widetilde{\mathcal{V}}$).

The results show the corpora generated by \alg\ preserve much of the predictive accuracy of the original texts, while satisfying DP. Compared to either variant of AugPE, \alg\ achieves better accuracy with fewer prompts to the LLM, about $\times10$ less prompts for comparable accuracy. 
Furthermore, \alg\ is more consistent in improving as the privacy budget $\varepsilon$ is increased.



\noindent\textbf{Additional experiments.} More experimental results and ablations are reported in \Cref{sec:experiments_appendix}.

\noindent\textbf{\alg\ Examples.}
\Cref{tbl:example1} contains examples of \alg\ for three classes from DBPedia-14 (examples for all classes are given in  \Cref{tbl:example1full,tbl:example2} in the appendix). For each class we show two seed phrase sequences generated by \alg, and the LLM output when prompted to ``generate a summary of a fictitious wikipedia-style article'' that contains them. The intended class is not specified in the prompt, and for each class we show one ``good'' example, where we (subjectively) judge the topic of the output to be of the intended class, and one ``bad'' example where it is not. %Most outputs are ``good''. %, leading to the downstream accuracy reported above. 

Examples for MIMIC synthetic records are not included here due to an abundance of caution in complying with the dataset's legal terms of use, which require a certification for viewing the data.


%\noindent\textbf{Baseline.}
%AugPE~\cite{api2} is used as a baseline. This method was developed in parallel to ours and has a similar goal of generating private synthetic text via the prompt only.

%Here is a brief description of their approach: (1) Draw a random sample of documents from an LLM. 
%(2) Skew the synthetic samples towards private samples by repeating these three steps. (2.1) each private sample votes for their nearest synthetic sample. Noise is added to the induced histogram to preserve privacy. (2.2) Text is resampled according to the histogram. (2.3) An LLM is used to generate similar samples. These new synthetic samples form the basis of the next repetition.

%AugPE differs from our approach in a few ways.
%Our prompt, e.g., with 10 phrases, helps quickly pin down the nature of the desired content. In contrast, AugPE's first sample is random -- if the initial sample is far from the desired content it may be challenging to approach the desired content. This issue is particularly acute when the LLM has not been trained on data of the appropriate type, e.g., MIMIC medical records.
%Finally, our definition of $\epsilon$-DP is stronger than what they used, namely $(\epsilon,\delta)$-privacy.

%We had to change AugPE's implementation to not provide the label in the prompt. For example, to classify a Yelp review, Table 14 in~\cite{xie2018differentially} shows that the category of the business is provided to the LLM. Since the class label leaks information to the LLM, we modified the prompt to not include the LLM. 



%\subsection{Experimental Setup}


%\subsection{Downstream Classification Accuracy}

%\begin{figure}
%    \begin{center}
%        \includegraphics[width=\textwidth]{figures/results_dp_text.pdf}
%        \caption{Comparison to Aug-PE}
%        \label{fig:result_augpe_eps6_11}
%    \end{center}
%\end{figure}






% NOTE: Let's skip this to avoid needlessly triggering ethics review issues
%Another point of note is that when prompting the LLM with the above unseeded prompts, it sometimes refused to fulfill the prompt, apparently due to guardrails meant to protect from misinformation. It claimed, for example, that was unable to write ``fake'' medical records, or to produce summaries of fictitious encyclopedic entries. We did not notice this behavior with the seeded prompts. 
%Avoiding false positive trigering of guardrails may be a point of consideration for future work that aims to make automatic use of LLMs. \textcolor{blue}

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/results_dp_text2.pdf}
        \caption{Classification performance of data generated by DP-KPS (ours - star marker), AugPE+$\widetilde{\mathcal{V}}$ (dotted line) and AugPE (solid line) with varying privacy and prompt budget.}% DP-KPS requires about one tenth the number of prompts required by AugPE+$\widetilde{\mathcal{V}}$ to reach a comparable performance. }
        \label{fig:result_augpe_epsall}
    \end{center}
\end{figure}



\begin{table*}
\caption{\textcolor{goodgreen}{Good} and \textcolor{badred}{bad} examples for classes 1--3 in DBPedia-14, with $\varepsilon_{\mathrm{voc}}=1$ and $\varepsilon_{\mathrm{kde}}=10$.} \label{tbl:example1}
{\renewcommand{\arraystretch}{2}
\begin{centering}
\setlength\extrarowheight{-3pt}
\small
%\begin{adjustwidth}{-0.5in}{-0.1in}
\begin{tabular}{p{0.08\linewidth}p{0.87\linewidth}}
\toprule
 Class & \alg\ results: Keyphrase sequence and corresponding LLM output \\
\midrule
Company &

['plants', 'company', 'distribution', 'operating', 'social', 'ships', 'andrew', 'based', 'michigan', 'york']: \newline 
\textcolor{goodgreen}{Plants Distribution Company is a Michigan-based company operating ships to distribute plants and other goods. Founded by Andrew York in New York, the small social enterprise distributes products across the region.} \\
&
['director', 'institution', 'royal', 'schools', 'municipality', 'club', 'journal', 'johnson', 'office', 'oil']: \newline 
\textcolor{badred}{John Johnson was appointed director of the Royal Schools institution in the municipality in 2020. He previously worked in the office of the Oil Club journal.} \\
\midrule

Educational institution
&
['1908', 'school', 'school', 'college', 'institute', 'school', 'germany', 'university', 'college', 'manufacturing']: \newline 
\textcolor{goodgreen}{The College Institute was founded in Germany in 1908 as a manufacturing school. Originally a vocational college, it later became a university specializing in engineering and technology. The school focuses on hands-on training in manufacturing techniques.} \\
&
['scott', 'school', 'brazilian', 'guard', 'sciences', '1996', 'high', 'lee', 'mystery', 'england']: \newline 
\textcolor{badred}{Scott Lee was born in England in 1996. He attended a Brazilian jiu-jitsu school in high school where he learned grappling and the martial art of Brazilian jiu-jitsu. The mystery sciences behind the techniques fascinated Scott during his time there.}\\
\midrule

Artist &

['species', 'february', '1944', 'european', '1929', 'music', 'musician', 'duo', 'frank', 'light']: \newline 
\textcolor{goodgreen}{The musical duo Frank \& Light was formed in February 1944 in Europe by Frank Smith and Light Williams. The duo, known for their unique blend of jazz and classical music, released their debut album "Species of Music" in 1929 to critical acclaim. Though popular in the late 1940s, they disbanded as a musical act in the early 1950s.} \\

& ['england', 'june', '1972', '2003', 'english', 'animated', 'nonfiction', 'publisher', 'utah', 'november']: \newline 
\textcolor{badred}{In November 2003, an English animated nonfiction publisher based in Utah released a book about events that took place in England in June 1972. The book was well-received upon its November 2003 publication. }\\








\bottomrule
\end{tabular}
%\end{adjustwidth}
\end{centering}}
\end{table*}


\section{Conclusion}
The question of how best to generate private synthetic data is acutely important -- particularly when hospitals want to build predictive models but cannot share medical records.  A method for generating private keyphrases is introduced, carefully combining high-dimensional embeddings with KDE and DP to generate sequences of phrases that capture the private text corpus. Experiments suggest that our presented method generates medical records that can be used to train an ML model with higher accuracy, stronger privacy and lower LLM budget than baselines. 

Exciting challenges lie ahead.  Medical records are richer than just text.  They contain numbers (from blood reports), time series (of heart rate, respiration rate, etc) and images (x-rays, MRI).  The question of how to privately and jointly generate these diverse components is an unsolved grand challenge.




