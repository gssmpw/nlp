\newpage
\section{Impact Statement}
Our work suggests the potential use of LLM generated texts as synthetic data in machine learning tasks, as a manner of preserving privacy. Along with the many potential benefits of LLMs, using them is prone to known risks like hallucinations, misinformation, and introduction of biases from the pre-training set into the generated data. 
AI generated data is automated and probabilistic, and may often be inaccurate or inappropriate. 
These risks are present in virtually any use of generative AI models and one should always be mindful of them, but our view is that the specific use our work proposes, of using them to protect privacy, does not encompass substantial risks beyond those always present in generative model usage. 

A best practice for prospective users of our method would be to validate that the generated data faithfully represents the essential attributes of the original data (for example by comparing it to the original data in sample downstream learning tasks) prior to releasing it, and to transparently convey and emphasize with its release that the privatized data, while seeded with real private data, had been automatically generated by an AI generative model.

On the client side, a responsible use of AI generated data (by our method or any other one), particularly for any consequential decisions with impact on humans, should implement appropriate human oversight, testing, and other use case-specific safeguards to mitigate the associated risks.