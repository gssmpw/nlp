\section{\alg: Method Description}\label{sec:method_main}

\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{figures/method_dp_text.pdf}
        \captionsetup{font=footnotesize}
        \caption{\textbf{\alg} method overview. We illustrate the process of generating privatized synthetic medical records. For simplicity, each medical record is represented by a single sentence. The two example private documents shown here contain terms related to heart conditions. This results in the DP KDE having a higher concentration near the words related to the heart, and  hence sampling a term such as `beta blocker' for the synthetic key-phrase.} 
        %Color coding: \textcolor{methodred}{red -- private data}, \textcolor{methodpurple}{purple -- differentially privatized data (safe to release)}, \textcolor{methodgreen}{green -- public data}, \textcolor{methodblue}{blue -- public pre-trained model}. }
        \label{fig:detailedmethod}
    \end{center}
\vspace{-0.15 in}
\end{figure}

The core of \alg\ is prompting the LLM to generate synthetic text, by seeding the prompt with a sequence of keyphrases that are already privatized. 
We now describe how we instantiate this idea in detail.
%
The input to \alg\ consists of the following:
\begin{itemize}
 \item A private dataset $\mathcal{D}$ of text documents.
 \item A large public vocabulary $\mathcal{V}$. It need not be specialized to $\mathcal{D}$, but should be suitable for the data domain (for example, a glossary of medical terms for medical records, or a full English dictionary for Wikipedia articles).
 \item A public pre-trained embedding model $\mathcal{E}$, that can map terms from $\mathcal{V}$ into embeddings in $\R^d$. 
 \item Prompting access to an LLM $\mathcal L$. It can be prompted, but it is not allowed to see the private data, and therefore the prompts it receives must be already privatized.
\end{itemize}
The steps of \alg~are described next and illustrated in \Cref{fig:detailedmethod}.

%\subsection{Pre-processing: Privatized Vocabulary}\label{sec:method_prep}
\noindent\textbf{Pre-processing: Privatized vocabulary.}
We first expend some of the privacy budget on producing a limited privatized vocabulary $\widetilde{\mathcal{V}}$ from $\mathcal{V}$ (Step 1 in \Cref{fig:detailedmethod}). This serves two purposes: limiting the vocabulary size for better computational efficiency, and rendering it more relevant to the private dataset $\mathcal{D}$. We denote the privacy budget we expend here by $\varepsilon_{\mathrm{voc}}$.
%
We produce $\widetilde{\mathcal{V}}$ using vanilla DP histograms \cite{dwork2014algorithmic}; see appendix for details.
%\begin{CompactEnumerate}
% \item From each text document in $\mathcal{D}$, extract that first $S$ terms that appear in $\mathcal{V}$. %(Our implementation uses $S=10$.)
% \item Build a privatized histogram $\widetilde{\mathcal{H}}$ over $\mathcal{V}$ from the $S\cdot|\mathcal{D}|$ extracted terms, by adding an i.i.d.~sample from $\mathrm{Laplace}(S/\varepsilon_{\mathrm{voc}})$ to each count.
% \item The private vocabulary $\widetilde{\mathcal{V}}$ consists of the $N$ terms from $\mathcal{V}$ with the highest counts in $\widetilde{\mathcal{H}}$.
%\end{CompactEnumerate}
%It is easily seen that the $\ell_1$-sensitivity of the histogram is $S$, and therefore, the DP Laplace mechanism \cite{dwork2014algorithmic} ensures that $\widetilde{\mathcal{V}}$ is $\varepsilon_{\mathrm{voc}}$-differentially private.

%\subsection{Generating Private Keyphrase Sequences}\label{sec:method_main}
\noindent\textbf{Generating private keyphrase sequences.}
The main step of \alg\ is generating private sequences of keyphrases from $\widetilde{\mathcal{V}}$, that capture the private dataset $\mathcal{D}$ while maintaining differential privacy. 
We denote the privacy expended on this step by $\varepsilon_{\mathrm{kde}}$. Together with the preprocessing step, by the differential privacy composition theorem \cite{dwork2014algorithmic}, the total privacy budget of \alg\ is $\varepsilon_{\mathrm{voc}}+\varepsilon_{\mathrm{kde}}$.

We start by embedding all individual keyphrases $v\in\widetilde{\mathcal{V}}$ into vectors in $\R^d$, using the pre-trained embedding model $\mathcal{E}$ (Step 2 in \Cref{fig:detailedmethod}). The embedding dimension $d$ is typically high (e.g., $d=768$ is a common choice), and therefore subsequent operations must run in time at most linear in $d$.

Next, let $L$ be the desired number of keyphrases in an output sequence. Our current goal is to generate a collection of sequences from $\widetilde{\mathcal{V}}^L$ that would successfully represent $\mathcal{D}$ in downstream tasks. 
%We describe three approaches to generating sequences privately, and later discuss their different merits and test all of them in our experiments. 
%
%All three methods are based on constructing 
To this end, we will build a collection of suitable DP-KDE distributions over the private data (Step 3 in \Cref{fig:detailedmethod}), and generate private sequences from the associated DP-KDE scores (Step 4 in \Cref{fig:detailedmethod}). 
This is the most technically involved step in our method, and we explain it in stages: first, how to sample one keyphrase, and then, how to generate a sequence of keyphrases. 

\noindent{\emph{(i) Sampling one keyphrase.}}
To sample a single keyphrase, we use the high-dimensional DP-KDE mechanism from \cite{wagner2023fast}. It builds the KDE data structure on a given set of vectors $X\subset\R^d$ in time $O(d|X|)$, and then allows querying the KDE score of any point in $\R^d$ in time $O(d)$. 
%For a private dataset of vectors $V\subset\R^d$ and desired accuracy $\alpha$, it builds the DP-KDE data structure in time $O(d|V|/\alpha^2)$, and then allows querying the KDE score of any point in $\R^d$ in time $O(d/\alpha^2)$ up to additive error $\alpha$.
%
However, in order to generate a sequence, we need to draw sample of new points from the DP-KDE distribution, rather than query the scores at given points. 
Unfortunately, \cite{wagner2023fast} did not present an efficient sampling algorithm for their mechanism. All known sampling methods take time exponential in $d$ --- e.g., by enumerating over a $d$-dimensional grid --- which is infeasible for high-dimensional embeddings. 

To resolve this issue, 
we exploit the fact that we are only interested in samples from our privatized vocabulary $\widetilde{\mathcal{V}}$, rather than from all of $\R^d$. 
Thus, $\widetilde{\mathcal{V}}$ can serve as a feasible replacement for grid enumeration. 
We query the DP-KDE for the density $\tilde p_v$ of every keyphrase $v\in\widetilde{\mathcal{V}}$, which entails a feasible running time of $O(d|\widetilde{\mathcal{V}}|)$. Then, we may draw samples from the resulting multinomial distribution over $\widetilde{\mathcal{V}}$, wherein each $v\in \widetilde{\mathcal{V}}$ is sampled with probability $\tilde p_v/\sum_{v'\in\widetilde{\mathcal{V}}}\tilde p_{v'}$.

%For a private dataset of vectors $V\subset\R^d$ and desired accuracy $\alpha$, it builds the DP-KDE data structure in time $O(d|V|/\alpha^2)$, and then allows querying the KDE score of any point in $\R^d$ in time $O(d/\alpha^2)$ up to additive error $\alpha$. 
%One difficulty is that in order to generate a sequence, we need to draw a sample from the DP-KDE distribution, whereas unfortunately, \cite{wagner2023fast} did not present an efficient sampling algorithm for their high-dimensional mechanism. All known sampling methods take time exponential in $d$, which is infeasible for high-dimensional embeddings. To circumvent this issue, we will query the DP-KDE data structure for every embedding of a keyphrase in $\widetilde{\mathcal{V}}$, which entails a feasible running time of $O(d|\widetilde{\mathcal{V}}|/\alpha^2)$, and then draw samples from the resulting multinomial distribution over $\widetilde{\mathcal{V}}$, wherein each keyphrase is sampled with probability proportional to its DP-KDE score.

\noindent{\emph{(ii) Generating a sequence of keyphrases.}}
There are two natural ways to generate a sequence of $L$ keyphrases with the above procedure for sampling one keyphrase. We explore both of them.

The first method is to concatenate $L$ single keyphrase samples drawn independently. 
One advantage of this method is its fast running time. One possible downside is that it may fail to capture correlations and dependencies between phrases that tend to occur together in texts, which may be important in downstream learning. Yet, as our experiments will show, this method can be highly effective.

The second method is to generate the sequence iteratively, where each new keyphrase is added to the sequence while taking into account the prefix of keyphrases that were already generated. In more detail, iterative sequence generating proceeds as follows:

%Therefore, we next consider sequence generation that preserves such correlations.

%Next, we present two ways to generate sequences: choosing keyphrases independently, versus choosing them iteratively depending on previous choices. 
%To ease notation, in what follows we treat the target additive error $\alpha$ as a small constant, and suppress it in asymptotic running time bounds.

%\noindent\textbf{Method I: Independent keyphrase generation.}
%A simple way to privately generate a length-$L$ sequence is to build a single DP-KDE distribution over $\widetilde{\mathcal{V}}$, using all single keyphrases from $\mathcal{D}$, and then draw $L$ i.i.d.~samples from this distribution to produce a length-$L$ sequence. 
%The preprocsessing time of this method is $O(d|\mathcal{D}|M+d|\widetilde{\mathcal{V}}|)$, where $M$ is the maximum number of keyphrases in a document, to first construct the DP-KDE data structure, and then query the density of each embedding in $\widetilde{\mathcal{V}}$. Then, a length-$L$ sequence can be generated in time $O(L)$ by drawing $L$ i.i.d.~samples from the induced multinomial distribution over $\widetilde{\mathcal{V}}$.

%While this method is fast, independent keyphrase sampling may fail to capture correlations and dependencies between often co-occurring keyphrases, which may be important in downstream learning tasks. Therefore, we next consider sequence generation that preserves such correlations.

%\noindent\textbf{Method II: Iterative sequence generation.}
%Ideally, we would have liked to draw a sample from a DP-KDE distribution over all of $\widetilde{\mathcal{V}}^L$. However, computing the induced multinomial distribution over $\widetilde{\mathcal{V}}^L$ would require $|\widetilde{\mathcal{V}}|^L$ queries to the DP-KDE data structure, which is prohibitive even for moderate values of $|\widetilde{\mathcal{V}}|$ and $L$. 
%Instead, drawing on intuition from conditional sampling and auto-regressive models, we generate a sequence iteratively by scoring the next keyphrase given the already generated prefix, as follows.

\begin{enumerate}
 \item[1.] Initialize an empty sequence of keyphrases $P$.
 \item[2.] For $i=1,\ldots,L:$
 \begin{enumerate}
 \item[2.1] For every single keyphrase in $w\in\widetilde{\mathcal{V}}$, let $Pw$ be the concatenation of $P$ and $w$. Thus $Pw$ is a keyphrase sequence of length $i$. Enumerate over all possible concatenations $\{Pw:w\in\R^d\}$, and compute the DP-KDE score of each.
 \item[2.2] Choose among them a high scoring sequence $Pw^*_i$. % (either by sampling proportionally to the scores, or by taking the highest scoring sequence).
 \item[2.3] $P\leftarrow Pw^*_i$.
 \end{enumerate}
 \item[3.] Return the output sequence $P$.
\end{enumerate}
In each iteration, this scheme makes only $|\widetilde{\mathcal{V}}|$ queries to the KDE data structure, for a total of $L|\widetilde{\mathcal{V}}|$, rendering its running time feasible. At the same time, jointly scoring keyphrase prefixes rather than just individual keyphrases promotes preserving dependencies between co-occurring keyphrases. 

One downside of this method is that it queries the KDE scores for keyphrase prefixes of multiple lengths --- that is, of vectors with varying dimensions --- and thus requires multiple DP KDE data structures. The privacy budget needs to be partitioned between these data structure, leading to decreased accuracy in each. Na\"ively, the method requires $L$ DP KDE data structures, one per each prefix length. In \Cref{sec:seqgen_appendix} we show how this can be improved to $O(\log L)$. 



%This scheme performs only $L|\widetilde{\mathcal{V}}|$ DP KDE queries overall ($|\widetilde{\mathcal{V}}|$ queries in each of the $L$ iterations), rendering its running time feasible. 
%
%Yet, a hurdle toward implementing it is that the DP KDE mechanism operates on vectors of a fixed dimension, while here we need to query vectors of varying dimensions (for $i=1,\ldots,L$, the length-$i$ prefixes $Pw$ in iteration $i$ have dimension $di$). 
%We handle this by using an \emph{ensemble} of DP KDE data structures.
%We describe two ways to handle this, by using an ensemble of DP-KDE data structures. 

%\noindent\textbf{Method II: Linear DP-KDE ensemble.}
%One way is to simply 
%The straightforward solution is to create $L$ DP KDE mechanism $K_1,\ldots,K_L$, where each $K_i$ is built over the first $di$ dimensions the dataset.
%, and is used for queries in iteration $i$. 
%operates on dimension $di$. 
%In the above sequence generation scheme, iteration $i$ would make its $|\widetilde{\mathcal{V}}|$ DP-KDE queries to $K_i$. 
%
%Let us calculate the computational parameters of this method. The privacy budget $\varepsilon_{\mathrm{kde}}$ needs to be allocated among the $K_i$'s, so each is constructed with privacy parameter $\varepsilon'=\varepsilon_{\mathrm{kde}}/L$. Since the error of the DP-KDE data structure from \cite{wagner2023fast} degrades with $\varepsilon$ like $\sqrt{1/\varepsilon}$, the error of each $K_i$ degrades by $\sqrt{L}$ (compared to using a single DP KDE as Method I does). 
%The overall running time is $O(dL^2|\mathcal D|M)$ for building the DP KDEs (each $K_i$ takes time $O(di|\mathcal D|M)$ to build), and $O(dL^2|\widetilde{\mathcal{V}}|)$ to generate every length-$L$ sequence. This is considerably more expensive than the independent sampling of Method I, but has the advantage of preserving keyphrase correlations.

%This method works well as long as $L=O(1)$, but as $L$ grows, the linear decay of the privacy budget may result in inaccurate density scores.
%With additional work, and under the assumption that the pre-trained embedding model $\mathcal E$ outputs normalized vectors (which is the case for many such models), we are able to show that the number of DP KDEs in the ensemble can be reduced from $L$ to $\lceil \log L \rceil$. Thus, the privacy budget per KDE becomes $\varepsilon'=\varepsilon_{\mathrm{kde}}/\lceil \log L \rceil$, which asymptotically improves the error as $L$ grows. Since the range of query dimensionalities in the iterative sampling process is now much larger than the range of dimensionalities in the ensemble ($L$ vs.~$\lceil \log L \rceil$), our proof requires analyzing \emph{asymmetric} Gaussian kernels, wherein dataset and query dimensionalities may differ. We provide the details of the construction and the proof in \Cref{sec:logensemble}.

%\noindent\textbf{Method III: Logarithmic DP-KDE ensemble.}
%To improve the error and computational cost of the linear DP-KDE ensemble, we also propose a method more frugal in the number of DP-KDEs. 
%Suppose we construct just a single DP-KDE data structure $K$, over the full length-$L$ sequence dimension, $dL$. In iteration $i<L$, to retrieve the DP-KDE score of a $di$-dimensional prefix $x\in\R^{di}$, we pad $x$ with zero blocks into a $dL$-dimensional vector $\bar x\in\R^{dL}$, and query $K$ on $\bar x$. If the vectors over which $K$ is constructed are normalized---which is the case for many widely used pre-trained embedding models---then it is possible to prove that zero padding returns an accurate DP-KDE estimate even for prefixes (we give the proof in the appendix):
%\begin{proposition}\label{prp:ensemble}
%If $K$ is built over a private dataset $\mathcal D\in\R^{dL}$ of unit length vectors, then querying $K$ on $\bar x$ returns a private and accurate KDE estimate for $x$ w.r.t.~the dataset of all $di$-dimensional prefixes of the vectors in $\mathcal D$. 
%\end{proposition}
%Ostensibly, it now suffices to build just one DP-KDE. However, a small hurdle is that different prefix lengths also require different kernel bandwidth settings when computing their KDE. This is slightly technical (see appendix for details), but the upshot is that if too many blocks in $\bar x$ are the result of zero padding, then its KDE score would not be informative even if we were to compute it exactly.
%
%To remedy this, we build logarithmically many DP-KDEs, $K_1,\ldots,K_\ell$, with $\ell=\lceil \log L \rceil$. 
%Each $K_j$ is built for vectors of dimension $d\cdot2^j$. 
%In the iterative sequence generation scheme, iteration $i$ makes its queries to $K_{\lceil \log i\rceil}$, by zero-padding them from dimension $di$ to dimension $d\cdot2^{\lceil \log i\rceil}$, which is between $di$ to $2di$. This ensures that at most half the query blocks are the result of zero padding, thus ensuring thar the DP-KDE score is informative.
%
%We calculate the computational parameters of this method and compare them to Method II. 
%The privacy budget $\varepsilon_{\mathrm{kde}}$ is allocated among the $K_j$'s, so each is constructed with privacy parameter $\varepsilon=\varepsilon_{\mathrm{kde}}/\ell$ (recall that $\ell=O(\log L)$), and thus the DP-KDE error of each $K_j$ in the logarithmic ensemble degrades by $\sqrt{\log L}$ (compared to $\sqrt L$ in Method II). The time to build each $K_j$ is $O(d\cdot 2^j|\mathcal D|M)$, hence the total building time is $O(dL|\mathcal D|M)$ (compared to $O(dL^2|\mathcal D|M)$ in Method II). The time to generate a sequence is $O(dL^2|\mathcal D|M)$, similarly to Method II. 
%Thus, the logarithmic DP-KDE ensemble has better accuracy and building time compared to the linear ensemble, with the same sequence generation time.

%\subsection{Prompting the LLM and Post-processing}\label{sec:method_post}
\noindent\textbf{Prompting the LLM and post-processing.}
Once the privatized keyphrase sequences have been generated, we query the LLM once per sequence to generate a corresponding synthethic text document (Step 5 in \Cref{fig:detailedmethod}). The prompt can be tweaked in various ways to improve the output quality for specific applications, but since our focus here is on developing a general method, we restrict our scope to generic prompts that only mention the desired document type (e.g., ``write a medical record that contains the following terms: $\langle$\textit{sequence of keyphrases}$\rangle$''). Since the keyphrase sequences are $(\varepsilon_{\mathrm{voc}}+\varepsilon_{\mathrm{kde}})$-DP, then by DP post-processing \cite{dwork2014algorithmic}, the LLM output is $(\varepsilon_{\mathrm{voc}}+\varepsilon_{\mathrm{kde}})$-DP as well.

The privatized documents generated by the LLM are intended for use in downstream ML by a client with its own test set. 
One issue is that the LLM output might look very different than the test set. For example, if we prompt the LLM to generate a ``medical record'', it might generate one in a completely different format than those in a client hospital's test set.
%
To resolve this, the final step in \alg\ is off-the-shelf domain adaptation on the client side, between the synthetic data and the test data. 

Another possibility we consider is \emph{few-shot prompting}, where the client provides a small number of examples from their test set (say, a few example medical records with the desired format), which can augment the prompt, so that the LLM is prompted to produce outputs adhering to this format. This fits application scenarios where the client can communicate with the curator during the generation of the private data. We examine this in \Cref{app:fewshot}.

%We consider two ways to resolve this discrepancy, such that either one or both can be applied, depending on the application scenario.

%\begin{CompactItemize}
% \item\textit{Domain adaptation:} The client can apply any off-the-shelf domain adaptation method, to align the training set generated by the LLM with its own test set. This requires no communication from the client to the data curator.
% \item\textit{Few-shot prompting:} If the application scenario allows the client to communicate with the curator, the client may provide a small number of examples from its test set (say, a few example medical records with the desired format), with can augment the prompt, so that the LLM is prompted to produce outputs adhering to this format. 
%\end{CompactItemize}
%Our experiments will show that each of these method is beneficial on its own, and that they work best when combined.

%This completes the description of DP-KPS.


%\subsection{Overall System Structure}\label{sec:system}
%The input to our method is a collection $\mathcal{C}$ of text documents (e.g., textual medical records kept by a hospital) and a dictionary $\mathcal{D}$ of all possible keywords in the domain (e.g., all possible medical diagnoses). The method proceeds as follows:
%\begin{enumerate}
% \item Extract from each document in $\mathcal{C}$ a sequence of $L$ keywords from $\mathcal{D}$ that appear in it. 
% \item Embed each such sequence $(w_1,w_2,\ldots,w_L)$ into a $dL$-dimensional vector given by the concatenation $(\mathrm{Emb}(w_1),\ldots,\mathrm{Emb}(w_2)),\ldots,\mathrm{Emb}(w_L))\in\R^{dL}$.\\
% (Recall that $\mathrm{Emb}(w)$ is a unit-length embedding of $w$ in $\R^d$.)
% \item Let $X$ denote the collection of all those $dL$-dimensional vectors extracted from $\mathcal C$.
% \item Construct a DP-KDE distribution in $R^{dL}$ over the set $X$.
% \item Choose a vector $y\in\R^{dL}$ with a high DP-KDE score. Decode it into a sequence of keywords $(\hat{w}_1,\hat{w}_2,\ldots,\hat{w}_L)$. 
% \item Prompt an LLM to produce a text document of the desired form, seeded with the sequence of keywords $(\hat{w}_1,\hat{w}_2,\ldots,\hat{w}_L)$.
%\end{enumerate}






%\subsection{Training a Classifier}
%We outline how to privately train a model for text classification using our method. 
%Suppose we have a collection $\mathcal{C}$ of text documents, each labeled with one of $m$ classes. For every $j=1,\ldots,m$, let $C_j\subset\mathcal C$ denote the subset of documents with label $j$. 
%\begin{enumerate}
% \item Using our method, for every sub-collection of documents $C_j$ with the same label, the party holding the private data (say, the hospital) constructs a private synthetic collection of documents $\widehat{C}_j$. 
% \item Their union $\widehat{\mathcal{C}}=\cup_{j=1}^m\widehat{C}_j$ is a synthetic labeled collection of documents, which satisfies differential privacy and can be safely shared externally, say with a third-party ML vendor.
% \item The ML vendor trains a classifier on the synthetic labeled collection $\widehat{\mathcal{C}}$.
% \item The model is provided back to the party holding the private data (the hospital), which can use it to classify new unseen text documents.
%\end{enumerate}
%The scheme is illustrated in Figure~\ref{fig:classification}.

