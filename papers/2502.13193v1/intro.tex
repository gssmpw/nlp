\section{Introduction}
\label{sec:intro}



Organizations have large text corpora that they wish to share with others for analytical insights.  When this data contains information about people, privacy considerations abound.  For example, a hospital that collects clinical notes may wish to predict who is at risk of developing a particular health condition.  
They wish to share their clinical notes so others can train a model.  However, privacy and legal regulations prevent data sharing in the clear.

\noindent\textbf{Motivating case.}
We are motivated by the following true story. A hospital has an unexpected rise in deaths due to a particular condition and hence wants to predict who is at risk so preventative measures can be taken. An external organization is consulted to build an ML model, since no in-hospital ML expertise is available. Since patient records are private and hospitals do not know how to make records private, one year passes, data is not shared, and the surge in deaths remains unaddressed.

Thus, we are driven to find ways to generate useful yet privacy-preserving synthetic records. 'Useful', in this context, means the generated corpus of synthetic records could serve as an alternative training set for downstream ML models that would normally be trained on real textual records. 

%\noindent\textbf{Risks of plain PII removal.}
How should `privacy' be defined?
One common practice %to overcome privacy constraints 
is anonymization, where personally identifiable information (PII) such as name, address, social security number is deliberately concealed.  Tools for anonymizing text such as~\cite{philter,comprehendMedical} can identify PII to anonymize with high accuracy.  However, even if PII Is obscured, the rest of the text may still be stitched together to identify an individual by a unique set of traits (e.g., a set of medical conditions).  Moreover, anonymized data may be joined with other public datasets to re-identify individuals, such as car accident data~\cite{car_accidents} or voting records~\cite{latanya}.  
This led researchers to look for more rigorous notions of privacy that provide formal guarantees. 

\noindent\textbf{Differential Privacy} (DP)
\cite{dwork2006calibrating} has emerged as a rigorous alternative, and is now considered the gold standard for privacy in data analysis and machine learning. Intuitively, it ensures that the result of a computation over a dataset is not significantly altered by the inclusion of any individual in the dataset, and thus information about any individual cannot be recovered from the result. 

DP can be enforced at different parts of the data analysis pipeline. Perhaps the most challenging one is \emph{synthetic data generation} \cite{ponomareva2023dp}. 
The goal is to create a synthetic dataset, which is DP w.r.t.~the original private dataset, yet preserves its essential global properties for purposes of analysis and machine learning. Due to the post-processing property of DP, the synthetic dataset can be released and used as a proxy in downstream computations without further privacy concerns. 
In this work, our goal is to create synthetic private collections of text documents.

\noindent\textbf{Large Language Models} (LLMs)
are by now renowned for their ability to generate high-quality natural language text. 
Thus, it is natural to seek to harness their power for creating private synthetic text \cite{hu2023sok}, and much work has been dedicated to training them privately (cf.~\Cref{sec:related}). 
However, in many real scenarios, these methods are difficult to apply. Due to the extreme cost and complexity of training and maintaining LLMs, most organizations use proprietary LLM services offered by third parties, who often limit the ability to train or modify the LLM. Moreover, the LLM, being owned by a third party, may itself be considered a privacy risk, and client organizations are often unable to share their sensitive data with it. 
In fact, even publicly available medical datasets widely used in research, such as MIMIC \cite{goldberger2000physiobank}, enforce limits on using their data in LLM prompts in the clear.\footnote{\url{https://physionet.org/news/post/gpt-responsible-use}}


As a result, several works have recently emphasized the necessity and importance of developing machine learning methods that interact with large pre-trained models as \emph{interfaces}, accessing them only through inference or prompt, and feeding them only with data which is safe for public release \cite{pryzant-etal-2023-automatic,duan2023flocks,lin2023differentially,cohen2023hot}. 
Such methods enable a wider audience of prospective users---including those with limited access and resources---to harness state-of-the-art machine learning for their ends, alleviating many logistical and regulatory barriers. 
Our work subscribes to this emerging trend. 

\noindent\textbf{Problem specification.}
The foregoing discussion leads us to the following challenge: 
Given a private dataset $\mathcal D$ of text documents, create a privatized synthetic dataset of text documents, which can be used as a proxy for $\mathcal D$ in downstream machine learning. At our disposal is an LLM, but it can be accessed only through a prompting interface, and is only allowed to see data which is already privatized. Privacy is defined as document-level DP, where each text document in $\mathcal D$ corresponds to an individual (e.g., a medical record), and thus the output of our method is required to be insensitive to the inclusion of any single entire document in $\mathcal D$. See \Cref{sec:prelim} for the formal definition.

%\textcolor{red}{Cost is a consideration since text is generated via an LLM, and LLMs charge by the number of input and output tokens. 
%Combining the above, the problem is to generate synthetic, privatized, small yet predictive, text corpora at a low-cost. }


\subsection{Our Method: \alg}\label{sec:intro_method}
We outline our method, Differentially Private  Keyphrase Prompt Seeding. See also \Cref{fig:method2,fig:flow}.



Our starting point is the recent work \cite{eldan2023tinystories}. 
Unrelated to privacy, their goal was to generate a synthetic dataset of children's stories with an LLM.  
A major challenge turned out to be \emph{output diversity}:
they found that when prompting the LLM for stories without further specifics, even at a high setting of the temperature parameter, the resulting stories were not diverse enough to accomplish the downstream task (which in their case was to train a smaller language model). Their solution was to prompt the LLM to write a story that contains three specific words (e.g., the adjective `bad', the noun `wolf' and the verb `huff'). By prompting each time with different words, drawn at random from a pre-specified vocabulary, they achieved the requisite level of diversity in the synthetic dataset of stories. 

We wish to adapt this idea, of \emph{seeding the LLM prompt with keyphrases},\footnote{We opt to use \emph{keyphrase} instead of \emph{keyword}, since in many application contexts, vocabulary elements consist of several words (e.g., ``congestive heart failure'' in medical records).} to our problem of generating DP text. 
While \cite{eldan2023tinystories} could choose their keyphrases at random (having no privacy constraints, nor a given dataset to begin with), our challenge is to choose them in a way that captures the given private dataset $\mathcal D$, while adhering to DP.  The general approach is illustrated in Figure~\ref{fig:method2}.

% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\textwidth]{figures/fig_idea}}
% \caption{ General idea behind the approach. We start with a collection of private documents.  Next, a private sequence of keyphrases is generated (the key technical contribution). The keyphrases are then used to prompt an LLM for a document containing the keyphrases.  This process is repeated multiple times to obtain a collection of private synthetic documents.}
% \label{fig:general_idea}
% \end{center}
% \vskip -0.2in
% \end{figure*}


To this end, we use recent results on private kernel density estimation. 
KDE is a common way to model a finite dataset in $\R^d$ as a probability distribution, which can then be sampled from. 
Recently, \cite{wagner2023fast} gave a DP KDE method for high-dimensional data. 
To us this suggests the following plan: 
\begin{enumerate}
    \item Take a public vocabulary (say, an English dictionary);
    \item Embed it in $\R^d$ using pre-trained term embeddings (following \cite{mikolov2013efficient});
    \item Construct a DP KDE distribution over vocabulary terms from the private dataset $\mathcal D$, using \cite{wagner2023fast};
    \item Draw samples of terms from the DP KDE distribution with which to seed the LLM prompts. 
\end{enumerate}   
The sampled terms would capture $\mathcal D$, since they are drawn from a distribution that approximates its KDE, while also satisfying differentially privacy.

This plan entails several challenges. 
For one, while \cite{wagner2023fast} gave an efficient way to privately estimate the KDE at any given point in $\R^d$, they did not provide a way to efficiently draw samples from the DP KDE distribution, and the problem appears infeasible in high dimensions. 
Our observation here is that the vocabulary, while large, is still feasible to enumerate over linearly. Thus, we can draw a term from the DP KDE distribution by querying the KDE of each term in the vocabulary, and then sampling from the resulting multinomial distribution. 


Beyond sampling a single term, we wish to sample a sequence of terms. Much of the signal we want to capture in $\mathcal D$ may lie not just in individual terms, but in the joint distribution of co-occurring terms, e.g., ‘lung’ and ‘pulmonary embolism’ and ‘shortness of breath’. While enumerating over a large vocabulary to sample one keyphrase is feasible, enumerating over $k$-tuples to sample a sequence becomes infeasible, even for small values of $k$. 
To address this, we develop ways to sequentially sample dependent keyphrases, using \emph{ensembles} of DP KDEs. 


By prompting the LLM with a DP sequence of keyphrases, a text record is privately generated.  A final consideration is that the text may not be written in the style of the data recipient (say, a different hospital).  
To overcome this, the final step in our method employs off-the-shelf domain adaption.
%
Ultimately, the quality of a set of synthetic texts is quantified by how effectively an external party can train an ML model.  Our experiments demonstrate how an ML model trained on a private synthetic text corpus generated by \alg\ can achieve high accuracy performance.


%\begin{figure*}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=\textwidth]%{figures/fig_idea2.pdf}}
%\caption{ General idea behind the approach. We start with a collection of private documents.  Next, a private sequence of keyphrases is generated (the key technical contribution). The keyphrases are then used to prompt an LLM for a document containing the keyphrases.  This process is repeated multiple times to obtain a collection of private synthetic documents.}
%\label{fig:general_idea}
%\end{center}
%\vskip -0.2in
%\end{figure*}

%\begin{figure*}[t]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=\textwidth]%{figures/fig_idea2.pdf}}
%\caption{\textbf{\alg}~general approach. A privately generated sequence of keyphrase is used to seed an LLM prompt for generating each synthetic text document (e.g., a medical record).\label{fig:method2}\\
%\\}
%\centerline{\includegraphics[width=\textwidth]{figures/fig_method}}
%\caption{\textbf{\alg}~detailed method overview. Color coding: \textcolor{methodred}{red -- private data}, \textcolor{methodpurple}{purple -- differentially privatized data (safe to release)}, \textcolor{methodgreen}{green -- public data}, \textcolor{methodblue}{blue -- public pre-trained model}. The pre-trained models are only used for inference, and on already privatized data.}\label{fig:flow}
%\end{center}
%\vskip -0.2in
%\end{figure*}

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/fig_idea2.pdf}}
\caption{\textbf{\alg}~general approach. A privately generated sequence of keyphrase is used to seed an LLM prompt for generating each synthetic text document (e.g., a medical record).\label{fig:method2}}
\end{center}
\vskip -0.2in
\end{figure*}

