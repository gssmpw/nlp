\section{Additional Method Details}

%\begin{figure*}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=1.0\textwidth]{figures/fig_vocab}}
%\caption{Generating a Private Vocabulary. The party that holds the private collection of documents selects a public dictionary as the basis for the vocabulary shown in the x-axis of the middle panel. This party then creates the the actual histogram of counts from their private documents (middle panel). In order to make the histogram differentially private, the party adds noise to the counts (last panel). Finally, the most popular keyphrases are chosen to winnow down the vocabulary. In the figure, the two most popular keyphrases are selected.}
%\label{fig:vocab}
%\end{center}
%\vskip -0.2in
%\end{figure*}

\subsection{DP-KPS Vocabulary Privatization}
The privatized vocabulary $\widetilde{\mathcal V}$ is generated as follows. 
%See Figure~\ref{fig:vocab}
\begin{enumerate}
 \item From each text document in $\mathcal{D}$, extract that first $S$ terms that appear in $\mathcal{V}$. %(Our implementation uses $S=10$.)
 \item Build a privatized histogram $\widetilde{\mathcal{H}}$ over $\mathcal{V}$ from the $S\cdot|\mathcal{D}|$ extracted terms, by adding an i.i.d.~sample from $\mathrm{Laplace}(S/\varepsilon_{\mathrm{voc}})$ to each count.
 \item The private vocabulary $\widetilde{\mathcal{V}}$ consists of the $N$ terms from $\mathcal{V}$ with the highest counts in $\widetilde{\mathcal{H}}$.
\end{enumerate}
It is easily seen that the $\ell_1$-sensitivity of the histogram is $S$, and therefore, the standard DP Laplace mechanism (cf.~\cite{dwork2014algorithmic}) ensures that $\widetilde{\mathcal{V}}$ is $\varepsilon_{\mathrm{voc}}$-differentially private.

\subsection{Details on Sequence Generation Methods}\label{sec:seqgen_appendix}
As discussed in \Cref{sec:method_main}, the main step of DP-KPS generates private sequences of keyphrases from $\widetilde{\mathcal{V}}$. We now detail the methods to do this with their computational parameters. 

Let $L$ be the desired number of keyphrases.  Our sequence generation methods are based on constructing a collection of suitable DP-KDE distributions over the private data, and generating private sequences from the associated DP-KDE scores. We use the high-dimensional DP-KDE mechanism from \cite{wagner2023fast}. 
For a private dataset of vectors $V\subset\R^d$ and desired accuracy $\alpha$, it builds the DP-KDE data structure in time $O(d|V|/\alpha^2)$, and then allows querying the KDE score of any point in $\R^d$ in time $O(d/\alpha^2)$ up to additive error $\alpha$. 
To ease notation, in what follows we treat $\alpha$ as a small constant, and suppress it in asymptotic running time bounds.

\noindent\textbf{Independent keyphrase generation.}
A simple way to privately generate a length-$L$ sequence is to build a single DP-KDE distribution over $\widetilde{\mathcal{V}}$ using all single keyphrases from $\mathcal{D}$, and then draw $L$ i.i.d.~samples from this distribution to produce a length-$L$ sequence. 
The preprocsessing time of this method is $O(d|\mathcal{D}|M+d|\widetilde{\mathcal{V}}|)$, where $M$ is the maximum number of keyphrases in a document, to first construct the DP-KDE data structure, and then query the density of each embedding in $\widetilde{\mathcal{V}}$. Then, a length-$L$ sequence can be generated in time $O(L)$ by drawing $L$ i.i.d.~samples from the induced multinomial distribution over $\widetilde{\mathcal{V}}$.

While this method is fast, independent keyphrase sampling may fail to capture correlations and dependencies between often co-occurring keyphrases, which may be important in downstream learning tasks. Therefore, we next consider sequence generation methods that preserve such correlations.

\noindent\textbf{Iterative sequence generation.}
Ideally, we would have liked to draw a sample from a DP-KDE distribution over all of $\widetilde{\mathcal{V}}^L$. However, computing the induced multinomial distribution over $\widetilde{\mathcal{V}}^L$ would require $|\widetilde{\mathcal{V}}|^L$ queries to the DP-KDE data structure, which is prohibitive even for moderate values of $|\widetilde{\mathcal{V}}|$ and $L$. 
Instead, drawing on intuition from conditional sampling and auto-regressive models, we generate a sequence by the iterative process described in \Cref{sec:method_main}.

This scheme performs only $L|\widetilde{\mathcal{V}}|$ DP-KDE queries overall ($|\widetilde{\mathcal{V}}|$ queries in each of the $L$ iterations), rendering its running time feasible. 
%
Yet, a hurdle toward implementing it is that the DP-KDE mechanism operates on vectors of a fixed dimension, while here we need to query vectors of varying dimensions (the length-$i$ prefixes $Pw$ in iteration $i$ have dimension $di$). 
We describe how to handle this by using an ensemble of DP-KDE data structures. 

\noindent\emph{Linear size DP-KDE ensemble.}
One way is to simply create $L$ DP-KDE mechanism $K_1,\ldots,K_L$, where $K_i$ operates on dimension $di$. 
In the above sequence generation scheme, iteration $i$ would make $|\widetilde{\mathcal{V}}|$ DP-KDE queries to $K_i$. 

We calculate the computational parameters of this method. The privacy budget $\varepsilon_{\mathrm{kde}}$ needs to be allocated among the $K_i$'s, so each is constructed with privacy parameter $\varepsilon=\varepsilon_{\mathrm{kde}}/L$. Since the error of the DP-KDE data structure from \cite{wagner2023fast} degrades with $\varepsilon$ like $\sqrt{1/\varepsilon}$, the error of each $K_i$ degrades by $\sqrt{L}$ (compared to using a single DP-KDE as Method I does). 
The overall running time is $O(dL^2|\mathcal D|M)$ for building tge DP-KDEs (each $K_i$ takes time $O(di|\mathcal D|M)$ to build), and $O(dL^2|\widetilde{\mathcal{V}}|)$ to generate every length-$L$ sequence. This is considerably more expensive than independent sequence generation, but has the potential advantage of preserving keyphrase correlations.

\noindent\emph{Logarithmic size DP-KDE ensemble.}
To improve the error and computational cost of the linear DP-KDE ensemble from \Cref{sec:method_main}, we also propose a method more frugal in the number of DP-KDEs. 
Suppose we construct just a single DP-KDE data structure $K$, over the full length-$L$ sequence dimension, $dL$. In iteration $i<L$, to retrieve the DP-KDE score of a $di$-dimensional prefix $x\in\R^{di}$, we pad $x$ with zero blocks into a $dL$-dimensional vector $\bar x\in\R^{dL}$, and query $K$ on $\bar x$. If the vectors over which $K$ is constructed are normalized---which is the case for many widely used pre-trained embedding models---then we prove that zero padding returns an accurate DP-KDE estimate even for prefixes. This is formalized in \Cref{thm:ensemblekde} below.

Ostensibly, it now suffices to build just one DP-KDE. However, a small hurdle is that different prefix lengths also require different kernel bandwidth settings when computing their KDE. This is slightly technical, and we give the details below, but the upshot is that if too many blocks in $\bar x$ are the result of zero padding, then its KDE score would not be informative even if we were to compute it exactly.

To remedy this, we build logarithmically many DP-KDEs, $K_1,\ldots,K_\ell$, with $\ell=\lceil \log L \rceil$. 
Each $K_j$ is built for vectors of dimension $d\cdot2^j$. 
In the iterative sequence generation scheme, iteration $i$ makes its queries to $K_{\lceil \log i\rceil}$, by zero-padding them from dimension $di$ to dimension $d\cdot2^{\lceil \log i\rceil}$, which is between $di$ to $2di$. This ensures that at most half the query blocks are the result of zero padding, thus ensuring thar the DP-KDE score is informative.

We calculate the computational parameters of this method. 
The privacy budget $\varepsilon_{\mathrm{kde}}$ is allocated among the $K_j$'s, so each is constructed with privacy parameter $\varepsilon=\varepsilon_{\mathrm{kde}}/\ell$ (recall that $\ell=O(\log L)$), and thus the DP-KDE error of each $K_j$ in the logarithmic ensemble degrades by $\sqrt{\log L}$ (compared to $\sqrt L$ in \Cref{sec:method_main}). The time to build each $K_j$ is $O(d\cdot 2^j|\mathcal D|M)$, hence the total building time is $O(dL|\mathcal D|M)$ (compared to $O(dL^2|\mathcal D|M)$ for the linear size ensemble). The time to generate a sequence is $O(dL^2|\mathcal D|M)$, similarly to \Cref{sec:method_main}. 
Thus, the logarithmic DP-KDE ensemble has better accuracy and building time compared to the linear ensemble, with the same sequence generation time.


To state and prove our result for logarithmic ensemble formally, we introduce some notation. 
We consider a collection of embedding vectors in $\R^d$, and assume they all have the same squared norm, $u>0$. 
Furthermore we are interested in sequences of length $L$ of such embedding, which are vectors in $\R^{dL}$ of uniform length $uL$. For such a vector $x\in\R^{dL}$, which is say the concatenation of $L$ vectors $x_1,\ldots,x_L\in\R^d$, we will denote its length-$\ell$ prefix, for every $\ell=1,\ldots,L$, by $x^{[:\ell]}$, and its remaining suffix by $x^{[\ell:]}$. 
Thus, $x^{[:\ell]}$ is the vector in $\R^{d\ell}$ given by the concatenation of $x_1,\ldots,x_\ell\in\R^d$, and $x^{[\ell:]}$ is the vector in $\R^{d(L-\ell)}$ given by the concatenation of $x_
{\ell+1},\ldots,x_L$. Note that $\norm{x^{[:\ell]}}_2^2=u\cdot\ell$, and $\norm{x^{[\ell:]}}_2^2=u\cdot(L-\ell)$

To prove our result, we define an \emph{asymmetric} Gaussian kernel, $k^{[:\ell]}:\R^{dL}\times\R^{d\ell}\rightarrow[0,1]$, by
\[ k^{[:\ell]}(x,y) = \exp(- 
 \norm{x^{[:\ell]}-y}_2^2). 
\]
Note that this kernel measures similarity between two spaces of different dimensionality: the first input $x$ is from $\R^{dL}$, and the second input $y$ is from $\R^{d\ell}$. Of course, its value coincides with the usual (symmetric) Gaussian kernel over $\R^d$ by truncating $x$ to dimension $d\ell$, but this asymmetric notation will be useful for us below in handling multiple prefix lengths simultaneously. 


Let $X\subset\R^{dL}$ by a given dataset. For every prefix length $\ell\in\{1,\ldots,L\}$, the induced KDE function on the length-$\ell$ prefixes of $X$ is given by
\[
 KDE_X^{[:\ell]}(y) := \frac{1}{|X|}\sum_{x\in X}k^{[:\ell]}(x,y)
\]
for every $y\in\R^{d\ell}$. In order to sequentially sample a length-$L$ sequence, we need to evaluate these KDE functions sequentially for each value of $\ell$. 

We start by recalling the formal DP KDE result from \cite{wagner2023fast}, adapted to our notation.
\begin{lemma}[Theorem 1.1 from \cite{wagner2023fast}]\label{thm:dpkde}
 Let $\varepsilon>0$ and $\alpha\in(0,1)$ be such that $|X|\geq O(1/(\varepsilon\alpha^2))$. Then, one can construct in time $O(|X|dL/\alpha^2)$ an $\varepsilon$-DP data structure for $KDE^{[:L]}_X$, such that for every $y\in\R^{dL}$, the value of $KDE^{[:L]}_X(y)$ can be reported in time $O(d/\alpha^2)$ with probability $0.99$ up to additive error at most $\alpha$. 
\end{lemma}

We extend this result to show that the same $\varepsilon$-DP data structure can in fact be used for all the KDE functions $\{KDE_X^{[:\ell]}:\ell=1,\ldots,L\}$ simultaneously, by padding any query $y\in\R^{d\ell}$ into $\bar{y}\in\R^{dL}$ by placing zeros the missing dimensions, and query the DP KDE data structure for $KDE^{[:L]}_X(\bar y)$. This what enables our Logarithmic DP KDE Ensemble to be frugal in the number of DP KDE data structures. 

\begin{theorem}\label{thm:ensemblekde}
 In notation of \Cref{thm:dpkde}, suppose that every vector in $X$ is the concatenation of $L$ $d$-dimensional vectors of squared norm $u$. Then, for every $\ell\in\{1,\ldots,L\}$, and for every $y\in\R^{d\ell}$ which is the concatenation of $\ell$ $d$-dimensional vectors of squared norm $u$, the $\varepsilon$-DP data structure for $KDE^{[:L]}_X$ from \Cref{thm:dpkde} can be used to report $KDE^{[:\ell]}_X(y)$ up to an additive error of $\alpha\cdot e^{2u(L-\ell)}$ with probability $0.99$, by querying $KDE^{[:L]}_X(\bar y)$, where $\bar y\in\R^{dL}$ is the result of zero-padding $y$. 
\end{theorem}
\begin{proof}
We recap the DP KDE construction of \cite{wagner2023fast}, which is based on Random Fourier Features \cite{rahimi2007random}. 
To construct the DP KDE data structure,
let $I=O(1/\alpha^2)$. For every $i=1,\ldots I$, 
sample $\omega_i\sim N(0,I_{dL})$ (a $dL$-dimensional vector of independent standard Gaussians), and a uniformly random $\beta_i\in[0,2\pi)$. Let $f_i:\R^{dL}\rightarrow[-\sqrt{2},\sqrt{2}]$ be defined as $f_i(z)=\sqrt{2}\cos(\sqrt{2}\omega_i^Tz+\beta_i)$, and let $g_i$ denote the same function as $f_i$ (the reason for this duplicate notation would become clear later, where our extention of this analysis would use different $f$ and $g$). Let $\tilde F_i(X)=\frac{1}{|X|}\sum_{x\in X}f_i(x)+\Lambda_i$, where $\Lambda_i\sim\mathrm{Laplace}(2\sqrt{2}I/\varepsilon)$. 
By the standard DP Laplace mechanism, the collection $\{\tilde F_i(X):i=1,\ldots,I\}$ is $\varepsilon$-DP and safe to release. 
Upon receiving a query $y\in\R^{dL}$, return $\frac{1}{I}\sum_{i=1}^I\tilde F_i(X)g_i(y)$ as the DP estimate for $KDE_{X}^{[:L]}(y)$. The analysis is based on the following lemma from \cite{wagner2023fast},
\begin{lemma}\label{lmm:dpkde}
If $\E[f_i(x)g_i(y)]=k^{[:L]}(x,y)$ for every $x,y\in\R^{dL}$ (where the randomness is over the sample of $\omega_i$ and $\beta_i$), the DP KDE estimate is accurate under the conditions of \Cref{thm:dpkde}.
\end{lemma}
Since \cite{rahimi2007random} showed that $\E[f_i(x)g_i(y)]=k^{[:L]}(x,y)$ for the above defined $f_i$ and $g_i$, it follows from \cref{lmm:dpkde} that the DP KDE estimate for $KDE_X^{[:L]}(y)$ is accurate.

Now, for some $\ell\in\{1,\ldots,L\}$, consider a prefix query $y\in\R^{d\ell}$ and its zero-padded completion $\bar y\in\R^{dL}$. Note that in our notation from above, $\bar y^{[:\ell]}=y$ and $\bar y^{[\ell:]}=0$. 
Consider what happens when we query the DP KDE structure for $\bar y$. For every fixed $i\in\{1,\ldots,I\}$ and $x\in X$, we have
\begin{align*}
 \E[f_i(x)g_i(\bar y)] = k^{[:L]}(x,\bar y) &= \exp(-\norm{x-\bar y}_2^2) & \text{by \cite{rahimi2007random}} \\
 &= \exp(-(\norm{x^{[:\ell]}-\bar y^{[:\ell]}}_2^2 + \norm{x^{[\ell:]}-\bar y^{[\ell:]}}_2^2)) & \\
 &= \exp(-(\norm{x^{[:\ell]}-y}_2^2 + \norm{x^{[\ell:]}}_2^2)) & \text{$\bar y^{[:\ell]}=y$ and $\bar y^{[\ell:]}=0$}\\
 &= \exp(-(\norm{x^{[:\ell]}-y}_2^2 + u(L-\ell))) & \norm{x^{[\ell:]}}_2^2=u(L-\ell) \\
 &= \exp(-\norm{x^{[:\ell]}-y}_2^2)\cdot \exp(-u(L-\ell)) & \\
 &= k^{[:\ell]}(x,y)\cdot \exp(-u(L-\ell)) . &
\end{align*}
Thus, if we now define $g_i^{(\ell)}:\R^{d\ell}\rightarrow\R$ as $g_i^{(\ell)}(y) = e^{u(L-\ell)} \cdot g_i(\bar y)$,
we get
\[ \E[f_i(x)g_i^{(\ell)}(\bar y)] = k^{[:\ell]}(x,y) . \]
In the terminology of \cite{wagner2023fast}, this means that $\{f_i,g_i^{(\ell)}\}$ is a $(1,e^{u(L-\ell)},1)$-LSQ family for the asymmetric kernel $k^{[:\ell]}$. \Cref{lmm:dpkde} now implies that the private KDE estimate we report is accurate up to an additive error of $\alpha\cdot e^{2u(L-\ell)}$, as the theorem claims (the $e^2{u(L-\ell)}$ blowup is due to the $e^{u(L-\ell)}$ term in the middle LSQ parameter; see Lemma 2.5 in \cite{wagner2023fast}). 
This holds for every $\ell=1,\ldots,L$. Furthermore, the left-function $f_i^{\ell}$ in the LSQ family is the same ($f_i$) for every $\ell$, and the privatized values released by the DP KDE mechanism, $\{\tilde F_i(X):i=1,\ldots,I\}$, depend only the left function. Therefore, the same DP KDE mechanism can be used simultaneously for all the KDE functions $KDE^{[:\ell]}_X$, i.e., for all subsequence lengths $\ell=1,\ldots,L$.
\end{proof}

\noindent\emph{Bandwidth selection.}
We now explain how to deal with the error blowup $e^{2u(L-\ell)}$ incurred in \Cref{thm:ensemblekde}. 
This has to do with the notion of bandwidth in KDE. 
Normally, the Gaussian KDE is defined with a bandwidth parameter $\sigma>0$, as $KDE_X(y)=\frac{1}{|X|}\sum_{x\in X}\exp(-\norm{y-x}_2^2/\sigma^2)$. 
Note that selecting a bandwidth $\sigma$ is equivalent to multiplying all vectors $X$ and $y$ by the scalar $\sigma$. 
Thus, the purpose of the bandwidth is to offset the length of the input vectors, and ensure that the KDE function around any point in $X$ decays not too fast and not too slowly, rendering its density scores meaningful. 

In the case of a pre-trained embedding model that returns unit-length embeddings (like many commonly used ones do), one simply sets the bandwidth to $1$. In our case, when we deal with the concatenation of $\ell$ unit-length embeddings, the Euclidean norm of the concatenation is $\sqrt{\ell}$, and thus we wish to set the bandwidth to $\sigma=1/\sqrt\ell$. This is equivalent to setting $u=1/\ell$ in the notation of \Cref{thm:ensemblekde} (recall that $u$ is the squared Euclidean norm of every $d$-dimensional block in the concatenation).

Thus, to get the desired bandwidth for prefix length $\ell$ (i.e., to ger meaningful density scores from $KDE^{[:\ell]}_X$), ideally we would normalize the given $d$-dimensional embeddings to length set $u=1/\ell$. 
%
However, if we use a single DP KDE data structure, we can only choose one setting of $u$ for all values of $\ell$. This is the motivation for using logarithmically many DP KDE data structures, leading to the logarithmic ensemble from \Cref{sec:method_main}. 
To explain this, suppose we build a DP KDE data structure for $KDE^{[:L]}_X$, and set $u=2/L$. Let $\ell$ be such that $\ell\geq L/2$. 
On the one hand, the ideal setting of $u$ for $\ell$ would have been $1/\ell$, while the actual setting $2/L$ is in $[1/\ell, 2/\ell]$. Thus, the bandwidth is off by only a constant, which suffices for the KDE density scores to still be meaningful. 
On the other hand, the error blowup $e^{2u(L-\ell)}$ is now at most $e^2$, i.e., \Cref{thm:ensemblekde} incurs only a constant blowup in the error. Hence, if we pad no more than half of the blocks of a KDE query with zeros, we get both approximately the desired bandwidth and the desired error blowup. To achieve this for every $\ell$, we build $O(\log L)$ many DP KDE data structures, such that for every $\ell$ we can query one that requires no more than doubling the dimension of the query by zero padding, as described in detail earlier in this section. This is the motivation for the Logarithmic Ensemble, and this is how it achieves private and accurate DP KDE estimates for all prefix lengths with a logarithmic number of KDE data structures.

\section{Additional Experiments}\label{sec:experiments_appendix}

\subsection{Experimental Details}
\subsubsection{Medical Task Details}\label{app:mimic}
\noindent\textit{Groundtruth labels.}
As the groundtruth labels for the binary classification task on MIMIC, we consider as the positive class all medical records whose billing code includes an ICD code associated with heart failure. These are the codes 428.x (for any x) in ICD-9,\footnote{\url{http://www.icd9data.com/2012/Volume1/390-459/420-429/428/default.htm}}
and the codes I50.x in ICD-10.\footnote{\url{https://www.icd10data.com/ICD10CM/Codes/I00-I99/I30-I5A/I50-}}

\noindent\textit{Clinical Vocabulary.}
Physicians express the same concept in different ways in clinical notes. For example, the term hypertension may be expressed as ‘high blood pressure’ or ‘elevated blood pressure’. Abbreviations are also common such as ‘htn’ or ‘hbp’. Ideally, we would treat these terms as the concept. Fortunately, the UMLS has a meta-thesaurus with 3.2M English medical concepts. Each concept, known as a CUI, has a set of keyphrases associated with it.
%
To limit the vocabulary size, we chose a subset of common concepts based on SemMedDB \cite{kilicoglu2012semmeddb}, a relational graph over these CUIs based on the PubMed database. The graph contains relationships such as ``‘Lisinopril’ is a medication for ‘high blood pressure’''. We chose concepts that were supported by at least 10 PubMed papers. This resulted in 386,725 CUIs that formed the final public vocabulary for our MIMIC experiments. 


\subsubsection{Domain Adaptation Details}\label{sec:deep_coral_details}
For the domain adaptation step in our implementation of \alg, we use the Deep CORAL method of \cite{sun2016deep}. We now describe it in more detail. 

Recall the setting: Hospital A has generated a privacy-preserving synthetic corpus $D_A$ of labeled texts, which is safe for release, and has sent it to Hospital B. Hospital B now intends to train a downstream ML model $\mathcal{M}$ on $D_A$ for some specific task (say, classification), and then use it for inference on its own unlabeled text corpus, $D_B$. However, since the texts in $D_A$ might have different characteristics than $D_B$ (e.g., a different formats of medical records), domain adaptation is needed in order to use $D_A$ as the training set for $\mathcal{M}$ and yet get good inference accuracy on $D_B$. 

Let $\ell_{\mathcal M}$ be the loss used to train $\mathcal{M}$ (say, classification loss). Deep CORAL modifies $\ell_{\mathcal M}$ into a new loss $\ell_{\mathcal M}'$ by adding an additive domain adaptation loss term, defined as $1/(4d^2\norm{C_A-C_B}_F^2)$, where $C_A$ and $C_B$ are the $d\times d$ covariance matrices constructed from $D_A$ and $D_B$ respectively. $\mathcal{M}$ is trained with the modified loss $\ell_{\mathcal M}'$ instead of the original loss $\ell_{\mathcal M}$. See \cite{sun2016deep} for further details.

This method requires Hospital B to allocate a set $D_B$ of its medical records for training  $\mathcal{M}$. The size we set for $D_B$ is half the size of $D_A$ (thus, $|D_B|$ contains 7k texts for DBPedia-14, and 3k texts for MIMIC-IV). The test set on which we evaluate the trained model $\mathcal{M}$ for the accuracy results we report is kept separate from the set $D_B$ used for training $\mathcal{M}$ with domain adaptation, to ensure $\mathcal{M}$ has no access to any test records during training. 


\subsubsection{AugPE Details}\label{sec:augpe_details}
In all invocations of AugPE, we use 10 epochs, similarly to \cite{api2}. Thus, AugPE uses 10 sequential prompts to the LLM per synthetic text generated. To vary the prompt budget of AugPE (or AugPE-$\widetilde{\mathcal{V}}$), we generate as many synthetics texts as possible under the budget in 10 epochs (thus, with a prompt budget $B$, we generate $B/10$ synthetic documents with AugPE). 
We have also run experiments varying AugPE's prompt budget by using less epochs (which allows generating more documents with the same prompt budget; namely, with a prompt budget $B$ and $M<10$ epochs, one can generate $B/M$ synthetic texts). However, this led to degraded performance compared to fixing the number of epochs to 10 as above---which stands to reason, since evolution over epochs is the key idea in AugPE---and thus we report results with the better variant of the algorithm.

%Note that no private clinical records were used to select the vocabulary – just published PubMed papers. But the density associated with each CUI was based on a private kernel density estimate.

%Public data was used to generate a clinical vocabulary. However,

\subsection{Additional Experimental Results}

%\subsection{Ablations}
%\subsection{Post-Processing and Domain Adaptation}
%\noindent\textbf{Domain adaptation.}
\subsubsection{Ablation: Domain Adaptation}
\Cref{tbl:domainadaptation} shows that when domain adaptation (see \Cref{sec:method_main}) is removed from \alg, downstream accuracy is significantly degraded.

\begin{table*}
% \renewcommand{\arraystretch}{2}
\caption{Results with and without domain adaption (abbrev.~DA).} \label{tbl:domainadaptation}
\centering
\begin{tabular}{c c c c c}
\toprule
 & \multicolumn{2}{c}{MIMIC} & \multicolumn{2}{c}{DBPedia-14} \\
 $\varepsilon_{\mathrm{total}}$ & with DA & w/o DA & with DA & w/o DA \\
\midrule
\rule{0pt}{3ex}
6 & 71.6\% & 58.7\% & 77.9\% & 72.8\% \\
10 & 71.7\% & 58.4\% & 80.4\% & 77.1\% \\
11 & 72.2\% & 55.0\% & 82.4\% & 77.0\% \\
15 & 72.2\% & 60.3\% & 85.1\% & 81.2\% \\
\bottomrule
\end{tabular}
\end{table*}

\subsubsection{Ablation: No LLM}\label{sec:nollm}
%\vspace{-8pt}
%\noindent\textbf{No LLM.}
We consider the ablation of eliminating the LLM from \alg. 
This pertains to the question of what is the role of the LLM in the synthetic text generation pipeline. Clearly, it plays a role in \emph{form}, by incorporating the sequences of isolated phrases into natural language. However, potentially, it could also play a role in \emph{function}: either by augmenting and enriching the synthetic texts with knowledge about the world from its pre-training data, which could assist downstream tasks, or conversely, by adding ``noise'' that would only interfere. 
Note that is it generally not at all clear (and may depend on the specific context) whether it is desirable or not for the LLM to affect the output texts beyond their form. For example, in medical contexts, generic information about the general population from the pre-training data might obscure predictive attributes in datasets that target specific sub-populations.

To test this, we run downstream classification directly on the generated phrases sequences, instead of using them to seed an LLM prompt. Each test record is also turned into a (non private) sequence of phrases from the vocabulary, to be compatible in form with the raw sequence training set. 

The results are in \Cref{tbl:nollm}. They show that on DBPedia-14, the accuracy remains similar with or without the LLM. On MIMIC, the classification accuracy of the sequences is in fact better. However, this gap might not be due to the LLM, since the same gap is observed between classifying the original texts versus classifying the sequences extracted from them (the final lines in \cref{tbl:mainres,tbl:nollm},
respectively). 
The reason is that medical records are long and packed with a lot of free text, and classification becomes easier once key medical terms from a standardized glossary (UMLS) are extracted from the text. To summarize, we do not observe evidence that the LLM substantially affects downstream accuracy either positively or negatively, and the role it plays seems to be primarily in the form of the output texts.

%suggesting that it only plays a role in form, and does not substantially affect function either positively or negatively. 
%Note that in the context of DBPedia-14, this may suggest that the LLM indeed does not retrieve the Wikipedia articles corresponding to the dataset records from its pre-training set.

\begin{table*}
% \renewcommand{\arraystretch}{2}
\caption{Classification accuracy on the generated phrase sequences, without running them through the LLM.} \label{tbl:nollm}
\centering\begin{tabular}{c c c c c}
\toprule
$\varepsilon_{\mathrm{total}}$ & $\varepsilon_{\mathrm{voc}}$ & $\varepsilon_{\mathrm{kde}}$ & MIMIC & DBPedia-14 \\
\midrule
\rule{0pt}{3ex}
6 & 1 & 5 & 75.1\% & 71.7\% \\
10 & 5 & 5 & 74.9\% & 81.5\% \\
11 & 1 & 10 & 75.5\% & 80.6\% \\
15 & 5 & 10 & 75.9\% & 84.2\% \\
\midrule
\multicolumn{3}{c}{Original training set} & 80.0\% & 85.2\% \\
\bottomrule
\end{tabular}
\end{table*}

\subsubsection{Ablation: Only LLM}
%\vspace{-10pt}
%\noindent\textbf{Only LLM.}
The flipside of the previous ablation is cutting out everything \emph{but} the LLM from the pipeline, not using private data at all. Perhaps the LLM has enough knowledge from its pre-training set to generate the requisite texts for the downstream task, without needing any private data?

To test this, we prompted the LLM without seeding to generate documents for each class of each dataset, and ran downstream classification on the generated texts. Note that here, unlike our \alg\ experiments, we did include the intended class in the prompt (say, ``generate a medical record of a patient with a heart condition''), since without phrase seeding, there is no other input for the prompt to rely on. 

The resulting texts were extremely non-diverse (e.g., all ``mean of transportation'' articles were about Honda Civic). 
The downstream classification accuracy was 57\% on MIMIC and 60\% on DBPedia-14. While non-trivial, these accuracies fail to match those based on \alg\ seeded prompts. 
This result aligns with and corroborates a similar finding in \cite{eldan2023tinystories}, discussed in \Cref{sec:intro_method}.
%
%We remark that lack of diversity in the LLM outputs for the unseeded prompts, and their poor performance on downstream, corroborate and essentially replicate a similar finding from \cite{eldan2023tinystories}, as discussed in \Cref{sec:intro_method}. It also corroborates the motivation of \cite{cohen2023hot} to rigorously study the interplay between differential privacy and generated output diversity. 

\subsubsection{Sequence Generation Method}

We compare independent versus iterative sequence generation (see \Cref{sec:method_main}). The results vary substantially on the two datasets: while on MIMIC independent generation works much better, the situation is reversed on DBPedia-14. There are a few possible causes: one is that MIMIC medical records are long and detailed, and cover many medical aspects not directly related to each other nor to the classification task, while DBPedia-14 entries are focused and concise. Another is that the MIMIC dataset is much smaller, perhaps too small to display strong correlations between longer tuples of phrases (particularly in the presence of DP noise). The results imply that the choice between the method is dataset-dependent, and is best done via validation. Note that validation does not require costly interaction with the LLM, and can be done cheaply directly on the generated sequences, as done in the experiment in \Cref{tbl:nollm}.

\begin{table}
\caption{Independent vs.~iterative phrase generation.} \label{tbl:iidvsiter}
  \centering
\begin{tabular}{c c c c c}
\toprule
 & \multicolumn{2}{c}{MIMIC} & \multicolumn{2}{c}{DBPedia-14} \\
 $\varepsilon_{\mathrm{total}}$ & ind. & iter. & ind. & iter. \\
\midrule
\rule{0pt}{3ex}
6 & 71.6\% & 51.3\% & 30.2\% & 77.9\% \\
10 & 71.7\% & 61.5\% & 26.3\% & 80.4\% \\
11 & 72.2\% & 55.3\% & 29.0\% & 82.4\% \\
15 & 72.2\% & 55.5\% & 27.1\% & 85.1\% \\
\bottomrule
\end{tabular}
\end{table}



\subsubsection{Few-Shot Prompting}\label{app:fewshot}

As mentioned in \Cref{sec:method_main}, the privatized documents generated are intended to be used by a client for downstream machine learning tasks on their own set of documents. The synthetic documents that are produced by the LLM might look different compared to the test set. While one way to overcome this is through domain adaptation as described in \Cref{sec:deep_coral_details}, another approach is through few-shot prompting. 
%
In this approach, the client provides a few examples from their dataset in the desired format. These examples are then included in the prompt along with the synthetic keyphrases given to the LLM. The idea here is to nudge the LLM to generate documents that are aligned with the client's test set. 

We explore this post-processing method on the MIMIC and DBPedia-14 text classification tasks. We provide six example texts (3 from each class) from the dataset along with their corresponding keyphrases in the prompt. We ensure that these few-shot reports are not in the test-set used in our experiments. We compute the downstream task performance of a classifier trained on the synthetic data with few-shot prompting. The downstream results using the different combinations of post-processing methods are shown in \Cref{tbl:fs_results_mimic} and \Cref{tbl:fs_results_dbpedia}. We find that few-shot prompting along with domain adaptation results in the best downstream performance. 

To gauge the effect of few-shot prompting qualitatively, we visualize the t-SNE plot of the BioBERT embeddings of the real MIMIC data, the synthetic data (DP-KPS outputs w/o DA) and the synthetic data with few-shot prompting in \Cref{fig:tsne_biobert}. Our intuition for using few-shot prompting was to make the styles of the real and synthetic data similar. We see in \Cref{fig:tsne_biobert} that few-shot prompting brings the embeddings closer to the real data. 
 
\begin{table*}
\caption{Accuracy of a downstream classifier trained on DP-KPS outputs on MIMIC with different combinations of post-processing methods. These results are for the setting where $\varepsilon_{\mathrm{voc}}=1$ and $\varepsilon_{\mathrm{kde}} = 5$ } \label{tbl:fs_results_mimic}
\centering
\begin{tabular}{c c c}
\toprule
Domain adaptation & Few-shot prompting & Accuracy \\
\midrule
\rule{0pt}{3ex}
{\text{\sffamily X}} & {\text{\sffamily X}} & 58.7\% \\
\checkmark & {\text{\sffamily X}} & 71.6\% \\
{\text{\sffamily X}} & \checkmark & 71.0\% \\
\checkmark & \checkmark & 72.37\% \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}
\caption{Accuracy of a downstream classifier trained on DP-KPS outputs on DBPedia with different combinations of post-processing methods. These results are for the setting where $\varepsilon_{\mathrm{voc}}=1$ and $\varepsilon_{\mathrm{kde}} = 10$ } \label{tbl:fs_results_dbpedia}
\centering
\begin{tabular}{c c c}
\toprule
Domain adaptation & Few-shot prompting & Accuracy \\
\midrule
\rule{0pt}{3ex}
{\text{\sffamily X}} & {\text{\sffamily X}} & 77.0\% \\
\checkmark & {\text{\sffamily X}} & 82.4\% \\
{\text{\sffamily X}} & \checkmark & 79.6\% \\
\checkmark & \checkmark & 83.3\% \\
\bottomrule
\end{tabular}
\end{table*}



\begin{figure}
 \centering
 \includegraphics[scale=0.45]{figures/tsne_mimic.png}
 \caption{t-SNE plot of the BioBERT embeddings of real MIMIC data, synthetic data (DP-KPS output w/o DA) and synthetic data with few-show prompting.}
 \label{fig:tsne_biobert}
\end{figure}





\subsection{Examples of Generated keyphrase Sequences and LLM Outputs}
\Cref{tbl:example2,tbl:example1full} complete \Cref{tbl:example1} by showing positive and negative examples for all classes of the DBPedia-14 dataset. 

\begin{table*}
\caption{\textcolor{goodgreen}{Good} and \textcolor{badred}{bad} examples for classes 1--7 in DBPedia-14, with $\varepsilon_{\mathrm{voc}}=1$ and $\varepsilon_{\mathrm{kde}}=10$.} \label{tbl:example1full}
{\renewcommand{\arraystretch}{2}
\begin{centering}
\setlength\extrarowheight{-3pt}
\small
\begin{adjustwidth}{-0.5in}{-0.1in}
\begin{tabular}{p{0.08\linewidth}p{0.92\linewidth}}
\toprule
 Class & \alg\ results: Keyphrase sequence and corresponding LLM output \\
\midrule
Company &

['plants', 'company', 'distribution', 'operating', 'social', 'ships', 'andrew', 'based', 'michigan', 'york']: \newline 
\textcolor{goodgreen}{Plants Distribution Company is a Michigan-based company operating ships to distribute plants and other goods. Founded by Andrew York in New York, the small social enterprise distributes products across the region.} \\
&
['director', 'institution', 'royal', 'schools', 'municipality', 'club', 'journal', 'johnson', 'office', 'oil']: \newline 
\textcolor{badred}{John Johnson was appointed director of the Royal Schools institution in the municipality in 2020. He previously worked in the office of the Oil Club journal.} \\
\midrule

Educational institution
&
['1908', 'school', 'school', 'college', 'institute', 'school', 'germany', 'university', 'college', 'manufacturing']: \newline 
\textcolor{goodgreen}{The College Institute was founded in Germany in 1908 as a manufacturing school. Originally a vocational college, it later became a university specializing in engineering and technology. The school focuses on hands-on training in manufacturing techniques.} \\
&
['scott', 'school', 'brazilian', 'guard', 'sciences', '1996', 'high', 'lee', 'mystery', 'england']: \newline 
\textcolor{badred}{Scott Lee was born in England in 1996. He attended a Brazilian jiu-jitsu school in high school where he learned grappling and the martial art of Brazilian jiu-jitsu. The mystery sciences behind the techniques fascinated Scott during his time there.}\\
\midrule

Artist &

['species', 'february', '1944', 'european', '1929', 'music', 'musician', 'duo', 'frank', 'light']: \newline 
\textcolor{goodgreen}{The musical duo Frank \& Light was formed in February 1944 in Europe by Frank Smith and Light Williams. The duo, known for their unique blend of jazz and classical music, released their debut album "Species of Music" in 1929 to critical acclaim. Though popular in the late 1940s, they disbanded as a musical act in the early 1950s.} \\

& ['england', 'june', '1972', '2003', 'english', 'animated', 'nonfiction', 'publisher', 'utah', 'november']: \newline 
\textcolor{badred}{In November 2003, an English animated nonfiction publisher based in Utah released a book about events that took place in England in June 1972. The book was well-received upon its November 2003 publication. }\\

\midrule
 Athlete &

['early', 'born', '1966', '1970', 'class', 'professional', 'playing', 'plays', 'lee', 'brown']: \newline 
\textcolor{goodgreen}{Lee Brown was born in 1966 and grew up playing sports from an early age. He was a standout athlete in high school and went on to play professionally in the late 1980s and early 1990s before retiring in his late 20s. After retiring from playing, Brown began coaching youth sports and teaching physical education classes. } \\

&


['movie', 'october', '1988', 'irish', 'british', 'footballer', 'league', 'attack', '1951', 'formed']: \newline 
\textcolor{badred}{The 1988 Irish movie October follows a fictional British footballer in the Irish league who was attacked in 1951. The footballer then forms a new team in October 1988.} \\


\midrule
Office holder &

['china', 'march', '1948', '1999', 'politician', 'canadian', 'national', 'leader', 'anthony', 'district']: \newline 
\textcolor{goodgreen}{In March 1948, Anthony, a Canadian politician and national leader, was born in a district of China. In 1999, he became the leader of his district.} \\
&
['chapel', 'january', 'november', '1947', 'kentucky', 'senior', 'year', 'journalist', 'democratic', 'hop']: \newline 
\textcolor{badred}{The Little Chapel was founded in January 1947 in Kentucky by a senior journalist from the Democratic Party who hoped it would last. The Little Chapel opened its doors in November 1947 but closed within a year due to lack of funding. The Little Chapel story was covered by the local Kentucky paper in 1947. } \\



\midrule
 Mean of transportation &

['malayalam', 'mexican', 'ocean', 'car', 'submarine', 'finnish', 'frigate', 'called', 'sea', 'union']: \newline 
\textcolor{goodgreen}{The Malayalam is a Mexican frigate that operates in the Pacific Ocean. The sea vessel, called the Union, carries specially designed submarines and cars for traversing the ocean floor. The Finnish captain aims to better understand deep sea life. }\\
&
['girls', 'ship', 'oil', 'automobile', 'navy', 'massachusetts', 'ships', 'islands', 'military', 'wales']: \newline 
\textcolor{badred}{The girls were on a ship transporting oil to Wales when it was attacked by the Massachusetts navy near some islands. The military ships fired on the automobile transport ship until it sank.}
\\



\midrule
 Building
&
['career', 'museum', 'tower', 'representing', 'headquarters', 'residence', 'japanese', 'elected', 'music', 'forest']: \newline 
\textcolor{goodgreen}{The Japanese music museum's tower, located in a forest, was the former residence and career headquarters of an elected official representing Japanese culture before becoming a museum. The tower in the forest now serves as the music museum's main headquarters. }\\
&
['mexico', 'city', 'headquartered', 'west', 'chapel', 'news', 'game', 'alpine', 'northwest', 'comprehensive']: \newline 
\textcolor{badred}{The city of Alpine is located in northwest Mexico and is the headquarters of West Chapel News, a company that publishes a comprehensive game and news website. }\\

\bottomrule
\end{tabular}
\end{adjustwidth}
\end{centering}}
\end{table*}

\begin{table*}
\caption{\textcolor{goodgreen}{Good} and \textcolor{badred}{bad} examples for classes 8--14 in DBPedia-14, with $\varepsilon_{\mathrm{voc}}=1$ and $\varepsilon_{\mathrm{kde}}=10$.} \label{tbl:example2}
{\renewcommand{\arraystretch}{2}
\begin{centering}
\setlength\extrarowheight{-3pt}
\small
\begin{adjustwidth}{-0.5in}{-0.1in}
\begin{tabular}{p{0.08\linewidth}p{0.92\linewidth}}
\toprule
 Class & \alg\ result: Keyphrase sequence and corresponding LLM output \\
\midrule
Natural place &

['1955', 'mountain', 'lakes', 'province', 'carolina', 'north', 'north', 'jersey', 'maryland', 'region']: \newline 
\textcolor{goodgreen}{In 1955, the mountain lakes region spanning parts of North Carolina, North Jersey, and Maryland was designated as a new province. This area contains beautiful mountain lakes and forests, and the region attracts many tourists.} \\
&
['presbyterian', 'mountain', 'swimmer', 'east', 'forest', 'rapper', 'lakes', 'international', 'arts', 'india']: \newline 
\textcolor{badred}{The Presbyterian Mountain Swimmer is an international rapper and performance artist known for swimming across lakes and forests in East India. His arts performances often address social issues in local mountain communities near the lakes and forests where he swims.} \\
\midrule

Village
&
['2009', 'village', 'atlantic', 'spring', 'southern', 'folk', '1986', 'area', 'female', 'big']: \newline 
\textcolor{goodgreen}{The village of Big Spring is located in the southern area of the Atlantic coast. Founded in 1986, it has become a popular tourist destination known for its folk music scene. In 2009, a local female musician released a popular album that brought more visitors to the village.} \\
&
['1988', 'district', 'pass', 'general', 'manufacturing', 'power', '1953', 'landing', 'leader', 'cemetery']: \newline 
\textcolor{badred}{In 1953, the district leader passed a general manufacturing power bill to allow more factories to be built in the area. However, residents complained about pollution from the new factories near the local cemetery. So in 1988, the district passed new restrictions on factory emissions to address residents' concerns over poor air quality.}\\
\midrule

Animal &

['martin', 'species', 'african', 'patrol', 'creek', 'portuguese', 'asia', 'ukrainian', 'plants', '1927']: \newline 
\textcolor{goodgreen}{The African creek martin (Riparia africanus) is a species of small insectivorous bird in the swallow family. First described in 1927 by a Portuguese naturalist, the creek martin nests in burrows dug into the banks of creeks and rivers across sub-Saharan Africa. Though once abundant, habitat loss has caused populations to decline, leading conservation groups like the Ukrainian Wildlife Patrol to list the species as near threatened.} \\

& ['feet', 'species', 'argentine', 'public', 'metal', 'russian', 'bay', 'lakes', 'governor', 'wild']: \newline 
\textcolor{badred}{The public metal sculptures in Lakes Bay have become a popular tourist attraction. Featuring over 50 feet tall structures depicting various species, the sculptures were a gift from the Russian governor to the people of Argentine. However, some environmental groups have expressed concern that the wild installations could negatively impact the local wildlife.}\\

\midrule
 Plant &

['plant', 'flowering', 'plant', 'family', 'common', 'danish', 'northwestern', 'india', 'norway', 'church']: \newline 
\textcolor{goodgreen}{The flowering plant family Commonaceae is commonly found in northwestern India, Norway, and other Northern European countries. The plant is known for its use in traditional medicine and as decoration in churches across Denmark and northern India.} \\

&


['june', 'plant', 'plant', 'fish', 'czech', 'ohio', 'multinational', 'western', 'east', 'representing']: \newline 
\textcolor{badred}{In June 2023, a multinational corporation headquartered in Ohio announced plans to build a new manufacturing plant in the Czech Republic, representing the company's expansion into Eastern Europe. The plant, to be located west of Prague, will produce parts used in automotive and aerospace industries. Environmental activists raised concerns about the plant's potential impact on local fish populations in nearby rivers.} \\


\midrule
Album &

['pass', 'album', 'album', 'released', 'duo', 'release', 'album', 'early', '1999', '2004']: \newline 
\textcolor{goodgreen}{The album Pass was released in early 1999 by the duo. This was their first album release, coming five years after they formed in 2004. The album contains 10 songs in the pop genre.} \\
&
['operated', 'album', 'album', 'distributed', 'hits', 'title', 'music', 'michael', 'shrub', 'lee']: \newline 
\textcolor{badred}{Michael Shrub is an American musician who released the album "Evergreen Hits" in 2022, which was distributed by Lee Records. The album contains several popular songs that showcase Shrub's smooth vocals and guitar playing.} \\



\midrule
Film &

['published', '1966', 'film', 'norwegian', 'director', 'stage', 'food', 'light', 'sports', 'production']: \newline 
\textcolor{goodgreen}{The 1966 Norwegian film "Bright Lights" was directed by famous stage director Lars Berg and produced by Nordic Productions. The sports comedy film about competitive eating features innovative lighting and sumptuous food.}\\
&
['named', '1966', 'film', 'swedish', 'actress', 'operates', 'animated', 'lit', '1986', 'music']: \newline 
\textcolor{badred}{The Swedish actress named Greta was born in 1966 and first operated an animated film lit in 1986. She later began making music.}
\\



\midrule
 Written work

&
['comic', 'english', 'nonfiction', 'newspaper', '1965', 'institute', '1992', 'documentary', 'jack', '1990']: \newline 
\textcolor{goodgreen}{The English comic strip Jack debuted in 1965 in a nonfiction newspaper published by the Institute. The comic ran until 1992 and was known for its documentary-style depiction of everyday life. In 1990, the Institute published a documentary about the 25-year history of the Jack comic strip.}\\
&
['songwriter', 'language', 'period', 'greater', 'louisiana', 'privately', 'literary', 'france', 'multinational', 'ireland']: \newline 
\textcolor{badred}{John Smith (songwriter) was a privately educated language teacher from Ireland who moved to France during the greater Louisiana Purchase period. He wrote literary works while working for a multinational corporation before returning to Ireland. }\\


\bottomrule
\end{tabular}
\end{adjustwidth}
\end{centering}}
\end{table*}


