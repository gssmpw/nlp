\section{Related Work}
\label{sec:related}

%\noindent\textbf{Differentially private training and fine-tuning.}

Recent years have seen ample and successful work on DP training of large deep generative models, including **Abadi et al., "Deep Learning with Differential Privacy"**, with some work specifically on private text generation (e.g., **McMahan et al., "Learning Differentially Private Recurrent Language Models"**).
While effective, these methods generally require the ability to train (or fine-tune) the generative model, and allow it to directly access the private data. In our setting, where the data owner has only inference access to the LLM, and is not allowed to feed it the private data in the clear, these methods are not applicable. The line of work of **Inoue et al., "Differentially Private Text Generation"** is closer to ours, in that the generative model is trained only on data which is already privatized. However, the method still requires training the model, and with a specially designed objective function, which is inapplicable to LLMs. 

Private Aggregation of Teacher Ensembles (PATE) **Papernot et al., "Private Aggregation of Teachers for Partially Labelled Data"** is a prominent paradigm for DP machine learning, with recent extensions specialized to generative models **Li et al., "Differentially Private Text Generation via PATE"** and LLMs 
**Wu et al., "Private Adversarial Training for Large Language Models"**. It is based on private knowledge transfer from the private dataset to public data, which can then be used to train models without further privacy loss. The recent PromptPATE method **Jordon et al., "PromptPATE: Private Aggregation of Teacher Ensembles with Prompts"** realizes this paradigm in a way that only accesses an LLM with already privatized prompts, rendering it similar in spirit to ours. However, like prior PATE methods, it requires access to public data of a similar domain (though possibly differently distributed), to which private knowledge can be transferred.
In certain application contexts such data may be unavailable, for example in medical applications, where even public datasets released for research purposes are scarce and highly regulated. 

%a key limitation of this method (as well as prior PATE methods) is that it requires an available public dataset sufficiently similar to the private one, to which private knowledge can be transferred. In many scenarios, such public data is not necessarily available.

Most directly related to us is the recent concurrent work on Augmented Private Evolution (AugPE) for DP text generation**Xu et al., "Differentially Private Text Generation via Augmented Private Evolution"**, adapting a precursor work for private images**Goodfellow et al., "Private Image Synthesis with Generative Adversarial Networks"**. It addresses the same problem setting as ours. Our experimental section thoroughly compares the two methods. 

%Two very recent/concurrent works seem closest to ours. One is Private Evolution (PE) **Zhu et al., "Private Evolution for Differentially Private Text Generation"**, which also studied generating synthetic DP data with interface-only access to a pre-trained generative model, and suggested a method that ``evolves'' the pre-trained generative distribution toward the private data by iterative prompts. However, they instantiate and test their method only for image generation via pre-trained diffusion models. The other relevant work is HotPATE **Jordon et al., "HotPATE: Private Aggregation of Teacher Ensembles with Prompts"**, which extends PATE to privately learning a distribution over responses, by iteratively prompting a pre-trained LLM with prompts constructed from the private data. However, their prompts are not privatized (rather, their focus is on aggregating an ensemble of private distributions while preserving both privacy and diversity), and thus their method allows the LLM direct access to the private data, which we consider prohibited. 

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/fig_method}}
\caption{\textbf{\alg}~detailed method overview. Color coding: \textcolor{methodred}{red -- private data}, \textcolor{methodpurple}{purple -- differentially privatized data (safe to release)}, \textcolor{methodgreen}{green -- public data}, \textcolor{methodblue}{blue -- public pre-trained model}. The pre-trained models are only used for inference, and on already privatized data.}\label{fig:flow}
\end{center}
\vskip -0.2in
\end{figure*}