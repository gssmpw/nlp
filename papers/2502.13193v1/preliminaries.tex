\subsection{Preliminaries}\label{sec:prelim}

\noindent\textbf{Differential Privacy.}
%We review the definition of DP \cite{dwork2006calibrating}. 
Let $\mathbb D$ be a ``universe'' of data records. Two datasets $\mathcal{D},\mathcal{D'}\subset\mathbb D$ are called \emph{adjacent} if one is obtained from the other by dropping a single record. 
Let $M$ be a randomized algorithm that takes a dataset $\mathcal{D}\subset\mathbb D$ as input. $M$ is said to be \emph{$(\varepsilon,\delta)$-DP} if for every adjacent $\mathcal{D},\mathcal{D'}\subset\mathbb D$, and every set $\mathcal S$ of possible inputs of $M$, it holds that
\[ \Pr[M(\mathcal{D})\in \mathcal S] \leq e^\varepsilon \Pr[M(\mathcal{D'})\in \mathcal S] + \delta . \]
Intuitively, the output of $M$ is insensitive to the inclusion of any single data record in the input dataset, and thus information about any single record should not be possible to glean from its output. 
%
In our case, $\mathbb D$ is the universe of all possible text documents, and $\mathcal{D},\mathcal{D'}$ are datasets of text documents. Thus, DP means that the output should be insensitive to the presence of any single text document.

The case $\delta=0$ is called ``pure'' DP, and in that case $M$ is said to be \emph{$\varepsilon$-DP}. Our method is pure DP. 
%Straightforward extensions to non-pure DP are possible, but a drop-in replacement of the $\varepsilon$-DP building blocks we use with their $(\varepsilon,\delta)$-DP analogs.


\noindent\textbf{Kernel density estimation.}
Given a set $X\subset\R^d$, the Gaussian KDE function $KDE_X:\R^d\rightarrow[0,1]$ is defined as $KDE_X(y)=\tfrac{1}{|X|}\sum_{x\in X}e^{-\norm{y-x}_2^2}$. Up to normalization, $KDE_X(y)$ is the density at $y$ of a mixture of Gaussians centered at each $x\in X$. This is a common way to model a finite dataset $X$ as a distribution over all $\R^d$. 

A DP mechanism for estimating $KDE_X(y)$ at any given $y\in\R^d$, up to a bounded error, was given in \cite{wagner2023fast}. It runs in time linear in the dataset size $|X|$ and in the dimension $d$. It can thus be used for high-dimensional embeddings obtained from pre-trained deep learning models.
