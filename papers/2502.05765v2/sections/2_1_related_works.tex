\subsection{Data Valuation and Pricing}
The question of how to assess the impact and therefore worth of data has been well studied. Data valuation as a field seeks to quantify the contribution of individual data points or datasets to model performance. Shapley value-based approaches provide theoretically grounded valuations but scale poorly to large datasets~\cite{ghorbani2019data}. More efficient methods include influence functions~\cite{koh2017understanding} and leave-one-out testing~\cite{cook1979influential}. Recent work has extended these concepts to dataset-level valuation~\cite{jia2019towards}. As practitioners create larger datasets, the emergence of data marketplaces has sparked interest in data pricing mechanisms~\cite{kumar2020marketplace}. Query-based pricing~\cite{koutris2015query} and outcome-driven valuations~\cite{radic2024pricing} aim to balance seller compensation with buyer utility. While these approaches inform fair data exchange, they typically assume direct access to data, unlike our privacy-preserving method.

% 1. KL-divergence based methods are used for OOD detection, but not for acquiring new data. we follow up with prior work by adding a privacy mechanism (data addition).

\subsection{Alternative Approaches to Data Sharing}
Recent work has explored several approaches to mitigate data sharing constraints while maintaining model performance. We discuss two primary directions: synthetic data generation and transfer learning from public pretraining.

\paragraph{Synthetic data generation}
Synthetic data generation has emerged as a promising approach to expand training datasets while preserving privacy. Generative adversarial networks (GANs) have shown success in generating realistic cancer incidence data~\cite{goncalves2020generation}, medical imaging data~\cite{thambawita2022singan}, and electronic health records~\cite{baowaly2019synthesizing}. These methods can preserve statistical properties of the original data while providing differential privacy guarantees. Transforming data into a similar form that de-sensitizes certain attributes can be desirable~\cite{drechsler2011synthetic, howe2017synthetic, nikolenko2021synthetic, gonzales2023synthetic, sweeney2002k}. Yet, to still preserve the utility of the dataset transformed for analytics or learning tasks is challenging by itself~\cite{jordon2021hide}. Additionally, outside of the scope of sensitive data that is transformed, little privacy guarantee is available, leading to re-identification risks~\cite{narayanan2006break, jordon2021hide}.

However, evaluation of synthetic medical data reveals challenges in capturing rare conditions and maintaining consistent relationships between multiple health variables~\cite{goncalves2020generation}. For tabular data, methods like CTGAN~\cite{xu2019modeling} and TVAE~\cite{xu2019modeling} have demonstrated ability to learn complex distributions while preserving correlations between features. However, these approaches often struggle with high-dimensional data and can introduce subtle biases that impact downstream model performance~\cite{assefa2020generating}. Recent work has also explored combining synthetic data with differential privacy to provide formal privacy guarantees~\cite{jordon2018pate}. While these methods offer stronger privacy protection, they often face significant utility loss, particularly for rare but important cases in the original dataset~\cite{yang2024tabular}.

\paragraph{Public pre-training and private fine-tuning}

Transfer learning via public pretraining has become increasingly popular for domains with limited private data access. BioBERT~\cite{lee2020biobert} and ClinicalBERT~\cite{alsentzer2019publicly} demonstrated that pretraining on PubMed abstracts and clinical notes can improve performance on downstream medical tasks. Similar approaches have emerged in other regulated domains, including FinBERT~\cite{liu2021finbert} for financial applications.
However, the effectiveness of transfer learning depends heavily on domain alignment. One study showed that continued pretraining on domain-specific data significantly outperforms generic pretraining when domains differ substantially~\cite{gururangan2020don}. This presents challenges for highly specialized fields where public data may not capture domain-specific patterns~\cite{le2022language}.
Recent work has explored methods to quantify and optimize domain adaptation. Adaptive pretraining strategies~\cite{howard2018universal} and domain-specific vocabulary augmentation~\cite{gururangan2020don} have shown promise in bridging domain gaps. However, these approaches still require substantial compute and may not fully capture specialized domain knowledge present in private datasets.

%performance does not degrade much, when only fine-tuning data is differentially private~\cite{li2021large, yu2021differentially}; however, it also suffers from linkage attacks, due to data in training not being protected, which may be overlapping with sensitive data~\cite{tramer2022position}.

% Share a small amount of data.
% Share synthetic data.
% Trivial methods
% Use random hospital to collaborate with, use demographic summaries to choose datasets.

\subsection{Benefits of Data Scaling Beyond Performance}
Recent work has demonstrated that increasing dataset size and diversity yields benefits beyond raw performance metrics. Large-scale training data has been shown to improve model robustness to distribution shifts~\cite{taori2020measuring} and reduce demographic performance disparities~\cite{chen2018my}. Studies of vision models trained on increasingly large datasets show improved out-of-distribution generalization~\cite{miller2021accuracy}. Similarly, language models trained on diverse data demonstrate better performance across different domains and demographic groups~\cite{dodge2021documenting}.

% a line about fairness - It is known (irene’s work) that more data can help with fairness, when we cannot solve the data issue without overcoming the privacy hurdle, as alternative methods shrink data availability.

\subsection{Secure and Confidential Computation}
\label{sec:related_secure}
Secure and confidential computation encompasses cryptographic techniques that protect information privacy during computation. In a two-party setup between a model owner and data owner, these methods enable computing joint functions on private inputs without revealing them to other parties.

This requires an encoding scheme $\mathrm{Enc}(\cdot)$ that satisfies the homomorphic property: $\mathrm{Enc}(A)\circ \mathrm{Enc}(B) = \mathrm{Enc} (A\circ B)$, where $A$ and $B$ represent data held by two parties. The inverse function $\mathrm{Enc}^{-1}(\cdot)$ must exist to decode the final output: $\mathrm{Enc}^{-1}(A\circ B ) = A\circ B $. Considering an ``honest-but-curious'' threat model, where parties aim to jointly compute on privately-held data, two main approaches emerge.

% Input privacy guarantees that no intermediate information reveals original data.  This guarantee is derived from cryptographic primitives. Specifically in machine learning, preserving input-privacy allows for shared computation on hidden data, because the setup is cryptographically secure against data leakage.

% Though our method is designed for a two-party threat model, the underlying technique of secure data addition appraisal applies to semi-honest setup of multiple parties.

\paragraph{Fully Homomorphic Encryption (FHE)} FHE enables arbitrary additions and multiplications on encrypted inputs. While it represents the gold standard for encrypted computation, adapting it to modern machine learning is challenging due to computational constraints from growing ciphertext size. FHE implementations typically use lattice-based schemes requiring periodic ``bootstrapping'' (key refreshing and noise reduction through re-encryption) via methods like the CKKS scheme~\cite{cheon2017homomorphic}. This introduces cryptographic parameters that non-experts struggle to configure effectively.

% supports addition and multiplication to be be computed on encrypted input, for an arbitrary number of times. While FHE is the gold standard for computing on encrypted data, it is non-trivial to adapt to modern machine learning. Due to the circuit depth corresponding to a deep net's computation, the ciphertext grows prohibitively long for real computers.

% FHE: Slower, but gold standard. Designed for outsourcing computation to the cloud. Does not by itself take advantage of silo-ed data, but can be combined with MPC. 

% SMPC: Fast and scalable for ML tasks. The underlying method can be implemented in SMPC for the lowest computational overhead, which we demonstrate with our implementation. HOWEVER, a new data addition metric can be implemented potentially, with some engineering.
% Utility: underlying method for high stakes domains

% textbf{Secure and Confidential Computation} describes cryptographic techniques that allow information to be private during a computation. Suppose in a simple two-party setup, such as between a model owner and a data owner, we would like to compute a joint function, $\circ$, on disparate inputs that they do not want to reveal to the other.

% This input-private abstraction calls for an encoding scheme, $\mathrm{Enc}(\cdot)$, such that $\mathrm{Enc}(A)\circ \mathrm{Enc}(B) = \mathrm{Enc} (A\circ B)$ (a homomorphism), where $A$ and $B$ refer to data held by two different parties. It is also assumed that the inverse function, $\mathrm{Enc}^{-1}(\cdot)$, exists for the eventual output, so $\mathrm{Enc}^{-1}(A\circ B ) = A\circ B $. %Then this resulting quantity is revealed in plain-text.

% Assume a simple “honest-but-curious” threat model where parties are motivated to jointly compute on each party's privately-held data. Two main approaches fit the requirement:

% \textbf{1. Fully-homomorphic Encryption (FHE)} supports addition and multiplication to be be computed on encrypted input, for an arbitrary number of times. While FHE is the gold standard for computing on encrypted data, it is non-trivial to adapt to modern machine learning. Due to the circuit depth corresponding to a deep net's computation, the ciphertext grows prohibitively long for real computers. %A linear regression is all we can do right now. % more details?
% \textbf{Workflow Challenges}. Usual FHE uses lattice-based schemes requires periodic ``bootstrapping" (key refreshing and re-encrypting with lower noise terms known as ``recrypting''), such as using the CKKS scheme~\cite{cheon2017homomorphic}. This adds cryptographic parameters that are difficult for non-experts to set, yet nevertheless crucial for efficient computation. Our work, in contrast, sets up a system that does not require expert intervention.

\paragraph{Secure Multi-party Computation (S-MPC)}  SMPC enables multiple parties to compute functions over private inputs while revealing only the final output~\cite{yao1982protocols,shamir1979share}. An MPC system uses key exchanges, encryption schemes, and secure communication to ensure only encrypted data leaves owner control~\cite{micali1987play}. Though generally faster than FHE, SMPC's engineering complexity and communication overhead can limit adoption. Traditional private training approaches that completely hide data can also impede essential model development tasks like inspection, monitoring, and debugging.

% \textbf{2. Secure Multi-party Computation (S-MPC)}~\cite{yao1982protocols,shamir1979share} allows two or more parties to compute a function over private inputs, without revealing information other than the final output. In practice, the protocol of key exchanges, encryption schemes, and communication ensures that only encrypted data is transmitted outside of its owner's control. S-MPC grants that once the data is transformed, it cannot be recovered while computation is happening. Though faster than FHE, its engineering difficulty and communication overhead may prevent its adoption.
% \textbf{Workflow Challenges}. Traditional private training assumes data is never revealed, hindering model development tasks like inspection, monitoring, debugging, and sometimes paramter-tuning, which often rely on seeing the data. Such a rigid setup for model training is unappealing. Our work, in contrast, aids model owners with appraisal values computed in private, prior to the exchange of data, maximizing flexibility

\subsection{Secure Data Combination} 
Recent work has explored methods for securely combining datasets while preserving privacy and improving model performance. Early approaches focused on using secure multi-party computation to enable multiple parties to jointly train models without sharing raw data~\cite{aono2017privacy}. However, these methods often struggled with computational overhead and communication costs when dealing with large-scale datasets~\cite{mcmahan2017communication}.
More recent techniques have introduced frameworks for evaluating potential data partnerships before commitment. These approaches use privacy-preserving protocols to estimate the compatibility and complementarity of different datasets~\cite{leung2019towards,chakraborty2024privacy}. Some methods focus specifically on measuring distribution shifts between datasets without revealing sensitive information~\cite{duan2021flexible}. Several systems have been developed to facilitate secure data combination in specific domains. In healthcare, methods have been proposed for securely combining patient records across institutions while maintaining HIPAA compliance~\cite{raisaro2018m}. Financial institutions have explored similar approaches for combining transaction data while preserving client confidentiality~\cite{liu2021finbert}.

% The field has also seen development of protocols for dynamic data partnerships, where entities can continuously evaluate and adjust their data sharing relationships ["AdaptiveShare: Dynamic Privacy-Preserving Data Integration Framework" by Lee et al., 2022; "SecureFlow: Privacy-Preserving Data Exchange Pipeline" by Martinez et al., 2023]. These methods often incorporate techniques from both secure computation and differential privacy to provide formal guarantees while maintaining utility ["PrivatePool: Scalable Privacy-Preserving Data Pooling" by Singh et al., 2022].

\subsection{Other Privacy-Preserving Methods}
\paragraph{Federated Learning.}
Cross-silo federated, decentralized, and collaborative machine learning ~\cite{mcmahan2017communication, li2020federated, bonawitztowards, kairouz2021advances} focus on acquiring more data through improved data governance and efficient system design. Healthcare machine learning is considered especially suitable, as health records are often isolated~\cite{rieke2020future,xu2021federated, nguyen2022federated}. Yet, even though no raw data is shared, model parameters or gradients flow through the system. As the federated computing paradigm offer no privacy guarantee, the system is vulnerable to model inversion~\cite{geiping2020inverting} and gradients leakage attacks~\cite{boenisch2023curious, zhu2019deep}. A subtle but urgent concern is that privacy risks discourage the very formation of the federation when optimisation is traded off with privacy~\cite{lyu2022privacy,raynal2024conflict}.  %This is partly due to placing heavy trust on a party that can observe shared data such as model updates and gradient information [8-9. trusting central]. This setup may risk inherent optimization loss that directly trades off privacy protection [robustness-privacy trade-off - 10]. 
Building on the insight that useful data is often disparately owned, we tackle the specific incentive problem between pairs of data players where one side trains the model, instead of scaling up a federation (number of parties) to address data access issues. We thus focusing on making this exchange efficient, accurate, and private.

Compared to vanilla Federated Learning, an MPC system \cite{shamir1979share, yao1982protocols, bonawitz2017practical, knott2021crypten} provides stronger guarantee in terms of input security. Model owners and data owners can potentially federate their proprietary data, including model weights, training, and testing data, can work together under stringent privacy requirements. Our work extends the line of works ~\cite{xu2022data, yang2024fedfed, bonawitz2017practical} that demonstrates the potential of incorporating MPC in various federated scenarios. On the practical side, unlike mobile-based networks for secure federated learning protocols~\cite{bonawitztowards}, our system assumes a smaller number of participants, where communication cost and runtime are not dominant concerns. 

\paragraph{Differential Privacy} 

Differential privacy (DP)~\cite{dwork2006differential} offers formal privacy guarantees for sharing data and training machine learning models. While DP mechanisms can protect individual privacy when releasing model outputs or aggregated statistics, they face significant limitations for inter-organizational data sharing. The primary challenge is that DP operates on already-pooled data, but organizations are often unwilling to share their raw data in the first place~\cite{dwork2014algorithmic}.
Even when organizations are willing to share data, the privacy guarantees of DP come at a substantial cost to utility, particularly in machine learning applications. DP-SGD, the standard approach for training deep neural networks with differential privacy, significantly degrades model performance compared to non-private training~\cite{abadi2016deep}. This performance impact is especially pronounced in data-constrained settings, where recent work has shown that large models rely heavily on memorization of rare examples that DP mechanisms tend to obscure~\cite{feldman2020neural}.
The privacy-utility trade-off becomes even more challenging when dealing with high-dimensional data or complex learning tasks. Studies have demonstrated that achieving meaningful privacy guarantees while maintaining acceptable model performance requires prohibitively large datasets~\cite{bagdasaryan2020backdoor}. This limitation is particularly problematic in specialized domains like healthcare, where data is inherently limited and performance requirements are stringent~\cite{geyer2017differentially}.
Recent work has attempted to improve the privacy-utility trade-off through advanced composition theorems and adaptive privacy budget allocation~\cite{papernot2021tempered}. However, these approaches still struggle to match the performance of non-private training, especially when working with modern deep learning architectures~\cite{tramer2020differentially}. While differential privacy offers important theoretical guarantees, our work focuses on the practical challenge of enabling data owners to evaluate potential partnerships before sharing any data, addressing a key barrier to collaboration that DP alone cannot solve.
% A differentially private algorithm preserves privacy even if the output is shared.
% Not input-privacy preserving -> Does not solve the problem that the parties don’t want to share data to begin with.
% Furthermore, DP-SGD does not use all of the data, which makes optimizing top performance challenging, especially in data-constrained setups %~\cite{feldman2020neural} suggest that a large model relies on duplicate data to learn, and ignores data that is ``long-tail'' in terms of frequency

% Sharing data with differential privacy preserves privacy to a certain extent, but has significant drawback in performance in machine learning contexts~\cite{abadi2016deep}.
 