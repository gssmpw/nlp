\begin{figure}[ht]
  \centering
  %\includegraphics[width=0.85\linewidth]{figures/method.pdf}
  \includegraphics[width=0.85\linewidth]{figures/KL-based-measure_with_symbol.pdf}
  \caption{\textbf{Our Method $\mathrm{SecureKL}(\Do, \Di)$}. Each side encrypts their data, and a model is trained on their joint data. Using private KL-based measures, their distance is computed, and the final result is revealed after decryption, which requires both parties.}
  % \Description{Data is encrypted on each side.}
  \label{fig:method}
\end{figure}

We summarize our data acquisition strategies differentiated by leakage risks, which correlate with potential costs:

\textbf{Category 1, medium-to-high leakage}, sharing raw data. $\pi_s(n, k)$ supposes each hospital to share a dataset of size $k$; a default setting of $1\%$ is commonplace practice in some contracts, as a pre-requisite to being considered~\cite{lds}. Though leakage can be controlled through $k$, the data is inherently sensitive. The underlying distance uses ~\priorp's KL-XY Score.

\textbf{Category 2, medium leakage}, sharing summary statistics. $\pi_d(n)$ uses demographic metadata to guide data selection. This is implemented through ratio distance between source and target distributions, which may be considered aggregates therefore potentially not sensitive, such as when the underlying aggregation function $\phi$ is differentially private.

\textbf{Category 3, zero leakage}, sharing no \emph{additional} information besides what is assumed public. There are two methods: a. \textbf{Blind selection baseline}: $\pi_0(n)$ randomly selects $n$ disjoint data sources, until data purchasing budget runs out. Prior works suggests that when $n=1$, randomly selecting a source in hospital ICU may be risky and inefficient. b. \textbf{Our method} $\pi_p(n)$ selects data sources based on privacy-preserving measure for data combination, specifically Private KL-XY. 

\subsection{Trivial Baseline: Blind Selection}
Blind selection refers to the process when no information is provided. $\pi_0(n)$ randomly selects $n$ disjoint data sources, until data purchasing budget runs out. This random strategy may evade selection biases and help gather diverse data. Yet, prior work ~\priorp suggests that $\pi_0 (1)$ -- randomly selecting one source -- for ICU is risky and inefficient for mortality prediction.

\subsection{Sharing Summary Statistics}
A relaxation to sharing no sensitive data is to share metadata. While demographic traits are often \emph{causal} and available, their exact cause in relation to the task is not a priori established (without a highly effective model), therefore their success in distributional-matching is not guaranteed to be strong. Additionally, in practice, the most effective model that results from data combination may or may not be causally-sound. Nevertheless, we posit alternative strategy
$\pi_d(n)$ to find the demographically close candidates to guide data selection: Let $\phi:\mathcal{D}\to\mathbb{R}^m$ be an $m$-dimensional summary statistic of a demographic feature i.e. the racial distribution of patients. Then, we use the distributional distance between $\Do$ and $\Di$, characterized by their $L_2$-distance through $\phi$:
\begin{equation}
\tag{Demographic-based Strategy}
\pi_d (n=1) = \argmin _{i\in [1..N]} \, L_2(\phi(\Do) || \phi(\Di)).
\end{equation}

\subsection{KL-based Methods, in Plaintext}
$\pi_s(n, k)$ assumes each of the candidate hospitals will share a set of raw data. In ICU data, simulate that a default of $1\%$ is shared, so $k=3000\times 1\% = 30$, though we run experiments with $k\in\{3, 30. 300, 3000\}$ (Section~\ref{sec:consistency}). Though leakage can be controlled through $k$, the data is inherently sensitive.

This is implemented with KL-based measures similar to \priorp. To recap, $\mathrm{KL}(P||Q)$ is not symmetrical, meaning that it is not a "metric" that satisfies triangle inequality. Intuitively, this means the measure is directional: a hospital's distribution $P_o$ may be "close" to the target distribution $P_i$, but not the other way around:
\begin{equation}
\tag{Ideal Estimator}
\mathrm{KL}(P_o||P_i) = \int_{x\in \mathcal{X}} \log\frac{P_o(\diff x)}{P_i(\diff x)}P_i(\diff x)
\end{equation}
Because we only have access to finite data $\Do$ and $\Di$, approximations are needed.
Typically, a learned model can capture distributional information, used to estimate continuous entropy.
Thus the joint distribution of features and labels from both the source and target are included, with the goal of deriving an efficient estimator for $\mathrm{KL}(P_o||P_i)$ that captures distributional shift from source to target.

Specifically, $\KLXY$ score used in $\mathrm{Secure}\KLXY$ first trains a logistic regression model \cite{Cox1958TheRA} on $\Do \cup \Di$ -- where the labels are folded into the covaraiates -- with the goal of inferring dataset membership.
% \begin{equation}
%     \tag{Logistic Regression}
%     \mathrm{LR}(x)
% \end{equation}
% We may wanna plug in the datasets in an equation here for clarity.
A score of $0.5$ or less means the datasets are not distinguishable, making the data potentially useful. 
~\priorp established the insight that in data-limited domains of heterogeneous data sources, domain shifts of the covariates are useful for predicting whether the additional data helps the original task.
We note again that even though this model is trained on both parties' data, the final algorithm that the hospital uses to train on combined data is not restricted.

Then, the resulting model's probability score function $\text{Score}(\cdot): \mathcal{X, Y} \to [0,1]$ is averaged over a dataset in $H_o$, obtaining
  
\begin{equation}
\tag{KL-X}
\KLX = \mathbb{E}_{(x)\sim \Do}(\text{Score}(x)).
\end{equation} 
\begin{equation}
\tag{KL-XY}
\KLXY = \mathbb{E}_{(x,y)\sim \Do}(\text{Score}(x, y)).
\end{equation} 
We focus on $\KLXY$, and reproduce \priorp's results that it is predictive of downstream change in AUC.

Let the score function $g_\mathrm{KL}$ be the approximate of $ \mathrm{KL}(\Do||\Di)$. The strategy selects the most likely hospital with the closest distance under the measure:
\begin{equation}
\tag{KL-based Strategy, \emph{in plaintext}}
\pi_s (n=1, k=K) = \argmin _{i\in [1..N]} \, g_\mathrm{KL}(\Do,\Di).
\end{equation}

When only a subset is available, this function is adjusted by swapping $\Di$ for $\Di' \subseteq \Di$ where $|\Di'| = k$. We denote the full dataset size as $K=|\Di|$.

\subsection{SecureKL: Private KL-based Method}
Using MPC, we extend on $\KLXY$ to require no information sharing (besides what was already assumed public).
Specifically we leverage the MPC based framework provided by CrypTen~\cite{knott2021crypten}, a library designed for privacy-preserving machine learning, to implement private $\KLX$ and $\KLXY$. As illustrated in Figure~\ref{fig:method}, the logistic regression as well as the scoring need to be implemented in private. Our code is publicly available \footnote{\url{https://anonymous.4open.science/r/Private-Preserving_Data_Combination-451E}}.

Denote the private encoding of $x$ as $[x]$.
\begin{equation}
\tag{Secure KL-X}
\mathrm{Secure}\KLX = \mathbb{E}_{(x)\sim \Do}(\text{Score}([x])).
\end{equation} 
\begin{equation}
\tag{Secure KL-XY}
\mathrm{Secure}\KLXY= \mathbb{E}_{(x,y)\sim \Do}(\text{Score}([x, y])).
\end{equation} 

Let the score function $g_\mathrm{SKL}$ be the secure approximation of $ \mathrm{KL}(\Do||\Di)$. The strategy selects the most likely hospital with the closest distance under the measure:
\begin{equation}
\tag{SecureKL Strategy, \emph{encrypted}}
\pi_p (n=1) = \argmin _{i\in [1..N]} \, g_\mathrm{SKL}(\Do,\Di).
\end{equation}

As shown in Figure~\ref{fig:method}, any KL-based measure $g_\mathrm{SKL}$ can be adapted to our setup. We mainly use $\mathrm{Secure}\KLXY$ as the underlying measure. Its performance is detailed in Section~\ref{sec:positivity}. 
Additionally, even though our implementation measures distance of data between one source and one target party, the setup readily extends to accommodating multiple parties. We note the engineering limitations in Section~\ref{sec:eng-limits}.