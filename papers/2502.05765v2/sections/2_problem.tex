Consider a binary prediction task for ICU patient mortality based on electronic medical records. A source hospital $H_o$ has historical patient data $\Do$ containing static past patient characteristics, prior medical records, and ICU outcomes. Other hospitals $\{H_i\}$ each has their patient data: $\{\Di \mid i\in [1.. N]\}$. 

For this binary prediction task, hospitals typically optimize for performance metrics, for example the area under the receiver-operating characteristic curve (AUC). Using only their data, $H_o$ can train a model $\mathcal{M}$ with parameters $\theta$ to achieve:
\begin{equation}
\tag{Baseline Performance}
\AUCo = \max_{f(\theta)} \, \AUC(\mathcal{M}, \Do)
\end{equation}
where $f$ is their chosen algorithm with parameter $\theta$.
%\footnote{\textcolor{red}{note that this algorithm can change downstream(for now we can omit)}}
%\AUC(H_o, \emptyset)
%, obtaining model parameters $\theta_o$, choosing an algorithm $f_o$, and an initial AUC of $\AUC^{[o]}$.

When $H_o$ has exhausted their own internal data, they may benefit from incorporating additional target data sources $T\subset [1.. N]$. By combining datasets, i.e., $\DT = \{\Di\mid i\in T\}\cup \Do$, $H_o$ can potentially achieve better results:
\begin{equation}
\tag{Combined Performance}
\AUCT = \max_{f(\theta)} \, \AUC(\mathcal{M}, \DT).
\end{equation}
We define the potential improvement from data addition as $\delta_{T} = \delta_{(o, T)} = \AUCT-\AUCo$. To add a single additional data source by setting $T=\{i\}$, the improvement is $\delta_{i} =\delta_{(o, i)}=\AUC_i-\AUCo$.
This leads to our central question:
% \begin{quote}
    \textbf{\emph{Without seeing target data, how does a hospital ascertain potential data sources to combine with?}}
% \end{quote}

Formally, given $n\leq N$, we seek a strategy $\pi$ that selects $n$ target datasets $T=\pi(\Do, n)$ to maximize model utility:
\begin{equation}
\tag{Ideal Dataset Combination}
\pi^*(\Do, n) = \argmax_{T\subset {[1...N]\choose n}}\AUCT%\quad\forall n.
\end{equation}
\paragraph{Practical Considerations.} Computing every subset $T\subset {[1...N]\choose n}$'s associated $\delta_{T}$ is exponential in $n$. To make this problem tractable, we make two key assumptions. First, we apply strategies greedily, selecting top-ranked target datasets. With the ultimate objective of improving the source hospital's prediction task, we fix $H_o$; to compare the trade-offs between strategies in Section~\ref{sec:methods}, we apply each $\pi$ greedily to select top-$n$ institution(s) for $H_o$ without replacement. Second, in in data constrained settings, we aim to maximize the probability of positive improvement: $P_{H_o\sim \mathbf{H}}(\delta_T > 0)$. 
%\textcolor{red}{Add additional caveats here for folktable setups.}
\paragraph{Kullback–Leibler Divergence.} Our approach uses Kullback-Leibler (KL)-divergence-based methods to gauge data utility, building on prior work~\cite{shen2024data}. KL divergence~\cite{kullback1951information}, also called \emph{information gain}~\cite{quinlan1986induction}, describes a measure of how much a model probability distribution $Q$ is different from a true probability distribution $P$:
\begin{equation}
\tag{Kullback–Leibler Divergence}
\mathrm{KL}(P||Q) = \int_{x\in \mathcal{X}} \log\frac{P(\diff x)}{Q(\diff x)}P(\diff x)
\end{equation}
Because computing KL-divergence on datasets $\Do$ and $\Di$ is non-trivial, ~\priorp proposes two groups of scores to make this divergence approximation tractable from small samples.
% \begin{equation}
% \tag{Ideal Estimator}
% \mathrm{KL}(P_o||P_i) = \int_{x\in \mathcal{X}} \log\frac{P_o(\diff x)}{P_i(\diff x)}P_i(\diff x)
% \end{equation}
 Specifically, score $\KLXY$ first trains a logistic regression model on $\Do \cup \Di$ -- where the labels are folded into the covariates --- with the goal of inferring dataset membership. Then, the resulting model's probability score function $\text{Score}(\cdot): \mathcal{X, Y} \to [0,1]$ is averaged over a dataset in $H_o$, obtaining

\begin{equation}
\tag{KL-XY Score}
\KLXY = \mathbb{E}_{(x,y)\sim \Do}(\text{Score}(x, y)).
\end{equation}
Details are described in Section~\ref{sec:methods}.
\paragraph{Privacy Model for $\pi_p$.} We operate under a semi-honest privacy model---also known as \emph{honest-but-curious} or \emph{passive security}---where parties follow protocols but may probe intermediate values. Parties are  ``curious'', meaning that they can probe into the intermediate values to avoid paying for the data. This assumes a weaker security model than malicious security where a corrupted party may input foul data, but ensures the algorithm to be private throughout the computation. This privacy preservation model incentivizes collaboration, improving upon methods in ~\priorp.



\paragraph{MPC Preliminary}
To secure this divergence computation cryptographically, Secure Multiparty Computation (MPC)~\cite{yao1982protocols, shamir1979share} protocols are leveraged. Specifically, in $\mathrm{SecureKL}$, each party encodes $\Do$ and $\Di$ to preserve privacy for both parties. This is implemented with the research framework CrypTen~\cite{knott2021crypten}, specialized for MPC and machine learning. Our algorithmic and engineering details are in Sections ~\ref{sec:methods} and ~\ref{sec:exp}, respectively. For related secure techniques, see Section~\ref{sec:related_secure}.
\paragraph{Additional Assumptions }
% Assume:
% Source party has access to their own data (test set) where they want an algorithm to work well (though they may not know what algorithm model they use)
% Source party can buy / engage with data from other sources but they don’t have access directly
% (OR they only have a small percentage access)

% Goal: without compromising on data privacy, ascertain among candidate data sources, which ones would be sensible to combine with my setting and existing data?

% What is being done here?
Generally, we consider high stakes domains where disparate data may have additive benefits to the existing data.
In order to make privacy boundaries tractable, we make the following additional assumptions:
\begin{enumerate}
\itemsep0em
\item \textbf{Existing knowledge} is not private. The hospitals are aware of each other having such data to begin with. The hospitals may know of the available underlying dataset size and format, which is assumed to be uniform across the hospitals in the setup to simulate unit-cost. Hospitals frequently know of each other's resources, and the available ICU units are contentious, not kept secret. 
\item \textbf{Uniformity} of $|\Di|$. Though each hospital gets to price their data and set their own budget, for generality, the uniformity assumption allows us to use the number of additional data sources $n$ as the main "budget proxy" across different strategies.
\item \textbf{Legal risks} of sharing \emph{any} data are omnipresent in high stakes domains. The risks with sharing sensitive data in $\pi_d$ and $\pi_s$ are not made explicit, but assumed to be "medium" and "medium-to-high" respectively. This abstraction side-steps legal discussion, which would go beyond the scope of our paper.
\item \textbf{No malice} is assumed on any of the parties involved, as each hospital wants to authentically sell their data and set up a potential collaboration. This assumption becomes stronger when the number of parties grows or when the setup changes to potentially more competitive industries with less trust. We note our limitations in Section~\ref{sec:limits}.
\end{enumerate}