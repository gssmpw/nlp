
\begin{table*}
\centering
\setlength{\tabcolsep}{2pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllllll}
\toprule
Type & Name & Languages & Domains & Sample creation & Annotation creators & Nb Samples* \\
    \midrule
    \multirow[t]{7}{*}{BitextMining} & BornholmBitextMining \cite{derczynskiBornholmskNaturalLanguage2019} & ['dan'] & ['Web', 'Social', 'Fiction', ...] & created & expert-annotated & 500 \\
     & BibleNLPBitextMining \cite{akerman2023ebible} & ['aai', 'aak', 'aau', ...] & ['Religious', 'Written'] & created & expert-annotated & 417452 \\
     & BUCC.v2 \cite{zweigenbaum-etal-2017-overview} & ['cmn', 'deu', 'eng', ...] & ['Written'] & human-translated & human-annotated & 35000 \\
     & DiaBlaBitextMining \cite{gonzalez2019diabla} & ['eng', 'fra'] & ['Social', 'Written'] & created & human-annotated & 11496 \\
     & FloresBitextMining \cite{goyal2022flores} & ['ace', 'acm', 'acq', ...] & ['Non-fiction', 'Encyclopaedic', 'Written'] & created & human-annotated & 41908944 \\
     & NorwegianCourtsBitextMining \cite{opus4} & ['nno', 'nob'] & ['Legal', 'Written'] & found & human-annotated & 228 \\
     & NTREXBitextMining \cite{federmann-etal-2022-ntrex} & ['afr', 'amh', 'arb', ...] & ['News', 'Written'] & human-translated and localized & expert-annotated & 3826252 \\
    \cline{1-7}
    \multirow[t]{21}{*}{Classification} & BulgarianStoreReviewSentimentClassfication \cite{DVN/TXIK9P_2018} & ['bul'] & ['Reviews', 'Written'] & found & human-annotated & 182 \\
     & CzechProductReviewSentimentClassification \cite{habernal-etal-2013-sentiment} & ['ces'] & ['Reviews', 'Written'] & found & derived & 2048 \\
     & GreekLegalCodeClassification \cite{papaloukas-etal-2021-glc} & ['ell'] & ['Legal', 'Written'] & found & human-annotated & 4096 \\
     & DBpediaClassification \cite{NIPS2015_250cf8b5} & ['eng'] & ['Encyclopaedic', 'Written'] & found & derived & 2048 \\
     & FinancialPhrasebankClassification \cite{Malo2014GoodDO} & ['eng'] & ['News', 'Written', 'Financial'] & found & expert-annotated & 2264 \\
     & PoemSentimentClassification \cite{sheng2020investigating} & ['eng'] & ['Reviews', 'Written'] & found & human-annotated & 209 \\
     & ToxicChatClassification \cite{lin2023toxicchat} & ['eng'] & ['Constructed', 'Written'] & found & expert-annotated & 1164 \\
     & ToxicConversationsClassification \cite{jigsaw-unintended-bias-in-toxicity-classification} & ['eng'] & ['Social', 'Written'] & found & human-annotated & 2048 \\
     & EstonianValenceClassification \cite{Pajupuu2023} & ['est'] & ['News', 'Written'] & found & human-annotated & 818 \\
     & ItaCaseholdClassification \cite{10.1145/3594536.3595177} & ['ita'] & ['Legal', 'Government', 'Written'] & found & expert-annotated & 221 \\
     & AmazonCounterfactualClassification \cite{oneill-etal-2021-wish} & ['deu', 'eng', 'jpn'] & ['Reviews', 'Written'] & found & human-annotated & 5805 \\
     & MassiveScenarioClassification \cite{fitzgerald2022massive} & ['afr', 'amh', 'ara', ...] & ['Spoken'] & human-translated and localized & human-annotated & 255357 \\
     & MultiHateClassification \cite{rottger-etal-2021-hatecheck} & ['ara', 'cmn', 'deu', ...] & ['Constructed', 'Written'] & created & expert-annotated & 11000 \\
     & NordicLangClassification \cite{haas-derczynski-2021-discriminating} & ['dan', 'fao', 'isl', ...] & ['Encyclopaedic'] & found & derived & 3000 \\
     & ScalaClassification \cite{nielsen-2023-scandeval} & ['dan', 'nno', 'nob', ...] & ['Fiction', 'News', 'Non-fiction', ...] & created & human-annotated & 8192 \\
     & SwissJudgementClassification \cite{niklaus2022empirical} & ['deu', 'fra', 'ita'] & ['Legal', 'Written'] & found & expert-annotated & 4908 \\
     & TweetSentimentClassification \cite{barbieri-etal-2022-xlm} & ['ara', 'deu', 'eng', ...] & ['Social', 'Written'] & found & human-annotated & 2048 \\
     & CBD \cite{ogr:kob:19:poleval} & ['pol'] & ['Written', 'Social'] & found & human-annotated & 1000 \\
     & PolEmo2.0-OUT  & ['pol'] & ['Written', 'Social'] & & & 494 \\
     & CSFDSKMovieReviewSentimentClassification \cite{stefanik2023resources} & ['slk'] & ['Reviews', 'Written'] & found & derived & 2048 \\
     & DalajClassification \cite{2105.06681} & ['swe'] & ['Non-fiction', 'Written'] & created & expert-annotated & 888 \\
    \cline{1-7}
    \multirow[t]{8}{*}{Clustering} & WikiCitiesClustering \cite{wikidump} & ['eng'] & ['Encyclopaedic', 'Written'] & found & derived & 1 \\
     & RomaniBibleClustering  & ['rom'] & ['Religious', 'Written'] & human-translated and localized & derived & 4 \\
     & BigPatentClustering.v2 \cite{DBLP:journals/corr/abs-1906-03741} & ['eng'] & ['Legal', 'Written'] & found & derived & 2048 \\
     & BiorxivClusteringP2P.v2  & ['eng'] & ['Academic', 'Written'] & created & derived & 53787 \\
     & AlloProfClusteringS2S.v2 \cite{lef23} & ['fra'] & ['Encyclopaedic', 'Written'] & found & human-annotated & 2556 \\
     & HALClusteringS2S.v2 \cite{ciancone2024extending} & ['fra'] & ['Academic', 'Written'] & found & human-annotated & 2048 \\
     & SIB200ClusteringS2S \cite{adelani2023sib} & ['ace', 'acm', 'acq', ...] & ['News', 'Written'] & human-translated and localized & expert-annotated & 197788 \\
     & WikiClusteringP2P.v2  & ['bos', 'cat', 'ces', ...] & ['Encyclopaedic', 'Written'] & created & derived & 28672 \\
    \cline{1-7}
    \multirow[t]{15}{*}{Retrieval} & StackOverflowQA \cite{li2024coircomprehensivebenchmarkcode} & ['eng'] & ['Programming', 'Written'] & found & derived & 19931 - 1994 \\
     & TwitterHjerneRetrieval \cite{holm2024gllms} & ['dan'] & ['Social', 'Written'] & found & derived & 262 - 78 \\
     & LegalQuAD \cite{9723721} & ['deu'] & ['Legal', 'Written'] & found & derived & 200 - 200 \\
     & ArguAna \cite{boteva2016} & ['eng'] & ['Medical', 'Written'] & & & 8674 - 1406 \\
     & HagridRetrieval \cite{hagrid} & ['eng'] & ['Encyclopaedic', 'Written'] & found & expert-annotated & 496 - 496 \\
     & LegalBenchCorporateLobbying \cite{guha2023legalbench} & ['eng'] & ['Legal', 'Written'] & found & derived & 319 - 340 \\
     & LEMBPasskeyRetrieval \cite{zhu2024longembed} & ['eng'] & ['Fiction', 'Written'] & found & derived & 800 - 400 \\
     & SCIDOCS \cite{specter2020cohan} & ['eng'] & ['Academic', 'Written', 'Non-fiction'] & found & & 25657 - 1000 \\
     & SpartQA \cite{xiao2024rar} & ['eng'] & ['Encyclopaedic', 'Written'] & found & derived & 1592 - 3594 \\
     & TempReasonL1 \cite{xiao2024rar} & ['eng'] & ['Encyclopaedic', 'Written'] & found & derived & 12504 - 4000 \\
     & WinoGrande \cite{xiao2024rar} & ['eng'] & ['Encyclopaedic', 'Written'] & found & derived & 5095 - 1267 \\
     & AlloprofRetrieval \cite{lef23} & ['fra'] & ['Encyclopaedic', 'Written'] & found & human-annotated & 2556 - 2316 \\
     & BelebeleRetrieval \cite{bandarkar2023belebele} & ['acm', 'afr', 'als', ...] & ['Web', 'News', 'Written'] & created & expert-annotated & 183488 - 338378 \\
     & StatcanDialogueDatasetRetrieval \cite{lu-etal-2023-statcan} & ['eng', 'fra'] & ['Government', 'Web', 'Written'] & found & derived & 23628 - 9436 \\
     & WikipediaRetrievalMultilingual  & ['ben', 'bul', 'ces', ...] & ['Encyclopaedic', 'Written'] & LM-generated and verified & LM-generated and reviewed & 216000 - 24000 \\
    \cline{1-7}
    \multirow[t]{3}{*}{InstructionReranking} & Core17InstructionRetrieval \cite{weller2024followir} & ['eng'] & ['News', 'Written'] & found & derived & 19939 \\
     & News21InstructionRetrieval \cite{weller2024followir} & ['eng'] & ['News', 'Written'] & found & derived & 30985 \\
     & Robust04InstructionRetrieval \cite{weller2024followir} & ['eng'] & ['News', 'Written'] & found & derived & 47596 \\
    \cline{1-7}
    \multirow[t]{2}{*}{MultilabelClassification} & MalteseNewsClassification \cite{maltese-news-datasets} & ['mlt'] & ['Constructed', 'Written'] & found & expert-annotated & 2297 \\
     & MultiEURLEXMultilabelClassification \cite{chalkidis-etal-2021-multieurlex} & ['bul', 'ces', 'dan', ...] & ['Legal', 'Government', 'Written'] & found & expert-annotated & 115000 \\
    \cline{1-7}
    \multirow[t]{6}{*}{PairClassification} & CTKFactsNLI \cite{ullrich2023csfever} & ['ces'] & ['News', 'Written'] & found & human-annotated & 680 \\
     & SprintDuplicateQuestions \cite{shah-etal-2018-adversarial} & ['eng'] & ['Programming', 'Written'] & found & derived & 101000 \\
     & OpusparcusPC \cite{creutz2018open} & ['deu', 'eng', 'fin', ...] & ['Spoken', 'Spoken'] & created & human-annotated & 18207 \\
     & RTE3 \cite{giampiccolo-etal-2007-third} & ['deu', 'eng', 'fra', ...] & ['News', 'Web', 'Encyclopaedic', ...] & found & expert-annotated & 1923 \\
     & XNLI \cite{conneau2018xnli} & ['ara', 'bul', 'deu', ...] & ['Non-fiction', 'Fiction', 'Government', ...] & created & expert-annotated & 38220 \\
     & PSC \cite{ogrodniczuk-kopec-2014-polish} & ['pol'] & ['News', 'Written'] & found & derived & 1078 \\
    \cline{1-7}
    \multirow[t]{3}{*}{Reranking} & WebLINXCandidatesReranking \cite{l√π2024weblinx} & ['eng'] & ['Academic', 'Web', 'Written'] & created & expert-annotated & 5592142 \\
     & AlloprofReranking \cite{lef23} & ['fra'] & ['Web', 'Academic', 'Written'] & found & expert-annotated & 27355 \\
     & WikipediaRerankingMultilingual \cite{wikidump} & ['ben', 'bul', 'ces', ...] & ['Encyclopaedic', 'Written'] & LM-generated and verified & LM-generated and reviewed & 240000 \\
    \cline{1-7}
    \multirow[t]{9}{*}{STS} & SICK-R \cite{marelli-etal-2014-sick} & ['eng'] & ['Web', 'Written'] & & human-annotated & 9927 \\
     & STS12 \cite{10.5555/2387636.2387697} & ['eng'] & ['Encyclopaedic', 'News', 'Written'] & created & human-annotated & 3108 \\
     & STS14 \cite{bandhakavi-etal-2014-generating} & ['eng'] & ['Blog', 'Web', 'Spoken'] & created & derived & 3750 \\
     & STS15 \cite{bicici-2015-rtm} & ['eng'] & ['Blog', 'News', 'Web', ...] & created & human-annotated & 3000 \\
     & STSBenchmark \cite{huggingface:dataset:stsb_multi_mt} & ['eng'] & ['Blog', 'News', 'Written'] & machine-translated and verified & human-annotated & 1379 \\
     & FinParaSTS \cite{kanerva-etal-2021-finnish} & ['fin'] & ['News', 'Subtitles', 'Written'] & found & expert-annotated & 2000 \\
     & STS17 \cite{cer-etal-2017-semeval} & ['ara', 'deu', 'eng', ...] & ['News', 'Web', 'Written'] & created & human-annotated & 5346 \\
     & SICK-R-PL \cite{dadas-etal-2020-evaluation} & ['pol'] & ['Web', 'Written'] & human-translated and localized & human-annotated & 4906 \\
     & STSES \cite{agirre2015semeval} & ['spa'] & ['Written'] & & & 155 \\
    \cline{1-7}
    \bottomrule
    \end{tabular}
    
}
\caption{The tasks included in \texttt{MTEB(Europe)}. The language column shows all the languages of the task. When running the tasks we limit it to the languages specified in the benchmark. * For the number of samples, are given the total number of samples all languages included, for Retrieval tasks are given the (number of queries - number of documents).}
\label{tab:mteb_europe_task_overview}
\end{table*}