\vspace{-1.5mm}
\section{Related Work}

\paragraph{Text Embedding Benchmarks.}
BEIR \citep{thakur2021beir} pioneered the use of publicly available datasets from diverse information retrieval (IR) tasks and domains and evaluated 10 various retrieval systems. MTEB \citep{muennighoff2023mteb} introduced a comprehensive text embedding benchmark that spans not only IR but also 8 other task categories, including clustering and re-ranking. MTEB covers a total of 58 tasks and 112 languages, though this multilinguality is mainly derived from machine-translated tasks or bitext mining. Its leaderboard has grown in popularity and evolved into the de facto embedding model benchmark that supports over 300 models. MIRACL \citep{zhang2022making} supports 18 languages from different language families for monolingual retrieval. MINERS~\citep{winata2024miners} is designed to evaluate the ability of multilingual LMs in semantic retrieval tasks including classification and bitext mining tasks in more than 200 languages, including code-switching. 
% Our work extends the support to cross-lingual retrieval tasks \sara{important to frame here the degree of extension}. 
Our work extends the number of languages to over 1000 (250 excluding bitext-mining tasks), particularly to cover more low-resource languages. We also expand the MTEB's 8 embedding tasks to 10 and the 58 datasets to over 400, significantly broadening the scope of multilingual benchmarking.

\paragraph{Massive Collaborative Projects.}
Open research initiatives and participatory approaches to science have been shown to stimulate innovation \citep{Park2023PapersAP}, reduce negative biases \citep{gudowsky2021limits, gomez2022leading}, and increase the diversity of data sources \citep{hanley2020ethical,singh2024aya,winata2024worldcuisines}. By involving diverse stakeholders, these practices enhance ethical, robust, and reproducible research \citep{hagerty2019global}. Recently, the field of natural language processing has seen a growing number of community-driven collaborative projects. These can be grouped into several categories. \emph{(a) Model creation}, such as BLOOM~\citep{workshop2023bloom,muennighoff2023crosslingual}, StarCoder~\citep{li2023starcoder,lozhkov2024starcoder2stackv2}, Aya model \citep{ustun2024aya}, and Cendol~\citep{cahyawijaya2024cendol}; \emph{(b) Dataset creation}, such as NusaX~\citep{winata2023nusax}, OpenAssistant~\citep{k√∂pf2023openassistant}, NusaWrites~\citep{cahyawijaya2023nusawrites}, and Aya dataset~\citep{singh2024aya}; \emph{(c) Benchmark creation}, such as BIG-Bench~\citep{srivastava2022beyond}, NusaCrowd~\citep{cahyawijaya2023nusacrowd}, WorldCuisines~\citep{winata2024worldcuisines}, HLE~\citep{phan2025humanitysexam}, SEACrowd~\citep{lovenia2024seacrowd}, and Eval-Harnesses~\citep{eval-harness,bigcode-evaluation-harness,biderman2024lessons}; and \emph{(d) Other artifacts}, such as NL-Augmenter~\citep{dhole2022nlaugmenter}, the Data Provenance Initiative~\citep{longpre2023data,longpre2024consentcrisisrapiddecline,longpre2024bridgingdataprovenancegap} or the Wikibench annotation tool~\citep{Kuo2024ACM}. MMTEB expands upon earlier work within the \emph{Benchmark creation} category. Our effort significantly differs from prior collaborative benchmarks as we focus on text embeddings, use a custom point system to incentivize contributions, and handle all communication openly via GitHub.
