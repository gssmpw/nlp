\section{Analysis and Discussion}

\autoref{tab:overall-performance} shows the performance across the three presented multilingual benchmarks.
Two trends are clearly observable;  

\textbf{Models trained with instruction-tuning perform significantly better compared to those without it}. This is especially clear when comparing the multilingual-e5-large to its instruction-tuned counterpart (multilingual-e5-large-instruct). Instruction tuning increases performance most drastically on bitext mining and clustering, though the effect remains pronounced across all task categories. Notably, this happens despite many tasks using generic prompts for the task category and no model-specific tuning of prompts per task.
Surprisingly, multilingual-e5-large(-instruct) models, based on XLM-R Large \citep{conneau2019unsupervised} generally outperform the considerably larger e5-mistral-7b-instruct and GritLM-7B, both of which are based on Mistral-7B~\citep{jiang2023mistral}. 
This effect is notably pronounced for mid-to-low resource languages (<300M speaker; see Appendix~\ref{sec:appendix-perf-by-speakers}) and likely emerges due to differences in pre-training, with Mistral being predominantly pre-trained on English, while XLM-R targets 100 languages. All three models utilize similarly multilingual datasets for fine-tuning.  However, GritLM still remains best in class for retrieval on MTEB(Multilingual), it has a higher maximum sequence length (see \autoref{fig:performance-x-speed}) and outperforms the multilingual-e5-large-instruct on MTEB(Code) and MTEB(eng, v2).



\begin{figure}
    \centering
    \includegraphics[width=.95\linewidth]{figures/perf_n_speakers_plot.pdf}
    \caption{Performance rank of top 3 multilingual models on languages in \texttt{MTEB(Europe)} and \texttt{MTEB(Indic)} and by the number of native speakers.
    We see that Mistral-based models are outperformed by multilingual-e5-large-instruct on lower-resource languages, despite it having substantially fewer parameters.
    }
    \label{fig:perf_per_speakers}
\end{figure}

\textbf{Discrepancies in Multilingual benchmarks ranking seem to stem from discrepancies in pre-training.} While the multilingual benchmarks obtain seemingly similar performance rankings, we see a few notable discrepancies. These discrepancies seem to mainly stem from a narrow multilingual focus (GritLM-7B, e5-mistral-7b-instruct, multilingual-mpnet-base) during training, resulting in disproportionally higher performance on the targeted languages (typically mid-high resource or European ones). 
These are typically outperformed by the multilingually pre trained XLM-Roberta-based multilingual-e5-large-instruct on lower-resource languages in \texttt{MTEB(Europe)} and all languages in \texttt{MTEB(Indic)} (see \autoref{fig:perf_per_speakers}),
despite being substantially smaller than Mistral models,
the performance of which steadily decreases and becomes more volatile for languages with increasingly lower number of native speakers and this trade-off is well-known~\citep{xue2020mt5}. 

Besides these, we observe the expected detrimental performance of English models (all-MiniLM-L12, all-MiniLM-L6, all-mpnet-base) applied to non-English languages and a relatively high bitext performance of LaBSE (see ~\autoref{fig:multilingual_effect}).

\begin{figure}
    \centering
    \includegraphics[width=0.60\linewidth]{figures/multi_english_comparison.pdf}
    \caption{Performance difference on \texttt{MTEB(eng, v1)} (flag) and \texttt{MTEB(Multilingual)} (globe).}
    \label{fig:multilingual_effect}
\end{figure}


\paragraph{MTEB(eng, v1) vs. zero-shot MTEB(eng, v2)}
\label{sec:mteb_english_vs_lite}

We compare the performance of \texttt{MTEB(eng, v1)} and \texttt{MTEB(eng, v2)} in \autoref{fig:mteb_v_mteb-lite} obtaining a Spearman correlation of 0.90, $p<0.0001$ (Pearson 0.96, $p<0.0001$). For the precise scores, we refer to \autoref{sec:appendix-mteb-lite-perf}. This includes a reduction from 56 to 40 tasks along with optimized task runtime speeding up the runtime on the benchmark (3.11 hours for GritLM-7B and 0.81 hours for all-MiniLM-L12 on an H100). We see that notably, the smaller English models (all-MiniLM-L12, all-MiniLM-L6, all-mpnet-base) perform worse on the new benchmark. This is likely because they were trained on MS MARCO and Natural questions, which were removed as part of the benchmark conversion to a zero-shot benchmark.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/old_lite_comparison.pdf}
    \caption{Performance on \texttt{MTEB(eng, v1)} and \texttt{MTEB(eng, v2)}}.
    \label{fig:mteb_v_mteb-lite}
\end{figure}