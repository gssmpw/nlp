
\section{Benchmark Construction and Overview}
\subsection{Benchmark creation}
\label{sec:benchmark-creation}

The following section introduces benchmarks created as a part of the MMTEB open contribution, which aren't introduced within the main article. MTEB additionally includes a variety of benchmark including the language-specific, notably the original English MTEB, \texttt{MTEB(eng, v2)} \citep{muennighoff2023mteb}, the Scandinavian embedding benchmark \texttt{MTEB(Scandinavian)} \citep{enevoldsen2024scandinavian}, the French benchmark \texttt{MTEB(fra)} \citep{ciancone2024extending}, the German benchmark \texttt{MTEB(deu)} \citep{wehrli2024germantextembeddingclustering}, the Korean benchmark \texttt{MTEB(kor)}, the Chinese benchmark \citep{xiao2024cpack}, the Polish benchmark \texttt{MTEB(pol)} \citep{poswiata2024plmteb}. Along with these MTEB also include an instruction based retrieval based benchmark \texttt{MTEB(FollowIR)} \citep{weller2024followir}, a benchmark for law \texttt{MTEB(Law)}, the bitext section of the MINER benchmark \texttt{MINERSBitextMining} target at low resource languages \citep{winata2024miners}, and the CoIR benchmark for code retrieval \texttt{CoIR} \citep{li2024coircomprehensivebenchmarkcode}. For this benchmark, we refer to their associated paper and pull requests.

For an up-to-date overview of maintained benchmarks please see the benchmark registry.\footnote{\url{https://github.com/embeddings-benchmark/mteb/blob/main/mteb/benchmarks.py}}

\noindent
\textbf{MTEB(rus)} \citep{snegirev2024russianfocusedembeddersexplorationrumteb}: Although Russian has approximately 258 million speakers world-wide, it was almost completely absent from the original benchmark and represented only in few multilingual datasets (e.g., MassiveIntentClassification). To address this problem, we included a number of Russian datasets in the new multilingual benchmark. For this, we selected popular Russian time-tested and community-tested datasets representing the main MMTEB tasks. Additionally, we performed data cleaning and automatic filtering, where necessary, and formatted datasets in the MMTEB format. The final Russian part includes 18 datasets covering 7 main tasks: Classification (7 datasets), Clustering (3 datasets), MultiLabelClassification (2 tasks), PairClassification (1 task), Reranking (1 task), Retrieval (2 tasks), and STS (2 tasks). This dataset was manually constructed.


\textbf{RAR-b}: The Reasoning as Retrieval Benchmark (RAR-b) \citep{xiao2024rar} evaluates reasoning-level understanding abilities stored in embedding models, and assesses whether correct answers to reasoning questions can be retrieved as top similar to queries, under w/ and w/o instruction settings. The benchmark provides insights into whether representations of nuanced expressions are aligned and well-encoded by current embedding models, going beyond the established reliance on evaluating with STS or traditional topical-level IR tasks.

The benchmark puts together 17 tasks made from 15 datasets (with reasoning questions from 12 datasets and 3 extra datasets to enlarge the corpus), covering 1) commonsense reasoning: WinoGrande, PIQA, SIQA, $\alpha$NLI, HellaSwag, ARC-Challenge, Quail, CSTS \citep{sakaguchi2021winogrande,bisk2020piqa,sap2019social,bhagavatula2019abductive,zellers2019hellaswag,clark2018think,rogers2020getting,deshpande2023csts}, 2) temporal reasoning \citep{tan2023towards}, 3) spatial reasoning: SpartQA \citep{mirzaee2021spartqa}, 4) numerical reasoning: GSM8K, MATH \citep{hendrycks2021measuring,cobbe2021training,yu2023metamath}, and 5) symbolic reasoning: HumanEvalPack and MBPP \citep{husain2019codesearchnet,austin2021program,chen2021evaluating,muennighoff2023octopack}. The comprehensive assessment provides an early checkpoint for abilities envisioned to be necessary for next-generation embedding models \citep{xiao2024rar}. 

\textbf{MTEB(Europe)}:
We begin by selecting 56 official languages of the European Union, along with languages recognized by Schengen-area countries, such as Norwegian Bokm√•l, Icelandic, Romani, and Basque. This initial selection results in 420 tasks. We then reduce this selection by filtering out machine-translated datasets, datasets with unclear licenses, and highly specialized datasets (e.g., code retrieval datasets). Additionally, we remove tasks such as \texttt{AfriSentiClassification}, which, while containing European languages, primarily target African or Indic languages. After these exclusions, 228 tasks remain.
Next, we run a representative selection of models (see Section [\ref{sec:models}]) and iteratively filter out the most predictable tasks (see Section [\ref{sec:taskselection}]).
To preserve language diversity and ensure fair representation across task categories, we avoid removing any task if it would eliminate a language from a particular task category. Furthermore, we retain tasks where the mean squared error between predicted and observed performance exceeds 0.5 standard deviations. This process continues until the most predictable tasks yield a Spearman correlation of less than 0.8 between predicted and observed scores, or until no further tasks can be removed. Ultimately, this results in a final selection of 96 tasks. Finally, contributors proficient in the target languages review the selected tasks, replacing some manually with higher-quality alternatives if necessary.

\textbf{MTEB(Indic)}:
This benchmark is constructed similarly to the previous European benchmark but focuses on a set of Indic languages.\footnote{The following iso639-3 codes: \texttt{asm, awa, ben, bgc, bho, doi, gbm, gom, guj, hin, hne, kan, kas, mai, mal, mar, mni, mup, mwr, nep, npi, ori, ory, pan, raj, san, snd, tam, tel, urd}}
Initially, we selected 55 tasks. After manual filtering, 44 tasks remain, and following task selection and review, the final benchmark contains 23 tasks.

\subsection{Benchmark task overview}
\label{sec:appendix-benchmark-overview}

The following tables give an overview of the tasks available within constructed benchmarks. For more information about the specific tasks, we refer to the task metadata available through the mteb package. \footnote{\url{https://github.com/embeddings-benchmark/mteb}}

\begin{itemize}
    \item \autoref{tab:mteb_multilingual_task_overview1} and \autoref{tab:mteb_multilingual_task_overview2}: Gives an overview of the `MTEB(Multilingual)` benchmark
    \item \autoref{tab:mteb_europe_task_overview}: Gives an overview of the `MTEB(Europe)` benchmark
    \item \autoref{tab:mteb_indic_task_overview}: Gives an overview of the `MTEB(Indic)` benchmark
    \item \autoref{tab:mteb_lite_task_overview}: Gives an overview of the `MTEB(eng, v2)` benchmark
    \item \autoref{tab:mteb_code_task_overview}: Gives an overview of the `MTEB(Code)` benchmark
\end{itemize}

\input{tables/mteb_multilingual_overview}
\input{tables/mteb_europe_tasks_overview}
\input{tables/mteb_indic_tasks_overview}
\input{tables/mteb_eng_lite_tasks_overview}
\input{tables/mteb_code_tasks_overview}

\subsection{Performance on \texttt{MTEB(eng, v2)}}
\label{sec:appendix-mteb-lite-perf}

\autoref{tab:mteb_lite_results} show the performance of our representative set of model on \texttt{MTEB(eng, v2)}.

\input{tables/mteb_eng_lite_results}

\subsection{Performance on \texttt{MTEB(Code)}}
\label{sec:appendix-mteb-code-perf}

\autoref{tab:mteb_code_results} show the performance of our representative set of model on \texttt{MTEB(Code)}.

\input{tables/mteb_code_results}