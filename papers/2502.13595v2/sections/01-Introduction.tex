\section{Introduction}
\label{sec:intro}
Text embeddings are used in many applications, such as semantic search \citep{reimers2019sentencebert,muennighoff2022sgpt, hendriksen2023scene,winata2023efficient,winata2024miners} and classification tasks \citep{wang-etal-2018-glue,wang2019superglue}. Additionally, text embeddings play a crucial role in retrieval-augmented generation (RAG; \citealt{borgeaud2022improving,lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}), and often provide significant gains in performance on low- to mid-resource languages, enabling the incorporation of previously inaccessible information. Despite the wide range of applications, there's a lack of benchmarks that evaluate text embeddings across multiple domains, languages, and tasks.
Existing benchmarks tend to focus on specific domains, demarcated by subject (e.g., medical, legal, fiction ~\citep{thorne2018fever}), particular tasks (e.g., retrieval ~\citep{thakur2021beir}), literary type (e.g., fiction, and non-fiction) 
 or form (e.g., spoken and written).  Embeddings also tend to focus on a subset of languages~\citep{norregaard-derczynski-2021-danfever}.


While recent efforts \citep{thakur2021beir, muennighoff2023mteb, zhang2022making} have aimed to broaden the scope by encompassing more tasks, domains, or languages \citep{cohan2020specter, wrzalik-krechel-2021-gerdalir}, a large gap in language coverage remains. This work bridges this gap by creating a benchmark that includes a much broader range of low- to mid-resource languages, along with broader coverage of domains and task categories.
To create such an expansive benchmark, we initiated a large-scale, open collaboration. Contributors include native speakers from diverse linguistic backgrounds, NLP practitioners, academic and industry researchers, and enthusiasts. To ensure high-quality submissions, each dataset required systematic tests, detailed metadata, and a review.

The result of this extensive collaborative effort is MMTEB, the \textbf{M}assive \textbf{M}ultilingual \textbf{T}ext \textbf{E}mbedding \textbf{B}enchmark, which comprises more than 500 distinct tasks across 10 task categories, covering over 250 languages, and spans a wide array of domains such as fiction, social media, medical texts, and technical programming documentation. It also integrates recent, high-quality benchmarks that test a model's capabilities in following instructions \citep{winata2021language,weller2024followir}, embedding long documents \citep{zhu2024longembed}, solving reasoning tasks \citep{xiao2024rar,su2024brightrealisticchallengingbenchmark}, and cross-lingual retrieval \citep{franco-salvador-etal-2014-knowledge}. For an overview see \autoref{fig:overview}.

Given the known co-occurrence of limited computational resources and low-resource languages, often referred to as the ``low-resource double bind''~\citep{ahia-etal-2021-low-resource}, we made it our goal to make the MMTEB benchmark accessible to low-resource communities.
Evaluating models extensively is often resource-intensive. For example, evaluating a single 7B large language model (LLM) on the HELM benchmark consumes over 4,000 GPU hours~\citep{liang2022holistic}.
Similarly, the English MTEB (henceforth referred to as \texttt{MTEB(eng, v1)}) benchmark requires up to two days of processing on a single A100 GPU even for moderately sized LLMs~\citep{muennighoff2023mteb, behnamghader2024llm2vec}. These high resource demands pose a challenge for low-resource language communities that often lack access to powerful computing resources. MMTEB addresses these challenges by expanding its coverage and optimizing the evaluation process. It significantly reduces computational cost (3.11 hours on an H100 GPU for a 7B model) by using only 2\% of the original documents (6\% of the original number of characters) while maintaining sensitivity as a benchmark to rank models accurately.
% We propose in this work an extension of MTEB to multiple languages including low-resource ones. We also provide a faster version of the benchmark (by simplifying the evaluation of some tasks and using downsampling).

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/mmteb_overview_wide_centered.pdf}
    \caption{An overview of MMTEB. The boxes represent the overall task categories with a sample of task categories represented within each. Blue borders represent closely-related task categories.}
    \label{fig:overview}
    \vspace{-1.5mm}
\end{figure*}