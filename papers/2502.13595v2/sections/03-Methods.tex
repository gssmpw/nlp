\section{MMTEB Construction}

\subsection{Open science effort}
\label{sec:open-source-effort}
To ensure the broad applicability of MMTEB across various domains, we recruited a diverse group of contributors. We actively encouraged participation from industry professionals, low-resource language communities, and academic researchers. To clarify authorship assignment and recognize desired contributions, we implemented a point-based system, similar to \citet{lovenia2024seacrowd}.
To facilitate transparency, coordination was managed through GitHub. 
A detailed breakdown of contributors and the point system can be found in Appendix~\ref{sec:contributions}.

\subsection{Ensuring task quality}

To guarantee the quality of the added tasks,\footnote{A task includes a dataset and an implementation for model evaluation.} each task was reviewed by at least one of the main contributors. In addition, we required task submissions to include metadata fields. These fields included details such as annotation source, dataset source, license, dialects, and citation information. Appendix~\ref{appendix:task_metadata} provides a comprehensive description of each field. 

Furthermore, we ensured that the performance on submitted tasks fell within a reasonable range to avoid trivially low or unrealistically high performance. Therefore, we required two multilingual models to be run on the task; multilingual-e5-small
% \footnote{\url{https://huggingface.co/intfloat/multilingual-e5-small}}
~\citep{wang2022text} and MiniLM-L12
% \footnote{\url{https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}}
~\citep{reimers2019sentencebert}.
A task was examined further if the models obtained scores close to a random baseline (within a 2\% margin), a near-perfect score, or if both models obtained roughly similar scores. 
% Similarly, if the two models obtained roughly similar scores. 
These tasks were examined for flawed implementation or poor data quality. Afterwards, a decision was made to either exclude or include the task. We consulted with contributors who are familiar with the target language whenever possible before the final decision. A task could be included despite failing these checks. For example, scores close to the random baseline might be due to the task's inherent difficulty rather than poor data quality.

\subsection{Accessibility and benchmark optimization}
\label{sec:benchmark-optimization}

As detailed in \autoref{sec:intro}, extensive benchmark evaluations often require significant computational resources. This trend is also observed in \texttt{MTEB(eng, v1)} \citep{muennighoff2023mteb}, where running moderately sized LLMs can take up to two days on a single A100 GPU. Accessibility for low-resource communities is particularly important for MMTEB, considering the common co-occurrence of computational constraints \citep{ahia-etal-2021-low-resource}. 

Below, we discuss three main strategies implemented to make our benchmark more efficient.  We additionally elaborate further code optimization in Appendix~\ref{sec:appendix-code-optimizations}.

\subsubsection{Downsampling and caching embeddings} 
The first strategy involves optimizing the evaluation process by downsampling datasets and caching embeddings. Encoding a large volume of documents for tasks such as retrieval and clustering can be a significant bottleneck in evaluation. Downsampling involves selecting a representative subset of the dataset and reducing the number of documents that require processing. Caching embeddings prevents redundant encoding by using already processed documents.

\paragraph{Clustering.} In MTEB, clustering is evaluated by computing the v-measure score \citep{rosenberg-hirschberg-2007-v} on text embeddings clustered using k-means. This process is repeated over multiple distinct sets, inevitably resulting in a large number of documents being encoded. To reduce this encoding burden, we propose a bootstrapping approach that reuses encoded documents across sets. We first encode a 4\% subsample of the corpus and sample 10 sets without replacement. Each set undergoes k-means clustering, and we record performance estimates. For certain tasks, this approach reduces the number of documents encoded by 100$\times$. In Appendix \ref{sec:task-construction}, we compare both approaches and find an average speedup of 16.11x across tasks, while preserving the relative ranking of models (Average Spearman correlation: 0.96).

\paragraph{Retrieval.} A key challenge in retrieval tasks is encoding large document collections, which can contain millions of entries \cite{nguyenhendriksen2024multimodal}. To maintain performance comparable to the original datasets while reducing the collection size, we adopted the TREC pooling strategy \citep{buckley2007bias,soboroff2003building}, which aggregates scores from multiple models to select representative documents.\footnote{We utilized a range of models: BM25 for lexical hard negatives, e5-multilingual-large as a top-performing BERT-large multilingual model, and e5-Mistral-Instruct 7B, the largest model leveraging instruction-based data.}  For each dataset, we retained the top 250 ranked documents per query, a threshold determined through initial tests that showed negligible differences in absolute scores and no changes in relative rankings across representative models (see Appendix~\ref{app:retrieval_downsample} for details on downsampling effects). These documents are merged to form a smaller representative collection. For datasets exceeding 1,000 queries, we randomly sampled 1,000 queries, reducing the largest datasets from over 5 million documents to a maximum of 250,000. This approach accelerated evaluation while preserving ranking performance.

\paragraph{Bitext Mining.} We apply similar optimization to bitext mining tasks. Some datasets, such as Flores \citep{nllb2022flores} share the same sentences across several language pairs (e.g., English sentences are the same in the English-Hindi pair and the English-Bosnian pair). By caching the embeddings, we reduce the number of embedding computations, making it linear in the number of languages instead of quadratic. For the English documents within Flores this results in a reduction of documents needed to be embedded from ~410,000 in \texttt{MTEB(eng, v1)} to just 1,012 in our benchmark.

\subsubsection{Encouraging smaller dataset submissions} 
\label{sec:smaller-dataset-submissions}
The second strategy focused on encouraging contributors to downsample datasets before submission. To achieve this, we used a stratified split based on target categories. This helped us to ensure that the downsampled datasets could effectively differentiate between candidate models. To validate the process, we compared scores before and after downsampling. For details, we refer to Appendix~\ref{sec:speedup}.

\subsubsection{Task Selection}
\label{sec:taskselection}

To further reduce the computation overhead we seek to construct a task subset that can reliably predict task scores outside the subset.

For task selection, we followed an approach inspired by \citet{Xia2020PredictingPerformance}. We seek to estimate the model $m_i \in M$ scores $s_{t, m_i}$ on an unobserved task $t$ based on scores on observed tasks $s_{j, m_k} \in S, j \neq t$. This allows us to consider the performance of tasks as features within a prediction problem. Thus we can treat task selection as feature reduction, a well-formulated task within machine learning. Note that this formulation allows us to keep the unobserved task arbitrary, representing generalization to unseen tasks \citep{cholletMeasureIntelligence2019}. We used a backward selection method, where one task is left out to be predicted, an estimator\footnote{We use the term ``estimator" to differentiate between the evaluated embedding model. For our estimator, we use linear regression.}
is fitted on the performance of all models except one, and the score of the held-out model is predicted. This process is repeated until predicted scores are generated for all models on all tasks.
% We used a backward selection method, where one model-task pair is left out to be predicted. An estimator\footnote{We use the term "estimator" to differentiate between the evaluated embedding model. For our estimator, we use linear regression.} is fitted on the performance scores of all other model-task pairs, and the score for the held-out pair is predicted. This process is repeated until predicted scores are generated for all models across all tasks.
The most predictable task is then removed, leaving the estimators in the task subset group. Optionally, we can add additional criteria to ensure task diversity and language representation. Spearman's rank correlation was chosen as the similarity score, as it best preserved the relative ranking when applied to the \texttt{MTEB(eng, v1)}.


\subsection{Benchmark construction}
\label{sec:benchmarkconstruction}
From the extensive collection of tasks in MMTEB, we developed several representative benchmarks, including a highly multilingual benchmark, \texttt{MTEB(Multilingual)}, as well as regional geopolitical benchmarks, \texttt{MTEB(Europe)} and \texttt{MTEB(Indic)}. Additionally, we introduce a faster version of \texttt{MTEB(eng, v1)} \citep{muennighoff2023mteb}, which we refer to as \texttt{MTEB(eng, v2)}. MMTEB also integrates domain-specific benchmarks like CoIR for code retrieval \citep{li2024coircomprehensivebenchmarkcode} and LongEmbed for long document retrieval  \citep{zhu2024longembed}. MMTEB also introduces language-specific benchmarks, extending the existing suite that includes Scandinavian \citep{enevoldsen2024scandinavian}, Chinese \citep{xiao2024cpack}, Polish \citep{poswiata2024plmteb}, and French \citep{ciancone2024extending}. For an overview of the benchmarks, we refer to Appendix~\ref{sec:benchmark-creation}.

In the following section, we detail a methodology that we designed to create more targeted and concise benchmarks. This methodology includes: 1) clearly defining the initial scope of the benchmark \textbf{(Initial Scope)}, 2) reducing the number of tasks by iterative task selection tasks based on intertask correlation \textbf{(Refined Scope)}, and 3) performing a thorough manual review \textbf{(Task Selection and Review)}. We provide an overview in \autoref{tab:numberoftasks}.

In addition to these benchmarks, we provide accompanying code to facilitate the creation of new benchmarks, to allow communities and companies to create tailored benchmarks. In the following, we present \texttt{MTEB(Multilingual)} and \texttt{MTEB(eng, v2)} as two example cases. For a comprehensive overview of benchmark construction and the tasks included in each benchmark, we refer to Appendix~\ref{sec:appendix-benchmark-overview}.
\newline




\begin{table}
\centering
{\footnotesize
    \begin{tabular}{lcccc}
\toprule
\textbf{Benchmark} & \textbf{Initial Scope}  & \textbf{Refined Scope} & \textbf{Task Selection and Review} \\
\midrule
\texttt{MTEB(Multilingual)} & >500 & 343 & 132 \\
\texttt{MTEB(Europe)} & 420 & 228 & 74 \\
\texttt{MTEB(Indic)} & 55 & 44 & 23 \\
\texttt{MTEB(eng, v2)} & 56 & 54 & 41 \\
\bottomrule
    \end{tabular}
}
    \caption{Number of tasks in each benchmark after each filtering step. The initial scope includes tasks relevant to the benchmark goal, notably language of interest. The refined scope further reduced the scope, e.g. removing datasets with underspecified licenses.}
    \label{tab:numberoftasks}
    \vspace{-3mm}
\end{table}

\noindent
\header{MTEB(Multilingual)}:
We select all available languages within MMTEB as the initial scope of the benchmark. This results in 550 tasks. We reduce this selection by removing machine-translated datasets, datasets with under-specified licenses, and highly domain-specific datasets such as code-retrieval datasets. This results in 343 tasks covering $>$250 languages. Following this selection, we evaluate this subset using a representative selection of models (See Section~\ref{sec:models}) and apply task selection to remove the most predictable tasks. To ensure language diversity and representation across task categories, we avoid removing a task that would eliminate a language from the respective task category. Additionally, we did not remove a task if the mean squared error between predicted and observed scores exceeded 0.5 standard deviations. This is to avoid inadvertantly overindexing to easier tasks. The process of iterative task removal (Section~\ref{sec:taskselection}) is repeated until the most predictable held-out task obtained a Spearman correlation of less than 0.8 between predicted and observed scores, or if no tasks were available for filtering. This results in a final selection of 131 diverse tasks. Finally, the selected tasks were reviewed, if possible, by contributors who spoke the target language. If needed, the selection criteria were updated, and some tasks were manually replaced with higher-quality alternatives. 
\newline

\noindent
\header{MTEB(eng, v2)}:
Unlike the multilingual benchmarks which target a language group, this benchmark is designed to match \texttt{MTEB(eng, v1)}, incorporating computational efficiencies (see Section~\ref{sec:benchmark-optimization}) and reducing the intertask correlation using task selection. To prevent overfitting, we intend it as a zero-shot benchmark, excluding tasks like MS MARCO \citep{NguyenRSGTMD16} and Natural Questions \citep{kwiatkowski2019natural}, which are frequently used in fine-tuning.

We start the construction by replacing each task with its optimized variant. This updated set obtains a Spearman correlation of $0.97$, $p<.0001$ (Pearson $0.99$, $p<.0001$) with \texttt{MTEB(eng, v1)} using mean aggregation for the selected models  (see \autoref{sec:models}).
The task selection process then proceeds similarly to \texttt{MTEB(Multilingual)}, ensuring task diversity by retaining a task if its removal would eliminate a task category. Tasks, where the mean squared error between predicted and observed performance exceeds 0.2 standard deviations, are also retained. This process continues until the most predictable held-out task yields a Spearman correlation below 0.9 between predicted and observed scores. The final selection consists of 41 tasks. We compare this with \texttt{MTEB(eng, v1)} \citep{muennighoff2023mteb} in Section~\ref{sec:mteb_english_vs_lite}.
