\section{Experimental Settings}

\subsection{Models} 
\label{sec:models}

We select a representative set of models, focusing on multilingual models across various size categories. We benchmark the multilingual LaBSE \citep{feng-etal-2022-language}, trained on paraphrase corpora, English and multilingual versions of MPNet \citep{song2020mpnet}, and MiniLM \citep{wang-etal-2021-minilmv2} model, trained on diverse datasets. We also evaluate the multilingual e5 series models \citep{wang2024multilingual, wang2022text} trained using a two-step approach utilizing weak supervision. Additionally, to understand the role of scale as well as instruction finetuning, we benchmark GritLM-7B \citep{muennighoff2024generative} and e5-multilingual-7b-instruct \citep{wang2023improving}, which are both based on the Mistral 7B model \citep{jiang2023mistral}.

Revision IDs, model implementation, and prompts used are available in \autoref{sec:appendix-models}. We ran the models on all the implemented tasks to encourage further analysis of the model results.
Results, including multiple performance metrics, runtime, CO2 emissions, model metadata, etc., are publicly available in the versioned results repository.\footnote{\url{https://github.com/embeddings-benchmark/results}.}

\subsection{Evaluation Scores}
For our performance metrics, we report average scores across all tasks, scores per task category, and weighted by task category. We compute model ranks using the Borda count method \citep{NEURIPS2022_ac4920f4}, derived from social choice theory. This method, which is also employed in election systems based on preference ranking, has been shown to be more robust for comparing NLP systems. To compute this score, we consider each task as a preference voter voting for each model, and scores are aggregated according to the Borda Count method. In the case of ties, we use the tournament Borda count method.

\subsection{Multilingual performance} 

While MMTEB includes multiple benchmarks (see Appendix~\ref{sec:benchmark-creation}), we select three multilingual benchmarks to showcase. These constitute a fully multilingual benchmark \texttt{MTEB(Multilingual)} and two targeting languages with varying levels of resources: \texttt{MTEB(Europe)} and \texttt{MTEB(Indic)}. The performance of our selected models on these tasks can be seen in \autoref{tab:overall-performance}.
For performance metrics per task, across domains, etc., we refer to \autoref{sec:fullresults}. 
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/performance-x-parameters.pdf}
\caption{Mean performance across tasks on MTEB(Multilingual) according to the number of parameters. The circle size denotes the embedding size, while the color denotes the maximum sequence length of the model. To improve readability, only certain labels are shown. We refer to the public leaderboard
%\footnote{https://huggingface.co/spaces/mteb/leaderboard} 
for interactive visualization. We see that the notably smaller model obtains comparable performance to Mistral 7B and GritLM-7B, note that these overlap in the figure due to the similarity of the two models.}
    \label{fig:performance-x-speed}
    \vspace{-3mm}
\end{figure}


\input{tables/mteb_results_overall}
