\section{Overview and Construction of Tasks}
\label{sec:dataset_construction}

In this appendix, we first provide an overview of existing tasks in MTEB benchmark and newly introduced tasks in our benchmark (Section~\ref{sec:task-intro}). We proceed by explaining how the tasks were constructed (Section~\ref{sec:task-construction}) from existing datasets. Lastly, we introduce newly constructed datasets specifically designed for MMTEB (Section~\ref{sec:newds}).

\subsection{Introduction to benchmark tasks}
\label{sec:task-intro}

\noindent
\textbf{Classification} First, a train set is constructed by sampling $n$ (8-16) samples for each label. If only a test set is available, a section is split off as a training set. Both sets are then embedded and used to train a logistic regression using a maximum of 100 iterations. Afterwards, performance metrics are calculated. For robustness, this process is repeated 10 times.

\noindent
\textbf{Pair classification}
For two paired texts, the goal is to predict the label. Examples of such tasks include paraphrase detection or duplicate detection. The task is solved by embedding all documents and then computing the distance either using a model-specified metric, cosine, euclidean, dot product, or Manhattan. Using the best binary threshold, performance metrics are computed.

\noindent
\header{Bitext mining}
The dataset consists of matching pairs of sentences, and the goal is to find the match. All matching pairs of sentences are embedded, and the closest match is found using cosine similarity, and metrics are reported.

\noindent
\header{Clustering and hierarchical clustering}
Clustering starts with a set of documents and an associated set of labels. 
First we embed all documents, then take subsets of the data of size $k$ for each of 10 consecutive experiments.
All the documents are embedded, and a set of size $k$ is sampled from the embedded documents.
The embeddings are then clustered using K-means clustering, and performance metrics are calculated between the estimated clusters and labels.
If the clustering problem is hierarchical, this procedure is repeated for each level of the hierarchy separately.
Hierarchical tasks were formerly either split into multiple tasks, or later levels of the cluster hierarchy were ignored.

Note that this formulation differs from that of MTEB in that the sets are randomly sampled from the embedded documents instead of being specified a-priori.
This drastically reduced runtime as one document can be used in multiple subsets without the need to embed it multiple times.
The new formulation also allows us to gain a robust estimate of performance with a lower number of documents.


\noindent
\header{Retrieval} 
Retrieval tasks consist of a corpus, queries, and mapping between the queries and their relevant documents. The goal is to retrieve these relevant documents. Both queries and documents are embedded using the model. We allow these to be embedded differently depending on the model. For each query, the corpus documents are ranked using a similarity score, and performance metrics are calculated based on the reference mapping.

\noindent
\header{Multi-label classification}
Classification tasks in MTEB were previously limited to utilizing only one label per document.
As such, some, otherwise useful multi-label classification tasks had to be dropped or reformulated.
We addressed this by introducing a multi-label classification task type
Similarly to our novel clustering task, we down sample training sets for 10 experiments.
We limit the training sets to include 8 instances of each unique label, and train a K Nearest-Neighbours classifier.
Every classifier is then evaluated on the same test set.
We opted for Accuracy, $F_1$ and Label Ranking Average Precision (LRAP) as evaluation metrics.

\noindent
\header{Instruction retrieval}
Instruction retrieval builds on the traditional retrieval task by incorporating detailed instructions alongside the queries. Unlike standard retrieval, where queries are usually brief keywords, instruction retrieval pairs each query with a comprehensive instruction that outlines the criteria for document relevance. These instructions are specific to each query and not generic to the entire dataset.
Therefore, the task involves using both the query and its associated instruction to retrieve relevant documents from the corpus. For the main metric, we use Robustness@10.

\noindent
\header{Reranking}
Similar to the retrieval task, reranking includes a corpus, query, and a list of relevant and irrelevant reference texts. The aim is to rank the results according to their relevance to the query. References and queries are embedded and references are compared to the query using cosine similarity. The resulting ranking is scored for each query and averaged across all queries, and performance metrics computed. For the main metric, we use MAP@1000.

\noindent
\header{Semantic text similarity} 
Semantic text similarity (STS) tasks consist of sentence pairs, where the goal is to determine their similarity. Labels are continuous scores, with higher numbers indicating more similar sentences. All sentences are embedded using the model, and the similarity of the pair is computed using various distance metrics, allowing for model-specified similarity metrics. Distances are benchmarked with ground truth similarities using Pearson and Spearman correlations. Spearman correlation based on highest similarity serves as the main metric \citep{reimers-etal-2016-task}


\subsection{Task construction}
\label{sec:task-construction}

This section outlines our approach to constructing tasks, primarily from pre-existing data.
For details on the newly introduced dataset in MMTEB, we refer to Section \ref{sec:newds}.

Task construction from existing datasets consisted of a number of steps to ensure that the task is compatible with formulations in the benchmark and matches our standards:
\begin{enumerate*}
    \item \textit{Dataset preprocessing}: we start by applying minimal additional processing to ensure the data is in the required format.
    \item \textit{Dataset size reduction}: to maintain manageable evaluation times, we proceed by reducing dataset size whenever applicable.
    \item \textit{Relevance filtering}: To ensure the datasets are relevant for the types of tasks being evaluated, we apply relevance-based dataset filtering.
    \item \textit{Differentiation testing}: we assess the task's ability to differentiate between the performance of two candidate models.
\end{enumerate*}


For further details on dataset transformations for specific tasks, we refer to the \texttt{dataset\_transform} method implementation for each task.


\header{Classification and pair classification}
For both classification tasks, we used existing datasets with minimal adjustments, primarily trimming them down to more manageable sizes. For performance evaluation, we rely on such metrics as $F_1$ score, accuracy, or average precision. Whenever feasible, we align our choice of the primary metric with those used in related publications. If no specific guidance exists, we default to accuracy for general classification tasks and average precision for pairwise classification. In scenarios with significant class imbalance, the $F_1$ score is prioritized.

\header{Bitext mining}
Bitext mining tasks were constructed using established paired datasets. Similar to the classification tasks, the primary focus was on adjusting the dataset sizes to maintain the same model rank while reducing computational load.
$F_1$ scores were chosen to be the primary metric, unless specified otherwise.

\header{Clustering and hierarchical clustering}
Clustering tasks were derived from existing corpora, such as news articles or encyclopedic entries. The source datasets typically included categories or labels assigned by their original authors or publishers. In some cases, like the SNL and VG datasets \citep{navjord2023beyond}, which featured hierarchical labels, we reformulated the tasks from flat to hierarchical clustering.

\header{Retrieval}
A variety of tasks were integrated as retrieval tasks, including existing retrieval, question-answer, and news datasets. For question-answer datasets, the questions were used as queries, and the answers formed the corpus, with correct answers identified as properly retrieved documents. In news datasets, headlines were treated as queries, and both the full articles were considered part of the corpus, with matched summaries and articles serving as relevant documents. For the primary metric, we use \texttt{nDCG@10}, unless otherwise specified by the dataset publication.

\header{Multi-label classification}
For multi-label classification, we used existing datasets that required minimal adjustments. A critical aspect of these tasks was maintaining the balance of label distributions across the training and evaluation splits. To achieve this, we employed advanced stratification techniques \citep{szymanski17a, sechidis2011stratification} that consider higher-order relationships between labels, ensuring balanced samples and improved classification quality. For the main metric, we use accuracy.

\header{Instruction Retrieval}
For instruction retrieval tasks, we incorporated datasets like FollowIR~\citep{weller2024followir,weller2025mfollowirmultilingualbenchmarkinstruction}, which consist of comprehensive narratives created by professional assessors. These datasets were initially developed for TREC shared tasks and included rich, context-heavy queries to evaluate retrieval systems' performance on more intricate retrieval problems.

\header{Reranking}
For reranking tasks, we adapted datasets covering a range of topics and languages, including academic paper ranking, news articles \citep{wu2020mind}, QA pair relevance from online platforms, and passage ranking \citep{xie2023t2ranking}. For the primary metric, we use MAP unless otherwise specified by the dataset publication.

\header{Semantic text similarity}
For STS tasks, we adapted well-known benchmarks like STSbenchmark \citep{may2024stsb} and cross-lingual STS datasets from SemEval \citep{agirre2015semeval}. We also adapted paraphrase datasets in various languages, such as the Russian ParaPhraser \citep{pivovarova2017paraphraser} and the Finnish Paraphrase Corpus \citep{kanerva-etal-2021-finnish}. As the main metric, we use Spearman correlation based on the highest similarity~\citep{reimers-etal-2016-task}.


\subsection{Novel datasets}
\label{sec:newds}

This section introduces task specifically created as a part of the MMTEB contributions. For information on how existing datasets were adapted to MTEB we refer to \autoref{sec:dataset_construction}.

\textbf{PublicHealthQA}: This retrieval task is built on top of a novel dataset containing question-and-answer pairs in Public Health, specifically related to the COVID-19 disease. They are sourced from Q\&A pages and Frequently Asked Questions (FAQ) sections of the Centers for Disease Control and Prevention (CDC) and World Health Organization (WHO) websites. They were produced and collected between 2019-12 and 2020-04.

\textbf{WebLINXReranking}: This is a novel HTML reranking task derived from WebLINX, a benchmark for training and evaluating web agents with conversational capabilities \citep{lù2024weblinx}. Whereas the original work introduces a retrieval task with the goal of retrieving HTML elements using a conversational context, we propose the first task with the goal of reranking HTML elements based on their relevance for actions executed in web environments, including clicks, hovers, and text insertions.

\textbf{WikiClustering}: is a multilingual clustering benchmark based on Wikipedia's main topic classifications. The goal is to create a clustering benchmark that works for multiple languages.

To construct a WikiClustering dataset for a given language, we apply the following steps. First, download the wiki dump of the categories, the articles, and the category links. Second, we find the main topic classifications for all articles. The main topic classifications can be found by looking at the category page for the language\footnote{for details, we refer to \url{https://en.wikipedia.org/wiki/Category:Main_topic_classifications for English}}. We only use the first paragraph of each article to construct a paragraph-to-paragraph (P2P) task similar to other P2P tasks within MTEB. Third, we filter out articles with more than one main topic and remove any topic with only one article associated with it. This step avoids ambiguity in the clustering task. Finally, we sample 2048 articles with associated main topics. 

While the WikiClustering benchmark can be extended to any language with main topic classifications, it is currently implemented for the following: Bosnian, Catalan, Czech, Danish, Basque, Manx, Ilokano, Kurdish, Latvian, Minangkabau, Maltese, Scots, Albanian, and Walloon. All code is available on GitHub.


\textbf{WikipediaRetrievalMultilingual} and \textbf{WikipediaRerankingMultilingual}: This is a multilingual retrieval and reranking dataset based on succinct queries generated by a strong multilingual LLM grounded in Wikipedia articles. The dataset was made to resemble SQuAD. Sampled Wikipedia articles of a target language were chunked and passed to GPT4-o using the following prompt:


\begin{verbatim}
"""
Your task is to anticipate possible search queries by users in the form of a question 
for a given document.
- The question must be written in {{ language }}
- The question should be formulated concretely and precisely and relate to the 
information from the given document
- The question must be coherent and should make sense without knowing the document
- The question must be answerable by the document
- The question should focus on one aspect and avoid using subclauses connected with 
'and'
- The question should not be overly specific and should mimic a request of a user who
is just starting to research the given topic
- Do not draw on your prior knowledge

Generate a question in {{ language }} for the following document:
<document>
{{ document }}
</document>

Search query:
"""
\end{verbatim}

We filtered articles with less than 9 paragraphs and sampled 1500 articles from the top 100k viewed articles. We then selected a random window of 9 consecutive paragraphs per article and chose the middle one to be the positive context and generated a query for it with gpt-4o. The surrounding 8 paragraphs act as hard negatives. The 9 paragraphs per article are used for the reranking task with one positive and 8 negatives. The one positive, 8 hard negatives, and the remaining corpus as negatives are used in the retrieval task.

These datasets where constructed fro the following languages: "bul-Cyrl", "ben-Beng", "ces-Latn", "dan-Latn", "deu-Latn", "eng-Latn", "fas-Arab", "fin-Latn", "hin-Deva", "ita-Latn", "nld-Latn", "por-Latn", "ron-Latn", "srp-Cyrl", "dan-Latn", "nob-Latn", "swe-Latn".

To estimate the quality of these samples we compare it to the GermanQuAD \citep{möller2021germanquad} in \autoref{fig:app-germanquad}. We obtain a Spearman rank correlation of 0.93 with a 95\% CI of [0.69; 1.].

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/wikiretrieval_test.pdf}
    \caption{Comparison of MRR on synthetic retrieval and gold (GermanQuAD). The synthetic dataset was generated using GPT4-turbo.}
    \label{fig:app-germanquad}
\end{figure}



\subsection{Task Metadata}\label{appendix:task_metadata}
Table \ref{tab:task_metadata} shows the required metadata to fill before adding a task to the benchmark. We provide a detailed description of each field, along with examples and possible values.

\begin{table*}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lp{\linewidth}}
        \toprule
        \textbf{Field} & \textbf{Description} \\
        \midrule
        Name & A concise name for the task. \\
        Description & A brief explanation of the task's goals and objectives.. \\
        Type & The primary task category (e.g., classification, summarization, retrieval). \\
        Category & The general data structure or format of the task. This can be specified using a combination of single-letter codes (e.g., "s" for sentence, "p" for paragraph, "d" for document). For example, "s2s" indicates a sentence-to-sentence task, "s2p" indicates a sentence-to-paragraph task, and "p2p" indicates a paragraph-to-paragraph task. \\
        Task Subtype & A more specific subcategory within the primary task type. This can be used to further refine the task and provide additional context. For example, "Summarization" might have subtypes like "Extractive Summarization" or "Abstractive Summarization". \\
        Reference & A URL or citation to the original source material (e.g., paper, dataset repository). \\
        Evaluation Splits & The specific subsets of the data used for training, validation, and testing. \\
        Evaluation Languages & A list of ISO 639-3 language codes (e.g., "eng", "fra") followed by ISO 15924 script codes (e.g., "Latn", "Cyrl") for each language used in the evaluation. For example: [("eng", "Latn"), ("fra", "Latn")]. If multiple scripts are used within a single language, we specify them as a list (e.g., [("eng", ["Latn", "Grek"])]). \\
        Date & The time period when the data was gathered. Specified as a tuple of two dates.\\
        Main score & The primary metric used to evaluate task performance. \\
        Form & The format of the data (e.g., "spoken", "written") \\
        License & The licensing terms for the dataset (e.g., CC BY-SA, MIT). \\
        Domains & The subject areas or fields covered by the data (e.g., medical, legal, news). One dataset can belong to multiple domains. \\
        Annotation Creators & The type of the annotators. Includes "expert-annotated" (annotated by experts), "human-annotated" (annotated e.g. by mturkers), "derived" (derived from structure in the data), "LM-generated" (generated using a language model) and "LM-generated and reviewed" (generated using a language model and reviewed by humans or experts).\\
        Dialect & The specific dialect or regional variation of the language. \\
        Text Creation & How the text was generated. Includes "found", "created", "human-translated and localized", "human-translated", "machine-translated", "machine-translated and verified", "machine-translated and localized", "LM-generated and verified". \\
        Bibtex Citation & The BibTeX format citation for the dataset. \\
        Number of samples & The total number of data points in the dataset. \\
        Avg. Number of characters & The average character length of the samples in the dataset.\\ \bottomrule
    \end{tabular}
    }  % end resizebox
    \caption{Required metadata for adding a new task to MMTEB.}
    \label{tab:task_metadata}
\end{table*}


\subsubsection{Domains}\label{appendix:Domains}
For our domains, we include the following: 

\begin{itemize}
    \item \textbf{Academic}: Scholarly writing and research publications typically found in journals, theses, and dissertations.
    \item \textbf{Blog}: Informal or conversational posts often found on websites or personal pages, covering a wide range of topics.
    \item \textbf{Constructed}: Text or speech that is deliberately invented or constructed, often used for experimental purposes to target specific abilities.
    \item \textbf{Encyclopaedic}: Structured, reference-based texts that provide comprehensive and factual information on a wide range of subjects.
    \item \textbf{Fiction}: Narrative writing based on imaginative content, including novels, short stories, and other forms of storytelling.
    \item \textbf{Government}: Official documents, reports, and publications produced by governmental bodies.
    \item \textbf{Legal}: Documents and texts relating to laws, legal proceedings, contracts, and legal theory.
    \item \textbf{Medical}: Scientific and clinical literature related to healthcare, treatments, medical research, and patient care.
    \item \textbf{News}: Journalistic content that covers current events, politics, economy, and other topical issues.
    \item \textbf{Non-fiction}: Writing based on factual accounts and real-world subjects, such as biographies, essays, and documentaries.
    \item \textbf{Poetry}: Literary form focused on expressive language, often structured with meter, rhyme, or free verse.
    \item \textbf{Religious}: Texts related to religious teachings, doctrines, sacred scriptures, and spiritual discussions.
    \item \textbf{Reviews}: Critical evaluations of works such as books, movies, music, products, or services.
    \item \textbf{Social}: Written or spoken communication on social media platforms, forums, and other digital environments.
    \item \textbf{Spoken}: Oral communication, including speeches, dialogues, interviews, and recorded conversations.
    \item \textbf{Subtitles}: Textual transcriptions or translations of spoken language in films, videos, or multimedia presentations.
    \item \textbf{Web}: Text content found on websites, covering a wide range of subjects, often hyperlinked and multimedia-enriched.
    \item \textbf{Written}: General term for any form of text-based communication, whether printed or digital.
    \item \textbf{Programming}: Text written in programming languages to instruct computers, often for software development.
\end{itemize}

Our definition of domain aligns with that of the Universal Dependencies project~\citep{nivreUniversalDependenciesV12016}. We do not claim that our definition is neither precise nor comprehensive. However, 
and include subject fields such as "medical", "legal", and "news" and literary type such as "fiction", "non-fiction". They are not mutually exclusive.