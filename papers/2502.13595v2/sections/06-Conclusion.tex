\section{Conclusion}
This work introduced the Massive Multilingual Text Embedding Benchmark (MMTEB), a large-scale open collaboration resulting in a benchmark with more than 500 tasks covering more than 1000 languages. From these, we constructed three multilingual benchmarks: one fully multilingual (\texttt{MTEB(Multilingual)}) and two targeting Indic (\texttt{MTEB(Indic)}) and European languages (\texttt{MTEB(Europe)}) respectively. Acknowledging that multiple additional benchmarks can be constructed from the MMTEB additions, we propose a simple approach to constructing new benchmarks. To make these benchmarks accessible to low-resource communities, we introduced several optimizations by downsampling retrieval tasks using hard negative mining and bootstrapping clustering evaluation to re-use encoded documents across sets. This leads to a notable reduction in the number of text samples that need to be embedded.

Our findings indicate that while large (7B) LLM-based embedding models obtain state-of-the-art performance on the English benchmark, they are still outperformed in highly multilingual or low-resource settings by smaller models based on XLM-R Large, even when accounting for notable improvements like prompt-based embeddings.