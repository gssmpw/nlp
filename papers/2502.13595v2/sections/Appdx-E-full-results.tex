\section{Full results}
\label{sec:fullresults}

During this work, multiple models were evaluated on more than >500 tasks, with multiple tasks containing multiple language subsets covering more than 1000 languages. This makes a comprehensive overview unreasonable.  While we have supplied scores aggregated across task categories, we realize that readers might be interested in examining scores for their specific language, domain of interest, and task. To ensure that such aggregation is available and easily accessible, we make all results available on the public and versioned results repository \footnote{\url{https://github.com/embeddings-benchmark/results} for the specific version of the repository used for this work see commit id \texttt{9a79f7e07542ad2f5cb47490fa1e5ac2ba57d7a8}}. These results include time of run, evaluation time, and a wide set of performance metrics pr. language subset, CO2 emission, version number, and more. 

To make these detailed results subject to easy analysis, we have added functionality for loading and aggregating these results within the \texttt{mteb} package. It is, for instance, possible to retrieve the scores for specific models on all English (eng) and French (fra) retrieval tasks within the Legal domain using the code snippet in \autoref{fig:code-results}


\begin{figure}
\begin{lstlisting}[style=pythonstyle]
import mteb
from mteb.task_selection import results_to_dataframe

tasks = mteb.get_tasks(
    task_types=["Retrieval"], 
    languages=["eng", "fra"], 
    domains=["legal"]
)

model_names = [
    "intfloat/multilingual-e5-small",
    "intfloat/multilingual-e5-base",
    "intfloat/multilingual-e5-large",
]

models = [mteb.get_model_meta(name) for name in model_names]

results = mteb.load_results(models=models, tasks=tasks)

df = results_to_dataframe(results)

\end{lstlisting}
\caption{Simple example of how to obtain all scores on English (eng) and French (fra) retrieval tasks within the Legal domain for a set of models.}
\label{fig:code-results}
\end{figure}



We refer to the documentation\footnote{\url{https://github.com/embeddings-benchmark/mteb}} for the latest version of this code.

\subsection{Performance per Number of Speakers}
\label{sec:appendix-perf-by-speakers}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/rank_by_lang_size.pdf}
    \caption{Models' rank on the \texttt{MTEB(Multilingual)} by the total number of speakers of a language.\\
    \textit{Trendlines represent moving average with a window size of 10}
    }
    \label{fig:rank_by_lang_size}
\end{figure}

