
\section{Benchmark Optimizations}
\subsection{Speeding Up Tasks}
\label{sec:speedup}
We aim to reduce the total amount of time needed to run the complete set of MTEB task. In particular, we investigate how to drastically reduce runtime on clustering and retrieval tasks while maintaining relative model rankings. This appendix provides full details of the approach described in Section \ref{sec:smaller-dataset-submissions}.

\subsubsection{Clustering}

\begin{figure*}
    \centering
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_BiorxivClusteringP2P.pdf}}\hfill
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_BiorxivClusteringS2S.pdf}}\hfill
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_MedrxivClusteringP2P.pdf}}\par
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_MedrxivClusteringS2S.pdf}}\hfill
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_RedditClustering.pdf}}\hfill
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_RedditClusteringP2P.pdf}}\par
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_StackExchangeClustering.pdf}}\hfill
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_StackExchangeClusteringP2P.pdf}}\hfill
    \subfloat[][]{\includegraphics[width=.3\textwidth]{figures/task_plot_TwentyNewsgroupsClustering.pdf}}
    \caption{Distribution of scores per task across models.}
    \label{fig:clusterfast-v-measures}
\end{figure*}


\begin{table}[!th]
\centering
\resizebox{.49\textwidth}{!}{%
    \begin{tabular}{lcr}
    \toprule
        \textbf{Task} & \textbf{Spearman} &  \textbf{Speedup} \\ \midrule
        Biorxiv P2P & 0.9505 &  31.50x \\ 
        Biorxiv S2S & 0.9890 &  14.31x \\ 
        Medrxiv P2P & 0.9615 &  21.48x \\ 
        Medrxiv S2S & 0.9560 &  8.39x \\ 
        Reddit S2S & 0.9670 &  11.72x \\ 
        Reddit P2P & 0.9670 &  22.77x \\ 
        StackExchange S2S & 0.9121 &  9.55x \\ 
        StackExchange P2P & 0.9670 &  20.20x \\ 
        TwentyNewsgroups & 1.0000 &  5.02x \\ \midrule
        Average & 0.9634 &  16.11x \\ \bottomrule
    \end{tabular}
}
\caption{Agreement on model rankings on a selection of English clustering tasks using Spearman's correlation across the scores of 13 models of various sizes.}
\label{table:cluster-rank-spearman}
\end{table}

In the main paper, we present a down-sampled and bootstrapped version of the clustering task. We highlight the main results in \autoref{table:cluster-rank-spearman} but refer to. We observe an average speedup across tasks of 16.11x while maintaining the relative ordering of models on the evaluated tasks. The largest average speed-up was seen for e5-large (16.93x), but we expect this effect to be even more pronounced among 7b or larger models.

9 single-level English clustering tasks are evaluated on 13 models across various sizes. A fraction of the documents are sampled and stratified by their target categories. At the same time, we wish to maintain robustness of the evaluation, i.e. the fast approach should be able to determine highly similar model ranking to that from the original approach. As such, we investigate the extent of agreement between the original clustering task and ours in each task on the model rankings. 

The model ranking is determined from the mean of V-measure scores from evaluations, where a higher mean gives a higher model rank. Spearman's rank correlation score is then calculated based on the ranks from ours and the original approach. 
We additionally calculate the significant model rank which is determined by computing the significance of the given model's V-measure bootstrapped distribution based on its mean of V-measure scores using our approach against that of the original approach. Significant \textit{S} is then calculated based on the significant ranks from our and the original approach. 

\begin{table}[!ht]
    \centering
    \begin{tabular}{lccr}
    \toprule
        \textbf{Task} & Sig. \textit{S} \\ \midrule
        Biorxiv P2P & 0.9390 \\ 
        Biorxiv S2S & 0.9679 \\ 
        Medrxiv P2P & 0.8200 \\
        Medrxiv S2S & 0.9510 \\ 
        Reddit S2S & 0.9790 \\ 
        Reddit P2P & 0.7370 \\ 
        StackExchange S2S & 0.9486 \\ 
        StackExchange P2P & 0.9497 \\ 
        TwentyNewsgroups & 0.9832 \\ \midrule
        Average & 0.9195 \\ \bottomrule
    \end{tabular}
    \caption{Agreement on model rankings on English clustering tasks using significant Spearman's rank correlation with selected models of various sizes.}
    \label{table:appdx-cluster-rank-spearman}
\end{table}

To find a balance between speedup and the robustness of the approach, 4\% of the dataset is chosen as the fraction to down-sample to, with the exception of RedditS2S and StackExchange where $n\_samples=32768$. Table \ref{table:appdx-cluster-rank-spearman} shows that all evaluated datasets have very high significant Spearman's rank scores between our and the original approach. Figure \ref{fig:clusterfast-v-measures} reports the distribution of V-measure scores obtained from evaluation per model in each dataset for the ClusteringFast and the original approach. There is generally strong agreement between the rankings from both approaches. We also observe that the ClusteringFast approach often (5 out of 9 datasets) produces a smaller spread (i.e. smaller variance) in its V-measure distributions. Reddit P2P has the lowest significant Spearman score among this set. It also has the lowest average character length for its documents.


\subsubsection{Retrieval}
\label{app:retrieval_downsample}
In this section we provide details about the method used to downsample retrieval datasets.

\begin{figure*}
    \centering
    \includegraphics[width=.49\textwidth]{figures/rank_plot_nq.pdf}
    {\includegraphics[width=.49\textwidth]{figures/rank_plot_trec_covid.pdf}}
    \caption{Ranking of different models on subsampled versions of the datasets using hard negatives. We see that NQ can be reduced to just two documents per query (relevant + 1 hard negative) while still maintaining the rank while TREC-COVID is less stable.}
    \label{fig:ranking_scores_retrieval}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=.49\textwidth]{figures/score_plot_nq.pdf}
    {\includegraphics[width=.49\textwidth]{figures/score_plot_trec_covid.pdf}}
    \caption{Absolute scores of different models on subsampled versions of the datasets using hard negatives. NQ has 1 relevant document per query while TREC-COVID has 500+ relevant documents per query which is why we see NQ scores gradually increasing whereas TREC-COVID scores vary.}
    \label{fig:absolute_scores_retrieval}
\end{figure*}

To ensure the downsampling kept the efficacy of the evaluation we aimed to examine several axes: (1) a wide range of models to be sure that the evaluation task could still properly rank the models - just as if it were not downsampled (2) that this method works for retrieval datasets that are sparsely judged \textit{and} densely judged and (3) seeing if it was possible to use hard negatives from a smaller set of models due to the computational expense to gather these hard negatives on the full datasets.\footnote{We also tested whether ensuring that the ground truth relevant document is present in these hard negatives made a difference - we found that it did not, as most models ranked the ground truth in the top N, so manually including it was little help as it was already included.}

To meet these goals we chose NQ (for sparse relevance annotations, one per query) and TREC-COVID (for dense judgements, > 500 per query). To test using a small set of hard negatives, we gather the hard negatives with e5-large-v2 only.  We evaluate a wide range of models for this analysis, including the current state-of-the-art and some of the previous state-of-the-art: NV-Embed-v1 \citep{lee2024nvembed}, SFR-Embedding-Mistral \citep{SFRAIResearch2024}, e5-mistral-7b-instruct \citep{wang2023improving}, e5-large-v2 \citep{wang2022text}, gte-base-en-v1.5 \citep{li2023gte}, bge-large-en-v1.5 \citep{bge_embedding}, and contriever-msmarco \citep{izacard2021unsupervised}. We then evaluated the models on versions of the datasets with N hard negatives documents per query where $N \in $\{2, 5, 10, 50, 100, 500, all\}. We then compared the absolute scores and the relative rank positions to see what settings best retain the difficulty of the original task.

\paragraph{Ability to rank models correctly} For a good evaluation, it must be able to rank models correctly and determine the best model. For this we examine how the ranking of the models change when we lower the number of hard negatives. For NQ the rank remains stable even with just one hard negatives (Figure~\ref{fig:ranking_scores_retrieval}). For TREC-COVID the ranking becomes unstable starting at 100 hard negatives, continuing to change as the number gets smaller. 

\paragraph{Keeping the absolute score similar} In an ideal case the scores for the task should remain similar and not trend towards perfect scores, remaining useful. We see that scores go very high when there are only a few hard negatives for NQ (Figure~\ref{fig:absolute_scores_retrieval}). For TREC-COVID it is more stable, but we see some wider swings with smaller documents. Overall, the scores are relatively similar at 100+ hard negatives.

\paragraph{Summary} Overall, we see that staying above 100 hard negatives gives similar absolute scores while maintaining the ranking ability. Thus we opted for a conservative 250 documents per query to keep these characteristics.

%Lorem ipsum etc etc.


\subsection{Code Optimizations}
\label{sec:appendix-code-optimizations}

We here document the major code optimizations within MTEB not related to dataset scores, task reformulation 

\header{Dataset loading}
One important issue identified was about loading multilingual and cross-lingual datasets composed of numerous small files in their repositories. Even for total dataset sizes under 10MB, loading could take hours due to significant overhead from managing a high number of network requests and the improper opening and closing of gzipped files. In collaboration with the datasets team \citep{datasets_paper}, we addressed these problems with two-side implementation improvements: the datasets library optimized the loading of a large number of requested files, and we restructured the datasets and our codebase to leverage the benefits of the newer implementation. This ultimately reduced loading times by almost a factor of 100, bringing the largely cross-lingual dataset bitext-mining loading to under a minute.

\header{Deduplication}
Upon in-depth scrutiny of all datasets, cases with repeated samples were identified and deduplicated (e.g. MindSmallReranking). As this led to a change in scores, a second version of the task was introduced to maintain compatible scores with existing benchmarks. To move the optimizations to existing MTEB tasks we implement a local cache to avoid encoding a sample twice.