\section{New Metrics}
\label{sec:new_metrics}

\subsection{Abstention for retrieval and reranking tasks}

In addition to the existing ranking metrics used for Retrieval and Reranking tasks \citep{muennighoff2023mteb}, we propose to assess score calibration through the evaluation of model abstention ability, using the implementation of \citet{gisserot2024towards}.

Intuitively, a model abstains on a given instance $\left( q, d_1, \cdots, d_k \right)$ (one query and $k$ candidate documents) if $c \left( q, d_1, \cdots, d_k \right) < \tau$, where $c$ is a confidence function\footnote{In our implementation, we rely on three simple confidence functions all taking the instance's query-document cosine similarity scores as input: the maximum score, the standard deviation of scores and the difference between the highest and second highest scores.} and $\tau$ is a threshold regulating abstention likelihood. Therefore, to evaluate abstention capacity on a given test set $\mathcal{S}$, an approach consists of making $\tau$ vary to achieve several abstention rates. In the case of effective abstention, the metric score increases with the abstention rate.

More formally, models' ability to abstain is evaluated by computing the normalized area under the metric-abstention curve ($n\textsf{AUC}$). Given a confidence function $c$, a metric function $m$\footnote{We utilize the metrics initially implemented for the evaluation of Retrieval and Reranking MTEB tasks \citep{muennighoff2023mteb}.} and a labeled test dataset $\mathcal{S}$, $n\textsf{AUC}$ is computed as follows:

\begin{enumerate}
    \item \textbf{Multi-thresholding:} Given a model \( f \) and dataset \( \mathcal{D} \), we define a set of abstention thresholds \( \tau_1, \dots, \tau_n \), such that \( \tau_1 < \dots < \tau_n \). For each threshold \( \tau_i \), we construct a corresponding sub-dataset \( \mathcal{S}_i \subseteq \mathcal{D} \) by applying the abstention criterion. We then evaluate the model \( f \) on each sub-dataset \( \mathcal{S}_i \) using the metric function \( m \). To quantify the model’s performance across these thresholds, we compute the area under the metric-abstention curve, denoted as \( \textsf{AUC}_{model} \).
    \item \textbf{Compute lower-bound:} Since \( \textsf{AUC}_{model} \) depends on the model’s raw performance without abstention, we compute the effective lower bound \( \textsf{AUC}^- \). This corresponds to the area under the curve when the metric remains constant as abstention increases, representing the baseline where abstention does not improve the metric.
    \item \textbf{Compute upper-bound:} To establish the upper bound, \( \textsf{AUC}^+ \), we evaluate an oracle model that has access to the true labels. The oracle can selectively retain the best instances at each abstention rate, yielding the theoretical maximum area under the metric-abstention curve. This represents the optimal model performance under abstention.
    \item \textbf{Compute normalized \textsf{AUC}:} Finally, we compute the normalized area under the curve, denoted \( n\textsf{AUC}_{model} \), by scaling \( \textsf{AUC}_{model} \) between the lower and upper bounds:
    $$n\textsf{AUC}_{model} = \frac{\textsf{AUC}_{model} - \textsf{AUC}^- }{ \textsf{AUC}^+ - \textsf{AUC}^- }$$.
\end{enumerate}
