%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{afterpage}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% \usepackage{algpseudocode}

%Our packages
\usepackage{float}
\usepackage{booktabs}        % For professional table lines
\usepackage{array}           % For better column control
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{tabularx}        % For flexible column widths
\usepackage{threeparttable}  % For table footnotes
\usepackage{caption}         % For caption control
\usepackage{siunitx}         % For number alignment
\usepackage{subcaption}
\usepackage{makecell}
\usepackage{placeins}
\usepackage{listings}
\usepackage{enumitem} % For localized list formatting
\usepackage{multirow} 
\usepackage{subcaption}
\DeclareUnicodeCharacter{2032}{'}

\lstset{
  basicstyle=\ttfamily\footnotesize, % Small font for code
  breaklines=true,                   % Enable line wrapping
  frame=single,                      % Add a border around the code block
  columns=fullflexible,              % Adjust column width for monospaced text
  keepspaces=true                    % Preserve spaces for alignment
}


% Define a new column type for fixed width in Model column with left alignment and no extra space


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{FragmentNet}

\begin{document}

\twocolumn[
% \icmltitle{Molecular Fragments Are All You Need}
% \icmltitle{FragmentNet: Adaptive Fragmentation Meets Hybrid Graph-Sequence Modeling for Molecules}
\icmltitle{FragmentNet: Adaptive Graph Fragmentation for Graph-to-Sequence Molecular Representation Learning}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ankur Samanta}{equal,a}
\icmlauthor{Rohan Gupta}{equal,a}
\icmlauthor{Aditi Misra}{equal,a}
\icmlauthor{Christian McIntosh Clarke}{equal,a}
\icmlauthor{Jayakumar Rajadas}{b}
\end{icmlauthorlist}

\icmlaffiliation{a}{Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada}
\icmlaffiliation{b}{Advanced Drug Delivery and Regenerative Biomaterials Laboratory, Stanford Cardiovascular Institute, Palo Alto, USA}

\icmlcorrespondingauthor{Ankur Samanta}{ankur.samanta@alumni.utoronto.ca}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Molecular representation learning, graph-to-sequence foundation model, learned graph tokenizer, fragment-based molecule optimization}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Molecular property prediction uses molecular structure to infer chemical properties. Chemically interpretable representations that capture meaningful intramolecular interactions enhance the usability and effectiveness of these predictions. However, existing methods often rely on atom-based or rule-based fragment tokenization, which can be chemically suboptimal and lack scalability. We introduce FragmentNet, a graph-to-sequence foundation model with an adaptive, learned tokenizer that decomposes molecular graphs into chemically valid fragments while preserving structural connectivity. FragmentNet integrates VQVAE-GCN for hierarchical fragment embeddings, spatial positional encodings for graph serialization, global molecular descriptors, and a transformer. Pre-trained with Masked Fragment Modeling and fine-tuned on MoleculeNet tasks, FragmentNet outperforms models with similarly scaled architectures and datasets while rivaling larger state-of-the-art models requiring significantly more resources. This novel framework enables adaptive decomposition, serialization, and reconstruction of molecular graphs, facilitating fragment-based editing and visualization of property trends in learned embeddings—a powerful tool for molecular design and optimization.
\end{abstract}

\section{Introduction}
\label{Introduction}

Pre-trained models have transformed natural language processing (NLP) by capturing contextual information from large unlabeled corpora \cite{devlin2018bert}. In parallel, self-supervised learning techniques have been applied to molecular representation learning on graphs of atoms and bonds \cite{gilmer2017neural}. However, early adaptations of Masked Language Modeling at the atom level \cite{hu2019strategies} often fail to capture a broader chemical context, leading to negative transfer where pre-trained models underperform simpler baselines \cite{xia2023molebert}.

Fragment-based modeling has emerged as a more chemically coherent alternative by focusing on substructures rather than individual atoms. However, rule-based methods, such as BRICS \cite{vangala2023pbrics}, can be inflexible and struggle to generalize across chemical space. On the other hand, sequence-based tokenization methods, such as Byte-Pair Encoding on SMILES, often lose crucial topological information.

We introduce FragmentNet, which presents two key innovations: (1) the first adaptive, learned tokenizer for molecular graphs used for Masked Fragment Modeling (MFM) and (2) a graph-to-sequence foundation model architecture. Our novel graph tokenizer infers recurring substructures directly from molecular graphs, preserving both connectivity and scalability. These molecular graphs are tokenized into chemically valid fragments and compressed using a joint VQVAE+GCN hierarchical fragment encoder, which integrates atom-level features with fragment-level structures. To capture spatial relationships, molecular graph spatial positional encodings serialize the topological structure into chemically valid positional embeddings, added to the sequence of fragment embeddings. Molecular descriptors, included as a CLS token, reflect global molecular characteristics. Together, these elements enable the Transformer to effectively refine fragment embeddings through MFM, producing chemically meaningful representations.

We evaluate FragmentNet through pre-training with MFM and fine-tuning on the MoleculeNet \cite{wu2018moleculenet} and Malaria \cite{Gamo2010-iq} benchmarks. Despite using fewer parameters and a smaller pre-training set than many competing methods, FragmentNet consistently outperforms similarly sized baselines. Its fragment-level representations enhance chemical interpretability, as attention maps highlight the substructures driving predictions, and embedding visualizations demonstrate the model’s ability to cluster molecules based on property-specific similarity.

Fragment-based drug design involves generating molecules by swapping functional groups to achieve desired properties, mirroring strategies used by medicinal chemists \cite{Narayanan2022}. We present a chemical editing module enabled by the learned tokenizer. By decomposing molecules into fragment sequences of adjustable granularity, we can automate substituting substructures to generate valid analogues and systematically explore how targeted modifications affect properties. This fragment-swapping capability paves the way for future iterative design strategies.

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{icml2025/images/kaio_system_diagram.jpg}}
    \caption{(Left) A molecule is tokenized into discrete substructures, with one fragment masked. Atom-level attributes are encoded using a VQVAE, and fragment-level attributes are learned using a GCN. The pooled sequence, enriched with spatial positional encodings and a CLS token, is passed through a Transformer. (Right) In MFM, the masked token embedding predicts the masked fragment, while for downstream tasks, the pooled representation encodes the full molecule, which can be reconstructed.}
    \label{fig: system diagram}
    \end{center}
    \vskip -0.2in
\end{figure*}

Our contributions are summarized as follows:
\begin{itemize}
  \item FragmentNet: A novel graph-sequence transformer pre-trained using masked fragment modeling.
  \item The first learned adaptive tokenizer for molecular graphs used for MFM, producing chemically valid fragments while preserving structural connectivity information.
  \item Novel spatial positional encodings that capture molecular topology in a sequence-compatible format.
  \item A fragment-based chemical editing module enabling scalable, targeted molecular modifications with applications in analogue generation and optimization.
  \item FragmentNet outperforms similar-scale foundation models while competing with larger models trained with significantly more data and computing.
  \item Empirical and theoretical demonstration of how MFM with adaptive graph tokenization is more data-efficient, scalable, and chemically informed than legacy pre-training methods, with room for future research on optimizing fragment granularity.
\end{itemize}

\section{Preliminaries}
\label{Preliminaries}

\subsection{Molecule Models Pre-trained with Masked Atom Modeling on Graphs}
Traditional masked modeling strategies often focus on atom-level masking, limiting a model's ability to capture chemical substructure context. For example, AttrMask \cite{xia2023molebert} randomly hides node and edge attributes in molecular graphs, while MoleBERT \cite{xia2023molebert} employs masked atom modeling (MAM) with a Vector Quantized Variational Autoencoder (VQ-VAE) to enhance atom-level representations. However, atom-level masking often creates broken or chemically inconsistent environments in the visible context, restricting the model's understanding of bonding and functional group interactions. SimSGT \cite{liu2024rethinking} takes a step beyond simple atom masking by training a GNN to predict subgraphs representing the local neighborhoods around masked atoms. Although it offers flexibility in identifying subgraphs, it does not explicitly model interactions between them, limiting its ability to capture long-range dependencies and interactions among functional groups.

\subsection{Masked Fragment Modeling (MFM)}
Masking entire fragments addresses the aforementioned limitation by preserving chemically meaningful contexts, enabling more stable learning of bonding rules and functional relationships \cite{jin2018junction, schwaller2019molecular}. Rooted in organic chemistry principles, this approach ensures that contextual dependencies within fragments remain intact. It is analogous to reconstructing a multi-word phrase in natural language models, where additional context enhances semantic understanding \cite{sennrich2016neural}. Two primary methods have been explored for its implementation: adaptive tokenization of SMILES strings and rule-based subgraph masking on molecular graphs.

\subsubsection{BERT-Style Pre-Training}
\label{BERT-style pre-training}
Transformer-based architectures \cite{devlin2018bert} have demonstrated remarkable capabilities in efficiently learning contextual relationships within sequential data. ChemBERTa \cite{chithrananda2020chemberta} extends this paradigm to molecular SMILES representations, using an adaptive tokenizer to split SMILES strings into subsets of characters, leveraging masked language modeling to enhance chemical feature extraction. MolFormer \cite{ross2022large} scales up this technique while incorporating rotary positional embeddings, efficiently pre-training on SMILES sequences from 1.1 billion molecules. MolTRES \cite{park2024moltres} introduces a hierarchical masking strategy for SMILES sequences targeting multiple granularities of chemical substructures, from individual atoms to entire functional groups. However, these SMILES-based transformers often neglect the topological relationships inherent to molecular graphs, sacrificing structurally informed representation learning for modeling efficiency \cite{nguyen2024smiles}.

\subsubsection{Rule-based Masked Fragment Modeling (MFM)}
Existing graph-based methods often rely on rule-based tokenization methods, such as BRICS \cite{vangala2023pbrics}. IBM's r-BRICS extends this by introducing more flexible fragmentation patterns to handle a wider variety of molecules, primarily targeting long aliphatic chains and complex ring structures. UniCorn \cite{feng2024unicorn} employs BRICS-based fragmentation for 2D graph masking. 	However, these methods are limited by their rigidity and might struggle to generalize across diverse chemical spaces \cite{nature2024fragment}. Data-driven approaches overcome this by learning adaptable fragment representations.

\subsection{Graph-Sequence Foundation Models}
While the aforementioned architectures use graph-only or sequence-only representations, combining the two can provide significant benefits. Graph-BERT \cite{zhang2020graphbert} employs a limited subgraph sampling approach around a single anchor node to transform graphs into sequences. While effective for large knowledge graphs, this method discards too much chemical information, making it less suitable for molecular fragment modeling.

\section{FragmentNet Tokenizing Method}

\subsection{Graph Tokenization}

In natural language processing (NLP), the quality of tokenization is paramount \cite{vaswani2017attention}, motivating the development of data-driven subword tokenizers \cite{sennrich2016neural,schuster2012japanese}. We introduce a \emph{learned tokenization} technique generalizable to all graph structures. This approach yields significant advantages over rule-based methods (e.g., \cite{lewellyn1998recap,degen2008art}) by capturing empirical substructure distributions more effectively. In \cref{app: theoretical rationale for adaptive tokenization}, we provide a detailed theoretical foundation for this claim. The proven benefits of learned tokenization in NLP naturally extend to the molecular domain, offering a robust framework for improved molecular representation learning.

\section*{Algorithm: Iterative Pairwise Merging of Molecular Fragments}

\begin{algorithm}[tb]
    \caption{Iterative Pairwise Merging of Molecular Fragments}
    \label{alg:pairwise_merging}
\begin{algorithmic}

\STATE \textbf{Input:} Corpus of molecules $\mathcal{C}$, where:
\STATE \hspace{1em} Each molecule $M \in \mathcal{C}$ is represented as a graph with:
\STATE \hspace{2em} Nodes $a_1, a_2, \dots$ indexed by  $\text{atomic\_number}(a)$
\STATE \hspace{2em} Bonds $b_{ij}$ labeled by bond types $\text{type}(b_{ij})$, connecting nodes $a_i$ and $a_j$
\STATE \textbf{Output:} Merged fragments and transformation history

\STATE $\text{merges} \gets \emptyset$
\STATE $\text{next\_node\_ID} \gets \max(\text{atomic\_number}(a) \text{ for all } a \text{ in } \mathcal{C}) + 1$

\FOR{$i = 1$ to $\text{num\_iter}$}
  \STATE $\text{pair\_count} \gets \emptyset$, $\text{node\_count} \gets \emptyset$

  \FOR{each molecule $M \in \mathcal{C}$}
    \FOR{each connected pair of nodes $(a_i, a_j)$ in $M$ with bond $b_{ij}$}
      \STATE $\text{pair\_count}[(a_i, a_j, b_{ij})] \mathrel{+}= 1$
      \STATE $\text{node\_count}[a_i] \mathrel{+}= 1$, $\text{node\_count}[a_j] \mathrel{+}= 1$
    \ENDFOR
  \ENDFOR

  \FOR{each pair $(a_i, a_j, b_{ij})$ in $\text{pair\_count}$}
    \STATE $\text{scores}[(a_i, a_j, b_{ij})] \gets \frac{\text{pair\_count}[(a_i, a_j, b_{ij})]}{\sqrt{\text{node\_count}[a_i] \cdot \text{node\_count}[a_j]}}$
  \ENDFOR

  \STATE $\text{best\_pair} \gets \arg\max_{(a_i, a_j, b_{ij})} \text{scores}[(a_i, a_j, b_{ij})]$

  \FOR{each molecule $M \in \mathcal{C}$}
    \FOR{each pair $(a_i, a_j)$ in $M$ such that $(a_i, a_j, b_{ij}) = \text{best\_pair}$}
      \STATE Replace $a_i$, $a_j$, and $b_{ij}$ with a new node $a_{\text{new}}$ with $a_\text{new}.ID = \text{next\_node\_ID}$
      \STATE $\text{next\_node\_ID} \mathrel{+}= 1$
    \ENDFOR
  \ENDFOR

  \STATE $\text{merges} \gets \text{merges} \cup \{\text{best\_pair}\}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Levels of Granularity}

The tokenizer first splits a molecule into its individual atoms and iteratively merges them based on the learned merge history. The number of merges performed controls the size of the tokens. After training the tokenizer for \( T \) iterations, the set of stored merges \( \mathcal{M} \) has length \( T \). A molecule can then be tokenized using the first \( t \) merges from \( \mathcal{M} \) for any \( t \leq T \) without requiring retraining. This flexibility, unique to our tokenizer, allows for adjustable granularity per task.

In this work, we chose 100 merge iterations based on empirical evaluation; further research will explore the impact of the levels of granularity on task-specific downstream performance. \cref{fig: token size distribution} illustrates the impact of these training iterations on the distribution of fragment sizes within our representative dataset of 2 million SMILES strings.

\subsubsection{Token Dictionary}

We tokenized 17,000 molecules to create a vocabulary for our foundation model, then stored the tokens efficiently via hashing. We preserved SMILES, graph representations for reconstruction, and special tokens like [UNK], as discussed further in \cref{app: tokenizer discussion}.

\subsubsection{Treatment of dangling bonds}   
Fragments are created by breaking bonds in the original molecule; we cache the fragments in the token dict, attaching a 'dummy atom' represented by an atom with an atomic number of 0 and no chemical attributes to the other end of the broken bond. This leads to the number and types of broken bonds being an additional differentiator of otherwise identical fragments: A carbon atom with 1 broken single bond,  a carbon atom with 2 broken single bonds, and a carbon atom with 1 broken double bond are all treated as different fragments in the token dictionary.

\subsubsection{Molecule Hashing Method}

To uniquely learn embeddings for different molecular fragments, a one-to-one hashing mechanism is required to distinguish between stereoisomers and tautomers and accommodate dangling bonds. Traditional approaches like SMILES lack uniqueness, as a single fragment can correspond to multiple SMILES strings and do not distinguish isomeric molecules \cite{gilmer2017neural}.

We address this with the Weisfeiler-Lehman (WL) graph hashing algorithm \cite{weisfeiler1968reduction}, ensuring that non-isomorphic graphs receive distinct hashes. Formally, the WL algorithm iteratively refines node labels based on their neighborhood structures:
\begin{equation*}
    l_i^{(t)} = \text{Hash}\left( l_i^{(t-1)}, \{ l_j^{(t-1)} \mid v_j \in \mathcal{N}(v_i) \} \right)
    \label{eq:wl_hash}
\end{equation*}
Here, \( l_i^{(t)} \) is the label of node \( v_i \) at iteration \( t \), and \( \mathcal{N}(v_i) \) is the set of neighbors of \( v_i \).

Atom labels incorporate the atomic number \( Z \), hybridization state \( H \), number of radical electrons \( R \), and hydrogen count \( H_d \). Bond labels include bond type \( B_t \), conjugation status \( C \), stereoisomeric properties \( S \), and ring membership \( R_m \). The resulting hashes construct the token dictionary, mapping molecular fragments to their corresponding learned embeddings.

\section{FragmentNet Model Architecture}
A high-level schematic of FragmentNet's hybrid graph-to-sequence architecture can be found in \cref{fig: system diagram}.

\subsection{Input Representation}
Given a starting molecular graph, FragmentNet receives the following input data: the tokenized fragment graphs (with atom/bond chemical features), the arrangement of the fragments relative to each other in the molecule, and the fragment charges \cref{app: fragment charges}.

\subsection{VQVAE-GCN Encoder for Hierarchical Input Embeddings}

We propose a VQVAE-GCN encoder to construct hierarchical molecular fragment embeddings by integrating Vector Quantized Variational Autoencoders (VQ-VAEs) and Graph Convolutional Networks (GCNs). VQ-VAEs encode discrete atomic-level features \cite{van2017neural}, mapping atomic features into a discrete latent space. For an input \( \mathbf{x} \), the encoder \( E \) maps it to \( \mathbf{z}_e \), which is quantized to the closest vector \( \mathbf{z}_q = \arg\min_{\mathbf{c} \in \mathcal{C}} \|\mathbf{z}_e - \mathbf{c}\|_2 \) in a learned codebook \( \mathcal{C} \). The decoder reconstructs \( \hat{\mathbf{x}} = D(\mathbf{z}_q) \), and the training loss is:
\[
\mathcal{L}_{\text{VQ-VAE}} = \|\mathbf{x} - \hat{\mathbf{x}}\|_2^2 
+ \|\text{sg}[\mathbf{z}_e] - \mathbf{c}\|_2^2 
+ \beta \|\mathbf{z}_e - \text{sg}[\mathbf{c}]\|_2^2
\]
where \( \beta \) balances the commitment loss, and \( \text{sg}[\cdot] \) denotes stop-gradient. Atomic embeddings are averaged to form fragment-level representations, \( \mathbf{z}_{\text{VQ}, f} = \frac{1}{|f|} \sum_{i \in f} \mathbf{z}_{\text{VQ}, i} \).

The GCN captures fragment-level relationships by aggregating features from neighboring nodes within the discrete fragments. Node embeddings are combined in graph convolutions using global mean pooling to generate compressed fragment-level feature representations. The final fragment embedding is obtained by combining VQ-VAE and GCN features as \( \mathbf{r}_f = \mathbf{T}(\mathbf{z}_{\text{VQ}, f}) + \mathbf{z}_{\text{GCN}, f} \), where \( \mathbf{T} \) is a learnable transformation. This integration leverages the strengths of both architectures: VQ-VAEs encode discrete atomic features, while GCNs model structural relationships between atoms. 

\subsection{Graph Spatial Positional Encodings (SPEs)}

After computing embeddings for each fragment, we embed spatial information about the fragments' arrangement in the molecule using Graph Spatial Positional Encodings (SPEs) inspired by \cite{zhang2020graphbert}. We employ three types of SPEs: Hop-based Positional Encoding, Weisfeiler-Lehman (WL) Absolute Positional Encoding, and Coulomb Matrix Positional Encoding. 

\textbf{Hop-based Positional Encoding} captures a node's relative 'connectedness' by aggregating its hop distances from all other nodes in the graph. For a molecular graph \( G = (V, E) \), the hop distance \( H_{i,j} \) from node \( v_i \) to node \( v_j \) is defined as the length of the shortest path between them, or 0 if no path exists. For each node \( v_i \), we define its hop-based encoding as \( \mathbf{h}_i = \sum_{k=1}^{N} \text{Emb}_{\text{hop}}(H_{i,k}) \), where the sum aggregates the encoding of the hop distance relative to all other nodes \( v_j \).

\textbf{Weisfeiler-Lehman (WL) Absolute Positional Encoding} labels nodes uniquely based on their position within the graph structure, ensuring that nodes in two graphs with identical structures receive the same labels. After \( T \) iterations of label refinement using the Weisfeiler-Lehman algorithm, each node \( v_i \) is assigned a unique WL role ID \( w_i \), which is embedded as \( \mathbf{w}_i = \sum_{j=1}^{N} \text{Emb}_{\text{WL}}(w_j) \). Appendix~\ref{app: wl hashing} demonstrates how isomeric molecules receive distinct WL-based hashes through this approach despite sharing the same molecular formula.

\textbf{Coulomb Matrix Positional Encoding} models molecular fragment interactions based on inverse-square law distances. Given node charges \( q_i \) and a fixed distance \( d_0 \), the mean Coulomb interaction for node \( v_i \), aggregated over all anchor nodes, is embedded as \( \mathbf{c}_i = \sum_{j=1}^{N} \text{Emb}_{\text{clb}}(C_{i,j}) \), where \( C_{i,j} \) is defined as:
\[
C_{i,j} = \frac{1}{N} \sum_{k=1}^{N} \left(0.5\, Z_j^{2.4} \delta_{j,k} + \frac{Z_j Z_k}{d_0^2} \left(1 - \delta_{j,k}\right)\right)
\]
Here, \( \delta_{j,k} \) represents the Kronecker delta. This formulation ensures that Coulomb interactions are considered relative to all anchor nodes by directly incorporating both the self-interaction term \(0.5\, Z_j^{2.4} \delta_{j,k}\) and the pairwise interaction term \(\frac{Z_j Z_k}{d_0^2} \left(1 - \delta_{j,k}\right)\) within the aggregate equation.

\subsubsection{Combined Positional Encoding} This encoding integrates hop-based, WL, and Coulomb positional encodings by summing \( \mathbf{h}_i \), \( \mathbf{w}_i \), and \( \mathbf{c}_i \) for each fragment-node \( \mathbf{v}_f \). The aggregated encoding \( \mathbf{p}_f \) comprehensively encapsulates spatial and topological information, contributing to the fragment embedding \( \mathbf{r}_f + \mathbf{p}_f \). Further details and visualizations are provided in \cref{app: spatial posenc visualization}.

\subsection{Molecular Descriptor CLS Token}
In transformer-based architectures, the CLS token aggregates the overall context of the input sequence for classification tasks \cite{devlin2018bert}. In FragmentNet, we replace this token with a Molecular Descriptor Vector, prepending it to the sequence of fragment tokens. The molecular descriptor vector uses RDKit's \texttt{CalcMolDescriptors} method; we compute a vector \( \mathbf{d} = [d_1, d_2, \dots, d_M] \) of standard molecular descriptors. This vector, denoted \( \mathbf{d}_{\text{CLS}} \), is refined during training by attention layers to encode holistic molecular features for downstream tasks.

\subsection{Transformer Encoder Layers}

With the graph structures serialized into a sequence of fragment embeddings, we pass them through a series of BERT transformer encoder blocks \cite{devlin2018bert}, consisting of multi-head self-attention and feed-forward networks with residual connections and layer normalization. The series of attention layers models fragment-to-fragment relationships and captures contextual dependencies and interactions critical for effective structural representation learning.

\subsection{Property Prediction Head}

The property prediction head processes the fragment network's sequence output using max pooling across the sequence length, chosen for its effectiveness in preserving salient features while maintaining padding invariance \cite{suarez2018evaluation}. The pooled representation is then linearly projected to match the target property dimension.

\subsection{Fragment Swapping Module}
\label{frag:swap}
We propose a fragment-swapping algorithm that systematically replaces target fragments with alternatives while preserving the chemical integrity of the core molecular scaffold. The granularity of fragments can be dynamically adapted to suit specific tasks, enabling optimizations such as tailoring molecular analogues for pharmacokinetics or structure-activity relationships (SAR). This serialization process bypasses the need for substructure matching or computationally expensive graph search methods typically used in traditional fragment-based workflows \cite{vangala2023pbrics}. \cref{fragment swapping algorithm} discusses the algorithm and implementation details.

\section{Training Method}

\subsection{Masked Fragment Modeling Pre-training}
\label{sec:mfm}
We pre-train using Masked Fragment Modeling (MFM). The molecular graph is serialized into a sequence of chemically valid fragments, preserving the structural information needed to generate spatial positional encodings (SPE). The SPE encodes the original graph relationships within this sequence. We denote the serialized molecular graph as \( \mathbf{F} = [f_1, f_2, \dots, f_N] \), where each \( f_i \)represents a fragment in the sequence.

In the MFM task, we randomly mask a single fragment \(f_i\) from \( \mathcal{M} \subseteq \{1, 2, \dots, N\} \) and replace it with a special [MASK] token, resulting in the masked sequence \( \mathbf{F}_{\text{masked}} \). A key benefit of our graph serialization is that the token can be directly masked in the sequence, ensuring no data leakage from the original masked fragment to subsequent layers. The model is then trained to predict the original fragment at the masked positions using the context provided by the unmasked fragments. The loss function for MFM is defined as $
\mathcal{L}_{\text{MFM}} 
= - \log P\bigl(f_i \mid \mathbf{F}_{\text{masked}}\bigr)$.

Based on our analysis of fragment sequence length distributions in \cref{app: frag token size dist}, we limit masking to a single token per sequence to preserve context. Prior research indicates diminishing returns with high masking rates in small models \cite{wettig2023mask}. This conservative approach balances task difficulty and model capacity, with plans to explore percentage-based masking in future work.

For a detailed comparison of tokenized fragment-based masking with atom-level and rule-based substructure masking, along with formal definitions and theoretical justifications, see \cref{app: theoretical rationale for adaptive tokenization}.

\subsection{Property Prediction Evaluation}

We evaluated our model's performance on molecular property prediction tasks using datasets from MoleculeNet \cite{wu2018moleculenet} and Malaria \cite{Gamo2010-iq} . These datasets cover physical chemistry, biophysics, and physiology properties, providing a comprehensive benchmark. We employed an 80-10-10 train-validation-test split. The training was conducted with 10 random seeds for each dataset, and the results were aggregated, following the approach used in previous works \cite{xia2023molebert}.

\subsection{Compute and Training Configuration}
Due to funding and compute limitations, FragmentNet was configured and trained on a MacBook Pro M2 laptop using Apple’s Metal Performance Shaders framework. For details on the training setup, methods, hyperparameters, and compute optimization, see \cref{app:model_training_configuration}.

\section{Results and Discussion} 
We benchmark against modeling approaches grouped by scale into three brackets: the top bracket contains models with parameter counts and pre-training dataset sizes comparable to FragmentNet, the middle bracket highlights our FragmentNet variants to illustrate our approach, and the bottom bracket lists significantly larger-scale models (over 100 million parameters or 1 billion SMILES). This allows a more informative comparison of different pre-training strategies. Blank entries indicate results that were not reported for specific datasets.

\begin{table*}[ht]
\vskip 0.15in
\begin{center}
\caption{Performance on Property Prediction Datasets from MoleculeNet \cite{wu2018moleculenet} and Malaria \cite{Gamo2010-iq} \tnote{a}}
\begin{tablenotes}
\footnotesize 
[1] \cite{rong2020self}; [2] \cite{hu2019strategies}; [3] \cite{hou2022graphmae}; [4] \cite{xia2023molebert}; [5] \cite{liu2024rethinking}; [6] \cite{feng2024unicorn}; [7] \cite{park2024moltres}; [8] \cite{ross2022large}.  
[Y] pre-trained, [N] Not pre-trained. FN-Atom: FragmentNet with num\_iters $=0$. FN-Fragment: FragmentNet with num\_iters $=100$. Best results in each model class are \textbf{bold}. Format: Median(SD). Model parameter and dataset details in \cref{tab:model_size_comparison}.
\end{tablenotes}
\label{tab:combined_results}
\vskip 0.15in
\begin{small}
\begin{threeparttable}
\begin{tabularx}{\textwidth}{@{}X *{5}{c} *{3}{c}@{}}
\toprule
\textbf{Model} & 
\multicolumn{5}{c}{\textbf{Classification (ROC-AUC) ↑}} & 
\multicolumn{3}{c}{\textbf{Regression (RMSE) ↓}} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-9}
 & \textbf{BBBP} & \textbf{Tox21} & \textbf{ToxCast} & \textbf{BACE} & \textbf{SIDER} & \textbf{ESOL} & \textbf{Lipo} & \textbf{MAL} \\
\midrule
\multicolumn{9}{c}{\textbf{Small Molecule Models}} \\
\midrule
AttrMask\textsuperscript{2}        & 65.2(1) & 75.1(0.9) & 63.3(0.6) & 77.8(2) & 60.5(0.9) & 1.11(0.05)  & 0.730(0.004)  & 1.12(0.01) \\
Mole-BERT\textsuperscript{4}       & 71.9(2) & 76.8(0.5) & 64.3(0.2) & 80.8(1.4) & 62.8(1) & 1.02(0.03)  & 0.676(0.02)  & \textbf{1.074(0.009)} \\
SimSGT\textsuperscript{5}          & 72.2(0.9)   & 76.8(0.9)   & 65.9(0.8)   & 84.3(0.6)   & 61.7(0.8)   & 0.917(0.03)      & 0.695(0.01)      & 1.078(0.01) \\
GraphMAE\textsuperscript{3}        & 72.0(0.6) & 75.5(0.6) & 64.1(0.3) & 83.1(0.9) & 60.3(1) & -              & -              & -  \\
UniCorn\textsuperscript{6}         & \textbf{74.2(1)}   & \textbf{79.3(0.5)}   & \textbf{69.4(1)}   & \textbf{85.8(1)}   & \textbf{64.0(2)}   & \textbf{0.817(0.03)}      & \textbf{0.591(0.02)}      & -  \\
\midrule
\multicolumn{9}{c}{\textbf{FragmentNet Variants (15-17M Params, 2M Molecules)}} \\
\midrule
FN-Atom [N]        & 67.2(0.02) & 69.2(3) & 63.8(1) & 61.6(0.6) & 57.5(2) & 1.07(0.1)  & 1.18(0.04)  & 1.20(0.04) \\
FN-Fragment [N]    & 81.9(6.3)  & 70.6(3.6) & 59.9(0.8) & 62.8(2.2) & 55.1(0.9)& 0.900(0.06) & 1.20(0.05) & 1.19(0.04) \\
FN-Atom [Y]        & \textbf{94.0(1.1)}  & 79.5(0.8) & 69.7(1) & \textbf{86.0(1)} & \textbf{64.2(3)} & \textbf{0.676(0.05)} & 0.768(0.06) & 1.063(0.04) \\
FN-Fragment [Y]    & 90.8(1)  & \textbf{82.0(1)} & \textbf{70.2(0.8)} & 84.6(0.8) & 62.5(2) & 0.722(0.1) & \textbf{0.754(0.05)} & \textbf{1.06 (0.04)} \\
\midrule
\multicolumn{9}{c}{\textbf{Medium+ Molecule Models (100M+ Params or Trained on 1B+ Molecules)}} \\
\midrule
GROVER\textsuperscript{1}          & 94.0(0.02)   & 83.1( 0.03)  & \textbf{73.7(0.01)}   & 89.4(0.03)   & 65.8(0.02)   & 0.831(0.1)      & 0.560(0.04)  & -  \\
MolFormer-XL\textsuperscript{8}     & 93.7   & 84.7   & 65.6   & 88.2   & 66.9   & 0.279   & \textbf{0.231}      & - \\
MolTRES\textsuperscript{7}         & \textbf{96.1}   & \textbf{85.3}   & 70.1   & \textbf{91.7}   & \textbf{69.8}   & \textbf{0.274}      & 0.504      & -  \\
\bottomrule
\end{tabularx}
\end{threeparttable}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\textbf{Takeaway 1 - Token Granularity Impacts Task Complexity and Structural Context.} \\
We empirically demonstrate the importance of an adaptive graph tokenizer for chemically accurate masked fragment modeling. Token granularity, controlled by the number of merge iterations, affects the complexity and informativeness of the Masked Fragment Modeling (MFM) task. At 0 merge iterations (smallest granularity), fragments consist of a single atom, its bond information, and dummy atoms indicating neighboring connections. When pre-trained on 2 million molecules, as shown in (\cref{app: pre-training curves}), this setup leads to stable convergence, achieving good reconstruction accuracy within the first epoch. Increasing granularity through additional merges (100 iterations) makes the task more complex and data-efficient, with slower but sustained learning, highlighting the potential for improved performance when paired with larger, more diverse datasets. Larger tokens capture higher-level structural motifs, enabling better generalization in downstream tasks. Our pre-trained models consistently outperform un-pretrained models across benchmarks, verifying the effectiveness of MFM and addressing the negative transfer issues common in atom-level masking.

\textbf{Takeaway 2 - Token Granularity Can Be Optimized for Task-Specific Performance.} \\
Our results suggest that token granularity, which controls the size and distribution of fragment tokens, can be treated as an optimizable hyperparameter to improve both pre-training and downstream task performance. As shown in \cref{tab:combined_results}, neither fine-grained nor coarse-grained tokens consistently outperform the other across all tasks, highlighting the need for task-specific tuning. By adjusting the number of merge iterations and token dictionary distribution, future work could explore optimal granularity for specific chemical properties or molecular tasks, enhancing generalizability across diverse applications.

\textbf{Takeaway 3 - FragmentNet Outperforms Comparable-Scale Models.} \\
FragmentNet demonstrates state-of-the-art performance on 7 of 8 MoleculeNet \cite{wu2018moleculenet} and Malaria \cite{Gamo2010-iq} benchmark tasks among foundation models of comparable (small) scale, highlighting the effectiveness of MFM pre-training and adaptive tokenization over legacy pre-training approaches.

\textbf{Takeaway 4 - FragmentNet Is Competitive with Large-Scale Models Using a Chemically Informed, Data-Efficient Framework.} \\
FragmentNet achieves competitive performance against medium- to large-scale models trained on 1000x more data using extensive GPU clusters. In contrast, our approach trains a small foundation model on a modest setup (a laptop with limited computational resources) while leveraging a chemically informed Masked Fragment Modeling (MFM) task and adaptive tokenization, rather than rule-based substructure masking or SMILES-based masked modeling, for more efficient learning.

Unlike frontier SMILES Transformers \cite{ross2022large}, which converge quickly but require large datasets and struggle with generalization \cite{park2024moltres}, our MFM approach leverages the underlying structure and chemical physics of molecules to achieve comparable performance with far less data and compute. By compressing molecular graphs into fewer, chemically meaningful tokens, fragment-based tokenization shortens sequence lengths and lowers the computational cost typical of Transformer-based models. These combined benefits of graph- and sequence-based modeling position FragmentNet to efficiently scale to trillions of molecules \cite{neumann2023relevance}.

\subsection{Dataset Segregation Visualizations}
\label{Dataset Segregation Visualizations}
\begin{figure}[h]
\vskip 0.2in
\begin{center}
\includegraphics[width=1.0\linewidth]{icml2025/images/lipo.png}
    \caption{t-SNE of Embedding space for Lipophilicity prediction task before (left) and after (right) fine-tuning, extracted from the second-to-last model layer. Color depicts true values of each data-point.}
    \label{fig:Lipo t-sne}
    \end{center}
    \vskip -0.2in
\end{figure}

We leverage t-SNE to visualize how the embedding space evolves during fine-tuning. Taking the Lipophilicity dataset as an example, we analyze embeddings from the penultimate layer of the fine-tuned model. In \cref{fig:Lipo t-sne}, before training, the points corresponding to molecules with different logD values are randomly scattered across the embedding space. Through training, they become increasingly organized, with emerging clusters that suggest meaningful separations related to molecular properties, including lipophilicity. t-SNE effectively captures global patterns and clustering within the molecular dataset, offering insights into overall relational trends. However, it is less suited for detailed chemical interpretability at the level of individual molecules. Additional t-SNE visualizations are provided in \cref{t-sne appendix analysis}.

\subsection{Attention Maps}
\begin{figure}
\vskip 0.2in
\begin{center}
    % Image 1: molecule_attention
\centerline{\includegraphics[width=0.5\textwidth]{icml2025/images/esol_molecule_attention.png}}
    \caption{2-methyl-1-butanol, CCC(C)CO Attention Map for ESOL (aqueous solubility)}
    \label{fig:2-methyl-1-butanol att map}
\end{center}
\vskip -0.2in
\end{figure}
Our approach reveals chemically intuitive patterns by capturing fragment-fragment relationships. As shown in \cref{fig:2-methyl-1-butanol att map}, different attention heads naturally identify distinct molecular features that influence solubility. Most heads emphasize the hydroxyl group (-OH) (red regions), aligning with its role in forming hydrogen bonds with water. Notably, one head focuses on the hydrophobic alkyl fragments—the methyl substituent and carbon backbone—which limit water solubility. This demonstrates how attention mechanisms capture the competing effects of hydrophilic and hydrophobic fragments, highlighting chemical interpretability. Additional attention-map analyses are provided in \cref{Appendix: attention maps discussion}.

\subsection{Fragment Swapping Discussion}
To illustrate the capabilities of the fragment swapping module discussed in \cref{frag:swap}, we start with the molecule Ibuprofen. The adaptive tokenizer serializes Ibuprofen into tokenized fragments, allowing us to identify and modify the carboxylic acid and methyl groups precisely. We then swap out fragments at the target modification site for alternatives using our fragment-swapping algorithm, which avoids the need for substructure matching or graph-based searches.

Using this approach, we generate two chemically valid analogues: \texttt{CC(C)Cc1ccc(Cl)cc1} and \texttt{CC(C)Cc1ccc(cc1)C(=O)O} (\cref{fig:ibuprofen fragment swapping}). These analogues preserve the molecular scaffold of Ibuprofen while introducing structural diversity, demonstrating how the module facilitates efficient, precise analogue generation.

This approach highlights the novelty of our framework: by leveraging adaptive tokenization to simplify and serialize molecular representation, we can rapidly and systematically explore chemical spaces. This is particularly valuable for applications such as drug discovery and materials design. Further examples are in \cref{appendix:generate_analogues}.
\begin{figure}[h]
\vskip 0.2in
\begin{center}
    % Image 1: molecule_attention
\centerline{\includegraphics[width=0.5\textwidth]{icml2025/images/ibuprofen_candidates.jpg}}
    \caption{Left: Ibuprofen. Center: We fragment off the carboxylic acid and methyl groups. Right: We generate two alternative molecules through fragment swapping, CC(C)Cc1ccc(Cl)cc1, and CC(C)Cc1ccc(cc1)C(=O)O}
    \label{fig:ibuprofen fragment swapping}
    \end{center}
    \vskip -0.2in

\end{figure}

\section{Limitations and Future Work}
This study was conducted on a relatively small scale for foundation models, utilizing a pre-training dataset of only 2 million molecules. Future research will address these limitations by expanding the dataset to encompass billions of molecules, unlocking the training of larger and more comprehensive models. Incorporating auxiliary techniques such as contrastive learning and further optimizing token granularity is expected to enhance predictive performance. Exploring adherence to scaling laws observed in the language domain \cite{kaplan2020scaling} will help assess how well FragmentNet’s performance scales with increased data and model size. Additionally, the limited size of MoleculeNet datasets presents overfitting challenges, emphasizing the need for improved chemical pre-training to enhance generalization. Collaborating with chemists will help design more interpretable models aligned with domain knowledge.

\section{Conclusion}
We introduce FragmentNet, a novel graph-to-sequence molecular foundation model featuring the first adaptive graph tokenizer used for MFM. Our framework enables the decomposition, serialization, and reconstruction of molecular graphs, allowing sequential architectures to model them effectively while preserving key topological information. Using chemically informed, data-efficient pre-training tasks, FragmentNet captures meaningful, domain-specific relationships, outperforming similarly scaled models and achieving competitive performance with larger foundation models—all while being trained on a single MacBook Pro. Additionally, our fragment-based chemical editing module opens new possibilities for targeted molecular optimization and deeper insights into structure-property relationships. Moving forward, we aim to scale FragmentNet to establish the leading molecular graph foundation model, driving advancements in molecular discovery and optimization.

% Acknowledgements should only appear in the accepted version.

\section*{Impact Statement}
This paper aims to advance the field of Machine Learning in Drug Discovery and Optimization by leveraging AI to enhance the understanding and design of molecules. Our work has significant potential societal implications, particularly in enabling researchers and industry professionals to develop and refine therapeutics more effectively. By advancing the scientific methodologies underlying these efforts, we hope to contribute to creating safer, more effective treatments that address critical healthcare challenges.

\bibliography{example_paper}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \subsection{Related Work - Model and Tokenizer Comparison}

% \begin{table}[H]
%     \centering
%     \caption{Summary of Molecular Representation Learning Methods}
%     \begin{tabularx}{\textwidth}{|l|X|X|X|X|X|}
%         \hline
%         \textbf{Paper} & \textbf{pre-training} & \textbf{Data} & \textbf{Tokenizer} & \textbf{Model} & \textbf{Masking} \\
%         \hline
%         GROVER & Masked nodes/edges for property prediction & Graphs (RDKit features) & None; direct graph ops & GNN+Transformer & Masks subgraphs; predicts node/edge counts \\
%         \hline
%         AttrMask & Attribute masking for nodes/edges & Graphs & None & GNN & Randomly masks node or edge attrs \\
%         \hline
%         MoleBERT & Masked Atoms Modeling (MAM) & Graphs & VQ-VAE context-aware & GNN & Not specified \\
%         \hline
%         SimSGT & Subgraph masking/remasking & Graphs & None; learns latent embeddings & Lightweight GNN & Masks entire subgraphs; dynamic selection \\
%         \hline
%         UniCorn & 2D masking, 2D-3D contrastive, 3D denoising & 2D graphs, 3D conformations & BRICS decomposition & Contrastive learning & Masks groups/rings via BRICS \\
%         \hline
%         MolTRES & Substructure masking w/ generator-discriminator & SMILES & Rule-based fixed tokens & Transformer + knowledge transfer & Not specified \\
%         \hline
%         Molformer & Dynamic substructure masking & SMILES & WordPiece-inspired tokenizer & Transformer & Not specified \\
%         \hline
%         \makecell{Molecular \\ Transformer} & Seq-to-seq reaction pred. & SMILES & Rule-based SMILES syntax & Transformer w/ attention & N/A (reaction pred.) \\
%         \hline
%     \end{tabularx}
%     \label{tab:molecular_representation_learning_methods}
% \end{table}

% \subsection{Fragment-Based Masking vs.\ Atom-Based Masking: Formal Definitions and Theoretical Justifications}
% \label{app:appendix_fragment_masking}

% In this appendix, we provide a systematic overview of \emph{masked fragment modeling} (MFM) and contrast it with \emph{masked atom modeling} (MAM), focusing on the differences in tokenization and masking strategies. We begin by defining the molecular graph representation and introduce an intermediate method for cases without a dedicated fragment tokenizer. We then discuss the theoretical rationale and practical benefits of fragment-level masking, highlighting its capacity to capture coherent chemical contexts while reducing computational overhead.

% \subsection{Formal Definitions}

% \paragraph{Molecular Graph.}
% A molecule is represented as a graph \( G = (A, B) \), where:
% \begin{itemize}
%     \item \( A = \{a_1, a_2, \dots, a_{|A|}\} \) is the set of atoms.
%     \item \( B \subseteq A \times A \) is the set of chemical bonds, where each bond \( b_{ij} \) connects atoms \( a_i \) and \( a_j \) and may carry annotations (e.g., single, double, aromatic).
% \end{itemize}

% \paragraph{Masked Atom Modeling (MAM), Masked Substructure Modeling (MSM), and Masked Fragment Modeling (MFM).}
% We define three masking strategies based on how atoms and substructures are treated as tokens:

% \begin{itemize}
%     \item \textbf{Masked Atom Modeling (MAM).} In MAM, each atom \( a_i \in A \) is treated as an independent token, converting the molecule into a sequence
%     \[
%     \mathbf{A} = [a_1, a_2, \dots, a_L],
%     \]
%     where each \( a_k \) represents a single atom (and any local context features). A subset of positions \(\mathcal{M}_\mathrm{atom} \subset \{1, \dots, L\}\) is randomly chosen and replaced with the \texttt{[MASK]} token. The model is trained to reconstruct the masked atoms:
%     \[
%     \mathcal{L}_{\mathrm{MAM}} 
%     = -\sum_{m \in \mathcal{M}_\mathrm{atom}} \log P\bigl(a_m \,\big\vert\, \mathbf{A}_{\text{masked}}\bigr).
%     \]
%     While MAM allows fine-grained control over masking, it can disrupt functional substructures and local chemical contexts, especially when multiple isolated atoms are masked \cite{hu2019strategies, jin2018junction}.
    
%     \item \textbf{Masked Substructure Modeling (MSM).} MSM replaces a \emph{connected subgraph} of atoms without relying on a fragment-level tokenizer. Formally, let
%     \[
%     S = (A_S, B_S), \quad A_S \subseteq A, \quad B_S \subseteq B
%     \]
%     be a connected subgraph of \( G \). One can randomly select a central atom and expand outward until reaching a preset size. All atoms in \( A_S \) and bonds in \( B_S \) are then replaced with one or more \texttt{[MASK]} tokens, yielding:
%     \[
%     G_{\text{masked}} = (A \setminus A_S, B \setminus B_S) \cup \{\texttt{[MASK]}\}.
%     \]
%     The model is trained to reconstruct the entire subgraph:
%     \[
%     \mathcal{L}_{\mathrm{MSM}} 
%     = - \log P\bigl(S \mid G_{\text{masked}}\bigr).
%     \]
%     By masking a connected region, MSM preserves local bonding context more effectively than MAM. However, without a defined fragment inventory, it may not respect chemically meaningful boundaries such as functional groups or aromatic rings. Additionally, sampling and merging multiple subgraphs into a single mask can be computationally costly at scale, complicating data preprocessing and batching.
    
%     \item \textbf{Masked Fragment Modeling (MFM).} In MFM, the molecule is explicitly decomposed into coherent subgraphs (fragments),
%     \[
%     \mathbf{F} = [f_1, f_2, \dots, f_N],
%     \]
%     where each fragment \( f_i \) comprises multiple atoms and their associated bonds from \( A \) and \( B \). One or more fragments in \(\mathbf{F}\) are replaced with the \texttt{[MASK]} token, and the model learns to reconstruct the missing subgraph(s):
%     \[
%     \mathcal{L}_{\mathrm{MFM}} 
%     = -\sum_{m \in \mathcal{M}_\mathrm{frag}} \log P\bigl(f_m \,\big\vert\, \mathbf{F}_{\text{masked}}\bigr).
%     \]
%     Since each fragment corresponds to a chemically coherent unit (e.g., a functional group or aromatic ring), the unmasked fragments collectively provide a more realistic chemical context \cite{liu2022structured, hu2019strategies}.
% \end{itemize}

\section{Theoretical Justifications for Adaptive Tokenization in Large-Scale Molecular Modeling}
\label{app: theoretical rationale for adaptive tokenization}

\subsection{Information-Theoretic Motivation in the Molecular Domain}

Large chemical databases such as ZINC \cite{irwin2005zinc} and ChEMBL \cite{gaulton2017chembl} contain millions of molecules with high structural diversity and skewed substructure frequency distributions. A few ring systems and functional groups occur frequently, while many motifs rarely appear. Rule-based tokenizers, such as RECAP \cite{lewellyn1998recap} or BRICS \cite{degen2008art}, rely on static, chemistry-driven heuristics—e.g., predefined bonds to cleave or functional groups to isolate—without adapting to actual usage frequencies. This often leads to misaligned fragmentations and inefficiencies in downstream modeling.

Shannon's source coding theorem \cite{shannon1948mathematical} indicates that the optimal average code length for a random variable \( X \) with probability distribution \( p(x) \) is bounded below by its entropy: \( H(X) = - \sum_{x \in \mathcal{X}} p(x)\,\log p(x) \).
In a molecular context, \( X \) can represent common substructures in the dataset, with \( p(x) \) as their empirical frequencies. A rule-based fragmenter defines a static vocabulary that ignores these frequencies. Contrastingly, an adaptive tokenizer seeks a vocabulary aligned with \( p(x) \), minimizing the overall code length. Frequent motifs become single tokens, while rarer motifs are split more finely.

\subsection{Compression and the MDL Principle in Molecular Data}

The principle of minimum description length (MDL) \cite{rissanen1978modeling} states that the best representation of the data minimizes both the complexity of the model and the encoded length of the data. Subword tokenizers in natural language processing—such as BPE \cite{sennrich2016neural} and WordPiece \cite{schuster2012japanese}—achieve compression by merging frequent character or subword sequences. In the molecular domain, a similar procedure merges atoms or smaller functional groups that co-occur in local structures. Iterating this process yields a vocabulary of substructures that reduces the corpus's encoded length.

Such a data-driven vocabulary can be viewed as a fragment discovery method, where tokens correspond to commonly reused chemical units (e.g., rings, linkers, functional groups). Unlike fixed heuristics, an adaptive tokenizer calibrates fragment size to the corpus distribution, balancing frequent and rare substructures.

\subsection{Universal Coding Perspective and Long-Tail Frequencies}

From the perspective of universal coding \cite{ziv1977universal}, an ideal encoder adaptively identifies and stores repeating patterns to minimize code length. BPE-like algorithms can be viewed as simplified universal coders, merging frequent local sequences into tokens. Applied to molecules, this naturally captures recurring motifs like ring scaffolds, linkers, or functional groups, thereby shortening sequences and producing more coherent representations.

\section{Tokenizer Discussion}
\label{app: tokenizer discussion}
\subsection{Sample Readout from Token Dictionary}

The below token dictionaries were generated using FragmentNet's adaptive tokenizer with 0 and 100 merge iterations for atoms and fragments, respectively, rather than relying on rule-based fragmentation approaches. Each entry in the token dictionary is a tuple comprising the fragment's hash ID, its SMILES representation (where \texttt{*} denotes a placeholder atom), a graph representation that includes node features (\texttt{x}), edge connections (\texttt{edge\_index}), edge features (\texttt{edge\_attr}), and the number of atoms (\texttt{num\_atoms}), along with additional metadata (currently \texttt{None}).

\subsection{Granularity \texttt{num\_iters=0} Sample Entries}

Within our tokenization framework, the smallest level of granularity retains the structure of common fragments through bonds and dummy atoms; it keeps a single atom within the fragment. However, retaining the connection points makes the fragment more informative than a singular atom. For example:

1. Double-bonded oxygen (=O) with one connection point:
\begin{lstlisting}
(('a55e82208e74da72a2918a7c63ed516a', '*=O', Data(x=[2, 133], edge_index=[2, 2], edge_attr=[2, 147], num_atoms=2), None))
\end{lstlisting}
Functional groups like aldehydes (*CH=O), ketones (C()=O), or carboxylic acids (*C(=O)OH) could have been preserved if fragments were tokenized at a coarser granularity.

2. Carbon with one double bond and two connection points:
\begin{lstlisting}
(('4bbdcb3d627f9a34eeb0c15cfb26b630', '*C=*', Data(x=[3, 133], edge_index=[2, 4], edge_attr=[4, 147], num_atoms=3), None))
\end{lstlisting}
Coarser fragments might better represent substructures like vinyl groups (CH=CH2), acrylic esters (CH=C()O), or conjugated dienes (CH=CH-CH=CH).

3. Nitrogen with three connection points:
\begin{lstlisting}
(('99b11f1d4f8f8cb2956189b2117d5b02', '*N(*)*', Data(x=[4, 133], edge_index=[2, 6], edge_attr=[6, 147], num_atoms=4), None))
\end{lstlisting}
Such fragments could include amide groups (N()C(=O)), sulfonamides (N()S(=O)2*), or tertiary amines (N(CH3)2).

% 4. Oxygen with two connection points:
% \begin{lstlisting}
% (('1981ea02744e9a1f8804a6a844fff8fd', '*O*', Data(x=[3, 133], edge_index=[2, 4], edge_attr=[4, 147], num_atoms=3), None))
% \end{lstlisting}
% Coarser tokenization would allow representation of ethers (COC), esters (C(=O)OC), or carbamates (NC(=O)O).

% 5. Carbon with one connection point:
% \begin{lstlisting}
% (('746308d88275602fbb647692b0c622b0', '*C', Data(x=[2, 133], edge_index=[2, 2], edge_attr=[2, 147], num_atoms=2), None))
% \end{lstlisting}
% Such fragments might correspond to methyl groups (*CH3), ethyl groups (*CH2CH3), or isopropyl groups (*CH(CH3)2).

\subsection{Fragment Token Dictionary Sample Entries}

As we increase the granularity, fragment-based tokenization creates more chemically meaningful tokens. Note that the fragment dictionary retains the atom fragments, which can be used as needed. Below are a few samples from the fragment token dictionary:

1. A chiral cyclic amine with a carbonyl group:
\begin{lstlisting}
(('dd071a4542f22e18743e1be19dcd20b2', '*[C@@H]1CCCN1C=O', Data(x=[8, 133], edge_index=[2, 16], edge_attr=[16, 147], num_atoms=8), None))
\end{lstlisting}

2. A hydroxyl group attached to a benzene ring with a placeholder connection point:
\begin{lstlisting}
(('ec766bc887f489cc1b0d875eb133c2d6', '*Oc1cccc(*)c1*', Data(x=[10, 133], edge_index=[2, 20], edge_attr=[20, 147], num_atoms=10), None))
\end{lstlisting}

% 3. A carbonyl group with one connection point, representing a terminal double-bonded oxygen:
% \begin{lstlisting}
% (('a55e82208e74da72a2918a7c63ed516a', '*=O', Data(x=[2, 133], edge_index=[2, 2], edge_attr=[2, 147], num_atoms=2), None))
% \end{lstlisting}

% 2. A nitrogen atom with a single connection point, representing a simple amine:
% \begin{lstlisting}
% (('70beee19717c033cc128c318041a2eef', '*N*', Data(x=[3, 133], edge_index=[2, 4], edge_attr=[4, 147], num_atoms=3), None))
% \end{lstlisting}

% 5. A benzene ring with a sulfonic acid substituent:
% \begin{lstlisting}
% (('6e40824dec53ac12fd251c4a614b7c1e', '*c1ccc([SH](=O)=O)cc1', Data(x=[10, 133], edge_index=[2, 20], edge_attr=[20, 147], num_atoms=10), None))
% \end{lstlisting}

\subsection{Token Dictionary Length}
As this is an adaptive tokenizer, the size of the token dict tends to scale based on the size of the token dict training dataset and the tokenizer granularity.

Due to our computing constraints, it was not feasible for us to train a token dictionary with the same dataset as our pre-training task, which consisted of 2 million SMILES. Therefore, we decided to use a 17k subset of the pre-training dataset and saw an atom token dictionary of length 98 and a fragment token dictionary length of 8737.

\subsection{Fragment Token Size Distribution}
\label{app: frag token size dist}

\begin{figure}[H]
\vskip 0.2in

\begin{center}
    \centerline{\includegraphics[width=0.45\textwidth]{icml2025/images/token_dict_distribution.png}}
    \centerline{\includegraphics[width=0.45\textwidth]{icml2025/images/num_fragments_per_molecule.png}}
    \caption{(Top) Distribution of number of atoms in each token in our token dictionary, (Bottom) Number of fragments for each molecule in the pre-training dataset}
    \label{fig: token size distribution}
\end{center}
    \vskip -0.2in

\end{figure}

\cref{fig: token size distribution} illustrates key statistics about our molecular tokenization scheme after applying 100 merge operations. The first distribution shows the number of fragments per molecule, indicating that most molecules contain approximately 7 fragments on average. However, the distribution exhibits a tail of larger molecules with more fragments. This suggests that our tokenizer effectively compresses molecular structures while maintaining meaningful substructures.

The second distribution displays the number of atoms per token, revealing that most tokens correspond to fragment sizes of around 10 atoms. This indicates that our tokenization approach balances fragmentation and meaningful chemical subunits, crucial for learning robust molecular representations.

In our Masked Fragment Modeling (MFM) task, we leverage this tokenization scheme by randomly masking a fragment and training the model to predict the missing part. This task encourages the model to capture local chemical dependencies while learning a high-level structural understanding of molecules. The observed fragment distribution suggests that our tokenizer segments molecules into chemically meaningful building blocks, which could enhance generalization in downstream molecular property prediction tasks.

\subsubsection{Discussion on Tokenizer Granularity}

The atom and fragment token dictionaries demonstrate the flexibility of our tokenizer framework, which allows granularity to be parameterized based on the number of merge iterations. This enables direct comparison of masked pre-training tasks within the same modeling framework - a capability previously unavailable for molecular graphs.

By tuning the number of merge iterations, we can influence the distribution of token granularities learned by the tokenizer. This parameterization enables exploration of how granularity distribution affects different prediction tasks. However, further research is needed to evaluate the optimality of different granularity distributions specific to downstream property task performance.


\section{Weisfeiler-Lehman Molecule Hashing}
\label{app: wl hashing}
The WL-hashing algorithm we have adapted to molecules can uniquely hash them while distinguishing many forms of isomerism, which is not currently possible with other string-based molecular representations such as Smiles \cite{weininger1988smiles} and InChi \cite{heller2015inchi}. In Table 2, we review a well-known case of isomerism and note that molecules that exhibit forms of isomerism are assigned unique hashes through our proposed WL-molecular hashing methods, despite having the same smiles strings.
\begin{table}[H]
\centering
\begin{tabular}{m{0.1\textwidth}|m{0.4\textwidth}|m{0.4\textwidth}}
% Defines three columns (centered)
% \toprule
\textbf{Isomerism Type} & \textbf{Molecule 1} & \textbf{Molecule 2} \\ % Column headers
\midrule
Geometric Isomerism & \includegraphics[width=0.2\textwidth]{icml2025/images/WL-hashed-molecules/cis2butene.png} \newline \textit{Cis-2-butene. Smiles: C/C=C/C \newline Hash: c8cf1d7c17272855dc186976e7075a43} & \includegraphics[width=0.2\textwidth]{icml2025/images/WL-hashed-molecules/trans2butene.png} \newline \textit{Trans-2-butene. Smiles: C/C=C\textbackslash C \newline Hash: 610a093419128912a6f09f82b9321df3} \\ 
\label{tab:isomerism table}
\end{tabular}
\caption{Common forms of isomerism in molecules.}
\end{table}

% \midrule
% Enantiomers & \includegraphics[width=0.2\textwidth]{icml2025/images/WL-hashed-molecules/r2chlorobutane.png} \newline \textit{(R)-2-chlorobutane. Smiles: CC(CCl)C \newline Hash: 03c84bb7937e293cd9e060fc8af34629} &  \includegraphics[width=0.2\textwidth]{icml2025/images/WL-hashed-molecules/s2chlorobutane.png} \newline \textit{(S)-2-chlorobutane. Smiles: CC([C@@H](C)Cl)C \newline Hash: a59bb90b4ad7140ac40b11624a66268a} \\

% \midrule
% Keto-enol tautomerism & \includegraphics[width=0.2\textwidth]{icml2025/images/WL-hashed-molecules/acetone.png} \newline \textit{Acetone. Smiles: CC(=O)C \newline Hash: c85f404d886cb32f836d36f8ff72b0bd} &  \includegraphics[width=0.2\textwidth]{icml2025/images/WL-hashed-molecules/propen2ol.png} \newline \textit{Propen-2-ol. Smiles: C=C(O)C \newline Hash: 1560e9b1d87a08ec7532ee4b13151fdf} \\

% \midrule
% Constitutional Isomers & \includegraphics[width=0.2\textwidth]{icml2025/images/WL-hashed-molecules/nbutane.png} \newline \textit{N-butane. Smiles: CCCC \newline Hash: 563710678b26ddeebfe02e65788b715a} &  \includegraphics[width=0.2\textwidth]{icml2025/images/WL-hashed-molecules/isobutane.png} \newline \textit{Isobutane. Smiles: CC(C)C \newline Hash: ffc6b52475ba96e936f691ce811e73bb} \\
\section{Fragment Charges}
\label{app: fragment charges}
\textbf{Fragment Charges.} We define the charge of a fragment as the sum of the Gasteiger-Marsili partial charges for all non-dummy atoms in the fragment, i.e., \( Q_{\mathcal{F}} = \sum_{a \in \mathcal{F}} q_a \), where \( q_a \) is the partial charge of atom \( a \) if the total charge is not a number (NaN), set \( Q_{\mathcal{F}} = 0 \).

\section{Spatial Positional Encodings Visualization}
\label{app: spatial posenc visualization}
Figure \cref{fig: posenc visualization} Provides a visualization of the principal component analysis (PCA) applied to fragments of a molecule under different positional encoding schemes: Weisfeiler-Lehman (WL), multi-anchor hop-based (HOP), Coulomb (CLB), and their aggregated encoding (POS). The WL encoding captures the relative positional relationships between nodes by using a breadth-first search (BFS)-type algorithm to encode the connectedness of nodes within the molecular graph. The HOP encoding generalizes positional relationships further by considering the connectivity of each node to all other nodes in the graph, providing a more global sense of structural embedding. The CLB encoding, in contrast, leverages electrochemical properties, encoding nodes based on the Coulombic interactions, which emphasize the physical and chemical characteristics of the molecule. When aggregated, these encodings (POS) yield a comprehensive differentiation of molecular fragments, as seen in the rightmost PCA plots. This demonstrates these encodings' complementary strengths in capturing structural and chemical nuances, effectively distinguishing molecular substructures.

\begin{figure} [H]
\vskip 0.2in
\begin{center}
    \centerline{\includegraphics[width=0.6\textwidth]{icml2025/images/positional_encoding_vis.jpg}}
    \caption{The principal component differentiation of the molecule-fragments under Weisfeiler-Lehman (WL) positional-encoding, multi-anchor hop-based (HOP), Coulomb (CLB) and the aggregated encoding (POS).}
    \label{fig: posenc visualization}
    \end{center}
\vskip -0.2in

\end{figure}


\section{Fragment Swapping Algorithm}
\label{appendix:generate_analogues}
The \textit{Fragment Swapping Algorithm} is designed to generate chemically valid analogues of a given molecule by systematically substituting molecular fragments at predefined positions. The input molecule ($\mathcal{M}$) contains dummy atoms denoting points of substitution, and a set of candidate fragments ($f_i \in \mathcal{F}$) with matching dummy atoms is provided. The algorithm ensures chemical validity by matching the bond environments of the dummy atoms in both the input molecule and candidate fragments, followed by reconstruction and sanitization of the substituted structure.

This approach facilitates the controlled exploration of structural diversity, maintaining the core scaffold of the original molecule while varying its substituents. This is particularly valuable for drug discovery and materials design, where analogues with similar structural frameworks but differing properties can offer insights into structure-activity relationships (SAR).

\subsection{Algorithm}
\label{fragment swapping algorithm}
The pseudocode for the \textit{Fragment Swapping Algorithm} is presented below:

\begin{algorithm}[h]
\caption{Generate Analogues}
\begin{algorithmic}
\STATE \textbf{Input:} Molecule $\mathcal{M}$ with dummy atoms denoting dangling bonds for fragment insertion.
\STATE Set of candidate fragments $f_i \in \mathcal{F}$ with dummy atoms denoting dangling bonds.
\STATE \textbf{Output:} List of chemically valid generated analogues, with identical structure to inputted molecule $\mathcal{M}$ but with fragment substitution.

\STATE \textbf{function} \texttt{get\_atom\_bond\_hash}($\mathcal{M}$) 
    \STATE \hspace{1em} hash dict $\gets \emptyset$
    \STATE \hspace{1em} \textbf{for} dummy atom $a_i$ \textbf{in} $\mathcal{M}$ \textbf{do}
        \STATE \hspace{2em} num bonds per type $\gets \emptyset$
        \STATE \hspace{2em} \textbf{for} bond $b_{i,j}$ \textbf{connected to} $a_i$ \textbf{do}
            \STATE \hspace{3em} num bonds per type[type($b_{i,j}$)] $\gets$ num bonds per type[type($b_{i,j}$)] + 1
        \STATE \hspace{2em} \textbf{end for}
        \STATE \hspace{2em} hash dict[hash(num bonds per type)].append($a_i$)
    \STATE \hspace{1em} \textbf{end for}
    \STATE \hspace{1em} \textbf{return} hash dict
\STATE \textbf{end function}

\STATE \textbf{for} each $f_i$ \textbf{in} $\mathcal{F}$ \textbf{do}
    \STATE \hspace{1em} fragment hash dict $\gets$ \texttt{get\_atom\_bond\_hash}($f_i$)
    \STATE \hspace{1em} \textbf{if} keys(fragment hash dict) $\neq$ keys(mol hash dict) \textbf{then}
        \STATE \hspace{2em} \textbf{continue}
    \STATE \hspace{1em} \textbf{end if}
    \STATE \hspace{1em} \textbf{if} length(fragment hash dict[$k$]) $\neq$ length(mol hash dict[$k$]) $\forall k \in$ keys(fragment hash dict) \textbf{then}
        \STATE \hspace{2em} \textbf{continue}
    \STATE \hspace{1em} \textbf{end if}

    \STATE \hspace{1em} \textbf{for} key \textbf{in} keys(fragment hash dict) \textbf{do}
        \STATE \hspace{2em} Generate all possible mappings between fragment hash dict[key] and mol hash dict[key].
    \STATE \hspace{1em} \textbf{end for}
    \STATE \hspace{1em} \textbf{for} all unique mappings of dummy atoms \textbf{do}
        \STATE \hspace{2em} For each mapping, reconstruct $\mathcal{A}$ by removing paired dummy atoms and bonds.
        \STATE \hspace{2em} \textbf{if} \texttt{RdKit.Chem.SanitizeMol}($\mathcal{A}$) without error \textbf{then}
            \STATE \hspace{3em} analogues $\gets$ analogues $\cup \mathcal{A}$
        \STATE \hspace{2em} \textbf{end if}
    \STATE \hspace{1em} \textbf{end for}
\STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}

\subsection{Example 1: Aspirin}
In this example, the starting molecule is Aspirin (\texttt{CC(C)CC1=CC=C(C=C1)C(C)C(=O)O}). We fragment off the carboxylic acid functional group and substitute it with two chemically valid fragments. The resulting analogues are depicted in \cref{fig:aspirin_candidates}. 

\textbf{Left:} The original Aspirin molecule. \\
\textbf{Center:} The carboxylic acid group is removed, leaving a dummy atom for substitution. \\
\textbf{Right:} Two analogues are generated: \texttt{CC(=O)Oc1ccccc1C(O)C} and \texttt{CC(=O)Oc1ccccc1OC(C)C}. Both analogues preserve the overall molecular scaffold while introducing new functional groups, potentially altering biological activity or physicochemical properties.


\begin{figure}[H]
\vskip 0.2in
\begin{center}
    \centerline{\includegraphics[width=0.75\textwidth]{icml2025/images/aspirin_candidates.jpg}}
    \caption{Fragment swapping example for Aspirin.}
    \label{fig:aspirin_candidates}
    \end{center}
\vskip -0.2in

\end{figure}

\subsection{Example 2: Diazepam}
In this example, the starting molecule is Diazepam (\texttt{CN1C(=O)c2ccccc2C1c3ccccc3Cl}). We fragment off the chlorobenzene functional group and substitute it with two alternative fragments, as shown in \cref{fig:diazepam_candidates}. 

\textbf{Left:} The original Diazepam molecule. \\
\textbf{Center:} The chlorobenzene group is removed, leaving a dummy atom for substitution. \\
\textbf{Right:} Two analogues are generated: \texttt{CN1C(=O)c2ccccc2C1c1ccc(Cl)cc1} and \texttt{CN1C(=O)C2=CC=CC=C2C1(O)C}. These modifications alter the electronic properties and potential receptor binding of Diazepam derivatives. \\

\begin{figure}[H]
\vskip 0.2in
\begin{center}
    \centerline{\includegraphics[width=0.75\textwidth]{icml2025/images/diazepam_candidates.jpg}}
    \caption{Fragment swapping example for Diazepam.}
    \label{fig:diazepam_candidates}
    \end{center}
\vskip -0.2in

\end{figure}

\subsection{Significance of Adaptive Fragmentation and Fragment Swapping}

Adaptive fragmentation, coupled with the \textit{Fragment Swapping Algorithm}, offers a robust framework for efficiently generating analogues and exploring chemical space. By dynamically identifying frequently occurring substructures, adaptive fragmentation eliminates reliance on rigid heuristics or predefined fragment libraries, ensuring chemical validity and reducing computational overhead. Future work involves integrating the algorithm with machine learning models to prioritize analogues based on predicted properties.

\section{Property Prediction Embedding Space Visualization}
\label{t-sne appendix analysis}

 Before fine-tuning, embeddings are unstructured across all datasets. Regression tasks like ESOL lack alignment with continuous target values, while classification datasets like BBBP exhibit significant class overlap.

\begin{figure}
\vskip 0.2in
\begin{center}
    \centerline{\includegraphics[width=0.8\linewidth]{icml2025/images/esol.png}}
    \centerline{\includegraphics[width=0.8\linewidth]{icml2025/images/BBBP.png}}
    \caption{t-SNE visualization of FragmentNet embedding spaces. Top: ESOL (regression); Bottom: BBBP (classification). Left: before fine-tuning; Right: after fine-tuning. Colors represent true values or labels.}
    \label{fig:app t-SNE}
    \end{center}
\vskip -0.2in
\end{figure}

Fine-tuning significantly improves embedding organization in both regression and classification tasks. In the ESOL dataset containing aqueous solubility data, embeddings initially appear scattered but later align into a gradient reflecting solubility values, indicating improved molecular representation. In contrast, the BBBP dataset, with binary blood-brain barrier permeability labels, initially shows overlapping class embeddings. After fine-tuning, clusters corresponding to labels "0" and "1" emerge, with minor overlap at decision boundaries reflecting the complexity of this classification task. Future work will explore contrastive learning to refine clustering and feature separability further.


\section{Chemical Interpretability of FragmentNet through Attention Maps}
\label{Appendix: attention maps discussion}

We analyze attention maps of two molecules across datasets to showcase the model’s ability to identify chemically meaningful fragments linked to solubility, permeability, and bioactivity. This approach aligns closely with traditional chemical reasoning, improving interpretability for solubility, permeability, and bioactivity predictions.

\subsection{N-methyl-2-amino-3-phenylpropane (BBBP Dataset)}

\begin{figure}[H]
\vskip 0.2in
\begin{center}
    \centerline{\includegraphics[width=0.6\linewidth]{icml2025/images/bbbp_molecule_attention.png}}
    \caption{Attention Map for N-methyl-2-amino-3-phenylpropane (task: brain blood barrier prediction)}
    \label{fig:attention map for N-methyl-2-amino-3-phenylpropane appendix}
\end{center}
\vskip -0.2in

\end{figure}

The compound \textit{N-methyl-2-amino-3-phenylpropane} (\texttt{CC(CC1=CC=CC=C1)N}) from the BBBP dataset shows high blood-brain barrier permeability, and its attention map highlights structural features known to influence BBB transport. The amine group (\(-\text{NH}_2\)) receives strong attention, consistent with its role in hydrogen bonding and potential interactions with transport proteins, while attention to the benzene ring aligns with its contribution to membrane permeability through lipophilic interactions \cite{cornelissen2023explaining}.

\subsection{CC(C)CN(CC1=CC=CC=C1)C2=NC3=CC=CC=C3N=C2 (Malaria Dataset)}

\begin{figure}[H]
\vskip 0.2in
\begin{center}
    \centerline{\includegraphics[width=0.6\linewidth]{icml2025/images/malaria_molecule_attention.png}}
    \caption{Attention Map for CC(C)CN(CC1=CC=CC=C1)C2=NC3=CC=CC=C3N=C2 (task: malaria)}
    \label{fig:attention map for CC(C)CN(CC1=CC=CC=C1)C2=NC3=CC=CC=C3N=C2 appendix}
\end{center}
\vskip -0.2in

\end{figure}

For \texttt{CC(C)CN(CC1=CC=CC=C1)C2=NC3=CC=CC=C3N=C2}, the attention map highlights the quinazoline core and benzylamine moiety as key contributors to bioactivity against malaria-related targets. The quinazoline core, a known pharmacophore, facilitates $\pi$-$\pi$ stacking and hydrogen bonding with targets such as dihydrofolate reductase (DHFR), crucial for antimalarial activity \cite{nzila2010preclinical}. The benzyl group enhances binding through hydrophobic interactions, while its flexible linker allows optimal positioning within the binding pocket.

\section{Model and Training Configuration}
\label{app:model_training_configuration}

\subsection{Pre-training Model Configuration}

\begin{table*}[ht]
\centering
\caption{Model and Pre-training Configuration}
\begin{minipage}{0.48\linewidth}
    \centering
    \subcaption{Key Configuration Parameters for FragmentNet}
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Component} & \textbf{Parameter} & \textbf{Value} \\ \midrule
        \multirow{4}{*}{GCN}  & Node Features  & 133  \\
                              & Feature Dim    & 64   \\
                              & Layers        & 2    \\
                              & Dropout       & 0.2  \\ \midrule
        \multirow{4}{*}{VQVAE} & Codebook Dim  & 133  \\
                               & Dropout       & 0.15 \\
                               & Num Codebooks & 4    \\
                               & Codebook Sizes & [64, 64, 64, 64] \\ \midrule
        Sequence Encoder & Dimension & 256 \\
                         & Layers    & 8   \\
                         & Heads     & 8   \\ \midrule
    \end{tabular}
    \label{tab:magbert_config}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \subcaption{Pre-training Hyperparameters}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\ \midrule
        Training Set Size      & 2M molecules \\
        Batch Size             & 128 \\
        Optimizer              & Adam \\
        \quad Learning Rate (\texttt{lr}) & \{0.0001, 0.0002, 0.0008\} \\
        \quad Betas (\texttt{betas})       & (0.9, 0.999) \\
        Weight Decay           & 0 \\
        Loss Function          & CrossEntropyLoss \\
        \quad Reduction        & Mean \\ \bottomrule
    \end{tabular}
    \label{tab:hyperparameters_pre-training}
\end{minipage}
\end{table*}

The \textbf{GCN} module captures molecular fragment structure, leveraging two layers with a hidden size of 64 and a dropout rate of 0.2. The \textbf{VQVAE} employs four codebooks of size 64 for vector quantization. The \textbf{Sequence Encoder} and \textbf{BERT} layers share a hidden size of 256 with eight encoder blocks and eight attention heads to ensure effective representation learning. 

The model was pre-trained on a dataset of 2 million molecules using the Adam optimizer with a varying learning rate schedule. The CrossEntropyLoss function guided optimization, with the loss averaged across the elements.

\subsubsection{Fine-tuning Model Configuration}

For fine-tuning, the Adam optimizer maintained a learning rate of 0.0001 with betas set to (0.8, 0.99) and an epsilon of \(1 \times 10^{-8}\) to enhance numerical stability. A batch size of 32 was used, and training was limited to 100 epochs, with early stopping triggered at 40 epochs to mitigate overfitting. A dropout rate of 0.6 and max pooling ensured robust feature selection.

A grid search was conducted over key hyperparameters (\cref{tab:grid_search_hyperparameters}), varying learning rates, batch sizes, hidden dimensions, layers, dropout rates, and early stopping criteria to refine performance further. The best combination was selected based on validation performance across property prediction datasets.

\begin{table*}[hbt!]
\centering
\caption{Fine-tuning Hyperparameters and Grid Search Configurations}
\begin{minipage}{0.48\linewidth}
    \centering
    \subcaption{Fine-tuning Hyperparameters}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\ \midrule
        Training Set Size        & Varies by dataset \\
        Batch Size               & 32 \\
        Optimizer                & Adam \\
        \quad Learning Rate (\texttt{lr}) & 0.0001 \\
        \quad Betas (\texttt{betas})       & (0.8, 0.99) \\
        \quad Epsilon (\texttt{eps})       & $1 \times 10^{-8}$ \\
        Number of Epochs         & 100 \\
        Early Stopping Patience  & 40 epochs \\
        Dropout Rate             & 0.6 \\
        Pooling Method           & Max, Mean, Att, CLS (None) \\ \bottomrule
    \end{tabular}
    \label{tab:hyperparameters_fine_tuning}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \subcaption{Hyperparameter Grid Search Configurations}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values Explored} \\ \midrule
        Learning Rate (\texttt{lr})    & \{0.0001, 0.00008, 0.0002\} \\
        Batch Size                     & \{16, 32, 64\} \\
        Hidden Dimension               & \{64, 128, 256\} \\
        Number of Layers (\texttt{num\_layers}) & \{1, 2, 4\} \\
        Dropout Rate                   & \{0.4, 0.6\} \\
        Early Stopping Patience        & \{10, 40, 60\} \\ \bottomrule
    \end{tabular}
    \label{tab:grid_search_hyperparameters}
\end{minipage}
\end{table*}

\FloatBarrier

% \todo{bold the final parameters chosen}

\subsection{Training Compute}
Our compute setup highlights the feasibility of training small foundation models on consumer-grade MacBook laptops equipped with MPS (Metal Performance Shaders). Using a MacBook Pro M2 Max with 32GB of RAM, we trained models on datasets exceeding two million entries. This demonstrates that resource-constrained environments can still yield scalable results with proper optimization. Key to our approach was designing a fragment encoder to split large graphs into smaller disconnected subgraphs. These subgraphs were tokenized, compressed, and embedded into dense representations, avoiding the need to perform convolutions on the entire graph. While this improved performance, graph operations ran faster on the CPU than on the GPU, as MPS offered limited support for graph-based frameworks like Torch Geometric. In contrast, sequence-based tasks benefited significantly from GPU acceleration, particularly for large batch sizes and deep models.

Prudent memory management and data movement were critical to our training. Data pre-fetching enabled data to be stored in the cache, preventing multiple memory reads. Additionally, data transfers between devices were limited to a single transfer per forward pass, resulting in a 30-40\% reduction in training time. To address inefficiencies in PyTorch’s multiprocessing on macOS—such as file serialization errors caused by long file paths—we adopted the spawn method over the fork and reduced the size of shared objects. Process-to-thread mapping was also optimized, with threading used for lightweight tasks to avoid the overhead of multiprocessing.

While data was stored locally for this project, scaling to larger datasets would benefit from cloud storage and sharding to improve data handling. Ultimately, our setup enabled us to complete an epoch in under 11 hours with consistent results, validating that MPS-enabled MacBook laptops are a viable option for training small foundation models when coupled with thoughtful hardware-aware optimizations.

% \section{Pre-training Curves and Scalability}
% \label{app: pre-training curves and scalability}

\subsection{FragmentNet Pre-training Curves}
% \label{app: pre-training curves}
% \begin{figure}[H]
% \vskip 0.2in
% \begin{center}
%     \includegraphics[width=0.48\textwidth]{epoch_1_train_batch_loss.png}
%     % \includegraphics[width=0.48\textwidth]{epoch_1_train_batch_accuracy.png}
%     \caption{\texttt{num\_iters=100} Epoch 1 pre-training batch loss} % (left) and batch accuracy (right).}
%     \label{fig:pre-training_metrics fragment}
% \end{center}
% \vskip -0.2in

% \end{figure}

% \begin{figure}[H]
% \vskip 0.2in
% \begin{center}
%     \includegraphics[width=0.48\linewidth]{atom_epoch_1_train_batch_loss.png}
%     % \includegraphics[width=0.48\linewidth]{atom_epoch_1_train_batch_accuracy.png}
%     \caption{\texttt{num\_iters=0} Epoch 1 pre-training batch loss}% (left) and batch accuracy (right).}
%     \label{fig:pre-training_metrics atom}
% \end{center}
% \vskip -0.2in
% \end{figure}

\label{app: pre-training curves}
\begin{figure}[H]
\vskip 0.2in
\begin{center}
    \includegraphics[width=0.48\linewidth]{icml2025/images/atom_epoch_1_train_batch_accuracy.png}
    \includegraphics[width=0.48\textwidth]{icml2025/images/epoch_1_train_batch_accuracy.png}
    \caption{FragmentNet pre-training epoch 1 batch accuracy. Left: \texttt{num\_iters = 0}, Right: \texttt{num\_iters = 100}.}
    \label{fig:pre-training_metrics accuracy}
    \end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
    \includegraphics[width=0.48\linewidth]{icml2025/images/atom_epoch_1_train_batch_loss.png}
    \includegraphics[width=0.48\textwidth]{icml2025/images/epoch_1_train_batch_loss.png}
    \caption{FragmentNet pre-training epoch 1 batch loss. Left: \texttt{num\_iters = 0}, Right: \texttt{num\_iters = 100}.}
    \label{fig:pre-training_metrics loss}
    \end{center}
\vskip -0.2in
\end{figure}

\subsection{Model Size Comparison}
\begin{table*}[ht]
\centering
\caption{Model Size Comparison Against Benchmarks\tnote{a}}
\begin{tablenotes}
\footnotesize
\item[a] M = Million ($10^6$), B = Billion ($10^9$).
[1] \cite{rong2020self}; [2] \cite{hu2019strategies}; [3] \cite{hou2022graphmae}; [4] \cite{xia2023molebert}; [5] \cite{liu2024rethinking}; [6] \cite{feng2024unicorn}; [7] \cite{park2024moltres}; [8] \cite{ross2022large}.  
\end{tablenotes}
\label{tab:model_size_comparison}
\vskip 0.15in
\small
\begin{threeparttable}
\begin{tabularx}{\textwidth}{@{}X X X@{}} % X auto-adjusts
\toprule
\textbf{Model} & \textbf{Number of Parameters} & \textbf{Dataset Size} \\
\midrule
\multicolumn{3}{c}{\textbf{Small Molecule Models}} \\
\midrule
AttrMask\textsuperscript{2}        & 1.86M  & 2M   \\
Mole-BERT\textsuperscript{4}       & 2.01M  & 2M   \\
SimSGT\textsuperscript{5}          & 11.6M  & 2M   \\
GraphMAE\textsuperscript{3}        & -      & -    \\
UniCorn\textsuperscript{6}         & -      & 15M  \\
\midrule
\multicolumn{3}{c}{\textbf{FragmentNet Variants (15-17M Params, 2M Molecules)}} \\
\midrule
FN-Atom             & 15.2M  & 2M  \\
FN-Fragment         & 17.4M    & 2M  \\
\midrule
\multicolumn{3}{c}{\textbf{Medium+ Molecule Models (100M+ Params or Trained on 1B+ Molecules)}} \\
\midrule
GROVER\textsuperscript{1}          & 100M   & 10M   \\
MolFormer-XL\textsuperscript{8}     & 46.8M  & 1.10B \\
MolTRES\textsuperscript{7}         & -      & 2.00B \\
\bottomrule
\end{tabularx}
\end{threeparttable}
\vskip -0.1in
\end{table*}


% Despite training on a modest dataset of 2 million molecules, FragmentNet demonstrates strong convergence and competitive downstream performance comparable to state-of-the-art models trained on significantly larger datasets. As illustrated in Figure \ref{fig:pre-training_metrics}, FragmentNet's training loss decreases consistently throughout the first epoch, indicating stable optimization and effective learning.

% A key observation from our pre-training curves is the steady, near-linear growth in masked fragment prediction accuracy, increasing to 26.4\% in the first epoch. Unlike traditional Transformer-based models, which exhibit rapid improvement followed by early plateauing and achieve most of their learning in the first epoch itself, FragmentNet continues to enhance its performance with additional data in the first epoch, indicating a strong potential for improving performance by using more parameters and data. Given our compute restrictions, we train on the 2 million molecules for 8 epochs, achieving 62\% reconstruction accuracy, again indicating that there is far more potential for performance improvement given more data. This sustained improvement suggests that our pre-training task is more informative, enabling the model to learn deeper structural relationships rather than relying on superficial statistical patterns.

% For example, while FragmentNet achieves 79\% reconstruction accuracy in the first epoch on a masked atom modeling task, its performance on masked fragment modeling—though initially lower—is superior for downstream tasks. This highlights that the quality of the pre-training task is crucial for downstream performance, outweighing the immediate performance on the pre-training task itself.

% In contrast, SMILES-based Transformers such as MolFormer and MolTRES show rapid convergence and early stagnation, indicating that their pre-training tasks are more simplistic and less effective at capturing underlying chemical structures. FragmentNet's sustained learning trajectory indicates that it actively refines its understanding of molecular structures rather than merely memorizing token co-occurrences.

% The inherent difficulty of our pre-training task results in lower initial accuracy, but the learning curves strongly suggest that with larger datasets and increased model capacity, FragmentNet will continue to scale effectively, in line with established deep learning scaling laws.

% These findings underscore FragmentNet's potential as a scalable and highly expressive framework for molecular representation learning. With additional training data and computational resources, FragmentNet could surpass existing molecular modeling techniques, indicating that we have only begun to explore its full potential.

% \todo{reference figures and make more concise}




\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
