
\newcommand{\nt}[1]{\textcolor{Green4}{#1}} %
\newcommand{\uwt}[1]{\textcolor{DarkOrchid2}{#1}} %

\begin{appendices}

\section{Supplementary Material}

\noindent Code can be found on the project Github repository:

\githubliteral

\noindent Videos and more information can be found on the project website:

{\small\websiteliteral}

\section{Design and Implementation Details}\label{app:implementation}

The typical ranges on friction, mechanical tolerances, damages, etc. for low-cost platforms are far from ideal. However, iterating and developing on individual platforms in unique environments is practically and pedagogically critical, especially for real-world robotics. We hope that this work allows practitioners to focus their efforts on precisely this development cycle rather than infrastructure. The following implementation details should be regarded as a starting point and potential reference for troubleshooting inevitable issues.

We recommend interested readers view the configuration files for each task found in GitHub (\githubliteral) for a detailed overview. Isaac Lab environment configurations primarily include parameter settings and reward functions and can be quickly parsed while efficiently communicating environment behavior. We highlight some notable details in this section.

\subsection{Drifting}\label{app:drift-impl}

We found that drifting was an exemplary task to demonstrate the physical Sim2Real gap. Much of the development cycle was focused on closing this gap, as supported by countless other seminal works~\cite{openai_solving_2019, hoeller_anymal_2024}. 

\subsubsection{Actuators} In many related works for agile tasks, data is typically collected to assess actuator response and estimate gains. However, this process can be unfamiliar to beginning roboticists or non-traditional enthusiasts. \textit{Instead, all actuator parameters are extracted exactly from their technical specification sheet online, and gains are domain-randomized across environments.} Achieving the task in this manner helps to convey that, while domain expertise can certainly improve performance (domain randomized policies are known to find conservative solutions~\cite{liao_berkeley_2024}), it is not strictly necessary through modern methods.

It is worth noting that a few ``Sim2Real2Sim'' (or Sim2Sim) cycles were spent narrowing the randomized gain ranges for the throttle actuator settings. Due to neural network policy tendencies to exhibit ``bang-bang'' (on-off) behavior, gain values can become extremely important to the actual forces exerted on the vehicle.

Establishing and improving pipelines for this process can be an accessible yet effective future contribution for use by potential practitioners.

\subsubsection{Friction}

Initially, the domain randomization over friction was used to cover a wide range (0.2 to 0.8) of potential friction values between the tape-covered tires and the carpeted ground. However, resulting policies would perform poorly and inconsistently on deployment. A long thread of Sim2Sim cycles was used to search for friction parameters but still resulting policies were not reliably transferring.

In the end, a cheap (less than 5 USD) spring scale was used to estimate the coefficient of friction, measured by dragging the vehicle with the scale along the carpet. This value (about 0.4) was then used as the midpoint of the friction randomization during training.

The project \website~also contains videos of deployments on different surfaces (e.g. finished concrete). Lower friction values appear to cause the vehicle to take wider turns (due to lack of turning friction) and spin out quickly during drift maneuvers. Enabling the platform to drift over multiple surfaces is also of fundamental interest to the robotics community as it is a form of domain adaptation.

\subsubsection{Articulations}

While we initially used the MuSHR~\cite{srinivasa_mushr_2023} articulations defined by the open-source URDF file, we found that its modeling inaccuracies affected the precise control expectations of the platform.
As a result, we improved the articulations: (1) by moving the steering linkages from the wheel axis to the actual steering joint and (2) by articulating the suspension joints of the vehicle.

Steering inaccuracies were noticeable due to the gap between turning radii in real and sim. The moment arm on the linkage can also adversely affect the tracking of the steering joint during high-velocity maneuvers. This can even be observed in simulation when steering joints behave strangely asymmetrically with unsuitable actuator settings.

Suspension helps close the gap on load shifting during turns. With a suspension travel of 4 cm, these shifts have consequences in the total friction and ``thrust'' produced while throttling through turns. Interestingly, suspension articulation also helped to stabilize the simulation. At a wheel speed of 3 m/s, the rotation rate of the wheels reaches 60 rad/s, occasionally resulting in collision penetrations during simulation which cause unrealistic ``jumping'' through drifted turns. Damped suspension joints helped to stabilize these collisions without requiring smaller simulation time steps which would significantly affect training times.

\subsubsection{Rewards}

Six rewards were designed for this task:

\begin{enumerate}
    \item[i.)] \textbf{Cross-Track Distance}. The lateral distance of the vehicle from an oval track line. This is defined without time-dependent reference trajectories as often done in model-based approaches or even in deep RL methods~\cite{cai_high-speed_2020}. This is highly weighted otherwise the agent tends to take larger, safer turns.
    \item[ii.)] \textbf{Velocity}. Reward for high speeds.
    \item[iii.)] \textbf{Side-Slip}. Only stable side-slip angles rewarded. Stable side-slip angles are manageable through the steering limits (0-30$^\circ$).
    \item[iv.)] \textbf{Progress}. Reward for making progress along an arbitrary direction (counter-clockwise, in this work).
    \item[v.)] \textbf{Turn-Energy}. Shaping term for increasing velocity rewards specifically when inside turn regions.
    \item[vi.)] \textbf{Turn-Left-Go-Right}. Shaping term for encouraging exploration to discover an alternate turning mode (e.g. drifting) by counter-steering. Strictly positive when angular velocity and steering command are opposite to each other. 0 otherwise.
\end{enumerate}

Penalties were also given for going out-of-bounds.

\subsection{Elevation}

\subsubsection{Training}

Training elevation in simulation proved to be a challenging task. Training would often collapse into one of: (1) seeking goals while avoiding obstacles or (2) climbing ramps. Striking a balance between the two often required re-designing the scene or task configuration in simulation.

As observed for model-based methods~\cite{han_dynamics_2024}, evaluating uneven terrain traversal for vehicles can be challenging due to the reasonable alternate mode of avoiding risky terrain altogether, which may often be faster. However, the primary goal of this work is to better enable the broader community to engage with state-of-the-art methods in elevation-based robotics. Thus, the reward and scene were constructed to incentivize the agent to actively seek out risky terrain while penalizing failures.

Supported by machine learning literature, noise injection becomes less effective for generalization at higher dimensions. For states such as elevation maps, perceived noise is likely more structured than the additive Gaussians injected through observation corruption. While debugging, we found that the policy would improve if the elevation features were brought closer in shape to the features seen in the simulation. To resolve this without ``training on test'', the scene was augmented with slopes of various gradations and heights.

\subsubsection{Suspension}

As mentioned in Appendix \ref{app:drift-impl}, suspension joints were added to the open-source MuSHR articulations. This is critical for elevation where maintaining momentum up a ramp helps training stability. Without suspension articulation, climbing ramps have an abrupt and destabilizing impact at higher speeds.

\subsubsection{Rewards}
\begin{enumerate}
    \item[i.)] \textbf{Goal Velocity}. Rewards velocity projected onto goal heading vector.
    \item[ii.)] \textbf{Z-Position}. Rewards larger z-positions (gained through ascending ramps).
    \item[iii.)] \textbf{Falling Penalty}. Penalizes negative body-frame z velocities.
    \item[iv.)] \textbf{Roll on elevation}. Penalizes roll angles while on elevation features.
\end{enumerate}

\subsection{Visual}\label{app:vis}
While visual information is essential in modern robotics to solve complex real-world problems, simulating such information requires state-of-the-art rendering techniques to close the Sim2Real gap.
Despite such computationally heavy efforts, machine learning models trained with simulated data often fail at real-world generalization due to diverse types of environmental changes, e.g., illuminations, weather, etc.
Instead, in this work, we ``simplify'' visual information from both simulation and real by compressing RGB pixels to grayscale, closing the Sim2Real gap.
Still, we observed that such compressed visual information exhibits a non-trivial amount of Sim2Real gap.
This section describes how we tackle this problem by providing details of our environment, training augmentations, architectures, and reward designs.

\subsubsection{Visual map generation}
We designed the problem by encoding traversability into color information.
Black tiles are not-traversable regions while white indicates safe to travel.
We divided the whole map into multiple sub-environments that are individually configurable. 
Then, we randomized white paths over the black area in each sub-environment.
Specifically, each sub-environment comprises 9 cells, and we sample a start point in each cell and generate paths toward random endpoints in the sub-environment.
We made the number of random paths from a single start point configurable, allowing the users to adjust the difficulty of the training (see Fig.~\ref{fig:visual_policy_maps}).
This can later be used for curriculum learning by assigning different difficulties for sub-environments.
We tested the generated map with more than 2,000 agents in parallel running seamlessly with a single GPU.
The algorithm overview for sub-environment generation is presented in Alg.~\ref{alg:visual_map_generation}.
\begin{algorithm}[!ht]
\centering
\footnotesize
\caption{Visual Policy Sub-environment Generation} 
\begin{algorithmic}[1]
\State \textbf{Input:}
\State $(H_\text{env}, W_\text{env}) \gets$ \# of rows and \# of columns for each sub environment
\State $(H_\text{cell}, W_\text{cell}) \gets$ \# of rows and \# of columns for each cell
\State $N_\text{walkers} \gets$ \# of walkers for path generation 
\State
\State \textbf{Pseudo-code:}
\LeftComment{Initialize traversability map; 0 - not traversable / 1 - traversable}
\State $\mathbf{T}\in\{0, 1\}^{H_\text{env}\times W_\text{env}} \gets \mathrm{\mathbf{0}}$ 
\State \LeftComment{Sample start points from each cell}
\State $\mathbf{P}^\text{start} \gets [\,]$ \Comment{Initialize an array to save start points}
\LeftComment{Sample random start points for each cell}
\For{row $r=0, 1, \ldots, H_\text{env} / H_\text{cell}$}
  \For{col $c=0, 1, \ldots, W_\text{env} / W_\text{cell}$}
    \State $p_\text{row} \gets \text{UniformSampling}(0, H_\text{cell}) + r \cdot H_\text{cell}$
    \State $p_\text{col} \gets \text{UniformSampling}(0, W_\text{cell}) + c \cdot W_\text{cell}$
    \State $\mathbf{P}^\text{start}\, \mathrel{+}= [(p_\text{row}, p_\text{col})]$ \Comment{Append the start point to $\mathbf{P}^\text{start}$}
  \EndFor
\EndFor
\Statex

\For{$(p_\text{row}, p_\text{col})$ in $\mathbf{P}^\text{start}$}
  \For{iter$=0, 1, \ldots, N_\text{walkers}$}
    \Statex \ \ \ \ \ \ \ \ \ \(\triangleright\) Sample random end points
    \Do
      \State \ \ \ $q_\text{row} \gets \text{UniformSampling}(0, H_\text{env})$
      \State \ \ \ $q_\text{col} \gets \text{UniformSampling}(0, W_\text{env})$
    \DoWhile{$\mathbf{T}(q_\text{row}, q_\text{col}) == 1$}
    \Statex \ \ \ \ \ \ \ \ \ \(\triangleright\) Generate random paths and update traversability map
    \State GenerateRandomPaths$(\mathbf{T},\, (p_\text{row}, p_\text{col}),\, (q_\text{row}, q_\text{col}))$
  \EndFor
\EndFor
\State \textbf{Return:} $\mathbf{T}$
\end{algorithmic} 
\label{alg:visual_map_generation}
\end{algorithm}

\subsubsection{Model Architectures}
We adopt multi-layer perceptron (MLP) as our basic policy network.
For ablations, we add a simple 3-layer convolutional neural network (CNN) that serves as an image feature extractor before the policy networks.
The extracted features are concatenated to the rest of the observation terms such as linear/angular velocities and last action and fed into the MLP policy networks.
We observe that inducing CNN takes longer training time to learn optimal actions and often collapses into a trivial solution - agents decide to spin in a circle, only maximizing the velocity rewards.

\subsubsection{Image Augmentations}
To close the sim-to-real gap, we apply image augmentations for policy training. This includes aggressive color jittering (\textit{i.e.,} brightness, contrast, saturation, and hue adjustments) and Gaussian blur.
As demonstrated in both qualitative and quantitative evaluations, image augmentations play a crucial role in closing sim-to-real gaps.
Further augmentations or randomization can be easily applied to our code design.



\subsubsection{Rewards}
Two rewards were designed for this task
\begin{enumerate}
    \item[i.)] \textbf{Velocity Reward}. Rewards higher velocities
    \item[ii.)] \textbf{Traversability Reward and Penalty}. Rewards being on white regions, and penalizes it for being on black regions
\end{enumerate}

\subsubsection{Terminal Conditions}
We experimented with different terminations. In addition to the standard time-out and out-of-bounds terminal conditions, we added a terminal condition when the robot is on black. However, this often resulted in a lack of exploration, and policies that produced ``jerky'' and undesirable actions, such as idling at the initial location.



\end{appendices}




