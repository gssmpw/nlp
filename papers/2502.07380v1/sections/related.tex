
Considerable prior work in Sim2Real has addressed open-source wheeled robots, including~\cite{datar_toward_2024, salimpour_sim--real_2025, balaji_deepracer_2020, samak_towards_2023, xiao_anycar_2024, cutler_autonomous_2016, williams_information_2017, evans_comparing_2023}. Some of these works produce one of $\pdrift$, $\pelev$, or $\pvis$ with partial or full Sim2Real. While the individual accomplishments of these works motivate our choice of demonstrations, they also help us highlight that \textit{varied state-of-the-art approaches can be coherently integrated under the framework presented here}.

\textbf{Drifting} \textbf{Policy}. Drifting is a challenging task in both optimal control and reinforcement learning (RL)~\cite{gonzales_autonomous_2016, cai_high-speed_2020, djeumou_reference-free_2024}. It is a fundamentally unstable maneuver whose control solutions are traditionally determined by vehicle and ground parameters~\cite{chen_dynamic_2023}. In many cases (ours included), researchers are unable to demonstrate a controlled drift for imitation-based methods~\cite{cutler_autonomous_2016}. 

These challenges make drifting an appealing task for RL. RL-based approaches have thus far been achievable only through further real-world fine-tuning~\cite{cutler_autonomous_2016, williams_information_2017}.
\textit{In this work, we show that aggressive domain randomization, perturbation simulation, and massive parallelization can enable direct transfer of a drifting policy.}
To reduce the barrier-to-entry, we deliberately distance our methods from techniques such as gain tuning and extensive system-identification that require additional domain expertise and tooling.

\textbf{Elevation Policy.} It has become standard practice for locomotion policies on outdoor terrain to be trained with elevation maps~\cite{hoeller_anymal_2024}. 
In parallel, autonomous off-road vehicles have also demonstrated the importance of incorporating elevation in model-based methods~\cite{han_dynamics_2024, frey_roadrunner_2024, gibson_multi-step_2023}. Generally, these elevation-based tasks address challenges that arise at the interface of agent morphology and perception, which are often difficult to model. Therefore, 
lower-cost wheeled platforms have traditionally been oriented toward flat terrain~\cite{okelly_f1tenth_2020, balaji_deepracer_2020}, revealing a gap in methodology between the broader community and the state-of-the-art. Notable exceptions  
include~\cite{datar_toward_2024, stachowicz_fastrlap_2023, xu_reinforcement_2024}. However, \textit{no work before this integrates these platforms with RL-facing simulators.}

\textbf{Visual Policy.} Improvements in camera and photorealistic scene simulation have advanced policies capable of semantic reasoning, bridging the gap between pre-trained visual models and embodied actions~\cite{kang_generalization_2019, yuan_learning_2024}.
Demonstrations of small-scale platforms with pre-trained visual models have also been investigated~\cite{stachowicz_fastrlap_2023}. In general, cameras provide an approachable and interpretable sensor modality for modern robotics education and research. Unfortunately, similar to elevation policies, visual policy pipelines on low-cost mobile platforms are not readily available even though the success of proprietary camera-based Sim2Real platforms suggests a clear demand~\cite{balaji_deepracer_2020}. \textit{This work demonstrates simulation, end-to-end training, and deployment with cameras.}

\textbf{Isaac Lab Simulation Framework.} This work builds on Isaac Lab, an open-source, widely adopted, and rapidly growing simulation framework for robotics research~\cite{mittal_orbit_2023, liao_berkeley_2024} that is heavily supported by industry and academia alike~\cite{technologies_1x_nodate, noauthor_crossing_nodate, noauthor_get_nodate, noauthor_nvidia_nodate, noauthor_menteebot_nodate, liao_berkeley_2024}. Isaac Lab has become a primary learning framework of choice for recent robotics research due to its impressive performance and ease-of-use features~\cite{ma_eureka_2024, hoeller_anymal_2024, tao_maniskill3_2024, yang_agile_2024}.
