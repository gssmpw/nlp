\documentclass[conference]{IEEEtran}
\usepackage{times}
\input{preamble}

\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{internationalorange}{rgb}{1.0, 0.31, 0.0}

\newcommand{\add}[1]{{\leavevmode \color{red}#1}}
\newcommand{\tyler}[1]{{\leavevmode \color{blue} Tyler: #1}}
\newcommand{\SidT}[1]{{\leavevmode \color{green} SidT: #1}}
\newcommand{\Preet}[1]{{\leavevmode \color{cyan} Preet: #1}}
\newcommand{\rms}[1]{{\leavevmode \color{internationalorange} RMS: #1}}
\input{defs}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

\title{Demonstrating Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics}

\author{Author Names Omitted for Anonymous Review. Paper-ID 596}


\author{\authorblockN{Tyler Han,
Preet Shah,
Sidharth Rajagopal, 
Yanda Bao,
Sanghun Jung,
Sidharth Talia,\\
Gabriel Guo,
Bryan Xu,
Bhaumik Mehta,
Emma Romig,
Rosario Scalise,
Byron Boots\\
University of Washington,
Seattle, Washington 98104, USA\\ \\ Email:%
\ttfamily \{than123, psshah10, sidhraja, yandabao, shjung13, sidtalia,\\ gabeguo1, bryanx, bm87, emmar, rosario,  bboots\}@uw.edu}%
\\
}
\makeatletter
\let\@oldmaketitle\@maketitle%
\renewcommand{\@maketitle}{\@oldmaketitle%
    \centering
    \includegraphics[width=.9\linewidth]{figures/fig1.pdf}
    \captionof{figure}{Wheeled Lab bridges popular low-cost open-source wheeled platforms~\cite{talia_demonstrating_2024, srinivasa_mushr_2023, okelly_f1tenth_2020} with the research-backed robotics ecosystem Isaac Lab~\cite{mittal_orbit_2023}. This integration helps introduce modern Sim2Real methods to more accessible platforms for a broader audience. Three complete Sim2Real pipelines for state-of-the-art policies are developed in this work to demonstrate advantages of modern methods and help kickstart further education and research.}
    \label{fig:fig1}
}%
\makeatother

\maketitle
\clearpage
\newpage


\begin{abstract}
Simulation has been pivotal in recent robotics milestones and is poised to play a prominent role in the field's future.
However, recent robotic advances often rely on expensive and high-maintenance platforms, limiting access to broader robotics audiences. This work introduces Wheeled Lab, a framework for the low-cost, open-source wheeled platforms that are already widely established in education and research. Through integration with Isaac Lab, Wheeled Lab introduces 
modern techniques in Sim2Real,  such as domain randomization, sensor simulation, and end-to-end learning, to new user communities. To kickstart education and demonstrate the framework's capabilities, we develop three state-of-the-art policies for small-scale RC cars: controlled drifting, elevation traversal, and visual navigation, each trained in simulation and deployed in the 
real world. By bridging the gap between advanced Sim2Real methods and affordable, available robotics, Wheeled Lab aims to democratize access to cutting-edge tools, fostering innovation and education in a broader robotics context. The full stack, from hardware to software, is low cost and open-source.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}\label{sec:intro}

\input{sections/intro} %

\section{Intended Demonstration}

We will run the policies on three separate platforms in front of the live audience. Where necessary, our evaluations of each policy use simple household objects and can also be brought to the demonstration. Running onboard and in real-time, each task and policy can also demonstrate their robustness and recovery behaviors. We will invite the audience to (safely) perturb the scene and robot during the demonstration. In particular, the drifting task can be deceptively challenging. The live audience will be invited to attempt the task themselves to experience the task's complexity.

Importantly, these platforms are still entirely low-cost and open-source. We will invite the audience to experience the simple construction and design to help convey its low maintenance costs for use in education and research.

If time permits, the audience may also train their own models using the ecosystem and deploy them on our platforms.

\section{Related Work}\label{sec:related}

\begin{table*}[t]
\renewcommand{\arraystretch}{1.5}
\footnotesize
\scalebox{.93}{
\begin{tabular}{lccccccccc}
\toprule
 & \multicolumn{3}{c}{\textbf{Sensor Simulation}} & \multicolumn{2}{c}{\textbf{Agent Physics}} & \multicolumn{4}{c}{\textbf{Reinforcement Learning}} \\
 \cmidrule(r){2-4} \cmidrule(r){5-6} \cmidrule(r){7-10}
 Ecosystem & \multicolumn{1}{c}{Elevation} & \multicolumn{1}{c}{LiDAR} & \multicolumn{1}{c}{Camera} & \multicolumn{1}{c}{Dynamics} & \multicolumn{1}{c}{Parallelization} & \multicolumn{1}{c}{Perturbation} & \multicolumn{1}{c}{Corruption} & \multicolumn{1}{c}{Community} & \multicolumn{1}{c}{Platforms} \\
 \midrule
Wheeled Lab & \cmark & 3D & Depth, RGB & \makecell{16 DOF \\+ Collisions} & High & \cmark & \cmark & RSL, SB3 & \makecell{HOUND, MuSHR,\\ F1Tenth} \\
AutoDrive~\cite{samak_autodrive_2023} & \xmark & 2D & \xmark & 16 DOF & Low & \xmark & \xmark & Gym & F1Tenth, Nigel \\
F1Tenth~\cite{okelly_f1tenth_2020}& \xmark & 2D & \xmark & Kinematic & \xmark & \xmark & \xmark & Gym & F1Tenth \\
CARLA~\cite{dosovitskiy_carla_2017}& \xmark & 3D & Depth, RGB & \makecell{16 DOF \\+ Collisions} & \xmark & \makecell{Steering\\Only} & \cmark & SB3 & \xmark \\
\shortstack[l]{Brunnbauer \textit{et al.}~\cite{brunnbauer_latent_2022}\\(PyBullet)} & \xmark & 2D & RGB & 12 DOF & \xmark & \xmark & \xmark & \xmark & F1Tenth \\
\shortstack[l]{Hamilton \textit{et al.}~\cite{hamilton_zero-shot_2022}\\(Gazebo)} & \xmark & 2D & \xmark & Kinematic &  \xmark & \xmark & \xmark & \xmark & F1Tenth \\
\shortstack[l]{\textit{PIETRA}~\cite{cai_pietra_2025}\\(Chrono)} & \cmark & \xmark & \xmark & 12 DOF & \xmark & \xmark & \xmark & \xmark & F1Tenth \\
RoboTHOR~\cite{deitke_robothor_2020} & \xmark & \xmark & RGB & Kinematic & \xmark & \xmark & \cmark & AllenAct & Household \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Comparisons of existing ecosystems on their capabilities.}  Various simulation, learning, and deployment ecosystems have been integrated with accessible wheeled platforms for Sim2Real. However, these ecosystems are noticeably isolated from the research community and, where applicable, have outdated features.
}
\label{tab:integrations}
\end{table*}

\input{sections/related} %

\begin{figure*}[t]
    \centering
    \includegraphics[width=.8\linewidth]{figures/diagrams/puzzle.pdf}
    \caption{\textbf{Modular training framework imagined as the assembly of a puzzle.} Three main components make up training: \texttt{Run}, \texttt{Agent}, \texttt{Environment}. Outlined puzzle pieces (e.g. \texttt{Observation}, \texttt{Reward}) are sub-components whose behaviors are defined by the pieces below them. For instance, \texttt{High Speed} is a reward setting in all of our RL environments, as denoted by the star (\includegraphics[height=\fontcharht\font`B]{figures/misc/star.png}) icon.
    Wheel (\includegraphics[height=\fontcharht\font`B]{figures/misc/wheel.png}), camera (\includegraphics[height=\fontcharht\font`B]{figures/misc/cam.png}), and mountain (\includegraphics[height=\fontcharht\font`B]{figures/misc/mtn.png}) icons indicate specific design choices for $\pdrift$, $\pvis$, and $\pelev$, respectively. Components shown are non-exhaustive.}
    \label{fig:overview}
\vspace{-10pt}
\end{figure*}


\section{Wheeled Lab}\label{sec:wheeled-lab}
\input{sections/wheeled-lab} 


\section{Demonstrations}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/media/houndsv3_top.jpg}
    \caption{Family of HOUND platforms used for a robotics class at the University of Washington.}
    \label{fig:hound-family}
\end{figure}

For the demonstrations in this work, we use the HOUND~\cite{talia_demonstrating_2024} for $\pdrift$ and the MuSHR\footnote{This work's MuSHR uses the same casing as the HOUND but is otherwise a MuSHR in terms of mechanics and electronics.}~\cite{srinivasa_mushr_2023} for $\pelev$ and $\pvis$. The HOUND's estimated total price is about 3000 USD for a complete autonomy package with sensing (i.e., LiDAR, RGB) and compute (i.e., Jetson Orin NX). The MuSHR is estimated at 930 USD.
Within the context of our work, the HOUND platform's hardware is nearly equivalent to that of the F1tenth platform (3800 USD).
A single NVIDIA RTX 3080 GPU is used for training.

To demonstrate the capabilities of the framework, Wheeled Lab implements, deploys, and evaluates three state-of-the-art policy types: $\pdrift$, $\pelev$, and $\pvis$ on the HOUND platform.

In each task, we compare the policy trained with modern Sim2Real techniques (e.g. domain randomization, perturbations, etc) against a baseline policy trained without them to demonstrate their effectiveness in Sim2Real transfer in the presented domains.
Settings for drifting and elevation tasks can be found in \Cref{tab:drift-elev-ablation}.
All other configuration settings not mentioned (e.g. rewards, actuator parameters, articulations) can be assumed to be the same between $\boldsymbol{\pi}$ and $\overline{\boldsymbol{\pi}}$. 
Each baseline is a liberal representation of what training ecosystems currently provide to low-cost wheeled platforms as shown in \Cref{tab:integrations}.
We use $\overline{(\cdot)}$ to denote the baseline version of a policy (e.g. $\bdrift$). 
Across all policies, we use Proximal Policy Optimization (PPO)~\cite{schulman_proximal_2017} as implemented in the RSL RL library to train the agent in simulation~\cite{lee_learning_2020}.


\begin{table}[t]
    \centering
    \begin{tabular}{l | c c c c c}
    \toprule
    Policy & Corruption & Perturbation & DR & \# Envs & Epochs \\
    \midrule
    $\bdrift$ & \cmark & \xmark & \xmark & 64 & 5000 \\
    $\pdrift$  & \cmark & \cmark & \cmark & 1024 & 5000 \\
    \midrule
    $\belev$ & \xmark & \xmark & \xmark & 128 & 1000 \\
    $\pelev$  & \cmark & \cmark & \cmark & 1024 & 1000 \\ 
    \bottomrule
    \end{tabular}
    \caption{\textbf{Training Settings}. Baseline policies $\pdrift$ and $\pelev$ (liberally) reflect capabilities currently available to low-cost open-source wheeled platforms.}
    \label{tab:drift-elev-ablation}
\end{table}

\subsection{Drifting}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/results/drift-vis.png}
    \caption{\textbf{A control strategy arises for drifting through turns.} \textbf{Left column:} (Overlay: colored trajectories over multiple runs.) One run is spotlighted for this visualization. Arrows indicate the direction of travel, and colorbar denotes speeds (in m/s). \textbf{Middle column:} Magnification of drifted turns with markers visualizing vehicle orientation, steering (front wheel direction), and throttle (rear wheels; blue: $-1$, pink: $+1$). \textbf{Right column:} Slip angle, speed, steering, and throttle are plotted for each turn. To initiate the drift, the platform quickly cuts the throttle to destabilize the rear wheels then steers inwards sharply to throw them outwards. With the platform now facing the track center, it throttles through the remainder of the turn while counter-steering to control its residual angular momentum from the initial maneuver. The entire sequence occurs in just over 1 second. Visualization inspired by~\cite{djeumou_reference-free_2024}.}
    \label{fig:drift-vis}
\vspace{-10pt}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/results/baseline-fails.pdf}
    \caption{\textbf{Captured trajectories from the baseline drift policy in real experiments.} Baseline is unable to complete a valid turn without crashing or spinning out.}
    \label{fig:drift-baseline}
\vspace{-10pt}
\end{figure}

\textbf{Background.} Drifting is characterized by a vehicle's side-slip angle, the angle between the heading and the velocity, as it maneuvers through a turn. In high-performance driving, drifting allows the vehicle to take much sharper turns at higher speeds than otherwise allowable by the no-slip turn radius~\cite{acosta_hybrid_2017}. The total mass, center-of-mass, friction properties of the surface and the wheels, actuator properties, state estimation, steering linkages, and suspension are all relevant factors in the narrow dynamics of a controlled drift.

The sensitivity of these dynamics becomes especially obvious when we, the researchers, attempt the task manually despite the deceptively simple action space (see \website~for videos). When the maneuver is not executed correctly, the vehicle spins uncontrollably off track, losing traction and usually requiring a reset (also known as a \textit{spin out}).



\textbf{Task.} For our evaluation, the task involves maintaining speed while minimizing the cross-track distance from an oval racing line without spinning out. 

\textbf{Setup.} State estimation is provided primarily by onboard VIO. A motion capture system is used for evaluation and occasional (1 hz) integrator-drift correction due to space limitations of our indoor testing environment. Successful VIO-only runs not used for data collection can be found on our \website. To enable the platform to be physically capable of drifting, tape is wrapped around the platform's rubber wheels to reduce friction and the base is converted to a rear-wheel drive.

\textbf{Results.} We find that the baseline is unable to make a stable turn, much less complete a full lap (\Cref{fig:drift-baseline}). However, it occasionally attempts to counter-steer despite showing no indication during training. Baseline training runs consistently do not discover the drifting mode. Motor cogging, noisy state estimates, constant slipping, and steering biases prove to cause significant covariate shift.

On the other hand, $\pdrift$ is able to complete full laps. In addition, we observe evidence of high robustness in $\pdrift$ not yet seen in previous literature~\cite{williams_information_2017, djeumou_reference-free_2024}. Namely, when the platform does spin out, instead of attempting to regain control by cutting the throttle, the policy maintains its angular momentum for a full spin (or more) before returning to the track. A visualization of the results gives insight into the policy's precise control in \Cref{fig:drift-vis}. The maximum (controlled) slip angle is $58^\circ$ and the average speed is about $1.6$ m/s.

Further details about the design and implementation of $\pdrift$ can be found in Appendix~\ref{app:drift-impl}. 


\subsection{Elevation}
\begin{figure}[t]
    \centering\includegraphics[width=\linewidth]{figures/results/elev-compare.pdf}
    \caption{\textbf{Captured trajectories of elevation policy in real experiments.} \textbf{Top:} $\pelev$ demonstrates spatial reasoning through ramp traversal and obstacle avoidance in one sequence using only a local elevation map. \textbf{Bottom:}
    Nearing a rollover, the Baseline $\belev$ falls off the ramp and then crashes into it.}
    \label{fig:enter-label}
\end{figure}

\textbf{Background.}
On unstructured and uneven terrain, an elevation map is essential for identifying what areas in the scene are traversable. However, traversability is strongly dependent on agent morphology and dynamic state. Along with the ground geometry, the platform's ground clearance, center-of-mass height, wheel size, maximum torque, suspension, and momentum, all affect how to safely traverse uneven terrain. Poor traversal can result in catastrophic failures such as rollovers and motor (or engine) stalling.

\textbf{Task.}
To show that the trained elevation-based policy is capable of geometric reasoning, we construct a scene with two types of elevation features which contribute to the overall elevation map: walls and ramps. Crucially, the incorporation of ramps sets the task apart from sole obstacle avoidance, since ramps are features with height but are still traversable with the correct approach strategy.
The starting and goal positions are placed on opposite sides of the testing area with the elevation features placed in between. The platform must safely traverse the scene to arrive at the goal position, avoiding obstacles and ascending ramps when locally feasible. 

\textbf{Setup.}
States provided to the policy are goal-relative position, orientation, and a local ($2.5\times2.5$ meters) body-centric elevation map. A motion capture system is used for creating the global map from which we sample local maps. While the platform is capable of creating onboard maps~\cite{talia_demonstrating_2024}, a configurable indoor environment requires higher mapping resolution than available for method evaluation.

\textbf{Results.}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/results/elev-vis.png}
    \caption{\textbf{Three-dimensional view of global elevation map.} $\pelev$ (yellow/blue) and $\belev$ (purple/white) trajectories are visualized over it with elevation values denoted using color. Note that the policies only have access to their local $2.5\times2.5$ m maps during evaluation. }
    \label{fig:elev-vis}
\end{figure}

We find that the baseline primarily deviates from any elevation features to approach the goal. When the baseline does ascend the ramp, it quickly falls off the sides.

However, $\pelev$ is capable of both traversing the ramp safely and navigating through the subsequent obstacles to reach the goal. Similar to $\bdrift$, the lower number of simulated agents gives the baseline policy significantly fewer opportunities to explore correct trajectories in a narrow range of safe states.


\subsection{Visual}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/media/visual/trajectory.png}
    \caption{\textbf{Captured trajectories of visual policy in real experiments}. The trajectories are obtained from the motion capture system.
    Videos of each setting and trial are included in the supplementary materials.}
    \label{fig:visual_policy_trajectory}
\end{figure*}

\begin{table}[t]
    \centering
    \begin{tabular}{c|cc|cc}
        \toprule
        & \multicolumn{2}{c|}{MLP} & \multicolumn{2}{c}{CNN} \\
         & No-aug. & Aug. & No-aug. & Aug. \\
       \midrule
        \# of Success / Trial & 0 / 5 & 3 / 5 & 0 / 5 & 1 / 5 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Visual Policy Results.} This demonstrates the effectiveness of image augmentations for Sim2Real generalization. While we expect CNN to be more capable of learning, MLP presents better generalization capability. Videos of each run are included in the supplementary materials.}
    \label{tab:exp_visual_policy_results}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/media/visual/wheellab_visual_stages.pdf}
    \caption{\textbf{Examples of randomly generated environments to train $\pvis$.} Black and white represent penalizing and safe regions, respectively. We adjust the number of walkers to train the policy across different difficulties. Using a smaller number of walkers will make the training environments more challenging.}
    \label{fig:visual_policy_maps}
\end{figure}


\textbf{Background.} Visual navigation traditionally relies heavily on a semantic understanding of the environment through visual feedback~\cite{jung_v-strong_2024}. Despite its importance, embedding information such as traversability through this modality has its unique challenges~\cite{yasuda_autonomous_2021}. 
First, visual observations (e.g., RGB images) from robots are represented in high-dimensional tensors, requiring exponentially more training data.
Also, small changes in environments coming from illumination, weather, or time of day may result in catastrophic failure of machine learning models~\cite{choi_robustnet_2021}.

These challenges can be effectively addressed through the simulation of visual information.
In addition to generating large amounts of task-relevant data, domain randomization can help improve generalization during transfer.


\textbf{Task.} We construct our task scene using black and white colored foam tiles (see Fig.~\ref{fig:visual_policy_trajectory}). These tiles are inexpensive, safe, and widely available. Surrounded by black tiles, a ``figure-8'' path is laid using white tiles. The platform must remain on the white path while avoiding the surrounding black tiles.
A dynamic version of this task is also used in evaluation; black ``barricade'' tiles are dynamically removed and placed to demonstrate robustness and real-time navigation reasoning.

We generate simulation environments with black and white areas as shown in Fig.~\ref{fig:visual_policy_maps} to train the visual policy.
Each example in the figure represents each sub-environment, and our entire map is composed of sub-environments. This allows us to assign different difficulties for each sub-environment.
More details of map generation can be found in Appendix~\ref{app:vis}.

\textbf{Evaluation.} We measure success or failure over the number of five trials.
We define success if the robot finishes at least one lap without staying in one place for more than 5 seconds.


\textbf{Setup.} Our input states include grayscale observation (resolution of $40\times60$), linear and angular velocity, and last action. Linear and angular velocities are obtained from the motion capture system.
We design experiments to demonstrate the effectiveness of image augmentations (\textit{i.e.,} color jittering and Gaussian blur) and different architectures (\textit{i.e.,} MLP vs CNN) in Sim2Real generalization.

\textbf{Results.} For quantitative evaluation, we run experiments five times and count the number of successes for each setting. As reported in Table~\ref{tab:exp_visual_policy_results}, image augmentations play an essential role in Sim2Real generalization. The policies without augmentation fail at generalization.
Also, despite the better learning capability of CNNs, they result in worse generalization to real.
We found CNN training often collapses to a trivial solution with the reward designs remaining the same as MLP trainings.
Such results imply that CNNs may overfit to simulated data and require more careful design in rewards and loss functions for stable training. 

We qualitatively evaluate the results through recorded videos in the supplementary materials. While both policies with augmentation predict meaningful actions, the -based policy generates more smooth and precise actions. On the other hand, the CNN-based policy often generates noisy actions as shown in fast steering changes and motor cogging noise. 


\section{Conclusion}\label{sec:conclusion}
\input{sections/conclusions}

\section{Limitations}
\input{sections/limitations}

\section*{Acknowledgments}

TH is supported by the NSF GRFP under Grant No. DGE 2140004.


\bibliographystyle{plainnat}
\bibliography{references}


\clearpage
\newpage
\input{sections/appendix}

\end{document}
