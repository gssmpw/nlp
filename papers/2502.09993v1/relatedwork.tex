\section{Related Work}
\subsection{Facial Expression Recognition}
% \subsection{Facial Expression Recognition in the Wild}
In-the-wild FER scenarios present two major challenges: 1) noisy labels, which are common in image classification but more prevalent in FER due to the inherent ambiguity of facial expressions, and 2) class imbalance resulting from varying expression frequencies. 
Recently, numerous methods have been proposed to alleviate these issues. 
For instance, SCN~\cite{wang2020suppressing_SCN} uses re-labeling and ranking regularization to mitigate noise, while EAC~\cite{zhang2022learn_EAC} minimizes overfitting to noisy samples by aligning Class Activation Maps~\cite{zhou2016learning_CAM} of the original and flipped images.
LA-Net \cite{wu2023net_LA-Net} utilizes landmark data to enhance expression features via expression-landmark interactions.  
Alternatively, paying more attention to ambiguity, DMUE \cite{she2021dive_DMUE} explores latent distributions with multiple branches based on uncertainty estimation, and RUL~\cite{zhang2021relative_RUL} uses a multi-branch framework with feature mixup to learn from relative sample difficulty. MAN~\cite{zhang2022man} employs a two-branch network with a co-division module to enhance discriminative ability by focusing on clean samples.

Regarding class imbalance, Face2Exp~\cite{zeng2022face2exp} improves FER by using large-scale unlabeled face recognition data within a meta-optimization framework, while MEK~\cite{zhang2024leave_MEK} emphasizes minority classes through re-balanced attention consistency and label smoothing. Despite these advances, real-world FER is still limited to improving generalization due to the combined challenges of noisy labels and class imbalance. Unlike previous works, our method addresses both issues simultaneously by dynamically exploring each sample's ambiguity.

\subsection{Learning with Ambiguity}
Recent approaches to handling ambiguity can be broadly categorized into small loss selection and disagreement/agreement strategies. Previous research~\cite{arpit2017closer_small_loss} has shown that deep neural networks initially learn simple patterns, leading to the assumption that small-loss samples are treated as clean. Inspired by this, MentorNet~\cite{jiang2018mentornet} uses a teacher-student framework, where the student network is trained only on small-loss samples selected by the teacher network. Co-teaching~\cite{han2018co_Co-teaching} further cross-updates two networks by exchanging small-loss samples in each mini-batch.

Alternatively, the disagreement/agreement strategy focuses on instances where predictions between two networks differ, which is similar to hard sample mining. 
As representative works, Decoupling~\cite{malach2017decoupling} and Co-teaching+~\cite{yu2019does_coteaching+} update networks based on these disagreements. In contrast, JoCoR~\cite{wei2020combating_JoCoR} aligns the predictions of the two networks using Jensen-Shannon divergence to ensure agreement. Drawing ideas from this approach, we apply a consistency regularization method that aligns the probabilities between the original data and its flipped version within a single network.

\begin{figure*}[htb!]
    \centerline{\includegraphics[width=1\textwidth]{main_fig.png}}
    \caption{\textbf{The framework of Navigating Label Ambiguity (NLA).} 
    NLA consists of two main components: 1) a Noise-aware Adaptive Weighting (NAW), which dynamically assigns weights to each sample based on the intermediate prediction scores for GT and NN, and 2) consistency regularization using pairs of original and horizontally flipped images.}
    \label{fig:main}
\end{figure*}

\subsection{Learning with Imbalanced Data}
Training samples in wild datasets often exhibit imbalanced class distributions, leading to models biased toward majority classes. One basic approach to address this issue is re-balancing class distributions using different sampling frequencies for each class. Decoupling~\cite{kang2019decoupling_imbalanced} evaluates various sampling strategies and finds that progressively balanced sampling is particularly effective. Dynamic Curriculum Learning (DCL)~\cite{wang2019dynamic_DCL} introduces an adaptive sampling strategy that starts with random sampling and later shifts focus to minority classes.

Another approach involves re-weighting the loss for each class to ensure balanced contributions. CB loss~\cite{cui2019class_CB} uses the effective number, which is inversely proportional to class size, ensuring equal loss contribution. Balanced Softmax~\cite{ren2020balanced} adjusts predicted logits based on label frequencies by considering class priors before calculating the final loss. Our approach combines these strategies by proposing loss re-weighting based on sample ambiguity, rather than class distribution, allowing the model to prioritize ambiguous samples in the minority class during later training stages.