\section{Related Work}
\subsection{Facial Expression Recognition}
% \subsection{Facial Expression Recognition in the Wild}
In-the-wild FER scenarios present two major challenges: 1) noisy labels, which are common in image classification but more prevalent in FER due to the inherent ambiguity of facial expressions, and 2) class imbalance resulting from varying expression frequencies. 
Recently, numerous methods have been proposed to alleviate these issues. 
For instance, SCN____ uses re-labeling and ranking regularization to mitigate noise, while EAC____ minimizes overfitting to noisy samples by aligning Class Activation Maps____ of the original and flipped images.
LA-Net ____ utilizes landmark data to enhance expression features via expression-landmark interactions.  
Alternatively, paying more attention to ambiguity, DMUE ____ explores latent distributions with multiple branches based on uncertainty estimation, and RUL____ uses a multi-branch framework with feature mixup to learn from relative sample difficulty. MAN____ employs a two-branch network with a co-division module to enhance discriminative ability by focusing on clean samples.

Regarding class imbalance, Face2Exp____ improves FER by using large-scale unlabeled face recognition data within a meta-optimization framework, while MEK____ emphasizes minority classes through re-balanced attention consistency and label smoothing. Despite these advances, real-world FER is still limited to improving generalization due to the combined challenges of noisy labels and class imbalance. Unlike previous works, our method addresses both issues simultaneously by dynamically exploring each sample's ambiguity.

\subsection{Learning with Ambiguity}
Recent approaches to handling ambiguity can be broadly categorized into small loss selection and disagreement/agreement strategies. Previous research____ has shown that deep neural networks initially learn simple patterns, leading to the assumption that small-loss samples are treated as clean. Inspired by this, MentorNet____ uses a teacher-student framework, where the student network is trained only on small-loss samples selected by the teacher network. Co-teaching____ further cross-updates two networks by exchanging small-loss samples in each mini-batch.

Alternatively, the disagreement/agreement strategy focuses on instances where predictions between two networks differ, which is similar to hard sample mining. 
As representative works, Decoupling____ and Co-teaching+____ update networks based on these disagreements. In contrast, JoCoR____ aligns the predictions of the two networks using Jensen-Shannon divergence to ensure agreement. Drawing ideas from this approach, we apply a consistency regularization method that aligns the probabilities between the original data and its flipped version within a single network.

\begin{figure*}[htb!]
    \centerline{\includegraphics[width=1\textwidth]{main_fig.png}}
    \caption{\textbf{The framework of Navigating Label Ambiguity (NLA).} 
    NLA consists of two main components: 1) a Noise-aware Adaptive Weighting (NAW), which dynamically assigns weights to each sample based on the intermediate prediction scores for GT and NN, and 2) consistency regularization using pairs of original and horizontally flipped images.}
    \label{fig:main}
\end{figure*}

\subsection{Learning with Imbalanced Data}
Training samples in wild datasets often exhibit imbalanced class distributions, leading to models biased toward majority classes. One basic approach to address this issue is re-balancing class distributions using different sampling frequencies for each class. Decoupling____ evaluates various sampling strategies and finds that progressively balanced sampling is particularly effective. Dynamic Curriculum Learning (DCL)____ introduces an adaptive sampling strategy that starts with random sampling and later shifts focus to minority classes.

Another approach involves re-weighting the loss for each class to ensure balanced contributions. CB loss____ uses the effective number, which is inversely proportional to class size, ensuring equal loss contribution. Balanced Softmax____ adjusts predicted logits based on label frequencies by considering class priors before calculating the final loss. Our approach combines these strategies by proposing loss re-weighting based on sample ambiguity, rather than class distribution, allowing the model to prioritize ambiguous samples in the minority class during later training stages.