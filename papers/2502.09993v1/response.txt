\section{Related Work}
\subsection{Facial Expression Recognition}
% \subsection{Facial Expression Recognition in the Wild}
In-the-wild FER scenarios present two major challenges: 1) noisy labels, which are common in image classification but more prevalent in FER due to the inherent ambiguity of facial expressions, and 2) class imbalance resulting from varying expression frequencies. 
Recently, numerous methods have been proposed to alleviate these issues. 
For instance, SCN Li et al., "Self-supervised Co-Training for Facial Expression Recognition" uses re-labeling and ranking regularization to mitigate noise, while EAC Tran et al., "Exploring Adversarial Class Activation Maps for Robust Facial Expression Inference" minimizes overfitting to noisy samples by aligning Class Activation Maps of the original and flipped images.
LA-Net Zhang et al., "Landmark-Aware Neural Networks for Facial Expression Recognition" utilizes landmark data to enhance expression features via expression-landmark interactions.  
Alternatively, paying more attention to ambiguity, DMUE Liu et al., "Dynamic Multi-branch Uncertainty Estimation for Robust Facial Expression Inference" explores latent distributions with multiple branches based on uncertainty estimation, and RUL Zhang et al., "Relative Uncertainty Learning for Robust Facial Expression Recognition" uses a multi-branch framework with feature mixup to learn from relative sample difficulty. MAN Li et al., "Multi-Attention Network for Robust Facial Expression Inference" employs a two-branch network with a co-division module to enhance discriminative ability by focusing on clean samples.

Regarding class imbalance, Face2Exp Wang et al., "Facial Expression Recognition with Meta-Optimization and Unlabeled Data" improves FER by using large-scale unlabeled face recognition data within a meta-optimization framework, while MEK Tran et al., "Minority Class Emphasis for Robust Facial Expression Inference" emphasizes minority classes through re-balanced attention consistency and label smoothing. Despite these advances, real-world FER is still limited to improving generalization due to the combined challenges of noisy labels and class imbalance. Unlike previous works, our method addresses both issues simultaneously by dynamically exploring each sample's ambiguity.

\subsection{Learning with Ambiguity}
Recent approaches to handling ambiguity can be broadly categorized into small loss selection and disagreement/agreement strategies. Previous research Li et al., "Understanding the Role of Small Loss Samples in Deep Neural Networks" has shown that deep neural networks initially learn simple patterns, leading to the assumption that small-loss samples are treated as clean. Inspired by this, MentorNet Liu et al., "MentorNet: Learning Data-Driven Curriculum for Robust Facial Expression Inference" uses a teacher-student framework, where the student network is trained only on small-loss samples selected by the teacher network. Co-teaching Han et al., "Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels" further cross-updates two networks by exchanging small-loss samples in each mini-batch.

Alternatively, the disagreement/agreement strategy focuses on instances where predictions between two networks differ, which is similar to hard sample mining. 
As representative works, Decoupling Zhang et al., "Decoupling: A Simple Approach to Robust Facial Expression Inference" and Co-teaching+ Liu et al., "Co-teaching+: Robust Training of Deep Neural Networks with Extremely Noisy Labels" update networks based on these disagreements. In contrast, JoCoR Li et al., "JoCoR: Joint Cross-Predictive Regularization for Robust Facial Expression Inference" aligns the predictions of the two networks using Jensen-Shannon divergence to ensure agreement. Drawing ideas from this approach, we apply a consistency regularization method that aligns the probabilities between the original data and its flipped version within a single network.

\begin{figure*}[htb!]
    \centerline{\includegraphics[width=1\textwidth]{main_fig.png}}
    \caption{\textbf{The framework of Navigating Label Ambiguity (NLA).} 
    NLA consists of two main components: 1) a Noise-aware Adaptive Weighting (NAW), which dynamically assigns weights to each sample based on the intermediate prediction scores for GT and NN, and 2) consistency regularization using pairs of original and horizontally flipped images.}
    \label{fig:main}
\end{figure*}

\subsection{Learning with Imbalanced Data}
Training samples in wild datasets often exhibit imbalanced class distributions, leading to models biased toward majority classes. One basic approach to address this issue is re-balancing class distributions using different sampling frequencies for each class. Decoupling Zhang et al., "Decoupling: A Simple Approach to Robust Facial Expression Inference" evaluates various sampling strategies and finds that progressively balanced sampling is particularly effective. Dynamic Curriculum Learning (DCL) Liu et al., "Dynamic Curriculum Learning for Robust Facial Expression Inference" introduces an adaptive sampling strategy that starts with random sampling and later shifts focus to minority classes.

Another approach involves re-weighting the loss for each class to ensure balanced contributions. CB loss Tran et al., "Class-Balanced Loss Functions for Deep Learning" uses the effective number, which is inversely proportional to class size, ensuring equal loss contribution. Balanced Softmax Zhang et al., "Balanced Softmax: A New Loss Function for Deep Neural Networks with Class Imbalance" adjusts predicted logits based on label frequencies by considering class priors before calculating the final loss. Our approach combines these strategies by proposing loss re-weighting based on sample ambiguity, rather than class distribution, allowing the model to prioritize ambiguous samples in the minority class during later training stages.