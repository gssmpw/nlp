\begin{table}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lllccc}
\hline
\multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Backbone}}} & \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Lang}}} & \multicolumn{3}{c}{$\mathbf{F_{0.5}}$ on test set} \\ \cline{4-6} 
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \textbf{EN} & \textbf{DE} & \textbf{RU} \\ \hline
\multicolumn{6}{c}{\textbf{Fine-tuned GEC Single Model}} \\ \hline
\multicolumn{1}{l|}{\citet{rothe2021clang8_gec}} & \multicolumn{1}{l|}{gT5 xxl} & \multicolumn{1}{l|}{Mono} & 65.7 & \textbf{76.0} & \textbf{51.6} \\
\multicolumn{1}{l|}{\citet{luhtaru-etal-2024-nllb-gec}} & \multicolumn{1}{l|}{NLLB} & \multicolumn{1}{l|}{Multi} &  & 73.9 &  \\
\multicolumn{1}{l|}{\citet{zhou-etal-2023-decoding-interventions}} & \multicolumn{1}{l|}{BART} & \multicolumn{1}{l|}{Mono} & \textbf{69.6} &  &  \\ \hline
\multicolumn{6}{c}{\textbf{Inference of LLMs}} \\ \hline
\multicolumn{1}{l|}{\citet{davis-etal-2024-prompting}} & \multicolumn{1}{l|}{GPT-3.5-Turbo} & \multicolumn{1}{l|}{-} & 57.2 &  &  \\
\multicolumn{1}{l|}{\citet{tang-etal-2024-ungrammatical}} & \multicolumn{1}{l|}{GPT-3.5-Turbo} & \multicolumn{1}{l|}{-} & 58.8 &  &  \\
\multicolumn{1}{l|}{Ours} & \multicolumn{1}{l|}{Deepseek2.5} & \multicolumn{1}{l|}{-} & \textbf{59.4} & \textbf{63.4} & \textbf{43.7} \\ \hline
\end{tabular}
}
\caption{
Some reported metrics for state-of-the-art (SOTA) multilingual GEC methods based on fine-tuning or direct inference. "EN", "DE", and "RU" denote the CoNLL-14, Falko-Merlin, and RULEC datasets, respectively. For the fine-tuning methods, some models are trained using multilingual mixed data, indicated as "Multi" in the "Lang" column, while other methods fine-tune models separately for each language, marked as "Mono".
}
\label{tab:sota}
\end{table}