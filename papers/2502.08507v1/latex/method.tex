\section{Methods}
This section presents our design of a GEC system based on few-shot in-context learning using grammatical error explanation (GEE). First, we describe how to construct a database with GEE using labeled data in advance. Then, we explain the retrieval process based on explanations during inference, where no labels are available. Finally, we outline the few-shot template for GEC.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/naacl25_main_new.drawio.pdf}
    \caption{The proposed pipeline for few-shot GEC using the explanation-based demonstration retrieval method. As shown on the left side, we construct sample databases that include explanations. As illustrated on the right side, in the prediction stage, the erroneous samples for in-context demonstrations are retrieved using explanations.
}
    \label{fig:model}
\end{figure*}


\subsection{Explanation Database}

% With the continuous enhancement of the foundational and multilingual capabilities of Large Language Models (LLMs), these models can achieve notable performance on grammatical error explanation (GEE) tasks \(\cite{song-etal-2024-gee}\). In this paper, to ensure the scalability of the method, we directly prompt LLMs to generate corresponding GEEs for erroneous samples, as illustrated in Figure \ref{fig:model}.

% Constructing the database requires a series of labeled data. These labeled data can be sourced from the training sets of GEC datasets or from typical cases compiled manually by language instructors. In this work, without involving manual annotation, we choose the former as the data source. After applying quality filtering, we obtain a collection of text pairs denoted as \(\mathcal{S}\).

As the foundational and multilingual capabilities of LLMs continue to improve, LLMs can achieve strong performance on the GEE task \cite{song-etal-2024-gee}. In this paper, to ensure extensibility, we guide the LLMs to generate GEE for erroneous samples directly via prompts, as shown in Figure \ref{fig:model}. 

Building the database requires labeled datasets, which can either come from the training set of a GEC dataset or from typical grammatical correction cases manually compiled by language teachers. In this work, we choose the former as the data source, avoiding manual annotation. After applying quality filtering, we obtain a collection of samples to build the database, denoted as $\mathcal{S}$ in Equation \ref{eq:raw_dataset}.
\begin{equation}
\mathcal{S} = \{ ( x_1, y_1 ), \dots , ( x_n, y_n ) \}
\label{eq:raw_dataset}
\end{equation}

Here, $x_i$ and $y_i$ represent the \textbf{input text} (potentially erroneous) and the \textbf{corrected text} for the $i$ th sample, respectively. Next, $\mathcal{S}$ is divided into two parts: the \textbf{erroneous samples} where $x \ne y$ and the \textbf{correct samples} where $x = y$. For the former, we use an LLM, referred to as the teacher model, to generate corresponding GEE. The explanations serve as keys in the database. The database for erroneous samples is shown in Equation \ref{eq:error_database}.
\begin{equation}
(\mathcal{K},  \mathcal{V})_E  = \{ \left( e, \left(x, y \right) \right) | \forall (x, y) \in \mathcal{S}, x \ne y \}
\label{eq:error_database}
\end{equation}

Here, $\mathcal{K}$ represents the key, and $\mathcal{V}$ represents the value. The $e$ represents the \textbf{GEE} provided by the teacher model based on the pair $(x, y)$. Since GEC methods must also handle input without grammatical errors, a database for correct samples must also be constructed. As GEE cannot be generated for this part, we use the input text $x$ as the key to form the database for correct samples, as shown in Equation \ref{eq:correct_database}.
\begin{equation}
( \mathcal{K}, \mathcal{V})_C = \{ \left( x, (x, y) \right) | \forall (x, y) \in \mathcal{S}, x = y \}
\label{eq:correct_database}
\end{equation}

For each language's labeled data, we can construct two parts of the database as shown in Equations \ref{eq:error_database} and \ref{eq:correct_database}.

\subsection{Demonstrations Retrieval}

When dealing with the test data, denoted as $x^T$, the first step is to retrieve a set of samples from the database to serve as demonstrations for few-shot inference. Since the erroneous samples database uses explanations as keys, we need to rewrite the query based on the test input. Here, we use a detection prompt to guide the LLM predictor in detecting potential grammatical errors and the corresponding grammatical knowledge in the test input, which we denote as $d^T$ for an \textbf{initial explanation}. This will then serve as the query to retrieve similar samples from the erroneous samples database $(\mathcal{K}, \mathcal{V})_E$. For correct samples, we can directly use $x^T$ as the query, as shown in Equation \ref{eq:query}:
\begin{equation}
q^T_E=d^T=\mathrm{LLM}_p \left( \mathrm{prompt}_d (x^T) \right), \quad q^T_C=x^T
\label{eq:query}
\end{equation}

Here, $x^T$ represents the \textbf{test input} text that may contain grammatical errors. $q^T_E$ and $q^T_C$ are the queries used for retrieving samples from the erroneous and correct samples databases, respectively. $\mathrm{LLM}_p$ denotes the prediction model, and $\mathrm{prompt}_d$ represents the detection prompt. Once the queries are obtained, the kNN method is used to retrieve the top $k_E$ and $k_C$ samples from the two databases, as shown in Equations \ref{eq:error_demo} and \ref{eq:correct_demo}:
\begin{equation}
\mathcal{N}_E = \left\{ \left( e^{(j)} , ( x^{(j)}, y^{(j)} ) \right) \in (\mathcal{K}, \mathcal{V})_E \right\}_{j=1}^{k_E}
\label{eq:error_demo}
\end{equation}

\begin{equation}
\mathcal{N}_C = \left\{ \left( x^{(j)} , ( x^{(j)}, y^{(j)} ) \right) \in (\mathcal{K}, \mathcal{V})_C \right\}_{j=1}^{k_C}
\label{eq:correct_demo}
\end{equation}

\subsection{In-Context Learning}

In few-shot inference, the keys of the retrieved data samples are no longer used, as incorporating long explanations into the context can negatively impact the final GEC performance and significantly slow down inference. The selected demonstrations are simply the list of text pairs retrieved from the database, as shown in Equation \ref{eq:demonstration}.
\begin{equation}
D = D_E \oplus D_C
\label{eq:demonstration}
\end{equation}

Here, $D_E$ and $D_C$ are the lists of $(x, y)$ text pairs from $\mathcal{N}_E$ and $\mathcal{N}_C$, respectively. The concatenated list of samples, $D$, serves as the demonstrations, which are then combined with the few-shot prompt template for the final GEC prediction. The whole prediction process is outlined at the right half of Figure \ref{fig:model}. Given that few-shot examples can provide strong instruction-following capabilities, the prompt template is consistently written in English, with the demonstrations formatted and inserted sequentially. Further details on the prompt can be found in Appendix \ref{sec:appendix-prompt}.
