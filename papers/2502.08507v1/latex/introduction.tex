\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/fig-introduction.jpg}
    \caption{The comparison between input-based demonstrations and explanation-based demonstrations for GEC. Samples with similar inputs do not necessarily contain the same grammatical error patterns. However, through preliminary checks and initial explanations, it is possible to retrieve samples with similar errors from a database indexed by grammatical error explanations, even if the semantics of the demonstrations differs significantly from the test input.}
    \label{fig:intro}
\end{figure}


The goal of grammatical error correction (GEC) is to correct errors in natural language text, including grammatical, spelling, and certain semantic errors \cite{2023gecsurvey}. In the context of language learning, GEC methods can assist learners in correcting mistakes in written text \cite{katinskaia2021assessing_in_learning, caines2023teaching_application_gec}. GEC can be considered a form of machine translation, where the input is a potentially erroneous text, and the output is its corrected version \cite{yuan-briscoe-2016-grammatical}. As a result, text-to-text generation models underpin many GEC approaches \cite{junczys-dowmunt-etal-2018-approaching, katsumata-komachi-2020-stronger}. While edit-based methods exist that modify specific parts of the text for correction \cite{awasthi2019parallel_edit_seq2edit_gec, omelianchuk-etal-2020-gector}, the rise of large language models (LLMs) \cite{brown2020gpt3, touvron2023llama} has made direct text generation the dominant approach in GEC. Given the need for language learning across different languages, multilingual GEC has become an important area of research. This challenge is often tackled by constructing datasets and training models separately for each language \cite{ijcai2022-unified-mgec, rothe2021clang8_gec}. With large-scale pre-training on multilingual data, the performance of LLMs in multilingual contexts is gradually improving \cite{qin2024multilingualllms}, and LLMs are increasingly being applied to the multilingual GEC domain \cite{luhtaru-etal-2024-nllb-gec}.

Few-shot inference can achieve strong performance on downstream tasks through in-context learning (ICL) without the need for fine-tuning LLMs \cite{brown2020gpt3}. Effective ICL depends on selecting appropriate in-context examples, or demonstrations, which can be guided by established principles \cite{agrawal-etal-2023-context}. The selection process can be facilitated through retrieval mechanisms, such as k-nearest neighbors (kNN)-based retrieval \cite{vasselli-watanabe-2023-knngec}. One advantage of example-based GEC systems is that they can enhance system interpretability by providing learners with illustrative examples \cite{kaneko-etal-2022-interpretability}.

For retrieving in-context examples, previous studies primarily use the test input as the query and the input texts of labeled data as keys for the database \cite{retrieve-demonstration-survey}. Matching between query and keys is typically based on semantic or syntactic similarities, or methods like BM25 \cite{hongjin2022selective, tang-etal-2024-ungrammatical, robertson2009probabilistic}. This approach is intuitive since both the test input and the input text of labeled data are from the same domainâ€”if the inputs are similar, the corresponding labels are likely similar as well. However, GEC tasks pose a unique challenge, as grammatical errors are often local and bear little correlation to the overall structure of the text. Consequently, input-based retrieval often fails to retrieve references with analogous grammatical errors.

We propose that a more fundamental solution lies in using the relationship between the input text and the label as the basis for retrieval. Recent works have begun to explore leveraging the interaction between text and label to improve in-context example selection \cite{sun2024retrieved-by-mistake}. For GEC, we hypothesize that this relationship can be effectively captured through grammatical error explanations (GEE) \cite{song-etal-2024-gee}.

In this paper, we introduce a novel retrieval method based on natural language explanations, applicable in multilingual settings. Our method uses GEE as both the query and key for retrieval, aiming to retrieve examples with similar grammatical errors to serve as demonstrations for in-context learning. Without requiring model training, this approach enhances LLM few-shot performance on multilingual GEC tasks. Figure \ref{fig:intro} presents an illustrative example where traditional semantic matching fails to retrieve relevant samples, while explanation-based matching successfully identifies a sample with a similar grammatical error.

To implement this, we first use labeled data and LLMs to generate GEE and construct a database. During inference, the input text undergoes a grammar check, providing an initial explanation, which is then matched against the GEE database to retrieve suitable demonstrations. These demonstrations are used in the ICL process to produce the final grammatical corrections.

We conducted experiments on GEC datasets across five languages. Our proposed method consistently outperforms semantic and BM25 retrieval in terms of the $F_{0.5}$ score across both open-source and closed-source models. The advantages of the proposed method contain: 1) It requires no additional training, leveraging ICL to tap into the intrinsic GEC capabilities of LLMs. Some studies suggest that the $F_{0.5}$ score underestimates LLM GEC performance, with human evaluations yielding better results \cite{coyne2023analyzing}. 2) It is highly extendable to multilingual settings, as the use of natural language explanations ensures applicability across languages. 3) The system is more interpretable, with each demonstration linked to a GEE.

The main contributions of this work are as follows:
\begin{itemize}
    \item We propose a novel demonstration retrieval method for in-context learning of GEC based on natural language explanations and design a GEC process involving grammar checks, retrieval, and final corrections.
    \item We construct GEC databases with grammatical error explanations across multiple languages, which can be applied to different datasets within the same language.
    \item To the best of our knowledge, this work provides the first aligned evaluation of LLMs' few-shot capabilities for GEC, approaching state-of-the-art performance across several languages.
\end{itemize}
