\input{tables/cot}

\section{Discussion}
\subsection{The Extension to Other Datasets}

\input{tables/transfer}

To verify that the constructed database is effective across multiple test sets of the same language, we conducted experiments on another commonly used English GEC test set, BEA-19 \cite{bryant-etal-2019-bea-19}, using Llama3.1 and Qwen2.5. As shown in Table \ref{tab:transfer}, the methods exhibit similar trend to those observed in the main experiments. On the Qwen2.5 model, the advantage of the proposed method is more pronounced. These results demonstrate that the explanation-based erroneous samples database we constructed can be transferred to different test scenarios to a certain extent while maintaining the advantage in performance.


\subsection{The Performance of In-context Explanations}
\label{sec:in-context-explanation}
A key point in using explanations for demonstration retrieval is how to generate a query for retrieval when the corrected text is unknown. We employ a detection prompt to perform grammar checking on the test input, obtaining an initial explanation to serve as the query. This raises the question: \textit{why not directly use this initial explanation as the chain of thought (CoT) in the reasoning process and let the model derive the final GEC answer based on it?} We experimented with this CoT approach, and as shown in Table \ref{tab:cot}, the first two rows present the results of the zero-shot and CoT experiments with the Llama3.1 model. The significant drop in the $F_{0.5}$ score indicates that this CoT method is completely ineffective for GEC. Possible reasons include: 1) Without labeled data, the quality of the initial explanation is poor, making it only useful as a retrieval tool but misleading when used as an intermediate reasoning step, especially given the minimal difference between input and output in GEC tasks; 2) The intermediate reasoning step may interfere with the modelâ€™s ability to follow instructions for the final output.

Given the poor performance of CoT in the zero-shot scenario, \textit{What about adding explanations in the context of the few-shot inference?} We conduct experiments on this as well. There are two possible placements for the explanation text: one is before the answer, following an input-explanation-output pattern, and the other is after the answer, following an input-output-explanation pattern. In the experiments, the same format is applied across all examples and test data, with explanations set to "No error in text" for error-free samples. Notably, in the latter pattern, the predicted sample directly outputs the corrected text without using the initial explanation as an intermediate reasoning step as in the former case. The results in rows 4 and 5 of Table \ref{tab:cot} show that adding explanation text generally degrades GEC performance. This may be because the lengthy explanation takes up a large portion of the context, which negatively impacts the LLMs' performance on the GEC task itself. Moreover, in row 4, we observe a trend similar to that in row 2, further validating that using the initial explanation as an intermediate reasoning step weakens the results. The proposed method of using it for demonstration retrieval, rather than reasoning, is one of the correct ways to utilize it.


\subsection{The Balance of Erroneous and Correct Demonstrations}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/metrics_vs_kc.png}
    \caption{GEC metrics on 4 datasets as the number of the correct samples $k_C$ varies, with a total of $k_E + k_C = 8$ in-context demonstrations consisting of both erroneous and correct samples.}
    \label{fig:trade-off}
\end{figure*}

As described in Section \ref{sec:model-metric}, the number of correct and erroneous samples in the demonstrations is set to 4 each. The ratio between these two types of samples can affect precision and recall, thereby influencing the final GEC performance measured by $F_{0.5}$. By fixing the total number of samples at 8 and varying the number of correct samples from 0 to 8, we can create a series of experiments to observe how GEC performance changes with different ratios of correct to erroneous samples. As shown in Figure \ref{fig:trade-off}, as the number of correct samples increases and the number of erroneous samples decreases, there is a general trend of increasing precision and decreasing recall. The $F_{0.5}$ score achieves its maximum value when the number of correct samples is 4 or 5. Therefore, for ICL of multilingual GEC, it is reasonable to use an equal number of correct and erroneous samples, which is also our setting.



\subsection{The Guidance of Explanation Generation}
\label{sec:exp_prompt_influence}

\input{tables/exp_prompt}

In previous works on generating grammatical error explanations (GEE), the approach typically involved extracting edit operations by aligning input text with corrected text \cite{kaneko-okazaki-2024-controlled, song-etal-2024-gee}. In our proposed method, we directly allow the teacher model to generate GEE based on the original text pairs. Additionally, we also crafted a version that utilizes extracted edits in the explanation prompt and conduct the experiments on LLama3.1. Two prompts are both shown in Appendix \ref{sec:appendix-prompt}. The results are shown in the first three columns of Table \ref{tab:exp-prompt}, with the last column representing the $F_{0.5}$ performance of the proposed method in the main experiment. It is evident that both types of explanations have their strengths and weaknesses. Ultimately, we opted for the GEE generated directly based on the original text pairs, as correcting a grammatical error may involve multiple edit operations. Not specifying the edit operations in advance allows LLMs greater freedom in generating explanations.

