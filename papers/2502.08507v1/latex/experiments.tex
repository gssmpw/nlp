\section{Experiments}
\input{tables/main_result}


% \cite{ng-etal-2014-conll14}
% \cite{bryant-etal-2019-bea-19}
% \cite{zhao2018nlpcc-gec}
% \cite{xu-etal-2022-fcgec}
% \cite{zhang2009hsk}

% \cite{boyd-2018-falko_merlin}
% \cite{Rozovskaya-rulec}

% \footnote{\href{https://github.com/TartuNLP/estgec}{https://github.com/TartuNLP/estgec}} 

\subsection{Datasets}
\label{sec:datasets}
To evaluate the effectiveness of the proposed method, we conduct experiments on GEC datasets in multiple languages. For English, we used the W\&I+LOCNESS \cite{bryant-etal-2019-bea-19} training dataset as the database, and the CoNLL-14 \cite{ng-etal-2014-conll14} and BEA-19 datasets as the testing datasets. We also perform experiments on GEC datasets for Chinese, Russian, and German, which have a relatively high number of users. For Chinese, we utilize the HSK dataset \cite{zhang2009hsk} as the database and the NLPCC-18 dataset as the test data \cite{zhao2018nlpcc-gec}. For Russian and German, we employ the RULEC and Falko-Merlin datasets, which include both training and testing sets \cite{Rozovskaya-rulec, boyd-2018-falko_merlin}. Furthermore, we conduct experiments on the relatively low-resource Estonian language, using data from the Tartu learner corpus \footnote{\href{https://github.com/TartuNLP/estgec}{https://github.com/TartuNLP/estgec}}. In our setting, the L2 learner corpus is used as the database, while the L1 corpus serves as the test data. The database and retriever is implemented by LlamaIndex \cite{Liu_LlamaIndex_2022}.

\subsection{Models and Metrics}
\label{sec:model-metric}

% \cite{dubey2024llama3}
% \cite{qwen2.5}
% \cite{deepseekv2}
% \cite{gemma_2024}
% \cite{xlm_roberta}

% \cite{bryant-etal-2017-errant}
% \cite{zhang-etal-2022-mucgec}
% \cite{dahlmeier-ng-2012-m2scorer}

We selected state-of-the-art multilingual LLMs for our experiments, including Llama-3.1-8B-Instruct by Meta \cite{dubey2024llama3} and Qwen2.5-7B-Instruct \cite{qwen2.5} developed by Tongyi. Among the closed-source models, we choose Deepseek2.5 \cite{deepseekv2} and GPT-4o-mini \cite{achiam2023gpt4-report} for its cost-effectiveness. 

In the experiments, the teacher model used to generate the GEE-based database is uniformly set to Llama-3.1-8B-Instruct. For constructing the database, we used xlm-roberta-large \cite{xlm_roberta} as the embedding model. For the prediction model in the few-shot inference stage, we conducted experiments with the four LLMs mentioned above, which serve as our primary experimental models. For the test data, we used ERRANT edit extraction results as the labels \cite{bryant-etal-2017-errant, zhang-etal-2022-mucgec}, and we evaluated the precision, recall, and $F_{0.5}$ values using the M2Scorer \cite{dahlmeier-ng-2012-m2scorer}.

The multilingual baseline methods used for comparison were: 1) \textit{Random}: randomly selecting samples from the database as demonstrations; 2) \textit{Semantic}: using the input text embedding to perform kNN retrieval \cite{khandelwal2020knnmt}; and 3) \textit{BM25}: employing BM25, a term-based retriever, for retrieval \cite{robertson2009probabilistic}. For all methods, including our proposed method, we set $k_E = 4$ erroneous samples and $k_C = 4$ correct samples as demonstrations. The final experimental results are shown in Table \ref{tab:main_results}.


\subsection{Results}

\input{tables/sota}


\noindent \textbf{Performance of the proposed method} \quad From Table \ref{tab:main_results}, we can see that when using $F_{0.5}$ as the evaluation metric, our proposed method generally outperforms other reproduced methods under the same experimental setup. Overall, the proposed method significantly outperforms random selection. This is particularly evident with GPT4o-mini, where the $F_{0.5}$ score of the proposed method exceeds that of \textit{Random} by approximately 2 points. Among the baseline methods, the \textit{Semantic} method performs best with open-source LLMs, averaging about 1 point higher than the \textit{Random} method. However, this is not the case for the Deepseek2.5, possibly due to randomness introduced by the temperature setting. As the representative models, Llama3.1 (open-source) and GPT4o-mini (closed-source), the proposed method's average $F_{0.5}$ value exceeds that of the \textit{Semantic} method by 0.36 and 0.84, respectively. Given that the explanation texts in erroneous samples differ entirely from the input text, this result shows that using explanations as a tool for in-context demonstration retrieval is indeed a feasible approach, which is slightly more effective than directly matching based on input text embeddings. Several examples demonstrating improvements have been included in the Appendix \ref{sec:appendix-cases}.

\noindent \textbf{In-context learning performance for multilingual GEC} \quad From Table \ref{tab:main_results}, we can observe that both open-source and closed-source LLMs demonstrate certain multilingual GEC capabilities. However, their performances vary significantly across different languages, possibly due to the different capability among languages and the different levels of difficulty of the datasets. for instance, the L1 learner dataset for Estonian is more challenging, resulting in lower performance metrics across all models. Overall, closed-source models tend to outperform open-source models with around 8B parameters, which is reasonable given that closed-source models often have larger architectures and pre-training scales. Additionally, it is noteworthy that, in many cases, regardless of the method used to select demonstrations, the few-shot performance of large models does not differ significantly. For example, when using Llama3.1 on the Falko-Merlin dataset, the $F_{0.5}$ scores differ by only 0.6 across 4 demonstration retrieval strategies. These results indicate that the GEC capabilities of large models largely depend on their inherent abilities, while the selection of examples has some influence around the inherent few-shot performance level.

\noindent \textbf{Comparison with SOTA} \quad We compared the performance of the LLMs few-shot inference with several state-of-the-art (SOTA) multilingual GEC methods, as shown in Table \ref{tab:sota}. Most SOTA methods are all fine-tuned on labeled datasets, with some methods tuning a generative model for each language individually. Only results using the same test sets are listed in the table for comparison. We can see that current few-shot ICL lags behind supervised finetuning (SFT) by about 10 points in terms of $F_{0.5}$. Considering that the GEC performance of LLMs is actually underestimated \cite{coyne2023analyzing}, this gap may be smaller. As the capabilities of LLMs continue to improve, we can expect that in the future, ICL could replace SFT as the most efficient method for multilingual GEC.
