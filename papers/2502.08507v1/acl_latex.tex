% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
% others
\usepackage{multirow} 
\usepackage{booktabs} 
\usepackage{bbding}
\usepackage{graphicx}
\usepackage{amssymb}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl}
\usepackage{amsmath} 
\usepackage[normalem]{ulem}
\usepackage{inconsolata}
\usepackage{arydshln}
\useunder{\uline}{\ul}{}
\pagestyle{empty}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Wei Li, Wen Luo, Guangyue Peng, Houfeng Wang \\
  State Key Laboratory of Multimedia Information Processing \\
  School of Computer Science, Peking University \\
  weili22@stu.pku.edu.cn, llvvvv22222@gmail.com,\{agy, wanghf\}@pku.edu.cn}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the GEE of the test input with that of pre-constructed database samples, where explanations for erroneous samples are generated by LLMs. We conducted multilingual GEC few-shot experiments on both major open-source and closed-source LLMs. Experiments across five languages show that our method outperforms existing semantic and BM25-based retrieval techniques, without requiring additional training or language adaptation. This also suggests that matching error patterns is key to selecting examples. Our code is available at \href{https://github.com/GMago-LeWay/FewShotGEC}{https://github.com/GMago-LeWay/FewShotGEC}.

\end{abstract}


\input{latex/introduction}

\input{latex/related_work}

\input{latex/method}

\input{latex/experiments}

\input{latex/discussion}


% \section{Conclusion}

% In conclusion, this paper introduces a novel approach to retrieve demonstrations based on grammatical error explanations (GEE) for in-context learning of multilingual grammatical error correction task (GEC). Our method addresses the challenge of selecting effective in-context demonstrations by matching GEE in the constructed demonstration database, enhancing the LLMs few-shot performance in multilingual GEC without additional training. Future work will explore: 1) How to index the correct samples with suitable explanations, which is not achieved in this work; 2) Design the retrieval method more precisely. In our work, only the text embedding is utilized as representations for explanations. We believe that the explanation-based sample retrieval method has implications for in-context learning across all natural language processing downstream tasks. For GEC, we consider few-shot inference to be the most reasonable approach for constructing an explainable multilingual GEC system based on LLMs. As the multilingual capabilities of LLMs continue to improve, we anticipate that the few-shot ICL will become a cost-effective and efficient method for GEC.

\section{Conclusion}

In conclusion, this paper presents a novel approach for retrieving demonstrations based on grammatical error explanations (GEE) to enhance in-context learning (ICL) for the multilingual grammatical error correction (GEC) task. Our method addresses the challenge of selecting effective in-context demonstrations by matching GEE within a constructed demonstration database, improving the few-shot performance of LLMs in multilingual GEC without requiring additional training. 

Future work will explore: 1) methods for indexing correct samples with suitable explanations, which was not fully addressed in this work; 2) refining the retrieval method. In this study, only text embeddings were used as representations for explanations; and 3) Further refinement of GEE. A more systematic form of GEE and its evaluation methodologies would facilitate the evolution of retrieval systems based on this framework. Additionally, it would also promote the application of GEE within the domain of GEC.

We believe that explanation-based demonstration retrieval has broader implications for ICL across all natural language processing downstream tasks. For GEC, we consider few-shot inference to be the most effective approach for building an explainable multilingual GEC system using LLMs. As LLMs' multilingual capabilities continue to improve, we expect that few-shot ICL will become an efficient and cost-effective solution for GEC.



\section*{Limitations}
Firstly, due to limitations in computational resources, we only used datasets from 5 languages and conducted experiments on 2 open-source LLMs and 1 closed-source LLM. To further validate the effectiveness of the proposed method, experiments should be conducted on a wider variety of LLMs with different sizes and on more datasets across more languages. Secondly, in this paper, the teacher model used to generate the explanation database is the open-source Llama3.1 with 8B parameters. Since generating the database requires high-quality explanations, using a closed-source LLM with stronger base capabilities could be a better choice and may further improve the performance of the proposed method. Although the database can be reused once it is built, its large scale makes the cost of using closed-source LLMs to build it relatively high. Additionally, we only compared the proposed method with 3 baseline methods. There are indeed many ICL demonstration retrieval methods, but many are implemented for specific tasks or specific languages, making it challenging to reproduce and apply them to multilingual GEC. For example, BM25 retrieval based on LlamaIndex encountered issues with the Estonian dataset, leaving this part of the experiment incomplete.


The current research on GEE also limits the performance of the method proposed in this paper. Currently, there is no systematic and automated method for evaluating the quality of GEE, and there are no unified standards regarding the contents that should be included within GEE. This paper can only demonstrate that the quality of explanations generated by LLMs is sufficient to support improvements in the performance of the few-shot GEC. We hope that future work will involve the elaboration for GEE.


\section*{Ethics Statement}
The datasets and models used in this work are publicly available and have been employed exclusively for research purposes. The datasets do not contain any personally identifiable information or offensive content. The large language models (LLMs) utilized in our experiments are consistent with their permitted use in academic research. The code for our proposed method will be made publicly available for academic research in grammatical error correction, adhering to the access conditions of the LLMs used in this study.

LLMs are almost unlikely to generate harmful content under our few-shot settings, as the input remains largely unchanged in the grammatical error correction (GEC) task. However, there is still potential risk of hallucinations, particularly when certain words involving facts are modified by the models.

Additionally, GPT-4o from the ChatGPT platform was used as an AI assistant for refining the paper writing.


\section*{Acknowledgments}
This work was supported by National Science and Technology Major Project (No. 2022ZD0116308)  and National Natural Science Foundation of China (62036001) . The corresponding author is Houfeng Wang.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Dataset Statistics}

\input{tables/statistics}

As described in Section \ref{sec:datasets}, our dataset usage consists of two parts: the labeled data used to build the database and the test data used to evaluate the proposed method. The labeled data samples used to construct the database are initially filtered by length, with a minimum of 10 and a maximum of 60 tokens to ensure quality. Additionally, we limited the number of total samples to 25,000. The filtering process also helped to reduce the cost of constructing the explanation database. The statistics of the datasets used in this paper are shown in Table \ref{tab:statistics}.


\section{Experimental settings}
\subsection{Model Settings}

We conducted experiments using open-source LLMs available on the Huggingface community, including \href{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}{Llama-3.1-8B-Instruct} and \href{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}{Qwen2.5-7B-Instruct}. During generation, we set the temperature to 0 and avoided any random sampling strategies to eliminate output randomness for all open-source LLMs. For closed-source LLMs, we utilized their official APIs and applied their default settings to get better performance, which introduced some degree of randomness. As a baseline for comparison, the method of "Random" selecting samples for open-source LLMs is evaluated by running it with 3 different random seeds and reporting the average result, while the other method will get a fixed output so only one-round inference is performed. Due to the cost of usage, only the single-round results of API-based closed LLMs are reported.


\subsection{Prompt Settings}
\label{sec:appendix-prompt}
\input{tables/prompt}

Three types of prompts are used in our experiments, as illustrated in Figure \ref{fig:model}. During the database construction phase, an explanation prompt is employed to generate explanations. In the prediction phase, a detection prompt is used to provide an initial explanation when no ground truth correction is available. The few-shot prompt template is used for the final few-shot inference incorporating demonstrations, and it is applied both in the comparative methods and the proposed method. Table \ref{tab:prompts} displays the actual prompts used in our experiments. For the detection prompt, we leveraged GPT-4o\footnote{\href{https://chatgpt.com}{https://chatgpt.com}} to generate both detailed and short version. As for the explanation prompt, one approach was to directly combine the detection prompt with the corrected sentence, which served as our primary method. In Section \ref{sec:exp_prompt_influence}, we also explored an edit-guided explanation prompt. The few-shot prompt was formulated based on prior work \cite{tang-etal-2024-ungrammatical, davis-etal-2024-prompting}.



\section{Case Study}
\label{sec:appendix-cases}
In this section, two cases are presented in which explanation-based retrieval exerts a significant influence in improving the prediction. Due to space limitations, for each input, only the detection result and the explanation of the first retrieved sample will be shown. In Figures \ref{fig:case-russian} and \ref{fig:case-english}, we present the detection results generated from the input sentences. Additionally, we illustrate the retrieval results of erroneous samples using explanation-based retrieval and input-based ("Semantic" method) retrieval, along with the few-shot prediction results when using the retrieved examples as context.

As shown in Figure \ref{fig:case-russian}, the input Russian sentence contains errors in verb conjugation and adjective declension. The detection result produced by the LLMs (referred to as the initial explanation in the proposed method) accurately identified these errors. The first sample retrieved based on this explanation not only includes both types of errors but also shares similar structure around the erroneous part with the input sentence. In contrast, samples retrieved based on the input alone contained different types of errors, which led to unsuccessful correction of the adjective declension in particular. Using the samples retrieved based on the explanation, all errors were correctly corrected.

In practice, the results of detection and explanation-based retrieval are not always perfect; however, they can still improve model performance in certain scenarios. As shown in Figure \ref{fig:case-english}, which illustrates a case in English GEC, the detection result includes multiple errors identified by LLMs. Among these, the "wordy" error is accurately identified and required correction. The first sample retrieved using this initial explanation indeed contained a similar error, with the segment highlighted by an underline to indicate the similarity of GEE. In contrast, samples retrieved based on the input do not contain similar errors, resulting in no modifications being made in the final few-shot prediction. Utilizing the sample retrieved based on the explanation, the few-shot prediction successfully corrected the "wordy" error of "will be likely" but failed to correct the preposition error at the end of the sentence. Interestingly, despite multiple errors being detected, the LLMs did not exhibit a tendency towards overcorrection under the influence of in-context learning. This observation partly explains why incorporating explanations into the context, as discussed in Section \ref{sec:in-context-explanation}, will lead to a degradation in performance compared to simply placing samples in context.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/NAACL25_russian_example.drawio.pdf}
    \caption{An example of the retrieval and generation result from the Russian GEC.}
    \label{fig:case-russian}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/NAACL25_english_example.drawio.pdf}
    \caption{An example of the retrieval and generation result from the English GEC.}
    \label{fig:case-english}
\end{figure*}

\end{document}
