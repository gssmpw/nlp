\section{Related Work}
\label{sec:related work}

\subsection{Network Tomography}
Network Tomography involves inferring internal network characteristics using performance metrics, which can be broadly classified as additive or non-additive. \textit{Additive metrics} frame the network tomography problem as a linear inverse problem, often assuming a known network topology and link-path relationships \citep{gurewitz2001estimating,liang2003maximum,chen2010network}. Statistical methods such as Maximum Likelihood Estimation (MLE) \citep{wandong2011research,teng2024learning}, Expectation Maximization (EM) \citep{bu2002network,wei2007mobile,wandong2011research}, and Bayesian estimation \citep{zhang2006origin,wandong2011research} are employed to solve this problem. Algebraic approaches, such as System of Linear Equations (SLE) \citep{bejerano2003robust,chen2003tomography,gopalan2011identifying} and Singular Value Decomposition (SVD) \citep{chua2005efficient,song2008netquest}, that rely on traceroute work well in certain scenarios but are often blocked by network providers to maintain the confidentiality of their routing strategies. When link performance metrics are sparse, compressive sensing techniques are used to identify all sparse link metrics \citep{firooz2010network,xu2011compressive}. Furthermore, studies have explored the sufficient and necessary conditions to identify all link performance metrics with minimal measurements \citep{gopalan2011identifying,alon2014economical}. \textit{Non-additive metrics}, such as boolean metrics, introduce additional complexity and constraints. These studies often assume that multiple simultaneous failures are rare, focusing on identifying network bottlenecks \citep{bejerano2003robust,horton2003number}. However, the assumption of rare simultaneous failures is not always valid. Some works address this by identifying the minimum set of network failures or reducing the number of measurements required \citep{duffield2006network,zeng2012automatic,ikeuchi2022network}. Additionally, several papers have proposed conditions and algorithms to efficiently detect network failures \citep{he2018distributed,nicola2018tight,bartolini2020fundamental,ibraheem2023network}, and some studies have attempted to apply deep learning to this field \citep{ma2020neural, sartzetakis2022machine, tao2023network}. However, most existing works rely on hand-crafted rules and specific assumptions, making them specialized for certain applications and unsuitable where prior knowledge of network properties or topology is unavailable.

\subsection{GNNs for Graph Structure Learning}
GNNs for Graph Structure Learning can be classified into approaches for learning discrete graph structures (i.e., binary adjacency matrices) and weighted graph structures (i.e., weighted adjacency matrices). Discrete graph structure approaches typically sample discrete structures from learned probabilistic adjacency matrices and subsequently feed these graphs into GNN models. Notable methods in this category include variational inference \citep{chen2018structured}, bilevel optimization \citep{franceschi2019learning}, and reinforcement learning \citep{kazemi2020representation}. However, the non-differentiability of discrete graph structures poses significant challenges, leading to the adoption of weighted graph structures, which encode richer edge information. A common approach involves establishing graph similarity metrics based on the assumption that node embeddings during training will resemble those during inference. Popular similarity metrics include cosine similarity \citep{nguyen2010cosine}, radial basis function (RBF) kernel \citep{yeung2007kernel}, and attention mechanisms \citep{chorowski2015attention}. While graph similarity techniques are applied in fully-connected graphs, graph sparsification techniques explicitly enforce sparsity to better reflect the characteristics of real-world graphs \citep{chen2020reinforcement, jin2020graph}. Additionally, graph regularization is employed in GNN models to enhance generalization and robustness \citep{chen2020iterative}. In this work, we leverage GNNs to learn end-node pair representations, enabling simultaneous prediction of path performance metrics and inference of the network topology.

\begin{figure*}[tb]
  \centering
  % \captionsetup{font=scriptsize} % Font size for the main captions
  \includegraphics[width=\textwidth]{figs/flow.png}
  \caption{Overall framework of proposed deep network tomography solution.}
  \label{Flow}
  \vspace{-7pt}
\end{figure*}