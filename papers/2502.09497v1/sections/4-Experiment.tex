\section{Experiments}
\label{experiments}
\subsection{Datasets \& Linguistic Features}
\label{dataset-and-evaluation-metrics}
We conducted our experiments on two widely used essay-grading datasets:

\paragraph{ASAP}
\label{asap-and-asap++}
ASAP (\cite{asap-aes}) is one of the most widely used evaluation datasets for AES, with 12,980 essays written by students in grades 7 through 10. It is divided into 8 subsets based on the formulation of essay prompts, including argumentative (1, 2), source-dependent (3, 4, 5, 6), and narrative (7, 8) essay prompts. The scores are annotated by at least two graders to ensure validity. We split the ASAP dataset into 5 equally sized folds. We use 3 folds for training (BERT baseline fine-tuning), 1 fold for validation (BERT baseline model selection and prompt tuning for prompting method), and 1 fold for evaluation.

\paragraph{ELLIPSE}
\label{ellipse}
\textit{English Language Learners Insight, Proficiency and Skills Evaluation} (\textit{ELLIPSE}) by \cite{crossleyEnglishLanguageLearner2024} is composed of 6483 essays from English language learners in the United States educational system's grades 8-12 get from an unnamed standardized test. The dataset features the full essay of each student and demographic information such as race/ethnicity and income background. There are 29 different argumentative essay prompts covering topics such as cell phones in school and community service. Each essay includes an overall holistic score as well as six rubric-based scores, all scored from one to five (in intervals of 0.5) by one of two annotators (see Appendix \ref{ellipse-rubric} for the full rubric). Note that we do not split the data since we want to treat this dataset as an entirely out-of-distribution dataset to examine the generalizability of our method across different essay prompts and contents.

\paragraph{Linguistic Features}
\label{linguistic-features}

While the ASAP dataset had these features pre-calculated thanks to the aforementioned prior work, the ELLIPSE dataset needed a custom pipeline to re-implement these features. We followed \cite{ridleyAutomatedCrosspromptScoring2021} as exactly as possible to recreate the features from Section \ref{linguistic-features-2} as closely as possible. See Appendix \ref{app:linguistic-features} for the implementation details.

\subsection{Evaluation Metrics}
For both ASAP and ELLIPSE, the most important task is to predict the overall scoring. We follow the Kaggle competition evaluation metric of \cite{asap-aes} and use Quadratic Weighted Kappa (QWK) to measure the agreement between the predicted and annotated overall scores.

% Since we conducted not only holistic prompting but also fine-grained prompting, we also evaluate the fine-grained prompting with traits level QWK (\texttt{QWK-Fine-grained}) when the fine-grained scores of each trait is available (i.e., on ASAP++ rubric and ELLIPSE original rubric). 

\subsection{Models and Experiment Setup}
\label{models-and-experiment-setup}

\paragraph{Our Pipeline}
\label{our-pipeline-(Mistral7B)}
As mentioned in Sec. \ref{methods}, our main method consists of a prompt construction module and a zero-shot prompting module with an LLM followed by a parsing module powered by another LLM. For prompt construction, we include three setups to integrate linguistic features: the most correlated feature (i.e., unique word count), the top 3 correlated features (i.e., unique word count, lemma count, and complex word count), and all 10 features, described in Sec. \ref{linguistic-features}. For prompting and parsing, we use \texttt{Mistral-7B-Instruct-v0.2} (\cite{jiang2023mistral7b}) for all LLM-related jobs, and we use 0 temperature and 4096 max token length, following \cite{stahl-etal-2024-exploring}\footnote{In early experiments, Llama 3 results are significantly worse than others; so we drop it and keep Mistral.}. We use default sampling parameters in \texttt{vllm} framework during decoding\footnote{\url{https://docs.vllm.ai/en/v0.6.1.post1/dev/sampling_params.html}}. All experiments are conducted with the VLLM\footnote{\url{https://docs.vllm.ai/en/v0.6.1.post1/index.html}} (\cite{kwon2023efficient}) on a single-card NVIDIA L40S device. 

% \joey{prompt tuning}
\paragraph{Unsupervised Baseline (GPT-4)}
\label{unsupervised-baseline-(gpt4)}
We also include one of the most capable models available to us, GPT-4 (\texttt{gpt-4-0613}), as the strong, unsupervised baseline. The experiment setup is exactly the same as in our pipeline above, with two differences, both due to the limited budget: 1) we only experiment with no more than 500 randomly sampled essays per essay set in ASAP (some essay sets have less than 500 essays in the test set) and 500 randomly sampled essays in ELLIPSE; 2) we only experiment with no linguistic feature and the best-performing linguistic feature setup (i.e., all top 10 features) based on the performance on the \texttt{dev} set of ASAP.

\paragraph{Supervised Baseline (BERT)}
\label{supervised-baseline-(bert)}
We also experimented a supervised method on ASAP to establish a performance upper bound. Our supervised baseline is a BERT-based architecture utilizing three main feature classes: document-, token- and segment-scale features~\cite{wang-etal-2022-use}. We base our fine-tuning on the authors' available code. The authors only released the fine-tuned model for ASAP prompt 8, so---to obtain models for all prompts---we fine-tuned \texttt{bert-base-uncased} as they did in the original paper. We used our splits of the ASAP dataset for fine-tuning, validation and testing (see \ref{asap-and-asap++}). More details about fine-tuning is in Appendix \ref{supervised-baseline-details}.
% Our supervised baseline is a BERT-based architecture comprised of two sub-components---each pretrained BERT models (\cite{devlin})---which analyze three main feature classes: document-, token- and segment-scale features. The first sub-component receives the document- and token-scale features~\cite{wang-etal-2022-use}. It is fine-tuned to learn the document-scale feature representation through the \texttt{[CLS]} (start) token\footnote{There can be multiple text segments per essay as their input length is set to 510.} and the token-scale features through the BERT word embeddings. Its output goes through a final max pooling layer to represent the sub-component's score. The segment-scale features are received by the second sub-component, which takes in an essay as a series of segments each of size $k$ (except the last segment, which is smaller). A list of these segment series of varying sizes $k_{i}$ are input into the model sequentially, and a final LSTM and attention and dense pooling layer is used to output the sub-component's score. Lastly, the output from the two sub-components are added together to produce the final holistic score. The model's loss function is additive between mean squared error ($MSE$), cosine similarity ($CS$) and margin ranking loss ($MLR$): $\mathcal{L}_{\mathrm{Total}}(\mathbf{x}, \mathbf{y}) = \alpha \mathcal{L}_{MSE}(\mathbf{x}, \mathbf{y}) + \beta \mathcal{L}_{CS}(\mathbf{x}, \mathbf{y}) + \gamma \mathcal{L}_{MLR}(\mathbf{x}, \mathbf{y})$.

% We base our fine-tuning on the authors' available code\footnote{\url{https://github.com/AlejandroCiuba/Multi-Scale-BERT-AES}.}. The authors only released their fine-tuned model for ASAP prompt 8, so---to obtain models for all prompts---we fine-tuned \texttt{bert-base-uncased} as they did in the original paper. We used our splits of the ASAP dataset for fine-tuning, validation and testing (see \ref{asap-and-asap++}). We fine-tune for 80 epochs, our hyperparameters for $\alpha$, $\beta$ and $\gamma$ were all set to 0.5 and with cosine similarity \texttt{dim=1} and margin ranking loss \texttt{margin=0}. Everything is implemented in PyTorch (\cite{paszke2019}) and HuggingFace (\cite{wolf2019}) using \texttt{google-bert/bert-base-uncased}. We run the test set on the prompt's model with the best loss.

