\section{Conclusion}
\label{conclusion}
In this work, we explore the combination of linguistic features and zero-shot prompting with SoTA LLMs in the task of automatic essay scoring. Empirical experiments show performance improvement when linguistic features are integrated into the zero-shot prompt. However, the performance improvement varies depending on the type of the essay, proving the challenging nature of generalizability in AES systems. We hope our work can serve as a starting point for the research in more interpretable and more generalizable LLM-based AES methods.

% To make quality cross-prompt AES systems, we need to leverage linguistically-motivated features and build upon the latest our field has to offer. We factor in both these considerations by comparing prompt-tuned LLMs with linguistic features to a fine-tuned baseline. As our experiments show, exploring the combination of LLM prompt-tuning and hand-crafted linguistic features is a promising area of research as linguistic features improved performance across both LLMs. We hope our work is the starting point of what is to be a budding overlap between LLM AES research and AES feature engineering.
