\section{Introduction}
\label{introduction}
From the keystone work of Page with ``Project Essay Grader'' to the start of industry-scale automatic essay grading (AES) back in the late 90s (\cite{hearst2000}), automatic essay grading has shown to be a field ripe with research potential. The complexities of accurately grading nuanced essays and varied writing styles poses a great benefit to teachers, who---even all the way back in 1968, \textit{``. . . [were] often overworked and underpaid, harassed by mounting piles of student themes, or twinged with guilt over not assigning enough for a solid basis of student practice and feedback''} \cite{page1968}. To create systems that has sufficient capabilities to aid teachers in teh essay grading task requires us to make systems that examine more than just simple word surface forms and incorporate more linguistically motivated features (e.g., \cite{burstein1998}). We see this in recent work such as \cite{ridleyPromptAgnosticEssay2020} and \cite{uto-etal-2020-neural}, where linguistic features are paired with supervised methods to bolster AES quality. However, there is little work on exploring linguistic features in the context of instruction-based large language models (LLMs), with most work focusing on other features such as rubric-incorporation \cite{hashemi-etal-2024-llm} and prompting techniques (e.g., \cite{liu-etal-2023-g} and \cite{chiang-lee-2023-closer}. Therefore, it is natural to explore the possibility of combining the merits of both.

In our work, we address this gap by integrating linguistic features as part of the zero-shot prompt for LLMs. Via experiments in both in and out of distribution data, we find that LLMs align better with human judgments when given linguistic features. Our main contribution can be summarized as follows: 1) through prompt tuning and feature engineering, we have shown incorporating linguistic features into existing zero-shot prompting methods can notably improve the overall score prediction; 2) even for out-of-distribution data (i.e., essay from an entirely different dataset), the improvement holds; 3) there is still notable headroom for open-source LLM to automatically evaluate student essay, compared to their closed-source counterparts and smaller, supervised language models; we hypothesize that it is due to poorly-calibrated prior that is built-in to the LLM.