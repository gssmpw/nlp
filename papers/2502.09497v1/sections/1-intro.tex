\section{Introduction}
\label{introduction}
Research in Automatic Essay Scoring (AES), the task of automatically assessing the quality of an essay, dates back to over five decades ago \cite{page1968}. Since then, researchers in this domain have taken various perspectives; some focus on building hand-crafted features \cite{chen-he-2013-automated, uto-etal-2020-neural}, some leverage the computational power of neural-network to learn effective representation of essays \cite{dong-etal-2017-attention, ridley2020promptagnosticessayscorer, jin-etal-2018-tdnn}, and some adapt pre-trained language models as the starting point for fine-tuning \cite{wang-etal-2022-use, hierarchicalbert}. 

However, as much of the research above has shown, AES remains an open question. The key challenge of this task lies in the balance between generalization and specification: ideally, the method should be applicable to any grading scenario given a concrete grading rubric. Yet, numerous factors, including but not limited to instructors, education institutions, the essay's purpose, and the type of the essay (from the literature point of view) make essay grading context-specific. To this end, the cross-prompt AES system, which aims to work similarly well for different essay prompts and scoring rubrics, has been an important direction that draws much attention~\cite{ridley2020promptagnosticessayscorer, jin-etal-2018-tdnn, li-ng-2024-conundrums, phandi-etal-2015-flexible,Ridley_He_Dai_Huang_Chen_2021}.

% One of the most popular lines of work for cross-prompt AES systems is the use of large language models (LLMs). LLMs demonstrate great adaptability to various tasks, even under zero-shot prompting settings~\cite{mansour-etal-2024-large, stahl-etal-2024-exploring, mutlitaskAESforEssayGrading}. Although they have shown decent performance without training or fine-tuning, these LLM-based methods still fall short in terms of interpretability due to the black-box nature of LLMs. In our work, we still try to leverage the power LLMs but with linguistic features as part of the zero-shot prompt. In this way, we hope to improve the inteprebility of the overall pipeline. Through limited prompt-tuning and feature selection, we conduct experiments on both open (Mistral) and closed-source large language models (GPT-4) and find that LLMs align better with human judgments when given linguistic features via experiments in both in and out of distribution data. 

 Creating cross-prompt AES systems with sufficient capabilities requires the system to examine more than just simple word surface forms and incorporate more linguistically motivated features (e.g., \cite{burstein1998}). Recent work (\cite{ridleyPromptAgnosticEssay2020, uto-etal-2020-neural}) pairs linguistic features with supervised methods to boost AES quality. However, there is little work on exploring linguistic features in the context of instruction-based large language models (LLMs), with most work focusing on other features such as rubric-incorporation \cite{hashemi-etal-2024-llm} and prompting techniques (e.g., \cite{liu-etal-2023-g} and \cite{chiang-lee-2023-closer}. To bridge the gap, we explore adding linguistic features to the LLM prompt. We conduct experiments in the cross-prompt AES setting with both open- (Mistral) and closed-source (GPT-4) large language models and find that LLMs align better with human judgments when given linguistic features. 
 
Our main contribution can be summarized as follows: 1) through prompt tuning and feature engineering, we have shown incorporating linguistic features into existing zero-shot prompting methods can notably improve the overall score prediction; 2) even for out-of-distribution data (i.e., essay from an entirely different dataset), the improvement holds; 3) there is still notable headroom for open-source LLM to automatically evaluate student essay, compared to their closed-source counterparts and smaller, supervised language models; we hypothesize that it is due to poorly-calibrated prior that is built-in to the LLM.