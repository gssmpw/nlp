\section{Methods}
\label{methods}
\subsection{Zero-shot Prompts with LLM}
We build on top of the prompt template and instruction strategy by \cite{stahl-etal-2024-exploring} for our zero-shot prompt design. Each prompt follows the following structure: \textbf{persona pattern}, \textbf{essay prompt}, \textbf{analysis task}, \textbf{student's essay}, \textbf{additional information}, and \textbf{format instruction}. We adopt the best-performing combination of each component based on \cite{stahl-etal-2024-exploring}, i.e., \textit{educational researcher} as the prompt template, \textit{Explanation â†’ Scoring} as the analysis instruction. We also add the \textbf{additional information} section to incorporate linguistic features of the essay (see Sec. \ref{linguistic-features-2}). Below is the prompt template we use. More details about the prompt structure and examples can be found in Appendix \ref{app-scoring-prompts}.
\begin{center}
\footnotesize
\begin{quote}
You are part of an educational research team analyzing the writing skills of students in grades 7 to 10. You have been given a student's essay and the prompt they responded to.

\#\#\# Essay Prompt: \textit{\{ essay prompt \}}

\#\#\# Analysis Task: \textit{\{ analysis instruction \}}

\#\#\# Analyzed Student Essay: \textit{\{ essay \}}

\#\#\# Additional Information: Studies show that the following features are highly, positively correlated with the grade of the essay (i.e., higher features typically mean higher end score): \textit{\{ linguistic features \}}

\#\#\# Analysis: Conclude your analysis with a grade and comments in the following format: \textit{\{ format instruction \}}
\end{quote}
\end{center}

\subsection{Linguistic Features}
\label{linguistic-features-2}
In addition to the naive zero-shot baseline, we experimented with incorporating linguistic features into the prompts. We base our model's linguistic features off of \cite{ridleyPromptAgnosticEssay2020}, which were additionally used by \cite{li-ng-2024-conundrums} and cited as being some of the most impactful features in essay grading, having a Pearson's correlation score of 0.6 or above with the essay's score. While the original works contain much more linguistic features, limiting our linguistic features helped avoid overly long prompts, which could negatively impact LLM performance. The linguistic features we used are listed below.

\textbf{Unique Words} refers to the number of single-instance words in the essay. For \textbf{essay character length}, we only count the number of non-space, non-punctuation characters\footnote{Words are not normalized as in the original paper.}. \textbf{Word/Sentence counts} are the total number of words/sentences in a given essay. We get the \textbf{individual counts for the total number of lemmas, nouns, and stop-words.} Finally, we get the essay's \textbf{Dale-Chall} (\cite{dale1948}) word count, \textbf{total character count} (all characters) and \textbf{long word count.}
% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%     Component &  \\
%     \hline
%          & 
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

% We propose two main improvements on existing LLM-based AES system (\cite{stahl-etal-2024-exploring}): fine-grained prompting and the inclusion of linguistic features, both of which will be discussed in more detail below.
% \subsection{Fine-grained Prompting with LLM}
% \label{fine-grained-prompt}
% % \subsubsection{Holistic Prompt}
% Unlike holistic prompting (\cite{stahl-etal-2024-exploring}), where an overall score is obtained with one single prompt, the fine-grained prompting strategy aims at getting the overall score of a given essay by multiple independent prompting to the same LLM, each focusing on one of the grading traits. The scores for all traits are then accumulated to make a final overall score prediction (to simplify the process, we use a simple average of the fine-grained scores as the overall score prediction). 

% In practice, since each essay prompt focuses on different aspects of writing, the fine-grained traits vary across essay sets. Hence, how to break down the overall score into a combination of multiple scoring traits is a critical part of effective fine-grained prompting. We present three different ways of getting fine-grained traits: 
% \begin{itemize}
%     \item \textbf{Manually Collected} For each essay set, there is a grading rubric for annotators as a reference (see Appendix ?? for an example). Based on that document, one of the researchers manually break down the grading into several traits. Note that, all the traits and one-sentence descriptions of the traits are directly from the rubric file. The only cognitive process involved here is identify and re-organizing information.
%     \item \textbf{GPT4-generated} Given the rising capability of LLMs in synthesizing information, we also try to use GPT-4o \footnote{https://openai.com/index/hello-gpt-4o/} to extract fine-grained rubrics from grading rubric document (see Appendix ?? for zero-shot instruction prompt). The resulting fine-grained traits and descriptions are recorded into the same format as manually-collected rubrics.
%     \item \textbf{ASAP++} Previously, \cite{mathias-bhattacharyya-2018-asap} has already proposed the idea of fine-grained traits in essay scoring. We also include their traits and descriptions as another setting of fine-grained prompting. Note that the main difference between the traits from ASAP++ and from previous two is that the traits from ASAP++ are all prompt-independent, which means they are generic perspectives to score an essay. In contrast, both manual collected and GPT4-generated traits are essay prompt dependent and are naturally more specific.
% \end{itemize}

% \begin{itemize}
%     \item \textbf{Unique Words:} The number of single-instance words in the essay, more formally known as hapax legomena.
%     \item  \textbf{Essay Character Length:} The number of non-space, non-punctuation characters.
%     \item \textbf{Word/Sentence Count:} The number of words/sentences in the essay via \texttt{nltk} (\cite{loper2002}) tokenizers.
%     \item \textbf{SpaCy Counts:} The number of lemmas, nouns and stop-words as parsed by \\ \texttt{en\_core\_web\_sm} in \texttt{spaCy} (\cite{honnibal2020}).
%     \item \textbf{Readability Measures:} The Dale-Chall (\cite{dale1948}) word count, total character count and long word count as gotten by the \texttt{readability}\footnote{\url{https://pypi.org/project/readability/}} Python package.
% \end{itemize}

% Although previous work includes a much longer list of linguistic features, we only include the ones described above because they have above 0.6 Pearson correlation coefficient with the ASAP ground truth scores, based on \cite{li-ng-2024-conundrums}. The limited number of linguistic features also helped avoid overly long prompts, which could negatively impact LLM performance.

During prompt construction, the linguistic features are formatted as an unordered list of a short feature description followed by feature value. The formatted text containing all linguistic features is inserted into the prompt as the \textbf{additional information} section. 

\subsection{Output Parsing via LLM}
The output format varies across essay sets since each essay set has its own scoring schema. To make the pipeline generic to any input, we implement a few-shot parsing module powered by a stand-alone LLM. More details can be found in Appendix \ref{app-parsing}.