\section{Related Works}
\label{related-works}
\subsection{Automatic Essay Scoring}
\label{automatic-essay-scoring}
% Recent studies in AES can be roughly divided into the following two perspectives.

\paragraph{Feature Engineering} approaches leverage various features to predict essay scores, including linguistic features, e.g., readability metrics and word length ~\cite{ridley2020promptagnosticessayscorer, uto-etal-2020-neural, jin-etal-2018-tdnn, FoltLahaLand19993j, chen-he-2013-automated}, and content features, e.g., content quality and organization ~\cite{mathias-bhattacharyya-2018-asap, crossley2023english}. Models that utilize these features range from simple logistic regression models ~\cite{chen-he-2013-automated} to deep neural networks ~\cite{uto-etal-2020-neural}. These approaches assess the quality of essays in an interpretable manner with well-defined features.

\paragraph{Language-model-based} approaches emerge with the rising popularity of Transformer architecture, including BERT-based methods that require supervised fine-tuning \cite{wang-etal-2022-use, mutlitaskAESforEssayGrading, hierarchicalbert} and LLM-based methods that focus on prompt-engineering \cite{mansour-etal-2024-large, stahl-etal-2024-exploring}. In particular, \cite{stahl-etal-2024-exploring} explores zero-shot prompting with persona prompts and analysis instructions. Building on this, our work aims to utilize linguistic features in LLM prompting.

\subsection{LLM as Evaluator}
\label{llm-as-evaluator}
Given the increasing capability of LLMs and their scalable nature, researchers in various domains have explored how to use them for the automatic evaluation of text content \cite{zubiaga-etal-2024-llm, alhafni-etal-2024-personalized, gao2024llmbasednlgevaluationcurrent, fu-etal-2024-gptscore}. Although some research has shown proper prompt tuning, such as explanation-guided generation, clear rubric guidance, and chain-of-thought (COT) could improve the alignment between human and LLMs \cite{chiang-lee-2023-closer, liu-etal-2023-g, hashemi-etal-2024-llm}, the LLM-based evaluators still perform underwhelming in more complex tasks, such as reviewing papers \cite{zhou-etal-2024-llm} and scoring students essay \cite{mansour-etal-2024-large, stahl-etal-2024-exploring}. In this work, we specifically focus on improving LLMs as student essay graders by incorporating the linguistic features of essays. Additionally, we examine the transferability of the prompts, i.e., how a prompt that is tuned in the in-distribution data would perform out-of-distribution in the same task.