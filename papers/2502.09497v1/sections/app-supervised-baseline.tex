\section{Supervised Baseline Details}
\label{supervised-baseline-details}
Our supervised baseline~\cite{wang-etal-2022-use}\footnote{\url{https://github.com/lingochamp/Multi-Scale-BERT-AES}}is a BERT-based architecture comprised of two sub-components---each pretrained BERT models (\cite{devlin})---which analyze three main feature classes: document-, token- and segment-scale features. The first sub-component receives the document- and token-scale features. It is fine-tuned to learn the document-scale feature representation through the \texttt{[CLS]} (start) token\footnote{There can be multiple text segments per essay as their input length is set to 510.} and the token-scale features through the BERT word embeddings. Its output goes through a final max pooling layer to represent the sub-component's score. The segment-scale features are received by the second sub-component, which takes in an essay as a series of segments each of size $k$ (except the last segment, which is smaller). A list of these segment series of varying sizes $k_{i}$ are input into the model sequentially, and a final LSTM and attention and dense pooling layer is used to output the sub-component's score. Lastly, the output from the two sub-components are added together to produce the final holistic score. The model's loss function is additive between mean squared error ($MSE$), cosine similarity ($CS$) and margin ranking loss ($MLR$): $\mathcal{L}_{\mathrm{Total}}(\mathbf{x}, \mathbf{y}) = \alpha \mathcal{L}_{MSE}(\mathbf{x}, \mathbf{y}) + \beta \mathcal{L}_{CS}(\mathbf{x}, \mathbf{y}) + \gamma \mathcal{L}_{MLR}(\mathbf{x}, \mathbf{y})$.

We base our fine-tuning on the authors' available code\footnote{Available upon acceptance.}. The authors only released their fine-tuned model for ASAP prompt 8, so---to obtain models for all prompts---we fine-tuned \texttt{bert-base-uncased} as they did in the original paper. We used our splits of the ASAP dataset for fine-tuning, validation and testing (see \ref{asap-and-asap++}). We fine-tune for 80 epochs, our hyperparameters for $\alpha$, $\beta$ and $\gamma$ were all set to 0.5 and with cosine similarity \texttt{dim=1} and margin ranking loss \texttt{margin=0}. Everything is implemented in PyTorch (\cite{paszke2019}) and HuggingFace (\cite{wolf2019}) using \texttt{google-bert/bert-base-uncased}. We run the test set on the prompt's model with the best loss.
