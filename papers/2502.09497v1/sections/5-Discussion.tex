\section{Result and Discussion}
\label{discussion}
% Results on ASAP/ASAP++ and ELLIPSE are summarized in Table ?? and Table ?? \joey{add table here}. Note that 

\subsection{Cross-dataset Performances}
\label{cross-dataset-performances}
% All the experiment results are shown in Table~\ref{table:results}.
% For ASAP, the best-performing model by average QWK is the BERT-based supervised baseline. This match our intuition that AES for prompting LLMs is remains a hard task. We observed a mixed performance when comparing GPT4 and Mistral models without linguistic features for ASAP and ELLIPSE. 
The experimental results are summarized in Table~\ref{table:results}. For ASAP, the BERT-based supervised baseline achieved the highest average QWK, aligning with our expectation that AES remains a challenging task for naive LLMs prompting. When comparing cross LLMs, the performance between GPT-4 and Mistral showed mixed results across the two datasets.
% However, it is an entirely different story for ELLIPSE, where GPT-4 performs underwhelmingly compared to Mistral. To this non-intuitive observation,
% we hypothesis the Ellipse essays are argumentative and are similar only to the essay set 1 and 2 in ASAP (other essay sets in ASAP are not argumentative); second, the grading rubric of ASAP essay set 2 is more focused (i.e., ``Writing Application'' and ``Language Conventions'') than both ELLIPSE and ASAP essay set 1 (i.e., just ``Overall'' quality). Based on these two observations, LLMs' performances on ELLIPSE will be most similar to that on ASAP essay set 1 instead of the entire ASAP dataset, which is also confirmed by the result in the table. 
We suspect this is due to the differences in the essay type. ELLIPSE essays are argumentative, similar to essay sets 1 and 2 in ASAP. However, the grading rubric for ASAP essay set 2 is more specific (focusing on "Writing Application" and "Language Conventions") compared to the broader "Overall" quality criteria used for ELLIPSE and ASAP essay set 1. These observations suggest that LLM performance on ELLIPSE is more comparable to ASAP essay set 1 rather than the entire ASAP dataset, a pattern confirmed by the results in Table~\ref{table:results}. Another research \cite{mutlitaskAESforEssayGrading} that conducted a zero-shot prompting experiment on ELLIPSE with ChatGPT \footnote{\url{https://chatgpt.com/}} also got similar results (QWK = 0.29).

\subsection{Benefit of Linguistic Features}
\label{effect-of-linguistic-features}
As shown in Table \ref{table:results}, the prompts with linguistic features almost always perform better than the ones without -- with the exception of GPT-4. This trend holds even for out-of-distribution data (ELLIPSE). When it comes to Mistral 7B, for ASAP, \textit{Top-10} features are the most effective by both average and subset QWK measurement; for ELLIPSE, the best performing linguistic feature choice is only \textit{Top-3} feature, while the difference is marginal. In addition, the improvement brought by the linguistic features even pushes the Mistral performance on ASAP close to GPT-4 performance. Based on these observations, we can conclude that including linguistic features can benefit LLM-based zero-shot AES.

\begin{table*}
\small
\setlength{\tabcolsep}{3pt}
\centering
  \begin{tabular}{cc|ccccccccc|c}
    \toprule
    % \hline
    \multirow{2}{*}{\textbf{Model}} &
    \multirow{2}{*}{\textbf{\shortstack{Linguistic \\ Features}}} &
      \multicolumn{9}{c}{\textbf{ASAP}} & 
     \multirow{2}{*}{\textbf{ELLIPSE}} \\
        & & \textbf{Avg.} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & \\
        % && {$\uparrow$ \textit{Acc.}} & {$\uparrow$ \textit{ R (p-value)}} & { $\downarrow$ \textit{KL Divergence}} & {$\uparrow$ \textit{R (p-value)}} & {$\uparrow$ \textit{Acc.}}  \\
    \midrule
        \multirow{1}{*}{BERT}
            & None  & .545 & .741 & .447  & .331  & .430 & .734  & .552 & .715 & .413 & N/A  \\
    
    \midrule
        \multirow{2}{*}{\shortstack{GPT-4}}
            & None  & \textbf{.499} & .221 & .581  & \textbf{.514}  & \textbf{.631} & .561  & \textbf{.686} & .250 & \textbf{.553} & .307  \\
            & Top 10  & .488 & \textbf{.285} & \textbf{.592} & .444 & .578 &  \textbf{.620} & .645 & \textbf{.251} & .491 &  \textbf{.345}  \\

    \midrule     		
        \multirow{4}{*}{Mistral 7B}
            & None & .454 & .254 & .474 & \textbf{.526} & .549 &  .506 & \textbf{.567} & .388 & \textbf{.367} & .454  \\
            & Unique Word & .458 & .362 & \textbf{.516} & .454 & .552 &  .492 & .539 & .438 & .313 & .475  \\
            & Top 3  & .461 & .383 & .516 & .453 & .567 & .503 & .542 & .409 & .318 & \textbf{.481} \\
            & Top 10  & \textbf{.492} & \textbf{.423} & .483 & .493 & \textbf{.623} &  \textbf{.511} & .537 & \textbf{.508} & .360 & .468  \\ 
    \toprule
  \end{tabular}
  
  \caption{Results of our experiments; BERT model is trained on ASAP training set and not applicable to ELLIPSE; Bold numbers are best-performing setups of the model.}
  \label{table:results}
  % \vspace{-4mm}
\end{table*}