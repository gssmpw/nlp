% One of the main challenges in Automatic Essay Scoring (AES) is how to balance generalizability and essay-specific grading rubrics while also keeping the pipeline interpretable. Recent work has looked into this by incorporating linguistic features with supervised models to improve interpretability, while other work has looked into utilizing large language models (LLMs), factoring in a given essay's scoring rubric, to improve generalizability and rubric-specific alignment. Our work combines these two research paths, leveraging LLMs with prompt-tuning and hand-crafted linguistically-motivated features. We show that this approach improves model-annotator essay score alignment on both in and out-of-distribution essays with minimum prompt tuning \footnote{Code and data will be released upon acceptance.}.

Automatic Essay Scoring (AES) assigns scores to student essays, reducing the grading workload for instructors. Developing a scoring system capable of handling essays across diverse prompts is challenging due to the flexibility and diverse nature of the writing task. Existing methods typically fall into two categories: supervised feature-based approaches and large language model (LLM)-based methods. Supervised feature-based approaches often achieve higher performance but require resource-intensive training. In contrast, LLM-based methods are computationally efficient during inference but tend to suffer from lower performance. This paper combines these approaches by incorporating linguistic features into LLM-based scoring. Experimental results show that this hybrid method outperforms baseline models for both in-domain and out-of-domain writing prompts\footnote{Codebase: \url{https://github.com/JoeyHou/essay_eval}}.

% \footnote{\url{https://github.com/JoeyHou/essay_eval/tree/ling_features_joey}}.