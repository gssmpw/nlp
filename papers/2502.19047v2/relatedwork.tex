\section{Related Work}
\label{section:related-work}

\subsection{Backdoor Attacks in Diffusion Models}
While early diffusion models only contain a UNet~\cite{ronneberger2015u} for image generation, recent state-of-the-art diffusion models like stable diffusion further integrate conditional models (e.g., text encoders) connected to the UNet via cross-attention layers, enabling multi-modal constraints. Among the components, backdooring the UNet remains the most fundamental attack strategy, as explored in three notable studies: BadDiffusion~\cite{Chou2023CVPR}, TrojDiff~\cite{chen2023trojdiff}, and VillanDiffusion~\cite{chou2024villandiffusion}. 
To backdoor diffusion models, both BadDiffusion~\cite{Chou2023CVPR} and TrojDiff~\cite{chen2023trojdiff} rely on poisoning a subset of the training dataset, altering the diffusion process, and modifying the training objective. This approach gradually embeds a trigger into the prior noise distribution across multiple diffusion steps. Once backdoor training is complete, images generated from trigger-stamped noise become the desired target images. The key distinction between these two frameworks lies in their trigger schedules (i.e., parameters that control the scale of the trigger added at each diffusion step), resulting in different performance outcomes and attack scenarios. 
VillanDiffusion~\cite{chou2024villandiffusion}, on the other hand, extends the capabilities of BadDiffusion by generalizing backdoor attacks to various samplers (e.g., ODE and SDE samplers) and diffusion model categories (e.g., DDPM, DDIM, score-based models, and conditional models). This is achieved using numerical methods to compute a unified diffusion transition, allowing the formulation of generalizable attack strategies rather than relying on specific settings.
Despite these advances, all the presented backdoor attacks operate under black-box scenarios. They require customization for each target model and assume prior knowledge of the noise schedule, which limits their applicability and generalizability.

Several studies have explored backdooring diffusion models by targeting conditional models instead of the UNet. For instance, the authors in~\cite{struppek2023rickrolling} use non-Latin characters (e.g., \Smiley{}) as backdoor triggers, while the target is a specific text prompt such as ``a cat wearing glasses". By misleading the text embeddings to a specific target prompt before feeding them to diffusion models via cross-attention layers, the final generation results of diffusion models are manipulated correspondingly. Some other works also target on the textual modality to backdoor diffusion models~\cite{zhai2023text, pan2023trojan, wang2023stronger}. In general, they all share the same principle of poisoning a subset of the training data with a textual trigger and an associated backdoor target, differing primarily in how the trigger and target are selected. 
% For example, the work~\cite{zhai2023text} uses a special character [T] as trigger, while \cite{pan2023trojan} chooses a natural word among exisiting classes of the training dataset (e.g., ``deer" in CIFAR10) as trigger to make it imperceptible by human inspection. The targets of backdoor attacks are also diverse among the studies, such as object replacement~\cite{zhai2023text}, content drift~\cite{pan2023trojan}, and copyright infringement~\cite{wang2023stronger}.
Essentially, in these backdoor attacks, attackers only manipulate the training dataset without interfering in the diffusion processes and the training objectives.


\subsection{Backdoor Defenses for Diffusion Models}
Backdoor defense mechanisms for diffusion models typically involve two main stages: trigger inversion and backdoor detection. Trigger inversion is the first and foremost stage, in which suspicious diffusion models are examined based on essential diffusion properties to identify one or more candidate backdoor triggers. Typically, the trigger is often a learnable value which is learned via optimization techniques\cite{truong2024attacks}. Backdoor detection forms the second stage, where the candidate triggers and the outputs generated using these triggers are analyzed to determine whether the suspicious models have been backdoored. 

Elijah~\cite{an2024elijah} is the first backdoor defense framework that offers both trigger inversion and backdoor detection. For trigger inversion, it identifies a candidate trigger by analyzing the distribution shift at the first denoising step. Backdoor detection is performed by computing the total variance loss of generated samples and the absolute distance between them in the presence of the inverted trigger to determine if the model is backdoored. The authors argue that a model is more likely to be backdoored if both the total variance score and absolute distance are low.
% For backdoor removal, the authors introduce a loss function designed to realign the backdoored outputâ€™s distribution with the benign distribution. This benign distribution is obtained by feeding benign inputs (i.e., normal noise) into the frozen diffusion model. 
However, our experiments show that Elijah's trigger inversion method becomes inefficient when handling difficult trigger patterns. This limitation arises because the framework focuses solely on the first denoising step, missing critical information about the distribution shifts that occur across the other steps.
Another work named TERD\cite{mo2024terd} proposed a different approach for trigger inversion, which contains two steps: trigger estimation and trigger refinement. Trigger estimation is based on a loss function that compares the difference between denoising results from two different input noises in the presence of the learnable trigger. Trigger refinement technique uses the estimated triggers to generate potential backdoor targets. Then, the authors assume that the generated potential targets are such the true targets, thus using them to improve the quality of the estimated triggers. Regarding backdoor detection, instead of analyzing samples generated by the inverted trigger as in Elijah, TERD analyzes the inverted trigger itself to determine if a model is backdoored. However, TERD's trigger inversion results in low performance for difficult trigger-target pairs, while its backdoor detection could be bypassed if the trigger is intentionally constrained to resemble a normal noise\cite{li2023learnable}. On the other hand, our framework PureDiffusion offers high performance for both trigger inversion and backdoor detection even with challenging triggers which could not be detected by both Elijah and TERD. 

DisDet~\cite{sui2024disdet} and UFID~\cite{guan2024ufid} are only designed for backdoor detection, neglecting the trigger inversion stage. This means that both framework assume that there are already candidate triggers to be validated, which is an impractical assumption. Other studies such as T2Ishield~\cite{wang2025t2ishield} and CopyrightShield~\cite{guo2024copyrightshield} focus on defending conditional diffusion models by analyzing the text encoders and attention layers. Therefore, these framework cannot resist backdoor attacks targeting on the UNet model, which is the main goal of our work.

\subsection{Discussion}
Our work primarily serves as a defense against backdoor attacks targeting denoising UNet models. To the best of our knowledge, only Elijah \cite{an2024elijah} and TERD \cite{mo2024terd} have addressed this issue with practical solutions. However, both frameworks perform well only on default triggers (e.g., a white square box) and struggle with more complex ones.

Preliminary results of this work were published in \cite{Vu2025ICC}; however, this journal version makes several significant extensions compared to the conference version as can be summarized as follows. First, analysis of the trigger shifts  for three different backdoor attacks, namely BadDiffusion \cite{Chou2023CVPR}, TrojDiff \cite{chen2023trojdiff}, and VillanDiffusion \cite{chou2024villandiffusion} are performed in this version while only the analysis for BadDiffusion is conducted in \cite{Vu2025ICC}. Second, while we employ only a single loss function in  \cite{Vu2025ICC}, two different loss functions are employed for trigger inversion in this version for enhanced performance. Third, much more extensive experimental studies and results are presented in this version where we study various diffusion models (both DDPM-based and score-based diffusion models). %compared to those in the conference version.
Finally, we conduct the ablation study, complexity, and sensitivity analysis in this version, which are not pursued in \cite{Vu2025ICC}.

%In our previous work \cite{truong2024purediffusion}, we introduced an efficient method to estimate backdoor triggers by analyzing distribution shifts across multiple denoising steps. This paper extends that framework by incorporating a novel denoising-consistency loss ($L_{DC}$), expanding our evaluation to various diffusion models (both DDPM-based and score-based) and multiple backdoor attacks, including BadDiffusion \cite{Chou2023CVPR}, TrojDiff \cite{chen2023trojdiff}, and VillanDiffusion \cite{chou2024villandiffusion}. Additionally, we demonstrate an unexpected application of PureDiffusion for optimizing backdoor triggers, significantly enhancing attack efficiency.

Our study focuses exclusively on backdoor attacks targeting the UNet, without considering attacks on other components, such as modality encoders (e.g., text encoders).