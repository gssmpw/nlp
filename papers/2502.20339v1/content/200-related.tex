\section{Related Work}
\label{sec:related}


\subsection{Scaling Inference Time Compute for Reasoning}
Scaling inference time compute has emerged as a promising strategy to improve the performance of LLMs.
Techniques such as Chain of Thought (CoT) and its variants have demonstrated significant performance improvements across various reasoning benchmarks by decomposing complex tasks into intermediate steps~\citep{wei2023chainofthoughtpromptingelicitsreasoning, yao2023treethoughtsdeliberateproblem} 

While these approaches improve reasoning through task decomposition, they also increase computational demands due to longer generation sequences. Recent work suggests that this additional compute may itself contribute to improved model abilities~\citep{pfau2024letsthinkdotdot}. Dynamic compute allocation during inference has further advanced this paradigm. For instance, \citet{goyal2024thinkspeaktraininglanguage} introduced pause tokens into the LLM vocabulary, enabling models to allocate compute more effectively and achieve better reasoning and task performance.

Another prominent approach involves generating and searching through multiple model outputs to select the best answer. Various sampling algorithms have been proposed to increase the diversity and quality of generated outputs to increase the likelihood of the correct or best answer being selected~\citep{wang2023selfconsistencyimproveschainthought, renze2024effectsamplingtemperatureproblem, zhang2023planninglargelanguagemodels}. In parallel, outcome and process reward models (ORMs and PRMs) have been introduced to help evaluate the best response and guide intermediate generation steps within the LLM model~\citep{lightman2023letsverifystepstep, zhang2024restmctsllmselftrainingprocess, luo2024improvemathematicalreasoninglanguage, uesato2022solvingmathwordproblems}. 


Recent work has shown that smaller LLMs, when scaled through inference-time compute (e.g., via majority voting or PRM-guided search), can outperform larger models under fixed compute budgets~\citep{snell2024scalingllmtesttimecompute, wu2024inferencescalinglawsempirical, beeching2024scalingtesttimecompute}. However, these findings are primarily limited to Transformer-based architectures. The extent to which these scaling laws apply to subquadratic architectures, which offer faster inference but may trade off expressiveness, remains underexplored.

\subsection{Subquadratic Architecture Alternatives}
While Transformers dominate the landscape of reasoning models~\citep{grattafiori2024llama3herdmodels, qwen2025qwen25technicalreport}, alternative architectures have been proposed to mitigate their high computational cost.
These models, based on RNNs~\citep{beck2024xlstmextendedlongshortterm, peng2023rwkvreinventingrnnstransformer}, SSMs~\citep{gu2022efficientlymodelinglongsequences, gu2024mambalineartimesequencemodeling}, and linear attention mechanisms~\citep{katharopoulos2020transformersrnnsfastautoregressive, yang2024gatedlinearattentiontransformers}, offer improved inference and memory efficiency especially for long-context tasks and large-batch generation, making them attractive for large-scale language modeling. 
Notably, the Mamba family of models (Mamba-1 and Mamba-2) has introduced selective state spaces, enabling linear-time sequence modeling without sacrificing performance~\citep{gu2024mambalineartimesequencemodeling, dao2024transformersssmsgeneralizedmodels}. Hybrid architectures that combine subquadratic layers (e.g., Mamba) with a limited number of self-attention layers have also emerged, achieving superior performance compared to pure Transformer or subquadratic models~\citep{lieber2024jambahybridtransformermambalanguage, ren2024sambasimplehybridstate, dong2024hymbahybridheadarchitecturesmall}. These architectures are particularly well-suited for the increased compute demands of inference-time scaling. Our work evaluates the inference-time scaling properties of both pure and hybrid subquadratic models. 

\subsection{Knowledge Distillation}
Knowledge distillation has proven effective in transferring capabilities from large teacher models to smaller, more efficient student models~\citep{hinton2015distillingknowledgeneuralnetwork}. In the context of LLMs, distillation is commonly used to compress a larger pre-trained LLM into a smaller version while maintaining core knowledge and functionality~\citep{gu2024minillmknowledgedistillationlarge, xu2024surveyknowledgedistillationlarge}.
Although larger models exhibit better reasoning and overall abilities due to the properties of scale~\citep{xu2025largereasoningmodelssurvey, wei2022emergentabilitieslargelanguage}, distillation has enabled smaller models to achieve strong reasoning performance~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability, bespoke_stratos}. While most distillation efforts focus on within-architecture transfer (e.g., Transformer to Transformer), recent work has explored cross-architecture distillation. Pretrained Transformers have been successfully distilled into recurrent architectures such as RNNs~\citep{kasai2021finetuningpretrainedtransformersrnns, mercat2024linearizinglargelanguagemodels}, linear attention~\citep{zhang2024hedgehogporcupineexpressive}, convolutions~\citep{ralambomihanta2024scavenginghyenadistillingtransformers}, and SSMs~\citep{bick2024transformersssmsdistillingquadratic,wang2025mamballamadistillingaccelerating}. Whether strong reasoning can be distilled across architectures remains an open question.