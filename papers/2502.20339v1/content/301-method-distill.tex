\section{Distilling Student Reasoners}
\label{sec:method:distill}


\begin{figure*}[h]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/speedup-1b.pdf}
  \caption{1B Models.}
  \label{fig:speedup-1b}
  \end{subfigure}\hspace{4em}
  \begin{subfigure}[b]{0.39\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/speedup-3b.pdf}
  \caption{3B Models.}
  \label{fig:speedup-3b}
  \end{subfigure}
\caption{\textbf{Faster generation of distilled models.}
In \textbf{(a)} and \textbf{(b)}, we show the inference time measured for the baseline Llama models as well as our distilled Llamba (pure Mamba) and MambaInLlama (hybrid) models at the 1B and 3B scale. We denote the speedup for our MambaInLlama model at each batch size. We use prompts of $512$ tokens and measure the time required to generate $512$ tokens. The times measured do not include the prefilling of the prompt. Overall, distilled models can generate tokens much faster with the speedup being greater for larger batch sizes. Moreover, our distilled models are more memory efficient, as shown in \textbf{(b)}, using a batch size of $512$ yields an Out of Memory (OOM) error for Llama 3B, but not for our models. To obtain $512$ completions with Llama-3B, the two batches of $256$, result in an inference time of $58.8$s. In comparison, our MambaInLlama model would take $11.6$s, a speedup of $\times5.1$.   
 }
\label{fig:speedup}
\end{figure*}



\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/coverage-math-llama-ft.pdf}
  \caption{Coverage.}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/maj-acc-math-llama-ft.pdf}
  \caption{Majority voting.}
  \end{subfigure}
\caption{\textbf{Negligible effect of finetuning Llama baselines on distillation dataset.} As our distillation dataset includes math content, we also finetune Llama models on the distillation dataset OpenMathInstruct-2. Those models are marked with the "+FT" prefix. We plot the coverage \textbf{(a)} and majority voting accuracies \textbf{(b)} as a function of the number of completions. We observe that finetuning Llama models on the distillation dataset has a negligible effect on those metrics. 
 }
\label{fig:llama-ft}
\end{figure}



\begin{algorithm}[tb]
    % \caption{Attention-Initialized Mamba}
    \small
    \label{alg:group_ssm}
    \begin{algorithmic}[1]
        \STATE \textbf{Shapes:} $B$ - Batch, $L$ - Length, $D$ - embed size, \\  
        \hspace{1cm} $N = D  / \text{Heads}$, $N'$ - expand  
        \STATE \textbf{Input:} $\vo_t$: (B,  D)
        \STATE \textbf{Output:} output: (B, D) 
        \STATE \textbf{New Params:} MLP, $\rmA$  
        \FOR{ each head $\rmW^k, \rmW^q, \rmW^v, \rmW^o : (N, D)$ \\
             \hspace{1cm} expanding grouped KVs}
            \STATE \textbf{Head Parameter:} $\rmA : (N, N')$
            \STATE \text{for all positions $t$:}
            \STATE $\ \vx_t : (B, N) \gets \rmW^V \vo_t$
            % \STATE $\vx : (B, L, n_{\text{kv\_head}}, d_{\text{head}}) \gets \text{rearrange}(x_\text{ssm})$ 
            % \STATE $\vx : (B, L, n_{\text{attn\_head}}, d_{\text{head}}) \gets \text{repeat\_kv}(x_{\text{ssm}}, n_{\text{attn\_head}} // n_{\text{kv\_head}})$
            \STATE $\ \rmB_t : (B, N) \gets \rmW^K \vo_t$ 
            \STATE $\ \rmC_t : (B, N) \gets \rmW^Q \vo_t$
            \STATE $\ \Delta_t : (B, N') \gets \text{MLP}(\vx_t)$
            \STATE $\overline{\rmA}_{1:T}, \overline{\rmB}_{1:T}, \overline{\rmC}_{1:T}: (B, N, N') \gets \textsc{Disc}(\rmA, \rmB, \rmC, \Delta)$
            \STATE $\vy \gets \textsc{LinearRNN}(\overline{\rmA}, \overline{\rmB}, \overline{\rmC}, \vx)$
            \STATE output $\gets \text{output} + \rmW^{O\top} \vy$
        \ENDFOR{}
        \STATE \textbf{return} output
    \end{algorithmic}
    \caption{Initializing MambaInLlama from Llama}
\end{algorithm}


In this section, we describe how we distilled Llama models into pure Mamba and hybrid architectures. We refer to our pure Mamba models as \textbf{Llamba}, and our hybrid models as \textbf{MambaInLlama}. We distill both our hybrid and pure Mamba models using Llama 3.2-1B-Instruct and Llama 3.2-3B-Instruct from the Llama family of models~\citep{grattafiori2024llama3herdmodels}. 

\subsection{Distilling into Llamba}

\textbf{Distillation method.} In order to distill pure Mamba models, we modify the MOHAWK distillation procedure introduced by~\citet{bick2024transformersssmsdistillingquadratic}.
MOHAWK is composed of three stages: 1) matrix orientation, 2) hidden state alignment, and 3) weight transfer and knowledge distillation. Stage 1 (matrix orientation) aligns the Mamba-2 model's SSM matrix mixer~\citep{dao2024transformersssmsgeneralizedmodels} with the teacher's self-attention matrix by minimizing the distance between the two matrices. Stage 2 (hidden state alignment) matches the student and teacher's layers' hidden state outputs. Both of these stages are run independently across layers to prevent previous optimization gaps from propagation through the model. This is done by setting the input of the student layer to be that of the previous teacher layer's output.
Stage 3 (weight transfer and knowledge distillation) transfers the remaining, unoptimized parameters, e.g., MLPs, embeddings, and norms, and finetunes the complete end-to-end student model using a distillation loss on the student and teacher logits~\citep{hinton2015distillingknowledgeneuralnetwork}. We deviate from the original MOHAWK paper by transferring the MLP weights and norms of each teacher decoder layer to the student and training those parameters as well during Stage 2. This stems from the architectural differences between Phi~\citep{li2023textbooksneediiphi15} (MLP and self-attention in parallel) and Llama~\citep{grattafiori2024llama3herdmodels} (sequential self-attention and MLP). Stage 3 remains the same with fewer weights transferred.


\textbf{Experimental details.} Our pure Mamba-distilled models, Llamba-1B and Llamba-4B, dubbed after \citet{llamba}, which uses a similar methodology and the same teacher models, are distilled from their respective teacher models using our adjusted MOHAWK distillation approach with only 8 billion tokens total each. Following \citet{bick2024transformersssmsdistillingquadratic}, we use a variant of Mamba-2 that converts the SSM head structure to multi-head (compared to the original multi-value and Llama's grouped-query structure) and converts the sequence mixer to entirely discrete-time. The post-convolution activation and pre-output projection normalization are also removed.
The 8B token distillation dataset is composed of 4B tokens from FineMath-4+~\citep{lozhkov2024finemath}, allocated as 1B and 3B to Stages 1 and 2 respectively, and 4B tokens from OpenMathInstruct-2~\citep{toshniwal2024openmathinstruct2acceleratingaimath} used in Stage 3, which is the only stage in which we apply the chat template to the inputs. Unlike for our hybrid models, we find that computing the loss on both the assistant output and user prompt improves model performance over just the assistant output. All three distillation stages use the AdamW optimizer with $\beta=(0.9, 0.95)$ and weight decay of $0.1$ and a Warmup-Stable-Decay (WSD) scheduler with $10$\% warmup and $10$\% decay~\citep{hu2024minicpmunveilingpotentialsmall} at a 2048 context length. In Stages 1 and 2, we set the learning rate to $1\times 10^{-4}$, while in Stage 3, it is set to $1\times 10^{-5}$. The hyperparameters are the same for the 1B and 4B distillation runs. Our final Llamba-1B model has 16 layers of our Mamba-2 variant with a state size of $64$ and multi-head pattern of 32 heads and state size of $64$. Likewise, our Llamba-4B utilizes the same pattern with $24$ heads and state size of $128$ for $28$ layers. We note that our pure Mamba-2 models are slightly larger than their Transformer counterparts due to the pattern conversion from grouped-query to multi-head and additional parameters found within the Mamba-2 layer, e.g., gating.

\subsection{Distilling into MambaInLlama}

\textbf{Distillation method.} We follow two separate directions for distillation.
For the hybrid models, we modify the protocol proposed by ~\citet{wang2025mamballamadistillingaccelerating} in order to distill some specific capabilities. These techniques have been shown to be effective for hybrid architectures.
The Mamba-in-Llama framework ~\citep{wang2025mamballamadistillingaccelerating} introduces a method for distilling hybrid Transformer-Mamba models by reusing weights from the attention layers. In the distillation process shown in Alg~1, the linear projections for $\rmQ$, $\rmK$, $\rmV$ and $\rmO$ are initialized using the corresponding linear projections for $\rmC$, $\rmB$, $\rmX$ and $\rmO$ respectively. The only additional learned parameters in the new layers are the sampling rate $\Delta$ and the dynamic $\rmA$. These new parameters will control the constructed Mamba through
the discretization function. Specifically, we take $\Delta \in \R^{N'}$ to discretize $\rmB_t, \rmC_t \in \R^{N \times 1}$ and obtain $\overline{\rmB}_t, \overline{\rmC}_t \in \R^{N'\times N\times 1}$ as shown in Alg~1.
We directly reuse the MLP layers. Differently from \citet{wang2025mamballamadistillingaccelerating}, we replace the attention layers with Mamba layers in a single round and finetune the whole model. For distillation, we employ token-level KL divergence. The full probability distribution of the student model, $p(\cdot; \theta)$, is trained to align with the full distribution of the teacher model, $p(\cdot; \theta_T)$, by minimizing the KL divergence across all possible next tokens at position $t$. Different from \citep{wang2025mamballamadistillingaccelerating}, we use the reverse KL divergence, $ D_{\text{KL}}(p(\cdot; \theta) \parallel p(\cdot; \theta_T))$ instead of the forward KL divergence as the loss function,
since reverse KL behaves more like mode-seeking and better mimics the peak values. And we find that it yields better results empirically.
We adopt the Mamba-1 architecture for our hybrid models, as~\citet{lieber2024jambahybridtransformermambalanguage,junxiongdaniele2024mambainllama,dong2024hymbahybridheadarchitecturesmall} demonstrates that using Mamba-1 in a hybrid architecture yields better results, especially for challenging reasoning tasks.
 
\paragraph{Experimental details.} Our hybrid Mamba models, named MambaInLlama-1B (with $4$ attention layers in $16$ total layers) and MambaInLlama-3B (with $6$ attention layers in $26$ total layers), are distilled with 8B tokens from OpenMathInstruct-2 \citep{toshniwal2024openmathinstruct2acceleratingaimath}. We apply the Llama chat template, mask the user prompt, and compute the loss only over the tokens generated in the assistant's output. Thus, the total number of supervised tokens is reduced to roughly 7B. To speed up training, we use data packing to merge different sequences into a single one until we reach the maximum sequence length which is set to $8192$. We use the AdamW optimizer with learning rate $2\times 10^{-5}$, $\beta=(0.9, 0.95)$ and a weight decay of $0.1$. The hyperparameters are the same for the 1B and 3B models. 
In Mamba layers, we set the SSM state size to $16$. Consequently, the number of SSM groups after expansion is $2048/16 = 128$ for the 1B model and $3072/16 = 192$ for the 3B model.



\textbf{Remarks on the distillation dataset.}
We finetune the Llama teacher models on the same data used during distillation to avoid our models from potentially gaining an unfair advantage. The results, reported in Figure~\ref{fig:llama-ft}, show that the continuous training of the base model on the distillation data mix has a negligible effect on performance. Moreover, we find that the selection of data used during distillation has a significant impact on the final capabilities of the distilled models. Switching the Stage 3 dataset in Llamba-1B from OpenMathInstruct-2 to OpenHermes-2.5~\citep{openhermes-2p5} decreased greedy decoding accuracy on MATH (acc@1) by more than 10 percentage points.

\textbf{Lack of correlation between reasoning and general benchmarks.} We also highlight the lack of correlation between common multiple choice-based benchmarks and mathematical reasoning, as the OpenHermes variant of Llamba-1B outperforms the final Llamba-1B by more than 5 points on MMLU~\citep{hendryckstest2021} and $0.5$ point on ARC-Challenge~\citep{clark2018thinksolvedquestionanswering}. Moreover, analyzing the acc@1 performance of Llamba-1B and Llamba-4B on MATH after each of the three stages, we see that the sharp increase in reasoning ability between Stage 2 and Stage 3 is not reflected in general knowledge benchmarks (Fig.~\ref{fig:mohawk-reasoning}).

\subsection{Improving performance after distillation}
We show that it is possible to improve the accuracy and coverage of our distilled models by performing some supervised fine-tuning (SFT) after distillation. 
This is inspired by \citet{wang2025mamballamadistillingaccelerating}, where SFT is an integral part of the distillation protocol.
Starting from the distilled MambainLlama-1B and 3B, we fine-tune the models for two epochs using 8 billion tokens from OpenMathInstruct-2.
The distilled models achieve impressive performance both in coverage and accuracy, even surpassing the original Llama models. This is illustrated in Figure~\ref{fig:sftdpo}.

