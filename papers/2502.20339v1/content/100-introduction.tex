\begin{figure*}[t!]
  \centering
  \begin{subfigure}[b]{0.575\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/coverage-math-time.pdf}
  \caption{Scaling with time.}
  \label{fig:coverage-math-b}
  \end{subfigure}
  \begin{subfigure}[b]{0.38\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/coverage-math.pdf}
  \caption{Scaling with number of completions.}
  \label{fig:coverage-math-a}
  \end{subfigure}
\caption{\textbf{Distilled models have better coverage on MATH for most time budgets.}
In \textbf{(b)}, we show the coverage as we increase the number of sampled answers $k$. Compared to their associated Llama baselines, our distilled models provide a lower coverage for a given $k$. In \textbf{(a)}, we now show the shortest time required to reach a given coverage. For each curve in \textbf{(b)}, we map the $k$-values on the x-axis to the time required to generate that many samples for each model. Ideally, we would want to reach the highest coverage for short time budget. For a given time budget, our distilled models can generate many more completions than their respective baselines. As such, the higher throughput of our models, shown in Figure~\ref{fig:speedup}, allows them to overcome their lower per-sample coverage. As a result, our models push the Pareto front forward for most time budgets. 
 }
\label{fig:coverage-math}
\end{figure*}

\section{Introduction}
Reasoning in large language models (LLMs) has seen a significant boost in performance recently, largely driven by scaling inference compute. 
A key technique to enhance ``reasoning'' performance is the use of intermediate reasoning steps before producing a final answer, known as Chain-of-Thought (CoT)~\citep{wei2023chainofthoughtpromptingelicitsreasoning}.
Building on this, many test-time compute techniques often involve generating multiple CoTs ~\citep{wu2024inferencescalinglawsempirical, snell2024scalingllmtesttimecompute} and selecting the best one. 
Even simple strategies, such as majority voting, can be surprisingly effective ~\citep{brown2024largelanguagemonkeysscaling, beeching2024scalingtesttimecompute}.
Furthermore, trained reward models can provide scores for the final model answers and even for the individual steps of the CoTs ~\citep{luo2024improvemathematicalreasoninglanguage}.



However, these test-time compute techniques introduce significant challenges for LLM systems. Generating long CoT sequences or large batches of completions places substantial demands on memory and compute resources. 
Transformers, in particular, struggle with such workloads due to their linear memory scaling and memory-bound nature during generation.
This raises an important question: \emph{how should we optimize model architectures to best scale test-time compute?}
In particular, can alternative architectures with faster and more efficient generation outperform current LLMs under fixed compute budgets? Addressing this problem could unlock new avenues for deploying reasoning models with different architectures, enabling them to run and scale more efficiently on hardware and environments with limited memory and compute.


Recent subquadratic architectures have training time or prefill time linear in sequence length, and constant memory requirement (instead of linear) during inference.
This enables up to 5$\times$ higher inference throughput ~\citep{gu2024mambalineartimesequencemodeling, peng2023rwkvreinventingrnnstransformer} as inference time for large batch size or long sequences is dominated by the time to load the model states (KV cache or RNN states).  
Despite their efficiency, subquadratic models have not been extensively explored in reasoning tasks, primarily due to the lack of large-scale pretrained models compared to Transformer-based counterparts. As a result, it remains unclear whether: (1) scaling inference compute for subquadratic models improves reasoning performance, and (2) subquadratic models can match or exceed Transformers models under fixed compute budgets.

In this work, we explore the reasoning capabilities of subquadratic architectures by distilling knowledge from pretrained Transformers into hybrid and pure Mamba models. 
To address the scarcity of pretrained subquadratic models with robust reasoning abilities, we develop recipes to distill specific reasoning skills into these architectures.
We then benchmark the models for multiple Chain-of-Thought (CoT) completions, providing a comprehensive analysis of performance under fixed compute and memory constraints.
Our approach advances the Pareto front established by existing models, achieving a better trade-off between efficiency and reasoning capability.

Our distilled pure and hybrid subquadratic reasoners are able to outperform their Transformer teachers on both coverage and accuracy on MATH~\citep{lightman2023letsverifystepstep} and GSM8K~\citep{cobbe2021trainingverifierssolvemath} mathematical reasoning tasks on most time budgets, reaching the same quality with 2.5$\times$ less inference time. Our results highlight the potential for distilling reasoning and math capabilities across architectures in a cost-effective manner while maintaining the inference compute scaling properties of Transformers. 

