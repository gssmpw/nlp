
\section{Results}
\label{sec:results}

We (i) measure the inference speedup of our distilled models (\S~\ref{subsec:speedup}), and (ii) show that this speedup can result in better scaling for a given inference time budget (\S~\ref{subsec:scaling}).

\subsection{Inference time results}
\label{subsec:speedup}

\textbf{Experimental protocol.} In order to measure the inference time and throughput of our distilled models, we focus on creating a realistic setup that matches the prompt and CoTs lengths that we find for MATH and GSM8K. We compare the runtime of a Llama 3.2 architecture against our distilled models. For Llama, we use FlashAttention2 \citep{dao2023flashattention2fasterattentionbetter} and torch compile. The implementations of our models rely on the standard Mamba implementation. Given a varying batch size, we consider the tasks of generating $512$ tokens from a $512$ token prompt, which reflect the length found in the evaluation datasets. The prefilling time to process the prompt is not included in the benchmark, as it may depend on the redundancy of a given prompt within the batch. In fact, in our setting, we are only interested in the time to generate the multiple completions given one prompt. Our benchmark is done on a single NVIDIA H100 GPU, and averaged results over multiple runs are shown in Figure~\ref{fig:speedup}.



\textbf{Distilled models are significantly faster.} Results in Figure~\ref{fig:speedup} show our distilled models are up to $\times3.7$ and $\times4.2$ faster than their respective Llama 1B and 3B baselines. Moreover, MambaInLlama and Llamba are more memory efficient and thus can run larger batches. In Figure~\ref{fig:speedup-3b}, our models can accommodate batches of $512$ while Llama-3B returns an out-of-memory error. We also notice that MambaInLlama models are slightly faster than Llamba. We speculate that this is because MambaInLlama has a smaller SSM state size of $16$, while Llamba uses a larger SSM state size of $64$. Additionally, Llamba has larger projections because of its multi-head structure.

\textbf{Remark.} In our speed experiments, both the prompt and the generation lengths are relatively short compared to many other domains. 
When evaluating multi-turn conversations, for example, where distilled Mamba architectures are already shown to excel~\citep{wang2025mamballamadistillingaccelerating}, it is clear that the length of the context and of the CoTs can be significantly larger than what has been considered in our experiments. Such longer sequences would significantly increase the throughput advantage of our models. Unfortunately, it is not yet clear how to evaluate and exploit test-time compute for more subjective, conversational tasks.

\textbf{Limitations.} We try to ensure fair speed comparisons between the model types by utilizing roughly equivalent implementations: standard implementation of Mamba-1/2 and FlashAttention-based self-attention with Pytorch compilation. However, it is worth noting that there exist optimizations for current Transformer architectures, such as memory management improvements~\citep{kwon2023efficient}, that enable faster generation; similar boosts are currently not readily available for alternative architectures. Deeper optimizations for each architecture are beyond the scope of this work. 


\begin{figure*}[t!]
    \centering
    \begin{tcolorbox}[]
        \textbf{MATH System Prompt:} \small\textit{Solve the following math problem efficiently and clearly:\textbackslash n\textbackslash n- For simple problems (2 steps or fewer):\textbackslash nProvide a concise solution with minimal explanation.\textbackslash n\textbackslash n- For complex problems (3 steps or more):\textbackslash nUse this step-by-step format:\textbackslash n\textbackslash n\#\# Step 1: [Concise description]\textbackslash n[Brief explanation and calculations]\textbackslash n\textbackslash n\#\# Step 2: [Concise description]\textbackslash n[Brief explanation and calculations]\textbackslash n\textbackslash n...\textbackslash n\textbackslash nRegardless of the approach, always conclude with:\textbackslash n\textbackslash nTherefore, the final answer is: \$\textbackslash boxed\{answer\}\$. I hope it is correct.\textbackslash n\textbackslash nWhere [answer] is just the final number or expression that solves the problem.
    }

    \end{tcolorbox}
    \begin{tcolorbox}[]
            \textbf{GSM8K System Prompt:} \small\textit{\textbackslash n\textbackslash nGiven the following problem, reason and give a final answer to the problem.\textbackslash nYour response should end with "The final answer is [answer]" where [answer] is the response to the problem.\textbackslash nProblem:
    }
    \end{tcolorbox}
    
    \caption{\textbf{System prompts for MATH and GSM8K.} 
    System prompts passed to all models (pure and hybrid distilled models and Llama baseline) to be used within the chat template when generating solutions to questions in the MATH and GSM8K datasets.}
    \label{fig:dataset_prompt}
\end{figure*}

\subsection{Results on reasoning tasks} 
\label{subsec:scaling}

\textbf{Experimental protocol.} We benchmark the teacher models (Llama-3.2 1B-Instruct and 3B-Instruct) and the distilled Mamba students (MambaInLlama-1B, MambaInLlama-3B, Llamba-1B, Llamba-4B) on the MATH-500 subset of the MATH dataset~\citep{lightman2023letsverifystepstep, hendrycks2021measuringmathematicalproblemsolving} and a randomly selected 500-sample subset of GSM8K~\citep{cobbe2021trainingverifierssolvemath}. We evaluate performance based on coverage and accuracy with majority voting and weighted Best-of-N (\S~\ref{sec:method:scale}). We sample responses from the distilled models with temperatures $T=0.6,0.8$ and $\mathsf{top\_k=-1}$ to consider all tokens. For each response, we sample up to $2048$ tokens. For the distilled models, we find that $T=0.6$ and $T=0.8$ are ideal for the 1B and 3B scale, respectively, and subsequent results are obtained using these temperatures. The official Llama evaluation system prompt is used for the MATH dataset, while the original prompt is kept for GSM8K as displayed in Figure~\ref{fig:dataset_prompt}. For our process reward model, we utilize a base Llama3.1-8B-Instruct model trained on Mistral-generated data~\citep{xiong2024rlhflowmath}.


\begin{figure*}[h]
  \centering
  \begin{subfigure}[b]{0.363\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/coverage-math-sft.pdf}
  \caption{Coverage.}
  \label{fig:sft-coverage-1b}
  \end{subfigure}\hspace{2em}
  %\begin{subfigure}[b]{0.25\linewidth}
  %\includegraphics[width=\textwidth,clip]%{./figs/maj-acc-math-dpo.pdf}
  %\caption{DPO \& Majority voting.}
  %\label{fig:speedup-3b}
  %\end{subfigure}
  \begin{subfigure}[b]{0.59\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/maj-acc-math-sft.pdf}
  \caption{Majority voting.}
  \label{fig:sft-acc-1b}
  \end{subfigure}
\caption{\textbf{SFT can further improve distilled models.}
In \textbf{(a)}, and \textbf{(b)}, we show the coverage and majority voting accuracies for MambaInLlama distilled models. We show how coverage and majority voting accuracies can be further improved through Supervised Fine-Tuning (SFT). The improvement is especially striking for our 1B model. 
 }
\label{fig:sftdpo}
\end{figure*}



% , i.e., what percentage of problems have at least one sample give the correct solution, and accuracy, i.e., what percentage of problems can we correctly solve by choosing one sample via some heuristic. For a model's coverage, we generate increasing amounts of samples, up to 256, for each question and check whether the correct answer is among the generated solutions. For measuring accuracy, we employ a few heuristics: majority voting, naive reward model, and weighted reward model.
\textbf{Distilled Models can Cover like Teachers.} When observing the scaling of coverage as the number of generation $k$ increases in Figure~\ref{fig:coverage-math-b} (resp. Fig.~\ref{fig:coverage-gsm8k-b} for GSM8K), we see distilled models closely matching the coverage of their teachers. Only a small degradation is observed. When we plot the coverage as a function of the time budget, by associating each coverage measurement to the time required to generate the associated number of completions (see Fig.~\ref{fig:coverage-math-a}), we find that our distilled models are exceptional in their ability to generate correct answers fast. By generating many more completions for the same time budget, the overall Pareto front for coverage in both MATH and GSM8K (Fig.~\ref{fig:coverage-math-a},~\ref{fig:coverage-gsm8k-a}) is heavily dominated by our distilled models where both pure and hybrid Mamba reasoners are able to achieve the same degree of coverage in nearly half the time of their respective teachers. Given a sufficiently strong reward model or verifiable solutions, those coverage scores would in large parts translate to accuracy. 


\textbf{Distilled Models Achieve Competitive Accuracy Under Fixed Time.}
Training with distillation and utilizing subquadratic architectures---which are less expressive than Transformers---can degrade model quality. However, we find that this is a worthy trade-off when comparing performance under fixed time budgets in Figure~\ref{fig:acc-gsm8k} and Figure~\ref{fig:acc-math}.

Similarly to coverage, the lighter and faster batch inference of the distilled models, allowing for more generations, results in a better accuracy/time Pareto front at several completion scales. Interestingly, while comparing models of similar sizes indicates that larger time budgets are dominated by the teacher models, we observe that the larger distilled model can provide better accuracy than the smaller baseline while still being faster. For example, while Llama-1B provides better accuracy than MambaInLlama-1B for larger time budgets, MambaInLlama-3B takes over where MambaInLlama-1B left off, providing a better accuracy than Llama-1B for inference time. Similarly, we conjecture that it would be possible to distill a larger baseline model that would outperform Llama-3B, providing better accuracy for a given time budget. 




\textbf{Larger Students Faster and Better than Smaller Teachers.} The core driving force behind the growing interest in subquadratic models is their computational efficiency. Such properties enable our distilled models of larger sizes (3B scale) to generate samples faster than even a smaller Transformer (1B). Our MambaInLlama-3B and Llamba-4B models outperform the slower Llama-1B baseline on coverage and accuracy while being faster. These inference speedups, rooted in the underlying architecture, allow larger and more capable models to be used in time-constrained environments, despite their increased number of parameters.
% our distilled models of a larger size (3/4B) are able to outperform their transformer counterpart under time constraint (1B) on both coverage and accuracy. can be 
\textbf{Smaller models have great coverage.} When focusing on coverage, we observe in Figure~\ref{fig:coverage-math-a} and Figure~\ref{fig:coverage-gsm8k-a} that most of the Pareto front is occupied by 1B models. Contrasting those results with the majority voting accuracy results in Figure~\ref{fig:acc-math-a} and Figure~\ref{fig:acc-gsm8k-a} where the gap between 1B and 3B models is much more significant. An interpretation is that, while smaller models have the ability to generate the correct answer, the probability of generating it within a constrained number of samples increases with the model size. This finding has implications in choosing model size for tasks involving formal language where the answer is easily verifiable, such as coding and mathematical proofs. In those applications, coverage matters most, and smaller models may be preferred given their better time/coverage efficiency compared to larger models.


\textbf{SFT improves the models significantly}
Following distillation, which establishes a strong foundation for our models, we observe that additional supervised fine-tuning (SFT) significantly enhances their performance, as illustrated in Figure~\ref{fig:sftdpo}. This suggests that while distillation effectively transfers knowledge from the teacher model, SFT further refines and aligns the model's capabilities, enabling it to become highly competitive. Our results indicate that by leveraging both distillation and SFT, subquadratic architectures can match or even surpass their Transformer teachers in absolute performance on reasoning tasks.