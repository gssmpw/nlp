\newpage
\appendix
\onecolumn
\section{Additional results}
Figures \ref{fig:w-acc-math} and \ref{fig:w-acc-gsm8k} show accuracy as a function of the number of completions.


\begin{figure*}[h]
  \centering
\begin{subfigure}[b]{0.38\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/weighted-acc-math.pdf}
  \caption{Weighted Best-of-N.}
  \end{subfigure}
  \begin{subfigure}[b]{0.575\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/maj-acc-math.pdf}
  \caption{Majority voting.}
  \end{subfigure}
\caption{\textbf{Accuracy scores on MATH per completion without considering inference time.}
 }
\label{fig:w-acc-math}
\end{figure*}


\begin{figure*}[h]
  \centering
  \begin{subfigure}[b]{0.38\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/weighted-acc-gsm8k.pdf}
  \caption{Weighted Best-of-N.}
  \end{subfigure}
  \begin{subfigure}[b]{0.575\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/maj-acc-gsm8k.pdf}
  \caption{Majority voting.}
  \end{subfigure}
\caption{\textbf{Accuracy scores on GSM8K per completion without considering inference time.}
 }
\label{fig:w-acc-gsm8k}
\end{figure*}

\newpage
\section{Distillation loss curves}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth,clip]{./figs/MiL_3B_loss.png}
\caption{\textbf{The distillation loss decreases steadily over time, showing a clear downward trend and converge at around 0.03.}}
\label{fig:mil3B_loss}
\end{figure}

Figure~\ref{fig:mil3B_loss} shows the distillation loss of MambaInLlama 3B when training on the \citet{toshniwal2024openmathinstruct2acceleratingaimath} dataset. We observed a stable decrease in loss until it converged to 0.03.

\section{Pass@k implementation}
\label{app:pass_k}
A numerically stable script for calculating an unbiased
estimate of pass@k, from~\citet{chen2021evaluatinglargelanguagemodels}.
\begin{lstlisting}[language=Python]
def pass_at_k(n, c, k):
    """
    :param n: total number of samples
    :param c: number of correct samples
    :param k: k in pass@$k$
    """
    if n - c < k: return 1.0
    return 1.0 - np.prod(1.0 - k /
                         np.arange(n - c + 1, n + 1))
\end{lstlisting}

\newpage
\section{MOHAWK (Pure Mamba) Distillation on Reasoning and General Performance}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth,clip]{./figs/mohawk_stage_performance}
\caption{\textbf{Reasoning and general performance of models after various stages of MOHAWK distillation.} Distilling reasoning capabilities, measured by accuracy using greedy decoding (acc@1) on the MATH benchmark, does not correlate with general multiple-choice reasoning benchmarks. The performance of Llamba-1B and 4B increase significantly only after the last stage of the MOHAWK distillation, but Stage 2 is able to significantly improve general benchmark performance.}
\label{fig:mohawk-reasoning}
\end{figure}
