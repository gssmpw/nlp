\section{Scaling Inference Time Compute}

\begin{figure*}[t!]
  \centering
  \begin{subfigure}[b]{0.575\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/maj-acc-math-time.pdf}
  \caption{Majority voting.}
  \label{fig:acc-math-a}
  \end{subfigure}
  \begin{subfigure}[b]{0.38\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/weighted-acc-math-time.pdf}
  \caption{Weighted Best-of-N.}
  \label{fig:acc-math-b}
  \end{subfigure}
\caption{\textbf{Distilled models provide better accuracies on MATH for most time budgets.} Figures are similar to Figure~\ref{fig:coverage-math-a}.
In \textbf{(a)} and \textbf{(b)}, we show the majority-voting accuracy and the weight Best-of-N accuracy (the selected answer is the one with the highest sum of reward model scores, as introduced in \citet{beeching2024scalingtesttimecompute}), for different time budgets. Similarly to Figure~\ref{fig:coverage-math-a}, we observe how the higher throughput of our distilled models allows them to push the Pareto front for most time budgets. Interestingly, when comparing models of a given size, Llama models are better for larger time budgets. However, looking at both model sizes together reveals that larger distilled models can compensate for the lower accuracies of smaller models. As a result, the Pareto front is defined by our hybrid models. While Llama-3B is still more efficient for a large time budget, one could imagine distilling a larger subquadratic model that generates quicker nonetheless.
}
\label{fig:acc-math}
\end{figure*}


\label{sec:method:scale}

\begin{figure*}[h]
  \centering
  \begin{subfigure}[b]{0.575\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/coverage-gsm8k-time.pdf}
  \caption{Scaling with time.}
  \label{fig:coverage-gsm8k-b}
  \end{subfigure}
  \begin{subfigure}[b]{0.38\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/coverage-gsm8k.pdf}
  \caption{Scaling with number of completions.}
  \label{fig:coverage-gsm8k-a}
  \end{subfigure}
\caption{\textbf{Distilled models have better coverage on GSM8K for most time budgets.}
Observations are similar to Figure~\ref{fig:coverage-math}, despite a small degradation in coverage per number of completions, the better throughput of distilled models pushes the Pareto front forward. Hybrid models are more efficient for most time budgets.  
}
\label{fig:coverage-gsm8k}
\end{figure*}

\begin{figure*}[h]
  \centering
  \begin{subfigure}[b]{0.575\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/maj-acc-gsm8k-time.pdf}
  \caption{Majority voting.}
  \label{fig:acc-gsm8k-a}
  \end{subfigure}
  \begin{subfigure}[b]{0.38\linewidth}
  \includegraphics[width=\textwidth,clip]{./figs/weighted-acc-gsm8k-time.pdf}
  \caption{Weighted Best-of-N.}
  \label{fig:acc-gsm8k-b}
  \end{subfigure}
\caption{\textbf{Distilled models provide better accuracies on GSM8K for most time budgets.}
Observations are similar to Figure~\ref{fig:acc-math}. The higher throughput of our distilled models allows them to push the Pareto front for most time budgets.}
\label{fig:acc-gsm8k}
\end{figure*}


We scale test-time compute using our distilled models by generating multiple CoTs to solve a set of math problems.
The system prompt (Figure~\ref{fig:dataset_prompt}) contains instructions on how to properly format the response.
The model outputs are parsed to extract the final solution, which is then compared to the ground truth. This approach enables us to evaluate the model's performance in generating correct solutions across multiple attempts.
Moreover, the results demonstrate that the models are able to retain their instruction following ability after distillation.


\paragraph{Evaluation metrics.} We evaluate our model using two primary metrics: coverage and accuracy.
In domains like coding and formal proofs, where answers can be automatically verified, coverage directly translates to improved performance and has been widely adopted~\citep{chen2021evaluatinglargelanguagemodels, brown2024largelanguagemonkeysscaling}.
Coverage is commonly referred to as the pass@k metric, where $k$ denotes the number of samples per problem~\citep{chen2021evaluatinglargelanguagemodels, brown2024largelanguagemonkeysscaling}.
This metric estimates the probability that at least one correct solution exists among the $k$ samples.
To reduce the variance when calculating coverage, we
adopt the unbiased estimation formula from~\citet{chen2021evaluatinglargelanguagemodels}. Specifically, we generate $N\geq k$ total samples per task. The probability that a correct solution exists among a pool of $k$ generated samples can then be determined given the total number of correct solutions $C_i$ for each task.

\[
\text{pass@k} = \frac{1}{\# \text{ of problems}} \sum_{i=1}^{\# \text{ of problems}} \left( 1 - \frac{\binom{N - C_i}{k}}{\binom{N}{k}} \right)
\]

We implement this formula using a numerically stable approach as suggested by~\citet{chen2021evaluatinglargelanguagemodels}(see Appendix~\ref{app:pass_k}).

For accuracy, we use multiple aggregation strategies.
Majority voting, or self-consistency decoding~\citep{wang2023selfconsistencyimproveschainthought}
is the most straightforward method to aggregate responses and compute an accuracy score.
A more refined strategy involves using a trained verifier to select the best response (we call this approach Best-of-N).

As our verifier, we utilize a reward model trained using process supervision, where the model receives feedback on each step of the reasoning process.
Inspired by~\citet{snell2024scalingllmtesttimecompute}, we utilize a Llama-3.1 8B-based reward model to score the solutions for Best-of-N. As PRMs produce a cumulative sequence of step-level scores per solution, we perform a reduction over the steps to obtain a single solution-level score which we use for answer selection. 
Following~\citet{snell2024scalingllmtesttimecompute}, we use the final score in all the steps as the score for Best-of-N.


While Best-of-N simply selects the generation with the highest reward, weighted Best-of-N aggregates the rewards of samples that share the same response and returns the solution with the highest combined score. In this work, we chose to report weighted Best-of-N, since we often found it to be superior to Best-of-N, most likely due to it identifying highly-rated but also common solutions.
We focus on the performance of the models for a fixed compute budget.
Inspired by the scaling laws for inference-time compute ~\citep{brown2024largelanguagemonkeysscaling}, we speculate that having a model that can scale to very large number of completions can be very effective, even more so than increasing model size by a large amount ~\citep{snell2024scalingllmtesttimecompute}.  




