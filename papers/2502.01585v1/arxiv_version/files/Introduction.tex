\section{Introduction}
The number of parameters, i.e., model size, provides a basic measure of the capacity of a machine learning (ML) model.
However it is well known that it might not describe the effective model capacity \citep{bartlett1998sample}, especially for over-parameterized neural networks \citep{belkin2018understand,zhang2021understanding} and large language models \citep{brown2020language}.
The focus on the number of parameters results in an inaccurate characterization of the relationship between the test risk $\mathcal{R}$,  training data size $n$, and model size $p$, which is central in ML to understand the bias-variance trade-off \citep{Vapnik2000The}, double descent \citep{belkin2019reconciling} and scaling laws \citep{kaplan2020scaling,xiao2024rethinking}.
For example, even for the same architecture (model size), the test error behavior can be totally different \citep{nakkiran2020optimal,nakkiran2021deep}, e.g., double descent may disappear.

Here we shift the focus from model size to weights and consider their norm, a perspective pioneered in the classical results in \citet{bartlett1998sample}. Indeed, norm based capacity/complexity are widely considered to be more effective in characterizing generalization behavior, see e.g.\ \citet{neyshabur2015norm,savarese2019infinite,domingo2022tighter,liu2024learning} and references therein.
Correspondingly, minimum norm-based solution received much attention as a possible way to understand the learning performance of over-parameterized neural networks in the interpolation regime, see e.g.\ 
\citet{liang2020just,wang2022tight,belkin2018understand,zhang2021understanding,nakkiran2021deep}.

Despite the above results, the role of weight norms in double descent and scaling laws remains largely unclear. While some empirical observations have been made \citep[Fig. 8.12]{ngcs229}, a precise theoretical understanding is still lacking. The goal of this paper is to investigate this relationship by addressing the following fundamental question:

\begin{center}\vspace{-0.cm}
    \emph{What is the relationship between test risk and norm-based model capacity, and how can it be precisely characterized?} 
\end{center}\vspace{-0.cm}

In this work, we take the first steps toward answering this question, as described next.


\begin{figure}[t]
    \centering
    \subfigure[Test Risk vs. $\gamma:=p/n$]{\label{fig:rfma}
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_random_feature_regression/risk.pdf}
    }
    \subfigure[$\ell_2$ norm vs. $\gamma$]{\label{fig:rfmb}
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_random_feature_regression/norm.pdf}
    }
    \subfigure[Test Risk vs. $\ell_2$ norm]{\label{fig:rfmc}
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_random_feature_regression/risk_vs_norm.pdf}
    }
    \subfigure[Risk vs. norm ($\lambda\!\!=\!\!0.001$)]{\label{fig:rfmd}
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_random_feature_regression/risk_vs_norm_single.pdf}
    }
    \vspace{-0.0cm}
    \caption{Relationship between test risk, ratio $\gamma := p/n$, and $\ell_2$ norm of the random feature ridge regression estimator (the regularization parameter is defined in \cref{sec:preli}). Points in these four figures are given by our experimental results, centering around the curves given by deterministic equivalents we derive. Training data \(\{(\bx_i, y_i)\}_{i \in [n]}\), \(n = 100\), sampled from the model \(y_i = \bg_i^\sT \btheta_* + \varepsilon_i\), \(\sigma^2 = 0.01\), \(\bg_i \sim \mathcal{N}(0, \id)\), \(\boldf_i \sim \mathcal{N}(0, \bLambda)\) (\(\bg_i\) and \(\boldf_i\) is defined in \cref{sec:preli}), with \(\xi^2_k(\bLambda)=k^{-\nicefrac{3}{2}}\) and \(\btheta_{*,k}=k^{-\nicefrac{11}{10}}\), given by \(\alpha=1.5\), \(r=0.4\) in \cref{ass:powerlaw_rf}.}
    \label{fig:random_feature_risk_vs_norm}
    \vspace{-0.0cm}
\end{figure}


\subsection{Contributions and findings}

We consider linear and random features models (RFMs) regression  to precisely characterize the relationship between the test risk and the capacity  measured by the estimator's norm. 
As shown in \cref{fig:random_feature_risk_vs_norm} for RFMs, compared to the classical double descent curve in \cref{fig:rfma} w.r.t. model size $p$, we quantitatively  characterize the relationships: norm-based capacity \emph{vs.} $p$ in \cref{fig:rfmb}, and test risk \emph{vs.} norm-based capacity in \cref{fig:rfmc} and \ref{fig:rfmd}, respectively.

The key technical tool we  leverage is  the \emph{deterministic equivalence} technique from random matrix theory \citep{cheng2022dimension,defilippis2024dimension},  that allows one to well approximate the test risk $\mathcal{R}$ by a  deterministic quantity $\sR$. In our work, we explore different deterministic equivalents $\sN$ of the estimator's $\ell_2$ norm $\mathcal{N}$.
We then derive a corresponding relationship between $\sR$ and $\sN$, and accordingly {\em reshape} double descent and scaling law with respect to $\ell_2$ norm based capacity. Our contributions are summarized as below (also tabularized in   \cref{tabres}).

\begin{table}[!htb]
        \centering
        %\scriptsize
        \fontsize{7}{6}\selectfont
        \begin{threeparttable}
                \caption{Summary of our main results on deterministic equivalents and their relationship.}
                \label{tabres}
                \setlength{\tabcolsep}{3pt} 
                \begin{tabular}{cccccccccccccccccccc}
                        \toprule
                        Model &Results & Regularization &Deterministic equivalents $\sN$ & Relationship between $\sR$ and $\sN$   \cr
                        \midrule
                        \multirow{13}{3cm}{Linear regression} 
                        &\cref{prop:asy_equiv_norm_LR}      & $\lambda > 0$    & Asymptotic      & - \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:asy_equiv_norm_LR_minnorm}  & $\lambda \to 0$    & Asymptotic  & - \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:non-asy_equiv_norm_LR}  & $\lambda > 0$    & Non-asymptotic  & - \cr
                         \cmidrule(lr){2-5}
                        &\cref{prop:relation_id}            & $\lambda > 0$    & -               & Under $\bSigma=\id_d$ \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:relation_minnorm_underparam}            & $\lambda \to 0$    & -               & Under-parameterized regime \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:relation_minnorm_id}    & $\lambda \to 0$  & -               & Under $\bSigma=\id_d$ \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:relation_minnorm_pl}    & $\lambda \to 0$  & -               & Under \cref{ass:powerlaw} (power-law) \cr
                        \midrule
                        \multirow{13}{3.5cm}{Random feature regression} 
                        &\cref{prop:asy_equiv_norm_RFRR}    & $\lambda > 0$    & Asymptotic      & - \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:asy_equiv_norm_RFRR_minnorm}    & $\lambda \to 0$    & Asymptotic      & - \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:non-asy_equiv_norm_RFRR_V}    & $\lambda > 0$    & Non-asymptotic  & - \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:relation_minnorm_overparam}       & $\lambda \to 0$    & -    & Over-parameterized regime \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:relation_minnorm_id_rf}       & $\lambda \to 0$    & -    & Under $\bLambda=\id_m$ ($n<m<\infty$) \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:relation_minnorm_powerlaw_rf} & $\lambda \to 0$    & -    & Under \cref{ass:powerlaw_rf} (power-law) \cr
                        \cmidrule(lr){2-5}
                        &\cref{prop:scaling_law_norm_based_capacity} & $\lambda > 0$    & -    & Under \cref{ass:powerlaw_rf} (power-law) \cr
                        \bottomrule
                \end{tabular}
        \end{threeparttable}
        \vspace{-0.0cm}
\end{table}

\begin{itemize}
    \item In \cref{sec:linear}, we consider a bias-variance decomposition of the $\ell_2$ norm of the least squares and min-$\ell_2$-norm solution estimators. Then, we  derive the corresponding non-asymptotic deterministic equivalents denoted by $\sN^{\tt LS}_{\lambda}$,   where  $\lambda$ is the regularization parameter and $\lambda=0$ corresponds to min-$\ell_2$-norm solution. 
    To derive our results on norm-based capacity, we derive additional equivalent quantities, which are of independent interest and may be more broadly useful. We then characterize the test risk $\sR^{\tt LS}_{\lambda}$ and $\sN^{\tt LS}_{\lambda}$ under isotropic features and power-law features.
    \vspace{-0.1cm}
    \item In \cref{sec:rff}, we generalize our results from linear regression to random features models (RFMs). The deterministic equivalents are more complex in this case due to the additional randomness in feature mapping. Accordingly, we first present asymptotic results, and then derive non-asymptotic way results.
    We take min-$\ell_2$-norm estimators as an example to characterize the relationship between $\sR^{\tt RFM}_{0}$ and $\sN^{\tt RFM}_{0}$ under isotropic features and further illustrate the scaling law under classical power law scaling assumptions. 
\end{itemize}
In summary, our results provide the following insights. 
\begin{itemize}
    \item {\bf Phase transition exists but double descent does not exist under norm-based capacity:} Considering the relationship between the test risk $\sR$ and norm $\sN$, there exists a phase transition from under-parameterized regime to over-parameterized regime. In the under-parameterized regime, we still observe the same U-shaped curve as classical bias-variance trade-off, whether we conider the norm $\sN$ or model size $p$ as the model capacity. This curve is precisely described by a hyperbola for the  min-norm estimator in  linear regression under isotropic features.
    
    But in the over-parameterized regime, when the norm $\sN$ increases, the test risk $\sR$ also increases (almost linearly if the regularization is small). This differs from double descent: when the model size $p$ increases, the test risk decreases. Our empirical results on \cref{fig:rfmc} and \ref{fig:rfmd} verify this theoretical prediction.
    \vspace{-0.1cm}
    
    \item {\bf Over-parameterization is still better than under-parameterization under norm-based capacity:} There always exist two scenarios---in the under and over-parameterized regimes---with the same $\ell_2$ norm. However, the estimator in the over-parameterized regime consistently achieves lower test risk. Again this observation is supported by our numerical experiments in \cref{fig:random_feature_risk_vs_norm}.
    \vspace{-0.1cm}
    
    \item {\bf Scaling law is not monotone in norm-based capacity:} We study the scaling law of RFMs under norm-based capacity in a multiplication style by taking model size $p:=n^q$ ($q\geq 0$), leading to $\sR = C n^{-a} \sN^{b}$ with $a \geq 0$, $b \in \mathbb{R}$, and $C>0$. Note that $b \in \mathbb{R}$ can be positive or negative, resulting in different behaviors of $\sR$. This differs from the classical scaling law that is monotonically decreasing in the model size.
    \vspace{-0.1cm}
    
    \item {\bf The solution of self-consistent equations can be given by norm-based capacity:} Self-consistent equations play an important role in random matrix theory (as discussed in \cref{sec:preli}), but they can only be numerically solved/approximated in most cases. Interestingly, our results show that the solution of the self-consistent equation can be described by the variance of the $\ell_2$ norm of the min-norm solution to linear regression (i.e., $\sV_{\sN,0}^{\tt LS}$), which is easier to solve. This is another example of the use of norm-based capacity.
\end{itemize}

We emphasize that we do \emph{not} claim that $\ell_2$ norm-based capacity (or other norm-based capacity) is the best metric of model capacity. Rather, this work aims to show how the test risk behaves when weights, rather than size, are used to measure capacity. For completeness, we discuss other metrics of model capacity and experiments on real-world datasets in \cref{sec:discussion} and \cref{app:experiment}, respectively.

\noindent{\bf Notations:} In this paper we generally adopt the following convention. Caligraphic letters (e.g., \(\mathcal{N}_{\lambda}, \mathcal{R}_{\lambda}, \mathcal{B}_{\mathcal{N},\lambda}, \mathcal{V}_{\mathcal{R},\lambda}, \)) denote random quantities, and upright letters (e.g., \(\sN_{\lambda}\), \(\sR_{\lambda}\) \(\sB_{\sN,\lambda}\), \(\sV_{\sR,\lambda}\)) denote their deterministic equivalents. The letters N, R, B, V above (in any font) signify  quantifies related to the solution norm, test risk, bias, and variance, respectively. 
With $\lambda$ denoting the $\ell_2$-regularization parameter, setting $\lambda=0$ corresponds to the min-norm interpolator. The superscripts $\textsuperscript{\tt LS}$ and $\textsuperscript{\tt RFM}$ denote quantities defined for linear regression and random feature regression, respectively.


We denote by $\gamma$  the ratio between the parameter size and the data size, i.e., $\gamma:=d/n$ in ridge regression and $\gamma:=p/n$ in RFMs.
For asymptotic results, we adopt the notation \( u \sim v \), meaning that the ratio \( \nicefrac{u}{v} \) tends to one as the dimensions \( n \), \( d \) (\( p \) for RFMs) tend to infinity. A complete list notations can be found in \cref{app:notation}.

\subsection{Related works}

The relationship between the test risk, the data size, and the model size is classically characterized by the  \emph{bias-variance trade-offs} \citep{Vapnik2000The}: larger models tend to overfit. More recently, a double descent behavior has been observed \citep{belkin2019reconciling}: the test risk has a second descent when transitioning from under-parameterized to over-parameterized regimes. Moreover recent scaling law \citep{kaplan2020scaling} show that  the test risk is monotonically decreasing with model size, typically in the under-parameterized regime. 

\noindent {\bf Model capacity metrics:} Beyond model size as a capacity measure, there is a considerable effort to  define alternative capacity measures,  e.g. degrees of freedom from statistics \citep{efron1986biased, efron2004estimation,patil2024revisiting}, effective dimension/rank \citep{zhang2002effective,bartlett2020benign}, smoother \citep{curth2024u}, flatness \citep{petzka2021relative}, as well as norm-based capacity \citep{neyshabur2015norm,liu2024learning}. 
The norm's asymptotic characterization is given in specific settings \citep{hu2024asymptotics} but the risk-norm relationship is not directly studied. Besides, training strategies can be also explained as implicit regularization \citep{yao2007early,neyshabur2014search}, affecting the model capacity as well. We refer to the survey \cite{jiang2019fantastic} for details.

\noindent {\bf Random Matrix Theory (RMT) and Deterministic equivalents:}
RMT provides powerful mathematical tools to precisely characterize the relationship between the test risk $\mathcal{R}$ and $n,p,d$ via deterministic equivalence, in an asymptotic way ($n,p,d \rightarrow \infty$, \citealt{mei2022generalization,ghorbani2021linearized,wu2020optimal,xiao2022precise,bach2024high}), or non-asymptotic way \citep{hastie2022surprises,cheng2022dimension,misiakiewicz2024non}. 
We refer the reader to \citet{couillet2022random} for further details.

From a technical viewpoint, our work borrows results from  \citet{bach2024high,misiakiewicz2024non,defilippis2024dimension}.
However,  previous results are not insufficient to  derive the deterministic equivalence of the norm. 
Indeed,  in this paper we generalize the deterministic equivalence w.r.t.\ \(\Tr(\bA \bX^\sT\bX(\bX^\sT\bX + \lambda)^{-1})\) to any positive semi-definite (PSD) matrix $\bA$ while previous work only handled $\bm A := \id$.
Moreover, deriving results on norm-based capacity is more difficult than that of model size.  Additional technical conditions are needed to handle the deterministic equivalents of the difference between two random quantities.