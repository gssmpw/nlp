\section{Preliminaries}\label{sec:preli}

We give an overview of linear regression and RFMs via deterministic equivalents, see more details in \cref{app:pre_result}.

{\bf Linear Regression:} 
We consider $n$ samples $\{ \bx_i \}_{i=1}^n$ sampled i.i.d.\ from a distribution $\mu_\bx$ over $\mathbb{R}^d$ with covariance matrix $\bSigma := \mathbb{E}[\bx \bx^\sT] \in \mathbb{R}^{d \times d}$.
The label $y_i$ is generated by a linear target function parameterized by $\bbeta_\ast \in \mathbb{R}^d$, i.e., $y_i = \bx_i^\sT \bbeta_\ast + \varepsilon_i$, where $\varepsilon_i$ is additive noise independent of $\bx_i$ satisfying $\E[\varepsilon_i] = 0$ and $\text{var}(\varepsilon_i) = \sigma^2$.
We can write the model in a compact form as $\by = \bX \bbeta_\ast + \bm{\varepsilon}$, where the data matrix as $\bX \in \mathbb{R}^{n \times d}$, the label vector $\by \in \mathbb{R}^n$, and the noise vector as $\bm \varepsilon \in \mathbb{R}^n$.
The estimator of ridge regression is given by $\hat{\bbeta} = \left( \bX^\sT \bX + \lambda \id \right)^{-1} \bX^\sT \by$.
We also consider min-$\ell_2$-norm solution in the over-parameterized regime, i.e., $\hat{\bbeta}_{\min} = \argmin_{\bbeta} \| \bbeta \|_2, \text{s.t. } \bX \bbeta = \by $.
The excess risk of $\hat{\bbeta}$ admits a bias-variance decomposition
\[
    \mathcal{R}^{\tt LS} := \E_{\varepsilon}\|\bbeta_* - \hat{\bbeta}\|_{\bSigma}^2 \!=\! {\color{red}\|\bbeta_* - \mathbb{E}_{\varepsilon}[\hat{\bbeta}]\|_{\bSigma}^2} +  {\color{blue}\Tr(\bSigma \mathrm{Cov}_{\varepsilon}(\hat{\bbeta}))}\,,
\]
where the first RHS term is the {\color{red}\emph{bias}}, denoted by  $\mathcal{B}^{\tt LS}_{\mathcal{R},\lambda}$, and the second term is the {\color{blue}\emph{variance}}, denoted by $\mathcal{V}^{\tt LS}_{\mathcal{R},\lambda}$.
Under proper assumptions (to be detailed later), we have the following deterministic equivalents, asymptotically \citep{bach2024high} and non-asymptotically \citep{cheng2022dimension}
\begin{equation}\label{eq:de_risk}
       \mathcal{B}^{\tt LS}_{\mathcal{R},\lambda} \sim \sB^{\tt LS}_{\sR,\lambda} := \frac{\lambda_*^2\<\bbeta_*,\bSigma(\bSigma+\lambda_*\id)^{-2}\bbeta_*\>}{1-n^{-1}\Tr(\bSigma^2(\bSigma+\lambda_*\id)^{-2})}\,, \quad \mathcal{V}^{\tt LS}_{\mathcal{R},\lambda} \sim  \sV^{\tt LS}_{\sR,\lambda} := \frac{\sigma^2\Tr({\color{blue}\bSigma^2}(\bSigma+\lambda_*\id)^{-2})}{n-\Tr(\bSigma^2(\bSigma+\lambda_*\id)^{-2})}\,,
\end{equation}
where $\lambda_*$ is the non-negative solution to the self-consistent equation $n - \frac{\lambda}{\lambda_*} = \Tr ( \bSigma ( \bSigma + \lambda_*\id )^{-1} )$.

{\bf Random features regression:} Different from linear regression, random features models (RFMs) \citep{rahimi2007random, liu2021random} generalizes the linear function output $f(\bm x; {\bm \beta}) = \bm x^{\sT} {\bm \beta}$ to the weighted sum of nonlinear features, i.e., $f(\bm x; \bm a) = \frac{1}{\sqrt{p}} \sum_{j=1}^p \bm a_j \varphi(\bm x, \bm w_j)$, where $\varphi: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ is a nonlinear activation function, and $\{ \bw_i \}_{i=1}^p$ are sampled i.i.d.\ from measure $\mu_\bw$ over $\mathbb{R}^d$ and kept unchanged during training.
We only train $\bm a$ by solving
\begin{equation}\label{eq:rffa}
    \begin{split}
        \hat{\ba} := \argmin_{\ba \in \R^p} 
    \left\{ \sum_{i=1}^n \left( y_i - \hat{f}(\bx_i; \ba) \right)^2 
    + \lambda \|\ba\|_2^2 \right\} = \left( \bZ^{\!\top} \bZ + \lambda \id_p \right)^{-1} \bZ^{\!\top} \by\,, \quad \bZ \in \mathbb{R}^{n \times p}\,,
    \end{split}
\end{equation}
where the feature matrix is $[{\bm Z}]_{ij} = p^{-1/2} \varphi(\bm{x}_i; \bm{w}_j)$. We also consider min-$\ell_2$-norm solution, i.e., $\hat{\ba}_{\min} = \argmin_{\ba} \| \ba \|_2, s.t. \bZ \ba = \by$. 

Following \citet{defilippis2024dimension}, under proper assumptions on $\varphi$ (e.g., bounded, or square-integrable, continuous), we can define a compact integral operator $\mathbb{T}:L^2(\mu_\bx) \to \mathcal{V}  \subseteq L^2(\mu_\bw)$ for any $ f \in L_2(\mu_\bx)$ such that
\[
(\mathbb{T}f)(\bw) := \int_{\mathbb{R}^d} \varphi(\bx; \bw) f(\bx) \mathrm{d}\mu_\bx \,,\quad\, \mathbb{T} = \sum_{k=1}^\infty \xi_k \psi_k \phi_k^*\,,
\]
where $(\xi_k)_{k\geq1} \subseteq \R$ are the eigenvalues and $(\psi_k)_{k\geq1}$ and $(\phi_k)_{k\geq1}$ are orthonormal bases of $L^2(\mu_\bx)$ and $\mathcal{V}$ for spectral decomposition respectively. 
We denote $\bLambda := \operatorname{diag}(\xi_1^2, \xi_2^2, \ldots) \in \mathbb{R}^{\infty \times \infty}$ and assume all eigenvalues are non-zero and arranged in non-increasing order.

 Accordingly, the covariate feature matrix can be represented as $\bG \!:=\! [\bg_1, \ldots, \bg_n]^\sT \!\in\! \mathbb{R}^{n \times \infty}$ with $\bg_i := (\psi_k(\bx_i))_{k \geq 1}$ and the weight feature matrix is $\bF \!:=\! [\boldf_1, \ldots, \boldf_p]^\sT \!\in\! \mathbb{R}^{p \times \infty}$ with $\boldf_j := (\xi_k \phi_k(\bw_j))_{k \geq 1}$. Then the feature matrix can be denoted by $\bZ = \frac{1}{\sqrt{p}} \bm{G} \bm{F}^\sT \in \mathbb{R}^{n \times p}$. Note that $\boldf$ has covariance matrix $\E[\boldf\boldf^\sT]=\bLambda$, and we further introduce $\hbLambda_\bF := \E_\bz[\bz\bz^\sT|\bF] = \frac{1}{p}\bF\bF^\sT \in \R^{p \times p}$.

Assuming that $f_* \in L^2(\mu_\bx)$ admits $f_*(\bx)=\sum_{k\geq1}\btheta_{*,k}\psi_k(\bx)$, we have a similar bias-variance decomposition on the excess risk
\[
\begin{aligned}
    \mathcal{R}^{\tt RFM} := \E_{\varepsilon} \left\|\btheta_* - \frac{1}{\sqrt{p}} \bF^\sT\hat{\ba} \right\|_2^2
    = {\color{red}\left\|\btheta_* - \frac{1}{\sqrt{p}}\bF^\sT \mathbb{E}_{\varepsilon}[\hat{\ba}] \right\|_2^2} +  {\color{blue}\Tr\left(\hbLambda_\bF \mathrm{Cov}_{\varepsilon}(\hat{\ba})\right)}\,,
\end{aligned}
\]
where the first RHS term is the {\color{red}\emph{bias}}, denoted by $\mathcal{B}^{\tt RFM}_{\mathcal{R},\lambda}$, and the second term is the {\color{blue}\emph{variance}}, denoted by $\mathcal{V}^{\tt RFM}_{\mathcal{R},\lambda}$. 
Similarly, under proper assumptions (to be detailed later), we have the following deterministic equivalents, asymptotically \citep{simonmore} and non-asymptotically \citep{defilippis2024dimension}
\begin{align}
    \mathcal{B}^{\tt RFM}_{\mathcal{R},\lambda} \sim \sB^{\tt RFM}_{\sR,\lambda} :=& \frac{\nu_2^2}{1 - \Upsilon(\nu_1, \nu_2)} \big[ \< \btheta_*, (\bLambda + \nu_2\id)^{-2} \btheta_* \> \notag + \chi(\nu_2) \< \btheta_*, \bLambda (\bLambda + \nu_2\id)^{-2} \btheta_* \> \big]\,, \\
        \mathcal{V}^{\tt RFM}_{\mathcal{R},\lambda} \sim \sV^{\tt RFM}_{\sR,\lambda} :=& \sigma^2\frac{{\color{blue}\Upsilon(\nu_1, \nu_2)}}{1 - \Upsilon(\nu_1, \nu_2)}\,, \label{eq:de_risk_rf} 
\end{align}
where $(\nu_1,\nu_2)$ satisfy the self-consistent equations
\begin{equation*}\label{eq:def_nu}
        n - \frac{\lambda}{\nu_1} \!=\! \Tr ( \bLambda ( \bLambda + \nu_2 \id )^{-1} )\,,\quad p - \frac{p\nu_1}{\nu_2} \!=\! \Tr ( \bLambda ( \bLambda + \nu_2 \id )^{-1} )\,,
\end{equation*}
and $\Upsilon(\nu_1, \nu_2)$ and $\chi(\nu_2)$ are defined as
\[
\begin{aligned}
    \Upsilon(\nu_1, \nu_2) \!:= \frac{p}{n}\!\left[\! \left(\!1 \!-\! \frac{\nu_1}{\nu_2}\right)^2 \!\!\!+\!\! \left(\frac{\nu_1}{\nu_2}\right)^{2}\!\!\!\! \frac{\Tr\left(\bLambda^2 (\bLambda + \nu_2)^{-2}\right)}{p \!-\! \Tr\left(\bLambda^2 (\bLambda + \nu_2)^{-2}\right)} \right]\,, \quad \chi(\nu_2) \!:= \frac{\Tr\left(\bLambda (\bLambda + \nu_2)^{-2}\right)}{p - \Tr\left(\bLambda^2 (\bLambda + \nu_2)^{-2}\!\right)}\,.
\end{aligned}
\]
