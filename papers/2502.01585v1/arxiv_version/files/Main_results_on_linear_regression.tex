\section{Main results on linear regression}
\label{sec:linear}


\begin{figure*}[t]
    \centering
    \subfigure[Test Risk vs. $\gamma:=d/n$]{
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/risk.pdf}
    }\label{fig:linear_regression_risk_vs_norm_1}
    \subfigure[$\ell_2$ norm vs. $\gamma$]{
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/norm.pdf}
    }\label{fig:linear_regression_risk_vs_norm_2}
    \subfigure[Test Risk vs. $\ell_2$ norm]{
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/risk_vs_norm.pdf}
    }\label{fig:linear_regression_risk_vs_norm_3}
    \subfigure[Risk vs. norm ($\lambda\!=\!0.05$)]{
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/risk_vs_norm_single.pdf}
    }\label{fig:linear_regression_risk_vs_norm_4}
    \caption{Results for the ridge regression estimator. Points in these four figures are given by our experimental results, and the curves are given by our theoretical results via deterministic equivalents. Training data \(\{(\bx_i, y_i)\}_{i \in [n]}\), \(d = 1000\), sampled from a linear model \(y_i = \bx_i^\sT \bbeta_* + \varepsilon_i\), \(\sigma^2 = 0.0004\), \(\bx_i \sim \mathcal{N}(0, \bSigma)\), with \(\sigma_k(\bSigma)=k^{-1}\), \(\bbeta_{*,k}=k^{-\nicefrac{3}{2}}\).} 
    \label{fig:linear_regression_risk_vs_norm}
\end{figure*}

In this section, we study the non-asymptotic deterministic equivalent of the norm of the (ridge/ridgeless) estimator for linear regression. 
We also deliver the asymptotic results in \cref{app:asy_deter_equiv_lr}, which lays a foundation of asymptotic results for RFMs in \cref{sec:rff}.
Based on these results, we are able to mathematically characterize the test risk under norm-based capacity as shown in \cref{fig:linear_regression_risk_vs_norm}.

To deliver our results, we need the following lemma for the bias-variance decomposition of the estimator's norm.
\begin{lemma}[Bias-variance decomposition of $\mathcal{N}_{\lambda}^{\tt LS}$]
\label{lemma:biasvariance}
We have the bias-variance decomposition $\E_{\varepsilon}\|\hat{\bbeta}\|_2^2 =: \mathcal{N}_{\lambda}^{\tt LS} = \mathcal{B}^{\tt LS}_{\mathcal{N},\lambda} + \mathcal{V}^{\tt LS}_{\mathcal{N},\lambda}$, where $\mathcal{B}^{\tt LS}_{\mathcal{N},\lambda}$ and $\mathcal{V}^{\tt LS}_{\mathcal{N},\lambda}$ are defined as 
\[
\begin{aligned}
    \mathcal{B}^{\tt LS}_{\mathcal{N},\lambda} := \<\bbeta_*, (\bX^\sT\bX)^2(\bX^\sT\bX + \lambda\id)^{-2}\bbeta_*\>\,, \quad \mathcal{V}^{\tt LS}_{\mathcal{N},\lambda} := \sigma^2\Tr(\bX^\sT\bX(\bX^\sT\bX + \lambda\id)^{-2})\,.
\end{aligned}
\]
\end{lemma}

Our goal is to relate $\mathcal{B}^{\tt LS}_{\mathcal{N},\lambda}$ and $\mathcal{V}^{\tt LS}_{\mathcal{N},\lambda}$ to their respective deterministic equivalents defined below (proved in \cref{sec:linear_nonasym})
\begin{align}
    \sB_{\sN,\lambda}^{\tt LS} :=&~ \<\bbeta_*, \bSigma^2(\bSigma + \lambda_*\id)^{-2}\bbeta_*\> +{\color{red}\frac{\Tr(\bSigma(\bSigma + \lambda_*\id)^{-2})}{n}} \cdot \frac{\lambda_*^2 \langle \bbeta_*,\bSigma(\bSigma + \lambda_*\id)^{-2}\bbeta_*\rangle}{1-n^{-1}\Tr(\bSigma^2(\bSigma + \lambda_*\id)^{-2})} \,, \notag \\
    \sV_{\sN,\lambda}^{\tt LS} :=&~ \frac{\sigma^2\Tr({\color{blue}\bSigma}(\bSigma+\lambda_*\id)^{-2})}{n-\Tr(\bSigma^2(\bSigma+\lambda_*\id)^{-2})}\,. \label{eq:equiv-linear}
\end{align}
When checking \cref{eq:de_risk} and \cref{eq:equiv-linear}, we find that \textit{i)} the variance term is almost the same except that $\sV^{\tt LS}_{\sR,\lambda}$ has an additional $\bSigma$ ({\color{blue}in blue}). That means, under isotropic features $\bm \Sigma = \id_d$, they are the same.
\textit{ii)} For the bias term, we find that the second term of $\sB_{\sN,\lambda}^{\tt LS}$ ({\color{red}in red}) rescales $\sB_{\sR,\lambda}^{\tt LS}$ in \cref{eq:de_risk} by a factor $n^{-1}\Tr(\bSigma(\bSigma + \lambda_*\id)^{-2})$.

Accordingly, the norm-based capacity is able to characterize the bias and variance of the excess risk.
We will quantitatively characterize this relationship in \cref{sec:relationship_lrr}.


\subsection{Non-asymptotic analysis}
\label{sec:linear_nonasym}

To derive the non-asymptotic results, we make the following assumption on well-behaved data.
\begin{assumption}[Data concentration, \citealt{misiakiewicz2024non}]\label{ass:concentrated_LR} There exist $C_* > 0$ such that for any PSD matrix $\bA \in \mathbb{R}^{d \times d}$ with $\Tr(\bm{\Sigma A}) < \infty$ and $t\ge 0$, we have
    \[
    \begin{aligned}
         &~\mathbb{P}\left(\left| \bX^\sT \bA \bX - \Tr(\bm{\Sigma A}) \right| \geq t\|\bSigma^{1/2} \bA \bSigma^{1/2}\|_{\mathrm{F}} \right) \leq C_* e^{-\frac{t}{C_*}}\,.
    \end{aligned}
    \]
\end{assumption}


\begin{assumption}[Power-law assumption]\label{ass:powerlaw}
For the covariance matrix $\bm \Sigma$ and the target function $\bm \beta_*$, we assume that $ \sigma_k(\bm \Sigma) = k^{-\alpha}, \alpha >0$ and $ \bbeta_{*,k} =k^{-\nicefrac{\alpha\beta}{2}},  \beta \in \mathbb{R}$.
\end{assumption}

This assumption is close to classical source condition and capacity condition~\citep{caponnetto2007optimal} and is similarly used in \citet[Assumption 1]{paquette20244+}.

Based on the above two assumptions, we are ready to deliver the following result, see the proof in \cref{app:nonasy_deter_equiv_lr}.
\begin{theorem}[Deterministic equivalents of $\mathcal{N}_{\lambda}^{\tt LS}$, simplified version of \cref{prop:det_equiv_LR}, see \cref{fig:linear_regression_risk_vs_norm}]\label{prop:non-asy_equiv_norm_LR}
    Under \cref{ass:concentrated_LR} and \ref{ass:powerlaw}, for any $D,K >0$, if $\lambda > n^{-K}$, with probability at least $1-n^{-D}$, we have 
    \[
    \left|\mathcal{B}^{\tt LS}_{\mathcal{N},\lambda} - \sB_{\sN,\lambda}^{\tt LS}\right| \leq \widetilde{\mathcal{O}} (n^{-\frac{1}{2}}) \cdot \sB_{\sN,\lambda}^{\tt LS} \quad \text{and} \quad \left|\mathcal{V}^{\tt LS}_{\mathcal{N},\lambda} - \sV_{\sN,\lambda}^{\tt LS}\right| \leq \widetilde{\mathcal{O}} (n^{-\frac{1}{2}}) \cdot \sV_{\sN,\lambda}^{\tt LS}\,,
    \]
where these quantities are from \cref{lemma:biasvariance} and \cref{eq:equiv-linear}.
\end{theorem}
\noindent{\bf Remark:} Our results are numerically validated by \cref{fig:linear_regression_risk_vs_norm}. Besides, our theory is still valid under weaker assumptions related to \emph{effective dimension} used in \citet{misiakiewicz2024non} but the formulation will be more complex. We detail this in \cref{app:nonasy_deter_equiv_lr}.
The results for min-$\ell_2$-norm interpolator are given by \cref{prop:asy_equiv_norm_LR_minnorm}. We show that the solution $\lambda_n$ to the self-consistent equation $\Tr(\bSigma(\bSigma+\lambda_n\id)^{-1}) \sim n$ can be obtained from the variance $\sV_{\sN,0}^{\tt LS}=\sigma^2/\lambda_n$.


\subsection{Relationship between test risk and norm}
\label{sec:relationship_lrr}

Here we give some concrete examples on the relationship between $\sR$ and  $\sN$ in terms of isotropic features and power-law setting, see the proof in \cref{app:relationship}.

\begin{proposition}[Isotropic features for ridge regression, see \cref{fig:lampls}]\label{prop:relation_id}
    Consider covariance matrix $\bSigma = \id_d$, the deterministic equivalents $\sR^{\tt LS}_{\lambda}$ and $\sN^{\tt LS}_{\lambda}$ satisfy
    \[
    \begin{aligned}
        & \left(\|\bbeta_*\|_2^2 - \sR^{\tt LS}_{\lambda} - \sN^{\tt LS}_{\lambda}\right)\left(\|\bbeta_*\|_2^2 + \sR^{\tt LS}_{\lambda} - \sN^{\tt LS}_{\lambda}\right)^2d + 2\|\bbeta_*\|_2^2\left(\left(\|\bbeta_*\|_2^2 + \sR^{\tt LS}_{\lambda} - \sN^{\tt LS}_{\lambda}\right)^2-4\|\bbeta_*\|_2^2\sR^{\tt LS}_{\lambda} \right) \lambda\\
        &= 2\left( \left(\sR^{\tt LS}_{\lambda} - \sN^{\tt LS}_{\lambda}\right)^2 - \|\bbeta_*\|_2^4 \right) d \sigma^2\,.
    \end{aligned}
    \]
\end{proposition}
\vspace{-0.2cm}
\noindent{\bf Remark:} $\sR^{\tt LS}_\lambda$ and $\sN^{\tt LS}_\lambda$ formulates a third-order polynomial.
When $\lambda \to \infty$, it degenerates to $\sR^{\tt LS}_{\lambda} = (\|\bbeta_*\|_2 - \sqrt{\sN^{\tt LS}_{\lambda}})^2 $ when $ \sN^{\tt LS}_{\lambda} \leq \|\bbeta_*\|_2$.
Hence \( \sR^{\tt LS}_{\lambda} \) is monotonically decreasing with respect to \( \sN^{\tt LS}_{\lambda} \), empirically verified by \cref{fig:lampls}.
Besides, if we take $\lambda = \frac{d\sigma^2}{\|\bbeta_*\|_2^2}$, which is the {\bf optimal regularization parameter} discussed in \citet{wu2020optimal, nakkiran2020optimal}, the relationship in \cref{prop:relation_id} will become $\sR^{\tt LS}_{\lambda} = \|\bbeta_*\|_2^2 - \sN^{\tt LS}_{\lambda}$, which corresponds to a straight line. This is empirically shown in \cref{fig:lampls} with $\lambda=50$.
   

Apart from sufficiently large $\lambda$ and optimal $\lambda$ mentioned before, below we consider min-$\ell_2$-norm estimator. 
Note that when $\lambda \to 0$, the ridge regression estimator $\hat{\bbeta}$ converges to the min-$\ell_2$-norm estimator $\hat{\bbeta}_{\min}$.
However, the behavior of \(\lambda_*\) differs between the under-parameterized and over-parameterized regimes as \(\lambda \to 0\). In the under-parameterized regime, \(\lambda_*\) approaches 0, while in the over-parameterized regime, \(\lambda_*\) approaches a constant that satisfies \(\Tr(\bSigma(\bSigma + \lambda_n \id)^{-1}) = n\).
Thus, the min-\(\ell_2\)-norm estimator requires {\bf separate analysis of the two regimes}. 


\begin{figure}[t]
    \centering
    \subfigure[Test Risk vs.\ $\ell_2$-norm]{\label{fig:lampls}
        \includegraphics[width=0.3\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/linear_regression_risk_vs_norm_id_multi_lambda.pdf}
    }
    \subfigure[Risk vs.\ norm ($\lambda \to 0$)]{\label{fig:lam0ls}
        \includegraphics[width=0.3\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/risk_vs_norm_id_ridgeless.pdf}
    }
    \caption{Relationship between $\sR^{\tt LS}_\lambda$ and $\sN^{\tt LS}_\lambda$ in \cref{fig:lampls}; $\sR^{\tt LS}_0$ and $\sN^{\tt LS}_0$ in \cref{fig:lam0ls} under the linear model \(y_i = \bx_i^\sT \bbeta_* + \varepsilon_i\), with $d=500$, \(\bSigma = \id_d\), \(\|\bbeta_*\|_2^2=10\), and \(\sigma^2 = 1\).} %\fh{this is for isotropic data?} 
    \label{fig:linear_risk}\vspace{-0.2cm}
\end{figure}


\begin{proposition}[Relationship for min-$\ell_2$-norm interpolator in the {\bf under-parameterized} regime]\label{prop:relation_minnorm_underparam}
The deterministic equivalents $\sR^{\tt LS}_{0}$ and $\sN^{\tt LS}_{0}$, in under-parameterized regimes ($d < n$) admit the linear relationship
\[
    \begin{aligned}
        \sR^{\tt LS}_0 = {d}\left(\sN^{\tt LS}_0 - \|\bbeta_*\|_2^2\right)/{\Tr(\bSigma^{-1})}\,.
    \end{aligned}
\]
\end{proposition}

The relationship in the over-parameterized regime is more complicated. We present it in the special case of isotropic features in \cref{prop:relation_minnorm_id} of \cref{prop:relation_id}, and we also give an approximation in \cref{prop:relation_minnorm_pl} under the power-law assumption.


\begin{corollary}[Isotropic features for min-$\ell_2$-norm interpolator, see \cref{fig:lam0ls}]\label{prop:relation_minnorm_id}
    Consider covariance matrix $\bSigma = \id_d$, the relationship between $\sR^{\tt LS}_0$ and $\sN^{\tt LS}_0$ from under-parameterized to over-parameterized regimes admit
    \begin{equation*}
		\sR^{\tt LS}_0 = \left\{
		\begin{array}{rcl}
			\begin{aligned}
				&  \sN^{\tt LS}_0 - \|\bbeta_*\|_2^2\,,  ~~\text{if}~~ d<n ~\mbox{(under-parameterized)} ; \\
				& \sqrt{\left[\sN^{\tt LS}_0 - (\|\bbeta_*\|_2^2 - \sigma^2)\right]^2 + 4\|\bbeta_*\|_2^2 \sigma^2 } - \sigma^2 \,, \mbox{o/w}\,.
			\end{aligned}
		\end{array} \right.
    \end{equation*}
    For the variance part of $\sR^{\tt LS}_0$ and $\sN^{\tt LS}_0$, we have $\sV_{\sR,0}^{\tt LS} = \sV_{\sN,0}^{\tt LS}$; For the respective bias part, we have $\sB_{\sR,0}^{\tt LS} + \sB_{\sN,0}^{\tt LS} = \| \bm \beta_* \|_2^2$.
\end{corollary}
\noindent{\bf Remark:} 
In the under-parameterized regime, the test error $\sR^{\tt LS}_0$ is a linear function of the norm $\sN^{\tt LS}_0$. 
In the over-parameterized regime, $\sR^{\tt LS}_0$ and $\sN^{\tt LS}_0$ formulates a rectangular hyperbola: $\sR^{\tt LS}_0$ decreases with $\sN^{\tt LS}_0$ if $\sN^{\tt LS}_0 < \|\bbeta_*\|_2^2 - \sigma^2$ while $\sR^{\tt LS}_0$ increases with $\sN^{\tt LS}_0$ if $\sN^{\tt LS}_0 > \|\bbeta_*\|_2^2 - \sigma^2$.

Instead of assuming $\bSigma = \id_d$, we consider power-law features in \cref{ass:powerlaw} and characterize the relationship.
\begin{proposition}[Power-law features for min-$\ell_2$ norm estimator]\label{prop:relation_minnorm_pl}
    Under \cref{ass:powerlaw}, in the over-parameterized regime ($d>n$), we consider some special cases for analytic formulation: if $\alpha=1$, when $n \to d$, we have\footnote{The symbol $\approx$ here represents two types of approximations: i) approximation for self-consistent equations; ii) Taylor approximation of logarithmic function around zero (related to $n \to d$).}
    \[
    \begin{aligned}
        \sV_{\sR, 0}^{\tt LS} \approx \frac{2(\sV_{\sN, 0}^{\tt LS})^2}{d\sV_{\sN, 0}^{\tt LS}-d^2\sigma^2}\,, 
    \end{aligned}
    \]
    and further for different $\beta$, when $n \to d$, we have
    \begin{equation*}
		\sB_{\sR,0}^{\tt LS} \approx \left\{
		\begin{array}{rcl}
			\begin{aligned}
                    &  \frac{2\sB_{\sN, 0}^{\tt LS}(d-\sB_{\sN,0}^{\tt LS})}{d^2}\,,  ~~\text{$\beta=0$} ; \\
                    & \frac{2(\sB_{\sN, 0}^{\tt LS} - \Tr(\bSigma))}{d\sqrt{1+2\sB_{\sN, 0}^{\tt LS}-2\Tr(\bSigma)}} \,, ~~\text{$\beta=1$} ; \\
                    & \frac{216 (\sB_{\sN, 0}^{\tt LS})^4 \!-\! 324d^2 (\sB_{\sN, 0}^{\tt LS})^3 \!+\! 126d^4 (\sB_{\sN, 0}^{\tt LS})^2 \!+\! d^6 \sB_{\sN, 0}^{\tt LS} \!-\! 5d^8}{2d^5(6 \sB_{\sN, 0}^{\tt LS}-d^2)} \,, ~~\text{$\beta=-1$}.
			\end{aligned}
		\end{array} \right.
    \end{equation*}
\end{proposition}
\noindent{\bf Remark:}
The relationship between $\sR^{\tt LS}_0$ and $\sN^{\tt LS}_0$ is quite complex in the over-parameterized regime. We characterize some special cases here and find that they are still precise by our experiments in \cref{fig:linear_regression_power_law}.

\begin{figure*}[!ht]
    \centering
    \subfigure[\(\beta = 0\)]{\label{fig:lrpla}
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/beta_0_B.pdf}
    }
    \subfigure[\(\beta = 1\)]{\label{fig:lrplb}
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/beta_1_B.pdf}
    }
    \subfigure[\(\beta = -1\)]{\label{fig:lrplc}
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/beta_-1_B.pdf}
    }
    \subfigure[$\sV_{\sR,0}^{\tt LS}$ vs. $\sV_{\sN,0}^{\tt LS}$]{\label{fig:lrpld}
        \includegraphics[width=0.22\textwidth]{arxiv_version/figures/Main_results_on_linear_regression/V.pdf}
    }
    \caption{\cref{fig:lrpla,fig:lrplb,fig:lrplc} show the relationship between $\sB_{\sR,0}^{\tt LS}$ and $\sB_{\sN,0}^{\tt LS}$ when $\alpha =1$ and $\beta$ takes on different values. \cref{fig:lrpld} shows the relationship between $\sV_{\sR,0}^{\tt LS}$ and $\sV_{\sN,0}^{\tt LS}$ when $\alpha = 1$. The {\color{blue}blue line} is the relationship obtained by deterministic equivalent experiments, and the {\color{red}red line} is the approximate relationship we give.}
    \label{fig:linear_regression_power_law}
\end{figure*}