\section{Experiment}
\label{app:experiment}

In this section, we investigate the relationship between test loss and different norm-based capacities for two-layer fully connected neural networks. Specifically, we evaluate four norm-based capacities: \textbf{Frobenius norm}, \textbf{Frobenius distance}, \textbf{spectral complexity}, and \textbf{path norm}. Our empirical results indicate that the path norm is the most suitable model capacity among these three norm-based capacities, which coincides with 
\citet{jiang2019fantastic}.

In our experiments, we use a balanced subset of the MNIST dataset, consisting of 4,000 training samples from all the 10 classes. To simulate real-world noisy data, a noise level $\eta$ is introduced, meaning $\eta \cdot 100\%$ of the training labels are randomly corrupted. 

The model is chosen as a two-layer fully connected neural network with parameters including a bias term. The network is initialized using the Xavier initialization scheme and trained using the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.1 and momentum of 0.95 over 2,000 epochs. During training, a batch size of 128 is used. 

To control model complexity, we vary the number of neurons in the hidden layer, thereby adjusting the number of model parameters. To ensure the robustness of the results, each experiment is repeated 10 times for each hidden layer dimension. The modelâ€™s performance is evaluated using the Mean Squared Error (MSE) loss on both the training and test sets.

{\bf Frobenius norm:}
The parameter Frobenius norm is defined as for such two-layer neural networks
\[
\mu_{\text{fro}}(f_\bw) = \sum_{j=1}^{2} \|\bW_j\|_{\mathrm{F}}^2\,,
\]
where \(\bW_j\) is the parameter matrix of layer \(j\).

\cref{fig:two-layer_NNs_risk_vs_frobenius_norm_noise_0.2} illustrates the relationship between test loss, Frobenius norm \(\mu_{\text{fro}}\), and the number of parameters \(p\). As the number of model parameters increases, the test loss exhibits the typical double descent phenomenon. However, the Frobenius norm consistently increases monotonically (with a slowdown in the growth rate in the over-parameterized regime). Consequently, when using the Frobenius norm as a measure of model capacity, the double descent phenomenon remains observable.


\begin{figure*}[!ht]
    \centering
    \subfigure[Test (training) Loss vs. \(p\)]{\label{fig:fro_a}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_0.2.pdf}
    }
    \subfigure[\(\mu_{\text{fro}}\) vs. \(p\)]{
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/frobenius_norm_0.2.pdf}
    }
    \subfigure[Test Loss vs. \(\mu_{\text{fro}}\)]{
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_vs_Frobenius_Norm_0.2.pdf}
    }
    \caption{Experiments on two-layer fully connected neural networks with noise level $\eta=0.2$. The \textbf{left} figure shows the relationship between test (training) loss and the number of the parameters \(p\). The \textbf{middle} figure shows the relationship between the Frobenius norm \(\mu_{\text{fro}}\) and \(p\). The \textbf{right} figure shows the relationship between the test loss and \(\mu_{\text{fro}}\).}
    \label{fig:two-layer_NNs_risk_vs_frobenius_norm_noise_0.2}
\end{figure*}

{\bf Frobenius distance:}
The Frobenius distance is defined as for such two-layer neural networks
\[
\mu_{\text{fro-dis}}(f_\bw) = \sum_{j=1}^{2} \|\bW_j - \bW_j^0\|_{\mathrm{F}}^2\,,
\]
where \(\bW_j^0\) is the initialization of \(\bW_j^0\).

\begin{figure*}[!ht]
    \centering
    \subfigure[Test (training) Loss vs. \(p\)]{
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_0.2.pdf}
    }
    \subfigure[\(\mu_{\text{fro-dis}}\) vs. \(p\)]{
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/frobenius_distance_0.2.pdf}
    }
    \subfigure[Test Loss vs. \(\mu_{\text{fro-dis}}\)]{
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_vs_Frobenius_Distance_0.2.pdf}
    }
    \caption{Experiments on two-layer fully connected neural networks with noise level $\eta=0.2$. The \textbf{left} figure is the same as \cref{fig:fro_a}. The \textbf{middle} figure shows the relationship between the Frobenius distance \(\mu_{\text{fro-dis}}\) and \(p\). The \textbf{right} figure shows the relationship between the test loss and \(\mu_{\text{fro-dis}}\) .}
    \label{fig:two-layer_NNs_risk_vs_frobenius_distance_noise_0.2}
\end{figure*}

\cref{fig:two-layer_NNs_risk_vs_frobenius_distance_noise_0.2} illustrates the relationship between test loss, Frobenius distance \(\mu_{\text{fro}}\), and the number of parameters \(p\). Different from Frobenius norm, Frobenius distance monotonically increases in the under-parameterized regime, but shows a decrease in the over-parameterized regime. However, since the change of Frobenius distance in the over-parameterized regime is gentle and even eventually appears to rise, using Frobenius distance as the model capacity does not reflect the generalization capacity of the model.


{\bf Spectral complexity:}
The spectral complexity is defined as for such two-layer neural networks
\[
\mu_{\text{spec}}(f_\bw) = \left( \prod_{i=1}^{2} \|\bW_i\| \right) \left( \sum_{i=1}^{2} \frac{\|\bW_i\|_{2,1}^{2/3}}{\|\bW_i\|^{2/3}} \right)^{3/2}\,,
\]
where \(\|\cdot\|\) denote the spectral norm, and \(\|\cdot\|_{p,q}\) denotes the \((p,q)\)-norm of a matrix, defined as \(\|\bM\|_{p,q} := \|\left(\|\bM_{:,1}\|_p,\cdots,\|\bM_{:,m}\|_p\right)\|_q\) for \(\bM \in \R^{d\times m}\).

\begin{figure*}[!ht]
    \centering
    \subfigure[Test (training) Loss vs. \(p\)]{
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_0.2.pdf}
    }
    \subfigure[\(\mu_{\text{spec}}\) vs. \(p\)]{
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/spectral_complexity_0.2.pdf}
    }
    \subfigure[Test Loss vs. \(\mu_{\text{spec}}\)]{
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_vs_Spectral_Complexity_0.2.pdf}
    }
    \caption{Experiments on two-layer fully connected neural networks with noise level $\eta=0.2$. The \textbf{left} figure is the same as \cref{fig:fro_a}. The \textbf{middle} figure shows the relationship between the path norm \(\mu_{\text{spec}}\) and \(p\). The \textbf{right} figure shows the relationship between the test loss and \(\mu_{\text{spec}}\).}
    \label{fig:two-layer_NNs_risk_vs_spectral_complexity_noise_0.2}
\end{figure*}

\cref{fig:two-layer_NNs_risk_vs_spectral_complexity_noise_0.2} illustrates the relationship between test loss, Spectral complexity \(\mu_{\text{spec}}\), and the number of parameters \(p\). We can see that \(\mu_{\text{spec}}\) increases monotonically with \(p\), so the same double descent phenomenon occurs with spectral complexity as model capacity.

{\bf Path norm:} The path norm is defined as 
\[
\mu_{\text{path-norm}}(f_\bw) = \sum_{i} f_{\bw^2}(\bm{1})[i],
\]
where \(\bw^2 = \bw \circ \bw\) is the element-wise square of the parameters, and \(\bm{1}\) for all-one vector. The path norm represents the sum of the outputs of the neural network after squaring all the parameters and inputting the all-one vector.

\cref{fig:two-layer_NNs_risk_vs_path_norm_noise_0.2} illustrates the relationship between test loss, Path norm \(\mu_{\text{path}}\), and the number of parameters \(p\). Path norm increases monotonically in the under-parameterized regime and decreases monotonically in the over-parameterized regime. This behavior resembles that of the \(\ell_2\) norm of random feature estimators. Additionally, the relationship between test loss and path norm forms a U-shaped curve in the under-parameterized regime and increases monotonically in the over-parameterized regime. This pattern is strikingly similar to the relationship between test loss and the \(\ell_2\) norm in random feature models.


\begin{figure*}[!ht]
    \centering
    \subfigure[Test (training) Loss vs. \(p\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.2_1}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_0.2.pdf}
    }
    \subfigure[\(\mu_{\text{path-norm}}\) vs. \(p\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.2_2}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/path_norm_0.2.pdf}
    }
    \subfigure[Test Loss vs. \(\mu_{\text{path-norm}}\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.2_3}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_vs_path_norm_0.2.pdf}
    }
    \caption{Experiments on two-layer fully connected neural networks with noise level $\eta=0.2$. The \textbf{left} figure is the same as \cref{fig:fro_a}. The \textbf{middle} figure shows the relationship between the path norm \(\mu_{\text{path-norm}}\) and \(p\). The \textbf{right} figure shows the relationship between the test loss and the path norm.}
    \label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.2}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \subfigure[Test (training) Loss vs. \(p\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.1_1}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_0.1.pdf}
    }
    \subfigure[\(\mu_{\text{path-norm}}\) vs. \(p\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.1_2}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/path_norm_0.1.pdf}
    }
    \subfigure[Test Loss vs. \(\mu_{\text{path-norm}}\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.1_3}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_vs_path_norm_0.1.pdf}
    }
    \caption{Experiments on two-layer fully connected neural networks with noise level $\eta=0.1$.}
    \label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.1}
\end{figure*}


\begin{figure*}[!ht]
    \centering
    \subfigure[Test (training) Loss vs. \(p\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.3_1}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_0.3.pdf}
    }
    \subfigure[\(\mu_{\text{path-norm}}\) vs. \(p\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.3_2}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/path_norm_0.3.pdf}
    }
    \subfigure[Test Loss vs. \(\mu_{\text{path-norm}}\)]{\label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.3_3}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/two_layer_NNs/loss_vs_path_norm_0.3.pdf}
    }
    \caption{Experiments on two-layer fully connected neural networks with noise level $\eta=0.3$.}
    \label{fig:two-layer_NNs_risk_vs_path_norm_noise_0.3}
\end{figure*}

Besides, we also conduct experiments with the noise level \(\eta=0.1\) and \(\eta=0.3\) in
\cref{fig:two-layer_NNs_risk_vs_path_norm_noise_0.1,fig:two-layer_NNs_risk_vs_path_norm_noise_0.3}, respectively. 
We can see that, when the noise level increases, we observe stronger peaks in the test loss for double descent. However, the trend of test loss is similar at different noise levels with Path norm \(\mu_{\text{path-norm}}\) as the model capacity, i.e., it shows a U-shape at the under-parameterized regime and an almost linear relationship at the over-parameterized regime.

These observations demonstrates the relationship between the test loss and norm, which is general, not limited to RFMs in the main text.


