\section{Discussion with other model capacities}
\label{sec:discussion}

In this section, we discuss two other model capacities: {\em generalized effective number of parameters} and {\em degrees of freedom}, which are widely used to describe a model's generalization ability. From this discussion, we conclude that these two capacities are less suitable compared to norm-based model capacity.

\paragraph{Generalized Effective Number of Parameters} \citet{curth2024u} assesses model complexity from the perspective of smoother by introducing a variance-based effective-parameter measure, termed the {\bf generalized effective number of parameters}. In the context of ridge regression, this measure is given by
\[
\begin{aligned}
    p_{\hat{\bm{s}}}^{\text{test}} = \frac{n}{|\mathcal{I}_{\text{test}}|}\sum_{j \in \mathcal{I}_{\text{test}}} \|\bx_{j}^{\text{test}}(\bX^\sT \bX + \lambda)^{-1}\bX^\sT\|_2^2\,,
\end{aligned}
\]
where $\{\bx_{j}^{\text{test}}\}_{j\in\mathcal{I}_{\text{test}}}$ is the set of test inputs. Taking the expectation with respect to the test set yields
\[
    p_{\hat{\bm{s}}}^{\text{test}} = n \E_{\bx_{j}^{\text{test}}} \|\bx_{j}^{\text{test}}(\bX^\sT \bX + \lambda)^{-1}\bX^\sT\|_2^2 = n\Tr(\bSigma\bX^\sT\bX(\bX^\sT\bX + \lambda)^{-2})\,,
\]
which corresponds to the variance of the test risk $\mathcal{V}^{\tt LS}_\mathcal{R}$ scaled by the factor $\frac{n}{\sigma^2}$. 

For the random feature ridge regression, the generalized effective number of parameters can be similarly given by
\[
    p_{\hat{\bm{s}}}^{\text{test}} = n \E_{\bz_{j}^{\text{test}}} \|\bz_{j}^{\text{test}}(\bZ^\sT \bZ + \lambda)^{-1}\bZ^\sT\|_2^2 = n\Tr(\hbLambda_\bF\bZ^\sT\bZ(\bZ^\sT\bZ + \lambda)^{-2})\,,
\]
which corresponds to the variance of the test risk $\mathcal{V}^{\tt RFM}_\mathcal{R}$ scaled by the factor $\frac{n}{\sigma^2}$.

The connection between variance and \( p_{\hat{\bm{s}}}^{\text{test}} \) enables it to effectively capture the variance of test risk. However, due to the lack of information about the target function (without label information $y$), this model capacity cannot fully describe the behavior of test risk, as it neglects the bias component. This limitation becomes apparent when the test risk is dominated by bias.


\paragraph{Degrees of freedom} For linear ridge regression, another measure of model capacity, known as the ``degrees of freedom'' \citep{caponnetto2007optimal, hastie2017generalized, bach2024high}, is defined as
\[
     {\rm df}_1(\lambda_*) := \Tr(\bSigma(\bSigma + \lambda_*)^{-1})\,, \quad {\rm df}_2(\lambda_*) := \Tr(\bSigma^2(\bSigma + \lambda_*)^{-2})\,.
\]
\({\rm df}_1(\lambda_*)\) and \({\rm df}_2(\lambda_*)\) measures the number of ``effective'' parameters the model can fit. As the regularization strength \(\lambda\) increases, model complexity decreases. From \cref{def:effective_regularization}, we have \(n - \frac{\lambda}{\lambda_*} = \Tr(\bSigma(\bSigma+\lambda_*)^{-1})\), implying that an increase in \(\lambda\) raises \(\lambda_*\), leading to a reduction in \({\rm df}_1(\lambda_*)\) and \({\rm df}_2(\lambda_*)\). This suggests that degrees of freedom can, to some extent, represent model complexity.

However, it is worth noting that since in linear ridge regression we only vary the number of training data \(n\), according to the self-consistent equation \(n - \frac{\lambda
}{\lambda_*} = \Tr(\bSigma(\bSigma+\lambda_*)^{-1})\) we can tell that \(\lambda_*\) decreases monotonically as \(n\) increases, which leads to \({\rm df}_1\) and \({\rm df}_2\) increasing monotonically as \(n\) increases. This monotonic relationship with \(n\) suggests that when using degrees of freedom as a measure of model capacity, the double descent phenomenon still exists, as the effective capacity of the model continues to increase even beyond the interpolation threshold.

Similar to the generalized effective number of parameters mentioned above, these degrees of freedom also lack information about the target function, making them insufficient for accurately capturing the model's generalization ability.


\cref{fig:model_capacity_discussion} illustrates the relationship between test risk and different model capacity for linear ridge regression. It shows that double descent persists for degrees of freedom \({\rm df}_1\) and \({\rm df}_2\), indicating that degrees of freedom is not an appropriate measure of model capacity.

\begin{figure*}[t]
    \centering
    \subfigure[Test Risk vs. Norm]{\label{fig:mcda}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/model_capacity_discussion/risk_vs_norm.pdf}
    }
    \subfigure[Test Risk vs. ${\rm df}_1$]{\label{fig:mcdc}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/model_capacity_discussion/risk_vs_df1.pdf}
    }
    \subfigure[Test Risk vs. ${\rm df}_2$]{\label{fig:mcdd}
        \includegraphics[width=0.30\textwidth]{arxiv_version/figures/model_capacity_discussion/risk_vs_df2.pdf}
    }
    \caption{Relationship between test risk and different model capacities. Training data \(\{(\bx_i, y_i)\}_{i \in [n]}\), \(d = 1000\), sampled from a linear model \(y_i = \bx_i^\sT \bbeta_* + \varepsilon_i\), \(\sigma^2 = 0.0004\), \(\bx_i \sim \mathcal{N}(0, \bSigma)\), with \(\sigma_k(\bSigma)=k^{-1}\), \(\bbeta_{*,k}=k^{-\nicefrac{3}{2}}\).} 
    \label{fig:model_capacity_discussion}
\end{figure*}