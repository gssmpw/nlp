\subsection{Asymptotic deterministic equivalence for ridge regression}
\label{app:asy_deter_equiv_lr}

In this section, we establish the asymptotic approximation guarantees for linear regression, focusing on the relationships between the $\ell_2$ norm of the estimator and its deterministic equivalent.
These results can be recovered by our non-asymptotic results, but we put them here just for completeness.

Before presenting the results on deterministic equivalence for ridge regression and their proofs, we begin by introducing a couple of useful corollaries from \cref{prop:spectral,prop:spectralK}.

\begin{corollary}
\label{prop:spectral2}
    Under the same condition of \cref{prop:spectral}, we have
    \begin{align}\label{eq:trA3}
        \Tr ( \bA \bX^\sT \bX ( \bX^\sT \bX + \lambda )^{-2}) \sim&~ \frac{\Tr(\bA\bSigma(\bSigma + \lambda_*\id)^{-2})}{n - {\rm df}_2(\lambda_*)}\,.
        \end{align}
        Specifically, if $\bA = \bSigma$, we have
        \begin{align}\label{eq:trS3}
        \Tr ( \bSigma \bX^\sT \bX ( \bX^\sT \bX + \lambda )^{-2}) \sim&~ \frac{{\rm df}_2(\lambda_*)}{n - {\rm df}_2(\lambda_*)}\,.
    \end{align}
\end{corollary}

\begin{corollary}
\label{prop:spectralK2}
Under the same condition of \cref{prop:spectralK}, we have
\begin{align}
\label{eq:trA3K}
\Tr ( \bA \bT^\sT ( \bT \bSigma \bT^\sT + \lambda )^{-2} \bT) \sim&~ \frac{\Tr ( \bA ( \bSigma + \lambda_* )^{-2})}{ n -  {\rm df}_2(\lambda_*) }\,.
\end{align}
\end{corollary}

Using the equation 
\[
\Tr ( \bA \bX^\sT \bX ( \bX^\sT \bX + \lambda )^{-2}) = \frac{1}{\lambda} \left( \Tr ( \bA \bX^\sT \bX ( \bX^\sT \bX + \lambda )^{-1}) - \Tr ( \bA (\bX^\sT \bX)^2 ( \bX^\sT \bX + \lambda )^{-2}) \right)\,,
\] 
we can directly obtain \cref{prop:spectral2,prop:spectralK2} from \cref{prop:spectral,prop:spectralK}

After introduce the two corollaries above, we first give the proof of the bias-variance decomposition in \cref{lemma:biasvariance}.

\begin{proof}[Proof of \cref{lemma:biasvariance}]
Here we give the bias-variance decomposition of $\E_{\varepsilon}\|\hat{\bbeta}\|_2^2$. The formulation of $\E_{\varepsilon}\|\hat{\bbeta}\|_2^2$ is given by
\[
\begin{aligned}
     \E_{\varepsilon}\|\hat{\bbeta}\|_2^2 =\|\left( \bX^\sT \bX + \lambda \id \right)^{-1} \bX^\sT \by \|_2^2\,,
\end{aligned}
\]
which can be decomposed as
\[
\begin{aligned}
    \E_{\varepsilon}\|\hat{\bbeta}\|_2^2 =&~ \E_{\varepsilon}\|\left( \bX^\sT \bX + \lambda \id \right)^{-1} \bX^\sT (\bX\bbeta_* + \bm\varepsilon) \|_2^2\\
    =&~ \|\left( \bX^\sT \bX + \lambda \id \right)^{-1} \bX^\sT \bX\bbeta_* \|_2^2 + \E_{\varepsilon}\|\left( \bX^\sT \bX + \lambda \id \right)^{-1} \bX^\sT \bm\varepsilon \|_2^2\\
    =&~\<\bbeta_*, (\bX^\sT\bX)^2(\bX^\sT\bX + \lambda\id)^{-2}\bbeta_*\> + \sigma^2\Tr(\bX^\sT\bX(\bX^\sT\bX + \lambda\id)^{-2})\\
    =:&~ \mathcal{B}_{\mathcal{N},\lambda}^{\tt LS} + \mathcal{V}_{\mathcal{N},\lambda}^{\tt LS}\,.
\end{aligned}
\]
Accordingly, we can see that it shares the similar spirit with the bias-variance decomposition.
\end{proof}

Now we are ready to derive the deterministic equivalence, i.e., $\E_{\varepsilon}\|\hat{\bbeta}\|_2^2$, under the bias-variance decomposition. Our results can handle ridge estimator $\hat{\bbeta}$ in \cref{prop:asy_equiv_norm_LR} and interpolator $\hat{\bbeta}_{\min}$ in \cref{prop:asy_equiv_norm_LR_minnorm}, respectively.
\begin{proposition}[Asymptotic deterministic equivalence of the norm of ridge regression estimator]\label{prop:asy_equiv_norm_LR}
    Given the bias variance decomposition of $\E_{\varepsilon}\|\hat{\bbeta}\|_2^2$ in \cref{lemma:biasvariance}, 
    under \cref{ass:asym}, we have the following asymptotic deterministic equivalents $\mathcal{N}^{\tt LS}_{\lambda}  \sim \sN^{\tt LS}_{\lambda} := \sB_{\sN,\lambda}^{\tt LS} + \sV_{\sN,\lambda}^{\tt LS}$ such that $\mathcal{B}^{\tt LS}_{\mathcal{N},\lambda} \sim \sB_{\sN,\lambda}^{\tt LS}$, $\mathcal{V}^{\tt LS}_{\mathcal{N},\lambda} \sim \sV_{\sN,\lambda}^{\tt LS}$, where $\sB_{\sN,\lambda}^{\tt LS}$ and $\sV_{\sN,\lambda}^{\tt LS}$ are defined by \cref{eq:equiv-linear}.
\end{proposition}


And we present the proof of \cref{prop:asy_equiv_norm_LR} as below.
\begin{proof}[Proof of \cref{prop:asy_equiv_norm_LR}]
We give the asymptotic deterministic equivalents for $\mathcal{B}_{\mathcal{N},\lambda}^{\tt LS}$ and $\mathcal{V}_{\mathcal{N},\lambda}^{\tt LS}$, respectively. For the bias term $\mathcal{B}_{\mathcal{N},\lambda}^{\tt LS}$, we use \cref{eq:trAB1} by taking $\bA = \bbeta_*\bbeta_*^\sT$ and $\bB = \id$ and thus obtain
\[
\begin{aligned}
    \mathcal{B}_{\mathcal{N},\lambda}^{\tt LS} = &~ \<\bbeta_*, (\bX^\sT\bX)^2(\bX^\sT\bX + \lambda\id)^{-2}\bbeta_*\>\\
    = &~ \Tr(\bbeta_*\bbeta_*^\sT(\bX^\sT\bX)^2(\bX^\sT\bX + \lambda\id)^{-2})\\
    \sim &~ \Tr(\bbeta_*\bbeta_*^\sT\bSigma^2(\bSigma + \lambda_*\id)^{-2}) + \lambda_*^2 \Tr(\bbeta_*\bbeta_*^\sT\bSigma(\bSigma + \lambda_*\id)^{-2}) \cdot \Tr(\bSigma(\bSigma + \lambda_*\id)^{-2}) \cdot \frac{1}{n-\Tr(\bSigma^2(\bSigma + \lambda_*\id)^{-2})}\\
    = &~ \<\bbeta_*, \bSigma^2(\bSigma + \lambda_*\id)^{-2}\bbeta_*\> + \frac{\Tr(\bSigma(\bSigma + \lambda_*\id)^{-2})}{n} \cdot \frac{\lambda_*^2 \<\bbeta_*,\bSigma(\bSigma + \lambda_*\id)^{-2}\bbeta_*\>}{1-n^{-1}\Tr(\bSigma^2(\bSigma + \lambda_*\id)^{-2})}\\
    =: &~ \sB_{\sN, \lambda}^{\tt LS}\,.
\end{aligned}
\]
For the variance term $\mathcal{V}_{\mathcal{N}}^{\tt LS}$, we use \cref{eq:trA3} by taking $\bA = \id$ and obtain
\[
\begin{aligned}
    \mathcal{V}_{\mathcal{N}}^{\tt LS} = &~ \sigma^2\Tr(\bX^\sT\bX(\bX^\sT\bX + \lambda\id)^{-2}) \sim \frac{\sigma^2\Tr(\bSigma(\bSigma + \lambda_*\id)^{-2})}{n - \Tr(\bSigma^2(\bSigma + \lambda_*\id)^{-2})} =: \sV_{\sN, \lambda}^{\tt LS}\,.
\end{aligned}
\]
\end{proof}

As discussed in the main text, the asymptotic behavior of $\lambda_*$ differs between the under-parameterized and over-parameterized regimes as $\lambda \to 0$, though the ridge regression estimator $\hat{\bbeta}$ converges to the min-$\ell_2$-norm estimator $\hat{\bbeta}_{\min}$.
To be specific, in the under-parameterized regime, $\lambda_*$ converges to $0$ as $\lambda \to 0$; while in the over-parameterized regime, $\lambda_*$ converges to a constant that admits $\Tr(\bSigma(\bSigma + \lambda_n \id)^{-1}) \sim n$ when $\lambda \to 0$. 
Accordingly, for the minimum $\ell_2$-norm estimator, it is necessary to analyze the two regimes separately.
We have the following results on the characterization of the deterministic equivalence of $\| \hat{\bbeta}_{\min} \|_2$. 

\begin{corollary}[Asymptotic deterministic equivalence of the norm of interpolator]\label{prop:asy_equiv_norm_LR_minnorm}
    Under \cref{ass:asym}, for the minimum $\ell_2$-norm estimator $\hat{\bbeta}_{\min}$, we have the following deterministic equivalence: for the under-parameterized regime ($d<n$), we have
    \[
    \begin{aligned}
        \mathcal{B}^{\tt LS}_{\mathcal{N},0} = \|\bbeta_*\|_2^2\,,\quad \mathcal{V}^{\tt LS}_{\mathcal{N},0} \sim&~ \frac{\sigma^2}{n-d}\Tr(\bSigma^{-1})\,.
    \end{aligned}
    \]
    In the over-parameterized regime ($d>n$), we have
    \[
    \begin{aligned}
        \mathcal{B}^{\tt LS}_{\mathcal{N},0} \sim&~ \<\bbeta_*,\bSigma(\bSigma+\lambda_n\id)^{-1}\bbeta_*\>\,,\\
        \mathcal{V}^{\tt LS}_{\mathcal{N},0} \sim&~ \frac{\sigma^2\Tr(\bSigma(\bSigma+\lambda_n\id)^{-2})}{n-\Tr(\bSigma^2(\bSigma+\lambda_n\id)^{-2})} = \frac{\sigma^2}{\lambda_n}\,,
    \end{aligned}
    \]
    where $\lambda_n$ is defined by $\Tr(\bSigma(\bSigma+\lambda_n\id)^{-1}) \sim n$.
\end{corollary}
\begin{proof}[Proof of \cref{prop:asy_equiv_norm_LR_minnorm}]
We separate the results in the under-parameterized and over-parameterized regimes.

In the under-parameterized regime ($d<n$), for minimum norm estimator $\hat{\bbeta}_{\min}$, we have (for $\bX^\sT\bX$ is invertible)
\[
\begin{aligned}
    \hat{\bbeta}_{\min} = \left(\bX^\sT\bX\right)^{-1}\bX^\sT\by = \left(\bX^\sT\bX\right)^{-1}\bX^\sT(\bX\bbeta_*+\bm\varepsilon) = \bbeta_* + \left(\bX^\sT\bX\right)^{-1}\bX^\sT\bm\varepsilon\,.
\end{aligned}
\]
Accordingly, we can directly obtain the bias-variance decomposition as well as their deterministic equivalents
\[
\begin{aligned}
    \mathcal{B}_{\mathcal{N},0}^{\tt LS} = \|\bbeta_*\|_2^2\,, \quad \mathcal{V}_{\mathcal{N},0}^{\tt LS} = \sigma^2\Tr(\bX^\sT\bX(\bX^\sT\bX)^{-2}) \sim \sigma^2\frac{\Tr(\bSigma^{-1})}{n-d}\,,
\end{aligned}
\]
where we use \cref{eq:trA3} and take $\lambda \to 0$ for the variance term.

In the over-parameterized regime ($d>n$), we take the limit $\lambda \to 0$ within ridge regression and use \cref{prop:asy_equiv_norm_LR}.
Define $\lambda_n$ as $\Tr(\bSigma(\bSigma+\lambda_n\id)^{-1}) \sim n$, we have for the bias term
\[
\begin{aligned}
    \mathcal{B}_{\mathcal{N},0}^{\tt LS} \sim &~ \<\bbeta_*, \bSigma^2(\bSigma + \lambda_n\id)^{-2}\bbeta_*\> + \frac{\Tr(\bSigma(\bSigma + \lambda_n\id)^{-2})}{n} \cdot \frac{\lambda_n^2 \<\bbeta_*,\bSigma(\bSigma + \lambda_n\id)^{-2}\bbeta_*\>}{1-n^{-1}\Tr(\bSigma^2(\bSigma + \lambda_n\id)^{-2})}\\
    =&~ \<\bbeta_*, \bSigma(\bSigma + \lambda_n\id)^{-1}\bbeta_*\> - \lambda_n \<\bbeta_*, \bSigma(\bSigma + \lambda_n\id)^{-2}\bbeta_*\> + \frac{\Tr(\bSigma(\bSigma + \lambda_n\id)^{-2})}{n} \cdot \frac{\lambda_n^2 \<\bbeta_*,\bSigma(\bSigma + \lambda_n\id)^{-2}\bbeta_*\>}{1-n^{-1}\Tr(\bSigma^2(\bSigma + \lambda_n\id)^{-2})}\\
    =&~ \<\bbeta_*, \bSigma(\bSigma + \lambda_n\id)^{-1}\bbeta_*\>\,.
\end{aligned}
\]
For the variance term, we have
\[
\begin{aligned}
    \mathcal{V}_{\mathcal{N},0}^{\tt LS} \sim&~ \frac{\sigma^2\Tr(\bSigma(\bSigma+\lambda_n\id)^{-2})}{n-\Tr(\bSigma^2(\bSigma+\lambda_n\id)^{-2})}\,.
\end{aligned}
\]
Finally we conclude the proof.
\end{proof}