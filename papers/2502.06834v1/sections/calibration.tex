\subsection{Semi-Supervised Cross-Stage Knowledge Distillation}
\label{subsec:semi_consistency_calib}

The typical knowledge distillation setup involves a teacher model and a student model. The student model is the model that is used to serve production traffic, which is also the model that we aim at improving. The teacher model is usually a separate model or an ensemble that's more complex and more accurate. Due to the scale of data and variety of models, it is difficulty and cost inefficient to develop and maintain separate dedicated teachers for all models in an ads delivery system. Within the framework of UKDSL, we solve this problem by semi-supervised cross-stage distillation, where we used later stage models to teach earlier stage models on both labeled and unlabeled data.

Suppose an ads delivery system consists of $N$ stages. Each stage $j$ has a model $\mathbf{M_j}$, which ranks and selects the top candidates from its candidate pool. $\sS_j$ is the set of candidates selected by $\mathbf{M_j}$. $\sS_0$ is the set of all ads candidates before any ranking and filtering. 
Further suppose that this cascade design~\cite{wang2023towards} in multi-stage ranking systems, which is a widely used design pattern in the industry, early stage models need to rank a large pool of candidates (potentially in the order of billions), hence they need to be fast and relatively simple. 
The later stage models only need to rank the selected top candidates by the previous stage. Depending on the stage, the pool size could be in the order of 10k or even hundreds. Therefore they can afford to be slower. As a result, the later stage models usually are larger, more complex, trained using more data/features, thus performing a lot better than earlier stage models. In our notation,  $\mathbf{M_j}$ is one stage after  $\mathbf{M_{j-1}}$.
Consequently, the following properties hold:

\begin{enumerate}
    \item \label{cali-property} If both $\mathbf{M_{j-1}}$ and $\mathbf{M_j}$ are unbiased, then $\mathbf{M_j}$ is well calibrated on $\sS_{j-1}$, whereas $\mathbf{M_{j-1}}$ is mis-calibrated on $\sS_{j-1}$ due to the inherent miscalibration discussed in Section \ref{subsec:inherent_miscalibration}.
    \item \label{accu-property} $\mathbf{M_i}$ is more accurate than $\mathbf{M_j}$ if $i > j$
    \item \label{data-availability-property} $\mathbf{M_j}$ is used in production to serve $\sS_j$, thus all features required by $\mathbf{M_j}$ and its predictions are available for $\sS_j$ from production logging
    \item \label{label-property} $\vx \in \sS_0$ is labeled if and only if $\vx \in \sS_N$
    \item \label{set-property} $\sS_N \subset \sS_{N-1} \subset \dots \subset \sS_0$
\end{enumerate}

Due to (\ref{cali-property}) and (\ref{accu-property}), $\mathbf{M_j}$ is a good teacher model for $\mathbf{M_{j-1}}$ for all $j \in [2, N]$. Further, due to (\ref{data-availability-property}), (\ref{label-property}) and (\ref{set-property}), $\mathbf{M_j}$ can be used to impute pseudo labels for $\mathbf{M_{j-1}}$ on unlabeled data that is much larger than the labeled data, thus harvesting the benefits of semi-supervised learning. Remarkably, (\ref{data-availability-property}) means development and maintenance of new teacher models is not required, as the teacher predictions are readily available in the set they are expected to function as teachers. This not only enables knowledge distillation in an extremely cost effective way, but also overcomes the scale and diversity of model types and ranking stages. Figure \ref{fig:cross-stage-distillation} illustrates the semi-supervised cross-stage distillation with an example 3-stage system.

\begin{figure}[t]
  \centering
   \includegraphics[width=8.5cm]{figs/cross-stage-distillation.png}
  \caption{Semi-Supervised Cross-Stage Distillation illustrated with a 3-stage system: each model acts as the teacher for the previous stage on both labeled and unlabeled data}
  \label{fig:cross-stage-distillation}
\end{figure}


\subsubsection{Semi-Supervised Knowledge Distillation}
We make use of the standard binary cross entropy loss for cross-stage distillation. Typical ads ranking models such as CTR and CVR models produces a single probability value denoted by $\vz$, and the models are trained using binary cross entropy loss $\mathcal{L_{\text{BCE}}}(\vy,\vz)$ where $\vy$ is the ground truth label:

\begin{equation}
\label{eq:bce}
    \mathcal{L_{\text{BCE}}}(\vy,\vz) = -\left[\vy  \log({\vz}) + (1 - \vy)  \log{(1 - \vz)}\right]
\end{equation}
In our cross-stage distillation setup, instead of relying on the ground truth labels $\vy$, we rely on the predictions made by a teacher model $\vz_T$ as pseudo labels. The student model is similarly trained using binary cross entropy loss $ \mathcal{L_{\text{BCE}}}(\vz_T,\vz)$. 

\subsubsection{Performance of Semi-Supervised Cross-Stage Distillation}
In table \ref{cross_stage_cd_distill}, we present our model performance results after applying cross-stage distillation. We use the following two metrics for performance measurement. 
\newline
\begin{enumerate}
    \item \textbf{Calibration}: $\sum{\hat{p}_{pre-ranking}}/\sum{\hat{p}_{ranking}}$. Calibration is defined as the ratio of the average predictions of the test model over the average predictions of a reference model or the ground truths. A value close to 1 indicates consistency across two ranking stages when using a reference model, or unbiasedness when using ground truths.
    \item \textbf{Normalized Entropy (NE)}: NE is defined as:\\ $-\sum_{i,c}{\vy_{i,c}\log({p_{i,c}})}/-\sum_{i,c}{\vy_{i,c}\log({\bar{\vy_c}})}$, where $\vy_{i,c}$ is the label for the $i^{th}$ data on class $c$, and $p_{i,c}$ is the corresponding prediction. $\bar{\vy_c}$ is the average probability for class $c$ being positive. The smaller NE suggest better model performance. In the case of consideration data, where there is no ground truth label, as they were not sent to users, we use the later stage model's prediction as the label to calculate NE as a measure of cross stage consistency. 
\end{enumerate}

\vspace{6pt}

The results show that on impression data, the average predictions from pre-ranking model and late stage ranking model are very close to each other. However, pre-ranking model generates higher prediction values than late stage ranking model on consideration data which aligns with the theoretical analysis done in Section ~\ref{subsec:inherent_miscalibration}. Given consideration data represents the actual serving traffic of our models, improving the consistency on consideration data traffic would be crucial for our system performance. By following the analysis done in Section ~\ref{subsec:inherent_miscalibration} and adding cross stage co-training and distillation on consideration data, we allow late stage models generate predictions as supervision for pre-ranking models on consideration data on the fly. This approach improved cross stage consistency by reducing cross stage mis-calibration and reduce cross stage NE, with minimal impact on performances on impression traffic. 

\begin{table}[t]
    \caption{Offline performance comparison between model trained with cross stage distillation on consideration data and baseline.
    NE relative change: lower is better ($\downarrow$).
    Calibration: 1.0 is best.
    }
    \label{cross_stage_cd_distill}
    \centering
    \begin{tabular}{ccc}
    \toprule
     Data & Baseline &  w/ Distillation\\
    \toprule
     &\multicolumn{2}{c}{Model calibration}\\
    \midrule
    Impression Data & 1.01 & 1.03 \\
    \hline
    Consideration Data & 1.27 & 1.12 \\
    \toprule
     &\multicolumn{2}{c}{NE relative change ($\downarrow$)}\\
    \midrule
    Impression Data & 0\% & 0.01\%  \\
    \hline
    Consideration data & 0\% & -1.98\%  \\
    \bottomrule
    \end{tabular}
\end{table}