\subsection{Semi-Supervised Feature Selection (SSFS)}
Selection bias in training data can inadvertently lead to the choice of features that aren't necessarily the most beneficial for the serving data. 
This challenge highlights the necessity for a robust feature selection approach, specifically optimized for distillation, that can effectively reconcile the differences between training and serving data. This forms the motivation for a novel feature selection methodology.

Our approach is illustrated in Figure~\ref{fig:cdfaf}, which consists of following main steps:
\newline
\begin{enumerate}
    \item \textbf{Building the Teacher Model:} We begin by training a teacher model on the impression dataset. This teacher model is then used to generate labels for the unlabeled consideration dataset.
    \item \textbf{Label Augmentation:} Next, we augment the unlabeled dataset with the labels generated by the teacher model.
    \item \textbf{Training a Simplified Model:} Subsequently, we train a simpler version of the student model using all candidate features on a mixed dataset with both labeled and unlabeld data. 
    \item \textbf{Perturbation-based Features Importance:} Then, we compute the feature importance scores for each feature in a withheld dataset using a perturbation-based feature importance algorithm with the simplified model.
    \item \textbf{Combine biased and unbiased features:} Finally, we combine the biased features ranked using regular approach and the unbiased features ranked using our approach based on their relative rank and/or weighted feature importance, and select the best mix that balances the results on both the impression and consideration data sets.
\end{enumerate}
\vspace{6pt}
A high level pseudo-code algorithm for perturbation-based feature importance is given below:

\begin{algorithm}[h]
\caption{Perturbation-based feature importance}\label{alg:fi}
\begin{algorithmic}
\For{every batch of data}
    \For{every feature}
        \State 1. Measure the original loss with feature
        \State 2. Random shuffle feature values in the batch
        \State 3. Measure the loss on the shuffled batch
        \State 4. Report feature importance as loss change
    \EndFor
\EndFor
\State Report mean \& stdev for feature importance
\end{algorithmic}
\end{algorithm}


\begin{figure}[t]
\centering
\hspace*{0.5cm} 
\includegraphics[width=8.0cm]{figs/featsel_process.png}
\caption{This diagram illustrates the Semi-Supervised Feature Selection module used by UKDSL. A set of unbiased features is selected and combined with the regular biased features to create the final set of features for use in the model.}
\label{fig:cdfaf}
\end{figure}




Table~\ref{tb:cdfaf} compares different methods for combining biased and unbiased features in terms of NE improvement on the \emph{impression data set (IMP)} and \emph{consideration data set (CD)}. 
CD Importance Only leads to a 0.023\% gain in IMP NE and a 0.079\% gain in CD NE. Combining the CD Feature Importance with IMP Feature Importance by their relative rank in feature importance improves CD NE further but negatively impacts IMP NE. The best method, Union of Top Features, which selects top features using IMP feature importance and CD feature importance independently and take the union of the two sets, led to 0.137\% gain in IMP NE and 0.725\% gain in CD NE, proving its superior generalizability in consideration data.

\begin{table}[t]
\caption{Comparison of different methods for combining biased and unbiased features in terms of relative NE change (lower is better ($\downarrow$)) on the impression data and consideration data.}
\label{tb:cdfaf}
\centering
\begin{tabular}{lcc}
\toprule
& \multicolumn{2}{c}{NE relatvie change  ($\downarrow$)} \\
\midrule
Method & Impression Data& Consideration Data\\
\midrule
IMP Importance Only & 0.000\% & 0.000\% \\
\hline
CD Importance Only & -0.023\%& -0.079\%\\
\hline
Average Rank & 0.209\%& -0.371\%\\
\hline
Average Importance & -0.009\%& -0.094\%\\
\hline
Intersection of Top Features & 0.018\%& -0.079\%\\
\hline
\textbf{Union of Top Features} & -0.137\%& -0.725\%\\
\bottomrule
\end{tabular}
\end{table}
