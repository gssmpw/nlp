\section{Related Work}
\label{sec:related}
\textbf{Multi-stage ranking} A multi-stage ranking framework is widely adopted within an industrial recommendation system in order to balance between the efficiency and accuracy \cite{deepRSsurvey}. Covington et al. \cite{youtube} employed deep neural network models for both candidate generate stage and ranking stage for Youtube recommendation, Huang et al. \cite{fbsearchemb} designed the Facebook search system with three stages (indexing, retrieval and ranking) under a unified embedding based framework. Six different stages including retrieval, pre-ranking, relevance ranking, ranking, re-ranking, mix-ranking were deployed to build a search system as described in \cite{taobaosearch}.  \\

\noindent \textbf{Sample selection bias} Within a multi-stage ranking system, it has been shown that sample selection bias may be induced in earlier stages, if the data related to delivered items was used to train the early stage models~\cite{pin2024kdd}. There have been different types of approaches to mitigate this bias in the literature. For instance, authors in \cite{pin2024kdd} proposed two variants of unsupervised domain adaptation, and have associated a manually chosen threshold for hard labeling. 
Qin el, al. \cite{rankflow} trained different stages in a cascading flow fashion by training each stage separately followed by cross stage distillations. Building on top of \cite{rankflow}, Zhao et, al.\cite{COPR} emphasized the importance of rank order alignment between stages with bid information, and designed a chunk based data sampling schema to facilitate learning this order alignment. Zheng et al. \cite{zheng2022multiobjective} tackled this problem from a multi-objective perspective by treating samples at by different stages as different class of labels for the model to learn the preference order among relevance, exposure, click and purchase. 
 
Another line of research is around finding or creating new data sources to break the data-model feedback loop within an existing system. Both \cite{rec4ad} and \cite{zhang2023rethinkingsearch} leveraged user interactions from other similar and related scenarios to provide proxy labels for the samples considered by the early stages in the target scenario. \cite{autodebias} and \cite{uniform2020} used a small fraction of traffic to collect uniformly distributed data through random policy.  \\


\noindent \textbf{Knowledge distillation} Knowledge distillation \cite{hinton2015distilling} is a technique for transferring knowledge from a teacher model to a student model with the goal of improving the performance of the student model. A typical setup is that the teacher model is more complex and accurate while the student model is simpler and less accurate. Initially this technique has been applied to domains like computer vision \cite{ba2014l2} \cite{romero2015fitnets} and neural language processing \cite{kim2016sequencelevel}. Later recommendation system started to adopt this technique as well. Tang el al. \cite{rankingdistill} proposed to use the knowledge distillation method to compress their ranking models which had an inference latency constraint while in the serving the traffic. The predictions from multiple teachers are ensembled together through adaptive gating in \cite{ensembledistill} to further boost the performance gain of the distilled student model. 

