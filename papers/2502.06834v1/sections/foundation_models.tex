\subsection{Semi-Supervised Learning from Foundation Models (SSLFM)}
In our semi-supervised cross-stage distillation described in Section \ref{subsec:semi_consistency_calib}, we always use the next-stage model as the teacher. However, this approach does not work for the model in the last stage, for which no production model can be used as a teacher. With the recent rise in popularity of foundation models, we leverage foundation models as the teacher models. Unlike production models, the foundation models are not used to serve production traffic. Therefore, they are not subject to the same capacity constraint as the production models. This allows them to have increased model complexity, consume more features, and make use of multiple related tasks with a multi-task learning setup to gain additional information during training. As a result, the quality of predictions by the foundation models are far superior than the production models thus can be used as teacher labels for the production models.

In our experiments, we found that using multi-task learning helps improve the knowledge transfer efficiency when foundation models are used as teachers. Specifically, we add an auxiliary task and a dependent task in addition to the main task (e.g. CTR task) in a multi-task learning manner. This approach is illustrated in Figure \ref{fig:ssl_em_fig}.
\newline
\begin{enumerate}
    \item \textbf{Dependent task}: On top of the main task (e.g. CTR prediction), we add a dependent task that takes the output of the CTR task as input. This dependent task is trained on unlabeled data only, with the foundation model's output as teacher label.
    \item \textbf{Auxiliary task}: We add another auxiliary task that takes the shared feature representation as input. Like the dependent task, this auxiliary task is trained on unlabeled data only, with the foundation model's output as teacher label.
\end{enumerate}
\vspace{6pt}
\begin{figure}[t]
  \centering
   \includegraphics[width=8cm]{figs/ssl_kd_fig.png}
  \caption{Diagram of knowledge distillation implementation in the SSLFM module for an example CTR prediction model. We add an auxiliary task and a dependent task to the student model in a multi-task setting. Both tasks are trained to predict the teacher model's predictions}.
\label{fig:ssl_em_fig}
\end{figure}

Table \ref{ssl-em} summarizes our experiment results for a CTR model. Notably, we found our semi-supervised learning implementation with foundation models significantly improved the student model's NE on the impression data.

\begin{table}[t]
\caption{Performance comparison between different variants of SSLEM models and baseline: NE relative change (lower the better$\downarrow$)}
\label{ssl-em}
\centering
\begin{tabular}{lcc}
\toprule
 Model & NE relative change ($\downarrow$)   \\
\midrule
Baseline & 0.000\%\\
\hline
Dependent task only & -0.151\%  \\
\hline
Auxiliary task only & -0.159\%  \\
\hline
Dependent + auxiliary task & -0.163\% \\
\bottomrule
\end{tabular}
\end{table}
