\section{Experiments}\label{sec:experiment}
\subsection{Experiment Setup}
\smallsection{Compared Methods}
We consider baselines including synchronous methods, FedAvg~\cite{mcmahan2017communication}, FedProx~\cite{li2020federated}, FedAdam~\cite{reddi2020adaptive}, semi-asynchrounous methods, FedBuff~\cite{nguyen2022federated} and CA$^2$FL~\cite{wang2024tackling}, and fully-asynchrounous methods, FedAsync~\cite{xie2019asynchronous}. The detailed descriptions of these compared methods are provided in Appendix~\ref{sec:baseline-methods}.

\begin{wrapfigure}{r}{0.5\textwidth}
\renewcommand\tabcolsep{3pt}
    \centering
    \captionsetup{type=table}
    \caption{Datasets and models in the experiments.}
    \small
    \scalebox{0.81}{
    \begin{tabular}{lccccc}
    \toprule
    Datasets & Clients & Avg. $\lvert \mathbf{D}_m\rvert$ & Model & Data Type \\
    \midrule
    CIFAR-10          & 10 & 4000 & VGG11        & image\\
    MNIST            & 10 & 6000 & LeNet5        & image\\
    20 Newsgroups    & 20 & 566 & DistilBERT     & text\\
    HAR              & 21 & 350 & ResNet18       & time-series\\
    CIFAR-100         & 100 & 400 & MobileNetV2  & image\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:exp-dataset-and-model}
\end{wrapfigure}
\smallsection{Applications and Datasets}
Table~\ref{tab:exp-dataset-and-model} summarizes the setups for each dataset. 
We conduct experiments on five datasets, CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning}, MNIST~\cite{deng2012mnist}, 20 Newsgroups~\cite{lang1995newsweeder}, and HAR~\cite{anguita2013public}, including three distinct data types: image, text, and time series.
To assess the robustness of \our across different model architectures, we pair each dataset with a model suited to its data type. To evaluate \our's performance in parameter-efficient fine-tuning (PEFT) settings~\cite{han2024parameter}, we employ a pretrained DistilBERT~\cite{sanh2019distilbert} from Hugging Face\footnote{\url{https://huggingface.co/distilbert/distilbert-base-uncased}} for evaluations on the 20 Newsgroups dataset and fine-tune it using Low-Rank Adaptation (LoRA)~\cite{hu2021lora}, which reduces the number of trainable parameters. More details are given in Appendix~\ref{sec:application-and-dataset}.

\smallsection{Data Heterogeneity} For the HAR dataset, clients are naturally divided based on the individual subjects, as each subject represents a client. For the other datasets, we set the number of clients equal to the number of classes in each dataset. To create non-IID client distributions, we follow prior work~\cite{hsu2019measuring} and use a Dirichlet distribution $Dir(\alpha=0.1)$ to derive class distribution.

\smallsection{Delay Simulation}
To ensure controlled evaluation and avoid variability in federated learning deployments, we simulate client delays using measurements (including communication and computational latency) from prior work~\cite{yu2023async}, which were collected using Raspberry Pi (RPi) devices in different home environments with wireless connectivity. To account for differences in the size of models and datasets, we use an RPi 4B which has comparable computational capabilities as the reported ones to measure latency for training one round under each model and dataset configuration. Each configuration is tested five times to derive the average training time. The computational latency for datasets in our experiment is then adjusted based on the ratio between our measured time and the computation latency in~\cite{yu2023async}. Similarly, communication latency was scaled based on the model size in bytes compared to the model used in the original delay collection.
Using these measurements, we calculate the mean and variance of latency for every device. 
During simulations, we assign the statistics to clients randomly and sample latency for each round from a Gaussian distribution parameterized by the assigned statistics.
For fair comparisons, the order of client updates is kept consistent with a fixed set of random seeds across compared methods.
Besides these real-world measurements, we also investigate model performance under additional delay distributions in Section~\ref{sec:exp-exploratory}.

\smallsection{Evaluation Metrics}
We report accuracy after training for sufficient clock cycles to ensure the method reaches stable performance. For a fair comparison, we fix the same training time across all methods. In addition, we compare the time spent in reaching a target accuracy---set as the 95\% of the lowest final accuracy among all compared methods. FedAvg serves as the baseline of time consumption (i.e., 1$\times$), and we report the relative time for other methods.

\subsection{Main Experiment Results}
Table~\ref{tab:exp-main} summarizes the results. \our consistently outperforms the compared methods across all datasets—it not only converges faster but also achieves higher accuracy. Notably, the advantage becomes more obvious in the setting with a larger number of clients, as seen in CIFAR-100. This is because, for synchronous methods, the client sampling rate decreases as the number of clients increases (e.g., only 10 out of 100 clients are sampled per round), leading to longer wait times and slower convergence. Similarly, for baseline asynchronous methods, although their aggregation mechanisms are designed to mitigate the influence of model staleness, they fail to effectively address the challenges posed by data heterogeneity. In contrast, the calibration mechanism in \our alleviates the negative impact of both stale model updates and the model divergence caused by data heterogeneity, ensuring faster convergence and improved performance.

Furthermore, as shown in Figure~\ref{fig:exp-main}, \our demonstrates reduced fluctuations in accuracy over training time. These fluctuations, observed in compared methods, are caused by conflicting updates from clients with non-identical data distributions.

\begin{table*}[t]
  \centering
  \caption{Main results (\%) including average accuracy, standard deviation, and time relative to FedAvg. \our reaches the target accuracy more quickly and achieves higher accuracy.}
  \scalebox{0.77}{
    \begin{tabular}{lcccccccccc}
    \toprule
    \multicolumn{1}{l}{\multirow{2}[2]{*}{Methods}} & \multicolumn{2}{c}{MNIST} & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{20 Newsgroups} & \multicolumn{2}{c}{HAR} & \multicolumn{2}{c}{CIFAR-100} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11} & Accuracy   & Time  & Accuracy   & Time  & Accuracy & Time  & Accuracy & Time & Accuracy & Time\\
    \midrule
    FedAvg~\cite{mcmahan2017communication} & 92.2±1.1 & 1$\times$ & 73.8±2.4 & 1$\times$     & 58.1±3.1 & 1$\times$ & 84.2±0.6 & 1$\times$ & 22.2±0.4 & 1$\times$ \\
    FedProx~\cite{li2020federated} & 90.5±0.9 & 1.09$\times$ & 73.3±2.2 & 0.90$\times$ & 58.6±3.1 & 0.98$\times$ & 84.0±0.9 & 0.98$\times$ & 20.4±0.6 & 1.18$\times$\\
    FedAdam~\cite{reddi2020adaptive} & 93.8±2.3 & 0.91$\times$ & \underline{74.6±1.5} & 0.78$\times$ & 58.9±1.8 & 1.08$\times$ & 87.1±3.3 & 0.68$\times$ & 42.1±1.0 & 0.36$\times$\\ \midrule
    FedAsync~\cite{xie2019asynchronous} & 95.4±0.7 & 0.39$\times$ & 74.3±1.1 & \underline{0.48$\times$} & 61.8±3.8 & 0.27$\times$ & 87.6±1.9 & \underline{0.26$\times$} & 47.9±0.9  & 0.20$\times$\\
    FedBuff~\cite{nguyen2022federated} & 93.6±2.1 & 0.46$\times$ & 73.6±3.2 & 0.55$\times$ & 62.1±2.0 & 0.34$\times$ & 87.4±1.0 & 0.30$\times$ & \underline{62.3±0.5} & 0.12$\times$\\
    CA$^2$FL~\cite{wang2024tackling} & \underline{96.1±1.4} & \underline{0.30$\times$} & 69.7±8.0 & 0.60$\times$ & \underline{65.7±1.6} & \underline{0.24$\times$} & \underline{87.8±1.9} & 0.29$\times$ & 61.1±0.7 & \underline{0.09$\times$}\\
    \midrule
    OrthoFL (ours) & \textbf{98.2±0.3} & \textbf{0.18$\times$} & \textbf{76.5±3.3} & \textbf{0.19$\times$} & \textbf{66.2±1.3} & \textbf{0.09$\times$} & \textbf{89.7±1.0} & \textbf{0.23$\times$} & \textbf{63.0±0.6} & \textbf{0.03$\times$} \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:exp-main}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/exp_main.pdf}
    \caption{
    Accuracy w.r.t. training time. 
    }
\label{fig:exp-main}
\end{figure*}

\subsection{Overhead Analysis}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/exp_ablation.pdf}
    \caption{Ablation study. The results emphasize the importance of decoupling global and local learning to address objective inconsistencies. Besides, effective orthogonal calibration can be achieved via multiple viable approaches.
    }
\label{fig:exp-ablation}
\end{figure}
\label{sec:overhead-analysis}
Compared to baseline methods, \our does not introduce additional communication overhead. The extra computational overhead occurs on the server during the orthogonalization process.

\begin{wrapfigure}{r}{0.45\textwidth}
\renewcommand\tabcolsep{2pt}
\centering
\captionsetup{type=table}
\caption{Aggregation time of \our.}
\scalebox{0.83}{
\begin{tabular}{llcc}
    \toprule
    Dataset         & Model         & Train Params & Time        \\
    \midrule
    MNIST           & LeNet5        & 44K              & 0.003s      \\
    HAR             & ResNet18      & 119K             & 0.199s      \\
    20 Newsgroups   & DistilBERT    & 753K             & 0.110s      \\
    CIFAR-10        & VGG11         & 9.2M             & 0.188s      \\
    CIFAR-100       & MobileNetV2   & 2.4M             & 0.512s      \\
    \bottomrule
\end{tabular}
}
\label{tab:exp-aggregation-overhead}
\end{wrapfigure}
The overhead of orthogonal aggregation depends on the model size, as orthogonalization is performed through matrix operations on the weight changes of each layer. As shown in Table~\ref{tab:exp-aggregation-overhead}, the orthogonalization operation in our experiments takes between 3 ms (for MNIST) and 512 ms (for CIFAR-100) when running aggregation on a server equipped with an AMD EPYC 7713 64-Core Processor (3.72 GHz max clock) and 3.9 TiB RAM. This additional time for weight aggregation is negligible compared to the computational and communication latency on clients (shown in Figure~\ref{fig:exp-delay-simulation}). 


\subsection{Ablation Studies}

We conduct ablation studies to evaluate our key design choices. First, we set FedAsync as a baseline (denoted as \our w/o Calib.), since it can be viewed as our ablation without calibration. Second, we assess the contribution of global aggregation by removing the moving average and directly loading the calibrated client weight into the global model (denoted as \our w/o MA). Furthermore, we investigate an alternative orthogonalization strategy that projects the incoming client update onto the orthogonal subspace of the most recent updates from all other clients, (denoted as \our-Pairwise Proj.). The orthogonality is achieved through the same process (i.e., removing parallel components) as in \our. 

We observe performance decreases when clients and the global model share the same weights (\our w/o Calib. and \our w/o MA). \our w/o MA performs particularly poorly as the absence of a global moving average causes overfitting to individual client distributions. These results emphasize the need to decouple global and local learning to address objective inconsistencies. Furthermore, \our-pairwise Proj. achieves performance comparable to \our, suggesting that effective orthogonal calibration can be realized through multiple viable ways. We expect that fine-tuning the orthogonalization strategy could further bring improvement to the model performance and we leave such exploration for future work. 

\begin{figure}[t]
\centering
    \includegraphics[width=0.98\linewidth]{figures/exp_delay_dist.pdf}
    \caption{Performance with different delay distributions.}
\label{fig:exp-delay-dist}
\end{figure}

\subsection{Exploratory Studies}
\label{sec:exp-exploratory}
\noindent\textbf{How do algorithms perform under different delay distributions?}
Since real-world deployment of federated learning may present diverse delay patterns, we explore other possible delay distributions in real-world setups, such as following log-normal, half-normal~\cite{sui2016characterizing}, and uniform distributions~\cite{nguyen2022federated}. 

Each distribution is parameterized by the mean and variance of latency (communication and computation) observed across the RPi devices. Details on deriving these distributions are provided in Appendix~\ref{sec:derivation-delay-distribution}. Figure~\ref{fig:exp-delay-dist} presents the accuracy curves for each algorithm under different delay distributions. With real-world measured RPi latency, some clients experience substantially longer latencies, causing synchronous methods like FedAvg to converge more slowly as the server waits for stragglers. In this case, asynchronous methods generally have better performance than synchronous ones. Under the lognormal, half-normal, and uniform delay distributions, extreme latencies are less common, so the performance gap between synchronous and asynchronous methods narrows. However, the two buffer-based semi-asynchronous methods are sensitive to the delay patterns as they show lower performance under half-normal and uniform latency. In general, \our performs the best across all scenarios, demonstrating its robustness against different delay patterns.

\begin{figure}[t]
\centering
    \includegraphics[width=0.95\linewidth]{figures/exp_data_hetero.pdf}
    \caption{Performance under different data heterogeneity levels.}
\label{fig:exp-data-hetero}
\end{figure}

\noindent\textbf{How does data heterogeneity impact performance?}
To control the level of data heterogeneity, we change $\alpha$ for Dirichlet distribution from $\{0.01, 0.1, 0.5, 10^4\}$, where $\alpha = 10^4$ simulates the IID case. We present experiments on MNIST and CIFAR-10 as shown in Figure~\ref{fig:exp-data-hetero}. As the client data distribution becomes more heterogeneous (i.e., lower values of $\alpha$), we observe the performance of baseline methods has more fluctuations and decreases in final accuracy. This is because client models trained on non-IID data distributions have larger divergences in their weights. Aggregating divergent updates amplifies inconsistencies, leading to slower convergence and lower accuracy for baseline methods. By contrast, \our exhibit stable performance across all settings. In the IID case, where data is uniformly distributed across clients, \our still outperforms the compared methods. This is because our orthogonal calibration also addresses model staleness.

\subsection{Sensitivity Analyses}
\label{sec:sensitivity}
\begin{figure}[t]
    \centering
    \subfigure[Aggregation Hyperparameter $\beta$.]{
    \centering
    \includegraphics[width=0.48\linewidth]{figures/exp_sensitivity_beta.pdf}
    \label{fig:exp-sensitivity-beta}
    }
    \subfigure[Local Training Epochs $E$]{
    \centering
    \includegraphics[width=0.48\linewidth]{figures/exp_sensitivity_epochs.pdf}
    \label{fig:exp-sensitivity-epochs}
    }
    \caption{Sensitivity analysis. (a) \our is robust to aggregation hyperparameter $\beta$, maintaining higher accuracy than FedAvg and a low relative time. (b) An appropriately chosen number of local epochs $E$ improves accuracy within the same training duration and expedites convergence.}
\end{figure}

\smallsection{Aggregation Hyperparameter} 
The parameter $\beta$ in Equation~\ref{eq:aggregation} acts as a smoothing factor that balances the contribution of client updates to the global model and the retention of the global model's weights from the previous round. A larger $\beta$ allows the global model to incorporate more of the client updates, potentially accelerating learning, while a smaller $\beta$ preserves more of the global model's previous state, enhancing stability. We vary the value of $\beta$ from $\{0.2, 0.4, 0.6, 0.8\}$. As shown in Figure~\ref{fig:exp-sensitivity-beta}, \our is robust to different $\beta$ values and always achieves higher accuracy than FedAvg after a fixed training time, and the relative time of \our is low.

\smallsection{Number of Local Training Epochs} The number of local training epochs $E$ determines how much information is learned during a round and impacts the overall convergence speed. A larger $E$ allows clients to learn more information from their local data, potentially improving local model performance. However, it has the risk of larger divergence among client models and global models. On the other hand, setting a smaller $E$ ensures closer alignment between client and global models but comes at the cost of higher communication latency due to more frequent synchronization. We vary the value of $E$ from $\{1, 5, 10\}$ and show the results in Figure~\ref{fig:exp-sensitivity-epochs}. The upper row shows the final accuracy after a fixed training time and the bottom row presents the relative time to reach 95\% of FedAvg's target accuracy when $E=5$. We observe that when $E=1$, both methods reach lower accuracy compared to larger $E$. This is due to insufficient local learning, requiring more communication rounds to achieve comparable performance. \our achieves similar final accuracy when $E=5$ and $E=10$. Notably, \our outperforms FedAvg across different $E$ values. The speedup of \our is more obvious at smaller $E$ values (e.g., $E=1$).