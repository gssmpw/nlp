\section{Related Works}
We review two core directions relevant to our study. Connections with other areas including asynchronous stochastic gradient descent and continual learning are discussed in Appendix~\ref{sec:connections-with-other-areas}.

\smallsection{Federated Learning and Heterogeneity Problem}
Federated learning____ is a distributed learning paradigm that allows multiple parties to jointly train machine learning models without data sharing, preserving data privacy. Despite the potential, it faces significant challenges due to heterogeneity among participating clients, which is typically classified into two main categories: data heterogeneity and system heterogeneity. Data heterogeneity appears as clients own non-IID (independent and identically distributed) data____. The difference in data distribution causes the local updates to deviate from the global objective, making the aggregation of these models drift from the global optimum and deteriorating convergence. System heterogeneity refers to variations in client device capabilities, such as computational power, network bandwidth, and availability____. These disparities lead to uneven progress among clients, and the overall training process is delayed by slow devices. Traditional federated learning approaches rely on synchronization for weight aggregation____, where the server waits for all clients selected in a round to complete and return model updates before proceeding with aggregation. This synchronization leads to inefficient resource utilization and extended training times, particularly in large-scale deployments involving hundreds or thousands of clients. Addressing the heterogeneity issues is a critical problem for improving the scalability and efficiency of federated learning systems in real-world deployment.

\smallsection{Asynchronous Federated Learning}
Much of the asynchronous federated learning literature focuses on staleness management by assigning weights for aggregating updates according to factors including delay in updates____, divergence from the global model____ and local losses____. For example, ____ lets the server aggregate client updates into the global model with a weight determined by staleness. Another line of research caches client updates at the server and reuses them to calibrate global updates____. For example, ____ maintains the latest update for every client to estimate their contribution to the current aggregation and calibrate global updates. Furthermore, semi-asynchronous methods____ balance between efficiency and training stability. For example, ____ buffers a fixed number of client updates before aggregation. We select representative methods from each category for our comparisons. Besides, some works improve efficiency from a different perspective---through enhanced parallelization. Methods include decoupling local computation and communication____ and parallelizing server-client computation____. In addition, asynchronous architectures have been explored in other paradigms such as vertical____ and clustered____ federated learning. While these directions complement our work, they fall outside the scope of this study.