\section{Conclusions}
In this paper, we introduce \our, an orthogonal calibration mechanism for asynchronous federated learning. \our exploits the high-dimensional parameter space of neural networks and projects the global weight shift during a client's delay onto a subspace orthogonal to its stale update. This projection ensures global progress is integrated into client models without disrupting local learning. Experiments demonstrate that \our consistently outperforms state-of-the-art synchronous and asynchronous baselines, achieving notable gains in accuracy, convergence speed, and robustness under diverse delay patterns and data heterogeneity.

For future work, we plan to explore more complicated scenarios such as dynamic client participation and failure tolerance. Discussions on the adaptability of \our to these challenges are provided in Appendix~\ref{sec:discussion}. We also aim to further optimize learning efficiency, potentially through integrating adaptive client selection into \our to prioritize impactful clients for participation. These explorations will further enhance the robustness and scalability of federated learning. 