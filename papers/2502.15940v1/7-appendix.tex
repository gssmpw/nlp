\newpage
\clearpage
\appendix
\section{Proof}
\label{sec:proof}
In this section, we present the proof for Lemma~\ref{eq:lemma}. We restate the lemma below:
\begin{lemma}
    Let $v \in \mathbb R^d$ and $\mathcal{U} = \{u_1, \cdots, u_k\}$ be an orthonormal set for some $k < d$. Then for any $w \in (\mathrm{span}\,\mathcal{U})^\perp$, 
    \begin{equation}
        \label{eq:appendix-lemma}
        \Vert v - v^\perp \Vert \le \Vert v - w \Vert,
    \end{equation}
    where $v^\perp:= v - \sum_{i=1}^k \langle v, u_i\rangle u_i$ denote the component of $v$ orthogonal to $\mathcal{U}$. Moreover, the angle between $v$ and $v^\perp$ is less than any angle between $v$ and $w$ for $w \in (\mathrm{span}\,\mathcal{U})^\perp$.
\end{lemma}

\begin{proof}
    Extend $\mathcal{U}$ to an orthonormal basis $\mathcal{B} := \{u_1, \cdots, u_k, u_{k+1}, \cdots, u_d\}$ on $\mathbb R^d$. Write $w = \sum_{i=1}^d w_i u_i$ and $v = \sum_{i=1}^d v_i u_i$, where $w_i:= \langle w, u_i\rangle$ and $v_i := \langle v, u_i \rangle$ denote the coordinates of $w$ and $v$ with respect to the basis $\mathcal{B}$ respectively. Since $w \in (\mathrm{span}\,\mathcal{U})^\perp$, $w_i = 0$ for $1\le i \le k$. It follows from Pythagorean theorem that
    \begin{equation}
    \begin{aligned}
        \Vert v - w \Vert^2 &= \left\Vert \sum_{i=1}^d (v_i - w_i) u_i \right\Vert^2 \\
        &= \left\Vert \sum_{i=1}^k v_i u_i + \sum_{j=k+1}^d (v_j - w_j) u_j \right\Vert^2 \\
        &= \left\Vert \sum_{i=1}^k v_i u_i \right\Vert^2 + \left\Vert\sum_{j=k+1}^d (v_j - w_j) u_j \right\Vert^2 \\
        &\ge \left\Vert \sum_{i=1}^k v_i u_i \right\Vert^2.
    \end{aligned}
    \end{equation}
    Taking square root of both sides and noting that by definition $v - v^\perp = \sum_{i=1}^k v_i u_i$, equation~\eqref{eq:appendix-lemma} follows.
    As for the second claim, let $\theta(v,w)$ denote the angle between $v$ and $w$, $\theta(v, v^\perp)$ the angle between $v$ and $v^\perp$. 
    By the Cauchy-Schwarz inequality,
    \begin{equation}
        \begin{aligned}
            \langle v ,w \rangle &= \left\langle v, \sum_{j = k+1}^d w_j u_j\right\rangle \\
            &= \sum_{j=k+1}^d w_j  \langle v, u_j \rangle \\
            &\le \left(\sum_{j=k+1}^d w_j^2\right)^{1/2} \cdot \left(\sum_{j=k+1}^d v_j^2 \right)^{1/2}\\
            &= \Vert w \Vert \cdot \Vert v^\perp \Vert.
        \end{aligned}
    \end{equation}
    It follows that 
    \begin{equation}
        \cos \theta(v,w) := \frac{\langle v, w\rangle}{\Vert v\Vert \, \Vert w\Vert} \le \frac{\Vert v^\perp\Vert}{\Vert v\Vert} = \frac{\langle v, v^\perp \rangle}{\Vert v\Vert\, \Vert v^\perp\Vert} = \cos \theta (v, v^\perp).
    \end{equation}
    Since the consine function is monotonically decreasing on $[0, \pi]$, $\theta(v,w) \ge \theta(v, v^\perp)$ as claimed.
\end{proof}

\section{Details of Experiment Setup}
\subsection{Applications and Datasets}
\label{sec:application-and-dataset}
The experiments are conducted with the following three applications. 
\begin{enumerate}[leftmargin=*]
    \item \textbf{Image Classification.} We evaluate our framework on three widely used image datasets: MNIST~\cite{deng2012mnist}, CIFAR-10, and CIFAR-100~\cite{krizhevsky2009learning}. For MNIST, we use LeNet5~\cite{lecun1998gradient}, a lightweight convolutional network. For CIFAR-10, we adopt VGG11~\cite{simonyan2014very}, a deeper convolutional architecture. For CIFAR-100, we employ MobileNetV2~\cite{sandler2018mobilenetv2}, a compact and efficient model ideal for large-scale image classification tasks.
    \item \textbf{Text Classification.} We experiment with the 20 Newsgroups dataset~\cite{lang1995newsweeder}, a benchmark dataset for multi-class text categorization. We adopt DistillBERT~\cite{sanh2019distilbert}, a small transformer model suitable for resource-constrained devices. 
    \item \textbf{Human Activity Recognition.} We use the HAR~\cite{anguita2013public} dataset, which contains time-series sensor data for different physical activities. We adopt the 1D version of ResNet18~\cite{he2016deep}, a modified ResNet architecture for processing 1D sequential data.
\end{enumerate}

\subsection{Baseline Methods}
\label{sec:baseline-methods}
Below are the baseline methods that we compared in the experiments:
\begin{itemize}[leftmargin=*]
    \item \textbf{FedAvg}~\cite{mcmahan2017communication} is the classical synchronous algorithm where the server selects a subset of clients to conduct training in each round and synchronizes updates from these clients before aggregation.
    \item \textbf{FedProx}~\cite{li2020federated} is a synchronous method that addresses data heterogeneity by incorporating L2 regularization during local training to constrain the divergence between global and client models.
    \item \textbf{FedAdam}~\cite{reddi2020adaptive} is a synchronous algorithm that integrates Adam optimizer for the server. It adapts learning rates for each parameter using first- and second-moment estimates, improving convergence and accelerating training under data heterogeneity.
    \item \textbf{FedAsync}~\cite{xie2019asynchronous} is a fully-asynchronous framework that lets the server immediately aggregate the client updates into the global model upon receipt. It uses a weighting mechanism as in Equation~\ref{eq:fedasync} to account for the staleness of the updates. 
    \item \textbf{FedBuff}~\cite{nguyen2022federated} is a semi-asynchronous framework that introduces a buffered aggregation strategy. It maintains a buffer to collect client updates. Once the buffer is full, the server aggregates the client updates in the buffer and updates the global model.
    \item \textbf{CA$^2$FL}~\cite{wang2024tackling} is another semi-asynchronous framework based on buffered aggregation. It caches the latest update from every client on the server and uses them to estimate the clients' contribution to the update of the current round and calibrate global updates.
\end{itemize}

\subsection{Federated Learning Configuration}
For reproducibility, we report the configurations in our experiments. 
The number of local training epochs $E=5$. For the FedAvg algorithm, the number of sampled clients at each round is 10. The aggregation hyperparameters in Equation~\ref{eq:aggregation} are set to $\beta = 0.6$ and $a = 0.5$, following the values used in prior work~\cite{xie2019asynchronous}. The learning rate for local training at all clients is $5 \times 10^{-5}$ for the 20 Newsgroups dataset and 0.01 for the other datasets. 

\subsection{Derivation of Delay Distributions}
\label{sec:derivation-delay-distribution}

Figure~\ref{fig:exp-delay-simulation} presents the average communication and computation latency on Raspberry Pis scaled for the dataset and model configuration in our experiments. For 20 Newsgroups, the converted computational time is sufficiently long, making communication time negligible.

\begin{figure}[htbp]
\centering
    \includegraphics[width=0.8\linewidth]{figures/exp_delay.pdf}
    \caption{Average latency per round across clients.}
\label{fig:exp-delay-simulation}
\end{figure}


\smallsection{Additional Delay Distributions} In Section~\ref{sec:exp-exploratory}, we present the performance of compared methods under additional delay distributions following physical deployments. The derivations of the additional delay distributions are as follows.
\begin{itemize}[leftmargin=*]
    \item \textbf{Lognormal distribution}: The parameters $\mu$ (mean) and $\sigma$ (standard deviation) of the natural logarithm of delays are derived from the measurements on Raspberry Pis. Specifically, $\mu_R$ and  $\sigma_R$ represent the arithmetic mean and standard deviation of the measured delays for all rounds, respectively. 
        $$
        \sigma = \sqrt{\ln\left(\frac{\sigma_R^2}{\mu_R^2} + 1\right)},\quad
        \mu = \ln(\mu_R) - \frac{\sigma^2}{2}
        $$
    Then, the latency for each client is sampled from the lognormal distribution to capture skewed and heavy-tailed delays. 
    \item \textbf{Half-normal distribution}: The mean and standard deviation of the delays (i.e., $\mu_R$ and  $\sigma_R$) are calculated from the delay measurements on Raspberry Pis. Then, for each client, its latency is sampled from the half-normal distribution, ensuring non-negative values and a skewed distribution toward smaller delays.
    \item \textbf{Uniform distribution}: Client latency is sampled from a uniform distribution with bounds set between the 5th and 95th percentiles of the measurements from Raspberry Pis. This ensures outliers are excluded while covering the majority of the observed range.
\end{itemize}

\begin{figure}[htbp]
\centering
    \includegraphics[width=0.8\linewidth]{figures/exp_diff_delays.pdf}
    \caption{Delay patterns under different distributions: measurements from RPis show discrete peaks and high variability, half-normal and lognormal distributions have long tails, and uniform distribution assumes equal probability within a bounded range.
    }
\label{fig:exp-diff-delays}
\end{figure}

Figure~\ref{fig:exp-diff-delays} shows the simulated latency for 100 clients, each running CIFAR-100 with MobileNetV2 over 10,000 rounds. 

\section{Connections with Other Areas}
\label{sec:connections-with-other-areas}
\smallsection{Asynchronous Stochastic Gradient Descent}
Asynchronous stochastic gradient descent (SGD) is closely related to asynchronous federated learning and has provided theoretical and empirical foundations for scalable distributed training. Early studies analyzed error-runtime trade-offs, showing that incorporating stale gradients can alleviate system bottlenecks without significantly compromising accuracy~\cite{dutta2018slow}. Subsequent work refined convergence bounds based on maximum~\cite{stich2019error} or average~\cite{koloskova2022sharper} delay and demonstrated that asynchronous SGD can converge faster than traditional minibatch SGD~\cite{mishchenko2022asynchronous}. To tackle challenges such as gradient staleness, communication delays, and convergence guarantees, various strategies have been proposed, such as filtering out outlier gradients~\cite{xie2020zeno++,cohen2021asynchronous}, adjusting update steps according to delay~\cite{mishchenko2018delay,aviv2021learning}, and approximating gradients to compensate for delayed information~\cite{zheng2017asynchronous}. However, unlike federated learning, most asynchronous SGD formulations do not explicitly address non-i.i.d. data distributions or the strict data privacy constraints inherent in federated settings, which limits their direct applicability. 

\smallsection{Continual Learning} There are works in continual learning~\cite{farajtabar2020orthogonal,chaudhry2020continual,saha2021gradient,lin2022trgp,chen2022class} that leverage the idea of orthogonalization to project gradients onto non-conflicting subspaces across tasks. Continual learning aims to prevent catastrophic forgetting, where new knowledge overrides previously learned information when training a model on a sequence of tasks. It shares a common goal with asynchronous federated learning, which is to mitigate knowledge interference. Despite the similarities, the two fields follow distinct training paradigms. Continual learning typically processes tasks sequentially in a centralized setting. Methods can reuse data or access sample-level information (e.g., gradient) from previous tasks. In contrast, federated learning involves repeated rounds of training from distributed clients with data constraints. The model must integrate updates without knowing data or gradients from clients to preserve privacy.
Therefore, the two fields require different optimization techniques and system designs.

\section{Adaptability to Dynamic Environments}
\label{sec:discussion}
The asynchrony and orthogonal calibration mechanisms not only accelerate training but also extend its applicability to more complex scenarios. These features make \our suitable in dynamic federated learning environments. We discuss the following situations:

\smallsection{Dynamic Client Participation}
In real-world federated learning applications, it is common for new clients to join the training process~\cite{park2021tackling} or for previously unseen data with new tasks to appear over time~\cite{yang2024federated,yoon2021federated,ma2022continual,dong2022federated}. Scalability to new clients and tasks is essential for long-term performance. \our is readily applicable to these settings. By orthogonal calibration on asynchronous updates, \our mitigates the disruptive impact of newly joined clients, especially if the global model is well-trained and initial updates from newly joined clients are noisy or biased due to limited data or distribution shifts. Similarly, when new classes are introduced, \our preserves knowledge of previously learned classes, reducing forgetting and performance degradation caused by sudden distribution shifts.

\smallsection{Failure Handling} Large-scale federated learning faces a high risk of client or system failures due to issues such as out-of-memory (OOM) errors during training, hardware malfunctions, or network interruptions~\cite{jang2023oobleck,gupta2017failures,jeon2019analysis,weng2022mlaas}, particularly when training large models across many devices. Failure rates tend to increase with larger models and more clients, as the computational and communication demands scale up significantly. The ability to effectively handle these failures is critical for ensuring robust and reliable training in federated learning systems. The asynchronous nature of \our makes it robust to such failures. When a client fails, training continues with updates from the remaining clients. A single node's failure does not block global progress.

\section{Pseudo-code of \our}
\label{sec:pseudo-code}
\begin{algorithm}[htbp]
    \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
    \SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    
    \Input{Total number of updates $T$, local training epochs $E$, initial global model weights $W^{(0)}$. 
    }
    \Output{Global model weights $W^{(T_+)}$.}
    
    \SetAlgoNlRelativeSize{0}
    
    \underline{\textbf{Server execution:}} \\
    Send $W^{(0)}$ to all available clients\;
    \For{$t = 1, \dots, T$}{
            Receive update from a client\;
            Calculate delay $t - \tau$\;
                Compute global weight shift: $\Delta W = W^{(t)} - W^{(\tau_+)}$ \;
                Compute client weight change: $\Delta W_m = W_m^{(t)} - W_m^{(\tau_+)}$ \;
                Orthogonalize each layer: $\Delta W^{l\perp} = \Delta W^{l} - \frac{\Delta W^{l} \cdot \Delta W^{l}_m}{\Delta W^{l}_m \cdot \Delta W^{l}_m} \Delta W^{l}_m$\;
                Merge weights for client: $W_m^{(t_+)} = W_m^{(t)} + \Delta W^\perp$\;
            Update global model: $W^{(t_+)} = (1 - \beta_t) W^{(t)} + \beta_t W_m^{(t)}$\;
            ClientUpdate($m$, $W_m^{(t_+)}$)
    }
    \Return $W^{(T_+)}$\;
    
    \underline{\textbf{ClientUpdate}}($m$, $\tilde{W}$):\\
    Initialize local model: $W_{m} \leftarrow \tilde{W}$\;
    \For{$e = 1, \dots, E$}{
        Partition $\mathbf{D}_m$ into mini-batches $\bigcup_{i=1}^{j_m} B_i^{(m)}$\; 
        \For{$i = 1, \dots, j_m$}{
            Update local weights: $W_m \leftarrow W_m - \eta_c\nabla_{W_{m}}{\mathcal{L}}_{m}(W_m; B^{(m)}_i)$ \; 
        }
    }
    \Return $W_{m}$ to server\;
  
  \caption{\our Framework}
  \label{algo:federated-framework}
\end{algorithm}
