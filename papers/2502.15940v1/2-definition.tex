\section{Preliminaries} \label{sec:preliminary}

\subsection{Asynchronous System Architecture}
In an asynchronous federated learning setup, a central server coordinates the training of a global model $W$ using data distributed across $M$ clients. Each client $m \in \{1, 2, \ldots, M\}$ possesses its own local dataset $\mathbf{D}_m$. The data distribution of client \(m\) is denoted as \(\mathcal{P}_m\).
The objective is to train a global model $W$ that generalizes well across the combined data distribution of all clients.
Formally, we aim to solve the following optimization problem:
\begin{align}
W^* = \arg \min_{W} \frac{1}{M} \sum_{m=1}^{M} \mathbb{E}_{(x,y)\sim \mathcal{P}_m} \ell \left( f(x; W), y \right),
\end{align}
where $W$ denotes the global weights, \(\ell\) the loss function, and $f(x; W)$ the prediction of the model on data $x$ with model weights $W$.

Clients perform local training and communicate their updates to the central server at different times.
Let $T$ be the number of global rounds. For $t \in \{1, \dots, T\}$, denote $m_t \in \{1, \dots, M\}$ as the client that communicates with the server at the $t$-th round, and $\tau_t$ as the round when client $m_t$ last communicated with the server. We define the staleness of the client update as follows: 
\begin{definition}[Staleness] Staleness quantifies the delay between a client's updates, representing the number of global rounds since the client last communicated with the server. Formally, let $t$ be the current global round, and $\tau_t$ the global round when the server last received updates from client $m_t$. The staleness of client $m_t$ is defined as $t - \tau_t$, where $t - \tau_t \geq 1$. A staleness of 1 indicates no delay. 
\end{definition}
For simplicity, we will drop the subscripts on $m_t$ and $\tau_t$ with no ambiguity from now on.

\subsection{A Motivating Study}
We conduct an experiment on MNIST~\cite{deng2012mnist} with a LeNet5~\cite{lecun1998gradient} model to analyze the challenges in asynchronous federated learning.
We simulate the scenario with two clients: one with a 10-second latency and the other with 30, 60, or 100 seconds. We adopt the asynchronous method, FedAsync~\cite{xie2019asynchronous}, where client updates are aggregated with decay factors based on latency. Let $W^{(t)}$ denote the global weights at $t$-th round before aggregation, and $W^{(t_+)}$ the global weights after aggregation. Similarly, let $W_m^{(t)}$ represent the model weights of client $m$ at $t$-th round. The aggregation follows:
\begin{align}
\beta_t &= (t-\tau)^{-a}\cdot\beta, \\
W^{(t_+)} &= (1-\beta_t) W^{(t)} + \beta_t W^{(t)}_m
\label{eq:fedasync}
\end{align}
where $\beta$ and $a$ are hyperparameters set to $\beta=0.6$ and $a=0.5$ as reported in FedAsync.
To create non-IID data, each client is assigned a non-overlapping half of the MNIST classes. 

\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \subfigure[Global Accuracy]{
    \centering
    \includegraphics[width=0.47\linewidth]{figures/motivation_mnist_alpha0.0_acc.pdf}
    \label{fig:motivation-noniid-mnist-acc}
    }
    \subfigure[Global Weight Change]{
    \centering
    \includegraphics[width=0.47\linewidth]{figures/motivation_mnist_alpha0.0_grad.pdf}
    \label{fig:motivation-noniid-mnist-grad}
    }
    \caption{
    Asynchronous learning with a fast client (10s latency) and a slow client (30/60/100s) assigned non-overlapping classes. Due to objective inconsistency: (a) accuracy spikes when the slow client updates, followed by drops as the fast client updates; (b) update directions shift abruptly when the active client switches.
    }
\end{wrapfigure}

The global performance is shown in Figure~\ref{fig:motivation-noniid-mnist-acc}, where markers represent updates from the ``slow'' client with longer latency. We observe an increase in accuracy when the server aggregates updates from the slow client, as these updates introduce knowledge of previously undertrained classes. However, this gain is gradually lost, with accuracy declining to around 0.5 after several updates from the faster client. 
This suggests the fast client's updates override the contributions of the slower client. 
Moreover, as the latency of the slower client increases, the decay factor $\beta_t$ for integrating its updates decreases. This weakens its contribution to the global model and slows convergence, especially under non-IID data, as valuable knowledge from the slower client is not fully utilized.

Figure~\ref{fig:motivation-noniid-mnist-grad} visualizes changes in global model weights in the final hidden layer before the classifier in the case where the latency of the two clients is 10 and 100 seconds respectively.
The y-axis represents neurons, and the x-axis represents the number of updates.
The color indicates the direction and degree of global weight changes, with red representing an increase and blue a decrease. 
We observe abrupt shifts occur when switching between clients. Updates from the slow client often decrease the neuron weights (blue), while subsequent updates from the fast client increase the weight values (red), pulling the model in opposite directions. 
The antagonistic behavior is due to objective inconsistency---while the global model optimizes for the overall distribution, client updates follow distinct local objectives, driving oscillations in weight aggregation.