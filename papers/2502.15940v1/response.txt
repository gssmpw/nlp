\section{Related Works}
We review two core directions relevant to our study. Connections with other areas including asynchronous stochastic gradient descent and continual learning are discussed in Appendix~\ref{sec:connections-with-other-areas}.

\smallsection{Federated Learning and Heterogeneity Problem}
Federated learning**McMahan, "Communication-Efficient Learning of Deep Networks from Partially Ordered Data"** is a distributed learning paradigm that allows multiple parties to jointly train machine learning models without data sharing, preserving data privacy. Despite the potential, it faces significant challenges due to heterogeneity among participating clients, which is typically classified into two main categories: data heterogeneity and system heterogeneity. Data heterogeneity appears as clients own non-IID (independent and identically distributed) data**Hard, "Federated Learning with Non-IID Data"**. The difference in data distribution causes the local updates to deviate from the global objective, making the aggregation of these models drift from the global optimum and deteriorating convergence. System heterogeneity refers to variations in client device capabilities, such as computational power, network bandwidth, and availability**Konečnỳ, "Federated Learning: Strategies for Improving Communication Efficiency"**. These disparities lead to uneven progress among clients, and the overall training process is delayed by slow devices. Traditional federated learning approaches rely on synchronization for weight aggregation**McMahan, "Communication-Efficient Learning of Deep Networks from Partially Ordered Data"**, where the server waits for all clients selected in a round to complete and return model updates before proceeding with aggregation. This synchronization leads to inefficient resource utilization and extended training times, particularly in large-scale deployments involving hundreds or thousands of clients. Addressing the heterogeneity issues is a critical problem for improving the scalability and efficiency of federated learning systems in real-world deployment.

\smallsection{Asynchronous Federated Learning}
Much of the asynchronous federated learning literature focuses on staleness management by assigning weights for aggregating updates according to factors including delay in updates**Haddad, "Communication-Efficient Distributed Learning with Sparse Network Structures"**, divergence from the global model**Li, "On the Convergence of Federated Learning"** and local losses**Wu, "Federated Learning with Local Updates and Staleness Aware Weighting"**. For example, **Zhang, "Asynchronous Federated Learning: A Systematic Review"** lets the server aggregate client updates into the global model with a weight determined by staleness. Another line of research caches client updates at the server and reuses them to calibrate global updates**Konečnỳ, "Federated Learning: Strategies for Improving Communication Efficiency"**. For example, **Li, "On the Convergence of Federated Learning"** maintains the latest update for every client to estimate their contribution to the current aggregation and calibrate global updates. Furthermore, semi-asynchronous methods**Wang, "Semi-Asynchronous Federated Learning with Adaptive Sampling"** balance between efficiency and training stability. For example, **Liu, "Semi-Sync: Semi-Synchronous Federated Learning for Resource-Constrained Clients"** buffers a fixed number of client updates before aggregation. We select representative methods from each category for our comparisons. Besides, some works improve efficiency from a different perspective---through enhanced parallelization. Methods include decoupling local computation and communication**Mao, "Decentralized Federated Learning with Adaptive Local Computation"**, and parallelizing server-client computation**Ji, "Federated Learning with Parallelized Server-Client Computation"**. In addition, asynchronous architectures have been explored in other paradigms such as vertical**Kong, "Vertical Federated Learning: An Efficient Approach for Heterogeneous Data Sources"** and clustered**Gao, "Clustered Federated Learning: A Scalable Framework for Multi-Agent Systems"** federated learning. While these directions complement our work, they fall outside the scope of this study.