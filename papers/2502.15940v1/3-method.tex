\section{Method: Orthogonal Weight Calibration}
\subsection{\our Algorithm}
\label{sec:method}

Once receiving a client update, \our immediately integrates it into the global model. 
\our maintains \emph{separate} variables for global and client models. 
Before merging global weight shift to the client model, the server orthogonalizes the global shift against the received client update. This orthogonality allows the client to incorporate global progress while continuing its local optimization without disruption. The pseudo-code is presented in Appendix~\ref{sec:pseudo-code}. 

\smallsection{Global Aggregation via Moving Average} 
Denote $W^{(t)}$ as the global model weights at the $t$-th round before client update and $W^{(t_+)}$ after update. Similarly, let $W_m^{(t)}$ be the client $m_t$'s local model weights at the $t$-th round before update and $W_m^{(t_+)}$ after update. Note that $W^{(t+1)} := W^{(t_+)}$ as the global model weights stay unchanged after communication with a client before the next client update. We update the global model with a moving average:
\begin{equation}
    W^{(t_+)} = (1-\beta_t) W^{(t)} + \beta_t W_m^{(t)},
\label{eq:aggregation}
\end{equation}
where $\beta_t$ controls the contribution of client $m$'s current update to the aggregation. We let $\beta_t := s_a(t-\tau) \cdot \beta$ with $\beta \in (0,1)$ and $s_a(x) = x^{-a}$ for some $a > 0$ so that update with a larger staleness has a smaller contribution, and thereby decreasing the influence of client update with long delay.

\smallsection{Calibration on Client Updates} 
To minimize interference caused by asynchronous updates, we orthogonalize the global weight change that occurs between the client's successive updates against client's local udpate before sending it to the client for the next round of training.
Formally, when the server receives an update from client $m$, if the staleness $t - \tau > 1$, it calculates the local weight change from its last update to its current update:
\begin{equation}
  \Delta W_m = W_m^{(t)} - W_m^{(\tau_+)} .
\end{equation}

Similarly, the server calculates the global weight shift due to aggregating updates from other clients during this period:
\begin{equation}
\Delta W = W^{(t)} - W^{(\tau_+)}.
\end{equation}

To update client $m$ with global progress, the server computes the orthogonal component of $\Delta W$ with respect to $\Delta W_m$. The orthogonalization is done for the weight of each layer through removing the component that is parallel to $\Delta W_m$. Let $\Delta W^{l}$ and $\Delta W^{l}_m$ be the change in the layer $l$ of the global weights and the local weights respectively. The component of $\Delta W^l$ orthogonal to $\Delta W_m^l$ is:
\begin{equation}
\label{eq:8}
\begin{aligned}
    \Delta W^{l\perp} = \Delta W^{l} - \text{proj}_{\Delta W_m^{l}}(\Delta W^{l})
    = \Delta W^{l} - \frac{\Delta W^{l} \cdot \Delta W_m^{l}}{\Delta W_m^{l} \cdot \Delta W_m^{l}} \Delta W_m^{l}.
\end{aligned} 
\end{equation}

Let $\Delta W^{\perp}$ represent the aggregation of $\Delta W^{l\perp}$ across all layers.
This orthogonal component $\Delta W^{\perp}$ ensures that the updates from the other clients during delay $t-\tau$ do not interfere with the client's local progress, as it removes any component of the global weight change during delay along the direction of the local update.
$\Delta W^{\perp}$ is then sent to current client $m$ to form the new client model weight for the subsequent round of local training on  client $m$:
\begin{equation}
    W_m^{(t_+)} = W_m^{(t)} + \Delta W^\perp.
\end{equation}

\subsection{Mathematical Basis of Orthogonalization}
\label{sec:math}
A gradient update orthogonal to previously accumulated gradients helps preserve existing model behavior and minimizes unintended changes to its outputs~\cite{farajtabar2020orthogonal}.
Due to the high-dimensional parameter space of neural networks, there are multiple directions that are orthogonal to the stale local weight update. We have the following lemma showing that the orthogonalization strategy in \our preserves the maximal information from the global weight shift vectors perpendicular to the local update direction. The proof is given in Appendix~\ref{sec:proof}.

For $v, w \in \mathbb R^d$, let $\langle v, w \rangle := \sum_{i=1}^d v_i w_i$ be the standard inner product on $\mathbb R^d$.
\begin{lemma}
\label{eq:lemma}
    Let $v \in \mathbb R^d$ and $\mathcal{U} = \{u_1, \cdots, u_k\}$ be an orthonormal set for some $k < d$. Then for any $w \in (\mathrm{span}\,\mathcal{U})^\perp$, 
    \begin{equation}
        \label{eq:4}
        \Vert v - v^\perp \Vert \le \Vert v - w \Vert,
    \end{equation}
    where $v^\perp:= v - \sum_{i=1}^k \langle v, u_i\rangle u_i$ denote the component of $v$ orthogonal to $\mathcal{U}$. Moreover, the angle between $v$ and $v^\perp$ is less than any angle between $v$ and $w$ for $w \in (\mathrm{span}\,\mathcal{U})^\perp$.
\end{lemma}

We apply this lemma in our case when $v = \Delta W^l$ and $\mathcal{U} = \{\Delta W_m^l\}$. It implies $\Delta W^{l\perp}$ in equation~\eqref{eq:8} is the unique vector among those perpendicular to $\Delta W_m^l$ with the smallest magnitude of $\Vert \Delta W^l - \Delta W^{l\perp}\Vert$ and the smallest angle with $\Delta W^l$. This allows the server to pass the maximum knowledge from global progress to client $m$ without interfering with its most recent update.

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{figures/method-orthogonal-calibration.pdf}
    \caption{An example of optimization trajectories. Shaded regions represent iso-loss contours for client A (yellow) and other clients (gray). Deeper colors indicate lower loss areas. \our removes conflicting components via orthogonalization, merging updates with minimal interference.
    }
\label{fig:method-orthogonal-calibration}
\end{figure}

\subsection{Visualization of Optimization Trajectories}
We illustrate the advantage of our method by visualizing an example of optimization trajectories. As shown in Figure~\ref{fig:method-orthogonal-calibration}, client A begins local training from the global state $W^{(t=0)}_\text{global}$. Before A's update arrives, the server aggregates two updates from other clients into the global model. Consequently, just before aggregating A's update at $t=3$, the global model has evolved to $W^{(t=3)}_\text{global}$. Meanwhile, client A finishes local training and submits the updated parameters $W^{(t=3)}_A$. The shaded regions show the iso-loss contours for client A (yellow) and the collective optimization space of other clients (gray).

The global weight is updated via a moving average and becomes $W^{(t=3_+)}_\text{global}$. In conventional asynchronous methods, this global weight is directly assigned to client A (gray dashed line). This would push the model farther from A's optimization objective than $W^{(t=3)}_A$, reversing A's learning progress. \our mitigates this by removing the component of the global weight shift that is parallel to $\Delta W_A$. This ensures that the calibrated parameters are orthogonal to A's update direction. Finally, the calibrated global shift is merged into A's model (red dashed line), which becomes $W^{(t=3+)}_A$. This way, \our reduces interference due to staleness and objective inconsistencies while preserving meaningful contributions at both global and local levels. 