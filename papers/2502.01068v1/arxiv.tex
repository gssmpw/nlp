%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{caption}
\usepackage{algpseudocode}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml_to_arxiv}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmicx}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\icmltitlerunning{FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation}

\begin{document}

\twocolumn[
\icmltitle{FastKV: KV Cache Compression for Fast Long-Context Processing\\with Token-Selective Propagation}
% \icmltitle{FastKV: KV Cache Compression for Fast Prefill and Generation\\with Token-Selective Propagation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dongwon Jo }{equal,snu}
\icmlauthor{Jiwon Song}{equal,snu}
\icmlauthor{Yulhwa Kim}{skku}
\icmlauthor{Jae-Joon Kim}{snu}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{snu}{Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea}
\icmlaffiliation{skku}{Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea}

\icmlcorrespondingauthor{Yulhwa Kim}{yulhwakim@skku.edu}
\icmlcorrespondingauthor{Jae-Joon Kim}{kimjaejoon@snu.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage.
Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency.
% Previous efforts to compress these KV caches primarily focus on reducing memory demands but failed to significantly enhance latency, especially during the prefill stage.
To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences.
% To address this issue, we introduce FastKV, an innovative KV cache compression method designed to enhance latency for long-context sequences, even during the prefill stages.
To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage.
Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency.
Our experimental results show that FastKV achieves 2.00$\times$ and 1.40$\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines.
Our code is available at \url{https://github.com/dongwonjo/FastKV}.
% There has been a surge in the development of large language models (LLMs) capable of handling long contexts. However, the rapid growth of key-value (KV) cache sizes imposes significant constraints on computational efficiency and memory requirements. While existing methods on KV cache compression primarily focus on accelerating the generation stage by selectively compressing KV caches during the prefill stage, they struggle to reduce the computational overhead during the prefill stage. To address this limitation, we propose FastKV, a novel method that leverages Token-Selective Propagation (TSP) to significantly reduce computation by propagating only the most important tokens to subsequent layers during the prefill stage, while ensuring efficient KV cache management in the generation stage. This TSP approach implicitly preserves the computational information of the full-context in the early layers, enabling inference in deep layers with only compressed tokens. Additionally, FastKV is designed to be fully compatible with grouped-query attention (GQA) models, compressing KV caches in a group manner. Our experimental results demonstrate that FastKV achieves up to 1.91$\times$ and 4.96$\times$ speedup for time-to-first-token (TTFT) and throughput respectively, compared to the baseline when processing a context of 128k, maintaining high accuracy on long context benchmarks.
\end{abstract}

\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/Fig1.PNG}
    % \vspace{-12pt}
     \vspace{-22pt}
    \caption{
        Comparison of accuracy, TTFT, throughput across different KV cache compression methods on LLaMA-3.1-8B-Instruct
        % FastKV achieves performance comparable to existing KV cache compression methods while demonstrating low TTFT and high throughput.
    }
    \vspace{-12pt}
    \label{fig:introduction}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/Fig2.PNG}
    % \vspace{-12pt}
     \vspace{-20pt}
    \caption{
    Comparison of the number of tokens processed in each layer/head of LLMs during prefill computation and KV caching across different KV cache compression techniques. 
    As each token produces its corresponding KV, the KV cache size is directly proportional to the number of tokens processed.
    The blue background box indicates the set of sharing selected token indices.
    % Comparison of the number of tokens processed in each layer and head of LLMs during prefill and generation stage across different KV cache compression techniques. As each token produces its corresponding KV and only cached KV data is used for generation, the KV cache size is directly proportional to the number of tokens processed during generation.
        % Brief overview for various KV cache compression methods. SnapKV performs inference over the entire input sequence while storing a fixed KV budget uniformly across all heads. AdaKV maintains the average KV budget for each layer while dynamically allocating KV budgets to each heads, whereas HeadKV fixes the total KV budget and flexibly assigns KV budgets. GemFilter employs recomputation to perform inference using only the important tokens. FastKV introduces Token-Selective Propagation to accelerate the prefill stage through the integration of contextual information for all tokens in the early layers.
    }
    \vspace{-14pt}
    \label{fig:background}
\end{figure*}

Recent advancements in large language models (LLMs) have enabled the processing of long-context sequences, such as those comprising 128k tokens~\cite{gpt4,gemini,claude3}. This capability significantly broadens the range of applications for LLMs~\cite{intro1, intro2, intro3}. 
% However, it is expensive to handle long contexts due to their computational cost and memory requirements to manage key-value (KV) caches. 
However, as the length of the input sequence increases, the size of these key-value (KV) caches also increases, making them a significant bottleneck in the serving of long-context LLMs. Therefore, compression of KV caches is essential for optimizing the operation of LLMs.
% However, the key-value (KV) cache overhead increases with sequence length, introducing a major inefficiency in long-context processing scenarios. For instance, in the \textcolor{red}{LLaMA-3-8B} model, managing a sequence of 128k tokens requires \textcolor{red}{xx}GB of memory for KV caches, whereas the model itself only requires 16GB. This excessive demand for KV cache memory creates a critical bottleneck in LLM inference during long-context processing, and efficient compression of KV caches is crucial for streamlined LLM operation.

% Large language models (LLMs) have demonstrated exceptional performance in handling long contexts, making them widely applicable in various domains such as conversational systems, document summarization, and question answering. Notably, recent open-source models, such as LLaMA-3.1/3.2, Mistral-Nemo, Phi-3.5-mini and Qwen2.5, support input sequence lengths of up to 128k tokens. However, it is expensive to handle long contexts due to their computational cost and memory requirements to manage key-value (KV) caches. As the input sequence length and batch sizes required for LLM serving increase, these KV cache bottlenecks pose challenges to throughput, which is critical for real-time applications serving many users concurrently. 

There has been active research on compressing KV cache to alleviate its burden in long-context handling~\cite{h2o, streamingllm, duoattention, tova, nacl}.
Some techniques have demonstrated the ability to preserve accuracy comparable to full-context processing after the compression~\cite{snapkv,adakv,headkv}, while there exists a compression technique that has achieved inference speedup in both the prefill and generation stages~\cite{gemfilter}.
% Some techniques have demonstrated the ability to preserve accuracy with less than a 2\% drop compared to full-context processing with a KV budget of 512, meaning that KV caches for an average of 512 tokens are saved per attention head~\cite{adakv,headkv}.
% Additionally, there exists a technique that has achieved inference speedup in both the prefill and decoding stages through KV cache compression~\cite{gemfilter}.
However, no KV cache compression techniques currently exist that can simultaneously preserve accuracy and achieve inference speedup, particularly during the prefill stage.
% KV cache compression techniques for long-context processing have focused on pruning tokens to eliminate their associated KV caches. The dominant approach in this area utilizes the attention scores generated by each attention head to selectively prune tokens in a fine-grained manner, based on the premise that the importance of tokens may differ across attention heads. While these methods are effective in preserving accuracy, they have demonstrated limitations in terms of enhancing latency and throughput.

% Recently, GemFilter has introduced a unique strategy by directly pruning tokens from input sequences, based on the premise that only a small fraction of tokens are genuinely significant in long-context scenarios. By adopting sequence-level coarse-grained pruning for KV cache compression, it offers advantages in enhancing latency and throughput. However, this approach introduces substantial risks to the accuracy of LLMs by potentially omitting critical tokens that may be relevant in broader contexts.


% The prevailing approach in KV cache compression employs the attention scores generated by each attention head to selectively prune tokens that receive low attention scores, thereby eliminating their associated KV caches. The choice of tokens and their KV caches for pruning varies across the attention heads. Although these methods effectively compress KV caches, they require a complete prefill process with long-context input sequences, as they require attention scores from all attention heads in their entirety. Consequently, these techniques fail to enhance the efficiency of the prefill stage, which is directly linked to the time-to-first token (TTFT), an essential metric for user experience.

% Recently, GemFilter has introduced the concept of ``retrieval layer'' to mitigate this issue. It operates under the premise that only a small fraction of tokens in long-context sequences are truly significant, and these can be effectively identified using the outputs from the retrieval layer, typically located in the middle of the LLMs.
% By processing half of the LLM models with full context and utilizing the output of the retrieval layer, GemFilter identifies the critical tokens. It then returns to the initial prefill stage with shorter sequences containing only these selected tokens and re-computes the streamlined prefill process. 
% % Additionally, this method enhances the throughput of overall long-context processing compared to prior KV cache compression strategies through coarse-grained pruning of tokens at the sequence level. This eliminates the need for attention-head-wise operations and allows for seamless integration with existing LLM serving platforms, similar to how shorter sequences are handled. While this approach significantly boosts processing speed, it poses a substantial risk of compromising the accuracy of LLMs.
% While this method accelerates the processing of long-context sequences through coarse-grained pruning of tokens at the sequence level, it risks significantly compromising the accuracy of LLMs.
% % To address these issues, recent works such as SnapKV, AdaKV, and HeadKV have successfully managed to compress the KV cache size by evicting tokens with attention-based strategy during the prefill stage. These methods achieve a high compression ratio for input sequences without significant degradation of performance, effectively improving throughput during the generation stage. 

% However, they still require computation for the entire input sequence during the prefill stage and introduce additional overhead in evicting tokens based on the each attention heads to store only the most important tokens in the KV cache. These issues pose another challenge, as they prevent acceleration of the prefill stage. When serving LLMs with a long context in practical scenarios, they limit the time-to-first token (TTFT), which is a crucial metric for user experience, particularly in interactive applications where immediate feedback is expected. 

% Furthermore, recent works have introduced more fine-grained and complex techniques aimed at enhancing the performance of long context compression. However, these approaches often compromise the primary objective of long context compression by sacrificing throughput, thereby reducing efficiency in practical scenarios.

% Meanwhlie, despite effectively reducing memory requirements through KV cache compression, previous works remain incompatible with recent models employing Grouped-Query Attention (GQA). These methods rely on attention heads to select important tokens for eviction, but in GQA models, each head within the same group exhibits different attention heads due to their queries. As a result, KV caches must be stored separately for each head, further exacerbating memory and computational costs.

In this paper, we propose FastKV, an innovative approach designed to expedite long-context handling with LLMs while maintaining their accuracy.
FastKV stems from our finding that the properties of attention maps differ between the early and later layers of LLMs. Through detailed analysis of LLMs, we discover that in the later layers, attention maps concentrate on a limited set of significant tokens, which remain consistent across layers. In contrast, the early layers engage with a broader array of tokens and exhibit diverse attention patterns as they process the context of the input prmopt.
Based on these findings, FastKV adopts a novel Token-Selective Propagation (TSP) method that applies different strategies of KV cache compression at the early and later layers respectively
%, as illustrated in Figure~\ref{fig:introduction}. 
% Based on these findings, FastKV adopts a novel Token-Selective Propagation (TSP) method that implements fine-grained KV cache compression strategies in the initial layers and applies coarse-grained compression techniques in the subsequent layers (Figure~\ref{fig:introduction}).
As shown in Figure~\ref{fig:introduction}, this dual-strategy approach enables FastKV to improve both time-to-first-token (TTFT) and throughput of long-context processing while effectively preserves the accuracy of LLMs.
% This dual-strategy approach enables us to devise a KV cache compression technique that improves both latency and throughput of long-context processing while effectively preserves the accuracy of LLMs.
Our experimental results show that FastKV can achieve 2.00$\times$ speedup for TTFT and 1.40$\times$ improvement for throughput compared to HeadKV, while maintaining a similar level of accuracy with less than a 1\% accuracy gap.
%other KV cache compression methods, maintaining high accuracy. 
These results demonstrate that FastKV promises a practical solution for long context scenarios, particularly in real-time applications that require efficient KV cache management and low latency for the prefill and generation stages.
% In this paper, we propose FastKV, a novel approach that accelerates the prefill and generation stages with the Token-Selective Propagation (TSP) method, while effectively compressing KV caches. Unlike conventional KV cache compression methods, TSP avoids performing computations on the entire input sequence across all layers during the prefill stage. Instead, it selectively evicts less important tokens at a selected layer and propagates only the most critical tokens to deep layers through attention-based indexing. Based on our analysis, this approach ensures computation in early layers, which capture broad contextual information. In contrast, deep layers focus on the important tokens propagated from the selected layer and implicitly preserving the information of full-contexts, thus substantially reducing computational overhead during the prefill stage. Moreover, FastKV is designed to be fully compatible with GQA models by caching a single KV per head group, enabling efficient KV cache management during the generation stage.

% As shown in Figure~\ref{fig:introduction}, this design allows FastKV to accelerate both the prefill and the generation stages, achieving up to x.xx$\times$ speedup for TTFT and x.xx$\times$ improvement for throughput, compared to other KV cache compression methods, maintaining high accuracy. These results demonstrate that FastKV promises a practical solution for long context scenarios, particularly in real-time applications that require efficient KV cache management and low latency for the prefill and generation stages.

\section{Background}

% \subsection{Attention Mechanism (?)}

\subsection{Long-Context Processing with LLMs}
LLMs have significantly enhanced the natural language processing (NLP) abilities of AI systems~\cite{background1, background2, background3, background4} through the use of attention mechanisms~\cite{attention}.
These attention mechanisms carefully evaluate the relationships between tokens within sentences to construct context vectors by aggregating token data according to the attention scores to extract context-specific information~\cite{attention_survey}.
% These attention mechanisms carefully evaluate the relationships between tokens within sentences to derive essential context-specific information. 
Distinct from earlier NLP models such as RNNs~\cite{rnn} and LSTMs~\cite{lstm}, attention mechanisms consider the entire token data from input sequences, thereby avoiding issues of prompt forgetting. Benefiting from these advancements, recent researches indicates that LLMs are capable of processing long-context sequences. 

However, the very nature of LLMs, which retain all token data to facilitate attention mechanisms, leads to significant memory overhead. Typically, token data for attention mechanisms are stored in the form of KV cache, and the size of KV cache escalates with sequence length, becoming a primary source of inefficiency in LLMs when processing long-context scenarios. For instance, in the LLaMA-3.1-8B model~\cite{llama3}, managing a sequence of 128k tokens requires 17.12GB of memory for KV cache, whereas the model itself only requires 16.06GB. Therefore, the excessive KV cache demands form a critical bottleneck in LLM inference during long-context processing, and efficient compression of KV cache is crucial for streamlined LLM operation.

% \subsection{LLM Inference}
% In LLM inference, the attention mechanism exhibits distinct characteristics across two stages. In the prefill stage, attention is computed in parallel over the input sequence. This stage is compute-bounded, and the computation increases quadratically with the sequence length. During the generation stage, the GEMV operation is performed on a single query and the key-value pairs in cache. Since batching is not available in this stage and loading for the KV cache is a bottleneck, it is inherently memory-bounded. As a result, accelerating LLM inference for long contexts remains challenging in both the prefill and generation stages due to their different characteristics.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{images/Fig3.PNG}
%     % \vspace{-12pt}
%      \vspace{-6pt}
%     \caption{
%         Illustration of FastKV scheme. FastKV introduces Token-Selective Propagation to reduce the number of tokens required for computation while simultaneously storing compressed KV caches.
%     }
%     \vspace{-8pt}
%     \label{fig:FastKV}
% \end{figure*}


\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

\begin{algorithm}
% \caption{Prefill stage using FastKV.}
    \caption{FastKV algorithm for the KV cache compression during the prefill stage}
    \label{algo:fastkv}
\begin{algorithmic}[1]
\Require $input$ $sequence$ $\{I\},$ $\#layers$ $\{L\},$ $TSPlayer,$ $TSPlength$ $\{B_{TSP}\},$ $KVbudget$ $\{B_{KV}\}$
\Ensure $generated$ $token$ $\{O\},$ $KV$ $Cache$ $\{C\}$
\State $X \leftarrow Embedding(I)$ 
    \For{$l$ = 0 {\bfseries to} $L-1$}
        \If{$l \leq TSPlayer$}
            \State $X,Att_l,K_X,V_X\leftarrow layer_l(X)$ 
            %\Comment{$Att:Attention$ $Map$}
            \State $K,V\leftarrow KVCompress(K_X,V_X,Att_l,B_{KV})$ 
            \If{$l==TSPlayer$}
                % \State $H,Att,K_H,V_H\longleftarrow layer_i(H)$
                % \State $K',V'\longleftarrow Compress(Att,K_H,V_H,KVbudget)$
                \State $x \leftarrow HiddenCompress(X,Att_l,B_{TSP})$
                %\Comment{$h:hidden$ $states$ $of$ $selected$ $tokens$}
            \EndIf
        \Else
            \State $x,Att_l,K_x,V_x=layer_l(x)$
            \State $K,V\leftarrow KVCompress(K_x,V_x,Att_l,B_{KV})$
        \EndIf
        \State $C\leftarrow update(K,V)$
    \EndFor
    \State $O\leftarrow LMHead(x)$
    % \State $O\longleftarrow lm\_head(h[-1])$
    \State \Return{$O,C$}

\end{algorithmic}
\end{algorithm}


\subsection{KV Cache Compression}
% \textcolor{red}{ref Figure 2!}
KV cache compression techniques for long-context processing are primarily designed to prune tokens, thereby reducing the associated KV caches and effectively managing memory usage. A common approach leverages the attention scores generated by each attention head to selectively prune tokens in an attention-head-wise manner. This method operates on the premise that token importance varies across attention heads.
% A notable example is SnapKV~\cite{snapkv}, which evaluates token importance for each attention head by summing the attention scores assigned to each token within the observation window that corresponds to the last segment of the input sequence. \textcolor{red}{(maybe we can add equation for this)}
A notable example is SnapKV~\cite{snapkv} in Figure~\ref{fig:background}(a). It discovers that each attention head in the model consistently focuses on specific prompt token during generation, and this robust pattern can be obtained from an observation window located at the end of the input prompt. Thus, SnapKV evaluates token importance score for each attention head by summing the attention scores assigned to each token as follows:
\begin{equation}
    \label{eq:snpaKV-1}
    S^{l,h}_{i}=\frac{1}{2w_p+1}\sum_{m=-w_p}^{w_p} \sum_{n=0}^{N_{\text{obs}}} Att_l[h,\,N_I-n,\,i+m]
\end{equation}
% \vspace{-2px}
Here, $S_i^{l,h}$ is the importance score of $i$-th token in $h$-th attention head of the $l$-th layer. $Att_l$ denotes the attention score matrix of $l$-th layer, while $N_I$ and $N_{obs}$ indicate the number of tokens in the input prompt and the observation window, respectively. SnapKV applies average pooling on the scores to prevent sparse token selection. Here, $2w_p+1$ indicates the size of pooling window.
SnapKV selects tokens from the observation window first and then selects tokens with the highest importance scores for KV caching.
Since SnapKV relies on the attention scores produced by each attention head, it requires propagationg of the full input prompt throughout the entire model, during the prefill stage. As a result, SnapKV cannot improve computational efficiency or reduce the latency of the prefill stage, which directly impacts TTFT, a key factor in optimizing user experience for LLM serving.

AdaKV~\cite{adakv} in Figure~\ref{fig:background}(b) expands SnapKV approach to grouped-query-attention (GQA) mechanism~\cite{gqa} by introducing dynamic KV budget allocation across attention groups and selecting important tokens in attention-group-wise manner.
To enhance accuracy preservation, HeadKV~\cite{headkv} in Figure~\ref{fig:background}(c) builds upon SnapKV by introducing dynamic KV budget allocation across attention heads. 
% HeadKV~\cite{headkv} in Figure~\ref{fig:background}(c) builds upon SnapKV by introducing dynamic KV budget allocation across attention heads. 
% AdaKV~\cite{adakv} in Figure~\ref{fig:background}(b) presents grouped-query-attention (GQA) compatible mechanism~\cite{gqa} by introducing dynamic KV budget allocation across attention groups and selecting important tokens in attention-group-wise manner.
This feature has shown effective in preserving accuracy under aggressive KV cache compression scenarios, such as when the average KV budget is as low as 128.
% These methods have shown advantages in preserving accuracy under aggressive KV cache compression scenarios, such as when the average KV budget is as low as 128.
However, when applied to more realistic KV budget targets, such as 512, dynamic KV budget allocation exhibits marginal accuracy improvements or even a slight accuracy degradation compared to SnapKV, while they introduces notable latency and throughput overhead.
% However, when applied to more realistic KV budget targets, such as 512, these methods exhibit marginal accuracy improvements or even a slight accuracy degradation compared to SnapKV, while they introduces notable latency and throughput overhead.
% Furthermore, the varying KV budgets across attention heads introduces notable latency and throughput overhead at these budget levels. 
The primary reason of the overhead is that the processing speed of the attention mechanism is limited by the attention head with the longest KV cache. 
As the size of KV budget increases, the imbalance in KV cache sizes across attention heads becomes more pronounced and it amplifies latency overhead.
This overhead makes these methods less practical for real-world applications, where achieving low latency is critical.
% Recent advancements, such as AdaKV and HeadKV, build upon SnapKV by introducing dynamic KV budget allocation across attention heads. These methods have shown advantages in preserving accuracy under aggressive KV cache compression scenarios, such as when the average KV budget is as low as 128.
% % In these extreme cases, despite their improvements, achieving accuracy comparable to that of a full KV budget remains a challenge. 
% However, when applied to more realistic KV budget targets, such as 512, these methods exhibit marginal accuracy improvements or even a slight accuracy degradation compared to SnapKV, while they introduces notable latency and throughput overhead.
% % Furthermore, the varying KV budgets across attention heads introduces notable latency and throughput overhead at these budget levels. 
% The primary reason of the overhead is that the processing speed of the attention mechanism is limited by the attention head with the longest KV cache. 
% As the size of KV budget increases, the imbalance in KV cache sizes across attention heads becomes more pronounced and it amplifies latency overhead.
% This overhead makes these methods less practical for real-world applications, where achieving low latency is critical.
% Many prior works have focused on KV cache compression techniques to accelerate the generation stage. Some of these methods select important tokens based on attention heads and evict less critical tokens. By retaining only the important tokens in the KV cache, they reduce KV cache memory usage and accelerate the generation stage as shown in Figure~\ref{fig:background}. %Following this movement, various KV cache compression methods have been developed, as shown in Figure~\ref{fig:background}. 
% Among them, SnapKV represents the most naive approach, where important tokens are selected based on a local window corresponding to the last segment of the input sequence for each attention head. This method is straightforward to implement and achieves a high compression ratio for KV cache without significant accuracy degradation. However, SnapKV requires additional computation of attention heads over the entire input sequence to vote for important tokens, introducing computing overhead during the prefill stage. 



% % Moreover, They store KV caches for all attention heads, making it incompatible with GQA models.

% Building upon SnapKV, AdaKV introduces a head-wise fine-grained approach to KV cache compression. This method fixes the average KV budget for each layer while allowing individual attention heads within a layer to store KV budgets of varying lengths. HeadKV further extends AdaKV by fixing the total KV budget across all layers and flexibly allocating KV budgets of different lengths to individual heads. This flexibility enables the assignment of larger KV budgets to specific heads, improving performance for certain tasks.



% % Furthermore, AdaKV attempts to support GQA models by averaging the attention heads within the same group.

% For implementation of attention across heads with varying KV budgets, AdaKV and HeadKV customize the FlashAttention-2 kernel. By processing the varying KV budgets of each head as separate batches, this approach effectively handles heads with varying lengths in a manner similar to processing multiple batches. However, the latency of these methods is bounded by the head with the largest KV budget. Larger budgets for specific head lead to significant degradation for throughput, conflicting with the goal of the long context compression to accelerate the generation stage.

In contrast, GemFilter~\cite{gemfilter} in Figure~\ref{fig:background}(d) adopts a fundamentally different approach that enhances the computational efficiency of the prefill stage. GemFilter is based on the observation that LLMs can identify relevant tokens in intermediate layers, referred to as the filter layer. 
The filter layer selects the indices of relevant tokens at the layer level rather then head level, and GemFilter uses these indices to compress the input prompt. 
Then, the prefill stage is recomputed with compressed input prompt.
% The prefill stage is then restarted from the first layer, this time using the compressed input prompt.
By processing full-context with part of the LLM, GemFilter effectively reduces TTFT.
% By partially processing the LLM with full-context data initially and then completing the process with the compressed sequence, GemFilter effectively reduces latency during the prefill stage.
The GemFilter paper reports successful accuracy preservation when using a KV cache budget of 2k, where the input prompt is compressed with 2k tokens. 
Moreover, as GemFilter focuses on the selected tokens more intensively than a full-context model by only processing selected tokens, with sufficient KV budget, there are some cases where it achieves even better accuracy than the full-context model. 
However, as the removal of tokens from the input prompt results in the complete loss of any information embedded in the discarded tokens, GemFilter struggles to maintain accuracy with lower KV budgets, such as 512, and even with 2k KV budgets when handling complex tasks.
% However, our analysis reveals that GemFilter struggles to maintain accuracy with lower KV budgets, such as 512.
% This is because its approach removes tokens from the input prompt, and it results in the complete loss of any information embedded in the discarded tokens.
% While it effectively prioritizes selected tokens, its approach involves removing tokens from the input sequence, which results in the complete loss of any information embedded in the discarded portion. This makes GemFilter highly vulnerable to the size of the KV budget. 
% Additionally, by directly compressing input sequences, GemFilter fails to extract sufficient contextual information, making it less reliable for applications that require a more comprehensive representation of the input context. Consequently, its ability to preserve accuracy varies significantly across tasks.
Thus, there are currently no KV cache compression techniques that can adequately preserve accuracy while simultaneously improving TTFT/throughput. 
This underscores the need for new methods that effectively balance these objectives.


% On the other hand, unlike previous KV cache compression methods, GemFilter accelerates the prefill stage with KV cache compression simultaneously. This method selects a filter layer that effectively identifies important information (referred to as needles) and performs inference only up to the filter layer during the prefill stage. The attention heads of the filter layer are aggregated to retain only the most important tokens, after which the compressed tokens are reintroduced into the LLM for recomputation, starting from the first layer. This recomputation enables acceleration of the prefill stage for long contexts, as computations on the entire input sequences are only performed up to the filter layer, not across all layers. 

% However, in the process of recomputation, GemFilter excludes non-compressed tokens from the prefill stage and focuses solely on compressed tokens. This results in input sequences composed of only compressed tokens, which can be unnatural from the perspective of the LLMs. Consequently, this approach leads to significant degradation in general performance across various tasks.

% In this work, we propose FastKV, a method that bridges the gap between LLM performance and acceleration in the prefill and generation stages, preserving the core benefits of long context compression. Our approach reduces computation during the prefill stage through the Token-Selective Propagation (TSP) method, which compresses the full-context information in early layers and propagates this information to deep layers. Furthermore, this approach is fully compatible with grouped-query attention (GQA)~\cite{gqa}, enabling efficient KV cache compression. Consequently, FastKV achieves high accuracy on long context benchmarks while accelerating both the prefill and generation stages.


\section{Proposed FastKV}
\subsection{Overview of FastKV}
% - focus on describing the overall flow (early layer - full propagation, later layer - selective propagation)

The overview of FastKV is illustrated in Figure~\ref{fig:background}(e) and Algorithm~\ref{algo:fastkv}. FastKV implements distinct KV cache compression strategies for the early and later layers of LLMs.
To speedup the prefill stage, FastKV employs the TSP approach in the later layers. This approach is based on the insight that after the early layers have analyzed contextual information, the later layers tend to focus consistently on the same crucial tokens. The TSP approach, therefore, involves selectively propagating only a limited set of tokens (e.g. 2k tokens) identified by the TSP layer, which is strategically positioned in the middle of the LLM architecture.
The TSP layer utilizes its attention map to pinpoint these critical tokens. Unlike the early layers, which propagate the entire set of tokens, the TSP approach in the later layers transmits only the selected tokens to subsequent layers. This targeted propagation means that the later layers process far fewer tokens, thus it can accelerate the processing speed in these layers.
While the TSP approach results in a smaller set of KV data for the selected tokens, FastKV still applies the same KV cache compression techniques used in the early layers to optimize the compression ratio. 
In the early layers, FastKV evaluates the attention scores generated by each attention head to determine the importance of KV data. 
% Similar to AdaKV,
GQA compatible KV cache compression of FastKV selectively removes the KV data of tokens that receive low attention scores in an attention-group-wise manner.
% By employing the GQA strategy in the compression of the KV cache, FastKV enhances its ability to manage KV cache efficiently in these early layers.
It is important to note that KV data compression occurs solely during the saving of KV data to establish KV caches for input prompt. Consequently, FastKV ensures that the entire set of KV data, including those of the removed tokens, is propagated to the subsequent layer until the TSP layer. This approach guarantees full context propagation to preserve essential contextual information from the input prompt.
% In the early layers, FastKV evaluates the attention scores generated by each attention head to determine the importance of KV data. 
% %Unlike previous methods, 
% FastKV incorporates the core concept of the grouped-query attention (GQA) approach, which involves sharing KV data across attention groups. FastKV selectively removes the KV data of tokens that receive low attention scores in an attention-group-wise manner. By employing the GQA strategy in the compression of the KV cache, FastKV enhances its ability to manage KV cache efficiently in these early layers.
% It is important to note that KV data compression occurs solely during the saving of KV data to establish KV caches for input sequences in the prefill stage. Consequently, FastKV ensures that the entire set of KV data, including those of the removed tokens, is propagated to the subsequent layer until the TSP layer. This approach guarantees full context propagation to preserve essential contextual information from the input sequence.
This dual strategy enables FastKV to achieve significant speed improvements without compromising accuracy, as it properly preserves the contextual information of input sequences. 
% The rationale behind FastKV is based on our observation that the properties of attention map vary significantly between the early and later layers of LLMs. In the following section, we will analyze how these properties differ across the layers. Then, we will provide a detailed explanation of the TSP approach and GQA compatible KV cache compression techniques employed by FastKV.  






% The overview of FastKV is illustrated in Figure~\ref{fig:FastKV}. FastKV utilizes two hyperparameters: the TSP index and the TSP length. The TSP index determines the layer at which TSP is applied, while the TSP length specifies the sequence length compressed when propagated to the next layer.

% As an example, the LLaMA-3.1-8B-Instruct model can be configured with a KV budget of 2048, a TSP index of 15, and a TSP length of 2048. Given an input sequence of 128k tokens, 2k tokens corresponding to the important tokens are stored in the KV cache during inference. At 15th layer, instead of propagating a hidden state of 128k$\times$4k dimension to 16th layer, the hidden state is compressed to 2k$\times$4k dimension before being propagated. This compression reduces the size of the hidden state for computations in subsequent layers by a factor of 64, thereby substantially reducing the latency during the compute-bounded prefill stage.

% The workflow of FastKV avoids recomputation and directly propagates compressed tokens from the TSP layer to the subsequent layer. Furthermore, the propagated tokens implicitly preserve full-context information computed in the early layers, thereby achieving high accuracy.

\subsection{Proposed TSP}

TSP aims to enhance the efficiency of the prefill stage by propagating only a selected set of important tokens, similar to the approach used by GemFilter. However, in response to the insights obtained from the previous sections, TSP is designed to retain the necessary scope of token propagation to ensure that critical contextual information is preserved. 
The operation of TSP is governed by three key factors: the layer for token selection (TSP layer), identification of important tokens, and the number of tokens selected for propagation (TSP length). 
TSP begins by identifying TSP length important tokens from the full tokens at the TSP layer. 
The method for identifying important tokens in TSP builds upon Equation~\ref{eq:snpaKV-1}. 
% In TSP, 
The propagation of selected tokens across LLM layers involves transmitting the hidden states associated with those selected tokens. Consequently, TSP necessitates the assessment of token importance on a layer-by-layer basis. 
To facilitate this, TSP includes an additional step that aggregates the attention scores from all attention heads within the TSP layer as follows:
\begin{equation}
    \label{eq:TSP}
    S_{i}^{TSPlayer}=\frac{1}{H}\sum_{h=0}^{H-1}S_i^{TSPlayer,h}
\end{equation}
Here, $H$ denotes the number of attention head in each layer.
From the TSP layer onward, TSP propagates only selected important tokens through the later layers. Unlike GemFilter, which reverts to the first layer of the LLM with selected tokens and recomputes the prefill stage, TSP directly propagates the selected tokens to subsequent layers without returning to the first layer.
% This approach preserves the contextual information extracted in the early layers while focusing on the important tokens in the later layers.
% By incorporating all these design principles, TSP can accelerate the long-context handling while maintaining accuracy.
% TSP length, which defines the number of tokens selected for propagation, operates independently of the KV cache budget. For instance, if the KV cache budget is set to 512 while the TSP length is 2048, TSP propagates 2048 tokens during its operation, but only the final 512 tokens are stored in the KV cache. This separation allows TSP to process a significantly larger set of tokens during propagation, enabling it to capture more contextual information while maintaining the KV cache within its predefined budget. Based on experimental results, a TSP length of 2048 has been found sufficient to preserve accuracy while maintaining efficiency (Appendix~\ref{appendix:TSPlength}).
Meanwhile, although previous approaches to KV cache compression aimed at high accuracy by propagating the entire context throughout the prefill stage, the proposed TSP method discards the unimportant tokens after the TSP layer. Therefore, the implementation of TSP must be carefully designed based on a detailed analysis of the properties of token importance and its impact on the LLM output.

\subsection{Impact of Important Tokens}
\label{sec:token_impact}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{images/Fig4.PNG}
    % \vspace{-12pt}
     \vspace{-12pt}
    \caption{
    Rate of sum of attention scores for the top 2k important tokens selected to the total attention score at each layer.
    % Minimum match rate for various LLMs. The y-axis represents the minimum overlap percentage between the indices of 2048 important tokens selected by each layer and those selected by subsequent layers.
    }
    \vspace{-12pt}
    \label{fig:sum_rate}
\end{figure}
In the LLM, the attention mechanism is the critical component for extracting contextual information, while other parts of the model process input in a more token-isolated manner. Within the attention mechanism, each token generates a new embedding by integrating the embeddings of all preceding tokens, with this integration weighted by the attention scores. Consequently, tokens that receive low attention scores have only a marginal impact on the processing of contextual information.
To ensure the removal of the unimportant tokens without significant impact on the LLM output, the important tokens must receive significantly higher attention scores.

We evaluate the actual impact of these important tokens by examining their attention scores relative to those of the remaining tokens, as shown in Figure~\ref{fig:sum_rate}.
In this analysis, we calculate the sum of the attention scores assigned to 2k important tokens selected in each layer and compare this sum to the attention scores allocated to all 128k tokens.
% If the important tokens receive significantly higher scores, it indicates that they are central to the LLM processing and the remaining tokens could be removed with a marginal impact on the LLM output.
Despite important tokens comprising only 1.56\% of the total number of tokens, they receive a majority of the attention scores. This is particularly notable in the later layers, where the proportion of attention scores attributed to these tokens consistently exceeds 50\% and often approaches 80\%.
This result indicates that they are central to the LLM processing and the remaining tokens could be removed with a marginal impact on the LLM output.
% In the LLM, the attention mechanism is the critical component for extracting contextual information, while other parts of the model process input in a more token-isolated manner. Within the attention mechanism, each token generates a new embedding by integrating the embeddings of all preceding tokens, with this integration weighted by the attention scores. Consequently, tokens that receive low attention scores have only a marginal impact on the processing of contextual information.
% Therefore, although important tokens represent a small fraction of the entire input, focusing on these tokens is sufficient for effective contextual processing.


% \subsection{Importance of Tokens across LLM Layers}
\subsection{Dynamics of Token Importance}
\label{sec:token_dynamic}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Fig3.PNG}
    % \vspace{-12pt}
     \vspace{-10pt}
    \caption{
    Minimum match rate between indices of 2k important tokens selected at each layer and its subsequent layers.
    % Minimum match rate for various LLMs. The y-axis represents the minimum overlap percentage between the indices of 2048 important tokens selected by each layer and those selected by subsequent layers.
    }
    % \vspace{-8pt}
    \vspace{-12pt}
    \label{fig:match_rate}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/Fig5.PNG}
    % \vspace{-12pt}
     \vspace{-22pt}
    \caption{
     Visualization of output logits by t-SNE for (left) LLaMA-3.1-8B-Instruct and (right) Mistral-Nemo-12B-Instruct.
    %(a) Visualization of output logits by t-SNE and (b) LongBench results with propagation of full-context and selected-tokens. Selection layers are classified into early (0-15) layers and later (16-31) layers out of 32 layers of LLaMA-3.1-8B-insturct.
        % Visualization of the output logits for LLaMA-3.1-8B-Instruct, projected into 2D space with t-SNE analysis. The input sequence length is 128k. The full-context denotes the output logits of LLMs with full-context propagation, while the others correspond to cases where early or later layer is selected as the TSP layer with a compressed length of 2k. %\textbf{(b)} MSE between the TSP logits, measured for each layer and depth, and those obtained with full-context inference.
    }
    % \vspace{-12pt}
    \vspace{-12pt}
    \label{fig:t-SNE}
\end{figure}
% - Describe the attention map analysis 

%  key point 1) the set of important tokens are consistent in the early layer, while it is consistent in the later layers
%  (rationale behind TSP)
% \textcolor{red}{Figure 4 will be changed to highlight the importance of each Key/Value data in token-wise manner}

% key point 2) affect of token information removal in the final result (Figure 5)

% - Explain why the property of important tokens varies between early layers and later layers 
% (we can explain it with contextual information processing throughout the LLM forward propagation - token information mixing) 
% - \textcolor{red}{maybe we need to add new figure to support this explanation}
Given that tokens discarded after the TSP layer are not propagated in subsequent layers, there must be consistency in identifying important tokens across layers to facilitate TSP.
This section examines the dynamics of token importance across layers of LLMs. % and its implications for model accuracy. 
Specifically, we investigate whether tokens deemed important based on their high attention scores remain consistent across LLM layers. A significant overlap in the set of important tokens would suggest the potential for pre-selecting these tokens before propagating them to subsequent layers.
%Initially, we explore whether tokens deemed important based on their high attention scores remain consistent across LLM layers. A significant overlap in the set of important tokens suggests the possibility of pre-selecting these tokens before they are propagated to subsequent layers. Then, we assess the impact of discarding tokens considered unimportant on the final output logits of the LLM.
% Next, we evaluate the actual impact of these important tokens by examining their attention scores relative to those of the remaining tokens. If the important tokens receive significantly higher scores, it indicates that they are central to the LLM processing and the remaining tokens could be removed with a marginal impact on the LLM output.
To evaluate the consistency of important tokens across layers, we analyze the important tokens in each layer of the LLM and calculate the minimum match rate of the indices of these tokens with those in subsequent layers. The minimum match rate serves as an indicator of an information bottleneck. 

Figure~\ref{fig:match_rate} presents our findings from this analysis on various LLMs, where each layer extracts 2k important tokens from 128k tokens. The minimum match rate in the first layer is below 10\%, indicating low correlation between important tokens in the first and later layers. However, as we progress to subsequent layers, this match rate increases, surpassing 25\% by the middle layer. This indicates that among the 2k tokens, at least 512 consistently appear as important across layers. Therefore, if the KV cache budget per layer is set to 512, it suffices to examine the 2k important tokens selected from the middle layer.
The observed consistency in important token selection in later layers likely stems from the increasing robustness of contextual information extraction. Each layer in an LLM uses attention scores to integrate token information, progressively enhancing contextual understanding. As the model advances through its layers, contextual analysis becomes more refined, enabling a more precise identification of relevant tokens.
Thus, later layers of LLMs exhibit a consistent recognition of important tokens, underscoring their refined understanding of token importance in the context of the overall sequence.



% \begin{figure}[t]
%     \centering
%     % \includegraphics[width=1.0\linewidth]{images/temp3.PNG}
%     \includegraphics[width=1.0\linewidth]{images/Fig4.PNG}
%     % \vspace{-12pt}
%      \vspace{-10pt}
%     \caption{
%     Heatmap of attention scores in LLaMA-3.1-8B-Instruct \textcolor{red}{wrong data. it will be revised.}
%         % Attention pattern for LLaMA-3.1-8B-Instruct in retrieval task. early layer uniformly attends to the entire input sequence. In contrast, the later layer assigns high attention scores to a narrow range.
%     }
%     \vspace{-8pt}
%     \label{fig:attention}
% \end{figure}

% The observed consistency in important token selection in later layers likely stems from the increasing robustness of contextual information extraction. Each layer in an LLM uses attention scores to integrate token information, progressively enhancing contextual understanding. As the model advances through its layers, contextual analysis becomes more refined, enabling a more precise identification of relevant tokens.
% %The consistency observed in the selection of important tokens in later layers of LLMs is likely a result of the depth at which the process of contextual information extraction becomes robust enough to reliably identify the most relevant tokens.
% %In LLMs, each layer utilizes attention scores to integrate token information, thereby enhancing contextual understanding. 
% %% After the initial layers, particularly beyond the second layer, the range of token visibility remains stable since each token is integrated with all the information from its preceding tokens in the initial layer. \textcolor{red}{(may be we need to add Figure to describe this LLM operation)} 
% %As the LLM progresses through its layers, the process of contextual analysis becomes increasingly refined, and it facilitates more precise identification of key information. 
% As shown in Figure~\ref{fig:attention}, attention maps from early layers tend to assign high attention scores to a broad range of tokens, reflecting the model's initial efforts to identify crucial tokens.
% %It indicates that the model is still in the process of identifying the crucial tokens. 
% In contrast, attention maps from later layers show high attention scores on a narrower set of tokens, with most attention scores approaching zero.
% %the later layers exhibit attention maps where high attention scores are assigned to a narrow range, with most of the attention scores approaching zero. 
% This pattern indicates that the model has effectively identified the important tokens and is now focusing its attention on these select tokens.
% % As shown in \textcolor{red}{Figure 5}, attention maps in early layers appear dense and it reflects a broad focus as the model has yet to determine the crucial tokens. 
% % In contrast, later layers exhibit attention maps that are focused on narrower ranges of tokens and it indicates a more targeted and refined examination of contextual data.
% Thus, later layers of LLMs exhibit a consistent recognition of important tokens, underscoring their refined understanding of token importance in the context of the overall sequence.
% %Therefore, with a deeper understanding on the token importance, the later layers demonstrate a consistent recognition of important tokens.


% We can expect that the removal of unimportant tokens identified from the middle layer does not noticeably affect the result of the LLM for three reasons. First, there is consistency in the presence of unimportant tokens in the middle layer and its subsequent layers. Second, the unimportant tokens have minimal influence because of their low attention scores. Finally, as each layer integrates each token with all the information from its preceding tokens, after the initial layers, the range of token visibility remains stable. \textcolor{red}{(may be we need to add Figure to describe this LLM operation)} This means that the important contextual information can be successfully maintained in this scenario. 
% To examine the impact of removing tokens considered unimportant, we analyze how the output logits of an LLM change after we select 2k tokens from each layer and then eliminate the unselected tokens before they are propagated to subsequent layers. 
% For this analysis, t-SNE (t-distributed Stochastic Neighbor Embedding) is utilized to project the final output logits of LLMs into a 2D space.
% \textcolor{red}{Figure 6(a)} demonstrates the distances between output logits obtained with full-context propagation and those obtained through propagation focusing only on important tokens. 
% This visual representation clearly shows that output logits derived from removing tokens in the early layers significantly deviate from those obtained with full-context propagation.
% However, as anticipated, the removal of tokens from middle and later layers shows negligible effects on the output logits, preserving a similarity to the results from full-context models. 
% % In detail, selecting an early layer as the TSP layer demonstrates that, regardless of the depth at which the needle is located, the distance between the logits from full-context inference and those from TSP generally increases. Furthermore, the distance not only increases but also exhibits directional variation depending on the layer selected as the TSP layer. This suggests that the inability to reflect full-context information in early layers results in a distributional deviation of the final logits from full-context. Conversely, when a middle and deep layers are selected as the TSP layer, the final logits exhibit a high distributional similarity to the original logits. 
% % Figure~\ref{fig:t-SNE}(b) presents the mean squared error (MSE) between the logits with TSP and the original logits, supporting the observations from the t-SNE analysis. Based on this analysis, our hypothesis suggests that effectively reducing computation in the prefill stage can be achieved by preserving global contextual information implicitly in the early layers while focusing computations on important tokens in the middle and deep layers.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{images/Fig6.PNG}
%     % \vspace{-12pt}
%      \vspace{-8pt}
%     \caption{
%     (a) Visualization of output logits by t-SNE and (b) LongBench results with propagation of full-context and selected-tokens. Selection layers are classified into early (0-15) layers and later (16-31) layers out of 32 layers of LLaMA-3.1-8B-insturct.
%         % Visualization of the output logits for LLaMA-3.1-8B-Instruct, projected into 2D space with t-SNE analysis. The input sequence length is 128k. The full-context denotes the output logits of LLMs with full-context propagation, while the others correspond to cases where early or later layer is selected as the TSP layer with a compressed length of 2k. %\textbf{(b)} MSE between the TSP logits, measured for each layer and depth, and those obtained with full-context inference.
%     }
%     \vspace{-12pt}
%     \label{fig:t-SNE}
% \end{figure}

% Our analysis provide two insights into why GemFilter struggles to maintain accuracy. 
% First, GemFilter selects important tokens from the filter layer, reconstructs the input sequence using only these tokens, and then recomputes the prefill stage. However, this approach does not properly account for the variability in token importance across layers, as the important tokens identified in the middle layer differ significantly from those deemed important in the initial layers.
% Second, GemFilter restricts the number of important tokens selected for propagation to match the final KV cache budget. For instance, if the KV cache budget is set to 512, GemFilter selects 512 important tokens for propagation. However, our analysis indicates that while token importance becomes more consistent in later layers, the match rate for these tokens hardly reaches 100\%. To ensure comprehensive coverage and maintain accuracy, it may be necessary to propagate a slightly larger set of tokens than the target 512, accommodating all potentially important tokens across layers.
% % The analysis results provide insight into why GemFilter, which propagates only important tokens during the recomputation of the prefill stage, struggles to maintain accuracy. % with a limited KV cache budget. 
% % %GemFilter fails primarily for two reasons.
% % First, GemFilter selects important tokens from the middle layer and reconstructs the input sequence using only these tokens, then recomputes the prefill stage. This method does not properly account for the variability in token importance across layers, as the important tokens identified in the middle layer differ significantly from those perceived as important in the initial layers.
% % Second, it overlooks the fact that tokens in different layers encapsulate different pieces of information, as each token is processed to integrate information from preceding tokens in each LLM layer. Consequently, reconstructing only the important tokens at the input sequence level hampers the mixing of token information, which is essential for capturing the contextual information.
% % Third, GemFilter aligns the number of important tokens selected for propagation with the final budget of the KV cache. For instance, if the target budget of the KV cache is set at 512, GemFilter selects 512 important tokens for propagation. However, our analysis indicates that while the information consistency of important tokens improves in later layers, the match rate for these tokens hardly reaches 100\%. To ensure comprehensive coverage and accuracy, it may be necessary to propagate a slightly larger set of tokens than the 512 target, to accommodate all potentially important tokens across the layers.




% \subsection{Impact of Selected Token Propagation on LLM Outputs}
% \subsection{Layer Selection for TSP}
%- key concept: later layer - selective propagation / kv budget != # of tokens selected for the propagation
%- briefly describe how to select important tokens (based on previous works)
%- briefly mention how to select TSP layer (key point - as long as later layers are used as TSP layers, it does not affect the accuracy, but to sufficiently guarantee speed we choose xx as TSP layer

\subsection{Overall Impact of TSP on LLM Output}
% \subsection{Selection of TSP Layer}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{images/Fig6.PNG}
    % \vspace{-12pt}
     \vspace{-8pt}
    \caption{
    LongBench results of LLaMA-3.1-8B-insturct with propagation of tokens selected at each layer.
     % LongBench results with propagation of full-context and selected-tokens. Selection layers are classified into early (0-15) layers and later (16-31) layers out of 32 layers of LLaMA-3.1-8B-insturct.
    }
    % \vspace{-12pt}
    \vspace{-12pt}
    \label{fig:layer_scan}
\end{figure}

% For the propagation of selected tokens, a critical decision involves determining the layer from which to select important tokens and the starting point for their propagation.
% First, to evaluate the impact of the layer chosen for selecting important tokens, we analyze changes in the output logits of an LLM after selecting 2k tokens from each layer and discarding the unselected tokens before propagating them to subsequent layers.
Building on the insights from Section 3.3 and 3.4, we can expect that the removal of unimportant tokens identified from the middle layer will not significantly impact the performance of the LLM for two main reasons.
First, only a small set of important tokens plays an important role in the attention mechanism and there is the consistency in the identification of important tokens in the middle layer and subsequent layers.
% First, there is notable consistency in the occurrence of these unimportant tokens between the middle layer and its subsequent layers. Second, these tokens exert minimal influence on the model's outcomes due to their low attention scores. 
We analyze real changes in the output logits of an LLM after selecting 2k tokens from each layer and discarding the unselected tokens before propagating them to subsequent layers.
For this analysis, we use t-SNE (t-distributed Stochastic Neighbor Embedding) to project the final output logits of LLMs into a 2D space.
Figure~\ref{fig:t-SNE} demonstrates the distances between output logits obtained with full-context propagation and those obtained through propagation focusing exclusively on important tokens. 
This visual representation shows that removing tokens in the early layers causes significant deviations in output logits compared to full-context propagation.
% However, removing tokens from later layers has negligible effects on the output logits, preserving a high degree of similarity to the results from full-context models.
However, as expected, removing tokens from later layers preserves a high degree of similarity to the results from full-context models.
% As there is consistency in the identification of unimportant tokens in the middle layer and later layers, removal of unimportant tokens from later layers is unlikely to significantly affect the output of the LLM.
% Thus, we designate the middle layer of each LLM as the TSP layer.
% The analysis in the previous section reveals that there is consistency in the identification of unimportant tokens in the middle layer and later layers, so removal of unimportant tokens from later layers is unlikely to significantly affect the output of the LLM.
This contributes to stable accuracy results on the LongBench benchmark, as illustrated in Figure~\ref{fig:layer_scan}. 
In light of this observation, we designate the middle layer of each LLM as the TSP layer for the remainder of this paper. 

Moreover, in Figure~\ref{fig:layer_scan}, we compared the accuracy obtained with TSP and a GemFilter-like propagation. 
TSP consistently outperforms GemFilter in terms of accuracy. This superior performance can primarily be attributed to how TSP manages the variability in token importance across layers. Unlike GemFilter, which reverts to the first layer of the LLM with selected tokens and recomputes the prefill stage, TSP directly propagates the selected tokens to subsequent layers without reverting to the initial layer. This direct propagation allows TSP to better accommodate the variability in token importance. 
Furthermore, the large accuracy gap between TSP and GemFilter when tokens are selected in the initial layers highlights the importance of leveraging token information that has been processed with the full context before making important token selections. 

% Second, to evaluate the impact of where propagation of selected tokens begins, we compare the accuracy of two approaches: propagating selected tokens from the input prompt, as done by GemFilter, and from the layer where important tokens are identified. Figure~\ref{fig:layer_scan} displays the LongBench results with these two approaches. Notably, while the GemFilter paper excludes the code repository understanding task from LongBench due to its complexity, our analysis includes this task for a more comprehensive evaluation. Regardless of the layer chosen for selecting important tokens, Token-Selective Propagation (TSP) approach demonstrates much better accuracy than the GemFilter approach. This superior performance is because each token in an LLM is processed to integrate information from preceding tokens through the attention mechanism. Consequently, reconstructing only the important tokens at the input sequence level disrupts the essential mixing of token information, crucial for capturing comprehensive contextual information.

% Our analysis provide two insights into why GemFilter struggles to maintain accuracy. 
% First, GemFilter selects important tokens from the filter layer, reconstructs the input sequence using only these tokens, and then recomputes the prefill stage. However, this approach does not properly account for the variability in token importance across layers, as the important tokens identified in the middle layer differ significantly from those deemed important in the initial layers.
% Second, it overlooks the fact that tokens in different layers encapsulate different pieces of information, as each token is processed to integrate information from preceding tokens in each LLM layer. Consequently, reconstructing only the important tokens at the input sequence level hampers the mixing of token information, which is essential for capturing the contextual information.


\subsection{TSP Length and KV Budget}



The match rate of the important tokens is around 25\% (Figure~\ref{fig:match_rate}). 
This rate is sufficiently high to justify the use of TSP approach with an adequate TSP length (e.g. 2k). However, a short TSP length (e.g. 512) may lead to significant information loss. 
Therefore, TSP length, which defines the number of tokens selected for propagation, is strategically designed to operate independently of the KV cache budget. For instance, if the KV cache budget is set to 512 while the TSP length is 2048, TSP propagates 2048 tokens during its operation, but only the final 512 tokens are stored in the KV cache. This separation allows TSP to process a significantly larger set of tokens during propagation, enabling it to capture more contextual information while maintaining the KV cache within its predefined budget.
As shown in Figure~\ref{fig:plength_main}, a TSP length of 2048 has been found sufficient to preserve accuracy of the model under specific KV budget while maintaining efficiency in terms of TTFT.



% We can expect that removing unimportant tokens identified from the middle layer is unlikely to significantly affect the output of the LLM for three reasons. First, there is consistency in the identification of unimportant tokens in the middle layer and subsequent layers. Second, the unimportant tokens have minimal influence due to their low attention scores. 
% Finally, as each layer integrates tokens with information from all preceding tokens, the range of token visibility remains stable after the initial layer. This means that the important contextual information can be successfully maintained in this scenario. 
% To evaluate the impact of removing tokens considered unimportant, we analyze how the output logits of an LLM change after selecting 2k tokens from each layer and discarding the unselected tokens before propagating them to subsequent layers. 
% For this analysis, we use t-SNE (t-distributed Stochastic Neighbor Embedding) to project the final output logits of LLMs into a 2D space.
% \textcolor{red}{Figure~\ref{fig:t-SNE}(a)} demonstrates the distances between output logits obtained with full-context propagation and those obtained through propagation focusing exclusively on important tokens. 
% This visual representation shows that removing tokens in the early layers causes significant deviations in output logits compared to full-context propagation.
% %output logits derived from removing tokens in the early layers significantly deviate from those obtained with full-context propagation.
% However, as expected, removing tokens from middle and later layers has negligible effects on the output logits, preserving a high degree of similarity to the results from full-context models. 
% % In detail, selecting an early layer as the TSP layer demonstrates that, regardless of the depth at which the needle is located, the distance between the logits from full-context inference and those from TSP generally increases. Furthermore, the distance not only increases but also exhibits directional variation depending on the layer selected as the TSP layer. This suggests that the inability to reflect full-context information in early layers results in a distributional deviation of the final logits from full-context. Conversely, when a middle and deep layers are selected as the TSP layer, the final logits exhibit a high distributional similarity to the original logits. 
% % Figure~\ref{fig:t-SNE}(b) presents the mean squared error (MSE) between the logits with TSP and the original logits, supporting the observations from the t-SNE analysis. Based on this analysis, our hypothesis suggests that effectively reducing computation in the prefill stage can be achieved by preserving global contextual information implicitly in the early layers while focusing computations on important tokens in the middle and deep layers.




% \subsection{Proposed TSP}

% TSP aims to enhance the efficiency of the prefill stage by propagating only a select subset of important tokens, similar to the approach used by GemFilter. However, in response to the insights obtained from the previous sections, TSP is designed to retain the necessary scope of token propagation to ensure that critical contextual information is preserved. 
% The operation of TSP is governed by three key factors: the layer for token selection (TSP layer), identification of important tokens, and the number of tokens selected for propagation (TSP length). 

% TSP begins with full-token propagation in the early layers and identifies important tokens at the TSP layer. 
% As the accuracy of TSP remains stable as long as token selection is applied at or after the middle layer, the middle layer of each LLM is adopted as the TSP layer.
% % One key advantage of TSP is that the accuracy of TSP remains stable as long as token selection is applied at or after the middle layer. This stability removes the need for fine-tuning the TSP layer index for each model, allowing the middle layer of the LLM to be adopted as the TSP layer.
% % In contrast, GemFilter is highly sensitive to the choice of the retrieval layer, requiring careful and model-specific tuning. This robustness of TSP compared to the sensitivity of GemFilter will be explored further in the experimental section.

% The method for identifying important tokens in TSP builds upon the approach introduced in SnapKV (Equation~\ref{eq:snpaKV-1}). 
% % SnapKV identifies important tokens by summing the attention scores assigned to each token within a predefined window, head by head, in each layer. 
% In TSP, the propagation of selected tokens across LLM layers involves transmitting the hidden states associated with those selected tokens. Consequently, TSP necessitates the assessment of token importance on a layer-by-layer basis. 
% To facilitate this, TSP includes an additional step that aggregates the attention scores from all attention heads within the TSP layer as follows:
% % For TSP, an additional step aggregates the attention scores across all attention heads within the TSP layer to perform sequence-level token selection.
% \begin{equation}
%     \label{eq:TSP}
%     S_{i}^{TSPlayer}=\frac{1}{H}\sum_{h=0}^{H}S_i^{TSPlayer,h}
%     % \mathbf{S_{G}}=\frac{1}{N}\sum_{h=j}^{j+N}(\text{pooling}(\sum_{i=0}^{L_{\text {W}}} \mathbf{A^h}[:,\,i,\,:-L_{W}]))
% \end{equation}
% Here, $H$ denotes the number of attention head in each layer.
% % Tokens with the highest aggregated attention scores $S_{i}^{TSPlayer}$ are then selected as important tokens. 

% TSP length, which defines the number of tokens selected for propagation, operates independently of the KV cache budget. For instance, if the KV cache budget is set to 512 while the TSP length is 2048, TSP propagates 2048 tokens during its operation, but only the final 512 tokens are stored in the KV cache. This separation allows TSP to process a significantly larger set of tokens during propagation, enabling it to capture more contextual information while maintaining the KV cache within its predefined budget. Based on experimental results, a TSP length of 2048 has been found sufficient to preserve accuracy while maintaining efficiency (Appendix~\ref{appendix:TSPlength}).

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{images/Fig7.pdf}
%     % \vspace{-12pt}
%      \vspace{-6mm}
%     \caption{
%         Average LongBench accuracy and TTFT at context length 128k and KV with varying TSP lengths.
%     }
%     \vspace{-2mm}
%     \label{fig:plength}
% \end{figure}


% From the TSP layer onward, TSP propagates only these TSP-length important tokens through the later layers. Unlike GemFilter, which reverts to the first layer of the LLM with selected tokens and recomputes the prefill stage, TSP directly propagates the selected tokens to subsequent layers without returning to the first layer. This approach preserves the contextual information extracted in the early layers while focusing on the important tokens in the later layers.
% By incorporating all these design principles, TSP can accelerate the long-context handling while maintaining accuracy.


% TSP aims to enhance the efficiency of the prefill stage by propagating only a select subset of important tokens, similar to the approach used by GemFilter. However, in response to the insights obtained from the analysis, it is designed to retain the necessary scope of token propagation to ensure that critical contextual information is preserved. 
% To achieve this, TSP propagates all tokens in the early layers (full-token propagation) and transitions to selected token propagation in the later layers. Unlike GemFilter, which propagates all tokens in the early layers and then restarts from the beginning with selected tokens, TSP avoids redundant computations for the early layers. This method can retain the contextual information extracted in the early layers, and it can also avoid repeated computations. As a result, our approach can process more information within the same KV cache budget compared to GemFilter.
% Then, TSP introduces a new parameter called TSP length, which is independent of the KV cache budget and defines the number of tokens selected for propagation. For instance, if the KV cache budget is set to 512 and the TSP length is defined as 2048, TSP propagates 2048 tokens during its operations. However, the KV cache will only store 512 important tokens. This separation allows TSP to handle a larger set of tokens during propagation while ensuring that the final KV cache remains within the predefined budget.

% \textcolor{red}{If the method for selecting important is based on the previous works (e.g. SnapKV), just mention that we adopt their approach}
% The score for selecting important tokens can be defined as follows:

% \vspace{-6px}
% \begin{equation}
%     \label{eq:TSP-1}
%     A^h=\operatorname{softmax}\left(Q^h \cdot\left(K^h\right)^{\top}/\sqrt{d_k}\right)
% \end{equation}
% \vspace{-6px}
% \begin{equation}
%     \label{eq:TSP-2}
%     \mathbf{S}=\frac{1}{H}\sum_{h=0}^{H}(\text{pooling}(\sum_{i=0}^{L_{\text{W}}} A^h[:,\,i,\,:-L_{W}]))
% \end{equation}
% \vspace{-2px}

% Here, $A^h$ is the attention head for head $h$, while $H$ is the number of head. $L_{\text{W}}$ denotes the window length and $\mathbf{S}$ represents the score. Initially, we adopt the attention-based selection strategy from previous work~\cite{snapkv}. Using the attention heads corresponding to each query within a local window, we compute the attention weights. Since compressing the final layer output requires a single index set, we average the attention weights to calculate a score $\mathbf{S}$ that represents the importance of tokens at the layer level, as outlined in Equation~\ref{eq:TSP-2}. Based on this score, we select the index to propagate to the next layer and indexing approach can be defined as follows:

% \vspace{-2px}
% \begin{equation}
%     \label{eq:TSP-3}
%     I_{S}=\operatorname{Top}_k(\mathbf{Score}, k)
% \end{equation}
% \vspace{-6px}
% \begin{equation}
%     \label{eq:TSP-4}
%     I_{T}=\text{concat}(I_{S}, \,I_{W})
% \end{equation}
% \vspace{-8px}

% where $I_{S}$, $I_{W}$ and $I_{T}$ are the indices for selected tokens, window tokens, and TSP tokens, respectively. As shown in Equation~\ref{eq:TSP-3}, we select the top-k indices to determine the selected tokens. Notably, the window tokens are always propagated to the next layer; therefore, $k$ refers to the number of selected tokens excluding the window tokens. Finally, by concatenating the indices of the selected tokens and window tokens, we determine the final indices of the TSP tokens to propagate to the next layer. These TSP indices are used to index the output of the layer after the MLP block, effectively compressing the hidden states as input to the next layer.

% \textcolor{red}{Highlight TSP layer selection is important for GemFilter, but TSP (our FastKV) is robust to it.}
% Following the analysis of TSP, we describe the method for selecting an appropriate TSP layer to balance the trade-off between acceleration and performance in LLM inference. GemFilter selects a layer based on the ability to attend to the needle token. However, since our method does not perform recomputation using important tokens from the selected layer, we evaluate the layer based on how effectively its computations propagate full-context information. Thus, the TSP layer is selected by measuring the similarity between the final output logits with TSP and the original full-context logits. To ensure generalization, we measure the MSE between logits at various depths. Additionally, to maintain the objective of accelerating inference, we define the maximum TSP layer to prevent the selection of excessively deep layers and set it to half the total number of layers in the LLMs.


% \subsection{Token-Selective Propagation}
% To design our approach, we first analyze the characteristics of attention heads across layers to understand their behavior. Figure~\ref{fig:attention} illustrates the aggregated attention heads for each layer. We observe that early layers (e.g., 5th and 10th layers), particularly those closer to the first layer, tend to densely attend to the entire input sequence. This dense pattern highlights the importance of early layers in capturing contextual information globally from the input sequence. In contrast, deep layers (e.g., 20th and 25th layers) exhibit relatively sparse attention patterns compared to early layers. This sparsity suggests that deep layers focus on retrieval information, such as needles, based on the global information aggregated by the early layers. This observation highlights a key transition from global context aggregation in the early layers to focused attention on important information in the deep layers, which we emphasize as a critical factor for understanding the role of different layers during inference.

% % Fundamentally, we identify this transition from global context aggregation in the early layers to focused attention on important information in the deep layers during inference.

% Based on these findings, a potential limitation of GemFilter can be observed. By employing recomputation, GemFilter focuses computations on selected tokens in the early layers, thereby failing to aggregate global contextual information across the entire input sequence. This limitation, while enabling acceleration of the prefill stage, leads to significant performance degradation. To address this issue, we propose Token-Selective Propagation (TSP) method. Unlike GemFilter, TSP performs full-context computations up to a selected layer, ensuring that global contextual information is fully aggregated. At the selected layer, the TSP identifies important tokens and incorporates their indexing information into the layer output, enabling inference with compressed tokens in deep layers. The score for selecting important tokens can be defined as follows:

% \vspace{-6px}
% \begin{equation}
%     \label{eq:TSP-1}
%     A^h=\operatorname{softmax}\left(Q^h \cdot\left(K^h\right)^{\top}/\sqrt{d_k}\right)
% \end{equation}
% \vspace{-6px}
% \begin{equation}
%     \label{eq:TSP-2}
%     \mathbf{S}=\frac{1}{H}\sum_{h=0}^{H}(\text{pooling}(\sum_{i=0}^{L_{\text{W}}} A^h[:,\,i,\,:-L_{W}]))
% \end{equation}
% \vspace{-2px}

% Here, $A^h$ is the attention head for head $h$, while $H$ is the number of head. $L_{\text{W}}$ denotes the window length and $\mathbf{S}$ represents the score. Initially, we adopt the attention-based selection strategy from previous work~\cite{snapkv}. Using the attention heads corresponding to each query within a local window, we compute the attention weights. Since compressing the final layer output requires a single index set, we average the attention weights to calculate a score $\mathbf{S}$ that represents the importance of tokens at the layer level, as outlined in Equation~\ref{eq:TSP-2}. Based on this score, we select the index to propagate to the next layer and indexing approach can be defined as follows:

% \vspace{-2px}
% \begin{equation}
%     \label{eq:TSP-3}
%     I_{S}=\operatorname{Top}_k(\mathbf{Score}, k)
% \end{equation}
% \vspace{-6px}
% \begin{equation}
%     \label{eq:TSP-4}
%     I_{T}=\text{concat}(I_{S}, \,I_{W})
% \end{equation}
% \vspace{-8px}

% where $I_{S}$, $I_{W}$ and $I_{T}$ are the indices for selected tokens, window tokens, and TSP tokens, respectively. As shown in Equation~\ref{eq:TSP-3}, we select the top-k indices to determine the selected tokens. Notably, the window tokens are always propagated to the next layer; therefore, $k$ refers to the number of selected tokens excluding the window tokens. Finally, by concatenating the indices of the selected tokens and window tokens, we determine the final indices of the TSP tokens to propagate to the next layer. These TSP indices are used to index the output of the layer after the MLP block, effectively compressing the hidden states as input to the next layer.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{images/Fig5.PNG}
%     % \vspace{-12pt}
%      \vspace{-6pt}
%     \caption{
%         Illustration of FastKV scheme. FastKV introduces Token-Selective Propagation to reduce the number of tokens required for computation while simultaneously storing compressed KV caches.
%     }
%     \vspace{-8pt}
%     \label{fig:FastKV}
% \end{figure*}

% \subsection{TSP Analysis and Layer Selection}
% \label{subsection:tsp_anal_layer_selection}
% To analyze the impact of TSP on the final output logits of LLMs, we perform a t-SNE analysis, as shown in Figure~\ref{fig:t-SNE}(a). The t-SNE projects the final output logits of LLMs into a 2D space, visualizing the distance between logits obtained from full-context inference and those computed when each layer is selected as the TSP layer. Since each logit corresponds to a different sequence length, comparisons are performed on logits within a local window that is commonly contained. Note that selecting a specific layer as the TSP layer implies that computations up to that layer are performed with full-context information, while subsequent layers perform inference on compressed hidden states. 

% In detail, selecting an early layer as the TSP layer demonstrates that, regardless of the depth at which the needle is located, the distance between the logits from full-context inference and those from TSP generally increases. Furthermore, the distance not only increases but also exhibits directional variation depending on the layer selected as the TSP layer. This suggests that the inability to reflect full-context information in early layers results in a distributional deviation of the final logits from full-context. Conversely, when a middle and deep layers are selected as the TSP layer, the final logits exhibit a high distributional similarity to the original logits. 

% Figure~\ref{fig:t-SNE}(b) presents the mean squared error (MSE) between the logits with TSP and the original logits, supporting the observations from the t-SNE analysis. Based on this analysis, our hypothesis suggests that effectively reducing computation in the prefill stage can be achieved by preserving global contextual information implicitly in the early layers while focusing computations on important tokens in the middle and deep layers.

% Following the analysis of TSP, we describe the method for selecting an appropriate TSP layer to balance the trade-off between acceleration and performance in LLM inference. GemFilter selects a layer based on the ability to attend to the needle token. However, since our method does not perform recomputation using important tokens from the selected layer, we evaluate the layer based on how effectively its computations propagate full-context information. Thus, the TSP layer is selected by measuring the similarity between the final output logits with TSP and the original full-context logits. To ensure generalization, we measure the MSE between logits at various depths. Additionally, to maintain the objective of accelerating inference, we define the maximum TSP layer to prevent the selection of excessively deep layers and set it to half the total number of layers in the LLMs.

\begin{table*}[ht]
\setlength{\tabcolsep}{4pt}
\centering
\vspace{-8pt}
\caption{LongBench results comparison between baseline methods and FastKV.}
\label{tab:longbench}
\renewcommand{\arraystretch}{0.8}
\scalebox{0.75}{
\begin{tabular}{@{}!{\color{white}\vrule}lccccccccccccccc!{\color{white}\vrule}@{}}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{3}{c|}{Single-Doc QA} & \multicolumn{3}{c|}{Multi-Doc QA} & \multicolumn{3}{c|}{Summarization} & \multicolumn{3}{c|}{Few-shot Learning} & \multicolumn{2}{c|}{Coding} &  \\ \cmidrule(lr){2-15}
\multicolumn{1}{c|}{\multirow{-2}{*}{\raisebox{0.5cm}[0pt][0pt]{Method}}} & \rotatebox{45}{NrtvQA} &\rotatebox{45}{Qasper} & \multicolumn{1}{c|}{\rotatebox{45}{MF-en}} & \rotatebox{45}{HotpotQA} & \rotatebox{45}{2WikiMQA} & \multicolumn{1}{c|}{\rotatebox{45}{Musique}} & \rotatebox{45}{GovReport} & \rotatebox{45}{QMSum} & \multicolumn{1}{c|}{\rotatebox{45}{MultiNews}} & \rotatebox{45}{TREC} & \rotatebox{45}{TriviaQA} & \multicolumn{1}{c|}{\rotatebox{45}{SAMSum}} & \rotatebox{45}{LCC} & \multicolumn{1}{c|}{\rotatebox{45}{RB-P}} & \multirow{-2}{*}{\raisebox{0.5cm}[0pt][0pt]{\textbf{Avg.}}} \\ \midrule
\multicolumn{16}{c}{LLaMA-3.1-8B-Inustruct, KV Budget = Full} \\ \midrule
\multicolumn{1}{l|}{Full KV} & 30.21 & 45.53 & 55.01 & 56.01 & 46.65 & 31.28 & 35.13 & 25.28 & 27.25 & 73.00 & 91.64 & 43.80 & 63.38 & \multicolumn{1}{c|}{56.64} & \textbf{48.63} \\ \midrule

\multicolumn{16}{c}{LLaMA-3.1-8B-Instruct, KV Budget = 512} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 30.09 & 41.62 & 53.89 & 54.77 & 45.12 & 31.22 & 27.37 & 24.16 & 24.86 & 71.00 & 91.90 & 42.43 & 61.53 & \multicolumn{1}{c|}{52.49} & \textbf{46.60} \\
% \multicolumn{1}{l|}{AdaKV} & 30.16 & 40.11 & 54.13 & 54.27 & 44.73 & 31.19 & 27.08 & 23.82 & 24.39 & 71.00 & 91.90 & 41.64 & 60.54 & \multicolumn{1}{c|}{53.15} & \textbf{46.29} \\
\multicolumn{1}{l|}{AdaKV} & 29.07 & 40.16 & 52.44 & 53.90 & 43.05 & 31.10 & 25.75 & 24.39 & 24.85 & 69.00 & 92.34 & 42.05 & 63.43 & \multicolumn{1}{c|}{55.32} & \textbf{46.20} \\
%\multicolumn{1}{l|}{HeadKV-R} & 31.12 & 42.25 & 53.76 & 55.26 & 44.80 & 31.34 & 28.73 & 24.74 & 25.81 & 71.50 & 91.44 & 42.11 & 62.60 & \multicolumn{1}{c|}{54.61} & 47.15 \\
\multicolumn{1}{l|}{HeadKV} & 30.17 & 44.03 & 54.29 & 54.71 & 46.10 & 31.58 & 31.14 & 24.46 & 26.66 & 73.00 & 91.72 & 42.17 & 62.89 & \multicolumn{1}{c|}{55.71} & \textbf{47.76} \\
% \multicolumn{1}{l|}{GemFilter} & 16.33 & 22.02 & 37.75 & 43.78 & 31.13 & 23.49 & 26.43 & 17.40 & 23.42 & 61.00 & 89.55 & 39.98 & 36.76 & \multicolumn{1}{c|}{34.19} & 35.95 \\
\multicolumn{1}{l|}{GemFilter} & 18.52 & 22.15 & 37.73 & 46.88 & 32.56 & 24.52 & 27.05 & 17.75 & 23.25 & 60.50 & 89.49 & 40.16 & 27.35 & \multicolumn{1}{c|}{31.02} & \textbf{35.64} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 30.38 & 41.12 & 55.71 & 54.35 & 46.69 & 30.89 & 26.78 & 23.82 & 24.37 & 72.50 & 92.04 & 42.64 & 62.60 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}52.74} & \textbf{46.90} \\ \midrule
\multicolumn{16}{c}{LLaMA-3.1-8B-Instruct,   KV Budget = 2048} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 31.31 & 44.96 & 55.15 & 55.48 & 45.29 & 30.78 & 31.92 & 24.63 & 26.97 & 71.50 & 91.48 & 43.38 & 63.14 & \multicolumn{1}{c|}{56.04} & \textbf{48.00} \\
% \multicolumn{1}{l|}{AdaKV} & 31.03 & 45.39 & 55.22 & 55.10 & 45.88 & 30.31 & 30.96 & 24.46 & 26.93 & 72.00 & 91.65 & 43.76 & 63.02 & \multicolumn{1}{c|}{56.10} & \textbf{47.99} \\
\multicolumn{1}{l|}{AdaKV} & 30.64 & 45.23 & 55.46 & 55.55 & 45.26 & 31.14 & 31.28 & 24.89 & 26.72 & 72.50 & 91.64 & 42.75 & 64.88 & \multicolumn{1}{c|}{59.25} & \textbf{48.37} \\
%\multicolumn{1}{l|}{HeadKV-R} & 31.11 & 44.63 & 55.26 & 55.81 & 46.07 & 31.99 & 32.59 & 25.11 & 27.13 & 72.50 & 91.48 & 43.13 & 63.42 & \multicolumn{1}{c|}{56.30} & 48.32 \\
\multicolumn{1}{l|}{HeadKV} & 30.53 & 44.99 & 54.98 & 55.47 & 46.08 & 31.42 & 32.41 & 25.05 & 27.20 & 72.50 & 91.57 & 42.84 & 63.45 & \multicolumn{1}{c|}{56.75} & \textbf{48.23} \\
% \multicolumn{1}{l|}{GemFilter} & 23.94 & 40.65 & 51.42 & 54.87 & 46.90 & 28.80 & 31.59 & 20.60 & 26.87 & 69.00 & 91.34 & 42.96 & 49.21 & \multicolumn{1}{c|}{39.94} & 44.15 \\
\multicolumn{1}{l|}{GemFilter} & 23.64 & 41.17 & 51.39 & 53.97 & 45.32 & 29.24 & 32.01 & 20.50 & 26.91 & 70.00 & 91.59 & 42.59 & 47.35 & \multicolumn{1}{c|}{38.91} & \textbf{43.90} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 30.31 & 45.41 & 54.79 & 55.11 & 46.57 & 30.46 & 31.25 & 24.82 & 27.02 & 73.50 & 91.48 & 43.66 & 63.17 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}55.82} & \textbf{48.10} \\ \midrule
% \multicolumn{16}{c}{LLaMA-3.2-3B-Instruct,   KV Budget = Full} \\ \midrule
% \multicolumn{1}{l|}{Full KV} & 23.41 & 40.60 & 49.79 & 48.71 & 38.89 & 20.54 & 34.07 & 23.65 & 26.67 & 71.50 & 88.89 & 43.20 & 52.13 & \multicolumn{1}{c|}{54.16} & 44.02 \\ \midrule
% \multicolumn{16}{c}{LLaMA-3.2-3B-Instruct, KV Budget = 512} \\ \midrule
% \multicolumn{1}{l|}{SnapKV} & 21.03 & 32.79 & 49.01 & 48.68 & 37.59 & 20.07 & 25.91 & 22.76 & 24.22 & 69.00 & 88.75 & 41.78 & 51.68 & \multicolumn{1}{c|}{51.25} & 41.75 \\
% \multicolumn{1}{l|}{AdaKV} & 19.87 & 32.44 & 48.50 & 47.84 & 37.47 & 20.40 & 25.93 & 22.68 & 24.04 & 69.50 & 89.22 & 42.18 & 50.86 & \multicolumn{1}{c|}{51.86} & 41.63 \\
% %\multicolumn{1}{l|}{HeadKV-R} & 20.90 & 34.62 & 48.55 & 47.76 & 36.51 & 20.52 & 28.47 & 22.43 & 25.42 & 71.00 & 89.27 & 40.54 & 52.08 & \multicolumn{1}{c|}{52.17} & 42.16 \\
% \multicolumn{1}{l|}{HeadKV} & 22.79 & 36.45 & 50.57 & 48.85 & 39.16 & 20.44 & 29.79 & 23.24 & 26.10 & 70.50 & 89.66 & 40.80 & 53.72 & \multicolumn{1}{c|}{53.83} & 43.28 \\
% \multicolumn{1}{l|}{GemFilter} & 20.88 & 31.37 & 44.31 & 47.60 & 34.87 & 23.59 & 25.89 & 19.82 & 24.01 & 55.00 & 85.73 & 35.29 & 26.68 & \multicolumn{1}{c|}{32.72} & 36.27 \\
% \rowcolor[HTML]{DAE8FC} 
% \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 21.73 & 31.55 & 49.88 & 47.31 & 36.54 & 22.14 & 25.47 & 22.38 & 24.05 & 71.50 & 88.99 & 40.91 & 52.46 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}52.17} & 41.93 \\ \midrule
% \multicolumn{16}{c}{LLaMA-3.2-3B-Instruct,   KV Budget = 2048} \\ \midrule
% \multicolumn{1}{l|}{SnapKV} & 22.89 & 38.70 & 50.91 & 48.38 & 39.29 & 20.55 & 30.79 & 23.17 & 26.46 & 70.50 & 88.89 & 42.25 & 52.16 & \multicolumn{1}{c|}{54.03} & 43.50 \\
% \multicolumn{1}{l|}{AdaKV} & 23.38 & 38.61 & 51.11 & 48.65 & 39.22 & 20.30 & 29.65 & 23.76 & 26.46 & 71.00 & 88.89 & 42.23 & 52.37 & \multicolumn{1}{c|}{54.30} & 43.57 \\
% %\multicolumn{1}{l|}{HeadKV-R} & 22.11 & 38.81 & 50.37 & 47.54 & 38.96 & 19.53 & 31.67 & 23.28 & 26.60 & 71.00 & 88.89 & 43.11 & 53.17 & \multicolumn{1}{c|}{53.62} & 43.48 \\
% \multicolumn{1}{l|}{HeadKV} & 22.56 & 39.84 & 50.40 & 49.15 & 38.95 & 20.25 & 32.98 & 23.73 & 26.71 & 71.00 & 89.28 & 42.74 & 53.56 & \multicolumn{1}{c|}{55.30} & 44.03 \\
% \multicolumn{1}{l|}{GemFilter} & 17.48 & 38.31 & 49.93 & 50.57 & 40.29 & 19.02 & 30.95 & 21.50 & 26.28 & 64.50 & 90.07 & 40.59 & 37.97 & \multicolumn{1}{c|}{38.88} & 40.45 \\
% \rowcolor[HTML]{DAE8FC} 
% \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 24.02 & 37.99 & 50.06 & 47.58 & 38.76 & 21.93 & 30.45 & 23.24 & 26.29 & 72.50 & 89.22 & 41.73 & 52.71 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}55.52} & 43.71 \\ \midrule
\multicolumn{16}{c}{Mistral-Nemo-12B-Instruct,   KV Budget = Full} \\ \midrule
\multicolumn{1}{l|}{Full KV} & 26.27 & 43.64 & 58.11 & 49.34 & 45.85 & 26.26 & 31.31 & 24.15 & 26.08 & 75.00 & 89.66 & 44.32 & 68.58 & \multicolumn{1}{c|}{68.11} & \textbf{48.33} \\ \midrule

\multicolumn{16}{c}{Mistral-Nemo-12B-Instruct,   KV Budget = 512} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 23.58 & 40.12 & 55.17 & 47.91 & 45.69 & 25.17 & 23.77 & 22.48 & 24.08 & 74.00 & 89.44 & 43.09 & 68.31 & \multicolumn{1}{c|}{62.15} & \textbf{46.07} \\
% \multicolumn{1}{l|}{AdaKV} & 24.19 & 39.65 & 54.30 & 48.16 & 45.43 & 26.60 & 23.06 & 22.85 & 23.31 & 74.00 & 89.26 & 43.09 & 67.38 & \multicolumn{1}{c|}{62.41} & \textbf{45.98} \\
\multicolumn{1}{l|}{AdaKV} & 24.46 & 41.52 & 57.34 & 47.89 & 45.83 & 25.57 & 23.58 & 22.11 & 23.88 & 74.00 & 89.52 & 43.20 & 68.21 & \multicolumn{1}{c|}{64.15} & \textbf{46.52} \\
% \multicolumn{1}{l|}{HeadKV-R} & 25.83 & 41.93 & 57.87 & 48.32 & 45.57 & 25.53 & 27.03 & 22.68 & 25.04 & 74.50 & 90.09 & 43.62 & 68.62 & \multicolumn{1}{c|}{64.98} & 47.26 \\
\multicolumn{1}{l|}{HeadKV} & 25.88 & 40.57 & 57.12 & 48.11 & 46.11 & 25.26 & 26.19 & 22.92 & 25.25 & 74.50 & 89.74 & 41.81 & 68.79 & \multicolumn{1}{c|}{64.76} & \textbf{46.93} \\
\multicolumn{1}{l|}{GemFilter} & 25.45 & 37.64 & 53.91 & 52.83 & 51.26 & 33.02 & 26.23 & 19.42 & 23.87 & 65.50 & 84.16 & 40.05 & 38.03 & \multicolumn{1}{c|}{41.13} & \textbf{42.32} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 25.51 & 41.48 & 56.76 & 48.73 & 45.43 & 26.53 & 23.42 & 22.15 & 24.02 & 74.00 & 89.42 & 43.22 & 68.61 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}63.57} & \textbf{46.63} \\ \midrule

\multicolumn{16}{c}{Mistral-Nemo-12B-Instruct,   KV Budget = 2048} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 24.35 & 42.95 & 56.92 & 49.08 & 46.43 & 26.26 & 28.27 & 23.68 & 26.07 & 75.00 & 89.82 & 44.20 & 68.89 & \multicolumn{1}{c|}{68.00} & \textbf{47.85} \\
% \multicolumn{1}{l|}{AdaKV} & 24.67 & 42.46 & 56.05 & 48.60 & 45.97 & 26.23 & 26.58 & 23.39 & 25.74 & 75.00 & 89.66 & 44.09 & 68.25 & \multicolumn{1}{c|}{67.70} & \textbf{47.46} \\
\multicolumn{1}{l|}{AdaKV} & 26.02 & 43.08 & 57.43 & 48.76 & 46.53 & 26.12 & 27.80 & 23.44 & 25.93 & 75.00 & 89.66 & 44.49 & 68.53 & \multicolumn{1}{c|}{67.57} & \textbf{47.88} \\
% \multicolumn{1}{l|}{HeadKV-R} & 26.46 & 42.03 & 57.67 & 48.92 & 46.51 & 25.87 & 30.08 & 24.33 & 26.15 & 75.00 & 89.66 & 44.29 & 68.67 & \multicolumn{1}{c|}{67.38} & 48.07 \\
\multicolumn{1}{l|}{HeadKV} & 26.05 & 42.15 & 57.46 & 48.49 & 45.85 & 25.86 & 30.27 & 24.22 & 26.17 & 75.00 & 89.66 & 43.74 & 68.64 & \multicolumn{1}{c|}{67.29} & \textbf{47.92} \\
\multicolumn{1}{l|}{GemFilter} & 26.42 & 42.40 & 56.98 & 57.64 & 53.92 & 34.33 & 30.51 & 21.60 & 25.96 & 72.00 & 89.65 & 44.48 & 48.34 & \multicolumn{1}{c|}{48.06} & \textbf{46.59} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 26.61 & 43.37 & 57.12 & 49.41 & 47.22 & 26.27 & 28.40 & 23.22 & 25.81 & 75.00 & 89.72 & 43.04 & 68.67 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}67.61} & \textbf{48.11} \\ \bottomrule
\end{tabular}
}
\vspace{-8pt}
\end{table*}



% \begin{table*}[t]
% \setlength{\tabcolsep}{4pt}
% \centering
% \caption{LongBench results comparison of baseline methods and FastKV.}
% \label{tab:longbench}
% \renewcommand{\arraystretch}{0.9}
% \scalebox{0.8}{
% \begin{tabular}{@{}lccccccccccc!{\color{white}\vrule}@{}}
% \toprule
% \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{} & \multicolumn{5}{c|}{KV Budget = 512} & \multicolumn{5}{c}{KV  Budget = 2048} \\ \cmidrule(l){3-12} 

% \multicolumn{1}{c|}{\multirow{-2}{*}{\raisebox{0.1cm}[0pt][0pt]{Benchmark}}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\raisebox{0.1cm}[0pt][0pt]{Full KV}}} & \rotatebox{0}{SnapKV} & \rotatebox{0}{AdaKV} &  \rotatebox{0}{HeadKV} & \rotatebox{0}{GemFilter} & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}\rotatebox{0}{FastKV}} & \rotatebox{0}{SnapKV} & \rotatebox{0}{AdaKV} & \rotatebox{0}{HeadKV} & \rotatebox{0}{GemFilter} & \cellcolor[HTML]{DAE8FC}\rotatebox{0}{FastKV} \\ \midrule

% \multicolumn{12}{c}{LLaMA-3.1-8B-Instruct} \\ \midrule
% \multicolumn{1}{l|}{Single-Doc QA} & \multicolumn{1}{c|}{43.58} & 41.87 & 41.47 & 42.83 & 25.37 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}42.40} & 43.81 & 43.88 & 43.50 & 38.67 & \cellcolor[HTML]{DAE8FC}43.50 \\
% \multicolumn{1}{l|}{Multi-Doc QA} & \multicolumn{1}{c|}{44.65} & 43.70 & 43.40 & 44.13 & 32.80 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}43.98} & 43.85 & 43.76 & 44.32 & 43.52 & \cellcolor[HTML]{DAE8FC}44.05 \\
% \multicolumn{1}{l|}{Summarization} & \multicolumn{1}{c|}{29.22} & 25.46 & 25.10 & 27.42 & 22.42 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}24.99} & 27.84 & 27.45 & 28.22 & 26.35 & \cellcolor[HTML]{DAE8FC}27.70 \\
% \multicolumn{1}{l|}{Few-shot Learning} & \multicolumn{1}{c|}{69.48} & 68.44 & 68.18 & 68.96 & 63.51 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}69.06} & 68.79 & 69.14 & 68.97 & 67.77 & \cellcolor[HTML]{DAE8FC}69.55 \\
% \multicolumn{1}{l|}{Code} & \multicolumn{1}{c|}{60.01} & 57.01 & 56.85 & 59.30 & 35.48 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}57.67} & 59.59 & 59.56 & 60.10 & 44.58 & \cellcolor[HTML]{DAE8FC}59.50 \\ \midrule
% \multicolumn{1}{l|}{\textbf{Avg.}} & \multicolumn{1}{c|}{\textbf{48.63}} & \textbf{46.60} & \textbf{46.29} & \textbf{47.76} & \textbf{35.95} & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}\textbf{46.90}} & \textbf{48.00} & \textbf{47.99} & \textbf{48.23} & \textbf{44.15} & \cellcolor[HTML]{DAE8FC}\textbf{48.10} \\ \midrule
% \multicolumn{12}{c}{LLaMA-3.2-3B-Instruct} \\ \midrule
% \multicolumn{1}{l|}{Single-Doc QA} & \multicolumn{1}{c|}{37.93} & 34.28 & 33.60 & 36.60 & 32.19 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}34.39} & 37.50 & 37.70 & 37.60 & 35.24 & \cellcolor[HTML]{DAE8FC}37.36 \\
% \multicolumn{1}{l|}{Multi-Doc QA} & \multicolumn{1}{c|}{36.05} & 35.45 & 35.24 & 36.15 & 35.35 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}35.33} & 36.07 & 36.06 & 36.12 & 36.63 & \cellcolor[HTML]{DAE8FC}36.09 \\
% \multicolumn{1}{l|}{Summarization} & \multicolumn{1}{c|}{28.13} & 24.30 & 24.22 & 26.38 & 23.24 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}23.97} & 26.81 & 26.62 & 27.81 & 26.24 & \cellcolor[HTML]{DAE8FC}26.66 \\
% \multicolumn{1}{l|}{Few-shot Learning} & \multicolumn{1}{c|}{67.86} & 66.51 & 66.97 & 66.99 & 58.67 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}67.13} & 67.21 & 67.37 & 67.67 & 65.05 & \cellcolor[HTML]{DAE8FC}67.82 \\
% \multicolumn{1}{l|}{Code} & \multicolumn{1}{c|}{53.15} & 51.47 & 51.36 & 53.78 & 29.70 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}52.32} & 53.10 & 53.34 & 54.43 & 38.43 & \cellcolor[HTML]{DAE8FC}54.12 \\ \midrule
% \multicolumn{1}{l|}{\textbf{Avg.}} & \multicolumn{1}{c|}{\textbf{44.02}} & \textbf{41.75} & \textbf{41.63} & \textbf{43.28} & \textbf{36.27} & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}\textbf{41.93}} & \textbf{43.50} & \textbf{43.57} & \textbf{44.03} & \textbf{40.45} & \cellcolor[HTML]{DAE8FC}\textbf{43.71} \\ \midrule
% \multicolumn{12}{c}{Mistral-Nemo-12B-Instruct} \\ \midrule
% \multicolumn{1}{l|}{Single-Doc QA} & \multicolumn{1}{c|}{42.67} & 39.62 & 39.38 & 41.19 & 39.00 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}41.25} & 41.41 & 41.06 & 41.89 & 41.93 & \cellcolor[HTML]{DAE8FC}42.37 \\
% \multicolumn{1}{l|}{Multi-Doc QA} & \multicolumn{1}{c|}{40.48} & 39.59 & 40.06 & 39.83 & 45.70 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}40.23} & 40.59 & 40.27 & 40.07 & 48.63 & \cellcolor[HTML]{DAE8FC}40.97 \\
% \multicolumn{1}{l|}{Summarization} & \multicolumn{1}{c|}{27.18} & 23.44 & 23.07 & 24.79 & 23.17 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}23.20} & 26.01 & 25.24 & 26.89 & 26.02 & \cellcolor[HTML]{DAE8FC}25.81 \\
% \multicolumn{1}{l|}{Few-shot Learning} & \multicolumn{1}{c|}{69.66} & 68.84 & 68.78 & 68.68 & 63.24 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}68.88} & 69.67 & 69.58 & 69.47 & 68.71 & \cellcolor[HTML]{DAE8FC}69.92 \\
% \multicolumn{1}{l|}{Code} & \multicolumn{1}{c|}{68.35} & 65.23 & 64.90 & 66.78 & 39.58 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}66.09} & 68.45 & 67.98 & 67.97 & 48.20 & \cellcolor[HTML]{DAE8FC}68.14 \\ \midrule
% \multicolumn{1}{l|}{\textbf{Avg.}} & \multicolumn{1}{c|}{\textbf{48.33}} & \textbf{46.07} & \textbf{45.98} & \textbf{46.93} & \textbf{42.32} & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}\textbf{46.63}} & \textbf{47.85} & \textbf{47.46} & \textbf{47.92} & \textbf{46.59} & \cellcolor[HTML]{DAE8FC}\textbf{48.11} \\ \bottomrule
% \end{tabular}
% }
% \end{table*}



% \subsection{Grouped-Query Attention Compatibility}
\subsection{GQA Compatible KV Cache Compression}
% - 1) why we have to apply GQA compatible KV Cache compression? - otherwise, it might increase the overhead even after the compression

% - 2) how we handle GQA compatible KV Cache compression? - the core concept of the GQA - share KV across query within group, KV info can be shared within group -> important tokens might be shared -> averaging scroe as mentioned in the AdaKV github -> it does not lead to acc drop so we adopted it.

While FastKV incorporates TSP for the latter half of the LLM, it takes a different approach to KV cache compression by propagating full-context data in the early layers and evaluating the importance of KV data using attention scores generated by each layer.
As the state-of-the-art LLMs~\cite{qwen2, phi4} increasingly adopt GQA~\cite{gqa}, which groups multiple attention heads together and shares KV data within each group, FastKV incorporates GQA compatible KV cache compression technique presented in AdaKV to the early layers.
This technique adapts SnapKV approach (Equation~\ref{eq:snpaKV-1}) to GQA by aggregating attention scores across all heads within the group for each token as follows:
% However, FastKV adapts to GQA by aggregating attention scores across all heads within the group for each token as follows:
\vspace{-6pt}
\begin{equation}
    \label{eq:GQA}
    S_{i}^{l,g}=\frac{1}{H_G}\sum_{h=h_g}^{h_g+H_G}S_i^{j,h}
    % \mathbf{S_{G}}=\frac{1}{N}\sum_{h=j}^{j+N}(\text{pooling}(\sum_{i=0}^{L_{\text {W}}} \mathbf{A^h}[:,\,i,\,:-L_{W}]))
\end{equation}
% \vspace{-4pt}
Here, $H_G$ denotes the number of attention heads in each group. $g$ and $h_g$ denotes the group index and the index of the first head in the group.
This aggregation evaluates the importance of tokens at the group level. FastKV then selects and stores KV cache entries corresponding to the top-KV budget important tokens in an attention-group-wise manner.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/Fig7.pdf}
    \vspace{-24pt}
     % \vspace{-6mm}
    \caption{
    LongBench results (line) and TTFT (bar) of LLaMA-3.1-8B-insturct across various KV budgets and TSP lengths.
        % Average LongBench accuracy and TTFT at context length 128k and KV with varying TSP lengths.
    }
    % \vspace{-2mm}
    \vspace{-12pt}
    \label{fig:plength_main}
\end{figure}



% This adaptation allows FastKV to leverage the benefits of GQA effectively, achieving a more meaningful compression ratio while maintaining the integrity of important KV data across the attention groups.
% Moreover, our experimental results present that the attention-group-wise KV cache compression can also improve accuracy compared to conventional attention-head-wise compression techniques. 
% This is because our approach inherently maintains the shared relevance of KV data within an attention group, and it can avoid the fragmentation that occurs in head-wise approaches.

% While FastKV incorporates TSP for the latter half of the LLM, it takes a different approach to KV cache compression by propagating full-context data in the early layers and evaluating the importance of KV data using attention scores generated by each layer. Previous approaches, such as SnapKV and HeadKV, process KV cache compression at the attention-head level, as the attention head is traditionally the basic unit of the attention mechanism. 
% However, state-of-the-art LLMs~\cite{qwen2, phi4} increasingly adopt GQA~\cite{gqa}, which groups multiple attention heads together and shares KV data within each group. In GQA, the same KV data interacts with different Q data from each attention head within the group. If previous head-wise KV cache compression methods are applied directly, they select KV data independently for each head rather than the group. This leads to inefficiencies in compression. Specifically, the actual compression ratio is not determined solely by the KV budget over the original sequence length but by the product of the KV budget and the number of heads per group divided by the original sequence length.


% The validity of GQA suggests that important KV data are highly correlated within each attention group. Therefore, an attention-group-wise KV cache compression approach is more appropriate. 
% % When applying KV cache compression in the early stages, FastKV evaluates the importance of the tokens based on the attention scores, similar to previous approaches. 
% FastKV adapts SnapKV approach (Equation~\ref{eq:snpaKV-1}) to GQA by aggregating attention scores across all heads within the group for each token as follows:
% % However, FastKV adapts to GQA by aggregating attention scores across all heads within the group for each token as follows:
% % \vspace{-4pt}
% \begin{equation}
%     \label{eq:GQA}
%     S_{i}^{l,g}=\frac{1}{H_G}\sum_{h=h_g}^{h_g+H_G}S_i^{j,h}
%     % \mathbf{S_{G}}=\frac{1}{N}\sum_{h=j}^{j+N}(\text{pooling}(\sum_{i=0}^{L_{\text {W}}} \mathbf{A^h}[:,\,i,\,:-L_{W}]))
% \end{equation}
% % \vspace{-4pt}
% Here,$H_G$ denotes the number of attention heads in each group. $g$ and $h_g$ denotes the group index and the index of the first head in the group.
% This aggregation evaluates the importance of tokens at the group level. FastKV then selects and stores KV cache entries corresponding to the top-KV budget important tokens in an attention-group-wise manner.
% This adaptation allows FastKV to leverage the benefits of GQA effectively, achieving a more meaningful compression ratio while maintaining the integrity of important KV data across the attention groups.
% Moreover, our experimental results present that the attention-group-wise KV cache compression can also improve accuracy compared to conventional attention-head-wise compression techniques. 
% This is because our approach inherently maintains the shared relevance of KV data within an attention group, and it can avoid the fragmentation that occurs in head-wise approaches.



% Following the KV cache compression strategy~\cite{}, we design FastKV to be compatible with Grouped-Query Attention (GQA). Previous KV cache compression methods rely on attention heads to select important tokens for eviction. However, in GQA models, each head within the same group exhibits distinct attention patterns due to variations in their queries. Consequently, KV caches must be stored separately for each head, significantly increasing memory and computational costs. To address this issue, GQA compatibility technique is calculated using the following equation:

% \vspace{-4pt}
% \begin{equation}
%     \label{eq:GQA}
%     \mathbf{S_{G}}=\frac{1}{N}\sum_{h=j}^{j+N}(\text{pooling}(\sum_{i=0}^{L_{\text {W}}} \mathbf{A^h}[:,\,i,\,:-L_{W}]))
% \end{equation}
% \vspace{-4pt}

% where $j$ denotes the index of the head group, $N$ represents the number of heads within the group, and $\mathbf{S_G}$ is the importance score of tokens at the group level. This technique enables KV cache compression at the group level for GQA models but is less effective for head-wise fine-grained methods. As a naive KV cache compression method, FastKV can effectively leverage this GQA compatibility to compress KV caches in a group manner, improving throughput while maintaining computational efficiency.

\begin{figure*}[t]
    \centering
    \vspace{-4pt}
    \includegraphics[width=1.0\linewidth]{images/Fig8.PNG}
    % \vspace{-12pt}
     \vspace{-28pt}
    \caption{
        Needle-in-a-Haystack results of LLaMA-3.1-8B-Instruct with 512 KV budget.
    }
    \vspace{-8pt}
    \label{fig:niah_512}
\end{figure*}

\section{Experiments}

\subsection{Setup}

\textbf{Models \& Datasets.} 
% We evaluate three open-source LLMs of different sizes: LLaMA-3.1-8B-Instruct, LLaMA-3.2-3B-Instruct~\cite{llama3}, and Mistral-Nemo-12B-Instruct~\cite{mistral}.
%These models have 32, 28, and 40 decoder layers, respectively, and employ GQA with a context window size of 128k tokens.
We evaluate two open-source LLMs of different sizes: LLaMA-3.1-8B-Instruct~\cite{llama3} and Mistral-Nemo-12B-Instruct~\cite{mistral}.
These models have 32 and 40 decoder layers, respectively, and employ GQA~\cite{gqa} with a context window size of 128k tokens.

To assess long-context understanding and retrieval capabilities, we use two benchmark datasets: LongBench~\cite{longbench} and Needle-in-a-Haystack~\cite{needle}.

\textbf{Implementation Details.}
% Following the procedure in Section~\ref{subsection:tsp_anal_layer_selection}, we select layers 15, 13, and 19 as the TSP layers for LLaMA-3.1-8B-Instruct, LLaMA-3.2-3B-Instruct, and Mistral-Nemo-12B-Instruct, respectively.
We integrated our proposed FastKV method upon self-attention implementation of HuggingFace Transformers library, which utilizes FlashAttention-2~\cite{flashattention2} kernel.
We select layer 15 and 19 as the TSP layer for LLaMA-3.1-8B-Instruct, and Mistral-Nemo-12B-Instruct, respectively.
The TSP length is set to 2048.
We fix the observation window size ($N_{obs}$) to 8 and the kernel size of average pooling ($2w_p+1$) to 7.
% We fix the local window size to 8 and use average pooling with a kernel size of 7.

\textbf{Baselines.}
We compare our approach with four baseline methods for KV cache compression: SnapKV~\cite{snapkv}, AdaKV~\cite{adakv}, HeadKV~\cite{headkv}, and GemFilter~\cite{gemfilter}.
% HeadKV is subdivided into HeadKV-R and HeadKV-R2.
% HeadKV-R allocates larger KV budgets to attention heads that are critical for retrieval, while HeadKV-R2 targets heads essential for both retrieval and reasoning.
% HeadKV-R2 employs datasets specifically designed to capture both retrieval and reasoning skills, whereas HeadKV-R focuses on retrieval-oriented data.
SnapKV, AdaKV, and HeadKV use the same local window size, pooling method, and pooling kernel size as FastKV.
%The filter layer of GemFilter for each model is chosen to match the TSP layer used by FastKV.
The indices of GemFilter filter layers for LLaMA-3.1-8B-Instruct and Mistral-Nemo-12B-Instruct are 13 and 19, respectively \cite{gemfilter}.
% The GemFilter filter layers for LLaMA-3.1-8B-Instruct and Mistral-Nemo-12B-Instruct are 13 and 19, respectively, as chosen in \cite{gemfilter}.


% \subsection{Main Results}

% \textbf{LongBench.} The results of the LongBench evaluations are presented in Table~\ref{tab:longbench}.
% We report average accuracy of each subarea of LongBench and total average accuracy.
% The full evaluation results with the accuracy of each dataset are presented in Appendix~\ref{appendix:longbench}.
% FastKV showcases its clear competitiveness against previous KV cache compression methods, performing on par with the strongest baselines.

% Methods that adopt fine-grained KV budget allocation procedures for each head achieve high accuracy by trading off decoding latency for model accuracy. Among them, HeadKV is more powerful than AdaKV due to its more flexible KV budget allocation scheme, which allows the average head budget of each layer to vary. HeadKV-R2 appears to be a superior variant of HeadKV compared to HeadKV-R when the KV budget is 512, but this gap narrows or even reverses when the KV budget is 2048.

% For both models, FastKV significantly reduces the accuracy disparity with Full KV when the KV budget is 2048, showing accuracy similar to HeadKV. When the KV budget is 512, the accuracy of FastKV is slightly lower than HeadKV variants but remains competitive. FastKV consistently surpasses the accuracy of AdaKV regardless of the model and KV budget.

% GemFilter, which discards unselected tokens and recomputes KV cache only with the retained tokens, shows a large discrepancy in accuracy compared to other methods, with significantly lower average accuracy. 
% The deviation becomes even larger when the KV budget is small, as it cannot incorporate any information from discarded tokens.
% GemFilter shows high accuracy at multi-document QA tasks for Mistral-Nemo-12B-Instruct, even surpassing the accuracy of full KV.
% However, the extent to which this tendency appears varies greatly from model to model, and excluding this area, the average accuracy of other subareas becomes worse.
% This indicates that GemFilter is not broadly applicable to diverse tasks in practice, despite its unique attempt to reduce TTFT.

% FastKV also effectively reduces TTFT without compromising the models' long-context understanding capability. It consistently outperforms GemFilter, particularly in single-document QA and coding tasks. Moreover, FastKV does not exhibit any particular weakness in specific areas or datasets within LongBench, demonstrating its applicability across a wide range of tasks.

% \subsection{Main Results}
\subsection{Accuracy Evaluation}

\textbf{LongBench.}
The accuracy evaluation results on LongBench are summarized in Table~\ref{tab:longbench}. The full breakdown of results is provided in Appendix~\ref{appendix:longbench}.
Previous works, such as SnapKV, AdaKV, and HeadKV, which use attention-head-wise or attention-group-wise token removal, successfully maintain accuracy after KV cache compression, with an average accuracy drop of less than 2.5\% and 0.51\% compared to FullKV for 512 and 2048 KV budgets, respectively.
% Especially, HeadKV and AdaKV push accuracy even further by varying KV budget itself across attention-heads or attention-groups.
GemFilter, which discards unselected tokens from the input prompt and recomputes the prefill stage to achieve both memory and computational efficiency, results in a significantly larger accuracy drop, with reductions of up to 13.0\% and 4.7\% compared to FullKV for 512 and 2048 KV budgets, respectively.
On the other hand, while the proposed FastKV is designed to achieve memory and computational efficiency comparable to GemFilter, it successfully preserves accuracy on par with SnapKV, AdaKV, and HeadKV. It presents the effectiveness of FastKV’s distinct KV cache compression strategies for the early and later layers of LLMs.%, carefully designed with consideration of the unique properties of attention maps in LLMs. 
% The results of the LongBench evaluations are presented in Table~\ref{tab:longbench}.
% We report the average accuracy for each LongBench subarea, as well as the total average accuracy.
% A full breakdown of each dataset’s accuracy is provided in Appendix~\ref{appendix:longbench}.
% FastKV demonstrates clear competitiveness with prior KV cache compression methods, achieving performance on par with the strongest baselines.

% Methods employing fine-grained KV budget allocation per attention head typically attain higher accuracy at the cost of increased decoding latency.
% Among these, HeadKV outperforms AdaKV due to its more flexible budget allocation scheme, which allows varying average head budgets across layers.
% HeadKV-R2 appears stronger than HeadKV-R when the KV budget is 512, but this gap narrows or even reverses for a KV budget of 2048.

% FastKV substantially reduces the accuracy gap with Full KV at a budget of 2048, reaching accuracy comparable to HeadKV.
% With a KV budget of 512, FastKV’s accuracy is slightly lower than that of HeadKV but remains competitive, and it consistently surpasses AdaKV across all models and KV budgets.

% GemFilter, which discards unselected tokens and recomputes the KV cache only with retained tokens, exhibits a large performance drop compared to other methods. 
% The discrepancy grows more pronounced at smaller budgets, since discarded tokens provide no additional information.
% Interestingly, GemFilter achieves high accuracy on multi-document QA for Mistral-Nemo-12B-Instruct, even exceeding Full KV. 
% However, this improvement does not generalize across other models and tasks, and once these outlier cases are excluded, GemFilter’s average accuracy is significantly lower.
% This indicates that, despite its unique approach to reducing TTFT, GemFilter is not broadly effective on diverse tasks.

% FastKV, on the other hand, effectively reduces TTFT without compromising long-context understanding.
% It consistently outperforms GemFilter in single-document QA, few-shot learning and code tasks, and does not exhibit pronounced weaknesses on any particular dataset or subarea within LongBench.
% As a result, FastKV shows robust applicability across a wide range of tasks, delivering near-Full-KV performance within significantly reduced budgets.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \begin{table*}[t]
% \setlength{\tabcolsep}{5pt}
% \centering
% \caption{InfiniteBench results}
% \label{tab:infinitebench}
% \scalebox{0.9}{
% \begin{tabular}{@{}!{\color{white}\vrule}lcccccccc!{\color{white}\vrule}@{}}
% \toprule
% \multicolumn{1}{c|}{Method} & En.MC & En.QA & En.Diag & Retr.P & Retr.N & Math.F & \multicolumn{1}{c|}{Code.D} & Avg. \\ \midrule
% \multicolumn{9}{c}{LLaMA-3.1-8B-Inustruct, KV Budget = Full} \\ \midrule
% \multicolumn{1}{l|}{Full KV} & 65.50 & 27.07 & 17.50 & 98.14 & 98.31 & 23.71 & \multicolumn{1}{c|}{21.32} & 50.22 \\ \midrule
% \multicolumn{9}{c}{LLaMA-3.1-8B-Inustruct, KV Budget = 512} \\ \midrule
% \multicolumn{1}{l|}{SnapKV} & 65.50 & 24.31 & 16.00 & 98.14 & 80.00 & 23.71 & \multicolumn{1}{c|}{13.96} & 45.95 \\
% \multicolumn{1}{l|}{AdaKV} & 65.50 & 22.68 & 14.50 & 98.14 & 80.34 & 23.71 & \multicolumn{1}{c|}{17.51} & 46.05 \\
% \multicolumn{1}{l|}{HeadKV-R} & 65.50 & 24.72 & 14.50 & 98.14 & 83.56 & 23.71 & \multicolumn{1}{c|}{15.99} & 46.59 \\
% \multicolumn{1}{l|}{HeadKV-R2} & 65.50 & 24.72 & 13.50 & 98.14 & 87.97 & 23.71 & \multicolumn{1}{c|}{12.44} & 46.57 \\
% \multicolumn{1}{l|}{GemFilter} & 13.10 & 6.18 & 8.50 & 100.00 & 99.66 & 30.86 & \multicolumn{1}{c|}{24.87} & 40.45 \\
% \rowcolor[HTML]{DAE8FC} 
% \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 65.50 & 26.90 & 13.50 & 97.63 & 75.76 & 22.29 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}10.91} & 44.64 \\ \midrule
% \multicolumn{9}{c}{LLaMA-3.1-8B-Inustruct,   KV Budget = 2048} \\ \midrule
% \multicolumn{1}{l|}{SnapKV} & 65.50 & 25.77 & 15.50 & 98.14 & 87.46 & 23.71 & \multicolumn{1}{c|}{17.26} & 47.62 \\
% \multicolumn{1}{l|}{AdaKV} & 65.50 & 26.16 & 16.50 & 98.14 & 95.93 & 23.71 & \multicolumn{1}{c|}{20.05} & 49.43 \\
% \multicolumn{1}{l|}{HeadKV-R} & 65.50 & 25.95 & 16.50 & 98.14 & 83.22 & 23.71 & \multicolumn{1}{c|}{19.04} & 47.44 \\
% \multicolumn{1}{l|}{HeadKV-R2} & 65.50 & 26.26 & 18.00 & 98.14 & 79.66 & 23.71 & \multicolumn{1}{c|}{13.45} & 46.39 \\
% \multicolumn{1}{l|}{GemFilter} & 35.37 & 13.86 & 15.00 & 100.00 & 100.00 & 33.14 & \multicolumn{1}{c|}{23.86} & 45.89 \\
% \rowcolor[HTML]{DAE8FC} 
% \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 65.50 & 27.05 & 15.50 & 97.97 & 86.10 & 26.00 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}14.72} & 47.55 \\ \midrule
% \multicolumn{9}{c}{LLaMA-3.2-3B-Inustruct,   KV Budget = Full} \\ \midrule
% \multicolumn{1}{l|}{Full KV} & 48.03 & 14.17 & 11.50 & 5.08 & 100.00 & 14.57 & \multicolumn{1}{c|}{18.27} & 30.23 \\ \midrule
% \multicolumn{9}{c}{LLaMA-3.2-3B-Inustruct,   KV Budget = 512} \\ \midrule
% \multicolumn{1}{l|}{SnapKV} & 50.22 & 13.12 & 9.00 & 5.08 & 99.66 & 14.57 & \multicolumn{1}{c|}{19.29} & 30.13 \\
% \multicolumn{1}{l|}{AdaKV} & 49.78 & 11.83 & 11.00 & 5.08 & 99.15 & 14.57 & \multicolumn{1}{c|}{18.78} & 30.03 \\
% \multicolumn{1}{l|}{HeadKV-R} & 49.34 & 13.45 & 8.50 & 5.08 & 96.10 & 14.57 & \multicolumn{1}{c|}{19.04} & 29.44 \\
% \multicolumn{1}{l|}{HeadKV-R2} & 49.34 & 14.69 & 9.50 & 5.08 & 96.27 & 14.57 & \multicolumn{1}{c|}{19.04} & 29.78 \\
% \multicolumn{1}{l|}{GemFilter} & 32.31 & 7.75 & 10.00 & 19.66 & 100.00 & 27.14 & \multicolumn{1}{c|}{16.24} & 30.44 \\
% \rowcolor[HTML]{DAE8FC} 
% \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 50.22 & 14.44 & 8.00 & 6.44 & 99.66 & 19.43 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}19.29} & 31.07 \\ \midrule
% \multicolumn{9}{c}{LLaMA-3.2-3B-Inustruct,   KV Budget = 2048} \\ \midrule
% \multicolumn{1}{l|}{SnapKV} & 48.91 & 14.23 & 9.50 & 5.08 & 99.83 & 14.57 & \multicolumn{1}{c|}{19.54} & 30.24 \\
% \multicolumn{1}{l|}{AdaKV} & 49.78 & 14.75 & 9.00 & 5.08 & 99.66 & 14.57 & \multicolumn{1}{c|}{18.53} & 30.20 \\
% \multicolumn{1}{l|}{HeadKV-R} & 49.34 & 14.53 & 7.50 & 5.08 & 99.66 & 14.57 & \multicolumn{1}{c|}{20.30} & 30.14 \\
% \multicolumn{1}{l|}{HeadKV-R2} & 48.03 & 15.02 & 10.50 & 5.08 & 99.83 & 14.57 & \multicolumn{1}{c|}{18.78} & 30.26 \\
% \multicolumn{1}{l|}{GemFilter} & 35.37 & 9.86 & 12.00 & 96.10 & 100.00 & 21.43 & \multicolumn{1}{c|}{16.24} & 41.57 \\
% \rowcolor[HTML]{DAE8FC} 
% \multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 50.22 & 15.25 & 8.50 & 5.59 & 100.00 & 20.00 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}21.07} & 31.52 \\ \bottomrule
% \end{tabular}
% }
% \end{table*}


% \textbf{InfiniteBench} The results of the InfiniteBench evaluations are depicted in Table \ref{tab:infinitebench}.
% FastKV demonstrates its competitiveness against the baselines.
% In case of LLaMA-3.1-8B-Instruct, with KV budget 2048, FastKV shows accuracy comparable to the baselines.
% For LLaMA-3.2-3B-Instruct, FastKV outperforms baselines with both KV budget options.
% With KV budget 2048, GemFilter achieves exceptionally high average score due to the score of Retrieve.Passkey task.
% This phenomenon was also observed, although to a lesser extent, when the KV budget was 512.
% The characteristic of GemFilter that discards unselected tokens completely excludes noisy information, which sometimes increases the benchmark score for a specific subtask.
% However, considering the overall benchmark results on various datasets and the stability of scores when measured on multiple models, it is difficult to say that these guarantee the high performance of GemFilter.





% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% % \usepackage[table,xcdraw]{xcolor}
% % Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \begin{table}[]
% \setlength{\tabcolsep}{6pt}
% \centering
% \caption{Needle-in-a-Haystack results}
% \label{tab:niah}
% \renewcommand{\arraystretch}{0.9}
% \scalebox{0.9}{
% \begin{tabular}{@{}!{\color{white}\vrule}l|cc|cc!{\color{white}\vrule}@{}}
% \toprule
% \multicolumn{1}{c|}{Model} & \multicolumn{2}{c|}{LLaMA-3.1-8B-Instruct} & \multicolumn{2}{c}{LLaMA-3.2-3B-Instruct} \\ \midrule
% Full KV & \multicolumn{2}{c|}{0.938} & \multicolumn{2}{c}{0.510} \\ \midrule
% \multicolumn{1}{c|}{KV Budget} & 512 & 2048 & 512 & 2048 \\ \midrule
% SnapKV & 0.824 & 0.906 & 0.355 & 0.419 \\
% AdaKV & 0.807 & 0.917 & 0.403 & 0.461 \\
% HeadKV-R & 0.872 & 0.926 & 0.366 & 0.392 \\
% HeadKV-R2 & 0.879 & 0.934 & 0.398 & 0.398 \\
% GemFilter & 0.690 & 0.788 & 0.335 & 0.406 \\
% \rowcolor[HTML]{DAE8FC} 
% FastKV & 0.884 & 0.922 & 0.384 & 0.490 \\ \bottomrule
% \end{tabular}
% }
% \end{table}


\textbf{Needle-in-a-Haystack.}
Figure~\ref{fig:niah_512} presents the Needle-in-a-Haystack evaluations for LLaMA-3.1-8B-Instruct with 512 KV budget. Additional analysis results with various LLM models and KV budgets can be found in Appendix~\ref{appendix:niah}.
Consistent with the results observed in LongBench, GemFilter shows the worst performance, while FastKV achieves the best performance with a slight improvement over HeadKV.
% In this evaluation, GemFilter shows the worst performance, consistent with the results observed in LongBench.
% Additionally, there is a substantial average score gap between HeadKV and SnapKV/AdaKV, underscoring that while attention-head-wise or attention-group-wise pruning offers greater flexibility for KV cache compression, it necessitates a highly sophisticated design, as demonstrated by HeadKV. 
% Without such refinement, as observed in SnapKV, this fine-grained pruning approach can lead to increased performance variability.
% Despite employing a coarser-grained token pruning approach, FastKV achieves the best performance among KV cache compression techniques. This is attributed to its design, \textcolor{red}{which carefully considers the unique properties of attention maps.}

% Figure~\ref{fig:niah_2048} presents the results of Full KV and various KV compression methods on LLaMA-3.1-8B-Instruct with a KV budget of 2048.
% %For each method, the heatmap shows retrieval performance (color-coded from green to red) as a function of context length (8k to 128k tokens) on the horizontal axis and depth percent (0\% to 100\%) on the vertical axis, indicating how “deeply” within the document the target information is located.
% Additional Needle-in-a-Haystack evaluations, including different KV budgets and other model configurations, can be found in Appendix~\ref{appendix:niah}.

% Overall, the trend mirrors that of the LongBench evaluations.
% FastKV achieves a noteworthy average score (\(\textbf{0.922}\)), outperforming or matching most baselines that leverage all input tokens during the prefill stage. 
% HeadKV-R2 exhibits the highest average score among the compression-based methods (\(\textbf{0.934}\)), closely approaching Full KV (\(\textbf{0.938}\)).
% AdaKV also attains a relatively strong average (\(\textbf{0.917}\)), but FastKV slightly surpasses it across a wide range of context lengths.
% SnapKV lags behind (\(\textbf{0.906}\)), showing inconsistent performance at extremely long contexts.

% FastKV demonstrates robust retrieval capabilities across diverse context lengths, remaining consistently green up to around 80K tokens, with only mild performance dips at the upper limits (e.g., beyond 100K tokens) or very deep targets.
% Such stability indicates that FastKV effectively retains crucial information within the KV cache, even in challenging long-context scenarios.
% HeadKV-R2’s fine-grained allocation of KV resources to specific heads yields marginally higher scores at extreme depths, but the difference between HeadKV-R2 and FastKV becomes negligible for most mid-range contexts.

% In stark contrast, GemFilter produces a significantly lower average score (\(\textbf{0.788}\)) and struggles to maintain reliable retrieval quality, even at shorter context lengths.
% Because GemFilter discards unselected tokens outright, it often cannot recover essential information if those tokens happen to contain crucial clues, resulting in visibly red areas on the heatmap.
% Its occasional successes at very distant depths are highly uneven and do not compensate for its overall tendency to underperform on most depth-length combinations.

% Overall, FastKV stands out for its adaptability and robustness, delivering near-Full-KV retrieval accuracy across a wide range of context lengths and needle depths.
% Unlike HeadKV, FastKV does not require per-head KV budget tuning, yet it still exhibits minimal performance degradation relative to Full KV.
% This makes FastKV a compelling choice for practical scenarios demanding both efficient memory usage and stable retrieval accuracy in extremely long contexts.



\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{images/Fig9.pdf}
    \vspace{-24pt}
     % \vspace{-6mm}
    \caption{
        \textbf{(a)} TTFT and \textbf{(b)} throughput results across different methods on LLaMA-3.1-8B-Instruct. (dashed line: Full KV)
    }
    % \vspace{-6mm}
    \vspace{-12pt}
    \label{fig:speed}
\end{figure}

\subsection{Latency and Throughput Evaluation}

We evaluate the latency of the prefill stage (time-to-first-token (TTFT)) and throughput of token generation with LLaMA-3.1-8B-Instruct model and a single NVIDIA A100 GPU. 
%The evaluation results with 128k input context an KV budget are reported inFigure~\ref{fig:speed}. 
Figure~\ref{fig:speed} reports TTFT results with 64k/128k input with 512 KV budget, and throughput results with 128k input with 512/2048 KV budget.
% The evaluation results with 64k/128k input context and 512 KV budget for TTFT and with 128k input context and 512/2048 KV budget for TTFT reported in Figure~\ref{fig:speed}.
% The evaluation results with 64k/128k input context and 512/2048 KV budget are reported in Figure~\ref{fig:speed}.
More detailed evaluation results can be found in Appendix~\ref{appendix:speedup}.

\textbf{TTFT.} 
As shown in Figure~\ref{fig:speed}(a), both GemFilter and FastKV achieve significant TTFT improvements over other baselines by reducing the computational complexity of the prefill stage through the concept of propagating only selected tokens during this stage. 
As a result, FastKV achieves TTFT that is 1.60$\times$ shorter and 1.97$\times$ shorter than Full KV, with 64k and 128k input, respecitvely.
While both methods enhance efficiency, GemFilter incurs additional overhead due to the need for prefill recomputation, whereas FastKV introduces extra processes for compressing generated KV caches. 
These two factors offset each other.
Consequently, GemFilter's slight advantage in TTFT is primarily attributed to its earlier filter layer (13) compared to the TSP layer (15) for LLaMA-3.1-8B-Instruct.
In contrast, SnapKV, AdaKV, and HeadKV exhibit even longer TTFT than Full KV. This is because they process the full-context information during the prefill stage, similar to Full KV, while also incurring additional overhead from indexing and selecting crucial tokens.

\textbf{Throughput.} 
We measure the throughput for generating a total of 128 tokens. As shown in Figure~\ref{fig:speed}(b), all KV compression techniques result in throughput improvements compared to Full KV.
Both FastKV and GemFilter achieve a significant level of throughput improvement.
FastKV's throughput reaches 5.07$\times$ of Full KV and 1.40$\times$ of HeadKV with 512 KV budget.
In contrast, SnapKV, which stores KV caches of different tokens set for each attention head, shows slightly lower throughput.
Additionally, HeadKV, which allocates varying KV budgets across attention heads, exhibits the lowest throughput among KV cache compression techniques. 
Managing different sequence lengths for keys and values across attention heads increases the complexity of the attention mechanism.
This effect gets stronger when the KV cache size is large, thereby HeadKV experiences steep decline in throughput as KV budget increases, whereas other methods' throughput remain relatively consistent.
AdaKV also adopts fine-grained KV budget allocation similar to HeadKV except that its granularity is attention-group.
AdaKV mitigates the influence of fragmented KV budget to some extent thanks to GQA-aware compression, but it does not achieve the same level of throughput with FastKV or GemFilter.




% \subsection{Speedup Evaluation}

% We compare Time-To-First-Token (TTFT) and token throughput of FastKV and the baseline methods. 
% All results are evaluated on a single-node server with a single NVIDIA A100 GPU. 
% We use the LLaMA-3.1-8B-Instruct model for these experiments.

% \textbf{TTFT.} We evaluate TTFT across various context lengths: 8k, 16k, 32k, 64k, and 128k. 
% The results are depicted in Figure~\ref{fig:speed}(a). 
% FastKV decreases TTFT by up to half compared to Full KV, thanks to Token-Selective Propagation, which maintains a constant computation amount after the TSP layer regardless of the input context length. 
% Specifically, FastKV achieves TTFT that is 1.56$\times$ shorter at a context length of 8k and 1.97$\times$ shorter at 128k compared to Full KV. 

% SnapKV, AdaKV, and HeadKV exhibit longer TTFT than Full KV due to the overhead of indexing and selecting crucial tokens during the prefill stage. 
% Among these, AdaKV has the longest TTFT as it dynamically calculates KV budgets for each attention head at runtime. 
% Therefore, FastKV has TTFT 2.00$\times$ shorter than AdaKV at a context length of 128k.

% GemFilter also reduces TTFT by conducting the prefill stage only with selected tokens. 
% However, its practical usability is limited due to its significant accuracy degradation, despite achieving reductions similar to FastKV.


% \textbf{Token Throughput.} Token throughput is measured for KV budgets of 512, 1024, and 2048, with the input context length fixed at 128k. 
% We average throughput over the generation of 128 tokens after the prefill stage, as shown in Figure~\ref{fig:speed}(b). 
% All methods improve throughput compared to Full KV, albeit to varying degrees.

% FastKV achieves the highest throughput among all methods, reaching up to 2.31$\times$ that of HeadKV.
% Its throughput nearly reaches 60 tokens/sec with a KV budget of 2048, attributed to its compatibility with GQA.

% SnapKV shows lower throughput due to storing KV caches in a repeated form. 
% AdaKV and HeadKV, which allocate varying KV budgets across attention heads, face reduced throughput due to the overhead of handling different sequence lengths for keys and values across attention heads.
% This issue becomes more pronounced as KV budgets grow, causing a steep decline in HeadKV's throughput.

% Notably, FastKV-2048 achieves higher throughput than HeadKV, AdaKV, and SnapKV with a KV budget of 512, while also outperforming them in accuracy on LongBench and Needle-in-a-Haystack.

% GemFilter achieves relatively high throughput due to its natural compatibility with GQA.
% However, this comes at the cost of significantly reduced model capability, limiting its practical application despite achieving throughput comparable to FastKV.

% FastKV is the only method that achieves high throughput without compromising accuracy, demonstrating its ability to balance efficiency and performance effectively.

% \subsection{Ablation Study on TSP Length}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=1.0\linewidth]{images/plength.pdf}
%     % \vspace{-12pt}
%      \vspace{-6mm}
%     \caption{
%         Average LongBench accuracy and TTFT at context length 128k and KV with varying TSP lengths.
%     }
%     \vspace{-2mm}
%     \label{fig:plength}
% \end{figure}


% We conduct an ablation study by varying the TSP length.

% We assess FastKV with TSP lengths of 512, 1024, 2048, 4096, and 8192 tokens with LLaMA-3.1-8B-Instruct. 
% Cases where the TSP length is smaller than the KV budget are excluded, as they are not feasible. 
% We evaluate both TTFT and average accuracy on the LongBench dataset.
% TTFT evaluation is taken with a context length of 128k.

% Shorter TSP lengths of 512 or 1024 result in a significant accuracy drop due to insufficient token propagation. 
% As the TSP length increases to 2048, accuracy improves substantially, with further increases providing negligible gains. 
% This indicates that larger TSP lengths do not meaningfully enhance accuracy for this model.

% TTFT increases as the TSP length grows due to the additional computation required after the TSP layer. 
% A TSP length of 2048 balances efficiency and performance effectively, offering a trade-off between maintaining accuracy and minimizing TTFT.

% These results highlight the importance of selecting an appropriate TSP length. 
% While shorter TSP lengths improve TTFT, they may sacrifice accuracy. 
% Conversely, excessively long TSP lengths offer little accuracy gain while increasing TTFT. 


\section{Conclusion}
In this paper, we introduce FastKV, a novel KV cache compression method designed to improve the efficiency of long-context LLM inference while preserving accuracy. Unlike previous KV cache compression techniques, FastKV enhances computational efficiency by introducing Token-Selective Propagation (TSP). This method strategically retains full-context information in early layers and propagates a compressed subset of tokens in later layers, reducing the computational cost of the prefill stage. Furthermore, our approach is optimized by integrating GQA compatible KV cache compression, which enables more efficient memory and computation management by leveraging group-wise token selection. Our experimental results demonstrate that FastKV achieves higher throughput and lower TTFT than baseline methods while maintaining high accuracy on long-context benchmarks.

% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Reference
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{icml}
\bibliographystyle{icml2025}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\onecolumn

\newpage
% \section{Detailed Experimental Setup}
% \label{appendix:detailed_settings}
% We set the local window size of SnapKV, AdaKV, and HeadKV to 8, as well as FastKV. 
% For every method using pooling, we use average pooling with kernel size 7.
% For AdaKV, we set $\alpha=0.2$. 
% For HeadKV, we set $\alpha=0.2$ and $\beta=1.2$.
% For GemFilter, we choose the filter layer of each model to be the same layer as the TSP layer of FastKV.

%\newpage

% \section{Ablation Study on TSP Length}
% \label{appendix:TSPlength}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.6\linewidth]{images/Fig7.pdf}
%     % \vspace{-12pt}
%      \vspace{-6mm}
%     \caption{
%         Average LongBench accuracy and TTFT at context length 128k and KV with varying TSP lengths.
%     }
%     \vspace{-2mm}
%     \label{fig:plength}
% \end{figure}


% We conduct an ablation study by varying the TSP length. We assess FastKV with TSP lengths of 512, 1024, 2048, 4096, and 8192 tokens with LLaMA-3.1-8B-Instruct. Cases where the TSP length is smaller than the KV budget are excluded, as they are not feasible. We evaluate both TTFT and average accuracy on the LongBench dataset. TTFT evaluation is taken with a context length of 128k.

% Shorter TSP lengths of 512 or 1024 result in a significant accuracy drop due to insufficient token propagation. 
% As the TSP length increases to 2048, accuracy improves substantially, with further increases providing negligible gains. 
% This indicates that larger TSP lengths do not meaningfully enhance accuracy for this model. TTFT increases as the TSP length grows due to the additional computation required after the TSP layer. A TSP length of 2048 balances efficiency and performance effectively, offering a trade-off between maintaining accuracy and minimizing TTFT. These results highlight the importance of selecting an appropriate TSP length. While shorter TSP lengths improve TTFT, they may sacrifice accuracy. Conversely, excessively long TSP lengths offer little accuracy gain while increasing TTFT.

% \clearpage
% \newpage

\section{More Experimental Results}
\label{appendix:more_experimental_results}
\subsection{LongBench}
\label{appendix:longbench}

We provide a full breakdown of the LongBench evaluation results for LLaMA-3.1-8B-Instruct (Table~\ref{tab:longbench_llama_8b}), LLaMA-3.2-3B-Instruct (Table~\ref{tab:longbench_llama_3b}), and Mistral-Nemo-12B-Instruct (Table~\ref{tab:longbench_mistral}). For LLaMA-3.2-3B-Instruct, which has 28 decoder layers, we set the TSP layer and GemFilter filter layer to 13 both.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[h]
\setlength{\tabcolsep}{3pt}
\centering
\caption{LongBench results of LLaMA-3.1-8B-Instruct.}
\label{tab:longbench_llama_8b}
\renewcommand{\arraystretch}{1.0}
\scalebox{0.8}{
\begin{tabular}{@{}!{\color{white}\vrule}lccccccccccccccc!{\color{white}\vrule}@{}}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{3}{c|}{Single-Doc QA} & \multicolumn{3}{c|}{Multi-Doc QA} & \multicolumn{3}{c|}{Summarization} & \multicolumn{3}{c|}{Few-shot Learning} & \multicolumn{2}{c|}{Coding} &  \\ \cmidrule(lr){2-15}
\multicolumn{1}{c|}{\multirow{-2}{*}{\raisebox{0.5cm}[0pt][0pt]{Method}}} & \rotatebox{45}{NrtvQA} &\rotatebox{45}{Qasper} & \multicolumn{1}{c|}{\rotatebox{45}{MF-en}} & \rotatebox{45}{HotpotQA} & \rotatebox{45}{2WikiMQA} & \multicolumn{1}{c|}{\rotatebox{45}{Musique}} & \rotatebox{45}{GovReport} & \rotatebox{45}{QMSum} & \multicolumn{1}{c|}{\rotatebox{45}{MultiNews}} & \rotatebox{45}{TREC} & \rotatebox{45}{TriviaQA} & \multicolumn{1}{c|}{\rotatebox{45}{SAMSum}} & \rotatebox{45}{LCC} & \multicolumn{1}{c|}{\rotatebox{45}{RB-P}} & \multirow{-2}{*}{\raisebox{0.5cm}[0pt][0pt]{\textbf{Avg.}}} \\ \midrule

\multicolumn{16}{c}{LLaMA-3.1-8B-Instruct,   KV Cache Size = Full} \\ \midrule
\multicolumn{1}{l|}{Full   KV} & 30.21 & 45.53 & 55.01 & 56.01 & 46.65 & 31.28 & 35.13 & 25.28 & 27.25 & 73.00 & 91.64 & 43.80 & 63.38 & \multicolumn{1}{c|}{56.64} & \textbf{48.63} \\ \midrule
\multicolumn{16}{c}{LLaMA-3.1-8B-Inustruct,   KV Budget = 128} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 27.39 & 30.40 & 50.35 & 53.04 & 43.49 & 28.89 & 22.87 & 23.00 & 21.74 & 62.00 & 91.19 & 40.31 & 58.43 & \multicolumn{1}{c|}{49.33} & \textbf{43.03} \\
%\multicolumn{1}{l|}{AdaKV} & 27.93 & 30.22 & 50.20 & 51.70 & 40.89 & 29.15 & 23.58 & 23.00 & 21.70 & 67.00 & 91.10 & 40.33 & 58.63 & \multicolumn{1}{c|}{50.07} & \textbf{43.25} \\
\multicolumn{1}{l|}{AdaKV} & 24.90 & 24.41 & 49.95 & 53.15 & 41.73 & 28.55 & 20.54 & 23.21 & 20.28 & 50.50 & 89.49 & 40.71 & 58.74 & \multicolumn{1}{c|}{52.40} & \textbf{41.33} \\
% \multicolumn{1}{l|}{HeadKV-R} & 30.33 & 39.84 & 53.09 & 54.02 & 43.79 & 30.33 & 24.16 & 23.90 & 23.12 & 70.00 & 90.50 & 40.88 & 60.02 & \multicolumn{1}{c|}{52.00} & 45.43 \\
\multicolumn{1}{l|}{HeadKV} & 29.74 & 40.34 & 53.51 & 54.83 & 45.35 & 30.62 & 26.40 & 24.23 & 24.58 & 71.50 & 88.51 & 40.14 & 60.19 & \multicolumn{1}{c|}{52.92} & \textbf{45.92} \\
% \multicolumn{1}{l|}{GemFilter} & 11.78 & 12.00 & 20.35 & 28.26 & 21.41 & 11.96 & 19.75 & 17.25 & 14.54 & 55.00 & 81.38 & 34.55 & 25.84 & \multicolumn{1}{c|}{27.78} & 27.28 \\
\multicolumn{1}{l|}{GemFilter} & 12.50 & 11.00 & 16.52 & 28.84 & 18.82 & 13.53 & 19.66 & 17.67 & 14.34 & 58.17 & 78.51 & 32.99 & 19.92 & \multicolumn{1}{c|}{21.35} & \textbf{25.99} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 27.63 & 27.84 & 52.32 & 52.00 & 45.27 & 28.35 & 22.00 & 22.81 & 20.64 & 69.50 & 91.28 & 40.17 & 59.08 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}48.38} & \textbf{42.66} \\ \midrule
\multicolumn{16}{c}{LLaMA-3.1-8B-Instruct, KV Budget = 512} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 30.09 & 41.62 & 53.89 & 54.77 & 45.12 & 31.22 & 27.37 & 24.16 & 24.86 & 71.00 & 91.90 & 42.43 & 61.53 & \multicolumn{1}{c|}{52.49} & \textbf{46.60} \\
%\multicolumn{1}{l|}{AdaKV} & 30.16 & 40.11 & 54.13 & 54.27 & 44.73 & 31.19 & 27.08 & 23.82 & 24.39 & 71.00 & 91.90 & 41.64 & 60.54 & \multicolumn{1}{c|}{53.15} & \textbf{46.29} \\
\multicolumn{1}{l|}{AdaKV} & 29.07 & 40.16 & 52.44 & 53.90 & 43.05 & 31.10 & 25.75 & 24.39 & 24.85 & 69.00 & 92.34 & 42.05 & 63.43 & \multicolumn{1}{c|}{55.32} & \textbf{46.20} \\
% \multicolumn{1}{l|}{HeadKV-R} & 31.12 & 42.25 & 53.76 & 55.26 & 44.80 & 31.34 & 28.73 & 24.74 & 25.81 & 71.50 & 91.44 & 42.11 & 62.60 & \multicolumn{1}{c|}{54.61} & 47.15 \\
\multicolumn{1}{l|}{HeadKV} & 30.17 & 44.03 & 54.29 & 54.71 & 46.10 & 31.58 & 31.14 & 24.46 & 26.66 & 73.00 & 91.72 & 42.17 & 62.89 & \multicolumn{1}{c|}{55.71} & \textbf{47.76} \\
% \multicolumn{1}{l|}{GemFilter} & 16.33 & 22.02 & 37.75 & 43.78 & 31.13 & 23.49 & 26.43 & 17.40 & 23.42 & 61.00 & 89.55 & 39.98 & 36.76 & \multicolumn{1}{c|}{34.19} & 35.95 \\
\multicolumn{1}{l|}{GemFilter} & 18.52 & 22.15 & 37.73 & 46.88 & 32.56 & 24.52 & 27.05 & 17.75 & 23.25 & 60.50 & 89.49 & 40.16 & 27.35 & \multicolumn{1}{c|}{31.02} & \textbf{35.64} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 30.38 & 41.12 & 55.71 & 54.35 & 46.69 & 30.89 & 26.78 & 23.82 & 24.37 & 72.50 & 92.04 & 42.64 & 62.60 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}52.74} & \textbf{46.90} \\ \midrule
\multicolumn{16}{c}{LLaMA-3.1-8B-Instruct,   KV Budget = 1024} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 31.23 & 42.52 & 53.96 & 55.48 & 45.43 & 31.50 & 29.54 & 24.78 & 26.17 & 70.50 & 91.73 & 42.52 & 62.25 & \multicolumn{1}{c|}{54.72} & \textbf{47.31} \\
%\multicolumn{1}{l|}{AdaKV} & 30.83 & 44.33 & 54.54 & 55.86 & 45.39 & 31.96 & 28.70 & 24.67 & 26.10 & 71.50 & 91.48 & 42.40 & 61.71 & \multicolumn{1}{c|}{54.81} & \textbf{47.45} \\
\multicolumn{1}{l|}{AdaKV} & 29.23 & 44.09 & 53.82 & 54.80 & 44.01 & 31.40 & 28.86 & 24.73 & 26.04 & 72.50 & 91.72 & 42.56 & 63.22 & \multicolumn{1}{c|}{56.33} & \textbf{47.38} \\
% \multicolumn{1}{l|}{HeadKV-R} & 30.79 & 43.99 & 54.85 & 55.81 & 45.63 & 31.86 & 31.02 & 25.24 & 26.63 & 73.00 & 91.43 & 43.02 & 63.20 & \multicolumn{1}{c|}{56.75} & 48.09 \\
\multicolumn{1}{l|}{HeadKV} & 30.55 & 44.66 & 54.69 & 55.47 & 46.20 & 31.63 & 32.88 & 25.14 & 27.10 & 73.00 & 91.57 & 42.88 & 63.76 & \multicolumn{1}{c|}{56.06} & \textbf{48.26} \\
% \multicolumn{1}{l|}{GemFilter} & 20.54 & 33.77 & 46.78 & 52.39 & 43.55 & 25.26 & 29.46 & 19.37 & 25.83 & 61.50 & 91.46 & 40.99 & 43.66 & \multicolumn{1}{c|}{35.66} & 40.73 \\
\multicolumn{1}{l|}{GemFilter} & 19.34 & 34.76 & 46.58 & 46.82 & 42.83 & 27.51 & 29.89 & 18.96 & 25.68 & 63.00 & 90.70 & 42.50 & 38.09 & \multicolumn{1}{c|}{35.14} & \textbf{40.13} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 30.22 & 44.01 & 55.04 & 54.76 & 46.76 & 30.59 & 29.43 & 24.66 & 26.19 & 73.50 & 91.76 & 42.42 & 62.54 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}55.02} & \textbf{47.64} \\ \midrule
\multicolumn{16}{c}{LLaMA-3.1-8B-Instruct,   KV Budget = 2048} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 31.31 & 44.96 & 55.15 & 55.48 & 45.29 & 30.78 & 31.92 & 24.63 & 26.97 & 71.50 & 91.48 & 43.38 & 63.14 & \multicolumn{1}{c|}{56.04} & \textbf{48.00} \\
%\multicolumn{1}{l|}{AdaKV} & 31.03 & 45.39 & 55.22 & 55.10 & 45.88 & 30.31 & 30.96 & 24.46 & 26.93 & 72.00 & 91.65 & 43.76 & 63.02 & \multicolumn{1}{c|}{56.10} & \textbf{47.99} \\
\multicolumn{1}{l|}{AdaKV} & 30.64 & 45.23 & 55.46 & 55.55 & 45.26 & 31.14 & 31.28 & 24.89 & 26.72 & 72.50 & 91.64 & 42.75 & 64.88 & \multicolumn{1}{c|}{59.25} & \textbf{48.37} \\
% \multicolumn{1}{l|}{HeadKV-R} & 31.11 & 44.63 & 55.26 & 55.81 & 46.07 & 31.99 & 32.59 & 25.11 & 27.13 & 72.50 & 91.48 & 43.13 & 63.42 & \multicolumn{1}{c|}{56.30} & 48.32 \\
\multicolumn{1}{l|}{HeadKV} & 30.53 & 44.99 & 54.98 & 55.47 & 46.08 & 31.42 & 32.41 & 25.05 & 27.20 & 72.50 & 91.57 & 42.84 & 63.45 & \multicolumn{1}{c|}{56.75} & \textbf{48.23} \\
% \multicolumn{1}{l|}{GemFilter} & 23.94 & 40.65 & 51.42 & 54.87 & 46.90 & 28.80 & 31.59 & 20.60 & 26.87 & 69.00 & 91.34 & 42.96 & 49.21 & \multicolumn{1}{c|}{39.94} & 44.15 \\
\multicolumn{1}{l|}{GemFilter} & 23.64 & 41.17 & 51.39 & 53.97 & 45.32 & 29.24 & 32.01 & 20.50 & 26.91 & 70.00 & 91.59 & 42.59 & 47.35 & \multicolumn{1}{c|}{38.91} & \textbf{43.90} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 30.31 & 45.41 & 54.79 & 55.11 & 46.57 & 30.46 & 31.25 & 24.82 & 27.02 & 73.50 & 91.48 & 43.66 & 63.17 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}55.82} & \textbf{48.10} \\  \bottomrule
\end{tabular}
}
\end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[h]
\setlength{\tabcolsep}{3pt}
\centering
\caption{LongBench results of LLaMA-3.2-3B-Instruct.}
\label{tab:longbench_llama_3b}
\renewcommand{\arraystretch}{1.0}
\scalebox{0.8}{
\begin{tabular}{@{}!{\color{white}\vrule}lccccccccccccccc!{\color{white}\vrule}@{}}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{3}{c|}{Single-Doc QA} & \multicolumn{3}{c|}{Multi-Doc QA} & \multicolumn{3}{c|}{Summarization} & \multicolumn{3}{c|}{Few-shot Learning} & \multicolumn{2}{c|}{Coding} &  \\ \cmidrule(lr){2-15}
\multicolumn{1}{c|}{\multirow{-2}{*}{\raisebox{0.5cm}[0pt][0pt]{Method}}} & \rotatebox{45}{NrtvQA} &\rotatebox{45}{Qasper} & \multicolumn{1}{c|}{\rotatebox{45}{MF-en}} & \rotatebox{45}{HotpotQA} & \rotatebox{45}{2WikiMQA} & \multicolumn{1}{c|}{\rotatebox{45}{Musique}} & \rotatebox{45}{GovReport} & \rotatebox{45}{QMSum} & \multicolumn{1}{c|}{\rotatebox{45}{MultiNews}} & \rotatebox{45}{TREC} & \rotatebox{45}{TriviaQA} & \multicolumn{1}{c|}{\rotatebox{45}{SAMSum}} & \rotatebox{45}{LCC} & \multicolumn{1}{c|}{\rotatebox{45}{RB-P}} & \multirow{-2}{*}{\raisebox{0.5cm}[0pt][0pt]{Avg.}} \\ \midrule
\multicolumn{16}{c}{LLaMA-3.2-3B-Instruct,   KV Budget = Full} \\ \midrule
\multicolumn{1}{l|}{Full   KV} & 23.41 & 40.60 & 49.79 & 48.71 & 38.89 & 20.54 & 34.07 & 23.65 & 26.67 & 71.50 & 88.89 & 43.20 & 52.13 & \multicolumn{1}{c|}{54.16} & \textbf{44.02} \\ \midrule
\multicolumn{16}{c}{LLaMA-3.2-3B-Instruct,   KV Budget= 128} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 17.92 & 23.18 & 44.59 & 45.35 & 35.08 & 17.07 & 21.96 & 21.05 & 20.38 & 66.00 & 87.55 & 38.39 & 48.31 & \multicolumn{1}{c|}{45.82} & \textbf{38.05} \\
% \multicolumn{1}{l|}{AdaKV} & 18.44 & 22.93 & 45.93 & 45.59 & 34.90 & 17.22 & 22.47 & 21.36 & 20.90 & 68.50 & 88.61 & 39.50 & 48.54 & \multicolumn{1}{c|}{47.47} & \textbf{38.74} \\
\multicolumn{1}{l|}{AdaKV} & 19.94 & 23.15 & 44.50 & 45.17 & 36.51 & 18.68 & 21.86 & 21.46 & 20.57 & 67.00 & 87.82 & 39.26 & 49.39 & \multicolumn{1}{c|}{47.61} & \textbf{38.78} \\
% \multicolumn{1}{l|}{HeadKV-R} & 19.68 & 29.41 & 47.14 & 46.81 & 34.65 & 18.86 & 24.08 & 21.13 & 22.32 & 69.00 & 88.49 & 37.93 & 50.77 & \multicolumn{1}{c|}{48.99} & 39.95 \\
\multicolumn{1}{l|}{HeadKV} & 21.59 & 32.15 & 48.00 & 48.79 & 38.28 & 18.51 & 25.30 & 21.94 & 23.86 & 69.50 & 88.45 & 38.34 & 52.11 & \multicolumn{1}{c|}{50.25} & \textbf{41.22} \\
\multicolumn{1}{l|}{GemFilter} & 10.36 & 10.70 & 26.36 & 35.84 & 27.75 & 11.63 & 19.23 & 17.00 & 13.59 & 55.50 & 71.58 & 28.40 & 19.13 & \multicolumn{1}{c|}{27.46} & \textbf{26.75} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 19.95 & 23.09 & 44.34 & 45.25 & 34.49 & 19.89 & 21.34 & 20.63 & 20.02 & 66.00 & 88.05 & 38.37 & 48.91 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}47.13} & \textbf{38.39} \\ \midrule

\multicolumn{16}{c}{LLaMA-3.2-3B-Instruct, KV Budget = 512} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 21.03 & 32.79 & 49.01 & 48.68 & 37.59 & 20.07 & 25.91 & 22.76 & 24.22 & 69.00 & 88.75 & 41.78 & 51.68 & \multicolumn{1}{c|}{51.25} & \textbf{41.75} \\
% \multicolumn{1}{l|}{AdaKV} & 19.87 & 32.44 & 48.50 & 47.84 & 37.47 & 20.40 & 25.93 & 22.68 & 24.04 & 69.50 & 89.22 & 42.18 & 50.86 & \multicolumn{1}{c|}{51.86} & \textbf{41.63} \\
\multicolumn{1}{l|}{AdaKV} & 21.30 & 32.88 & 49.92 & 47.65 & 37.20 & 19.72 & 25.96 & 22.65 & 24.31 & 69.50 & 89.03 & 41.55 & 51.63 & \multicolumn{1}{c|}{51.87} & \textbf{41.80} \\
% \multicolumn{1}{l|}{HeadKV-R} & 20.90 & 34.62 & 48.55 & 47.76 & 36.51 & 20.52 & 28.47 & 22.43 & 25.42 & 71.00 & 89.27 & 40.54 & 52.08 & \multicolumn{1}{c|}{52.17} & 42.16 \\
\multicolumn{1}{l|}{HeadKV} & 22.79 & 36.45 & 50.57 & 48.85 & 39.16 & 20.44 & 29.79 & 23.24 & 26.10 & 70.50 & 89.66 & 40.80 & 53.72 & \multicolumn{1}{c|}{53.83} & \textbf{43.28} \\
\multicolumn{1}{l|}{GemFilter} & 20.88 & 31.37 & 44.31 & 47.60 & 34.87 & 23.59 & 25.89 & 19.82 & 24.01 & 55.00 & 85.73 & 35.29 & 26.68 & \multicolumn{1}{c|}{32.72} & \textbf{36.27} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 21.73 & 31.55 & 49.88 & 47.31 & 36.54 & 22.14 & 25.47 & 22.38 & 24.05 & 71.50 & 88.99 & 40.91 & 52.46 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}52.17} & \textbf{41.93} \\ \midrule

\multicolumn{16}{c}{LLaMA-3.2-3B-Instruct,   KV Budget = 1024} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 21.71 & 36.22 & 49.04 & 48.78 & 38.14 & 21.21 & 28.32 & 23.25 & 25.62 & 69.00 & 89.03 & 41.68 & 53.01 & \multicolumn{1}{c|}{53.42} & \textbf{42.75} \\
% \multicolumn{1}{l|}{AdaKV} & 20.50 & 36.58 & 50.45 & 48.54 & 39.22 & 20.09 & 27.37 & 23.22 & 25.43 & 70.50 & 89.03 & 42.54 & 52.58 & \multicolumn{1}{c|}{53.02} & \textbf{42.79} \\
\multicolumn{1}{l|}{AdaKV} & 21.69 & 36.91 & 49.76 & 48.10 & 39.27 & 21.25 & 28.01 & 22.78 & 25.66 & 70.00 & 88.96 & 42.62 & 53.00 & \multicolumn{1}{c|}{53.35} & \textbf{42.96} \\
% \multicolumn{1}{l|}{HeadKV-R} & 22.02 & 35.52 & 49.40 & 47.03 & 38.90 & 19.15 & 30.23 & 22.88 & 26.12 & 71.00 & 88.99 & 41.49 & 52.38 & \multicolumn{1}{c|}{53.06} & 42.73 \\
\multicolumn{1}{l|}{HeadKV} & 23.44 & 38.06 & 50.37 & 49.14 & 39.51 & 20.33 & 31.48 & 23.26 & 26.59 & 71.00 & 89.53 & 41.41 & 53.84 & \multicolumn{1}{c|}{54.14} & \textbf{43.72} \\
\multicolumn{1}{l|}{GemFilter} & 21.34 & 33.72 & 48.52 & 50.13 & 33.75 & 20.82 & 28.90 & 20.73 & 24.73 & 61.50 & 89.17 & 38.44 & 29.89 & \multicolumn{1}{c|}{34.05} & \textbf{38.26} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 23.13 & 35.27 & 49.48 & 47.93 & 38.74 & 21.42 & 28.09 & 22.66 & 25.86 & 71.50 & 89.46 & 41.46 & 53.09 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}54.37} & \textbf{43.03} \\ 
\midrule

\multicolumn{16}{c}{LLaMA-3.2-3B-Instruct,   KV Budget = 2048} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 22.89 & 38.70 & 50.91 & 48.38 & 39.29 & 20.55 & 30.79 & 23.17 & 26.46 & 70.50 & 88.89 & 42.25 & 52.16 & \multicolumn{1}{c|}{54.03} & \textbf{43.50} \\
% \multicolumn{1}{l|}{AdaKV} & 23.38 & 38.61 & 51.11 & 48.65 & 39.22 & 20.30 & 29.65 & 23.76 & 26.46 & 71.00 & 88.89 & 42.23 & 52.37 & \multicolumn{1}{c|}{54.30} & \textbf{43.57} \\
\multicolumn{1}{l|}{AdaKV} & 23.43 & 39.28 & 50.26 & 48.61 & 39.60 & 20.82 & 30.39 & 22.94 & 26.65 & 71.00 & 88.89 & 42.57 & 52.33 & \multicolumn{1}{c|}{54.60} & \textbf{43.67} \\
% \multicolumn{1}{l|}{HeadKV-R} & 22.11 & 38.81 & 50.37 & 47.54 & 38.96 & 19.53 & 31.67 & 23.28 & 26.60 & 71.00 & 88.89 & 43.11 & 53.17 & \multicolumn{1}{c|}{53.62} & 43.48 \\
\multicolumn{1}{l|}{HeadKV} & 22.56 & 39.84 & 50.40 & 49.15 & 38.95 & 20.25 & 32.98 & 23.73 & 26.71 & 71.00 & 89.28 & 42.74 & 53.56 & \multicolumn{1}{c|}{55.30} & \textbf{44.03} \\
\multicolumn{1}{l|}{GemFilter} & 17.48 & 38.31 & 49.93 & 50.57 & 40.29 & 19.02 & 30.95 & 21.50 & 26.28 & 64.50 & 90.07 & 40.59 & 37.97 & \multicolumn{1}{c|}{38.88} & \textbf{40.45} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 24.02 & 37.99 & 50.06 & 47.58 & 38.76 & 21.93 & 30.45 & 23.24 & 26.29 & 72.50 & 89.22 & 41.73 & 52.71 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}55.52} & \textbf{43.71} \\

\bottomrule
\end{tabular}
}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[h]
\setlength{\tabcolsep}{3pt}
\centering
\caption{LongBench results of Mistral-Nemo-12B-Instruct.}
\label{tab:longbench_mistral}
\renewcommand{\arraystretch}{1.0}
\scalebox{0.8}{
\begin{tabular}{@{}!{\color{white}\vrule}lccccccccccccccc!{\color{white}\vrule}@{}}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{3}{c|}{Single-Doc QA} & \multicolumn{3}{c|}{Multi-Doc QA} & \multicolumn{3}{c|}{Summarization} & \multicolumn{3}{c|}{Few-shot Learning} & \multicolumn{2}{c|}{Coding} &  \\ \cmidrule(lr){2-15}
\multicolumn{1}{c|}{\multirow{-2}{*}{\raisebox{0.5cm}[0pt][0pt]{Method}}} & \rotatebox{45}{NrtvQA} &\rotatebox{45}{Qasper} & \multicolumn{1}{c|}{\rotatebox{45}{MF-en}} & \rotatebox{45}{HotpotQA} & \rotatebox{45}{2WikiMQA} & \multicolumn{1}{c|}{\rotatebox{45}{Musique}} & \rotatebox{45}{GovReport} & \rotatebox{45}{QMSum} & \multicolumn{1}{c|}{\rotatebox{45}{MultiNews}} & \rotatebox{45}{TREC} & \rotatebox{45}{TriviaQA} & \multicolumn{1}{c|}{\rotatebox{45}{SAMSum}} & \rotatebox{45}{LCC} & \multicolumn{1}{c|}{\rotatebox{45}{RB-P}} & \multirow{-2}{*}{\raisebox{0.5cm}[0pt][0pt]{Avg.}} \\ \midrule
\multicolumn{16}{c}{Mistral-Nemo-12B-Instruct,   Budget = Full} \\ \midrule
\multicolumn{1}{l|}{Full KV} & 26.27 & 43.64 & 58.11 & 49.34 & 45.85 & 26.26 & 31.31 & 24.15 & 26.08 & 75.00 & 89.66 & 44.32 & 68.58 & \multicolumn{1}{c|}{68.11} & \textbf{48.33} \\ \midrule

\multicolumn{16}{c}{Mistral-Nemo-12B-Instruct,   KV Budget = 128} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 21.29 & 34.24 & 51.53 & 48.07 & 44.55 & 23.99 & 19.93 & 20.73 & 20.69 & 69.50 & 89.52 & 39.92 & 62.85 & \multicolumn{1}{c|}{53.08} & \textbf{42.85} \\
% \multicolumn{1}{l|}{AdaKV} & 20.29 & 36.82 & 51.23 & 46.85 & 44.70 & 21.15 & 19.59 & 20.64 & 20.98 & 67.50 & 88.33 & 38.62 & 62.86 & \multicolumn{1}{c|}{52.58} & \textbf{42.30} \\
\multicolumn{1}{l|}{AdaKV} & 22.29 & 36.52 & 52.50 & 47.28 & 44.73 & 21.84 & 19.15 & 20.61 & 20.74 & 65.50 & 88.64 & 39.25 & 63.79 & \multicolumn{1}{c|}{54.07} & \textbf{42.64} \\
% \multicolumn{1}{l|}{HeadKV-R} & 23.46 & 39.56 & 54.15 & 47.91 & 45.98 & 22.78 & 21.66 & 21.40 & 22.30 & 74.50 & 89.78 & 37.71 & 65.94 & \multicolumn{1}{c|}{58.67} & 44.70 \\
\multicolumn{1}{l|}{HeadKV} & 22.62 & 38.76 & 52.54 & 46.49 & 45.97 & 21.80 & 20.74 & 21.37 & 22.10 & 73.00 & 89.42 & 39.16 & 64.92 & \multicolumn{1}{c|}{56.15} & \textbf{43.93} \\
\multicolumn{1}{l|}{GemFilter} & 15.74 & 18.30 & 42.98 & 41.86 & 34.20 & 18.44 & 21.72 & 16.56 & 18.81 & 57.00 & 63.02 & 31.78 & 25.74 & \multicolumn{1}{c|}{30.62} & \textbf{31.20} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 22.26 & 35.09 & 53.33 & 49.15 & 45.63 & 23.67 & 19.15 & 20.80 & 20.47 & 65.00 & 90.06 & 39.88 & 63.65 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}55.68} & \textbf{43.13} \\ \midrule

\multicolumn{16}{c}{Mistral-Nemo-12B-Instruct,   KV Budget = 512} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 23.58 & 40.12 & 55.17 & 47.91 & 45.69 & 25.17 & 23.77 & 22.48 & 24.08 & 74.00 & 89.44 & 43.09 & 68.31 & \multicolumn{1}{c|}{62.15} & \textbf{46.07} \\
% \multicolumn{1}{l|}{AdaKV} & 24.19 & 39.65 & 54.30 & 48.16 & 45.43 & 26.60 & 23.06 & 22.85 & 23.31 & 74.00 & 89.26 & 43.09 & 67.38 & \multicolumn{1}{c|}{62.41} & \textbf{45.98} \\
\multicolumn{1}{l|}{AdaKV} & 24.46 & 41.52 & 57.34 & 47.89 & 45.83 & 25.57 & 23.58 & 22.11 & 23.88 & 74.00 & 89.52 & 43.20 & 68.21 & \multicolumn{1}{c|}{64.15} & \textbf{46.52} \\
% \multicolumn{1}{l|}{HeadKV-R} & 25.83 & 41.93 & 57.87 & 48.32 & 45.57 & 25.53 & 27.03 & 22.68 & 25.04 & 74.50 & 90.09 & 43.62 & 68.62 & \multicolumn{1}{c|}{64.98} & 47.26 \\
\multicolumn{1}{l|}{HeadKV} & 25.88 & 40.57 & 57.12 & 48.11 & 46.11 & 25.26 & 26.19 & 22.92 & 25.25 & 74.50 & 89.74 & 41.81 & 68.79 & \multicolumn{1}{c|}{64.76} & \textbf{46.93} \\
\multicolumn{1}{l|}{GemFilter} & 25.45 & 37.64 & 53.91 & 52.83 & 51.26 & 33.02 & 26.23 & 19.42 & 23.87 & 65.50 & 84.16 & 40.05 & 38.03 & \multicolumn{1}{c|}{41.13} & \textbf{42.32} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 25.51 & 41.48 & 56.76 & 48.73 & 45.43 & 26.53 & 23.42 & 22.15 & 24.02 & 74.00 & 89.42 & 43.22 & 68.61 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}63.57} & \textbf{46.63} \\ \midrule


\multicolumn{16}{c}{Mistral-Nemo-12B-Instruct,   KV Budget = 1024} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 23.27 & 41.42 & 56.01 & 47.85 & 46.09 & 25.83 & 25.97 & 23.70 & 25.37 & 74.00 & 89.82 & 43.48 & 68.46 & \multicolumn{1}{c|}{65.84} & \textbf{46.94} \\
% \multicolumn{1}{l|}{AdaKV} & 24.29 & 41.50 & 55.45 & 47.70 & 46.72 & 26.12 & 24.94 & 22.96 & 24.84 & 74.00 & 89.82 & 44.11 & 68.05 & \multicolumn{1}{c|}{65.29} & \textbf{46.84} \\
\multicolumn{1}{l|}{AdaKV} & 25.33 & 41.82 & 57.52 & 48.02 & 46.48 & 26.34 & 25.68 & 23.12 & 25.26 & 74.50 & 89.82 & 44.65 & 68.49 & \multicolumn{1}{c|}{66.21} & \textbf{47.37} \\
% \multicolumn{1}{l|}{HeadKV-R} & 26.41 & 41.44 & 57.43 & 48.30 & 46.79 & 26.41 & 28.67 & 23.75 & 25.68 & 75.00 & 89.84 & 42.94 & 69.11 & \multicolumn{1}{c|}{66.31} & 47.72 \\
\multicolumn{1}{l|}{HeadKV} & 25.94 & 41.38 & 56.45 & 48.22 & 45.90 & 25.05 & 28.67 & 23.72 & 25.72 & 75.00 & 89.84 & 42.64 & 68.66 & \multicolumn{1}{c|}{66.60} & \textbf{47.41} \\
\multicolumn{1}{l|}{GemFilter} & 26.94 & 40.67 & 53.63 & 55.82 & 56.90 & 38.23 & 28.68 & 20.76 & 25.34 & 69.00 & 87.32 & 42.49 & 44.33 & \multicolumn{1}{c|}{45.41} & \textbf{45.39} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 26.26 & 42.37 & 57.71 & 49.40 & 46.88 & 25.58 & 26.17 & 23.08 & 25.29 & 75.00 & 89.72 & 43.56 & 68.53 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}65.79} & \textbf{47.52} \\ \midrule

\multicolumn{16}{c}{Mistral-Nemo-12B-Instruct,   KV Budget = 2048} \\ \midrule
\multicolumn{1}{l|}{SnapKV} & 24.35 & 42.95 & 56.92 & 49.08 & 46.43 & 26.26 & 28.27 & 23.68 & 26.07 & 75.00 & 89.82 & 44.20 & 68.89 & \multicolumn{1}{c|}{68.00} & \textbf{47.85} \\
% \multicolumn{1}{l|}{AdaKV} & 24.67 & 42.46 & 56.05 & 48.60 & 45.97 & 26.23 & 26.58 & 23.39 & 25.74 & 75.00 & 89.66 & 44.09 & 68.25 & \multicolumn{1}{c|}{67.70} & \textbf{47.46} \\
\multicolumn{1}{l|}{AdaKV} & 26.02 & 43.08 & 57.43 & 48.76 & 46.53 & 26.12 & 27.80 & 23.44 & 25.93 & 75.00 & 89.66 & 44.49 & 68.53 & \multicolumn{1}{c|}{67.57} & \textbf{47.88} \\
% \multicolumn{1}{l|}{HeadKV-R} & 26.46 & 42.03 & 57.67 & 48.92 & 46.51 & 25.87 & 30.08 & 24.33 & 26.15 & 75.00 & 89.66 & 44.29 & 68.67 & \multicolumn{1}{c|}{67.38} & 48.07 \\
\multicolumn{1}{l|}{HeadKV} & 26.05 & 42.15 & 57.46 & 48.49 & 45.85 & 25.86 & 30.27 & 24.22 & 26.17 & 75.00 & 89.66 & 43.74 & 68.64 & \multicolumn{1}{c|}{67.29} & \textbf{47.92} \\
\multicolumn{1}{l|}{GemFilter} & 26.42 & 42.40 & 56.98 & 57.64 & 53.92 & 34.33 & 30.51 & 21.60 & 25.96 & 72.00 & 89.65 & 44.48 & 48.34 & \multicolumn{1}{c|}{48.06} & \textbf{46.59} \\
\rowcolor[HTML]{DAE8FC} 
\multicolumn{1}{l|}{\cellcolor[HTML]{DAE8FC}FastKV} & 26.61 & 43.37 & 57.12 & 49.41 & 47.22 & 26.27 & 28.40 & 23.22 & 25.81 & 75.00 & 89.72 & 43.04 & 68.67 & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}67.61} & \textbf{48.11} \\
\bottomrule
\end{tabular}
}
\end{table*}

\clearpage

%\newpage
%\subsection{InfiniteBench}
%\label{appendix:infinitebench}

\newpage
\subsection{Needle-in-a-Haystack}
\label{appendix:niah}

We performed further Needle-in-a-Haystack evaluations with varying KV budgets for LLaMA-3.1-8B-Instruct (Figure \ref{fig:niah_llama-3.1}), LLaMA-3.2-3B-Instruct (Figure \ref{fig:niah_llama-3.2}), and Mistral-Nemo-12B-Instruct (Figure \ref{fig:niah_mistral}). For LLaMA-3.1-8B-Instruct, the overall retrieval trend remains consistent. FastKV continues to achieve strong performance across multiple budgets, often matching or surpassing baseline methods, including HeadKV.

By contrast, LLaMA-3.2-3B-Instruct produces notably lower scores overall, even when using Full KV. Its smaller parameter count appears to limit retrieval capabilities at long contexts, making it more difficult for fine-grained KV allocation techniques to improve performance. Although HeadKV excelled with LLaMA-3.1-8B-Instruct, it does not show similar gains on LLaMA-3.2-3B-Instruct, suggesting that certain highly flexible optimizations may not translate effectively to smaller models. 
%FastKV also shows diminished improvements in this setting, largely because the model’s baseline retrieval capacity is relatively constrained, overshadowing the benefits of compression strategies.
Nonetheless, FastKV outperforms SnapKV, HeadKV, and GemFilter, and achieves an average score on par with AdaKV, indicating that its core compression strategy retains efficacy when applied to a compact model.


Mistral-Nemo-12B-Instruct exhibits a shorter effective context window than the nominal 128k, frequently struggling to retrieve needles located at extreme depths while handling nearer contexts reasonably well. Surprisingly, GemFilter significantly outperform Full KV, and FastKV also records above-Full-KV scores. GemFilter discards a subset of tokens and restarts its prefill stage with the reduced sequence, and FastKV continues prefilling the remaining layers with reduced hidden states after TSP. This may lead temporary enhancement of retrieval by focusing attention on fewer tokens. However, GemFilter’s poor results in LongBench confirm that these gains do not generalize to complex tasks and models. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{images/appendix_NIAH_LLaMA-31.png}
     \vspace{-2mm}
    \caption{
        Needle-in-a-Haystack results of LLaMA-3.1-8B-Instruct.
    }
    \vspace{-2mm}
    \label{fig:niah_llama-3.1}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{images/appendix_NIAH_LLaMA-32.png}
     \vspace{-6mm}
    \caption{
        Needle-in-a-Haystack results of LLaMA-3.2-3B-Instruct.
    }
    \vspace{-2mm}
    \label{fig:niah_llama-3.2}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{images/appendix_NIAH_Mistral-Nemo.png}
     \vspace{-6mm}
    \caption{
        Needle-in-a-Haystack results of Mistral-Nemo-12B-Instruct.
    }
    \vspace{-2mm}
    \label{fig:niah_mistral}
\end{figure*}

\clearpage
\newpage

\subsection{Latency and Throughput Evaluation}
\label{appendix:speedup}

We present the TTFT and the throughput results of the baselines and FastKV in Table~\ref{tab:ttft} and Table ~\ref{tab:throughput}. The throughput results of Full KV is depicted in Table ~\ref{tab:fullkv_throughput} for effective comparison. The throughput results of KV cache compression methods are evaluated with input context length 128k.  As the input context length increases, the relative TTFT gain of FastKV compared to SnapKV, AdaKV and HeadKV increases. For Mistral-Nemo-12B-instruct, FastKV achieves TTFT similar to GemFilter because the TSP layer index is same with the GemFilter filter layer. The token throughput gain of the baselines and FastKV using fixed KV budget grows rapidly as the throughput of Full KV declines due to the increasing size of KV cache. FastKV consistently achieves the highest throughput across all scenarios.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[h]
\setlength{\tabcolsep}{14pt}
\centering
\caption{TTFT (sec) comparison of baselines and FastKV.}
\label{tab:ttft}
\renewcommand{\arraystretch}{0.85}
\scalebox{0.9}{
\begin{tabular}{@{}!{\color{white}\vrule}l|ccccc!{\color{white}\vrule}@{}}
\toprule
\multicolumn{1}{c|}{} & \multicolumn{5}{c}{Context Length} \\ \cmidrule(l){2-6} 
\multicolumn{1}{c|}{\multirow{-2}{*}{Method}} & 8k & 16k & 32k & 64k & 128k \\ \midrule
\multicolumn{6}{c}{LLaMA-3.1-8B-Instruct} \\ \midrule
Full KV & 0.64 & 1.41 & 3.40 & 9.05 & 27.94 \\ \midrule
SnapKV & 0.67 & 1.45 & 3.49 & 9.22 & 28.27 \\
AdaKV & 0.75 & 1.53 & 3.57 & 9.31 & 28.36 \\
HeadKV & 0.70 & 1.48 & 3.51 & 9.25 & 28.30 \\
% GemFilter & 0.48 & 0.86 & 1.99 & 4.69 & 14.14 \\
GemFilter & 0.44 & 0.78 & 1.66 & 4.15 & 12.51 \\
\rowcolor[HTML]{DAE8FC} 
FastKV & 0.41 & 0.80 & 1.81 & 4.66 & 14.15 \\ \midrule
\multicolumn{6}{c}{Mistral-Nemo-12B-Instruct} \\ \midrule
Full KV & 0.92 & 2.04 & 4.85 & 12.71 & 37.94 \\ \midrule
SnapKV & 0.95 & 2.08 & 4.90 & 12.80 & 38.11 \\
AdaKV & 1.05 & 2.18 & 5.01 & 12.92 & 38.23 \\
HeadKV & 0.99 & 2.12 & 4.95 & 12.85 & 38.18 \\
GemFilter & 0.69 & 1.25 & 2.65 & 6.59 & 19.22 \\
\rowcolor[HTML]{DAE8FC} 
FastKV & 0.59 & 1.16 & 2.57 & 6.53 & 19.22 \\ \bottomrule
\end{tabular}
}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\setlength{\tabcolsep}{7pt}
\centering
\caption{Throughput (Tokens/sec) with full KV budget at varying context length.}
\vspace{2mm}
\label{tab:fullkv_throughput}
\renewcommand{\arraystretch}{0.85}
\scalebox{0.9}{
\begin{tabular}{@{}!{\color{white}\vrule}c|ccccc!{\color{white}\vrule}@{}}
\toprule
Context Length & 8k & 16k & 32k & 64k & 128k \\ \midrule
LLaMA-3.1-8B-Instruct & 51.55 & 43.07 & 32.29 & 21.24 & 12.60 \\ \midrule
Mistral-Nemo-12B-Instruct & 36.10 & 30.88 & 23.67 & 16.07 & 9.68 \\
\bottomrule
\end{tabular}
}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table}[h]
\setlength{\tabcolsep}{25pt}
\centering
\caption{Throughput (Tokens/sec) comparison of baselines and FastKV.}
\vspace{2mm}
\label{tab:throughput}
\renewcommand{\arraystretch}{0.85}
\scalebox{0.9}{
\begin{tabular}{@{}!{\color{white}\vrule}l|ccc!{\color{white}\vrule}@{}}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{KV Budget} \\ \cmidrule(l){2-4} 
\multicolumn{1}{c|}{} & 512 & 1024 & 2048 \\ \midrule
\multicolumn{4}{c}{LLaMA-3.1-8B-Instruct} \\ \midrule
SnapKV & 48.71 & 48.62 & 48.29 \\
% AdaKV & 36.92 & 31.68 & 26.35 \\
AdaKV & 50.57 & 49.53 & 49.47 \\
HeadKV & 45.64 & 39.48 & 30.78 \\
GemFilter & 63.40 & 62.32 & 60.37 \\
\rowcolor[HTML]{DAE8FC} 
FastKV & 63.84 & 62.97 & 60.74 \\ \midrule
\multicolumn{4}{c}{Mistral-Nemo-12B-Instruct} \\ \midrule
SnapKV & 38.04 & 38.03 & 35.44 \\
AdaKV & 28.53 & 28.34 & 27.82 \\
HeadKV & 27.20 & 26.97 & 24.92 \\
GemFilter & 41.94 & 41.69 & 40.17 \\
\rowcolor[HTML]{DAE8FC} 
FastKV & 42.01 & 41.85 & 41.49 \\
\bottomrule
\end{tabular}
}
\end{table}





% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2k. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
