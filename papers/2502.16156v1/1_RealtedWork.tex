\section{Related Work} \label{sec::related_work}
Many reviews have examined causality or decision-making, but to the best of our knowledge, they typically focus on an individual paradigm or an individual task, with some emphasizing methodologies without clearly delineating the fundamental connections between causality and decision-making. In contrast, this review offers a unified framework that integrates all key steps, explicitly illustrating the role of causality and the relationships across different stages of the decision-making process. A detailed discussion of related surveys follows.

%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Causal Inference and Causal Structural Learning.} In recent years, several review papers have emerged in the field of causal inference, typically categorized into two main frameworks: the \acrfull{SEM} framework, introduced by Pearl \citep{pearl1995causal}, and the potential outcomes framework, pioneered by Rubin \citep{rubin1974estimating, rubin1978bayesian}.

The \acrshort{SEM} framework models causal relationships using graphical structures, where nodes represent variables and directed edges depict cause-effect relationships. Pearlâ€™s work \citep{pearl2003statistics} was instrumental in developing the do-calculus to formalize the effect of interventions on causal diagrams. Subsequent review papers \citep{pearl2009causal, pearl2010causal, pearl2010foundations} also provide comprehensive overviews of \acrshort{CSL}, with detailed discussions on related topics such as confounding issues and and mediation analysis. To the best of our knowledge, there is no recent review that comprehensively summarizes the latest advances in causal discovery under \acrshort{SEM}, which is one of our focus in Section \ref{Sec:CSL}.

The potential outcome framework, also known as the \acrfull{RCM} \citep{rubin1974estimating}, defines causal effects by comparing potential outcomes under different treatment conditions. A key resource in this area is \citet{imbens2015causal}, which systematically summarizes the origins and development of causal inference with the \acrshort{RCM}, covering topics from estimation and inference to sensitivity analysis. Recent reviews have also addressed specific aspects of causal inference, including observational studies \citep{yao2021survey}, matching methods \citep{stuart2010matching}, handling missing data \citep{ding2018causal}, and addressing confounding in text analysis \citep{keith2020text}. This part of the work closely aligns with \acrshort{CEL}, where previous studies may fall short in comprehensively summarizing effect learning across different data structures and paradigms. Our work addresses this gap by incorporating scenarios where assumptions are both satisfied and violated, positioning it as a critical intermediate task in the decision-making process.

\noindent 
\textbf{Policy Learning.} In the offline policy learning area, the related review papers can be classified as focusing on \acrfull{OPE} and \acrfull{OPO}.
For \acrshort{OPE}, 
\citet{voloshin2019empirical} systemetically studys the empirical performance of a list of common \acrshort{OPE} methods for the offline \acrfull{RL} setting (i.e. paradigm 2). 
\citet{uehara2022review} is the latest review of the key methods and theories in \acrshort{OPE}, covering paradigms 1-3. For \acrshort{OPO}, \citet{prudencio2023survey} and  \citet{Sergey2020offlineRL} both review the key concepts, methods and open problems in offline \acrshort{RL} (paradigm 2). Besides, from the statistics perspective, \citet{kosorok2019precision} comprehensively reviewed the progress of applying \acrshort{DTR} to precision medicine, covering paradigms 1 and 3. 

In contrast, most reviews on online policy learning focus on policy optimization, with online policy evaluation being a newer and less explored area. For policy optimization in paradigm 4, \citet{lattimore2020bandit} and \citet{slivkins2024introductionmultiarmedbandits} offer the most recent comprehensive texts on bandit algorithms. These works cover a broad range of topics, with a particular emphasis on algorithm design and regret analysis, including stochastic bandits, adversarial bandits, contextual bandits, etc.  
In the broader context of online policy learning, problems modeled as \acrshort{MDP}s (paradigm 5) are typically studied through \acrshort{RL}. \citet{shakya2023reinforcement} offers a thorough overview of \acrshort{RL} fundamentals, while \citet{wang2022deep} and \citet{arulkumaran2017brief} focus on RL's integration with deep learning. \citet{gu2022review} reviews \acrshort{RL} methods designed to address safety concerns in real-world applications, and \citet{canese2021multi} and \citet{gronauer2022multi} review multi-agent RL. For more complex settings like \acrshort{POMDP}s (paradigm 6), \citet{xiang2021recent} provides a detailed review of recent advances. However, these reviews generally overlook the relationship between causality and policy learning.

Recently, the integration of causal knowledge into policy learning has garnered growing attention, leading to the emergence of the field of causal \acrshort{RL}. For example, \citet{scholkopf2021toward} briefly discusses the role and importance of causality in \acrshort{RL}, with a main focus on causal representation learning. \citet{kaddour2022causal} examined causal machine learning, including a brief chapter summarizing how \acrshort{RL} can benefit from exploiting causal paradigms. Additionally, \citet{grimbly2021causal} reviewed causal multi-agent \acrshort{RL}, and \citet{bannon2020causality} focused on causality in batch \acrshort{RL}. The reviews by \citet{zeng2023survey} and \citet{deng2023causal} are the most comprehensive, outlining how causal knowledge from causal discovery and causal inference can address challenges faced by non-causal \acrshort{RL} and systematically reviewing existing causal \acrshort{RL} methods.

%Overall, compared with these related review papers, our biggest difference is on the scope and focus: {we do not focus on the technical details of one task in one paradigm, but instead provide a framework for both tasks covering paradigms 1-3; and we empahsize the strong connections with causality and CEL. }

%Despite the breadth of these surveys, they focus solely on online policy learning, neglecting its connection to causality and recent advances in incorporating causal techniques.

%However, these works primarily focus on subfields of \acrshort{CDM}, specifically the \acrshort{CPL} within our taxonomy. In contrast, this study aims to provide a broader perspective, offering a comprehensive and structured overview of the entire field of \acrshort{CDM} by covering various stages and identifying classic problem structures.

%survey papers have reviewed advances in causal \acrfull{RL}, focusing on the integration of causal knowledge into reinforcement learning. 
%%%%%%%%%%%%%%%%%%%%%

