\begin{enumerate}
    \item \textbf{with prior Causal Information}: There are two common objectives: 1) eliminate the confounding bias for scenario with latent confounders, and 2) sample efficiency, causal explanation and generalization for unconfounded scenarios

    \begin{enumerate}
    \item MAB with single intervention variable
        \begin{enumerate}
            \item eliminating the confounding bias (with unobserved confounders)
            \begin{enumerate}
                \item "Bandits with unobserved confounders: A causal approach": [known causal model (graph+distribution)] a new bandit problem with unobserved confounders using causal graphical representation (the unobserved confounder affect both the action and the outcome). Developing an optimization metric employing both experimental and observational quantities (Is the metric just the effect of the treatment on the treated group?), based on which a Causal TS algorithm is proposed.
                \item "Counterfactual data-fusion for online reinforcement learners": the same motivation as the previous one, but proposed a counterfactual-based decision making. generating counterfactual experiences, then combined the observational, experimental, and counterfactual data to learn the environments in the presence of latent confounder. the assumed causal structure is slightly different from the one assumed in the previous paper, in that there is an intent action $I_t$ between $U_t$ and the policy $\pi_t$.
            \end{enumerate}

        \item sample efficiency
            \begin{enumerate}        
                \item “Regret analysis of bandit problems with causal background knowledge”: [known causal model (graph+distribution)] C-UCB and C-TS, assuming the distribution of parents of the reward for each intervention is known. The proposed method aim to reduce the amount of needed exploration. 
                 \item “Budgeted and non-budgeted causal bandits”: [known causal model (graph+distribution)] $\gamma$-NB-UCB and CRM-NB-ALG for bandits with budget constraint; C-UCB-2 for cases without budget constraint, has the same assumption as the previous one (what is the difference between C-UCB and C-UCB-2 then??????)
            \end{enumerate}
        \end{enumerate}
    \item Contextual bandits
        \begin{enumerate}
            \item sample efficiency (major) \& eliminating confounding bias
            \begin{enumerate}
                \item “Causal contextual bandits with targeted interventions”: standard contextual bandits relies on large sample interactions for each arm, which is costly. Proposed a causal CB to show that even the experiments of each arm is only conducted on a subset of population, the learned reward can still be used to inform an optimal policy regarding the whole population, given casual-side information
                \item \textcolor{red}{“Unifying offline causal inference and online bandit learning for data driven decision”}: utilized the inform from logged data to improve the accuracy. (both context-free and context-dependent, consider the unobserved confounder but not focus on the UC bias): While an AB test may make wrong decisions that harm the user experience and create irreparable damage, offline causal inference is not adaptive to new incoming data. As a result, the authors proposed unifying offline causal inference and online bandit learning. Specifically, they used the counterfactual estimation using offline logged data to assist in warming up the online bandit algorithm.
                \item A Thompson Sampling Approach to Unifying Causal Inference and Bandit Learning: a TS variant as an extension of the above one
                \item “Transfer learning in multi-armed bandit: a causal approach”: transferring knowledge across bandit agents. the first to connect CEL algo and the OPE algo. (also consider the unobserved confounder)
            \end{enumerate}
            \item fairness
            \begin{enumerate}
                \item Achieving Counterfactual Fairness for Causal Bandit: incorporating causal inference into bandits
        
            \end{enumerate}
        \end{enumerate}
    \item SCM-MAB (combinatorial causal bandits): SCM-MAB is a type of MAB, in which there are multiple manipulable intervention variables
        \begin{enumerate}
        \item assume known causal model and no unobserved confounders: somehow show the sample efficiency as the extra causal information can help learn a better policy more quickly
        \begin{enumerate}
            \item “Causal bandits: Learning good interventions via causal inference”: the similar causal bandit problem as the first one. agent can control the allowed actions, estimate the optimal action assuming the arbitrary causal graph was given
            \item “Identifying best interventions through online importance sampling”: considering the same problem as the above one. an efficient successive rejects algorithm with importance sampling, assuming known causal structure.
        \end{enumerate} 
        \item assume known causal structure and no unobserved confounder
        \begin{enumerate}
            \item “Causal bandits with propagating inference”: all previous considered localized interventions. here, propagating inference to propagating throughout the given causal graph.
            \item Causal Bandits for Linear Structural Equation Models
        \end{enumerate}
        \item assume known causal structure and address unobserved confounders
        \begin{enumerate}
            \item “Structural causal bandits: where to intervene?”: explore the possibly optimal minimal intervention set (assuming all observed variables are manipulable)
            \item “Structural causal bandits with non-manipulable variables”: further address the possibility that only a subset of intervention variables is manipulable.
            \item “Characterizing optimal mixed policies: Where to intervene and what to observe”: defined the mixed policy, which determining where to intervene and what to observe. first, find the class of \textbf{non-redundant} policies; then, take the policy with optimal performance. \textbf{The non-redundancy ca help faster the convergence to the optimum}
            \item A Causal Bandit Approach to Learning Good Atomic Interventions in Presence of Unobserved Confounders
            \item Combinatorial causal bandits: provide theoretical analysis compared to the first three papers, which focus on empirical study
        \end{enumerate}
        \end{enumerate}

    \item Semi-parametric bandits + causal inference (Rubins): Semi-parametric Bandits: semi-parametic bandits is proposed to bridge the non-contextual MAB and the contextual bandits, by partially modeling the reward function while leaving some components flexible. However, they easily suffer from increased computational complexity and potentially greater difficulty in obtaining reliable estimates, which an integration with causal inference can help address.
        \begin{enumerate}
        \item optimizing treatment effect instead of reward
            \begin{enumerate}
            \item \textcolor{red}{Contextual Multi-Armed Bandits for Causal Marketing} [single-stage heterogeneous treatment effect estimators + Online Bandits]: The authors estimate and optimize the Conditional Average Treatment Effect (CATE) (i.e., $\tau_{a} = Y(A = a)- Y(A\neq a)$) instead of modeling and maximizing $Y(A = a)$. Primarily, it is driven by the necessity for practical applications, such as advertising, that seek to optimize the return on investment; it is wasteful to send advertisements to customers who already possess a high probability of making a purchase.
            \item An Application of Causal Bandit to Content Optimization
            \item Mitigating Targeting Bias in Content Recommendation with Causal Bandits
            \item \textcolor{red}{Flexible and efficient contextual bandits with heterogeneous treatment effect oracles} [R-learner + contextual bandits]: Unlike a), it is inspired by the finding that 1) genuine reward can contain action-independent redundancy that are irrelevant for decision making and 2) treatment effect estimate bias is no greater than reward model estimation bias under the same model class. It is assumed that $Y(a) = g(x,a) + h(x)$, with $h()$ representing a confounder model and $g()$ representing a true treatment effect model. R-learner is used to estimate $g()$; the likelihood of an action being selected is defined as a function of $g()$; an action with a higher $g(x,a)$ is more likely to be played.
            \end{enumerate}
        \item Experimental Design
            \begin{enumerate}
            \item Synthetically controlled bandits: it used bandits to help traditional synthetic control method saving experimental cost (i.e., if observed reward - estimated reward that would be observed if no treatment for the unit i is negative, then the treatment is discontinued for unit i) 
            \item Multi-armed Bandit Experimental Design: Online Decision-making and Adaptive Inference
            \end{enumerate}
        \end{enumerate}
    \end{enumerate}


    \item \textbf{With no prior causal information}: 
    \begin{enumerate}
    \item causal explanation
    \begin{enumerate}
        \item "Playing against nature: causal discovery for decision making under uncertainty": three cases are considered: 1) the causal model is fully known; 2) only the causal structure is known; 3) the causal model is fully unknown. Focusing on how to acquire causal information from continuous iteration. 

    \end{enumerate}
    \item Causal Bandits with Unknown Graph Structure
    \item Causal Bandits without prior knowledge using separating sets.
    \item Adaptively Exploiting d-Separators with Causal Bandits.
    \item Causal discovery for causal bandits utilizing separating sets
    \item Causal bandits without graph learning
    \end{enumerate}
\end{enumerate}


