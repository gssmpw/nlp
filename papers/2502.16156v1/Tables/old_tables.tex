\begin{table}
\begin{tabular}{ | m{2cm} | m{2.5cm}| m{2.5cm} | m{2.5cm} |m{1.7cm} |m{2cm} | m{2cm} | } 
% \begin{tabular}{ | | |  |  || | } 
  \hline
  Study & State & Action & Reward & Algorithm & Data & Contribution\\ 
  \hline

\citet{ chan2001electronic} & inventory of agent, order imbalance, bid-ask spread and some other market quantity measures functions of price change, etc.)& change in bid and ask prices & a linear combination of profit, inventory risk and
market quality & SARSA and actor-critic algorithm with policy by Boltzmann distribution & Using the simulator of their own & The first to apply RL in market making
\\
\hline


  \citet{Patel2018MM} & Macro agent: historical price data; Market Indicator Features (Price Level, Price Change, Volume Level,
Volume Change, Volatility); Micro agent: time, inventory, end-to-end market information & Macro agent: Buy, sell, or hold the asset; Micro agent: price of limit orders & Macro agent: clipped reward of profit based on decision; Micro agent: profit based on price difference & DQN & Bitcoin data based on historical orderbook& Propose a novel hierarchical decision structure for Market Making Agent\\ 
  \hline
  
  % \citet{Patel2018MM} & Macro agent: historical price data; Market Indicator Features (Price Level, Price Change, Volume Level,Volume Change, Volatility) & Macro agent: Buy, sell, or hold  & Macro agent: clippted reward based on decisionprice information, inventory information, volatility; micro agent: bid-ask information, trade information & Macro agent: Buy, sell, or hold ; micro agent: price of limit order & Macro agent: clippted reward based on decision;  micro agent:  a function of Volume Weighted Average Price& DQN & Bitcoin data based on historical orderbook& Propose a novel hierarchical decision structure for Mmarket Mmaking Aagent\\ 
  % \hline

  
  \citet{Spooner2018MM} & Inventory, active quoting distances, and 
market states (market spread, mid-price move,
book/queue imbalance, signed volume, 
volatility, relative strength index) & Place limit orders at fixed distances to the mid-price; Clear inventory  &  PnL from placing orders and inventory PnL, with asymmetrically dampened reward function designed to encourage making orders rather than profiting from speculation (keeping the inventory) & Q-learning, 
SARSA 
and R-learning & Data from their simulator, which is developed based on historical data & Propose the asymmetrically dampened reward function\\
%   \hline
%   \citet{spooner2020robust} & Current time and the agent's inventory &   Quoted spread and reservation price & Mark-to-market
% (MtM) value of the agent's holding (include both profit of executing orders and value change of the holding) &  NAC-S($\lambda$) algorithm & Data generated by a simulator & Proposes adversarial RL in market making\\
%   \hline
%   \citet{gavsperov2021market} &  Agent’s inventory; price range and trend predictions  & placing bid/ask orders with distances in tick size
% relative to the current best bid/ask  & symmetrically dampened PnL reward function in \cite{Spooner2018MM}, and inventory penalizing term & Policy gradient with fully-connected neural network & Bitstamp
% for the pair Bitcoin/US Dollar
% \\
  \hline
  
  \citet{Ganesh2019MM} & Trades executed in previous time step; Inventory; Reference 
mid-price and spread curves; Market share; & Pricing; Hedging;  & Total PnL with different penalty for inventory PnL & PPO & Data generated by their own simulator & Introduce a competing MM agent and let RL agent to learn its behavior\\ 
  \hline
\end{tabular}




\caption{RL Methods in Market Making}
\label{table:market_making}
\end{table}

\begin{table}[]
    \label{table:market_making_2}
    \caption{RL Methods in Market Making}
    \centering
\begin{tabular}{| m{2cm} | m{2.5cm}| m{2.5cm} | m{2.5cm} |m{1.7cm} |m{2cm} | m{2cm} |} 
  \hline
  Study & State & Action & Reward & Algorithm & Data & Contribution\\
  \hline
  \citet{mani2019applications} & volume imbalance and the market-maker’s bid-ask spread &changes in the
market-maker’s ask and bid quotes &a linear function of profit, inventory penalty 
and market-maker’s bid-ask spread & SARSA and Double SARSA & Using the simulator of their own & Use risk sensitive RL in market making
\\
  \hline
  \citet{spooner2020robust} & Current time and the agent's inventory &   Quoted spread and reservation price & Mark-to-market
(MtM) value of the agent's holding with penalty term of terminal inventory &  NAC-S($\lambda$) algorithm & Data generated by a simulator & Proposes adversarial RL in market making\\
  \hline
  \citet{gavsperov2021market} &  Agent’s inventory; price range and trend predictions  & placing bid/ask orders with distances in tick size
relative to the current best bid/ask  & symmetrically dampened PnL reward function in \cite{Spooner2018MM}, and inventory penalizing term & Policy optimization by genetic algorithms & Bitstamp
for the pair Bitcoin/US Dollar& Use  signal generating unit to predict price change
\\
\hline

\citet{selser2021optimal} & mid-price of the asset, inventory of the agent,  and time & (discretized) bid price and ask price &a profit minus the estimated variance of profit with a constant & Deep Q-Learning and Tabular Q-Learning & Using the simulator of their own & Use a mean minus variance formulation of reward to recover the optimal agent  
\\
\hline

\citet{haider2021gaussian} & volatility, relative strength index,
book imbalance, inventory, ask level and bid level & Place limit
orders at
fixed distances
to the mid-
price; Clear
inventory & a variant of asymmetrically dampened reward function proposed in \citet{Spooner2018MM} & Gaussian distribution based nonlinear
function approximation (GBNLFA, a variant of SARSA) & Data from their simulator, which is developed based on historical data & Propose the RL algorithm GBNLFA to handle function approximation from continuous states to discrete actions
\\
\hline


\end{tabular}
\end{table}


\begin{table}[H]
\caption{Surveyed paper for Reinforcement Learning in Portfolio Management}
\label{table:multi_stage_portfolio_management}
\begin{tabular}{| m{2cm} | m{3cm}| m{2.5cm} | m{2.5cm} |m{1.7cm} |m{2.3cm} | }
\hline
Application & State& Action & Reward & Algorithm& Data \\ \hline
\cite{jiang2017deep} & Prices, Portfolio& Asset Allocation Weight & Return& Policy Gradient& Cryptocurrency \\ \hline
\cite{yu2019model}& Prices, Portfolio, Market Indicators, Predictions & Asset Allocation Weight & Return& DDPG& Stock \\ \hline
\cite{zhang2020deep} & Prices, Portfolio, Technical Indicators& Long/Hold/Short & Volatility-Adjusted Return & PG, DQN, A2C & Futures \\ \hline
\cite{ye2020reinforcement} & Prices, Predictions & Asset Allocation Weight & Return& Policy Gradient& Cryptocurrency, Stock \\ \hline
\cite{nan2020sentiment} & Prices, Portfolio, Predictions & Long/Hold/Short & Return& DQN & Stock \\ \hline
\cite{wang2021deeptrader} & Prices, Market Indicators, Predictions & Asset Allocation Weight & Return& Policy Gradient& Stock \\ \hline
\cite{wang2021commission} & Prices, Portfolio, LOB, Predictions & Asset Allocation Weight, Long/Hold/Short & Return& Reinforce, Double-Q & Stock \\ \hline
\cite{benhamou2020aamdrl} & Prices, Market Indicators & Asset Allocation Weight & Return& Policy Gradient& Portfolio, Strategies \\ \hline
\cite{benhamou2021detecting} & Prices, Portfolio, Market Indicators & Asset Allocation Weight & Return& Policy Gradient& Portfolio, Strategies \\ \hline
\end{tabular}
\end{table}


\input{Tables/table_optimal_execution}
