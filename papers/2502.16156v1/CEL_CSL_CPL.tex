%\section{Connections}
%\section{Bridging CSL and CEL}
% sub

\section{Causal Assumptions Violated Scenarios}\label{sec:assump_violated}
In the sections above, much of the literature proceeds under the causal identification assumption outlined in Section \ref{sec:prelim_assump}. However, real-world scenarios sometimes violate these assumptions. 
For instance, the \acrshort{SUTVA} assumption assumes that a unit's outcome isn't influenced by the treatment of other units. 
Yet, for example, in the spread of COVID-19, if we simply assume each individual as one unit, the \acrshort{SUTVA} is violated as each patient's health can be affected by the immunization status of the entire community; 
the \acrshort{NUC} assumption is commonly violated in observational data due to those unmeasured confounders that researchers can't fully control without experimental design; 
the positivity assumption is violated when some individuals can not receive a specific treatment or control with certainty due to ethical issues or budget constraints. 

It is crucial to handle these cases for better decision-making, as accurately estimating causal effects in the face of assumption violations is usually the first step towards optimizing policies for higher rewards. Recently, literature has started to focus on addressing each assumption violation using various tools, most of which are originally developed in the area of causal inference. In this section, we'll provide a concise overview and outline open questions in the literature that need further exploration.

 
\begin{comment}
\begin{table}[tbh]
\centering\renewcommand\cellalign{lc}
\begin{tabular}{l|ll}
\hline
Paradigm & confounders & interference \\ 
\hline
Paradigm 1    &      \makecell{\acrshort{IV} \cite{angrist1996identification, wang2018bounded,qiu2021optimal,cui2021semiparametric},\\ data integration \cite{wu2022integrative}}    &   

\makecell{experimental design \cite{viviano2020experimental, aronow2017estimating, leung2022rate, viviano2024policy, leung2022causal},\\ observational studies \cite{forastiere2021identification, qu2021efficient, bargagli2020heterogeneous, su2019modelling}}      \\
\hline
Paradigm 2/5    &     \makecell{\acrshort{IV} \cite{li2021causal, fu2022offline, xu2023instrumental},\\ data integration \cite{gasse2021causal,imbens2022long}}        &         
\makecell{multi-agent RL \cite{yang2018mean,shi2022multi,luo2024policy,chen2021pessimism,yang2024spatially}\\
\cite{jia2024multi,yang2021believe, pan2022plan, jiang2021offline}
}
\\
\hline
Paradigm 3         &         \makecell{\acrshort{IV} \cite{chen2023estimating, xu2023instrumental},\\ data integration \cite{athey2020combining}, \\  POMDP \cite{shi2022minimax}}    &      \acrshort{DTR}  with interference \cite{jiang2023dynamic}        \\
\hline
Paradigm 4   &      \makecell{\acrshort{IV} \cite{kallus2018instrument},\\ data integration \cite{sen2017contextual,bareinboim2015bandits,xu2021deep}}       &    
Multi-agent bandits \cite{verstraeten2020multi, bargiacchi2018learning, dubey2020kernel,jia2024multi}
%\makecell{Multi-agent bandits \cite{verstraeten2020multi, bargiacchi2018learning, dubey2020kernel,jia2024multi}\\
%Multiple-play bandits \cite{anantharam1987asymptotically, lagree2016multiple}, \\
%MAB with interference \cite{jia2024multi} }   
\\ 
\hline
\end{tabular}
\caption{A brief summary of papers in assumption violated cases}\label{table:1}
\end{table}
\end{comment}





\subsection{Unmeasured Confounders}
\begin{table}[tbh]
\centering\renewcommand\cellalign{lc}
\begin{tabular}{l|ll}
\hline
Paradigm & \acrlong{IV} & Data Integration \\ 
\hline
Paradigm 1    &      \makecell{\cite{angrist1996identification}\\
\cite{wang2018bounded}\\ \cite{qiu2021optimal}\\
\cite{cui2021semiparametric}} & \cite{wu2022integrative}     \\
\hline
Paradigm 2/5    &     \makecell{\cite{li2021causal, fu2022offline}\\
\cite{xu2023instrumental}} & \makecell{\cite{gasse2021causal}\\
\cite{imbens2022long} }
\\
\hline
Paradigm 3         &        \makecell{\cite{chen2023estimating}\\
\cite{xu2023instrumental}} &  \cite{athey2020combining}        \\
\hline
Paradigm 4   &       \cite{kallus2018instrument} &\makecell{\cite{bareinboim2015bandits}\\
\cite{sen2017contextual}\\
\cite{xu2021deep}}
%\makecell{Multi-agent bandits \cite{verstraeten2020multi, bargiacchi2018learning, dubey2020kernel,jia2024multi}\\
%Multiple-play bandits \cite{anantharam1987asymptotically, lagree2016multiple}, \\
%MAB with interference \cite{jia2024multi} }   
\\ 
\hline
\end{tabular}
\caption{A brief summary of papers when unmeasured confounders exist}\label{table:1}
\end{table}

Table \ref{table:1} provides a brief summary of some representative approaches in handling the violation of the \acrshort{NUC} assumption. 
When unmeasured confounder $U$ exists, traditional methods for \acrshort{CEL} and \acrshort{CPL} % including DM, IPW, and DR estimators introduced in Equation \eqref{eq:CEL_p1_DM}-\eqref{eq:CEL_p1_DR} 
would results in biased estimates. To adjust for potential bias, the current literature can be broadly divided into two categories: (1) using proxies such as \acrfull{IV}, or (2) incorporating additional data sources, typically by integrating experimental data without unmeasured confounders with existing observational data (commonly referred to as data integration).

The use of \acrshort{IV} can be traced back to the 1920s \citep{wright1928tariff}, gaining widespread recognition with the introduction of the \acrfull{2SLS} method \citep{angrist1995two}. Typically, a variable $X$ is called an \acrshort{IV} when it satisfies the following three conditions: 
\begin{enumerate}
    \item[a.] \acrshort{IV} independence: $Z\perp U|S$.
    \item[b.] \acrshort{IV} relevance: $Z\not\perp A|S$.
    \item[c.] Exclusion restriction: $R(z,a)=R(a)$ for any $(z,a)\in \mathcal{Z}\otimes \mathcal{A}$.
\end{enumerate}

Notably, \acrshort{IV} has been employed without stringent modeling assumptions to effectively estimate \acrshort{ATE}s \citep{angrist1996identification}, where certain monotonicity assumption is usually imposed to guarantee the identifiability. In single-stage setting (Paradigm 1), recent advancements, such as \citet{wang2018bounded}, have focused on developing semi-parametric efficiency estimators for \acrshort{HTE} with \acrshort{IV}. Additionally, \citet{qiu2021optimal} and \citet{cui2021semiparametric} have contributed to the literature by deriving optimal policies that maximize rewards in the presence of \acrshort{IV}. 

%Another perspective on combining \acrshort{CEL} and CPL in bandit settings lies in the flexibility of \acrshort{CEL} in handling correct counterfactual estimation in cases where assumptions are violated. [\acrshort{CEL} how to handle assumption violated cases]


Under \acrshort{MDP}s (Paradigms 2 or 5), the existence of unobserved state variables also greatly influences both estimation and decision making process. This problem has been explored in the literature of \acrshort{RL}, as evidenced by works such as \citet{li2021causal, fu2022offline, xu2023instrumental,gasse2021causal,imbens2022long}. In offline \acrshort{RL}, \citet{fu2022offline} and \citet{xu2023instrumental} introduced \acrshort{IV}-based methods to ensure consistent \acrshort{OPE} in scenarios involving confounded sequential decision-making. \citet{fu2022offline} focus on pessimistic \acrshort{RL} and offer finite-sample suboptimality guarantees, while \citet{xu2023instrumental} emphasize semi-parametric efficiency, statistical inference, and extensions to high-order \acrshort{MDP}s and \acrshort{POMDP}s.
In online settings, \citet{li2021causal} proposed an \acrshort{IV}-based Q-learning algorithm to learn optimal policies in an interactive decision making environment.
Aside from \acrshort{IV}s, another line of research aims to combine interventional data and observational data for better decision making. For instance, \citet{imbens2022long} combined short-term experimental data and long-term observational data with potential confounders to handle the identification and estimation of long-term treatment effects with asymptotic theory guarantees. \citet{gasse2021causal}, on the other hand, tailored on better policy learning by utilizing confounded information in offline data. 

In scenarios where the Markov assumption doesn't hold (Paradigm 3), addressing confounders requires tailored approaches depending on differences in data-generating structures. For instance, \citet{shi2022minimax} proposed an \acrshort{OPE} approach within the framework of \acrshort{POMDP}s, which is a natural extension of \acrshort{MDP}s to handle unmeasured confounders present in the latent state. In \acrshort{DTR}, \citet{chen2023estimating} also employed \acrshort{IV} for consistent treatment effect estimation and policy optimization. When only a short-term variable (i.e., a surrogate) is observed for long-term treatment effect estimation, and there are no clear state transitions defined as \acrshort{DTR}  or \acrshort{MDP}s, the data structure introduced by \citet{athey2020combining} is more suitable for identifying and estimating long-term effects using short-term experimental data.


In online bandits learning (Paradigm 4), some work has also been done with \acrshort{IV} to help detect possible unmeasured confounders and avoid biased policy learning \citep{sen2017contextual,kallus2018instrument,bareinboim2015bandits,xu2021deep}. %In bandits, a crucial challenge is to determine the optimal policy that maximizes cumulative rewards based on our understanding and estimation of the reward-generating model. 
In the context of causal inference, the bandits problem is also equivalent to first estimating $R(a)$ and adding proper exploration to obtain a suboptimal regret bound. When confounders exist, at least a portion of the relationship between $A$ and $R$ is not captured in the reward modeling process, leading to biased reward modeling and, consequently, biased policy learning outcomes. Given the flexible approaches in handling unmeasured confounders in \acrshort{CEL}, recent research has been developed to address this problem. Most of the existing literature focuses on introducing proxy variables, such as \acrshort{IV}s \citep{kallus2018instrument}, where authors investigate optimal policies to maximize intent-to-treat regret in the presence of potential non-compliance and unmeasured confounders, or combining observational data for confounding adjustment within the structural causal equation model \citep{bareinboim2015bandits}. Additionally, \citet{xu2021deep} proposed a two-stage regression scheme based on proxy variables to handle unmeasured confounders, especially when the data is high-dimensional and non-linear. 


% \subsubsection{Extensions}
% % Those from my slides have been moved here
% The causal framework is further leveraged to design various extensions on the top of the main ideas we introduce above. 
% \citet{xu2021deep} and \citet{kallus2021minimax} developed OPE and OPO in Paradigm 1 to account for unmeasured confounders, and \citet{xu2023instrumental, shi2022off, fu2022offline, wang2022blessing} extend the techniques to Paradigm 2. 



\subsection{Interference}
\begin{table}[tbh]
\centering
\renewcommand
\cellalign{lc}
\begin{tabular}{l|l}
\hline
Paradigm &  Methogologies \\ 
\hline
Paradigm 1    &    

\makecell{Experimental design: \makecell{\cite{viviano2020experimental, aronow2017estimating}\\
\cite{leung2022rate, viviano2024policy, leung2022causal}}\\ Observational studies: \makecell{\cite{forastiere2021identification, qu2021efficient}\\\cite{bargagli2020heterogeneous, su2019modelling}}}      \\
\hline
Paradigm 2/5       &         
Multi-agent \acrshort{RL}: \makecell{\cite{yang2018mean,shi2022multi,luo2024policy}\\
\cite{chen2021pessimism,yang2024spatially,jia2024multi}\\
\cite{yang2021believe, pan2022plan, jiang2021offline}
}
\\
\hline
Paradigm 3        &      \acrshort{DTR}  with interference: \cite{jiang2023dynamic}        \\
\hline
Paradigm 4      &    
Multi-agent bandits: \makecell{\cite{verstraeten2020multi, bargiacchi2018learning}\\
\cite{dubey2020kernel,jia2024multi}}
%\makecell{Multi-agent bandits \cite{verstraeten2020multi, bargiacchi2018learning, dubey2020kernel,jia2024multi}\\
%Multiple-play bandits \cite{anantharam1987asymptotically, lagree2016multiple}, \\
%MAB with interference \cite{jia2024multi} }   
\\ 
\hline
\end{tabular}
\caption{A brief summary of papers when interference exists}\label{table:2}
\end{table}


Table \ref{table:2}  provides a brief summary of some representative approaches in handling the violation of the \acrshort{SUTVA} assumption. 
Interference, often known as the existence of \acrfull{SE}, is a commonly encountered problem in causal inference. 
Generally, it requires extending the \acrshort{SUTVA} assumption in that the potential outcomes of unit $i$ depend on not only $A_i$, but also the actions of other units. 
For example, the reward of unit $i$ under interference is denoted by $R_i(\boldsymbol{A})$, where $\boldsymbol{A} = \{A_1,\dots, A_n\}$. 
Following this definition, the treatment effect can be decomposed into two parts: \acrshort{DE} and \acrshort{SE} : 
\begin{equation*}
    \begin{aligned}
    &\text{DE}_i(a_i,a_i',\boldsymbol{a}_{-i})=R_i(a_i,\boldsymbol{a}_{-i})-R_i(a_i',\boldsymbol{a}_{-i}),\\
    &\text{SE}_i(a_i,\boldsymbol{a}_{-i},\boldsymbol{a}_{-i}')=R_i(a_i,\boldsymbol{a}_{-i})-R_i(a_i,\boldsymbol{a}_{-i}'),
    \end{aligned}
\end{equation*}
where $\boldsymbol{a}_{-i}$ denotes the action assignment vector for all units except unit $i$.

Due to this dependency, directly modeling the treatment effect without considering the actions of other units can lead to bias. 
% This presents a great challenge in addressing interference problems. 
% \textbf{RZ: not following}
However, in extreme situations where each unit's reward depends on every single unit's action, any reward modeling approach to generalize findings across units would fail, making causal effect identification impossible. 
Consequently, there is a growing trend in the literature toward identifying and estimating the \acrfull{DE} and \acrfull{SE} under various model structures, aiming to strike a balance between avoiding overly stringent assumptions on
interference structure and allowing learning from existing data. 

In existing literature, various interference structures have been considered, including but not limited to partial interference \citep{sobel2006randomized, qu2021efficient}, stratified interference \citep{hudgens2008toward}, neighborhood interference \citep{forastiere2021identification}, spatial interference \citep{leung2022rate}, and cluster network interference \citep{bargagli2020heterogeneous}. Despite various definitions, most types of interference share a similar two-step structure for simplifying the problem. First, units are typically categorized into groups through clustering or partitioning, assuming that interference occurs only within each group. Extensions of this assumption allow for interference between any units, where degree of interference decreases with distance but does not necessarily zero \citep{leung2022rate, leung2022causal}. 
Second, to further simplify causal identification and estimation within each cluster, where the strength of interference level may vary by domain assumption, interference is also commonly quantified by \textit{exposure mapping}. This concept, proposed by \citet{aronow2017estimating}, is similar to the ``effective treatments'' function in \citet{manski2013identification}. Generally, exposure mapping assumes that interference among units is passed through a lower-dimensional exposure mapping function, often known and assumed to follow a parametric form to easily quantify the \acrshort{SE}  between interfering units. This approach has been widely adopted in various papers under specific mapping forms, such as \citet{su2019modelling,qu2021efficient, viviano2024policy}.

Depending on the quantity of interest and the flexibility of managing treatment assignment, different methods have been developed to address various application needs. There is a growing trend of research focusing on estimating causal \acrshort{DE}s and \acrshort{SE}s in both (1) experimental design \citep{viviano2020experimental, aronow2017estimating, leung2022rate, viviano2024policy, leung2022causal} and (2) observational studies \citep{forastiere2021identification, qu2021efficient, bargagli2020heterogeneous, su2019modelling}. Specifically, among the approaches focusing on (1) experimental design, \citet{viviano2020experimental} and \citet{leung2022rate} concentrate on developing optimal designs to maximize specific goals, such as achieving near-optimal rates of convergence for global average causal effect estimation \citep{leung2022rate} or minimizing the variance of the estimators. In contrast, \citet{aronow2017estimating}, \citet{leung2022causal}, and \citet{viviano2024policy} focus more on estimation under randomized experiments, providing asymptotic guarantees for the proposed estimators for both average effects \citep{aronow2017estimating, leung2022causal} and heterogeneous effects \citep{viviano2024policy}. Among the approaches focusing on (2) observational studies, methods vary based on assumptions about the type of interference and reward modeling. For example, \citet{su2019modelling} considers the reward as a linear function of neighbors’ covariates and treatments, extending Q-learning and A-learning to scenarios with interference under certain structural assumptions. \citet{forastiere2021identification, qu2021efficient, bargagli2020heterogeneous} emphasize semi-parametric or non-parametric estimation and inference under looser modeling assumptions. Users can select the appropriate method based on their willingness to assume different interference structures and modeling approaches.


Moving to multi-stage settings, including \acrshort{MDP}s \citep{yang2018mean}, \acrshort{DTR} \citep{jiang2023dynamic} and beyond, interference often arises in a muti-agent system. In \acrfull{MARL}, the concept of neighborhood interference and exposure mapping is incorporated into the Q function. Here, the Q value of agent $j$ depends not only on $(s_j, a_j)$ but also on the actions of a neighborhood set $\boldsymbol{a}_{\mathcal{N}_j}$, which is the so-called mean-field approximation strategy \citep{yang2018mean}. Later on, this mean-field approximation method has been applied in various \acrshort{MARL} studies, including \citet{shi2022multi,luo2024policy,chen2021pessimism,yang2024spatially, jia2024multi}. Among these, \citet{shi2022multi} and \citet{luo2024policy} focus more on \acrshort{OPE} with observational data in the presence of both temporal and/or spatial dependences among agents, while
\citet{yang2024spatially} studies the efficiency of spatial randomization to account for interference in the experimental design/exploration side. 
Other related work in offline \acrshort{MARL} includes but not limited to \citet{yang2021believe, pan2022plan, jiang2021offline}. Specifically, \citet{pan2022plan} tackles challenges posed by an increasing number of agents on conservative offline \acrshort{RL}
algorithms, \citet{jiang2021offline} exploits value deviation and transition normalization in non-stationary transition dynamics, and \citet{yang2021believe} focuses on mitigating extrapolation error in offline evaluation.


The multi-agent system is also present in online bandits (paradigm 4) \citep{bargiacchi2018learning, verstraeten2020multi, dubey2020kernel}. In round $t$, each agent $i$ is able to interact with the environment and pull an arm. Interference exists since the actions of agents are mutually affected by each other due to the neighboring relationships among agents. Similar to single-stage setting, a common approach to handle this problem is to decompose the agents into fixed but potentially overlapping subgroups. This decomposition simplifies the reward of the joint action space to the summation of local reward functions, reducing the complexity of global exploration. Following this approach, \citet{bargiacchi2018learning} extended the \acrshort{UCB} algorithm from classical \acrshort{MAB} to the multi-agent scenario. Shortly after, \citet{verstraeten2020multi} extended the \acrshort{TS} algorithm to the multi-agent case under similar interference assumptions. \citet{dubey2020kernel} considered another reward modeling structure where interference is transmitted through network contexts, and proposed a kernelized \acrshort{UCB} algorithm to cooperatively maximize cumulative rewards. Rather than handling interference implicitly within multi-agent systems, recent pioneering works have explicitly addressed the interference issue in bandit settings. For example, \citet{jia2024multi} and \citet{agarwal2024multi} focus on \acrshort{MAB}, while \citet{xu2024linear} examines interference in contextual bandits.
%\citet{jia2024multi} proposed algorithms to handle  proposed an algorithm using cluster randomization, a concept from causal inference, to approach MAB with interference and provide theoretical guarantees. 
Despite these advancements, this field remains relatively underexplored, highlighting a pressing need for more flexible and general approaches to address broader bandit settings using causal methodologies.


\begin{comment}
\textbf{[previous version]} In the single-stage setting (paradigm 1), various interference structures are commonly considered, including  partial interference \cite{sobel2006randomized, hudgens2008toward}, neighborhood interference \cite{forastiere2021identification}, spatial interference \cite{leung2022rate}, and cluster network interference \cite{aronow2017estimating, bargagli2020heterogeneous}. Among these, partial interference is the most frequently utilized structure. It assumes a partition of units into disjoint clusters, where interference only occurs within each cluster. This assumption is generally reasonable when clusters are sufficiently separated. Neighborhood, spatial, and cluster network interference extend this notion by allowing overlapping unit clusters under different conditions, which might be more suitable for different real-world applications. 
Depending on the purpose of study, some concentrate on estimating average \acrfull{DE} and SE \cite{aronow2017estimating, qu2021efficient, leung2022rate}, while others delve into estimating heterogeneous DE/SE and optimal decision making \cite{bargagli2020heterogeneous,su2019modelling, viviano2024policy}. For example, \citet{su2019modelling} 
considers the reward as a linear function of neighbours’ covariates and treatments, and extends Q-learning and A-learning to interference-exsiting scenarios with certain structural assumptions. In contrast,
\cite{viviano2024policy} relax these assumptions and leverage experimental
variation to optimize treatment allocation policy under network interference. Additionally, existing research can also be categorized based on emphasis: some emphasizes estimation and inference in observational studies \cite{qu2021efficient, bargagli2020heterogeneous,su2019modelling}, while others focus on optimizing experimental design to achieve robust estimators \cite{aronow2017estimating, leung2022rate,viviano2024policy}. % Notably, allow full dependence of all units's actions but with parametric modeling: \cite{su2019modelling}
% 4 types in https://www.math.pku.edu.cn/teachers/yaof/papers/2023-STVCM.pdf



The structures mentioned above for Paradigm 1 have also been extended to Paradigms 2 and 3. 
\citep{shi2022multi} studies a neighborhood interference structure motived by applications in ride-sharing, where the authors assume inteference exists between neighborhoods and the indirect effect on an agent is through some summary statistics of the variables of neighbor agents, i.e., the so-called mean-field approximation strategy. 
The same idea is later extended to \cite{luo2024policy} with more parametric structures, where a few varying-coefficient decision process models are proposed. 
The network interference structure is considered in
\cite{jiang2023dynamic},  where weighted ordinary least squares and network propensity function are coupled to obtain a doubly robust estimator under interference associated with network links, in a \acrshort{DTR}  setting (i.e. paradigm 3). 
In the experimental design/exploration side, 
\cite{yang2024spatially} studies the efficiency of  spatial randomization to account for interference in OPE. 
Finally, when there is a large number of agents, the mean-field multi-agent framework has also been applied \citep{chen2021pessimism}, usually together with the pessimism technique in OPO.  
\cite{yang2021believe, pan2022plan, jiang2021offline} decompose the global value function into a set of local value functions and apply policy or value regularizations at the local level. 
These simplified approach may fail to guarantee a regularization valid at the global level. 
\cite{wang2024offline} provides a more principled framework by converting global-level value function regularization into local value regularizations in an equivalent but implicit way. 



% Interference, a challenge that arises when rewards depend on the treatments of other units, commonly occurs in scenarios with network structures. 
In paradigm 4, several relevant works exist within the context of multi-agent bandits \cite{bargiacchi2018learning,verstraeten2020multi,dubey2020kernel} (where each player controls a separate but related bandit instance) or multi-play bandits \cite{anantharam1987asymptotically, lagree2016multiple} (where all players aim to control the same bandit instance), which typically rely on the careful modeling of the interactions between agents. 
Recently, \cite{jia2024multi} propose an algorithm using the cluster-randomization originated from causal inference to approach MAB with interference and provide theoretical guarantees. 
Despite recent advancements, this field remains relatively underexplored, and there is a pressing need for more flexible and general approaches to address broader bandit settings using causal methodologies.

% The multiplayer variant of this problem has also known a recent interest, motivated
% by cognitive radio networks. It considers multiple decentralized players acting on a single
% Multi-Armed Bandits instance. If several of them pull the same arm at the same time, a
% collision occurs and causes a loss (of the message in the previous example). This leads to
% a decrease in the received reward and makes the problem much more intricate.

%and online RL \cite{zhang2021multi}. 


% CPL \cite{su2019modelling, viviano2024policy}} 

% \acrshort{DTR}  with interference \cite{jiang2023dynamic}    



% sherman2020general,
%   title={General identification of dynamic treatment regimes under interference


% We investigate the \acrshort{DTR}  estimation method of dynamic  (dWOLS), which boasts of easy implementation and the so-called double-robustness property,
% but relies on the assumption of no interference. We define a network propensity function and build on
% it to establish an implementation of dWOLS that remains doubly robust under interference associated
% with network links. The method’s properties are demonstrated via simulation and applied to data fro

% models; the outcome considered is a network
% mean. Their result, using a double summation switching technique, is that under their model, the
% optimal treatment at the individual level is dependent only on that individual’s own covariates
% and the degree (number of neighbours) of that individual. 
% Because their models do not include
% interactions between others’ treatment assignments and one’s own treatment assignment, the
% optimal treatment rules do not rely on the neighbours’ treatments. 
% In another approach, assuming
% a treatment is randomized and depends on the unit’s own covariates and neighbour’s covariates,
% and assuming network exogeneity, Viviano (2020) proposed a methodology for estimating
% treatment allocation policy under network interference. This work considered a semiparametric
% model to estimate a network’s empirical welfare, which is a function of policy. Additionally, to
% identify the optimal policy, an exact optimization algorithm was employed to solve optimization
% questions with capacity constraints.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{comment}
\subsection{Positivity}


The positivity (or overlap) assumption is a fundamental requirement ensuring that the data area covered by the target policy has been observed. This assumption posits that the conditional probability of any unit group taking each action is strictly greater than zero. However, in observational studies where actions are not controlled a priori, satisfying this assumption is often challenging due to several factors: (1) in continuous action spaces, it is naturally impossible for an observational study to exhaustively cover all actions; (2) when the feature space for state information $\boldsymbol{S}$ is high-dimensional, ensuring sufficient overlap becomes difficult; and (3) specific treatment options might lack observational data due to ethical concerns, high costs, or oversights during data collection. These issues are prevalent in offline studies (paradigms 1-3), whereas in online experiments, actions are typically selected by designers, thereby mitigating the non-overlapping problem.

Although the violation of the positivity assumption receives less attention compared to the previous two assumptions, existing literature still offers some solutions to address this issue. Given that the general strategies are quite similar across different data structures and that this problem is generally less studied in paradigms 2 and 3, we will focus on introducing approaches to handle the violation of the positivity assumption under paradigm 1.

In the three scenarios mentioned, continuous action space and high-dimensional covariate space are cases where existing papers may not specifically address non-overlapping issues. However, these approaches provide a general framework that naturally tackles this problem. For example, \citet{d2021overlap} discuss the trade-off between incorporating more covariates to mitigate unmeasured confounders and the difficulty of satisfying the positivity assumption. They argue that strict overlap is more restrictive than expected in many studies and derive explicit bounds on the average imbalance in covariate means.

In addressing continuous action space, most existing work tackles this problem through non-parametric smoothing \citep{kallus2018policy, chen2016personalized} or by combining it with (semi)-parametric modeling \citep{chernozhukov2019semi}. For instance, in the \acrshort{IPW} estimator, \citet{kallus2018policy} handle both policy evaluation and optimization by smoothly relaxing the indicator function using a kernel function, while \citet{chen2016personalized} replace the indicator function with a hinge loss. Continuous action space, as a well-established research area in \acrshort{CDM}, has extensive literature beyond the examples listed, and a more detailed review is beyond the scope of this paper.

The last stream of work addresses non-overlapping issues directly, without assuming specific dimensions for the action or covariate space. Due to the scarcity of data points in low-overlap areas, inferring causal relationships in these regions is challenging without additional smoothing or parametric modeling assumptions. Traditional approaches often use trimming \citep{rosenbaum1983central, crump2009dealing, petersen2012diagnosing}, which involves discarding units with estimated propensity scores outside a specific range $[0.1, 0.9]$, or matching \citep{visconti2018handling}. Recently, in \acrshort{OPE}, \citet{khan2023off} applied partial identification techniques from causal inference to derive \acrshort{OPE} results under non-parametric Lipschitz smoothness assumptions on the reward function. In \acrshort{OPO}, the pessimism technique becomes particularly important due to the increased uncertainty for candidate policies with poor coverage. For instance, \citet{jin2022policy} used pessimism with generalized empirical Bernstein’s inequality to study \acrshort{OPO} without the uniform overlap condition, and \citet{chen2023steel} proposed a general and adaptive framework that decomposes \acrshort{OPE} error into positive and singular parts, using corresponding uncertainty bounds to derive a pessimism-based \acrshort{OPO} algorithm.

% {\color{red}: Offline discussion with Runzhe}

% % some words about online. 

% \textbf{[RZ]} The positivity (or overlap) assumption is fundmenatal requirement on that we have seen the data area covered by the target policy. 
% In both OPE and OPO, the existing approaches essentially rely on extrapolation assumptions to cover the non-overlap region, typically using either kernel smoothing \citep{kallus2018policy, chen2016personalized} or a well-specified environment model \citep{moodie2012q, chernozhukov2019semi}. 
% In OPE, \cite{khan2023off} recently applies the partial identification techniques from causal inference to derive OPE results under non-parametric Lipschitz smoothness assumptions on the reward function. 
% % \cite{lawrence2017counterfactual}
% % The kernel smoothing \citep{chen2016personalized} or environment model assumptions \citep{chernozhukov2019semi} are also widely adopted in the OPO problem. 
% In OPO, unlike OPE, the pessimism technique discussed in Section \ref{sec:pessimism} becomes particular important in OPO, due to the inflated uncertainty for candidate policies with poor coverage: \cite{jin2022policy} uses pessimism with generalized empirical Bernstein’s inequality to study OPO without uniform overlap condition; \citep{chen2023steel} proposes a general and adaptive framework that decomposes the OPE error into the positive part and the singular part, and uses corresponding uncertainty bound to derive a pessimism-based OPO algorithm. 

% {\color{blue}[A GENERAL OVERVIEW OF WORK to be added]}
\begin{comment}
The positivity (or overlap) assumption is fundmenatal requirement on that we have seen the data area covered by the target policy. 
In both OPE and OPO, the existing approaches essentially rely on extrapolation assumptions to cover the non-overlap region, typically using either kernel smoothing \citep{kallus2018policy, chen2016personalized} or a well-specified environment model \citep{moodie2012q, chernozhukov2019semi}. 
In OPE, \cite{khan2023off} recently applies the partial identification techniques from causal inference to derive OPE results under non-parametric Lipschitz smoothness assumptions on the reward function. 
% \cite{lawrence2017counterfactual}
% The kernel smoothing \citep{chen2016personalized} or environment model assumptions \citep{chernozhukov2019semi} are also widely adopted in the OPO problem. 
In OPO, unlike OPE, the pessimism technique discussed in Section \ref{sec:pessimism} becomes particular important in OPO, due to the inflated uncertainty for candidate policies with poor coverage: \cite{jin2022policy} uses pessimism with generalized empirical Bernstein’s inequality to study OPO without uniform overlap condition; \citep{chen2023steel} proposes a general and adaptive framework that decomposes the OPE error into the positive part and the singular part, and uses corresponding uncertainty bound to derive a pessimism-based OPO algorithm. 

\end{comment}
% in OPE or all candidate policies in OPO. 




% To address such singularity issue induced by the deterministic policy with respect to the stochastic one, existing solutions either adopt the kernel smoothing techniques on $\pi$ in order to approximately estimate $\calV_0(\pi)$ \citep[e.g.,][]{chen2016personalized} or impose some structure assumption on the reward function $r$ \citep[e.g.,][]{chernozhukov2019semi}. The first type of approaches requires the selection of kernel and the tuning on the bandwidth for approximating all deterministic policies, while the latter one could suffer from the model mis-specification due to the strong (parametric) model assumption. It is also well-known that the performance of the kernel smoothing deteriorates when the dimension of the action space increases. In addition, these methods cannot handle general settings such as that $\nu$ is also singular to $\nu_0$, i.e., there exists a \textit{covariate shift} problem. This problem becomes more severe when considering the dynamic setting.


