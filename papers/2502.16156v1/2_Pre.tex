\section{Preliminary} \label{sec:preliminary}
In this section, we provide a brief overview of the key concepts and assumptions commonly used throughout this paper. We begin with an introduction to the general principles of causal inference and reinforcement learning. Next, we delve into the specific concepts related to \acrshort{CSL}, \acrshort{CEL}, and \acrshort{CPL} in Section \ref{sec:prelim_CSL}-\ref{sec:p2_MDP}, and conclude with the assumptions outlined in Section \ref{sec:prelim_assump}.

\begin{definition} (Potential Outcome): For each individual, denote $A=a$ as the action or treatment assignment. We define $R(A=a)$ as the outcome/reward if the individual receives action $A=a$.
The potential outcomes framework, also known as the Neyman-Rubin Causal Model, is a foundational concept in causal inference.
\end{definition} 

\begin{definition}(Do-Operator): Given any two variables $X$ and $Y$ in a causal system, the do-operator denotes an intervention on $X$, which is often defined as $do(X=x)$. The conditional probability of $Y$ given $do(X=x)$ is defined as $\mathbb{P}(Y|do(X=x))$.
\end{definition}
Without further assumptions about the causal structure involving $(A,R)$, the probability $\mathbb{P}(R|do(A=a))$ generally differs from $\mathbb{P}(R|A=a)$. The discrepancy arises because $\mathbb{P}(R|do(A=a))$ represents the probability of $R$ under an intervention where $A$ is forcibly set to $a$, while all other potential causes of $R$, whether observed or not, are held fixed. Mathematically, if we denote $Z$ as the set of all other variables that are causally upstream of $R$ (excluding $X$), the intervention probability can be expressed as
$$
\mathbb{P}(R|do(A=a)) = \sum_z \mathbb{P}(R|A=a,Z=z)\mathbb{P}(Z=z),
$$
which captures how intervening on $A$ with do-operator disrupts the natural causal mechanisms.
\begin{definition}(Confounder): In causal structures, a variable $C$ is considered a confounder between $A$ and $R$ if $C$ is a common cause of $(A,R)$, i.e. $C\rightarrow A$ and $C\rightarrow R$.
    
\end{definition}

\begin{definition}(Mediator): A variable $M$ is considered a mediator between $A$ and $R$ if $M$ is causally downstream of $A$ but upstream of $R$, i.e. $A\rightarrow M\rightarrow R$.
    
\end{definition}


\begin{definition}(Decision Process): 
A decision process is a framework used to describe the evolution of states, actions and rewards over time. In this general setting, with the dataset being $\{s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_t, a_t, r_t, \dots\}$, the probability of reaching a future state and receiving a reward can depend on the entire history of states and actions up to that point as 
$
P(s_{t+1}, r_{t+1} \mid s_0, a_0, s_1, a_1, \ldots, s_t, a_t).
$
\end{definition}

\begin{definition}\label{def:MDP}(\acrfull{MDP}): 
\acrshort{MDP} is a special type of decision process where the probability of transitioning to the next state and receiving a reward depends only on the current state and action, and not on any previous states or actions. This ``memoryless'' property simplifies decision-making because it allows the process to be fully described by the current state and action alone. Formally, this is represented as
$
P(s_{t+1}, r_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0) = P(s_{t+1}, r_{t+1} \mid s_t, a_t).
$
\end{definition}



% \subsection{Introduction to RL methods} 


\subsection{Causal Graphical Model under a Potential Outcome Framework} \label{sec:prelim_CSL}


% \subsection{Preliminaries}
% \subsubsection{Causal Graph Terminology}
%Consider a graph $\mathcal{G} =(\mathbf{Z},\mathbf{E})$ with a node set $\mathbf{Z}$ and an edge set $\mathbf{E}$. A node $Z_i$ is said to be a parent of $Z_j$ if there is a directed edge from $Z_i$ to $Z_j$. Let the set of all parents of node $Z_j$ in $\mathcal{G}$ as $PA_{Z_j} (\mathcal{G})$. A directed graph that does not contain directed cycles is called a directed acyclic graph (DAG). Suppose a DAG $\mathcal{G}=(\mathbf{Z},\mathbf{E})$ that characterizes the causal relationship among $|\mathbf{Z}|=d$ nodes, where $\mathbf{Z}=[Z_1,Z_2,\cdots,Z_d]^\top $ represents a random vector and an edge $Z_i\rightarrow Z_j$ means that $Z_i$ is a direct cause of $Z_j$. 
Consider a graph $\mathcal{G} =({X},{D}_{{X}})$ with a node set ${X}$ and an edge set ${D}_{{X}}$. There is at most one edge between any pair of nodes. If there is an edge between $X_i$ and $X_j$, then $X_i$ and $X_j$ are adjacent. A node $X_i$ is said to be a parent of $X_j$ if there is a directed edge from $X_i$ to $X_j$, i.e., $X_i$ is a direct cause of $X_j$. A node $X_k$ is said to be an ancestor of $X_j$ if there is a directed path from $X_k$ to $X_j$ regulated by at least one additional node $X_i$ for $i\not =k$ and $i \not =j$, i.e., $X_k$ is an indirect cause of $X_j$. Let the set of all parents/ancestors of node $X_j$ in $\mathcal{G}$ as $\textsc{PA}_{X_j} (\mathcal{G})$. A path from $X_i$ to $X_j$ in $\mathcal{G}$ is a sequence of distinct vertices, $\pi := \{a_0, a_1,\cdots,a_L\}\subset V$ such that $a_0 =X_i$, and $a_L=X_j$. A directed path from $X_i$ to $X_j$ is a path between $X_i$ and $X_j$ where all edges are directed towards $X_j$.  A directed graph $\mathcal{G}$ that does not contain directed cycles is called a \acrfull{DAG}. A directed graph is acyclic if and only if it has a topological ordering. 


% A general causal DAG, $\mathcal{G}$, may not be identifiable from the distribution of ${X}$. According to \cite{pearl2000causality}, a DAG only encodes conditional independence relationships through the concept of $d$-separation. In general, several DAGs can encode the same conditional independence relationships, and such DAGs form a Markov equivalence class. Two DAGs belong to the same Markov equivalence class if and only if they have the same skeleton and the same v-structures \citep{kalisch2007estimating}. A Markov equivalence class of DAGs can be uniquely represented by a completed partially directed acyclic graph (CPDAG) \citep{spirtes2000constructing}, which is a graph that can contain both directed and undirected edges. A CPDAG satisfies the following: $X_i \leftrightarrow X_j$ in the CPDAG if the Markov equivalence class contains a DAG including $X_i \rightarrow X_j$, as well as another DAG including $X_j \rightarrow X_i$. The Markov equivalence class for a fixed CPDAG $\mathcal{C}$ is denoted by $\operatorname{MEC}(\mathcal{C})$, which is a set containing all DAGs $\mathcal{G}$ that have the CPDAG structure $\mathcal{C}$. 
% % In general, if the error distribution is joint Gaussian with different diagonal elements in the LSEM satisfying our structure assumption, the data distribution can only give a CPDAG and thus a MEC \citep{shimizu2006linear}. In the linear setting, however, if the error distribution is non-Gaussian, the distribution of $X$ will completely give a unique DAG; see Theorem 11.4 in \cite{neal2020introduction}. Since in practice we usually do not know whether the error is Gaussian, it is safer to assume that we can only obtain a CPDAG instead of a DAG from the data. 
% If we can obtain the true DAG from the data, we can simply treat it as a special case of the "MEC" containing only this DAG, i.e., $\operatorname{MEC}(\mathcal{G})= \{ \mathcal{G} \}$. %For simplicity, we denote the corresponding causal structure for the mediators $M$ as $\mathcal{G}_M$, which can be obtained by deleting nodes $C, A, Y$, and the corresponding edges from $\mathcal{G}$. 
% % Specially, we denote the causal structure of mediators as $\mathcal{G}_{M}$. 
% % Obviously, the adjacency matrix $\mathcal{G}_{M}$ satisfies $\mathcal{G}_{M} = \big[ \mathcal{{G}}_{kk} \big]_{k \in \{ t + 1, \ldots, t + p\}}$. 
% % The CPDAG of mediators is similarly denoted as $\mathcal{C}_M$. For simplicity and with a minor stretch of notation, we employ $\mathcal{G}_{M}$ and $\mathcal{C}_M$ to denote the causal DAG and CPDAG of $X$, respectively, such that their corresponding mediators' causal DAG and CPDAG are represented by $\mathcal{G}_{M}$ and $\mathcal{C}_M$ exactly.
%  We introduce some commonly considered causal graphical models as follows.
% \subsubsection{Causal Graphical Model}

The \acrfull{SCM} characterizes the causal relationship among $|{X}|=d$ nodes via a DAG $\mathcal{G}$ and noises ${e}_{{X}} = [e_{X_1},\cdots,e_{X_d}]^\top$ such that
$X_i := h_i\{\textsc{PA}_{X_i} (\mathcal{G}), e_{X_i}\}$ for some unknown $h_i$ and $i=1,\cdots,d$. Here, we allow the collections of nodes to take different causal roles in the causal graph. For instance, let $A \in \mathbb{R}$ be an exposure/treatment, $M := (M_1,M_2,\cdots,M_p)^{\top} \in \mathbb{R}^p$ be mediators with dimension $p$ in its support ${M} = \mathcal{M}_1 \times \cdots \times \mathcal{M}_p \subseteq \mathbb{R}^p$, and $R \in \mathbb{R}$ be the outcome of interest. Additionally, we also consider that there are $t - 1$ confounders ${S}: = (S_1, \ldots, S_{t - 1})^{\top} \in \mathbb{R}^{t - 1}$ in its support $\mathcal{S} \subseteq \mathbb{R}^{t - 1}$. We would just let $t = 1$ here to represent the absence of confounders, that is $\mathcal{S} = \varnothing$. Suppose that there exists a \acrshort{DAG} $\mathcal{G} =({X},{D}_{{X}})$ that characterizes the causal relationship among ${X}=({S}^{\top}, A, {M}^\top, R)^\top$, where the dimension of ${X}$ is $d = t + p + 1$. 

In such a scenario, we can also define the potential outcome framework \citep{rubin1978bayesian} through the `do-operator' \citep{pearl2000causality}. Specifically, using the reward and action as an example, let $R(a)\equiv R(A=a)$ be the potential reward $R$ that would be observed after setting the action variable $A$ as $a$, following notation in \citet{rubin1978bayesian}'s framework. This term is stated to be equivalent to the value of $R$ by imposing a `do-operator' of $do(A=a)$ as in \citet{pearl2009causal}:
\begin{eqnarray*}
    R(a)\equiv R(A=a)\equiv R\{do(A=a)\},
\end{eqnarray*}
where $do(A=a)$ is a mathematical operator to simulate physical interventions that hold $A$ constant as $a$ while keeping the rest of the model unchanged, which corresponds to removing edges into $A$ and replacing $A$ by the constant $a$ in $\mathcal{G}$. 
Similarly, one can define the potential outcome,  $R(X_i=x_i)$, by setting an individual variable $X_i$ as $x_i$, while keeping the rest of the model unchanged.  
% In this paper, we allow the dimension of mediator $p = p_n$ can increase with the sample size $n$. 
Suppose we observe data  ${X} = ({S}^{\top}, A, {M}^\top, R)^\top$ for $n$ subjects. The goal is to learn decision-oriented causal graphs $\mathcal{G}$ presenting the causal relationship among ${X}$ based on observed data.
% We introduce some commonly considered causal graphical models as follows.


\subsection{Treatment Effect Estimation under a Potential Outcome Framework}
%The potential outcomes framework, also known as the Neyman-Rubin Causal Model, is a foundational concept in causal inference. Most existing causal literature is grounded in either the structural causal model or the potential outcomes framework. In the potential outcomes framework, for each unit or individual $X$, we consider what the outcome would be under different possible treatments or actions. For instance, we denote $R(a)$ as the potential outcome or reward that would result from executing action $A=a$. 

The fundamental challenge in causal inference is counterfactual estimation. Specifically, once a decision has been made and action $A=a$ has been taken, we can only observe the outcome for that action. As a result, estimating the missing potential outcome $R(a')$ for the alternative action $A=a'$ becomes crucial. As a primary task in causal inference, treatment effect estimation can involve different concepts depending on the specific problem setting. Common causal estimands include the \acrfull{ATE}, \acrfull{HTE}, and Mediation Effect. 
\begin{definition} (\acrshort{ATE}): Under either potential outcome's framework or do-operator system, the Average treatment effect is defined as
        \begin{equation*}
    \text{ATE} = \mathbb{E}[R(1) - R(0)] = \mathbb{E}[ R|do(A=1)] -  \mathbb{E}[ R|do(A=0)].
    \end{equation*}
\end{definition}

For instance, when investigating the volume of fluids administered to patients with diabetes and its impact on their health status within 48 hours, the first question to address is, ``Is this IV fluid generally effective in reducing the mortality rate?'' This question pertains to estimating the treatment effect on the overall patient population, which is quantified by ATE as defined above.

\begin{definition} (\acrshort{HTE}): To account for the heterogeneous effects across different individuals or contextual groups, the HTE is defined as
    \begin{equation*}
    \tau(s) = \mathbb{E}[R(1) - R(0)|S=s] = \mathbb{E}[ R|do(A=1),S=s] -  \mathbb{E}[ R|do(A=0),S=s].
    \end{equation*}
\end{definition}
Unlike ATE which focuses on the overall effect across the population, HTE further explores the variation in treatment effects across different subgroups or individuals. In the diabetes example, HTE aims to understand whether IV fluid administration leads to different levels of causal effects for patients with varying characteristics, such as age, gender, or prescription history, as captured by the state variable $S$.

\begin{definition} (Mediation Effect): When mediators involved, the \acrfull{TE} can be decomposed into the natural \acrfull{DE}, and the natural \acrfull{IE}, where
    \begin{equation*}
    \begin{aligned}
    \text{TE}&= \mathbb{E}[R|do(A=a_1)]-\mathbb{E}[R|do(A=a_0)]\\
    \text{DE}&= \mathbb{E}[R|do(A=a_1,M=m^{(a_0)})]-\mathbb{E}[R|do(A=a_0), M=m^{(a_0)}]\\
    \text{IE}&= \mathbb{E}[R|do(A=a_1,M=m^{(a_1)})]-\mathbb{E}[R|do(A=a_0,M=m^{(a_1)})]\\
    \end{aligned}
    \end{equation*}
with $\text{TE} = \text{DE}+ \text{IE}$. 
\end{definition}
In the context of diabetes, the \acrfull{SOFA} score can be considered a mediator influenced by the administration of IV input. This score, in turn, affects the mortality rate within the next 48 hours. The mediation effect in this case allows us to decompose the total effect of IV input on mortality into two components: the direct effect, which directly measures how IV input impacts mortality, and the indirect effect, which operates through the SOFA score before ultimately influencing mortality.



\subsection{Causal Policy Learning under a Potential Outcome Framework} \label{sec:p2_MDP}
% \subsection{Markov Decision Process} \label{sec:prelim_CPL}

% In this section, we introduce the fundamental concepts in causal policy learning, with a focus on Markov Decision Processes (MDPs). %in the causal graphical model \citep{pearl2000causality}, RL, the Markov Decision Process (\acrshort{MDP}) \citep{sutton2018reinforcement}. 
% Most  causal decison problems be unified under the XX formulation. 
% An MDP can be denoted as a tuple $\mathcal{M} = \{\mathcal{S}, \mathcal{A}, \mathcal{T}, r\}$, where $\mathcal{S}$ represents the market state, including the observable state $\mathcal{O}$ and underlying information, at each time step $t \in {1, ..., T}$. $\mathcal{A}$ represents the agent's actions to maximize its cumulative reward 
% % $\sum_{i = 1}^T \alpha r_i$ .. where $\alpha$ is the discount factor 
% at every time step. 
% $\mathcal{T}$ denotes the state transition function for the environment's dynamics, i.e. how the environment will respond to its actions, where $\mathcal{T}(s_t, a_t) = s_{t + 1}$. 
% Under deterministic setting, $\mathcal{R}$ denotes the immediate reward received by taking an action under a given state, $r_{t}$ is a function from the state-action pair $s_{t} \times a_{t}\rightarrow r_{t}$. 

% As the underlying data generation model for RL, we consider an infinite-horizon discounted Markov Decision Process (MDP) \citep{puterman2014markov}. 
% For any $t\ge 0$, let $\bar{a}_t=(a_0,a_1,\cdots,a_t)^\top\in \mathcal{A}^{t+1}$ denote a treatment history vector up to time $t$. Let $\mathbb{S} \subset \mathbb{R}^d$ denote the support of state variables and $S_0$ denote the initial state variable. 
% For any $(\bar{a}_{t-1},\bar{a}_{t})$, let $S_{t}^*(\bar{a}_{t-1})$ and $Y_t^*(\bar{a}_t)$ be the counterfactual state and counterfactual outcome, respectively,  that would occur at time $t$ had the agent followed the treatment history $\bar{a}_{t}$. 
% The set of potential outcomes up to time $t$ is given by
% \begin{eqnarray*}
% 	W_t^*(\bar{a}_t)=\{S_0,Y_0^*(a_0),S_1^*(a_0),\cdots,S_{t}^*(\bar{a}_{t-1}),Y_t^*(\bar{a}_t)\}.
% \end{eqnarray*}
% Let $W^*=\cup_{t\ge 0,\bar{a}_t\in \{0,1\}^{t+1}} W_t^*(\bar{a}_t)$ be the set of all potential outcomes.

% A deterministic policy $\pi$ is a time-homogeneous function that maps the space of state variables to the set of available actions. 
% Following $\pi$, the agent will assign actions according to $\pi$ at each time. 
% We use $S_t^*(\pi)$ and $Y_t^*(\pi)$ to denote the associated potential state and outcome that would occur at time $t$ had the agent followed $\pi$. 

\begin{definition}\label{def:policy} (Policy): 
The policy $\pi$ is the agent's strategy, denoted as a function from the relevant information (context/state in Paradigm 1-2 or 4-5; all historical information in non-markovian decision process)  to the action (with a deterministic policy) or the action's probability space (with a random policy). 
% , $\pi: s_{t} \rightarrow a_{t}$. 
\end{definition}

We commonly use the value functions to evaluate 
the goodness of a policy. 
We use a discounted infinite-horizon setting to illustrate. 

\begin{definition} (V-function): 
A policy $\pi$'s state value function (V-function) is  
\begin{eqnarray*}
	V^{\pi}(s)=\sum_{t\ge 0} \gamma^t \mathbb{E} \{Y_t^*(\pi)|S_0=s\},
\end{eqnarray*}
where $0<\gamma<1$ is a discount factor that reflects the trade-off between immediate and future outcomes. The value function measures the discounted cumulative outcome that the agent would receive had they followed $\pi$. 
\end{definition}

\begin{definition} (Q-function): 
A policy $\pi$'s state-action value function (Q-function) is  
% Similarly, we define the Q-function by
\begin{eqnarray*}
	Q^{\pi}(a,s)=\sum_{t\ge 0} \gamma^t \mathbb{E} \{Y_t^*(\pi)|S_0=s, A_0 = a\}. 
\end{eqnarray*}
\end{definition}

\begin{definition}(Optimal Policy):
The optimal policy, $\pi^*$, is defined as $$\pi^* = \arg \max_{\pi} V^\pi(s), \forall s \in \mathcal{S}.$$
\end{definition}

% Note that our definition of the value function is slightly different from those in the existing literature \citep{sutton2018reinforcement}. Specifically, $V(\pi;s)$ is defined through potential outcomes rather than the observed data. 



% The state-value function (V-function), $V^\pi(s_t) = \sum_{t' = t}^{T} \mathbb{E}[r(s_{t'}, a_{t'}))|s_t]$, represents the expected cumulative reward the agent can obtain under policy $\pi$ starting from state $s_t$. 
% The state-action function (Q-function), $Q^\pi(s_t, a_t) = \sum_{t' = t}^{T} \mathbb{E}[r(s_{t'}, a_{t'}))|s_t, a_t]$, denotes the expected cumulative reward the agent can get under policy $\pi$ after taking action $a_t$ at state $s_t$.



% \textbf{Markovian Property}:
% The following property is central to a decision process being an MDP. 
% $$
% P(s_{t+1}, r_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0) = P(s_{t+1}, r_{t+1} \mid s_t, a_t).
% $$




% \textbf{(MA) Markov assumption}:  there exists a Markov transition kernel $\mathcal{P}$ such that  for any $t\ge 0$, $\bar{a}_{t}\in \{0,1\}^{t+1}$ and $\mathcal{S}\subseteq \mathbb{R}^d$, we have 
% $\mathbb{P}\{S_{t+1}^*(\bar{a}_{t})\in \mathcal{S}|W_t^*(\bar{a}_t)\}=\mathcal{P}(\mathcal{S};a_t,S_t^*(\bar{a}_{t-1})).$
% Noteably, the MA assumption is testable \citep{shi2020does}. 

% \textbf{(CMIA) Conditional mean independence assumption}: there exists a function $r$ such that  for any $t\ge 0, \bar{a}_{t}\in \{0,1\}^{t+1}$, we have 
% $\mathbb{E} \{Y_t^*(\bar{a}_t)|S_t^*(\bar{a}_{t-1}),W_{t-1}^*(\bar{a}_{t-1})\}=r(a_t,S_t^*(\bar{a}_{t-1}))$.

% They assume (i) the process is stationary, and (ii) the state variables shall be chosen to include those that serve as important mediators between past treatments and current outcomes. 
% These two conditions are central to the empirical validity of most RL algorithms. 
% Specifically, under these two conditions, one can show that there exists an optimal time-homogenous stationary policy whose value is no worse than any history-dependent policy \citep{puterman2014markov}. 

\begin{definition} (Bellman optimality equations): 
The Q-learning-type policy learning is commonly based on the Bellman optimality equation, which characterizes the optimal policy $\pi^*$ and is commonly used in policy optimization. 
Specifically, $Q^*$ is the unique solution of 
\begin{equation}
    Q(a, s) = \mathbb{E} \Big(R_t + \gamma \arg \max_{a'} Q(a, S_{t+1})  | A_t = a, S_t = s \Big).\label{eqn:bellman_Q}
\end{equation}
The concepts above can be extended to the non-markovian case as well, with the state variable replaced by the full history. 
\end{definition}

% \subsection{Problem Formulation of Causal Policy Learning}
% Offline Policy Evaluation and Optimization

% \textbf{TODO: ALSO talk about single-stage case? Or maybe only talk about MDP, and then mention how paradigm 1 and 3 can be handled? }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Three Key Causal Identifiability Assumptions}\label{sec:prelim_assump}
To address the problem of counterfactural estimation, causal inference typically relies on three key assumptions. While recent research has focused on relaxing these assumptions, we will first detail them here and discuss scenarios where these assumptions may be violated in Section \ref{sec:assump_violated}.



\begin{assumption}\label{assump:SUTVA} \acrfull{SUTVA} states that
    \begin{equation}
    R_i = \sum_{A=a}R_i(a) {1}\{A_i=a\} , i \in\{ 1, \cdots, n\},
    \end{equation}
    which can be broken down into two key sub-assumptions: (i) No interference between units, meaning the potential outcomes for one unit are unaffected by the actions assigned to other units, and (ii) Consistency of treatment, meaning there are no different versions of the same action that could lead to different potential outcomes. 
\end{assumption}

\begin{assumption}\label{assump:NUC} \acrfull{NUC} assumption states that 
$$R(a) \perp\!\!\!\perp A|S,\quad \forall a\in\mathcal{A},$$  
which quantifies the conditional independence of potential outcomes on the action being taken. 
\end{assumption}
For example, when investigating whether regular exercise reduces the risk of heart disease, genetic factors might influence both a personâ€™s likelihood to exercise and their risk of heart disease. In this case, genetic predisposition acts as an unmeasured confounder, violating the \acrshort{NUC} assumption by affecting both the treatment (exercise) and the outcome (heart disease risk).

\begin{assumption}\label{assump:Positivity} Positivity, or Overlap assumption states that 
$$0 < c_0< P(A=a|S) <c_1< 1, \quad \forall a\in\mathcal{A},$$
which assumes that every unit in the study population has a non-zero probability of receiving each possible treatment or intervention.
\end{assumption}


