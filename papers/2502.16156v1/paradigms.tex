\section{Three Tasks and Six Paradigms for \acrshort{CDM} }\label{sec:3task6paradigm}

\subsection{Three Tasks} \label{sec:3tasks}

\textbf{\acrlong{CSL}.} In recent years, \textit{causal discovery}, also known as \acrshort{CSL} \citep[e.g.,][]{pearl2000causality,peters2017elements}, has gained great attention for disentangling complex causal relationships in many areas \citep[e.g.,][]{nandy2017estimating,chakrabortty2018inference,cai2020anoce}. Building upon causal graphical models, \citep[see Section \ref{sec:prelim_CSL} and][for a comprehensive review]{pearl2009causal}, several CSL methods have been proposed \citep[e.g.,][]{spirtes2000constructing,shimizu2006linear,kalisch2007estimating,buhlmann2014cam,zheng2018dags,yu2019dag,zhu2019causal} to estimate the causal graphs from observed data, which is a crucial step in understanding the underlying mechanisms that govern changes in a system. It involves identifying the causal relationships between variables (see an illustration in the first panel of Figure \ref{fig:cdm}), which is fundamental for any subsequent analysis aiming to understand causal effects and make causal decisions. Most existing methodologies for average/heterogeneous treatment effects and personalized decision-making rely on a known causal structure, which provides the convenience of identifying the right variables to control (e.g., confounders), to intervene (e.g., treatments), and to optimize (e.g., rewards). However, such convenience is absent in many emerging real applications where the causal structure is unknown. Causal discovery thus has attracted more and more attention recently to infer causal structure from data and disentangle the complex relationship among variables.  In Section \ref{Sec:CSL}, we present state-of-the-art techniques for learning the skeleton of unknown causal relationships among input variables with embedded treatments.

\textbf{\acrlong{CEL}.} \acrshort{CEL} is a process of determining cause-and-effect relationships between variables \citep{pearl2003statistics,pearl2009causal}. Given a causal structure, either pre-assumed based on domain knowledge or derived using \acrshort{CSL} methods, the goal of \acrshort{CEL} is to identify, estimate, and infer the causal effect of interest. \acrshort{CEL} primarily focuses on estimating several key effects, including the \acrfull{ATE} \citep{hirano2003efficient,imbens2004nonparametric}, which measures the overall impact of a treatment across the entire population; the \acrfull{HTE} \citep{wager2018estimation,curth2021nonparametric}, which captures how the treatment's effect varies across different subgroups or individuals; and the mediation effect, which decomposes the causal effect by considering intermediate variables or mediators \citep{vanderweele2016mediation} that influence the relationship between the treatment and the outcome. In Section \ref{sec:CEL}, we provide a comprehensive review of the literature on \acrshort{CEL}, covering various paradigms and data structures.

\begin{figure}[t!]
        \subfloat[Offline Policy Optimization]{%
            \includegraphics[width=.48\linewidth]{Figure/Offline_Policy_Optimization.png}%
            \label{Fig:Off_PO}
        }\hfill
        \subfloat[Online Policy Optimization]{%
            \includegraphics[width=.48\linewidth]{Figure/Online_Policy_Optimization.png}%
            \label{Fig:On_PO}
        }\\
        \subfloat[Offline Policy Evaluation]{%
            \includegraphics[width=.48\linewidth]{Figure/Offline_Policy_Evaluation.png}%
            \label{Fig:Off_PE}
        }\hfill
        \subfloat[Online Policy Evaluation]{%
            \includegraphics[width=.48\linewidth]{Figure/Online_Policy_Evaluation.png}%
            \label{Fig:On_PE}
        }
        
        \caption{Causal Policy Learning}
        \label{Fig:CPL}
\end{figure}

\textbf{\acrlong{CPL}.} 
With the causal structure and effects between variables in mind, the ultimate goal is typically to evaluate and optimize our decision makings. 
When the decision-making is purely based on a fixed historical dataset (i.e., does not involve continuous data collection), we call such a setting \textit{offline} or \textit{off-policy} (see Figures \ref{Fig:Off_PO} 
and \ref{Fig:Off_PE}); 
while when the decision-making process involves continuous data collection and real-time policy updates based on incoming outcomes, we call such a setting \textit{online} (see Figures \ref{Fig:On_PO} 
and \ref{Fig:On_PE}). 
Regardless of the data collection method, two fundamental tasks in \acrshort{CPL} are 
\textit{Policy Evaluation} and \textit{Policy Optimization}. Policy Evaluation \citep{voloshin2019empirical, uehara2022review, ye2023doubly} involves estimating the value of a given \textit{target policy} with respect to a specific state distribution (see Figure \ref{Fig:Off_PE}) or assessing the value of the estimated optimal policy in online learning (see Figure \ref{Fig:On_PE}). Policy Optimization 
\citep{prudencio2023survey, Sergey2020offlineRL, liu2021map,bouneffouf2020survey,silva2022multi,zhou2015survey, shakya2023reinforcement,ladosz2022exploration,wang2022deep,moerland2023model}
focuses on determining the optimal policy that maximizes its value under certain problem-specific requirements (see Figure \ref{Fig:Off_PO}) or identifying the optimal actions that maximize the cumulative rewards during online interactions (see Figure \ref{Fig:On_PO}). In Section \ref{sec:offline_DM}, we review the literature on offline \acrshort{CPL}, while online \acrshort{CPL} is discussed in Section \ref{sec:Online CPL}.




% In an online or on-policy setting, , allowing for adaptive strategies that respond to changing environments. This setting requires balancing exploration of new strategies with exploitation of known effective ones to optimize decision-making over time.

% \textbf{Online policy learning.} Online policy learning involves two key tasks: online policy optimization and online policy evaluation. The optimization task targets to maximizing rewards, identifiying the best arm (policy), or optimizing sequential experiments to efficiently gather information. Meanwhile, policy evaluation assesses a policy's performance in real-time, allowing for continuous adjustments based on observed outcomes.

\subsection{Six Paradigms}\label{sec:paradigms}
Regardless of the specific \acrshort{CDM} task, decision-making problems in the literature can be categorized into six paradigms, each capturing common data dependencies typically assumed in practice, as illustrated in Figure \ref{Fig:paradigms} and detailed below.

\textbf{Paradigm 1: Fixed Policy with Independent States.}
As illustrated in Figure \ref{Fig:paradigms}, observations in Paradigm 1 are i.i.d. samples. Each observation consists of three components: $S_i$ is the contextual information (if available), $A_i$ is the action taken, and $R_i$ is the reward received. When contextual information is present, it would influence the choice of the action, while both the contextual information and the action jointly determine the final reward. A classical class of problems that are widely studied in this context is the single-stage \acrshort{DTR} \citep{tsiatis2019dynamic}. Literature on \acrshort{CSL} within this paradigm is discussed in Section \ref{sec:CSL_P1}, while studies on \acrshort{ATE}, \acrshort{HTE}, and mediation effect analysis are reviewed in Section \ref{sec:CEL_p1}. Additionally, this paradigm serves as the main setting in Section \ref{sec:offline_DM} to illustrate offline policy learning methods for evaluating or learning (personalized) policies that aim to maximize the immediate rewards. 

\textbf{Paradigm 2: Fixed Policy with Markovian State Transition.}
The Paradigm 2 is widely recognized as \acrfull{MDP}, characterized by the Markovian state transitions. In particular, while $A_t$ is only affected by $S_t$, both $R_t$ and $S_{t+1}$ would be affected by the pair $(S_t,A_t)$. Given $S_{t}$ and $A_t$, a standard assumption in \acrshort{MDP} problems is that $R_t$ and $S_{t+1}$ are conditionally independent of all previous observations. In Section
\ref{sec:offline_DM}, we also extend the policy evaluation or optimization techniques developed in Paradigm 1 to this setting, where the policy aims to maximize the long-term reward. 

\textbf{Paradigm 3: Fixed Policy with Non-Markovian State Transition.}
Paradigm 3, commonly assumed in multi-stage \acrshort{DTR} problems \citep{tsiatis2019dynamic} and offline non-Markovian \acrshort{RL} problems, considers all possible causal relationships under a history-independent policy. \acrshort{CPL}-related studies within this paradigm are briefly reviewed in Section \ref{sec:offline_DM}. In the context of \acrshort{CEL} within paradigm 3, we primarily focus on a specific panel data setting and discuss effect estimation with respect to evolving time, as detailed in Section \ref{sec:CEL_p3}. 


\textbf{Paradigm 4: Adaptive Policy with Independent States.}
Paradigm 4 is extensively studied in the online decision-making literature as bandit problem, where the treatment policy is time-adaptive. In this paradigm, the history $H_{t-1}$, which includes all prior observations up to time $t-1$, is used to update the action policy at time $t$, thereby influencing the decision making of action $A_t$. While $S_t$ is sampled i.i.d. from the corresponding distribution, the reward $R_t$ is influenced by both $A_t$ and $S_t$. The new observation $(S_t,A_t,R_t)$, combined with all previous observations, forms the updated history $H_{t+1}$, which then affects the next action $A_{t+1}$. A common variation of this structure is when the contextual information $S_t$ is absent. Relevant literature on policy optimization under Paradigm 4 is reviewed in Section \ref{sec:bandit_optimization}, while related policy evaluation approaches are discussed in Section \ref{sec:bandit_evaluation}.


\textbf{Paradigm 5: Adaptive Policy with Markovian State Transition.}
Building on Paradigm 4, Paradigm 5 introduces the \acrshort{MDP} framework, with adaptive policy and Markovian state transitions governing the data generation process. Specifically, $S_t$ follows a Markovian state transition, depending only on the most recent state and action, and $A_t$ is determined by the entire observation history $H_{t-1}$ through the dynamically updated action policy. This setup corresponds to the typical online \acrshort{RL} setup, which is reviewed in Section \ref{sec:RL_optimization}.


\textbf{Paradigm 6: Adaptive Policy with Non-Markovian State Transition.}
Paradigm 6 extends Paradigm 5 by relaxing the Markovian assumption, allowing for non-Markovian state transitions. This paradigm encompasses problems such as the \acrfull{POMDP}, with relevant approaches briefly reviewed in Section \ref{sec:RL_optimization}.


%Along the y-axis, we can further consider the case where the data collection policy depends on some unobservable variables, which correspond to the confounded problems. 
