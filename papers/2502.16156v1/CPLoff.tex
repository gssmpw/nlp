\section{Offline \acrlong{CPL}}
\label{sec:offline_DM}

This section presents policy evaluation and optimization methods for offline/off-policy settings (i.e., Paradigms 1-3). In contrast to online policy learning, additional data collection is infeasible in the offline setting, resulting in distribution shifts across multiple dimensions—particularly in actions and states—a critical challenge. These shifts 1) introduce selection bias that necessitates causal adjustments as employed in \acrshort{CEL}, and 2) increase uncertainty in policy evaluation and hence optimization, requiring a pessimistic or penalty-based approach to avoid over-optimization. %In this section, we primarily use Paradigm 1 to illustrate the core concepts, followed by discussions on Paradigms 2 and 3 to highlight the inherent connections among paradigms.

We begin with formal definitions of the tasks in \acrshort{CPL}. 
We use Paradigm 1 to illustrate, where the observed data consists of $n$ data points $\{(S_{i},A_{i},R_{i})\}_{1\le i\le n}$. The dataset is collected by following a stationary policy $\pi_b$, known as the \textit{behavior policy}. 
We study two tasks in offline \acrshort{CPL}:
\begin{itemize}
    \item \textbf{\acrfull{OPE}}: The goal of \acrshort{OPE} is to estimate the goodness of a given \textit{target policy} $\pi$, which is typically evaluated by the integrated value 
$\eta^{\pi} =  \mathbb{E}_{s \sim \mathbb{G}} V^{\pi}(s)$ with respect to some state distribution $\mathbb{G}$. 
    \item \textbf{\acrfull{OPO}}: The goal of \acrshort{OPO} is to solve the optimal policy $\pi^*$, or in other words, to learn a policy $\hat{\pi}$ so as to minimize the regret $\eta^{\pi^*} - \eta^{\hat{\pi}}$. 
\end{itemize}

% By definition, we directly have $\eta^{\pi} = \mathbb{E}_{s \sim \mathbb{G}, a \sim \pi(\cdot|s)} Q^{\pi}(a, s)$. 
% In addition to a point estimator, many applications would benefit from having a CI for $\eta^{\pi}$. 

% We refer to an interval $[\hat{\eta}^{\pi}_l, \hat{\eta}^{\pi}_u]$ as an $(1-\alpha)$-CI for $\eta^{\pi}$ if and only if $P(\hat{\eta}^{\pi}_l \le \eta^{\pi} \le \hat{\eta}^{\pi}_u) \ge 1 - \alpha$, for any $\alpha \in (0, 1)$.  


% \textbf{Off-Policy Optimization(OPO).} 
% \end{definition}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%% Causal Identifiability for OPO in MDP %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \textbf{Causal Identifiability: }
% In general, the set $W^*$ cannot be observed, whereas at time $t$, we observe the state-action-outcome triplet $(S_t,A_t,Y_t)$. 
% For any $t\ge 0$, let $\bar{A}_t=(A_0,A_1,\cdots,A_t)^\top$ denote the observed treatment history. 
% Similar to our discussions in previous chapters, 
% the off-policy evaluation/optimization tasks requires certain assumptions to ensure the causal identifiability. 
% The two critical assumptions are: 

% \textbf{(CA) Consistency assumption}: $S_{t+1}=S_{t+1}^*(\bar{A}_{t})$ and $Y_t=Y_t^*(\bar{A}_t)$ for all $t\ge 0$, almost surely.

% \textbf{(SRA) Sequential randomization assumption}: $A_t\perp W^*| S_{t}, \{S_j,A_j,Y_j\}_{0\le j<t}$.

% The CA requires that the observed state and outcome correspond to the potential state and outcome whose treatments are assigned according to the observed treatment history. 
% It generalizes SUTVA to our setting, allowing the potential outcomes to depend on past treatments. 
% The SRA implies that  there are no unmeasured confounders and it automatically holds in online randomized experiments (or when all trajectories are collected by following policies that depend on the same set of state variables), in which the treatment assignment mechanism is pre-specified. 
% In SRA, we allow $A_t$ to depend on the observed data history $S_{t}, \{S_j,A_j,Y_j\}_{0\le j<t}$ and thus, the treatments can be adaptively chosen.  


% In addition, these two conditions guarantee that MA and CMIA hold on the observed dataset as well.
% \begin{eqnarray}\label{eqn:Markovobserve}
% 	P(S_{t+1}\in \mathcal{S}|A_t,S_t,\{S_j,A_j,Y_j\}_{0\le j<t})&=&\mathcal{P}(\mathcal{S};A_t,S_t),\\\label{eqn:robserve}
% 	\mathbb{E}(Y_t|A_t,S_t,\{S_j,A_j,Y_j\}_{0\le j<t})&=&r(A_t,S_t).
% \end{eqnarray}
% As such, $\mathcal{P}$ corresponds to the transition function that defines the next state distribution conditional on the current state-action pair and $r$ corresponds to the conditional expectation of the immediate reward as a function of the state-action pair. 
% In this paper, we may use both the potential outcomes and the observed variables interchangeably. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%% Below are for the MDP SETTING from cc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% In the off-policy setting, the observed data consists of $n$ i.i.d. trajectories $\{(S_{i,t},A_{i,t},R_{i,t},S_{i,t+1})\}_{0\le t<T_i,1\le i\le n}$, where $T_i$ denotes the length of the $i$th trajectory. Without loss of generality, we assume $T_1=\cdots=T_n=T$ and the immediate rewards are uniformly bounded. 
% The dataset is collected by following a stationary policy $b$, known as the \textit{behavior policy}. 

% \textbf{Off-Policy Evaluation(OPE).} The goal of OPE is to estimate the value of a given \textit{target policy} $\pi$ with respect to the initial state distribution $\mathbb{G}$, defined as 
% \begin{eqnarray}\label{eqn:def_value}
% 	\eta^{\pi} =  \mathbb{E}_{s \sim \mathbb{G}} V^{\pi}(s). 
% \end{eqnarray} 
% By definition, we directly have $\eta^{\pi} = \mathbb{E}_{s \sim \mathbb{G}, a \sim \pi(\cdot|s)} Q^{\pi}(a, s)$. 

% In addition to a point estimator, many applications would benefit from having a CI for $\eta^{\pi}$. 
% We refer to an interval $[\hat{\eta}^{\pi}_l, \hat{\eta}^{\pi}_u]$ as an $(1-\alpha)$-CI for $\eta^{\pi}$ if and only if $P(\hat{\eta}^{\pi}_l \le \eta^{\pi} \le \hat{\eta}^{\pi}_u) \ge 1 - \alpha$, for any $\alpha \in (0, 1)$.  

% \textbf{Off-Policy Optimization(OPO).} The goal of OPO is to solve the optimal policy $\pi^*$, or in other words, to learn a policy $\hat{\pi}$ so as to minimize the regret $\eta^{\pi^*} - \eta^{\hat{\pi}}$. 


% \textbf{Causal Identifiability: }
% In general, the set $W^*$ cannot be observed, whereas at time $t$, we observe the state-action-outcome triplet $(S_t,A_t,Y_t)$. 
% For any $t\ge 0$, let $\bar{A}_t=(A_0,A_1,\cdots,A_t)^\top$ denote the observed treatment history. 
% Similar to our discussions in previous chapters, 
% the off-policy evaluation/optimization tasks requires certain assumptions to ensure the causal identifiability. 
% The two critical assumptions are: 

% \textbf{(CA) Consistency assumption}: $S_{t+1}=S_{t+1}^*(\bar{A}_{t})$ and $Y_t=Y_t^*(\bar{A}_t)$ for all $t\ge 0$, almost surely.

% \textbf{(SRA) Sequential randomization assumption}: $A_t\perp W^*| S_{t}, \{S_j,A_j,Y_j\}_{0\le j<t}$.

% The CA requires that the observed state and outcome correspond to the potential state and outcome whose treatments are assigned according to the observed treatment history. 
% It generalizes SUTVA to our setting, allowing the potential outcomes to depend on past treatments. 
% The SRA implies that  there are no unmeasured confounders and it automatically holds in online randomized experiments (or when all trajectories are collected by following policies that depend on the same set of state variables), in which the treatment assignment mechanism is pre-specified. 
% In SRA, we allow $A_t$ to depend on the observed data history $S_{t}, \{S_j,A_j,Y_j\}_{0\le j<t}$ and thus, the treatments can be adaptively chosen.  


% In addition, these two conditions guarantee that MA and CMIA hold on the observed dataset as well.
% \begin{eqnarray}\label{eqn:Markovobserve}
% 	P(S_{t+1}\in \mathcal{S}|A_t,S_t,\{S_j,A_j,Y_j\}_{0\le j<t})&=&\mathcal{P}(\mathcal{S};A_t,S_t),\\\label{eqn:robserve}
% 	\mathbb{E}(Y_t|A_t,S_t,\{S_j,A_j,Y_j\}_{0\le j<t})&=&r(A_t,S_t).
% \end{eqnarray}
% As such, $\mathcal{P}$ corresponds to the transition function that defines the next state distribution conditional on the current state-action pair and $r$ corresponds to the conditional expectation of the immediate reward as a function of the state-action pair. 
% In this paper, we may use both the potential outcomes and the observed variables interchangeably. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Should we learn a policy with action as Value Price Ratio or Product Quality? 
% What confounders (contexts/features) to include in learning the policy? 



% Key methods and references include  
% offline policy learning  methods with i.i.d. data \citep{murphy2005generalization}, 
% off-policy evaluation methods in Markovian environments \citep{ thomas2015safe, jiang2016doubly, shi2020statistical}, 
% offline policy optimization methods in Markovian environments \citep{ernst2005tree}, 
% offline policy learning in non-Markovian environments \citep{ shi2022off}. 
% The focus would be on the internal relation and differences between these paradigms and related methods. 



% �� Data is from a fixed policy
% Learning mode is familiar to most people
% Inference is easier
% Infrastructure is simpler




% no counter-factual
% notations with OWL, ...


% etc. 


% remove the confounders with  XX. 

% The most straightforward approach for OPE is the direct method (DM). As suggested by the name, methods belonging to this category will first directly impose a model for either the environment or the Q-function, and then learn the model by regarding the task as a regression (or classification) problem, and finally calculate the value of the target policy via a plug-in estimator according to the definition of 



\subsection{Offline Policy Evaluation}\label{sec:OPE}
OPE focuses on estimating the expected reward of an evaluation policy using historical data generated by a different behavior policy. This is particularly valuable in offline RL settings, where experimenting with policies is not possible due to ethical, financial, or safety concerns. OPE methods have gained importance across fields, including healthcare, education, and recommendation systems, where reliable evaluation of new policies without online testing is critical.


\textbf{Model-based estimators.} Model-based \acrshort{OPE} \citep{paduraru2013off, yin2020asymptotically} approaches estimate state transition and reward functions directly from data, which can then be used to simulate trajectories and estimate policy value. These methods achieve asymptotic efficiency in discrete \acrshort{MDP}s. 
Such estimators often leverage probabilistic neural networks to model transitions, improving performance in complex continuous control tasks. Although model-based methods allow for easier parameter tuning, particularly through supervised learning techniques, they can struggle in high-dimensional settings where modeling state transitions becomes more complex than direct estimation of value functions.


% \subsubsection{Main Ideas of OPE}\label{sec:OPE_1}
\textbf{Model-free estimators.}
To adjust for selection bias caused by the distribution shift in offline dataset, the most popular methods include \acrshort{DM}, \acrshort{IPW}, and \acrshort{DR} estimators. 
The classic forms of these methods can be derived from the following relationship: 
\begin{align}
% \begin{split}
    \eta(\pi) 
    = \mathbb{E}_{a \sim \pi(\cdot|S), s \sim p(S) } R(a) 
    &= \mathbb{E}_{a \sim \pi(S), s \sim p(S)} \Big[\mathbb{E} \big\{R(a)|S = s \big\}\Big]\label{eqn:ope_1}\\
    &= \mathbb{E}_{a \sim b(S), s \sim p(S)} \frac{\pi(a|s)}{b(a|s)} R(a)\label{eqn:ope_2}\\
    &= \mathbb{E}_{a \sim b(S), s \sim p(S)} \frac{\pi(a|s)}{b(a|s)} \Big[R(a) - V(s)\Big] + \mathbb{E}_{s \sim p(S)} V(s)\label{eqn:ope_3}. 
\end{align}
% \end{split}
% \end{equation}
Specifically, by replacing $\mathbb{E} \big\{R(a)|S = s \big\} = Q(s, a)$ in \eqref{eqn:ope_1} with its estimator, we obtain the direct method estimator; 
by replacing the expectation over $R(a)$ in \eqref{eqn:ope_2} with a sample average of the observed rewards under action $a$, we obtain the \acrshort{IPW} estimator; 
and finally we can combine these two approaches in \eqref{eqn:ope_3} to derive the \acrshort{DR} estimator. 
Notably, it is easy to see that these three methods are direct extensions of their counterparts in \acrshort{CEL} (Equations \eqref{eq:CEL_p1_DM}, \eqref{eq:CEL_p1_IS} and 
 \eqref{eq:CEL_p1_DR}), by taking additional expectation over the state and action distributions. 
Similar to the argument in \acrshort{CEL}, these methods effectively employ different ways to adjust for the confounding effect from $s$, by either removing its imbalance across actions or its impact on the rewards. 
% \textbf{[Add some references]}
% Any refenrences? 
% https://arxiv.org/pdf/1103.4601
% 

 


% \subsubsection{Extensions to Paradigm 2 and 3}


% introduced in Section \ref{sec:OPE_1} for Paradigm 1 
\textbf{Extensions to paradigms 2 and 3.}
These methods can all be extended to more complicated settings in Paradigms 2 and 3, by additionally accounting for the dependency over decision points. 
To simplify the problem in Paradigm 2, we can utilize the \textit{recursive} or \textit{iterative} structure. 
Take the direct method as an example, where as long as we can obtain an estimate of the Q-function, we can directly take its expectation to calculate the value as in \eqref{eqn:ope_1}. 
To estimate the $Q$-function, we introduce two prominent approaches. 
The most straightforward method, \acrfull{FQE} \citep{le2019batch}, leverages the Bellman Optimality Equation \eqref{eqn:bellman_Q} which characterizes the sequential dependency structure. 
\acrshort{FQE} solves the loss function corresponding to \eqref{eqn:bellman_Q} until we converge to a final value function estimator. 
Another method is Minimax Q-Learning \citep{uehara2020minimax}, which enhances Q-function evaluation by framing it as a competition between two components: the Q-function itself and a discriminator function. This method leverages the Bellman equation, where the discriminator is introduced to assess differences between the predicted and actual rewards, guiding the learning process to focus on areas of high prediction error. By balancing estimation errors with a tuning parameter and carefully choosing model classes, the approach becomes robust against specific data patterns and high variance.




% We can first extend Equation \eqref{eqn:ope_1} to Paradigm 2 as follows (the extensions to non-markovian or non-stationary cases can be similarly derived): 
% $    \eta({\pi}) 
%     = \mathbb{E}^{{\pi}}_{a_0 \sim \pi(s_0), s_0 \sim p_0(s)} \Big[\sum_{t \ge 0} \gamma^t R_t\Big]
%     = \mathbb{E}_{a_0 \sim \pi(S_0), s_0 \sim p_0(s)} \Big[ 
%     Q_0^{{\pi}}(s_0, a_0)
%     \Big]\label{eqn:ope_11}. 
% $


The \acrshort{IPW} and \acrshort{DR} methods can similarly be extended to Paradigms 2 and 3. For example, for \acrshort{IPW}, we can replace the density ratio by that along the entire trajectory $\prod_{t'=0}^{t} [\pi(A_{i,t'}|S_{i,t'}) /b(A_{i,t'}|S_{i,t'})].$ 
However, these traditional \acrshort{IS} methods (and related \acrshort{DR} methods) have exponential variance with the number of steps and hence will soon become unstable when the trajectory is long.  
To avoid this issue, various structural assumptions have been utilized. 
One notable advance is by considering the marginal importance ratio under stationary assumptions \citep{liu2018breaking, dai2020coindice, shi2021deeply, zhu2023robust}, where essentially we consider the average density of visiting a state instead of considering the different densities at different time points, which allows us to greatly reduce the problem dimension.  
Similar techniques have been extended to the \acrshort{DR} estimator as well \citep{jiang2016doubly}, notably the Double Reinforcement Learning method \citep{kallus2022efficiently}

% We introduce the average visitation distribution under a policy $\pi$ as  
% $d^{\pi}(s)= (1 - \gamma)^{-1} \sum_{t=0}^{+\infty} \gamma^{t} p_t^{\pi}(s)$, where $p_t^{\pi}(s)$ denotes the probability of $\{S_t = s\}$ following policy $\pi$ with  $S_{0}\sim \mathbb{G}$. 
% Define $\widetilde{\omega}^{\pi}(s) = d^{\pi}(s) / d^{b}(s)$. 
% Therefore, $\widetilde{\omega}^{\pi}(s)$ can be understood as a marginalized version of the importance ratio. 
% With a similar change-of-measure trick as in IS, we can obtain the relationship that 
% $
%     \eta (\pi) =  \mathbb{E}_{(s,a) \sim b(s, a), r \sim \mathcal{R}(\cdot; s, a)} 
% \widetilde{\omega}^{\pi}(s) \frac{\pi(a|s)}{b(a|s)} r. 
% $

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Specifically, it learns the $Q$-function by recognizing that the true value function $Q^\pi$ is the unique solution to the Bellman equation \eqref{eqn:bellman_Q}, with the right-hand side of \eqref{eqn:bellman_Q} as a contraction mapping. 
% Starting with an initial estimate $\widehat{Q}^{0}$, 
% FQE iteratively solves the following optimization problem, 
% $\widehat{Q}^{{\ell}}=\arg \min_{Q} 
% 	\sum_{\substack{i \le n}}\sum_{t<T}
% 	\Big\{
% 	\gamma \mathbb{E}_{a' \sim \pi(\cdot| S_{i, t+1})} \widehat{Q}^{\ell-1}(a',S_{i, t+1}) 
% 	+R_{i,t}- Q(A_{i, t}, S_{i, t})  
% \Big\}^2,
% $
% for $\ell=1,2,\cdots$, until convergence. 

% Similarly, the IPW and DR methods can be extended to MDP as well, using the relationships 
% $
% \eta({\pi}) 
% = \mathbb{E}^{b}_{a_0 \sim b(S_0), s_0 \sim p_0(s)} 
% \prod_{t \ge 0} \frac{{\pi}(A_{t}|S_{t})}{b(A_{t}|S_{t})}
% \sum_{t \ge 0} \gamma^t R_t
% $
% and 
% $
% \eta({\pi}) 
% = \mathbb{E}^{{b}}_{a_0 \sim b(S_0), s_0 \sim p_0(s)} 
% \prod_{t \ge 0} \frac{{\pi}(A_{t}|S_{t})}{{b}(A_{t}|S_{t})}
% \Bigg[ \sum_{t \ge 0} \gamma^t R_t - V^{{\pi}}_0(s_0)\Bigg] + \mathbb{E}_{s_0 \sim p_0(s)} V^{{\pi}}_0(s_0). 
% $, respectively. 

% from which the DM, IPW, and DR estimators can be similarly derived. 

%\textbf{Recursive/iterative structure.} 


% The most straightforward approach for OPE is the direct method (DM). 
% As suggested by the name, methods belonging to this category will first directly impose a model for either the environment or the Q-function, and then learn the model by regarding the task as a regression (or classification) problem, and finally calculate the value of the target policy via a plug-in estimator according to the definition of $\eta^\pi$
% The Q-function based approach and the environment-based approach are also called as model-free and  model-based, respectively. 

% It is observed to perform consistently well in a large-scale empirical study {cite:p}`voloshin2019empirical`. 

% ***Advantages***:
% 1. Conceptually simple and easy to implement
% 2. Good numerical results when the the model class is chosen appropriately 

% **Q-function.**
% The Q-function-based approach aims to direct learn the state-action value function (referred to as the Q-function) 
% \begin{eqnarray}
% Q^\pi(a,s)&= \mathbb{E}^{\pi} (\sum_{t=0}^{+\infty} \gamma^t R_{t}|A_{0}=a,S_{0}=s)   
% \end{eqnarray}
% of the policy $\pi$ that we aim to evaluate. 

% The final estimator can then be constructed by plugging $\hat{Q}^{\pi}$ in the definition $\eta^{\pi} = \mathbb{E}_{s \sim \mathbb{G}, a \sim \pi(\cdot|s)} Q^{\pi}(a, s)$. 

% **Bellman equations.**
% The Q-learning-type evaluation is commonly based on the Bellman equation for the Q-function of a given policy $\pi$ 
% \begin{equation}\label{eqn:bellman_Q}
%     Q^\pi(a, s) = \mathbb{E}^\pi \Big(R_t + \gamma Q^\pi(A_{t + 1}, S_{t+1})  | A_t = a, S_t = s \Big).  \;\;\;\;\; \text{(1)} 
% \end{equation}

% TODO: Highlight the counter-factual in the above notations. 


% My_Written_Prelim: https://www.overleaf.com/project/602dd710e0162de8e98f2999
%\textbf{Breaking the curse of horizon. } 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MDP + IPW/DR: 
% Many variants to reduce the variance caused by the long horizon
% Importance sampling estimators: Liu et al. (2018); Dai et al. (2020);etc. 
% Doubly robust style estimators: Jiang and Li (2016); Kallus and Uehara(2019); etc.



% \subsubsection{Extensions}
% %%% No from my slides - few from our website
% % More from the review papers. : not that helpful...
% The OPE has received great attention in the past decade with a large number of works extending the main ideas above. 
% A comprehensive review can be found in \cite{uehara2022review}. 
% A remarkable observation is that the causal tools have an increasing impact on this area. 
% For example, \cite{shi2022multi} studies accomodating the failure of the SUTVA assumption with inteference. 
% %   title={A multi-agent reinforcement learning framework for off-policy evaluation in two-sided markets
% The unmeasure confounder issue also receives much attentions: \citep{namkoong2020off, kallus2020confounding} aim to to develop partial identification bounds for the value based on sensitivity analysis inspired on the causal literature, 
% and \citep{fu2022offline, shi2022off} develop estimators based on certain proxy variables under the confounded MDP model. 




%   title={Off-policy confidence interval estimation with confounded markov decision process},

% @inproceedings{,
% 	title        = {Safe exploration for efficient policy evaluation and comparison},



% voloshin2019empirical


% shi2020does,
%   title={Does the markov decision process fit the data: Testing for the markov property in sequential decision making

\subsection{Offline Policy Optimization}\label{sec:OPO}

Another central task is to learn a good policy from the offline data. 
Formally, we want to find
$\pi^* = \arg \max_{\pi} \eta\big(\pi\big). $
Similar to OPE, one key challenge is to adjust the selection bias caused by the distribution shift in offline data, therefore we will see similar tools in \acrshort{OPE} (and also \acrshort{CEL}) are extended here. 
% We will focus on two main categories of OPO methods, namely value- and policy-based approches. 
We will additionally consider the unique goal of policy learning itself to design the so-called \textit{pessimism}-based algorithms. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Model-Free Value-based Approches}
The first class of algorithms is \textit{value}-based, focusing on utilizing the value function. \textit{Q-learning} is, arguably, the most popular algorithm due to its simplicity and good performance. Noting that 
% To find the optimal policy is equivalent to solve
$\pi^*(\boldsymbol{s}) =  \text{arg max}_{a}Q(\boldsymbol{s}, a),$ 
   % \label{eqn:q-learning}
% \end{align} 
the core of Q-learning is a regression modeling problem based on positing regression models for outcome. 
Overall, Q-learning is practical and easy to understand, as it allows straightforward implementation of diverse established regression methods. 
Different Q-function model classes (such as linear models, sparse linear models, neural networks, etc.) and their statistical properties have been studied extensively in the literature \citep{song2015penalized, zhu2019proper}. 


In some cases, Q-learning could be overkill for policy optimization: for decision making what we need to know is which action is the best, which could be similar to knowing the expected potential outcome of all actions. In such cases, Advantage-learning (A-learning) \citep{murphy2003optimal, robins2004optimal, schulte2014q} offers a more efficient alternative by modeling only the contrasts between treatments and a control action, as 
% Alternatively, with the contrast function $C_j(\boldsymbol{S})$ which will be defined later,
$
Q(\boldsymbol{s}, a) = Q(\boldsymbol{s},0) + A(\boldsymbol{s}, a). 
% ,\quad j=0,\dots,m-1.
$
With the A-function $A(\boldsymbol{s}, a)$, we have that
$
\pi^*(\boldsymbol{s}) =  \text{arg max}_{a \neq 0}A(\boldsymbol{s}, a) \mathbb{I}(\text{max}_{a \neq 0}A(\boldsymbol{s}, a) > 0).
$
Similar to Q-learning, various regression functions can be used to specify the advantage function. Typically, the underlying relationship in the advantage functions is simpler than that in $Q(\boldsymbol{s},0)$, which is a \textit{nuisance} function in decision making. 
The extension of A-learning to high-dimensional \citep{shi2018high} and non-parametric models \citep{liang2018deep} have also been studied.


% \label{eqn:q-learning}

% at each decision point. 
% with finite decision points is mainly 

% Early in 2000, as a classic method of Reinforcement Learning, Q-learning was adapted to decision-making problems[1] and kept evolving with various extensions, such as 



% The target of Q-learning is to find an optimal policy $\pi$ that can maximize the expected reward received. In other words, by training a model with the observed data, we hope to find an optimal policy to predict the optimal action for each individual to maximize rewards. For example, considering the motivating example **Personalized Incentives**, Q-learning aims to find the best policy to assign different incentives ($A$) to different users to optimize the return-on-investment ($R$). 

% Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., A,B,C,D), and the outcome of interest R is **continuous** and **non-negative**, where the larger the $R$ the better.

% Q-learning with a single decision point is mainly a regression modeling problem, as the major component is to find the relationship between the expectation of potential reward $R(a)$ and $\{\boldsymbol{s},a\}$. 


% The key steps are as follows: 
% \begin{enumerate}
%     \item Fitted a model $\hat{Q}(\boldsymbol{s},a,\hat{\boldsymbol{\beta}})$, which can be solved directly by existing approaches (i.e., OLS, .etc),
%     \item For each individual find the optimal action $d^{opt}(\boldsymbol{s}_{i})$ such that $d^{opt}(\boldsymbol{s}_{i}) = \text{arg max}_{a}\hat{Q}(\boldsymbol{s}_{i},a,\hat{\boldsymbol{\beta}})$.
% \end{enumerate}



% \textbf{A-Learning (Single Stage). }
% A-Learning, also known as Advantage Learning, is one of the main approaches to learning the optimal regime and works similarly to Q-learning. However, while Q-learning requires positing regression models to fit the expected outcome, A-learning models the contrasts between treatments and control, directly informing the optimal decision. A detailed comparison between Q-learning and A-learning can be found in [1]. While [1] mainly focus on the case with binary treatment options, a complete review of A-learning with multiple treatment options can be found in [2]. Here, following the algorithm in [1], we consider contrast-based A-learning. However, there is an alternative regret-based A-learning introduced in [3]. Some recent extensions to conventional A-learning, such as deep A-learning [4] and high-dimensional A-Learning [5], will be added soon. Overall, A-learning is doubly-robust. In other words, it is less sensitive and more robust to model misspecification. 
% Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., 0,1,2,3,4, where 0 stands for the control group by convention), and the outcome of interest R is **continuous** and **non-negative**, where the larger the $R$ the better. 
% Suppose there are $m$ number of options, and the action space $\mathcal{A}=\{0,1,\dots,m-1\}$. Contrast-based A-learning, as the name suggested, aims to learn and estimate the constrast function, $C_{j}(\boldsymbol{S})$ for each treatment $j=1,2,\cdots, m-1$. Furthermore, we also need to posit a model for the conditional expected potential outcome for the control option (treatment $0$), $Q(\boldsymbol{S},0)$, and the propensity function $\omega(\boldsymbol{S},A)$, if the true values are not specified. Detailed definitions are provided in the following.
% *   Q-function:
%     \begin{align}
%     Q(\boldsymbol{s},a)=E[R(a)|\boldsymbol{S}=\boldsymbol{s}],
%     \end{align}
%     Alternatively, with the contrast function $C_j(\boldsymbol{S})$ which will be defined later,
%     \begin{align}
%     Q(\boldsymbol{s},j) = Q(\boldsymbol{s},0) + C_{j}(\boldsymbol{s}),\quad j=0,\dots,m-1.
%     \end{align}
% *   Contrast functions (optimal blip to zero functions)
%     \begin{align}
%     C_{j}(\boldsymbol{s})=Q(\boldsymbol{s},j)-Q(\boldsymbol{s},0),\quad j=0,\dots,m-1,
%     \end{align}
%     where $C_{0}(\boldsymbol{s}) = 0$.

% *   Optimal regime
%     \begin{align}
%     d^{opt}(\boldsymbol{s})=\arg\max_{j\in\mathcal{A}}C_{j}(\boldsymbol{s})
%     \end{align}
% Positting models, $C_{j}(\boldsymbol{s},\boldsymbol{\psi}_{j})$,$Q(\boldsymbol{s},0,\boldsymbol{\phi})$,and $\omega(\boldsymbol{s},a,\boldsymbol{\gamma})$, A-learning aims to estimate $\boldsymbol{\psi}_{j}$, $\boldsymbol{\phi}$, and $\boldsymbol{\gamma}$ by g-estimation. With the $\hat{\boldsymbol{\psi}}_{j}$ in hand, the optimal decision $d^{opt}(\boldsymbol{s})$ can be directly derived.


% *   Propensity score
%     \begin{align}
%     \omega(\boldsymbol{s},a)=P(A=a|\boldsymbol{S}=\boldsymbol{s})
%     \end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% extending \eqref{eqn:q-learning} and
\textbf{Extensions to paradigms 2 and 3.}
These approaches can be similarly extended to MDP and non-Markovian decision processes. 
For example, the fitted-Q iteration \citep{ernst2005tree} extends the single-period Q-learning, by noting that 
the optimal value function $Q^*$ is the unique solution to the Bellman optimality equation \eqref{eqn:bellman_Q}. 
Additionally, the right-hand side of \eqref{eqn:bellman_Q} is a contraction mapping, allowing us to consider a fixed-point method similar to fitted-Q evaluation. 
A-learning is also recently extended to the \acrshort{MDP} setting \citep{shi2024statistically}. 
Extending to non-Markovian problems, such as \acrshort{DTR} in precision medicine and decision science,  
we can estimate the optimal dynamic treatment regimes (policy) via G-estimation \citep{stephens2015chapter, robins2004proceedings}, a type of A-learning \citep{schulte2014q, shi2018high}. 
Q-learning can also be extended to non-markovian problems via recursive regression \citep{song2015penalized}. 
The main challenge comes from the increasing dimensionality with the expanding horizon, as the loss of the Markovian property requires us to use the full history in the feature space instead of only the latest state variable. 

% With an initial estimate $\widehat{Q}^{0}$, 
% FQI iteratively solves the following optimization problem: 
% $	\widehat{Q}^{{\ell}}=\arg \min_{Q} 
% 	\sum_{\substack{i \le n}}\sum_{t<T}
% 	\Big\{
% 	\gamma \max_{a'} \widehat{Q}^{\ell-1}(a',S_{i, t+1}) 
% 	+R_{i,t}- Q(A_{i, t}, S_{i, t})  
% \Big\}^2, 
% $
% for $\ell=1,2,\cdots$, until convergence. 
% The final estimate is denoted as $\widehat{Q}_{FQI}$. 




% The extension of A-learning to high-dimensional \cite{shi2018high} and non-parametric models \citep{liang2018deep} have also been studied.

% It is mainly motivated by the fact that, 

% MDP, DM: 
% FQE: Le et al. (2019); 
% Feng et al. (2020); Shi et al. (2021)

% (section:FQI)=
% # Fitted-Q Iteration

% ## Main Idea

% **Q-function.**
% The Q-function-based approach aims to direct learn the state-action value function (referred to as the Q-function) 
% \begin{eqnarray}
% Q^\pi(a,s)&= \mathbb{E}^{\pi} (\sum_{t=0}^{+\infty} \gamma^t R_{t}|A_{0}=a,S_{0}=s)   
% \end{eqnarray}
% of either the policy $\pi$ that we aim to evaluate or the optimal policy $\pi = \pi^*$. 


% **FQI.**
% Similar to [FQE](section:FQE), the fitted-Q iteration (FQI) {cite:p}`ernst2005tree` algorithm is also popular due to its simple form and good numerical performance. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Model-Free Policy-based Approaches}
For interpretability, domain constraints, or statistical efficiency, it may be preferable to directly learn a policy $\pi$ within a pre-specified (parametric or non-parametric) policy class $\Pi$ as 
$\pi^* = \argmax_{\pi \in \Pi} \eta(\pi), $
% The simplist approach is to
where the policy value $\eta(\cdot)$ can be estimated via various \acrshort{OPE} methods discussed above. This estimated value is then incorporated into an optimization process to solve the $\argmax$ via off-the-self \textit{optimization algorithms} (such as the L-BFGS-B) \citep{kitagawa2018should, zhao2012estimating, liu2018augmented}. 

In particular, when we use \acrshort{IPW} as the policy value estimator, we can re-write the objective as 
\[
    \pi^* = 
    % \argmax_{\pi \in \Pi} V(\pi) = 
    \argmax_{\pi \in \Pi} \mathbb{E} \Big[ \frac{R_i}{b(A_i | S_i)} \mathbb{I}(A_i \neq \pi(S_i) )\Big]. 
\]
When $R_i$ is non-negative, this goal corresponds to the objective function of a cost-sensitive classification problem 
with ${R_i}/{b(A_i|S_i)}$ as the weight, 
$A_i$ as the true label, 
and $\pi$ as the classifier to be learned. 
Then, any popular cost-sensitive classifiers, such as \acrfull{SVM} and \acrfull{CART}, can be applied to solve the policy learning problem. 
This is called outcome-weighted learning \citep{zhao2012estimating, liu2018augmented, song2015sparse}, providing flexibility in high-dimensional and complex scenarios. 
Furthermore, this framework can be extended to incorporate the \acrshort{DR} estimator, enhancing robustness against misspecifications of the propensity score model or the outcome model. 


Decision lists and tree-based structures are interpretable approaches in policy learning that provide a clear framework for treatment decisions within dynamic treatment regimes. 
Decision lists operate as sequential if-then rules, where each rule specifies conditions based on patient characteristics to guide treatment selection in a straightforward, deterministic manner \citep{zhang2018interpretable, tschernutter2022interpretable}. 
% In contrast, decision trees offer a hierarchical approach, using recursive splits to accommodate more complex interactions among features and allowing for a more flexible policy structure \citep{song2015sparse, zhang2015multi}. 
It is advantageous for transparency. For increased interpretability, sparse decision lists and pruned trees reduce model complexity, maintaining essential decision criteria without sacrificing clarity.  

\textbf{Extensions to paradigms 2 and 3.}
Extensions of this approach have also been developed to address more complex settings, such as paradigms 2 and 3, which involve multi-stage decision-making scenarios \citep{liao2022batch, chen2023steel}. 
Essentially, we replace the \acrshort{OPE} methods in Paradigm 1 with their counterparts in Paradigms 2 and 3 introduced above. 

% \citep{zhao2012estimating, zhang2015multi}.

% 3. Lou, Zhilan, Jun Shao, and Menggang Yu. "Optimal treatment assignment to maximize expected outcome with multiple treatments." Biometrics 74.2 (2018): 506-516.


% Other learning methods: Concordance-assisted learning (Fan et al. 2016); Entropy learning (Jiang et al., 2019)


% really OPO?



% Extend to MDP and non-Markovian
% \begin{align*}
% &\nabla_{\theta}
%     \mathbb{E}
%     % ^{{b}}_{a \sim b(S), s \sim p(S)} 
%     \prod_{t \ge 0} \frac{\pi_{\theta}(A_{t}|S_{t})}{b(A_{t}|S_{t})}
% \sum_{t \ge 0} R_t\\
% &= 
%     \mathbb{E}
%     \Bigg\{
%     % ^{{b}}_{a \sim b(S), s \sim p(S)} 
%     \Big[
%     \prod_{t \ge 0} \frac{\pi_{\theta}(A_{t}|S_{t})}{b(A_{t}|S_{t})}
% \sum_{t \ge 0} R_t
% \Big]
% \times
% \nabla_{\theta}
% \log 
% \prod_{t \ge 0} \pi_{\theta}(A_{t}|S_{t})
% \Bigg\}
% \end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Model-Based Approaches} 
\acrfull{MBRL} is a technique that leverages explicit models of the environment’s dynamics to guide policy learning. This approach, rather than relying solely on observed rewards, uses a parameterized model to predict state transitions and rewards, enabling it to generate synthetic experiences that help train policies without direct interaction with the environment. Techniques in \acrshort{MBRL} can include learning the dynamics of the environment to inform both planning and control, making it possible to learn policies even in complex, high-dimensional spaces \citep{deisenroth2011pilco}. These methods have shown effectiveness in offline reinforcement learning settings, as they mitigate the limitations of direct interaction by enabling supervised learning methods to fit the model and then use it for training or planning in a simulated environment \citep{sutton1991dyna, levine2016end}.

One key advantage of \acrshort{MBRL} is its potential for sample efficiency, as it reuses past experiences by generating additional trajectories, thus enhancing policy learning. Additionally, model-based methods are versatile; they can integrate uncertainty estimation techniques to counteract distributional shifts. 
By estimating epistemic uncertainty, \acrshort{MBRL} can prevent the exploitation of inaccuracies in the learned model. Recent studies, such as those using model-predictive control and policy rollouts, indicate promising results in high-dimensional tasks and show robust performance under various degrees of distributional shift, further affirming MBRL as a viable solution for offline policy learning \citep{nagabandi2018neural, chua2018deep}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Address Increased Uncertainty from Distribution Shift}\label{sec:pessimism}

Besides the selection bias caused by the distribution shift in offline data, another prominent issue is the inflated uncertainty. 
The increased uncertainty is resulted from the inherent limitations in the observational data used to inform policy decisions, which often fails to comprehensively represent the entire state and action space. 
Models trained on such non-representative data can yield overoptimistic predictions about the outcomes of actions, especially those that deviate substantially from the behavior policy used during data collection. As a result, suboptimal decisions would be made, as one policy may appear to be better just because its value estimate has a bigger variance. 
This problem is exacerbated in complex environments where the state and action spaces are vast and diverse, increasing the likelihood of encountering unrepresented scenarios. 

To mitigate the risks associated with overoptimistic predictions,  penalty or pessimism strategies are employed in offline policy learning. 
Penalty-based methods \citep{wu2019behavior, jaques2019way}  or constraint-based methods \citep{kumar2019stabilizing, fujimoto2019off, siegel2020keep} explicitly encourage or require the estimated optimal policy to stay close to the data distribution by introducing a penalty or constraint for taking actions that lead to high uncertainty. This penalty discourages the selection of such actions, steering the policy towards actions with more predictable outcomes based on the available data. 

In contrast, 
pessimism-based methods \citep{cief2022pessimistic, jeunen2021pessimistic, rashidinejad2021bridging, jeunen2021pessimistic, zhou2023optimizing, chen2023steel} use an implicit and data-driven way to stay conservative and close to data distribution. 
It is typically based on explicit uncertainty quantification for the value estimates and then selects the policy that optimizes the value lower bounds. 
This approach hence reduces the likelihood of the algorithm recommending policies that just happen to be optimal due to high uncertainty. 
Theoretically, the pessimism-based algorithms can find an optimal policy when the data cover the trajectories of an optimal policy, an assumption that is much weaker than the full-coverage requirement. 


In summary, the necessity for value pessimism, policy penalty or policy constraint in offline policy learning arises from the need to counteract the inherent uncertainties associated with training models on limited observational data. 
By adopting these strategies, the reliability and safety of the policies derived from offline learning are enhanced, leading to more robust and effective decision-making in practice.

% Paradigm 1 (more pessimistic): 
% Li et al. (2022), Dong et al. (2023)


% Survey for DTR: a few papers

% Paradigm II: 
% A survey on offline reinforcement learning:
% prudencio2023survey
% levine2020offline


% \textbf{The below is not closely related to causal? maybe mediation is? because no confounder and correct DM?}

%%% Already: 
% kallus2021minimax
%   title={Minimax-optimal policy learning under unobserved confounding

% xu2023instrumental
%   title={An instrumental variable approach to confounded off-policy evaluation


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Paradigm I: 
% Quantile treatment regimes (Wang et al. 2017)

% *   Lan Wang, Yu Zhou, Rui Song and Ben Sherwood. "Quantile-Optimal Treatment Regimes." Journal of the American Statistical Association 2018; 113(523): 1243–1254.


% Continuous action: Kernel based method (Zhu et al. 2019); deep jump Q-learning (Cai et al. 2022);
% Survival outcome (Jiang et al., 2017)

% High-dimensional covariates: Sparse Q-learning (Zhu et al. 2017); High-dimensional A-learning (Shi et al. 2018).

% Multiple data source and incomplete data structure: Maximin optimal policy when the data resources are heterogeneous (Shi, S., Lu and Fu, 2016)

% Long-term outcome (Cai et al. 2022)

% Subgroup analysis (Fan et al. 2016, Kang et al. 2017)  

%%% Deep jump learning
% [1] Cai, H., Shi, C., Song, R., & Lu, W. (2021). Deep jump learning for off-policy evaluation in continuous treatment settings. Advances in Neural Information Processing Systems, 34.

% [2] Cai, H., Shi, C., Song, R., & Lu, W. (2021). Jump Interval-Learning for Individualized Decision Making. arXiv preprint arXiv:2111.08885.

% [3] Zhu, L., Lu, W., Kosorok, M. R., & Song, R. (2020, August). Kernel assisted learning for personalized dose finding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 56-65).

% [4] Kallus, N., & Zhou, A. (2018, March). Policy evaluation and optimization with continuous treatments. In International conference on artificial intelligence and statistics (pp. 1243-1251). PMLR.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





