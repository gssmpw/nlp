\section{Online \acrlong{CPL}}\label{sec:Online CPL}

This section explores strategies for addressing online decision-making problems with data structures outlined in Paradigms 4-6, where the treatment policy dynamically adjusts in real time based on data continuously collected from interactions with the environment, with the goal typically as to optimize cumulative rewards. 
A critical distinction between offline and online policy learning is the mode of data collection. While the performance of offline \acrshort{CPL} could be constrained by the quality and representativeness of the already collected dataset, online \acrshort{CPL} allows us to constantly acquire new data, facilitating ongoing enhancement of the learned policy alongside swift environmental adaptation. 
Online \acrshort{CPL} problems face challenges due to environmental and learning model uncertainties, particularly unobserved counterfactuals of untaken actions. This leads to the exploration-exploitation dilemma, where the difficulty is in balancing exploring new actions to gather information and exploiting known actions to maximize rewards. 
% In this section, we provide an overview of the problem formulations within each paradigm and the methodologies proposed to navigate the exploration-exploitation balance effectively. 
%, we will delve into the roles of CSL and CEL in enhancing online CPL. %Similar to the preceding section, the focus will primarily be on Paradigm 4, with brief discussions on Paradigms 5 and 6 to elucidate the intrinsic links between them.

%\item whenever uncertainty exists, then counterfactual exists, therefore need the causal favor in online decision making

\subsection{Online Policy Optimization}
We categorize online policy optimization problems into three distinct groups based on their underlying causal structure assumptions. 
The first category encompasses problems that adhere to the data structure outlined in paradigm 4 and are widely studied within the framework of \textit{bandit} problems \citep{slivkins2019introduction,lattimore2020bandit}, characterized by sequentially updated policies and independent state information. 
To address the complexities introduced by potential long-term dependencies between states, the second category considers the data structure of paradigm 5 with a Markovian assumption on the state transition process, which is extensively researched as \textit{online \acrshort{RL}} \citep{sutton2018reinforcement}. 
The final category is a broader classification that captures all remaining online learning problems characterized by non-Markovian system dynamics in Paradigm 6. 
This includes, but is not limited to, \acrshort{POMDP} \citep{meng2021memory, spaan2012partially, zhu2017improving} and \acrshort{DTR} bandits \citep{hu2020dtr}. 
\acrshort{POMDP}s operate under the premise of an underlying \acrshort{MDP} model, albeit with the challenge that the state itself is not directly observable. 
Conversely, \acrshort{DTR} bandits leverage the entirety of historical information to iteratively learn an optimal treatment regime, typically within a short horizon due to computational complexity.

\subsubsection{Bandits (Paradigm 4)} \label{sec:bandit_optimization}
The bandit problems have been widely used in a variety of fields, including recommender systems \citep{zhou2017large}, clinical trials \citep{durand2018contextual}, and business economics scenarios \citep{shen2015portfolio, wei2023zero}. 
In essence, bandit algorithms are designed to select actions $A_t$ from a pre-defined, potentially time-varying action space $\mathcal{A}$ at each round $t$, and thereafter receive a reward $R_t$ from the environment based on the action taken. 
The inherent challenges lie in the unknown distributions of the counterfactual rewards $\{R_t(a)\}_{a\in \mathcal{A}}$, which necessitate decision-makers to interact with the environment in a sequential manner and learn reward distributions from received feedback. 
The objective of bandit learning is typically to maximize the cumulative rewards (i.e., $\sum_{t=0}^{T}R_t(A_t)$) or, equivalently, to minimize the cumulative regret (i.e., $\sum_{t=0}^{T}\max_{a\in \mathcal{A}}R_t(a) - R_t(A_t)$). 
The core of bandit algorithms lies in the process of selecting the arm based on the information collected so far, which involves addressing the exploration-exploitation tradeoff.
The most classical bandit problem is the \acrshort{MAB} \citep{slivkins2019introduction, bouneffouf2019survey}, whose action space is defined by a finite number of $K$ actions, or ``arms" (i.e., $\mathcal{A} = \{1,2,\dots, K\}$). At each round, the bandit agent selects an arm, receives a reward, and updates its knowledge of counterfactual outcomes to improve future selections.


Popular algorithms handling the exploration-exploitation tradeoff for \acrshort{MAB} problems can be grouped into three categories. 
The first category is \textbf{$\epsilon$-greedy} \citep{sutton1999reinforcement}, which takes the greedy action with the highest estimated mean with a probability of $\epsilon$ and selects a random action otherwise. 
A more adaptive variant is $\epsilon_{t}$-greedy, where the probability of taking a random action is defined as a decreasing function of $t$, achieving a lower regret bound than $\epsilon$-greedy \citep{cesa1998finite, auer2002finite}. 
While such an approach is simple, general, and intuitive, it lacks statistical efficiency in exploration because it does not quantify and utilize the uncertainty. 

To design a more efficient exploration process utilizing historical observations, 
the \textbf{\acrfull{UCB}}-type algorithm was proposed from a frequentist perspective, 
embodying the principle of \textit{optimism in the face of uncertainty} \citep{auer2002finite}. 
Essentially, at each round, \acrshort{UCB} estimates the upper confidence bound for $\mathbb{E}(R(a))$ of each arm 
$a$ by summing its estimated mean reward and a confidence radius reflecting uncertainty. The arm with the highest upper bound is then selected to balance choosing arms with higher estimated average rewards (exploitation) and those with greater uncertainty (exploration). While \acrshort{UCB} provides near-optimal theoretical guarantees \citep{slivkins2019introduction} and offers deterministic recommendations that facilitate tracking, it can be computationally inefficient for complex reward models and requires intricate derivations to obtain the confidence bounds. 

Alternatively, \textbf{\acrfull{TS}} takes a Bayesian approach to the exploration-exploitation trade-off, achieving theoretical performance comparable to \acrshort{UCB}-type algorithms \citep{russo2018tutorial,agrawal2013further}. \acrshort{TS} begins by assuming a prior distribution over each arm's mean reward. At each round, it updates its belief about the reward distribution by computing the posterior distribution of each armâ€™s mean reward based on observations collected so far. \acrshort{TS} then samples a mean reward from each posterior and selects the arm with the highest sampled reward. This sampling approach naturally balances exploration and exploitation, as arms with greater uncertainty would have wider posterior distributions, increasing their likelihood of selection. %and ensuring that less-explored arms remain viable options when uncertainty is high.
While the regret bound of \acrshort{TS} is similar to that of the \acrshort{UCB}, it is worth noting that \acrshort{TS} has been empirically observed to usually have a lower regret than \acrshort{UCB} in the long run \citep{chapelle2011empirical}. 
Furthermore, \acrshort{TS} is noted for its computational efficiency (especially with conjugate priors)  and its adaptability to complex models through bootstrapping techniques for approximating posteriors \citep{geman1984stochastic,chen2009bayesian,wan2023multiplier}. 
However, \acrshort{TS}'s efficacy heavily depends on the accuracy of the prior distribution \citep{lattimore2020bandit}. 
An inaccurate prior can lead to either excessive or insufficient exploration, and hence suboptimal regret. To further tackle this challenge, meta \acrshort{TS} \citep{kveton2021meta, wan2021metadata, wan2023towards} has recently been introduced, achieving better performance that is robust to the prior specifications. 

Recent research has examined numerous extensions of the \acrshort{MAB} framework to address complexities in a variety of real-world scenarios, 
such as personalization, large action spaces, and managing multiple decision-making tasks. 
Contextual Bandits \citep{chu2011contextual,agrawal2013thompson,kveton2020randomized,li2010contextual} use environmental contextual information, such as a user's gender, occupation, season, and so on, to tailor recommendations; 
structured bandits \citep{agrawal2017thompson,kveton2015cascading,chen2013combinatorial,wan2023towards} use diverse application-specific structures to specify the reward function and utilize the rich outcome information; 
and multi-task bandits \citep{kveton2021meta, wan2021metadata,hong2022hierarchical,basu2021no} share insights across similar tasks to optimize learning for newly introduced tasks. 
%It is worth noting that algorithms for these advanced settings typically follow either the UCB or TS frameworks, with the only differences being the processes of upper bound estimation and posterior distribution estimation, respectively. (shall we mention the adverserial bandits?)

\textbf{Policy-based approaches. }
All of the algorithms previously discussed can be classified as value-based, i.e., focusing on estimating the reward function and then determining the optimal action based on the estimated rewards. 
In contrast, there are also policy-based approaches that directly learn preferences over actions. 
For instance, gradient bandit algorithms \citep{sutton2018reinforcement,mei2023stochastic} for \acrshort{MAB} problems maintain a numerical preference, $H_t(a)$, for each action $a$. 
Here, the probability of selecting action $a$ at time $t$ is determined by a softmax function: $\frac{exp(H_t(a))}{\sum_{b\in\mathcal{A}}exp(H_{t}(b))}$. Shifting the focus to contextual bandits, classification oracle-based algorithms \citep{slivkins2019introduction, langford2007epoch, dudik2011efficient, agarwal2014taming, krishnamurthy2016contextual} consider policies that map contexts to actions and determine the optimal policy over a pre-specified policy class. They propose to approach policy class optimization as a cost-sensitive multi-class classification problem, utilizing historical collections of $(\boldsymbol{s}_t, a_t, r_t)$ triplets. This formulation enables the use of a variety of cutting-edge machine learning methods, significantly improving computational efficiency and reducing running time \citep{slivkins2019introduction}.

%\textbf{Bandits with \acrshort{IPW}/\acrshort{DR} value estimators. }
\textbf{Bandits with causality to enhance learning efficiency. }
Recent advances in \acrshort{CSL} and \acrshort{CEL} offer a promised opportunity to increase the learning efficiency of Bandits. For example, \acrshort{CEL} methods like \acrshort{IPW} and \acrshort{DR} estimation have been adapted for value estimation to mitigate potential biases in bandit learning, resulting from reward model misspecification or covariate imbalancesâ€”particularly when training data are sparse or unrepresentative in specific areas of the context space. Two primary approaches have emerged for integrating \acrshort{IPW}/\acrshort{DR} into bandit algorithms: (i) generating unbiased pseudo-rewards from observed rewards and propensity scores, adhering to the principles of \acrshort{IPW}/\acrshort{DR} estimators in \acrshort{CEL}, which are subsequently used in the bandit update process in place of the observed rewards \citep{bietti2021contextual, kim2019doubly, kim2021doubly, kim2023double}; and (ii) employing importance-weighted regression, wherein each observation is weighted by the inverse of its propensity score \citep{dimakopoulou2019balanced, bietti2021contextual}. While these methods remain focused on maximizing rewards, recent research has also explored integrating classical bandit frameworks with meta-learners to optimize the incremental benefits of an action (e.g., $\tau_{a} = R(A = a) - R(A \neq a)$), aiming to enhance the return on investment \citep{sawant2018contextual, kanase2022application, zhao2022mitigating} or potentially simplify the model estimation \citep{carranza2023flexible}.

Another line of recent research leverages causal techniques to transfer knowledge from logged data to ``warm up" bandit agents. This approach initiates agents with an informative estimate of the environment, thereby reducing the number of rounds required for online exploration. \citet{li2021unifying} propose creating a pseudo-environment using logged data to synthesize action outcomes via matching and weighting and introduce a two-stage learning process under the \acrshort{UCB} framework. Specifically, in the first stage, the pseudo-environment would be used for the simulation of interactions with the bandit agent in order to prepare the agent for real-world engagement. In the second stage, the bandit agent uses the knowledge gained in the first stage to interact with the real world, significantly reducing the possibility of unnecessary exploration. Similarly, \citet{xu2023thompson} provide a complementary view with a \acrshort{TS}-inspired variant. Distinct from creating a pseudo-environment, \citet{zhang2017transfer} employ \acrshort{SCM}s to derive causal bounds for potential outcomes, facilitating the transfer of learnings between bandit agents. By leveraging the causal structure of the environment and the observed trajectories from completed bandit agents, they proposed to derive the 2-sided bounds of the potential rewards over the action space for the target bandit agent. These bounds are subsequently utilized to eliminate less effective options during the initialization phase and to refine the \acrshort{UCB} estimates throughout the learning process.

Furthermore, side causal information is particularly effective in improving learning efficiency in scenarios with multiple intervention variables. For example, \citet{lattimore2016causal} is the first to introduce such a class of causal bandits problems. Given a causal graph among variables that include either interventional/uninterventional variables and reward, agents are able to select more than one variable in the causal graph to intervene at each round. Utilizing the causal graph to transfer information among interventional variables and hence reduce the number of explorations needed, \citet{lattimore2016causal} and \citet{sen2017identifying} focus on the best arm identification problem, while \citet{lu2020regret} and \citet{nair2021budgeted} propose algorithms to minimize the cumulated regret. However, when considering a large number of interventional variables, \citet{lee2018structural} empirically showed that a brute-force way to apply standard bandit algorithms on all interventions can suffer huge regret. To further enhance the sampling efficiency, \citet{lee2018structural, lee2019structural} proposed narrowing the action space by determining the possibly-optimal minimal intervention set before applying standard bandit learning algorithms, while \citet{subramanian2021causal} suggested performing target interventions to allocate more samples to targeted subpopulations that are more informative about the most valuable interventions.

In addition to improving learning efficiency, causality also addresses broader challenges in bandit learning, such as assumption violations, discussions of which can be found in Chapter \ref{sec:assump_violated}.


%Another category focuses on maximizing treatment effects using CEL techniques rather than directly optimizing the expected reward at each decision point. This shift is being driven by two primary motivations. First, there is a practical imperative in applications such as advertising to maximize return on investment efficiently. This necessitates a methodological shift toward optimizing actions that provide the greatest incremental benefits, ensuring resource efficiency. For instance, \citep{sawant2018contextual,kanase2022application,zhao2022mitigating} focuses on estimating and optimizing CATE (i.e., $\tau_{a} = Y(A = a) - Y(A \neq a)$), which quantifies the incrementality of each action option, to avoid redundant targeting, such as delivering advertisements to individuals who are already likely to make a purchase. Second, empirical findings indicate that rewards are likely to be confounded by action-independent factors, which have no influence on decision-making but can obscure the true impact of interventions. \citet{carranza2023flexible} proposes a method that combines the R-learner with contextual bandits, assuming $Y(a) = g(s,a) + h(s)$, where $h$ is a confounder model and $g$ is a treatment effect model. Under such model assumption, motivated by the fact that using the same model class, the bias in estimating treatment effects is no greater than the bias in reward estimation, they proposed to solely use the R-learner to estimate $g$ and then prioritizing actions based on their potential to deliver higher treatment effects, $g(x,a)$.



\subsubsection{Online \acrlong{RL} (Paradigm 5 \& 6)} \label{sec:RL_optimization}
Unlike bandits, which assume actions have only immediate effects, \textbf{online \acrlong{RL}} \citep{sutton2018reinforcement} considers the influence of actions on future outcomes through transitions of the environment's state. This consideration is crucial in real-world applications such as autonomous vehicles \citep{kiran2021deep} and robotics \citep{singh2022reinforcement}, where the long-term effects of actions must be factored into decision-making. To account for these complex state transitions, \acrshort{MDP}s are typically employed as the standard framework for online \acrshort{RL}. Similar to offline \acrshort{RL} (Paradigm 2), online \acrshort{RL} algorithms are generally classified into value-based and policy-based approaches, depending on whether they estimate $V^{\pi}(\boldsymbol{s})/Q^{\pi}(a,\boldsymbol{s})$ to assist with policy optimization.

\textbf{Value-based approaches. }
Among value-based \acrshort{RL} algorithms, Monte Carlo sampling  \citep{singh1996reinforcement} is the most straightforward approach. It samples complete trajectories and then uses the average cumulative reward from relevant sub-trajectories as an estimator for the $Q$ and $V$ functions, which are then used directly for policy optimization. This method can be adapted to non-Markovian environments (Paradigm 6), offering flexibility. However, it requires waiting until the end of a trajectory to collect data, making it less suitable for online \acrshort{RL} problems with an infinite horizon. In contrast, \acrfull{TD}\citep{sutton1988learning} learning iteratively improves the estimate of $Q^{\pi}$ by leveraging the recursive form of the Bellman equation (\ref{eqn:bellman_Q}). For example, SARSA \citep{rummery1994line}, a type of \acrshort{TD} learning, updates the Q-function as
\begin{equation*}
    \hat{Q}^\pi(a, \boldsymbol{s}) \leftarrow \hat{Q}^\pi(a, \boldsymbol{s}) + \alpha \Big[r + \gamma \hat{Q}^\pi(\pi(\boldsymbol{s}'), \boldsymbol{s}')  - \hat{Q}^\pi(a, \boldsymbol{s}) \Big], 
\end{equation*} 
where $(\boldsymbol{s},a,r,\boldsymbol{s}')$ is a newly collected transition tuple, $\alpha$ is the learning rate, and $[r + \gamma \hat{Q}^\pi(\pi(\boldsymbol{s}'), \boldsymbol{s}')  - \hat{Q}^\pi(a, \boldsymbol{s})]$ is the temporal difference. Similarly, Q-learning  \citep{watkins1992q} employs a \acrshort{TD} approach but defines the \acrshort{TD} as $[r + \gamma max_a\hat{Q}^\pi(a, \boldsymbol{s}')  - \hat{Q}^\pi(a, \boldsymbol{s})]$. Another major value-based \acrshort{RL} method is A-learning \citep{baird1993advantage,gu2016continuous,schulman2015high,li2019hierarchical}, which, like its offline counterpart, focuses on learning the relative value of action policies (i.e., $A^{\pi}(a,\boldsymbol{s}) = Q^{\pi}(a,\boldsymbol{s}) - V^{\pi}(\boldsymbol{s})$) instead of the absolute value functions. 

\textbf{Policy-based approaches. }
Policy-based approaches, such as REINFORCE \citep{williams1992simple}, PPO  \citep{schulman2015trust}, and TRPO  \citep{schulman2017proximal}, directly optimize the policy $\pi^*$ by maximizing the objective function $J(\theta) = \mathbb{E}_{\tau\sim p(\cdot;\theta)}r(\tau)$, where $\tau$ denotes a complete trajectory of states and actions, $r(\tau) = \sum_{t} R_t(\boldsymbol{S}_t, A_t)$ is the cumulative reward along the trajectory, $\theta$ parameterizes the policy, and $p(\cdot;\theta)$ is the probability distribution over trajectories induced by policy $\pi_{\theta}$. Policy-based methods can be further categorized into gradient-based  \citep[e.g., ][]{williams1992simple,schulman2015trust,schulman2017proximal,levine2013guided,peters2008reinforcement,baxter2001infinite} and gradient-free  \citep[e.g.,][]{salimans2017evolution,koutnik2013evolving} approaches. Gradient-based approaches rely on the policy gradient theorem  \citep{williams1992simple}, such that
\begin{align}
    \nabla_{\theta}J_{\theta} &= \mathbb{E}_{\tau\sim p(\cdot;\theta)}r(\tau)\nabla_{\theta}log p(\tau;\theta),\nonumber\\
    &=\mathbb{E}_{\tau\sim p(\cdot;\theta)}\left[\sum_{t} R_t(\boldsymbol{S}_t, A_t)\right]\left[\sum_{t}\nabla_{\theta}log\pi_{\theta}(A_t|\boldsymbol{S}_t)\right],\label{policy-gradient theom}
\end{align}
to update $\theta$ via gradient ascent. 
With (\ref{policy-gradient theom}), $\nabla_{\theta}J_{\theta}$ is typically estimated using Monte Carlo rollouts  \citep{williams1992simple} or importance sampling with previously collected trajectories  \citep{levine2013guided}. Notably, as the derivation of $\nabla_{\theta}J(\theta)$ does not depend on the Markov property, most gradient-based approaches can be applied to \acrshort{POMDP}s (paradigm 6) without modification. On the other hand, gradient-free approaches focus on searching for the optimal policy within a predefined policy class. 

One limitation of the policy-based approaches is their sample inefficiency, as they require frequent generation of new trajectories from scratch to evaluate the current policy, which can lead to high variance. To mitigate this, the \textit{Actor-Critic} method combines elements of both value-based and policy-based approaches to improve efficiency \citep{sutton1999policy,mnih2016asynchronous,schulman2015high,gu2016q}. It maintains a policy function estimator (the actor) to select actions and a value function estimator (the critic) to evaluate and guide policy updates through gradient descent.

\textbf{Exploration. }
All of the aforementioned algorithms share strong connections with the offline \acrshort{RL} algorithms discussed earlier. The key distinction lies in the addition of an exploration component, which addresses the issue of distributional shift in offline \acrshort{RL} by strategically collecting new interaction data to continuously improve the policy. Similar to bandits, various exploration strategies are studied to enhance exploration efficiency in online \acrshort{RL}. For example, DQN \citep{mnih2015human} employs $\epsilon$-greedy for exploration. Inspired by \acrshort{TS}, bootstrapped DQN \citep{osband2016deep} maintains multiple Q-value estimators derived from bootstrapped sample datasets to approximate a distribution over Q-functions, allowing for exploration by randomly selecting a policy according to the distribution. \citet{bellemare2016unifying} employs the \acrshort{UCB} framework, adding a count-based exploration bonus to incentivize the agent to explore new or less frequently visited areas of the state space. Additionally, noise-based exploration, which involves adding noise to observation or parameter spaces, is widely used in deep \acrshort{RL}. For a comprehensive review of exploration strategies in deep \acrshort{RL}, see \citet{ladosz2022exploration}.


More recently, causal graph structures have been utilized to enable more efficient exploration in online \acrshort{RL}. 
For instance, \citet{seitzer2021causal} introduces a framework that employs situation-dependent causal influence, measured via conditional mutual information, to identify states where an agent can effectively influence its environment. Integrating this measure into \acrshort{RL} algorithms enhances exploration and off-policy learning, significantly improving data efficiency in robotic manipulation tasks. \citet{hu2022causality} proposes the Causality-Driven Hierarchical \acrshort{RL} framework, which leverages causal relationships among environment variables to discover and construct high-quality hierarchical structures for exploration, thereby avoiding inefficient randomness-driven exploration. More discussions on how causality facilitates online \acrshort{RL} from other perspectives can be found in Chapter \ref{sec:assump_violated}.


%\textbf{Causal RL}

%Sampling efficiency has long been an important problem in online \acrshort{RL}. Unlike offline policy learning, which relies on pre-collected datasets, online policy learning necessitates the active collection of data through continuous interactions with the environment. In order to achieve the highest overall return, it is crucial for the effectiveness of online learning algorithms to learn the optimal strategy with the fewest possible interactions. Recent advances in \acrshort{CSL} and \acrshort{CEL} offer a promised opportunity to increase the sampling efficiency in online \acrshort{RL}. 
%The first direction is to reduce dimensions and noises via a deeper understanding of the causal relationship between variables. For example, the research on exogenous \acrshort{MDP} \citep{trimponias2023reinforcement, sinclair2023hindsight, chitnis2020learning} focuses on which state variables have exogenous processes and then can utilize such a structure to simplify the learnings; and the literature on factored \acrshort{MDP}s \citep{guestrin2003efficient, osband2014near} aim to identify the factored structure in the causal graph to decompose the learning problem; and the representation learning methods \citep{lee2021causal, huang2022action, wang2022causal} focus on identififying the low-dimensional state representaion to reduce noises.

% \citet{d2019exploiting} integrated bootstrapped DQN and TS; 


%policy optimization approaches can be divided into policy-based approaches, which learn a policy function directly by applying gradient descent to optimize its value, and valued-based approaches, which learn value functions with exploration. A third type of approach combines the benefits of both policy-based and value-based approaches to improve learning efficiency by using a policy function estimator and a value function estimator at the same time. More detailed reviews can be found on our online tutorial websites. 

%policy-based: We donâ€™t need to implement an exploration/exploitation trade-off by hand. Since we output a probability distribution over actions, the agent explores the state space without always taking the same trajectory.

%Value-based: epsilon greedy is commonly used, typically with a decreasing epsilon

%UCB: value based RL algo, ie.\citet{bellemare2016unifying},

%Online learning: A comprehensive survey; Realizing self-adaptive systems via online reinforcement learning and feature-model-guided exploration ($\epsilon-greedy$ RL); Exploration strategies in deep RL (https://lilianweng.github.io/posts/2020-06-07-exploration-drl/); Improving Offline-to-Online Reinforcement Learning with Q-Ensembles; Sergey Levine lec13-14

%Exploration in deep reinforcement learning: A survey

%Reinforcement learning algorithms: A brief survey

%POMDP: Balancing exploration and exploitation in episodic reinforcement learning


\subsection{Online Policy Evaluation (Paradigm 4 \& 5)} \label{sec:bandit_evaluation}

The evaluation of the performance of policies plays a vital role in many areas, including medicine and economics \citep[see e.g., ][]{chakraborty2013statistical,athey201921}. By evaluation, we aim to unbiasedly estimate the value of the optimal policy that the bandit policy is approaching and infer the corresponding estimate. Although there is an increasing trend in policy evaluation \citep[see e.g., ][]{li2011unbiased,dudik2011doubly,jiang2016doubly,swaminathan2017off,wang2017optimal,kallus2018policy,su2019doubly}, we note that all of these works focus on learning the value of a target policy offline using historical log data. Instead of a post-experiment investigation,  it has attracted more attention recently to evaluate the ongoing policy in real-time.
%In precision medicine, the physician aims to make the best treatment decision for each patient sequentially according to their baseline covariates. Estimating the mean outcome of the current treatment decision rule is crucial to answering several fundamental questions in health care, such as whether the current strategy significantly optimizes patient outcomes over some baseline strategies.  When the value under the ongoing rule is much lower than the desired average curative effect, the online trial must be terminated until more effective treatment options are available for the next round. Thus, policy evaluation in online learning is a new idea to provide the early stop of the online experiment and timely feedback from the environment,  
%as demonstrated in the right panel of Figure \ref{fig_illu}.



Despite the importance of policy evaluation in online learning, the current bandit literature suffers from three main challenges. 
First, the data, such as the actions and rewards sequentially collected from the online environment, are not independent and identically distributed (i.i.d.) since they depend on the previous history and the running policy. In contrast, the existing methods for the offline policy evaluation \citep[see e.g., ][]{li2011unbiased,dudik2011doubly} primarily assumed that the data are generated by the same behavior policy and i.i.d. across different individuals and time points. 
% Such assumptions allow them to evaluate a new policy using offline data by modeling the behavior policy or the conditional mean outcome.   In addition, we note that the target policy to be evaluated in offline policy evaluation is fixed and generally known, whereas for policy evaluation in online learning, {the optimal policy of interest needs to be estimated and updated in real time}. 
The second challenge lies in {estimating the mean outcome under the optimal policy online}. Although numerous methods have recently been proposed to evaluate the online sample mean for a fixed action \citep[see e.g., ][]{nie2018adaptively,neel2018mitigating,deshpande2018accurate,shin2019sample,shin2019bias,waisman2019online,hadad2019confidence,zhang2020inference}, none of these methods is directly applicable to online policy evaluation, as the sample mean only provides the impact of one particular arm, not the value of the optimal policy in bandits that considers the dynamics of the online environment. 
% For instance, in the contextual bandits, we aim to select an action for each subject based on its context/feature to optimize the overall outcome of interest. However, there may not exist a unified best action for all subjects due to heterogeneity, and thus evaluating the value of one single optimal action cannot fully address the policy evaluation in such a setting. However, although commonly used in the regret analysis, the average of collected outcomes is not a good estimator of the value under the optimal policy in the sense that it does not possess statistical efficiency (see details in Section \ref{sec:DREAMinf}).   
Third, given data generated by an online algorithm that maintains the exploration-and-exploitation trade-off sequentially, inferring the value of a policy online should consider such a trade-off and quantify the probability of exploration and exploitation. 
%The probability of exploring non-optimal actions is essential  in two ways. First, it determines the convergence rate of the online conditional mean estimator under each action. Second, it indicates the data points used to match the value under the optimal policy. To our knowledge, the regret analysis in the current bandit literature is based on the binding of variance information \citep[see e.g., ][]{auer2002using,srinivas2009gaussian,chu2011contextual,abbasi2011improved,bubeck2012regret,zhou2015survey}, y
% Yet, little effort has been made in formally quantifying the probability of exploration over time.  


There are very few studies directly related to the topic of online policy evaluation. \cite{chambaz2017targeted} established the asymptotic normality for the conditional mean outcome under an optimal policy for sequential decision making.  Later, \cite{chen2020statistical} proposed an inverse probability weighted value estimator to infer the value of optimal policy using the $\epsilon$-Greedy method. %These two works did not discuss how to account for the exploration-and-exploitation trade-off under commonly used bandit algorithms, such as Upper Confidence Bound (UCB) and Thompson Sampling (TS). %, as considered in this paper.  
Recently, to evaluate the value of a known policy based on the adaptive data, % collected from contextual bandits, 
\citet{bibaut2021post} and \citet{zhan2021off} proposed to utilize the stabilized doubly robust estimator and the adaptive weighting doubly robust estimator, respectively. Both methods focused on obtaining a valid inference of the value estimator under a fixed policy by conveniently assuming a desired exploration rate to ensure sufficient sampling of different arms. %Such an assumption can be violated in many commonly used bandits (see details shown in Theorem \ref{BoundPE}). 
% Although t
Also see other recent advances that focus on statistical inference for adaptively collected data \citep{dimakopoulou2021online,zhang2021statistical,khamaru2021near,ramprasad2022online} in bandit or \acrshort{RL} setting. 
To infer the value of the optimal policy by investigating the exploration rate in online learning, \citet{ye2023doubly} explicitly characterized the trade-off between exploration and exploitation in online policy optimization by deriving the probability of exploration in bandit algorithms. Their work 
  % Such a probability is new to the literature by quantifying the chance of taking the nongreedy policy (i.e., a nonoptimal action) given the current information over time, in contrast to the probability of exploitation for taking greedy actions. Specifically, we consider three commonly used bandit algorithms for exposition, including the UCB, TS, and EG methods. We note that the probability of exploration is prespecified by users in EG while remaining implicit in UCB and TS.  We use this probability to conduct valid inferences on the online conditional mean estimator under each action. 
% \textbf{The second contribution of this work is to 
proposed the doubly robust interval estimation (DREAM) method to infer the mean outcome of the optimal online policy with double protection.

% on the consistency of the proposed value estimator to the true value, given the product of the nuisance error rates of the probability of exploitation and the conditional mean outcome as $o_p(T^{-1/ 2})$ for $T$ as the termination time. Under standard assumptions for inferring the online sample mean, we show that the value estimator under DREAM is asymptotically normal with a Wald-type confidence interval provided.  To the best of our knowledge, this is the first work to establish the inference for the value under the optimal policy by taking the exploration-and-exploitation trade-off into thorough account and thus fills a crucial gap in the policy evaluation of online learning. 

%\smallskip

%\textbf{Bandits}: DREAM, direct estimator, \acrshort{IPW}, DR (connection with the CEL)
%\textbf{RL}: SAVE