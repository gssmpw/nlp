
%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% \usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{minted}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\definecolor{cornflowerblue}{rgb}{0.39, 0.58, 0.93}
\hypersetup{
    colorlinks=true,
    linkcolor=cornflowerblue,
    filecolor=magenta,      
    urlcolor=teal,
    citecolor=cornflowerblue,
    pdftitle={Laws of Scaling},
    pdfpagemode=FullScreen,
    }

\newcommand{\tom}[1]{{\color{red}[Tom: #1]}}
\newcommand{\ap}[1]{{\color{purple}[Ashwinee: #1]}}
\newcommand{\david}[1]{{\color{blue}[David: #1]}}
\newcommand{\sean}[1]{{\textcolor[rgb]{0.505, 0, 0.921}{Sean: \footnotesize\sf[#1]}}}
\newcommand{\jwk}[1]{{\textcolor{ForestGreen}{John: \footnotesize\sf[#1]}}}
\newcommand{\mg}[1]{{\textcolor{teal}{Micah: \footnotesize\sf[#1]}}}
\newcommand{\todo}[1]{{\color{orange}[TODO: #1]}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Are All Parameters Equal?}

\begin{document}

\twocolumn[
\icmltitle{Are All Parameters Equal?\\An Analysis of the Pitfalls and Potential of Scaling Laws}
% \icmltitle{Oops, All Researcher Degrees of Freedom!}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


% Classical scaling laws focus on computing the optimal total number of parameters for a given compute budget.
% However, these laws do not take into consideration the numerous possible ways that the total number of parameters can be used in the transformer architecture.
% We propose new scaling laws that are parameterized by the hidden dimension and number of layers in the decoder architecture.
% We train an extensive suite of models with varying ratios of hidden dimension to number of layers, fitting scaling laws which can prescribe the optimal ratio to achieve minimal loss.
% We compare and contrast with data collected from prior work on scaling laws and with current popular architectures.

\begin{figure}[h!]
    \centering
    % \includegraphics[width=\linewidth]{figs/slope_progression_combined.pdf}
    \includegraphics[width=0.5\textwidth, height=4cm]{example-image-a}
    \caption{\sean{TBC}}
    % \caption{We find that training on more than 100B tokens is critical to ensure a stable scaling law fit. \sean{Are we allowed figures above the abstract? this was a desk reject at ICLR} \\\sean{Add horizontal line for Kaplan and Chinchilla}\\\sean{This plot it currently not reproducible}}
    \label{fig:not-enough-tokens}
\end{figure}

% Lots of models
% Width-Depth scaling attempt
% Rainbow scaling plot
\begin{abstract}
Prior work on scaling laws for large language models (LLMs) typically trains models on fewer than 100 billion tokens and freezes consequential design choices including model shape and learning rate schedule. 
In this work, we demonstrate that these common practices lead to inaccurate scaling laws, and we accordingly compute new and improved laws.
To that end, we generate and open-source the most comprehensive scaling law dataset to date, with over 4000 model checkpoints corresponding to transformers with up to 2 billion parameters and spanning a wide range of learning rates, cooldown schedules, and architectural shapes.
Our findings are summarized as follows: (1) training on more than \(100\) billion tokens is essential for reliable scaling laws, as the slope of the scaling law changes significantly with the minimum number of tokens trained on (see \Cref{fig:not-enough-tokens}); (2) 
wider models often outperform deeper ones when considering training time efficiency, challenging the prevailing focus on FLOPs-optimal designs; (3) we highlight the negative impact of freezing design choices on scaling laws by showing that under identical training and fitting conditions, minor tweaks to the sampling of fitting points and learning rate schedule can significantly change the scaling law (see \Cref{fig:rainbow-plot}).


\iffalse Scaling laws for large language models are a keystone of model development, but existing best practices often overlook critical nuances in training dynamics and architecture design. 
Prior work typically trains models on fewer than 100 billion tokens, provides prescriptions that are optimal with respect to FLOPs, and freezes a host of critical design choices from model shape to learning rate schedule. 
In this work, we demonstrate that these practices lead to inaccurate scaling laws and introduce remediations for these overlooked pitfalls.
We release the most comprehensive scaling law dataset to date, consisting of over 4000 model checkpoints spanning a wide range of learning rates, cooldown schedules, and architectural shapes. 
Through extensive experimentation, we train and open-source a suite of 22 text transformer models ranging from 50 million to 2 billion parameters, varying the ratios of hidden dimension to the number of layers, and training each on 350 billion tokens. 
Our findings reveal that training on \(>100\) billion tokens is essential to reach the stable regime necessary for reliable scaling laws, as the slope of the scaling law changes significantly with the minimum number of tokens trained on (see \Cref{fig:not-enough-tokens}). 
We show that wider models often outperform deeper ones when considering training time efficiency, challenging the prevailing focus on FLOPs-optimal designs. 
We highlight the negative impact of freezing design choices on scaling laws by showing that under identical training and fitting conditions, minor tweaks to the sampling of fitting points and learning rate schedule can significantly change the scaling law (see \Cref{fig:rainbow-plot}).
\fi
% By fitting scaling laws to this diverse suite of models, we prescribe optimal width-to-depth ratios for minimizing loss and compare our findings against current popular architectures. Our results uncover previously overlooked pitfalls in common methods for fitting scaling laws, offering practical insights for future model development.
% text transformer architecture = as has been looked at for vit

% - We propose remediations for pitfalls overlooked by prior work
% - We release the most comprehensive scaling law dataset to date, consisting of over \(4000\) model checkpoints spanning {} (mention LR, cooldown, shape)
% - 

% Classical scaling laws do not take into consideration the numerous possible ways that the total number of parameters can be used in the text transformer architecture.
% We train and open source an extensive suite of \(22\) models from \(50\) million to \(2\) billion parameters with varying ratios of hidden dimension to number of layers for \(350\) billion tokens.
% Fitting scaling laws to this suite of models, we can prescribe the optimal ratio of width to depth to achieve minimal loss.
% We compare and contrast with current popular architectures, discovering overlooked pitfalls in common methods for fitting scaling laws.\\
% \textbf{Code: } \\
% \textbf{Hugging Face Collection: } \\
% \textbf{W\&B Report: } \\
\end{abstract}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/rainbow.pdf}
    \caption{
    We demonstrate the variability in fitting scaling laws by resampling our data many different ways.
    We label prescriptions found using approach one with ``Approach 1'' in the legend, otherwise approach three is used.
    All tokens counts available are used to fit the laws unless stated otherwise in the legend, for example \(\le100b\) means that only token counts less than or equal to \(100b\) are used in fitting.\\
    \textit{No Embeds}: Embedding parameters are not counted when fitting these laws.\\
    \textit{Cooldown}: Only data from the cooldown ablation is used to fit this law.\\
    \textit{LR Ablation}: Only data from the learning rate ablation training runs where the learning rate is halved is used to fit these laws.\\
    \textit{width=\(512\) Only}: Only models with width \(512\) are used to fit these laws.\\
    \textit{Chinchilla Reduced Sampling}: We subsample our data to be as close as possible to the token counts and model sizes that \citet{hoffmann2022empirical} use to fit their scaling laws and also fit new scaling laws on this subset of \citet{hoffmann2022empirical} data.
    }
    \label{fig:rainbow-plot}
\end{figure*}

\section{Introduction}~\label{sec:intro}
\begin{figure}[b!]
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/scaling laws cover fig.001.png}
    \caption{A visualization of a transformer architecture highlighting the quantities width and depth of a transformer.
    We use ``width'' to describe the size of the hidden dimension and ``depth'' to describe the number of transformer blocks in the model.}\label{fig:summary}
\end{figure}


\mg{add references to cover figures, add a sprinkling of citations}


\tom{A simple narrative for the intro is:  The results predicted by scaling lows depends on both the design of the architecture and design of the scaling law.  This paper will look at the impact of both.}

The architecture of choice for many AI applications has converged on the Transformer because its performance scales well with parameters and data \citep{hoffmann2022empirical}, but also because of its ability to handle long sequences and virtually any data modality. 
Hugging Face \citep{wolf2020huggingfaces} hosts more than a million models available for download, and most of these models are Transformers.
While on the surface it may appear that all practitioners are training the same models but on different data, a closer look reveals that even Transformers of similar size differ greatly in their architectural specifications, and large language models (LLMs) vary greatly in their training protocols including duration and hyperparameters.

Several influential papers \citep{kaplan2020scaling, hoffmann2022empirical} fit scaling laws that predict the final loss of a model as a function of its parameter count and the number of training tokens. 
These scaling laws are increasingly a keystone of model development since they allow us to make predictions about large models from smaller scale experiments.
\sean{Could cite chinchilla for single width depth but need to relax the language as there is some variance in it}
Existing works on scaling laws often restrict Transformer architectures to a single width--depth ratio, train on a small number of tokens, and fix training hyperparameters such as cooldown schedule across all training runs \citep{hoffmann2022empirical}.
These design choices, in turn, influence the resulting scaling laws.

If scaling laws are sensitive to such design choices, then they may only be useful for practitioners implementing similar setups to the ones employed for training runs that went into the scaling laws.
In practice, practitioners often take guidance from scaling laws that assume completely different design choices than their own implementation, casting doubt on the broad value of existing scaling laws.
In this work, we hypothesize that optimal design choices vary across parameter counts and dataset sizes and that design choices strongly influence the prescriptions of compute-optimal scaling laws.

We test these hypotheses by generating a vast number of LLM checkpoints at a variety of width--depth ratios, trained for a wide range of token counts, and with an array of learning rates and cooldown schedules.
Fitting scaling laws to these checkpoints, we confirm that scaling law parameters and interpretations indeed depend strongly on the precise choice of checkpoints used to fit them.
Our contributions are summarized as follows:
\sean{These contributions are different to the abstract}
\begin{itemize}
\item \textbf{We open-source more than 4000 checkpoints cumulatively trained on over 10 trillion tokens.}  The models we provide are diverse across architectural and training hyperparameter axes, enabling more granular studies than previous work.
\item \textbf{We highlight common pitfalls of prior scaling laws.}  Concretely, there are many possible decisions when choosing points to fit scaling laws that significantly change the slope of the law. For example, we find that training on more than 100 billion tokens is essential for reaching a stable regime where scaling laws are reliable, as the slope of the scaling law is sensitive to the minimum number of tokens on which the models are trained (see \Cref{fig:not-enough-tokens}).
\item \sean{I think this claim is a little too strong} \textbf{We compute optimal widths and depths at each compute budget and find that most industrial models are far skinnier and deeper than is optimal.}  To that end, we fit a parametric power law of a similar form as in \citet{hoffmann2022empirical} but which predicts loss as a function of depth, width, parameters, and number of training tokens.  This law enables us to estimate the compute-optimal settings of its arguments for any given training budget.
\end{itemize}


\iffalse




The architecture of choice for many AI applications has converged on the Transformer because its performance scales well with parameters and data \citep{hoffmann2022empirical}, but also because of its ability to handle long sequences and virtually any data modality. 
Hugging Face \citep{wolf2020huggingfaces} hosts more than a million models available for download, and most of these models are Transformers.
While on the surface it may appear that all practitioners are training the same models, a closer look at the model configurations reveals that even Transformers of similar size differ greatly in their specifications.
One particularly fundamental dimension across which the models vary is their depth-width ratio.
Even within the same organization, and the same model family, different models may have different depth-width ratios. 

A number of influential papers fit scaling laws that predict a model’s final loss as a function of its parameter count and the number of training tokens, but these papers limit the variety in depth to width ratio the depth-width ratio (see \Cref{fig:model_search_space}).
\sean{Need a new hypothesis to state here}
In this work, we hypothesize that the optimal depth-width ratio varies across parameter counts and dataset sizes.
If this hypothesis is true, then these existing scaling laws may actually mislead users to train the wrong sized model on the wrong number of tokens when the user optimizes the loss predicted by the scaling law as a function of compute budget, especially since users often employ a different depth-width ratio than was used when fitting the scaling law \mg{MIGHT WANT TO GIVE AN EXAMPLE OF SOMEONE DOING THAT}.

Adding to the confusion, many papers propose theories to explain the role of width and depth and arrive at contradictory conclusions \citep[e.g.][]{petty2024impact, scaleEfficiently}.  
However, such papers often conduct experiments at different model and data scales.
If the optimal depth-width ratio depends on these scales, then the studies’ differing experimental scales may explain why they arrive at superficially conflicting conclusions.

We test this hypothesis by training many Transformer language models of various widths and depths, and we fit a parametric power law of a similar form as found by \citet{hoffmann2022empirical} but which predicts loss as a function of depth, width, parameters and number of training tokens.

We summarize our contributions as:

% Lots of models
% Width-Depth scaling attempt
% Rainbow scaling plot
\textbf{We open source over 4000 checkpoints with over 10 trillion tokens trained on cumulatively.} We provide a comprehensive suite of models with a wide range of width and depths, providing ablations over cooldown and learning rate.

\textbf{We highlight common pitfalls many prior works fall into.} We find that training on \(>100\) billion tokens is essential to reach the stable regime necessary for reliable scaling laws, as the slope of the scaling law changes significantly with the minimum number of tokens trained on. 
We also find that there are many possible decisions when sampling points to fit scaling laws to that change the slope of the law also.

\textbf{We analyze the tradeoffs between width and depth.} Prior work performs preliminary analysis showing that deeper networks can be deep thinkers \citep{schwarzschild2021can, bansal2022endtoend}, capable of better reasoning, and that wider networks can retain more knowledge \mg{CITE}. While these papers have occasionally obtained contradictory conclusions, we unify previously proposed theories about the role of depth and width in Transformer architectures for LLMs by conducting experiments across parameter and token scales.
\fi


\section{Related Work}~\label{sec:related_work}
\subsection{Scaling Laws}
Scaling laws address the trade-off between parameter count and number of training tokens, attempting to find the minimum loss possible for a decoder only language model when constrained by a fixed FLOP budget.
\citet{kaplan2020scaling} claim these are related by power laws, fitting a compute-optimal parameter count law of the form \(N_{opt} \propto C^{p_N}\). 
Unfortunately, scaling laws treat model design and training as if it has a single dimension (parameter count).  In reality, training is sensitive to many choices. 
Notably, \citet{hoffmann2022empirical} find significantly different exponents compared to \citet{kaplan2020scaling}.
\citet{pearce2024reconciling} and \citet{porian2024resolving} attribute most of this discrepancy to the choice to exclude embedding parameters from the parameter count, both showing one law can be transformed into the other via controlled changes.
\citet{kaplan2020scaling} justify including embedding parameters by showing that non-embedding parameters have a cleaner relationship with test loss, while using total parameters introduces a dependency on depth.
Scaling laws are commonly included in large model releases \citep{hu2024minicpm, bi2024deepseek, dubey2024llama}.

% This is all the work **VERY** related to ours
\citet{choshen2024hitchhiker} collect both loss and benchmark performance metrics for a multitude of open source language models and offer a practitioner's guide to fitting scaling laws.
Stating that the early period of training data should be dropped when fitting scaling laws, but intermediate checkpoints may be used if the learning rate schedule is appropriate.
Most notably, they suggest \(5\) models is ample to fit a scaling law.
In contrast, \citet{misfitting} show that different data sampling and optimization techniques when fitting scaling laws can cause big swings in outcomes. \sean{Idk how to say this without it sounding like we just copied them?}
\citet{hagele2024scaling} suggest that a constant learning rate plus cooldown is preferable to a cosine learning rate schedule.
The authors also find that stochastic weight averaging should be encouraged in scaling laws analysis as it tends to lead to better models.
Furthermore, \citet{memmoves} observe that FLOPs cannot be used to predict wall-clock time nor memory movement, and suggest that fast-training architectures may be preferred over those prescribed by scaling laws.

% Scaling laws for downstream performance {This can be cut up as we get the lm eval results in}
There are multiple works analyzing whether scaling laws can be used to predict downstream performance.
\citet{observational} show that scaling laws can be predictive of benchmark performance.
% \citet{chen2024scaling} first map compute to loss with a power law relationship and then take this loss to estimate the performance of the model on benchmarks using a linear relationship. {These ^ also use intermediate checkpoints but don't say what scheduler they use}
\citet{caballero2023broken} observe that prior functional forms cannot model non-monotonic behavior nor inflection points.
The authors suggest broken scaling laws which can model these behaviors to predict performance both downstream and upstream tasks.

Scaling laws are broadly applied to many areas of Machine learning, from machine translation \citep{ghorbani2021scaling, gordon2021data, zhang2022examining, bansal2022data} to sparse architectures \citep{clark2022unified, frantar2023scaling, yun2024toward,krajewski2024scaling}.
Further analyses using scaling laws have extended to analyzing finetuning \citep{hernandez2021scaling, zhang2024scaling} and data-limited scaling \citep{muennighoff2023scaling}.
Works in this vein are myriad; see our extended literature review in \Cref{sec:app-rel-works}.

% We similarly observe that having too many (equally-weighted) points throws off the law, when we constructed a loss-based law from the \citet{porian2024resolving} data. \sean{I like this point but don't know where it goes, maybe somewhere in experiments/discussion}

\subsection{The Role of Model Shape}
\citet{interplay} find that, for large models, optimal depth grows logarithmically with width. 
\citet{henighan2020scaling} find there is an optimal aspect ratio for each modality they study which gives maximum performance, for example they find \(5\) to be optimal for math models.
\citet{petty2024impact} analyze small ($<$400M parameter) transformers and find that the benefit of depth quickly diminish.
\citet{brown2022wide} show that in some cases shallower models can beat their parameter equivalent deep models on tasks for encoder-decoder transformer architectures.
This differs from \citet{kaplan2020scaling} who suggest aspect ratio is not a determining factor for final loss.

Most similarly to our work, \citet{alabdulmohsin2024getting} study the impact of width and depth for encoder-decoder vision transformers, using their laws to create a smaller transformer model which has competitive downstream performance when compared with much larger models.
The architecture found in this study has since been used by \citet{beyer2024paligemma} in PaliGemma.

Model shape has also been analyzed for sparse mixture of expert models and in the context of finetuning.
\citet{krajewski2024scaling} use ``granularity'' to allow their law for mixture of expert models to predict the width of the experts. 
\citet{scaleEfficiently} show that downstream performance strongly depends on shape when finetuning but pretraining perplexity does not.

\subsection{Zero-shot Hyperparameter Transfer}\label{sec:mup-relwork}
% \tom{This section can be short - it's not super important for the paper other than a few cites.}
% \todo{Maximal Update Parameterization (\(\mu\)P) \citep{yang2021zero,everett2024scaling} hyperparameters can be zero-shot transferred over width/depth changes. \(\mu\)-transfer for ResNets trained on Tiny-ImageNet \citep{bordelon2024depthwise}. Transformers on OpenWebText \citep{cerebras2024mupguide} theory on MLPs: \citep{yaida2022meta,kalra2023universal}. Paper on hyperparam transfer and shape \citet{hayou2023width}.}

Work on zero shot hyperparameter transfer across transformer model \textit{widths} is mature \citep{yang2021zero,everett2024scaling,hayou2023width,cerebras2024mupguide}, but achieving transfer across diverse model \textit{depths} is less well studied, especially in transformer language models \citep{bordelon2024depthwise}. While solving this problem perfectly is still an active area of research, we deploy a combination of initialization rules and learning rate transformation functions derived from prior work to try and achieve the required level of hyperparameter transfer to enable us to train models of diverse shapes and sizes (\Cref{sec:hyperparams}).

\section{Experimental Setup}~\label{sec:experimental_setup}
% \subsection{Training}~\label{subsec:training-details}

\paragraph{Architecture}\label{subsec:training-details}

% assumptions
% during search
%  n_head = n_embd / 128
% assert n_head % 1 == 0, "num heads must be an integer"
% n_head = int(n_head)
% assert n_head % 2 == 0, "num heads must be div by 2"
% assert n_head >= 2, "num heads must be at least 2"

% n_query_groups = n_head / 2
% assert n_query_groups % 1 == 0, "num query groups must be an integer"
% assert n_query_groups >= 1, "num query groups must be more than 0"

% func_param_count = param_counter(
%                 width=n_embd, # swept
%                 depth=n_layer, # swept
%                 vocab_size=vocab_size, # fixed, pythia
%                 n_head=n_head, # n_embd / head_size
%                 head_size=128, # fixed, max size on ROCm
%                 n_query_groups=int(n_head/2), # GQA at 2:1
%                 intermediate_size=4*n_embd, # expansion factor of 4
%             )

% as Instantiation values
% https://github.com/tomg-group-umd/lit-gpt-dev/blob/scaling-laws-2/litgpt/config.py#L1316

To reduce the search space of all possible models we added some constraints, each of which are either based on precedent from a popular model series like Gemma \citep{team2024gemma1,team2024gemma2}, Llama \citep{touvron2023llama2} or Pythia \citep{biderman2023pythia}, or practical considerations such as hardware details. 

% All models have a head size of \(128\) as this is the maximum \tom{Did you mean minimum? \jwk{no, but needs a fix}} head dim supported by the AMD implementation of Flash Attention 2 we used. 
% https://github.com/ROCm/flash-attention/blob/main/flash_attn/flash_attn_interface.py#L25
All models have a head size of \(128\) because \(256\) is the maximum head dim supported by the AMD implementation of Flash Attention 2 we used and we constrain our search to models with $>1$ attention heads.
We assume the simple convention of the Llama series where the head dimension is always $embed\_dim / n\_heads$  implying the embedding dimension (width) be divisible by $128$. Following conventions from the Gemma suite, we constrain the head count to be even to enable Grouped Query Attention \citep{ainslie2023gqa} with a query to key ratio of $2:1$ and we fix the intermediate size to be \(\times 4\) the width of the model. Finally, we choose our vocabulary size to match the $50,304$ tokens in the Pythia tokenizer. While many of the architecture choices mirror those from Gemma, for simplicity we do not use logit softcapping nor do we tie the embedding and language modeling head weight matrices.

With these boundaries in place, we searched the set of feasible models within target parameter count groups \(50M, 100M, 500M, 1B\) and \(2B\) with a tolerance of $\pm 5\%$. At smaller scales where experiments are less expensive, we train up to 5 models at diverse widths and depths. At large parameter counts train only three models, aiming for one ``standard'' aspect ratio (similar to existing models), one ``wide'' model, and one ``deep'' model. We visualize the models we chose to train in \Cref{fig:model_search_space} overlayed with a sampling of existing models from prior work. In the Appendix we plot the entire discrete set of all possible models under our constraints (\Cref{fig:model-search-space-complete}). Our \(22\) different models range from \(50M\) to \(2b\) parameters, spanning \(11\) widths from \(256\) to \(3072\) and \(18\) depths from \(3\) to \(80\). We call our models the \textit{Gemstones} as they are based on the Gemma architecture but come in many shapes and sizes.


\paragraph{Polishing the Gemstones}

For the main set of training runs, we train each model for \(350\) billion tokens of Dolma \citep{dolma} data with a context length of \(2048\) keeping the world batch size fixed for all experiments at $2048$ sequences per optimization step.
We use a linear learning rate warm up of approximately \(80\) million tokens then a constant learning rate, which we adjust to model size as described in \Cref{para:mup-lr}.
We also perform ablations over both cooldown and learning rate. 
For the first \(100\) billion tokens we take checkpoints every \(10\) billion tokens and cool these down creating a second set of models which have had their learning rate cooled to \(0\) linearly.
Specifically, we cool each model down by training for a further \(10\%\) of the total tokens which it has seen during training, i.e. our cooled down set of models have training budgets ranging from \(11\) to \(110\) billion tokens.
% The details of the learning rate ablation are included in \Cref{para:mup-over-2}.

We open source all checkpoints sampled at every \(2\) billion tokens, amounting to over \(4,000\) checkpoints in total. 
We also open source the training code and logged metrics for all runs.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/width_depth_sampling/model_search_space_with_commerical_models.pdf}
    \caption{We find that all prior work has sampled models along a fairly consistent set of aspect ratios. In principle this is fine so long as the scaling law is only used to extrapolate behavior for models with such a fixed aspect ratio. However, when we compare the aspect ratios of real-world LLMs to those used in prior work, we find a divergence. This motivates our sampling procedure, which sweeps across a range of aspect ratios.}
    \label{fig:model_search_space}
\end{figure}

\subsection{Training Hyperparameters}~\label{sec:hyperparams}

\jwk{To the cutter, this entire ``Training Hyperparameters'' section covering init and LR is self contained. Move the uninteresting parts or entirety to \Cref{subsec:app-training-details-hyperparams}. Will yield a full page of space.}

To train all models, we use AdamW with \(\beta\) parameters \(0.9\) and \(0.95\) and a weight decay of \(0.1\) but we do not apply weight decay to the bias or normalization parameters. All models are trained with tensor parallelism \citep{singh2024axonn,singh2024hybrid} over multiple nodes of AMD MI250X GPUs with a world batch size of \(2048\). To the best of our knowledge this makes the Gemstone suite of models the largest trained on AMD GPUs. As mentioned in \Cref{sec:mup-relwork}, training a diverse suite of models to develop a scaling law necessitates the ability to determine hyperparameter settings that achieve both stable and \textit{near optimal} training across model sizes, shapes, and token budgets without having to redo hyperparameter search for every individual training run.  This is a tall order for the Gemstones, as they vary in both aspect ratio and size.


\paragraph{Scalable Parameter Initialization Rules.}\label{sec:mup-init}
% https://github.com/tomg-group-umd/lit-gpt-dev/blob/scaling-laws-2/litgpt/init.py#L482

Following the strategy implemented by the OLMo(1) family \citep{Groeneveld2023OLMo} we apply a parameter initialization strategy intended to enable stable training and learning rate transfer across scales. We initialize all parameters as truncated normal ($\mu=0,a=-3*\sigma, b=3*\sigma$) with modified variances dependent on the parameter type. We use $\sigma=1/\sqrt{width}$ except for the attention projections which are initialized as $\sigma = 1 / \sqrt{2 * width * (l + 1)}$ and the MLP projections as $\sigma = 1 / \sqrt{2 * (4\times width) * (l + 1)}$ where in each case $l$ is the layer index (not the total model $depth$) and the $4\times$ factor comes from the relation of width to MLP intermediate dimension.

% \paragraph{Scaling $\eta$ across model shapes and sizes.}\label{para:mup}
% \paragraph{One $\eta$ to rule them all.}\label{para:mup-lr}
\paragraph{One $\eta$ to rule them all.}\label{para:mup-lr}

%Due to the wide range of model widths and depths in our suite, in addition to initialization rules, we have to modulate the \textit{effective} learning rate during parameter updates which is a function of the \textit{width} of the individual trainable parameters as well as how many composed weight updates occur during backpropagation (\textit{depth)}.
Our choice of learning rate must vary smoothly across model architectures, and maintain stability of training across a wide range of aspect ratios and scales. 
$\mu P$ theory (\Cref{sec:mup-relwork}) prescribes learning rate schedules that control the magnitude of gradient updates as parameters go to infinity. Literature suggests that learning rates should be scaled as $lr_{eff} = lr_{base} / width$ to achieve transfer across widths.  Independent analysis suggests the rule $lr_{eff} = lr_{base} / \sqrt{depth}$ to scale to models of different depths. We choose a hybrid learning rate scaling rule $lr_{eff} = lr_{base} / (width \times \sqrt{depth})$ since we vary both width and depth.

To choose the value of $lr_{base}$, we stress test our learning rate rule on four extremal model shapes; one wide and one deep, at both the $100$M scale and $2$B scale. We trained each of them for a short time with a sweep of learning rates $lr_{eff}$ ranging from $1e-4$ to $5e-2$. Within the training budget, each model achieves its lowest possible loss with a different learning rate (\Cref{fig:mup-four-corners}). To simulate the effect of training under a scaling rule, we take the data and re-scale the x-axis via the inverse of our proposed rule (eg. $lr_{base} = lr_{eff} \times (width \times \sqrt{depth})$). A visual inspections shows that all models achieve nearly optimal loss at $lr_{base}=5e0$, which we then adopt for all runs.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figs/mup/mup.pdf}
    \caption{Left: Preliminary training runs with initialization rules active, but no learning rate scaling. Right: Same data, but with x-axis rescaled to simulate the application of learning rate scaling via the inverse of our proposed rule $lr_{base} = lr_{eff} \times (width \times \sqrt{depth})$.}
    \label{fig:mup-four-corners}
\end{figure}

Empirically, this scheme of minimizing loss after a short training budget seems to result in a fairly aggressive learning rate.
While we did successfully get all training runs across the $350$B token finish line, some models experienced loss spikes and needed babysitting. We discuss these issues in detail in \Cref{sec:loss-spikes}.
% \paragraph{Robustness of fits a redo with \(\eta / 2\).}\label{para:mup-over-2}
Finally, we ablate the choice of optimal learning rate by training an identical set of models for \(100\) billion tokens with a learning rate of \(\eta / 2\). We will discuss the impact of this ablation on our fitting process.

\subsection{Fitting Scaling Laws}
We fit scaling laws using methods similar to approach \(1\) and \(3\) from Chinchilla \citep{hoffmann2022empirical}.

% Trying to word the next sentence as the validation set it held out but may not be the exact same across node counts
We fit all laws using the log perplexity of all trained models on a sample of \(100\) million tokens from a fixed, held-out validation set from the training distribution.
We also collect log perplexity values for a range of open source models \citep{team2024gemma1, team2024gemma2, touvron2023llama2, dubey2024llama, yang2024qwen2, yang2024qwen25} on the same validation data to allow for a comparison between our predictions and a selection of widely used models.
We wrote a specialized FLOP counting function as we find that simple rules of thumb (e.g., FLOPs= \(6 \times parameters\) \citep{hoffmann2022empirical}) does not accurately account for differences in FLOPs between extremely wide and narrow architectures.
We discuss this further and present our function in \Cref{subsec:app-flops-counting}.

% \sean{This next sentence is needed}
Following prior work, we plot the Epoch AI Replication \citep{besiroglu2024chinchilla} of Chinchilla \citep{hoffmann2022empirical} on all plots and use the coefficients for Kaplan plotted by \citet{porian2024resolving} which were extracted from the original paper \citep{kaplan2020scaling}.

\paragraph{Approach 1}
% Approach 1 things
One approach to fitting a scaling law is to plot the curve of training FLOPs vs loss for a range of architectures, and superimpose these curves on the same chart. The lower envelope of these curves (the pareto-optimal architecture for each FLOP count) then defines the law (see \Cref{fig:approach-1}).
%The final loss value for each training run is then used to estimate the lowest possible loss at that FLOP budget, this is called approach 1 by \citet{hoffmann2022empirical}.
This is called {\em approach 1} by \citet{hoffmann2022empirical}.
As we use a constant learning rate, we can use all recorded validation losses to fit our law.
%However, most of these recorded validation losses are not optimal for their FLOP count.
\citet{hoffmann2022empirical} and \citet{kaplan2020scaling} sample model shapes so densely that they have a near-optimal architecture at each FLOP count.  This works when all architectures lie in a one-dimensional space (parameterized by parameter count), as each model is optimal in some FLOP regime, and the lower envelope is densely populated. In out two dimensional exploration (varying width and depth), some models are never optimal, and the ones that are do no densely populate the envelope.
To overcome this, we propose a new fitting method where we fit a lower {\em convex hull} to our loss curves.  This hull is only supported by a sparse set of optimal models.
This naturally excludes sub-optimal models that lie above the convex hull of optimality, and makes the resulting scaling law far more robust to the choice of model sampling.
This is visualized in \Cref{fig:approach-1}.

% Approach 2 things
\paragraph{Approach 2}
Another method to fit scaling laws is to put model runs into isoFLOP bins and choose the best parameter count in each bin. \citet{hoffmann2022empirical} call this approach 2.
\citet{hoffmann2022empirical} then interpolate the best configuration from the parabola fit to each FLOP budget. 
Our 2-dimensional set of models finely cluster into isoFLOP bins, meaning our data is not easily amenable to approach \(2\), hence we exclude this approach from our analysis.
\sean{Cite miniCPM as they also ignore this? @AP neither David nor I can find this in their paper} 


\paragraph{Approach 3}
The final approach described by  \citet{hoffmann2022empirical} is to fit a parametric formula to the loss values with the ansatz
\begin{equation}
L(p,T) = \frac{A}{p^{\alpha}}+\frac{B}{T^{\beta}}+\varepsilon
\label{eq:params}
\end{equation}
where \(p\) is parameter count and \(T\) is tokens.
We fit our models using L-BGFS \citep{liu1989limited} with a Huber loss (\(\delta = 10^{-5}\)) between the empirical log loss and the model prediction, and use multiple initializations following  \citet{besiroglu2024chinchilla}. % we try \(5\) initializations for every exponent, \(6\) initializations for every coefficient and \(5\) initializations for the irreducible error in the formula we are fitting.
In \Cref{app-subsec:approach-3-delta}, we highlight the interplay between \Cref{fig:not-enough-tokens} and the value of \(\delta\) being used in the Huber loss, finding varying delta without being in the limiting regime also leads to differences in fitted laws.

%When comparing to a baseline which is only parameterized by number of parameters in the model and number of training tokens, we fit laws of the form shown in \Cref{eq:params}.

% Where \(p\) is the number of parameters in the model and perform the same fitting procedure as we do for laws parameterized by the hidden dimension of the model, the number of layers in the model and the number of training tokens.
% \[\min_{A,\alpha,B,\beta,\varepsilon} \sum_{\text{run }i}Huber_\delta\left[\log\left(\frac{A}{p_i^{\alpha}}+\frac{B}{T_i^{\beta}}+\varepsilon\right)-\log(loss_i)\right]\]

\paragraph{A new approach:  Width--depth scaling}
Our broad sampling of model architectures enables us to study scaling laws that predict loss as a function of not only parameter count, but also model aspect ratio. 
For this purpose, we consider a perturbation of the standard scaling law with additional terms to account for the impact of model width and depth.
\begin{equation}
L(w,d,p,T) = \frac{A}{w^{\alpha}}+\frac{B}{d^{\beta}}+\frac{C}{p^{\gamma}}+\frac{D}{T^{\zeta}}+\varepsilon.
\label{eq:wd}
\end{equation}


% Variation around standard scaling law, same original params + peturbations
% Also ref to other eq in appendix
With \(w\) is the hidden dimension of the model, \(d\) is the number of layers.
All of \(A,\alpha,B,\beta,C,\gamma,D,\zeta\) and \(\varepsilon\) are optimized to fit our scaling runs.
We choose this form as generalizes the standard law \eqref{eq:params}, and it predicts the same behaviors as the standard law if width and depth are not implicated in any systematic trends.
%\sean{We need to defend this choice a bit more here}
We ablate another possible form of this law in \Cref{app-subsec:approach-3-alternative-forms}.


% We also independently collect validation data from several prior works which release widely used models \citep{bi2024deepseek, hu2024minicpm, touvron2023llama1, touvron2023llama2}.
% We collect the model dimensions from the tables presented in these works and extract the loss from plots using an online plot digitizer tool.
% Unfortunately, we cannot perform this data collection method for many other papers \citep{dubey2024llama, jiang2023mistral, team2024gemma1, team2024gemma2} as loss curves and values are not released.
% However, we can still analyze the predicted final loss of our scaling laws on these models. 
% The extracted data is presented in Appendix Section \ref{subsec:app-valdata}.

\section{Experiments}~\label{sec:experiments}
In this section, we show that our convex hull fitting method can recover a scaling law using approach 1, and we observe scaling laws that predict the impact of width and depth \Cref{subsec:experiments-approach-3}.
We finish this section with an analysis of how model shapes differ depending on whether FLOPs or training time are optimized \Cref{subsec:flops_vs_time}.

Before we begin, we investigate the impact of sampling methods on scaling laws.  Like other works before us, we see that scaling laws can be quite fragile and dependent on sampling methods.  For this reason, it is important to be aware of the sources of error that are incurred through the sampling and model fitting process.

\subsection{A Rainbow of Scaling Laws}
\subsubsection{Don't Taste the Rainbow!}
\tom{The front matter of this section doesn't clearly state the purpose of the section, or what the takeaways are.  This subsection is pretty messy and disorganized right now!}
We begin by fitting laws of the classical form which do not take into account width and depth.
In \Cref{fig:rainbow-plot} we show the optimal predictions of multiple possible laws sampled from different data.
We use Equation \(4\) derived by \citet{hoffmann2022empirical} to transform approach 3 laws into  prescriptions for the optimal parameter count.
This does assume FLOPs = \(6 \times parameters\), which we find to not accurately reflect FLOPs well as model shape varies but we find in \Cref{fig:approach-3-wd-full} that this does not make a significant impact when plotting prescriptions for these laws which do not involve width and depth. 

% Attempting to say we do 4 ablations which are a meh level of interesting
Firstly, we fit laws both including and excluding embedding parameters in the parameter count, which both \citet{pearce2024reconciling} and \citet{porian2024resolving} find to be a primary explanation of the discrepancies between the prescriptions found by \citet{kaplan2020scaling} and \citet{hoffmann2022empirical}. 
Secondly, we fit a law to the endpoints of our cooldown models; we find that this only changes the prescription of the law minorly, nevertheless this does change the prescription.
The third ablation we include is a learning rate ablation, where we train all models with half of the optimal learning rate, fitting a law to this we again finding a small change in the resulting prescription.
This highlights the need for optimal hyperparameter choice to maximize the applicability of the law across model shapes.
We also train a group of models with a width of \(512\): this allows for analysis both by us and the community on the specific impact of depth of a model. 
We find fitting on only these models creates a drastic difference in the fitted law, reducing the gradient of the law for optimal parameters by a large amount, highlighting the need for diversity of model size in multiple dimensions the fitting data. \david{small/large can be spelled out maybe, unless the figures speak for themselves, though it's always hard to say what, say, exponent difference is large or small}

% Attempting to explain the slim chinchilla ablations

Most interestingly, we look at the role of sampling in the derived law.
To do this, we take our data and the data created by \citet{hoffmann2022empirical} which was extracted by \citet{besiroglu2024chinchilla}.
We then look for model sizes in the Chinchilla data that are closest to models we train with the closest to conventional width--depth ratios.
Conversely, we subsample our token counts to align as closely as possible those used by \citet{hoffmann2022empirical}.
We call the two resulting subsets of data ``Chinchilla Reduced Sampling.''
We then fit scaling laws to both of these, and confirm the resulting laws are substantially closer: the law fitted on the subset of \citet{hoffmann2022empirical} increases the gradient of the line compared to the original Chinchilla law and, on our data, the reduced sampling reduces the gradient of the line.
This highlights that the sampling of the data we use to fit our laws on is extremely important and applies to even the most widely cited papers analyzing scaling laws.
Notably, there are \(5\) models in this subset for both \citet{hoffmann2022empirical} and our data, this meets the rule of thumb given by \citet{choshen2024hitchhiker} for the minimum number of models should be used to fit a scaling law.

From this analysis ``rainbow'', we repeatedly see that scaling laws are highly contextual, rules of thumb are easily broken without being intentionally adversarial and constraining compute can easily lead to misfit scaling laws.

\subsubsection{The Leprechaun at the End of the Rainbow}
We now move to possible ways to check that a scaling law is being fit in its limiting regime.
In \Cref{fig:not-enough-tokens}, we plot the gradient of the law as we remove the smallest samples of that quantity from the scaling law.
For example, in the tokens analysis, we remove all token counts less than or equal to the smallest token count from the dataset, fit a law then plot the slope of the law; we then repeat this for the next smallest token count until we have exhausted all token counts.
\sean{@AP describe bucketing please?}

% \citet{choshen2024hitchhiker} and \citet{chen2024sudden} -> removing tokens from beginning of training
We see that the tokens lines in \Cref{fig:not-enough-tokens} for both approach 1 and 3 increase in gradient as smaller token counts are removed. 
This supports and extends the findings of 
\citet{choshen2024hitchhiker}, who suggest excluding the beginning of training from the fit.
However, we show this has a much larger impact than previously known and much more of the initial training data should be discarded for valid scaling laws.

\subsection{Width and Depth through the Lens of Scaling Laws}
\subsubsection{Approach 1}~\label{subsec:experiments-approach-1}
\begin{figure*}[t!]
\centering
    \includegraphics[width=\textwidth]{figs/approach1/figure_2_gpu_hours.pdf}
    \caption{Row one: Validation loss over FLOPs (left) and GPU hours (right) We use approach one with rejection sampling to find the optimal points on the convex hull in each setting, marked with crosses.\\Row two: We fit a line to the tokens per parameter of these empirically optimal points and find a slightly higher but still constant tokens per parameter prescription than \citet{hoffmann2022empirical}. Full plot in \Cref{fig:approach-1-full}.}
    \label{fig:approach-1}
\end{figure*}

In \Cref{fig:approach-1} (row one), we see our validation losses plotted as both a function of FLOPs (left) and GPU hours (right) for the first \(100\) billion tokens of training.
We calculate GPU hours from the average recorded time each model took per optimizer step.
In \Cref{fig:approach-1} (row two), we see the prescription of the fitted laws for tokens per parameter.
We see that the tokens-per-parameter prescription of our approach one fitting is also close to constant, like \citet{hoffmann2022empirical}, but slightly higher.
We extend this plot showing predicted total parameters, tokens and over multiple ablations in \Cref{subsec:app-approach-1-ablations}.
We give a more detailed plot of each models individual validation loss in \Cref{sec:app-training-details}.

\subsubsection{Approach 3}~\label{subsec:experiments-approach-3}
Another approach to fitting a scaling law is to use a parametric function to predict the loss when given parameter count and number of tokens.
As our data is intentionally sampled to have a wide variety of widths and depths, we can also attempt to predict the optimal aspect ratio of a model for a given FLOP budget.
We do this by optimizing over the law shown in \Cref{eq:wd} with four terms so we can explicitly optimize the width and depth terms.
One flaw of the classical scaling law form is that optimizing the width--depth ratio for a fixed FLOP budget leads to infinitely wide models, meaning an analysis such as the one we present here is not possible.
We find that classical convex optimization struggles to find optimal points here due to the integer constraint we impose on the number of attention heads in the model.
Instead, we use a brute force approach by sampling approximately \(10^8\) model shapes and tokens counts, then using the law to predict the loss for each model.
We then find the minimum of all sampled points for a given FLOP range.
In \Cref{fig:approach-3-wd} we plot the output of this process.
In the left of the figure, we see that the prescribed width depth ratio increases with the FLOP budget, we plot a selection of widely used models trained by industry using \(6ND\) to calculate FLOPs with their published training token counts.
In \Cref{fig:approach-3-wd} (right), we see that the optimal tokens per parameter follows more closely to the prescription found by \citet{kaplan2020scaling}.
This suggests the optimal tokens per parameter decreases as the number of FLOPs increases.

\begin{figure*}[t!]
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_hot_100b+_width_depth_params_FLOPs.pdf}
    \caption{Approach 3 laws with the parametrization shown in \Cref{eq:wd}. We see the prescribed optimal width--depth ratio increases with the FLOPs (left) budget and the optimal tokens per parameter decreases as the FLOPs budget increases (right).}
    \label{fig:approach-3-wd}
\end{figure*}

% \subsection{The Computational Balancing Act: FLOPs vs. GPU Hours}
\subsection{If You Value Your Time, Train Wide Models}
\label{subsec:flops_vs_time}

We now have a prescription for the optimal width and depth for any amount of total training FLOPs. But how important is it to follow our prescriptions -- how suboptimal would it be to choose a width and depth not on the line, as most industry models do? 

In~\cref{fig:flops_gpu_hrs} we reflect on the consequences of such an architectural choice, by considering how much of a given resource---FLOPs or GPU hours---would be ``overspent'' to reach any target loss value with the plotted architecture rather than the prescribed width and depth. We find that choosing to train skinny models~(top left) wastes many FLOPs and GPU hours. The scale of overspend is quite different however, with the least efficient models only overspending about \(50\%\) on FLOPs but wasting more than \(200\%\) of the GPU hours spent by the best configuration. In other words, in the time taken to train a single super-suboptimal model to the desired loss value, one could train three optimal-width-depth models. We note that the time-optimal models tend to be the wider ones, and the explanation for this is in our training scheme. Similar to other open-source efforts such as~\citet{olmo20242}, we do not make any use of pipeline parallelism, only tensor parallelism. Therefore wider models are much easier to scale. This may mean our results do not generalize to every other setting. 

\begin{figure*}[t!]
\centering
    \includegraphics[width=\textwidth]{figs/efficiency_vs_loss_gpu_hours_100b+.pdf}
    \caption{Top: On the left, we show the difference in FLOPs compared to the loss optimal model at this token count \(300\) billion as a percentage of the loss optimal models FLOPS. 
    In the middle, we do the same but for GPU hours spent. On the right,  we plot the difference in percentage overspend for GPU hours and FLOPs.
    Bottom: We represent the overspends which are the y-axis in the top row as color to show the spread of FLOPs and GPU hours overspends over width and depth.}
    \label{fig:flops_gpu_hrs}
\end{figure*}

In order to prescribe the best width and depth for a given amount of FLOPs in~\cref{fig:approach-3-wd}, we need to compute the FLOPs spent by every feasible model shape, map those model shapes to predicted losses using our fitted function, and then choose the model shape for that value of FLOPs that achieves the best loss. However, FLOPs are not king -- most researchers care about the amount of time it actually takes to train a model. This is more challenging for Approach 3, because we would need to compute the amount of time each model takes to train. While we can do that in our training setup, by just initializing the model and profiling it for a few steps, this may not generalize to other training setups. Luckily, as we open source all artifacts, future work can transfer this analysis to their setup by measuring the time for one step for each model shape and transferring this time to our dataframes and replotting any figure shown.


% In \Cref{fig:flops_gpu_hrs} we plot the interplay between being FLOPs and GPU hours optimal at \(300\) billion tokens.
% \todo{what are the takeaways?}

One thing we do highlight in \Cref{fig:flops_gpu_hrs} is that in the bottom row, if we were to separate the depth axis into bins for GPU hours (bottom, middle) the widest models are often loss optimal, highlighted by stars.
We see in \Cref{fig:not-enough-tokens}, that the width lines have not leveled out, suggesting wider models are required to see limiting behavior here.
We also see that wider models would improve this analysis and suggest future work to clarify where the limiting width regime lies. \sean{Idk how to word this without shooting us in the foot}

We see in \Cref{fig:approach-1} that we recorded the time per step for each model and transformed this into a GPU hours taken to train axis for our models.
We highlight our tensor parallel only training is not unusual within the community \sean{CITE, maybe olmo 2}. \sean{Idk what to argue about AMD training? No one does that..}\david{OTOH, DeepSeek V3 trained with *only* pipeline parallelism, no tensor parallelism! (pg 4)}
Although analyses based on our time per step recordings are highly fitted to the training strategies and topologies we deploy, we emphasize that, as we open source all artifacts, future work can transfer this analysis to their setup by measuring the time for one step for each model shape and transferring this time to our dataframes and replotting any figure shown.

\section{Discussion}~\label{sec:discussion}
To summarize, we see that scaling laws are brittle (\Cref{fig:rainbow-plot}) but there are methods we could possibly use to determine if we are in the limiting regime (\Cref{fig:not-enough-tokens}).
We train a comprehensive set of \(22\) models with ablations using \(\mu p\) to optimize applicability to the community (\Cref{subsec:training-details}).
We then propose a new method for fitting Approach 1 style laws, using a convex hull (\Cref{fig:approach-1}) to remove points to reduce sampling bias in the fitting.
Using the rich range of widths and depths included in our suite of trained models, we fit Approach 3 laws with additional terms to allow us to optimize the width and depth of the prescribed models in a principled manner (\Cref{fig:approach-3-wd}).
Finally, we analyze the trade off between FLOP and time optimality finding wider models are very often time-optimal to train within our training set-up and this analysis can easily be extended to any training set-up (\Cref{fig:flops_gpu_hrs}).

We hope this work encourages a rich range of future work on the impact of width and depth within modern transformer architectures using the large amount of open source artifacts we produce.
This could also be extended to include more hyperparameters used when training transformer architectures, such as expansion factor.
Follow up work should also look to find more quantitative measures to analyze when a quantity being fitted in a scaling law is in the limiting regime.

\sean{Do we want limitations here?}

\section*{Software and Data}
We train all models using a fork of \texttt{litgpt} \citep{litgpt-2023} enhanced with AxoNN \citep{singh2024axonn,singh2024hybrid} tensor parallelism.
We open source all models used in our analysis to Hugging Face \citep{wolf2020huggingfaces} and the logging from training on Weights and Biases.
All model training code and code to fit the scaling laws is released on GitHub.
% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Extended Related Works}~\label{sec:app-rel-works}
Scaling laws are broadly applied to many areas of Machine learning, such as machine translation.
\citet{ghorbani2021scaling} split the parameters term into two, one for each of encoder and decoder, and similarly to \citet{gordon2021data} analyze the relationship between BLEU scores and scaling laws.
\citet{zhang2022examining} and \citet{bansal2022data} study the impact of architecture choice on the scaling law, finding increasing data or parameters can compensate for worse architectural decisions.

Scaling laws have also been applied to sparse architectures.
\citet{clark2022unified} analyze how the number of experts can be used in the law, studying both linear and quadratic interactions for many types of routing models.
\citet{yun2024toward} extend this analyzing the trade offs between optimal training and optimal inference.
\citet{krajewski2024scaling} find that with optimal settings a Mixture of Experts model always outperforms a transformer model at any computational budget.
\citet{frantar2023scaling} focus on weight sparsity within foundation models adding a multiplicative parameter on the parameters term in the law.

These techniques are not limited to generative text modeling only, they have also been applied to multi-model models.
\citet{henighan2020scaling} find optimal model size can be described as a power law for model modeling including images and video. 
The authors also find that model size does not help `strong generalization' for problem solving.
\citet{aghajanyan2023scaling} analyze text, images, code and speech, presenting a scaling law to describe the competition between these modalities and describe a regime for optimal hyperparameter transfer from the unimodal to multimodal regimes.
\citet{liang2024scaling} look at scaling laws for diffusion transformer models.
\citet{li2024bigger} analyze scaling laws for vision encoder commonly used to encode image inputs for transformer model backbones, finding increasing the size of the encoder alone can lead to performance degradation in some cases.

Further analyses using scaling laws have extended to analyzing finetuning and data limited scaling.
\citet{hernandez2021scaling} find that finetuning is much more compute efficient when the pretraining ignored.
\citet{zhang2024scaling} study parameter efficient finetuning regimes find a multiplicative law is better for the finetuning setting than the classical additive law used by others.
\citet{muennighoff2023scaling} analyze the data constrained training regimes, finding epoching data up to four times is as good as training on deduplicated data in terms of reducing loss.


\section{Just How Bad Is Under/Overtraining?}
In \Cref{fig:approach-3-wd}, we see the optimal predictions from our law of the form shown in \Cref{eq:wd}.
We can perturb the optimal points we see in \Cref{fig:approach-3-wd} to simulate overtraining.
To do this we take the optimal point at a FLOPs budget and an overtraining factor (for example \(\times 2\) tokens).
We then increase the optimal tokens prescribed by the law by this factor and re-optimize the model shape to be optimal at this FLOP budget and token count.
An overtraining factor of less than one corresponds to undertraining the model.
We show the results of this process in \Cref{fig:overtrain}, where we see that overtraining does increase loss at a given FLOP count but the loss can be reduced by increasing the FLOP budget.
In \Cref{fig:overtrain-parabola}, we show horizontal slices from \Cref{fig:overtrain} at four FLOP budgets which we include in the title of each subplot.
Here we see as the overtrain factor increases loss decreases to the minimum prescribed loss value when the overtraining factor is equal to one which corresponds to being optimal trained according to the law before increasing again, forming a parabola.
We include the loss values of open source models on the our validation set to allow readers to contextualize the y axis values.

\begin{figure*}
\centering
    \includegraphics[width=0.49\textwidth]{figs/overtrain/parabola_overtrained_predictions_wd_hot.pdf}
    \caption{We increase the optimal token count by multiplying by the overtraining factor. We then optimize the model shape to achieve the lowest loss possible at each FLOP budget and overtraining factor. We see that overtraining a smaller model increases loss a small amount.}
    \label{fig:overtrain}
\end{figure*}
\begin{figure*}
\centering
    \includegraphics[width=0.49\textwidth]{figs/overtrain/parabola_overtrained_predictions_wd_hot_2.pdf}
    \caption{We take a four FLOP budgets (title of each plot) and plot the loss as a function of overtraining factor seeing that under or overtraining increases predicted loss.}
    \label{fig:overtrain-parabola}
\end{figure*}

\section{Ablations for Approach 1}~\label{subsec:app-approach-1-ablations}
\subsection{Extended Paper Figures}
\begin{figure*}
\centering
    \includegraphics[width=0.8\textwidth]{figs/approach1/figure_2_gpu_hours_full.pdf}
    \caption{Approach 1 full, corresponds to \Cref{fig:approach-1}}
    \label{fig:approach-1-full}
\end{figure*}

\subsection{Alternative Learning Rates}
\begin{figure*}
\centering
    \includegraphics[width=0.8\textwidth]{figs/approach1/figure_2_gpu_hours_lr_ablation_full.pdf}
    \caption{Approach 1 lr ablation \(\frac{\eta}{2}\).}
    \label{fig:approach-1-lr}
\end{figure*}

\subsection{Cooldown}
\begin{figure*}
\centering
    \includegraphics[width=0.8\textwidth]{figs/approach1/figure_2_gpu_hours_cooldown_full.pdf}
    \caption{Approach 1 cooldown}
    \label{fig:approach-1-cool}
\end{figure*}

\section{Ablations for Approach 3}~\label{subsec:app-approach-3-ablations}
\subsection{Extended Paper Figures}
\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_hot_100b+_width_depth_params_FLOPs_full.pdf}
    \caption{Corresponds to \Cref{fig:approach-3-wd}.}
    \label{fig:approach-3-wd-full}
\end{figure*}

\subsection{Alternative Learning Rates}
\label{app-subsec:approach-3-lr-over-2}
We also fit approach 3 laws to the dataset we trained with \(\eta/2\) as described in \Cref{sec:mup-init}.
We again use a brute force approach as above to plot the results but to allow for precise comparison with later ablations we ignore the integer constraints on the number of heads, still enforcing that at least one head number be in every model.
We remove this constraint for all models shown in \Cref{app-subsec:approach-3-lr-over-2}, \Cref{app-subsec:approach-3-cooldown} and \Cref{fig:approach-3-cooldown}.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_lr_ablation_hot_width_depth_params_relaxed_FLOPs_full.pdf}
    \caption{Approach 3 Width and Depth \(\frac{\eta}{2}\)}
    \label{fig:approach-3-eta-over-2}
\end{figure*}

\subsection{Cooldown}
\label{app-subsec:approach-3-cooldown}
Here, we fit approach 3 laws to the subset of data for models for which we linearly decreased the learning rate to zero for.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_cool_end_width_depth_params_relaxed_FLOPs_full.pdf}
    \caption{Approach 3 Width and Depth -- Cooldown}
    \label{fig:approach-3-cooldown}
\end{figure*}

\subsection{Removing Smaller Token Counts}
\label{app-subsec:approach-3-remove-tokens}
\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_hot_120b+_only_width_depth_params_relaxed_FLOPs_full.pdf}
    \caption{Approach 3 Width and Depth -- Removing all token counts less than \(120\) billion}
    \label{fig:approach-3-remove-tokens}
\end{figure*}

\subsection{The Impact of Delta in the Huber Loss}
\label{app-subsec:approach-3-delta}
So far we have fit all approach three laws with a Huber loss delta of \(10^{-5}\). 
We now ablate this decision by refitting all laws with a delta of \(10^{-3}\).
We use an extremely large grid search of over \(4\) million initializations for the width--depth based law when fitting.

To begin we show the prescriptions of the approach 3 laws if the integer constraints are removed, as we did for the learning rate and cooldown ablations in Figures \ref{fig:approach-3-eta-over-2} and \ref{fig:approach-3-cooldown} respectively.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_hot_100b+_width_depth_params_relaxed_FLOPs_full.pdf}
    \caption{Corresponds to \Cref{fig:approach-3-wd}, this plot removes the integer constraints when optimizing.}
    \label{fig:approach-3-wd-full-relaxed}
\end{figure*}

We now compare all approach 3 laws found with the increased delta.
Specifically, we plot the full dataset laws with delta of \(10^{-5}\) in \Cref{fig:approach-3-wd-full-relaxed} and with \(10^{-3}\) in \Cref{fig:approach-3-wd-full-relaxed-delta-3}.
We plot the learning rate ablation laws with delta of \(10^{-5}\) in \Cref{fig:approach-3-eta-over-2} and with \(10^{-3}\) in \Cref{fig:approach-3-eta-over-2-delta-3}.
We plot the cooldown ablation laws with delta of \(10^{-5}\) in \Cref{fig:approach-3-cooldown} and with \(10^{-3}\) in \Cref{fig:approach-3-cooldown-delta-3}.
We see here that the difference for the full dataset and cooldown ablations laws is minimal when changing the delta.
However, we see that the prescriptions of the learning rate ablation laws do change, especially the width--depth ratio prescription.
This highlights the finding in \Cref{fig:not-enough-tokens} as the learning rate ablation dataset only has models trained for the first \(100\) billion tokens in it, the law is still very variable and even the slightest change in fitting parameters can lead to different outputs in from the law.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_hot_100b+_width_depth_params_relaxed_FLOPs_full_delta-3.pdf}
    \caption{All main data \(\delta = 10^{-3}\). Corresponds to \Cref{fig:approach-3-wd-full-relaxed}.}
    \label{fig:approach-3-wd-full-relaxed-delta-3}
\end{figure*}

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_lr_ablation_hot_width_depth_params_relaxed_FLOPs_full_delta-3.pdf}
    \caption{LR ablation \(\delta = 10^{-3}\). Corresponds to \Cref{fig:approach-3-eta-over-2}.}
    \label{fig:approach-3-eta-over-2-delta-3}
\end{figure*}

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_cool_end_width_depth_params_relaxed_FLOPs_full_delta-3.pdf}
    \caption{Cooldown \(\delta = 10^{-3}\). Corresponds to \Cref{fig:approach-3-cooldown-delta-3}}
    \label{fig:approach-3-cooldown-delta-3}
\end{figure*}


\subsection{Alternative Law Forms}
\label{app-subsec:approach-3-alternative-forms}
We also experiments with laws of the form shown in \Cref{eq:wdt-only}.
In \Cref{fig:approach-3-width-depth-tokens-law}, we see that the prescriptions of this law are approximately in line with those of \citet{kaplan2020scaling}.
Unlike the laws for shown in \Cref{eq:wd}, these laws tend to prescribe that the width--depth ratio should go to zero as the FLOPs budget increases, i.e. prescribing and infinite depth model at an infinite budget.

\begin{equation}
L(w,d,p,T) = \frac{A}{w^{\alpha}}+\frac{B}{d^{\beta}}+\frac{C}{T^{\gamma}}+\varepsilon
\label{eq:wdt-only}
\end{equation}

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_oracle_hot_100b+_relaxed_FLOPs_full.pdf}
    \caption{Approach 3 law of the form shown in \Cref{eq:wdt-only}.}
    \label{fig:approach-3-width-depth-tokens-law}
\end{figure*}

\section{Data Sampling}

We plot the entire space of all possible models subject to our design constraints discussed in \Cref{fig:model-search-space-complete}. While exploring the impact of finer grained depth differences during our experiments, we decided to add two additional models slightly outside of $\pm 5\%$ tolerance band at the $100$M scale; for $width=512$ in addition to the originally chosen depths of $12$ and $13$ we added $11$ and $14$ these appear as a dense collection of $4$ points at the same $width$.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/width_depth_sampling/model_search_space.pdf}
    \caption{
    The points we sampled.
    }
    \label{fig:model-search-space-complete}
\end{figure}

\section{Training}~\label{sec:app-training-details}
Despite our best efforts to sufficiently mix the training data, we still see slight jumps in the global training loss when the training switches between chunks of data, hence we use validation loss to fit all laws as this is smooth.

\subsection{Loss Curves}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/wandb_loss_vs_tokens.pdf}
    \caption{Loss curves for the main \(22\) training runs}
    \label{fig:loss_vs_tokens}
\end{figure}

\subsection{Additional Training Details}~\label{subsec:app-training-details-hyperparams}

\paragraph{Dealing with Training Instabilities}\label{sec:loss-spikes}

After some of the widest models were trained beyond \jwk{$50$B?} tokens we began to observe unrecoverable loss spikes that were proceeded by small wobbles in the loss trajectory. Under the general intuition that the culprit was most likely that the $width / depth$ ratios considered were simply \textit{too extreme} for existing initialization and learning rate scaling approaches to handle, we reran some of the models with a ``patch'' in place.

We modified the initialization rules and learning rate scaling factors to rescale the depth and layer indices of the model such that if $width / depth > 256$ scale variances and learning rates as if the depth of the model was actually $depth' = \lceil(width / 100)\rceil$. The overall effect of the patch is to initialize and scale learning rates more conservatively, as if the aspect ratio were only $100$ while keeping the original $width$ of the model. We found this allowed us to complete training out to $350$B tokens for even our most extreme models.

However, after $350$B tokens, despite these efforts we observed that most models still diverged anyway \jwk{\sean{} can we say only the extreme ones diverged or no?}. While a partial cause of this could be the constant learning rate scheduler employed during training, concurrent work, from the authors of the original OLMo paper and codebase \citep{Groeneveld2023OLMo} from which we derived some of our choices, reported that the initialization scheme dubbed the ``Mitchell-init'' is indeed systematically prone to instabilities later on in training \citep{olmo20242}. While an unfortunate finding, we were unable to rerun all of our experiments due to the consumption of significant non-fungible compute resources in the original experiments.

\paragraph{Models Lacking Cooldowns}\label{sec:models-lacking-cooldowns}

Our cooldown ablation is from initial experiments (below $100$B) tokens of training which do not use the patched learning rates scaling rules, meaning there is a minor discrepancy between the cooldown ablation and main set of training runs for the widest models from the three largest parameter count groups. We also do not cool down, the two width \(512\) models which do not fall into the \(\pm 5\%\) boundary of the \(100M\) parameter count.
This clipping allowed us to train a full set of \(22\) models to \(350\) billion tokens without unrecoverable loss spikes.



\section{FLOP counting matters}~\label{subsec:app-flops-counting}
In~\Cref{fig:flop_counting} we show that the common approximation of FLOPs per token\(=6N\), where N is the number of parameters, miscounts the true FLOPS by a significant amount. 
\begin{figure}[hbtp]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/flops_accounting_gpu_hours.pdf}
    \caption{Counting the FLOPs properly becomes more important for aspect ratios off the beaten path.}
    \label{fig:flop_counting}
\end{figure}


\begin{minted}{python}
VOCAB_OURS = 50304
SEQ_LEN = 2048
WORLD_BATCH_SIZE = 2048.0
HEAD_SIZE = 128
EXPAND_FACTOR = 4.0

def flops_per_token_gqa(
    width: NDArray[number] | number,
    depth: NDArray[number] | number,
    vocab_size=VOCAB_OURS,
    queries_per_group=2,
    seq_len=SEQ_LEN,
):
    """
    Some details (negligible even for extremely wide models) omitted, including:
    * numerically stable softmax
    * softmax addition only being over rows
    * dot products being only n-1 additions (fused multiply-add exists anyway)
    """
    num_qheads = width / HEAD_SIZE
    num_kvheads = (
        2 * num_qheads / queries_per_group
    )

    embeddings = 0  # 0 if sparse lookup, backward FLOPs negligible

    attention = 2.0 * seq_len * (num_qheads + num_kvheads) * width * HEAD_SIZE
    attention += (
        3.5 * seq_len * (num_qheads + num_kvheads / 2) * HEAD_SIZE
    )  # RoPE, as implemented here/GPT-NeoX
    # score FLOPs are halved because causal => triangular mask => usable sparsity
    kq_logits = 1.0 * seq_len * seq_len * HEAD_SIZE * num_qheads 
    softmax = 3.0 * seq_len * seq_len * num_qheads
    softmax_q_red = 2.0 * seq_len * seq_len * HEAD_SIZE * num_qheads
    final_linear = 2.0 * seq_len * width * HEAD_SIZE * num_qheads
    attn_bwd = (
        2.0 * attention
        + 2.5 * (kq_logits + softmax + softmax_q_red)
        + 2.0 * final_linear
    ) * depth
    attention += kq_logits + softmax + softmax_q_red + final_linear

    ffw_size = EXPAND_FACTOR * width
    dense_block = (
        6.0 * seq_len * width * ffw_size
    )  # three matmuls instead of usual two because of GEGLU
    dense_block += (
        10 * seq_len * ffw_size
    )  # 7 for other ops: 3 for cubic, two additions, two scalar mults
    dense_block += 2.0 * width * seq_len  # both/sandwich residual additions
    rmsnorm = 2 * 7.0 * width * seq_len 

    final_rms_norm = 7.0 * width * seq_len  # one last RMSNorm
    final_logits = 2.0 * seq_len * width * vocab_size
    nonattn_bwd = 2.0 * (
        embeddings + depth * (dense_block + rmsnorm) + final_rms_norm + final_logits
    )
    forward_pass = (
        embeddings
        + depth * (attention + dense_block + rmsnorm)
        + final_rms_norm
        + final_logits
    )
    backward_pass = attn_bwd + nonattn_bwd  # flash attention

    return (forward_pass + backward_pass) / seq_len
\end{minted}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
