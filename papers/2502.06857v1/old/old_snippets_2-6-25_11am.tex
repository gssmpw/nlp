
% Classical scaling laws focus on computing the optimal total number of parameters for a given compute budget.
% However, these laws do not take into consideration the numerous possible ways that the total number of parameters can be used in the transformer architecture.
% We propose new scaling laws that are parameterized by the hidden dimension and number of layers in the decoder architecture.
% We train an extensive suite of models with varying ratios of hidden dimension to number of layers, fitting scaling laws which can prescribe the optimal ratio to achieve minimal loss.
% We compare and contrast with data collected from prior work on scaling laws and with current popular architectures.

% Lots of models
% width-depth scaling attempt
% Rainbow scaling plot




%Our findings are summarized as follows: (1) 
%We open source an architecturally diverse suite of 22 language models, with ablations over cooldown and learning rate;
% (2) wider models often outperform deeper ones when considering training time efficiency, challenging the prevailing focus on FLOPs-optimal designs (see \Cref{fig:flops_gpu_hrs}); (3) we highlight the negative impact of freezing design choices on scaling laws by showing that under identical training and fitting conditions, minor tweaks to the sampling of fitting points and learning rate schedule can significantly change the scaling law (see \Cref{fig:rainbow-plot}).


% \iffalse Scaling laws for large language models are a keystone of model development, but existing best practices often overlook critical nuances in training dynamics and architecture design. 
% Prior work typically trains models on fewer than 100 billion tokens, provides prescriptions that are optimal with respect to FLOPs, and freezes a host of critical design choices from model shape to learning rate schedule. 
% In this work, we demonstrate that these practices lead to inaccurate scaling laws and introduce remediations for these overlooked pitfalls.
% We release the most comprehensive scaling law dataset to date, consisting of over 4000 model checkpoints spanning a wide range of learning rates, cooldown schedules, and architectural shapes. 
% Through extensive experimentation, we train and open-source a suite of 22 text transformer models ranging from 50 million to 2 billion parameters, varying the ratios of hidden dimension to the number of layers, and training each on 350 billion tokens. 
% Our findings reveal that training on \(>100\) billion tokens is essential to reach the stable regime necessary for reliable scaling laws, as the slope of the scaling law changes significantly with the minimum number of tokens trained on (see \Cref{fig:not-enough-tokens}). 
% We show that wider models often outperform deeper ones when considering training time efficiency, challenging the prevailing focus on FLOPs-optimal designs. 
% We highlight the negative impact of freezing design choices on scaling laws by showing that under identical training and fitting conditions, minor tweaks to the sampling of fitting points and learning rate schedule can significantly change the scaling law (see \Cref{fig:rainbow-plot}).
% \fi
% By fitting scaling laws to this diverse suite of models, we prescribe optimal width-to-depth ratios for minimizing loss and compare our findings against current popular architectures. Our results uncover previously overlooked pitfalls in common methods for fitting scaling laws, offering practical insights for future model development.
% text transformer architecture = as has been looked at for vit

% - We propose remediations for pitfalls overlooked by prior work
% - We release the most comprehensive scaling law dataset to date, consisting of over \(4000\) model checkpoints spanning {} (mention LR, cooldown, shape)
% - 

% Classical scaling laws do not take into consideration the numerous possible ways that the total number of parameters can be used in the text transformer architecture.
% We train and open source an extensive suite of \(22\) models from \(50\) million to \(2\) billion parameters with varying ratios of hidden dimension to number of layers for \(350\) billion tokens.
% Fitting scaling laws to this suite of models, we can prescribe the optimal ratio of width to depth to achieve minimal loss.
% We compare and contrast with current popular architectures, discovering overlooked pitfalls in common methods for fitting scaling laws.\\
% \textbf{Code: } \\
% \textbf{Hugging Face Collection: } \\
% \textbf{W\&B Report: } \\


% \mg{add references to cover figures, add a sprinkling of citations}
% \tom{A simple narrative for the intro is:  The results predicted by scaling lows depends on both the design of the architecture and design of the scaling law.  This paper will look at the impact of both.}

%The architecture of choice for many AI applications has converged on the Transformer because its performance scales well with parameters and data \citep{hoffmann2022empirical}, but also because of its ability to handle long sequences and virtually any data modality. 
%Hugging Face \citep{wolf2020huggingfaces} hosts more than a million models available for download, and most of these models are Transformers.
%While on the surface it may appear that all practitioners are training the same models but on different data, a closer look reveals that even Transformers of similar size differ greatly in their architectural specifications, and large language models (LLMs) vary greatly in their training protocols including duration and hyperparameters.

%Several influential papers \citep{kaplan2020scaling, hoffmann2022empirical} fit scaling laws that predict the final loss of a model as a function of its parameter count and the number of training tokens. 
%These scaling laws are increasingly a keystone of model development since they allow us to make predictions about large models from smaller scale experiments.


%In this work, we hypothesize that optimal design choices vary across parameter counts and dataset sizes and that design choices strongly influence the prescriptions of compute-optimal scaling laws.
%
%We test these hypotheses by generating a vast number of LLM checkpoints at a variety of width-depth ratios, trained for a wide range of token counts, and with an array of learning rates and cooldown schedules.

% \citet{chen2024scaling} first map compute to loss with a power law relationship and then take this loss to estimate the performance of the model on benchmarks using a linear relationship. {These ^ also use intermediate checkpoints but don't say what scheduler they use}


%Scaling laws are broadly applied to many areas of Machine learning, from machine translation \citep{ghorbani2021scaling, gordon2021data, zhang2022examining, bansal2022data} to sparse architectures \citep{clark2022unified, frantar2023scaling, yun2024toward,krajewski2024scaling}.
%Further analyses using scaling laws have extended to analyzing finetuning \citep{hernandez2021scaling, zhang2024scaling} and data-limited scaling \citep{muennighoff2023scaling}.



% We similarly observe that having too many (equally-weighted) points throws off the law, when we constructed a loss-based law from the \citet{porian2024resolving} data. \sean{I like this point but don't know where it goes, maybe somewhere in experiments/discussion}

% \tom{This section can be short - it's not super important for the paper other than a few cites.}
% \todo{Maximal Update Parameterization (\(\mu\)P) \citep{yang2021zero,everett2024scaling} hyperparameters can be zero-shot transferred over width/depth changes. \(\mu\)-transfer for ResNets trained on Tiny-ImageNet \citep{bordelon2024depthwise}. Transformers on OpenWebText \citep{cerebras2024mupguide} theory on MLPs: \citep{yaida2022meta,kalra2023universal}. Paper on hyperparam transfer and shape \citet{hayou2023width}.}

% assumptions
% during search
%  n_head = n_embd / 128
% assert n_head % 1 == 0, "num heads must be an integer"
% n_head = int(n_head)
% assert n_head % 2 == 0, "num heads must be div by 2"
% assert n_head >= 2, "num heads must be at least 2"

% n_query_groups = n_head / 2
% assert n_query_groups % 1 == 0, "num query groups must be an integer"
% assert n_query_groups >= 1, "num query groups must be more than 0"

% func_param_count = param_counter(
%                 width=n_embd, # swept
%                 depth=n_layer, # swept
%                 vocab_size=vocab_size, # fixed, pythia
%                 n_head=n_head, # n_embd / head_size
%                 head_size=128, # fixed, max size on ROCm
%                 n_query_groups=int(n_head/2), # GQA at 2:1
%                 intermediate_size=4*n_embd, # expansion factor of 4
%             )

% as Instantiation values
% https://github.com/tomg-group-umd/lit-gpt-dev/blob/scaling-laws-2/litgpt/config.py#L1316


% All models have a head size of \(128\) as this is the maximum \tom{Did you mean minimum? \jwk{no, but needs a fix}} head dim supported by the AMD implementation of Flash Attention 2 we used. 
% https://github.com/ROCm/flash-attention/blob/main/flash_attn/flash_attn_interface.py#L25


% \jwk{To the cutter, this entire ``Training Hyperparameters'' section covering init and LR is self contained. Move the uninteresting parts or entirety to \Cref{subsec:app-training-details-hyperparams}. Will yield a full page of space.}

%Due to the wide range of model widths and depths in our suite, in addition to initialization rules, we have to modulate the \textit{effective} learning rate during parameter updates which is a function of the \textit{width} of the individual trainable parameters as well as how many composed weight updates occur during backpropagation (\textit{depth)}.


% One approach to fitting a scaling law is to plot the curve of training FLOPs vs loss for a range of architectures, and superimpose these curves on the same chart. 
% The lower envelope of these curves (the pareto-optimal architecture for each FLOP count) then defines the law (see \Cref{fig:approach-1}).
%The final loss value for each training run is then used to estimate the lowest possible loss at that FLOP budget, this is called approach 1 by \citet{hoffmann2022empirical}.
% This is called approach 1 by \citet{hoffmann2022empirical}.

% we try \(5\) initializations for every exponent, \(6\) initializations for every coefficient and \(5\) initializations for the irreducible error in the formula we are fitting.
% In \Cref{app-subsec:approach-3-delta}, we ablate some approach 3 prescriptions using \(\delta = 10^{-3}\), finding little to no difference.

%When comparing to a baseline which is only parameterized by number of parameters in the model and number of training tokens, we fit laws of the form shown in \Cref{eq:params}.

% Where \(p\) is the number of parameters in the model and perform the same fitting procedure as we do for laws parameterized by the hidden dimension of the model, the number of layers in the model and the number of training tokens.
% \[\min_{A,\alpha,B,\beta,\varepsilon} \sum_{\text{run }i}Huber_\delta\left[\log\left(\frac{A}{p_i^{\alpha}}+\frac{B}{T_i^{\beta}}+\varepsilon\right)-\log(loss_i)\right]\]

% We also independently collect validation data from several prior works which release widely used models \citep{bi2024deepseek, hu2024minicpm, touvron2023llama1, touvron2023llama2}.
% We collect the model dimensions from the tables presented in these works and extract the loss from plots using an online plot digitizer tool.
% Unfortunately, we cannot perform this data collection method for many other papers \citep{dubey2024llama, jiang2023mistral, team2024gemma1, team2024gemma2} as loss curves and values are not released.
% However, we can still analyze the predicted final loss of our scaling laws on these models. 
% The extracted data is presented in Appendix Section \ref{subsec:app-valdata}.


% In this section, we show that our convex hull fitting method can recover a scaling law using approach 1, and we observe scaling laws that predict the impact of width and depth \Cref{subsec:experiments-approach-3}.
% We finish this section with an analysis of how model shapes differ depending on whether FLOPs or training time are optimized \Cref{subsec:flops_vs_time}.

% Before we begin, we investigate the impact of model sampling and design choices on scaling laws.
% We see that scaling laws can be quite fragile and dependent on sampling methods, extending on this observation in prior work.
% For this reason, it is important to be aware of the sources of error that are incurred through the sampling and model fitting process.


% \tom{The front matter of this section doesn't clearly state the purpose of the section, or what the takeaways are.  This subsection is pretty messy and disorganized right now!}


%This chart uses the rule FLOPs/token = \(6 \times parameters\). We see in \Cref{fig:approach-3-wd-full} that the innacuracies of his rule have little impact on the shape of the final law when width/depth are excluded.


%From this analysis ``rainbow'' we repeatedly see that scaling laws are highly contextual, rules of thumb are easily broken without being intentionally adversarial and constraining compute can easily lead to misfit scaling laws.

% \subsubsection{The Leprechaun at the End of the Rainbow}
% We now move to possible ways to check that a scaling law is being fit in its limiting regime.
% In \Cref{fig:not-enough-tokens}, we plot the gradient of the law as we remove the smallest samples of that quantity from the scaling law.
% For example, in the tokens analysis, we remove all token counts less than or equal to the smallest token count from the dataset, fit a law then plot the slope of the law; we then repeat this for the next smallest token count until we have exhausted all token counts.
% \sean{@AP describe bucketing please?}

% % \citet{choshen2024hitchhiker} and \citet{chen2024sudden} -> removing tokens from beginning of training
% We see that the tokens lines in \Cref{fig:not-enough-tokens} for both approach 1 and 3 increase in gradient as smaller token counts are removed. 
% This supports and extends the findings of 
% \citet{choshen2024hitchhiker}, who suggest excluding the beginning of training from the fit.
% However, we show this has a much larger impact than previously known and much more of the initial training data should be discarded for valid scaling laws.


% We now have a prescription for the optimal width and depth for any amount of total training FLOPs. But how important is it to follow our prescriptions? How suboptimal would it be to choose a width and depth not on the line, as most industry models do? 

% In \Cref{fig:flops_gpu_hrs}, we plot the interplay between being FLOPs and GPU hours optimal at \(300\) billion tokens.
% \todo{what are the takeaways?}

% One thing we do highlight in \Cref{fig:flops_gpu_hrs} is that in the bottom row, if we were to separate the depth axis into bins for GPU hours (bottom, middle) the widest models are often loss optimal, highlighted by stars.
% This suggests that wider models would improve this analysis and suggest future work to clarify where the limiting width regime lies. \sean{Idk how to words this without shooting us in the foot}

% In \Cref{fig:approach-1}, we use the recorded time per step for each model and transform this into an axis reflecting the GPU hours taken to train our models.
% We highlight our tensor parallel only training is not unusual within the community \sean{CITE, maybe olmo 2}. \sean{Idk what to argue about AMD training? No one does that..}
% Although analyses based on our time per step recordings are highly fitted to the training strategies and topologies we deploy, we emphasize that as we open source all artifacts, future work can transfer this analysis to their setup.
% This can easily be done by measuring the time for one step for each model shape and transferring this time to our dataframes and replotting any figure shown.

% In \Cref{fig:overtrain} (right), we show horizontal slices from \Cref{fig:overtrain} at four FLOP budgets which we include in the title of each subplot.
% Here we see, as the overtrain factor increases, loss decreases to the minimum prescribed loss value when the overtraining factor is equal to one, which corresponds to being optimally trained according to the law before increasing again, forming a parabola.
% We include the loss values of open source models on  our validation set to allow readers to contextualize the y-axis values.

% \tom{Mention the overtraining plots in the appendix, training more => can have smaller model = time better, even when spending more FLOPs}

%\david{Proposal: replace this section with Conclusion to save sentences, if we don't add a limitations subsection. Right now, it's mostly summary, and IMO doesn't help the reader synthesize anything.}

% \section{Discussion}~\label{sec:discussion}
% To summarize, we see that scaling laws are very brittle (\Cref{fig:rainbow-plot}).
% We train a comprehensive set of \(22\) models with ablations using \(\mu P\) to optimize applicability to the community (\Cref{subsec:training-details}).
% We then propose a new method for fitting approach 1 style laws, using a convex hull (\Cref{fig:approach-1}) to remove points to reduce sampling bias in the fitting.
% Using the rich range of widths and depths included in our suite of trained models, we fit approach 3 laws with additional terms to allow us to optimize the width and depth of the prescribed models in a principled manner (\Cref{fig:approach-3-wd}).
% Finally, we analyze the trade off between FLOP and time optimality, finding wider models are very often time-optimal to train within our training set up and this analysis can easily be translated to any training set up (\Cref{fig:flops_gpu_hrs}).

% We hope this work encourages a rich range of future work on the impact of width and depth within modern transformer architectures using the large amount of open source artifacts we produce.
% This could also be extended to other hyperparameters used when training transformer architectures, such as expansion factor.
% Follow up work should also seek more quantitative signals for when a quantity being fit in a scaling law is in the limiting regime.






