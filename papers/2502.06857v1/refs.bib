@article{porian2024resolving,
  title={Resolving Discrepancies in Compute-Optimal Scaling of Language Models},
  author={Porian, Tomer and Wortsman, Mitchell and Jitsev, Jenia and Schmidt, Ludwig and Carmon, Yair},
  journal={arXiv preprint arXiv:2406.19146},
  year={2024}
}
@misc{pearce2024reconciling,
      title={Reconciling Kaplan and Chinchilla Scaling Laws}, 
      author={Tim Pearce and Jinyeop Song},
      year={2024},
      eprint={2406.12907},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.12907}, 
}
@article{hoffmann2022empirical,
  title={An empirical analysis of compute-optimal large language model training},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30016--30030},
  year={2022}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@misc{observational,
      title={Observational Scaling Laws and the Predictability of Language Model Performance}, 
      author={Yangjun Ruan and Chris J. Maddison and Tatsunori Hashimoto},
      year={2024},
      eprint={2405.10938},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.10938}, 
}

@misc{memmoves,
	title = {Time Matters: Scaling Laws for Any Budget},
	url = {http://arxiv.org/abs/2406.18922},
	doi = {10.48550/arXiv.2406.18922},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Inbar, Itay and Sernau, Luke},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18922 [cs]},
}
@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}
@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}
@article{touvron2023llama1,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@article{team2024gemma1,
  title={Gemma: Open models based on Gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}
@article{team2024gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}
@article{liu1989limited,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C. and Nocedal, Jorge},
  journal={Mathematical Programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer},
  doi={10.1007/BF01589116}
}

@inproceedings{interplay,
 author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {22640--22651},
 publisher = {Curran Associates, Inc.},
 title = {The Depth-Width Interplay in Self-Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf},
 volume = {33},
 year = {2020},
  eprint={2006.12467},
  archivePrefix={arXiv},
}

@inproceedings{
scaleEfficiently,
title={Scale Efficiently: Insights from Pretraining and Finetuning Transformers},
author={Yi Tay and Mostafa Dehghani and Jinfeng Rao and William Fedus and Samira Abnar and Hyung Won Chung and Sharan Narang and Dani Yogatama and Ashish Vaswani and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=f2OYVDyfIB}
}


@misc{nguyen2021wide,
      title={Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth}, 
      author={Thao Nguyen and Maithra Raghu and Simon Kornblith},
      year={2021},
      eprint={2010.15327},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.15327}, 
}

@misc{petty2024impact,
      title={The Impact of Depth on Compositional Generalization in Transformer Language Models}, 
      author={Jackson Petty and Sjoerd van Steenkiste and Ishita Dasgupta and Fei Sha and Dan Garrette and Tal Linzen},
      year={2024},
      eprint={2310.19956},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.19956}, 
}


@InProceedings{everett2024scaling,
  title = 	 {Scaling Exponents Across Parameterizations and Optimizers},
  author =       {Everett, Katie E and Xiao, Lechao and Wortsman, Mitchell and Alemi, Alexander A and Novak, Roman and Liu, Peter J and Gur, Izzeddin and Sohl-Dickstein, Jascha and Kaelbling, Leslie Pack and Lee, Jaehoon and Pennington, Jeffrey},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {12666--12700},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/everett24a/everett24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/everett24a.html},
}

@article{bansal2022endtoend,
  title={End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking}, 
  author={Bansal, Aprit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}
@article{schwarzschild2021can,
  title={Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@inproceedings{gordon2021data,
  title={Data and parameter scaling laws for neural machine translation},
  author={Gordon, Mitchell A and Duh, Kevin and Kaplan, Jared},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5915--5922},
  year={2021}
}
@article{ghorbani2021scaling,
  title={Scaling laws for neural machine translation},
  author={Ghorbani, Behrooz and Firat, Orhan and Freitag, Markus and Bapna, Ankur and Krikun, Maxim and Garcia, Xavier and Chelba, Ciprian and Cherry, Colin},
  journal={arXiv preprint arXiv:2109.07740},
  year={2021}
}
@inproceedings{bansal2022data,
  title={Data scaling laws in NMT: The effect of noise and architecture},
  author={Bansal, Yamini and Ghorbani, Behrooz and Garg, Ankush and Zhang, Biao and Cherry, Colin and Neyshabur, Behnam and Firat, Orhan},
  booktitle={International Conference on Machine Learning},
  pages={1466--1482},
  year={2022},
  organization={PMLR}
}
@inproceedings{zhang2022examining,
  title={Examining scaling and transfer of language model architectures for machine translation},
  author={Zhang, Biao and Ghorbani, Behrooz and Bapna, Ankur and Cheng, Yong and Garcia, Xavier and Shen, Jonathan and Firat, Orhan},
  booktitle={International Conference on Machine Learning},
  pages={26176--26192},
  year={2022},
  organization={PMLR}
}
@inproceedings{aghajanyan2023scaling,
  title={Scaling laws for generative mixed-modal language models},
  author={Aghajanyan, Armen and Yu, Lili and Conneau, Alexis and Hsu, Wei-Ning and Hambardzumyan, Karen and Zhang, Susan and Roller, Stephen and Goyal, Naman and Levy, Omer and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={265--279},
  year={2023},
  organization={PMLR}
}
@article{frantar2023scaling,
  title={Scaling laws for sparsely-connected foundation models},
  author={Frantar, Elias and Riquelme, Carlos and Houlsby, Neil and Alistarh, Dan and Evci, Utku},
  journal={arXiv preprint arXiv:2309.08520},
  year={2023}
}
@article{hernandez2021scaling,
  title={Scaling laws for transfer},
  author={Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  journal={arXiv preprint arXiv:2102.01293},
  year={2021}
}
@article{zhang2024scaling,
    title={When scaling meets llm finetuning: The effect of data, model and finetuning method},
    author={Zhang, Biao and Liu, Zhongtao and Cherry, Colin and Firat, Orhan},
    journal={arXiv preprint arXiv:2402.17193},
    year={2024}
}
@inproceedings{yang2021zero,
    author = {Yang, Ge and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
    pages = {17084--17097},
    publisher = {Curran Associates, Inc.},
    title = {Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
    url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/8df7c2e3c3c3be098ef7b382bd2c37ba-Paper.pdf},
    volume = {34},
    year = {2021}
}
@inproceedings{bordelon2024depthwise,
    title={Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit},
    author={Blake Bordelon and Lorenzo Noci and Mufan Bill Li and Boris Hanin and Cengiz Pehlevan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=KZJehvRKGD}
}

@misc{yaida2022meta,
    title={Meta-Principled Family of Hyperparameter Scaling Strategies}, 
    author={Sho Yaida},
    year={2022},
    eprint={2210.04909},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2210.04909}, 
}

@misc{cerebras2024mupguide,
    author = {Dey, Nolan and Anthony, Quentin and Hestness, Joel},
    title = {The practitioner’s guide to the maximal update parameterization},
    month = sep,
    year = {2024},
    url = {https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization},
}

@misc{kalra2023universal,
    title={Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos}, 
    author={Dayal Singh Kalra and Tianyu He and Maissam Barkeshli},
    year={2023},
    eprint={2311.02076},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{liang2024scaling,
  title={Scaling Laws For Diffusion Transformers},
  author={Liang, Zhengyang and He, Hao and Yang, Ceyuan and Dai, Bo},
  journal={arXiv preprint arXiv:2410.08184},
  year={2024}
}
@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}
@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}
@article{alabdulmohsin2022revisiting,
  title={Revisiting neural scaling laws in language and vision},
  author={Alabdulmohsin, Ibrahim M and Neyshabur, Behnam and Zhai, Xiaohua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22300--22312},
  year={2022}
}
@article{li2024bigger,
  title={Are Bigger Encoders Always Better in Vision Large Models?},
  author={Li, Bozhou and Liang, Hao and Meng, Zimo and Zhang, Wentao},
  journal={arXiv preprint arXiv:2408.00620},
  year={2024}
}
@inproceedings{clark2022unified,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International conference on machine learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}
@article{yun2024toward,
  title={Toward Inference-optimal Mixture-of-Expert Large Language Models},
  author={Yun, Longfei and Zhuang, Yonghao and Fu, Yao and Xing, Eric P and Zhang, Hao},
  journal={arXiv preprint arXiv:2404.02852},
  year={2024}
}
@article{krajewski2024scaling,
  title={Scaling laws for fine-grained mixture of experts},
  author={Krajewski, Jakub and Ludziejewski, Jan and Adamczewski, Kamil and Pi{\'o}ro, Maciej and Krutul, Micha{\l} and Antoniak, Szymon and Ciebiera, Kamil and Kr{\'o}l, Krystian and Odrzyg{\'o}{\'z}d{\'z}, Tomasz and Sankowski, Piotr and others},
  journal={arXiv preprint arXiv:2402.07871},
  year={2024}
}
@article{isik2024scaling,
  title={Scaling laws for downstream task performance of large language models},
  author={Isik, Berivan and Ponomareva, Natalia and Hazimeh, Hussein and Paparas, Dimitris and Vassilvitskii, Sergei and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2402.04177},
  year={2024}
}
@misc{chen2024scaling,
      title={Scaling Laws for Predicting Downstream Performance in LLMs}, 
      author={Yangyi Chen and Binxuan Huang and Yifan Gao and Zhengyang Wang and Jingfeng Yang and Heng Ji},
      year={2024},
      eprint={2410.08527},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.08527}, 
}
@incollection{huber1992robust,
  title={Robust estimation of a location parameter},
  author={Huber, Peter J},
  booktitle={Breakthroughs in statistics: Methodology and distribution},
  pages={492--518},
  year={1992},
  publisher={Springer}
}
@misc{choshen2024hitchhiker,
      title={A Hitchhiker's Guide to Scaling Law Estimation}, 
      author={Leshem Choshen and Yang Zhang and Jacob Andreas},
      year={2024},
      eprint={2410.11840},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.11840}, 
}
@INPROCEEDINGS{singh2024axonn,
  author={Singh, Siddharth and Bhatele, Abhinav},
  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning}, 
  year={2022},
  volume={},
  number={},
  pages={606-616},
  doi={10.1109/IPDPS53621.2022.00065}
}
@misc{singh2024hybrid,
      title={A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs}, 
      author={Siddharth Singh and Prajwal Singhania and Aditya K. Ranjan and Zack Sating and Abhinav Bhatele},
      year={2024},
      eprint={2305.13525},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.13525}, 
}
@inproceedings{hayou2023width,
  title={Width and depth limits commute in residual networks},
  author={Hayou, Soufiane and Yang, Greg},
  booktitle={International Conference on Machine Learning},
  pages={12700--12723},
  year={2023},
  organization={PMLR}
}
@inproceedings{chen2024sudden,
    title={Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in {MLM}s},
    author={Angelica Chen and Ravid Shwartz-Ziv and Kyunghyun Cho and Matthew L Leavitt and Naomi Saphra},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=MO5PiKHELW}
}
@inproceedings{caballero2023broken,
    title={Broken Neural Scaling Laws},
    author={Ethan Caballero and Kshitij Gupta and Irina Rish and David Krueger},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=sckjveqlCZ}
}
@article{alabdulmohsin2024getting,
  title={Getting vit in shape: Scaling laws for compute-optimal model design},
  author={Alabdulmohsin, Ibrahim M and Zhai, Xiaohua and Kolesnikov, Alexander and Beyer, Lucas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{muennighoff2023scaling,
  title={Scaling data-constrained language models},
  author={Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Le Scao, Teven and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={50358--50376},
  year={2023}
}
@article{hagele2024scaling,
  title={Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations},
  author={H{\"a}gele, Alexander and Bakouch, Elie and Kosson, Atli and Allal, Loubna Ben and Von Werra, Leandro and Jaggi, Martin},
  journal={arXiv preprint arXiv:2405.18392},
  year={2024}
}
@article{brown2022wide,
  title={Wide attention is the way forward for transformers?},
  author={Brown, Jason Ross and Zhao, Yiren and Shumailov, Ilia and Mullins, Robert D},
  journal={arXiv preprint arXiv:2210.00640},
  year={2022}
}
@article{beyer2024paligemma,
  title={Paligemma: A versatile 3b vlm for transfer},
  author={Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and others},
  journal={arXiv preprint arXiv:2407.07726},
  year={2024}
}
@article{dolma,
  title = {{Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}},
  author={
    Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and
    Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and
    Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and
    Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and
    Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and
    Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and
    Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo
  },
  year = {2024},
  journal={arXiv preprint},
}
@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}
@misc{litgpt-2023,
  author       = {Lightning AI},
  title        = {LitGPT},
  howpublished = {\url{https://github.com/Lightning-AI/litgpt}},
  year         = {2023},
}
@article{besiroglu2024chinchilla,
  title={Chinchilla Scaling: A replication attempt},
  author={Besiroglu, Tamay and Erdil, Ege and Barnett, Matthew and You, Josh},
  journal={arXiv preprint arXiv:2404.10102},
  year={2024}
}
@inproceedings{
    misfitting,
    title={(Mis)Fitting Scaling Laws: A Survey of Scaling Law Fitting Techniques in Deep Learning},
    author={Li, Margaret and Kudugunta ,Sneha and Zettlemoyer, Luke},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=xI71dsS3o4},
    note={https://iclr.cc/virtual/2025/poster/27795}
}
@misc{wolf2020huggingfaces,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.03771}, 
}
@article{yang2024qwen25,
  title={Qwen2.5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}
@misc{yang2024qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}
@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}
@article{Groeneveld2023OLMo,
  title={OLMo: Accelerating the Science of Language Models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  journal={Preprint},
  year={2024}
}
@article{olmo20242,
  title={2 OLMo 2 Furious},
  author={OLMo, Team and Walsh, Pete and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Arora, Shane and Bhagia, Akshita and Gu, Yuling and Huang, Shengyi and Jordan, Matt and others},
  journal={arXiv preprint arXiv:2501.00656},
  year={2024}
}

@misc{mosaic_inference,
	title = {Beyond {Chinchilla}-{Optimal}: {Accounting} for {Inference} in {Language} {Model} {Scaling} {Laws}},
	shorttitle = {Beyond {Chinchilla}-{Optimal}},
	url = {http://arxiv.org/abs/2401.00448},
	doi = {10.48550/arXiv.2401.00448},
	abstract = {Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand ({\textasciitilde}1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Sardana, Nikhil and Portes, Jacob and Doubov, Sasha and Frankle, Jonathan},
	month = jul,
	year = {2024},
	note = {arXiv:2401.00448 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 16 pages, 7 figures, To appear in the 41st International Conference on Machine Learning, 2024},
	file = {Preprint PDF:C\:\\Users\\Dave\\Zotero\\storage\\QN3GJ4GE\\Sardana et al. - 2024 - Beyond Chinchilla-Optimal Accounting for Inference in Language Model Scaling Laws.pdf:application/pdf;Snapshot:C\:\\Users\\Dave\\Zotero\\storage\\33JBDUS9\\2401.html:text/html},
}

@misc{wu2024inferencescalinglawsempirical,
      title={Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models}, 
      author={Yangzhen Wu and Zhiqing Sun and Shanda Li and Sean Welleck and Yiming Yang},
      year={2024},
      eprint={2408.00724},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.00724}, 
}