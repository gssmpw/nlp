@article{alabdulmohsin2024getting,
  title={Getting vit in shape: Scaling laws for compute-optimal model design},
  author={Alabdulmohsin, Ibrahim M and Zhai, Xiaohua and Kolesnikov, Alexander and Beyer, Lucas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{beyer2024paligemma,
  title={Paligemma: A versatile 3b vlm for transfer},
  author={Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and others},
  journal={arXiv preprint arXiv:2407.07726},
  year={2024}
}

@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}

@inproceedings{bordelon2024depthwise,
    title={Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit},
    author={Blake Bordelon and Lorenzo Noci and Mufan Bill Li and Boris Hanin and Cengiz Pehlevan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=KZJehvRKGD}
}

@article{brown2022wide,
  title={Wide attention is the way forward for transformers?},
  author={Brown, Jason Ross and Zhao, Yiren and Shumailov, Ilia and Mullins, Robert D},
  journal={arXiv preprint arXiv:2210.00640},
  year={2022}
}

@inproceedings{caballero2023broken,
    title={Broken Neural Scaling Laws},
    author={Ethan Caballero and Kshitij Gupta and Irina Rish and David Krueger},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=sckjveqlCZ}
}

@misc{cerebras2024mupguide,
    author = {Dey, Nolan and Anthony, Quentin and Hestness, Joel},
    title = {The practitionerâ€™s guide to the maximal update parameterization},
    month = sep,
    year = {2024},
    url = {https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization},
}

@misc{choshen2024hitchhiker,
      title={A Hitchhiker's Guide to Scaling Law Estimation}, 
      author={Leshem Choshen and Yang Zhang and Jacob Andreas},
      year={2024},
      eprint={2410.11840},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.11840}, 
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@InProceedings{everett2024scaling,
  title = 	 {Scaling Exponents Across Parameterizations and Optimizers},
  author =       {Everett, Katie E and Xiao, Lechao and Wortsman, Mitchell and Alemi, Alexander A and Novak, Roman and Liu, Peter J and Gur, Izzeddin and Sohl-Dickstein, Jascha and Kaelbling, Leslie Pack and Lee, Jaehoon and Pennington, Jeffrey},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {12666--12700},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/everett24a/everett24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/everett24a.html},
}

@article{hagele2024scaling,
  title={Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations},
  author={H{\"a}gele, Alexander and Bakouch, Elie and Kosson, Atli and Allal, Loubna Ben and Von Werra, Leandro and Jaggi, Martin},
  journal={arXiv preprint arXiv:2405.18392},
  year={2024}
}

@inproceedings{hayou2023width,
  title={Width and depth limits commute in residual networks},
  author={Hayou, Soufiane and Yang, Greg},
  booktitle={International Conference on Machine Learning},
  pages={12700--12723},
  year={2023},
  organization={PMLR}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{hoffmann2022empirical,
  title={An empirical analysis of compute-optimal large language model training},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30016--30030},
  year={2022}
}

@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@inproceedings{interplay,
 author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {22640--22651},
 publisher = {Curran Associates, Inc.},
 title = {The Depth-Width Interplay in Self-Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf},
 volume = {33},
 year = {2020},
  eprint={2006.12467},
  archivePrefix={arXiv},
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{krajewski2024scaling,
  title={Scaling laws for fine-grained mixture of experts},
  author={Krajewski, Jakub and Ludziejewski, Jan and Adamczewski, Kamil and Pi{\'o}ro, Maciej and Krutul, Micha{\l} and Antoniak, Szymon and Ciebiera, Kamil and Kr{\'o}l, Krystian and Odrzyg{\'o}{\'z}d{\'z}, Tomasz and Sankowski, Piotr and others},
  journal={arXiv preprint arXiv:2402.07871},
  year={2024}
}

@misc{memmoves,
	title = {Time Matters: Scaling Laws for Any Budget},
	url = {http://arxiv.org/abs/2406.18922},
	doi = {10.48550/arXiv.2406.18922},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Inbar, Itay and Sernau, Luke},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18922 [cs]},
}

@misc{observational,
      title={Observational Scaling Laws and the Predictability of Language Model Performance}, 
      author={Yangjun Ruan and Chris J. Maddison and Tatsunori Hashimoto},
      year={2024},
      eprint={2405.10938},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.10938}, 
}

@misc{pearce2024reconciling,
      title={Reconciling Kaplan and Chinchilla Scaling Laws}, 
      author={Tim Pearce and Jinyeop Song},
      year={2024},
      eprint={2406.12907},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.12907}, 
}

@misc{petty2024impact,
      title={The Impact of Depth on Compositional Generalization in Transformer Language Models}, 
      author={Jackson Petty and Sjoerd van Steenkiste and Ishita Dasgupta and Fei Sha and Dan Garrette and Tal Linzen},
      year={2024},
      eprint={2310.19956},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.19956}, 
}

@article{porian2024resolving,
  title={Resolving Discrepancies in Compute-Optimal Scaling of Language Models},
  author={Porian, Tomer and Wortsman, Mitchell and Jitsev, Jenia and Schmidt, Ludwig and Carmon, Yair},
  journal={arXiv preprint arXiv:2406.19146},
  year={2024}
}

@inproceedings{yang2021zero,
    author = {Yang, Ge and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
    pages = {17084--17097},
    publisher = {Curran Associates, Inc.},
    title = {Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
    url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/8df7c2e3c3c3be098ef7b382bd2c37ba-Paper.pdf},
    volume = {34},
    year = {2021}
}

