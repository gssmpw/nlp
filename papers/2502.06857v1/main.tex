\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% \usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
% \usepackage{minted}
\usepackage[frozencache,cachedir=.]{minted}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{enumitem}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted, nohyperref]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\definecolor{cornflowerblue}{rgb}{0.39, 0.58, 0.93}
\hypersetup{
    colorlinks=true,
    linkcolor=cornflowerblue,
    filecolor=magenta,      
    urlcolor=teal,
    citecolor=cornflowerblue,
    pdftitle={Gemstones: A Model Suite for Multi-Faceted Scaling Laws},
    pdfpagemode=FullScreen,
    }

% \newcommand{\tom}[1]{{\color{red}[Tom: #1]}}
% \newcommand{\ap}[1]{{\color{purple}[Ashwinee: #1]}}
% \newcommand{\david}[1]{{\color{blue}[David: #1]}}
% \newcommand{\sean}[1]{{\textcolor[rgb]{0.505, 0, 0.921}{Sean: \footnotesize\sf[#1]}}}
% \newcommand{\jwk}[1]{{\textcolor{ForestGreen}{John: \footnotesize\sf[#1]}}}
% \newcommand{\mg}[1]{{\textcolor{teal}{Micah: \footnotesize\sf[#1]}}}
% \newcommand{\todo}[1]{{\color{orange}[TODO: #1]}}
% \newcommand{\tom}[1]{{\textcolor{red}{Tom: \footnotesize\sf[#1]}}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Gemstones: A Model Suite for Multi-Faceted Scaling Laws}

\begin{document}

\twocolumn[
\icmltitle{Gemstones: A Model Suite for Multi-Faceted Scaling Laws}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sean McLeish}{umd}
\icmlauthor{John Kirchenbauer}{umd}
\icmlauthor{David Yu Miller}{umd}
\icmlauthor{Siddharth Singh}{umd}
\icmlauthor{Abhinav Bhatele}{umd}
\icmlauthor{Micah Goldblum}{col}
\icmlauthor{Ashwinee Panda}{umd}
\icmlauthor{Tom Goldstein}{umd}
\end{icmlauthorlist}

\icmlaffiliation{umd}{University of Maryland}
\icmlaffiliation{col}{Columbia University}

\icmlcorrespondingauthor{Sean McLeish}{smcleish@umd.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. 
In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions.
As a primary artifact of our research, we release the \textbf{\textit{Gemstones}}: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth.
By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting.
% old sentences
%Prior work on scaling laws for large language models (LLMs) typically train models on fewer than 100 billion tokens and freeze consequential design choices including model shape and learning rate schedule. 
%In this work, we demonstrate that these common practices lead to inaccurate scaling laws, and we accordingly compute new and improved laws.
% We further modify scaling laws to account for practical considerations like GPU runtime and compute efficiency. We find that relatively wide models become optimal under these corrections, challenging the prevailing focus on FLOPs-optimal deep designs.
% Using our checkpoints, 
% We find that this sensitivity can be partially mitigated by erring on the side of overly wide and small models, as scaling laws predict that these errors maintain high efficiency.
% \textbf{Code}: {\footnotesize \url{github.com/mcleish7/gemstone-scaling-laws}}\\
\\ \textbf{Code}: {\footnotesize
\href{https://github.com/mcleish7/gemstone-scaling-laws}{github.com/mcleish7/gemstone-scaling-laws}}
\end{abstract}


\section{Introduction}\label{sec:intro}

Existing works on scaling laws often restrict Transformer architectures to a small range of width-depth ratios \citep{porian2024resolving}, train on a small number of tokens, and fix training hyperparameters such as cooldown schedule across training runs \citep{hoffmann2022empirical}.
These design choices, in turn, can dramatically influence the resulting scaling laws.
If a scaling law is sensitive to such design choices, then it may only be useful for practitioners implementing similar setups to those that produced the scaling law.
In practice, practitioners often take guidance from scaling laws that assume completely different design choices than their own implementation, often without understanding to degree to which these choices may impact optimal scaling.

% first mention of our name was linewrapped :/
\looseness -1 In this work, we produce a vast array of model checkpoints for studying how model design and model selection impact scaling laws. Our models, called the {\em Gemstones} because they are loosely based on scaled-down variants of the Gemma architecture, vary in their parameter count, width/depth ratio, training tokens, learning rates, and cooldown schedules.
By fitting scaling laws to these checkpoints, we confirm that scaling law parameters and interpretations indeed depend strongly on the selection of models and fitting procedure used, and we quantify the degree to which these decisions impact predictions.  By exploiting the variation among our model checkpoints, we also fit a number of unique scaling laws and analyze their predictions to discern whether they are consistent with design choices we see in industry models.
Our contributions are summarized as follows:

\begin{itemize}[leftmargin=15pt]
\item \textbf{We open-source more than 4000 checkpoints cumulatively trained on over 10 trillion tokens.}  The models we provide are diverse across architectural and training hyperparameter axes, enabling more granular studies than previous work (see \Cref{fig:model_search_space}).
\item \textbf{We highlight the fragility and common pitfalls of prior scaling laws.} There are many decisions to make when choosing points to fit scaling laws that significantly change the slope of the law (see \Cref{fig:rainbow-plot}).
% For example, we find that training on more than 100 billion tokens is essential for reaching a stable regime where scaling laws are reliable, as the slope of the scaling law is sensitive to the minimum number of tokens on which the models are trained (see \Cref{fig:not-enough-tokens}).
\item \textbf{We modify standard scaling laws to account for the width and depth of models.}  %To that end, we fit a parametric power law of a similar form as in \citet{hoffmann2022empirical} but which predicts loss as a function of depth, width, parameters, and number of training tokens.  
Our scaling laws estimate not only parameter count, but also the optimal aspect ratio for any given training budget. We compare our estimates to industrial models (see \Cref{fig:approach-3-wd}).
\item \textbf{We modify scaling laws to account for practical considerations like GPU efficiency and overtraining.} When distorted to account for training and inference costs, scaling laws predict considerably wider and shallower models (see \Cref{fig:flops_gpu_hrs}).
\item \textbf{We consider what happens when scaling laws are inaccurate.} Given that scaling laws are quite sensitive to design decisions, it is natural to ask how best to cope with their imperfections.  We find that it is much better to err on the side of wider models than narrow ones. We also find that it is better to err on the side of smaller models, as overtraining is quite efficient (see \Cref{fig:overtrain}).
\end{itemize}


\iffalse




The architecture of choice for many AI applications has converged on the Transformer because its performance scales well with parameters and data \citep{hoffmann2022empirical}, but also because of its ability to handle long sequences and virtually any data modality. 
Hugging Face \citep{wolf2020huggingfaces} hosts more than a million models available for download, and most of these models are Transformers.
While on the surface it may appear that all practitioners are training the same models, a closer look at the model configurations reveals that even Transformers of similar size differ greatly in their specifications.
One particularly fundamental dimension across which the models vary is their depth-width ratio.
Even within the same organization, and the same model family, different models may have different depth-width ratios. 

A number of influential papers fit scaling laws that predict a model’s final loss as a function of its parameter count and the number of training tokens, but these papers limit the variety in depth to width ratio (see \Cref{fig:model_search_space}).
\sean{Need a new hypothesis to state here}
In this work, we hypothesize that the optimal depth-width ratio varies across parameter counts and dataset sizes.
If this hypothesis is true, then these existing scaling laws may actually mislead users to train the wrong sized model on the wrong number of tokens when the user optimizes the loss predicted by the scaling law as a function of compute budget, especially since users often employ a different depth-width ratio than was used when fitting the scaling law \mg{MIGHT WANT TO GIVE AN EXAMPLE OF SOMEONE DOING THAT}.

Adding to the confusion, many papers propose theories to explain the role of width and depth and arrive at contradictory conclusions \citep[e.g.][]{petty2024impact, scaleEfficiently}.  
However, such papers often conduct experiments at different model and data scales.
If the optimal depth-width ratio depends on these scales, then the studies’ differing experimental scales may explain why they arrive at superficially conflicting conclusions.

We test this hypothesis by training many Transformer language models of various widths and depths, and we fit a parametric power law of a similar form as found by \citet{hoffmann2022empirical} but which predicts loss as a function of depth, width, parameters and number of training tokens.

We summarize our contributions as:

% Lots of models
% width-depth scaling attempt
% Rainbow scaling plot
\textbf{We open source over 4000 checkpoints with over 10 trillion tokens trained on cumulatively.} We provide a comprehensive suite of models with a wide range of width and depths, providing ablations over cooldown and learning rate.

\textbf{We highlight common pitfalls many prior works fall into.} We find that training on \(>100\) billion tokens is essential to reach the stable regime necessary for reliable scaling laws, as the slope of the scaling law changes significantly with the minimum number of tokens trained on. 
We also find that there are many possible decisions when selecting points to fit scaling laws to that change the slope of the law also.

\textbf{We analyze the tradeoffs between width and depth.} Prior work performs preliminary analysis showing that deeper networks can be deep thinkers \citep{schwarzschild2021can, bansal2022endtoend}, capable of better reasoning, and that wider networks can retain more knowledge \mg{CITE}. While these papers have occasionally obtained contradictory conclusions, we unify previously proposed theories about the role of depth and width in Transformer architectures for LLMs by conducting experiments across parameter and token scales.
\fi


\section{Related Work}\label{sec:related_work}
\subsection{Scaling Laws}
Scaling laws address the trade-off between parameter count and number of training tokens, attempting to find the minimum loss possible for a language model with a constrained FLOP budget.
%\citet{kaplan2020scaling} fit a compute-optimal parameter count law of the form \(N_{opt} \propto C^{p_N}\). 
Unfortunately, scaling laws treat model design and training as if it has a single dimension (parameter count).  In reality, training is sensitive to many choices. 
Notably, \citet{hoffmann2022empirical} find significantly different 
% exponents in their 
fitted laws (\Cref{eq:params}) compared to \citet{kaplan2020scaling}.
\citet{pearce2024reconciling} and \citet{porian2024resolving} attribute most of this discrepancy to the choice to exclude embedding parameters from the parameter count, both showing one law can be transformed into the other via controlled changes.
\citet{kaplan2020scaling} justify including embedding parameters by showing that non-embedding parameters have a cleaner relationship with test loss. %, while using total parameters introduces a dependency on depth.
Scaling laws are also commonly included in many large model releases \citep{hu2024minicpm, bi2024deepseek, dubey2024llama}.

% This is all the work **VERY** related to ours
\citet{choshen2024hitchhiker} collect both loss and benchmark performance metrics for a multitude of models and offer a practitioner's guide to fitting scaling laws.
%Stating that the early period of training data should be dropped when fitting scaling laws, but intermediate checkpoints may be used if the learning rate schedule is appropriate.
Most notably, they suggest \(5\) models are ample to fit a scaling law, and the early period of training should be excluded from the analysis.
In contrast, \citet{misfitting} show that selecting data according to different tokens-per-parameter ratios and using small grid searches when fitting scaling laws can cause big swings in outcomes.
\citet{hagele2024scaling} suggest that a constant learning rate plus cooldown is preferable to a cosine learning rate schedule.
The authors also find that stochastic weight averaging should be encouraged in scaling law analysis as it tends to lead to better models.
Furthermore, \citet{memmoves} observe that FLOPs cannot be used to predict wall-clock time nor memory movement, and suggest that fast-training architectures may be preferred over those prescribed by scaling laws.

% Scaling laws for downstream performance {This can be cut up as we get the lm eval results in}
There are multiple works analyzing whether scaling laws can be used to predict downstream performance.
\citet{observational} show that scaling laws can be predictive of benchmark performance.
\citet{caballero2023broken} %observe that prior functional forms cannot model non-monotonic behavior nor inflection points.
%The authors 
suggest broken scaling laws that predict performance of both downstream and upstream tasks. Works in this vein are myriad; see our extended literature review in \Cref{sec:app-rel-works}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/explainer.pdf}
    \caption{\textbf{The meaning of width and depth. }We visualize a standard transformer architecture, highlighting the ``width'' as the size of the hidden dimension and the ``depth'' as the number of transformer blocks. These are the quantities we're interested in providing a prescription for with our new scaling laws.}\label{fig:summary}
\end{figure}

\subsection{The Role of Model Shape}
\citet{interplay} find that, for large models, optimal depth grows logarithmically with width. 
\citet{henighan2020scaling} find there is an optimal aspect ratio for each modality they study which gives maximum performance, for example they find \(5\) to be optimal for math models.
\citet{petty2024impact} claim small ($<$400M) transformers have diminishing benefits from depth.
\citet{brown2022wide} show that in some cases shallower models can beat their parameter equivalent deep models on tasks for encoder-decoder transformer architectures.
This differs from \citet{kaplan2020scaling} who suggest aspect ratio is not a determining factor for final loss.

Most similarly to our work, \citet{alabdulmohsin2024getting} study the impact of width and depth for encoder-decoder vision transformers, using their laws to create a smaller transformer model which has competitive downstream performance when compared with much larger models.
The architecture found in this study has since been used by \citet{beyer2024paligemma} in PaliGemma.

Model shape has also been analyzed for sparse mixture of expert models and in the context of finetuning.
\citet{krajewski2024scaling} use ``granularity'' to allow their law for mixture of expert models to predict the width of the experts. 
\citet{scaleEfficiently} show that downstream performance strongly depends on shape when finetuning but pretraining perplexity does not.

\subsection{Zero-shot Hyperparameter Transfer}\label{sec:mup-relwork}

The ability to train a series of models with extremely different parameter counts is an implicit requirement of any scaling law analysis. Work on zero-shot hyperparameter transfer across transformer model \textit{widths} is mature \citep{yang2021zero,everett2024scaling,hayou2023width,cerebras2024mupguide}, but achieving transfer across diverse model \textit{depths} is less well studied, especially in transformer language models \citep{bordelon2024depthwise}. %While solving this problem perfectly is still an active area of research, we deploy a combination of initialization rules and learning rate transformation functions derived from prior work to try and achieve the required level of hyperparameter transfer to enable us to train models of diverse shapes and sizes (\Cref{sec:learning-rates}).

\section{Designing Our Scaling Laws}\label{sec:experimental_setup}

We detail the design of our scaling laws, including model selection, the choice of learning rate, and curve fitting schemes in the subsequent sections.
% \subsection{Training}~\label{subsec:training-details}

\paragraph{Architecture.}\label{subsec:training-details}

To reduce the search space of all possible models we add some constraints, each of which are either based on precedent from a popular model series like Gemma \citep{team2024gemma1,team2024gemma2}, Llama \citep{touvron2023llama2}, Pythia \citep{biderman2023pythia}, or practical considerations such as hardware details. 

All models have a head size of \(128\) because \(256\) is the maximum head dimension supported by the AMD implementation of Flash Attention 2 we utilize and we constrain our search to models with $>1$ attention heads.
We assume the simple convention of the Llama series where the head dimension is always the embedding dimension divided by the number of heads, implying that the embedding dimension (width) must be divisible by $128$. Following conventions from the Gemma suite, we constrain the head count to be even to enable Grouped Query Attention \citep{ainslie2023gqa} with a query to key ratio of $2:1$ and we fix the intermediate size to be \(4\times \) the width of the model. We choose our vocabulary size to match the $50,304$ tokens in the Pythia tokenizer. While many of the architecture choices mirror those from Gemma, for simplicity we do not use logit softcapping nor do we tie the embedding and language modeling head weight matrices.

Within these constraints, we search the set of feasible models within target parameter count groups \(50M, 100M, 500M, 1B\) and \(2B\) with a tolerance of $\pm 5\%$. At smaller scales we train up to 5 models at diverse widths and depths. At large parameter counts we train only three models, aiming for one ``standard'' aspect ratio (similar to existing models), one ``wide'' model, and one ``deep'' model. We visualize the models we choose to train in \Cref{fig:model_search_space} overlaid with a selection of existing models from prior work. In the Appendix we plot the entire discrete set of all possible models under our constraints (\Cref{fig:model-search-space-complete}). Our \(22\) different models range from \(50M\) to \(2B\) parameters, spanning \(11\) widths from \(256\) to \(3072\) and \(18\) depths from \(3\) to \(80\). %We call our models the \textit{Gemstones} as they are based on the Gemma architecture but come in many shapes and sizes.


\paragraph{Polishing the Gemstones.}

For the main set of training runs, we train each model for \(350B\) tokens of Dolma \citep{dolma} data with a context length of \(2048\) and a world batch size of $2048$ sequences.
We use a linear learning rate warm up over \(80\) million tokens, and then train at a constant learning rate, which we adjust for model size as described in \Cref{sec:learning-rates}.

In service of future research based on our model suite, we open source checkpoints for all models at \(2\) billion token intervals, amounting to over \(4,000\) checkpoints in total. 
We also open source the fitting code and logged metrics for all runs.

We also perform ablations over both cooldown and learning rate. 
For the cooldown ablation, we take the checkpoints saved every \(10\) billion tokens for the the first \(100\) billion tokens of training and cool these down creating a second set of models which have had their learning rate annealed to \(0\) linearly.
Specifically, we cool each model down by training for a further \(10\%\) of the total tokens which it has seen during training, i.e. our cooled down set of models have training budgets ranging from \(11\) to \(110\) billion tokens.
% The details of the learning rate ablation are included in \Cref{para:mup-over-2}.

\paragraph{Training Details}

We train with AdamW \citep{loshchilov2017decoupled} with \(\beta\) parameters \(0.9\) and \(0.95\) and a weight decay of \(0.1\). We do not apply weight decay to the bias or normalization parameters. All models are trained with tensor parallelism \citep{singh2024axonn,singh2024hybrid} over multiple nodes of AMD MI250X GPUs. To the best of our knowledge, this makes the Gemstone suite of models the largest collection trained on AMD GPUs. 


% \tom{A few paragraphs above you say you checkpoint every 10B tokens.  Now you say its every 2B tokens.  Which is it? \jwk{Clearer now?}} \tom{No.  You still say you take  ``checkpoints every \(10\) billion'' and then later say you checkpoint every 2 billion.}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/width_depth_sampling/model_search_space_with_commerical_models.pdf}
    \caption{\textbf{Distribution of prior scaling law models, industry models, and our models in terms of width and depth. }
    Prior work~(purple and green) and industry models~(blue and orange) mostly lie on a fixed width-depth line.
    If we want to prescribe the optimal width-depth ratio, we need to select models with different widths and depths~(our models, black).
    % We find that prior work has selected models with a narrows range of aspect ratios. In principle this is fine so long as the scaling law is only used to extrapolate behavior for models with such a fixed aspect ratio. This motivates our selection procedure, which sweeps across a range of aspect ratios.
    }
    \label{fig:model_search_space}
\end{figure}

\subsection{Optimal Learning Rates for Gemstones}~\label{sec:learning-rates}

Training models across diverse architectures and scales requires learning rates that ensure both stability and near-optimal performance. Suboptimal learning rates risk misrepresenting scaling laws, as they could conflate architectural preferences with hyperparameter sensitivity. For the Gemstone models---varying in width, depth, and size---we address this challenge through a unified learning rate scaling rule and a parameter initialization scheme tailored for stability.

\paragraph{Unified Learning Rate Scaling Rule}  
Existing scaling rules prescribe learning rates (\(lr\)) as \(lr_{\text{base}}/\text{width}\) for width scaling or \(lr_{\text{base}}/\sqrt{\text{depth}}\) for depth scaling. Since Gemstone models vary both dimensions, we propose a hybrid rule: $lr_{\text{eff}} = lr_{\text{base}} / ({\text{width} \times \sqrt{\text{depth}}})$
% \[
% lr_{\text{eff}} = \frac{lr_{\text{base}}}{\text{width} \times \sqrt{\text{depth}}}
% \]
This accounts for the compounding effect of gradient dynamics across width and depth, balancing update magnitudes during optimization.

\paragraph{Empirical Validation}  
To validate \(lr_{\text{base}}\), we stress-test four extreme model shapes: wide (\(64\) layers, \(768\) width) and deep (\(128\) layers, \(512\) width) at \(100\)M and \(2\)B parameter scales. Each is trained for \(2\)B tokens with \(lr_{\text{eff}}\) swept from \(10^{-4}\) to \(5\times10^{-2}\). As shown in \Cref{fig:mup-four-corners} (left), optimal \(lr_{\text{eff}}\) varies widely across architectures. However, rescaling the x-axis by \(\text{width} \times \sqrt{\text{depth}}\) collapses all curves onto a shared trend, revealing \(lr_{\text{base}}=5\) as the consistent optimum (right panel). This confirms our rule's efficacy for width-depth transfer.

\paragraph{Flaws in the Gemstones.}  
While \(lr_{\text{base}}=5\) achieves stable training for most models under the scheme described above, wider architectures (e.g., \(256\) width-depth ratio) occasionally exhibit loss spikes nonetheless.
% , necessitating gradient clipping and checkpoint rollbacks. 
% We attribute this to higher parameter correlation in deep networks, amplifying gradient variance---a known challenge in ultra-deep models \citep{Dehghani2023Deep}. 
Despite these instabilities, via rollbacks and minor modifications to the learning rates for the most extreme models, all models in the suite are trained to \(350\)B tokens without divergence.
We discuss these issues and our solutions further in \Cref{subsec:app-training-details-hyperparams}.

\paragraph{Ablation Study}  
To assess sensitivity to \(lr_{\text{base}}\), we replicate training for a subset of models with \(lr_{\text{base}}=2.5\) (e.g. dividing \(lr_{\text{eff}}\) by \(2\)). While losses are marginally higher, scaling law fits remain robust, suggesting our conclusions are not artifacts of aggressive learning rates.

% \subsection{Optimal Learning Rates for Gemstones}~\label{sec:learning-rates}

% % As mentioned in \Cref{sec:mup-relwork}, 
% Scaling laws forecast the trend of optimal models. If we train with suboptimal configurations, then we're not predicting the behavior for optimal configurations, but rather suboptimal ones.
% If we train a wide model and a deep model and then try to fit a scaling law without optimizing the learning rate for each model, it's likely that the scaling law will just prefer the model that had a better learning rate.
% Training our diverse suite of Gemstone models requires finding learning rates that achieve both stable and \textit{near optimal} training across model sizes, shapes, and token budgets without having to redo learning rate search for every individual training run.  This is a tall order for the Gemstones, as they vary in both aspect ratio and size.


% % \paragraph{Scaling $\eta$ across model shapes and sizes.}\label{para:mup}
% % \paragraph{One $\eta$ to rule them all.}\label{para:mup-lr}
% \paragraph{One $\eta$ to Rule Them All.}\label{para:mup-lr}

% % Learning rates must vary smoothly across model architectures, and maintain stability across aspect ratios and scales. 
% $\mu P$ theory (\Cref{sec:mup-relwork}) prescribes learning rate schedules that control the magnitude of gradient updates as parameters go to infinity. Literature suggests that learning rates should be scaled as $lr_{eff} = lr_{base} / width$ to achieve transfer across widths.  Independent analysis suggests the rule $lr_{eff} = lr_{base} / \sqrt{depth}$ to scale to models of different depths. We choose a hybrid learning rate scaling rule $lr_{eff} = lr_{base} / (width \times \sqrt{depth})$ since we vary both width and depth.

% We choose $lr_{base}$ by stress testing our learning rate rule on four extremal model shapes; one wide and one deep, at both the $100$M scale and $2$B scale. We trained each for a short time with a sweep of learning rates $lr_{eff}$ ranging from \(10^{-4}\) to \(5\times 10^{-2}\). Each model achieves its lowest loss with a different learning rate (\Cref{fig:mup-four-corners}). We then take the data and re-scale the x-axis via the inverse of our proposed rule (eg. $lr_{base} = lr_{eff} \times (width \times \sqrt{depth})$). A visual inspections shows that all models achieve nearly optimal loss at $lr_{base}=5$, which we then adopt for all runs.

% Empirically, this scheme of minimizing loss after a short training budget produces aggressive learning rates.
% While we did get all training runs across the $350$B token finish line, some models experienced loss spikes and needed babysitting. We discuss these issues in detail in \Cref{subsec:app-training-details-hyperparams}.
% % \paragraph{Robustness of fits a redo with \(\eta / 2\).}\label{para:mup-over-2}
% Finally, we ablate the choice of optimal learning rate by training an identical set of models for \(100\) billion tokens with a learning rate of \(\eta / 2\). We will discuss the impact of this ablation on our fitting process.

% this is deepseek's version


% \paragraph{Scalable Parameter Initialization}  
% To enable consistent learning rate transfer, we adopt the initialization strategy from OLMo \citep{Groeneveld2023OLMo}. All weights are sampled from a truncated normal distribution (\(\mu=0\), \(a=-3\sigma\), \(b=3\sigma\)) with layer- and module-specific variances. Attention projections use \(\sigma = 1 / \sqrt{2 \cdot \text{width} \cdot (l + 1)}\) and MLP projections use \(\sigma = 1 / \sqrt{2 \cdot (4\times\text{width}) \cdot (l + 1)}\), where \(l\) is the layer index. This ensures gradient magnitudes remain stable across layers during early training, a prerequisite for learning rate transfer.

\paragraph{Scalable Parameter Initialization Rules.}\label{sec:mup-init}
% https://github.com/tomg-group-umd/lit-gpt-dev/blob/scaling-laws-2/litgpt/init.py#L482

Finally, stable training across model shapes and scales also requires model specific tweaks to parameter initialization~\cite{yang2021zero}.
Following OLMo(1) \citep{Groeneveld2023OLMo}, we apply a parameter initialization strategy intended to enable stable training and learning rate transfer across scales. We initialize all parameters as truncated normal ($\mu=0,a=-3\cdot\sigma, b=3\cdot\sigma$) with modified variances dependent on the parameter type. We use $\sigma=1/\sqrt{\text{width}}$ except for the attention projections which are initialized as $\sigma = 1 / \sqrt{2 \cdot \text{width} \cdot (l + 1)}$ and the MLP projections as $\sigma = 1 / \sqrt{2 \cdot (4\times \text{width}) \cdot (l + 1)}$ where in each case $l$ is the layer index (not the total model depth) and the $4\times$ factor comes from the relation of width to MLP intermediate dimension.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{figs/mup/mup.pdf}
    \caption{\textbf{Learning rate scaling is necessary for width-depth transfer. }Left: Preliminary training runs with initialization rules active, but no learning rate scaling. Right: Same data, but with x-axis rescaled to simulate the application of learning rate scaling with $lr_{\text{base}} = lr_{\text{eff}} \times (\text{width} \times \sqrt{\text{depth}})$.}
    \label{fig:mup-four-corners}
\end{figure}

\begin{figure*}[htbp]
\centering
    \includegraphics[width=0.9\textwidth]{figs/approach1/figure_2_gpu_hours.pdf}
    \caption{\textbf{Approach 1 prescriptions. }Row one: Validation loss over FLOPs (left) and GPU hours (right). We use Approach 1 to find the optimal points on the convex hull in each setting, marked with \textbf{black crosses}. Row two: We fit a line to the tokens per parameter of empirically optimal models and find a slightly higher, but still constant, tokens per parameter prescription than \citet{hoffmann2022empirical}. 
    ~\citet{hoffmann2022empirical}'s Approach 1 creates \(250\) logarithmically-spaced FLOPs bins per order of magnitude, and in \textbf{red} we plot the minimizers over these bins, and the scaling law fitted to these minimizers (binning). Clearly, their Approach 1 is not well-suited for our data, and our convex hull approach is better when we select fewer models to fit our law on.
    Extended plot in \Cref{fig:approach-1-full}. 
    }\label{fig:approach-1}
\end{figure*}


\subsection{Fitting Scaling Laws}\label{subsec:fitting-scaling-laws}

We fit scaling laws using methods similar to approach \(1\) and \(3\) from Chinchilla \citep{hoffmann2022empirical}.
% Trying to word the next sentence as the validation set it held out but may not be the exact same across node counts
We fit all laws using the log perplexity of all trained models on a sample of \(100\) million tokens from a fixed, held-out validation set from the training distribution.
We also collect log perplexity values for a range of open source models \citep{team2024gemma1, team2024gemma2, touvron2023llama2, dubey2024llama, yang2024qwen2, yang2024qwen25} on the same validation data to allow for a comparison between our predictions and a selection of widely used models.
We design a specialized FLOP counting function as we find that simple rules of thumb (e.g., FLOPs= \(6 \times parameters\) \citep{hoffmann2022empirical}) do not accurately account for differences in FLOPs between extremely wide and narrow architectures.
We discuss this further and present our function in \Cref{subsec:app-flops-counting}.

% \sean{This next sentence is needed}
Following prior work, we plot the Epoch AI Replication \citep{besiroglu2024chinchilla} of Chinchilla \citep{hoffmann2022empirical} on all plots and use the coefficients for Kaplan plotted by \citet{porian2024resolving} which were extracted from the original paper \citep{kaplan2020scaling}.

\looseness=-1
\noindent \textbf{A More Robust Approach to Fitting Compute-Optimal Laws.}
% Approach 1 things
The first approach in~\citet{hoffmann2022empirical} fits a scaling law by plotting the loss against FLOPs for a range of architectures, and then fitting a line to the pareto-optimal architecture for each FLOP count~(see~\cref{fig:approach-1}).
Following~\citet{hoffmann2022empirical}, we refer to this as ``Approach 1''.
As we use a constant learning rate, we can use all recorded validation losses to fit our law.
%However, most of these recorded validation losses are not optimal for their FLOP count.
\citet{hoffmann2022empirical} and \citet{kaplan2020scaling} select model shapes so densely that they have a near-optimal architecture at each FLOP count.  This works when all architectures lie in a 1D space (parameterized by parameter count), as each model is optimal in some FLOP regime, and the lower envelope is densely populated. In our two dimensional exploration (varying width and depth), some models are never optimal, and the ones that are do not densely populate the envelope.
We therefore need to propose a new fitting method for less data-dense regimes.


% To overcome this, we propose a new fitting method where 
\noindent \textbf{Our New Method: The Convex Hull.} We fit a lower {\em convex hull} to our loss curves.  This hull is only supported by a sparse set of optimal models.
This naturally excludes sub-optimal models that lie above the convex hull of optimality, and as we will show, this makes the resulting scaling law far more robust to model selection choices.
% \jwk{Prior work would identify the optimal points by bucketing the data by flops and then selecting the model that achieved minimal loss in each bucket. However ILLUSTRATE (maybe pink X's?) that for our data this would include points that are strictly suboptimal with respect to the minimal loss envelope. Our method omits those points.}

% Approach 2 things
\noindent \textbf{Why We Skip Approach 2.}
Another method to fit scaling laws is to put model runs into isoFLOP bins and choose the best parameter count in each bin. ~\citet{hoffmann2022empirical} call this ``Approach 2''. %, and they then interpolate the best configuration from the parabola fit to each FLOP budget. 
Our 2-dimensional set of models do not finely cluster into isoFLOP bins, meaning our data is not easily amenable to Approach \(2\), hence we exclude this approach from our analysis. ~\citet{hu2024minicpm} also eschew this approach.
% \sean{Cite miniCPM as they also ignore this? @AP neither David nor I can find this in their paper} 

\begin{figure*}[t!]
\centering
    \includegraphics[width=0.9\textwidth]{figs/approach3/approach_3_brute_force_hot_100b+_width_depth_params_FLOPs.pdf}
    \caption{\textbf{Approach 3 laws with the parametrization shown in \Cref{eq:wd}.} We see the prescribed optimal width-depth ratio increases with the FLOPs (left) budget and the optimal tokens per parameter decreases as the FLOPs budget increases (right). 
    We see slight bumpiness in the lines due to the integer constraints we enforce on the attention heads, we also plot with this constraint removed in \Cref{fig:approach-3-wd-full-relaxed}.}
    \label{fig:approach-3-wd}
\end{figure*}

\noindent \textbf{Prescribing Optimal Widths and Depths by Fitting Power Laws.}
The final approach described by  \citet{hoffmann2022empirical} is to fit a parametric formula to the loss values with the ansatz
\begin{equation}
L(p,T) = \frac{A}{p^{\alpha}}+\frac{B}{T^{\beta}}+\varepsilon
\label{eq:params}
\end{equation}
where \(p\) is parameter count and \(T\) is tokens.
We fit our models using L-BGFS \citep{liu1989limited} with a Huber loss (\(\delta = 10^{-4}\)) between the empirical log loss and the model prediction, and use multiple initializations following  \citet{besiroglu2024chinchilla}. 
We ablate to check that our fitting procedure is robust to the size of the grid of initializations and the choice of delta in~\cref{app-subsec:approach-3-delta}.

% \paragraph{A new approach:  width-depth scaling}
Our broad selection of model architectures enables us to study scaling laws that predict loss as a function of not only parameter count, but also model aspect ratio. 
For this purpose, we consider a perturbation of the standard scaling law with additional terms to account for the impact of model width and depth.
\begin{equation}
L(w,d,p,T) = \frac{A}{w^{\alpha}}+\frac{B}{d^{\beta}}+\frac{C}{p^{\gamma}}+\frac{D}{T^{\zeta}}+\varepsilon.
\label{eq:wd}
\end{equation}


% Variation around standard scaling law, same original params + peturbations
% Also ref to other eq in appendix
Here, \(w\) is the hidden dimension of the model, \(d\) is the number of layers.
All of \(A,\alpha,B,\beta,C,\gamma,D,\zeta\) and \(\varepsilon\) are optimized to fit our scaling runs.
We choose this form as it predicts the same behaviors as the standard law if width and depth are not implicated in any systematic trends.
%\sean{We need to defend this choice a bit more here}
We ablate another possible form of this law in \Cref{app-subsec:approach-3-alternative-forms}.


\section{Experiments}\label{sec:experiments}

In~\cref{subsec:experiments-approach-3} we use our new convex hull fitting method to make a scaling law for the compute-optimal tokens-to-parameters ratio, and our new power law approach to provide a prescription for the compute-optimal width-to-depth ratio. We show how many seemingly innocuous design choices such as the learning rate schedule can significantly impact these prescribed scaling laws in~\cref{subsec:rainbow}. Finally, we reflect on the benefits of training compute-optimal and time-optimal models in~\cref{subsec:flops_vs_time}.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figs/rainbow.pdf}
    \caption{
    \textbf{We demonstrate the variability in fitting scaling laws by resampling our data many different ways.}
    We label prescriptions found using Approach 1 with ``Approach \(1\)'' in the legend, otherwise approach \(3\) is used.
    All tokens counts available are used to fit the laws unless stated otherwise in the legend, for example \(\le100B\) means that only token counts less than or equal to \(100B\) are used in fitting.\\
    \textit{No Embeds}: Embedding parameters are not counted when fitting these laws.\\
    \textit{Cooldown}: Only data from the cooldown ablation is used to fit this law.\\
    \textit{LR Ablation}: Only data from the learning rate ablation training runs, where the learning rate is halved, is used to fit these laws.\\
    \textit{width=\(512\) Only}: Only models with width \(512\) are used to fit these laws.\\
    \textit{Chinchilla Reduced Sampling}: We subsample our data to be as close as possible to the token counts and model sizes that \citet{hoffmann2022empirical} use to fit their scaling laws and also fit new scaling laws on this subset of \citet{hoffmann2022empirical} data. Details in \Cref{subsec:rainbow}.
    }
    \label{fig:rainbow-plot}
\end{figure*}


% \subsection{Width and Depth through the Lens of Scaling Laws}
\subsection{Sizing Up Our Scaling Laws Against Prior Laws and Industry Models}
\noindent \textbf{Approach 1.}~\label{subsec:experiments-approach-1}
In \Cref{fig:approach-1} (row one), we see our validation losses plotted as both a function of FLOPs (left) and GPU hours (right) for the first \(100\) billion tokens of training.
We calculate GPU hours from the average recorded optimizer step time for each model.

% We visualize this in ~\cref{fig:approach-1}. 
\textbf{Our convex hull fits the data better than prior approaches.}
~\citet{hoffmann2022empirical}'s Approach 1 creates \(250\) logarithmically-spaced FLOPs bins per order of magnitude and then uses the models that achieve the best loss in each FLOPs bin to fit the scaling law (a line). However, for our data, their approach does not work very well because it includes many points that are strictly suboptimal with respect to the minimal loss envelope. Our convex hull method omits these points, and fits the line with far fewer points.

In \Cref{fig:approach-1} (row two), we see the prescription of the fitted laws for tokens per parameter.
We see that the tokens per parameter prescription of our Approach 1 fitting is also close to constant, like \citet{hoffmann2022empirical}, but slightly higher, suggesting more tokens should be used per parameter in the model.
We extend this plot showing predicted total parameters, tokens, and over multiple ablations in \Cref{subsec:app-approach-1-ablations}.
We give a more detailed plot of each model's individual validation loss in \Cref{sec:app-training-details}.

\noindent \textbf{Approach 3.}~\label{subsec:experiments-approach-3}
This uses a parametric function to predict the loss when given parameter count and number of tokens.
As our data is intentionally designed to cover a wide variety of widths and depths, we can also predict the optimal aspect ratio of a model for a given FLOP budget.
We do this by optimizing over the law shown in \Cref{eq:wd} with four terms so we can explicitly optimize the width and depth.
%Note this analysis is not possible with One flaw of the classical scaling law form is that optimizing the width-depth ratio for a fixed FLOP budget leads to infinitely wide models, meaning an analysis such as the one we present here is not possible.

We find that convex optimization struggles to find optimal points due to the integer constraint we impose on the number of attention heads in the model.
Instead, we use a brute force approach by selecting approximately \(10^8\) model shapes and tokens counts, then use the law to predict the loss for each model, and choose the minimal loss among those in each FLOP range.
In \Cref{fig:approach-3-wd} we plot the output of this process as a solid line labeled ``Approach 3 (Ours)''.
In the left of the figure, we see that the prescribed width-depth ratio increases with the FLOP budget, but that width-depth ratio increases relatively slowly even as FLOPs is increased by many orders of magnitude (corroborating certain observations in \citet{interplay}). We also plot a selection of widely used models trained by industry estimating their cumulative FLOP expenditures (x-axis values) using FLOPs/token = \(6 \times parameters\) to calculate FLOPs with their published training token counts.
% We see the prescribed width-depth ratio increases slowly as FLOPs is quickly increased, something observed in prior work (\citep{interplay}).
In \Cref{fig:approach-3-wd} (right), we see that our optimal tokens per parameter ratio tracks the prescription of \citet{kaplan2020scaling} more closely than \citet{hoffmann2022empirical}; the prescribed tokens per parameter decreases as the number of available FLOPs increases.


\subsection{A Rainbow of Scaling Laws}
\label{subsec:rainbow}

To demonstrate the sensitivity of scaling laws to design choices, we fit laws with various assumptions and model selection rules. 
We begin by fitting laws of the classical form (without width-depth), and use equation \(4\) from \citet{hoffmann2022empirical} to provide compute-optimal parameter count prescriptions.
In \Cref{fig:rainbow-plot} we show the optimal predictions of multiple possible laws fitted on different data subsets, visualizing the amount of variation possible under an array of configurations.


% Attempting to explain the slim chinchilla ablations
One particular dimension of variability we wish to highlight briefly here is the interplay between model selection and the derived law.
To do this, we select 5 models from Gemstones that have an analogous model in \citet{hoffmann2022empirical} (using data extracted by \citet{besiroglu2024chinchilla}) with similar parameter count and aspect ratio. 
We select Gemstones checkpoints with token counts nearly matching the Hoffman points.
We call this ``Chinchilla Reduced Sampling.''
We then fit scaling laws to both of these sub-sampled datasets.
We find that fitting Hoffmann's data using this reduced sampling results in an increased slope relative to fitting on all data. Meanwhile this subsampling reduces the slope of the line fit on Gemstones.
This highlights that scaling law fitting can be quite sensitive to seemingly innocuous changes in model selection for both the Gemstones and the simpler model family selected by Hoffman.
Notably, there are \(5\) models in this subset for both \citet{hoffmann2022empirical} and our data, this meets the rule of thumb given by \citet{choshen2024hitchhiker} for the minimum number of models should be used to fit a scaling law.

% Attempting to say we do 5 ablations which are a meh level of interesting
We also perform many other ablations in \Cref{fig:rainbow-plot}, and we summarize these here. % and in the caption, which all impact the fitted law in some way and are all notable points for other practitioners to be aware of.
First, we fit laws both including and excluding embedding parameter count, which both \citet{pearce2024reconciling} and \citet{porian2024resolving} find to be a primary explanation of the discrepancies between the prescriptions found by \citet{kaplan2020scaling} and \citet{hoffmann2022empirical}. 
We also show the impact of fitting on our cooldown and learning rate ablation datasets in turn, seeing that both choices have a noticeable impact on the prescription for optimal parameter count.
%This emphasizes the need for optimal hyperparameter choice to maximize the applicability of the law across model shapes.
Next, we remove checkpoints from our data to simulate having only trained for \(100\) billion tokens or only having data for token counts greater than \(120\) billion.
%Removing token counts generally has a larger impact than the ablations on the slope of the line.
Finally, we also fit to only models with a width of \(512\) to isolate the role of depth.
Fitting on only these models creates a drastic difference in the law, significantly reducing the predicted optimal rate of growth of parameters as a function of FLOPs, highlighting the need for including diverse model shapes in the fitting data.

\subsection{The Price of Stepping Off the Scaling Law}\label{subsec:flops_vs_time}
\looseness=-1

\begin{figure*}[ht!]
\centering
    \includegraphics[width=0.8\textwidth]{figs/efficiency_vs_loss_gpu_hours_100b+.pdf}
    \caption{
    \textbf{The inefficiency of training models with suboptimal widths and depths. }
    We plot the FLOPs (left) and GPU Hours (right) \textit{overspend} after training our Gemstones for \(300\) billion tokens. 
    We define the overspend as how many resources (FLOPs or GPU hours) are required for a model with a given width-depth configuration to reach some target loss, relative to the models that achieve that target loss the fastest~(the ``points on (pareto)-frontier''). We can see that the skinny models~(top-left, dark points) use many more FLOPs or GPU hours to reach a target loss than the wide models. We note that these inefficiencies exist in our training setup because we only use tensor parallelism and not pipeline parallelism.
    % We quantify the overspend as the FLOPs (or GPU Hours) of the model divided by the FLOPs (or GPU hours) of the loss-optimal model at this token count. Scaling \tom{What does ``points on Frontier'' mean?  Frontier of what? Are these the industry models?}\sean{points on Frontier==points on the convex hull (here)}
    }
    \label{fig:flops_gpu_hrs}
\end{figure*}

\noindent In the previous sections we provide prescriptions for the optimal width, depth, parameters, and tokens as a function of total training FLOPs. As we have seen, industry models don't always comply with our prescriptions. 
At the same time, the variability in our analyses suggests that scaling laws are inherently fuzzy, providing only rough estimates of optimal settings. 
% In light of this, it is natural to ask how bad the consequences are of choosing sub-optimal hyper-parameters? 
With this in mind, we ask a natural question: if one happens to choose a \textit{sub-optimal} point in design space, how bad can it really be?

By analyzing the cost of stepping off of the scaling law, we find that some kinds of design errors are more damaging than others.  In particular, one pays a steep price for training models that are too narrow, and it is better to err on the side of too wide.  We also find that training on more tokens than is strictly recommended (aka ``overtraining'') is typically quite efficient in terms of pushing down loss.

\noindent \textbf{If You Value Your Time, Train Wide Models.} 
We first show that in our training setup, training wider models is far more efficient than training deep models.
In~\cref{fig:flops_gpu_hrs}, we reflect on the consequences of suboptimal architectural choices, by considering how much of a given resource---FLOPs or GPU hours---would be ``overspent'' to reach any target loss value with the plotted architecture rather than the prescribed width and depth. We find that choosing to train ``skinny'' models~(top left) wastes many FLOPs and GPU hours. The scale of overspend is quite different however, with the least efficient models only overspending about \(50\%\) on FLOPs but wasting more than \(200\%\) of the GPU hours spent by the best configuration. In other words, in the time taken to train a single (very) suboptimal model to the desired loss value, one could train three optimal-width-depth models. We note that while the time-optimal models tend to be the wider ones, this is probably due to our training scheme. Similar to other open-source efforts such as~\citet{olmo20242}, we do not make any use of pipeline parallelism, and only employ tensor parallelism (using a hybrid data and tensor parallel algorithm similar to the ubiquitous Fully Sharded Data Parallel strategy). In summary, for standard parallelism implementations, wider models are simply easier to scale, but as a result our observations regarding resource overspending may not generalize to other parallelism strategies.

\begin{figure}[h!]
\centering
    % \includegraphics[width=0.50\textwidth]{figs/overtrain/parabola_overtrained_predictions_wd_hot_smaller.pdf}
    \includegraphics[width=0.48\textwidth, trim = 0cm 0cm 0cm 0cm, clip]{figs/overtrain/parabola_overtrained_predictions_wd_hot_2.pdf}
    % \includegraphics[width=0.48\textwidth, trim = 0cm 14cm 0cm 0cm, clip]{figs/overtrain/parabola_overtrained_predictions_wd_hot_2.pdf}
    % \includegraphics[width=0.48\textwidth, trim = 0cm 0cm 0cm 12cm, clip]{figs/overtrain/parabola_overtrained_predictions_wd_hot_2.pdf}
    \caption{\textbf{Quantifying the cost of overtraining. } We simulate deviations from our prescriptions to assess their impact on model performance by increasing the optimal token count prescribed by \Cref{eq:wd} by an overtraining factor. We then optimize the model shape to achieve the lowest loss possible at each FLOP budget and overtraining factor. Note that \(10^0\), or \(1\times\), is the prescribed optimal point. We take four FLOP budgets (title of each plot) and plot the loss as a function of overtraining factor and see that under or overtraining increases predicted loss but by only a small amount. We plot the losses of selected open source models on our validation set to help ground the y-axis ranges.}
    \label{fig:overtrain}
\end{figure}

\noindent \textbf{Scaling Laws Predict That Overtraining Is Efficient.}

In \Cref{fig:approach-3-wd}, we see the optimal predictions from our law of the form shown in \Cref{eq:wd}.
We can shift those optimal points to simulate overtraining.
To do this, we fix a FLOP budget and trace out a path of model sizes and corresponding token counts to remain within that budget.  For each model size and token count, we record the ``overtraining factor,'' which is the selected number of training tokens divided by the optimal number of tokens for that model shape.  
An overtraining factor of less than one corresponds to undertraining the model, and a factor greater than one represents overtraining.
We show the results of this process in \Cref{fig:overtrain}. We see that overtraining does increase predicted loss at a given FLOP count but that these curves are actually quite flat. We include the loss values of open source models on our own validation set to allow readers to contextualize the y-axis values. Especially at high FLOP counts, overtraining becomes quite efficient in that it results in fairly small elevations in loss for a relatively large reduction in model size. 

Industry models often use fewer parameters and train on more tokens than prescribed in prior work (see \Cref{fig:approach-3-wd}).
%In Appendix \Cref{fig:overtrain}, we analyze the trade off of overtraining with respect to loss.
%We do this by taking the optimal parameter prescription, increasing it and then finding the optimal prescribed token count for this increased parameter count.
We find the impact of overtraining a smaller model on predicted loss to be small.
Combining this with \Cref{fig:flops_gpu_hrs}, where wider models are predicted to be optimal in terms of GPU hours, reinforces the message that FLOPs optimality is not the end of the story for training models.
Trading some FLOPs optimality for time optimality necessarily means over-/under-training, but \Cref{fig:overtrain} suggests the difference is marginal.
We believe this combined evidence makes significant progress towards explaining the differences between the prescriptions found in prior work and training choices observed in the wild out in industry.

\section{Limitations and Conclusions}
We hope this work encourages a rich range of future work on the impact of width and depth within modern transformer architectures using the large amount of open source artifacts we produce.
Future work should also extend to other hyperparameters involved in training transformer architectures, such as the expansion factor.
Although we endeavor to make our laws as generalizable as possible, we still expect that their applicability declines in training set-ups very different from our own.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
We thank (in alphabetical order): Brian Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Sachin Shah and Abhimanyu Hans for helpful feedback.

An award for computer time was provided by the U.S. Department of Energy’s (DOE) Innovative and Novel Computational Impact on Theory and Experiment (INCITE) Program. This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.

This work was made possible by the ONR MURI program, DAPRA TIAMAT, the National Science Foundation (IIS-2212182), and the NSF TRAILS Institute (2229885). Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. 

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{refs}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Software and Data}
We train all models using a fork of \texttt{litgpt} \citep{litgpt-2023} enhanced with AxoNN \citep{singh2024axonn,singh2024hybrid} tensor parallelism.
We open source all models used in our analysis to Hugging Face \citep{wolf2020huggingfaces} and the logging from training on Weights and Biases in json format.
All scaling-law fitting code is released on GitHub, with training code to be released shortly after publication.
 
\section{Extended Related Works}\label{sec:app-rel-works}
Scaling laws are broadly applied to many areas of machine learning, such as machine translation.
\citet{ghorbani2021scaling} split the parameters term into two, one for each of encoder and decoder, and similarly to \citet{gordon2021data} analyze the relationship between BLEU scores and scaling laws.
\citet{zhang2022examining} and \citet{bansal2022data} study the impact of architecture choice on the scaling law, finding increasing data or parameters can compensate for worse architectural decisions.

Scaling laws have also been applied to sparse architectures.
\citet{clark2022unified} analyze how the number of experts can be used in the law, studying both linear and quadratic interactions for many types of routing models.
\citet{yun2024toward} extend this, analyzing the trade offs between optimal training and optimal inference.
\citet{krajewski2024scaling} find that with optimal settings a Mixture of Experts model always outperforms a transformer model at any computational budget.
\citet{frantar2023scaling} focus on weight sparsity within foundation models, adding a multiplicative parameter on the parameters term in the law.

These techniques are not limited to generative text modeling only; they have also been applied to multi-model models.
\citet{henighan2020scaling} find optimal model size can be described as a power law for model modeling including images and video. 
The authors also find that model size does not help `strong generalization' for problem solving.
\citet{aghajanyan2023scaling} analyze text, images, code and speech, presenting a scaling law to describe the competition between these modalities and describe a regime for optimal hyperparameter transfer from the unimodal to multimodal regimes.
\citet{liang2024scaling} look at scaling laws for diffusion transformer models.
\citet{li2024bigger} analyze scaling laws for vision encoder commonly used to encode image inputs for transformer model backbones, finding increasing the size of the encoder alone can lead to performance degradation in some cases.

Further analyses using scaling laws have extended to analyzing finetuning and data limited scaling.
\citet{hernandez2021scaling} find that finetuning is much more compute efficient when the pretraining ignored.
\citet{zhang2024scaling} study parameter efficient finetuning regimes find a multiplicative law is better for the finetuning setting than the classical additive law used by others.
\citet{muennighoff2023scaling} analyze the data constrained training regimes, finding epoching data up to four times is as good as training on deduplicated data in terms of reducing loss.


% \section{Just How Bad Is Under/Overtraining?}
% \label{app-sec:overtrain}
% used to be overtraining section

\section{Ablations for Approach 1}~\label{subsec:app-approach-1-ablations}
\subsection{Extended Paper Figures}
In \Cref{fig:approach-1-full}, we plot an extended version of the Approach 1 plot we present in \Cref{fig:approach-1}.

\begin{figure*}
\centering
    \includegraphics[width=0.8\textwidth]{figs/approach1/figure_2_gpu_hours_full.pdf}
    \caption{Extended Approach 1 plot from \Cref{fig:approach-1}, including tokens and parameters axes. As in \Cref{fig:approach-1}, we present an analysis over FLOPs on the left and over GPU hours take to train on the right.}
    \label{fig:approach-1-full}
\end{figure*}

\subsection{Alternative Learning Rates}
In \Cref{fig:approach-1-lr}, we present the Approach 1 prescription when fitting on the learning rate ablation data.
\begin{figure*}
\centering
    \includegraphics[width=0.8\textwidth]{figs/approach1/figure_2_gpu_hours_lr_ablation_full.pdf}
    \caption{Approach 1 fitted on the learning rate ablation dataset. As in \Cref{fig:approach-1}, we present an analysis over FLOPs on the left and over GPU hours take to train on the right.}
    \label{fig:approach-1-lr}
\end{figure*}

\subsection{Cooldown}
In \Cref{fig:approach-1-cool}, we present the Approach 1 prescription when fitting on the cooldown ablation data.
\begin{figure*}
\centering
    \includegraphics[width=0.8\textwidth]{figs/approach1/figure_2_gpu_hours_cooldown_full.pdf}
    \caption{Approach 1 fitted on the cooldown ablation dataset. As in \Cref{fig:approach-1}, we present an analysis over FLOPs on the left and over GPU hours take to train on the right.}
    \label{fig:approach-1-cool}
\end{figure*}

\section{Ablations for Approach 3}\label{subsec:app-approach-3-ablations}
\subsection{Extended Paper Figures}
In \Cref{fig:approach-3-wd-full}, we plot an extended version of the Approach 3 plot we present in \Cref{fig:approach-3-wd}.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_hot_100b+_width_depth_params_FLOPs_full.pdf}
    \caption{Extended Approach 3 plot from \Cref{fig:approach-3-wd}, including tokens, parameters, width and depth axes. This also includes a comparison of equation 4 that \citet{hoffmann2022empirical} propose and a brute force plotting method which includes our FLOPs counting for laws of the form seen in \Cref{eq:params}. We show these are blue and red lines respectively in the right column seeing only a minor difference in the outcome. We remark that industry models fall systematically below our parameter per FLOPs prescriptions, or equivalently above our token per FLOPs prescriptions.}
    \label{fig:approach-3-wd-full}
\end{figure*}

\subsection{Alternative Learning Rates}
\label{app-subsec:approach-3-lr-over-2}
In \Cref{fig:approach-3-eta-over-2}, fit Approach 3 laws to the dataset we trained with half of the optimal learning rate as described in \Cref{sec:mup-init}.
We again use a brute force approach as above to plot the results but to allow for precise comparison with later ablations we ignore the integer constraints on the number of heads, still enforcing that at least one head number be in every model.
We remove this constraint for all models shown in \Cref{app-subsec:approach-3-lr-over-2}, \Cref{app-subsec:approach-3-cooldown}, \Cref{app-subsec:approach-3-over-120B} and \Cref{app-subsec:approach-3-delta}.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_lr_ablation_hot_width_depth_params_relaxed_FLOPs_full.pdf}
    \caption{Approach 3 fitted on the learning rate ablation dataset. As in \Cref{fig:approach-3-wd-full}, we present an analysis over laws of the from shown in \Cref{eq:wd} on the left and laws of the form shown in \Cref{eq:params} on the right.}
    \label{fig:approach-3-eta-over-2}
\end{figure*}

\subsection{Cooldown}
\label{app-subsec:approach-3-cooldown}
In \Cref{fig:approach-3-cooldown}, we fit Approach 3 laws to the subset of data for models for which we linearly decreased the learning rate to zero for.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_cool_end_width_depth_params_relaxed_FLOPs_full.pdf}
    \caption{Approach 3 fitted on the cooldown ablation dataset. As in \Cref{fig:approach-3-wd-full}, we present an analysis over laws of the from shown in \Cref{eq:wd} on the left and laws of the form shown in \Cref{eq:params} on the right.}
    \label{fig:approach-3-cooldown}
\end{figure*}

\subsection{Removing Smaller Token Counts}
\label{app-subsec:approach-3-over-120B}
In \Cref{fig:approach-3-remove-tokens}, we present the Approach 3 prescription when fitting on a dataset where all token counts less than \(120\) billion are removed.

\label{app-subsec:approach-3-remove-tokens}
\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_hot_120b+_only_width_depth_params_relaxed_FLOPs_full.pdf}
    \caption{Approach 3 fitted on a dataset where all token counts less than \(120\) billion are removed. As in \Cref{fig:approach-3-wd-full}, we present an analysis over laws of the from shown in \Cref{eq:wd} on the left and laws of the form shown in \Cref{eq:params} on the right.}
    \label{fig:approach-3-remove-tokens}
\end{figure*}

\subsection{Varying Delta in the Huber loss}
\label{app-subsec:approach-3-delta}
So far we have fit all approach three laws with a Huber loss delta of \(10^{-4}\). 
We now ablate this decision by refitting all laws with a delta of \(10^{-3}\).
We use an extremely large grid search of over \(4\) million initializations for the width-depth based law when fitting.

To begin we show the prescriptions of the Approach 3 laws if the integer constraints are removed, as we did for the learning rate and cooldown ablations in Figures \ref{fig:approach-3-eta-over-2} and \ref{fig:approach-3-cooldown} respectively.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_hot_100b+_width_depth_params_relaxed_FLOPs_full.pdf}
    \caption{Following from \Cref{fig:approach-3-wd}, this plot removes the integer constraints when optimizing. As in \Cref{fig:approach-3-wd-full}, we present an analysis over laws of the from shown in \Cref{eq:wd} on the left and laws of the form shown in \Cref{eq:params} on the right.}
    \label{fig:approach-3-wd-full-relaxed}
\end{figure*}

We now compare all Approach 3 laws found with the increased delta.
Specifically, we plot the full dataset laws with delta of \(10^{-4}\) in \Cref{fig:approach-3-wd-full-relaxed} and with \(10^{-3}\) in \Cref{fig:approach-3-wd-full-relaxed-delta-3}.
We plot the learning rate ablation laws with delta of \(10^{-4}\) in \Cref{fig:approach-3-eta-over-2} and with \(10^{-3}\) in \Cref{fig:approach-3-eta-over-2-delta-3}.
We plot the cooldown ablation laws with delta of \(10^{-4}\) in \Cref{fig:approach-3-cooldown} and with \(10^{-3}\) in \Cref{fig:approach-3-cooldown-delta-3}.
In these figures, we see the difference for the full dataset, cooldown and learning rate ablations laws is minimal when changing the delta.
We conclude with a cautionary figure about size of the grid search and the delta used in the Huber loss.
In \Cref{fig:delta-grid-search}, where we plot the exponents found by optimizing the Huber loss versus the size of the grid search used for optimization.
We see that a delta of \(10^{-5}\) is unstable for smaller grid sizes and including more tokens in the fitting data generally increases stability of the exponents found during optimization.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_hot_100b+_width_depth_params_relaxed_FLOPs_full_delta-3.pdf}
    \caption{Approach 3 fitted on our main dataset using \(\delta = 10^{-3}\) in the Huber loss. Corresponds to \Cref{fig:approach-3-wd-full-relaxed}. As in \Cref{fig:approach-3-wd-full}, we present an analysis over laws of the from shown in \Cref{eq:wd} on the left and laws of the form shown in \Cref{eq:params} on the right.}
    \label{fig:approach-3-wd-full-relaxed-delta-3}
\end{figure*}

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_lr_ablation_hot_width_depth_params_relaxed_FLOPs_full_delta-3.pdf}
    \caption{Approach 3 fitted on the learning rate ablation dataset using \(\delta = 10^{-3}\) in the Huber loss. Corresponds to \Cref{fig:approach-3-eta-over-2}. As in \Cref{fig:approach-3-wd-full}, we present an analysis over laws of the from shown in \Cref{eq:wd} on the left and laws of the form shown in \Cref{eq:params} on the right.}
    \label{fig:approach-3-eta-over-2-delta-3}
\end{figure*}

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_cool_end_width_depth_params_relaxed_FLOPs_full_delta-3.pdf}
    \caption{Approach 3 fitted on the cooldown ablation dataset using \(\delta = 10^{-3}\) in the Huber loss. Corresponds to \Cref{fig:approach-3-cooldown-delta-3}. As in \Cref{fig:approach-3-wd-full}, we present an analysis over laws of the from shown in \Cref{eq:wd} on the left and laws of the form shown in \Cref{eq:params} on the right.}
    \label{fig:approach-3-cooldown-delta-3}
\end{figure*}

\begin{figure*}
\centering
    \includegraphics[width=0.49\textwidth]{figs/slope_progression_combined_grid_axis_hot.pdf}
    \includegraphics[width=0.49\textwidth]{figs/slope_progression_combined_grid_axis_hot_100b+.pdf}
    \caption{We plot the size of the grid search as the x axis and the gradient of the prescribed tokens as the y axis. We vary delta and see that a delta of \(10^{-5}\) is highly unstable when fitting on smaller grid sizes. On the left, we plot only fitting on data less than \(100\) billion tokens. On the right, we plot fitting on all data up to \(350\) billion tokens. We see that including more data increases the stability of the exponents found for smaller grid sizes for deltas \(10^{-4},10^{-5}\).} 
    \label{fig:delta-grid-search}
\end{figure*}


\subsection{Alternative Law Forms}
\label{app-subsec:approach-3-alternative-forms}
We also experiment with laws of the form shown in \Cref{eq:wdt-only}.
In \Cref{fig:approach-3-width-depth-tokens-law}, we see that the prescriptions of this law are approximately in line with those of \citet{kaplan2020scaling}.
Unlike the laws for shown in \Cref{eq:wd}, these laws tend to prescribe that the width-depth ratio should go to zero as the FLOPs budget increases, i.e. prescribing an infinite-depth model in the infinite-budget limit.

\begin{equation}
L(w,d,p,T) = \frac{A}{w^{\alpha}}+\frac{B}{d^{\beta}}+\frac{C}{T^{\gamma}}+\varepsilon
\label{eq:wdt-only}
\end{equation}

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{figs/approach3/approach_3_brute_force_hot_100b+_FLOPs_full.pdf}
    \caption{Approach 3 fitted on our main dataset using an ansatz of the form shown in \Cref{eq:wdt-only}.}
    \label{fig:approach-3-width-depth-tokens-law}
\end{figure*}

\section{Data Sampling}

We plot the entire space of all possible models subject to our design constraints discussed in \Cref{fig:model-search-space-complete}. While exploring the impact of finer grained depth differences during our experiments, we decided to add two additional models slightly outside the $\pm 5\%$ tolerance band at the $100$M scale; for $width=512$, in addition to the originally chosen depths of $12$ and $13$, we added $11$ and $14$; these appear as a dense collection of $4$ points at the same $width$.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/width_depth_sampling/model_search_space.pdf}
    \caption{
    All possible model shapes we could have chosen based on our architecture within \(\pm 5\%\) are shown as circles. The points we selected are highlighted as stars, including the two extra points we select to have four models of width \(512\).
    }
    \label{fig:model-search-space-complete}
\end{figure}

\section{Training}\label{sec:app-training-details}
Despite our best efforts to sufficiently mix the training data, we still see slight jumps in the global training loss when the training switches between chunks of data, hence we use validation loss to fit all laws as this is smooth.

\subsection{Loss Curves}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/wandb_loss_vs_tokens.pdf}
    \caption{Loss curves for the main \(22\) training runs.}
    \label{fig:loss_vs_tokens}
\end{figure}

\subsection{Additional Training Complications}\label{subsec:app-training-details-hyperparams}

Any gemstone naturally contains a small number of inclusions or fractures. We discuss a few of the minor imperfections in our model collection below.

\paragraph{Dealing with Training Instabilities}\label{sec:loss-spikes}

After some of the widest models were trained beyond $50B$ tokens we began to observe unrecoverable loss spikes that were proceeded by small wobbles in the loss trajectory. Under the general intuition that the culprit was most likely that the $width / depth$ ratios considered were simply \textit{too extreme} for existing initialization and learning rate scaling approaches to handle, we reran some of the models with a ``patch'' in place.

We modified the initialization rules and learning rate scaling factors to rescale the depth and layer indices of the model such that if $width / depth > 256$ scale variances and learning rates as if the depth of the model was actually $depth' = \lceil(width / 100)\rceil$. The overall effect of the patch is to initialize and scale learning rates more conservatively, as if the aspect ratio were only $100$ while keeping the original $width$ of the model. We found this allowed us to complete training for a full set of \(22\) models out to $350$B tokens for even our most extreme models.

However, after $350B$ tokens, despite these efforts we observed that most extreme models which were patched still diverged anyway. While a partial cause of this could be the constant learning rate scheduler employed during training, concurrent work, from the authors of the original OLMo paper and codebase \citep{Groeneveld2023OLMo} from which we derived some of our choices, reported that the initialization scheme dubbed the ``Mitchell-init'' is indeed systematically prone to instabilities later on in training \citep{olmo20242}. While an unfortunate finding, we were unable to rerun all of our experiments due to the consumption of significant non-fungible compute resources in the original experiments.

\paragraph{Models Lacking Ablations}\label{sec:models-lacking-cooldowns}

Our cooldown ablation is from initial experiments below \(100B\) tokens of training which do not use the patched learning rates scaling rules. This means there are minor discrepancies between the cooldown ablation and main set of training runs for the widest models from the three largest parameter count groups (\(1792\times7\), \(2560\times8\), \(3072\times12\)). We also do not cool down the \(100B\) token checkpoint for the \(3072 \times 12\) model as it was experiencing a loss spike at that final point. Finally, we do not include ablations for the two width \(512\) models which do not fall into the \(\pm 5\%\) boundary of the \(100M\) parameter count (\(512\times11\), \(512\times14\)) as they were only added to the collection in later experiments. 


\section{FLOP counting matters}\label{subsec:app-flops-counting}
In~\Cref{fig:flop_counting} we show that the common approximation of FLOPs per token\(=6 \times parameters\), miscounts the true FLOPS by a significant amount. 
\begin{figure}[hbtp]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/flops_accounting_gpu_hours.pdf}
    \caption{We color the points based on the ratio of our calculated FLOPs per token which is shown in the code below and using \(6\times parameters\). We see counting the FLOPs properly becomes more important for aspect ratios off outside of the standard regime.}
    \label{fig:flop_counting}
\end{figure}


\begin{minted}{python}
VOCAB_OURS = 50304
SEQ_LEN = 2048
WORLD_BATCH_SIZE = 2048.0
HEAD_SIZE = 128
EXPAND_FACTOR = 4.0

def flops_per_token_gqa(
    width: NDArray[number] | number,
    depth: NDArray[number] | number,
    vocab_size=VOCAB_OURS,
    queries_per_group=2,
    seq_len=SEQ_LEN,
):
    """
    Some details (negligible even for extremely wide models) omitted, including:
    * numerically stable softmax
    * softmax addition only being over rows
    * dot products being only n-1 additions (fused multiply-add exists anyway)
    """
    num_qheads = width / HEAD_SIZE
    num_kvheads = (
        2 * num_qheads / queries_per_group
    )

    embeddings = 0  # 0 if sparse lookup, backward FLOPs negligible

    attention = 2.0 * seq_len * (num_qheads + num_kvheads) * width * HEAD_SIZE
    attention += (
        3.5 * seq_len * (num_qheads + num_kvheads / 2) * HEAD_SIZE
    )  # RoPE, as implemented here/GPT-NeoX
    # score FLOPs are halved because causal => triangular mask => usable sparsity
    kq_logits = 1.0 * seq_len * seq_len * HEAD_SIZE * num_qheads 
    softmax = 3.0 * seq_len * seq_len * num_qheads
    softmax_q_red = 2.0 * seq_len * seq_len * HEAD_SIZE * num_qheads
    final_linear = 2.0 * seq_len * width * HEAD_SIZE * num_qheads
    attn_bwd = (
        2.0 * attention
        + 2.5 * (kq_logits + softmax + softmax_q_red)
        + 2.0 * final_linear
    ) * depth
    attention += kq_logits + softmax + softmax_q_red + final_linear

    ffw_size = EXPAND_FACTOR * width
    dense_block = (
        6.0 * seq_len * width * ffw_size
    )  # three matmuls instead of usual two because of GEGLU
    dense_block += (
        10 * seq_len * ffw_size
    )  # 7 for other ops: 3 for cubic, two additions, two scalar mults
    dense_block += 2.0 * width * seq_len  # both/sandwich residual additions
    rmsnorm = 2 * 7.0 * width * seq_len 

    final_rms_norm = 7.0 * width * seq_len  # one last RMSNorm
    final_logits = 2.0 * seq_len * width * vocab_size
    nonattn_bwd = 2.0 * (
        embeddings + depth * (dense_block + rmsnorm) + final_rms_norm + final_logits
    )
    forward_pass = (
        embeddings
        + depth * (attention + dense_block + rmsnorm)
        + final_rms_norm
        + final_logits
    )
    backward_pass = attn_bwd + nonattn_bwd  # flash attention

    return (forward_pass + backward_pass) / seq_len
\end{minted}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
