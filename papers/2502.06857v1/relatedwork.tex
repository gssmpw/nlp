\section{Related Work}
\label{sec:related_work}
\subsection{Scaling Laws}
Scaling laws address the trade-off between parameter count and number of training tokens, attempting to find the minimum loss possible for a language model with a constrained FLOP budget.
%\citet{kaplan2020scaling} fit a compute-optimal parameter count law of the form \(N_{opt} \propto C^{p_N}\). 
Unfortunately, scaling laws treat model design and training as if it has a single dimension (parameter count).  In reality, training is sensitive to many choices. 
Notably, \citet{hoffmann2022empirical} find significantly different 
% exponents in their 
fitted laws (\Cref{eq:params}) compared to \citet{kaplan2020scaling}.
\citet{pearce2024reconciling} and \citet{porian2024resolving} attribute most of this discrepancy to the choice to exclude embedding parameters from the parameter count, both showing one law can be transformed into the other via controlled changes.
\citet{kaplan2020scaling} justify including embedding parameters by showing that non-embedding parameters have a cleaner relationship with test loss. %, while using total parameters introduces a dependency on depth.
Scaling laws are also commonly included in many large model releases \citep{hu2024minicpm, bi2024deepseek, dubey2024llama}.

% This is all the work **VERY** related to ours
\citet{choshen2024hitchhiker} collect both loss and benchmark performance metrics for a multitude of models and offer a practitioner's guide to fitting scaling laws.
%Stating that the early period of training data should be dropped when fitting scaling laws, but intermediate checkpoints may be used if the learning rate schedule is appropriate.
Most notably, they suggest \(5\) models are ample to fit a scaling law, and the early period of training should be excluded from the analysis.
In contrast, \citet{misfitting} show that selecting data according to different tokens-per-parameter ratios and using small grid searches when fitting scaling laws can cause big swings in outcomes.
\citet{hagele2024scaling} suggest that a constant learning rate plus cooldown is preferable to a cosine learning rate schedule.
The authors also find that stochastic weight averaging should be encouraged in scaling law analysis as it tends to lead to better models.
Furthermore, \citet{memmoves} observe that FLOPs cannot be used to predict wall-clock time nor memory movement, and suggest that fast-training architectures may be preferred over those prescribed by scaling laws.

% Scaling laws for downstream performance {This can be cut up as we get the lm eval results in}
There are multiple works analyzing whether scaling laws can be used to predict downstream performance.
\citet{observational} show that scaling laws can be predictive of benchmark performance.
\citet{caballero2023broken} %observe that prior functional forms cannot model non-monotonic behavior nor inflection points.
%The authors 
suggest broken scaling laws that predict performance of both downstream and upstream tasks. Works in this vein are myriad; see our extended literature review in \Cref{sec:app-rel-works}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/explainer.pdf}
    \caption{\textbf{The meaning of width and depth. }We visualize a standard transformer architecture, highlighting the ``width'' as the size of the hidden dimension and the ``depth'' as the number of transformer blocks. These are the quantities we're interested in providing a prescription for with our new scaling laws.}\label{fig:summary}
\end{figure}

\subsection{The Role of Model Shape}
\citet{interplay} find that, for large models, optimal depth grows logarithmically with width. 
\citet{henighan2020scaling} find there is an optimal aspect ratio for each modality they study which gives maximum performance, for example they find \(5\) to be optimal for math models.
\citet{petty2024impact} claim small ($<$400M) transformers have diminishing benefits from depth.
\citet{brown2022wide} show that in some cases shallower models can beat their parameter equivalent deep models on tasks for encoder-decoder transformer architectures.
This differs from \citet{kaplan2020scaling} who suggest aspect ratio is not a determining factor for final loss.

Most similarly to our work, \citet{alabdulmohsin2024getting} study the impact of width and depth for encoder-decoder vision transformers, using their laws to create a smaller transformer model which has competitive downstream performance when compared with much larger models.
The architecture found in this study has since been used by \citet{beyer2024paligemma} in PaliGemma.

Model shape has also been analyzed for sparse mixture of expert models and in the context of finetuning.
\citet{krajewski2024scaling} use ``granularity'' to allow their law for mixture of expert models to predict the width of the experts. 
\citet{scaleEfficiently} show that downstream performance strongly depends on shape when finetuning but pretraining perplexity does not.

\subsection{Zero-shot Hyperparameter Transfer}\label{sec:mup-relwork}

The ability to train a series of models with extremely different parameter counts is an implicit requirement of any scaling law analysis. Work on zero-shot hyperparameter transfer across transformer model \textit{widths} is mature \citep{yang2021zero,everett2024scaling,hayou2023width,cerebras2024mupguide}, but achieving transfer across diverse model \textit{depths} is less well studied, especially in transformer language models \citep{bordelon2024depthwise}. %While solving this problem perfectly is still an active area of research, we deploy a combination of initialization rules and learning rate transformation functions derived from prior work to try and achieve the required level of hyperparameter transfer to enable us to train models of diverse shapes and sizes (\Cref{sec:learning-rates}).