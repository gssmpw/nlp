\section{Related Work}
\label{full_related_work}
\parabf{Deep Learning for Code Intelligence}
% Neural language models have made significant strides in code intelligence~\cite{wan2024deep}, encompassing tasks such as code summarization~\cite{wan2018improving,wang2020reinforcement}, code search~\cite{gu2018deep,wan2019multi}, and code generation~\cite{bi2024iterative,DBLP:conf/kbse/Sun000J0L24}. 
% One key foundation of code intelligence is representing the source code into a vector, and much efforts have been made toward this direction via designing various deep neural networks, mainly from three categories: sequential code tokens (e.g., plain text, intermediate representation, API), Abstract Syntax Tree (AST) and code graphs (Control-Flow Graph (CFG), Data-Flow Graph (DFG), and Code Property Graph (CPG)).
% To represent the sequential code tokens the RNN, and CNN have been proposed to represent the plain text, intermediate representation, and API extracted from source code.
% To represent the AST of code, research works either proposed the structural RNN and CNN to capture the tree structure of AST, or linearize the AST into sequential traversals and apply the vanillia RNN and CNNs.
% To represent the code graphs, various Graph Neural Networks (GNNs) have been proposed.
% Recently, with the advancement of LLMs in text generation, several specialized LLMs for code have been introduced, including CodeT5+~\cite{wang2023codet5+}, InCoder~\cite{fried2022incoder}, StarCoder~\cite{Li2023StarCoderMT}, Code Llama~\cite{roziere2023code}, WizardCoder~\cite{luo2023wizardcoder}, Qwen-Coder~\cite{hui2024qwen2}, and DeepSeek-Coder~\cite{guo2024deepseek}. 
% These models have significantly transformed the software development landscape through various production tools, such as Copilot~\cite{GitHub2022copilot}, CodeWhisperer~\cite{Aws2022codewhisperer}, and Replit~\cite{Replit2022replit_ai}.
% Despite these advancements, one key limitation remains: LLMs always struggle with efficiently handling the dynamic nature of knowledge, particularly when it comes to keeping up with frequent updates in programming languages, frameworks, and coding practices. 
% In complementary to existing work, this paper focuses on integrating the dynamic knowledge into LLMs to synchronize with the dynamic code evolution.
Neural language models have made remarkable progress in code intelligence~\cite{wan2024deep}, encompassing a variety of tasks including code summarization~\cite{wan2018improving,wang2020reinforcement}, code search~\cite{gu2018deep,wan2019multi}, and code generation~\cite{bi2024iterative,sun2024sifting,li2024ircoco}. A central challenge in code intelligence is the effective representation of source code as vectors. Substantial effort has been devoted to this, primarily through the design of deep neural networks in three main categories: sequential code tokens (e.g., plain text, intermediate representations, APIs), Abstract Syntax Trees (ASTs), and code graphs (such as control-flow graphs, data-flow graphs, and code property graphs).
For sequential code tokens, approaches have employed Recurrent Neural Networks (RNNs)~\cite{graves2012long,chung2015gated,gu2018deep} and Convolutional Neural Networks (CNNs)~\cite{mou2016convolutional, yamashita2018convolutional} to process plain text~\cite{iyer2016summarizing,allamanis2016convolutional}, intermediate representations~\cite{venkatakeerthy2019ir2vec,peng2021how,gui2022cross}, and API calls~\cite{gu2016deep,nguyen2017exploring} extracted from source code. For ASTs, prior research has either developed structural RNNs~\cite{wan2018improving} and CNNs~\cite{mou2016convolutional} to capture the hierarchical structure of the tree or linearized the AST into sequential traversals~\cite{alon2019code2vec,alon2018code2seq} for processing with traditional RNNs or CNNs. To handle code graphs, various Graph Neural Networks (GNNs)~\cite{chu2024graph,allamanis2017learning} have been proposed, enabling more sophisticated representations of code structure and semantics.
Recently, advancements in LLMs for text generation have spurred the emergence of specialized code-focused LLMs, including CodeT5+~\cite{wang2023codet5+}, InCoder~\cite{fried2022incoder}, StarCoder~\cite{Li2023StarCoderMT}, Code Llama~\cite{Roziere2023codellama}, WizardCoder~\cite{luo2023wizardcoder}, Qwen-Coder~\cite{hui2024qwen2.5coder}, and DeepSeek-Coder~\cite{guo2024deepseek}. 
Despite recent advances, LLMs still struggle to keep pace with rapidly evolving programming knowledge. This paper explores methods for integrating dynamic knowledge, enabling LLMs to synchronize with the ongoing developments in programming languages, frameworks, and best practices.

% Present models rely on pre-existing data and cannot autonomously update their knowledge, necessitating regular synchronization with new information to maintain accuracy and relevance. 
% This issue emphasizes the need for ongoing knowledge integration to keep models aligned with the latest developments in the field.

\parabf{LLMs for Code Generation} Recently, LLMs such as the commercial/black-box GPT-4~\cite{openai2024gpt4}, Gemini~\cite{geminiteam2024geminifamilyhighlycapable}, and open-source models like Qwen-Coder~\cite{hui2024qwen2.5coder}, Code Llama~\cite{Roziere2023codellama}, and DeepSeek-Coder~\cite{guo2024deepseek}, have demonstrated impressive capabilities in generating high-quality code. 
% These models are typically fine-tuned on large-scale code corpora to enhance their performance. 
Building on these LLMs, several products, including Copilot~\cite{GitHub2022copilot} and Cursor~\cite{cursor}, have been developed. 
However, the security risks posed by outdated APIs are often overlooked, and existing studies on code knowledge update task have significant limitations. 
For example, the benchmark proposed by~\citet{liu2024codeupdatearena} generates API update pairs by prompting ChatGPT~\cite{openai2024gpt4o} rather than collect authentic APIs. 
\citet{li2024modeleditingllms4codefar} construct a instruction benchmark where the subject and object of knowledge are vaguely defined but applied knowledge model editing techniques to model tuning.
In this paper, we aim to benchmark knowledge updating methods for real-world API updates using authentic GitHub releases.
% , while developing \method, a data engine to automatically synchronize with the latest code knowledge.
% \textcolor{red}{
% These defects motivate us to focus on benchmarking knowledge updating methods for real-world API updates with authentic updates released on GitHub.
% Additionally, since code knowledge evolve rapidly, benchmark can quickly become outdated. To address it, we take a further step by developing a data engine, \method, designed to automatically construct and synchronize benchmarks with latest code knowledge.
% (shorten it to one sentence.)
% }


\parabf{Knowledge Updating for LLMs}
% LLMs are typically trained on data from a specific time period, preventing them from learning emerging information. As a result, they accumulate outdated or incorrect knowledge. Retraining these models is costly and impractical due to the high computational expense of pre-training. 
% To address this, knowledge updating techniques have been developed to modify or extend the LLMs' knowledge, enable it to incorporate new information without losing existing capabilities. 
LLMs often rely on data from a specific time period, leading to outdated knowledge that retraining can not easily fix due to its high computational cost. To address this, knowledge updating techniques offer a more efficient way to integrate new information without sacrificing the model's current capabilities.
One approach is supervised fine-tuning (SFT)~\cite{liu2024codeupdatearena, peng2023instruction}, which optimizes model parameters to integrate new knowledge directly. 
Other methods treat new knowledge as preferred behavior over outdated information, such as  reinforcement learning from human feedback (RLHF) methods~\cite{schulman2017ppo, meng2024simpo, rafailov2023dpo, hong2024orpo}, which is efficient for refining model behavior to align with new knowledge. 
% For more adaptive updates, reinforcement learning from human feedback (RLHF) methods ~\cite{schulman2017ppo, meng2024simpo, rafailov2023dpo, hong2024orpo} are efficient in refining model behavior for aligning with new knowledge. 
% Instead of tuning all parameters of the model, knowledge model editing (KME) techniques~\cite{meng2022rome, hartvigsen2023grace, meng2023memit} focus on modifying only significant parts. 
Knowledge neuron theory~\cite{dai2022knowledgeneuron} takes a further step by formulating knowledge as a tuple $\{s, r, o\}$, where $s$, $r$, and $o$ represent the \textbf{subject}, \textbf{relation}, and \textbf{object} of knowledge, respectively.
% Based on this, 
% knowledge model editing~\cite{meng2022rome, hartvigsen2023grace, meng2023memit} was proposed as a more cost- and time-efficient technique for knowledge updating.
% This line of methods first identifies pivot neurons associated with new knowledge and then optimize these neurons without destroying LLMs' capabilities. 
% However, \citet{li2024modeleditingllms4codefar} demonstrate that many of KME methods are ineffective and lack generalization. 
Based on this, knowledge model editing~\cite{meng2022rome, hartvigsen2023grace, meng2023memit} emerge as a more cost-effective and time-efficient approach for updating knowledge. These methods first identify key neurons linked to the new knowledge and then optimize them, carefully preserving the language model’s overall capabilities. However, \citet{li2024modeleditingllms4codefar} reveal that many KME techniques struggle with effectiveness and fail to generalize.

% Despite these advancements, existing studies often overlook the security risks posed by outdated APIs \citep{liu2024codeupdatearena,li2024modeleditingllms4codefar}, which motivates our focus on benchmarking knowledge updating methods for real-world API updates with authentic updates released on GitHub.

\parabf{Data Synthesized by LLMs} LLMs have demonstrated an impressive capacity for data generation, leading to their application in creating synthetic datasets for pretraining and finetuning, replacing the labor-intensive processes of manual data scraping and selection~\citep{liu2024best}.
Distinct from earlier methods that focus on traditional language models~\citep{schick2021generating}, LLMs offer enhanced prospects for producing high-quality synthetic data across a wide spectrum of applications, such as multilingual QA~\citep{riabi-etal-2021-synthetic}, chatbot conversation~\citep{zhao2023inthe,zhang-etal-2024-llm} and data diversity augmentation~\citep{dai2025auggpt,chung2023increasing,chen2024interleaved}. 
The concept of synthetic benchmarks takes a step further by demanding that the LLM-generated data be diverse accurate and systematically challenging~\citep{chen2025guiworld,wu2024unigen}. Moreover, synthetic benchmarks have also been constructed in evaluating LLM emergent capabilities such as trustworthiness~\citep{huang2024trustllm, ye2024justice,gao2024best}, persona-based conversation~\citep{jandaghi2023faithful}, and multimodal domain~\citep{zhang2024task,bao2024autobench,chen2024mllm}. Our research advances synthetic benchmark for code generation by developing a paradigm that integrates three challenging code generation tasks. Recently, in response to concerns about the quality of synthetic datasets, \citet{dekoninck2024understanding} conduct comprehensive experiments to evaluate the diversity and fidelity of synthetic data produced by LLMs, while \citet{dekoninck2024controlled} introduce a new inference framework, model arithmetic, to control the generated content.

% \clearpage
\section{Detailed Experiment Setups}
\subsection{Dataset}
\subsubsection{API Collection}
\label{appx_api_collection}

The initial step of \method pipeline involves collecting APIs from various libraries. To achieve this, we utilize the Python built-in module \textit{inspect}, which enables us to navigate through library files and compile a comprehensive list of all available APIs. In this part, we will delve into the detailed process of how to collect APIs comprehensively from libraries.

\parabf{C-extension APIs} C-extension methods and functions are a powerful feature in Python programming that are employed in many third-party libraries, (\emph{e.g.}, NumPy, PyTorch), to accelerate execution efficiency. One of the key feature of C-extension functions and methods is their support for function overloading. Function overloading allows a single API name to be used with multiple different parameter lists, or signatures. This means to collect various versions of signatures for each API.

\parabf{Inspect Module} Python built-in module, \textit{Inspect}, provides several useful functions for introspecting live objects, such as functions, classes, and modules. It allows us to retrieve information about source code of Python objects, such as signature, arguments and documentation. 

\parabf{Categories} Python offers a diverse range of APIs, each designed for specific purposes and governed by distinct invocation rules. In this study, we focus on three primary types: function APIs, method APIs, and initializer APIs. 
These categories not only highlight Python's core capabilities but also exhibit unique characteristics and behaviors. Function APIs are standalone entities that can be invoked without requiring a class or instance context. In contrast, method APIs are inherently tied to class instances, leveraging encapsulation and object-oriented programming principles. The invocation rules for methods differ significantly from those for functions, reflecting their object-oriented nature. Additionally, Python provides several magic methods that are denoted by double underscore (`\_\_') at the beginning and end of their names. Among these, initializers (\emph{i.e.}, `\_\_init\_\_') are the most commonly used, serving as a method for object creation and initialization. To evaluate and benchmark Python APIs evolution comprehensively, we select representatives from these three categories to construct our benchmark \benchmark.

\subsubsection{Identifying API updates}
\label{appx_api_update_identification}

\parabf{Multiple Types of Parameter} 
The three fundamental types of parameters are \textit{positional-only parameter}, \textit{keyword-only parameter} and \textit{positional \& keyword parameter}. The term `positional' refers to parameters that can be passed only according to its position in definition. `Keyword' is the name of parameter in the function signature, allowing passing parameter with marking it explicitly instead of position. There are two special symbols in API signatures (\emph{e.g.}, *, /). Parameters set before `*' are \textbf{positional-only parameters}, which must be passed in order according to theirs positions in definition, and parameters located after `/' are \textbf{keyword-only parameters}, requiring marking parameter name when used; otherwise, a syntax error will occur. Additionally, parameters can be also categorized according to default values into 2 types, \textbf{required parameters} and \textbf{optional} parameters. Therefore, changes of parameter types have impact on invocation rules, which should be considered when determining API update operations.

\parabf{API Update Determination} 
How to determine API update operations? 
The most straightforward changes include the addition or deletion of parameters. 
A more nuanced level of analysis involves examining changes in parameter types as these alterations can significantly impact the rules for invoking APIs. 
Therefore,  API updates can be categorized into 2 primary aspects, \textbf{the addition of deletion of parameters} and \textbf{changes in parameter types}. 
To effectively identify API updates, it is crucial to focus on parameter changes, including both the mapping relationships between parameters and modifications to their types. 
To systematically capture these changes, we construct \textbf{parameter mappings} for each pair of APIs, establishing connections between corresponding parameters in the outdated and latest version of their signatures. 
Specifically, parameter mapping enables categorize two distinct aspects. 
First, if a parameter mapping can be successfully constructed , it implies that all parameters are consistently present in both versions of signatures, indicating no additions or deletions. 
Following this, the next step involves a detailed examination of each parameter pair within mappings, focusing on comparing their attributes to identify any modifications or differences. 
This approach enables a clear and structured understanding of how APIs involve over time. 

\parabf{Parameter Renaming} 
Static analysis, however, has inherent limitations, especially in cases where parameter renaming occurs. 
It is challenging to infer changes in functionality solely based on parameter names.
For example, in \texttt{transformers==4.47.0}, the API \texttt{transformers.pipelines.get\_task} has a parameter named \texttt{use\_auth\_token}, whereas the keyword of this parameter was \texttt{token} in version \texttt{transformers==4.25.1}. 
In spite of the same functionality, renaming makes it impossible to recognize their equivalence solely by analyzing signatures.
In this process, we assume that keywords of parameters are strongly connected to their functionality. The similarity between keywords suggests the similarity of their functionality. Instead of excluding all of name modification situations, we first set a threshold and compute the keyword similarity scores to account for some simple modifications. Based on this, we will then construct parameter mapping according to keyword mappings for further explorations.

\parabf{Establishing Parameter Mappings} 
However, the inherent complexity of Python API signatures poses significant challenges in accurately establishing parameter mappings.
To address this, we establish three rules that must be satisfied to determine whether no modification has occurred.
Python introduces two special symbols (`/' and `*'), which divide parameters into three categories, \textbf{positional-only}, \textbf{keyword-only} and \textbf{positional-and-keyword} parameters. 
Specifically, we construct three individual parameters mappings for these types of parameters and establish three rules that must be satisfied to determine whether no modification has occurred.
\vspace{-10pt} 
\begin{itemize}[leftmargin=4mm, noitemsep]
    \item \textbf{Rule 1: Successful Parameter Mapping.} A valid parameter mapping must be constructed, ensuring that both the number of parameters and their corresponding keywords remain identical across different signatures.

    \item \textbf{Rule 2: Type-Specific Consistency.} Each parameter type must follow specific rules:
    \begin{itemize}
        \item For \textbf{positional-only} parameters, the order of parameters in the function definition must remain strictly unchanged across signatures.
        \item For \textbf{keyword-only} parameters, the parameter names (keywords) must remain consistent to preserve their correspondence.
        \item For \textbf{positional-and-keyword} parameters, both the order requirement and keyword consistency must be satisfied simultaneously.
    \end{itemize}
    \item \textbf{Rule 3: Required vs. Optional Parameters.} Parameters can be further categorized into two types: \textbf{required} parameters, which must be provided when invoking APIs, and \textbf{optional} parameters, which have default values. While revisions to default values are not considered API updates, the type of a parameter must remain unchanged.
\end{itemize}
\vspace{-10pt} 
These rules collectively provide a practical methodology for evaluating parameter modifications and determining API consistency, which is a crucial part of \method implementing completely autoamted pipeline.

\subsubsection{API Invoking Instances Crawling}
\label{appx_api_invocation_retrieval}

% \parabf{Code as the carrier of API knowledge} 
After obtaining updated APIs along with corresponding information, it is necessary to crawl API invocations from ground truth which will be used to inject API knowledge into LLM for further exploration. Actually, directly feeding signature to models for tuning is unlikely to be effective, and limited to reflect comprehensive information, such as invoking rules, which is hard to be formulated. Therefore, we collect a large dataset of invocation instances to implicitly reflecting relative knowledge. 
% After obtaining APIs with different signatures within two versions, it is necessary to update the model's knowledge of the API signatures. To update the model's knowledge of API signatures, directly feeding the signatures to the model for learning is unlikely to be effective, as the model may not properly utilize the new APIs to generate code, and the pre-trained model uses code as the carrier of API knowledge. Therefore, we need to collect a large dataset of code that invokes these APIs, to facilitate subsequent training and testing phases.

\parabf{Real-World API Invocation} Synthesizing invocation completely relied on LLM is a convenient method for constructing dataset. 
However, this method exists inherent limitations. 
For example, information implied in context of generated code is insufficient and the contextual scenario is restricted to LLMs' embedded knowledge. 
The inevitable bias therefore poses challenges to comprehensively reflect authentic invoking rules and habits.
Instead of synthesizing invocations, we try to crawl code from GitHub with the help of GitHub Code Search, a Code Search Engine developed by GitHub to effectively aggregate repositories or files using regular expression. 
% While regular expression might capture files where no statement invokes target API, we design \textbf{Search templates} for each library to enhance the percentage of API invocations among crawled files. 
Additionally, We involve \textbf{search templates} as shown in ~\ref{search template}, to enhance the effectiveness of invocation retrieval 

\parabf{Search Templates} 
\label{search template}
Python allows aliases declaration of import statements to simplify usage of third-party modules and APIs.
In the authentic programming scenario, directly invoking APIs with full name fails to align with developers' programming habits. 
We therefore design a set of templates for each library to expand searching scope. For example, while the module \texttt{torch.nn.functional} is imported, these statements might exist:

\begin{tcolorbox}[colback=gray!5!white,width=\linewidth, left=-3mm, right=-3mm, top=1mm, bottom=1mm, center]
 \begin{quote}
1. import torch.nn.functional as F

2. from torch.nn import functional as F
\end{quote}
\end{tcolorbox}

For any field in the API name (a segment separated by dots), an alias can be assigned and there are two formats: \texttt{import as} and \texttt{from import}. Based on these characteristics, we can generate a series of searching templates. Templates of \texttt{torch.nn.functional.softmax} are shown as below:
\begin{tcolorbox}[colback=gray!5!white,width=\linewidth, left=-3mm, right=-3mm, top=1mm, bottom=1mm, center]
 \begin{quote}
1. ``torch.nn.functional.softmax" (directly match)

2. ``import torch as" + ``.nn.functional.softmax" 

3. ``from torch import nn" + ``.functional.softmax"

4. ``import torch.nn as" + ``.functional.softmax"

5. ``from torch.nn import functional" + ``.softmax"

6. ``import torch.nn.functional as" + ``.softmax"

7. ``from torch.nn.functional import softmax"
\end{quote}
\end{tcolorbox}

In the second template, we match ``import torch as" instead of ``import torch". This is because when the module is imported without an alias (e.g., simply ``import torch"), the full path ``torch.nn.functional.softmax" will be directly used in the code. 
For \textbf{function} APIs and \textbf{initializer} APIs, the above patterns can be directly applied for decomposition. 
We next utilize GitHub Code Search to retrieve code that contains all segments for each template (with an upper limit of 500 files).

Different from function and initializer, \textbf{method} APIs requires a further step due to dynamic binding mechanism. 
A method API can be divided into two parts: \textbf{class name} and \textbf{method name}. For example, \texttt{torch.Tensor} and \texttt{shape} are class name and method name of \texttt{torch.Tensor.shape}, respectively.
In the most programming scenario, Python objects lack explicit type definitions. 
To align with subsequent procedures, we only take one specific situation into consideration where both type declaration and API invocation exist in the same file simultaneously.
Searching templates can be applied on method APIs retrieval as well, while an additional segments, \underline{\texttt{f".\{method name\}("}},should be included. 
For API \texttt{torch.Tensor.shape}, each template will include \underline{\texttt{".shape("}}.
Explicit type declarations will be clarified in~\cref{appx_api_invocation_filtering}.

\subsubsection{Locating Valid API Invocations}
\label{appx_api_invocation_filtering}

After retrieving a dataset of files that contain relative substring of target API invocation, further filtering is required to identify code that genuinely invokes the target API. 
The following illustration is divided into two parts: \textbf{function / initializer APIs locating} and \textbf{method APIs locating}.

\parabf{Function / Initializer APIs Locating}
Initializer APIs share similar invoking rules with those of function APIs. 
We can use abstract syntax tree(AST) to analyze crawled files for locating the target API invocations.
Specifically, this part contains two steps:
\textbf{(1) Alias Mapping}: 
We scan the import statements and construct mappings between original library/module name and aliases.
\textbf{(2) Invocation Analysis}:
Based on alias mapping, we traverse the AST of files and analyze each invocation statement to determine whether the target API are invoked. 
The start \& end line number of invocations will be recorded for subsequent process.

\parabf{Method APIs Locating} 
Invocations of method APIs are often associated with class instances. 
To determine method API invocations, we need to infer the types of variables that invoke the methods. 
However, variables are dynamically bound to types during program execution. We therefore focus on situations where the types of variables can be statically inferred from the raw code. 
There are three situations:
% \vspace{-0.7em}
\begin{itemize}
    \item Variables are assigned by using initializer of target class.
    \vspace{-0.6em}
    \item Type annotations are provided in function definitions.
    \vspace{-0.6em}
    \item Function definitions provide return type annotations.
\end{itemize}
The first step is to scan the whole file to record types of variables as well as their scopes. We next traverse the AST, tracking target class instances in their own scope to identify methods they invoked. 

\parabf{Format Conversion} 
After locating and recording API invocations in each file, we perform two operations to split the data:
\textbf{(1) Segment Split}: 
Treating the entire file as a single dataset item is inefficient and redundant.To better utilize the crawled files, we split each file into multiple segments based on function definition. In other words, each segment corresponds to a complete function definition and is treated as an individual dataset item. 
\textbf{(2) Metadata Convert}: 
Each segment is then further divided into three parts: \textbf{code context}, \textbf{target sequence} and \textbf{code suffix}. The code context is the prompt in subsequent tasks. To avoid knowledge leaking, the target sequence is the first invocation of target API within the segment.
These split operations allow for more efficient processing and better representation of the code's structure, ultimately improving the dataset's usability for subsequent tasks.

\subsection{Models}
\parabf{Qwen-2.5-7B-Instruct}
A 7-billion parameter instruction-tuned model designed for general-purpose tasks, offering robust performance across various applications by following user instructions effectively.

\parabf{Qwen-2.5-Coder-7B-Instruct}
A specialized 7-billion parameter model tailored for coding-related tasks, excelling in code generation, debugging, and understanding programming languages through instruction-following capabilities.

\parabf{Llama-3-8B-Instruct}
An 8-billion parameter instruction-tuned model built for versatile applications, providing strong performance in natural language understanding and task execution based on user instructions.

\parabf{CodeLlama-7B-Instruct}
A 7-billion parameter model fine-tuned for coding tasks, optimized for generating, analyzing, and refining code while adhering to user-provided instructions.

\parabf{DeepSeek-Coder-6.7B-Instruct}
A 6.7-billion parameter model specifically designed for coding and programming tasks, leveraging instruction-tuning to deliver accurate and efficient code-related solutions.

\subsection{Knowledge Updating Methods}
\subsubsection{\textsc{Direct Preference Optimization (DPO)}}

Traditional reinforcement learning algorithms (\emph{e.g.}, PPO~\cite{schulman2017ppo}) introduce reward models to guide LLMs to align with human preferences. While these methods exhibit superior performance in many fields, they suffer from extremely high computational costs and require a large amount of training data to optimize policy of reward models. To accelerate the process of training, DPO directly optimizes the model's policy to align with human preferences by leveraging pairwise comparison data. Each data pair consists of a preferred sample $\mathbf{y}_i^+$ and a dispreferred sample $\mathbf{y}_i^-$ for a given input $\mathbf{x}_i$. DPO adjusts the model to increase the likelihood of generating preferred outputs while reducing the probability of dispreferred ones. By implicitly encoding preference rankings into the objective function, DPO eliminates the need for explicit reward modeling or complex reinforcement learning pipelines, offering a simpler and more stable training framework.

The key insight of DPO is to reframe preference learning as a supervised likelihood optimization problem. Given preference pairs $(\mathbf{x}_i, \mathbf{y}_i^+, \mathbf{y}_i^-)$, the objective maximizes the log-likelihood difference between preferred and dispreferred outputs:
\[
\mathcal{L}_{\text{DPO}} = \sum_{i} \log \sigma\left(\log \frac{\pi_\theta(\mathbf{y}_i^+|\mathbf{x}_i)}{\pi_{\text{ref}}(\mathbf{y}_i^+|\mathbf{x}_i)} - \log \frac{\pi_\theta(\mathbf{y}_i^-|\mathbf{x}_i)}{\pi_{\text{ref}}(\mathbf{y}_i^-|\mathbf{x}_i)}\right)\,,
\]
where $\sigma$ denotes the sigmoid function and $\pi_{\text{ref}}$ represents the reference policy. This formulation ensures the model assigns higher probabilities to preferred responses relative to the reference policy while maintaining generation diversity through implicit regularization.

\subsubsection{\textsc{Odds Ratio Preference Optimization (ORPO)}}
ORPO introduce \textit{Odd Ratio} to quantify the preference learning. Specifically, it enhances preference learning by explicitly optimizing the odds ratio between preferred and dispreferred responses. The loss function combines log-odds maximization with KL-divergence regularization:
\[
\mathcal{L}_{\text{ORPO}} = \sum_{i} \log \frac{\pi_\theta(\mathbf{y}_i^+|\mathbf{x}_i)}{\pi_\theta(\mathbf{y}_i^-|\mathbf{x}_i)} - \lambda \cdot \text{KL}\left(\pi_\theta \| \pi_{\text{ref}}\right)\,,
\]
where $\lambda$ controls the regularization strength. This dual objective encourages preference alignment while preventing excessive deviation from the reference policy, addressing the exploration-exploitation trade-off inherent in policy optimization. ORPO's probabilistic framing improves sample efficiency in low-data regimes and enhances robustness to noisy preference labels.

\subsubsection{Simple Policy Optimization (SimPO)}
SimPO extends the paradigm of DPO through architectural simplifications that enhance both training efficiency and alignment precision. At its core, SimPO reinterprets the alignment task as a margin maximization problem, where the model learns to maintain a specified quality gap between preferred and dispreferred responses. This is achieved through two synergistic mechanisms:

\textbf{Dynamic Length Normalization}: Traditional probability-based rewards inherently favor longer sequences due to multiplicative probability chains. SimPO counteracts this bias by computing rewards as \textit{length-normalized} token probabilities:
$$R_\theta(y|x) = \frac{\beta}{|y|} \sum_{t=1}^{|y|} \log \pi_\theta(y_t|x, y_{<t})\,,$$ 
where the normalization factor $|y|$ (response length) ensures equal contribution per token, preventing length-based reward inflation. This design choice proves critical in tasks requiring concise yet high-quality responses, such as technical question answering or summarization.

\textbf{Adaptive Margin Enforcement}: Rather than relying on fixed hyperparameters, SimPO implements an intelligent margin threshold $m$ that interacts with the reward difference $\Delta R_\theta = R_\theta(y^+|x) - R_\theta(y^-|x)$:
    \[
    \mathcal{L}_{\text{SimPO}} = \sum_{i} \max\left(0, m - \Delta R_\theta(x_i)\right)\,.
    \]
The margin mechanism creates three distinct learning phases:
\begin{enumerate}
    \item \textit{Active Learning}: When $\Delta R_\theta < m$, gradients actively push the model to widen the reward gap
    \item \textit{Saturation Control}: Once $\Delta R_\theta \geq m$, gradient flow ceases to prevent over-optimization
    \item \textit{Implicit Regularization}: The margin $m$ automatically scales with batch statistics, adapting to varying preference strengths
\end{enumerate}
By eliminating reference policy computations and reward modeling, SimPO achieves faster convergence while maintaining competitive performance. The margin-based objective automatically suppresses gradient updates when preference distinctions become clear, preventing overoptimization and reducing computational overhead. This makes SimPO particularly effective for aligning LLMs with limited computational resources.

\clearpage
\section{Prompts}
\subsection{Prompt to update code legacy}
\label{appx_update_code_prompt}
\begin{tcolorbox}[colback=gray!5!white,width=\linewidth, left=-3mm, right=-3mm, top=1mm, bottom=1mm, center]
\begin{quote}
\small
{
I will provide a code snippet as the context, followed by a calling statement that contains a target API call and a suffix. Additionally, the latest and outdated function signatures of the API are accessible(referred to as latest\_signature and outdated\_signature). Your task is to update the calling statement according to both the latest and outdated API function signatures, producing two distinct answers: the "latest answer" and the "outdated answer". \\
---\\
You must adhere to the following guidelines: \\
1. Calling Statement Updates: Only update the calling statement based on the given signatures, ensuring the functionality and correctness of the calls.\\
2. Include Required Parameters: The updated calling statements should include only the required parameters from the API signatures. Optional parameters should only be included if they are explicitly used or necessary based on the provided code context.\\
3. Avoid Unnecessary Defaults: Do not include default values for optional parameters unless they are explicitly mentioned in the code or are necessary for functionality.\\
4. Reflect API Updates: Clearly showcase the differences between the latest and outdated API signatures through your modifications.\\
---\\
Latest API Signature: {[updated\_signature]}\\
Outdated API Signature: {[outdated\_signature]}\\
Context: {[context]}\\
Statement: {[target\_seq]}\\
suffix: {[suffix]}\\
% ``Please use the latest API given below to modify the first occurrence of the target API call in the code snippet, ensuring that all other statements remain unchanged.'' \\
% ``Latest API Signature: {[API\_PATH]}, Code: {[CODE]}''
}
\end{quote}
\end{tcolorbox}

\subsection{Prompt to Generate Wrong Choices for MCQ}
\begin{tcolorbox}[breakable, colback=gray!5!white,width=\linewidth, left=-3mm, right=-3mm, top=1mm, bottom=1mm, center]
\begin{quote}
\small
{
I want to create a multiple-choice question where, based on a specific code context, we identify the most appropriate parameter list for the target API. I will provide you with the following information:
\begin{itemize}
    \item \texttt{API\_path}: The full name of the API
    \item \texttt{updated\_signature}: The API's new signature
    \item \texttt{outdated\_signature}: The API's old signature
    \item \texttt{import}: The import statements in the code
    \item \texttt{context}: The preceding code context, ending with the target API's name
    \item \texttt{updated\_code}: The correct answer that matches the new signature
    \item \texttt{outdated\_code}: The incorrect answer that matches the old signature
\end{itemize}
I want to construct a multiple-choice question with four options. Among these, \texttt{updated\_code} will be the correct option, and \texttt{outdated\_code} is one incorrect option I have already provided. You need to create two additional incorrect options based on the differences between the new and old signatures—specifically, options that would be “misleading” if a model is still relying on the old signature. In other words, if the model only knows the old signature, it might be inclined to select these incorrect answers.

Here are four possible approaches for crafting these additional incorrect options:

\begin{enumerate}
    \item Remove some optional parameters from the correct answer (that is, \texttt{updated\_code}).
    \item Add some incorrect optional parameters, such as parameters that existed in the old signature but do not exist in the new one, or parameters that appear in neither signature (the name of these parameters should not be like \texttt{extra\_param}, which can be judged to error very easily).
    \item Rearrange the positions of any positional parameters based on \texttt{updated\_code}.
    \item Change parameter names, for example changing \texttt{add(x: int)} to something like \texttt{add(z=3)}.
\end{enumerate}

\textbf{WARNING}: Your two new incorrect options MUST differ from \textbf{both} \texttt{updated\_code} and \texttt{outdated\_code} that I give to you, as well as from EACH OTHER.

\textbf{Output Format:}

Provide your two new incorrect options as your answer, \textbf{without} any other output.

For example:
\begin{verbatim}
############ Your output ##############

Option 1: (paramA, paramB=123)

Option 2: (paramX="hello")

#######################################
\end{verbatim}
---\\
API\_path: {[API\_path]}\\
updated\_signature: {[updated\_signature]}\\
outdated\_signature: {[outdated\_signature]}\\
import: {[import]}\\
context: {[context]}\\
updated\_code: {[updated\_code]}\\
outdated\_code: {[outdated\_code]}\\
}
\end{quote}
\end{tcolorbox}

\clearpage
\section{Experiment Settings}

\subsection{Metrics}
\label{metrics}
\subsubsection{BLEU Metric}
\label{belu}
The BLEU score is used to evaluate the quality of generated text by comparing it to one or more reference texts. It is based on the precision of $n$-grams (contiguous sequences of words) in the generated text, with a brevity penalty to penalize overly short outputs.
The BLEU score is calculated as follows:
\[
\text{BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)\,,
\]
where
\begin{itemize}
    \vspace{-0.5em}
    \item \( BP \) is the \textbf{brevity penalty}, defined as:
    \[
    BP = 
    \begin{cases} 
    1 & \text{if } c > r \\
    e^{(1 - r/c)} & \text{if } c \leq r \,.
    \end{cases}
    \]
    Here, \( c \) is the length of the candidate (generated) text, and \( r \) is the length of the reference text.
    \vspace{-0.5em}
    \item \( p_n \) is the \textbf{n-gram precision}, calculated as:
    \[
    p_n = \frac{\text{Number of matching n-grams in candidate and reference}}{\text{Total number of n-grams in candidate}}\,,
    \]
    \vspace{-0.5em}
    \item \( w_n \) is the weight for the \( n \)-th n-gram precision, typically set to \( \frac{1}{N} \) for uniform weighting.
    \vspace{-0.5em}
    \item \( N \) is the maximum $n$-gram order (usually 4 for BLEU-4).
\end{itemize}
The BLEU score ranges from $0$ to $1$, where $1$ indicates a perfect match with the reference text and $0$ indicates no overlap with the reference text.

\subsubsection{ROUGE Metric}
The ROUGE metric is used to evaluate the quality of generated text by comparing it to one or more reference texts. It focuses on recall, measuring how much of the reference text is captured by the generated text. ROUGE has several variants, including ROUGE-N ($n$-gram overlap), ROUGE-L (longest common subsequence), and ROUGE-W (weighted longest common subsequence). 
In our experiments, we use ROUGE-L as the metric.

The ROUGE-L score is based on the longest common subsequence (LCS) between the candidate and reference texts. It is defined as:
\[
\text{ROUGE-L} = \frac{\text{LCS}(C, R)}{\text{Length}(R)}\,,
\]
where
\begin{itemize}
    \vspace{-0.5em}
    \item \( \text{LCS}(C, R) \) is the length of the longest common subsequence between the candidate text \( C \) and the reference text \( R \).
    \vspace{-0.5em}
    \item \( \text{Length}(R) \) is the length of the reference text.
\end{itemize}
The ROUGE score ranges from $0$ to $1$, where $1$ indicates that the candidate text perfectly captures the reference text and $0$ indicates no overlap with the reference text.

\subsubsection{Relative Edit Distance Metric}
The Relative Edit Distance (RED) is a normalized metric used to measure the dissimilarity between two strings. It is calculated as the edit distance (e.g., Levenshtein distance) between the two strings divided by the length of the longer string. This normalization ensures that the metric is scale-invariant and ranges between 0 and 1.

The RED is defined as:
\[
\text{RED} = \frac{\text{EditDistance}(S_1, S_2)}{\max(|S_1|, |S_2|)}\,,
\]
where
\begin{itemize}
    % \vspace{-0.5em}
    \item \( \text{EditDistance}(S_1, S_2) \) is the Levenshtein distance between strings \( S_1 \) and \( S_2 \), which measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform \( S_1 \) into \( S_2 \).
    \vspace{-0.5em}
    \item \( |S_1| \) and \( |S_2| \) are the lengths of strings \( S_1 \) and \( S_2 \), respectively.
    \vspace{-0.5em}
    \item \( \max(|S_1|, |S_2|) \) is the length of the longer string, used to normalize the edit distance.
\end{itemize}
The RED score ranges from $0$ to $1$, where $0$ indicates that the two strings are identical (no edits are needed) and $1$ indicates that the two strings are completely dissimilar (every character needs to be edited).

\subsubsection{Pass@k Metric}

The \text{Pass@}$k$ metric is a performance evaluation metric used to assess the quality of code generation models. It measures the probability that at least one correct solution is generated within the top \( k \) samples produced by the model. This metric is particularly useful for evaluating models in scenarios where multiple candidate solutions are generated, and the goal is to determine how often the model produces a correct solution within a limited number of attempts.

Given a set of \( n \) generated samples for a problem, the Pass@$k$ metric is calculated as follows:
\[
\text{Pass@}k = \frac{\text{Number of problems with at least one correct solution in the top } k \text{ samples}}{\text{Total number of problems}}\,.
\]
Alternatively, if the model generates \( k \) samples per problem, the Pass@$k$ metric can be computed as:
\[
\text{Pass@}k = \mathbb{E}_{\text{problems}} \left[ 1 - \frac{\binom{n - c}{k}}{\binom{n}{k}} \right]\,,
\]
where
\begin{itemize}
    \vspace{-0.5em}
    \item \( n \) is the total number of samples generated per problem.
    \vspace{-0.5em}
    \item \( c \) is the number of correct solutions among the \( n \) samples.
    \vspace{-0.5em}
    \item \( \binom{n - c}{k} \) is the number of ways to choose \( k \) samples that do not contain any correct solutions.
    \vspace{-0.5em}
    \item \( \binom{n}{k} \) is the total number of ways to choose \( k \) samples from \( n \).
\end{itemize}
The Pass@$k$ metric ranges from $0$ to $1$, where $1$ indicates that at least one correct solution is always found within the top \( k \) samples and $0$ indicates that no correct solution is ever found within the top \( k \) samples.

\subsection{RQ2. Experiment Settings}
\label{rq2_exp_settings}
In the process of RQ2, we train five open-source models using five knowledge update techniques, and evaluate trained models on \benchmark. In this section, we show the detailed experiment settings as follows.

\subsubsection{Model Training}
\parabf{Knowledge Update Methods} 
Supervised Fine-Tuning (SFT) is a widely used and traditional method for modifying and aligning model knowledge, relying on labeled data to train models.
For the SFT training dataset, the $context$ in metadata serves as the prompt, and the $updated\_data$ serves as the target sequence.
We also evaluate three instruction tuning methods (e.g., DPO~\cite{rafailov2023dpo}, ORPO~\cite{hong2024orpo}, SimPO~\cite{hong2024orpo}) to update the knowledge, relying on positive-negative data pairs to train models.
For their training datasets, we use $updated\_code$ and $outdated\_data$ as the positive and negative target sequences respectively.
We use LoRA for all instruction tuning experiments~\citep{hu2021lora} based on LoRA SFT on A800 servers. 

We adopt five knowledge update techniques: SFT, SFT (LoRA), DPO, ORPO, SimPO. Additionally, LoRA training requires less computation resources and is possessed of high efficiency.

We train DPO, ORPO and SimPO using LoRA techniques, which is more efficient than that of full training. 
% We uses automated tuning framework, LLaMA-Factory~\cite{zheng2024llamafactory}, which is easy-to-use and steady.
We use LLaMA-Factory~\cite{zheng2024llamafactory}, a user-friendly and reliable automated tuning framework.

% \parabf{Hyperparameter}
% \begin{table}[h]
%     \centering
%     \begin{tabular}{lcccc}
%     \hline
%     \textbf{Techniques} & \textbf{Epoch} & \textbf{Learning Rate} & \textbf{Warmup Ratio} & \textbf{Preference Beta} \\ \hline
%     SFT            & 3 & 1.0e-4   & 0.1          &               \\ 
%     SFT (LoRA)     & 3 & 1.0e-4   & 0.1          &               \\ 
%     DPO            & 3.5 & 5.0e-6   & 0.1          & 0.1           \\ 
%     OPPO           & 3.5 & 5.0e-6   & 0.1          & 0.1           \\
%     SimP0          & 3.5 & 5.0e-6   & 0.1          & 0.1           \\ \hline
%     \end{tabular}
%     \caption{Hyperparameters for RQ2}
%     \label{rq2_hparams}
% \end{table}

\parabf{Hyperparameter}
\begin{table}[h]
\small
\setlength{\tabcolsep}{7pt} 
\caption{\textbf{RQ2. Hyperparameters for Qwen2.5-7B-Instruct}}
% \vspace{0.5em}
\centering
\begin{tabular}{l|cccc}
    \toprule
    \textbf{Techniques} & \textbf{Epoch} & \textbf{Learning Rate} & \textbf{Warmup Ratio} & \textbf{Preference Beta} \\
    \midrule
    \texttt{SFT}        & 3   & 1.0e-4 & 0.1 & --   \\
    \texttt{SFT(LoRA)}  & 3   & 1.0e-4 & 0.1 & --   \\
    \texttt{DPO}        & 3.5 & 5.0e-6 & 0.1 & 0.1  \\
    \texttt{ORPO}       & 3.5 & 5.0e-6 & 0.1 & 0.1  \\
    \texttt{SimPO}      & 3.5 & 5.0e-6 & 0.1 & 0.1  \\
    \bottomrule
\end{tabular}
\label{rq2_hparams}
\vspace{-0.5em}
\end{table}


% \parabf{MetaData Format}
% \label{Metadata}
% \vspace{-0.5em}
% \begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
% \begin{Verbatim}[fontsize=\normalsize]
% MetaData

% [API] torch.optim.swa_utils.AveragedModel.load_state_dict

% [Code Context]
% def load_model_from_state_dict(state_dict, input_dim=None):
%     model = optim.swa_utils.AveragedModel(SNN(input_dim=input_dim,
%     num_hidden_units=hidden_dim))
%     model.load_state_dict
    
% [Updated Code]  (state_dict, strict=True, assign=False)
% [Outdated Code] (state_dict, strict=True)
% \end{Verbatim}
% \end{tcolorbox}

% \parabf{Training Data Format}
% % \label{Metadata}
% \begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
% \begin{Verbatim}[fontsize=\normalsize]
% SFT Training data
% [instruction] 
% Please fill the parameter list of api
% \"torch.optim.swa_utils.AveragedModel.load_state_dict\" 
% according to the given context.

% [input]
% def load_model_from_state_dict(state_dict, input_dim=None):
%     model = optim.swa_utils.AveragedModel(SNN(input_dim=input_dim,
%     num_hidden_units=hidden_dim))
%     model.load_state_dict
    
% [output] (state_dict, strict=True, assign=False)
% \end{Verbatim}
% \end{tcolorbox}

% \begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
% \begin{Verbatim}[fontsize=\normalsize]
% DPO/ORPO/SimPO Training data
% [conversations] 
%     [from] system
%     [value] Please complete subsequent API calling statement.

%     [from] human
%     [value]
%     def load_model_from_state_dict(state_dict, input_dim=None):
%         model = optim.swa_utils.AveragedModel(SNN(input_dim=input_dim,
%         num_hidden_units=hidden_dim))
%         model.load_state_dict
        
% [chosen]
%     [from] gpt
%     [value] (state_dict, strict=True, assign=False)
 
% [rejected] 
%     [from] gpt
%     [value] (state_dict, strict=True)
% \end{Verbatim}
% \end{tcolorbox}


% % \subsubsection{Evaluation on \benchmark}

% \parabf{Code Completion Task Format}

% \begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
% \begin{Verbatim}[fontsize=\normalsize]

% [API_path] flask.json.dump
% [question]
% def test_json_dump_to_file(self):
%     app = flask.Flask(__name__)
%     test_data = {'name': 'Flask'}
%     out = StringIO()
%     with app.app_context():
%         flask.json.dump
% [answer] (test_data, out)

% \end{Verbatim}
% \end{tcolorbox}

% \parabf{Error Correct Task Format}

% \begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
% \begin{Verbatim}[fontsize=\normalsize]

% [API_path] flask.json.dump
% [question]
% def test_json_dump_to_file(self):
%     app = flask.Flask(__name__)
%     test_data = {'name': 'Flask'}
%     out = StringIO()
%     with app.app_context():
%         flask.json.dump(token_data, file, app=None)
% [answer] (token_data, file)

% \end{Verbatim}
% \end{tcolorbox}

% \parabf{Multiple Choice Question Format}

% \begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
% \begin{Verbatim}[fontsize=\normalsize]

% [API_path] flask.json.dump
% [question]
% def test_json_dump_to_file(self):
%     app = flask.Flask(__name__)
%     test_data = {'name': 'Flask'}
%     out = StringIO()
%     with app.app_context():
%         flask.json.dump
% [A] (test_data, out, app=app)
% [B] (test_data, out)
% [C] (test_data, out, app=app, indent=4)
% [D] (test_data, out, app=None)
% [answer] B

% \end{Verbatim}
% \end{tcolorbox}

\subsubsection{Evaluation on HumanEval}
We utilize the open-source project Code Generation LM Evaluation Harness~\cite{bigcode-evaluation-harness} to assess our models on the HumanEval benchmark~\cite{chen2021humaneval}. This evaluation framework provides a standardized method for measuring the code generation capabilities of LLMs.

For each evaluation, we generate 10 independent samples per problem across all 164 programming tasks in the benchmark. We then compute the Pass@1, Pass@3, and Pass@5 metrics, which measure the probability of generating a correct solution within the top 1, 3, or 5 model outputs, respectively.

To further analyze model performance, we calculate the Pass@5 ratio between the trained models and the reference models. This comparison, visualized in Figure~\ref{fig_RQ2}, serves as a diagnostic tool to monitor the effectiveness of our training experiments. The results indicate that all models perform on par with the reference models, suggesting that catastrophic forgetting is minimal. Moreover, our approach successfully injects new knowledge into the models without degrading their existing capabilities.

This evaluation provides strong evidence that our training strategy effectively balances knowledge retention and expansion, ensuring that models maintain their baseline performance while learning new information.

\subsection{RQ3-1. Experiment Settings}
\label{rq3.1_exp_settings}
Retrieving invocation instances for each API presents challenges due to the limited number of available instances, which complicates the scaling of both training sets and benchmarks. In most cases, we only have access to a small number of instances. On the other hand, a limited sample size may lead to underfitting, while a larger sample size does not necessarily equate to better performance. In this section, we evaluate the impact of sample size on model performance. 

To address this, we prepare a series of training sets, each containing the same APIs but varying numbers of samples per API. Specifically, we explore four different sample sizes: 5, 10, 20, and 50, representing different levels—low, medium, high, and very high.

We construct these training sets from the original dataset. To control the experimental conditions, all four sets are derived from the same set of APIs. Consequently, we include APIs that have more than 50 samples. We then randomly select a fixed number of samples for each API. To reduce sample quality variance, we ensure that the sets overlap. For example, the 5-sample set is fully included in the 10-sample set, and so on.

Next, we train the model Qwen2.5-7B-Instruct~\cite{qwen2.5} on these sets. Due to the limited size of the subsets, we double the number of epochs (which was set to 3 in Appendix~\ref{rq2_exp_settings}, and thus set to 6 for this experiment). To ensure convergence of the loss value, we adjust the relevant hyperparameters, as shown in Table~\ref{rq3.1-hparams}.

% \begin{table}[h]
%     \label{rq3.1-hparams}
%     \centering
%     \begin{tabular}{lccccc}
%         & \textbf{Technique} & \textbf{Eval Steps} & \textbf{Learning Rate} & \textbf{Beta} \\ \hline
%         \multirow{4}{*}{\textbf{5}} & SFT & 30 & 1.0e-5 &  \\ 
%          & DPO & 30 & 5.0e-6 & 0.3 \\ 
%          & ORPO & 30 & 5.0e-6 & 0.1 \\ 
%          & SimPO & 30 & 5.0e-6 & 0.7 \\ \hline
%         \multirow{4}{*}{\textbf{10}} & SFT & 50 & 1.0e-5 &  \\ 
%          & DPO & 50 & 5.0e-6 & 0.3 \\ 
%          & ORPO & 50 & 5.0e-6 & 0.1 \\ 
%          & SimPO & 50 & 5.0e-6 & 0.7 \\ \hline
%         \multirow{4}{*}{\textbf{20}} & SFT & 200 & 1.0e-5 &  \\ 
%          & DPO & 200 & 5.0e-6 & 0.3 \\ 
%          & ORPO & 200 & 5.0e-6 & 0.1 \\ 
%          & SimPO & 200 & 5.0e-6 & 0.7 \\ \hline
%         \multirow{4}{*}{\textbf{50}} & SFT & 500 & 1.0e-5 &  \\ 
%          & DPO & 500 & 5.0e-6 & 0.3 \\ 
%          & ORPO & 500 & 5.0e-6 & 0.1 \\ 
%          & SimPO & 500 & 5.0e-6 & 0.7 \\ 
%          % \hline
%     \end{tabular}
%     \caption{Hyperparameter setting of RQ3.1}
%     \label{tab:methods_params}
% \end{table}

\begin{table}[h]
\small
\setlength{\tabcolsep}{10pt} 
\caption{\textbf{RQ3-1. Hyperparameters for Qwen2.5-7B-Instruct across different training datasets.}}
\label{rq3.1-hparams}
\vspace{0.5em}
\centering
\begin{tabular}{ll|ccc}
    \toprule
    \textbf{Counts} & \textbf{Technique} & \textbf{Eval Steps} & \textbf{Learning Rate} & \textbf{Preference Beta} \\
    \midrule
    \multirow{4}{*}{\textbf{5}} &
      \texttt{SFT(LoRA)}  & 30  & 1.0e-5    & --  \\
    & \texttt{DPO}        & 30  & 5.0e-6    & 0.3 \\
    & \texttt{ORPO}       & 30  & 5.0e-6    & 0.1 \\
    & \texttt{SimPO}      & 30  & 5.0e-6    & 0.7 \\
    \hline
    \multirow{4}{*}{\textbf{10}} &
      \texttt{SFT(LoRA)}  & 50  & 1.0e-5    & --  \\
    & \texttt{DPO}        & 50  & 5.0e-6    & 0.3 \\
    & \texttt{ORPO}       & 50  & 5.0e-6    & 0.1 \\
    & \texttt{SimPO}      & 50  & 5.0e-6    & 0.7 \\
    \hline
    \multirow{4}{*}{\textbf{20}} &
      \texttt{SFT(LoRA)}  & 200  & 1.0e-5    & --  \\
    & \texttt{DPO}        & 200  & 5.0e-6    & 0.3 \\
    & \texttt{ORPO}       & 200  & 5.0e-6    & 0.1 \\
    & \texttt{SimPO}      & 200  & 5.0e-6    & 0.7 \\
    \hline
    \multirow{4}{*}{\textbf{50}} &
      \texttt{SFT(LoRA)}  & 500  & 1.0e-5    & --  \\
    & \texttt{DPO}        & 500  & 5.0e-6    & 0.3 \\
    & \texttt{ORPO}       & 500  & 5.0e-6    & 0.1 \\
    & \texttt{SimPO}      & 500  & 5.0e-6    & 0.7 \\
    \bottomrule
\end{tabular}
\label{tab:methods_params}
\vspace{-0.5em}
\end{table}

\subsection{RQ3-2. Experiment Settings}
\label{rq3.2_exp_settings}
LLMs demonstrate varying capabilities across different categories of APIs. To align with RQ2 (see Appendix~\ref{rq2_exp_settings}), we evaluate the trained models from RQ2 on different subsets of CCT within \benchmark. Specifically, we categorize CCT in \benchmark into three distinct groups based on API types: functions, methods, and initializers. Each trained model is assessed separately on these subsets to analyze its performance across different API structures.

To ensure a fair and robust evaluation, we set the temperature to 0.9 and generate five output samples per prompt to account for variability in model responses. The model outputs are then compared against reference answers using BLEU scores, which serve as a metric for measuring output accuracy. The results of this evaluation are presented in Figure~\ref{fig_rq3_type}, providing insights into how model performance varies across API categories.

This analysis helps us understand whether LLMs exhibit strengths or weaknesses in handling specific API types, offering valuable guidance for improving future models and fine-tuning strategies.

\clearpage
\section{Data Format}

\subsection{MetaData Format}
\label{Metadata}
\vspace{-0.5em}
\begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
\begin{Verbatim}[fontsize=\normalsize]
MetaData

[API] torch.optim.swa_utils.AveragedModel.load_state_dict

[Code Context]
def load_model_from_state_dict(state_dict, input_dim=None):
    model = optim.swa_utils.AveragedModel(SNN(input_dim=input_dim,
    num_hidden_units=hidden_dim))
    model.load_state_dict
    
[Updated Code]  (state_dict, strict=True, assign=False)
[Outdated Code] (state_dict, strict=True)
\end{Verbatim}
\end{tcolorbox}

\subsection{Training Data Format}
\subsubsection{\textsc{SFT Training Data}}
\begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
\begin{Verbatim}[fontsize=\normalsize]
SFT Training data
[instruction] 
Please fill the parameter list of api
\"torch.optim.swa_utils.AveragedModel.load_state_dict\" 
according to the given context.

[input]
def load_model_from_state_dict(state_dict, input_dim=None):
    model = optim.swa_utils.AveragedModel(SNN(input_dim=input_dim,
    num_hidden_units=hidden_dim))
    model.load_state_dict
    
[output] (state_dict, strict=True, assign=False)
\end{Verbatim}
\end{tcolorbox}

\subsubsection{\textsc{DPO/ORPO/SimPO Training Data}}
\begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
\begin{Verbatim}[fontsize=\normalsize]
DPO/ORPO/SimPO Training data
[conversations] 
    [from] system
    [value] Please complete subsequent API calling statement.

    [from] human
    [value]
    def load_model_from_state_dict(state_dict, input_dim=None):
        model = optim.swa_utils.AveragedModel(SNN(input_dim=input_dim,
        num_hidden_units=hidden_dim))
        model.load_state_dict
        
[chosen]
    [from] gpt
    [value] (state_dict, strict=True, assign=False)
 
[rejected] 
    [from] gpt
    [value] (state_dict, strict=True)
\end{Verbatim}
\end{tcolorbox}


% \subsubsection{Evaluation on \benchmark}

\subsection{Code Completion Task Format}

\begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
\begin{Verbatim}[fontsize=\normalsize]

[API_path] flask.json.dump
[question]
def test_json_dump_to_file(self):
    app = flask.Flask(__name__)
    test_data = {'name': 'Flask'}
    out = StringIO()
    with app.app_context():
        flask.json.dump
[answer] (test_data, out)

\end{Verbatim}
\end{tcolorbox}

\subsection{Error Correct Task Format}

\begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
\begin{Verbatim}[fontsize=\normalsize]

[API_path] flask.json.dump
[question]
def test_json_dump_to_file(self):
    app = flask.Flask(__name__)
    test_data = {'name': 'Flask'}
    out = StringIO()
    with app.app_context():
        flask.json.dump(token_data, file, app=None)
[answer] (token_data, file)

\end{Verbatim}
\end{tcolorbox}

\subsection{Multiple Choice Question Format}

\begin{tcolorbox}[colback=gray!5!white, width=\linewidth, left=1mm, right=-3mm, top=1mm, bottom=1mm]
\begin{Verbatim}[fontsize=\normalsize]

[API_path] flask.json.dump
[question]
def test_json_dump_to_file(self):
    app = flask.Flask(__name__)
    test_data = {'name': 'Flask'}
    out = StringIO()
    with app.app_context():
        flask.json.dump
[A] (test_data, out, app=app)
[B] (test_data, out)
[C] (test_data, out, app=app, indent=4)
[D] (test_data, out, app=None)
[answer] B

\end{Verbatim}
\end{tcolorbox}
