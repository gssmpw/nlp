\section{Conclusion}
In this paper, we introduce \method, an innovative data engine designed to establish a structured benchmark, \benchmark, which aims at assessing the proficiency of LLMs in handling evolving code knowledge. We evaluate the performance of state-of-the-art LLMs on \benchmark and measure the efficacy of prevalent knowledge update techniques.
Our results reveal that LLMs face significant challenges in adapting to rapid API evolutions, often struggling with knowledge decay over time. Furthermore, existing knowledge update techniques show limitations in effectively injecting updated code knowledge into models. These findings underscore the necessity for further refinement of knowledge updating methods to improve models' capability to adapt to evolving code knowledge in dynamic environments.
