\section{Introduction}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{./figures/motivation.pdf}
    \vspace{-1em}
    \caption{
    \textbf{LLMs often struggle to adapt to API updates, leading to potential compatibility issues in generated code.} For example, the \texttt{device} parameter was removed from the \texttt{full} function in \texttt{numpy} version 2.1.0, making LLM failed to provide correct invocation. It highlights the need for API knowledge updating to synchronize LLM with the latest API changes and correctly generate updated API invocations.
    }
    \label{fig_motivation}
    \vspace{-1em}
\end{figure}
Large Language Models (LLMs), exemplified by DeepSeek-R1~\cite{guo2025deepseekr1}, CodeLlama~\cite{Roziere2023codellama}, and GPT-4o~\cite{openai2024gpt4o}, have demonstrated remarkable performance in automating software development through generating executable code~\cite{Jiang2024code_gen_survey}.
However, due to static pre-training datasets, they often struggle to adapt to the rapidly evolving knowledge in programming, especially the frequent updates of external library APIs~\cite{tao2012code_change, zhang2020python_api_evolve}. 

As illustrated in~\autoref{fig_motivation}, when prompted to create an array on a CUDA device, the LLM is unaware of the removal of the \texttt{device} parameter in the updated \texttt{numpy.full} function.
This oversight results in an error, \emph{i.e.}, \textit{``{TypeError: full() got an unexpected keyword argument `device'}''}.
The pitfalls of generating code containing outdated APIs can lead to parameter compatibility issues, which causes programs to crash or malfunction, undermining the reliability and stability of software~\cite{bai2024apilot, zhang2024pcart}. 
This challenge highlights the need for LLMs to \textit{synchronize} with the dynamic evolution of practical code knowledge, particularly the fast-paced API updates that have immediate and visible impacts on software development.



\begin{figure*}[!t]
	\centering
	\includegraphics[width=\linewidth]{./figures/RQ2.pdf}
         \vspace{-2em}
         \caption{\textbf{Performance comparison of knowledge updating methods across three evaluation tasks on five LLMs.} All LLMs shown in the figure are instruction-tuned versions. The results reveal that LLMs face challenges in adapting to dynamic API updates, even with the support of knowledge updating approaches, emphasizing the need for improvements in real-time code knowledge updating.}
	\label{fig_RQ2}
	\vspace{-1em}
\end{figure*}

Recently, \citet{liu2024codeupdatearena} made an initial attempt to address this gap by benchmarking LLMs' ability to access API updates through fine-tuning. 
However, their benchmark relies on \textbf{unauthentic} API updates synthesized by GPT-4~\cite{openai2024gpt4} rather than real-world library updates, resulting in potentially biased assessments of LLMs' adaptability to practical code evolution.
We argue that a authentic evaluation system should be established to answer the key question: \textit{Can LLMs be effectively and efficiently updated to handle real-time API modifications?}

To address this gap, this paper introduces \method, a data engine for collecting authentic code knowledge updates from Python third-party libraries across various domains, including data science (\emph{e.g.}, \texttt{pandas}),  artificial intelligence (\emph{e.g.}, \texttt{torch}), and web development (e.g., \texttt{flask}). 
Specifically, \method systematically identifies real-time API updates by tracking changes to API signatures across library versions. 
For each identified API with updates, it retrieves relevant code instances invoking the API from GitHub repositories using GitHub Code Search~\cite{github_code_search}.
Based on these real-world API invocations, \method employs DeepSeek-V3~\cite{liu2024deepseekv3} to synthesize contrastive invocations for legacy and updated API versions.

Based on \method, we develop \benchmark, an extensive benchmark for assessing LLMs’ ability to stay \textit{synchronized} with dynamic code evolution, which includes real-world updates for 220 APIs (130 functions, 59 initializers, and 31 methods) from 6 Python libraries, along with 3,300 legacy-updated pairs of API invocation instances.
The benchmark provides 3,300 test cases across three evaluation tasks, \emph{i.e.}, \textit{Code Complete Task} (CCT), \textit{Error Correction Task} (ECT), and \textit{Multiple Choice Question} (MCQ), accompanied by an update-aware instruction tuning dataset comprising 2,200 training samples.
Unlike retrieval-augmented frameworks that enhance LLMs at the expense of increased inference overhead and without reflecting true model updates, 
\benchmark focuses on evaluating and improving LLMs’ ability to internalize API update knowledge and accurately recall it during code generation.

\textbf{Take-Aways.}
We benchmark 14 state-of-the-art LLMs {(\emph{e.g.}, ChatGPT~\cite{openai2024gpt4}, DeepSeek~\cite{liu2024deepseekv3} and Claude~\cite{anthropic2024claude})}, including both proprietary and open-source models, as well as five knowledge updating methods {(\emph{e.g.}, DPO~\cite{rafailov2023dpo}, ORPO~\cite{hong2024orpo}, and SimPO~\cite{meng2024simpo})}.
Our findings reveal several key insights.
First, as shown in~\autoref{fig_RQ2}, assessment results indicate that LLMs struggle to adapt to dynamic API updates, even with the support of advanced knowledge updating approaches, highlighting the need for further advancements in real-time code knowledge updating. 
Moreover, the numbers of API invocations available for training and the types of updated APIs significantly impact the effectiveness of knowledge updating, increasing the complexity of handling real-world API modifications.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=.98\linewidth]{./figures/pipeline.pdf}
         \vspace{-1em}
         \caption{\textbf{An overview of our proposed \method framework.} \method consists of four key steps: \textbf{(1) Real-Time API Update Tracking} tracks and collects API updates by comparing legacy and latest versions of libraries. \textbf{(2) Real-World API Invocation Retrieval} is designed to crawl API invocations and locate valid API calls. \textbf{(3) Legacy-Updated API Invocation Synthesis} leverages LLMs to synthesize new API invocation statements based on legacy and updated signatures, respectively, and then recognizes them into metadata. \textbf{(4) \benchmark} is used to evaluate the performance of LLMs on API updating tasks, with a period spanning from January 1, 2023 (post-GPT-3.5 release) to current versions.}
	\label{fig_framework}
	\vspace{-1em}
\end{figure*}

Our primary contributions are summarized as follows.
\begin{itemize}[leftmargin=4mm, itemsep=0.05mm]
\item \textbf{A Data Engine.} We introduce \method, a data engine that systematically collects real-time code knowledge updates from various Python third-party libraries. 
\item \textbf{A Novel Benchmark.} We develop \benchmark, a novel benchmark covering updates for 220 APIs across six Python libraries. It offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset with 2,200 training samples.
This benchmark can serve as rigorous testbeds to facilitate the development of real-time code knowledge updating methods.
\item \textbf{Comprehensive Evaluation.} Our extensive experiments on 14 state-of-the-art LLMs, including both proprietary and open-source models, indicate that they still struggle to handle dynamic code evolution.
Additionally, our results reveal that knowledge updating methods can improve LLM synchronization with API updates, though challenges remain to be addressed. 
\end{itemize}

