\section{Can LLMs \textit{Sync} with Code Evolution?}
To comprehensively assess LLMs' ability to synchronize with dynamic code evolution, we investigate the following \textit{Research Questions} (RQs):
\begin{itemize}[leftmargin=4mm, itemsep=0.05mm]
    \item \textbf{RQ1: Benchmarking Large Language Models.}
        \textit{Can LLMs access real-time API updates without relying on retrieval-augmented frameworks?}
    \item \textbf{RQ2: Benchmarking Knowledge Updating Methods.} 
        \textit{Can LLMs be effectively and efficiently updated to synchronize with API changes using knowledge updating methods without compromising model utility?}
    \item \textbf{RQ3: Impact of API Update Settings.} 
        \textit{How do different API update settings, \emph{e.g.}, the numbers of API invocations available for training and the types of updated APIs, impact the performance of knowledge updating?} 
\end{itemize}

\subsection{RQ1: Benchmarking Large Language Models}
We benchmark nine state-of-the-art LLMs in accessing real-time API updates without retrieval-augmented settings, including four proprietary models (\emph{i.e.}, GPT-4o, GPT-4o-mini~\cite{openai2024gpt4o}, Claude-3.5-Sonnet~\cite{anthropic2024claude} and Gemini-1.5-Pro~\cite{geminiteam2024geminifamilyhighlycapable}) and five open-source models (\emph{i.e.}, DeepSeek-V3~\cite{liu2024deepseekv3}, DeepSeek-R1~\cite{guo2025deepseekr1}, and Qwen2.5-14/32/72B-Instruct~\cite{qwen2.5}).

As shown in~\autoref{tab_RQ1}, the results indicate that state-of-the-art LLMs face significant challenges in coding tasks involving API updates. For example, leading commercial models like GPT-4o and Claude-3.5-Sonnet exhibit poor performance, with BLEU scores below 20\% on the code completion task. Similarly, recently released models with up-to-date knowledge cutoffs, such as DeepSeek-V3 and DeepSeek-R1, which are expected to incorporate fresher code knowledge, also fail to accurately reflect API updates, yielding similarly low BLEU scores. These findings reveal systemic shortcomings in LLMs' ability to adapt to evolving APIs, highlighting the fundamental limitations of static pretraining paradigms. Thus, even the latest models suffer from knowledge decay as API versions evolve over time.

\begin{table*}[!t]
\small
\setlength{\tabcolsep}{5.4pt} 
\begin{minipage}[t]{\linewidth}
    \caption{\textbf{The overall performance of different knowledge updating methods across five open-source LLMs.} We train five models using different methods and evaluate their performance on \benchmark and HumanEval. All methods demonstrate limited effectiveness on \benchmark. (BU for BLEU, RL for ROUGE-L, and RED for Relative Edit Distance)}
    % \vspace{0.5em}
    \centering
    \begin{tabular}{l|ccc|ccc|ccc|cccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{CCT}} & \multicolumn{3}{c|}{\textbf{ECT}} & \multicolumn{3}{c|}{\textbf{MCQ}}  & \multicolumn{4}{c}{\textbf{HumanEval}} \\
         & \textbf{BU}$\uparrow$ & \textbf{RL}$\uparrow$ & \textbf{RED}$\downarrow$ & \textbf{BU}$\uparrow$ & \textbf{RL}$\uparrow$ & \textbf{RED}$\downarrow$ & \textbf{P@1}$\uparrow$ & \textbf{P@3}$\uparrow$ & \textbf{P@5}$\uparrow$ & \textbf{P@1}$\uparrow$ & \textbf{P@3}$\uparrow$ & \textbf{P@5}$\uparrow$ & \textbf{Ratio}$\uparrow$\\
        \midrule
        \multicolumn{13}{c}{\text{\textit{Qwen2.5-7B-Instruct}}}\\
        \hline
        Original & 7.95 & 25.70 & 73.61 & 32.24 & 56.79 & 50.77 & 28.48 & 41.61 & 46.91 & 25.49 & 49.69 & 61.61 & --\\
        SFT & 18.26 & 40.76 & 69.07 & 33.33 & 56.94 & 50.96 & \cellcolor{lightblue}\textbf{33.74} & \cellcolor{lightblue}\textbf{54.74} & \cellcolor{lightblue}\textbf{63.26} & 23.44 & 44.55 & 55.70 & 90.41\\
        SFT-LoRA & 12.17 & 34.59 & 68.76 & 26.63 & 44.81 & 57.15 & 32.83 & 47.55 & 53.21 & 25.67 & 49.47 & 61.01 & 99.03\\
        DPO & 24.45 & \cellcolor{lightblue}\textbf{52.94} & 57.12 & \cellcolor{lightblue}\textbf{46.24} & 64.87 & 42.99 & 33.39 & 45.61 & 50.05 & \cellcolor{lightblue}\textbf{26.59} & \cellcolor{lightblue}\textbf{51.27} & \cellcolor{lightblue}\textbf{62.93} & \cellcolor{lightblue}\textbf{102.14}\\
        ORPO & \cellcolor{lightblue}\textbf{24.90} & 52.33 & \cellcolor{lightblue}\textbf{56.37} & 40.98 & 58.92 & 47.63 & 32.85 & 47.74 & 53.35 & 21.59 & 42.46 & 52.73 & 85.59\\
        SimPO & 24.81 & 52.90 & 56.88 & 45.15 & \cellcolor{lightblue}\textbf{65.51} & \cellcolor{lightblue}\textbf{42.90} & 33.14 & 44.35 & 48.69 & 24.76 & 46.34 & 56.83 & 92.24\\
        \hline
        \multicolumn{13}{c}{\text{\textit{Qwen2.5-Coder-7B-Instruct}}}\\
        \hline
        Original & 5.89 & 21.56 & 76.58 & 11.64 & 26.78 & 71.81 & 32.56 & 41.28 & 44.57 & 29.15 & 55.15 & 67.00 & --\\
        SFT & 18.03 & 41.77 & 63.70 & 15.87 & 33.37 & 64.22 & 32.10 & 45.58 & 45.03 & 29.45 & 56.00 & 67.83 & 101.24\\
        SFT-LoRA & 15.44 & 37.40 & 66.55 & 19.20 & 40.68 & 60.93 & 35.16 & 48.63 & \cellcolor{lightblue}\textbf{55.02} & 31.16 & 57.91 & 69.75 & 104.10\\
        DPO & 23.36 & 51.82 & 46.12 & 55.57 & 59.07 & 46.12 & 37.00 & 46.39 & 50.40 & 32.56 & 59.00 & 69.82 & 104.21\\
        ORPO & 21.47 & 48.17 & 53.43 & \cellcolor{lightblue}\textbf{56.92} & 50.20 & 53.43 & 35.42 & \cellcolor{lightblue}\textbf{48.64} & 54.70 & 30.98 & 58.33 & 70.09 & 104.61\\
        SimPO & \cellcolor{lightblue}\textbf{23.86} & \cellcolor{lightblue}\textbf{53.17} & \cellcolor{lightblue}\textbf{45.22} & 54.57 & \cellcolor{lightblue}\textbf{60.31} & \cellcolor{lightblue}\textbf{45.22} & \cellcolor{lightblue}\textbf{37.87} & 44.92 & 47.80 & \cellcolor{lightblue}\textbf{33.05} & \cellcolor{lightblue}\textbf{59.29} & \cellcolor{lightblue}\textbf{70.99} & \cellcolor{lightblue}\textbf{105.96}\\
        \hline
        \multicolumn{13}{c}{\text{\textit{Llama-3.1-8B-Instruct}}}\\
        \hline
        Original & 5.99 & 22.45 & 75.70 & 17.68 & 40.98 & 63.41 & 29.08 & \cellcolor{lightblue}\textbf{54.39} & \cellcolor{lightblue}\textbf{66.28} & \cellcolor{lightblue}\textbf{51.77} & \cellcolor{lightblue}\textbf{71.03} & \cellcolor{lightblue}\textbf{76.68} & --\\
        SFT & 25.27 & 50.70 & \cellcolor{lightblue}\textbf{54.95} & 29.25 & 50.17 & 52.93 & 29.35 & 37.88 & 50.84 & 37.29 & 57.29 & 64.60 & 84.24\\
        SFT-LoRA & 13.21 & 36.70 & 72.01 & \cellcolor{lightblue}\textbf{43.78} & \cellcolor{lightblue}\textbf{65.76} & \cellcolor{lightblue}\textbf{41.84} & 22.28 & 38.74& 47.24 & 44.02 & 65.23 & 71.63 & 93.41\\
        DPO & 24.13 & 51.36 & 55.38 & 27.18 & 51.57 & 54.83 & 36.42 & 49.88 & 55.34 & 40.24 & 60.88 & 68.12 & 88.84\\
        ORPO & 21.55 & 44.19 & 60.62 & 24.27 & 42.21 & 62.09 & 31.47 & 50.30 & 58.74 & 44.26 & 64.16 & 72.21 & 94.17\\
        SimPO & \cellcolor{lightblue}\textbf{26.83} & \cellcolor{lightblue}\textbf{53.95} & 56.07 & 23.04 & 44.91 & 58.74 & \cellcolor{lightblue}\textbf{36.56} & 43.96 & 46.66 & 44.08 & 64.83 & 70.96 & 92.54\\
        \hline
        \multicolumn{13}{c}{\text{\textit{CodeLlama-7B-Instruct}}}\\
        \hline
        Original & 8.44 & 28.25 & 73.20 & 18.11 & 37.71 & 64.45 & 10.89 & 24.79 & 33.24 & \cellcolor{lightblue}\textbf{28.60} & \cellcolor{lightblue}\textbf{44.76} & \cellcolor{lightblue}\textbf{52.72} & --\\
        SFT & \cellcolor{lightblue}\textbf{31.59} & \cellcolor{lightblue}\textbf{58.92} & 46.04 & \cellcolor{lightblue}\textbf{46.53} & 65.21 & \cellcolor{lightblue}\textbf{39.08} & 9.13 & 15.92 & 20.91 & 24.58 & 35.72 & 45.21 & 85.75\\
        SFT-LoRA & 17.24 & 44.97 & 59.57 & 30.60 & 50.42 & 53.99 & 10.34 & 18.91 & 24.85 & 27.50 & 41.52 & 48.97 & 92.89\\
        DPO & 26.54 & 53.27 & \cellcolor{lightblue}\textbf{26.51} & 39.67 & 60.55 & 44.79 & 20.48 & 41.09 & 51.71 & 27.86 & 42.24 & 49.55 & 93.89\\
        ORPO & 24.37 & 50.70 & 54.61 & 36.06 & 55.69 & 49.00 & 18.07 & 39.17 & 51.26 & 26.76 & 42.19 & 49.92 & 94.69\\
        SimPO & 27.78 & 56.48 & 50.62 & 40.56 & \cellcolor{lightblue}\textbf{65.27} & 41.65 & \cellcolor{lightblue}\textbf{25.40} & \cellcolor{lightblue}\textbf{45.50} & \cellcolor{lightblue}\textbf{54.66} & 27.56 & 41.12 & 48.31 & 91.64\\
        \hline
        \multicolumn{13}{c}{\text{\textit{DeepSeek-Coder-6.7B-Instruct}}}\\
        \hline
        Original & 5.97 & 22.55 & 75.51 & 30.07 & 53.11 & 52.20 & \cellcolor{lightblue}\textbf{31.25} & \cellcolor{lightblue}\textbf{24.29} & \cellcolor{lightblue}\textbf{43.60} & \cellcolor{lightblue}\textbf{67.87} & \cellcolor{lightblue}\textbf{83.22} & \cellcolor{lightblue}\textbf{86.72}  & --\\
        SFT & 24.96 & 51.34 & \textbf{55.57} & \cellcolor{lightblue}\textbf{55.61} & \cellcolor{lightblue}\textbf{76.83} & \cellcolor{lightblue}\textbf{28.13} & 7.67 & 11.34 & 14.19 & 56.94 & 76.01 & 80.45 & 92.77\\
        SFT-LoRA & 14.96 & 41.42 & 62.45 & 47.79 & 71.25 & 34.32 & 7.88 & 8.89 & 9.32 & 59.51 & 80.06 & 85.00 & 98.02\\
        DPO & 26.77 & 55.72 & 50.86 & 43.29 & 64.95 & 41.91 & 6.37 & 8.61 & 9.00 & 58.78 & 79.87 & 85.54 & 98.99\\
        ORPO & \cellcolor{lightblue}\textbf{28.39} & \cellcolor{lightblue}\textbf{56.99} & \cellcolor{lightblue}\textbf{49.23} & 43.77 & 64.86 & 41.32 & 7.02 & 7.79 & 8.04 & 57.62 & 79.31 & 84.63 & 98.64\\
        SimPO & 25.10 & 53.69 & 52.97 & 41.47 & 64.06 & 42.50 & 6.75 & 9.21 & 10.55 & 57.13 & 77.69 & 83.28 & 96.03\\
        \bottomrule
        % \hline
    \end{tabular}
    \label{tab_RQ2}
    % \vspace{-1em}
\end{minipage}
\end{table*}

\subsection{RQ2: Benchmarking Knowledge Updating Methods}
We benchmark five knowledge updating methods including SFT, SFT-LoRA~\cite{peng2023instruction}, DPO~\cite{rafailov2023dpo}, SimPO~\cite{meng2024simpo}, and ORPO~\cite{hong2024orpo}, across five open-source LLMs including three code-specific LLMs (\emph{i.e.}, CodeLlama-7B-Instruct~\cite{Roziere2023codellama}, Qwen2.5-Coder-7B-Instruct~\cite{hui2024qwen2.5coder}, and DeepSeek-Coder-6.7B-Instruct~\cite{guo2024deepseek}) and two general-purpose LLMs (\emph{i.e.}, Llama-3.1-8B-Instruct~\cite{dubey2024llama3} and Qwen2.5-7B-Instruct~\cite{qwen2.5}). Detailed experiment settings are listed in~\cref{rq2_exp_settings}.

\parabf{Evaluation of Updating Effectiveness}
As illustrated in~\autoref{fig_RQ2} and~\autoref{tab_RQ2}, the results indicate that knowledge updating methods can improve LLMs' performance in handling API evolution across the three evaluation tasks. Notably, fine-tuned open-source LLMs with size 6.7B-8B can achieve scores comparable to or surpassing those of leading proprietary and open-source LLMs, such as Claude-3.5-Sonnet and DeepSeek-R1, with BLEU scores of 23.86\%-31.59 on the CCT task. However, despite these improvements, the absolute scores remain low, indicating that current methods are insufficient for effectively updating code knowledge of LLMs and necessitate further refinement.

Moreover, we observe that the DeepSeek-Coder-6.7B-Instruct model exhibits an anomaly on the MCQ task, where fine-tuning leads to significantly lower scores. Analysis of the model outputs reveals degraded instruction-following capabilities, resulting in non-compliant responses. In contrast, other models maintain compliant outputs, indicating a lack of robustness in this model.

\parabf{Evaluation of Updating Efficiency}
In addition to effective-ness, updating efficiency is a crucial factor that may influence developers' adoption in practice. For each model, we recorded the training time required for four knowledge updating methods, as shown in~\autoref{fig_rq3_time}. The results indicate that SFT-LoRA is the most efficient method overall. Moreover, we can observe that, across all models, the training durations follow the pattern: SimPO $<$ DPO $<$ ORPO, indicating that ORPO is the least efficient and SimPO is the most efficient. Additionally, it can be seen that the training duration for ORPO exhibits relatively larger fluctuation, indicating instability in efficiency.

\parabf{Evaluation of Model Utility Post-Updating}
We evaluate the general utility of the LLMs before and after updating using the widely used HumanEval benchmark~\cite{chen2021humaneval}. For each problem, we sample 10 answers (\emph{i.e.}, $n=10$) and calculate Pass@1, Pass@3, and Pass@5 scores. To assess the impact of updating, we computed the \textbf{ratio} of the Pass@5 scores for models trained with various methods to those of the original model. The results show that most updating methods incurred a score loss of no more than 10\%, indicating minor impact on the models' overall utility.

\begin{figure}[!t]
	\centering
	\includegraphics[width=.96\linewidth]{./figures/RQ3-Time.pdf}
         \vspace{-1.5em}
         \caption{\textbf{Efficiency of different knowledge updating techniques.} We measure and compare the time consumption of four knowledge updating techniques across five models. We can observe that the training durations follow the pattern: SimPO $<$ DPO $<$ ORPO.}
	\label{fig_rq3_time}
	\vspace{-1em}
\end{figure}

\subsection{RQ3: Impact of API Updating Settings}
We further investigate the impact of different API update settings such as the numbers of API invocations available for training and the types of updated APIs, on the performance of knowledge updating in API evolution tasks.

\parabf{Impact of Update-Aware Instruction Number}
To evaluate this, we filter 32 APIs from the original training set, each with more than 50 invocation samples, and construct four new training sets with 5, 10, 20, and 50 samples per API, respectively. We then train Qwen-2.5-7B-Instruct using four knowledge updating techniques (\emph{i.e.}, SFT-LoRA, DPO, ORPO, SimPO) on these sets and evaluate performance on the code completion task. As shown in~\autoref{fig_rq3_count}, using only 5 samples per API results in relatively poor performance. When the training sample number increases to 10 per API, the model demonstrates improved recall capabilities of the updated APIs. Further increases in sample number lead to performance stabilization with minor additional gains. These findings suggest that a moderate number of samples is sufficient for LLMs to internalize new code knowledge, with 10 samples per API striking an optimal balance between effectiveness and efficiency.

\begin{figure}[!t]
	\centering
	\includegraphics[width=.96\linewidth]{./figures/RQ3-Sample-Count.pdf}
         \vspace{-1.5em}
         \caption{\textbf{Model performance with varying numbers of invocation instances per API.}
         The original training set is divided into subsets containing different numbers of invocation samples per API (5, 10, 20, 50). The Qwen2.5-7B-Instruct model is trained on these subsets and evaluated on the Code Completion Task of \benchmark. The result indicates that 10 samples per API is sufficient for injecting knowledge, which keep balance between performance and efficiency.
         }
	\label{fig_rq3_count}
	\vspace{-1em}
\end{figure}

\parabf{Impact of Updated API Type}
We evaluate Qwen-2.5-7B-Instruct on the CCT task across different API types. As illustrated in~\autoref{fig_rq3_type}, a clear trend can be observed among the three API types. The knowledge updating methods perform similarly on function APIs and initializer APIs yet exhibit significantly lower performance on method APIs. This discrepancy can be attributed to the intrinsic complexity of method invocations, which typically involve class instantiations, object references, and dynamic method calls. Unlike function and initializer APIs that follow relatively straightforward invocation patterns, method APIs require LLMs to correctly infer object types, track dependencies, and manage class hierarchies. These additional layers of complexity increase the difficulty of accurately invoking API updates, making it more challenging for LLMs to learn and apply correctly. Addressing these challenges may require more sophisticated knowledge updating strategies to improve LLMs' adaptability to complex code knowledge.

\begin{figure}[!t]
	\centering
	\includegraphics[width=.94\linewidth]{./figures/RQ3-Type.pdf}
         \vspace{-1.5em}
         \caption{\textbf{Model performance on different types of APIs.}
         We evaluate the performance of Qwen-2.5-7B-Instruct, trained using various techniques, as well as a reference model, on different categories of APIs (functions, methods, and initializers). The result reveal significant differences in the models' capabilities across different categories. Notably, all models perform relatively worse on methods compared to functions and initializers. 
         % This could be attributed to the higher complexity of method invocation and the challenges associated with type inference.
         }
	\label{fig_rq3_type}
	\vspace{-1em}
\end{figure}

