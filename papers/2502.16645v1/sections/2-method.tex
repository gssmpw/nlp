\section{\dataset: A Data Engine for Real-Time Code Knowledge Collection}

As illustrated in~\autoref{fig_framework}, we propose \method, a data engine for real-time collection of code knowledge evolution, which operates through three key steps:
\textbf{(1) Real-Time API Update Tracking.} 
\method identifies and extracts API updates across diverse Python third-party libraries by systematically tracking changes to API signatures between library versions (see \autoref{sec_step1}). 
\textbf{(2) Real-World API Invocation Retrieval.} For each identified API with updates, \method retrieves relevant code instances invoking the API from GitHub repositories through GitHub Code Search~\cite{github_code_search} (see \autoref{sec_step2}).
\textbf{(3) Legacy-Updated API Invocation Synthesis.} 
Building on the retrieved real-world API invocations, \method employs DeepSeek-V3~\cite{liu2024deepseekv3} to synthesize contrastive code instances that invoke legacy and updated APIs, respectively (see \autoref{sec_step3}).
Based on \method, we establish \benchmark, a benchmark for assessing real-time code knowledge of LLMs, which collects updates for 220 APIs (including 130 functions, 59 initializers, and 31 methods) from 6 Python libraries, totaling 3,300 legacy-updated pairs of API invocation instances (see \autoref{sec_benchmark}).

\subsection{Step 1: Real-Time API Update Tracking}
\label{sec_step1}
% The functionality of APIs can be accessed through their signatures, which serve as the interface for users to invoke this knowledge within code.
% This characteristic allows us to systematically track the updates of library APIs by monitoring modifications in API signatures.
The functionality of APIs is exposed through their signatures, which provide an interface for developers to utilize this functionality within code. This feature enables systematic tracking of library API updates by monitoring changes in their signatures.

\vspace{-1em}
\parabf{Extracting API Signatures}
We target 6 widely used Python third-party libraries: \texttt{pandas}, \texttt{numpy}, \texttt{scipy}, \texttt{tensorflow}, \texttt{torch} and \texttt{flask}.
To collect complete API signatures from these libraries, we leverage Python's built-in \textit{inspect} module, a \textit{dynamic} reflection tool provided by the Python standard library~\cite{python_inspect}.
This tool enables runtime analysis and collection of information about Python objects, including modules, classes, functions, and methods.
For each library, we extract API signatures by performing \textit{inspections} within virtual environments configured with specific library versions.
Further details are provided in~\cref{appx_api_collection}.

\parabf{Identifying API Updates}
To evaluate LLMs' ability to \text{synchronize} with real-time API evolution, we consider the most recent library version before ChatGPT's release~\cite{openai2023chatgpt} as the legacy version and the current library version as the updated version. Then, we identify API updates by systematically comparing API signatures between versions. 
To determine whether an update exists for a given API, we perform a \textit{static} analysis to establish parameter mappings for same-name APIs across versions. These mappings allow us to analyze API changes at the parameter level by examining differences in attributes such as parameter name, position, and type. Using this approach, we identify 6,063 API updates from the six targeted Python libraries, as summarized in~\autoref{tab_step1}. More implementation details are provided in~\cref{appx_api_update_identification}.

\begin{table}[!t]
\small
\setlength{\tabcolsep}{4pt} 
\caption{\textbf{Statistics of tracked API updates.} We systematically identify API updates across diverse Python third-party libraries by monitoring changes in API signatures between the latest version and an outdated version around January 1, 2023. This period coincides with the introduction of the milestone GPT-3.5.}
\vspace{0.5em}
\centering
\begin{tabular}{l|ccc}
    \toprule
    \textbf{Library} & \textbf{Legacy Version} & \textbf{Updated Version} & \textbf{Num.} \\
    \midrule
    \texttt{pandas} & 2.0.3 & 2.1.4 & 1043 \\
    \texttt{numpy} & 1.24 & 2.1 & 55 \\
    \texttt{scipy} & 1.10.0 & 1.13.1 & 494 \\
    \texttt{tensorflow} & 2.11.0 & 2.18.0 & 161 \\
    \texttt{torch} & 2.0.0 & 2.5.0 & 4260 \\
    \texttt{flask} & 2.2.2 & 3.0.0 & 22 \\
    \bottomrule
\end{tabular}
\label{tab_step1}
\vspace{-0.5em}
\end{table}

\subsection{Step 2: Real-World API Invocation Retrieval}
\label{sec_step2}
While API updates are reflected in signature changes, collecting this information alone is insufficient to fully capture the evolution of code knowledge. To address this, we consider real-world API invocation scenarios, focusing on modifications in API usage within actual code contexts. For each API update identified in~\autoref{sec_step1}, we retrieve and filter relevant code instances that invoke the API from GitHub repositories.

\parabf{Retrieving Relevant Code Instances}
We use GitHub Code Search~\cite{github_code_search} to retrieve Python files that potentially contain API invocations by designing multiple matching templates. For example, to retrieve code invoking the function \texttt{torch.nn.Linear}, we match the API name (\emph{e.g.}, \texttt{.Linear}) along with relevant import statements (\emph{e.g.}, \texttt{import torch.nn as nn} and \texttt{from torch import nn}). Further details on the matching templates are provided in~\cref{appx_api_invocation_retrieval}.

\parabf{Locating Valid API Invocations}
Code instances retrieved via matching templates may only potentially invoke the target APIs, requiring precise localization to confirm valid invocations. To achieve this, we parse each code instance into an \textit{Abstract Syntax Tree} (AST) using Python's built-in \textit{ast} module~\cite{python_ast} and traverse all statements to identify those that genuinely contain targeted invocations. Moreover, we perform alias resolution on import statements to establish mappings between full module names (\emph{e.g.}, \texttt{numpy}) and their aliases (\emph{e.g.}, \texttt{np}), ensuring more accurate identification of valid API invocations. For example, we locate statements that contain \texttt{np.full} for the \texttt{full} function and \texttt{nn.Linear} for the \texttt{Linear} class initializer. Furthermore, regarding method invocation locating, the \textit{ast} module enables us to track objects whose types match the target class by examining class instantiations and assignments. For example, in the case of \texttt{x.reshape()}, we identify that \texttt{x} is of type \texttt{torch.Tensor}, confirming a valid invocation of the \texttt{reshape()} method from the \texttt{torch.Tensor} class. Detailed implementation is provided in~\cref{appx_api_invocation_filtering}.

Through retrieval and localization, we filter out APIs with fewer than 15 valid invocation instances. Out of 6,036 APIs, 220 meet the criteria, each with 15 valid invocation instances, resulting in a total of 3,300 instances.


\subsection{Step 3: Legacy-Updated API Invocation Synthesis}
\label{sec_step3}
While real-world code instances with valid API invocations can be retrieved from GitHub repositories, it is challenging to determine the exact library version of the invoked API. To address this, we synthesize the contrastive API invocation pairs—legacy and updated—using state-of-the-art LLMs, which have demonstrated strong capabilities in revising code while preserving both semantic and syntactic correctness~\cite{guo2024code_refinement}.

Specifically, for each API invocation instance retrieved in~\autoref{sec_step2}, we prompt DeepSeek-V3~\cite{liu2024deepseekv3} to adapt the target API invocation statement according to the legacy and updated API signatures, respectively, while preserving the integrity of the surrounding context. To ensure data quality, the authors manually verify the divergence between legacy and updated versions, instructing the LLM to re-synthesize cases with insufficient divergence. This approach ensures divergence in API usage while maintaining functional equivalence between legacy and updated implementations, enabling explicit modeling of API evolution. Through this process, we synthesize 3,300 legacy-updated API invocation pairs from 3,300 real-world code instances. The detailed prompt is provided in~\cref{appx_update_code_prompt}.

\begin{table}[!t]
\fontsize{7pt}{12pt}\selectfont
\setlength{\tabcolsep}{4pt} 
\begin{minipage}[t]{\linewidth}
\caption{\textbf{Statistics of data in \method.} We construct \benchmark and its associated training set step by step, from identifying real-time API updates,  retrieving real-world invocations, and synthesizing legacy-updated invocations to building training and test samples, as shown in~\autoref{fig_framework}.}
\centering
\vspace{0.5em}
\begin{tabular}{cccccc}
    \toprule[1.5pt]
    \textbf{Step} & \textbf{Setting} & \textbf{Input} & \textbf{Num.} & \textbf{Output} & \textbf{Num.} \\
    \midrule
    1 & - & Python Libraries & 6 & API Updates & 6036 \\
    \midrule
    2 & - & API Updates & 220 & API Invocations & 3300 \\
    \midrule
    3 & - & API Invocations & 3300 & \makecell{Legacy-Updated \\ Invocation Pairs} & 3300 \\
    \midrule
    \multirow{4}{*}{\makecell{\textsc{CodeSync} \\ \textsc{Bench}}} & Train & \makecell{Legacy-Updated \\ Invocation Pairs} & 2200 & \makecell{Update-Aware \\ Instructions} & 2200 \\
    \cmidrule{2-6}
     & \multirow{3}{*}{Test} & \multirow{3}{*}{\makecell{Legacy-Updated \\ Invocation Pairs}} & \multirow{3}{*}{1100} & CCT Tests & 1100 \\
     &  &  &  & ECT Tests & 1100 \\
     &  &  &  & MCQ Tests & 1100 \\
    \bottomrule[1.5pt]
\end{tabular}
\label{tab_dataset_statistics}
\end{minipage}
\vspace{-1em}
\end{table}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\linewidth]{./figures/example.pdf}
         \vspace{-2.5em}
         \caption{\textbf{An illustrative example of three evaluation tasks of \benchmark.} \textbf{(1) CCT} only provides the API call name at the end of the question, without explicitly listing the parameters, expecting the completion. \textbf{(2) ECT} includes an incorrect parameter list at the end of the question, expecting the correction. \textbf{(3) MCQ} does not explicitly listing the parameters, but presents one correct option and three incorrect options, expecting the most accurate answer.}
	\label{fig_benchmark}
	\vspace{-1.5em}
\end{figure*}

\begin{table*}[!t]
\small
\begin{minipage}[t]{\linewidth}
    \caption{\textbf{The performance of different LLMs in accessing API updates.} We evaluate nine popular LLMs on \benchmark, revealing their poor performance in API invocation tasks. The results highlight significant limitations in LLMs' ability to handle updated APIs, with even state-of-the-art models struggling to achieve high scores due to outdated knowledge. (BU for BLEU, RL for ROUGE-L, and RED for Relative Edit Distance)}
    \vspace{0.5em}
    \centering
    \setlength\tabcolsep{7pt} 

    \begin{tabular}{ll|ccc|ccc|ccc}
        \toprule
        \multirow{2}{*}{\textbf{LLM}} & \multirow{2}{*}{\makecell[l]{\textbf{Knowledge} \\ \textbf{Cutoff Date}}} & \multicolumn{3}{c|}{\textbf{CCT}} & \multicolumn{3}{c|}{\textbf{ECT}} & \multicolumn{3}{c}{\textbf{MCQ}} \\
        &  & \textbf{BU}$\uparrow$ & \textbf{RL}$\uparrow$ & \textbf{RED}$\downarrow$ & \textbf{BU}$\uparrow$ & \textbf{RL}$\uparrow$ & \textbf{RED}$\downarrow$ & \textbf{P@1}$\uparrow$ & \textbf{P@3}$\uparrow$ & \textbf{P@5}$\uparrow$ \\
        \midrule
        \multicolumn{10}{c}{\text{\textit{Closed Source Models}}}\\
        \hline
        GPT-4o & Oct. 2023 & 14.93 & 47.07 & 58.87 & 37.07 & \cellcolor{lightred}\textbf{67.13} & \cellcolor{lightred}\textbf{43.06} & \cellcolor{lightred}\textbf{38.98} & \cellcolor{lightred}\textbf{42.09} & 46.07 \\
        GPT-4o-mini & Oct. 2023 & 7.45 & 32.39 & 67.14 & 33.69 & 51.06 & 49.54 & 29.58 & 34.63 & \cellcolor{lightred}\textbf{35.58} \\
        Claude-3.5-Sonnet & Apr. 2024 & \cellcolor{lightred}\textbf{19.29} & 49.24 & \cellcolor{lightred}\textbf{57.07} & \cellcolor{lightred}\textbf{37.91} & 65.85 & 43.21 & 36.08 & 40.13 & 41.80 \\
        Gemini-1.5-Pro & Nov. 2023 & 17.62 & \cellcolor{lightred}\textbf{49.65} & 57.85 & 32.75 & 61.93 & 48.03 & 34.40 & 40.55 & 43.16 \\
        \hline
        \multicolumn{10}{c}{\text{\textit{Open Source Models}}}\\
        \hline
        DeepSeek-V3 & Jul. 2024 & 19.24 & \cellcolor{lightblue}\textbf{44.13} & 57.67 & 51.57 & 62.64 & 34.12 & 31.54 & 34.41 & 35.78 \\
        DeepSeek-R1 & Jul. 2024 & \cellcolor{lightblue}\textbf{19.32} & 44.09 & \cellcolor{lightblue}\textbf{57.54} & \cellcolor{lightblue}\textbf{51.81} & \cellcolor{lightblue}\textbf{62.76} & \cellcolor{lightblue}\textbf{34.05} & 31.61 & 34.41 & 35.78 \\
        Qwen2.5-14B-Instruct & Mar. 2024 & 10.46 & 36.94 & 63.89 & 30.82 & 49.60 & 54.45 & \cellcolor{lightblue}\textbf{37.28} & \cellcolor{lightblue}\textbf{38.88} & \cellcolor{lightblue}\textbf{39.45} \\
        Qwen2.5-32B-Instruct & Mar. 2024 & 13.97 & 39.43 & 62.24 & 40.31 & 55.58 & 42.81 & 35.35 & 37.50 & 38.16 \\
        Qwen2.5-72B-Instruct & Mar. 2024 & 16.06 & 41.53 & 59.76 & 45.03 & 57.92 & 38.23 & 33.49 & 36.41 & 37.41 \\
        \bottomrule
    \end{tabular}
    \label{tab_RQ1}
    \vspace{-1em}
\end{minipage}
\end{table*}

\subsection{\benchmark: A Benchmark for Real-Time Code Knowledge Assessment}
\label{sec_benchmark}
Based on \method, we develop \benchmark, a real-time benchmark for assessing how effectively LLMs adapt to evolving code knowledge, which comprises three evaluation tasks, including \textit{Code Completion Task} (CCT), \textit{Error Correction Task} (ECT), and \textit{Multiple Choice Question} (MCQ), as shown in~\autoref{fig_benchmark}. \benchmark covers updates for 220 APIs across 6 Python libraries, including 130 functions, 59 initializers, and 31 methods. Each API is associated with 15 legacy-updated invocation pairs (3,300 in total), with 5 pairs for evaluation (1,100 in total) and 10 for training (2,200 in total). Based on this, our benchmark builds 1,100 tests per evaluation task, accompanied by a training set comprising 2,200 update-aware instructions, providing a rigorous foundation for assessing LLMs’ ability to stay synchronized with API evolution.

\parabf{Code Completion Task (CCT){~\cite{Lu2021codexglue}}} 
This task evaluates whether LLMs have internalized the updated APIs and can recall them during code generation. Given a code snippet ending with an API name, the LLM is prompted to complete the parameter list, with the updated API invocation statement serving as the ground truth. To measure the accuracy of API invocation completion, we employ three widely used metrics: BLEU~\cite{papineni2002bleu} for evaluating lexical precision, ROUGE-L~\cite{lin2004rouge} for measuring semantic coverage, and \text{Relative Edit Distance} (RED)~\cite{ristad1998EditInstance} for quantifying structural deviation.

\parabf{Error Correction Task (ECT){~\cite{zheng2025opencodeinterpreter}}} 
This task simulates real-world \textit{debugging} scenarios, where an interpreter throws an exception related to a specific API invocation. Unlike passive knowledge recall, it evaluates the LLM's ability to actively correct potential errors. Given a code snippet ending with a legacy API invocation, the LLM is prompted to rectify it to the updated version. Similar to CCT, we assess the accuracy of API invocation correction using BLEU~\cite{papineni2002bleu}, ROUGE-L~\cite{lin2004rouge}, and \text{Relative Edit Distance} (RED)~\cite{ristad1998EditInstance}.

\parabf{Multiple Choice Question (MCQ){~\cite{nguyen2025codemmlu}}} 
This task evaluates the LLM's ability to discriminate between correct and incorrect API invocations, requiring a deep internalization of the updated APIs. Given four candidate API invocations, including one correct answer and three plausible distractors, the LLM is prompted to select the optimal choice. The distractors, synthesized by DeepSeek-V3~\cite{liu2024deepseekv3}, include perturbations such as adding an invalid parameter, removing a required parameter, and rearranging parameter order. We employ the Pass@$k$ metric~\cite{chen2021codex} to measure the probability that the LLM passes a test case within $k$ attempts, which is calculated by drawing $n \ge k$ answers from the LLM for each test case and counting the number of correct answers $c \le n$. We use $n = 10$ and $k \in \{1, 3, 5\}$ (abbreviated as P@1, P@3, and P@5).

\parabf{Training Set}
To evaluate knowledge updating methods, we build an instruction tuning dataset $\mathcal{D}=\{(\mathbf{i}, \mathbf{o}_\text{old}, \mathbf{o}_\text{new})\}$. As illustrated in\textcolor{red}{~\autoref{Metadata}}, $\mathbf{i}$ denotes an update-aware instruction containing a code snippet with an incomplete API invocation (\emph{e.g.}, ``\texttt{array=numpy.full(}''). $\mathbf{o}_\text{old}$ and $\mathbf{o}_\text{new}$ are output statements that accomplish the code. $\mathbf{o}_\text{new}$ represents the correct invocation with the updated API, while $\mathbf{o}_\text{old}$ reflects the legacy version. $\mathbf{o}_\text{old}$ and $\mathbf{o}_\text{new}$ share the same basic functionality, differing only in the parameters affected by the API update. The paired invocations allow the LLMs to identify update-related changes by computing token-level differences between $\mathbf{o}_\text{old}$ and $\mathbf{o}_\text{new}$.
