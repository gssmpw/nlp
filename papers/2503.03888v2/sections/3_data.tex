\section{Data} 
\label{sec:data}

We now describe the data used for training and deploying our AI-based approach. 
Gathering this data required a significant digitization process, extracting and processing 5.2 million pages of deeds stretching back to the 1850s (\S~\ref{sec:digit}). We then supplemented this data with historical deed records available online from around the country (\S~\ref{sec:otherdata}), which has the coincidental benefit of enabling us to assess the model's robustness across jurisdictions.  Finally, we manually annotate 3,801 deeds to build a training dataset and held-out evaluation dataset for our AI pipeline (\S~\ref{sec:annotation}).


\subsection{Digitization, Collection, and Sharing of Real Property Deeds}
\label{sec:digit}
The Santa Clara County Clerk-Recorder's Office has an extensive archive of over 24 million real property deeds. Of these records, approximately 18 million -- issued since 1980 -- are stored digitally, while the remaining 6 million deeds -- created before 1980 -- were originally preserved on physical microfiche sheets. More than a decade prior to our work, the County had engaged a vendor to scan these records into a proprietary system known as Digital Reel; however, as we discuss in Appendix \ref{appendix_ocr}, the quality of these scans was poor and required significant post-processing.

Our partnership around exploring the use of AI began in October 2022. One of the notable barriers to transparency around deed records in California lies in a statutory mandate to charge fees for any copies of recorded documents.\footnote{Cal.\ Gov.\ Code \S~27366 provides: ``The fee for any copy of any other record or paper on file in the office of the recorder, when the copy is made by the recorder, shall be set by the board of supervisors in an amount necessary to recover the direct and indirect costs of providing the product or service or the cost of enforcing any regulation for which the fee or charge is levied.'' This provision has been subject to extensive litigation. See, e.g., California Public Records Research, Inc.\ v.\ County of Stanislaus, 246 Cal.\ App.\ 4th 1432 (2016);  California Public Records Research v.\ County of Yolo, 4 Cal.\ App.\ 5th 150 (2016).}  In other words, despite their status as public records, deed documents are available only on an individual fee basis. Given the massive scale of the review task, purchasing deed records would, of course, have been prohibitively expensive.\footnote{At a cost of \$4 for the first page and \$2 for each subsequent page, purchasing the 5.2M pages (with the average deed running 2.5 pages) might have cost over \$13 million.} Through our partnership, we developed unique a data sharing agreement, enabling the Stanford team  to process deed data, with the County retaining ownership of the records.

We began our work on samples of 20,000 pages of property deeds filed between 1900 and 1940, manually exported from the County's Digital Reel system. This 20,000-page sample enabled us to rapidly develop and refine our automated detection pipeline. 

After this piloting phase, the County extracted the full collection of pre-1980 scans in February 2024. This represents roughly 5.2 million pages of real property documents from 1865 to 1980. We focus our analysis on documents from 1902 to 1980 for two reasons. First, deeds filed prior to 1902 were handwritten rather than typed, and we found no available OCR tools to be effective at transcribing these documents.\footnote{We did explore developing a bespoke computer vision or multimodal text-vision system.} Second, records after 1980 contain protected fields like Social Security information, so we avoided ingesting sensitive data and potentially training our model on it, which may have raised privacy and legal concerns. As we note above and consistent with our results in Section~\ref{sec:evolution}, 1902 to 1980 likely covers the vast majority of racial covenants in the County; the first racial covenant we find was filed in 1907 and the last in 1974.

\subsection{Data Augmentation}
\label{sec:otherdata}
Both within California and across the nation, historical property deeds vary significantly in format, phrasing, and, when digitized, OCR quality. In order to build a system that is robust to these variations, we supplemented the Santa Clara County dataset with property deeds from around the nation, both with and without racial covenants.

Since property records in California counties are not freely accessible, we expanded our search to other counties in the United States. Using \href{https://govos.com/products/public-record-access/channel/}{GovOS Cloud Search}, we identified seven counties whose ``Official Records Search’’ platforms allowed users to freely search and download real property deeds, although downloads were limited to fifty records per batch.\footnote{\url{https://kofilehelp.zendesk.com/hc/en-us/sections/4416665864343-Cloud-Search-Active-Sites}.} These platforms enabled searches by metadata and keyword terms. To gather a seed dataset of deeds with a high probability of containing racial covenants, we conducted manual searches for terms typically associated with such covenants, such as ``No person of,'' ``Caucasian,'' ``Negro,'' and other relevant racial terms. This method provided us with more than 10,000 property deeds from seven counties: Bexar County, Texas; Cuyahoga County, Ohio; Denton County, Texas; Franklin County, Ohio; Hidalgo County, Texas; and Lawrence County, Pennsylvania. This approach not only helped us collect relevant data but also allowed us to assess the generalizability of our model across different jurisdictions. As we discuss below, we specifically investigate the limitations of keyword-based approaches, and find that context-aware language models boost performance substantially. 




\subsection{Annotation} \label{sec:annotation}

We labeled our data collection by identifying quotes that contain racial covenants on each page. This annotation occurred over three stages: initial training data generation, model prediction review, and rich annotation.

In our initial round of annotation, we selected a sample of 3,000 pages in our collection based on keywords that almost certainly indicate the presence of a racial covenant in the deed text. These include terms like ``Negro,'' ``Mongolian,'' and ``Asiatic.'' We partnered with data annotation company CloudFactory to help us identify and label racial covenants in these pages.\footnote{During our collaboration with CloudFactory for data annotation, we carefully prepared comprehensive documentation to guide the annotators through the task. Given the potentially sensitive nature of the material—historical property deeds containing racially restrictive covenants, as well as accounts that could be considered offensive or harmful to some readers—we issued a clear advisory to approach the content with care. We emphasized that the annotators could stop the task at any point if they felt uncomfortable. In addition, we consulted with CloudFactory’s management to ensure that appropriate counseling and support resources would be available to their team, should any annotator feel the need for assistance or support. Our priority was to handle this material with the utmost sensitivity, while ensuring the well-being of those involved in the annotation process.}

After training models and generating predictions, we reviewed their performance. For all positive predictions, we labeled whether they were true positives or false positives. These ensured that we verified the small number of positive examples as well as hard negative examples. We additionally sampled and verified negative predictions to ensure some balance in the data. These new annotations were incorporated into the training set of future models.

Recognizing the need to easily validate model predictions and locate racial covenants on a page, we built a web application to assist with rich annotation. This made it easy to precisely select a bounding box on the image of the deed book page and compute a text span for the annotation process, while simultaneously allowing us to visualize predictions for verification.

All combined, including both Santa Clara County documents and documents from across the country, we collected 3,801 annotations of deed pages, of which 2,987 (78.6\%) contained a racially restrictive covenant. Notably, this annotation requires human review, but at a much smaller scale than reviewing all records.  






 




