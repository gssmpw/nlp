
\section{Evaluation}
\label{sec:evaluation}

We evaluate model performance using standard classification metrics on our evaluation set, which comprises 739 pages of human-annotated deeds from Santa Clara County. We also compare this against the resources required to run the model across the full dataset of Santa Clara County deeds. We find that a finetuned open model (Mistral) outperforms all other methods.


\subsection{Evaluation Metrics for Detection}
\label{sec:evalmetrics}

To assess models' abilities to detect racially restrictive covenants, we compute page-level precision, recall, and F1 and span-level BLEU scores~\citep{papineni2002bleu} (on the model's reproduction of the exact span of the racial covenant) on our evaluation set.

These are calculated on an evaluation set of 739 pages of deeds, which were held out and not used for training. Roughly 7 in 10 of the pages in our evaluation set contain a racial covenant. By contrast, our best-performing detector found that fewer than 2 in 1000 deeds in the full 5.2 million deed collection contain one.

AB 1466 requires that county counsel review every provision to be redacted, so high false positive rates (or low precision) can quickly become burdensome.
Meanwhile, a high false negative rate (or low recall) would mean that some racially restrictive covenants are missed by the system.
We use the BLEU score to assess the overlap between the annotated racially restrictive covenant and the identified text. If the BLEU score is low, this may mean that the identified text span doesn't neatly overlap with the annotated text span. Alternatively, it may mean that the language model ``hallucinated'' information that wasn't in the original document~\citep{rohrbach2018object,magesh2024hallucination}.

\subsection{Results for Detection}

\begin{table}[!htbp]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{BLEU} \\ \midrule
\textit{Keyword Matching} & 0.913 (0.889, 0.930) & 0.971 (0.955, 0.980) & 0.941 & - \\
\textit{Fuzzy Matching} & 0.992 (0.982, 0.996) & 0.898 (0.873, 0.917) & 0.943 & - \\
\textit{GPT Zero-Shot} & 0.993 (0.982, 0.996) & 0.771 (0.738, 0.799) & 0.868 & 0.787 \\
\textit{GPT Few-Shot} & 0.926 (0.904, 0.942) & 0.961 (0.944, 0.973) & 0.943 & 0.773 \\
\textit{Mistral Finetuned} & \textbf{1.000} (0.995, 1.000) & \textbf{0.994} (0.984, 0.997) & \textbf{0.997} & \textbf{0.932} \\ \bottomrule
\end{tabular}
\vspace{0.5em}
\caption{Page-level precision, recall, F1, and span-level BLEU score for each racial covenant detector, with 95\% Wilson score confidence intervals for precision and recall.}
\label{tab:metrics}
\end{table}

\textbf{Effectiveness of Detection.} Table \ref{tab:metrics} shows a summary of our evaluation.

First, we find that the finetuned Mistral model uniformly performed better than all the other detectors across all metrics that we examined.
Importantly, the Mistral model is able to identify more racially restrictive covenants than any other method (recall of 99.4\%), while never, in our evaluation set, misidentifying other text as a racially restrictive covenant (precision of 100\%).\footnote{We note in \Cref{ref:integration}, however, that at deployment time we ran additional manual review and identified some false positives in out-of-distribution documents, such as the example shown in Figure~\ref{fig:example_errors}, bottom left.} We also find that the finetuned Mistral model mitigates ``hallucinations'' that are found in other language model-based approaches. 

Second, keyword-based detectors exhibited a range of errors, illustrated in \Cref{fig:example_errors}. For keyword matching, lower OCR quality meant that some racial covenants would be missed due to racial terms being misspelled. Conversely, terms could wrongly appear within a misspelled word, causing a high false positive rate of 28.9\%. The regular expression-based fuzzy matching detector addressed some of these challenges and the false positive rate between the two dropped from 28.9\% to 2.2\%. However, the lack of understanding of context, for both of these approaches, resulted in more significant errors overall.

Third, off-the-shelf language models exhibited only modest improvements, if any, over keyword matching. The initial GPT 3.5 Turbo detector with zero shot prompting had a false negative rate of 22.9\%, likely due to the lack of similar language or tasks in the training data. Few-shot prompting provided the model with more examples of racial covenants. This reduced the false negative rate to 3.9\%, but the false positive rate increased to 23.9\%. The majority of false predictions from the few shot model appeared to be ``hallucinations,'' as evidenced by the lower BLEU score.  

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{images/FalseCases.pdf}
    \caption{False positive and false negative predictions from each detection approach considered in our study. These examples show typical OCR errors present in our pipeline.}
    \vspace{2mm}
    \label{fig:example_errors}
\end{figure}














\textbf{Efficiency.} We then compare the costs of the different approaches for processing all 5.2 million Santa Clara County documents, as seen in Table \ref{tab:cost_comparison}.\footnote{Please refer to Appendix~\ref{sec:resource-cost-comparison} for a detailed resource comparison of each approach.} 
Due to the immense volume of property deeds owned by the county, we estimate that manual review, while likely effective, would take 86,667 hours of paid labor for a single individual to complete exhaustively. That kind of review would have required an enormously expensive undertaking, making it nearly impossible to implement AB 1466 in a timely manner. Recall, for instance, that Los Angeles is contracting with a vendor for \$8M to complete the task over a seven year period. 

By contrast, our finetuned Mistral model can process 5.2 million pages for less than \$300 of compute costs on any commodity cloud provider. Consider the cost comparison to closed models. While an off-the-shelf language model like GPT-3.5 Turbo shows promising performance in a few-shot setup, running it across the County's entire collection would cost 43 times as much (more than \$13,000 compared to \$300).\footnote{See Appendix~\ref{sec:resource-cost-comparison}.}

These comparisons illustrate a significant performance and cost-advantage to open models in this context. 

\subsection{Evaluating the Geolocation Pipeline}

Due to the resource-intensive nature of validating our geolocation pipeline, we conduct a smaller-scale sample review. We sample 50 geolocated documents and manually verify correctness. Within this sample, we did not identify any misidentified geolocations, but it is possible that there are errors. Any discussion of specific locations, however, are manually verified. 
