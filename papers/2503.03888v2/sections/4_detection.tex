\section{An AI-based Racial Covenant Detection System}
\label{sec:pipeline}

Our racial covenant detection pipeline first takes deed images, converting them to text through optical character recognition (OCR) machine learning models. We pre-process and clean all of these documents (\S~\ref{sec:preprocessing}).
Next, we process this text to detect racial covenants. We test five different approaches for this, including a custom finetuned large language model (\S~\ref{sec:detection}). Then we identify the part of the original image containing the racial covenant, highlighting it for Santa Clara County officials (\S~\ref{sec:span}). Finally, we extract any geographic information from the deed to help map the location of the racial covenant (\S~\ref{sec:geo}). 

\subsection{Preprocessing and OCR}
\label{sec:preprocessing}
Our data preprocessing pipeline operates on property deeds split at the page level. We run OCR to retrieve the text of each page. We used the docTR OCR library~\citep{doctr2021} to run a VGG16 model~\citep{simonyan2014very} for text recognition and a DB-ResNet-50 model~\citep{liao2020real} for text detection.


\subsection{Racial Covenant Detection}
\label{sec:detection}

Once converted to text, we test five approaches for identifying and extracting racial covenants: keyword matching, fuzzy keyword matching, zero-shot GPT-3.5 Turbo, few-shot GPT-3.5 Turbo, and a custom finetuned Mistral 7B parameter open source model.

\textbf{Keyword matching.} The rudimentary approach approximates keyword searches that humans may conduct on digitized text, and follows the baseline that the county had established.  The approach matches historically common racial or ethnic terms in the text, such as ``Caucasian,'' ``Mongolian,'' or ``Negro.'' We implemented a simple substring matching approach to detect these keywords. We construct this list by first conducting a manual review of known racial covenants and then manually adding additional variations. We also compared this list to a list of terms compiled by the County, which was comparable.\footnote{The full list of keywords used by the County is included in Section~\ref{sec:scc-keyword-list} of the Appendix.}

\textbf{Fuzzy Keyword Matching.} Keyword matching identifies some racial covenants, but OCR errors can result in many false negatives. For instance, the term ``Caucasian'' was sometimes transcribed as ``Caucian'' or ``Causasan'' on scans with significant visual artifacts. To address these issues, we implement a simple augmentation to keyword matching: {fuzzy keyword matching}.

Our fuzzy matching approach first tokenizes deed text into words. Then, each word is broken down into a set of trigrams. We then compute the cosine similarity between the set of trigrams in each word of the deed and the trigrams of each keyword in our list. We conclude a deed contains a keyword if the maximum cosine similarity between any word in the deed and any keyword in our list is greater than the threshold value of 0.75.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/Pipeline_v3.pdf}
    \caption[]{\emph{Diagram of our pipeline for detecting racial  covenants.} The process begins by converting an image of a property deed into text using an OCR tool (docTR). The transcribed text is then analyzed for racially discriminatory language. If such unlawful language is found, the system highlights the content and extracts the property address. Both the highlighted language and the corresponding address are sent then to Santa Clara County for legal review and final confirmation.}
    \label{fig:annual_rrc_numbers}
\end{figure}

\textbf{Large Language Models.} Lexical approaches -- while computationally inexpensive and fast -- are still inherently limited by their lack of contextual understanding of the textual content. Both lexical approaches described above require the pre-specification of a set of keywords to match. Even minor perturbations of the term list can lead to significant changes in output quality. For instance, the term ``white'' is a key phrase used in many racial covenants, but those usages are far outnumbered by innocuous uses of the term, such as in street names, surnames, descriptions of an object's color, and other contexts. In addition, the specific terms commonly used in racial covenants vary significantly by time period and location.

To address this, we design detectors based on large language models (LLMs), which have shown promise in other natural language generation and understanding settings. We test three versions.

First, we evaluate the closed-weight GPT-3.5 Turbo model with a zero shot prompt (asking the model to directly identify a racial covenant); specifically, the \verb|gpt-3.5-turbo-1106| model.\footnote{At the time of the work described in this paper (mid-2024), the two most capable general-purpose LLMs were GPT-4 and GPT-3.5 Turbo, both closed models offered by OpenAI. As GPT-4 would have been much more expensive for the scale of the task, see Section~\ref{sec:resource-cost-comparison}, we excluded it from evaluation.} The input prompt we used can be found in Appendix \ref{sec:instruction-finetuning-prompt}.

Second, we test a few-shot approach with the GPT-3.5 Turbo Model, where two examples of correctly annotated racial covenants are additionally included in the prompt.


Third, we finetune an open LLM for the racial covenant detection task. Given the scale of the racial covenant task, the reliance on an open model poses particular potential benefits: we are able to take advantage of our full labeled training data and the cost of running inference may be substantially lower than paying per API request for GPT. We used Mistral 7B as a base model, then the state-of-the-art open LLM for its size \citep{jiang_mistral_2023}. We finetuned the model with low-rank adaptation (LoRA) \citep{hu_lora_2021} at 16-bit precision on a single A100 80GB GPU. Our custom model was trained on 80\% of our annotations; the remainder (739 pages\footnote{During the evaluation process, we discovered a small number of duplicate pages in our dataset and excluded them from evaluation; 739 pages is therefore slightly less than 20\% of the total dataset.}) was used to evaluate all of the detectors described in this section. The prompt template used for finetuning can be found in Figure~\ref{fig:instruction-finetuning-template}.

For each of our LLM detectors, we also compute a confidence score for the model's classification:
$$\text{confidence} = \text{softmax}(\mathbf{w})_{\text{yes}}$$
where $\mathbf{w} = [w_{\text{yes}}, w_{\text{no}}]$ are the logits for the ``yes'' and ``no'' tokens. We empirically determined that a 75\% confidence threshold resulted in the best performance for the LLM detectors; the evaluation results shown in Section~\ref{sec:evalmetrics} are computed based on that threshold.

\subsection{Racial Covenant Span Recovery} \label{sec:span}

In order to operationally integrate model output for county recorder and counsel review, it was important to pinpoint the racial covenant on the page image, not just in the OCR text. We hence develop an approach to plot a bounding box around the racial covenant provision on a deed page. Our OCR system provides us character-level positional metadata for each page, allowing us to compute a bounding box for any substring of the text. 

In practice, LLMs prompted (or trained) to return an exact span of input text do not always do so, and instead return text with minor variations. This is especially true for text with severe OCR artifacts. To solve this problem, we implement a fuzzy matching algorithm to identify the span of text in a deed page closest to the LLM's output. First, we break the deed text into chunks with a sliding window of size equal to the length of the LLM output. Then, we tokenize each chunk into trigrams and compute the Jaccard similarity between each chunk's tokens and the tokens of the LLM's output. Finally, we select the span with the highest similarity and apply simple heuristics to align the span to sentence and paragraph boundaries.

\subsection{Geolocation}
\label{sec:geo}

One of the major contributions of existing crowdsourcing efforts lies in the geographic characterization of racial covenants. Such geolocation information is important for understanding patterns of housing development, segregation, and mobility, and is hence also important for empirical research on and historical understanding of racial covenants. For instance, testing the long-term effects of covenants on housing segregation requires understanding which properties were and were not subject to racial covenants. 

Unfortunately, the County's pre-1980 microfilm deed records do not contain structured data, such as ownership information, present-day location, or parcel numbers. The initial perception by all parties was that resolving deed records to present day geolocation would be near impossible without significant manual effort. 

As Figure \ref{fig:location_in_deed} shows, however, the \emph{text} of older deeds does contain some limited information about the location of the properties in question. This text is difficult to parse, referencing, for instance, a ``Map [that] was recorded \dots June 6, 1896, in Book `I' of Maps at page 25.''  

By extracting these textual clues and cross-referencing them against multiple administrative datasets from the County's Surveyor's Office, we show that it is possible to recover the location of individual properties to the tract level (several blocks) for most properties. This is particularly significant as county recorders have engaged surveyors for custom projects to conduct such geo-referencing. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/location_in_deed_more_redact.png}
    \caption{Example of location information in a 1916 property deed. Crucially, we can extract the name of the map which depicts the property, as well as the book and page number on which the map appears. Other useful data, such as the names of the parties and the exact date on which the deed was recorded, can also be extracted.}
    \label{fig:location_in_deed}
\end{figure}

Our pipeline for geolocating properties consists of several steps:

\begin{enumerate}
    \item \textbf{Information Extraction}: We use a few-shot prompted LLM (\texttt{gpt-4o-mini}) to extract key geographic information from the OCR text, including the name of the referenced map, book number, page number, and other relevant details such as mentioned street names.
    
    \item \textbf{Map Matching}: We cross-reference the extracted map information against a list of maps published by the Santa Clara County Surveyor's Office. Individual maps typically encompass a single developer's subdivision, usually the size of a city block. This step involves fuzzy matching the extracted map name against the names in the spreadsheet to account for OCR errors and variations in naming conventions. The Surveyor's Office records provide exact book and page numbers for each map, as well as scans of the (often hand-drawn) maps themselves.
    
    \item \textbf{Geospatial Lookup}: Using the verified book and page numbers, we query the Surveyor's ArcGIS system to retrieve the precise geographic location of a given map. This system contains geospatial data for many historical maps, allowing us to pinpoint the location of the property in question.
    
    \item \textbf{Manual Augmentation}: For cases where the ArcGIS system lacks coverage, we conducted manual research using scans of historical maps available at the Surveyor's office. We focused on the most frequently occurring maps not found in the digital system, manually determining their present-day locations to enhance our dataset's coverage.
\end{enumerate}

While not exhaustive, this approach allows us to construct a granular picture of the geographic distribution of racial covenants over time, down to at least the level of individual neighborhoods. It also enables us to identify which developers and individuals were instrumental in their proliferation. The resulting dataset allows for a rich analysis of spatial patterns of restrictive covenants and their potential long-term impact on residential segregation in Santa Clara County.