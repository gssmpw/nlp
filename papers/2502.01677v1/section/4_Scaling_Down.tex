\section{Scaling Down: Refining Core Functions} \label{sec:scaling_down}
As models become increasingly large and complex through \textit{Scaling Up}, their training, deployment, and maintenance demand significant computational, memory, and energy resources. These challenges limit accessibility and scalability.
A critical question emerges: \textit{how can we maintain or improve model effectiveness while reducing size and computational requirements?}
Drawing inspiration from the human brain, where specialized small units handle essential functions while auxiliary components support adaptability and memory, the \textbf{Scaling Down} concept offers a novel approach. By identifying and extracting the essential functional modules of large models, Scaling Down makes it possible to reduce the model size and computation costs significantly while retaining or even enhancing key capabilities.
Scaling Down can be approached in \textit{two distinct ways}. The first involves directly reducing the model size by decreasing the number or precision of parameters (Section \ref{sec:scaling_down:small}). 
Alternatively, minimizing redundant or unnecessary computations can enhance computational efficiency without altering the number or precision of parameters (Section \ref{sec:scaling_down:computing}).

\subsection{Reducing the Size of Large Models} \label{sec:scaling_down:small}

    The most straightforward approach to reducing model size involves reducing the number of parameters within a model. \textbf{Pruning} achieves this by simplifying neural networks through the removal of less significant components \cite{lecun1989optimal, han2015deep, molchanov2016pruning}.
    LLM-Pruner \cite{ma2023llm} proposes a task-agnostic approach to structural pruning by selectively removing non-critical structures using gradient information.
    Wanda \cite{sun2023simple} emphasizes simplicity and efficiency by pruning weights based on the product of weight magnitudes and corresponding input activations without the need for retraining or weight updates.
        
    An alternative to directly removing parameters is the use of \textbf{low-rank approximations}, which employ smaller matrices to approximate larger ones \citet{sainath2013low}. 
    Low-Rank Adaptation (LoRA) \cite{hu2021lora} tackles the inefficiency of fine-tuning all model parameters by introducing trainable low-rank decomposition matrices into Transformer layers while keeping the pre-trained model weights frozen.
    Linformer \cite{wang2020linformer} leverages the observation that self-attention mechanisms in Transformers exhibit low-rank structures. By approximating the self-attention matrix with a low-rank factorization, Linformer reduces the time and space complexity of self-attention to a linear scale.
    
    Another effective strategy focuses on reducing parameter precision rather than quantity. \textbf{Quantization} reduces the bit-width of weights and activations by substituting floating-point parameters with integers \cite{gupta2015deep, nagel2020up}.
    GPTQ \cite{frantar2022gptq} introduces an efficient one-shot weight quantization method based on approximate second-order information. 
    AWQ \cite{lin2024awq} focuses on activation-aware weight quantization, leveraging the unequal importance of weights and optimal per-channel scaling to protect salient weights.
    QLoRA \cite{dettmers2024qlora} introduces a memory-efficient fine-tuning approach by combining 4-bit quantization with LoRA. 

    Rather than modifying existing models, \textbf{knowledge distillation} (KD) facilitates the transfer of knowledge from large and complex teachers to small and efficient students \cite{hinton2015distilling}. The students are trained to replicate the behavior of the teachers.
    \citet{yu2024distilling} propose to distill System 2 reasoning processes—such as Chain-of-Thought and System 2 Attention—into a single-step System 1 model, eliminating intermediate reasoning while retaining or improving task performance.
    Program-aided Distillation (PaD) \cite{zhu2024pad} introduces a new KD paradigm that uses reasoning programs to verify and refine synthetic CoT data, enhancing distilled reasoning quality. PaD automates error-checking, incorporates iterative self-refinement to address faulty reasoning chains, and employs step-wise beam search to validate reasoning steps progressively.

\subsection{Optimizing Computational Efficiency} \label{sec:scaling_down:computing}

    \textbf{Speculative decoding} optimizes the inference process by dynamically adapting decoding strategies.
    \citet{leviathan2023fast} introduce speculative decoding as a method that leverages more efficient approximation models to propose candidate tokens, which are then verified by the target model in parallel. 
    Similarly, \citet{chen2023accelerating} propose speculative sampling, employing a draft model to generate multiple token candidates, which are then validated using a modified rejection sampling scheme. 
    These methods underscore the potential of speculative execution to mitigate the inherent inefficiencies of autoregressive decoding, enabling faster inference without retraining or compromising output quality.
    
    \textbf{Key-value cache} is a pivotal strategy in autoregressive decoding, where intermediate states of attention mechanisms are stored to avoid recomputation in subsequent inference steps. This technique significantly accelerates the generation of long sequences by leveraging stored key-value pairs from previous layers. However, it introduces additional memory overhead, which must be carefully managed. 
    Sparse attention mechanisms \cite{zhang2023h2o, anagnostidis2024dynamic, liu2024scissorhands} use specialized sparsity patterns that prevent unnecessary token access. They use KV cache eviction and compression strategies to achieve significant improvements in latency, throughput, and memory savings.
    Block-wise KV cache management \cite{kwon2023efficient, prabhu2024vattention} adopts memory fragmentation techniques inspired by paged memory systems, offering efficient runtime memory allocation and reallocation.


    \textbf{Mixture of Experts} (MoE) introduced distributed specialization, enabling efficient scaling through task-specific sub-models controlled by a gating mechanism \citep{jacobs1991adaptive}.
    Early dense MoE models suffered computational inefficiencies \citep{jordan1994hierarchical}.
    Sparse architectures \citep{shazeer2017outrageously} improved efficiency by selectively activating relevant experts. Models like GShard \citep{lepikhin2020gshard}, Switch Transformer \citep{fedus2022review}, and GLaM \citep{du2022glam} leveraged MoE for state-of-the-art performance with reduced computation. Recent advances, including Mixtral \citep{jiang2024mixtral} and DeepSeekMoE \citep{dai2024deepseekmoe}, further optimized efficiency.

\subsection{Small Models for Large Impacts}

    Designing high-efficiency architectures is fundamental to developing small-scale models. 
    The most computationally intensive and memory-intensive component of Transformer-based models is the Attention mechanism. 
    Extensive research efforts have been devoted to enhancing the efficiency of Attention mechanisms. Notable advancements include \textit{Flash Attention} \cite{dao2022flashattention}, which is utilized by models such as Phi-1.5 \cite{li2023textbooks} and DeepSeek-LLM \cite{bi2024deepseek}, \textit{Grouped Query Attention} \cite{ainslie2023gqa}, which is utilized by MiniCPM \cite{hu2024minicpm}, Mistral \cite{jiang2023mistral}, Phi-3 \cite{abdin2024phi}, DeepSeek-LLM \cite{bi2024deepseek}, and DeepSeek-V2 \cite{liu2024deepseek}, and \textit{Multi-Head Latent Attention}, which was first introduced by DeepSeek-V2 \cite{liu2024deepseek} and has been adopted in its successor, DeepSeek-V3 \cite{liu2024deepseek3}.

    While such innovations enable the development of highly efficient small models, further improvements are necessary to bridge the performance gap between small and large-scale models. Key directions for achieving this include curating high-quality training data, designing scalable training strategies, and leveraging techniques such as mixture-of-experts (MoE), which allow for the selective activation of model components to optimize efficiency and performance.

    \textbf{High-Quality Training Data.}
    The Phi family of models \cite{gunasekar2023textbooks, li2023textbooks, javaheripi2023phi, abdin2024phi} highlights the importance of high-quality training data. Rather than relying on vast amounts of noisy web-scraped text, these models are trained on curated, synthetically generated textbook-style data, including structured exercises and carefully filtered educational content. This approach enhances efficiency and mitigates common issues such as hallucination and bias.

    \textbf{Scalable Training Strategies.}
    Training efficiency is another critical factor in developing compact yet powerful models. Mini-CPM \cite{hu2024minicpm} introduces Model Wind Tunnel Experiments (MWTE) to optimize hyperparameter selection, ensuring that smaller models are trained in a computationally efficient manner. Additionally, it employs the Warmup-Stable-Decay (WSD) learning rate scheduler, which segments training into distinct phases to maximize hardware utilization and improve convergence.
    
    \textbf{More Parameters but Less Activation.}
    A crucial trend in optimizing smaller models for efficiency is the adoption of Mixture-of-Experts (MoE) architectures, where a subset of model parameters is activated per token, reducing computation while maintaining a large overall parameter pool. Several recent models exemplify this technique:
    Mixtral \cite{jiang2024mixtral} consists of 8 expert models with a total of 56B parameters, while each token is processed by only 2 experts.
    Phi-3.5-MoE \cite{abdin2024phi} comprises 16 experts totaling 60.8B parameters but activates only 6.6B (10.9\%).
    DeepSeek-V2 \cite{liu2024deepseek} is a 236B-parameter model with 21B parameters activated per token (8.9\%).
    DeepSeek-V3 \cite{liu2024deepseek3} scales further to 671B total parameters while activating only 37B per token (5.5\%).
    These MoE-based models show that large parameter spaces can be efficiently leveraged without incurring excessive computational costs, allowing smaller models to achieve performance comparable to their larger counterparts with only a fraction of the active compute requirements.
    
\subsection{Future Trends}

    \textbf{Core Functional Module Refinement.}
    A promising direction for future research in Scaling Down models lies in refining core functional modules. While existing methods predominantly emphasize the balance between efficiency and effectiveness, a critical gap remains in identifying the minimal functional module within large models. This minimal module would represent the smallest possible unit that retains all essential functionalities without compromising performance. Future investigations may focus on developing systematic approaches to detect and characterize such modules, potentially leveraging advancements in model pruning and knowledge distillation. Establishing rigorous criteria for defining and verifying minimal functional modules could significantly contribute to optimizing model architectures while maintaining their operational integrity.
    
    \textbf{External Assistance.}
    Leveraging external assistance enables small-scale core models to dynamically extend their capacity to handle complex tasks. 
    Retrieval-Augmented Generation (RAG) \cite{lewis2020retrieval} is a method for \textit{external knowledge} augmentation. RAG combines pre-trained parametric memory with non-parametric memory,    which enables models to fetch contextually relevant information dynamically.
    Integrating \textit{external tools} allows models to assign specialized operations to certain systems.
    Toolformer \cite{schick2023toolformer} can autonomously learn to invoke external APIs, such as calculators, search engines, and translation systems.
    Beyond merely utilizing external tools, recent advancements suggest that models can also generate tools to extend their own capabilities. VISPROG \cite{gupta2023visual} can leverage in-context learning to produce modular, Python-like programs and execute them for complex visual reasoning tasks. 