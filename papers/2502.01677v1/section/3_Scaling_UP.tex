\section{Scaling Up: Expanding Foundation Models}
    Scaling Up is critical for advancing AI research and applications as it pushes the boundaries of what AI systems can achieve. Larger models act as high-quality foundational models for both academia and industry, further setting benchmarks and inspiring further innovations. These models are capable of solving a wide range of tasks while they can serve as a foundation for creating specialized and diverse AI interfaces through fine-tuning as well. 
    
\subsection{Scaling in AI models}
The past experience in AI Scaling Up is mostly based on increasing data size, model size and computational resources.

\textbf{Data Size.}
Expanding dataset size is a fundamental aspect of Scaling Up AI models, as it directly impacts the quality of the system. Large and diverse datasets expose models to a wide variety of knowledge, thereby enabling them to perform effectively across multiple domains. For instance, GPT-3~\cite{brown2020language} was trained on 570GB of cleaned and curated text data drawn from sources such as Common Crawl, BooksCorpus, and Wikipedia, which enabled it to generate human-like responses across diverse contexts. 
More recently, multi-modal datasets such as LAION-5B~\cite{schuhmann2022laion} have been used to scale vision-language models like Stable Diffusion~\cite{rombach2022high}, showcasing the impact of data size on model's capabilities. 
    
\textbf{Model Size.}
Larger models have greater representational power, allowing them to capture complex relationships within data. For example, the 175B parameters of GPT-3 significantly outperform their predecessors in tasks that require learning of few shots or zero shots~\cite{brown2020language}. Similarly, GLaM~\cite{du2022glam} scaled to 1.2 trillion parameters using a mixture of experts, activating only a subset of parameters per task, which reduced computational costs while maintaining high performance. The scaling laws proposed by~\cite{kaplan2020scaling} highlight that model performance improves predictably with increased size. This insight has guided the development of increasingly large models, unlocking capabilities like in-context learning and cross-modal understanding.


\textbf{Computational Resources.}
The process of scaling computational resources has evolved dramatically alongside advancements in AI, particularly in computer vision and NLP. Early in the development of these fields, training models required only a few plain GPUs. For instance, AlexNet~\cite{krizhevsky2012imagenet}, which revolutionized computer vision in 2012, was trained using just two GTX 580 GPUs. Similarly, early NLP models such as Word2Vec~\cite{church2017word2vec} were trained on modest computational setups. However, the era of LLMs has accelerated in unprecedented demands for computational resources. For example, OpenAI’s GPT-3~\cite{brown2020language} has 175B parameters and requires 10,000 NVIDIA V100 GPUs for training, consuming an estimated 1,287 MWh of electricity. Meanwhile, Meta’s LLaMA 3~\cite{touvron2023llama} scaled training to utilize thousands of NVIDIA A100 GPUs, representing the latest generation of high-performance accelerators optimized for AI workloads. This progression highlights the critical role of computational scaling in AI model's performance.


\subsection{Bottleneck}
From the \textit{data} perspective, as pointed out by many researchers, large-scale pretraining has already utilized most of the high-quality publicly available data on the web. The remaining data is either low-quality or consists of AI-generated content, which risks model degradation due to data contamination and reinforcement of biases~\cite{shumailov2024ai}. Simply increasing the dataset size will no longer yield the same level of improvement as before. From the \textit{model} perspective, while increasing parameters has led to substantial performance gains in recent years, the returns on scaling have shown diminishing improvements, and larger models suffer from inefficiencies such as redundancy in representation, overfitting to training distributions, and difficulties in interpretability and controllability. Additionally, the training and inference of massive models introduce challenges in optimization stability and robustness~\cite{dai2024enhancing}. From the \textit{computational resource} aspect, the exponential growth in required hardware, energy consumption, and costs is reaching unsustainable levels. The marginal benefit of adding more compute is decreasing while the environmental impact is rising~\cite{wu2024beyond}. The availability of high-performance GPUs poses financial constraints that limit the feasibility of further scaling. 
Together, these bottlenecks indicate that the traditional approach of scaling up is approaching its practical limits.

\subsection{Future Trends}
Despite bottlenecks in AI scaling, Scaling Up remains essential for pushing AI model's performance boundary. The future of Scaling Up should lie in balancing efficiency, adaptability and sustainability to meet the demands of larger models. Innovations in dataset optimization, efficient training, and test-time scaling will redefine AI Scaling Up.

\textbf{Dataset Optimization.}
As AI continues to scale, data optimization will become a cornerstone for advancing model efficiency and robustness. Future trends will focus on data-efficient training using smaller, high-quality datasets for faster learning. Curriculum learning~\cite{bengio2009curriculum} and active learning~\cite{settles2009active} will help models acquire knowledge incrementally and prioritize impactful samples. Techniques for handling noisy data, such as noise-robust loss functions and data augmentation, will enhance model resilience. Additionally, leveraging proprietary, domain-specific datasets will drive breakthroughs by providing richer insights beyond public data.
 
\textbf{Efficient Training.}
Another trend is developing efficient training methods to address the growing computational and environmental costs of training large models. Progressive training, where models gradually scale from smaller sub-models to full-capacity systems, will become a standard approach to reduce resource demands in the initial stages. Distributed optimization techniques, such as asynchronous training paradigms, will improve scalability across large computational infrastructures. Advances in mixed-precision training, sparse updates, and activation checkpointing will further minimize memory and compute overhead, making AI development more sustainable and scalable. 
% These trends are set to redefine how foundational models are trained, enabling powerful capabilities without imposing unsustainable hardware and energy requirements.
    
\textbf{Test-time Scaling.}
Recent research has highlighted the potential of scaling up test-time computing to enhance the performance of large language models (LLMs), providing an alternative to solely scaling up model parameters. For example,  
\citet{snell2024scaling} explore two strategies: adaptive output distribution and verifier-based search mechanisms, both improving model performance dynamically. Unlike previous inference-time optimization attempts, this approach tailors compute allocation to problem complexity, enabling smaller models to outperform larger ones on certain prompts. Adaptive test-time scaling presents a promising direction for optimizing efficiency without excessive pretraining.
