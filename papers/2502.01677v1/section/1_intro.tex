\section{Introduction}
The field of artificial intelligence (AI) has witnessed extraordinary advancements over the past decade, largely driven by the relentless pursuit of Scaling Up. Early breakthroughs were characterized by models with millions of parameters, such as AlexNet~\cite{krizhevsky2012imagenet}, word2vec~\cite{church2017word2vec} and BERT~\cite{devlin2018bert}, which paved the way for deep learning's success. This progression quickly escalated to models with billions of parameters, exemplified by GPT-3 (175 billion parameters)~\cite{brown2020language} and more recently GPT-4~\cite{achiam2023gpt}, which has further expanded the boundaries of language understanding and generation. Similarly, vision-language models like CLIP~\cite{radford2021learning} and Flamingo~\cite{alayrac2022flamingo} have showcased the transformative power of scaling multimodal architectures. These advancements highlight how Scaling Up has enabled AI systems to achieve remarkable generalization and versatility across diverse tasks.

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=1\linewidth]{assets/diagram-scaling.pdf}
    \vspace{-1em}
    \caption{The proposed framework for AI Scaling that integrates: (a) \textbf{Scale Up} increases model size and complexity, enhancing performance but demanding more computational resources. (b) \textbf{Scale Down} reduces model size and distills the essence of these systems into a smaller, more efficient core model. (c) \textbf{Scale Out} leverages the core model to derive multiple task-specific interfaces, enabling adaptation to diverse tasks and interaction with the environment.}
    \vspace{-1em}
    \label{fig:scaling-diagram}
\end{figure}

However, as Scaling Up progresses, the field faces a critical bottleneck: data~\cite{shumailov2024ai}. The success of scaling has been largely contingent on the availability of massive, high-quality datasets. Foundational datasets like Common Crawl\footnote{https://commoncrawl.org/} and large-scale multimodal Corpus have been extensively mined, leaving diminishing returns from further expansion. While multimodal data sources remain an underexplored frontier, their integration presents unique challenges, including alignment across modalities and domain-specific constraints. Moreover, the cost of processing this data at scale, in terms of both computational energy and infrastructure demands, compounds the difficulty of sustaining the current paradigm. These challenges underscore a pivotal question: \textit{can Scaling Up alone continue to deliver transformative progress, or are new paradigms required to achieve the ultimate vision of Artificial General Intelligence (AGI)}?


This position paper presents a holistic framework for AI scaling (Fig. \ref{fig:scaling-diagram}), encompassing Scaling Up, Scaling Down, and Scaling Out. It argues that \textbf{while Scaling Up of models faces inherent bottlenecks, the future trajectory of AI scaling lies in Scaling Down and Scaling Out}.
They form a natural progression in the evolution of AI, each building on the achievements and limitations of the previous. Scaling Up represents the exploratory frontier, pushing the boundaries of model performance by increasing parameter counts, training on vast datasets, and leveraging unprecedented computational resources. This phase is critical, as it establishes benchmarks and reveals the upper limits of what AI systems can achieve. For instance, Scaling Up to models like GPT-4 demonstrates the potential for generalization across tasks, offering a roadmap for what optimal performance could look like across diverse domains.

However, Scaling Up comes at a cost—computational, financial, and environmental. These costs, coupled with the diminishing returns due to data saturation, necessitate a shift to Scaling Down. Importantly, Scaling Down is guided by the insights gained from Scaling Up. By analyzing the structure and performance of large-scale models, researchers can identify redundancies, prune parameters, and distill the essence of these systems into smaller, more efficient models. Scaling Down thus becomes a process of optimization, reducing model size while retaining or even enhancing performance for specific tasks. For example, foundational models with hundreds of billions of parameters can be scaled down to a fraction of their size, enabling deployment on edge devices and in resource-constrained environments. This phase democratizes AI, making advanced capabilities accessible to a broader audience and reducing barriers to entry for smaller organizations.

Scaling Out builds upon the advancements achieved through Scaling Down, leveraging lightweight and efficient models to enable large-scale deployment across distributed environments. Rather than relying solely on a single, monolithic AI model, Scaling Out envisions an AI ecosystem where a foundation model serves as the core intelligence, from which specialized models emerge to address specific tasks or domains. These specialized models interact with the world through \emph{structured interfaces}, such as APIs or intelligent agents, forming a decentralized network of AI-driven applications. For example, on content creation platforms like TikTok, YouTube, and Instagram, foundation models such as LLaMA~\cite{touvron2023llama} and QWEN~\cite{yang2024qwen2} have given rise to numerous fine-tuned variants that cater to distinct creative needs, such as AI models optimized for video scriptwriting, personalized recommendation, or style-consistent image generation. These models, acting as interfaces, empower both human creators and AI-driven content generation pipelines, fostering a dynamic ecosystem where AI seamlessly integrates into workflows. Scaling Out does not seek to replace human creativity but instead enhances global cultural exchange by providing adaptable, interactive AI systems that extend AI’s reach beyond isolated models into an interconnected and ever-evolving ecosystem.

The progression from Scaling Up to Scaling Down and then Scaling Out is not merely sequential but interdependent. Scaling Up defines the theoretical and practical benchmarks for AI performance. Scaling Down operationalizes these benchmarks, ensuring they are achievable in diverse environments. Scaling Out amplifies these capabilities, enabling AI to thrive in dynamic, decentralized systems. Together, these paradigms form a cohesive framework that transforms AI from a centralized, high-resource endeavor into a distributed, inclusive, and adaptive force capable of addressing humanity's most complex challenges. 