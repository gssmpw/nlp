\documentclass[twoside]{article}
\usepackage[accepted]{aistats2025}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{url}
\usepackage{cleveref}
\usepackage{mathtools}
\usepackage{stackengine}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{authblk}

% \usepackage{natbib}

% \setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}
\crefname{table}{table}{tables}
\Crefname{table}{Table}{Tables}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
% \newtheorem{lemma}{Lemma}

\declaretheorem[name=Lemma,numberwithin=section]{lemma}
\declaretheorem[name=Theorem,numberwithin=section]{thm}

% \title{Deep Clustering via Probabilistic Ratio-Cut Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\runningauthor{Ayoub Ghriss, Claire Monteleoni}

\twocolumn[
    \aistatstitle{Deep Clustering via Probabilistic Ratio-Cut Optimization}
    \aistatsauthor{
        Ayoub Ghriss\textsuperscript{1}\\ayoub.ghriss@colorado.edu 
        \And 
        Claire Monteleoni\textsuperscript{1,2}\\cmontel@colorado.edu}
    \aistatsaddress{
        \textsuperscript{1}Department of Computer Science, University of Colorado Boulder \\
        \textsuperscript{2} INRIA Paris
    }
]
\begin{abstract}

	We propose a novel approach for optimizing the graph ratio-cut by modeling the binary assignments as random variables. We provide an upper bound on the expected ratio-cut, as well as an unbiased estimate of its gradient, to learn the parameters of the assignment variables in an online setting. The clustering resulting from our probabilistic approach (PRCut) outperforms the Rayleigh quotient relaxation of the combinatorial problem, its online learning extensions, and several widely used methods. We demonstrate that the PRCut clustering closely aligns with the similarity measure and can perform as well as a supervised classifier when label-based similarities are provided. This novel approach can leverage out-of-the-box self-supervised representations to achieve competitive performance and serve as an evaluation method for the quality of these representations.

\end{abstract}

\section{INTRODUCTION}

Unsupervised learning is based on the premise that labels are not necessary for the
training process, particularly for clustering tasks where samples that are highly
similar are grouped together. Various clustering
algorithms~\citep{clusteringsurvey} have been proposed in the context of machine
learning and data mining. The K-Means algorithm~\citep{lloyd} and ratio-cut
partitioning~\citep{ratiocut} were among the first approaches to address the
clustering problem. These methods were further refined and extended beyond binary
partitioning and Euclidean distances, and they were even shown to be fundamentally
equivalent in specific settings~\citep{kernelkmeans}.

The recent advances in generative models and self-supervised representation
learning have produced powerful embeddings that capture the similarity between the
original data samples. This makes leveraging these similarity measures to achieve
effective clustering more relevant than ever, as it can serve as pseudo-labels for
pre-training classifiers or eliminate the need for a decoder~\citep{decoder} in the
training of these generative models. Therefore, it is highly advantageous to
develop a method that can transform the similarity information into clustering of
equal quality. An efficient extension of spectral clustering to stochastic gradient
descent would facilitate the conversion of learned embeddings to cluster
assignments without relying on the full dataset or large batches to accurately
approximate the graph structure of the embedding space.

Several clustering methods have been applied to data streams to provide weak
signals for the pre-training of neural networks. Meanwhile, other approaches have
been proposed to utilize deep learning specifically for clustering, particularly
with autoencoders and generative neural networks~\citep{vade}. Generally, these
methods can be categorized into contrastive learning~\citep{contrastive}, where
samples are grouped based on pairwise distances in a large batch, or generative
models such as variational autoencoders that use a prior with an auxiliary variable
as the cluster assignment. The former methods do not fully capture the global
structure of the clusters, while the latter may suffer from overfitting the prior
due to the complexity of the neural network, with limited options to prevent this
without relying on labels. Spectral clustering avoids these issues by clustering
the data based on global similarities between clusters rather than just pairs of
samples.

Spectral clustering has long resisted attempts to extend it to parametric learning,
primarily due to the challenge of handling the spectral decomposition of large
matrices. Since it is based on the spectral decomposition of the relaxed ratio-cut
rather than the combinatorial version, it requires the projection of the data into
a principal eigenspace. A clustering algorithm is then applied to these
projections, further making the final performance dependent on the chosen
clustering algorithm.

In this paper, we develop a novel method to optimize the ratio-cut without relying
on the spectral decomposition of the Laplacian matrix by employing a probabilistic
approach that treats the cluster assignment as random variables. Furthermore, we
utilize neural networks to parameterize the assignment probabilities and address
the clustering problem in an online manner using stochastic gradient descent. The
result is an online learning algorithm that achieves a better ratio-cut objective
than the memory-intensive spectral method applied to the full graph Laplacian. We
also demonstrate that our approach achieves comparable performance to a supervised
classifier when utilizing supervised similarity (i.e., two samples are considered
similar if they share the same label). This showcases a high fidelity between the
resulting clustering and the given similarity measure. The proposed algorithm can
also serve as a drop-in replacement for any other clustering method used in the
pre-training of text or speech transformers, which opens up several opportunities
for enhancing the effectiveness of pre-training for downstream tasks.

\section{BACKGROUND}
\input{1_background}

\section{RELATED WORK}\label{sec:related}
\input{2_related}

\section{PROPOSED METHOD}
\input{3_method}

\section{EXPERIMENTS}
\input{4_experiments}
\bibliography{aistats2025_paper}
\bibliographystyle{iclr2025}
%\newpage
%\clearpage
%\input{5_checklist}
\clearpage
\onecolumn
\appendix
\input{6_appendix}

\end{document}
