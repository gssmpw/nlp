We train a neural network $ N_\theta: \sR^p \mapsto \Delta^{k-1} $ that maps the
vertex $v_i$ to its cluster assignment probabilities $ \mP^{\theta}_{i} $, where $
\theta \in \R^q $ is the parameter of the network. The last layer of the neural
network is a Softmax layer, ensuring that the constraint $ \sum_{\ell=1}^k
\emP_{i\ell} = 1 $ is always satisfied. We assume that the number of class labels
$k$ is provided. When computing the batch-based gradient
using~\Cref{eq:batchgradient}, we use the factor $\by{b}$ instead of $\by{n}$,
where $ b $ is the batch size\footnote{The code to reproduce PRCut is available at \href{https://github.com/ayghri/prcut}{\texttt{https://github.com/ayghri/prcut}}}.



Since $ \gL_{rc}(\mW, \mP^\theta) $ is linear in
$\mW$, we scale it by $\frac{1}{\normtwo{\mW}_1} $ to ensure consistency in the
gradient descent method across different datasets and similarity measures.

\paragraph{Metrics} To benchmark the performance of our algorithm, we assume that we have access to the
true labeling $\vy=(y_i)_i$ and the algorithm's clustering $\vc$. We use three
different metrics defined as follows:

\begin{itemize}
	\item \textbf{Unsupervised Accuracy (ACC)}: We use the \textit{Kuhn-Munkres} algorithm~\citep{munkres} to find the
	      optimal permutation$\sigma_k$ of $\{1,\ldots,k\}$ such that $\by{n}\max_{\sigma\in
			      \sigma_k} \sum_{i=1}^{n} 1_{y_i = \sigma(c_i)}$ is maximized between the
	      true labeling $(y_i)_i$ and the clustering $(c_i)_i$.

	\item \textbf{Normalized Mutual Information (NMI)} is defined as:
	      $\text{NMI}(\vy, \vc) = \frac{\mathcal{I}(\vy, \vc)}{\max\{\mathcal{H}(\vl),\mathcal{H}(\vc)\}}$
	      Where $\mathcal{I}(\vy, \vc)$ is the mutual information between $\vy$ and $\vc$ and
	      $\gH$ is the entropy measure.
	\item \textbf{Adjusted Rand Index (ARI)} to evaluates the agreement between
	      the true class labels and the learned clustering.
	\item \textbf{Ratio Cut (RC)} is computed using the formula in~\Cref{eq:rawlap}
	      using the raw Laplacian matrix.

\end{itemize}
%   By solving the assignment problem between clusters and true class labels so that 
%  defined as 	      where $\sigma_n$ is the set of permutations of $. The
% optimal $\sigma$ can be computed using theWhen the number of clusters is different from
% the number of class labels, we adopt 
%    we assign the cluster to its most common
% label.

Before we compare our method to spectral clustering and other clustering
approaches, we first assess the quality of the clustering when using the perfect
similarity measure, where $\gK(v_i,v_j)=1$ if $v_i$ and $v_j$ share the same label,
and $\gK(v_i,v_j)=0$ otherwise. For this experiment, we employ a simple Multi-Layer
Perceptron (MLP) consisting of 3 layers with 512 hidden units, followed by Gaussian
Error Linear Units (GeLU) activations. In the PRCut network, the last layer
utilizes the Softmax activation. We compare the two methods across three datasets:
MNIST~\citep{mnist}, Fashion-MNIST (F-MNIST)~\citep{fmnist}, and
CIFAR10~\citep{cifar}. The classifier is trained by optimizing the cross-entropy
loss (CE).

\begin{table}[ht]
	\caption{Benchmarking PRCut when using label-based similarity}
	\centering
	% \begin{tabular}{ll@{\hspace{8mm}}ll} 
	\begin{tabular}{l@{\hspace{5mm}}l@{\hspace{5mm}}ll}
		\toprule
		{Dataset}                & {Method} & ACC            & NMI            \\
		\hline
		\multirow{2}{*}{MNIST}   & CE       & 0.980          & \textbf{0.943} \\
		                         & PRCut    & \textbf{0.987} & 0.938          \\
		\hline
		\multirow{2}{*}{F-MNIST} & CE       & 0.885          & \textbf{0.803} \\
		                         & PRCut    & \textbf{0.887} & 0.789          \\

		\hline
		\multirow{2}{*}{CIFAR10} & CE       & \textbf{0.582} & \textbf{0.369} \\
		                         & PRCut    & 0.571          & 0.359          \\
		\hline
	\end{tabular}
\end{table}

The MLP classifier trained directly on the labeled data and the PRCut clustering
using the labeled similarity demonstrate similar performance. This shows that our
approach is more versatile and can be used in various learning methods while
perfectly reflecting the quality of the similarity in the resulting clustering.

In \Cref{table:exp2} we compare our approach to vanilla Spectral Clustering (SC)
using k-neighbor graph adjacency as a similarity measure for $k=150$ using the
entire training set. Since the final performance depends on the k-means
initialization, we report the best run for SC.

\begin{table}[ht]
	\caption{Comparison between PRCut and Spectral Clustering (SC)}
	\centering
	\begin{tabular}{l@{\hspace{5mm}}l@{\hspace{5mm}}lll}
		\toprule
		{Dataset}                & {Method}  & ACC            & NMI            & RC             \\
		\hline
		\multirow{3}{*}{MNIST}   & SC (Best) & 0.70           & 0.744          & 170.1          \\
		                         & PRCut     & \textbf{0.821} & \textbf{0.778} & \textbf{150.2} \\
		\hline
		\multirow{2}{*}{F-MNIST} & SC (Best) & 0.596          & 0.593          & 110.2          \\
		                         & PRCut     & \textbf{0.658} & \textbf{0.620} & \textbf{101.5} \\
		\hline

		\multirow{2}{*}{CIFAR10} & SC (Best) & 0.217          & 0.086          & 479.3          \\
		                         & PRCut     & \textbf{0.243} & \textbf{0.121} & \textbf{440.3} \\
	\end{tabular}
	\label{table:exp2}
\end{table}

Since the final objective is to minimize the ratio cut, our approach achieves a
better ratio cut value compared to vanilla spectral clustering. Whether such
improvement translates to an enhancement in the clustering metrics depends on the
similarity function. In all the 3 datasets, our approach outperforms the spectral
relaxation of the ratio-cut.

For the third set of experiments reported in \Cref{table:exp3}, we compare our
method to VaDE~\citep{vade}, VMM~\citep{VMM}, and Turtle~\citep{turtle}. We have
observed that VaDE does not generalize well across datasets, as its performance
drastically degrades when applied to the Fashion-MNIST dataset. The VMM approach is
a more consistent variational autoencoder (VAE) with a mixture model prior,
achieving the best reported performance on the Fashion-MNIST dataset in the
literature. We also report the results for methods with the suffix "-D," which use
the pre-trained representation model DINOv2~\citep{dinov2}, while the suffix "-V"
denotes methods that utilize the representation from the CLIP~\citep{clip} vision
transformer. In both of these approaches, the k-neighbors graph and the trained
neural networks take the samples in the representation spaces as input.

The neural network consists of a single linear layer followed by the softmax operator. The learning rate was set to a constant value of $10^{-4}$ and using a $10^{-7}$ weight decay, with a bach size of $b=2048$. We set the entropy regularization to $\gamma=100.0$ and the moving average parameter to $\beta=0.8$.

\begin{table}[ht]
	% \label{exp3}
	\caption{Comparison of PRCut to the best-performing clustering methods.}
	\centering
	% \begin{tabular}{ll@{\hspace{8mm}}ll} 
	\begin{tabular}{l@{\hspace{5mm}}l@{\hspace{5mm}}ll}
		\toprule
		{Dataset}                 & {Method} & ACC            & NMI          \\
		\hline
		\multirow{3}{*}{MNIST}    & SC       & 0.701          & 0.744        \\
		                          & VaDE     & 0.857          & 0.838          \\
		                          & VMM      & \textbf{0.960} & \textbf{0.907} \\
		                          & Turtle-D & 0.573          & 0.544          \\
		                          & PRCut-V  & 0.771           & 0.734         \\
		\hline
		\multirow{3}{*}{F-MNIST}
		                          & SC       & 0.596          & 0.593          \\
		                          & VaDE     & 0.352          & 0.496          \\
		                          & VMM      & 0.712          & 0.688          \\
		                          & Turtle-D & 0.764          & 0.723          \\
		                          & PRCut-D  & \textbf{0.791} & \textbf{0.758} \\
		\hline

		\hline
		\multirow{3}{*}{CIFAR10}  & SC       & 0.217          & 0.121          \\
		                          & Turtle-V & 0.972          & 0.929          \\
		                          & PRCut-V  & \textbf{0.975} & \textbf{0.934} \\
		\hline
		\multirow{3}{*}{CIFAR100} & Turtle-D & \textbf{0.806} & \textbf{0.870} \\
		                          & PRCut-D  & 0.789          & 0.856          \\
		\hline
	\end{tabular}
	\label{table:exp3}
\end{table}

Our method remains competitive compared to Turtle and achieves the best reported
performance on Fashion-MNIST, which was designed to be a more challenging dataset
than MNIST. Furthermore, our approach does not rely on any assumptions about the
structure of the representation spaces, in contrast to Turtle, which is based on
the premise that the best clusters are linearly separableâ€”a property that is
inherently valid for DINOv2 and CLIP. This may explain why it does not perform as
well on grayscale images such as MNIST and Fashion-MNIST, where linear separability
is not as prominent. We note that the Turtle results in our benchmark are based on
a single representation space for a fair comparison, as the original paper performs
best when it combines two representation spaces.

\begin{table}[ht]
	\caption{Comparison representations using PRCut}
	\centering
	% \begin{tabular}{ll@{\hspace{8mm}}ll} 
	\begin{tabular}{l@{\hspace{5mm}}l@{\hspace{5mm}}lll}
		\toprule
		{Dataset}               & {Rep}   & ACC            & NMI            \\
		\hline
		\multirow{3}{*}{CIFAR10} & Raw    & 0.243          & 0.121          \\
		                          & SimCLR  & 0.721          & 0.652          \\
		                          & All4One & 0.710          & 0.635          \\
		                          & VitL-14 & \textbf{0.975} & \textbf{0.934} \\
		                          & DinoV2  & 0.774          & 0.797          \\
		\hline
		\multirow{3}{*}{CIFAR100} & Raw   & 0.054          & 0.022          \\
		                          & SimCLR  & 0.362          & 0.483          \\
		                          & All4One & 0.382          & 0.511          \\
		                          & VitL-14 & 0.720          & 0.755          \\
		                          & DinoV2  & \textbf{0.789} & \textbf{0.856} \\
		\hline
	\end{tabular}
	\label{table:exp4}
\end{table}

In \Cref{table:exp4}, we compare the performance of our method using various
pre-trained self-supervised representation learning models. In particular, we rely
on the \textit{solo-learn} library~\citep{sololearn} to retrieve or fine-tune the
SimCLR and All4One models. While these two approaches perform well when evaluated
using linear probes or a k-nearest neighbor classifier, the similarities in the
embedding space for datasets with a higher number of class labels (CIFAR100) are
too sparse to capture useful clusters. In that regard, DinoV2 embedding performs
better than CLIP vision transformer.

\section{Conclusion and future work}

We have introduced a novel method that approaches the graph ratio-cut optimization
from a probabilistic perspective. Compared to the classical Spectral Clustering
based on the raw Laplacian, PRCut achieves better clustering performance and more
optimal ratio-cut values. However, the performance strongly depends on the sparsity
of the global similarity. With the recently developed self-supervised
representation models that have proven to be powerful, we have demonstrated that
PRCut translates the similarities between samples in the embedding space into high
quality clustering and achieve new best clusterings for Fashion-MNIST while
remaining competitive with state-of-the-art clustering methods.

While the current work serves as an introduction to this novel approach, the
potential extensions of our method appear boundless. For example, PRCut could be
leveraged as a dimensionality reduction technique by incorporating a bottleneck
layer within the clustering neural network. Furthermore, the methodology could be
adapted under the premise of linear separability of the true class labels, similar
to the approach adopted by~\citet{turtle}. The issue of dynamically determining the
optimal number of clusters remains unresolved, given our current assumption that
this information is supplied as an input to our algorithm.

The algorithm can also be extended to offline learning via~\Cref{eq:quad}. Notably,
when considering equally likely clusters, the problem simplifies to a quadratic
form, with the Hessian being proportional to $-\mW$. By leveraging the structure of
the similarity matrix, we can derive additional guarantees about the optimal
solution. This enables the definition of an iterative approach similar to the
Sinkhorn-Knopp algorithm~\citep{sinkhorn}, offering promising avenues for further
exploration.
