In our probabilistic approach, the minimization of the ratio-cut is addressed
differently. Instead of the deterministic assignments $\vone_{\sC_\ell}$, we use
the random assignment vector $\rva^{(\ell)} \in \lrcb{0,1}^n$ under the assumption
that $(\rva_i^{(\ell)})_i$ are independent random variables such that:

\[
	\Prob{v_i \in \sC_\ell} = \Prob{\rva^{(\ell)}_i = 1} \defeq  \emP_{i,\ell},
\]
where the rows of the matrix $\mP\in[0,1]^{n\times k}$ sum to $1$:
  $\sum_{\ell=1}^k \emP_{i,\ell} = 1$ for all $i\in\lrcb{1,\ldots,n}$.
% $\sum_{\ell=1}^k \evp_{i,\ell} = 1$ for all $i\in\lrcb{1,\ldots n}$.

The random assignment for each vertex $i$ follows a categorical distribution of
parameter $\mP_{i,:}$ and the random clustering $\gC_k$ is thus parameterized by
$\mP\in[0,1]^{n\times k}$. We define the random ratio-assignment vector
$\rvf^{(\ell)}$ for cluster $\sC_\ell$ as:
\[
	\rvf^{(\ell)} = \by{\sqrt{\widehat{|\sC_\ell|}}}\rva^{(\ell)},
\]
where $\widehat{|\sC_\ell|} = \sum_{i=1}^{n}\rva^{(\ell)}_i$, and all the
  elements of $\rvf^{(\ell)}$ are in the interval $\lrb{0,1}$ with the convention
  $\frac{0}{0} = 1$.

The stochastic counterpart of the quantity introduced earlier in \Cref{eq:ratiocut}
is:

\begin{align}
	% \widehat{\cut}(\sC_\ell,\comp{\sC_\ell})
	% & = \by{2}\sum_{1 \leq i,j\leq n} \emW_{ij} (\rva^{(\ell)}_i-\rva^{(\ell)}_j)^2                \nonumber \\
	\label{eq:stochasticrcut}
	\widehat{\rcut(\gC_k)}
	 & = \by{2}\sum_{\ell=1}^k \sum_{1 \leq i,j\leq n} \emW_{ij} (\rvf^{(\ell)}_i-\rvf^{(\ell)}_j)^2.
\end{align}

\Cref{eq:rawlap} can also be extended to the stochastic setting similarly.

The next step is to compute the expected ratio-cut for a clustering $\gC_k$
parameterized by $\mP$. Without loss of generality, we only have to compute
$\Ea{(\rvf^{(\ell)}_1-\rvf^{(\ell)}_2)^2}$ as a function of $\mP$ due to the
linearity of the expectation. We prove the following result
in~\Cref{appendix:diffexpect}:

\begin{restatable}[Difference expectation]{lemma}{diffexpect}
	\label{lemma:diffexpect}
	Let $\sC$ be a subset of $\gV$ and $\rva$ its random assignment vector
	parameterized by $\vp\in[0,1]^n$. Let $\rvf$ be its random ratio-assignment
	vector. Then we have the following:
	\begin{align*}
		\Ea{(\rvf_1-\rvf_2)^2} = (\evp_1 + \evp_2 -2 \evp_1 \evp_2)\Ea{\by{1 + \widehat{|\sC^{\perp(1,2)}|}}},
	\end{align*}
	where $\sC^{\perp(i,j)} = \sC \backslash \lrcb{v_i, v_j}$
\end{restatable}

To avoid excluding pairs $(i,i)_{1\leq i \leq n }$ from the summation in
~\Cref{eq:stochasticrcut} each time, we will henceforth assume that $\emW_{ii} = 0$
for all $i \in \lrcb{1,\ldots,n}$.

The random variable $\widehat{|\sC^{\perp(1,2)}|} = \sum_{i=3}^n \rva_i$ is the sum
of (n-2) independent (but not identical) Bernoulli random variables, also known as
a Poisson binomial. It coincides with the binomial distribution when the Bernoulli
random variables share the same parameter. See \Cref{appendix:poissonexpect} for a
compilation of properties of this distribution including the proof
of~\Cref{lemma:poissonexpect}.

\begin{restatable}[Poisson binomial expectation]{lemma}{poissonexpect}
	\label{lemma:poissonexpect}
	Let $\rZ$ be a Poisson binomial random variable of parameter
	$\bm{\alpha}=(\alpha_1,\ldots,\alpha_m)\in[0,1]^m$. Then we have:
	\[
		\Ea{\by{1 + \rZ}}  = \int_{0}^{1} \prod_{i=1}^{m} (1-\alpha_i t)dt
	\]
\end{restatable}

We can now express the expected ratio-cut by combining the results from
\Cref{lemma:diffexpect} and \Cref{lemma:poissonexpect}.

\begin{restatable}[Ratio-cut expectation]{thm}{ratiocutexp}
	\label{thm:ratiocutexp}
	The expected ratio-cut of the random clustering $\gC_k$ parameterized by
	$\mP\in[0,1]^{n\times k}$ can be computed as $\Ea{ \widehat{\rcut(\gC_k)} } =
		\sum_{\ell=1}^k \RC(\mP_{:,\ell}) $ where:
	\begin{align*}
		\RC(\vp)       & = \by{2} \sum_{i,j=1}^n \emW_{ij}\lrp{\evp_i + \evp_j -2\evp_i\evp_j} \sI(\evp, i, j) \\
		\sI(\vp, i, j) & \defeq \int_{0}^{1} \prod_{m\neq i,j}^{n} (1-\evp_m t)dt
	\end{align*}
	% and $$.
\end{restatable}
The expression for $\RC(\vp)$ can be further simplified by consolidating the terms linear in $\vp$. However, we retain the current formulation as it will prove useful when sampling random pairs $(i, j)$ in the implementation.

We will drop the cluster index $\ell$ in the next sections and use $\vp \in
[0,1]^n$ as the parameter of the random assignment. The goal now is to optimize
$\RC(\vp)$.

\subsection{Computation of the expected ratio-cut}\label{subsec:computeall}
A straightforward method to estimate the expected ratio-cut from
\Cref{thm:ratiocutexp} involves discretizing the interval $[0,1]$ to approximate
the integral. By using a uniform discretization with a step size of $\by{T}$ for
$T>0$, we can apply the formula:
\[
	\sI(\vp, i, j) = \by{T} \sum_{t=1}^{T} \prod_{m\neq i,j} (1-\evp_i \frac{t}{T})
\]
The quality of the approximation depends on $T$, but since the integrated
  function is polynomial in $t$, we have the luxury of using a weighted
  discretization scheme that ensures the exact computation of the integral using
  the following quadrature method proven in~\Cref{appendix:integralcompute}.

\begin{restatable}[Integral computation via quadrature]{lemma}{integralcompute}
	\label{lemma:integralcompute}
	Let $m>0$ be an integer and $c_m \defeq \lfloor\frac{m}{2}\rfloor + 1$ (the
	smallest integer such that $2c_m\geq m+1$), then there exist $c_m$ tuples
	$(s_q,t_q)_{1\leq q \leq c_m}$ such that:
	\[
		\int_{0}^{1} \prod_{i=1}^{m} (1-\evp_i t)dt = \sum_{q=1}^{c_m} s_q \prod_{i=1}^{m} (1-\evp_i t_q),
	\]
	for all $\vp\in [0,1]^m$.
\end{restatable}

The computation of $\sI(\vp, i,j)$ for a given pair $(i,j)$ costs $O(n^2)$. We can
still obtain the total $\RC(\vp)$ in $O(c_n n^2)$ instead of $O(n^4)$ by computing
the $c_n$ quantities $\lrp{\sum_{m=1}^n\log\left(1-\evp_{m} t_q\right)}_{1\leq
q\leq c_n}$ in advance. Another challenge encountered when using the quadrature is
the instability of the computations due to the potentially large number of elements
in the product. Even with a tight approximation using batch-based estimates, it
still costs $O(b^3)$, where $b$ is the batch size. Furthermore, the higher the
number of clusters, the larger the batch size should be to obtain a good estimate
of the integral for different clusters. These challenges are to be expected since
the original combinatorial problem is generally NP-hard. See
\Cref{appendix:batchestimate} for a detailed analysis of the batch-based estimate.
Instead, we prove in~\Cref{appendix:integralupperbound} that the expected ratio cut
can be upper-bounded by a much more accessible quantity, as shown
in~\Cref{lemma:integralupperbound}.

\begin{restatable}[Integral upper-bound]{lemma}{integralupperbound}
	\label{lemma:integralupperbound}
	We adopt the same notation of \Cref{lemma:poissonexpect} where
	$\bm{\alpha}=(\alpha_1,\ldots,\alpha_m)\in[0,1]^m$, and we assume that
	$\ov{\bm{\alpha}}\defeq\by{m}\sum_{i=1}^m \alpha_i > 0$. Then we have:
	\[
		\int_0^1 \prod_{i=1}^{m}(1-\alpha_i t)dt \leq \by{(m+1)\ov{\bm{\alpha}}}.
	\]
\end{restatable}
The assumption $\ov{\bm{\alpha}} > 0$ is not necessary if we extend the
property to $\R\cup\lrcb{+\infty}$. We can now use \Cref{lemma:integralupperbound}
to upper-bound $\RC(\vp)$.

\begin{restatable}[$\RC$ upper-bound]{lemma}{rcupper}\label{lemma:rcupper}
	Using the notations of \Cref{thm:ratiocutexp} and
	\Cref{lemma:integralupperbound}, we have:
	\[
		\RC(\vp) \leq \frac{e^2}{2n}\by{\ov{\vp}} \sum_{1 \leq i,j\leq n}
		w_{ij}(p_i + p_j  -2 p_i p_j)
	\]
\end{restatable}
\paragraph{Interpretation}

Before we proceed with determining an unbiased gradient estimator for the bound in
\Cref{lemma:rcupper}, we seek to understand the significance of the various
quantities introduced previously. In contrastive learning, the objective can be
written as $-2\sum_{ij} \emW_{ij} \evp_i \evp_j$, whose minimization aims to assign
highly similar samples to the same cluster. The objective in ratio-cut can be
expressed as $\sum_{ij} \emW_{ij} \lrb{\evp_i(1-\evp_j) + \evp_j(1-\evp_i)}$, which
is minimized when dissimilar samples are separated into different clusters, in
alignment with the initial goal of minimizing the cut. It is worth noting that the
minimum of the term $\emW_{ij} \lrb{\evp_i(1-\evp_j) + \evp_j(1-\evp_i)}$ is
$\emW_{ij}$, and it is achieved when either $v_i$ and $v_j$ are in different
clusters.

\Cref{lemma:poissonexpect} generalizes the
scaling by the inverse of the size of a cluster $\sC$ to the probabilistic setting.
Let's assume that $\vp \in \lrcb{0,1}^n$ (deterministic assignment), then
$(\evp_i+\evp_j-2\evp_i\evp_j)=1$ if and only if $v_i$ and $v_j$ are in different
clusters. In such case, we can easily prove the following:
% \begin{align*}
$$
	\Ea{\by{1 + \widehat{|\sC|}}} = \int_{0}^{1} \prod_{i=1,\evp_i = 1}^{n} (1-t) dt = \frac{1}{1+n\ov{\vp}}.
$$
% \end{align*}
As $n$ becomes sufficiently large, the quantity $\frac{1}{n}\by{\ov{\vp}}$ from
\Cref{lemma:rcupper} approximates $\frac{1}{1+n\ov{\vp}}$ and, therefore, also
approximates $\Ea{\by{1 + \widehat{|\sC|}}}$. Consequently, the bounds we've seen
so far are tight in the deterministic case.
\subsection{Stochastic gradient of the objective}

We will drop the scaling factor $\by{2n}$ when upper bounding $\RC(\gC_k)$, we can
thus succinctly write the following upper bound for the expected ratio-cut with
$\ov{\mP} = \diag \lrp{\ov{\mP_{:,1}},\ldots,\ov{\mP_{:,k}}}$ as $\RC(\gC_k) \leq
\gL_{rc}(\mW, \mP)$ such that:
\begin{align}
	\label{eq:quad}
	\gL_{rc}(\mW, \mP)
	 & = \sum_{\ell=1}^k\sum_{i,j=1}^n
	\by{\ov{\mP_{:,\ell}}}\emW_{ij}\lrp{\emP_{i,\ell} + \emP_{j,\ell} -2 \emP_{i,\ell}\emP_{j,\ell}} \nonumber \\
	 & =\Tr(\ov{\mP}^{-1}(\vone_{n,k}-\mP)^\top\mW \mP ),
\end{align}

Let us examine the derivative of $\gL_{rc}$ of a single cluster with respect to
$\evp_i$,$ \dot{p_i}= \frac{d \gL_{prcut}}{d\evp_i} $:
\begin{align}
	\label{eq:batchgradient}
	\dot{p_i}\propto \by{\ov{p}^2} \sum_{i,j=1}^{n}\emW_{ij}
	\Big[(1 -2p_j)\ov{p}                                       %                     \\
		\left. - \by{n}(p_i + p_j  -2 p_i p_j) \right],
	% + \gamma (\ov{p}-\by{k})\nonumber
\end{align}

In order to obtain an unbiased estimate for the gradient, we need to acquire an
accurate estimate of $\ov{\vp}$. We do this in the online setting by computing the
moving average:
\[
	{\ov\vp_t}^{ma} = (1-\beta_t){\ov\vp_{t-1}}^{ma} +\beta_t \ov{\vp}_t,
\]
such that $\beta_t = \frac{\beta}{t}$ and $\ov{\vp}_t$ is a batch-based estimate
  of $\ov{\vp}$ at time $t$. Then, the unbiased gradient can be obtained by
  back-propagating $\lrb{\stopgrad(\dot{p_i})p_i}$, with $\stopgrad$ being the
  gradient-stopping operator.

\begin{algorithm}[t]
	\caption{Probabilistic Ratio-Cut (PRCut) Algorithm}\label{alg:prcut}
	\begin{algorithmic}[1]
		\REQUIRE Dataset $(v_i)$, Similarity kernel $\gK$, batch size $b$,
		encoder $N_\theta$ parameterized by $\theta$, number of clusters $k$,$\ov{\mP}_0=\by{k}\vone_k$,$\beta>0$,
		polytope regularization weight $\gamma$, $t= 0$.
		% \Ensure $y = x^n$
		\WHILE{not terminated}
		\STATE $t\gets t+1$
		\STATE Sample left batch $S_l$ of size $b$
		\STATE Sample right batch $S_r$ of size $b$
		\STATE Compute $\mW = \gK(S_l,S_r)$
		\STATE compute $\mP_\theta^l, \mP_\theta^r \gets N_\theta(S_l),N_\theta(S_r)\in{\sR^{b\times k}}$
		\STATE Update $\ov{\mP}_t \gets (1-\beta_t)\ov{\mP}_{t-1} +\frac{\beta_t}{2}\lrp{\ov{\mP_\theta^l} + \ov{\mP_\theta^r}}$
		% and use it in the gradient estimation.
		\STATE Compute $\dot{\mP_\theta^l}$ and $\dot{\mP_\theta^r}$ using \Cref{eq:batchgradient}
		% where $i$ indexes $\mP_\theta^l$
		% and $j$ indexes $\mP_\theta^r$ and then alternate.
		\STATE Back-propagate $\Tr\lrb{\mP_\theta^l \stopgrad(\dot{\mP_\theta^l})^\top +\mP_\theta^r\stopgrad(\dot{\mP_\theta^r})^\top}$
		\STATE Back-propagate $\gamma\kl{\ov{[\mP_\theta^l, \mP_\theta^r]}}{\by{k}\mI_k}$
		\STATE Use the accumulated gradients $g_t$ to update $\theta$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}
\subsection{Regularization}
As we aim to optimize $\gL_{rc}(\mW, \mP)$ using gradient descent, the main
challenge our method faces is that the assignments may collapse to a single cluster
$\sC_m$ with a probability close to 1. This translates to
$\lrp{\ov{\mP_{:,\ell}}}_{\ell\neq m}\approx 0$, which renders the gradient updates
highly unstable. \citet{sinkhorn} have adopted the constraint that $\mP$ belongs to
the polytope of distributions $\gU_k$, defined as:
\[
	\gU_k=\lrcb{P\in \sR_{+}^{n\times k} | P^\top \vone_{n} = \by{k}\vone_{k} },
\]

Instead of restricting the clusters to be equally likely through the Bregman
projection into $\gU_k$, we only encourage such behavior using Kullback-Leibler
divergence regularization. By selecting the appropriate regularization weight
$\gamma$, the additional term will ensure that the likelihood of each of the $k$
clusters, $\ov{\mP}$, exceeds a certain threshold $\delta(\gamma) > 0$ for the
optimal $\mP$. This approach does not necessarily imply that all the clusters will
be utilized, as the assignment probability for cluster $\sC_\ell$ could be
significantly above $\delta$ without any sample being more likely assigned to
cluster $\sC_\ell$. The final objective that we aim to optimize is:
\[
	\gL_{prcut}(\mW,\mP) = \gL_{rc}(\mW,\mP) + \gamma\kl{\ov{P}}{\by{k}\vone_k}
\]

\subsection{Similarity measure}
We have assumed thus far that the kernel $\gK$ is provided as input. We also assert
that the performance of any similarity-based clustering ultimately depends on the
quality of the similarity function used. The simplest kernel we can use is the
adjacency matrix for a $k$-nearest neighbors graph, where $\gK(v_i,v_j)=1$ when
either $v_i$ or $v_j$ is among the $k$ nearest neighbors of each other (to ensure
that the kernel is symmetric). We can also use the \textit{Simple} (SimCLR) or the
\textit{Symbiotic} (All4One) contrastive learning methods~\citep{simCLR,all4one} to
train a neural network to learn the pairwise similarities. Once we train our
similarity network, we use the cosine of the representations of the samples as our
similarity function to either compute $\emW_{ij}=\exp\lrp{\frac{\cosine{z_i,
z_j}}{\tau}}$ for some temperature $\tau>0$ or to build the $k$-nearest neighbors
graph based on these representations.

\subsection{Computational and Memory Footprint}
If the similarity matrix $\mW$ is dense, the time complexity of computing the PRCut objective is $O(n^3)$, identical to a vanilla spectral clustering approach. In contrast, for a sparse graph where similarity is determined based on the $m$-nearest neighbors, the time complexity of conventional spectral clustering is $O(nmk)$, not accounting for the additional cost of $O(nk^2)$ incurred by running k-means on the computed eigenvectors.

In the case of batch PRCut, the expected computational cost for evaluating the batch loss with a batch size of $b$ is $O\left(\frac{m}{n}kb^2\right)$, while the memory requirement remains constant at $O(b^2)$. Consequently, when executed for $T$ steps, the overall time complexity of stochastic PRCut becomes $O\left(T\frac{m}{n}kb^2\right)$.
% Given a batch $S$ of samples $\lrcb{x_1,\ldots,x_b}$, the batch is augmented twice
% to obtain $S^{(1)} = \lrcb{x_1^{(1)},\ldots,x_b^{(1)}}$ and $S^{(2)} =
% \lrcb{x_1^{(2)},\ldots,x_b^{(2)}}$, where $x_i^{(1)}$ and $x_i^{(2)}$ are two
% different augmentations of the sample $x_i$.
%
% We denote $z_{2i}$ the representation of $x_i^{(1)}$ and $z_{2i+1}$ that of
% $x_i^{(2)}$. We then compute the logits:
%
% \[
% 	l_{ij} = -\log \frac{\exp[\cosine{z_i, z_j}]}{\sum_{k=1}^{2b}\exp[\cosine{z_i, z_k}]},
% \]
% These logits are then used to train the representation network as a classifier
%   using the cross-entropy loss where logit $l_{ij}$ is associated with the correct
%   label when $z_i$ and $z_j$ are associated with augmentations of the same sample
%   and with the wrong label otherwise.
%
