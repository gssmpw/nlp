\section{Proofs for the upper bound}
\subsection{Ratio-cut expectation proof}
\label{appendix:diffexpect}
\diffexpect*
\begin{proof}
	Using the probabilistic formulations. We have:
	\[
		\widehat{|C|} = \sum_{i=1}^{n} \rva^{}_i = \rva_1  + \rva_2
		+ \widehat{|C^{\perp(1,2)}|}
	\]
	We can then expand the expectation:
	\begin{align*}
		\Ea{(\widehat{\rvf}_1-\widehat{\rvf}_2)^2}
		 & = \Ea{\by{\rva_1+\rva_2 + \widehat{|C^{\perp(i,j)}|}}\lrp{\rva_1-\rva_2}^2} \\
		 & = \Ea{\by{1+\widehat{|C^{\perp(1,2)}|}}} \Ppar{\rva_1,\rva_2 = 1,0}
		+  \Ea{\by{1+\widehat{|C^{\perp(1,2)}|}}} \Ppar{\rva_1,\rva_2 = 0,1}           \\
		 & = \Ea{\by{1+\widehat{|C^{\perp(1,2)}|}}}\lrp{p_1 (1-p_2) + (1-p_1)p_2}      \\
		 & = \Ea{\by{1+\widehat{|C^{\perp(1,2)}|}}}\lrp{p_1+p_2-2p_1p_2}
	\end{align*}
\end{proof}
\subsection{Binomial Poisson distribution properties} \label{appendix:poissonprop}
Let $\rZ$ be a Poisson binomial random variable of parameter
$\vp=(\alpha_1,\ldots,\alpha_n)$. We can write $\rZ$ as:
\[
	\rZ=\sum_{i=1}^m \rvr_i,
\]
where $(\rvr_i)_i$ are independent random variable such that:
\[
	\rvr_i \sim \text{Bernoulli}(\alpha_i).
\]
\begin{restatable}{lemma}{poissonintro}
	\label{lemma:poissonintro}
	We denote by $G_Z$ the Probability-Generating Function (PGF) for the Poisson binomial
	$\rZ$ of parameter $\bm\alpha\in[0,1]^m$. We have the following:
	{\footnotesize
	\begin{align*}
		\mu      & \defeq \Ea{\rZ} = \sum_{i=1}^n \alpha_i                     \\
		\sigma^2 & \defeq \Ea{\lrp{\rZ-\mu}^2} = \mu - \sum_{i=1}^m \alpha^2_i \\
		G_Z(t)   & \defeq \Ea{t^\rZ} = \prod_{i=1}^m (1-\alpha_i+\alpha_i t)
	\end{align*}
	}
\end{restatable}

\begin{proof}
	\begin{align*}
		\mu & \defeq \Ea{\rZ} = \sum_{i=1}^m\Ea{ \rvr_i} = \sum_{i=1}^n \alpha_i \\
	\end{align*}
	Since $(\rvr_i)$ are independent random variable, the variance of their sum is the
	sum of the variance:
	\begin{align*}
		\sigma^2 & \defeq \text{Var}[Z] = \sum_{i=1}^n\text{Var}[\rvr_i] = \sum_{i=1}^n
		\alpha_i(1-\alpha_i)=
		\mu - \sum_{i=1}^n \alpha^2_i                                                   \\
	\end{align*}
	For the PGF:
	\begin{align*}
		G_Z(t)  \defeq \Ea{t^Z} & = \Ea{t^{\sum_{i=1}^n \rvr_i}}                                                 \\
		                        & = \prod_{i=1}^n \Ea{t^{\rvr_i}} \prod_{i=1}^n (1-\alpha_i) * t^0 +\alpha_i t^1 \\
		                        & = \prod_{i=1}^n (1-\alpha_i +\alpha_i t)
	\end{align*}
\end{proof}
The first approach for computing $\Ea{\by{1 + Z}}$ is to use the definition of the
expectation, which requires the knowledge of the probabilities $\lrp{\Ppar{Z=i}}_i$:
\[
	\Ea{\by{1 + Z}} = \sum_{i=0}^n \by{1+i} \Ppar{Z=i}.
\]
\begin{lemma}
	\label{lemma:probas}
	Let $\sP\lrb{n}$ denote the power set of $\lrcb{1,\ldots,n}$, and
	$\gI_i$ be the set of elements of $\sP\lrb{n}$ that contain exactly $i$
	unique integers. $\Ppar{Z=i}$ can be computed as:
	\begin{align}
		\Ppar{Z=i} = \sum_{I \in \gI_i} \prod_{j\in I}p_j\prod_{m\in \overline{I}}(1-p_m)
	\end{align}
\end{lemma}
\begin{proof}
	Refer to \cite{poissonbinomial}
\end{proof}

\subsection{Proof of the integral formula}
\label{appendix:poissonexpect}

We will use the properties introduced in \Cref{appendix:poissonprop} to prove the
following lemma: \poissonexpect*
\begin{proof}
	The PGF for $\rZ$ is
	\[
		G_\rZ(t) = \Ea{t^\rZ} = \prod_{i=1}^m (1-\alpha_i+\alpha_i t)
	\]
	Using the Probability-Generating Function (PGF), we can write:
	\begin{align*}
		\Ea{\by{1 + Z}}
		 & =\Ea{\int_{0}^{1} t^Z dt}
		= \int_{0}^{1} \Ea{t^Z}dt
		 &                                                                                                 \\
		 & = \int_{0}^{1} \prod_i (1-\alpha_i+\alpha_i t)dt &                                              \\
		 & = \int_{0}^{1} \prod_i (1-\alpha_i t)dt          & \text{change of variable: }  t\leftarrow 1-t \\
	\end{align*}

	Exchanging the expectation and the integral is based on the fact that the
	function we are integrating is measurable, non-negative, and bounded by the
	constant \(1\). We can then use Fubini's theorem to change the order of
	integration.
\end{proof}

\begin{corollary}
	Using the notations from \cref{lemma:probas} we can show that:
	\begin{align*}
		\Ea{\by{1 + Z}}  = \sum_{i=1}^{n} \frac{(-1)^i}{1+i} \sum_{I\in\gI_i} \prod_{j\in I} p_j,
	\end{align*}
	where $\gI_i$ is the set of subsets of $\lrcb{1,\ldots,n}$ that contain exactly
	$i$ elements.
\end{corollary}

This offers a straightforward approach for approximating the expectation, which is
notably simpler to compute iteratively. However, it's not practical as the number
of terms to sum is $n!$ and can only be implemented for small graphs.

\subsection{Integral computation}
\label{appendix:integralcompute}
\integralcompute*
\begin{proof}
	This theorem is a direct application of Gauss-Legendre quadrature (See~\citet{numericalanalysis}). It
	states that any integral of the form:
	\[
		\int_{-1}^{1} f_m(t)dt
	\]
	where $f_m$ is polynomial of degree at most $2m-1$ can be exactly computed
	using the sum:
	\[
		\sum_{i=1}^{m} w_i f(r_i)
	\]
	where $(r_i)$ are the roots of $P_m$, Legendre polynomial of degree $m$, and
	the weights are computed as:
	\[
		w_i = \frac{2}{(1-r_i)^2 [P_n'(r_i)]^2}\geq 0
	\]
	For an integral on the $[0,1]$ interval, we use change of variable to find the
	corresponding weights and nodes are:
	\begin{align*}
		s_j & = \frac{w_j}{2}          \\
		t_j & = \frac{r_j}{2} + \by{2}
	\end{align*}
\end{proof}

\subsection{Batch estimation of the expected ratio-cut}\label{appendix:batchestimate}

Returning to the probabilistic ratio-cut, we can combine
corollary~\cref{thm:ratiocutexp} and \cref{lemma:poissonexpect} to express the
expected ratio-cut as:
\begin{align*}
	\sum_{i=1}^{n}\sum_{j>i}^{n} w_{i,j}(p_i + p_j  -2 p_i p_j)
	\int_{0}^{1} \prod_{k\neq i,j} (1-p_k t)dt
\end{align*}

Practically, with large datasets, it is expensive to compute the quantity
$\int_{0}^{1} \prod_{k\neq i,j} (1-p_k t)dt$ on the entire dataset: the quadrature
calculation costs $\mathcal{O}(\frac{n^2}{2})$ and the total cost of computing the
objective is $\mathcal{O}(k n^3)$ for all the clusters.

We can also show that we can get a tighter bound if $S$ is a random subset such
that
\[
	\forall i\in[1,n] P(i\in S) = \gamma
\]

\begin{restatable}[Batch estimation]{thm}{batchbound}
	\label{thm:batchbound}
	Let $S$ is a random subset such
	that
	\[
		\forall i\in[1,n], P(i\in S) = \gamma < 1.
	\]
	Then we can show that:
	\begin{align}
		\int_{0}^{1} \prod_{i=1}^{n} (1-p_k t)dt \leq
		\Eb{S}{\int_{0}^{1}\prod_{s\in S} \lrp{1- p{s}t}^{\by{\gamma}}dt}
	\end{align}
\end{restatable}
\begin{proof}

	\begin{align*}
		\prod_{i=1}^n (1- p_{i}t)
		 & = \exp\lrb{\sum_{i=1}^n \log(1- p_i t)}                       \\
		 & = \exp\lrb{\by{\gamma} \Eb{S}{\sum_{i\in S}\log(1- p_i t)}}   \\
		 & = \exp\lrb{\Eb{S}{\sum_{i\in S}\by{\gamma} \log(1- p_i t)}}   \\
		 & \leq \Eb{S}{\exp\lrb{\sum_{i\in S}\by{\gamma}\log(1- p_i t)}} \\
		 & = \Eb{S}{\prod_{s\in S} (1- p_st)^\by{\gamma}}.
	\end{align*}

	\Cref{thm:batchbound} provides a bound on the integral based on the
	expectation of a random batch of samples. We can show that if $p_i\in\lrcb{0,1}$ that
	the expectation over the batch equal to the estimate over the whole dataset. If we sample
	a random batch of size $b$ then $\gamma=\frac{b}{n}$, the fraction of the entire data
	that we're using.
\end{proof}
\subsection{Objective upper bound}\label{appendix:integralupperbound}
\integralupperbound*
\begin{align*}
	\int_0^1 \prod_{i=1}^{m}(1-\alpha_i t)dt
	 & = \int_0^1 \exp \lrb{\sum_{i=1}^{m}\log(1-\alpha_i t)}dt                                              \\
	 & = \int_0^1 \exp \lrb{\by{m}\sum_{i=1}^{m}\log(1-\alpha_i t)}^m dt                                     \\
	 & \leq \int_0^1 \lrb{ \by{m} \sum_{i=1}^{m} \exp\log(1-\alpha_i t)}^m dt & \text{exponential is convex} \\
	 & =  \int_0^1 \lrb{\by{m}  \sum_{i=1}^{m} (1-\alpha_i t)}^m dt                                          \\
	 & =  \int_0^1 \lrp{1-\ov{p} t}^m dt                                                                     \\
	 & =  -\by{\ov{\bm\alpha}(m+1)} \lrb{\lrp{1-\ov{\alpha}t}^{m+1}}_0^1                                     \\
	 & =  \by{\ov{\bm\alpha}(m+1)} \lrb{1-\lrp{1-\ov{\alpha}}^{m+1}}                                         \\
	 & \leq \by{\ov{\bm\alpha}(m+1)}.
\end{align*}

Moving from the proof of \Cref{lemma:integralupperbound} to \Cref{lemma:rcupper}
requires a modification in the previous proof. To get an upper bound without making
any assumptions about $\ov{\vp}$, we can prove the following:
\begin{align*}
	\sI(\vp, 1, 2) & = \int_{0}^{1} \prod_{i\geq 3}^{n} (1-\evp_i t)dt                    \\
	               & \leq \int_0^1 \lrb{\by{n-2}  \sum_{i=3}^{n} (1-\evp_i t)}^{(n-2)} dt \\
	               & \leq \int_0^1 \lrb{\by{n-2}  \sum_{i=1}^{n} (1-\evp_i t)}^{(n-2)} dt \\
	               & = \int_0^1 \lrb{\frac{n}{n-2} (1-\ov{\vp} t)}^{(n-2)} dt             \\
	               & = \lrp{\frac{n}{n-2}}^{n-2}\by{(n-1)\ov{\vp}}                        \\
	               & = \lrp{\frac{n}{n-2}}^{n-2}\frac{n}{n-1}\by{n\ov{\vp}}               \\
	               & = (e^2-o(n))\by{n\ov{\vp}}                                           \\
                   & < \frac{e^2}{n\ov{\vp}}                                           \\
\end{align*}

But by assuming that $\by{n-2}\sum_{i=3}^{n} (1-\evp_i t) \approx
\by{n}\sum_{i=1}^{n} (1-\evp_i t)=\ov{\vp}$, we get the bound in
\Cref{lemma:rcupper}:
\begin{align*}
	\by{n-1}\by{\by{n-2}\sum_{i=3}^{n} \evp_i}
	 & = \frac{n-2}{n-1} \by{\sum_{i=3}^{n} \evp_i}       \\
	 & \approx \frac{n-2}{n-1} \by{\sum_{i=1}^{n} \evp_i} \\
	 & \leq \by{n}\by{\ov{\vp}}
\end{align*}

\section{Proofs for the stochastic gradient}
\subsection{Offline Gradient} \label{appendix:offlinegrad}
The offline gradient of the objective can be obtained directly by differentiating
the matrix version of the expression
\begin{align*}
	\frac{d \gL_{rc}}{d\mP}
	= \ov{\mP}^{-1}\lrb{\lrp{\vone_{n,k}-\mP}^\top\mW\vone_{n,k}-\vone_{n,k}^\top\mW\mP}
	-\by{n}\vone_{n,k}\lrb{\ov{\mP}^{-1}(\vone_{n,k}-\mP)^\top\mW \mP\ov{\mP}^{-\top}}
\end{align*}

This expression is particularly useful in the offline learning setting, especially
when $\mW$ is sparse, as it makes the computation graph more efficient due to the
limited support of sparse gradients in major deep learning libraries.
\subsection{Online Gradient}\label{appendix:onlinegrad}
To compute the derivative of with respect to $p_i$, we simply calculate the
derivatives of $\by{\ov{\vp}}$ and $(p_i + p_j -2 p_i p_j)$ w.r.t $p_i$ which are:
\begin{align*}
	\frac{d}{d p_i}\by{\ov{\vp}}           & = -\by{n\ov{\vp}^2} \\
	\frac{d}{d p_i}(p_i + p_j  -2 p_i p_j) & = (1-2p_j),         \\
\end{align*}
respectively.

By plugging these derivatives and considering the rule for the derivative of the
product $(fg)' = f'g + fg'$ of two functions $f$ and $g$, the result in
\Cref{eq:batchgradient} follows.

% \begin{algorithm}[tb]
% 	\caption{Ratio loss computation}
% 	\label{alg:ratioloss}
% 	\textbf{Input}: Your algorithm's input\\
% 	\textbf{Parameter}: Optional list of parameters\\
% 	\textbf{Output}: Your algorithm's output
% 	% 	\begin{algorithmic}[1]
% 	% 		\Require  Similarity measure $W$, assignment probabilities $\vp\in\sR^{b}$,
% 	% 		quadrature $(s_j,t_j)_{1 \leq j\leq b_n}$
% 	% 		\STATE Compute $A_j = \sum_{i=1}^{b} \log(1-p_i t_j) + \log s_j$ for $j$ in $[b_n]$
% 	% 		\STATE $L_{ratio} \gets 0$
% 	% 		\For{$i \in [b]$}
% 	% 		\For{$j \in [b] $ and $j>i$}
% 	% 		\STATE{$B \gets A - \log(1 - p_{i}t_*)- \log(1 - p_{j}t_*) $}
% 	% 		\STATE{$L_{ratio} \gets L_{ratio} + \sum_{j\leq [b_n]}\exp(B_{j})$}
% 	% 		\EndFor
% 	% 		\EndFor
% 	\begin{algorithmic}[1] %[1] enables line numbers
% 		% 		\Require  Similarity measure $W$, assignment probabilities $\vp\in\sR^{b}$,
% 		% 		quadrature $(s_j,t_j)_{1 \leq j\leq b_n}$
% 		\STATE Compute $A_j = \sum_{i=1}^{b} \log(1-p_i t_j) + \log s_j$ for $j$ in $[b_n]$
% 		\STATE $L_{ratio} \gets 0$
% 		\FOR{$i \in [b]$}
% 		\FOR{$j \in [b] $ and $j>i$}
% 		\STATE{$B \gets A - \log(1 - p_{i}t_*)- \log(1 - p_{j}t_*) $}
% 		\STATE{$L_{ratio} \gets L_{ratio} + \sum_{j\leq [b_n]}\exp(B_{j})$}
% 		\ENDFOR
% 		\ENDFOR
% 	\end{algorithmic}
% \end{algorithm}

% \begin{proof}
% 	\begin{align*}	\Ea{f^T L_{un} f}
% 		 & =\Ea{\sum_{i,j}^{n} w_{i,j}(f_i-f_\ell)^2}                           \\
% 		 & =2\sum_{i<j}^{n} w_{i,j}(p_i + p_j -2 p_i p_j) \by{T} \sum_{t=1}^T
% 		\prod_{k\neq i,j}^{T} (1-p_k \frac{t}{T})                               \\
% 		 & =\by{T}\sum_{m=1}^T \sum_{i\neq j}^{n} w_{i,j}(p_i + p_j -2 p_i p_j)
% 		\prod_{k\neq i,j} (1-p_k \frac{t}{T})                                   \\
% 		 & =\by{T}\sum_{t=1}^T \sum_{i\neq j}^{n} w_{i,j}(p_i + p_j -2 p_i p_j)
% 		\exp\lrb{\sum_{k\neq i,j} \log(1-p_i \frac{t}{T})}
% 		% \prod_{k\neq i,j}^{M} (1-p_k \frac{m}{M})
% 		% \int_{0}^{1} \prod_{k\neq i,j} (1-p_k t)dt \\
% 	\end{align*}
% \end{proof}
%
% \begin{align*}
% 	\deriv{\Ea{f^T L_{un} f}}{p_m}
% 	 & = \sum_{i,j}^{n} w_{i,j} \deriv{\Ea{(f_i-f_\ell)^2}}{p_m}                               \\
% 	 & = 2 \sum_{i\neq m}^{n}w_{i,m}(1-2p_i)\int_{0}^{1}\prod_{k\neq i,m}(1-p_k t)dt           \\
% 	 & -\sum_{i,j\neq m}^{n}w_{i,j}(p_i+p_j-2p_ip_j)\int_{0}^{1}t\prod_{k\neq m,i,j}(1-p_kt)dt
% \end{align*}
\section{Reproducibility method}\label{appendix:experiments}

\subsection{ Experiments setup}
In our sets of experiments, there are 2 variants:
\begin{itemize}
	\item \textbf{Original representation space}, where the PRCut algorithm is used to train a neural
	      network that takes the original as input
	\item \textbf{SSL representation space}, where the neural network takes as input
	      the transformed dataset via a pretrained self-supervised model
\end{itemize}

\subsubsection{Datasets}
We rely on the following datasets to benchmark our method:
\paragraph{MNIST}
MNIST~\citep{mnist} is a dataset consisting of 70,000 28x28 grayscale images of
handwritten digits, which are split into training (60,000) and test (10,000)
samples.
\paragraph{Fashion MNIST}
Fashion-MNIST~\citep{fmnist} is a dataset of Zalando's article images, comprising a
training set of 60,000 examples and a test set of 10,000 examples. Each example is
a 28x28 grayscale image linked to a label from 10 classes. Zalando designed
Fashion-MNIST to be a direct substitute for the original MNIST dataset in
benchmarking machine learning algorithms. However, it introduces more complexity to
machine learning tasks due to the increased difficulty in classifying the samples.

\paragraph{CIFAR}
The CIFAR-10 and CIFAR-100 datasets\citep{cifar} is a widely used benchmark for
image classification tasks in computer vision. It consists of 60,000 32x32 color
images, divided into 100 classes, with 600 images per class. The images are grouped
into 20 superclasses.

Key Characteristics: 60,000 images (50,000 for training and 10,000 for testing)
32x32 pixel resolution 100 classes, with 600 images per class 20 superclasses,
grouping the 100 classes

\subsubsection{Encoder Architecture}
The encoder is a simple stack of linear layers followed each by a GeLU activation.
The first layer and the last layer are always weight-normalized. The last
activation is a softmax layer. The hidden unit used is constant across layers.

\subsubsection{Hyperparameters}

\begin{table}[ht!]
	\caption{Hyperparameters}\label{tab:}
	\begin{center}
		\begin{tabular}[c]{|l|l|}
			\hline
			{Hyperparameter}                & Value     \\
			\hline
			$\beta$ (moving average)       & 0.8       \\
			\hline
			$\gamma$ (regularization weight) & 100.0     \\
			\hline
			Optimizer original space        & RMSProp   \\
			\hline
			Optimizer representation space  & Adam      \\
			\hline
			Learning rate                   & $10^{-4}$ \\
			\hline
			Weight decay                    & $10^{-7}$ \\
			\hline
			k (k neighbors graph)           & $100$     \\
			\hline
		\end{tabular}
	\end{center}
\end{table}