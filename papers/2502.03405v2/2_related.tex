There have been many attempts to extend Spectral Clustering to streaming
settings~\citep{streamclustering}, infinitely countable datasets, or continuous
input spaces. For instance, \citet{streamspec} proposed an extension to
non-parametric Spectral Clustering for data streams, particularly when the kernel
similarity is bilinear or can be approximated linearly. With the arrival of each
new stream batch, a batch-based Singular Value Decomposition (SVD) is used to embed
the stream and realign a facility set consisting of anchor points. These anchor
points are then utilized to run a batch $K$-means algorithm~\citep{fastkmeans} to
cluster the new points.

However, research on extending Spectral Clustering to the domain of parametric
learning, where the optimization variable is a mapping of the input space to the
eigenspace, is limited. This is primarily due to the sensitivity of the
eigendecomposition~\citep{sensitiveigen} and the proven difficulty of
eigendecomposition in non-parametric settings when the similarity kernel takes
specific forms~\citep{sparsenystrom}.

One of the early attempts in this direction was made in the context of Deep
Reinforcement Learning~\citep{deeprloptions}, where the Markov Decision Process
(MDP) is decomposed into sub-tasks that navigate the representation
space~\citep{eigenoption}. These sub-tasks, also called
\textit{options}~\citep{optiondiscover}, correspond to eigenvectors associated with
the largest eigenvalues of the graph Laplacian, where the vertices represent
elements of the state space and the similarity is the likelihood of the agent
moving from one state to another. Extending such an approach to a clustering
setting would require constructing an MDP for which the correspondence between
eigenvectors and options holds.

SpectralNets~\citep{spectralnet} were perhaps the first attempt at explicitly
training a neural network to approximate eigenvectors of the Laplacian for
large-scale clustering. The learned mapping is a neural network $N_\theta : \R^p
\rightarrow \R^k$, parameterized by $\theta$, that maps the input vertices to their
projection on the subspace spanned by the $k$-smoothest eigenvectors of the
Laplacian. The network is constructed to ensure that the output features are
approximately orthonormal:
\[
	\by{n}\sum_{i=1}^{n} {N_\theta(v_i)}^\top {N_\theta(v_i)} \approx I_k
\]
We denote by $\hat{N}_\theta$ the neural network without the last
  orthogonalization layer, which is based on the Cholesky decomposition of the
  estimated covariance matrix $\mSigma_\theta \in \R^{k \times k}$:
\[
	\mSigma_\theta = \by{n}\sum_{i=1}^{n} {\hat{N}_\theta(v_i)}^\top {\hat{N}_\theta(v_i)}.
\]
SpectralNets approach is then based on the minimization of:
\begin{align}
	\label{eq:spectralneloss}
	\gL_{spectral}(\theta)\defeq
	\sum_{i,j=1}^{n} \emW_{ij} \normtwo{N_\theta(v_i) -{N_\theta(v_j)}}^2
\end{align}

The matrix $\mPi_\theta \in \sR^{k \times k}$ serves as the parametric counterpart
of $\mF^\top \mL \mF$ derived from \cref{eq:rayleighquo}. Training the neural
network equates to an alternating optimization scheme, where one batch is utilized
to estimate $\mSigma_\theta$, while the other is employed to minimize the loss
$\gL_{spectral}(\theta)$.

During training, should the input exhibit noise, the SpectralNet's output may
converge to a constant mapping, resulting in the spectral loss approaching zero.
Consequently, this approach encounters numerical instability in high-dimensional
data scenarios. One common strategy to circumvent this involves offsetting the
matrix $\mSigma_\theta$ by a scaled identity and establishing an appropriate
stopping criterion. However, the resulting algorithm remains highly susceptible to
input and similarity noise. This sensitivity becomes particularly evident when
benchmarked against non-parametric Spectral Clustering. SpectralNets achieve
competitive performance only when the input consists of the code space from another
well-performing pre-trained variational encoder~\citep{vade}.

\textit{Spectral Inference Networks} (Spin)~\citep{spin} adopt a distinct strategy
for parametric optimization of the Rayleigh quotient. As defined previously, the
loss function takes the form:

\begin{align*}
	\gL_{spin}(\theta) = \Tr\lrp{{\mSigma_\theta}^{-1}\mPi_\theta}
\end{align*}

While primarily utilized within the domain of Deep Reinforcement Learning,
targeting the eigenfunctions associated with the largest eigenvalues, this approach
can be adapted for loss minimization rather than maximization.

Upon differentiation~\citep{matrixcook}, the full gradient can be expressed as:
\begin{align*}
	\Tr\lrp{{\mSigma_\theta}^{-1}\nabla_\theta\mPi_\theta} -
	\Tr\lrp{{\mSigma_\theta}^{-1}(\nabla_\theta{\mSigma_\theta}){\mSigma_\theta}^{-1}\mPi_\theta}
\end{align*}

The Spin algorithm estimates $\mSigma_\theta$ and $\nabla_\theta \mSigma_\theta$
utilizing moving averages. It addresses the issue of overlapping eigenvectors by
modifying the gradient to ensure sequential independence in updates: the gradients
of the first $l$ eigenfunctions are made independent of gradients from other
components ${l+1,\ldots, k}$. This alteration significantly slows down training, in
addition to the computational burden of computing $\nabla_\theta\mSigma_\theta$,
which necessitates calculating $k^2$ backward passes and storing $k^2 \times
\text{size}(\theta)$, thereby increasing memory and computational costs.

We assert that the performance of these attempts is ultimately hindered by the
difficulty of the eigendecomposition of large kernels. Therefore, we take a
different approach to optimizing the graph ratio-cut that circumvents the spectral
decomposition.
