\section{Related Work}
\subsection{Identifying Request and Offers}
Multiple efforts have been made to identify ____ and match ____ requests and offers shared on social media during crises. Early work by ____ used regular expressions and labelled data to classify requests and offers via two sequential Random Forest classifiers. ____ improved classification by incorporating topic models, URLs, hashtags, and user metadata. ____ further refined this approach using rule-based features.

Dense vector representations have been used for similar classification tasks. ____ employed GloVe vectors ____, n-grams, and POS tags to classify tweets as ``urgent" or ``not urgent," while ____ combined n-grams with word2vec ____ and trained an XGBoost classifier to detect ``logistical information" tweets. Recently, transformer-based models ____ have shown strong performance ____, with CrisisTransformers ____~---~trained on over 15 billion tokens from 30+ crisis events~---~setting the current state-of-the-art.

The dataset from ____ is seminal in this field. A recent study ____, however, reported that only 41\% of tweets classified as requests and 38\% as offers contained actionable content, with many irrelevant tweets misclassified as requests or offers.



\subsection{Pre-trained encoder-only models}
MLMs based on BERT ____ utilize only the encoder block of the transformer architecture ____, making them ideal for tasks that require contextual embeddings. Several variations of BERT have been introduced, such as RoBERTa ____, MPNet ____, BERTweet ____, and CrisisTransformers ____. Such models have been applied to various tasks in crisis informatics, including classification of humanitarian content ____, identifying disaster-related ____ and informative content ____, detecting location mentions ____, emotion classification ____, stance detection ____ and benchmarking ____.


\subsubsection{Mini Models}
Model compression techniques, such as weight pruning ____, quantization ____, and knowledge distillation in a \textit{student-teacher network} ____, aim to improve inference times and reduce model size. This study focuses on knowledge distillation, where a smaller student model mimics the output of a larger teacher model.

Several mini models, like DistilBERT ____, BERT$_{\text{medium/small/mini/tiny}}$ ____, PKD-BERT ____, and TinyBERT ____, have been trained with varying sizes and configurations. However, these models remain general-purpose, i.e., they are not specifically tailored for crisis-related texts.