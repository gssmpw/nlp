\section{Related Work}
\subsection{Identifying Request and Offers}
Multiple efforts have been made to identify \cite{purohit2014emergency,nazer2016finding,devaraj2020machine,ullah2021rweetminer,lamsal2024crema} and match \cite{purohit2014emergency,dutt2019utilizing,lamsal2024crema} requests and offers shared on social media during crises. Early work by \cite{purohit2014emergency} used regular expressions and labelled data to classify requests and offers via two sequential Random Forest classifiers. \cite{nazer2016finding} improved classification by incorporating topic models, URLs, hashtags, and user metadata. \cite{ullah2021rweetminer} further refined this approach using rule-based features.

Dense vector representations have been used for similar classification tasks. \cite{devaraj2020machine} employed GloVe vectors \cite{pennington2014glove}, n-grams, and POS tags to classify tweets as ``urgent" or ``not urgent," while \cite{he2017signals} combined n-grams with word2vec \cite{Mikolov2013EfficientEO} and trained an XGBoost classifier to detect ``logistical information" tweets. Recently, transformer-based models \cite{vaswani2017attention} have shown strong performance \cite{zhou2022victimfinder,lamsal2024crema}, with CrisisTransformers \cite{lamsal2024crisistransformers}~---~trained on over 15 billion tokens from 30+ crisis events~---~setting the current state-of-the-art.

The dataset from \cite{purohit2014emergency} is seminal in this field. A recent study \cite{lamsal2024crema}, however, reported that only 41\% of tweets classified as requests and 38\% as offers contained actionable content, with many irrelevant tweets misclassified as requests or offers.



\subsection{Pre-trained encoder-only models}
MLMs based on BERT \cite{devlin2018bert} utilize only the encoder block of the transformer architecture \cite{vaswani2017attention}, making them ideal for tasks that require contextual embeddings. Several variations of BERT have been introduced, such as RoBERTa \cite{liu2019roberta}, MPNet \cite{song2020mpnet}, BERTweet \cite{nguyen2020bertweet}, and CrisisTransformers \cite{lamsal2024crisistransformers}. Such models have been applied to various tasks in crisis informatics, including classification of humanitarian content \cite{alam2021crisisbench}, identifying disaster-related \cite{prasad2023identification} and informative content \cite{alam2021crisisbench,koshy2023multimodal}, detecting location mentions \cite{suwaileh2023idrisi}, emotion classification \cite{myint2024unveiling}, stance detection \cite{poddar2022winds,cotfas2021longest,hayawi2022anti} and benchmarking \cite{lamsal2024crisistransformers}.


\subsubsection{Mini Models}
Model compression techniques, such as weight pruning \cite{han2015deep}, quantization \cite{gong2014compressing}, and knowledge distillation in a \textit{student-teacher network} \cite{hinton2015distillingknowledgeneuralnetwork}, aim to improve inference times and reduce model size. This study focuses on knowledge distillation, where a smaller student model mimics the output of a larger teacher model.

Several mini models, like DistilBERT \cite{sanh2020distilbertdistilledversionbert}, BERT$_{\text{medium/small/mini/tiny}}$ \cite{turc2019well}, PKD-BERT \cite{sun2019patient}, and TinyBERT \cite{jiao2019tinybert}, have been trained with varying sizes and configurations. However, these models remain general-purpose, i.e., they are not specifically tailored for crisis-related texts.