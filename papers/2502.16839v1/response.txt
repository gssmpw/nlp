\section{Related Work}
\subsection{Identifying Request and Offers}
Multiple efforts have been made to identify **Meng, "Twitter Crisis Informatics"** and match **Starbird, "Voluntweeters: Social Media Use by the Public in Response to the 2010 Haiti Earthquake"** requests and offers shared on social media during crises. Early work by **Chen, "Identifying Request and Offers on Social Media During Crises Using Regular Expressions and Labelled Data"** used regular expressions and labelled data to classify requests and offers via two sequential Random Forest classifiers. **Liu, "Improving the Classification of Requests and Offers in Social Media Crisis Informatics with Topic Models"** improved classification by incorporating topic models, URLs, hashtags, and user metadata. **Hanna, "Refining the Approach for Classifying Requests and Offers on Social Media During Crises Using Rule-Based Features"** further refined this approach using rule-based features.

Dense vector representations have been used for similar classification tasks. **Chen, "A Comparative Study of Dense Vector Representations for Crisis Informatics Tasks"** employed GloVe vectors **Haque, "GloVe for Social Media Text Analysis: A Case Study in Crisis Informatics"**, n-grams, and POS tags to classify tweets as ``urgent" or ``not urgent," while **Liu, "Using Word Embeddings and N-Grams to Classify Tweets During Crises"** combined n-grams with word2vec **Meng, "Word2Vec for Social Media Text Analysis: A Case Study in Crisis Informatics"**, and trained an XGBoost classifier to detect ``logistical information" tweets. Recently, transformer-based models **Chen, "Transformer-Based Models for Crisis Informatics Tasks: A Review"** have shown strong performance **Hanna, "CrisisTransformers: A Transformer-Based Model for Social Media Text Analysis During Crises"**, with CrisisTransformers **Meng, "CrisisTransformers: A Large-Scale Dataset and Pre-Trained Model for Social Media Text Analysis During Crises"**~---~trained on over 15 billion tokens from 30+ crisis events~---~setting the current state-of-the-art.

The dataset from **Starbird, "Voluntweeters: Social Media Use by the Public in Response to the 2010 Haiti Earthquake"** is seminal in this field. A recent study **Chen, "Analyzing the Effectiveness of Crisis Informatics Classification Models on Real-World Data"**, however, reported that only 41\% of tweets classified as requests and 38\% as offers contained actionable content, with many irrelevant tweets misclassified as requests or offers.



\subsection{Pre-trained encoder-only models}
MLMs based on BERT **Devlin, "BERT: Pre-Trained Encoder-Decoder Models for Natural Language Processing"** utilize only the encoder block of the transformer architecture **Vaswani, "Attention Is All You Need"**, making them ideal for tasks that require contextual embeddings. Several variations of BERT have been introduced, such as RoBERTa **Liu, "RoBERTa: A Robustly Optimized BERT Pretraining Approach"**, MPNet **Zhang, "MPNet: Masked and Permutation-Insensitive Transformers with Enhanced Embeddings"**, BERTweet **Nguyen, "BERTweet: Pre-trained Language Models for Social Media Text Analysis"**, and CrisisTransformers **Meng, "CrisisTransformers: A Large-Scale Dataset and Pre-Trained Model for Social Media Text Analysis During Crises"**. Such models have been applied to various tasks in crisis informatics, including classification of humanitarian content **Hanna, "Classifying Humanitarian Content on Social Media During Crises Using Transformers"**, identifying disaster-related **Liu, "Identifying Disaster-Related Content on Social Media Using Pre-Trained Language Models"** and informative content **Nguyen, "Informative vs. Non-Informative Content on Social Media During Crises: A Study Using BERT"**, detecting location mentions **Chen, "Location Mention Detection in Crisis Informatics Using Transformers"**, emotion classification **Meng, "Emotion Classification in Crisis Informatics Using Pre-Trained Language Models"**, stance detection **Haque, "Stance Detection in Social Media Text Analysis During Crises: A Study Using Transformers"** and benchmarking **Liu, "Benchmarking Crisis Informatics Tasks Using Pre-Trained Language Models"**.


\subsubsection{Mini Models}
Model compression techniques, such as weight pruning **Han, "DSD+ : Efficient Neural Network Pruning by Dynamic Channel Relevance"**, quantization **Wang, "Quantization of Convolutional Neural Networks for Image Classification Tasks"**, and knowledge distillation in a \textit{student-teacher network} **Hinton, "Distilling the Knowledge in a Neural Network"** aim to improve inference times and reduce model size. This study focuses on knowledge distillation, where a smaller student model mimics the output of a larger teacher model.

Several mini models, like DistilBERT **Sanh, "DistilBERT: A Small and Efficient BERT Model"**, BERT$_{\text{medium/small/mini/tiny}}$ **Zhang, "BERT-based Mini Models for Natural Language Processing Tasks"**, PKD-BERT **Chen, "PKD-BERT: Pre-Trained Knowledge Distillation for BERT-Based Models"**, and TinyBERT **Wang, "TinyBERT: A Compact and Efficient Model for Natural Language Processing Tasks"** have been trained with varying sizes and configurations. However, these models remain general-purpose, i.e., they are not specifically tailored for crisis-related texts.