@article{Das2023state,
  title={The state of human-centered NLP technology for fact-checking},
  author={Das, Anubrata and Liu, Houjiang and Kovatchev, Venelin and Lease, Matthew},
  journal={Information Processing \& Management},
  volume={60},
  number={2},
  pages={103219},
  year={2023},
  publisher={Elsevier},
doi = {10.1016/j.ipm.2022.103219}
}

@misc{adair2020automatedjournalism,
    author = {Adair, Bill and Stencel, Mark},
    year = {2020},
    title = {Lesson in automated journalism: Bring back the humans},
publisher = {Nieman journalism lab},
URL = {https://www.niemanlab.org/2020/07/a-lesson-in-automated-journalism-bring-back-the-humans/}
}

@inproceedings{alhindi2018evidence,
    title = "Where is Your Evidence: Improving Fact-checking by Justification Modeling",
    author = "Alhindi, Tariq  and
      Petridis, Savvas  and
      Muresan, Smaranda",
    editor = "Thorne, James  and
      Vlachos, Andreas  and
      Cocarascu, Oana  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    booktitle = "Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER})",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5513",
    doi = "10.18653/v1/W18-5513",
    pages = "85--90",
    abstract = "Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).",
}

@article{amazeen2020journalistic,
  title={Journalistic interventions: The structural factors affecting the global emergence of fact-checking},
  author={Amazeen, Michelle A},
  journal={Journalism},
  volume={21},
  number={1},
  pages={95--111},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England},
doi = {10.1177/1464884917730217}
}

@inproceedings{atanasova2020adversarial,
    title = "Generating Label Cohesive and Well-Formed Adversarial Claims",
    author = "Atanasova, Pepa  and
      Wright, Dustin  and
      Augenstein, Isabelle",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.256",
    doi = "10.18653/v1/2020.emnlp-main.256",
    pages = "3168--3177",
    abstract = "Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimizing the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.",
}

@inproceedings{atanasova2020generating,
    title = "Generating Fact Checking Explanations",
    author = "Atanasova, Pepa  and
      Simonsen, Jakob Grue  and
      Lioma, Christina  and
      Augenstein, Isabelle",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.656",
    doi = "10.18653/v1/2020.acl-main.656",
    pages = "7352--7364",
    abstract = "Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process {--} generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.",
}

@inproceedings{augenstein-etal-2016-stance,
    title = "Stance Detection with Bidirectional Conditional Encoding",
    author = {Augenstein, Isabelle  and
      Rockt{\"a}schel, Tim  and
      Vlachos, Andreas  and
      Bontcheva, Kalina},
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1084/",
    doi = "10.18653/v1/D16-1084",
    pages = "876--885"
}

@inproceedings{augenstein2019multifc,
    title = "{M}ulti{FC}: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
    author = "Augenstein, Isabelle  and
      Lioma, Christina  and
      Wang, Dongsheng  and
      Chaves Lima, Lucas  and
      Hansen, Casper  and
      Hansen, Christian  and
      Simonsen, Jakob Grue",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1475",
    doi = "10.18653/v1/D19-1475",
    pages = "4685--4697",
    abstract = "We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2{\%}, showing that this is a challenging testbed for claim veracity prediction.",
}

@article{bengtsson2024rhetoricalfactcheckers,
  title={And That’s a Fact: A Rhetorical Perspective on the Role of Fact-Checkers},
  author={Bengtsson, Mette and Schousboe, Sabina},
  journal={Journalism Practice},
  pages={1--19},
  year={2024},
  publisher={Taylor \& Francis},
doi = {10.1080/17512786.2024.2340531}
}

@inproceedings{bibal2022attentionexpl,
    title = "Is Attention Explanation? An Introduction to the Debate",
    author = "Bibal, Adrien  and
      Cardon, R{\'e}mi  and
      Alfter, David  and
      Wilkens, Rodrigo  and
      Wang, Xiaoou  and
      Fran{\c{c}}ois, Thomas  and
      Watrin, Patrick",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.269",
    doi = "10.18653/v1/2022.acl-long.269",
    pages = "3889--3900",
    abstract = "The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.",
}

@inproceedings{clarke2020overview,
  title={Overview of the TREC 2020 Health Misinformation Track.},
  author={Clarke, Charles L.A. and Maistro, Maria and Smucker, Mark D.},
year = {2020},
booktitle = {Text Retrieval Conference 2020},
URL = {https://trec.nist.gov/pubs/trec30/papers/Overview-HM.pdf}
}

@inproceedings{das2022prototex,
    title = "{P}roto{TE}x: Explaining Model Decisions with Prototype Tensors",
    author = "Das, Anubrata  and
      Gupta, Chitrank  and
      Kovatchev, Venelin  and
      Lease, Matthew  and
      Li, Junyi Jessy",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.213",
    doi = "10.18653/v1/2022.acl-long.213",
    pages = "2986--2997",
    abstract = "We present ProtoTEx, a novel white-box NLP classification architecture based on prototype networks (Li et al., 2018). ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples. At inference time, classification decisions are based on the distances between the input text and the prototype tensors, explained via the training examples most similar to the most influential prototypes. We also describe a novel interleaved training algorithm that effectively handles classes characterized by ProtoTEx indicative features. On a propaganda detection task, ProtoTEx accuracy matches BART-large and exceeds BERTlarge with the added benefit of providing faithful explanations. A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news.",
}

@article{dierickx2023automated,
  title={Automated fact-checking to support professional practices: systematic literature review and meta-analysis},
  author={Dierickx, Laurence and Lind{\'e}n, Carl-Gustav and Opdahl, Andreas Lothe},
  journal={International Journal of Communication},
  volume={17},
  pages={21},
  year={2023},
URL = {https://ijoc.org/index.php/ijoc/article/view/21071/4287}
}

@misc{dmonte2024claimverificationagelarge,
      title={Claim Verification in the Age of Large Language Models: A Survey}, 
      author={Alphaeus Dmonte and Roland Oruche and Marcos Zampieri and Prasad Calyam and Isabelle Augenstein},
      year={2024},
      eprint={2408.14317},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.14317}, 
doi = {10.48550/arXiv.2408.14317}
}

@inproceedings{ehsan2024thewho,
author = {Ehsan, Upol and Passi, Samir and Liao, Q. Vera and Chan, Larry and Lee, I-Hsiang and Muller, Michael and Riedl, Mark O},
title = {The Who in XAI: How AI Background Shapes Perceptions of AI Explanations},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642474},
doi = {10.1145/3613904.3642474},
abstract = {Explainability of AI systems is critical for users to take informed actions. Understanding who opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups—people with and without AI background—perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design interventions to mitigate them.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {316},
numpages = {32},
keywords = {Explainable AI, Human-Centered Explainable AI, User Characteristics},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{farinneya2021activelearningrumor,
    title = "Active Learning for Rumor Identification on Social Media",
    author = "Farinneya, Parsa  and
      Abdollah Pour, Mohammad Mahdi  and
      Hamidian, Sardar  and
      Diab, Mona",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.387",
    doi = "10.18653/v1/2021.findings-emnlp.387",
    pages = "4556--4565",
    abstract = "Social media has emerged as a key channel for seeking information. Online users spend several hours reading, posting, and searching for news on microblogging platforms daily. However, this could act as a double-edged sword especially when not all information online is reliable. Moreover, the inherently unmoderated nature of social media renders identifying unverified information ever more challenging. Most of the existing approaches for rumor tracking are not scalable because of their dependency on a significant amount of labeled data. In this work, we investigate this problem from different angles. We design an Active-Transfer Learning (ATL) strategy to identify rumors with a limited amount of annotated data. We go beyond that and investigate the impact of leveraging various machine learning approaches in addition to different contextual representations. We discuss the impact of multiple classifiers on a limited amount of annotated data followed by an interactive approach to gradually update the models by adding the least certain samples (LCS) from the pool of unlabeled data. Our proposed Active Learning (AL) strategy achieves faster convergence in terms of the F-score while requiring fewer annotated samples (42{\%} of the whole dataset for the best model).",
}

@inproceedings{ferreira2016emergent,
           month = {June},
          author = {William Ferreira and Andreas Vlachos},
       booktitle = {The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
           title = {Emergent: a novel data-set for stance classification},
       publisher = {ACL},
         journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
            year = {2016},
             url = {https://eprints.whiterose.ac.uk/97416/},
        abstract = {We present Emergent, a novel dataset derived from a digital journalism project for rumour debunking. The dataset contains 300 rumoured claims and 2,595 associated news articles, collected and labelled by journalists with an estimation of their veracity (true, false or unverified). Each associated article is summarized into a headline and labelled to indicate whether its stance is for, against, or observing the claim, where observing indicates that the article merely repeats the claim. Thus, Emergent provides a real-world data source for a variety of natural language processing tasks in the context of fact-checking. Further to presenting the dataset, we address the task of determining the article headline stance with respect to the claim. For this purpose we use a logistic regression classifier and develop features that examine the headline and its agreement with the claim. The accuracy achieved was 73\% which is 26\% higher than the one achieved by the Excitement Open Platform (Magnini et al., 2014).}
}

@inproceedings{gad2019exfakt,
author = {Gad-Elrab, Mohamed H. and Stepanova, Daria and Urbani, Jacopo and Weikum, Gerhard},
title = {ExFaKT: A Framework for Explaining Facts over Knowledge Graphs and Text},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290996},
doi = {10.1145/3289600.3290996},
abstract = {Fact-checking is a crucial task for accurately populating, updating and curating knowledge graphs. Manually validating candidate facts is time-consuming. Prior work on automating this task focuses on estimating truthfulness using numerical scores which are not human-interpretable. Others extract explicit mentions of the candidate fact in the text as an evidence for the candidate fact, which can be hard to directly spot. In our work, we introduce ExFaKT, a framework focused on generating human-comprehensible explanations for candidate facts. ExFaKT uses background knowledge encoded in the form of Horn clauses to rewrite the fact in question into a set of other easier-to-spot facts. The final output of our framework is a set of semantic traces for the candidate fact from both text and knowledge graphs. The experiments demonstrate that our rewritings significantly increase the recall of fact-spotting while preserving high precision. Moreover, we show that the explanations effectively help humans to perform fact-checking and can also be exploited for automating this task.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {87–95},
numpages = {9},
keywords = {reasoning, knowledge graph, fact-checking, explainable evidence},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@article{graves2017anatomy,
  title={Anatomy of a fact check: Objective practice and the contested epistemology of fact checking},
  author={Graves, Lucas},
  journal={Communication, culture \& critique},
  volume={10},
  number={3},
  pages={518--537},
  year={2017},
  publisher={Oxford University Press},
doi = {https://doi.org/10.1111/cccr.12163}
}

@article{graves2019fact,
  title={Fact-checking as idea and practice in journalism},
  author={Graves, Lucas and Amazeen, Michelle},
  year={2019},
  publisher={Oxford University Press},
journal = {Oxford Research Encyclopedia of Communication},
doi = {10.1093/acrefore/9780190228613.013.808}
}

@article{guo2022survey,
  title={A survey on automated fact-checking},
  author={Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={178--206},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
doi = {10.1162/tacl_a_00454}
}

@inproceedings{gupta2021xfactmultilingual,
    title = "{X}-Fact: A New Benchmark Dataset for Multilingual Fact Checking",
    author = "Gupta, Ashim  and
      Srikumar, Vivek",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.86",
    doi = "10.18653/v1/2021.acl-short.86",
    pages = "675--682",
    abstract = "In this work, we introduce : the largest publicly available multilingual dataset for factual verification of naturally existing real-world claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models. Using state-of-the-art multilingual transformer-based models, we develop several automated fact-checking models that, along with textual claims, make use of additional metadata and evidence from news stories retrieved using a search engine. Empirically, our best model attains an F-score of around 40{\%}, suggesting that our dataset is a challenging benchmark for the evaluation of multilingual fact-checking models.",
}

@misc{hagstrom2024realitycheckcontextutilisation,
      title={A Reality Check on Context Utilisation for Retrieval-Augmented Generation}, 
      author={Lovisa Hagström and Sara Vera Marjanović and Haeun Yu and Arnav Arora and Christina Lioma and Maria Maistro and Pepa Atanasova and Isabelle Augenstein},
      year={2024},
      eprint={2412.17031},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.17031}, 
}

@inproceedings{hanselowski2019corpus,
    title = "A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking",
    author = "Hanselowski, Andreas  and
      Stab, Christian  and
      Schulz, Claudia  and
      Li, Zile  and
      Gurevych, Iryna",
    editor = "Bansal, Mohit  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1046",
    doi = "10.18653/v1/K19-1046",
    pages = "493--503",
    abstract = "Automated fact-checking based on machine learning is a promising approach to identify false information distributed on the web. In order to achieve satisfactory performance, machine learning methods require a large corpus with reliable annotations for the different tasks in the fact-checking process. Having analyzed existing fact-checking corpora, we found that none of them meets these criteria in full. They are either too small in size, do not provide detailed annotations, or are limited to a single domain. Motivated by this gap, we present a new substantially sized mixed-domain corpus with annotations of good quality for the core fact-checking tasks: document retrieval, evidence extraction, stance detection, and claim validation. To aid future corpus construction, we describe our methodology for corpus creation and annotation, and demonstrate that it results in substantial inter-annotator agreement. As baselines for future research, we perform experiments on our corpus with a number of model architectures that reach high performance in similar problem settings. Finally, to support the development of future models, we provide a detailed error analysis for each of the tasks. Our results show that the realistic, multi-domain setting defined by our data poses new challenges for the existing models, providing opportunities for considerable improvement by future systems.",
}

@inproceedings{hardalov-etal-2022-survey,
    title = "A Survey on Stance Detection for Mis- and Disinformation Identification",
    author = "Hardalov, Momchil  and
      Arora, Arnav  and
      Nakov, Preslav  and
      Augenstein, Isabelle",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.94/",
    doi = "10.18653/v1/2022.findings-naacl.94",
    pages = "1259--1277",
    abstract = "Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges."
}

@article{hassan2017ClaimBuster,
author = {Hassan, Naeemul and Zhang, Gensheng and Arslan, Fatma and Caraballo, Josue and Jimenez, Damian and Gawsane, Siddhant and Hasan, Shohedul and Joseph, Minumol and Kulkarni, Aaditya and Nayak, Anil Kumar and Sable, Vikas and Li, Chengkai and Tremayne, Mark},
title = {ClaimBuster: the first-ever end-to-end fact-checking system},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137815},
doi = {10.14778/3137765.3137815},
abstract = {Our society is struggling with an unprecedented amount of falsehoods, hyperboles, and half-truths. Politicians and organizations repeatedly make the same false claims. Fake news floods the cyberspace and even allegedly influenced the 2016 election. In fighting false information, the number of active fact-checking organizations has grown from 44 in 2014 to 114 in early 2017. 1 Fact-checkers vet claims by investigating relevant data and documents and publish their verdicts. For instance, PolitiFact.com, one of the earliest and most popular fact-checking projects, gives factual claims truthfulness ratings such as True, Mostly True, Half true, Mostly False, False, and even "Pants on Fire". In the U.S., the election year made fact-checking a part of household terminology. For example, during the first presidential debate on September 26, 2016, NPR.org's live fact-checking website drew 7.4 million page views and delivered its biggest traffic day ever.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1945–1948},
numpages = {4}
}

@inproceedings{hassan2017TowardsClaimBuster,
author = {Hassan, Naeemul and Arslan, Fatma and Li, Chengkai and Tremayne, Mark},
title = {Toward Automated Fact-Checking: Detecting Check-worthy Factual Claims by ClaimBuster},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098131},
doi = {10.1145/3097983.3098131},
abstract = {This paper introduces how ClaimBuster, a fact-checking platform, uses natural language processing and supervised learning to detect important factual claims in political discourses. The claim spotting model is built using a human-labeled dataset of check-worthy factual claims from the U.S. general election debate transcripts. The paper explains the architecture and the components of the system and the evaluation of the model. It presents a case study of how ClaimBuster live covers the 2016 U.S. presidential election debates and monitors social media and Australian Hansard for factual claims. It also describes the current status and the long-term goals of ClaimBuster as we keep developing and expanding it.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1803–1812},
numpages = {10},
keywords = {text mining, text classification, natural language processing, fact-checking, computational journalism},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{jain2019attention,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1357",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {``}explanations{''} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
}
}

@article{juneja2022human,
author = {Juneja, Prerna and Mitra, Tanushree},
title = {Human and Technological Infrastructures of Fact-checking},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555143},
doi = {10.1145/3555143},
abstract = {Increasing demands for fact-checking have led to a growing interest in developing systems and tools to automate the fact-checking process. However, such systems are limited in practice because their system design often does not take into account how fact-checking is done in the real world and ignores the insights and needs of various stakeholder groups core to the fact-checking process. This paper unpacks the fact-checking process by revealing the infrastructures---both human and technological---that support and shape fact-checking work. We interviewed 26 participants belonging to 16 fact-checking teams and organizations with representation from 4 continents. Through these interviews, we describe the human infrastructure of fact-checking by identifying and presenting, in-depth, the roles of six primary stakeholder groups, 1) Editors, 2) External fact-checkers, 3) In-house fact-checkers, 4) Investigators and researchers, 5) Social media managers, and 6) Advocators. Our findings highlight that the fact-checking process is a collaborative effort among various stakeholder groups and associated technological and informational infrastructures. By rendering visibility to the infrastructures, we reveal how fact-checking has evolved to include both short-term claims centric and long-term advocacy centric fact-checking. Our work also identifies key social and technical needs and challenges faced by each stakeholder group. Based on our findings, we suggest that improving the quality of fact-checking requires systematic changes in the civic, informational, and technological contexts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {418},
numpages = {36},
keywords = {technological infrastructure, misinformation, human infrastructure, fact-checking, collaboration}
}

@article{koliska2024epistemology,
  title={Epistemology of Fact Checking: An Examination of Practices and Beliefs of Fact Checkers Around the World},
  author={Koliska, Michael and Roberts, Jessica},
  journal={Digital Journalism},
  pages={1--21},
  year={2024},
  publisher={Taylor \& Francis},
doi = {10.1080/21670811.2024.2361264}
}

@inproceedings{kotonya2020explainablesurvey,
    title = "Explainable Automated Fact-Checking: A Survey",
    author = "Kotonya, Neema  and
      Toni, Francesca",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.474",
    doi = "10.18653/v1/2020.coling-main.474",
    pages = "5430--5443",
    abstract = "A number of exciting advances have been made in automated fact-checking thanks to increasingly larger datasets and more powerful systems, leading to improvements in the complexity of claims which can be accurately fact-checked. However, despite these advances, there are still desirable functionalities missing from the fact-checking pipeline. In this survey, we focus on the explanation functionality {--} that is fact-checking systems providing reasons for their predictions. We summarize existing methods for explaining the predictions of fact-checking systems and we explore trends in this topic. Further, we consider what makes for good explanations in this specific domain through a comparative analysis of existing fact-checking explanations against some desirable properties. Finally, we propose further research directions for generating fact-checking explanations, and describe how these may lead to improvements in the research area.",
}

@inproceedings{kotonya2020fcpublichealth,
    title = "Explainable Automated Fact-Checking for Public Health Claims",
    author = "Kotonya, Neema  and
      Toni, Francesca",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.623",
    doi = "10.18653/v1/2020.emnlp-main.623",
    pages = "7740--7754",
    abstract = "Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.",
}

@article{langer2021we,
  title={What do we want from Explainable Artificial Intelligence (XAI)?--A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research},
  author={Langer, Markus and Oster, Daniel and Speith, Timo and Hermanns, Holger and Kastner, Lena and Schmidt, Eva and Sesing, Andreas and Baum, Kevin},
  journal={Artificial Intelligence},
  volume={296},
  pages={103473},
  year={2021},
  publisher={Elsevier},
doi = {10.1016/j.artint.2021.103473}
}

@inproceedings{li2021anasearch,
author = {Li, Tongliang and Fang, Lei and Lou, Jian-Guang and Li, Zhoujun and Zhang, Dongmei},
title = {AnaSearch: Extract, Retrieve and Visualize Structured Results from Unstructured Text for Analytical Queries},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441694},
doi = {10.1145/3437963.3441694},
abstract = {Modern search engines retrieve results mainly based on the keyword matching techniques, and thus fail to answer analytical queries like "apps with more than 1 billion monthly active users" or "population growth of the US from 2015 to 2019", which requires numerical reasoning or aggregating results from multiple web pages. Such analytical queries are very common in the data analysis area, the expected results would be structured tables or charts. In most cases, these structured results are not available or accessible, they scatter in various text sources. In this work, we build AnaSearch, a search system to support analytical queries, and return structured results that can be visualized in the form of tables or charts. We collect and build structured quantitative data from the unstructured text on the web automatically. With AnaSearch, data analysts could easily derive insights for decision making with keyword or natural language queries. Specifically, we build AnaSearch under the COVID-19 news data, which makes it easy to compare with manually collected structured data.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {906–909},
numpages = {4},
keywords = {data visualization, information retrieval, quantitative information, structured data},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@misc{liao2022humancenteredexplainableaixai,
      title={Human-Centered Explainable AI (XAI): From Algorithms to User Experiences}, 
      author={Q. Vera Liao and Kush R. Varshney},
      year={2022},
      eprint={2110.10790},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2110.10790}, 
}

@inproceedings{lim2023xai,
author = {Lim, Gionnieve and Perrault, Simon T.},
title = {XAI in Automated Fact-Checking? The Benefits Are Modest and There's No One-Explanation-Fits-All},
year = {2024},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638380.3638388},
doi = {10.1145/3638380.3638388},
abstract = {The massive volume of online information along with the issue of misinformation has spurred active research in the automation of fact-checking. Like fact-checking by human experts, it is not enough for an automated fact-checker to just be accurate, but also be able to inform and convince the user of the validity of its predictions. This becomes viable with explainable artificial intelligence (XAI). In this work, we conduct a study of XAI fact-checkers involving 180 participants to determine how users’ actions towards news and their attitudes towards explanations are affected by the XAI. Our results suggest that XAI has limited effects on users’ agreement with the veracity prediction of the automated fact-checker and on their intent to share news. However, XAI nudges users towards forming uniform judgments of news veracity, thereby signaling their reliance on the explanations. We also found polarizing preferences towards XAI and raise several design considerations on them.},
booktitle = {Proceedings of the 35th Australian Computer-Human Interaction Conference},
pages = {624–638},
numpages = {15},
keywords = {automated fact-checking, explainable artificial intelligence, human-AI interaction, human-centered design, interpretable machine learning, misinformation},
location = {Wellington, New Zealand},
series = {OzCHI '23}
}

@article{linder2021level,
  title={How level of explanation detail affects human performance in interpretable intelligent systems: A study on explainable fact checking},
  author={Linder, Rhema and Mohseni, Sina and Yang, Fan and Pentyala, Shiva K and Ragan, Eric D and Hu, Xia Ben},
  journal={Applied AI Letters},
  volume={2},
  number={4},
  pages={e49},
  year={2021},
  publisher={Wiley Online Library},
doi= {10.1002/ail2.49}
}

@misc{liu2023humancenteredNLP,
      title={Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI}, 
      author={Houjiang Liu and Anubrata Das and Alexander Boltz and Didi Zhou and Daisy Pinaroc and Matthew Lease and Min Kyung Lee},
      year={2023},
      eprint={2308.07213},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
doi = {https://doi.org/10.48550/arXiv.2308.07213}
}

@article{micallef2022true,
author = {Micallef, Nicholas and Armacost, Vivienne and Memon, Nasir and Patil, Sameer},
title = {True or False: Studying the Work Practices of Professional Fact-Checkers},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512974},
doi = {10.1145/3512974},
abstract = {Misinformation has developed into a critical societal threat that can lead to disastrous societal consequences. Although fact-checking plays a key role in combating misinformation, relatively little research has empirically investigated work practices of professional fact-checkers. To address this gap, we conducted semi-structured interviews with 21 fact-checkers from 19 countries. The participants reported being inundated with information that needs filtering and prioritizing prior to fact-checking. The interviews surfaced a pipeline of practices fragmented across disparate tools that lack integration. Importantly, fact-checkers lack effective mechanisms for disseminating the outcomes of their efforts which prevents their work from fully achieving its potential impact. We found that the largely manual and labor intensive nature of current fact-checking practices is a barrier to scale. We apply these findings to propose a number of suggestions that can improve the effectiveness, efficiency, scale, and reach of fact-checking work and its outcomes.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {127},
numpages = {44},
keywords = {disinformation, fact-checker, fact-checking, journalism, misinformation, social media, work practices}
}

@inproceedings{miranda2019automated,
author = {Miranda, Sebasti\~{a}o and Nogueira, David and Mendes, Afonso and Vlachos, Andreas and Secker, Andrew and Garrett, Rebecca and Mitchel, Jeff and Marinho, Zita},
title = {Automated Fact Checking in the News Room},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3314135},
doi = {10.1145/3308558.3314135},
abstract = {Fact checking is an essential task in journalism; its importance has been highlighted due to recently increased concerns and efforts in combating misinformation. In this paper, we present an automated fact checking platform which given a claim, it retrieves relevant textual evidence from a document collection, predicts whether each piece of evidence supports or refutes the claim, and returns a final verdict. We describe the architecture of the system and the user interface, focusing on the choices made to improve its user friendliness and transparency. We conduct a user study of the fact-checking platform in a journalistic setting: we integrated it with a collection of news articles and provide an evaluation of the platform using feedback from journalists in their workflow. We found that the predictions of our platform were correct 58\% of the time, and 59\% of the returned evidence was relevant.},
booktitle = {The World Wide Web Conference},
pages = {3579–3583},
numpages = {5},
keywords = {Media Tools, Fact Checking, Computational Journalism},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{nakov2022clef,
  title={The clef-2022 checkthat! lab on fighting the covid-19 infodemic and fake news detection},
  author={Nakov, Preslav and Barr{\'o}n-Cede{\~n}o, Alberto and Da San Martino, Giovanni and Alam, Firoj and Stru{\ss}, Julia Maria and Mandl, Thomas and M{\'\i}guez, Rub{\'e}n and Caselli, Tommaso and Kutlu, Mucahid and Zaghouani, Wajdi and others},
  booktitle={European Conference on Information Retrieval},
  pages={416--428},
  year={2022},
  organization={Springer},
doi = {10.1007/978-3-030-99739-7_52}
}

@article{nauta2023xaireview,
author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl\"{o}tterer, J\"{o}rg and van Keulen, Maurice and Seifert, Christin},
title = {From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583558},
doi = {10.1145/3583558},
abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {295},
numpages = {42},
keywords = {Explainable artificial intelligence, interpretable machine learning, evaluation, explainability, interpretability, quantitative evaluation methods, explainable AI, XAI}
}

@article{pomerleau2017fakenews,
  title={The fake news challenge: Exploring how artificial intelligence technologies could be leveraged to combat fake news},
  author={Pomerleau, Dean and Rao, Delip},
  journal={Fake news challenge},
  year={2017},
URL = {http://www.fakenewschallenge.org/#}
}

@inproceedings{popat2018declare,
    title = "{D}e{C}lar{E}: Debunking Fake News and False Claims using Evidence-Aware Deep Learning",
    author = "Popat, Kashyap  and
      Mukherjee, Subhabrata  and
      Yates, Andrew  and
      Weikum, Gerhard",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1003",
    doi = "10.18653/v1/D18-1003",
    pages = "22--32",
    abstract = "Misinformation such as fake news is one of the big challenges of our society. Research on automated fact-checking has proposed methods based on supervised learning, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deficit by considering external sources related to a claim. However, these methods require substantial feature modeling and rich lexicons. This paper overcomes these limitations of prior work with an end-to-end model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Experiments with four datasets and ablation studies show the strength of our method.",
}

@inproceedings{schlichtkrull2023usesfactchecking,
    title = "The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who",
    author = "Schlichtkrull, Michael  and
      Ousidhoum, Nedjma  and
      Vlachos, Andreas",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.577",
    doi = "10.18653/v1/2023.findings-emnlp.577",
    pages = "8618--8642",
    abstract = "Automated fact-checking is often presented as an epistemic tool that fact-checkers, social media consumers, and other stakeholders can use to fight misinformation. Nevertheless, few papers thoroughly discuss \textit{how}. We document this by analysing 100 highly-cited papers, and annotating epistemic elements related to intended use, i.e., means, ends, and stakeholders. We find that narratives leaving out some of these aspects are common, that many papers propose inconsistent means and ends, and that the feasibility of suggested strategies rarely has empirical backing. We argue that this vagueness actively hinders the technology from reaching its goals, as it encourages overclaiming, limits criticism, and prevents stakeholder feedback. Accordingly, we provide several recommendations for thinking and writing about the use of fact-checking artefacts.",
}

@inproceedings{schmitt2024explhuman,
author = {Schmitt, Vera and Villa-Arenas, Luis-Felipe and Feldhus, Nils and Meyer, Joachim and Spang, Robert P. and M\"{o}ller, Sebastian},
title = {The Role of Explainability in Collaborative Human-AI Disinformation Detection},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659031},
doi = {10.1145/3630106.3659031},
abstract = {Manual verification has become very challenging based on the increasing volume of information shared online and the role of generative Artificial Intelligence (AI). Thus, AI systems are used to identify disinformation and deep fakes online. Previous research has shown that superior performance can be observed when combining AI and human expertise. Moreover, according to the EU AI Act, human oversight is inevitable when using AI systems in a domain where fundamental human rights, such as the right to free expression, might be affected. Thus, AI systems need to be transparent and offer sufficient explanations to be comprehensible. Much research has been done on integrating eXplainability (XAI) features to increase the transparency of AI systems; however, they lack human-centered evaluation. Additionally, the meaningfulness of explanations varies depending on users’ background knowledge and individual factors. Thus, this research implements a human-centered evaluation schema to evaluate different XAI features for the collaborative human-AI disinformation detection task. Hereby, objective and subjective evaluation dimensions, such as performance, perceived usefulness, understandability, and trust in the AI system, are used to evaluate different XAI features. A user study was conducted with an overall total of 433 participants, whereas 406 crowdworkers and 27 journalists participated as experts in detecting disinformation. The results show that free-text explanations contribute to improving non-expert performance but do not influence the performance of experts. The XAI features increase the perceived usefulness, understandability, and trust in the AI system, but they can also lead crowdworkers to blindly trust the AI system when its predictions are wrong.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2157–2174},
numpages = {18},
keywords = {Collaborative disinformation detection, XAI, expert and lay people evaluation, transparent AI systems},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@book{smith2004factcheckersbible,
  title={The Fact Checker's Bible: A Guide to Getting It Right},
  author={Smith, Sarah Harrison},
  year={2004},
  publisher={Anchor}
}

@inproceedings{sokol2020explassessment,
author = {Sokol, Kacper and Flach, Peter},
title = {Explainability fact sheets: a framework for systematic assessment of explainable approaches},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372870},
doi = {10.1145/3351095.3372870},
abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {56–67},
numpages = {12},
keywords = {AI, ML, desiderata, explainability, fact sheet, interpretability, taxonomy, transparency, work sheet},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{thorne2018fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1074",
    doi = "10.18653/v1/N18-1074",
    pages = "809--819",
    abstract = "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
}

@inproceedings{tschiatschek2018fakenewsdetection,
author = {Tschiatschek, Sebastian and Singla, Adish and Gomez Rodriguez, Manuel and Merchant, Arpit and Krause, Andreas},
title = {Fake News Detection in Social Networks via Crowd Signals},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3188722},
doi = {10.1145/3184558.3188722},
abstract = {Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {517–524},
numpages = {8},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{wright-augenstein-2020-claim,
    title = "Claim Check-Worthiness Detection as Positive Unlabelled Learning",
    author = "Wright, Dustin  and
      Augenstein, Isabelle",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.43/",
    doi = "10.18653/v1/2020.findings-emnlp.43",
    pages = "476--488",
    abstract = "As the first step of automatic fact checking, claim check-worthiness detection is a critical component of fact checking systems. There are multiple lines of research which study this problem: check-worthiness ranking from political speeches and debates, rumour detection on Twitter, and citation needed detection from Wikipedia. To date, there has been no structured comparison of these various tasks to understand their relatedness, and no investigation into whether or not a unified approach to all of them is achievable. In this work, we illuminate a central challenge in claim check-worthiness detection underlying all of these tasks, being that they hinge upon detecting both how factual a sentence is, as well as how likely a sentence is to be believed without verification. As such, annotators only mark those instances they judge to be clear-cut check-worthy. Our best performing method is a unified approach which automatically corrects for this using a variant of positive unlabelled learning that finds instances which were incorrectly labelled as not check-worthy. In applying this, we out-perform the state of the art in two of the three tasks studied for claim check-worthiness detection in English."
}

