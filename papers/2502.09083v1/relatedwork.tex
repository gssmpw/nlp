\section{Related work}
\label{relwork}

\subsection{The structure and practice of fact-checking} \label{relwork:factchecking}
% what is fact-checking?
Fact-checking has its origins in journalism and has emerged as a distinct practice in the last 20 years, spurred on by the proliferation of news through online and social media, and increased political polarisation \cite{smith2004factcheckersbible,graves2019fact,amazeen2020journalistic}.
Current literature distinguishes between
\textit{external} fact-checking, practised by independent fact-checking organisations 
(e.g., Snopes,\footnote{\url{https://www.snopes.com/}} Stowarzyszenie Demagog,\footnote{\url{https://demagog.org.pl/}} BOOM\footnote{\url{https://www.boomlive.in/}}), which involves analysing and verifying public claims such as those made in political statements, news reports, and on social media and
\textit{internal} fact-checking, practised by traditional news organisations, cross-checking and correcting other journalists' work before publication to filter inaccuracies and to protect the publisher from potential liabilities
\cite{graves2019fact,juneja2022human}.
In this work, we focus on the needs of independent (external) fact-checkers, although we anticipate that our findings will be applicable to internal fact-checkers, whose work tasks typically comprise a subset of the fact-checking tasks.
The practice of fact-checking encompasses close collaboration between fact-checkers and news editors who supervise the work, copy editors responsible for the quality of fact-checks, investigators and researchers with expertise in data analysis and visualisation tools, and social media managers who disseminate and maximise impact and engagement of fact-checks \cite{juneja2022human}. 
% Fact-checking organisations are also often involved in advocacy for press freedom, policy issues, and media literacy, as well as building collaboration with other organisations.
% what do fact-checkers do?
Fact-checkers are typically tasked with a wide remit of duties, such as monitoring social media, fielding reader requests, extracting and prioritising claims, researching, consulting data and domain experts, assigning veracity labels, and writing up fact-checks \cite{juneja2022human, micallef2022true, graves2017anatomy}.
Ethnographic and interview studies have documented how fact-checkers worldwide follow broadly similar processes \cite{graves2017anatomy,micallef2022true,juneja2022human,koliska2024epistemology}.
Previous work has grouped these processes into four steps that comprise an archetypal fact-checking pipeline: (i) choosing claims to check; (ii) searching for evidence; (iii) assigning a verdict; and (iv) writing and communicating the fact-check \cite{Das2023state,guo2022survey,micallef2022true,juneja2022human}.\footnote{Some studies (e.g., \cite{micallef2022true,juneja2022human}) also include the additional step of publishing and disseminating the fact-check in this pipeline, however, as this task is usually performed by social media managers rather than fact-checkers, we focus on the four listed here.}

%% ending point: what's missing from the literature -- what are the implications of these things for automated fact-checking
Previous qualitative studies have provided rich accounts of fact-checkers' workflows and the challenges they face \cite{micallef2022true}, detailing the human and organisational infrastructures and stakeholder groups that underlie them \cite{juneja2022human}, their core values \cite{dierickx2023automated} and tensions between their epistemological ideals and practices \cite{bengtsson2024rhetoricalfactcheckers}. However, the precise decision-making and reasoning processes employed by fact-checkers in their work remain unclear.
% and graves.
Studies frequently describe the antipathy of fact-checkers towards integrating automated fact-checking techniques in their work, referring to their lack of utility in practice and the absence of explanations provided by opaque systems \cite{micallef2022true,liu2023humancenteredNLP}.
\textcolor{black}{Yet existing literature does not present tangible solutions for model developers regarding how such flaws can be addressed to support how fact-checkers reason and make decisions about the claims they check.}
We aim to develop a more fine-grained understanding of how fact-checkers evaluate information, assign verdicts, and explain these processes \textcolor{black}{(\textbf{RQ1})}, as a prerequisite to designing computational systems and tools that support and enhance decision-making.

\subsection{The state of automated fact-checking} \label{relwork:automated_FC}
Automated fact-checking systems tend to follow a fact-checking pipeline similar to the one described in the section above \cite{Das2023state,guo2022survey,dmonte2024claimverificationagelarge}. The primary tasks are: 
(i) claim detection and claim filtering \cite{hassan2017ClaimBuster,hassan2017TowardsClaimBuster}; 
(ii) evidence retrieval \cite{clarke2020overview,li2021anasearch};
(iii) veracity prediction \cite{alhindi2018evidence,thorne2018fever,augenstein2019multifc}; and (iv) explanation generation \cite{atanasova2020generating,kotonya2020fcpublichealth}.
Claim detection involves identifying checkable claims 
%(i.e., that assert a verifiable fact, rather than a vague statement or opinion) 
from sources such as social media, news articles or live political debates \cite{hassan2017ClaimBuster}. 
Successful techniques involve human-in-the-loop and semi-supervised active learning approaches where the user of a system can provide feedback or labels for selected instances \cite{farinneya2021activelearningrumor, tschiatschek2018fakenewsdetection}. Additional methods for claim filtering and prioritisation identify urgent claims (based on virality or harmfulness) \cite{nakov2022clef,adair2020automatedjournalism,wright-augenstein-2020-claim}.
%brief summary of evidence retrieval
Stance detection techniques \cite{augenstein-etal-2016-stance,ferreira2016emergent,popat2018declare} are used to classify whether a given piece of evidence supports or refutes a claim. 
Although veracity prediction is performed based on the retrieved evidence, the evidence retrieval step is typically conducted in a coarse-grained way using standard search engines, which are optimised for relevance rather than for veracity prediction, leading to subpar retrieval performance \cite{Das2023state,hardalov-etal-2022-survey,hagstrom2024realitycheckcontextutilisation}. Previous work in information retrieval (IR) has examined the challenge of retrieving credible and relevant information from online sources \cite{clarke2020overview}.
%brief summary of veracity prediction
Automated veracity prediction techniques attempt to determine the veracity of a claim given provided evidence,
%reliance on secondary sources
tending to rely on secondary evidence documents such as news articles \cite{pomerleau2017fakenews,ferreira2016emergent}, Wikipedia \cite{thorne2018fever} or retrieved online sources \cite{augenstein2019multifc}.
%labels - binary, multi-category
These models vary in the number of veracity labels they consider. Where some retain the original labels assigned by fact-checkers \cite{augenstein2019multifc}, others collapse categories such as ``true'' and ``mostly true'', or ``mixed evidence'' and ``unproven'' \cite{hanselowski2019corpus,kotonya2020fcpublichealth,gupta2021xfactmultilingual}.
% highlight limitations of current methods, focus on secondary sources etc.


A growing body of work has examined methods of generating explanations for the decisions of automated fact-checking systems.
Explainable fact-checking methods can be grouped into five categories.
Attribution-based explanations highlight aspects of the evidence (e.g., individual tokens or words) that contributed to a predicted verdict \cite{popat2018declare}.
Rule-based explanations provide a set of rules that describe parts of the decision-making process \cite{gad2019exfakt}.
Counterfactual or adversarial examples that identify minimal changes in an input that can alter a model's prediction provide insight to models' weaknesses or biases \cite{atanasova2020adversarial}.
Case-based explanations provide a rationale for a model's decision by showing how similar instances were assigned the same label by a human \cite{das2022prototex}.
Summarisation explanation methods provide natural language summaries of the evidence to demonstrate that it justifies the verdict \cite{atanasova2020generating,kotonya2020fcpublichealth}.
% In practice, this wouldn't work and the system would predict and explain based on evidence.
All these explanation methods have practical limitations.
For example, the validity and utility of feature attributions as explanations is the subject of ongoing debate (e.g., \cite{jain2019attention,bibal2022attentionexpl}), while rule, summarisation, and case-based explanations assume the existence of an existing knowledge base, a fact-checking article already written by a fact-checker, and human-annotated similar examples, respectively.
Hence, \textcolor{black}{current AI tools for} veracity prediction and explanation have limited utility \textcolor{black}{in real-world applications, such as}  when applied to unseen claims, for which there are no curated evidence documents or fact-checking articles.


\textcolor{black}{Not only are comprehensive empirical evaluations of automated fact-checking tools sparse, automated fact-checking research has been criticised for its disconnect from the practical realities of fact-checking. Existing evidence demonstrates significant gaps between the capabilities of available automated fact-checking tools and fact-checker needs.
For example, professional journalists, evaluating one proposed automated tool for veracity prediction, reported that only 59\% of the claims were accurately classified, and just 58\% of the evidence sentences retrieved were relevant to the claim \cite{miranda2019automated}.
% problems with testing
%problems with researchers:
A content analysis of automated fact-checking research articles found that they were often vague and inconsistent in linking proposed methods to their claimed purposes: only 31\% of papers with the stated aim of automating the fact-checking process included evidence retrieval methods, while 81\% relied solely on classification (or veracity prediction)  \cite{schlichtkrull2023usesfactchecking}.
}
% RQ2: For which parts of the fact-checking processes are explanations of automated fact-checking systems useful?
% It remains unclear whether existing explanation methods address the actual explanation requirements of fact-checkers that are needed for the practical adoption of these techniques.
Our work seeks to document the specific tasks where fact-checkers use automated fact-checking tools and where explanations of automated fact-checking are needed (\textbf{RQ2}) and investigate how automated fact-checking tools can better align with fact-checker requirements.


% point to end on: so doesn't seem like any current methods would work on unseen claims.


\subsection{Explainable AI for fact-checking} \label{relwork:explainableAI}

% Distinction between explainability and interpretability --- here we focus on explainability.
% criteria for explanations
Various desiderata for explanations of AI systems have been proposed in recent years \cite{sokol2020explassessment,langer2021we,nauta2023xaireview}. Of these, researchers have stipulated eight criteria specific to fact-checking explanations \cite{kotonya2020explainablesurvey}. 
These criteria propose that fact-checking explanations should be actionable (i.e., provide steps towards desirable outcome), causal (i.e., use a causal model), coherent (i.e., follow natural laws, be rule-based or otherwise deterministic), context-full (i.e., presented in the context of the claim), interactive (i.e., allow users to provide feedback to the system), parsimonious (i.e., communicate necessary information with minimal redundancy), chronological (i.e., reflect when a statement was made and the information available at that time), and impartial (i.e., avoid partisan language or opinions).
While these criteria are useful for NLP researchers developing basic explanation techniques, an important caveat is that these criteria are what model developers intuit as satisfactory fact-checking explanation, rather than being grounded in the needs of the people that might use these explanations, such as fact-checkers using automated tools.
\textcolor{black}{This limitation is a further example of the disconnect between automated fact-checking research and real-world applications and illustrates a neglect of} stakeholder needs \cite{schlichtkrull2023usesfactchecking}, crucial for explanations to be truly useful \cite{liao2022humancenteredexplainableaixai,langer2021we}.
Automated fact-checking encompasses different groups of stakeholders (e.g., fact-checkers, content-moderators, model developers, and laypeople), each with distinct explanation needs, tailoring to \textcolor{black}{contextual factors}, goals, and levels of expertise \cite{langer2021we,juneja2022human,liao2022humancenteredexplainableaixai,ehsan2024thewho}.
However, existing empirical evaluations of explainable fact-checking are almost exclusively directed at and executed with laypeople \textcolor{black}{from a limited selection of Western countries}, rather than expert fact-checkers \textcolor{black}{with diverse and varied contexts and perspectives}.
\textcolor{black}{One such study indicated that neither feature-attribution nor example-based explanations of automated veracity prediction had an effect on laypeople's perceptions of the veracity of a news story or their intent to share it, but increased their tendency to over-rely on the AI system when it provided incorrect predictions \cite{lim2023xai}. A separate study also found no effect of example-based explanations on people's accuracy in predicting the veracity of a claim \cite{linder2021level}.}
Illustrating that current explanation methods have little utility for fact-checkers, a recent study found that free-text explanations for an automated disinformation detection system improved the performance of laypeople in identifying false information, but not those of journalists \cite{schmitt2024explhuman}.
% Furthermore, explanations led the non-expert users of the system to over-rely on its predictions, even when incorrect.
% providing explanations increased participants' subjective judgments of how useful, understandable, and trustworthy the AI system was, but they can also lead crowdworkers to blindly trust the AI system when its predictions are wrong."
% **deficiencies of existing explainability techniques w.r.t. fact-checkers’ needs – highlight inappropriate evaluation methods and measures in user studies, lack of engagement between fact-checkers + NLP researchers, reports from FC orgs about AI tool usage**
\textcolor{black}{Together, these studies suggest that existing explainability techniques are, at best, ineffective and, at worst, misleading and vulnerable to misuse, potentially bolstering misinformation instead of dispelling it.
Both empirical \cite{schmitt2024explhuman} and literature-based \cite{schlichtkrull2023usesfactchecking} analyses indicate that automated explainable fact-checking research} is shaped by researcher assumptions rather than by direct engagement with fact-checkers, which hinders their utility \cite{kotonya2020explainablesurvey,Das2023state}.
\textcolor{black}{We seek to address these shortcomings by identifying the explanation needs of fact-checkers and how they can be addressed by explainable fact-checking systems (\textbf{RQ3}}).


\begin{table*}[h]
    \centering
    \begin{tabular}{llllll}
\hline
\textbf{Participant ID} & \textbf{Country} & \textbf{Occupation}                  & \textbf{\begin{tabular}[l]{@{}l@{}}Organisational \\ Context\end{tabular}} & \textbf{\begin{tabular}[l]{@{}l@{}}Fact-Checking \\ Experience\end{tabular}} & \textbf{Gender} \\ \hline
P1                      & Ukraine          & Investigative Journalist             & Freelance                                                                  & 8 years                                                                      & Female          \\
P2                      & Argentina        & Fact-Checker                         & Independent                                                                & 4 years                                                                      & Female          \\
P3                      & Poland           & Fact-Checker                         & Independent                                                                & 4 years                                                                      & Male            \\
P4                      & USA              & Investigative Journalist  \& Trainer & Freelance                                                                  & 12 years                                                                     & Female          \\
P5                      & Poland           & Fact-Checker                         & Independent                                                                & 5 years                                                                      & Female          \\
P6                      & Ireland \& USA   & Fact-Checker \& Project Manager      & Independent                                                                & 4 years                                                                      & Male            \\
P7                      & Poland           & Director \& Journalist               & Independent                                                                & 4 years                                                                      & Male            \\
P8                      & Zimbabwe         & Fact-Checker                         & Independent                                                                & 2 years                                                                      & Female          \\
P9                      & Nigeria          & Investigative Journalist             & Independent                                                                & 4 years                                                                      & Male            \\
P10                     & India            & Fact-Checker                         & Independent                                                                & 6 years                                                                      & Male            \\ \hline
\end{tabular}
    \caption{Demographics of the interview participants}
    \label{tab:participants}
    \Description["Demographics of the interview participants" presents information about ten participants, focusing on their Participant ID, Country, Occupation, Organisational Context, Fact-Checking Experience, and Gender. The data for each participant is as follows:
P1 (Ukraine) - Investigative Journalist, Freelance, 8 years, Female
P2 (Argentina) - Fact-Checker, Independent, 4 years, Female
P3 (Poland) - Fact-Checker, Independent, 4 years, Male
P4 (USA) - Investigative Journalist \& Trainer, Freelance, 12 years, Female
P5 (Poland) - Fact-Checker, Independent, 5 years, Female
P6 (Ireland \& USA) - Fact-Checker \& Project Manager, Independent, 4 years, Male
P7 (Poland) - Director \& Journalist, Independent, 4 years, Male
P8 (Zimbabwe) - Fact-Checker, Independent, 2 years, Female
P9 (Nigeria) - Investigative Journalist, Independent, 4 years, Male
P10 (India) - Fact-Checker, Independent, 6 years, Male]
    
\end{table*}