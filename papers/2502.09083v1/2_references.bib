
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@inproceedings{Ostrowski2021p536,
  title     = {Multi-Hop Fact Checking of Political Claims},
  author    = {Ostrowski, Wojciech and Arora, Arnav and Atanasova, Pepa and Augenstein, Isabelle},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {3892--3898},
  year      = {2021},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2021/536},
  url       = {https://doi.org/10.24963/ijcai.2021/536},
}

@inproceedings{pan-etal-2023-fact,
    title = "Fact-Checking Complex Claims with Program-Guided Reasoning",
    author = "Pan, Liangming  and
      Wu, Xiaobao  and
      Lu, Xinyuan  and
      Luu, Anh Tuan  and
      Wang, William Yang  and
      Kan, Min-Yen  and
      Nakov, Preslav",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.386/",
    doi = "10.18653/v1/2023.acl-long.386",
    pages = "6981--7004",
    abstract = "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at \url{https://github.com/mbzuai-nlp/ProgramFC}."
}


@article{juneja2022human,
author = {Juneja, Prerna and Mitra, Tanushree},
title = {Human and Technological Infrastructures of Fact-checking},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555143},
doi = {10.1145/3555143},
abstract = {Increasing demands for fact-checking have led to a growing interest in developing systems and tools to automate the fact-checking process. However, such systems are limited in practice because their system design often does not take into account how fact-checking is done in the real world and ignores the insights and needs of various stakeholder groups core to the fact-checking process. This paper unpacks the fact-checking process by revealing the infrastructures---both human and technological---that support and shape fact-checking work. We interviewed 26 participants belonging to 16 fact-checking teams and organizations with representation from 4 continents. Through these interviews, we describe the human infrastructure of fact-checking by identifying and presenting, in-depth, the roles of six primary stakeholder groups, 1) Editors, 2) External fact-checkers, 3) In-house fact-checkers, 4) Investigators and researchers, 5) Social media managers, and 6) Advocators. Our findings highlight that the fact-checking process is a collaborative effort among various stakeholder groups and associated technological and informational infrastructures. By rendering visibility to the infrastructures, we reveal how fact-checking has evolved to include both short-term claims centric and long-term advocacy centric fact-checking. Our work also identifies key social and technical needs and challenges faced by each stakeholder group. Based on our findings, we suggest that improving the quality of fact-checking requires systematic changes in the civic, informational, and technological contexts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {418},
numpages = {36},
keywords = {technological infrastructure, misinformation, human infrastructure, fact-checking, collaboration}
}

@inproceedings{miranda2019automated,
author = {Miranda, Sebasti\~{a}o and Nogueira, David and Mendes, Afonso and Vlachos, Andreas and Secker, Andrew and Garrett, Rebecca and Mitchel, Jeff and Marinho, Zita},
title = {Automated Fact Checking in the News Room},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3314135},
doi = {10.1145/3308558.3314135},
abstract = {Fact checking is an essential task in journalism; its importance has been highlighted due to recently increased concerns and efforts in combating misinformation. In this paper, we present an automated fact checking platform which given a claim, it retrieves relevant textual evidence from a document collection, predicts whether each piece of evidence supports or refutes the claim, and returns a final verdict. We describe the architecture of the system and the user interface, focusing on the choices made to improve its user friendliness and transparency. We conduct a user study of the fact-checking platform in a journalistic setting: we integrated it with a collection of news articles and provide an evaluation of the platform using feedback from journalists in their workflow. We found that the predictions of our platform were correct 58\% of the time, and 59\% of the returned evidence was relevant.},
booktitle = {The World Wide Web Conference},
pages = {3579–3583},
numpages = {5},
keywords = {Media Tools, Fact Checking, Computational Journalism},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{Doshi-Velez2017TowardsLearning,
    title = {Towards A Rigorous Science of Interpretable Machine Learning},
    year = {2017},
    author = {Doshi-Velez, Finale and Kim, Been},
    journal = {arXiv},
    url = {http://arxiv.org/abs/1702.08608},
    arxivId = {1702.08608}
}
@inproceedings{kotonya2020explainablesurvey,
    title = "Explainable Automated Fact-Checking: A Survey",
    author = "Kotonya, Neema  and
      Toni, Francesca",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.474",
    doi = "10.18653/v1/2020.coling-main.474",
    pages = "5430--5443",
    abstract = "A number of exciting advances have been made in automated fact-checking thanks to increasingly larger datasets and more powerful systems, leading to improvements in the complexity of claims which can be accurately fact-checked. However, despite these advances, there are still desirable functionalities missing from the fact-checking pipeline. In this survey, we focus on the explanation functionality {--} that is fact-checking systems providing reasons for their predictions. We summarize existing methods for explaining the predictions of fact-checking systems and we explore trends in this topic. Further, we consider what makes for good explanations in this specific domain through a comparative analysis of existing fact-checking explanations against some desirable properties. Finally, we propose further research directions for generating fact-checking explanations, and describe how these may lead to improvements in the research area.",
}

@inproceedings{kotonya2020fcpublichealth,
    title = "Explainable Automated Fact-Checking for Public Health Claims",
    author = "Kotonya, Neema  and
      Toni, Francesca",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.623",
    doi = "10.18653/v1/2020.emnlp-main.623",
    pages = "7740--7754",
    abstract = "Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.",
}

@article{guo2022survey,
  title={A survey on automated fact-checking},
  author={Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={178--206},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
doi = {10.1162/tacl_a_00454}
}

@article{Das2023state,
  title={The state of human-centered NLP technology for fact-checking},
  author={Das, Anubrata and Liu, Houjiang and Kovatchev, Venelin and Lease, Matthew},
  journal={Information Processing \& Management},
  volume={60},
  number={2},
  pages={103219},
  year={2023},
  publisher={Elsevier},
doi = {10.1016/j.ipm.2022.103219}
}

@inproceedings{das2021ExFacto,
title={Exfacto: An explainable fact-checking tool},
  author={Das, Anubrata},
booktitle = {Knight Research Network Tool Demonstration Day},
  year={2021},
address = {Virtual Conference},
}

@inproceedings{das2022prototex,
    title = "{P}roto{TE}x: Explaining Model Decisions with Prototype Tensors",
    author = "Das, Anubrata  and
      Gupta, Chitrank  and
      Kovatchev, Venelin  and
      Lease, Matthew  and
      Li, Junyi Jessy",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.213",
    doi = "10.18653/v1/2022.acl-long.213",
    pages = "2986--2997",
    abstract = "We present ProtoTEx, a novel white-box NLP classification architecture based on prototype networks (Li et al., 2018). ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples. At inference time, classification decisions are based on the distances between the input text and the prototype tensors, explained via the training examples most similar to the most influential prototypes. We also describe a novel interleaved training algorithm that effectively handles classes characterized by ProtoTEx indicative features. On a propaganda detection task, ProtoTEx accuracy matches BART-large and exceeds BERTlarge with the added benefit of providing faithful explanations. A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news.",
}


@article{ribeiro2020testingNLP,
place = {Country unknown/Code not available}, year = {2020}, title = {Beyond Accuracy: Behavioral Testing of NLP Models with CheckList}, url = {https://par.nsf.gov/biblio/10347368}, DOI = {10.18653/v1/2020.acl-main.442}, abstractNote = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.}, journal = {Annual Meeting of the Association for Computational Linguistics}, author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer}}

@article{miller2019explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{miller2023explainable,
author = {Miller, Tim},
title = {Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven Decision Support using Evaluative AI},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594001},
doi = {10.1145/3593013.3594001},
abstract = {In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {333–342},
numpages = {10},
location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
series = {FAccT '23}
}
@article{dierickx2023journalism,
  title={Journalism and Fact-Checking Technologies: Understanding User Needs},
  author={Dierickx, Laurence and Lind{\'e}n, Carl-Gustav},
  journal={communication+ 1},
  volume={10},
  number={1},
  year={2023},
  publisher={University of Massachusetts Amherst Libraries}
}}

@inproceedings{lim2023xai,
author = {Lim, Gionnieve and Perrault, Simon T.},
title = {XAI in Automated Fact-Checking? The Benefits Are Modest and There's No One-Explanation-Fits-All},
year = {2024},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638380.3638388},
doi = {10.1145/3638380.3638388},
abstract = {The massive volume of online information along with the issue of misinformation has spurred active research in the automation of fact-checking. Like fact-checking by human experts, it is not enough for an automated fact-checker to just be accurate, but also be able to inform and convince the user of the validity of its predictions. This becomes viable with explainable artificial intelligence (XAI). In this work, we conduct a study of XAI fact-checkers involving 180 participants to determine how users’ actions towards news and their attitudes towards explanations are affected by the XAI. Our results suggest that XAI has limited effects on users’ agreement with the veracity prediction of the automated fact-checker and on their intent to share news. However, XAI nudges users towards forming uniform judgments of news veracity, thereby signaling their reliance on the explanations. We also found polarizing preferences towards XAI and raise several design considerations on them.},
booktitle = {Proceedings of the 35th Australian Computer-Human Interaction Conference},
pages = {624–638},
numpages = {15},
keywords = {automated fact-checking, explainable artificial intelligence, human-AI interaction, human-centered design, interpretable machine learning, misinformation},
location = {Wellington, New Zealand},
series = {OzCHI '23}
}

@article{langer2021we,
  title={What do we want from Explainable Artificial Intelligence (XAI)?--A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research},
  author={Langer, Markus and Oster, Daniel and Speith, Timo and Hermanns, Holger and Kastner, Lena and Schmidt, Eva and Sesing, Andreas and Baum, Kevin},
  journal={Artificial Intelligence},
  volume={296},
  pages={103473},
  year={2021},
  publisher={Elsevier},
doi = {10.1016/j.artint.2021.103473}
}

@article{micallef2022true,
author = {Micallef, Nicholas and Armacost, Vivienne and Memon, Nasir and Patil, Sameer},
title = {True or False: Studying the Work Practices of Professional Fact-Checkers},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512974},
doi = {10.1145/3512974},
abstract = {Misinformation has developed into a critical societal threat that can lead to disastrous societal consequences. Although fact-checking plays a key role in combating misinformation, relatively little research has empirically investigated work practices of professional fact-checkers. To address this gap, we conducted semi-structured interviews with 21 fact-checkers from 19 countries. The participants reported being inundated with information that needs filtering and prioritizing prior to fact-checking. The interviews surfaced a pipeline of practices fragmented across disparate tools that lack integration. Importantly, fact-checkers lack effective mechanisms for disseminating the outcomes of their efforts which prevents their work from fully achieving its potential impact. We found that the largely manual and labor intensive nature of current fact-checking practices is a barrier to scale. We apply these findings to propose a number of suggestions that can improve the effectiveness, efficiency, scale, and reach of fact-checking work and its outcomes.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {127},
numpages = {44},
keywords = {disinformation, fact-checker, fact-checking, journalism, misinformation, social media, work practices}
}

@book{corbin2015basics,
  title={Basics of Qualitative Research},
  author={Corbin, Juliet and Strauss, Anselm},
  volume={14},
  year={2015},
  publisher={sage}
}

@article{appling2022reactions,
  title={Reactions to Fact Checking},
  author={Appling, Scott and Bruckman, Amy and De Choudhury, Munmun},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW2},
  pages={1--17},
  year={2022},
  publisher={ACM New York, NY, USA}
}
@article{la2023combining,
  title={Combining human intelligence and machine learning for fact-checking: Towards a hybrid human-in-the-loop framework},
  author={La Barbera, David and Roitero, Kevin and Mizzaro, Stefano},
  journal={Intelligenza Artificiale},
  number={Preprint},
  pages={1--10},
  year={2023},
  publisher={IOS Press}
}
@article{hrckova2022automated,
  title={Automated, not Automatic: Needs and Practices in European Fact-checking Organizations as a basis for Designing Human-centered AI Systems},
  author={Hrckova, Andrea and Moro, Robert and Srba, Ivan and Simko, Jakub and Bielikova, Maria},
  journal={arXiv preprint arXiv:2211.12143},
  year={2022}
}
@inproceedings{nguyen2018believe,
  title={Believe it or not: Designing a human-ai partnership for mixed-initiative fact-checking},
  author={Nguyen, An T and Kharosekar, Aditya and Krishnan, Saumyaa and Krishnan, Siddhesh and Tate, Elizabeth and Wallace, Byron C and Lease, Matthew},
  booktitle={Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
  pages={189--199},
  year={2018}
}
@book{greifeneder2021psychology,
  title={The psychology of fake news: Accepting, sharing, and correcting misinformation},
  author={Greifeneder, Rainer and Jaffe, Mariela and Newman, Eryn and Schwarz, Norbert},
  year={2021}
}
@article{pennycook2021psychology,
  title={The psychology of fake news},
  author={Pennycook, Gordon and Rand, David G},
  journal={Trends in cognitive sciences},
  volume={25},
  number={5},
  pages={388--402},
  year={2021},
  publisher={Elsevier}
}
@article{lewandowsky2021countering,
  title={Countering misinformation and fake news through inoculation and prebunking},
  author={Lewandowsky, Stephan and Van Der Linden, Sander},
  journal={European Review of Social Psychology},
  volume={32},
  number={2},
  pages={348--384},
  year={2021},
  publisher={Taylor \& Francis}
}
@article{adams2023misinformation,
  title={(Why) is misinformation a problem?},
  author={Adams, Zo{\"e} and Osman, Magda and Bechlivanidis, Christos and Meder, Bj{\"o}rn},
  journal={Perspectives on Psychological Science},
  volume={18},
  number={6},
  pages={1436--1463},
  year={2023},
  publisher={Sage Publications Sage CA: Los Angeles, CA},
doi = {10.1177/17456916221141344}
}
@article{ecker2022psychological,
  title={The psychological drivers of misinformation belief and its resistance to correction},
  author={Ecker, Ullrich KH and Lewandowsky, Stephan and Cook, John and Schmid, Philipp and Fazio, Lisa K and Brashier, Nadia and Kendeou, Panayiota and Vraga, Emily K and Amazeen, Michelle A},
  journal={Nature Reviews Psychology},
  volume={1},
  number={1},
  pages={13--29},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@misc{singh2024factcheckingpoliticalneutrality,
      title={Independent fact-checking organizations exhibit a departure from political neutrality}, 
      author={Sahajpreet Singh and Sarah Masud and Tanmoy Chakraborty},
      year={2024},
      eprint={2407.19498},
      archivePrefix={arXiv},
      primaryClass={cs.SI},
      url={https://arxiv.org/abs/2407.19498}, 
}

@techreport{arnold2020onlineFC,
    author = {Arnold, Phoebe},
    title = {The challenges of online fact checking: how technology can (and can’t) help},
    institution = {Full Fact},
    year = {2020},
URL = {https://fullfact.org/blog/2020/dec/the-challenges-of-online-fact-checking-how-technology-can-and-cant-help/}
}

@article{donaldson2002understanding,
  title={Understanding self-report bias in organizational behavior research},
  author={Donaldson, Stewart I and Grant-Vallone, Elisa J},
  journal={Journal of business and Psychology},
  volume={17},
  pages={245--260},
  year={2002},
  publisher={Springer},
doi = {10.1023/A:1019637632584}
}
@article{cizmeciler2022leveraging,
  title={Leveraging semantic saliency maps for query-specific video summarization},
  author={Cizmeciler, Kemal and Erdem, Erkut and Erdem, Aykut},
  journal={Multimedia Tools and Applications},
  volume={81},
  number={12},
  pages={17457--17482},
  year={2022},
  publisher={Springer},
doi = {10.1007/s11042-022-12442-w}
}
@article{van2008faking,
  title={Faking it: social desirability response bias in self-report research},
  author={Van de Mortel, Thea F},
  journal={Australian Journal of Advanced Nursing, The},
  volume={25},
  number={4},
  pages={40--48},
  year={2008},
}

@techreport{IFCN2024report,
    author = {International Fact-Checking Network},
    title = {State of the Fact-Checkers Report},
    institution = {International Fact-Checking Network},
    year = {2024},
URL = {https://www.poynter.org/wp-content/uploads/2024/04/State-of-Fact-Checkers-2023.pdf}
}

@techreport{Duke2024FactCheckingSputters,
    author = {Stencel, Mark and Ryan, Erica and Luther, Joel},
    title = {With half the planet going to the polls in 2024, fact-checking sputters},
    institution = {Duke Reporters' Lab},
    year = {2024},
URL = {https://reporterslab.org/with-half-the-planet-going-to-the-polls-in-2024-fact-checking-sputters/}
}

@inproceedings{xu2023misinfogenai,
author = {Xu, Danni and Fan, Shaojing and Kankanhalli, Mohan},
title = {Combating Misinformation in the Era of Generative AI Models},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612704},
doi = {10.1145/3581783.3612704},
abstract = {Misinformation has been a persistent and harmful phenomenon affecting our society in various ways, including individuals' physical health and economic stability. With the rise of short video platforms and related applications, the spread of multi-modal misinformation, encompassing images, texts, audios, and videos have exacerbated these concerns. The introduction of generative AI models like ChatGPT and Stable Diffusion has further complicated matters, giving rise to Artificial Intelligence Generated Content (AIGC) and presenting new challenges in detecting and mitigating misinformation. Consequently, traditional approaches to misinformation detection and intervention have become inadequate in this evolving landscape. This paper explores the challenges posed by AIGC in the context of misinformation. It examines the issue from psychological and societal perspectives, and explores the subtle manipulation traces found in AIGC at signal, perceptual, semantic, and human levels. By scrutinizing manipulation traces such as signal manipulation, semantic inconsistencies, logical incoherence, and psychological strategies, our objective is to tackle AI-generated misinformation and provide a conceptual design of systematic explainable solution. Ultimately, we aim for this paper to contribute valuable insights into combating misinformation, particularly in the era of AIGC.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9291–9298},
numpages = {8},
keywords = {aigc, generative ai models, misinformation detection, multimodal},
location = {Ottawa ON, Canada},
series = {MM '23}
}
@inproceedings{bibal2022attentionexpl,
    title = "Is Attention Explanation? An Introduction to the Debate",
    author = "Bibal, Adrien  and
      Cardon, R{\'e}mi  and
      Alfter, David  and
      Wilkens, Rodrigo  and
      Wang, Xiaoou  and
      Fran{\c{c}}ois, Thomas  and
      Watrin, Patrick",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.269",
    doi = "10.18653/v1/2022.acl-long.269",
    pages = "3889--3900",
    abstract = "The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.",
}

@misc{liao2022humancenteredexplainableaixai,
      title={Human-Centered Explainable AI (XAI): From Algorithms to User Experiences}, 
      author={Q. Vera Liao and Kush R. Varshney},
      year={2022},
      eprint={2110.10790},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2110.10790}, 
}
@inproceedings{jain2019attention,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1357",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {``}explanations{''} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
}
}

@inproceedings{marjanović2024internalconflictcontextualadaptation,
    title = "{DYNAMICQA}: Tracing Internal Knowledge Conflicts in Language Models",
    author = "Marjanovic, Sara Vera  and
      Yu, Haeun  and
      Atanasova, Pepa  and
      Maistro, Maria  and
      Lioma, Christina  and
      Augenstein, Isabelle",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.838/",
    doi = "10.18653/v1/2024.findings-emnlp.838",
    pages = "14346--14360",
    abstract = "Knowledge-intensive language understanding tasks require Language Models (LMs) to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. However, conflicting knowledge can be present in the LM`s parameters, termed intra-memory conflict, which can affect a model`s propensity to accept contextual knowledge. To study the effect of intra-memory conflict on LM`s ability to accept the relevant context, we utilise two knowledge conflict measures and a novel dataset containing inherently conflicting data, DYNAMICQA. This dataset includes facts with a temporal dynamic nature where facts can change over time and disputable dynamic facts, which can change depending on the viewpoint. DYNAMICQA is the first to include real-world knowledge conflicts and provide context to study the link between the different types of knowledge conflicts. We also evaluate several measures on their ability to reflect the presence of intra-memory conflict: semantic entropy and a novel coherent persuasion score. With our extensive experiments, we verify that LMs show a greater degree of intra-memory conflict with dynamic facts compared to facts that have a single truth value. Further, we reveal that facts with intra-memory conflict are harder to update with context, suggesting that retrieval-augmented generation will struggle with the most commonly adapted facts"
}

@article{augenstein2024factualityLLMs,
  title={Factuality challenges in the era of large language models and opportunities for fact-checking},
  author={Augenstein, Isabelle and Baldwin, Timothy and Cha, Meeyoung and Chakraborty, Tanmoy and Ciampaglia, Giovanni Luca and Corney, David and DiResta, Renee and Ferrara, Emilio and Hale, Scott and Halevy, Alon and Hovy, Eduard and Ji, Heng and Menczer, Filippo and Miguez, Ruben and Nakov, Preslav and Scheufele, Dietram and Sharma, 
Shivam and Zagni, Giovanni},
  journal={Nature Machine Intelligence},
  pages={1--12},
  year={2024},
  publisher={Nature Publishing Group UK London},
doi = {10.1038/s42256-024-00881-z}
}

@misc{dmonte2024claimverificationagelarge,
      title={Claim Verification in the Age of Large Language Models: A Survey}, 
      author={Alphaeus Dmonte and Roland Oruche and Marcos Zampieri and Prasad Calyam and Isabelle Augenstein},
      year={2024},
      eprint={2408.14317},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.14317}, 
doi = {10.48550/arXiv.2408.14317}
}

@inproceedings{augenstein2019multifc,
    title = "{M}ulti{FC}: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
    author = "Augenstein, Isabelle  and
      Lioma, Christina  and
      Wang, Dongsheng  and
      Chaves Lima, Lucas  and
      Hansen, Casper  and
      Hansen, Christian  and
      Simonsen, Jakob Grue",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1475",
    doi = "10.18653/v1/D19-1475",
    pages = "4685--4697",
    abstract = "We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2{\%}, showing that this is a challenging testbed for claim veracity prediction.",
}
@inproceedings{popat2018declare,
    title = "{D}e{C}lar{E}: Debunking Fake News and False Claims using Evidence-Aware Deep Learning",
    author = "Popat, Kashyap  and
      Mukherjee, Subhabrata  and
      Yates, Andrew  and
      Weikum, Gerhard",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1003",
    doi = "10.18653/v1/D18-1003",
    pages = "22--32",
    abstract = "Misinformation such as fake news is one of the big challenges of our society. Research on automated fact-checking has proposed methods based on supervised learning, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deficit by considering external sources related to a claim. However, these methods require substantial feature modeling and rich lexicons. This paper overcomes these limitations of prior work with an end-to-end model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Experiments with four datasets and ablation studies show the strength of our method.",
}
@inproceedings{ahmadi2019explainable,
  title={Explainable Fact Checking with Probabilistic Answer Set Programming},
  author={Ahmadi, Naser and Lee, Joohyung and Papotti, Paolo and Saeed, Mohammed},
  booktitle={Conference on Truth and Trust Online},
  year={2019},
doi = {10.48550/arXiv.1906.09198}
}

@inproceedings{atanasova2020generating,
    title = "Generating Fact Checking Explanations",
    author = "Atanasova, Pepa  and
      Simonsen, Jakob Grue  and
      Lioma, Christina  and
      Augenstein, Isabelle",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.656",
    doi = "10.18653/v1/2020.acl-main.656",
    pages = "7352--7364",
    abstract = "Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process {--} generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.",
}

@inproceedings{atanasova2020adversarial,
    title = "Generating Label Cohesive and Well-Formed Adversarial Claims",
    author = "Atanasova, Pepa  and
      Wright, Dustin  and
      Augenstein, Isabelle",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.256",
    doi = "10.18653/v1/2020.emnlp-main.256",
    pages = "3168--3177",
    abstract = "Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimizing the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.",
}

@inproceedings{wright-augenstein-2020-claim,
    title = "Claim Check-Worthiness Detection as Positive Unlabelled Learning",
    author = "Wright, Dustin  and
      Augenstein, Isabelle",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.43/",
    doi = "10.18653/v1/2020.findings-emnlp.43",
    pages = "476--488",
    abstract = "As the first step of automatic fact checking, claim check-worthiness detection is a critical component of fact checking systems. There are multiple lines of research which study this problem: check-worthiness ranking from political speeches and debates, rumour detection on Twitter, and citation needed detection from Wikipedia. To date, there has been no structured comparison of these various tasks to understand their relatedness, and no investigation into whether or not a unified approach to all of them is achievable. In this work, we illuminate a central challenge in claim check-worthiness detection underlying all of these tasks, being that they hinge upon detecting both how factual a sentence is, as well as how likely a sentence is to be believed without verification. As such, annotators only mark those instances they judge to be clear-cut check-worthy. Our best performing method is a unified approach which automatically corrects for this using a variant of positive unlabelled learning that finds instances which were incorrectly labelled as not check-worthy. In applying this, we out-perform the state of the art in two of the three tasks studied for claim check-worthiness detection in English."
}

@inproceedings{augenstein-etal-2016-stance,
    title = "Stance Detection with Bidirectional Conditional Encoding",
    author = {Augenstein, Isabelle  and
      Rockt{\"a}schel, Tim  and
      Vlachos, Andreas  and
      Bontcheva, Kalina},
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1084/",
    doi = "10.18653/v1/D16-1084",
    pages = "876--885"
}
@article{alvarezmils2018robustness,
  author       = {David Alvarez{-}Melis and
                  Tommi S. Jaakkola},
  title        = {On the Robustness of Interpretability Methods},
  journal      = {CoRR},
  volume       = {abs/1806.08049},
  year         = {2018},
  url          = {http://arxiv.org/abs/1806.08049},
  eprinttype    = {arXiv},
  eprint       = {1806.08049},
  timestamp    = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1806-08049.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{slack2020fooling,
author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
title = {Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375830},
doi = {10.1145/3375627.3375830},
abstract = {As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {180–186},
numpages = {7},
keywords = {adversarial attacks, bias detection, black box explanations, model interpretability},
location = {New York, NY, USA},
series = {AIES '20}
}

@inproceedings{hardalov-etal-2022-survey,
    title = "A Survey on Stance Detection for Mis- and Disinformation Identification",
    author = "Hardalov, Momchil  and
      Arora, Arnav  and
      Nakov, Preslav  and
      Augenstein, Isabelle",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.94/",
    doi = "10.18653/v1/2022.findings-naacl.94",
    pages = "1259--1277",
    abstract = "Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges."
}

@misc{hagstrom2024realitycheckcontextutilisation,
      title={A Reality Check on Context Utilisation for Retrieval-Augmented Generation}, 
      author={Lovisa Hagström and Sara Vera Marjanović and Haeun Yu and Arnav Arora and Christina Lioma and Maria Maistro and Pepa Atanasova and Isabelle Augenstein},
      year={2024},
      eprint={2412.17031},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.17031}, 
}

@article{de2022invisible,
  title={Invisible friend or foe? How journalists use and perceive algorithmic-driven tools in their research process},
  author={de Haan, Yael and van den Berg, Eric and Goutier, Nele and Kruikemeier, Sanne and Lecheler, Sophie},
  journal={Digital Journalism},
  volume={10},
  number={10},
  pages={1775--1793},
  year={2022},
  publisher={Taylor \& Francis},
doi = {10.1080/21670811.2022.2027798}
}

@article{atanasova2022insufficient,
  title={Fact checking with insufficient evidence},
  author={Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={746--763},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
doi = {https://doi.org/10.1162/tacl_a_00486}
}
@inproceedings{thorne2018fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1074",
    doi = "10.18653/v1/N18-1074",
    pages = "809--819",
    abstract = "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
}

@inproceedings{alhindi2018evidence,
    title = "Where is Your Evidence: Improving Fact-checking by Justification Modeling",
    author = "Alhindi, Tariq  and
      Petridis, Savvas  and
      Muresan, Smaranda",
    editor = "Thorne, James  and
      Vlachos, Andreas  and
      Cocarascu, Oana  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    booktitle = "Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER})",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5513",
    doi = "10.18653/v1/W18-5513",
    pages = "85--90",
    abstract = "Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).",
}
@inproceedings{clarke2020overview,
  title={Overview of the TREC 2020 Health Misinformation Track.},
  author={Clarke, Charles L.A. and Maistro, Maria and Smucker, Mark D.},
year = {2020},
booktitle = {Text Retrieval Conference 2020},
URL = {https://trec.nist.gov/pubs/trec30/papers/Overview-HM.pdf}
}

@inproceedings{rogers-augenstein-2020-improve,
    title = "What Can We Do to Improve Peer Review in {NLP}?",
    author = "Rogers, Anna  and
      Augenstein, Isabelle",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.112",
    doi = "10.18653/v1/2020.findings-emnlp.112",
    pages = "1256--1262",
    abstract = "Peer review is our best tool for judging the quality of conference submissions, but it is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways forward, but the key difficulty is creating the incentives and mechanisms for their consistent implementation in the NLP community.",
}

@inproceedings{calabrese-etal-2024-explainability,
    title = "Explainability and Hate Speech: Structured Explanations Make Social Media Moderators Faster",
    author = {Calabrese, Agostina  and
      Neves, Leonardo  and
      Shah, Neil  and
      Bos, Maarten  and
      Ross, Bj{\"o}rn  and
      Lapata, Mirella  and
      Barbieri, Francesco},
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-short.38",
    doi = "10.18653/v1/2024.acl-short.38",
    pages = "398--408",
    abstract = "Content moderators play a key role in keeping the conversation on social media healthy. While the high volume of content they need to judge represents a bottleneck to the moderation pipeline, no studies have explored how models could support them to make faster decisions. There is, by now, a vast body of research into detecting hate speech, sometimes explicitly motivated by a desire to help improve content moderation, but published research using real content moderators is scarce. In this work we investigate the effect of explanations on the speed of real-world moderators. Our experiments show that while generic explanations do not affect their speed and are often ignored, structured explanations lower moderators{'} decision making time by 7.4{\%}.",
}

@inproceedings{wang2023GPTExplsHatefulContent,
author = {Wang, Han and Hee, Ming Shan and Awal, Md Rabiul and Choo, Kenny Tsu Wei and Lee, Roy Ka-Wei},
title = {Evaluating GPT-3 generated explanations for hateful content moderation},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/694},
doi = {10.24963/ijcai.2023/694},
abstract = {Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and nonhateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content. Our study underscores the need for caution in applying LLM-generated explanations for content moderation. Code and results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {694},
numpages = {9},
location = {Macao, P.R.China},
series = {IJCAI '23}
}

@inproceedings{li2021anasearch,
author = {Li, Tongliang and Fang, Lei and Lou, Jian-Guang and Li, Zhoujun and Zhang, Dongmei},
title = {AnaSearch: Extract, Retrieve and Visualize Structured Results from Unstructured Text for Analytical Queries},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441694},
doi = {10.1145/3437963.3441694},
abstract = {Modern search engines retrieve results mainly based on the keyword matching techniques, and thus fail to answer analytical queries like "apps with more than 1 billion monthly active users" or "population growth of the US from 2015 to 2019", which requires numerical reasoning or aggregating results from multiple web pages. Such analytical queries are very common in the data analysis area, the expected results would be structured tables or charts. In most cases, these structured results are not available or accessible, they scatter in various text sources. In this work, we build AnaSearch, a search system to support analytical queries, and return structured results that can be visualized in the form of tables or charts. We collect and build structured quantitative data from the unstructured text on the web automatically. With AnaSearch, data analysts could easily derive insights for decision making with keyword or natural language queries. Specifically, we build AnaSearch under the COVID-19 news data, which makes it easy to compare with manually collected structured data.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {906–909},
numpages = {4},
keywords = {data visualization, information retrieval, quantitative information, structured data},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{jiang2020hover,
    title = "{H}o{V}er: A Dataset for Many-Hop Fact Extraction And Claim Verification",
    author = "Jiang, Yichen  and
      Bordia, Shikha  and
      Zhong, Zheng  and
      Dognin, Charles  and
      Singh, Maneesh  and
      Bansal, Mohit",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.309",
    doi = "10.18653/v1/2020.findings-emnlp.309",
    pages = "3441--3460",
    abstract = "We introduce HoVer (HOppy VERification), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is supported or not-supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes. Moreover, most of the 3/4-hop claims are written in multiple sentences, which adds to the complexity of understanding long-range dependency relations such as coreference. We show that the performance of an existing state-of-the-art semantic-matching model degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of many-hop reasoning to achieve strong results. We hope that the introduction of this challenging dataset and the accompanying evaluation task will encourage research in many-hop fact retrieval and information verification.",
}
@inproceedings{schuster2021factcontrastive,
    title = "Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence",
    author = "Schuster, Tal  and
      Fisch, Adam  and
      Barzilay, Regina",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.52",
    doi = "10.18653/v1/2021.naacl-main.52",
    pages = "624--643",
    abstract = "Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness{---}improving accuracy by 10{\%} on adversarial fact verification and 6{\%} on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.",
}

@inproceedings{ferreira2016emergent,
           month = {June},
          author = {William Ferreira and Andreas Vlachos},
       booktitle = {The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
           title = {Emergent: a novel data-set for stance classification},
       publisher = {ACL},
         journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
            year = {2016},
             url = {https://eprints.whiterose.ac.uk/97416/},
        abstract = {We present Emergent, a novel dataset derived from a digital journalism project for rumour debunking. The dataset contains 300 rumoured claims and 2,595 associated news articles, collected and labelled by journalists with an estimation of their veracity (true, false or unverified). Each associated article is summarized into a headline and labelled to indicate whether its stance is for, against, or observing the claim, where observing indicates that the article merely repeats the claim. Thus, Emergent provides a real-world data source for a variety of natural language processing tasks in the context of fact-checking. Further to presenting the dataset, we address the task of determining the article headline stance with respect to the claim. For this purpose we use a logistic regression classifier and develop features that examine the headline and its agreement with the claim. The accuracy achieved was 73\% which is 26\% higher than the one achieved by the Excitement Open Platform (Magnini et al., 2014).}
}
@article{pomerleau2017fakenews,
  title={The fake news challenge: Exploring how artificial intelligence technologies could be leveraged to combat fake news},
  author={Pomerleau, Dean and Rao, Delip},
  journal={Fake news challenge},
  year={2017},
URL = {http://www.fakenewschallenge.org/#}
}

@inproceedings{hanselowski2019corpus,
    title = "A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking",
    author = "Hanselowski, Andreas  and
      Stab, Christian  and
      Schulz, Claudia  and
      Li, Zile  and
      Gurevych, Iryna",
    editor = "Bansal, Mohit  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1046",
    doi = "10.18653/v1/K19-1046",
    pages = "493--503",
    abstract = "Automated fact-checking based on machine learning is a promising approach to identify false information distributed on the web. In order to achieve satisfactory performance, machine learning methods require a large corpus with reliable annotations for the different tasks in the fact-checking process. Having analyzed existing fact-checking corpora, we found that none of them meets these criteria in full. They are either too small in size, do not provide detailed annotations, or are limited to a single domain. Motivated by this gap, we present a new substantially sized mixed-domain corpus with annotations of good quality for the core fact-checking tasks: document retrieval, evidence extraction, stance detection, and claim validation. To aid future corpus construction, we describe our methodology for corpus creation and annotation, and demonstrate that it results in substantial inter-annotator agreement. As baselines for future research, we perform experiments on our corpus with a number of model architectures that reach high performance in similar problem settings. Finally, to support the development of future models, we provide a detailed error analysis for each of the tasks. Our results show that the realistic, multi-domain setting defined by our data poses new challenges for the existing models, providing opportunities for considerable improvement by future systems.",
}

@inproceedings{gupta2021xfactmultilingual,
    title = "{X}-Fact: A New Benchmark Dataset for Multilingual Fact Checking",
    author = "Gupta, Ashim  and
      Srikumar, Vivek",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.86",
    doi = "10.18653/v1/2021.acl-short.86",
    pages = "675--682",
    abstract = "In this work, we introduce : the largest publicly available multilingual dataset for factual verification of naturally existing real-world claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models. Using state-of-the-art multilingual transformer-based models, we develop several automated fact-checking models that, along with textual claims, make use of additional metadata and evidence from news stories retrieved using a search engine. Empirically, our best model attains an F-score of around 40{\%}, suggesting that our dataset is a challenging benchmark for the evaluation of multilingual fact-checking models.",
}

@inproceedings{schlichtkrull2024Averitec,
author = {Schlichtkrull, Michael and Guo, Zhijiang and Vlachos, Andreas},
title = {AVERITEC: a dataset for real-world claim verification with evidence from the web},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper, we introduce AVERITEC, a new dataset of 4,568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of κ = 0.619 on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through question-answering against the open web.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {2842},
numpages = {40},
location = {New Orleans, LA, USA},
series = {NIPS '23},
doi = {10.5555/3666122.3668964}
}

@inproceedings{schlichtkrull2024generatingmediabackgroundchecks,
    title = "Generating Media Background Checks for Automated Source Critical Reasoning",
    author = "Schlichtkrull, Michael Sejr",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.283/",
    doi = "10.18653/v1/2024.findings-emnlp.283",
    pages = "4927--4947",
    abstract = "Not everything on the internet is true. This unfortunate fact requires both humans and models to perform complex reasoning about credibility when working with retrieved information. In NLP, this problem has seen little attention. Indeed, retrieval-augmented models are not typically expected to distrust retrieved documents. Human experts overcome the challenge by gathering signals about the context, reliability, and tendency of source documents - that is, they perform *source criticism*. We propose a novel NLP task focused on finding and summarising such signals. We introduce a new dataset of 6,709 {\textquotedblleft}media background checks{\textquotedblright} derived from Media Bias / Fact Check, a volunteer-run website documenting media bias. We test open-source and closed-source LLM baselines with and without retrieval on this dataset, finding that retrieval greatly improves performance. We furthermore carry out human evaluation, demonstrating that 1) media background checks are helpful for humans, and 2) media background checks are helpful for retrieval-augmented models."
}

@inproceedings{schlichtkrull2023usesfactchecking,
    title = "The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who",
    author = "Schlichtkrull, Michael  and
      Ousidhoum, Nedjma  and
      Vlachos, Andreas",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.577",
    doi = "10.18653/v1/2023.findings-emnlp.577",
    pages = "8618--8642",
    abstract = "Automated fact-checking is often presented as an epistemic tool that fact-checkers, social media consumers, and other stakeholders can use to fight misinformation. Nevertheless, few papers thoroughly discuss \textit{how}. We document this by analysing 100 highly-cited papers, and annotating epistemic elements related to intended use, i.e., means, ends, and stakeholders. We find that narratives leaving out some of these aspects are common, that many papers propose inconsistent means and ends, and that the feasibility of suggested strategies rarely has empirical backing. We argue that this vagueness actively hinders the technology from reaching its goals, as it encourages overclaiming, limits criticism, and prevents stakeholder feedback. Accordingly, we provide several recommendations for thinking and writing about the use of fact-checking artefacts.",
}

@inproceedings{tan2024blinded,
    title = "Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?",
    author = "Tan, Hexiang  and
      Sun, Fei  and
      Yang, Wanli  and
      Wang, Yuanzhuo  and
      Cao, Qi  and
      Cheng, Xueqi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.337/",
    doi = "10.18653/v1/2024.acl-long.337",
    pages = "6207--6227",
    abstract = "While auxiliary information has become a key to enhancing Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically contexts generated by LLMs and those retrieved from external sources.To investigate this, we formulate a systematic framework to identify whether LLMs' responses are attributed to either generated or retrieved contexts.To easily trace the origin of the response, we construct datasets with conflicting contexts, i.e., each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer.Our experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to favor generated contexts, even when they provide incorrect information.We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of being selected; ii) the segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs.Our analysis enhances the understanding of how LLMs merge diverse contexts, offers valuable insights for advancing current LLM augmentation methods, and highlights the risk of generated misinformation for retrieval-augmented LLMs."
}
@inproceedings{yuan2020credibility,
    title = "Early Detection of Fake News by Utilizing the Credibility of News, Publishers, and Users based on Weakly Supervised Learning",
    author = "Yuan, Chunyuan  and
      Ma, Qianwen  and
      Zhou, Wei  and
      Han, Jizhong  and
      Hu, Songlin",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.475",
    doi = "10.18653/v1/2020.coling-main.475",
    pages = "5444--5454",
    abstract = "The dissemination of fake news significantly affects personal reputation and public trust. Recently, fake news detection has attracted tremendous attention, and previous studies mainly focused on finding clues from news content or diffusion path. However, the required features of previous models are often unavailable or insufficient in early detection scenarios, resulting in poor performance. Thus, early fake news detection remains a tough challenge. Intuitively, the news from trusted and authoritative sources or shared by many users with a good reputation is more reliable than other news. Using the credibility of publishers and users as prior weakly supervised information, we can quickly locate fake news in massive news and detect them in the early stages of dissemination. In this paper, we propose a novel structure-aware multi-head attention network (SMAN), which combines the news content, publishing, and reposting relations of publishers and users, to jointly optimize the fake news detection and credibility prediction tasks. In this way, we can explicitly exploit the credibility of publishers and users for early fake news detection. We conducted experiments on three real-world datasets, and the results show that SMAN can detect fake news in 4 hours with an accuracy of over 91{\%}, which is much faster than the state-of-the-art models.",
}

@inproceedings{baly2018factualitybias, title = "Predicting Factuality of Reporting and Bias of News Media Sources",
    author = "Baly, Ramy  and
      Karadzhov, Georgi  and
      Alexandrov, Dimitar  and
      Glass, James  and
      Nakov, Preslav",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1389",
    doi = "10.18653/v1/D18-1389",
    pages = "3528--3539",
    abstract = "We present a study on predicting the factuality of reporting and bias of news media. While previous work has focused on studying the veracity of claims or documents, here we are interested in characterizing entire news media. This is an under-studied, but arguably important research problem, both in its own right and as a prior for fact-checking systems. We experiment with a large list of news websites and with a rich set of features derived from (i) a sample of articles from the target news media, (ii) its Wikipedia page, (iii) its Twitter account, (iv) the structure of its URL, and (v) information about the Web traffic it attracts. The experimental results show sizable performance gains over the baseline, and reveal the importance of each feature type.",
}

@inproceedings{baly2020written,
    title = "What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context",
    author = "Baly, Ramy  and
      Karadzhov, Georgi  and
      An, Jisun  and
      Kwak, Haewoon  and
      Dinkov, Yoan  and
      Ali, Ahmed  and
      Glass, James  and
      Nakov, Preslav",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.308",
    doi = "10.18653/v1/2020.acl-main.308",
    pages = "3364--3374",
    abstract = "Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically. Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content. This makes it possible to detect likely {``}fake news{''} the moment they are published, by simply checking the reliability of their source. From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium{'}s audience on social media). We further study (iii) what was written about the target medium (in Wikipedia). The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.",
}

@misc{MetaFactChecking2025, title={More speech and fewer mistakes}, url={https://about.fb.com/news/2025/01/meta-more-speech-fewer-mistakes/}, journal={Meta}, author={Kaplan, Joel}, year={2025}, month={Jan}} 

@misc{PoynterMeta2025, title={Meta is ending its third-party fact-checking partnership with US partners. Here’s how that program works.}, url={https://www.poynter.org/fact-checking/2025/meta-ends-fact-checking-community-notes-facebook/}, journal={Meta}, author={Catalanello, Rebecca and Sanders, Katie}, year={2025}, month={Jan}} 

@inproceedings{baly2020detectbias,
    title = "We Can Detect Your Bias: Predicting the Political Ideology of News Articles",
    author = "Baly, Ramy  and
      Da San Martino, Giovanni  and
      Glass, James  and
      Nakov, Preslav",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.404",
    doi = "10.18653/v1/2020.emnlp-main.404",
    pages = "4982--4991",
    abstract = "We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology {--}left, center, or right{--}, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.",
}

@article{lipton2018mythos,
  title={The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}


@misc{liu2023humancenteredNLP,
      title={Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI}, 
      author={Houjiang Liu and Anubrata Das and Alexander Boltz and Didi Zhou and Daisy Pinaroc and Matthew Lease and Min Kyung Lee},
      year={2023},
      eprint={2308.07213},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
doi = {https://doi.org/10.48550/arXiv.2308.07213}
}

@inproceedings{bertrand2022cognitive,
  title={How cognitive biases affect XAI-assisted decision-making: A systematic review},
  author={Bertrand, Astrid and Belloum, Rafik and Eagan, James R and Maxwell, Winston},
  booktitle={Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={78--91},
  year={2022}
}
@article{johs2022explainable,
  title={Explainable artificial intelligence and social science: Further insights for qualitative investigation},
  author={Johs, Adam J and Agosto, Denise E and Weber, Rosina O},
  journal={Applied AI Letters},
  volume={3},
  number={1},
  pages={e64},
  year={2022},
  publisher={Wiley Online Library}
}
@article{akata2020research,
  title={A research agenda for hybrid intelligence: augmenting human intellect with collaborative, adaptive, responsible, and explainable artificial intelligence},
  author={Akata, Zeynep and Balliet, Dan and De Rijke, Maarten and Dignum, Frank and Dignum, Virginia and Eiben, Guszti and Fokkens, Antske and Grossi, Davide and Hindriks, Koen and Hoos, Holger and others},
  journal={Computer},
  volume={53},
  number={8},
  pages={18--28},
  year={2020},
  publisher={IEEE}
}
@article{gongane2024survey,
  title={A survey of explainable AI techniques for detection of fake news and hate speech on social media platforms},
  author={Gongane, Vaishali U and Munot, Mousami V and Anuse, Alwin D},
  journal={Journal of Computational Social Science},
  pages={1--37},
  year={2024},
  publisher={Springer}
}
@article{beers2020examining,
  title={Examining the digital toolsets of journalists reporting on disinformation},
  author={Beers, Andrew and Haughey, Melinda McClure and Arif, Ahmer and Starbird, Kate},
  journal={Proceedings of computation+ journalism 2020 (C+ J’20)},
  year={2020},
url = {https://bpb-us-e1.wpmucdn.com/sites.northeastern.edu/dist/d/53/files/2020/02/CJ_2020_paper_50.pdf}
}

@article{diakopoulos2020computational,
  title={Computational news discovery: Towards design considerations for editorial orientation algorithms in journalism},
  author={Diakopoulos, Nicholas},
  journal={Digital journalism},
  volume={8},
  number={7},
  pages={945--967},
  year={2020},
  publisher={Taylor \& Francis},
doi = {10.1080/21670811.2020.1736946}
}
@inproceedings{adair2019human,
  title={The human touch in automated fact-checking},
  author={Adair, Bill and Stencel, Mark and Clabby, Cathy and Li, Chengkai},
  booktitle={Proceedings of the computation+ journalism symposium},
  year={2019}
}
@article{park2021presence,
  title={The presence of unexpected biases in online fact-checking},
  author={Park, Sungkyu and Park, Jaimie Yejean and Kang, Jeong-han and Cha, Meeyoung},
  journal={The Harvard Kennedy School Misinformation Review},
  year={2021},
  publisher={Shorenstein Center for Media, Politics and Public Policy, at Harvard~…}
}
@inproceedings{10.1145/3419249.3420105,
author = {Komatsu, Tomoko and Gutierrez Lopez, Marisela and Makri, Stephann and Porlezza, Colin and Cooper, Glenda and MacFarlane, Andrew and Missaoui, Sondess},
title = {AI should embody our values: Investigating journalistic values to inform AI technology design},
year = {2020},
isbn = {9781450375795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419249.3420105},
doi = {10.1145/3419249.3420105},
abstract = {In the current climate of shrinking newsrooms and revenues, journalists face increasing pressures exerted by the industry’s for-profit focus and the expectation of intensified output. While AI-enabled journalism has great potential to help alleviate journalists’ pressures, it might also disrupt journalistic norms and, at worst, interfere with their duty to inform the public. For AI systems to be as useful as possible, designers should understand journalists’ professional values and incorporate them into their designs. We report findings from interviews with journalists to understand their perceptions of how professional values that are important to them (such as truth, impartiality and originality) might be supported and/or undermined by AI technologies. Based on these findings, we provide design insight and guidelines for incorporating values into the design of AI systems. We argue HCI design can achieve the strongest possible value alignment by moving beyond merely supporting important values, to truly embodying them.},
booktitle = {Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society},
articleno = {11},
numpages = {13},
keywords = {algorithmic systems, artificial intelligence, automated journalism, value sensitive design, values},
location = {Tallinn, Estonia},
series = {NordiCHI '20}
}
@article{young2018factcheckingeffectiveness,
  title={Fact-checking effectiveness as a function of format and tone: Evaluating FactCheck. org and FlackCheck. org},
  author={Young, Dannagal G. and Jamieson, Kathleen Hall and Poulsen, Shannon and Goldring, Abigail},
  journal={Journalism \& Mass Communication Quarterly},
  volume={95},
  number={1},
  pages={49--75},
  year={2018},
  publisher={SAGE Publications Sage CA: Los Angeles, CA},
doi = {10.1177/1077699017710453}
}
@article{uscinski2015epistemology,
  title={The epistemology of fact checking (is still na{\`\i}ve): Rejoinder to Amazeen},
  author={Uscinski, Joseph E},
  journal={Critical Review},
  volume={27},
  number={2},
  pages={243--252},
  year={2015},
  publisher={Taylor \& Francis},
doi = {10.1080/08913811.2015.1055892}
}

@article{amazeen2020journalistic,
  title={Journalistic interventions: The structural factors affecting the global emergence of fact-checking},
  author={Amazeen, Michelle A},
  journal={Journalism},
  volume={21},
  number={1},
  pages={95--111},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England},
doi = {10.1177/1464884917730217}
}
@inproceedings{kaffee-etal-2023-thorny,
    title = "Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing",
    author = "Kaffee, Lucie-Aim{\'e}e  and
      Arora, Arnav  and
      Talat, Zeerak  and
      Augenstein, Isabelle",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.932",
    doi = "10.18653/v1/2023.findings-emnlp.932",
    pages = "13977--13998",
    abstract = "Dual use, the intentional, harmful reuse of technology and scientific artefacts, is an ill-defined problem within the context of Natural Language Processing (NLP). As large language models (LLMs) have advanced in their capabilities and become more accessible, the risk of their intentional misuse becomes more prevalent. To prevent such intentional malicious use, it is necessary for NLP researchers and practitioners to understand and mitigate the risks of their research. Hence, we present an NLP-specific definition of dual use informed by researchers and practitioners in the field. Further, we propose a checklist focusing on dual-use in NLP, that can be integrated into existing conference ethics-frameworks. The definition and checklist are created based on a survey of NLP researchers and practitioners.",
}
@misc{pawar2024surveyculturalawarenesslanguage,
      title={Survey of Cultural Awareness in Language Models: Text and Beyond}, 
      author={Siddhesh Pawar and Junyeong Park and Jiho Jin and Arnav Arora and Junho Myung and Srishti Yadav and Faiz Ghifari Haznitrama and Inhwa Song and Alice Oh and Isabelle Augenstein},
      year={2024},
      eprint={2411.00860},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.00860}, 
}
@inproceedings{cao-etal-2022-theory,
    title = "Theory-Grounded Measurement of {U}.{S}. Social Stereotypes in {E}nglish Language Models",
    author = "Cao, Yang Trista  and
      Sotnikova, Anna  and
      Daum{\'e} III, Hal  and
      Rudinger, Rachel  and
      Zou, Linda",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.92",
    doi = "10.18653/v1/2022.naacl-main.92",
    pages = "1276--1295",
    abstract = "NLP models trained on text have been shown to reproduce human stereotypes, which can magnify harms to marginalized groups when systems are deployed at scale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et al. (2016) from social psychology as a framework for the systematic study and discovery of stereotypic group-trait associations in language models (LMs). We introduce the sensitivity test (SeT) for measuring stereotypical associations from language models. To evaluate SeT and other measures using the ABC model, we collect group-trait judgments from U.S.-based subjects to compare with English LM stereotypes. Finally, we extend this framework to measure LM stereotyping of intersectional identities.",
}
@inproceedings{cao-etal-2023-assessing,
    title = "Assessing Cross-Cultural Alignment between {C}hat{GPT} and Human Societies: An Empirical Study",
    author = "Cao, Yong  and
      Zhou, Li  and
      Lee, Seolhwa  and
      Cabello, Laura  and
      Chen, Min  and
      Hershcovich, Daniel",
    editor = "Dev, Sunipa  and
      Prabhakaran, Vinodkumar  and
      Adelani, David  and
      Hovy, Dirk  and
      Benotti, Luciana",
    booktitle = "Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.c3nlp-1.7",
    doi = "10.18653/v1/2023.c3nlp-1.7",
    pages = "53--67",
    abstract = "The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like conversations. Given its usage by users from various nations and its training on a vast multilingual corpus that includes diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.",
}
@article{edwards2021eu,
  title={The EU AI Act: a summary of its significance and scope},
  author={Edwards, Lilian},
  journal={Artificial Intelligence (the EU AI Act)},
  volume={1},
  year={2021},
URL = {https://www.adalovelaceinstitute.org/wp-content/uploads/2022/04/Expert-explainer-The-EU-AI-Act-11-April-2022.pdf}
}

@article{lupo2022ethics,
  title={The ethics of Artificial Intelligence: An analysis of ethical frameworks disciplining AI in justice and other contexts of application},
  author={Lupo, Giampiero and others},
  journal={O{\~n}ati Socio-Legal Series},
  volume={12},
  number={3},
  pages={614--653},
  year={2022},
doi = {10.35295/osls.iisl/0000-0000-0000-1273}
}
@article{mclennan2022embedded,
  title={Embedded ethics: a proposal for integrating ethics into the development of medical AI},
  author={McLennan, Stuart and Fiske, Amelia and Tigard, Daniel and M{\"u}ller, Ruth and Haddadin, Sami and Buyx, Alena},
  journal={BMC Medical Ethics},
  volume={23},
  number={1},
  pages={6},
  year={2022},
  publisher={Springer},
doi = {10.1186/s12910-022-00746-3}
}
@article{graves2019fact,
  title={Fact-checking as idea and practice in journalism},
  author={Graves, Lucas and Amazeen, Michelle},
  year={2019},
  publisher={Oxford University Press},
journal = {Oxford Research Encyclopedia of Communication},
doi = {10.1093/acrefore/9780190228613.013.808}
}
@article{graves2018understanding,
  title={Understanding the promise and limits of automated fact-checking},
  author={Graves, Lucas},
  journal={Reuters Institute for the Study of Journalism},
  year={2018},
  publisher={Reuters Institute for the Study of Journalism},
URL = {https://www.digitalnewsreport.org/publications/2018/factsheet-understanding-promise-limits-automated-fact-checking/}
}

@article{graves2017anatomy,
  title={Anatomy of a fact check: Objective practice and the contested epistemology of fact checking},
  author={Graves, Lucas},
  journal={Communication, culture \& critique},
  volume={10},
  number={3},
  pages={518--537},
  year={2017},
  publisher={Oxford University Press},
doi = {https://doi.org/10.1111/cccr.12163}
}

@misc{procter2023observations,
      title={Some Observations on Fact-Checking Work with Implications for Computational Support}, 
      author={Rob Procter and Miguel Arana-Catania and Yulan He and Maria Liakata and Arkaitz Zubiaga and Elena Kochkina and Runcong Zhao},
      year={2023},
      eprint={2305.02224},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}






@article{Johnson2019SimplicityAccount,
    title = {Simplicity and complexity preferences in causal explanation: An opponent heuristic account},
    year = {2019},
    journal = {Cognitive Psychology},
    author = {Johnson, Samuel G.B. and Valenti, J. J. and Keil, Frank C.},
    number = {May},
    pages = {101222},
    volume = {113},
    publisher = {Elsevier},
    url = {https://doi.org/10.1016/j.cogpsych.2019.05.004},
    doi = {10.1016/j.cogpsych.2019.05.004},
    issn = {00100285},
    pmid = {31200208},
    keywords = {Causal reasoning, Explanation, Inference, Simplicity, Understanding}
}

@article{Lim2020ExplanatoryMatching,
    title = {Explanatory preferences for complexity matching},
    year = {2020},
    journal = {PLoS ONE},
    author = {Lim, Jonathan B. and Oppenheimer, Daniel M.},
    number = {4},
    month = {4},
    volume = {15},
    publisher = {Public Library of Science},
    doi = {10.1371/journal.pone.0230929},
    issn = {19326203},
    pmid = {32315308}
}


@article{Zemla2017EvaluatingExplanations,
    title = {Evaluating everyday explanations},
    year = {2017},
    journal = {Psychonomic Bulletin and Review},
    author = {Zemla, Jeffrey C. and Sloman, Steven A. and Bechlivanidis, Christos and Lagnado, David A.},
    number = {5},
    pages = {1488--1500},
    volume = {24},
    publisher = {Psychonomic Bulletin {\&} Review},
    doi = {10.3758/s13423-017-1258-z},
    issn = {15315320},
    pmid = {28275989},
    keywords = {Causal reasoning, Explanation, Knowledge}
}
@article{sulik2023explanations,
  title={Explanations in the wild},
  author={Sulik, Justin and van Paridon, Jeroen and Lupyan, Gary},
  journal={Cognition},
  volume={237},
  pages={105464},
  year={2023},
  publisher={Elsevier}
}
@inproceedings{Mittelstadt2019ExplainingAI,
    title = {Explaining explanations in AI},
    year = {2019},
    booktitle = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
    author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
    pages = {279--288},
    isbn = {9781450361255},
    doi = {10.1145/3287560.3287574},
    arxivId = {1811.01439},
    keywords = {Accountability, Explanations, Interpretability, Philosophy of Science}
}
@inproceedings{Schoenborn2019RecentInteractions,
    title = {Recent Trends in XAI: A Broad Overview on current Approaches , Methodologies and Interactions},
    year = {2019},
    booktitle = {Case-Based Reasoning for the Explanation of intelligent systems (XCBR) Workshop},
    author = {Schoenborn, Jakob M and Althoff, Klaus-dieter},
    url = {http://gaia.fdi.ucm.es/events/xcbr/papers/XCBR-19_paper_1.pdf},
    keywords = {case-based explanation, explanation, framework, xai}
}
@article{Sokol2020OneAll,
    title = {One Explanation Does Not Fit All},
    year = {2020},
    journal = {KI - K{\{u}nstliche Intelligenz}},
    author = {Sokol, Kacper and Flach, Peter},
    number = {2},
    pages = {235--250},
    volume = {34},
    publisher = {Springer Berlin Heidelberg},
    url = {https://doi.org/10.1007/s13218-020-00637-y},
    isbn = {0123456789},
    doi = {10.1007/s13218-020-00637-y},
    issn = {0933-1875},
    keywords = {Interactive,Personalised,Explanations,Counterfactu, counterfactuals, explanations, interactive, personalised}
}
@inproceedings{Moore1990Pointing:Dialogue,
    title = {Pointing: A Way Toward Explanation Dialogue},
    year = {1990},
    booktitle = {8th National Conference on Artificial Intelligence},
    author = {Moore, Johanna D and Swartout, William R},
    pages = {457--464},
    url = {http://www.aaai.org/Library/AAAI/1990/aaai90-069.php}
}
@inproceedings{Lage2019HumanInterpretability,
    title = {Human Evaluation of Models Built for Interpretability},
    year = {2019},
    booktitle = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
    author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Samuel J and Doshi-Velez, Finale},
    pages = {59--67},
    keywords = {Technical Papers}
}
@inproceedings{Forster2020EvaluatingAppreciate,
    title = {Evaluating Explainable Artificial Intelligence -- What Users Really Appreciate},
    year = {2020},
    booktitle = {Proceedings of the 28th European Conference on Information Systems (ECIS)},
    author = {F{\{o}}rster, Maximilian and Klier, Mathias and Kluge, Kilian and Sigler, Irina},
    url = {https://aisel.aisnet.org/ecis2020_rp/195},
    keywords = {Contrastive explanations, Explainable artificial intelligence, User Study}
}
@inproceedings{Dodge2019ExplainingJudgment,
author = {Dodge, Jonathan and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Dugan, Casey},
title = {Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment},
year = {2019},
isbn = {9781450362726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301275.3302310},
doi = {10.1145/3301275.3302310},
abstract = {Ensuring fairness of machine learning systems is a human-in-the-loop process. It relies on developers, users, and the general public to identify fairness problems and make improvements. To facilitate the process we need effective, unbiased, and user-friendly explanations that people can confidently rely on. Towards that end, we conducted an empirical study with four types of programmatically generated explanations to understand how they impact people's fairness judgments of ML systems. With an experiment involving more than 160 Mechanical Turk workers, we show that: 1) Certain explanations are considered inherently less fair, while others can enhance people's confidence in the fairness of the algorithm; 2) Different fairness problems-such as model-wide fairness issues versus case-specific fairness discrepancies-may be more effectively exposed through different styles of explanation; 3) Individual differences, including prior positions and judgment criteria of algorithmic fairness, impact how people react to different styles of explanation. We conclude with a discussion on providing personalized and adaptive explanations to support fairness judgments of ML systems.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
pages = {275--285},
numpages = {11},
keywords = {empirical studies, machine learning, fairness, explanation},
location = {Marina del Ray, California},
series = {IUI '19}
}
@inproceedings{Bucinca2020ProxySystems,
author = {Bu\c{c}inca, Zana and Lin, Phoebe and Gajos, Krzysztof Z. and Glassman, Elena L.},
title = {Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377498},
doi = {10.1145/3377325.3377498},
abstract = {Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {454--464},
numpages = {11},
keywords = {trust, artificial intelligence, explanations},
location = {Cagliari, Italy},
series = {IUI '20}
}
@article{Klein2021ModelingOthers,
    title = {Modeling the Process by Which People Try to Explain Complex Things to Others},
    year = {2021},
    journal = {Journal of Cognitive Engineering and Decision Making},
    author = {Klein, Gary and Hoffman, Robert and Mueller, Shane and Newsome, Emily},
    pages = {155534342110451},
    doi = {10.1177/15553434211045154},
    issn = {1555-3434}
}
@inproceedings{yang2021scalable,
  title={Scalable fact-checking with human-in-the-loop},
  author={Yang, Jing and Vega-Oliveros, Didier and Seibt, Tais and Rocha, Anderson},
  booktitle={2021 IEEE International Workshop on Information Forensics and Security (WIFS)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}
@article{nimmo2024user,
  title={User Characteristics in Explainable AI: The Rabbit Hole of Personalization?},
  author={Nimmo, Robert and Constantinides, Marios and Zhou, Ke and Quercia, Daniele and Stumpf, Simone},
  journal={arXiv preprint arXiv:2403.00137},
  year={2024}
}

@inproceedings{szymanski2021visual,
  title={Visual, textual or hybrid: the effect of user expertise on different explanations},
  author={Szymanski, Maxwell and Millecamp, Martijn and Verbert, Katrien},
  booktitle={26th international conference on intelligent user interfaces},
  pages={109--119},
  year={2021}
}
@inproceedings{park2021experimental,
  title={An experimental study to understand user experience and perception bias occurred by fact-checking messages},
  author={Park, Sungkyu and Park, Jamie Yejean and Chin, Hyojin and Kang, Jeong-han and Cha, Meeyoung},
  booktitle={Proceedings of the Web Conference 2021},
  pages={2769--2780},
  year={2021}
}
@article{linder2021level,
  title={How level of explanation detail affects human performance in interpretable intelligent systems: A study on explainable fact checking},
  author={Linder, Rhema and Mohseni, Sina and Yang, Fan and Pentyala, Shiva K and Ragan, Eric D and Hu, Xia Ben},
  journal={Applied AI Letters},
  volume={2},
  number={4},
  pages={e49},
  year={2021},
  publisher={Wiley Online Library},
doi= {10.1002/ail2.49}
}

@article{liu2023checking,
  title={Checking the Fact-Checkers: The Role of Source Type, Perceived Credibility, and Individual Differences in Fact-Checking Effectiveness},
  author={Liu, Xingyu and Qi, Li and Wang, Laurent and Metzger, Miriam J},
  journal={Communication Research},
  pages={00936502231206419},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
@article{walter2020fact,
  title={Fact-checking: A meta-analysis of what works and for whom},
  author={Walter, Nathan and Cohen, Jonathan and Holbert, R Lance and Morag, Yasmin},
  journal={Political Communication},
  volume={37},
  number={3},
  pages={350--375},
  year={2020},
  publisher={Taylor \& Francis}
}
@book{smith2004factcheckersbible,
  title={The Fact Checker's Bible: A Guide to Getting It Right},
  author={Smith, Sarah Harrison},
  year={2004},
  publisher={Anchor}
}
@inproceedings{hettiachchi2023designing,
  title={Designing and Evaluating Presentation Strategies for Fact-Checked Content},
  author={Hettiachchi, Danula and Ji, Kaixin and Kennedy, Jenny and McCosker, Anthony and Salim, Flora D and Sanderson, Mark and Scholer, Falk and Spina, Damiano},
  booktitle={Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
  pages={751--761},
  year={2023}
}
@article{bengtsson2024rhetoricalfactcheckers,
  title={And That’s a Fact: A Rhetorical Perspective on the Role of Fact-Checkers},
  author={Bengtsson, Mette and Schousboe, Sabina},
  journal={Journalism Practice},
  pages={1--19},
  year={2024},
  publisher={Taylor \& Francis},
doi = {10.1080/17512786.2024.2340531}
}
@article{schott2024derecodingfactcheckers,
author = {Anna Schjøtt and Mette Bengtsson},
title ={De- and recoding algorithmic systems: The case of fact checkers and fact checked users},
journal = {Convergence},
volume = {30},
number = {6},
pages = {1919-1938},
year = {2024},
doi = {10.1177/13548565241289233},
URL = {https://doi.org/10.1177/13548565241289233
},
eprint = {https://doi.org/10.1177/13548565241289233
}
,
    abstract = { With the recent development of debunking on social media as a dominant agenda, fact checkers have increasingly used machine learning (ML) to identify, verify and correct factual claims, as ML promises the scaling of fact checking practices. However, it also places a new actor in between the fact checkers and the fact checked users. In this paper, we conducted a contrasted analysis of how fact checkers and fact checked users understand, evaluate and act towards the algorithmic system and the data flows in Meta’s Third-Party Fact-Checking Program: We did ethnographic fieldwork in the fact checking newsroom and interviewed and did protocol analysis with the fact checked. For both professional users and end users, the algorithmic system is experienced as a black box in which they have limited insight, and their sense-making practices happen based on the data and metrics that are made visible to them. In the paper, we draw on and expanded theory on decoding algorithms by not only exploring how the two user groups engage in decoding the algorithmic system, but also actively engage in forms of recoding by attempting to adapt or modify the algorithmic system to better fit within their cultural and social context, which is characterised by both varying epistemic cultures and societal positions. While the fact checkers from their hegemonic (sometimes negotiated) position understand the program as a (sometimes stupid) tool and primarily engage in passive acts of recoding, fact checked users, from their oppositional position, understand the program as an unpredictable censoring machine and engage primarily in more active acts of recoding. Based on the analysis, we end the paper with a discussion in which we argued for understanding data reflexivity as highly relational and processual. }
}


@article{steyvers2024calibrationgapmodelhuman,
  title={What large language models know and what people think they know},
  author={Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas W and Smyth, Padhraic},
  journal={Nature Machine Intelligence},
  pages={1--11},
  year={2025},
  publisher={Nature Publishing Group UK London},
doi = {10.1038/s42256-024-00976-7}
}

@inproceedings{shi2022effects,
  title={The effects of interactive ai design on user behavior: An eye-Tracking study of fact-checking COVID-19 claims},
  author={Shi, Li and Bhattacharya, Nilavra and Das, Anubrata and Lease, Matt and Gwizdka, Jacek},
  booktitle={Proceedings of the 2022 Conference on Human Information Interaction and Retrieval},
  pages={315--320},
  year={2022}
}
@article{dierickx2023automated,
  title={Automated fact-checking to support professional practices: systematic literature review and meta-analysis},
  author={Dierickx, Laurence and Lind{\'e}n, Carl-Gustav and Opdahl, Andreas Lothe},
  journal={International Journal of Communication},
  volume={17},
  pages={21},
  year={2023},
URL = {https://ijoc.org/index.php/ijoc/article/view/21071/4287}
}
@article{soprano2024cognitive,
  title={Cognitive Biases in Fact-Checking and Their Countermeasures: A Review},
  author={Soprano, Michael and Roitero, Kevin and La Barbera, David and Ceolin, Davide and Spina, Damiano and Demartini, Gianluca and Mizzaro, Stefano},
  journal={Information Processing \& Management},
  volume={61},
  number={3},
  pages={103672},
  year={2024},
  publisher={Elsevier}
}

@article{Qu2024confidencetruthfulness,
author = {Qu, Yunke and Roitero, Kevin and Barbera, David La and Spina, Damiano and Mizzaro, Stefano and Demartini, Gianluca},
title = {Combining Human and Machine Confidence in Truthfulness Assessment},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3546916},
doi = {10.1145/3546916},
abstract = {Automatically detecting online misinformation at scale is a challenging and interdisciplinary problem. Deciding what is to be considered truthful information is sometimes controversial and also difficult for educated experts. As the scale of the problem increases, human-in-the-loop approaches to truthfulness that combine both the scalability of machine learning (ML) and the accuracy of human contributions have been considered.In this work, we look at the potential to automatically combine machine-based systems with human-based systems. The former exploit superviseds ML approaches; the latter involve either crowd workers (i.e., human non-experts) or human experts. Since both ML and crowdsourcing approaches can produce a score indicating the level of confidence on their truthfulness judgments (either algorithmic or self-reported, respectively), we address the question of whether it is feasible to make use of such confidence scores to effectively and efficiently combine three approaches: (i) machine-based methods, (ii) crowd workers, and (iii) human experts. The three approaches differ significantly, as they range from available, cheap, fast, scalable, but less accurate to scarce, expensive, slow, not scalable, but highly accurate.},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {5},
numpages = {17},
keywords = {Misinformation, crowdsourcing, hybrid intelligence}
}

@inproceedings{mcclure2022bridging,
  title={Bridging Contextual and Methodological Gaps on the “Misinformation Beat”: Insights from Journalist-Researcher Collaborations at Speed},
  author={McClure Haughey, Melinda and Povolo, Martina and Starbird, Kate},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2022}
}

@inproceedings{wolfe2024genAIfactchecking,
author = {Wolfe, Robert and Mitra, Tanushree},
title = {The Impact and Opportunities of Generative AI in Fact-Checking},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658987},
doi = {10.1145/3630106.3658987},
abstract = {Generative AI appears poised to transform white collar professions, with more than 90\% of Fortune 500 companies using OpenAI’s flagship GPT models, which have been characterized as “general purpose technologies” capable of effecting epochal changes in the economy. But how will such technologies impact organizations whose job is to verify and report factual information, and to ensure the health of the information ecosystem? To investigate this question, we conducted 30 interviews with N=38 participants working at 29 fact-checking organizations across six continents, asking about how they use generative AI and the opportunities and challenges they see in the technology. We found that uses of generative AI envisioned by fact-checkers differ based on organizational infrastructure, with applications for quality assurance in Editing, for trend analysis in Investigation, and for information literacy in Advocacy. We used the TOE framework to describe participant concerns ranging from the Technological (lack of transparency), to the Organizational (resource constraints), to the Environmental (uncertain and evolving policy). Building on the insights of our participants, we describe value tensions between fact-checking and generative AI, and propose a novel Verification dimension to the design space of generative models for information verification work. Finally, we outline an agenda for fairness, accountability, and transparency research to support the responsible use of generative AI in fact-checking. Throughout, we highlight the importance of human infrastructure and labor in producing verified information in collaboration with AI. We expect that this work will inform not only the scientific literature on fact-checking, but also contribute to understanding of organizational adaptation to a powerful but unreliable new technology.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1531–1543},
numpages = {13},
keywords = {Design, Fact-Checking, Generative AI, Sociotechnical Infrastructure, Transparency},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@article{bhuiyan2020investigating,
  title={Investigating differences in crowdsourced news credibility assessment: Raters, tasks, and expert criteria},
  author={Bhuiyan, Md Momen and Zhang, Amy X and Sehat, Connie Moon and Mitra, Tanushree},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={4},
  number={CSCW2},
  pages={1--26},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@article{kaufman2022who,
  title={Who's in the Crowd Matters: Cognitive Factors and Beliefs Predict Misinformation Assessment Accuracy},
  author={Kaufman, Robert A and Haupt, Michael Robert and Dow, Steven P},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW2},
  pages={1--18},
  year={2022},
  publisher={ACM New York, NY, USA}
}
@inproceedings{nguyen2018interpretable,
  title={An interpretable joint graphical model for fact-checking from crowds},
  author={Nguyen, An and Kharosekar, Aditya and Lease, Matthew and Wallace, Byron},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}
@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature machine intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{gilpin2018explaining,
  title={Explaining explanations: An overview of interpretability of machine learning},
  author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  booktitle={2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
  pages={80--89},
  year={2018},
  organization={IEEE}
}
@inproceedings{biran2017explanation,
  title={Explanation and justification in machine learning: A survey},
  author={Biran, Or and Cotton, Courtenay},
  booktitle={IJCAI-17 workshop on explainable AI (XAI)},
  volume={8},
  number={1},
  pages={8--13},
  year={2017}
}
@article{banerjee2020psychology,
  title={Psychology of misinformation and the media: Insights from the COVID-19 pandemic},
  author={Banerjee, Debanjan and Rao, TS Sathyanarayana},
  journal={Indian Journal of Social Psychiatry},
  volume={36},
  number={Suppl 1},
  pages={S131--S137},
  year={2020},
  publisher={Medknow}
}
@article{ecker2017rebuttals,
  title={Why rebuttals may not work: the psychology of misinformation},
  author={Ecker, Ullrich KH},
  journal={Media Asia},
  volume={44},
  number={2},
  pages={79--87},
  year={2017},
  publisher={Taylor \& Francis}
}
@article{cook2015misinformation,
  title={Misinformation and how to correct it},
  author={Cook, John and Ecker, Ullrich and Lewandowsky, Stephan},
  journal={Emerging trends in the social and behavioral sciences: An interdisciplinary, searchable, and linkable resource},
  pages={1--17},
  year={2015},
  publisher={John Wiley \& Sons, Inc. Hoboken, NJ, USA}
}
@inproceedings{mohseni2021machine,
  title={Machine learning explanations to prevent overtrust in fake news detection},
  author={Mohseni, Sina and Yang, Fan and Pentyala, Shiva and Du, Mengnan and Liu, Yi and Lupfer, Nic and Hu, Xia and Ji, Shuiwang and Ragan, Eric},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={15},
  pages={421--431},
  year={2021}
}

@misc{nakov2021automated,
      title={Automated Fact-Checking for Assisting Human Fact-Checkers}, 
      author={Preslav Nakov and David Corney and Maram Hasanain and Firoj Alam and Tamer Elsayed and Alberto Barrón-Cedeño and Paolo Papotti and Shaden Shaar and Giovanni Da San Martino},
      year={2021},
      eprint={2103.07769},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2103.07769}, 
}

@inproceedings{nakov2021clef,
  title={The CLEF-2021 CheckThat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news},
  author={Nakov, Preslav and Da San Martino, Giovanni and Elsayed, Tamer and Barr{\'o}n-Cedeno, Alberto and M{\'\i}guez, Rub{\'e}n and Shaar, Shaden and Alam, Firoj and Haouari, Fatima and Hasanain, Maram and Babulkov, Nikolay and others},
  booktitle={Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28--April 1, 2021, Proceedings, Part II 43},
  pages={639--649},
  year={2021},
  organization={Springer},
doi = {10.1007/978-3-030-72240-1_75}
}

@inproceedings{shaar2020knownlie,
    title = "That is a Known Lie: Detecting Previously Fact-Checked Claims",
    author = "Shaar, Shaden  and
      Babulkov, Nikolay  and
      Da San Martino, Giovanni  and
      Nakov, Preslav",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.332",
    doi = "10.18653/v1/2020.acl-main.332",
    pages = "3607--3618",
    abstract = "The recent proliferation of {''}fake news{''} has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.",
}

@misc{dufour2024ammebalargescalesurveydataset,
      title={AMMeBa: A Large-Scale Survey and Dataset of Media-Based Misinformation In-The-Wild}, 
      author={Nicholas Dufour and Arkanath Pathak and Pouya Samangouei and Nikki Hariri and Shashi Deshetti and Andrew Dudfield and Christopher Guess and Pablo Hernández Escayola and Bobby Tran and Mevan Babakar and Christoph Bregler},
      year={2024},
      eprint={2405.11697},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2405.11697}, 
}

@inproceedings{fan2020factcheckingbriefs,
    title = "Generating Fact Checking Briefs",
    author = "Fan, Angela  and
      Piktus, Aleksandra  and
      Petroni, Fabio  and
      Wenzek, Guillaume  and
      Saeidi, Marzieh  and
      Vlachos, Andreas  and
      Bordes, Antoine  and
      Riedel, Sebastian",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.580",
    doi = "10.18653/v1/2020.emnlp-main.580",
    pages = "7147--7161",
    abstract = "Fact checking at scale is difficult{---}while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim detection. We investigate how to increase the accuracy and efficiency of fact checking by providing information about the claim before performing the check, in the form of natural language briefs. We investigate passage-based briefs, containing a relevant passage from Wikipedia, entity-centric ones consisting of Wikipedia pages of mentioned entities, and Question-Answering Briefs, with questions decomposing the claim, and their answers. To produce QABriefs, we develop QABriefer, a model that generates a set of questions conditioned on the claim, searches the web for evidence, and generates answers. To train its components, we introduce QABriefDataset We show that fact checking with briefs {---} in particular QABriefs {---} increases the accuracy of crowdworkers by 10{\%} while slightly decreasing the time taken. For volunteer (unpaid) fact checkers, QABriefs slightly increase accuracy and reduce the time required by around 20{\%}.",
}

@inproceedings{hassan2017TowardsClaimBuster,
author = {Hassan, Naeemul and Arslan, Fatma and Li, Chengkai and Tremayne, Mark},
title = {Toward Automated Fact-Checking: Detecting Check-worthy Factual Claims by ClaimBuster},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098131},
doi = {10.1145/3097983.3098131},
abstract = {This paper introduces how ClaimBuster, a fact-checking platform, uses natural language processing and supervised learning to detect important factual claims in political discourses. The claim spotting model is built using a human-labeled dataset of check-worthy factual claims from the U.S. general election debate transcripts. The paper explains the architecture and the components of the system and the evaluation of the model. It presents a case study of how ClaimBuster live covers the 2016 U.S. presidential election debates and monitors social media and Australian Hansard for factual claims. It also describes the current status and the long-term goals of ClaimBuster as we keep developing and expanding it.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1803–1812},
numpages = {10},
keywords = {text mining, text classification, natural language processing, fact-checking, computational journalism},
location = {Halifax, NS, Canada},
series = {KDD '17}
}
@article{hassan2017ClaimBuster,
author = {Hassan, Naeemul and Zhang, Gensheng and Arslan, Fatma and Caraballo, Josue and Jimenez, Damian and Gawsane, Siddhant and Hasan, Shohedul and Joseph, Minumol and Kulkarni, Aaditya and Nayak, Anil Kumar and Sable, Vikas and Li, Chengkai and Tremayne, Mark},
title = {ClaimBuster: the first-ever end-to-end fact-checking system},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137815},
doi = {10.14778/3137765.3137815},
abstract = {Our society is struggling with an unprecedented amount of falsehoods, hyperboles, and half-truths. Politicians and organizations repeatedly make the same false claims. Fake news floods the cyberspace and even allegedly influenced the 2016 election. In fighting false information, the number of active fact-checking organizations has grown from 44 in 2014 to 114 in early 2017. 1 Fact-checkers vet claims by investigating relevant data and documents and publish their verdicts. For instance, PolitiFact.com, one of the earliest and most popular fact-checking projects, gives factual claims truthfulness ratings such as True, Mostly True, Half true, Mostly False, False, and even "Pants on Fire". In the U.S., the election year made fact-checking a part of household terminology. For example, during the first presidential debate on September 26, 2016, NPR.org's live fact-checking website drew 7.4 million page views and delivered its biggest traffic day ever.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1945–1948},
numpages = {4}
}

@inproceedings{cheng2019explaining,
  title={Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders},
  author={Cheng, Hao-Fei and Wang, Ruotong and Zhang, Zheng and O'connell, Fiona and Gray, Terrance and Harper, F Maxwell and Zhu, Haiyi},
  booktitle={Proceedings of the 2019 chi conference on human factors in computing systems},
  pages={1--12},
  year={2019}
}
@article{borch2022toward,
  title={Toward a sociology of machine learning explainability: Human--machine interaction in deep neural network-based automated trading},
  author={Borch, Christian and Hee Min, Bo},
  journal={Big Data \& Society},
  volume={9},
  number={2},
  pages={20539517221111361},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{huang2022ameliorating,
  title={Ameliorating algorithmic bias, or why explainable AI needs feminist philosophy},
  author={Huang, Linus Ta-Lun and Chen, Hsiang-Yun and Lin, Ying-Tung and Huang, Tsung-Ren and Hung, Tzu-Wei},
  journal={Feminist Philosophy Quarterly},
  volume={8},
  number={3/4},
  year={2022}
}
@article{ghassemi2021false,
  title={The false hope of current approaches to explainable artificial intelligence in health care},
  author={Ghassemi, Marzyeh and Oakden-Rayner, Luke and Beam, Andrew L},
  journal={The Lancet Digital Health},
  volume={3},
  number={11},
  pages={e745--e750},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{yurrita2023disentangling,
  title={Disentangling fairness perceptions in algorithmic decision-making: The effects of explanations, human oversight, and contestability},
  author={Yurrita, Mireia and Draws, Tim and Balayn, Agathe and Murray-Rust, Dave and Tintarev, Nava and Bozzon, Alessandro},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2023}
}
@inproceedings{kim2024LLMuncertainty,
author = {Kim, Sunnie S. Y. and Liao, Q. Vera and Vorvoreanu, Mihaela and Ballard, Stephanie and Vaughan, Jennifer Wortman},
title = {"I'm Not Sure, But...": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658941},
doi = {10.1145/3630106.3658941},
abstract = {Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {822–835},
numpages = {14},
keywords = {Human-AI interaction, Large language models, Overreliance, Trust in AI, Uncertainty expression},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}
@article{chen2023intuitionxai,
author = {Chen, Valerie and Liao, Q. Vera and Wortman Vaughan, Jennifer and Bansal, Gagan},
title = {Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610219},
doi = {10.1145/3610219},
abstract = {AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong. While many factors may affect reliance on AI support, one important factor is how decision-makers reconcile their own intuition---beliefs or heuristics, based on prior knowledge, experience, or pattern recognition, used to make judgments---with the information provided by the AI system to determine when to override AI predictions. We conduct a think-aloud, mixed-methods study with two explanation types (feature- and example-based) for two prediction tasks to explore how decision-makers' intuition affects their use of AI predictions and explanations, and ultimately their choice of when to rely on AI. Our results identify three types of intuition involved in reasoning about AI predictions and explanations: intuition about the task outcome, features, and AI limitations. Building on these, we summarize three observed pathways for decision-makers to apply their own intuition and override AI predictions. We use these pathways to explain why (1) the feature-based explanations we used did not improve participants' decision outcomes and increased their overreliance on AI, and (2) the example-based explanations we used improved decision-makers' performance over feature-based explanations and helped achieve complementary human-AI performance. Overall, our work identifies directions for further development of AI decision-support systems and explanation methods that help decision-makers effectively apply their intuition to achieve appropriate reliance on AI.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {370},
numpages = {32},
keywords = {decision support, explainable AI, human-AI interaction, interpretability}
}
@article{gilpin2019explaining,
  title={Explaining explanations to society},
  author={Gilpin, Leilani H and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
  journal={arXiv preprint arXiv:1901.06560},
  year={2019}
}
@inproceedings{kasirzadeh2021philosophicalframework,
author = {Kasirzadeh, Atoosa},
title = {Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445866},
doi = {10.1145/3442188.3445866},
abstract = {The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {14},
numpages = {1},
keywords = {Philosophy of Explanation, Philosophy of AI, Machine learning, Interpretable Machine Learning, Explainable Machine Learning, Explainable Artificial Intelligence, Explainable AI, Ethics of AI, Ethical AI},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
@inproceedings{farinneya2021activelearningrumor,
    title = "Active Learning for Rumor Identification on Social Media",
    author = "Farinneya, Parsa  and
      Abdollah Pour, Mohammad Mahdi  and
      Hamidian, Sardar  and
      Diab, Mona",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.387",
    doi = "10.18653/v1/2021.findings-emnlp.387",
    pages = "4556--4565",
    abstract = "Social media has emerged as a key channel for seeking information. Online users spend several hours reading, posting, and searching for news on microblogging platforms daily. However, this could act as a double-edged sword especially when not all information online is reliable. Moreover, the inherently unmoderated nature of social media renders identifying unverified information ever more challenging. Most of the existing approaches for rumor tracking are not scalable because of their dependency on a significant amount of labeled data. In this work, we investigate this problem from different angles. We design an Active-Transfer Learning (ATL) strategy to identify rumors with a limited amount of annotated data. We go beyond that and investigate the impact of leveraging various machine learning approaches in addition to different contextual representations. We discuss the impact of multiple classifiers on a limited amount of annotated data followed by an interactive approach to gradually update the models by adding the least certain samples (LCS) from the pool of unlabeled data. Our proposed Active Learning (AL) strategy achieves faster convergence in terms of the F-score while requiring fewer annotated samples (42{\%} of the whole dataset for the best model).",
}
@misc{adair2020automatedjournalism,
    author = {Adair, Bill and Stencel, Mark},
    year = {2020},
    title = {Lesson in automated journalism: Bring back the humans},
publisher = {Nieman journalism lab},
URL = {https://www.niemanlab.org/2020/07/a-lesson-in-automated-journalism-bring-back-the-humans/}
}

@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA},
doi = {10.1145/3458723}
}
@misc{zhang2024needlanguagespecificfactcheckingmodels,
      title={Do We Need Language-Specific Fact-Checking Models? The Case of Chinese}, 
      author={Caiqi Zhang and Zhijiang Guo and Andreas Vlachos},
      year={2024},
      eprint={2401.15498},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.15498}, 
}
@inproceedings{lai2023chatgptlanguage,
    title = "{C}hat{GPT} Beyond {E}nglish: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
    author = "Lai, Viet  and
      Ngo, Nghia  and
      Pouran Ben Veyseh, Amir  and
      Man, Hieu  and
      Dernoncourt, Franck  and
      Bui, Trung  and
      Nguyen, Thien",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.878",
    doi = "10.18653/v1/2023.findings-emnlp.878",
    pages = "13171--13189",
    abstract = "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. In particular, we evaluate ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. Compared to the performance of previous models, our extensive experiments demonstrate the worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.",
}
@article{du2024shortcuts,
author = {Du, Mengnan and He, Fengxiang and Zou, Na and Tao, Dacheng and Hu, Xia},
title = {Shortcut Learning of Large Language Models in Natural Language Understanding},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3596490},
doi = {10.1145/3596490},
abstract = {Shortcuts often hinder the robustness of large language models.},
journal = {Commun. ACM},
month = {dec},
pages = {110–120},
numpages = {11}
}
@article{van2020interpretable,
  title={Interpretable confidence measures for decision support systems},
  author={van der Waa, Jasper and Schoonderwoerd, Tjeerd and van Diggelen, Jurriaan and Neerincx, Mark},
  journal={International Journal of Human-Computer Studies},
  volume={144},
  pages={102493},
  year={2020},
  publisher={Elsevier},
doi = {10.1016/j.ijhcs.2020.102493}
}

@inproceedings{le2023explaining,
  title={Explaining model confidence using counterfactuals},
  author={Le, Thao and Miller, Tim and Singh, Ronal and Sonenberg, Liz},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={10},
  pages={11856--11864},
  year={2023},
doi = {10.1609/aaai.v37i10.26399}
}
@inproceedings{lundberg2017SHAP,
 author = {Lundberg, Scott M. and Lee, Su-In},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Approach to Interpreting Model Predictions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{ribeiro2016LIME,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {interpretable machine learning, interpretability, explaining machine learning, black box classifier},
location = {San Francisco, California, USA},
series = {KDD '16}
}
@InProceedings{petsiuk2021saliencymaps,
    author    = {Petsiuk, Vitali and Jain, Rajiv and Manjunatha, Varun and Morariu, Vlad I. and Mehra, Ashutosh and Ordonez, Vicente and Saenko, Kate},
    title     = {Black-Box Explanation of Object Detectors via Saliency Maps},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {11443-11452},
doi={10.1109/CVPR46437.2021.01128}
}
@incollection{kim2022designing,
  title={Designing a pragmatic explanation for the XAI system based on the user's context and background knowledge},
  author={Kim, Sangyeon and Huh, Insil and Park, Yujin and Lee, Sangwon},
  booktitle={Human-centered artificial intelligence},
  pages={117--125},
  year={2022},
  publisher={Elsevier},
doi = {10.1016/B978-0-323-85648-5.00012-8}
}
@InProceedings{tanneru2024uncertaintyLLMs,
  title = 	 {Quantifying Uncertainty in Natural Language Explanations of Large Language Models},
  author =       {Harsha Tanneru, Sree and Agarwal, Chirag and Lakkaraju, Himabindu},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1072--1080},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/harsha-tanneru24a/harsha-tanneru24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/harsha-tanneru24a.html},
  abstract = 	 {Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLM’s behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics — Verbalized Uncertainty and Probing Uncertainty — to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.}
}


@inproceedings{adebayo2018sanity,
 author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sanity Checks for Saliency Maps},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf},
 volume = {31},
 year = {2018}
}
@article{nauta2023xaireview,
author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl\"{o}tterer, J\"{o}rg and van Keulen, Maurice and Seifert, Christin},
title = {From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583558},
doi = {10.1145/3583558},
abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {295},
numpages = {42},
keywords = {Explainable artificial intelligence, interpretable machine learning, evaluation, explainability, interpretability, quantitative evaluation methods, explainable AI, XAI}
}
@inproceedings{akhtar2023multimodal,
    title = "Multimodal Automated Fact-Checking: A Survey",
    author = "Akhtar, Mubashara  and
      Schlichtkrull, Michael  and
      Guo, Zhijiang  and
      Cocarascu, Oana  and
      Simperl, Elena  and
      Vlachos, Andreas",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.361",
    doi = "10.18653/v1/2023.findings-emnlp.361",
    pages = "5430--5448",
    abstract = "Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned image. Multimodal misinformation is perceived as more credible by humans, and spreads faster than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on text. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terms used in different communities and map them to our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research",
}

@inproceedings{ahuja2024megaverse,
    title = "{MEGAVERSE}: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks",
    author = "Ahuja, Sanchit  and
      Aggarwal, Divyanshu  and
      Gumma, Varun  and
      Watts, Ishaan  and
      Sathe, Ashutosh  and
      Ochieng, Millicent  and
      Hada, Rishav  and
      Jain, Prachi  and
      Ahmed, Mohamed  and
      Bali, Kalika  and
      Sitaram, Sunayana",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.143",
    doi = "10.18653/v1/2024.naacl-long.143",
    pages = "2598--2637",
    abstract = "There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.",
}
@inproceedings{bang2023multitask,
    title = "A Multitask, Multilingual, Multimodal Evaluation of {C}hat{GPT} on Reasoning, Hallucination, and Interactivity",
    author = "Bang, Yejin  and
      Cahyawijaya, Samuel  and
      Lee, Nayeon  and
      Dai, Wenliang  and
      Su, Dan  and
      Wilie, Bryan  and
      Lovenia, Holy  and
      Ji, Ziwei  and
      Yu, Tiezheng  and
      Chung, Willy  and
      Do, Quyet V.  and
      Xu, Yan  and
      Fung, Pascale",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = nov,
    year = "2023",
    address = "Nusa Dua, Bali",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.ijcnlp-main.45",
    doi = "10.18653/v1/2023.ijcnlp-main.45",
    pages = "675--718",
}

@article{pavlik2023collaborating,
  title={Collaborating with ChatGPT: Considering the implications of generative artificial intelligence for journalism and media education},
  author={Pavlik, John V.},
  journal={Journalism \& mass communication educator},
  volume={78},
  number={1},
  pages={84--93},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA},
doi={10.1177/10776958221149577}
}

@inproceedings{mitchell2019modelcards,
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
title = {Model Cards for Model Reporting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287596},
doi = {10.1145/3287560.3287596},
abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {model cards, fairness evaluation, ethical considerations, documentation, disaggregated evaluation, datasheets, ML model evaluation},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{aly2023qanatver,
    title = "{QA}-{N}at{V}er: Question Answering for Natural Logic-based Fact Verification",
    author = "Aly, Rami  and
      Strong, Marek  and
      Vlachos, Andreas",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.521",
    doi = "10.18653/v1/2023.emnlp-main.521",
    pages = "8376--8391",
    abstract = "Fact verification systems assess a claim{'}s veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by 4.3 accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",
}

@inproceedings{tschiatschek2018fakenewsdetection,
author = {Tschiatschek, Sebastian and Singla, Adish and Gomez Rodriguez, Manuel and Merchant, Arpit and Krause, Andreas},
title = {Fake News Detection in Social Networks via Crowd Signals},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3188722},
doi = {10.1145/3184558.3188722},
abstract = {Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {517–524},
numpages = {8},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{nakov2022clef,
  title={The clef-2022 checkthat! lab on fighting the covid-19 infodemic and fake news detection},
  author={Nakov, Preslav and Barr{\'o}n-Cede{\~n}o, Alberto and Da San Martino, Giovanni and Alam, Firoj and Stru{\ss}, Julia Maria and Mandl, Thomas and M{\'\i}guez, Rub{\'e}n and Caselli, Tommaso and Kutlu, Mucahid and Zaghouani, Wajdi and others},
  booktitle={European Conference on Information Retrieval},
  pages={416--428},
  year={2022},
  organization={Springer},
doi = {10.1007/978-3-030-99739-7_52}
}
@article{Sokol2021ExplainabilityIntelligence,
    title = {Explainability Is in the Mind of the Beholder: Establishing the Foundations of Explainable Artificial Intelligence},
    year = {2021},
    author = {Sokol, Kacper and Flach, Peter},
    month = {12},
    url = {http://arxiv.org/abs/2112.14466},
    arxivId = {2112.14466}
}

@inproceedings{sokol2020explassessment,
author = {Sokol, Kacper and Flach, Peter},
title = {Explainability fact sheets: a framework for systematic assessment of explainable approaches},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372870},
doi = {10.1145/3351095.3372870},
abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {56–67},
numpages = {12},
keywords = {AI, ML, desiderata, explainability, fact sheet, interpretability, taxonomy, transparency, work sheet},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{xing2022automatic,
  title={Automatic explanation generation for climate science claims},
  author={Xing, Rui and Bhatia, Shraey and Baldwin, Timothy and Lau, Jey Han},
  booktitle={Proceedings of the The 20th Annual Workshop of the Australasian Language Technology Association},
  pages={122--129},
  year={2022}
}
@inproceedings{jacovi-goldberg-2020-towards,
    title = {Towards Faithfully Interpretable {NLP} Systems: How Should We Define and Evaluate Faithfulness?},
    author = {Jacovi, Alon  and
      Goldberg, Yoav},
    editor = {Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.acl-main.386},
    doi = {10.18653/v1/2020.acl-main.386},
    pages = {4198--4205},
    abstract = {With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is {``}defined{''} by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.}
}

@article{cabitza2024never,
  title={Never tell me the odds: Investigating pro-hoc explanations in medical decision making},
  author={Cabitza, Federico and Natali, Chiara and Famiglini, Lorenzo and Campagner, Andrea and Caccavella, Valerio and Gallazzi, Enrico},
  journal={Artificial Intelligence in Medicine},
  pages={102819},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{zhang2024humanAImedical,
author = {Zhang, Shao and Yu, Jianing and Xu, Xuhai and Yin, Changchang and Lu, Yuxuan and Yao, Bingsheng and Tory, Melanie and Padilla, Lace M. and Caterino, Jeffrey and Zhang, Ping and Wang, Dakuo},
title = {Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642343},
doi = {10.1145/3613904.3642343},
abstract = {Today’s AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {445},
numpages = {18},
keywords = {Human-AI collaboration, Medical decision making, Sepsis diagnosis},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{sadler2004intuitive,
  title={The intuitive executive: Understanding and applying ‘gut feel’in decision-making},
  author={Sadler-Smith, Eugene and Shefy, Erella},
  journal={Academy of Management Perspectives},
  volume={18},
  number={4},
  pages={76--91},
  year={2004},
  publisher={Academy of Management Briarcliff Manor, NY 10510},
doi = {10.5465/ame.2004.15268692}
}

@article{le2024towards,
  title={Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence},
  author={Le, Thao and Miller, Tim and Singh, Ronal and Sonenberg, Liz},
  journal={arXiv preprint arXiv:2402.01292},
  year={2024}
}
@inproceedings{shu2019defend,
  title={defend: Explainable fake news detection},
  author={Shu, Kai and Cui, Limeng and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={395--405},
  year={2019}
}

@inproceedings{gad2019exfakt,
author = {Gad-Elrab, Mohamed H. and Stepanova, Daria and Urbani, Jacopo and Weikum, Gerhard},
title = {ExFaKT: A Framework for Explaining Facts over Knowledge Graphs and Text},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290996},
doi = {10.1145/3289600.3290996},
abstract = {Fact-checking is a crucial task for accurately populating, updating and curating knowledge graphs. Manually validating candidate facts is time-consuming. Prior work on automating this task focuses on estimating truthfulness using numerical scores which are not human-interpretable. Others extract explicit mentions of the candidate fact in the text as an evidence for the candidate fact, which can be hard to directly spot. In our work, we introduce ExFaKT, a framework focused on generating human-comprehensible explanations for candidate facts. ExFaKT uses background knowledge encoded in the form of Horn clauses to rewrite the fact in question into a set of other easier-to-spot facts. The final output of our framework is a set of semantic traces for the candidate fact from both text and knowledge graphs. The experiments demonstrate that our rewritings significantly increase the recall of fact-spotting while preserving high precision. Moreover, we show that the explanations effectively help humans to perform fact-checking and can also be exploited for automating this task.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {87–95},
numpages = {9},
keywords = {reasoning, knowledge graph, fact-checking, explainable evidence},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{zhang2021faxplainac,
  title={FaxPlainAC: A fact-checking tool based on explainable models with human correction in the loop},
  author={Zhang, Zijian and Rudra, Koustav and Anand, Avishek},
  booktitle={Proceedings of the 30th ACM international conference on information \& knowledge management},
  pages={4823--4827},
  year={2021}
}
@inproceedings{10.1145/2678025.2701399,
author = {Kulesza, Todd and Burnett, Margaret and Wong, Weng-Keen and Stumpf, Simone},
title = {Principles of Explanatory Debugging to Personalize Interactive Machine Learning},
year = {2015},
isbn = {9781450333061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678025.2701399},
doi = {10.1145/2678025.2701399},
abstract = {How can end users efficiently influence the predictions that machine learning systems make on their behalf? This paper presents Explanatory Debugging, an approach in which the system explains to users how it made each of its predictions, and the user then explains any necessary corrections back to the learning system. We present the principles underlying this approach and a prototype instantiating it. An empirical evaluation shows that Explanatory Debugging increased participants' understanding of the learning system by 52\% and allowed participants to correct its mistakes up to twice as efficiently as participants using a traditional learning system.},
booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces},
pages = {126–137},
numpages = {12},
keywords = {interactive machine learning, end user programming},
location = {Atlanta, Georgia, USA},
series = {IUI '15}
}




@inproceedings{eiband2018bringing,
  title={Bringing transparency design into practice},
  author={Eiband, Malin and Schneider, Hanna and Bilandzic, Mark and Fazekas-Con, Julian and Haug, Mareike and Hussmann, Heinrich},
  booktitle={23rd international conference on intelligent user interfaces},
  pages={211--223},
  year={2018}
}


@article{Glickenhaus2019DARPAReport,
    title = {DARPA XAI Phase 1 Evaluations Report},
    year = {2019},
    author = {Glickenhaus, Ben and Karneeb, Justin and Aha, David},
    number = {3},
    pages = {1--34}
}
@inproceedings{Ehsan2021ExplainabilityAI,
    title = {Explainability Pitfalls: Beyond Dark Patterns in Explainable AI},
    year = {2021},
    author = {Ehsan, Upol and Riedl, Mark O.},
    journal = {arXiv},
    url = {http://arxiv.org/abs/2109.12480},
    arxivId = {2109.12480}
}

@inproceedings{ehsan2024thewho,
author = {Ehsan, Upol and Passi, Samir and Liao, Q. Vera and Chan, Larry and Lee, I-Hsiang and Muller, Michael and Riedl, Mark O},
title = {The Who in XAI: How AI Background Shapes Perceptions of AI Explanations},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642474},
doi = {10.1145/3613904.3642474},
abstract = {Explainability of AI systems is critical for users to take informed actions. Understanding who opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups—people with and without AI background—perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design interventions to mitigate them.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {316},
numpages = {32},
keywords = {Explainable AI, Human-Centered Explainable AI, User Characteristics},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{vanderWaa2021EvaluatingExplanations,
    title = {Evaluating XAI: A comparison of rule-based and example-based explanations},
    year = {2021},
    journal = {Artificial Intelligence},
    author = {van der Waa, Jasper and Nieuwburg, Elisabeth and Cremers, Anita and Neerincx, Mark},
    pages = {103404},
    volume = {291},
    publisher = {Elsevier B.V.},
    url = {https://doi.org/10.1016/j.artint.2020.103404},
    doi = {10.1016/j.artint.2020.103404},
    issn = {00043702},
    keywords = {Artificial Intelligence (AI), Contrastive explanations, Decision support systems, Explainable Artificial Intelligence (XAI), Machine learning, User evaluations}
}
@inproceedings{Lim2009WhySystems,
author = {Lim, Brian Y. and Dey, Anind K. and Avrahami, Daniel},
title = {Why and Why Not Explanations Improve the Intelligibility of Context-Aware Intelligent Systems},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1519023},
doi = {10.1145/1518701.1519023},
abstract = {Context-aware intelligent systems employ implicit inputs, and make decisions based on complex rules and machine learning models that are rarely clear to users. Such lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance of these systems. However, automatically providing explanations about a system's decision process can help mitigate this problem. In this paper we present results from a controlled study with over 200 participants in which the effectiveness of different types of explanations was examined. Participants were shown examples of a system's operation along with various automatically generated explanations, and then tested on their understanding of the system. We show, for example, that explanations describing why the system behaved a certain way resulted in better understanding and stronger feelings of trust. Explanations describing why the system did not behave a certain way, resulted in lower understanding yet adequate performance. We discuss implications for the use of our findings in real-world context-aware applications.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2119--2128},
numpages = {10},
keywords = {intelligibility, context-aware, explanations},
location = {Boston, MA, USA},
series = {CHI '09}
}

@article{sultana2021dissemination,
author = {Sultana, Sharifa and Fussell, Susan R.},
title = {Dissemination, Situated Fact-checking, and Social Effects of Misinformation among Rural Bangladeshi Villagers During the COVID-19 Pandemic},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479580},
doi = {10.1145/3479580},
abstract = {This paper investigates the dissemination, situated fact-checking processes, and social effects of COVID-19 related online and offline misinformation in rural Bangladeshi life. A six-month-long ethnographic study in three villages found villagers perceived a lack of knowledge and experience among local medical professionals and often fell for flashy promotions of unreliable and unconfirmed cures. Villagers built on their local beliefs and myths, religious faiths, and social justice sensibilities while fact-checking suspicious information. They often reported being misled by misinformation that caters to these values, and they further spread this information through conversations with friends and family. Based on our findings, we argue that CSCW and HCI researchers should study misinformation and situated fact-checking together as a communal practice to design appropriate wellbeing technologies and social media for given communities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {436},
numpages = {34},
keywords = {Bangladesh, COVID-19, Facebook, misinformation, rural, situated fact-checking}
}


@article{lee2024people,
  title={People who share encounters with racism are silenced online by humans and machines, but a guideline-reframing intervention holds promise},
  author={Lee, Cinoo and Gligori{\'c}, Kristina and Kalluri, Pratyusha Ria and Harrington, Maggie and Durmus, Esin and Sanchez, Kiara L and San, Nay and Tse, Danny and Zhao, Xuan and Hamedani, MarYam G and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={38},
  pages={e2322764121},
  year={2024},
  publisher={National Academy of Sciences},
doi = {10.1073/pnas.2322764121}
}
@article{dias2021fighting,
  title={Fighting hate speech, silencing drag queens? artificial intelligence in content moderation and risks to LGBTQ voices online},
  author={Dias Oliva, Thiago and Antonialli, Dennys Marcelo and Gomes, Alessandra},
  journal={Sexuality \& Culture},
  volume={25},
  pages={700--732},
  year={2021},
  publisher={Springer},
doi = {10.1007/s12119-020-09790-w}
}
@inproceedings{Forster2020FosteringSystems,
    title = {Fostering human agency: A process for the design of user-centric XAI systems},
    year = {2020},
    booktitle = {International Conference on Information Systems, ICIS 2020 - Making Digital Inclusive: Blending the Local and the Global},
    author = {F{\{o}}rster, Maximilian and Klier, Mathias and Kluge, Kilian and Sigler, Irina},
    pages = {0--17},
    isbn = {9781733632553},
    keywords = {Explainable artificial intelligence, Human-AI Interaction, User-Centric Design},
    url = {https://aisel.aisnet.org/icis2020/hci_artintel/hci_artintel/12/}
}
@inproceedings{Binns2018ItsPercentage,
author = {Binns, Reuben and Van Kleek, Max and Veale, Michael and Lyngs, Ulrik and Zhao, Jun and Shadbolt, Nigel},
title = {'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173951},
doi = {10.1145/3173574.3173951},
abstract = {Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to 'meaningful information about the logic' behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people's perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles---under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no 'best' approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1--14},
numpages = {14},
keywords = {explanation, fairness, machine learning, justice, algorithmic decision-making, transparency},
location = {Montreal QC, Canada},
series = {CHI '18}
}
@inproceedings{madumal2019grounded,
  title={A Grounded Interaction Protocol for Explainable Artificial Intelligence},
  author={Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={1033--1041},
  year={2019}
}

@inproceedings{draws2022effects,
  title={The effects of crowd worker biases in fact-checking tasks},
  author={Draws, Tim and La Barbera, David and Soprano, Michael and Roitero, Kevin and Ceolin, Davide and Checco, Alessandro and Mizzaro, Stefano},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2114--2124},
  year={2022}
}
@article{klumbyte2023explaining,
  title={Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour},
  author={Klumbyte, Goda and Piehl, Hannah and Draude, Claude},
  journal={arXiv preprint arXiv:2305.03376},
  year={2023}
}
@article{mueller2019explanation,
  title={Explanation in human-AI systems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable AI},
  author={Mueller, Shane T and Hoffman, Robert R and Clancey, William and Emrey, Abigail and Klein, Gary},
  journal={arXiv preprint arXiv:1902.01876},
  year={2019}
}


@inproceedings{roitero2020can,
  title={Can the crowd identify misinformation objectively? The effects of judgment scale and assessor's background},
  author={Roitero, Kevin and Soprano, Michael and Fan, Shaoyang and Spina, Damiano and Mizzaro, Stefano and Demartini, Gianluca},
  booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={439--448},
  year={2020}
}
@inproceedings{roitero2020covid,
  title={The covid-19 infodemic: Can the crowd judge recent misinformation objectively?},
  author={Roitero, Kevin and Soprano, Michael and Portelli, Beatrice and Spina, Damiano and Della Mea, Vincenzo and Serra, Giuseppe and Mizzaro, Stefano and Demartini, Gianluca},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={1305--1314},
  year={2020}
}
@article{hildebrandt2020debate,
  title={Debate Dynamics for Human-comprehensible Fact-checking on Knowledge Graphs},
  author={Hildebrandt, Marcel and Serna, Jorge Andres Quintero and Ma, Yunpu and Ringsquandl, Martin and Joblin, Mitchell and Tresp, Volker},
  journal={arXiv preprint arXiv:2001.03436},
  year={2020}
}
@article{10.1145/3534929,
author = {Jiang, Jialun Aaron and Nie, Peipei and Brubaker, Jed R. and Fiesler, Casey},
title = {A Trade-off-centered Framework of Content Moderation},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3534929},
doi = {10.1145/3534929},
abstract = {Content moderation research typically prioritizes representing and addressing challenges for one group of stakeholders or communities in one type of context. While taking a focused approach is reasonable or even favorable for empirical case studies, it does not address how content moderation works in multiple contexts. Through a systematic literature review of 86 content moderation articles that document empirical studies, we seek to uncover patterns and tensions within past content moderation research. We find that content moderation can be characterized as a series of tradeoffs around moderation actions, styles, philosophies, and values. We discuss how facilitating cooperation and preventing abuse, two key elements in Grimmelmann’s definition of moderation, are inherently dialectical in practice. We close by showing how researchers, designers, and moderators can use our framework of tradeoffs in their own work, and arguing that tradeoffs should be of central importance in investigating and designing content moderation.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {mar},
articleno = {3},
numpages = {34},
keywords = {literature review, social media, online communities, Content moderation}
}
@article{oppenheimer2006consequences,
  title={Consequences of erudite vernacular utilized irrespective of necessity: Problems with using long words needlessly},
  author={Oppenheimer, Daniel M},
  journal={Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition},
  volume={20},
  number={2},
  pages={139--156},
  year={2006},
  publisher={Wiley Online Library}
}


@article{li2016survey,
  title={A survey on truth discovery},
  author={Li, Yaliang and Gao, Jing and Meng, Chuishi and Li, Qi and Su, Lu and Zhao, Bo and Fan, Wei and Han, Jiawei},
  journal={ACM Sigkdd Explorations Newsletter},
  volume={17},
  number={2},
  pages={1--16},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{kutlu2020annotator,
  title={Annotator rationales for labeling tasks in crowdsourcing},
  author={Kutlu, Mucahid and McDonnell, Tyler and Elsayed, Tamer and Lease, Matthew},
  journal={Journal of Artificial Intelligence Research},
  volume={69},
  pages={143--189},
  year={2020}
}

@incollection{AlfanoForthcoming-ALFEPF,
	author = {Mark Alfano and Emily Sullivan and Amir Ebrahimi Fard},
	booktitle = {The Atlas of Language Analysis in Psychology},
	editor = {Morteza Dehghani and Ryan Boyd},
	publisher = {Guilford Press},
	title = {Ethical Pitfalls for Natural Language Processing in Psychology},
	year = {forthcoming}
}

@article{10.1145/3641022,
author = {Morrison, Katelyn and Spitzer, Philipp and Turri, Violet and Feng, Michelle and K\""{u}hl, Niklas and Perer, Adam},
title = {The Impact of Imperfect XAI on Human-AI Decision-Making},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3641022},
doi = {10.1145/3641022},
abstract = {Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {183},
numpages = {39},
keywords = {explainable AI, explainable AI for computer vision, human-AI collaboration}
}

@inproceedings{schmitt2024explhuman,
author = {Schmitt, Vera and Villa-Arenas, Luis-Felipe and Feldhus, Nils and Meyer, Joachim and Spang, Robert P. and M\"{o}ller, Sebastian},
title = {The Role of Explainability in Collaborative Human-AI Disinformation Detection},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659031},
doi = {10.1145/3630106.3659031},
abstract = {Manual verification has become very challenging based on the increasing volume of information shared online and the role of generative Artificial Intelligence (AI). Thus, AI systems are used to identify disinformation and deep fakes online. Previous research has shown that superior performance can be observed when combining AI and human expertise. Moreover, according to the EU AI Act, human oversight is inevitable when using AI systems in a domain where fundamental human rights, such as the right to free expression, might be affected. Thus, AI systems need to be transparent and offer sufficient explanations to be comprehensible. Much research has been done on integrating eXplainability (XAI) features to increase the transparency of AI systems; however, they lack human-centered evaluation. Additionally, the meaningfulness of explanations varies depending on users’ background knowledge and individual factors. Thus, this research implements a human-centered evaluation schema to evaluate different XAI features for the collaborative human-AI disinformation detection task. Hereby, objective and subjective evaluation dimensions, such as performance, perceived usefulness, understandability, and trust in the AI system, are used to evaluate different XAI features. A user study was conducted with an overall total of 433 participants, whereas 406 crowdworkers and 27 journalists participated as experts in detecting disinformation. The results show that free-text explanations contribute to improving non-expert performance but do not influence the performance of experts. The XAI features increase the perceived usefulness, understandability, and trust in the AI system, but they can also lead crowdworkers to blindly trust the AI system when its predictions are wrong.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2157–2174},
numpages = {18},
keywords = {Collaborative disinformation detection, XAI, expert and lay people evaluation, transparent AI systems},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@article{koliska2024epistemology,
  title={Epistemology of Fact Checking: An Examination of Practices and Beliefs of Fact Checkers Around the World},
  author={Koliska, Michael and Roberts, Jessica},
  journal={Digital Journalism},
  pages={1--21},
  year={2024},
  publisher={Taylor \& Francis},
doi = {10.1080/21670811.2024.2361264}
}


@inproceedings{Cole2022GTinHCI,
author = {Cole, Tom and Gillies, Marco},
title = {More than a bit of coding: (un-)Grounded (non-)Theory in HCI},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3516392},
doi = {10.1145/3491101.3516392},
abstract = {Grounded Theory Methodology (GTM) is a powerful way to develop theories where there is little existing research using a flexible but rigorous empirically-based approach. Although it originates from the fields of social and health sciences, it is a field-agnostic methodology that can be used in any discipline. However, it tends to be misunderstood by researchers within HCI. This paper sets out to explain what GTM is, how it can be useful to HCI researchers, and examples of how it has been misapplied. There is an overview of the decades of methodological debate that surrounds GTM, why it’s important to be aware of this debate, and how GTM differs from other, better understood, qualitative methodologies. It is hoped the reader is left with a greater understanding of GTM, and better able to judge the results of research which claims to use GTM, but often does not.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {11},
numpages = {11},
keywords = {research methods, grounded theory},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{Furniss2011GTExperience,
author = {Furniss, Dominic and Blandford, Ann and Curzon, Paul},
title = {Confessions from a grounded theory PhD: experiences and lessons learnt},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978960},
doi = {10.1145/1978942.1978960},
abstract = {Grounded Theory (GT) is used within HCI research, but nuances and more modern interpretations of the method are rarely discussed. This paper has two intentions: to offer guidance on practical issues when applying GT, and to clarify the space of methodological possibilities. We describe an extended GT study on understanding why practitioners choose particular usability evaluation methods. We describe five stages in this study to highlight our experiences and choices made. We draw out seven practical and methodological considerations in applying GT in a CHI context. This challenges the more traditional inductive and objective positions on GT use; it sensitizes novices of GT to these issues; and through the extended case study it provides substance for debate on issues that affect those that use qualitative methods more broadly.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {113–122},
numpages = {10},
keywords = {resilience engineering, qualitative, method, grounded theory, distributed cognition, constructivist},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}