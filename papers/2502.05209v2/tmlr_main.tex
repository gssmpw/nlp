
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% BEGIN ADDED BY CAS
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
% END ADDED BY CAS


\title{Model Tampering Attacks Enable More\\Rigorous Evaluations of LLM Capabilities}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\vspace{-5pt}\name Zora Che, 
\addr University of Maryland, ML Alignment \& Theory Scholars \email zche@umd.edu
\AND
\vspace{-5pt}\name Stephen Casper,
\addr MIT CSAIL, ML Alignment \& Theory Scholars \email scasper@mit.edu
\AND
\vspace{-5pt}\name Robert Kirk, \addr UK AI Security Institute
\AND
\vspace{-5pt}\name Anirudh Satheesh, \addr University of Maryland
\AND
\vspace{-5pt}\name Stewart Slocum, \addr MIT
\AND
\vspace{-5pt}\name Lev McKinney, 
\addr University of Toronto
\AND
\vspace{-5pt}\name Rohit Gandikota, 
\addr Northeastern University
\AND
\vspace{-5pt}\name Aidan Ewart, \addr Haize Labs
\AND
\vspace{-5pt}\name Domenic Rosati, \addr Dalhousie University
\AND
\vspace{-5pt}\name Zichu Wu, \addr University of Waterloo
\AND
\vspace{-5pt}\name Zikui Cai, \addr University of Maryland
\AND
\vspace{-5pt}\name Bilal Chughtai, \addr Apollo Research
\AND
\vspace{-5pt}\name Yarin Gal, \addr UK AI Security Institute,
University of Oxford
\AND
\vspace{-5pt}\name Furong Huang, \addr University of Maryland
\AND
\vspace{-5pt}\name Dylan Hadfield-Menell, \addr MIT
}


% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Evaluations of large language model (LLM) risks and capabilities are increasingly being incorporated into AI risk management and governance frameworks. 
Currently, most risk evaluations are conducted by designing \emph{inputs} that elicit harmful behaviors from the system.
However, this approach suffers from two limitations. 
First, input-output evaluations cannot evaluate realistic risks from open-weight models. 
Second, the behaviors identified during any particular input-output evaluation can only lower-bound the model's worst-possible-case input-output behavior.
As a complementary method for eliciting harmful behaviors, we propose evaluating LLMs with \emph{model tampering} attacks which allow for modifications to latent activations or weights.
We pit state-of-the-art techniques for removing harmful LLM capabilities against a suite of 5 input-space and 6 model tampering attacks.
In addition to benchmarking these methods against each other, we show that (1) model resilience to capability elicitation attacks lies on a low-dimensional robustness subspace; (2) the attack success rate of model tampering attacks can empirically predict and offer conservative estimates for the success of held-out input-space attacks; and (3) state-of-the-art unlearning methods can easily be undone within 16 steps of fine-tuning. 
Together these results highlight the difficulty of suppressing harmful LLM capabilities and show that model tampering attacks enable substantially more rigorous evaluations than input-space attacks alone.\footnote{We release 64 models at 
% [redacted for review].
\href{https://huggingface.co/LLM-GAT}{https://huggingface.co/LLM-GAT}.
}
\end{abstract}


\section{Introduction: Limitations of Input-Output Evaluations} 
\label{sec:intro}


Rigorous evaluations of large language models (LLMs) are widely recognized as key for risk mitigation \citep{raji2022outsider, anderljung2023publicly, schuett2023towards, shevlane2023model} and are being incorporated into AI governance frameworks \citep{airmf2023, dsit2023, eu_ai_act, GenerativeAIInterimMeasures, Bill2338, AIDAct, National_Assembly_of_the_Republic_of_Korea_2025}.
However, despite their efforts, developers often fail to identify overtly harmful LLM behaviors pre-deployment \citep{shayegani2023survey, andriushchenko2024jailbreaking, carlini2024aligned, yi2024jailbreak}. 
Current methods primarily rely on automated input-space attacks, where evaluators search for prompts that elicit harmful behaviors.
These are useful but often leave
% often fall short of identifying an LLM's potential for harm \citep{casper2024black}.
% For example, \citet{li2024llm} and \citet{zhou2023revisiting} show that even when common benchmark attacks fail to jailbreak an LLM, manually generated ones are still reliable. 
unidentified vulnerabilities.
A difficulty with input-space attacks is that they are poorly equipped to cover the attack surface. 
This happens for two reasons.
First, attackers can sometimes manipulate more than just model inputs (e.g., if a model is open-source).
Second, it is intractable to exhaustively search the input space.\footnote{For example, with modern tokenizers, there are vastly more 20-token strings than particles in the known universe.}
These challenges highlight a fundamental limitation of input-space evaluations: the worst behaviors identified during an assessment can only offer a lower bound of the model's overall worst-case behavior \citep{gal2024science, openai2024systemcard}.
% Poor evaluations can not underestimate risks but can be counterproductive if they create a false sense of security \citep{anderljung2023publicly}. 

% How can we design LLM evaluations that help to better estimate unforeseen risks and worst-case harms? 
% It will never be practical to exhaustively assess LLM behaviors under all potential inputs.\footnote{For example, with modern tokenizers, there are vastly more 20-token strings than particles in the known universe.}
To help address this challenge, we draw inspiration from a safety engineering principle: that safety-critical systems should be tested under stresses at least as extreme—if not more—than those expected in deployment \citep{clausen2006generalizing}.
% Instead, we take inspiration from an engineering principle: that safety-critical systems should be designed and tested to handle stresses equal to or greater than those they might face during use \citep{clausen2006generalizing}. 
For example, buildings are designed to withstand loads multiple times greater than their intended use.
Here, we take an analogous approach to evaluating and building safety cases \citep{clymer2024safety} for LLMs: stress-testing them under attacks that go beyond input-space manipulations. 

\begin{figure}[t!]
    \centering
\includegraphics[width=0.7\linewidth]{figs/fig1.pdf}
    \caption{\textbf{Model tampering attacks modify latents and weights.} In contrast to input-space attacks, model tampering attacks elicit capabilities from an LLM by making modifications to the internal activations or weights. In this paper, we use model tampering attacks to (1) directly evaluate risks from malicious tampering with open-weight models and (2) indirectly evaluate difficult-to-foresee input-space vulnerabilities in models.}
    \label{fig:fig1}
\end{figure}

We propose using \emph{model tampering} attacks, which allow for adversarial modifications to the model's weights or latent activations, in addition to evaluating systems under input-space attacks (see \Cref{fig:fig1}).
We attempt to answer two questions, each corresponding to a different threat model: 

\textbf{Question 1: How vulnerable are LLMs to model tampering attacks?} Answering this helps us understand how model tampering attacks can be used to study risks from models that are open-source,\footnote{It may seem obvious that model tampering attacks are needed to realistically assess threats from open-source models. However, there is a precedent for developers failing to use them prior to open-source releases. For example, before releasing Llama 2 and Llama 3 models, Meta's red-teaming efforts did not reportedly involve model tampering attacks \citep{touvron2023llama, dubey2024llama}.} have fine-tuning APIs, or may be leaked \citep{nevo2024securing}.

\textbf{Question 2: Can model tampering attacks inform evaluators about LLM vulnerabilities to novel input-space attacks?} Answering this will help us understand how model tampering attacks can help assess risks from both open- and closed-source models.  

To answer these questions, we pit state-of-the-art methods for unlearning and safety fine-tuning in LLMs against a suite of input-space and model tampering attacks.
We make four contributions:
\begin{enumerate}
    \item \textbf{Benchmarking:} We benchmark 8  unlearning methods and 9 safety fine-tuned LLMs, each against 11 capability elicitation attacks.
    \item \textbf{Science of robustness:} We show that LLM resilience to a variety of capability elicitation attacks lies on a low-dimensional robustness subspace.
    \item \textbf{Evaluation methodology:} We show that the success of some model tampering attacks correlates with that of held-out input-space attacks. We also find that few-shot fine-tuning attacks can empirically be used to conservatively over-estimate a model's robustness to held-out input-space threats. % In particular, we demonstrate that state-of-the-art defense methods can consistently be undone within 50 steps of few-shot fine-tuning.
    \item \textbf{Model suite:} To facilitate further research, we release a set of 64 models trained using 8 methods to unlearn dual-use biology knowledge at varying degrees of strength at 
    % [redacted for review].
    \href{https://huggingface.co/LLM-GAT}{https://huggingface.co/LLM-GAT}.
\end{enumerate}

% Overall, our results suggest that model tampering attacks can help evaluators gain more information about the potential worst-case behaviors of LLMs.

\section{Related Work}

\textbf{Latent-space attacks:} 
During a latent-space attack, an adversary can make modifications to a model's hidden activations.
Adversarial training under these attacks can improve the generality of a model's robustness \citep{sankaranarayanan2018regularizing, singh2019harnessing, zhang2023adversarial, schwinn2023adversarial, zeng2024beear}.
In particular, \citet{xhonneux2024efficient}, \citet{casper2024defending}, and \citet{sheshadri2024targeted} use latent adversarial training to improve defenses against held-out types of adversarial attacks. 
Other work on activation engineering has involved making modifications to a model's behavior via simple transformations to their latent states \citep{zou2023representation, wang2023backdoor, lu2024investigating, arditi2024refusal}. \citet{zhang2025catastrophicfailurellmunlearning} also showed that unlearning methods can be brittle to quantization methods. 

\textbf{Weight-space (fine-tuning) attacks:} During a few-shot fine-tuning attack \citep{huang2024harmful}, an adversary can modify model weights via fine-tuning on a limited number of samples. 
For example, \citet{qi2023fine} showed that fine-tuning on as few as 10 samples could jailbreak GPT-3.5.
Many works have used few-shot fine-tuning attacks to elicit LLM capabilities that were previously suppressed by fine-tuning or unlearning \citep{jain2023mechanistically, yang2023shadow, qi2023fine, bhardwaj2023language, lermen2023lora, zhan2023removing, ji2024language, qi2024safety, hu2024jogging, halawicovert, peng2024navigating, lo2024large, lucki2024adversarial, shumailov2024ununlearning, lynch2024eight, deeb2024unlearningmethodsremoveinformation, qi2024evaluating, yi2024vulnerability}.
% For example, \citet{greenblatt2024stress} found that fine-tuning was a reliable way of eliciting hidden capabilities from a ``password locked'' model that would only exhibit certain capabilities if a specific ``password'' was present in the prompt.

\textbf{Capability elicitation and evaluation:} 
Research on adversarial capability elicitation \citep{hofstatter2025elicitation} in LLMs has primarily been done in the context of machine unlearning \citep{liu2024rethinking, barez2025open} and jailbreaking \citep{yi2024jailbreak}. 
Here, we experiment in these two domains. However, capability elicitation has also been researched in the context of backdoors/trojans \citep{zhao2024survey}, ``password-locked models'' \citep{greenblatt2024stress, hofstatter2025elicitation}, and ``sandbagging'' \citep{van2024ai}.
In the unlearning field, several recent works have used adversarial methods to evaluate the robustness of unlearning algorithms \citep{patil2023can, lynch2024eight, lucki2024adversarial, hu2024jogging, liu2024rethinking, zhang2024does, liu2024threats}.
Here, we build off of \citet{li2024wmdp} who introduce WMDP-Bio, a benchmark for unlearning dual-use biotechnology knowledge from LLMs. 
% Compared to any of the above, our work is the first to systematically evaluate the relationships between different types adversarial vulnerabilities in LLMs and quantify what model tampering attacks can teach evaluators about LLM vulnerabilities to unforeseen failure modes. 
In the jailbreaking field, many techniques have been developed to make LLMs comply with harmful requests \citep{shayegani2023survey, yi2024jailbreak, jin2024jailbreakzoo, chowdhury2024breaking, lin2024against}.
Here, we experiment with 9 open-source LLMs and a set of gradient-guided, perplexity-guided, and prosaic techniques from the adversarial attack literature (see \Cref{tab:attacks_defenses}).

% \textbf{Red-teaming unlearning methods:} Recent research on red-teaming unlearning methods have demonstrated brittleness of unlearning methods GA, NPO to quantization \cite{zhang2025catastrophicfailurellmunlearning}, and brittleness of RMU, NPO, DPO to adapted input-space and fine-tuning attacks \cite{2025adversarialperspectivemachineunlearning}. Additionally, \citep{qi2024evaluating} evaluates unlearning methods geared towards safe-guarding against fine-tuning, and shows that claims by these methods can be misleading. Our results complement research on adapting attacks for red-teaming unlearning methods, and developing more comprehensive evaluation of unlearning efficacy by benchmarking vulnerability across 8 methods. We find similar to \citet{qi2024evaluating} that TAR results suffer from significant utility drops, which calls for careful evaluation of capability suppression methods. 








\section{Methods}
\label{sec:methods}


\begin{table*}[h!]
\centering
\resizebox{1\textwidth}{!}{
\begin{tabular}{lllll}
% \toprule
\large {\bfseries Defenses}& & & &  \\ 
\toprule
{\textit{Unlearning Methods}} & & Gradient Difference (\textbf{GradDiff}) &  \citet{liu2022continual} &  \\
{\textit{(We train 8x models each to unlearn WMDP-Bio)}} & & Random Misdirection for Unlearning (\textbf{RMU}) & \citet{li2024wmdp} &  \\ 
& & RMU with Latent Adversarial Training (\textbf{RMU+LAT}) & \citet{sheshadri2024targeted} &  \\
& & Representation Noising (\textbf{RepNoise}) & \citet{rosati2024representation} &  \\
& & Erasure of Language Memory (\textbf{ELM}) & \citet{anonymous2024erasing} &  \\
& & Representation Rerouting (\textbf{RR}) & \citet{zou2024improving} &  \\
% & & Random Mapping (\textbf{RandMap}) & \citet{tamirisa2024tamper} &  \\
& & Tamper Attack Resistance (\textbf{TAR})  & \citet{tamirisa2024tamper} &  \\
& & PullBack \& proJect (\textbf{PB\&J})  & \citet{mckinney2025pbnj} &  \\
\midrule
{\textit{Jailbreak Refusal-Tuned Models}} & & meta-llama/Meta-Llama-3-8B-Instruct &  \citet{dubey2024llama} & \\
\textit{(Off the shelf)} & & slz0106/llama3\_finetune\_refusal & \href{https://huggingface.co/slz0106/llama3_finetune_refusal}{Link} &  \\
& & JINJIN7987/llama3-8b-refusal-vpi & \href{https://huggingface.co/JINJIN7987/llama3-8b-refusal-vpi}{Link} &  \\
& & Youliang/llama3-8b-derta & \citet{yuan2024refuse} &  \\
& & GraySwanAI/Llama-3-8B-Instruct-RR & \citet{zou2024improving} &  \\
& & LLM-LAT/llama3-8b-instruct-rt-jailbreak-robust1 & \citet{sheshadri2024targeted} &  \\
& & LLM-LAT/robust-llama3-8b-instruct & \citet{sheshadri2024targeted} &  \\
& & lapisrocks/Llama-3-8B-Instruct-TAR-Refusal & \citet{tamirisa2024tamper} &  \\
% & & WhiteRabbitNeo/Llama-3-WhiteRabbitNeo-8B-v2.0 & \href{https://huggingface.co/WhiteRabbitNeo/Llama-3-WhiteRabbitNeo-8B-v2.0}{Link} &  \\
& & Orenguteng/Llama-3-8B-Lexi-Uncensored & \href{https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored}{Link} &  \\
\bottomrule
& & & &  \\ 
\large {\bfseries Attacks}& & & &  \\ 
\toprule
{ \textit{Input-Space}} & Gradient-guided & Greedy Coordinate Gradient (\textbf{GCG}) & \citet{zou2023universal}  &  \\ 
&  & \textbf{AutoPrompt} & \citet{shin2020autopromptelicitingknowledgelanguage} &  \\ 
 \cmidrule{2-4}
& Perplexity-guided & Beam Search-based Attack (\textbf{BEAST}) & \citet{sadasivan2024fastadversarialattackslanguage} &  \\ 
\cmidrule{2-4}
& Prosaic & Prompt Automatic Iterative Refinement (\textbf{PAIR}) & \citet{chao2024jailbreakingblackboxlarge} &  \\ 
& & \textbf{Human Prompt} & &  \\ 
\midrule
{\textit{Model Tampering}} & Latent space & \textbf{Embedding perturbation} & \citet{schwinn2024soft} &  \\ 
&  & \textbf{Latent perturbation } & \citet{sheshadri2024targeted} &  \\ 
\cmidrule{2-4}
& Weight space & \textbf{WandA Pruning} & \citet{sun2023simple} &  \\
& & \textbf{Benign LoRA} & \citet{qi2023fine} &  \\
& &  \textbf{LoRA} & \citet{hu2021lora} &   \\ 
& & \textbf{Full Parameter} & &   \\ 
\bottomrule 
\end{tabular}
}
\caption{\textbf{Table of capability elicitation (attack) and capability suppression (defense) defense.} We consider defenses in two different settings: (top) unlearning approaches that remove hazardous bio-knowledge and (bottom) refusal-tuned models that resist jailbreaks.}
\label{tab:attacks_defenses}
\end{table*}



\textbf{Our approach: pitting capability suppression defenses against capability elicitation attacks.}  
% We do this in both machine unlearning and jailbreak settings. 
For unlearning experiments, we experiment with 65 models trained using 8 different unlearning methods. 
For jailbreaking experiments, we experiment with 9 models off the shelf from prior works. 
In both cases, we pit these defenses against a set of 11 input-space and model tampering attacks to either elicit `unlearned' knowledge or jailbreak the model. 
In \Cref{tab:attacks_defenses}, we list all unlearning methods, off-the-shelf models, and attacks we use. 
Since the input-space attacks that we use are held out, we treat them as proxies for novel input-space attacks in our evaluations (see also \citet{hofstatter2025elicitation}).

\textbf{Defenses: machine unlearning methods:} We unlearn dual-use bio-hazardous knowledge on Llama-3-8B-Instruct \cite{dubey2024llama} with the unlearning methods listed in \Cref{tab:attacks_defenses} and outlined in \Cref{app:unlearning_methods}. 
For all methods, we train on 1,600 examples of max length 512 from the bio-remove-split of the WMDP `forget set' \citep{li2024wmdp}, and up to 1,600 examples of max length 512 from Wikitext as the `retain set'. 
For the 8 unlearning methods listed in \Cref{tab:attacks_defenses}, we take 8 checkpoints evenly spaced across training.
Finally, we also use the public release of the ``TAR-v2'' model from \citet{tamirisa2024tamper} as a 9th TAR model.
In total, the 8 checkpoints each from the 8 methods we implemented plus the TAR model from \citet{tamirisa2024tamper} resulted in 65 models. 
% We evaluate unlearning efficacy on the WMDP-Bio QA dataset.

\textbf{Defenses: refusal fine-tuned models:} For jailbreaking experiments, we use the 9 fine-tuned Llama3-8B-Instruct models off the shelf listed in \Cref{tab:attacks_defenses}. 
The first 8 are all fine-tuned for robust refusal of harmful requests. 
Of these, `RR' \citep{zou2023representation} and `LAT' \citep{sheshadri2024targeted} are state-of-the-art for open-weight jailbreak robust models \citep{li2024llm, haizelabs2023cascade}.
The final `Orenguteng' model was fine-tuned to be `helpful-only' and comply even with harmful requests. 
We discuss these models in more detail in \Cref{app:jb_models}.

\textbf{Attacks: capability elicitation methods:} We use 5 input-space attacks and 6 model tampering attacks on our unlearned models. 
We use these attacks (single-turn) to increase dual-use bio knowledge (as measured by WMDP-Bio performance \citep{li2024wmdp}) for unlearning experiments and to elicit compliance with harmful requests (as measured by the StrongReject AutoGrader \citep{souly2024strongreject}) for jailbreaking experiments. 
We selected attacks based on algorithmic diversity and prominence in the state of the art.
We list all 11 attacks in \Cref{tab:attacks_defenses}. 
\textbf{In all experiments, we produce \emph{universal} adversarial attacks optimized to work for \textit{any} prompt.}
This allows us to attribute attack success to capability elicitation rather than answer-forcing from the model (e.g., \citet{fort2023scaling}). 
For descriptions and implementation details for each attack method, see \Cref{sec:appendix-experiment-details}. 
Finally, we also used two proprietary attacks -- one for unlearning experiments and one for jailbreaking experiments which we will describe in \Cref{sec:experiments}.

% We note that input-space prefix and suffix finding attacks can be leveraged as black-box attacks via transfer attack \zora{is this clear?}.

% \begin{table}[h]
% \centering
% \resizebox{0.95\textwidth}{!}{
% \begin{tabular}{rlll}
% \toprule
%  &  &    & Black-box \; White-box  \\ 
%  \cmidrule{2-4}
% {\bfseries Input-space} & Gradient-guided & GCG \citep{zou2023universal}  & \checkmark
%  \;  \; \; \; \; \; \; \; \ \checkmark
%   \\ 
%  &  & Autoprompter \citep{shin2020autopromptelicitingknowledgelanguage} & \checkmark
%  \;  \; \; \; \; \; \; \; \ \checkmark  \\ 
%  \cmidrule{2-4}
% & Perplexity-guided & BEAST \citep{sadasivan2024fastadversarialattackslanguage} & \checkmark
%  \;  \; \; \; \; \; \; \; \ \checkmark \\
% \cmidrule{2-4}
% & Prosaic & PAIR \citep{} & \checkmark
%  \;  \; \; \; \; \; \; \; \ \checkmark \\
% \cmidrule{2-4}
% & Heuristic & Translation & \checkmark
%  \;  \; \; \; \; \; \; \; \  
% \\
% &  & Many-shot in-context& \checkmark
%  \;  \; \; \; \; \; \; \; \   
% \\
% \midrule
% {\bfseries Model tampering} & Finetuning & LoRA \citep{hu2021lora} &  \;\,
%  \;  \; \; \; \; \; \; \; \   \checkmark \\ 
% & &  Full parameter &  \;\,
%  \;  \; \; \; \; \; \; \; \   \checkmark \\ 
% \cmidrule{2-4}
% & Latent perturbation & Latent layer attack \citep{sheshadri2024targeted} &  \;\,
%  \;  \; \; \; \; \; \; \; \   \checkmark \\ 
% &  &  Embedding attack \citep{} &  \;\,
%  \;  \; \; \; \; \; \; \; \   \checkmark \\ 
% \bottomrule 
% \end{tabular}
% }
% \caption{Attacks to Elicit Harmful Capability}
% \label{tab:attacks}
% \end{table}

% \begin{table}
% \centering
% \resizebox{0.95\textwidth}{!}{
% \begin{tabular}{llll}
% \toprule
%  % &  &    & Black-box \; White-box  \\ 
%  % \cmidrule{2-4}
% {\bfseries Input-space} & Gradient-guided & GCG \citep{zou2023universal}  &  \\ 

%  &  & AutoPrompt \citep{shin2020autopromptelicitingknowledgelanguage} &  \\ 
%  \cmidrule{2-4}
% & Perplexity-guided & BEAST \citep{sadasivan2024fastadversarialattackslanguage} &  \\ 
% \cmidrule{2-4}
% & Prosaic & PAIR \citep{chao2024jailbreakingblackboxlarge} &  \\ 
% & & Manual prompt &  \\ 
% \midrule
% {\bfseries Model tampering} & Latent perturbation & Embedding space \citep{schwinn2024soft} &  \\ 
% &  & Latent space attack \citep{sheshadri2024targeted} &  \\ 
% \cmidrule{2-4}
% & Fine-tuning & Full parameter (on the WMDP-Bio forget or retain set) &  \\
% & & LoRA \citep{hu2021lora} (on the WMDP-Bio forget or retain set) &   \\ 
% & & LoRA \citep{hu2021lora} (on non-biology data) &   \\ 

% \bottomrule 
% \end{tabular}
% }
% \caption{\textbf{The ten capability elicitation methods we use to extract bio-hazardous knowledge from unlearned LLMs.}}
% \label{tab:attacks}
% \end{table}


\textbf{Attacks: data} % All attacks, except fine-tuning attacks, used 64 examples for attack generation.
\begin{itemize}
    \item \textbf{Attacks on unlearning -- non-fine-tuning:} we used 64 held-out examples of multiple-choice biology questions from the WMDP-Bio test set.
    \item \textbf{Attacks on unlearning -- adversarial fine-tuning:} we use the WMDP `bio retain' or `forget' sets. Both of which are comprised of biology papers. 
    \item \textbf{Attacks on refusal training -- all except benign fine-tuning:} we used held-out examples of compliance with harmful requests from \citet{sheshadri2024targeted}. Each example is a prompt + response pair. 
    \item \textbf{Attacks on unlearning and refusal training -- benign fine-tuning:} For all benign fine-tuning attacks, we used WikiText \cite{merity2016pointer}.
\end{itemize}

For details on attack configurations, including the number of examples, batch size, number of steps, and other hyper-parameters, see \Cref{tab:finetune-hypers}. 


\textbf{Attacks: model tampering attacks are efficient.} In \Cref{tab:attack_efficiency}, we show the number of forward and backward passes used in our implementations of attacks. Model tampering attacks were more efficient than state-of-the-art input-space attacks. 


\section{Experiments} \label{sec:experiments}
% For 6 of the 8 unlearning outlined in \Cref{tab:attacks_defenses} (GradDiff, RMU, RMU+LAT, RepNoise, ELM, and RR), we evaluate 8 checkpoints evenly spaced across training. 
% Due to computational costs and difficulty reproducing results from \citet{tamirisa2024tamper}, we used the public release TAR and RandMap models by the authors.
% This resulted in a total of 50 checkpoints to evaluate.
% For each model, we ran evaluations on their general capabilities, their biology knowledge, their vulnerability to input-space attacks, and their vulnerability to model tampering attacks.

\begin{table*}[t!]
\centering
% Reduce space between columns
\setlength{\tabcolsep}{3pt} % Default is 6pt
\resizebox{1\textwidth}{!}{
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{lccccccc}
\toprule
 \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{WMDP} $\downarrow$} & \textbf{WMDP, Best} & \textbf{WMDP, Best} & \multirow{2}{*}{\textbf{MMLU} $\uparrow$} & \multirow{2}{*}{\textbf{MT-Bench/10} $\uparrow$} &  \multirow{2}{*}{\textbf{AGIEval} $\uparrow$}& \textbf{Unlearning}   \\ 
  &  & \textbf{Input Attack} $\downarrow$ & \textbf{Tamp. Attack} $\downarrow$  &  &  & & \textbf{Score} $\uparrow$ \\ 
 \midrule
 Llama3-8B-Instruct &  0.70 & 0.75 & 0.71 &  0.64 & 0.78 & 0.41 & 0.00 \\
 \midrule
 Grad Diff &  0.25 & 0.27 & 0.67 & 0.52 & 0.13 & 0.32 & \textcolor[rgb]{0.92,0.35,0.23}{\textbf{0.17}} \\
 RMU & 0.26 & 0.34 & 0.57 & 0.59 &  0.68 & 0.42 & \textcolor[rgb]{0.27,0.68,0.36}{\textbf{0.84}} \\
 RMU + LAT & 0.32 & 0.39 & 0.64 & 0.60 & 0.71 & 0.39 &  \textcolor[rgb]{0.58,0.82,0.41}{\textbf{0.73}} \\
 RepNoise & 0.29 & 0.30 & 0.65 & 0.59  & 0.71 & 0.37 & \textcolor[rgb]{0.45,0.76,0.39}{\textbf{0.78}} \\
 ELM & 0.24 & 0.38 & 0.71 & 0.59 & 0.76 & 0.37 & \textcolor[rgb]{0.05,0.49,0.26}{\textbf{0.95}} \\
 RR & 0.26 & 0.28 & 0.66 & 0.61 & 0.76 & 0.44  & \textcolor[rgb]{0.04,0.48,0.25}{\textbf{\underline{0.96}}} \\
 TAR & 0.28 & 0.29 & 0.36 & 0.54 & 0.12 & 0.31 & \textcolor[rgb]{0.82,0.17,0.15}{\textbf{0.09}}  \\
 PB\&J & 0.31 & 0.32 & 0.64 & 0.63 & 0.78 & 0.40 & \textcolor[rgb]{0.25,0.67,0.35}{\textbf{0.85}} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Benchmarking LLM unlearning methods:} We report results for the checkpoint from each method with the highest unlearning score (\Cref{eq:unlearn_score}). We report original WMDP-Bio performance, worst-case WMDP-Bio performance across our attacks, and three measures of general utility: MMLU, MT-Bench, and AGIEval. Representation rerouting (RR) has the best unlearning score. No model has a WMDP-Bio performance less than 0.36 after the most effective attack. We note that Grad Diff and TAR models performed very poorly, often struggling with basic fluency.}
\label{tab:unlearning}
\end{table*}



As discussed in \Cref{sec:intro}, we have two motivations, each corresponding to a different threat model.
First, we want to directly evaluate robustness to model tampering attacks to better understand the risks of open-source, leaked, or API fine-tuneable LLMs. 
Second, we want to understand what model tampering attacks can tell us about novel, unforeseen input-space attacks in order to study risks from all types of LLMs. 
Unfortunately, unforeseen attacks are, by definition, ones that we do not have access to. 
Instead, since the input-space attacks that we use are held out during fine-tuning, we treat them as proxies for `unforeseen' input-space attacks.

\subsection{Unlearning Experiments} \label{sec:unlearning}

We first experiment with the unlearning of dual-use biology knowledge in LLMs by pitting unlearning methods against capability elicitation methods (see \Cref{tab:attacks_defenses}). 

\subsubsection{Benchmarking Unlearning Methods} \label{sec:benchmarking}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]
    {figs/benchmark_1.pdf}
    \caption{\textbf{Pitting capability suppression (unlearning) methods against capability elicitation attacks.} We use unlearning methods to suppress bio-hazardous knowledge from LLMs and pit these against capability elicitation attacks seeking to re-elicit the unlearned knowledge. All unlearning methods tested could be successfully attacked. \textbf{Left:} The \textit{unlearning score} (\Cref{eq:unlearn_score}) measures how effectively each unlearning method removed unwanted capabilities while preserving general model utility. Higher scores indicate better unlearning (scale 0-1). \textbf{Right:} Increase in the unlearned task performance after attacks. The first 5 columns are from input-space attacks while the final 6 are from model tampering attacks. In particular, finetuning attacks (rightmost columns) were especially effective at resurfacing suppressed capabilities.
    }
    \label{fig:all}
\end{figure*}

% The ability to robustly remove harmful knowledge has been proposed as a key goal for designing safer AI \citep{barez2025open}. However, here we find that state-of-the-art unlearning methods fail to achieve this goal. Even the best-performing unlearning approaches leave models vulnerable to few-shot fine-tuning. 

\textbf{Calculating an \emph{unlearning score}:} In our models, we evaluate \textit{unlearning efficacy} on the WMDP-Bio QA evaluation task \citep{li2024wmdp}. We evaluate \textit{general utility} using three benchmarks. 
First, we use MMLU \citep{hendrycks2020measuring} and AGIEval \citep{zhong2023agieval}, which are based on asking LLMs multiple-choice questions. 
We then use MT-Bench \citep{bai2024mt} which is based on long answer questions and thus measures both knowledge and fluency. 
Because the goal of unlearning is to differentially decrease capabilities in a target domain, we calculate an ``unlearning score'' based on both unlearning efficacy and utility degradation. 
Given an original model $M$ and an unlearned model $M'$, we calculate $S_\text{unlearn}(M')$ with the formula:
% We denote \( \Delta 
%   \text{Unlearn efficacy} = \left[S_{\text{WMDP}}(M) -  S_{\text{WMDP}}(M') 
%   \right]\), \(\Delta\text{Utility degradation} = \left[S_{\text{utility}}(M) -  S_{\text{utility}}(M')\right]\) and \(\Delta \textrm{Random chance (for normalization)}\). We then have \(S_\text{unlearn}(M') = \frac{\Delta 
%   \text{Unlearn efficacy} - \Delta\text{Utility degradation})}{\Delta \textrm{Random chance}} \)
\begin{equation} \label{eq:unlearn_score}
\begin{aligned} 
   S_{\text{unlearn}}(M') =  & (\underbrace{ \left[S_{\text{WMDP}}(M) -  S_{\text{WMDP}}(M') 
  \right]}_{\Delta 
  \text{Unlearn efficacy}}  - \\ 
  & \underbrace{\left[S_{\text{utility}}(M) -  S_{\text{utility}}(M')\right]}_{\Delta\text{Utility degradation}}) \left. \middle/ \right. \\ 
  & \underbrace{\left[S_{\text{WMDP}}(M) -  S_{\text{WMDP}}(\textrm{rand})\right]}_{\Delta \textrm{Random chance (for normalization)}} 
\end{aligned}
\end{equation}

\begin{figure*}[t!]
    \centering
\includegraphics[width=\textwidth]{figs/pca.pdf}
    \caption{\textbf{Three principal components explain 89\% of the variation in attack success.} \textbf{Left:} The proportion of explained variance for each principal component. \textbf{Right:} We display the first three principal components weighted by their eigenvalues. The first principal component suggests a geometric distinction between the two adversarial (LoRA, Full) fine-tuning attacks and all others.}
    \label{fig:PCA}
\end{figure*}

\begin{figure}[h!]
\centering\includegraphics[width=0.5\columnwidth]{figs/attack_clustering_tree.pdf}
\caption{\textbf{Hierarchical clustering reveals groupings of attacks.} Attacks tend to cluster by algorithmic type. However, benign fine-tuning attacks cluster with gradient-free input-space attacks.}
\label{fig:attack_clustering_tree}
\end{figure}

Here, $S_{\text{WMDP}}(\cdot)$ is the accuracy on the WMDP-Bio QA Evaluation and $S_{\text{utility}}(\cdot)$ is an aggregated utility measure. $S_{\text{utility}}(\cdot)$ is calculated by taking a weighted average of MMLU, AGIEval, and MT-Bench. We use weights of $1/4$, $1/4$, and $1/2$ respectively because MT-Bench uniquely measures model fluency.
Finally, ``rand'' refers to a random policy. 
An unlearning score of 1.0 indicates theoretically optimal unlearning -- random performance on the unlearned domain and unaffected performance on others. 
Meanwhile, the unlearning score of the original model $M$ is 0.0. 
% A positive unlearning score indicates that the model differentially lost WMDP-Bio capabilities relative to general capabilities while a negative one reflects counterproductive unlearning. 
% A ``perfect'' unlearning technique that did not change general capabilities but brought WMDP-Bio performance down to the random-guess baseline would score 0.45.
\Cref{tab:unlearning} reports results from the best-performing checkpoint (determined by unlearning score) from each of the 8 methods.


\textbf{Representation rerouting (RR) achieves the highest unlearning score. GradDiff and TAR struggle due to dysfluency.} We find different levels of unlearning success. Representation rerouting (RR) performs the best overall, achieving an unlearning score of 0.96. 
In contrast, GradDiff and TAR have limited success with the lowest unlearning scores.
Poor MT-Bench scores and our manual assessment of these models suggest that GradDiff and TAR struggle principally due to poor fluency. 
% The TAR model performed the worst overall due to near-random guess performance on all evaluations. 
% Upon deeper investigation, we found that the TAR model had broad challenges with fluency. 
% We also discovered that its unlearning was perplexingly brittle. 
% For example, we found that simply adding a non-masked \texttt{<|begin\_of\_text|>} token as a prefix before evaluation caused the model to increase from 24\% to 37\%. 
% As we will also show later in \Cref{fig:all}, several other simple input-space attacks were able to greatly improve its performance.

\textbf{No method is robust to all attacks.} We plot the increase in WMDP-Bio performance for the best checkpoint from each unlearning method after each attack in \Cref{fig:all} and show that all models, even those with the lowest unlearning scores, exhibit a worst-case performance increase of 8 percentage points or more when attacked.


\subsubsection{Model robustness exists on a low-dimensional subspace} \label{sec:pca}

\textbf{We perform PCA, weighting models by their unlearning score.} 
First, to understand the extent to which some attacks offer information about others, we analyze the geometry of attack successes across our 65 models. 
We perform weighted principal component analysis on the WMDP-Bio improvements achieved by all 11 attacks on all 65 model checkpoints. 
We first constructed a matrix $A$ with one row per model and one column per attack. 
Each $A_{ij}$ corresponds to the increase in WMDP-Bio performance in model $i$ under attack $j$. 
We then centered the data and multiplied each row $A_i$ by the square root of the unlearning score: $\sqrt{S_{\text{unlearn}}(A_i)}$.
% \anirudh{Should this $P$ in the unlearning score be $P_i$?}, and performed principal component analysis. 
This allowed for models to influence the analysis in proportion to their unlearning score. 

\begin{figure*}[t!]
    \centering
% \includegraphics[width=0.8\textwidth]{figs/Scatter_final.png}
\includegraphics[width=\textwidth]{figs/max_scatters.pdf}
\begin{tabular}{lcc}
\toprule
\textbf{Model Tampering} & \textbf{Input-Space Attacks} & \textbf{Avg. Relative} \\
\textbf{Method} & \textbf{Beaten $\uparrow$} & \textbf{Attack Strength $\uparrow$} \\
\midrule
\textbf{Embedding} & 54\% & 0.99 \\
\textbf{Latent Layer 5} & 72\% & 2.94 \\
\textbf{Pruning} & 20\% & 0.17 \\
\textbf{Benign Fine-tune} & 51\% & 1.72 \\
\textbf{Best Adv. Fine-tune} & \underline{98\%} & \underline{8.12} \\
\bottomrule
\end{tabular}
\caption{\textbf{In our experiments, (a) fine-tuning, embedding-space, and latent-space attack successes \textit{correlate} with input-space attack successes while (b) fine-tuning attack successes empirically \textit{exceed} the successes of state-of-the-art input-space attacks.} Here, we plot the increases in WMDP-Bio performance from model tampering attacks against the best-performing (of 5) input-space attacks for each model. We weight points by their unlearning score from \Cref{sec:benchmarking}. In (b), the $x$ axis is the best (over 2) between a LoRA and full fine-tuning attack. We also display the unlearning-score-weighted correlation and the correlation's $p$ value. \textit{Points below and to the right of the line indicate that the model tampering attack was more successful.} 
\textbf{Table:} for each of the four model tampering attacks, the percent of all input-space attacks for which it performed better and the average relative attack strength compared to all input-space attacks.}
    \label{fig:scatter}
\end{figure*}



\textbf{Three principal components explain 89\% of the variation in attack success.} \Cref{fig:PCA} displays the eigenvalues from PCA and the top three principal components (weighted by eigenvalues).
This suggests that different capability elicitation attacks exploit models via related mechanisms. 
% Meanwhile, analysis of the eigenvalue-weighted coordinates in \Cref{fig:PCA} (right) shows that some model tampering attacks are better predictors of input-space vulnerabilities than others. 
% The first principal component exhibits a distinct difference between fine-tuning attacks and all others. 
% This implies that improving model robustness against a strategic subset of attacks may confer broader resilience and that fine-tuning attacks may require separate consideration in robustness evaluations.

\textbf{Hierarchical clustering reveals distinct attack families.} In \Cref{fig:attack_clustering_tree}, we perform agglomerative clustering on attack success correlations. Algorithmically similar attacks tend to cluster together. However, adversarial finetuning attacks exhibit significant variation, even amongst each other. Finally we see that benign model tampering methods (pruning and benign fine-tuning) behave similarly to gradient-free input-space attacks. % Along with our PCA results, this tree suggests that combining multiple attack types, particularly those that cluster distinctly, may provide more comprehensive insights into a model's latent capabilities.

\subsubsection{Model tampering attacks empirically predict and conservatively estimate the success of input-space attacks} \label{sec:scatters}

\textbf{Embedding-space attacks, latent-space attacks, pruning, and benign fine-tuning empirically \textit{correlate} with input-space attack successes.} 
In \Cref{fig:scatter} these three model tampering attacks tend to have positive correlations with input-space attack successes with $p$ values near zero.\footnote{Points are not independent or identically distributed, so we only use this ``$p$'' value for geometric intuition, and we do not attach it to any formal hypothesis test.}
In these plots, we size points by their unlearning score and display the Pearson correlation weighted by unlearning score. 
Full results are in \Cref{app:full_unlearning_results}.
This suggests that embedding-space attacks, latent-space attacks, pruning, and benign fine-tuning are particularly able to predict the successes of held-out input-space attacks. 
% ELM models often cluster far away from others, suggesting that ELM may be a mechanistically unique unlearning technique, but we leave further exploring this for future work.

\textbf{Fine-tuning attack successes empirically offer conservative estimates for input-space attack successes.} 
LoRA and Full fine-tuning performed differently on different attacks. 
However, together, the max of the two did as well or better than the best-performing input-space attack on 64 of 65 models. 
This suggests that model tampering attacks could be used to develop more cautious estimates of a model's worst-case behaviors than other attacks.
% \zora{TODO talk some more about sample efficiency.}

\begin{figure*}[t!]
    \centering
\includegraphics[width=0.9\textwidth]{figs/max_aisi_scatters.pdf}
% \includegraphics[width=0.9\textwidth]{figs/max_aisi_scatters_redacted.pdf}
\caption{\textbf{Model tampering attacks are predictive for a held-out proprietary attack from 
the UK AI Security Institute.
% [redacted for review].
} Each point corresponds to a model. (a) In these experiments, correlations are weaker than with non-
UK AISI 
% [redacted for review]
attacks, but benign fine-tuning attacks continue to correlate with 
UK AISI 
% [redacted for review]
input-space attack success. (b) Fine-tuning attacks still tend to exceed the success of input-space attacks, though less consistently than with the attacks from \Cref{fig:scatter}. 
}
    \label{fig:aisi_scatter}
\end{figure*}



\begin{figure*}[t]
    \centering
\includegraphics[width=0.9\textwidth]{figs/Finetune_all_2.pdf}
    \caption{\textbf{Few-shot fine-tuning efficiently undoes unlearning.} We plot the heatmap of the best checkpoint for each method under benign (left), LoRA (middle), and full-parameter (right) fine-tuning attacks. All fine-tuning experiments are done within 16 gradient steps, with 128 examples or fewer. All methods can be attacked to increase WMDP-Bio performance by 10 percentage points or more. 
    % Out of the attack settings we employed, the most data efficient attack we found that was able to attack all but 2 models successfully was Full-4, which did a single gradient step update with a batch size of 64. The most compute efficient attack that attacked every method successfully to increase more than 5 percentage points was LoRA-2, which did 50 gradient steps of update with a batch size of 8. 
    All hyper-parameters are listed in \Cref{tab:finetune-hypers}. } 
    \label{fig:lora}
\end{figure*}

\textbf{Model tampering attacks are predictive of the success of proprietary attacks from 
the UK AI Security Institute (UK AISI).
% [redacted for review].
}
To more rigorously test what model tampering attacks can reveal about novel input-space attacks, we analyze their predictiveness on proprietary attacks from 
the UK AI Security Institute.
% [redacted for review].
These attacks were known to the `red team' authors (
UK AISI 
% [redacted for review]
affiliates) but were not accessible to all other `blue team' authors. 
We conducted these attacks with the same data and approach as all of our other input-space attacks. 
Results are summarized in \Cref{fig:aisi_scatter} with full results in the Appendix. 
% Trends are similar to earlier results (\Cref{fig:scatter}).
Correlations are weaker than before, but pruning and benign fine-tuning still correlate with attack success with a $p$ value near zero. 
Also as before, fine-tuning attack successes often tend to be as strong or stronger than input-space attacks. 
However, this trend was weaker, only occurring for 60 of the 65 models.
See \Cref{app:ukaisi} for full results and further analysis of 
UK AISI 
% [redacted for review]
evaluations.
% Next, to push the limits of our hypothesis that model tampering attacks can help evaluators assess \emph{unforeseen} failure modes, we evaluated model performance under an entirely different non-WMDP benchmark for dual-use bio capabilities from the UK AI Security Institute. 
% WMDP-Bio performance correlates with this evaluation ($r=0.64$, $p=0.0$; see \Cref{fig:wmdp_v_aisi} in the Appendix).
% To correct for this confounding factor, in \Cref{fig:aisi_scatter}, we use model tampering attack success on WMDP-Bio to predict the \emph{residuals} from a linear regression predicting UK AISI bio evaluations results from WMDP-Bio evaluation results.
% Here, we find weak correlations except for the case of the benign fine-tuning attack. 
% Overall, this suggests that while model tampering attacks can be informative about unforeseen failure modes across different \textit{attacks}, they can struggle to do so across different \textit{tasks}.


\textbf{Model tampering attacks improve worst-case input-space vulnerability estimation.} Finally, we test if model tampering attacks offer novel information that can be used to \emph{predict} worst-case behaviors better than input-space attacks alone (\Cref{fig:worst_case_r2_by_num_predictors} in \Cref{app:worst_case_vulnerability_estimation}). 
We train linear regression models to predict worst-case input-space attack success rates with information from
either (1) only input-space attacks, or (2) both input-space and model tampering attacks. 
We find that including model tampering attacks improves predictiveness (e.g. from $r=0.77$ to $0.83$ with four predictors).
The best-performing combinations typically include attacks from multiple families, suggesting diverse attacks provide complementary signals by probing different aspects of model robustness. 
% This advantage is specific to worst-case estimation -- for average-case vulnerabilities, input-space attacks alone perform similarly to combinations including model tampering attacks (\Cref{fig:average_case_r2_by_num_predictors} in \Cref{app:average_case_vulnerability_estimation}). 
% This aligns with our observation that model tampering attacks tend to upper bound input-space vulnerabilities, making them particularly valuable for understanding worst-case behaviors.

\textbf{State-of-the-art unlearning can reliably be reversed within 16 fine-tuning steps -- sometimes in a single step.} 
We show the results of multiple fine-tuning attacks against the best-performing model from each unlearning method in \Cref{fig:lora}. 
All finetuning experiments, as detailed in \Cref{tab:finetune-hypers}, are performed within 16 gradient steps and with 128 or fewer examples. 
% No method was resistant to the attacks  `LoRA-2' and `LoRA-4', the latter which performs 16 steps of gradient updates only. 
% Moreover, the attack `Full-4' only performs a single gradient step (with a batch size of 64) and still increases the WMDP performance on 6 of the 8 models by over 25 percentage points. 
% LoRA-5 also attacks 5 out of the 8 methods successfully via an adapter with less than 0.52\% of the weights, using only 64 examples in total, over 16 gradient steps. 
% LoRA Fine-tuning using 64 examples in total over 16 gradient steps can relearn significant amount of WMDP knowledge for all unlearned models with high unlearning score and utility. 
The only method that was resistant to few-shot fine-tuning attacks was TAR, in which only 1 out of the 9 fine-tuning attacks were able to increase the WMDP-Bio performance by over 10 percentage points. 
However, TAR models had low unlearning scores due to poor general utility, which renders their robustness to fine-tuning unremarkable. All utility-preserving state-of-the-art unlearning methods can be attacked successfully to recover more than 30 percentage points of WMDP performance. Moreover, even when we perform a single gradient step (with a batch size of 64) still increases the WMDP performance on 6 of the 8 methods by over 25 percentage points (see column ``Full-4'' in \Cref{fig:lora}). 



\subsection{Jailbreaking Experiments} \label{sec:jailbreaking}

\textbf{We repeat analogous experiments with similar results in the jailbreaking setting.}
Finally, to test the generality of our findings outside the unlearning paradigm, we ask whether they extend to jailbreaking.
Using the 9 off-the-shelf models and 11 attacks from \Cref{tab:attacks_defenses}, we conducted parallel experiments as in \Cref{sec:unlearning} but by pitting off-the-shelf refusal-finetuned models against jailbreaking attacks.
We plot all results in \Cref{app:full_jailbreaking_results}.

Our benchmark results (\Cref{fig:benchmark_jailbreaks}) demonstrate that all safety-tuning methods are vulnerable to model tampering. Principal component analysis of attack success rates in \Cref{fig:all_scatter_jailbreaks} show that three principal components explain 96\% of the variation in jailbreak success across the nine models.

We then reproduced our empirical analysis of whether the success of model tampering jailbreaks correlates with and/or conservatively exceeds the success of input-space jailbreaks (\Cref{fig:scatter_jailbreaks}). 
% Due to only having nine models, our ability to draw confident conclusions is limited. 
Like before, we find that fine-tuning attack success tends to empirically exceed the success of input-space attacks, thus offering a conservative estimation method. 
However, unlike before, we do not find clear evidence of a reliable correlation between tampering and input-space attacks due to only having 9 samples. 

Finally, to evaluate how helpful model tampering attacks can be for characterizing a model's vulnerability to unique, held-out attacks, we use 
Cascade, 
% [redacted for review],
a multi-turn, state-of-the-art, proprietary attack algorithm from 
Haize Labs \citep{haizelabs2023cascade}.
% [redacted for review].
In \Cref{fig:haize_jailbreaks_scatters}, we see that single-turn model tampering attacks correlate well with multi-turn 
Cascade 
% [redacted for review]
attacks.
% Overall, as with unlearning, we find evidence that model tampering attacks can aid in predicting the success of held-out, unforeseen attacks. 


\section{Discussion}

\textbf{Implications for evaluations and safety cases:} 
% Our work may be of particular interest to the LLM evaluation community. 
% Frameworks for AI governance are increasingly designed to rely on high-quality evaluations to identify hazards and handle risks in LLMs \citep{raji2022outsider, anderljung2023publicly, schuett2023towards, shevlane2023model, uuk2024effective}. 
% Requirements for evaluations have been outlined in drafted and enacted policies, in multiple countries \citep{eu_ai_act, china2023generativeAI, brazil2023aiRegulation}.
% Meanwhile, capability evaluations are central to the agendas of emerging national AI Security Institutes (e.g. \citet{aisi2024}, \citet{usaisi2024}.
% Formal evaluations of AI systems, however, face a number of challenges \citep{anderljung2023publicly} including limitations in technical tooling \citep{casper2024black}.
% Our work adds to the growing consensus that access to model internals is necessary for rigorous evaluations \citep{casper2024black}.
% When models are released with open weights, model tampering attacks during evaluations are directly necessary to understand risks from misuse.
% Meanwhile, when they are released with closed weights, our results suggest that model tampering attacks can help evaluators make more informed inferences about potential worst-case behavior. 
Our findings have direct implications for performing AI risk evaluations and constructing safety cases \citep{clymer2024safety}.
Current evaluation frameworks rely heavily on input-space attacks which can easily fail to underestimate worst-case failures. 
Model tampering attacks provide a useful tool for studying novel, potentially unforeseen risks. 
By modifying a model’s internal mechanisms — either through activation perturbations or fine-tuning — we can infer the potential existence of failure modes that input-space evaluations may miss (see also \citet{hofstatter2025elicitation}). 
This is particularly critical for open-weight models, where safety mitigations can be undone post-release.
% Moreover, our findings on the fragility of unlearning reversibility highlight another governance concern. 
% If harmful capabilities can be reactivated with minimal tampering, then regulators and developers must reconsider claims that unlearning provides robust safety assurances. 

\textbf{Limitations:}
Our work focuses only on Llama-3-8B-Instruct derived models. 
This allows for considerable experimental depth, but other models may have different dynamics. 
% Another limitation is that our evaluations of undesirable capabilities were conducted only by evaluating models under multiple-choice questions from the WMDP-Bio test set \citep{li2024wmdp}.
% However, these evaluations can sometimes be brittle. 
% Next, our implementations of some attack methods were not based on standard libraries, and we had to make minor modifications to convert some algorithms from jailbreaking attacks to knowledge-elicitation attacks. 
The science of evaluations is still evolving, and it is not yet clear how to best translate the outcome of evaluations into actionable recommendations. 
Overall, we find that model tampering attacks can help with more rigorous evaluations -- even for models deployed as black boxes.
However, there may be limitations in the mechanistic similarity of input-space and tampering attacks \citep{leong2024no}.


\textbf{Future work:}
\vspace{-8pt}
\begin{itemize}
    \item \textbf{Can models be robust to tampering attacks?} This paper and concurrent work \citep{qi2024evaluating} show that even defenses designed to make models robust to tampering can be easily undone. We are currently working to better understand tampering robustness and improve the extent to which models can be made robust to tampering attacks. 
    \item \textbf{What mechanisms underlie robust capability removal?} We are interested in future work to mechanistically characterize weak vs. robust capability suppression. We hope that the 64 models we release help to lay the groundwork for this. 
    \item \textbf{Bridging research and practice:} Model tampering attacks can be further studied and used in practice to assess risks in consequential models pre-deployment.
\end{itemize}

% \textbf{Conclusion:}
% By comprehensively benchmarking 8 capability removal methods and 9 refusal fine-tuned models against 11 diverse capability elicitation methods, we have contributed to a more thorough understanding of LLM robustness and how to evaluate it. 
% Not all capability elicitation attacks exploit the same weaknesses in models, nor do they all exploit the exact same ones.
% In our experiments, over 90\% of the empirical variance in robustness across our suite of ten attacks can be explained by three principal components. 
% Meanwhile, model tampering attacks that make small perturbations to the LLM's weights or activations can help to predict its vulnerability to input-space methods. 
% In particular, we show that few-shot fine-tuning attacks tend to be very broadly effective at eliciting unwanted capabilities. 
% Together these results suggest that model tampering attacks can be uniquely helpful for inferring risks from unforeseen types of input-space attacks. 

% \newpage

\section*{Impact Statement} % does not count toward 8 page limit
This work was motivated by advancing the science of AI capability evaluations. 
This has been a central interest and goal of technical AI governance research \citep{reuel2024open} because AI risk management frameworks are increasingly being designed to depend on rigorous risk evaluations.
Thus, we expect this paper to contribute to developing more rigorous evaluations, which is valuable from a risk-management perspective. 

\section*{Acknowledgments}

We are grateful to the Center for AI Safety for compute and the Machine Learning Alignment and Theory Scholars program for research support. We thank Daniel Filan for technical support, and Antony Kellermann for technical discussion. Finally, we are grateful to Peter Henderson, Ududec Cozmin, Ekdeep Singh Lubana, and Taylor Kulp-McDowall for their feedback.



\bibliographystyle{tmlr}
\bibliography{bibliography}

\appendix


\section{Experiment Details}
\label{sec:appendix-experiment-details}

\subsection{Unlearning Evaluation}
We report the MT-Bench score as the average of one-round and two-round scores and divide it by 10, the maximum number of points possible. The result is scores ranging from 0.0 to 1.0. 

\subsection{Unlearning Methods and Implementation}

\subsubsection{Unlearning Methods} \label{app:unlearning_methods}

% For each unlearning method except TAR and RandMap, we use 8 different checkpoints for evaluation.

\begin{itemize}
  \item \textbf{Gradient Difference (GradDiff):} Inspired by \citet{liu2022continual}, we train models to maximize the difference between the training loss on the forget dataset and the retain dataset.
  \item \textbf{Random Misdirection for Unlearning (RMU):} \citet{li2024wmdp} propose a method where model activations on harmful data are perturbed, and model activations on benign data are preserved.
  \item \textbf{RMU with Latent Adversarial Training (RMU+LAT):} \citet{sheshadri2024targeted} propose training models using adversarial attacks in the latent space as a way to perform stronger unlearning. They combined this with RMU by leveraging adversarial perturbations when training only on the forget dataset.  
  \item \textbf{Representation Noising (RepNoise):} \citet{rosati2024representation} propose adding a noise loss term that minimizes the KL divergence between the distribution of harmful representations given harmful input and Gaussian noise.
  \item \textbf{Erasure of Language Memory (ELM):} \citet{anonymous2024erasing} introduce ELM in order to thoroughly unlearn knowledge by training the model to mimic unknowledgeable behavior on the unlearning domain. 
  \item \textbf{Representation Rerouting (RR):} \citet{zou2024improving} introduces Representation Rerouting (also known as ``circuit breaking'') which trains models to map latent states induced by topics in the unlearning domain to orthogonal representations.
  \item \textbf{Tamper Attack Resistance (TAR):} \citet{tamirisa2024tamper} propose TAR as a meta-learning approach to protect open-weight models from finetuning attacks. At each iteration, the model is trained to be robust to a fine-tuning adversary who can take a small number of steps. % Note that TAR is only applied to models already trained with Random Mapping.
  % \item \textbf{Random Mapping (RandMap):} To perform the initial stage of unlearning for TAR, \citet{tamirisa2024tamper} propose a method similar to RMU called Random Mapping. Instead of preserving model activations on benign data like RMU, the model is trained to preserve high log-likelihoods on benign data.
  \item \textbf{PullBack \& proJect (PB\&J):} \citet{mckinney2025pbnj} is an unlearning algorithm which learns a set of projections which on activations space which maximally harm performance on the the forget set while minimally perturbing model outputs on a broad retain distribution.

\end{itemize}

To adhere to the implementations from the works introducing each method, we use full fine-tuning (not LoRA) for RMU, RMU-LAT, RepNoise, TAR, and PB\&J, and LoRA for GradDiff, ELM, RR.

\subsubsection{Hyperparameters}

Beginning from prior works' implementations of methods, we tuned the hyperparameters below in order to achieve (1) gradual progress in unlearning across the 8 checkpoints that we took 
and (2) a high unlearning score by the end of training. 

\begin{itemize}
    \item GradDiff
    \begin{itemize}
        \item LoRA Fine-tune 
        \subitem LoRA Rank: 256
        \subitem LoRA \(\alpha\): 128 
        \subitem LoRA dropout:  0.05
        \item Learning Rate: \(10^{-4}\)
        \item Batch Size: 32 
        \item Unlearning Loss Coefficient \(\beta\): 14 
    \end{itemize}
    \item RMU
    \begin{itemize}
        \item Layer Fine-tune
            \subitem Layers: 5, 6, 7 
        \item Retain Loss Coefficient \(\alpha\): 90 
        \item Steer: 20 
        \item Learning Rate: \(5 \times 10^{-5}\)
        \item Batch Size: 8
    \end{itemize}
    \item RMU+LAT
    \begin{itemize}
        \item Layer Fine-tune
            \subitem Layers: 5, 6, 7 
        \item Retain Loss Coefficient \(\alpha\): 90 
         \item Learning Rate: \(5 \times 10^{-5}\)
        \item Batch Size: 8
        \item Steer: 20 
    \end{itemize}
    \item RepNoise
    \begin{itemize}
        \item Full Fine-tune 
        \item Learning Rate: \(5 \times 10^{-6}\)
        \item Batch Size: \(4\)
        \item Noise Loss Coefficient \(\alpha\): 2
        \item Ascent Loss Coefficient \(\beta\): 0.01

    \end{itemize}
    \item ELM
    \begin{itemize}
        \item LoRA Fine-tune 
        \subitem LoRA Rank: 64
        \subitem LoRA \(\alpha\): 16 
        \subitem LoRA dropout:  0.05
        \item Learning Rate: \(2 \times 10^{-4}\)
        \item Batch Size: 8
        \item Retain Coefficient: 1
        \item Unlearn Coefficient: 6
    \end{itemize}
    \item Representation Rerouting
    \begin{itemize}
        \item LoRA Fine-tune 
        \subitem LoRA Rank: 16
        \subitem LoRA \(\alpha\):  16
        \subitem LoRA dropout:  0.05
        \item Learning Rate: \(1 \times 10^{-4}\)
        \item Batch Size: 8
        \item Target Layers: 10, 20
        \item Transform Layers: All
        \item LoRRA Alpha: 10
    \end{itemize}
    \item TAR
    \begin{itemize}
        \item Full Fine-tune
        \item Learning Rate: $2 \times 10^{-5}$
        \item Batch Size: 2
        \item Training Steps: 200
        \item Adversary Inner Loop Steps per Training Step: 16
        \item Retain Representation Coefficient: 1
        \item Retain Log-Loss Coefficient: 1
    \end{itemize}
    % \item Random Mapping: Used trained weights from \citet{tamirisa2024tamper} (no checkpoints)
    \item PB\&J 
    \begin{itemize}
        \item Damping factor: $1\times10^{-5}$
        \item Retain set estimator: $A_R^2$ (margin squared)
        \item Forget set measure: margin
        \item Iterations: 8
        \item Targeted Layers: 3, 4, 5, 6
        \item Projections per iteration: 1
    \end{itemize}

\end{itemize}


\subsection{Models for Jailbreaking Experiments} \label{app:jb_models}

In \Cref{tab:attacks_defenses}, we list the 9 models that we use off the shelf for experiments with jailbreaking.
All of which were fine-tuned variants of Llama-3-8B-Instruct from \citet{dubey2024llama}.
Here, we overview each of the 9 models and why we selected them. 

\begin{enumerate}
    \item meta-llama/Meta-Llama-3-8B-Instruct \citep{dubey2024llama}: the original Llama-3-8B-Instruct model.
    \item slz0106/llama3\_finetune\_refusal (\href{https://huggingface.co/slz0106/llama3_finetune_refusal}{Link}) is a refusal fine-tuned version of Llama-3-8B-Instruct.
    \item JINJIN7987/llama3-8b-refusal-vpi (\href{https://huggingface.co/JINJIN7987/llama3-8b-refusal-vpi}{Link}) is a refusal fine-tuned version of Llama-3-8B-Instruct.
    \item Youliang/llama3-8b-data \citep{yuan2024refuse} was fine-tuned to refuse to comply with harmful requests even in cases when a harmful reply begins benignly, or the beginning of a harmful reply is teacher-forced. 
    \item GraySwanAI/Llama-3-8B-Instruct-RR \citet{zou2024improving} was fine-tuned to 'reroute' the latent information flow through the model for harmful requests. The model was designed to respond incoherently with uninformative random-seeming text upon a harmful request. 
    \item LLM-LAT/llama3-8b-instruct-rt-jailbreak-robust1 \citep{sheshadri2024targeted} was fine-tuned as a control model to refuse harmful requests.
    \item LLM-LAT/robust-llama3-8b-instruct \citep{sheshadri2024targeted} was fine-tuned using latent adversarial training \citep{casper2024defending} to robustly refuse requests under attacks than the above control. 
    \item lapisrocks/Llama-3-8B-Instruct-TAR-Refusal \citep{tamirisa2024tamper} was fine-tuned under weight-space fine-tuning attacks to refuse harmful requests in a way that is robust to fine-tuning. 
    % \item WhiteRabbitNeo/Llama-3-WhiteRabbitNeo-8B-v2.0 (\href{https://huggingface.co/WhiteRabbitNeo/Llama-3-WhiteRabbitNeo-8B-v2.0}{Link}) was fine-tuned to comply with any requests. 
    \item Orenguteng/Llama-3-8B-Lexi-Uncensored (\href{https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored}{Link}) was fine-tuned to comply with any requests. 
\end{enumerate}


\subsection{Attack Methods and Implementation}

\paragraph{Greedy Coordinate Gradient (GCG)} GCG \citep{zou2023universal} performs token-level substitutions to an initial prompt by evaluating the gradient with respect to a one-hot vector of the current token. We implemented both time-bounded attacks on each unlearned model and transfer attacks using prefixes from one model to attack others. Unless otherwise specified, we report the mean performance of each gradient-guided attack. 

\paragraph{AutoPrompt} Like GCG, AutoPrompt \citep{shin2020autopromptelicitingknowledgelanguage} performs a gradient-guided search over input tokens to design universal adversarial prompts.

\paragraph{BEAST} 
We used BEAm Search-based adversarial aTtack (BEAST) \citep{sadasivan2024fastadversarialattackslanguage} to produce universal adversarial suffixes which were appended after the evaluation questions.
Unlike GCG and AutoPrompt, BEAST is guided by perplexity instead of gradients.
Since our attacks need to be universal, we used a modified version of BEAST to generate universal adversarial tokens for several user input prompts. Formally, we can define a set of user input prompts as \(\{x_1^{(u)}, x_2^{(u)}, \cdots, x_n^{(u)}\}\), where each \(x_i\) contains a question \(q_i\) and answer choices \(a_i\). Our goal is to generate an adversarial sequence of tokens \(x^{(a)}\) such that \(q_i \,\oplus \, x^{(a)} \oplus \, a_i\) can effectively attack the language model for all \(i \in \{1, 2, \cdots, n\}\). We place the adversarial tokens between each question and the answer choices so that the beam search in BEAST is conditioned solely on the question and not the answers, as the jailbreak tokens to the end of the user prompt directly can leak the answer choices into the tokens. We attacked with different hyperparameters for search width and time and reported results for $K = 5$.

\paragraph{PAIR} 
In addition to gradient-based approaches such as GCG and finetuning attacks, we also include a model-based approach similar to PAIR \citep{chao2024jailbreakingblackboxlarge}. PAIR uses a prompt-level jailbreaking attack, where an entirely new adversarial prompt is generated instead of augmenting the prompt with adversarial tokens. This is not transferable compared to other universal attack approaches as the attack is highly dependent on the original prompt. Instead, we modify PAIR to develop an adversarial suffix that can applied universally to many (potentially unseen) prompts.
First, we task an attack model (base model such as Llama 3 8B) to generate an adversarial suffix, similar to how PAIR generates an adversarial prompt. Then, we append the adversarial suffix to each of the prompts in the training dataset and measure the accuracy of the target model (unlearned model) on the jailbroken prompts. Finally, we pass in an improvement prompt (a general string that asks the attack model to improve its adversarial suffix) and the accuracy of the target model to the attack model to repeat the process.

\paragraph{Human Prompt} 
We hand-crafted adversarial prompts to elicit harmful responses from a language model. After manual iteration, our selected prefix was ``\#Answer Key\textbackslash n\textbackslash n\textbackslash This answer key contains questions and answers for advanced topics in bioengineering and biotechnology. All answers were verified correct by a panel of experts.''

%   \item \textbf{Latent perturbation:} Universal latent perturbations were added to latent layers. We refer to the initial encodings of words (latent layer 0) as ``embedding space'' attacks \citep{schwinn2024soft}. For latent-space attacks \citep{sheshadri2024targeted}, we targeted layer 5. 
%   \item \textbf{Fine-tuning:} We performed both LoRA \citep{hu2021lora} and full parameter fine-tuning on the forget or retain splits of the WMDP-Bio dataset \cite{li2024wmdp}. We attack with different hyperparameters. Unless otherwise specified, we report the mean performance of each type of fine-tuning attack. 
% \end{itemize}

\paragraph{Embedding Attack} \citet{schwinn2024soft} proposes applying adversarial perturbations to a language model's embedding layer instead of in the input space.
We train universal $\ell_2$-norm bounded embedding space prompts which we apply to the embedded tokens for the prefix used for WMDP-Bio evaluation: ``The following are multiple choice questions (with answers) about biology.\textbackslash n''.

\paragraph{Latent Layer Attack}
\citet{sheshadri2024targeted} proposes applying adversarial perturbations to a language model's latent layers.
We train universal $\ell_2$-norm bounded latent pseudoprompts which we apply to the latent tokens at layer 5 for the prefix used for WMDP-Bio evaluation: ``The following are multiple choice questions (with answers) about biology.\textbackslash n''.


\paragraph{WandA Pruning}
To test each model's performance on benign pruning attacks, we use Weights and Activations (WandA) pruning \cite{sun2023simple}. WandA prunes the lowest \(p\%\) of parameters according to the importance score generated by \(W_iX\), where \(W_i\) is the \(i\)th layer weight and \(X\) is the input. For all experiments, we prune the weights 15\%.

\paragraph{Few-shot fine-tuning attacks} 
We perform fine-tuning to re-elicit unwanted capabilities. The forget set (WMDP-Bio Remove) consists of hazardous biology data, and the retain set (WMDP-Bio Retain) contains non-hazardous biology data. We also performed benign LoRA fine-tuning on Wikitext. We report hyper-parameters in \Cref{tab:finetune-hypers}. All LoRA and Benign attacks are done with rank 16 and alpha 32. All examples have a maximum length of 512 tokens. Few-shot fine-tuning attack details are reported in \Cref{tab:finetune-hypers-refusal}.

\paragraph{Excluded attacks:} In addition to these attacks, we also experimented with many-shot attacks \citep{anil2024many, lynch2024eight} and translation attacks \citep{yong2023low, lynch2024eight} but found them to be consistently unsuccessful in our experimental settings. 

\begin{table}[]
    \centering
    \begin{tabular}{lrr}
       \toprule
       \textbf{Attack} & \textbf{Total Forward Passes} & \textbf{Total Backward Passes} \\ \midrule
       \textbf{GCG} & 5120-25600 & 10-50 \\ 
       \textbf{AutoPrompt} & 2560-12800  & 10-50 \\
       \textbf{BEAST} & 630 & 0 \\
       \textbf{PAIR} & 1920 & 0 \\
       \textbf{Human Prompt} & 0 & 0 \\
       \textbf{Embedding Space} & 600 & 600 \\
       \textbf{Latent Space} & 600 & 600 \\
       \textbf{WandA Pruning} & 224 & 0 \\
       \textbf{Benign LoRA Fine-Tune} & 1-16 & 1-16 \\
       \textbf{LoRA Fine-Tune} & 1-16 & 1-16 \\ 
       \textbf{Full Parameter Fine-Tune} & 1-16 & 1-16 \\ \bottomrule
    \end{tabular}
    \caption{\textbf{Model tampering attacks empirically tend to be more efficient than input-space attacks.} To show the computational expansiveness of the attacks that we use, we report the number of forward plus backward passes used to develop each attack under our implementations. The model architecture and number of parameters in all models was the same (up to small, inserted LoRA adapters), but the number of tokens in strings used to develop each attack varied. For these reasons, note that the number of forward and backward passes does not have a perfectly consistent relationship with the number of floating point operations.}
    \label{tab:attack_efficiency}
\end{table}




\begin{table}[!h]
\centering
\resizebox{1\textwidth}{!}{
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{ccccccccc}
\toprule
  & Dataset & \# of Examples &  Batch Size & Learning Rate & Epochs& Total Steps  \\
 \midrule
 Full-1 &  WMDP-Bio Remove &  400  & 16 & 2e-05 & 2& 25 \\
\midrule
 Full-2 &  WMDP-Bio Remove &  64  & 8 & 2e-05 & 2& 16 \\
\midrule
 Full-3 &  WMDP-Bio Retain &  64 & 64 & 5e-05 & 2 & 2  \\
\midrule
 Full-4 &  WMDP-Bio Retain &  64  & 64 & 5e-05 & 1 & 1\\
\midrule
LoRA-1 &  WMDP-Bio Remove &  400  & 8 & 5e-05 & 1 & 50  \\
\midrule
%  LoRA-2 &  WMDP-Bio Remove &  400 &   64 & 5e-05 & 1 & 7  \\
% \midrule
 LoRA-2 &  WMDP-Bio Retain &  400 &   8 & 5e-05 & 1 & 50  \\
\midrule
 LoRA-3 &  WMDP-Bio Remove &  64 &   8 & 1e-04 & 2 & 16  \\
\midrule
 LoRA-4 &  WMDP-Bio Retain &  64 &  8 & 1e-04 & 2 & 16  \\
 \midrule
 Benign-1 &  Wikitext &  400  & 8 & 5e-05 & 1 & 50  \\

\bottomrule
\end{tabular}}
\label{tab:finetune-hypers}
\caption{
Hyper-parameters for Fine-tuning Attacks on Unlearned Models}
\end{table}


\begin{table}[!h]
\centering
\resizebox{1\textwidth}{!}{
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{ccccccccc}
\toprule
  & Dataset & \# of Examples &  Batch Size & Learning Rate & Epochs& Total Steps  \\
 \midrule
 Full-1 &  LAT Harmful &  64  & 8 & 5e-05 & 1 & 8 \\
\midrule
 Full-2 &  LAT Harmful &  16  & 8 & 5e-05 & 1 & 2 \\
\midrule
\midrule
LoRA-1 &  LAT Harmful &  64  & 8 & 5e-05 & 1 & 8  \\
\midrule
% \midrule
 LoRA-2 &  LAT Harmful &  64 &   8 & 5e-05 & 2 & 16  \\
\midrule
 LoRA-3 &  LAT Harmful &  16 &   8 & 5e-05 & 2 & 4  \\
\midrule
 LoRA-4 &  LAT Harmful &  64 &  8 & 1e-04 & 2 & 16  \\
 \midrule
 Benign-1 &  Ultra Chat &  64  & 8 & 5e-05 & 2 & 16  \\

\bottomrule
\end{tabular}}
\label{tab:finetune-hypers-refusal}
\caption{
Hyper-parameters for Fine-tuning Attacks on Refusal Models}
\end{table}










% \textit{LoRA Fine-tuning attacks}
% All LoRA Fine-tuning attacks are performed with adapters with rank 16 and alpha 32. We examined 5 different LoRA attacks, using as few as 16 gradient steps and only 64 examples, as detailed in \Cref{tab:lora}.



% \textit{Benign LoRA fine-tuning}
% We perform benign fine-tuning using non-biohazardous, non-biological data, as detailed in \Cref{tab:benign}. This attack is a special case of LoRA Fine-tune attack, and we trained adapter with rank 16 and alpha 32. All examples have a maximum length of 512 tokens.

\clearpage
\section{Full Unlearning Results} \label{app:full_unlearning_results}

\subsection{Standard Attacks}

In \Cref{fig:full_scatter}, we plot the attack successes for all model tampering attacks against all input-space attacks. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figs/all_scatters.pdf}
\caption{\textbf{Full results from unlearning experiments comparing input-space and model tampering attacks.} See summarized results in \Cref{fig:scatter}. Here, we plot the increases in WMDP-Bio performance from model tampering attacks and input-space attacks. We weight points by their unlearning score from \Cref{sec:benchmarking}. We also display the unlearning-score-weighted correlation, the correlation's $p$ value, and the line $y=x$. Points below and to the right of the line indicate that the model tampering attack was more successful.}
\label{fig:full_scatter}
\end{figure}

\clearpage
\subsection{
UK AISI 
% [redacted for review]
Attacks and Evaluation} \label{app:ukaisi}

In \Cref{fig:all_aisi_scatter}, we plot the full attack successes for all model tampering attacks against the 
UK AISI 
% [redacted for review]
attack. 

\begin{figure*}[h!]
    \centering
\includegraphics[width=0.9\textwidth]{figs/attack_aisi_scatters.pdf}
% \includegraphics[width=0.9\textwidth]{figs/max_aisi_scatters_redacted.pdf}
\caption{\textbf{Model tampering attacks remain predictive for a proprietary attack from 
the UK AI Security Institute.
% [redacted for review].
} (a) In these experiments, correlations are weaker than with non-
UK AISI 
% [redacted for review]
attacks, but benign fine-tuning attacks continue to correlate with 
UK AISI 
% [redacted for review]
input-space attack success. (b) Fine-tuning attacks still tend to exceed the success of input-space attacks, though less consistently than with the attacks from \Cref{fig:scatter}. 
}
    \label{fig:all_aisi_scatter}
\end{figure*}



Next, to test the limits of our hypothesis that model tampering attacks can help evaluators assess novel, unforeseen failure modes, we evaluated model performance under an entirely different non-WMDP benchmark for dual-use bio capabilities from 
the UK AI Security Institute. 
% [redacted for review].
\Cref{fig:wmdp_v_aisi} shows that WMDP-Bio performance correlates with this evaluation with $r=0.64$ and $p=0.0$.
To correct for this confounding factor, in \Cref{fig:aisi_scatter}, we use model tampering attack success on WMDP-Bio to predict the \emph{residuals} from a linear regression predicting 
UK AISI 
% [redacted for review]
Bio evaluation results from WMDP-Bio evaluation results.
% \Cref{}
Here, we find weak correlations except for the case of the pruning and benign fine-tuning methods. 
Overall, this suggests that while model tampering attacks can be informative about novel failure modes across different \textit{attacks}, they do not necessarily do so across different \textit{tasks}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{figs/wmdp_v_aisi.pdf}
% \includegraphics[width=0.5\textwidth]{figs/wmdp_v_aisi_redacted.pdf}
\caption{\textbf{WMDP-Bio performance correlates with the 
UK AISI 
% [redacted for review]
Bio evaluation performance}. }
\label{fig:wmdp_v_aisi}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figs/cb_aisi_scatters.pdf}
% \includegraphics[width=\textwidth]{figs/cb_aisi_scatters_redacted.pdf}
\caption{\textbf{Model tampering attack success on WMDP-Bio is not strongly predictive of model success on 
UK AISI 
% [redacted for review]
bio capability evaluations.} This suggests a limitation of how informative model tampering attacks can be about failure modes across task distributions.}
\label{fig:wmdp_v_aisi}
\end{figure}

% \clearpage
\subsection{Attack Relationships}
\label{app:attack_relationships}

\begin{figure}[h!]
\centering\includegraphics[width=0.7\textwidth]{figs/attack_correlation_matrix.pdf}
\caption{\textbf{Attack Success Correlation Matrix.} We compute attack success rate correlations across all $n=65$ unlearning models. Input-space attacks show strong positive correlations (0.78-0.97) with each other, suggesting they exploit similar model vulnerabilities. In contrast, model tampering attacks show more varied and generally weaker correlations, both with each other and with input-space attacks. This suggests they probe model vulnerabilities through different mechanisms than input-space attacks, making them valuable complementary tools for harmful capability evaluations.}
\label{fig:attack_correlations}
\end{figure}

We visualize the relationships between attacks in \Cref{fig:attack_correlations} (attack correlation matrix) and \Cref{fig:attack_clustering_tree} (attack clustering tree). First, attacks with similar algorithmic mechanisms have highly correlated success rates. Second, full-finetuning attacks exhibit significant variation, even amongst each other. Since branching height indicates subtree similarity (higher height means less similar), \Cref{fig:attack_clustering_tree} implies that LoRA and Full-finetuning attacks are less similar to each other than input-space and latent space attacks are. Meanwhile, pruning and benign finetuning behave similarly to gradient-free input-space attacks. % Together, these results suggests that combining multiple attack types, particularly those that cluster distinctly, may provide more comprehensive insights into a model's latent capabilities.







% \clearpage
\section{Do model tampering attacks improve input-space vulnerability estimation?}
\label{app:vulnerability_estimation}

\subsection{Model tampering attacks improve predictive accuracy for worst-case input-space vulnerabilities}
\label{app:worst_case_vulnerability_estimation}

\begin{table}[h!]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Linear Regression Inputs} & \textbf{RMSE (\%)} & \textbf{$R^2$} \\ \midrule
BEAST, PAIR, Embedding & 0.0453\% & 0.5947 \\
Human Prompt, AutoPrompt, LoRA Fine-tune & 0.0457\% & 0.7596 \\
BEAST, AutoPrompt, LoRA Fine-tune & 0.0463\% & 0.7608 \\
GCG, PAIR, Benign Fine-tune & 0.0469\% & 0.8161 \\
Human Prompt, Embedding, LoRA Fine-tune & 0.0473\% & 0.6923 \\ \bottomrule
\end{tabular}
\caption{\textbf{Top-5 subsets of attacks most predictive of worst-case input-space success rate.} We compute all subsets of 3 attacks, and for each subset, we use linear regression to predict the worst-case input-space success rate from success rates of attacks in the subset. We show the top-5 subsets by RMSE. These top subsets lead to very accurate predictors of worst-case vulnerabilities and typically include diverse attack types (input-space gradient-free, input-space gradient-based, and model tampering).}
\label{tab:most_predictive_attacks}
\end{table}

\begin{figure}[h!]
\centering\includegraphics[width=0.7\textwidth]{figs/worst_case_r2_by_num_predictors.pdf}
\caption{\textbf{Model tampering attacks help predict worst-case input-space vulnerabilities.} We perform linear regressions to predict the worst-case input-space success rate from success rates of subsets of attacks. Including model tampering attacks in these subsets improves worst-case vulnerability estimation $R^2$ by 0.05-0.1. Ultimately, however, this is likely a conservative quantification of the marginal predictiveness of model tampering attacks for unforeseen input-space threats. The two most effective input space attacks were GCG and AutoPrompt, and as shown in \Cref{fig:attack_correlations}, their correlation is 0.88. However, unforeseen attacks in the real world are by no means guaranteed to be as similar to standard input-space attacks as GCG and AutoPrompt are to each other. As a result, this experiment is likely to paint a more pessimistic view on the value of model tampering attacks for predicting held-out input space attacks. }
\label{fig:worst_case_r2_by_num_predictors}
\end{figure}

In this section, we investigate the utility of model tampering attacks for worst-case input-space vulnerability estimation. While \Cref{fig:scatter} shows that fine-tuning attacks empirically offer conservative estimates for worst-case input-space vulnerabilities, in this section, we also show that model tampering attacks improve evaluators' ability to \emph{predict} worst-case vulnerabilities -- even if they already have access to input-space attacks.

For all experiments in this section, we assume the setting of an evaluator who only has access to a subset of attacks in order to estimate worst-case input-space vulnerabilities (potentially due to novel attacks). Whether due to resource constraints on the number of evaluations that are feasible to implement or due to the constant invention of new attack methods, evaluators will always be in this kind of setting. In our setup, we fit linear regression to predict worst-case input-space success rates given the success rates of a subset of attacks. Our dataset consists of a table of all unlearned models (and their 8 checkpoints throughout training) and all attack success scores (WMDP accuracy after attack - base WMDP accuracy). We perform $k$-fold cross-validation across model families by holding out all models trained by the same unlearning method, one method at a time. We then average statistics (e.g. RMSE, $R^2$) across the splits. Note that we include the 
UK AISI 
% [redacted for review]
input-space attack in these experiments, giving us 6 input-space attacks.

While our cross-validation procedure (with held-out model families) reflects the real-world setting of receiving a new model trained with unknown methods, it results in a validation set that is no longer i.i.d. with the train set. Due to this distribution shift, the assumption underlying the typical formula for $R^2$ is violated. So, when calculating $R^2  = (1 - mse / variance)$, instead of standard variance within the validation set, we use $\frac{1}{|val|} \sum_{s \in val} (s - \mu_{train})^2$ (where $\mu_{train}$ is the mean score in the train set instead of the validation set). Otherwise, the $\mu_{val}$ would use privileged information from the validation set that's not available in an i.i.d. setting. Note that because of this and our cross-validation procedure, the MSE and $R^2$ may lead to different rankings over performance of predictors.

\Cref{tab:most_predictive_attacks} shows the top-5 subsets of 3 attacks that lead to the lowest RMSE in predicting worst-case input-space attack success rate. Note that in all cases, at least one model tampering attack is present. Additionally, these subsets typically include diverse attack types. This supports the hypothesis that probing vulnerabilities through different mechanisms can improve worst-case held-out estimation.

\Cref{fig:worst_case_r2_by_num_predictors} shows that across subset sizes, including model tampering attacks lead to non-trivial improvements in worst-case predictive performance. Given the large size of subsets, we perform k-fold cross-validation over input-space attacks in addition to model families. Here, we loop through each input-space attack, holding out one input-space attack at a time so it is excluded as an input to linear regression. We then fit and evaluate the predictor's ability to estimate the worst-case success rate over all input-space attacks. Blue bars show the best $R^2$ over subsets made of input-space attacks only while orange bars show the best $R^2$ over all subsets.

\subsection{Input-space attacks are most predictive of average-case input-space vulnerabilities}
\label{app:average_case_vulnerability_estimation}

\begin{figure}[h!]
\centering\includegraphics[width=0.7\textwidth]{figs/average_case_r2_by_num_predictors.pdf}
\caption{\textbf{Input-space attacks are most predictive of average-case input-space vulnerabilities.} Here, we train linear regression to predict success rates of every input-space attack and average the $R^2$. Model tampering attacks do \emph{not} consistently improve predictive performance.}
\label{fig:average_case_r2_by_num_predictors}
\end{figure}

\Cref{fig:average_case_r2_by_num_predictors} shows average-case predictive performance with different subsets of attacks. Here, including model tampering attacks do not seem to improve predictive performance for the attacks tested here. We hypothesize that high correlations and similar attack mechanisms between input-space attacks make them more effective predictors of each other on average. In contrast, because model tampering attacks exploit distinct mechanisms, they are effective for predicting and bounding worst-case vulnerabilities.








\section{Full Jailbreaking Results} \label{app:full_jailbreaking_results}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/jailbreak_benchmark.pdf}
    % \includegraphics[width=0.9\linewidth]{figs/jailbreak_benchmark_redacted.pdf}
    \caption{\textbf{All safety-tuned models could be successfully jailbroken, particularly by fine-tuning and 
    Cascade 
    % [redacted for review]
    attacks.} We evaluate safety-tuning methods and jailbreak attacks. \textbf{Left:} The `Baseline' measures the compliance rate to direct harmful requests. \textbf{Right:} %Increase in WMDP-Bio accuracy after attack, showing that all unlearning methods were vulnerable to elicitation attacks. In particular, finetuning attacks (rightmost columns) were especially effective at revealing suppressed capabilities.
    Increase in harmful response rate after attack. All safety-tuning methods were vulnerable to elicitation of suppressed capabilities, especially by finetuning and 
    Cascade 
    % [redacted for review]
    attacks (rightmost columns).}
    \label{fig:benchmark_jailbreaks}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/pca_jailbreaks.pdf}
        \caption{Three principal components explain 96\% of the variation in attack success. \textbf{Left:} The proportion of explained variance for each principal component. \textbf{Right:} We display the first three principal components weighted by their eigenvalues. All coordinates of the first principal component are positive.}
    \label{fig:pca_jailbreaks}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/max_scatters_jailbreaks.pdf}
    \caption{\textbf{In our experiments, fine-tuning attack successes empirically \textit{exceed} the successes of state-of-the-art input-space attacks for jailbreaking.} Here, we plot the increases in compliance with harmful requests under model tampering attacks against the best-performing (out of 5) input-space attacks for each model. On the right, the $x$ axis is the best (over 2) between a LoRA and Full fine-tuning attack. We also display the correlation and the correlation's $p$ value. There are only 9 points in each figure, so we cannot draw strong conclusions. However, we see no clear evidence of a correlation between model tampering and input-space attack success. However, as before in \Cref{fig:scatter}, fine-tuning attacks empirically tend to offer conservative estimates of the success of input-space attacks. The only case out of 9 in which this was not the case was with the uncensored Orenguteng model (\href{https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored}{link}) which was unlike the other 8 in that it was not designed to be robust to jailbreaks.}
    \label{fig:scatter_jailbreaks}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/all_scatters_jailbreaks.pdf}
    \caption{\textbf{Full results from jailbreaking experiments comparing input-space and model tampering attacks.} See summarized results in \Cref{fig:scatter_jailbreaks}. Here, we plot the increases in WMDP-Bio performance from model tampering attacks and input-space attacks. We also display the unlearning-score-weighted correlation, the correlation's $p$ value, and the line $y=x$. Points below and to the right of the line indicate that the model tampering attack was more successful.}
    \label{fig:all_scatter_jailbreaks}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/max_haize_scatters_jailbreaks.pdf}
    % \includegraphics[width=0.9\linewidth]{figs/max_haize_scatters_jailbreaks_redacted.pdf}
    \caption{\textbf{Single-turn model tampering attack successes correlate with attacks from 
    Cascade, 
    % [redacted for review]
    a multi-turn, proprietary attack algorithm 
    % \citep{haizelabs2023cascade}
    }.
    Since 
    Cascade 
    % [redacted for review]
    is state-of-the-art and multi-turn, our single-turn model tampering attacks do not tend to empirically exceed the success of this attack as we find for unlearning experiments (\Cref{fig:scatter}). However, they empirically correlate with its success. }
    \label{fig:haize_jailbreaks_scatters}
\end{figure}

\end{document}
