[
  {
    "index": 0,
    "papers": [
      {
        "key": "sankaranarayanan2018regularizing",
        "author": "Sankaranarayanan, Swami and Jain, Arpit and Chellappa, Rama and Lim, Ser Nam",
        "title": "Regularizing deep networks using efficient layerwise adversarial training"
      },
      {
        "key": "singh2019harnessing",
        "author": "Mayank Singh and Abhishek Sinha and Nupur Kumari and Harshitha Machiraju and Balaji Krishnamurthy and Vineeth N Balasubramanian",
        "title": "Harnessing the Vulnerability of Latent Layers in Adversarially Trained Models"
      },
      {
        "key": "zhang2023adversarial",
        "author": "Zhang, Milin and Abdi, Mohammad and Restuccia, Francesco",
        "title": "Adversarial Machine Learning in Latent Representations of Neural Networks"
      },
      {
        "key": "schwinn2023adversarial",
        "author": "Leo Schwinn and David Dobre and Stephan G\u00fcnnemann and Gauthier Gidel",
        "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats"
      },
      {
        "key": "zeng2024beear",
        "author": "Zeng, Yi and Sun, Weiyu and Huynh, Tran Ngoc and Song, Dawn and Li, Bo and Jia, Ruoxi",
        "title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "xhonneux2024efficient",
        "author": "Xhonneux, Sophie and Sordoni, Alessandro and G{\\\"u}nnemann, Stephan and Gidel, Gauthier and Schwinn, Leo",
        "title": "Efficient Adversarial Training in LLMs with Continuous Attacks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "casper2024defending",
        "author": "Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan",
        "title": "Defending Against Unforeseen Failure Modes with Latent Adversarial Training"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "sheshadri2024targeted",
        "author": "Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and others",
        "title": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zou2023representation",
        "author": "Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others",
        "title": "Representation engineering: A top-down approach to ai transparency"
      },
      {
        "key": "wang2023backdoor",
        "author": "Wang, Haoran and Shu, Kai",
        "title": "Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment"
      },
      {
        "key": "lu2024investigating",
        "author": "Dawn Lu and Nina Rimsky",
        "title": "Investigating Bias Representations in Llama 2 Chat via Activation Steering"
      },
      {
        "key": "arditi2024refusal",
        "author": "Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Rimsky, Nina and Gurnee, Wes and Nanda, Neel",
        "title": "Refusal in Language Models Is Mediated by a Single Direction"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2025catastrophicfailurellmunlearning",
        "author": "Zhiwei Zhang and Fali Wang and Xiaomin Li and Zongyu Wu and Xianfeng Tang and Hui Liu and Qi He and Wenpeng Yin and Suhang Wang",
        "title": "Catastrophic Failure of LLM Unlearning via Quantization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "huang2024harmful",
        "author": "Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Liu, Ling",
        "title": "Harmful fine-tuning attacks and defenses for large language models: A survey"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "qi2023fine",
        "author": "Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter",
        "title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jain2023mechanistically",
        "author": "Jain, Samyak and Kirk, Robert and Lubana, Ekdeep Singh and Dick, Robert P and Tanaka, Hidenori and Grefenstette, Edward and Rockt{\\\"a}schel, Tim and Krueger, David Scott",
        "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks"
      },
      {
        "key": "yang2023shadow",
        "author": "Yang, Xianjun and Wang, Xiao and Zhang, Qi and Petzold, Linda and Wang, William Yang and Zhao, Xun and Lin, Dahua",
        "title": "Shadow alignment: The ease of subverting safely-aligned language models"
      },
      {
        "key": "qi2023fine",
        "author": "Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter",
        "title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!"
      },
      {
        "key": "bhardwaj2023language",
        "author": "Bhardwaj, Rishabh and Poria, Soujanya",
        "title": "Language model unalignment: Parametric red-teaming to expose hidden harms and biases"
      },
      {
        "key": "lermen2023lora",
        "author": "Lermen, Simon and Rogers-Smith, Charlie and Ladish, Jeffrey",
        "title": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"
      },
      {
        "key": "zhan2023removing",
        "author": "Zhan, Qiusi and Fang, Richard and Bindu, Rohan and Gupta, Akul and Hashimoto, Tatsunori and Kang, Daniel",
        "title": "Removing rlhf protections in gpt-4 via fine-tuning"
      },
      {
        "key": "ji2024language",
        "author": "Jiaming Ji and Kaile Wang and Tianyi Qiu and Boyuan Chen and Jiayi Zhou and Changye Li and Hantao Lou and Yaodong Yang",
        "title": "Language Models Resist Alignment"
      },
      {
        "key": "qi2024safety",
        "author": "Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson",
        "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep"
      },
      {
        "key": "hu2024jogging",
        "author": "Hu, Shengyuan and Fu, Yiwei and Wu, Zhiwei Steven and Smith, Virginia",
        "title": "Jogging the Memory of Unlearned Model Through Targeted Relearning Attack"
      },
      {
        "key": "halawicovert",
        "author": "Halawi, Danny and Wei, Alexander and Wallace, Eric and Wang, Tony Tong and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation"
      },
      {
        "key": "peng2024navigating",
        "author": "Peng, ShengYun and Chen, Pin-Yu and Hull, Matthew and Chau, Duen Horng",
        "title": "Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models"
      },
      {
        "key": "lo2024large",
        "author": "Lo, Michelle and Cohen, Shay B and Barez, Fazl",
        "title": "Large language models relearn removed concepts"
      },
      {
        "key": "lucki2024adversarial",
        "author": "{\\L}ucki, Jakub and Wei, Boyi and Huang, Yangsibo and Henderson, Peter and Tram{\\`e}r, Florian and Rando, Javier",
        "title": "An Adversarial Perspective on Machine Unlearning for AI Safety"
      },
      {
        "key": "shumailov2024ununlearning",
        "author": "Shumailov, Ilia and Hayes, Jamie and Triantafillou, Eleni and Ortiz-Jimenez, Guillermo and Papernot, Nicolas and Jagielski, Matthew and Yona, Itay and Howard, Heidi and Bagdasaryan, Eugene",
        "title": "UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI"
      },
      {
        "key": "lynch2024eight",
        "author": "Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and Hadfield-Menell, Dylan",
        "title": "Eight methods to evaluate robust unlearning in llms"
      },
      {
        "key": "deeb2024unlearningmethodsremoveinformation",
        "author": "Aghyad Deeb and Fabien Roger",
        "title": "Do Unlearning Methods Remove Information from Language Model Weights?"
      },
      {
        "key": "qi2024evaluating",
        "author": "Qi, Xiangyu and Wei, Boyi and Carlini, Nicholas and Huang, Yangsibo and Xie, Tinghao and He, Luxi and Jagielski, Matthew and Nasr, Milad and Mittal, Prateek and Henderson, Peter",
        "title": "On Evaluating the Durability of Safeguards for Open-Weight LLMs"
      },
      {
        "key": "yi2024vulnerability",
        "author": "Yi, Jingwei and Ye, Rui and Chen, Qisi and Zhu, Bin and Chen, Siheng and Lian, Defu and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao",
        "title": "On the vulnerability of safety alignment in open-access llms"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "greenblatt2024stress",
        "author": "Greenblatt, Ryan and Roger, Fabien and Krasheninnikov, Dmitrii and Krueger, David",
        "title": "Stress-Testing Capability Elicitation With Password-Locked Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hofstatter2025elicitation",
        "author": "Hofst{\\\"a}tter, Felix and van der Weij, Teun and Teoh, Jayden and Bartsch, Henning and Ward, Francis Rhys",
        "title": "The Elicitation Game: Evaluating Capability Elicitation Techniques"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2024rethinking",
        "author": "Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others",
        "title": "Rethinking Machine Unlearning for Large Language Models"
      },
      {
        "key": "barez2025open",
        "author": "Barez, Fazl and Fu, Tingchen and Prabhu, Ameya and Casper, Stephen and Sanyal, Amartya and Bibi, Adel and O'Gara, Aidan and Kirk, Robert and Bucknall, Ben and Fist, Tim and others",
        "title": "Open Problems in Machine Unlearning for AI Safety"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yi2024jailbreak",
        "author": "Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi",
        "title": "Jailbreak attacks and defenses against large language models: A survey"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhao2024survey",
        "author": "Zhao, Shuai and Jia, Meihuizi and Guo, Zhongliang and Gan, Leilei and Xu, Xiaoyu and Wu, Xiaobao and Fu, Jie and Feng, Yichao and Pan, Fengjun and Tuan, Luu Anh",
        "title": "A survey of backdoor attacks and defenses on large language models: Implications for security measures"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "greenblatt2024stress",
        "author": "Greenblatt, Ryan and Roger, Fabien and Krasheninnikov, Dmitrii and Krueger, David",
        "title": "Stress-Testing Capability Elicitation With Password-Locked Models"
      },
      {
        "key": "hofstatter2025elicitation",
        "author": "Hofst{\\\"a}tter, Felix and van der Weij, Teun and Teoh, Jayden and Bartsch, Henning and Ward, Francis Rhys",
        "title": "The Elicitation Game: Evaluating Capability Elicitation Techniques"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "van2024ai",
        "author": "van der Weij, Teun and Hofst{\\\"a}tter, Felix and Jaffe, Ollie and Brown, Samuel F and Ward, Francis Rhys",
        "title": "AI Sandbagging: Language Models can Strategically Underperform on Evaluations"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "patil2023can",
        "author": "Patil, Vaidehi and Hase, Peter and Bansal, Mohit",
        "title": "Can sensitive information be deleted from llms? objectives for defending against extraction attacks"
      },
      {
        "key": "lynch2024eight",
        "author": "Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and Hadfield-Menell, Dylan",
        "title": "Eight methods to evaluate robust unlearning in llms"
      },
      {
        "key": "lucki2024adversarial",
        "author": "{\\L}ucki, Jakub and Wei, Boyi and Huang, Yangsibo and Henderson, Peter and Tram{\\`e}r, Florian and Rando, Javier",
        "title": "An Adversarial Perspective on Machine Unlearning for AI Safety"
      },
      {
        "key": "hu2024jogging",
        "author": "Hu, Shengyuan and Fu, Yiwei and Wu, Zhiwei Steven and Smith, Virginia",
        "title": "Jogging the Memory of Unlearned Model Through Targeted Relearning Attack"
      },
      {
        "key": "liu2024rethinking",
        "author": "Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others",
        "title": "Rethinking Machine Unlearning for Large Language Models"
      },
      {
        "key": "zhang2024does",
        "author": "Zhang, Zhiwei and Wang, Fali and Li, Xiaomin and Wu, Zongyu and Tang, Xianfeng and Liu, Hui and He, Qi and Yin, Wenpeng and Wang, Suhang",
        "title": "Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge"
      },
      {
        "key": "liu2024threats",
        "author": "Liu, Ziyao and Ye, Huanyi and Chen, Chen and Zheng, Yongsen and Lam, Kwok-Yan",
        "title": "Threats, attacks, and defenses in machine unlearning: A survey"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2024wmdp",
        "author": "Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and others",
        "title": "The wmdp benchmark: Measuring and reducing malicious use with unlearning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "shayegani2023survey",
        "author": "Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and Abu-Ghazaleh, Nael",
        "title": "Survey of vulnerabilities in large language models revealed by adversarial attacks"
      },
      {
        "key": "yi2024jailbreak",
        "author": "Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi",
        "title": "Jailbreak attacks and defenses against large language models: A survey"
      },
      {
        "key": "jin2024jailbreakzoo",
        "author": "Jin, Haibo and Hu, Leyang and Li, Xinuo and Zhang, Peiyan and Chen, Chonghan and Zhuang, Jun and Wang, Haohan",
        "title": "Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models"
      },
      {
        "key": "chowdhury2024breaking",
        "author": "Chowdhury, Arijit Ghosh and Islam, Md Mofijul and Kumar, Vaibhav and Shezan, Faysal Hossain and Jain, Vinija and Chadha, Aman",
        "title": "Breaking down the defenses: A comparative survey of attacks on large language models"
      },
      {
        "key": "lin2024against",
        "author": "Lin, Lizhi and Mu, Honglin and Zhai, Zenan and Wang, Minghan and Wang, Yuxia and Wang, Renxi and Gao, Junjie and Zhang, Yixuan and Che, Wanxiang and Baldwin, Timothy and others",
        "title": "Against The Achilles' Heel: A Survey on Red Teaming for Generative Models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhang2025catastrophicfailurellmunlearning",
        "author": "Zhiwei Zhang and Fali Wang and Xiaomin Li and Zongyu Wu and Xianfeng Tang and Hui Liu and Qi He and Wenpeng Yin and Suhang Wang",
        "title": "Catastrophic Failure of LLM Unlearning via Quantization"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "2025adversarialperspectivemachineunlearning",
        "author": "Jakub \u0141ucki and Boyi Wei and Yangsibo Huang and Peter Henderson and Florian Tram\u00e8r and Javier Rando",
        "title": "An Adversarial Perspective on Machine Unlearning for AI Safety"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "qi2024evaluating",
        "author": "Qi, Xiangyu and Wei, Boyi and Carlini, Nicholas and Huang, Yangsibo and Xie, Tinghao and He, Luxi and Jagielski, Matthew and Nasr, Milad and Mittal, Prateek and Henderson, Peter",
        "title": "On Evaluating the Durability of Safeguards for Open-Weight LLMs"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "qi2024evaluating",
        "author": "Qi, Xiangyu and Wei, Boyi and Carlini, Nicholas and Huang, Yangsibo and Xie, Tinghao and He, Luxi and Jagielski, Matthew and Nasr, Milad and Mittal, Prateek and Henderson, Peter",
        "title": "On Evaluating the Durability of Safeguards for Open-Weight LLMs"
      }
    ]
  }
]