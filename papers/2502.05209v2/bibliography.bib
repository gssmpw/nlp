@article{li2024wmdp,
  title={The wmdp benchmark: Measuring and reducing malicious use with unlearning},
  author={Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and others},
  journal={arXiv preprint arXiv:2403.03218},
  year={2024}
}


@article{lynch2024eight,
  title={Eight methods to evaluate robust unlearning in llms},
  author={Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2402.16835},
  year={2024}
}

@article{fort2023scaling,
  title={Scaling laws for adversarial attacks on language model activations},
  author={Fort, Stanislav},
  journal={arXiv preprint arXiv:2312.02780},
  year={2023}
}

@article{rosati2024representation,
  title={Representation noising effectively prevents harmful fine-tuning on LLMs},
  author={Rosati, Domenic and Wehner, Jan and Williams, Kai and Bartoszcze, {\L}ukasz and Atanasov, David and Gonzales, Robie and Majumdar, Subhabrata and Maple, Carsten and Sajjad, Hassan and Rudzicz, Frank},
  journal={arXiv preprint arXiv:2405.14577},
  year={2024}
}



@article{sheshadri2024targeted,
  title={Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs},
  author={Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and others},
  journal={arXiv preprint arXiv:2407.15549},
  year={2024}
}


@article{zou2024improving,
  title={Improving alignment and robustness with circuit breakers},
  author={Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek and Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan and Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan},
  journal={arXiv preprint arXiv},
  volume={2406},
  year={2024}
}


@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}



@inproceedings{casper2024black,
  title={Black-box access is insufficient for rigorous ai audits},
  author={Casper, Stephen and Ezell, Carson and Siegmann, Charlotte and Kolt, Noam and Curtis, Taylor Lynn and Bucknall, Benjamin and Haupt, Andreas and Wei, Kevin and Scheurer, J{\'e}r{\'e}my and Hobbhahn, Marius and others},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2254--2272},
  year={2024}
}




@article{tamirisa2024tamper,
  title={Tamper-Resistant Safeguards for Open-Weight LLMs},
  author={Tamirisa, Rishub and Bharathi, Bhrugu and Phan, Long and Zhou, Andy and Gatti, Alice and Suresh, Tarun and Lin, Maxwell and Wang, Justin and Wang, Rowan and Arel, Ron and others},
  journal={arXiv preprint arXiv:2408.00761},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}


@article{schwinn2024soft,
  title={Soft prompt threats: Attacking safety alignment and unlearning in open-source llms through the embedding space},
  author={Schwinn, Leo and Dobre, David and Xhonneux, Sophie and Gidel, Gauthier and Gunnemann, Stephan},
  journal={arXiv preprint arXiv:2402.09063},
  year={2024}
}


@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{bai2024mt,
  title={Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues},
  author={Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and others},
  journal={arXiv preprint arXiv:2402.14762},
  year={2024}
}

@article{zhong2023agieval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}


@misc{shin2020autopromptelicitingknowledgelanguage,
      title={AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts}, 
      author={Taylor Shin and Yasaman Razeghi and Robert L. Logan IV au2 and Eric Wallace and Sameer Singh},
      year={2020},
      eprint={2010.15980},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.15980}, 
}

@misc{sadasivan2024fastadversarialattackslanguage,
      title={Fast Adversarial Attacks on Language Models In One GPU Minute}, 
      author={Vinu Sankar Sadasivan and Shoumik Saha and Gaurang Sriramanan and Priyatham Kattakinda and Atoosa Chegini and Soheil Feizi},
      year={2024},
      eprint={2402.15570},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2402.15570}, 
}

@misc{eu_ai_act,
  title        = {The EU Artificial Intelligence Act},
  author       = {EU},
  howpublished = {\url{https://artificialintelligenceact.eu/}},
  year         = {2023},
  note         = {Accessed: 2024-09-29}
}

@misc{biden2023executive,
	title = {Executive {Order} on the {Safe}, {Secure}, and {Trustworthy} {Development} and {Use} of {Artificial} {Intelligence}},
	url = {https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/},
	publisher = {The White House},
	author = {{POTUS}},
	month = oct,
	year = {2023},
}
@techreport{dsit2023,
	title = {A pro-innovation approach to {AI} regulation},
	url = {https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper},
	language = {en},
	urldate = {2023-11-05},
	author = {{UK DSIT}},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\kwei\\Zotero\\storage\\QPEQASHC\\white-paper.html:text/html},
}
@misc{airmf2023,
	address = {Gaithersburg, MD},
	title = {{AI} {Risk} {Management} {Framework}: {AI} {RMF} (1.0)},
	shorttitle = {{AI} {Risk} {Management} {Framework}},
	url = {https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf},
	doi = {10.6028/NIST.AI.100-1},
	abstract = {As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283), the goal of the AI RMF is to offer a resource to the organizations designing, developing, deploying, or using AI systems to help manage the many risks of AI and promote trustworthy and responsible development and use of AI systems. The Framework is intended to be voluntary, rights-preserving, non-sector specific, and use-case agnostic, providing flexibility to organizations of all sizes and in all sectors and throughout society to implement the approaches in the Framework. The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies continue to develop, and to be operationalized by organizations in varying degrees and capacities so society can benefit from AI while also being protected from its potential harms.},
	language = {en},
	urldate = {2023-09-30},
	publisher = {National Institute of Standards and Technology},
	author = {NIST},
	month = jan,
	year = {2023},
	doi = {10.6028/NIST.AI.100-1},
	file = {Tabassi - 2023 - AI Risk Management Framework AI RMF (1.0).pdf:C\:\\Users\\kwei\\Zotero\\storage\\KTB2AZC9\\Tabassi - 2023 - AI Risk Management Framework AI RMF (1.0).pdf:application/pdf},
}
@misc{california_sb1047_2024,
  author       = "{CA Legislature}",
  title        = "{California Senate Bill No. 1047: An act to amend Section 14005.27 of the Welfare and Institutions Code, relating to Medi-Cal}",
  howpublished = "{Draft legislation, not yet passed}",
  year         = "2024",
  note         = "{Accessed: September 29, 2024}. Available: \\url{https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047}"
}

@inproceedings{raji2022outsider,
  title={Outsider oversight: Designing a third party audit ecosystem for ai governance},
  author={Raji, Inioluwa Deborah and Xu, Peggy and Honigsberg, Colleen and Ho, Daniel},
  booktitle={Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={557--571},
  year={2022}
}

@article{anderljung2023publicly,
      title={Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem under the ASPIRE Framework}, 
      author={Markus Anderljung and Everett Thornton Smith and Joe O'Brien and Lisa Soder and Benjamin Bucknall and Emma Bluemke and Jonas Schuett and Robert Trager and Lacey Strahm and Rumman Chowdhury},
      year={2023},
      eprint={2311.14711},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{schuett2023towards,
  title={Towards best practices in AGI safety and governance: A survey of expert opinion},
  author={Schuett, Jonas and Dreksler, Noemi and Anderljung, Markus and McCaffary, David and Heim, Lennart and Bluemke, Emma and Garfinkel, Ben},
  journal={arXiv preprint arXiv:2305.07153},
  year={2023}
}

@article{shevlane2023model,
  title={Model evaluation for extreme risks},
  author={Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and others},
  journal={arXiv preprint arXiv:2305.15324},
  year={2023}
}
@article{li2024llm,
  title={LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet},
  author={Li, Nathaniel and Han, Ziwen and Steneker, Ian and Primack, Willow and Goodside, Riley and Zhang, Hugh and Wang, Zifan and Menghini, Cristina and Yue, Summer},
  journal={arXiv preprint arXiv:2408.15221},
  year={2024}
}
@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{shayegani2023survey,
  title={Survey of vulnerabilities in large language models revealed by adversarial attacks},
  author={Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2310.10844},
  year={2023}
}
@article{carlini2024aligned,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Koh, Pang Wei W and Ippolito, Daphne and Tramer, Florian and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lucki2024adversarial,
  title={An Adversarial Perspective on Machine Unlearning for AI Safety},
  author={{\L}ucki, Jakub and Wei, Boyi and Huang, Yangsibo and Henderson, Peter and Tram{\`e}r, Florian and Rando, Javier},
  journal={arXiv preprint arXiv:2409.18025},
  year={2024}
}

@article{clausen2006generalizing,
  title={Generalizing the safety factor approach},
  author={Clausen, Jonas and Hansson, Sven Ove and Nilsson, Fred},
  journal={Reliability Engineering \& System Safety},
  volume={91},
  number={8},
  pages={964--973},
  year={2006},
  publisher={Elsevier}
}


@book{nevo2024securing,
  title={Securing AI Model Weights: Preventing Theft and Misuse of Frontier Models},
  author={Nevo, Sella and Lahav, Dan and Karpur, Ajay and Bar-On, Yogev and Bradley, Henry Alexander},
  number={1},
  year={2024},
  publisher={Rand Corporation}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@article{casper2024defending,
  title={Defending Against Unforeseen Failure Modes with Latent Adversarial Training},
  author={Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2403.05030},
  year={2024}
}

@inproceedings{sankaranarayanan2018regularizing,
  title={Regularizing deep networks using efficient layerwise adversarial training},
  author={Sankaranarayanan, Swami and Jain, Arpit and Chellappa, Rama and Lim, Ser Nam},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@misc{singh2019harnessing,
      title={Harnessing the Vulnerability of Latent Layers in Adversarially Trained Models}, 
      author={Mayank Singh and Abhishek Sinha and Nupur Kumari and Harshitha Machiraju and Balaji Krishnamurthy and Vineeth N Balasubramanian},
      year={2019},
      eprint={1905.05186},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zhang2023adversarial,
  title={Adversarial Machine Learning in Latent Representations of Neural Networks},
  author={Zhang, Milin and Abdi, Mohammad and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2309.17401},
  year={2023}
}

@article{schwinn2023adversarial,
      title={Adversarial Attacks and Defenses in Large Language Models: Old and New Threats}, 
      author={Leo Schwinn and David Dobre and Stephan Günnemann and Gauthier Gidel},
      year={2023},
      eprint={2310.19737},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{xhonneux2024efficient,
  title={Efficient Adversarial Training in LLMs with Continuous Attacks},
  author={Xhonneux, Sophie and Sordoni, Alessandro and G{\"u}nnemann, Stephan and Gidel, Gauthier and Schwinn, Leo},
  journal={arXiv preprint arXiv:2405.15589},
  year={2024}
}

@article{zeng2024beear,
  title={BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models},
  author={Zeng, Yi and Sun, Weiyu and Huynh, Tran Ngoc and Song, Dawn and Li, Bo and Jia, Ruoxi},
  journal={arXiv preprint arXiv:2406.17092},
  year={2024}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{wang2023backdoor,
  title={Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment},
  author={Wang, Haoran and Shu, Kai},
  journal={arXiv preprint arXiv:2311.09433},
  year={2023}
}


@misc{lu2024investigating,
      title={Investigating Bias Representations in Llama 2 Chat via Activation Steering}, 
      author={Dawn Lu and Nina Rimsky},
      year={2024},
      eprint={2402.00402},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{arditi2024refusal,
  title={Refusal in Language Models Is Mediated by a Single Direction},
  author={Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Rimsky, Nina and Gurnee, Wes and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11717},
  year={2024}
}

@article{jain2023mechanistically,
  title={Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks},
  author={Jain, Samyak and Kirk, Robert and Lubana, Ekdeep Singh and Dick, Robert P and Tanaka, Hidenori and Grefenstette, Edward and Rockt{\"a}schel, Tim and Krueger, David Scott},
  journal={arXiv preprint arXiv:2311.12786},
  year={2023}
}

@article{yang2023shadow,
  title={Shadow alignment: The ease of subverting safely-aligned language models},
  author={Yang, Xianjun and Wang, Xiao and Zhang, Qi and Petzold, Linda and Wang, William Yang and Zhao, Xun and Lin, Dahua},
  journal={arXiv preprint arXiv:2310.02949},
  year={2023}
}

@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}

@article{lermen2023lora,
  title={LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B},
  author={Lermen, Simon and Rogers-Smith, Charlie and Ladish, Jeffrey},
  journal={arXiv preprint arXiv:2310.20624},
  year={2023}
}

@article{zhan2023removing,
  title={Removing rlhf protections in gpt-4 via fine-tuning},
  author={Zhan, Qiusi and Fang, Richard and Bindu, Rohan and Gupta, Akul and Hashimoto, Tatsunori and Kang, Daniel},
  journal={arXiv preprint arXiv:2311.05553},
  year={2023}
}

@article{bhardwaj2023language,
  title={Language model unalignment: Parametric red-teaming to expose hidden harms and biases},
  author={Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2310.14303},
  year={2023}
}

@misc{ji2024language,
      title={Language Models Resist Alignment}, 
      author={Jiaming Ji and Kaile Wang and Tianyi Qiu and Boyuan Chen and Jiayi Zhou and Changye Li and Hantao Lou and Yaodong Yang},
      year={2024},
      eprint={2406.06144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qi2024safety,
      title={Safety Alignment Should Be Made More Than Just a Few Tokens Deep}, 
      author={Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson},
      year={2024},
      eprint={2406.05946},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{halawicovert,
  title={Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation},
  author={Halawi, Danny and Wei, Alexander and Wallace, Eric and Wang, Tony Tong and Haghtalab, Nika and Steinhardt, Jacob},
  booktitle={Forty-first International Conference on Machine Learning}
}
@article{hu2024jogging,
  title={Jogging the Memory of Unlearned Model Through Targeted Relearning Attack},
  author={Hu, Shengyuan and Fu, Yiwei and Wu, Zhiwei Steven and Smith, Virginia},
  journal={arXiv preprint arXiv:2406.13356},
  year={2024}
}

@article{greenblatt2024stress,
  title={Stress-Testing Capability Elicitation With Password-Locked Models},
  author={Greenblatt, Ryan and Roger, Fabien and Krasheninnikov, Dmitrii and Krueger, David},
  journal={arXiv preprint arXiv:2405.19550},
  year={2024}
}

@article{peng2024navigating,
  title={Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models},
  author={Peng, ShengYun and Chen, Pin-Yu and Hull, Matthew and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2405.17374},
  year={2024}
}

@article{lo2024large,
  title={Large language models relearn removed concepts},
  author={Lo, Michelle and Cohen, Shay B and Barez, Fazl},
  journal={arXiv preprint arXiv:2401.01814},
  year={2024}
}

@article{liu2024rethinking,
  title={Rethinking Machine Unlearning for Large Language Models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@article{shumailov2024ununlearning,
  title={UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI},
  author={Shumailov, Ilia and Hayes, Jamie and Triantafillou, Eleni and Ortiz-Jimenez, Guillermo and Papernot, Nicolas and Jagielski, Matthew and Yona, Itay and Howard, Heidi and Bagdasaryan, Eugene},
  journal={arXiv preprint arXiv:2407.00106},
  year={2024}
}
@article{patil2023can,
  title={Can sensitive information be deleted from llms? objectives for defending against extraction attacks},
  author={Patil, Vaidehi and Hase, Peter and Bansal, Mohit},
  journal={arXiv preprint arXiv:2309.17410},
  year={2023}
}


@misc{chao2024jailbreakingblackboxlarge,
      title={Jailbreaking Black Box Large Language Models in Twenty Queries}, 
      author={Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
      year={2024},
      eprint={2310.08419},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.08419}, 
}

@inproceedings{liu2022continual,
  title={Continual learning and private unlearning},
  author={Liu, Bo and Liu, Qiang and Stone, Peter},
  booktitle={Conference on Lifelong Learning Agents},
  pages={243--254},
  year={2022},
  organization={PMLR}
}

@article{Bourtoule2019MachineU,
  title={Machine Unlearning},
  author={Lucas Bourtoule and Varun Chandrasekaran and Christopher A. Choquette-Choo and Hengrui Jia and Adelin Travers and Baiwu Zhang and David Lie and Nicolas Papernot},
  journal={2021 IEEE Symposium on Security and Privacy (SP)},
  year={2019},
  pages={141-159},
  url={https://api.semanticscholar.org/CorpusID:208909851}
}

@article{jang2022knowledge,
  title={Knowledge unlearning for mitigating privacy risks in language models},
  author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.01504},
  year={2022}
}

% ### Deanonymizing ELM (Rohit)
%article{anonymous2024erasing,
%  title={Erasing Conceptual Knowledge from Language Models},
%  author={Anonymous},
%  journal={Not-yet published preprint},
%  year={2024}
%}

@article{anonymous2024erasing,
  title={Erasing Conceptual Knowledge from Language Models},
  author={Gandikota, Rohit and Feucht, Sheridan and Marks, Samuel and Bau, David},
  journal={arXiv preprint arXiv:2410.02760},
  year={2024}
}


@misc{aisi2024,
  title        = {About the AI Safety Institute},
  author       = {{UK AISI}},
  year         = 2024,
  url          = {https://www.aisi.gov.uk/about},
  note         = {Accessed: 2024-09-30}
}

@misc{usaisi2024,
  title        = {U.S. Artificial Intelligence Safety Institute},
  author       = {{US AISI}},
  year         = 2024,
  url          = {https://www.nist.gov/aisi},
  note         = {Accessed: 2024-09-30}
}

@article{zhang2024does,
  title={Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge},
  author={Zhang, Zhiwei and Wang, Fali and Li, Xiaomin and Wu, Zongyu and Tang, Xianfeng and Liu, Hui and He, Qi and Yin, Wenpeng and Wang, Suhang},
  journal={arXiv preprint arXiv:2410.16454},
  year={2024}
}

@article{yong2023low,
  title={Low-resource languages jailbreak gpt-4},
  author={Yong, Zheng-Xin and Menghini, Cristina and Bach, Stephen H},
  journal={arXiv preprint arXiv:2310.02446},
  year={2023}
}

@article{anil2024many,
  title={Many-shot jailbreaking},
  author={Anil, Cem and Durmus, Esin and Sharma, Mrinank and Benton, Joe and Kundu, Sandipan and Batson, Joshua and Rimsky, Nina and Tong, Meg and Mu, Jesse and Ford, Daniel and others},
  journal={Anthropic, April},
  year={2024}
}

@misc{deeb2024unlearningmethodsremoveinformation,
      title={Do Unlearning Methods Remove Information from Language Model Weights?}, 
      author={Aghyad Deeb and Fabien Roger},
      year={2024},
      eprint={2410.08827},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08827}, 
}

@article{wang2024taylor,
  title={Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion},
  author={Wang, Guanchu and Chuang, Yu-Neng and Tang, Ruixiang and Zhong, Shaochen and Yuan, Jiayi and Jin, Hongye and Liu, Zirui and Chaudhary, Vipin and Xu, Shuai and Caverlee, James and others},
  journal={arXiv preprint arXiv:2410.05331},
  year={2024}
}

@article{huang2024harmful,
  title={Harmful fine-tuning attacks and defenses for large language models: A survey},
  author={Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Liu, Ling},
  journal={arXiv preprint arXiv:2409.18169},
  year={2024}
}

@article{huang2024vaccine,
  title={Vaccine: Perturbation-aware alignment for large language model},
  author={Huang, Tiansheng and Hu, Sihao and Liu, Ling},
  journal={arXiv preprint arXiv:2402.01109},
  year={2024}
}

@inproceedings{anderljung2023towards,
  title={Towards publicly accountable frontier llms},
  author={Anderljung, Markus and Smith, Everett and O'Brien, Joe and Soder, Lisa and Bucknall, Benjamin and Bluemke, Emma and Schuett, Jonas and Trager, Robert and Strahm, Lacey and Chowdhury, Rumman},
  booktitle={Socially Responsible Language Modelling Research},
  year={2023}
}

@article{uuk2024effective,
  title={Effective Mitigations for Systemic Risks from General-Purpose AI},
  author={Risto Uuk and Annemieke Brouwer and Noemi Dreksler and Valeria Pulignano and Rishi Bommasani},
  year={2024},
  journal={SSRN Electronic Journal},
  url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5021463},
}

@misc{china2023generativeAI,
  title        = {Interim Measures for the Management of Generative Artificial Intelligence Services},
  author       = {PRC},
  year         = {2023},
  month        = {July},
  note         = {Issued by the Cyberspace Administration of China (CAC)},
  url          = {https://www.chinalawtranslate.com/en/generative-ai-interim/}
}

@misc{brazil2023aiRegulation,
  title        = {Drafted Bill No. 2338 of 2023: Regulating the Use of Artificial Intelligence Including Algorithm Design and Technical Standards},
  author       = {NCB},
  year         = {2023},
  month        = {October},
  note         = {Proposed legislation to regulate artificial intelligence and establish technical standards},
  url          = {https://digitalpolicyalert.org/event/11237-introduced-bill-no-2338-of-2023-regulating-the-use-of-artificial-intelligence-including-algorithm-design-and-technical-standards}
}

@article{zhao2024survey,
  title={A survey of backdoor attacks and defenses on large language models: Implications for security measures},
  author={Zhao, Shuai and Jia, Meihuizi and Guo, Zhongliang and Gan, Leilei and Xu, Xiaoyu and Wu, Xiaobao and Fu, Jie and Feng, Yichao and Pan, Fengjun and Tuan, Luu Anh},
  journal={arXiv preprint arXiv:2406.06852},
  year={2024}
}

@article{van2024ai,
  title={AI Sandbagging: Language Models can Strategically Underperform on Evaluations},
  author={van der Weij, Teun and Hofst{\"a}tter, Felix and Jaffe, Ollie and Brown, Samuel F and Ward, Francis Rhys},
  journal={arXiv preprint arXiv:2406.07358},
  year={2024}
}

@article{yi2024jailbreak,
  title={Jailbreak attacks and defenses against large language models: A survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}

@article{lin2024against,
  title={Against The Achilles' Heel: A Survey on Red Teaming for Generative Models},
  author={Lin, Lizhi and Mu, Honglin and Zhai, Zenan and Wang, Minghan and Wang, Yuxia and Wang, Renxi and Gao, Junjie and Zhang, Yixuan and Che, Wanxiang and Baldwin, Timothy and others},
  journal={arXiv preprint arXiv:2404.00629},
  year={2024}
}

@article{reuel2024open,
  title={Open problems in technical ai governance},
  author={Reuel, Anka and Bucknall, Ben and Casper, Stephen and Fist, Tim and Soder, Lisa and Aarne, Onni and Hammond, Lewis and Ibrahim, Lujain and Chan, Alan and Wills, Peter and others},
  journal={arXiv preprint arXiv:2407.14981},
  year={2024}
}
@misc{National_Assembly_of_the_Republic_of_Korea_2025,
  title = {Act on the Protection of Personal Information},
  author = {{Korea}},
  year = {2025},
  url = {https://likms.assembly.go.kr/bill/billDetail.do?billId=PRC_R2V4H1W1T2K5M1O6E4Q9T0V7Q9S0U0}
}
@article{jin2024jailbreakzoo,
  title={Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models},
  author={Jin, Haibo and Hu, Leyang and Li, Xinuo and Zhang, Peiyan and Chen, Chonghan and Zhuang, Jun and Wang, Haohan},
  journal={arXiv preprint arXiv:2407.01599},
  year={2024}
}

@article{chowdhury2024breaking,
  title={Breaking down the defenses: A comparative survey of attacks on large language models},
  author={Chowdhury, Arijit Ghosh and Islam, Md Mofijul and Kumar, Vaibhav and Shezan, Faysal Hossain and Jain, Vinija and Chadha, Aman},
  journal={arXiv preprint arXiv:2403.04786},
  year={2024}
}

@misc{Bill2338,
  title = {{Bill No. 2338 of 2023: Regulating the Use of Artificial Intelligence, Including Algorithm Design and Technical Standards}},
  author = {Brazil},
  year = {2023},
  url = {https://digitalpolicyalert.org/event/11237-introduced-bill-no-2338-of-2023-regulating-the-use-of-artificial-intelligence-including-algorithm-design-and-technical-standards},
  note = {Accessed: 2024-11-21}
}

@misc{AIDAct,
  title = {{AI and Data Act: Part of Bill C-27, Digital Charter Implementation Act, 2022}},
  author = {Canada},
  year = {2022},
  url = {https://www.parl.ca/DocumentViewer/en/44-1/bill/C-27/first-reading},
  note = {Accessed: 2024-11-21}
}

@misc{GenerativeAIInterimMeasures,
  title = {{Interim Measures for the Management of Generative Artificial Intelligence Services}},
  author = {China},
  year = {2023},
  url = {https://www.chinalawtranslate.com/en/generative-ai-interim/},
  note = {Accessed: 2024-11-21}
}

@article{andriushchenko2024jailbreaking,
  title={Jailbreaking leading safety-aligned llms with simple adaptive attacks},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2404.02151},
  year={2024}
}


@misc{openai2024systemcard,
  title        = {OpenAI System Card: December 2024},
  author       = {OpenAI},
  year         = {2024},
  url          = {https://cdn.openai.com/o1-system-card-20241205.pdf},
  note         = {Accessed: 2024-12-07}
}

@article{qi2024evaluating,
  title={On Evaluating the Durability of Safeguards for Open-Weight LLMs},
  author={Qi, Xiangyu and Wei, Boyi and Carlini, Nicholas and Huang, Yangsibo and Xie, Tinghao and He, Luxi and Jagielski, Matthew and Nasr, Milad and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2412.07097},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{yuan2024refuse,
  title={Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training},
  author={Yuan, Youliang and Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and Xu, Jiahao and Liang, Tian and He, Pinjia and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2407.09121},
  year={2024}
}

@article{souly2024strongreject,
  title={A strongreject for empty jailbreaks},
  author={Souly, Alexandra and Lu, Qingyuan and Bowen, Dillon and Trinh, Tu and Hsieh, Elvis and Pandey, Sana and Abbeel, Pieter and Svegliato, Justin and Emmons, Scott and Watkins, Olivia and others},
  journal={arXiv preprint arXiv:2402.10260},
  year={2024}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@misc{haizelabs2023cascade,
  author    = {{Haize Labs}},
  title     = {Cascade: Exploring Hierarchical Inference in Language Models},
  year      = {2023},
  url       = {https://blog.haizelabs.com/posts/cascade/},
  note      = {Accessed: 2025-01-22}
}
@article{hofstatter2025elicitation,
  title={The Elicitation Game: Evaluating Capability Elicitation Techniques},
  author={Hofst{\"a}tter, Felix and van der Weij, Teun and Teoh, Jayden and Bartsch, Henning and Ward, Francis Rhys},
  journal={arXiv preprint arXiv:2502.02180},
  year={2025}
}

@article{clymer2024safety,
  title={Safety cases: How to justify the safety of advanced AI systems},
  author={Clymer, Joshua and Gabrieli, Nick and Krueger, David and Larsen, Thomas},
  journal={arXiv preprint arXiv:2403.10462},
  year={2024}
}

@misc{hofstätter2025elicitationgameevaluatingcapability,
      title={The Elicitation Game: Evaluating Capability Elicitation Techniques}, 
      author={Felix Hofstätter and Teun van der Weij and Jayden Teoh and Henning Bartsch and Francis Rhys Ward},
      year={2025},
      eprint={2502.02180},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.02180}, 
}

@article{zhang2024soft,
  title={Soft prompts go hard: Steering visual language models with hidden meta-instructions},
  author={Zhang, Tingwei and Zhang, Collin and Morris, John X and Bagdasarian, Eugene and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:2407.08970},
  year={2024}
}

@article{liu2024threats,
  title={Threats, attacks, and defenses in machine unlearning: A survey},
  author={Liu, Ziyao and Ye, Huanyi and Chen, Chen and Zheng, Yongsen and Lam, Kwok-Yan},
  journal={arXiv preprint arXiv:2403.13682},
  year={2024}
}

@inproceedings{yi2024vulnerability,
  title={On the vulnerability of safety alignment in open-access llms},
  author={Yi, Jingwei and Ye, Rui and Chen, Qisi and Zhu, Bin and Chen, Siheng and Lian, Defu and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={9236--9260},
  year={2024}
}

@article{barez2025open,
  title={Open Problems in Machine Unlearning for AI Safety},
  author={Barez, Fazl and Fu, Tingchen and Prabhu, Ameya and Casper, Stephen and Sanyal, Amartya and Bibi, Adel and O'Gara, Aidan and Kirk, Robert and Bucknall, Ben and Fist, Tim and others},
  journal={arXiv preprint arXiv:2501.04952},
  year={2025}
}

@misc{gal2024science,
  title = {Towards a Science of AI Evaluations},
  author = {Yarin Gal},
  year = {2024},
  howpublished = {\url{https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_98A8.html}},
  note = {Accessed: 2025-01-26}
}

@article{zhou2023revisiting,
  title={Revisiting Automated Prompting: Are We Actually Doing Better?},
  author={Zhou, Yulin and Zhao, Yiren and Shumailov, Ilia and Mullins, Robert and Gal, Yarin},
  journal={arXiv preprint arXiv:2304.03609},
  year={2023}
}

@article{leong2024no,
  title={No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks},
  author={Leong, Chak Tou and Cheng, Yi and Xu, Kaishuai and Wang, Jian and Wang, Hanlin and Li, Wenjie},
  journal={arXiv preprint arXiv:2405.16229},
  year={2024}
}


@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{cooper2024machine,
  title={Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy, Research, and Practice},
  author={Cooper, A Feder and Choquette-Choo, Christopher A and Bogen, Miranda and Jagielski, Matthew and Filippova, Katja and Liu, Ken Ziyu and Chouldechova, Alexandra and Hayes, Jamie and Huang, Yangsibo and Mireshghallah, Niloofar and others},
  journal={arXiv preprint arXiv:2412.06966},
  year={2024}
}

@misc{mckinney2025pbnj,
    title={Unlearning in Large Language Models via Activation Projections},
    author={Anonymous},
    journal={In submission at ICML},
    year={2025}
}


@misc{2025adversarialperspectivemachineunlearning,
      title={An Adversarial Perspective on Machine Unlearning for AI Safety}, 
      author={Jakub Łucki and Boyi Wei and Yangsibo Huang and Peter Henderson and Florian Tramèr and Javier Rando},
      year={2025},
      eprint={2409.18025},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.18025}, 
}


@misc{zhang2025catastrophicfailurellmunlearning,
      title={Catastrophic Failure of LLM Unlearning via Quantization}, 
      author={Zhiwei Zhang and Fali Wang and Xiaomin Li and Zongyu Wu and Xianfeng Tang and Hui Liu and Qi He and Wenpeng Yin and Suhang Wang},
      year={2025},
      eprint={2410.16454},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.16454}, 
}