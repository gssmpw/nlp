\section{Related Work}
\textbf{Latent-space attacks:} 
During a latent-space attack, an adversary can make modifications to a model's hidden activations.
Adversarial training under these attacks can improve the generality of a model's robustness ____.
In particular, ____, ____, and ____ use latent adversarial training to improve defenses against held-out types of adversarial attacks. 
Other work on activation engineering has involved making modifications to a model's behavior via simple transformations to their latent states ____. ____ also showed that unlearning methods can be brittle to quantization methods. 

\textbf{Weight-space (fine-tuning) attacks:} During a few-shot fine-tuning attack ____, an adversary can modify model weights via fine-tuning on a limited number of samples. 
For example, ____ showed that fine-tuning on as few as 10 samples could jailbreak GPT-3.5.
Many works have used few-shot fine-tuning attacks to elicit LLM capabilities that were previously suppressed by fine-tuning or unlearning ____.
% For example, ____ found that fine-tuning was a reliable way of eliciting hidden capabilities from a ``password locked'' model that would only exhibit certain capabilities if a specific ``password'' was present in the prompt.

\textbf{Capability elicitation and evaluation:} 
Research on adversarial capability elicitation ____ in LLMs has primarily been done in the context of machine unlearning ____ and jailbreaking ____. 
Here, we experiment in these two domains. However, capability elicitation has also been researched in the context of backdoors/trojans ____, ``password-locked models'' ____, and ``sandbagging'' ____.
In the unlearning field, several recent works have used adversarial methods to evaluate the robustness of unlearning algorithms ____.
Here, we build off of ____ who introduce WMDP-Bio, a benchmark for unlearning dual-use biotechnology knowledge from LLMs. 
% Compared to any of the above, our work is the first to systematically evaluate the relationships between different types adversarial vulnerabilities in LLMs and quantify what model tampering attacks can teach evaluators about LLM vulnerabilities to unforeseen failure modes. 
In the jailbreaking field, many techniques have been developed to make LLMs comply with harmful requests ____.
Here, we experiment with 9 open-source LLMs and a set of gradient-guided, perplexity-guided, and prosaic techniques from the adversarial attack literature (see \Cref{tab:attacks_defenses}).

% \textbf{Red-teaming unlearning methods:} Recent research on red-teaming unlearning methods have demonstrated brittleness of unlearning methods GA, NPO to quantization ____, and brittleness of RMU, NPO, DPO to adapted input-space and fine-tuning attacks ____. Additionally, ____ evaluates unlearning methods geared towards safe-guarding against fine-tuning, and shows that claims by these methods can be misleading. Our results complement research on adapting attacks for red-teaming unlearning methods, and developing more comprehensive evaluation of unlearning efficacy by benchmarking vulnerability across 8 methods. We find similar to ____ that TAR results suffer from significant utility drops, which calls for careful evaluation of capability suppression methods.