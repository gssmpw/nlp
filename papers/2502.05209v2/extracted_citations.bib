@misc{2025adversarialperspectivemachineunlearning,
      title={An Adversarial Perspective on Machine Unlearning for AI Safety}, 
      author={Jakub Łucki and Boyi Wei and Yangsibo Huang and Peter Henderson and Florian Tramèr and Javier Rando},
      year={2025},
      eprint={2409.18025},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.18025}, 
}

@article{arditi2024refusal,
  title={Refusal in Language Models Is Mediated by a Single Direction},
  author={Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Rimsky, Nina and Gurnee, Wes and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11717},
  year={2024}
}

@article{barez2025open,
  title={Open Problems in Machine Unlearning for AI Safety},
  author={Barez, Fazl and Fu, Tingchen and Prabhu, Ameya and Casper, Stephen and Sanyal, Amartya and Bibi, Adel and O'Gara, Aidan and Kirk, Robert and Bucknall, Ben and Fist, Tim and others},
  journal={arXiv preprint arXiv:2501.04952},
  year={2025}
}

@article{bhardwaj2023language,
  title={Language model unalignment: Parametric red-teaming to expose hidden harms and biases},
  author={Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2310.14303},
  year={2023}
}

@article{casper2024defending,
  title={Defending Against Unforeseen Failure Modes with Latent Adversarial Training},
  author={Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2403.05030},
  year={2024}
}

@article{chowdhury2024breaking,
  title={Breaking down the defenses: A comparative survey of attacks on large language models},
  author={Chowdhury, Arijit Ghosh and Islam, Md Mofijul and Kumar, Vaibhav and Shezan, Faysal Hossain and Jain, Vinija and Chadha, Aman},
  journal={arXiv preprint arXiv:2403.04786},
  year={2024}
}

@misc{deeb2024unlearningmethodsremoveinformation,
      title={Do Unlearning Methods Remove Information from Language Model Weights?}, 
      author={Aghyad Deeb and Fabien Roger},
      year={2024},
      eprint={2410.08827},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08827}, 
}

@article{greenblatt2024stress,
  title={Stress-Testing Capability Elicitation With Password-Locked Models},
  author={Greenblatt, Ryan and Roger, Fabien and Krasheninnikov, Dmitrii and Krueger, David},
  journal={arXiv preprint arXiv:2405.19550},
  year={2024}
}

@inproceedings{halawicovert,
  title={Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation},
  author={Halawi, Danny and Wei, Alexander and Wallace, Eric and Wang, Tony Tong and Haghtalab, Nika and Steinhardt, Jacob},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{hofstatter2025elicitation,
  title={The Elicitation Game: Evaluating Capability Elicitation Techniques},
  author={Hofst{\"a}tter, Felix and van der Weij, Teun and Teoh, Jayden and Bartsch, Henning and Ward, Francis Rhys},
  journal={arXiv preprint arXiv:2502.02180},
  year={2025}
}

@article{hu2024jogging,
  title={Jogging the Memory of Unlearned Model Through Targeted Relearning Attack},
  author={Hu, Shengyuan and Fu, Yiwei and Wu, Zhiwei Steven and Smith, Virginia},
  journal={arXiv preprint arXiv:2406.13356},
  year={2024}
}

@article{huang2024harmful,
  title={Harmful fine-tuning attacks and defenses for large language models: A survey},
  author={Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Liu, Ling},
  journal={arXiv preprint arXiv:2409.18169},
  year={2024}
}

@article{jain2023mechanistically,
  title={Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks},
  author={Jain, Samyak and Kirk, Robert and Lubana, Ekdeep Singh and Dick, Robert P and Tanaka, Hidenori and Grefenstette, Edward and Rockt{\"a}schel, Tim and Krueger, David Scott},
  journal={arXiv preprint arXiv:2311.12786},
  year={2023}
}

@misc{ji2024language,
      title={Language Models Resist Alignment}, 
      author={Jiaming Ji and Kaile Wang and Tianyi Qiu and Boyuan Chen and Jiayi Zhou and Changye Li and Hantao Lou and Yaodong Yang},
      year={2024},
      eprint={2406.06144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{jin2024jailbreakzoo,
  title={Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models},
  author={Jin, Haibo and Hu, Leyang and Li, Xinuo and Zhang, Peiyan and Chen, Chonghan and Zhuang, Jun and Wang, Haohan},
  journal={arXiv preprint arXiv:2407.01599},
  year={2024}
}

@article{lermen2023lora,
  title={LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B},
  author={Lermen, Simon and Rogers-Smith, Charlie and Ladish, Jeffrey},
  journal={arXiv preprint arXiv:2310.20624},
  year={2023}
}

@article{li2024wmdp,
  title={The wmdp benchmark: Measuring and reducing malicious use with unlearning},
  author={Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and others},
  journal={arXiv preprint arXiv:2403.03218},
  year={2024}
}

@article{lin2024against,
  title={Against The Achilles' Heel: A Survey on Red Teaming for Generative Models},
  author={Lin, Lizhi and Mu, Honglin and Zhai, Zenan and Wang, Minghan and Wang, Yuxia and Wang, Renxi and Gao, Junjie and Zhang, Yixuan and Che, Wanxiang and Baldwin, Timothy and others},
  journal={arXiv preprint arXiv:2404.00629},
  year={2024}
}

@article{liu2024rethinking,
  title={Rethinking Machine Unlearning for Large Language Models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@article{liu2024threats,
  title={Threats, attacks, and defenses in machine unlearning: A survey},
  author={Liu, Ziyao and Ye, Huanyi and Chen, Chen and Zheng, Yongsen and Lam, Kwok-Yan},
  journal={arXiv preprint arXiv:2403.13682},
  year={2024}
}

@article{lo2024large,
  title={Large language models relearn removed concepts},
  author={Lo, Michelle and Cohen, Shay B and Barez, Fazl},
  journal={arXiv preprint arXiv:2401.01814},
  year={2024}
}

@misc{lu2024investigating,
      title={Investigating Bias Representations in Llama 2 Chat via Activation Steering}, 
      author={Dawn Lu and Nina Rimsky},
      year={2024},
      eprint={2402.00402},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lucki2024adversarial,
  title={An Adversarial Perspective on Machine Unlearning for AI Safety},
  author={{\L}ucki, Jakub and Wei, Boyi and Huang, Yangsibo and Henderson, Peter and Tram{\`e}r, Florian and Rando, Javier},
  journal={arXiv preprint arXiv:2409.18025},
  year={2024}
}

@article{lynch2024eight,
  title={Eight methods to evaluate robust unlearning in llms},
  author={Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2402.16835},
  year={2024}
}

@article{patil2023can,
  title={Can sensitive information be deleted from llms? objectives for defending against extraction attacks},
  author={Patil, Vaidehi and Hase, Peter and Bansal, Mohit},
  journal={arXiv preprint arXiv:2309.17410},
  year={2023}
}

@article{peng2024navigating,
  title={Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models},
  author={Peng, ShengYun and Chen, Pin-Yu and Hull, Matthew and Chau, Duen Horng},
  journal={arXiv preprint arXiv:2405.17374},
  year={2024}
}

@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}

@article{qi2024evaluating,
  title={On Evaluating the Durability of Safeguards for Open-Weight LLMs},
  author={Qi, Xiangyu and Wei, Boyi and Carlini, Nicholas and Huang, Yangsibo and Xie, Tinghao and He, Luxi and Jagielski, Matthew and Nasr, Milad and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2412.07097},
  year={2024}
}

@misc{qi2024safety,
      title={Safety Alignment Should Be Made More Than Just a Few Tokens Deep}, 
      author={Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson},
      year={2024},
      eprint={2406.05946},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{sankaranarayanan2018regularizing,
  title={Regularizing deep networks using efficient layerwise adversarial training},
  author={Sankaranarayanan, Swami and Jain, Arpit and Chellappa, Rama and Lim, Ser Nam},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{schwinn2023adversarial,
      title={Adversarial Attacks and Defenses in Large Language Models: Old and New Threats}, 
      author={Leo Schwinn and David Dobre and Stephan Günnemann and Gauthier Gidel},
      year={2023},
      eprint={2310.19737},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{shayegani2023survey,
  title={Survey of vulnerabilities in large language models revealed by adversarial attacks},
  author={Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2310.10844},
  year={2023}
}

@article{sheshadri2024targeted,
  title={Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs},
  author={Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and others},
  journal={arXiv preprint arXiv:2407.15549},
  year={2024}
}

@article{shumailov2024ununlearning,
  title={UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI},
  author={Shumailov, Ilia and Hayes, Jamie and Triantafillou, Eleni and Ortiz-Jimenez, Guillermo and Papernot, Nicolas and Jagielski, Matthew and Yona, Itay and Howard, Heidi and Bagdasaryan, Eugene},
  journal={arXiv preprint arXiv:2407.00106},
  year={2024}
}

@misc{singh2019harnessing,
      title={Harnessing the Vulnerability of Latent Layers in Adversarially Trained Models}, 
      author={Mayank Singh and Abhishek Sinha and Nupur Kumari and Harshitha Machiraju and Balaji Krishnamurthy and Vineeth N Balasubramanian},
      year={2019},
      eprint={1905.05186},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{van2024ai,
  title={AI Sandbagging: Language Models can Strategically Underperform on Evaluations},
  author={van der Weij, Teun and Hofst{\"a}tter, Felix and Jaffe, Ollie and Brown, Samuel F and Ward, Francis Rhys},
  journal={arXiv preprint arXiv:2406.07358},
  year={2024}
}

@article{wang2023backdoor,
  title={Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment},
  author={Wang, Haoran and Shu, Kai},
  journal={arXiv preprint arXiv:2311.09433},
  year={2023}
}

@article{xhonneux2024efficient,
  title={Efficient Adversarial Training in LLMs with Continuous Attacks},
  author={Xhonneux, Sophie and Sordoni, Alessandro and G{\"u}nnemann, Stephan and Gidel, Gauthier and Schwinn, Leo},
  journal={arXiv preprint arXiv:2405.15589},
  year={2024}
}

@article{yang2023shadow,
  title={Shadow alignment: The ease of subverting safely-aligned language models},
  author={Yang, Xianjun and Wang, Xiao and Zhang, Qi and Petzold, Linda and Wang, William Yang and Zhao, Xun and Lin, Dahua},
  journal={arXiv preprint arXiv:2310.02949},
  year={2023}
}

@article{yi2024jailbreak,
  title={Jailbreak attacks and defenses against large language models: A survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}

@inproceedings{yi2024vulnerability,
  title={On the vulnerability of safety alignment in open-access llms},
  author={Yi, Jingwei and Ye, Rui and Chen, Qisi and Zhu, Bin and Chen, Siheng and Lian, Defu and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={9236--9260},
  year={2024}
}

@article{zeng2024beear,
  title={BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models},
  author={Zeng, Yi and Sun, Weiyu and Huynh, Tran Ngoc and Song, Dawn and Li, Bo and Jia, Ruoxi},
  journal={arXiv preprint arXiv:2406.17092},
  year={2024}
}

@article{zhan2023removing,
  title={Removing rlhf protections in gpt-4 via fine-tuning},
  author={Zhan, Qiusi and Fang, Richard and Bindu, Rohan and Gupta, Akul and Hashimoto, Tatsunori and Kang, Daniel},
  journal={arXiv preprint arXiv:2311.05553},
  year={2023}
}

@article{zhang2023adversarial,
  title={Adversarial Machine Learning in Latent Representations of Neural Networks},
  author={Zhang, Milin and Abdi, Mohammad and Restuccia, Francesco},
  journal={arXiv preprint arXiv:2309.17401},
  year={2023}
}

@article{zhang2024does,
  title={Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge},
  author={Zhang, Zhiwei and Wang, Fali and Li, Xiaomin and Wu, Zongyu and Tang, Xianfeng and Liu, Hui and He, Qi and Yin, Wenpeng and Wang, Suhang},
  journal={arXiv preprint arXiv:2410.16454},
  year={2024}
}

@misc{zhang2025catastrophicfailurellmunlearning,
      title={Catastrophic Failure of LLM Unlearning via Quantization}, 
      author={Zhiwei Zhang and Fali Wang and Xiaomin Li and Zongyu Wu and Xianfeng Tang and Hui Liu and Qi He and Wenpeng Yin and Suhang Wang},
      year={2025},
      eprint={2410.16454},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.16454}, 
}

@article{zhao2024survey,
  title={A survey of backdoor attacks and defenses on large language models: Implications for security measures},
  author={Zhao, Shuai and Jia, Meihuizi and Guo, Zhongliang and Gan, Leilei and Xu, Xiaoyu and Wu, Xiaobao and Fu, Jie and Feng, Yichao and Pan, Fengjun and Tuan, Luu Anh},
  journal={arXiv preprint arXiv:2406.06852},
  year={2024}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

