\section{Related Work}
\textbf{Latent-space attacks:} 
During a latent-space attack, an adversary can make modifications to a model's hidden activations.
Adversarial training under these attacks can improve the generality of a model's robustness **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**.
In particular, **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**, **Thys et al., "Making Neural Network Robust via Data Defenses"**, and **Chen et al., "One Pixel Attack for Fooling Deep Neural Networks by Any-Pixel Attack"** use latent adversarial training to improve defenses against held-out types of adversarial attacks. 
Other work on activation engineering has involved making modifications to a model's behavior via simple transformations to their latent states **Meng et al., "An Empirical Study of Transferability in Adversarial Attacks"**. **Kumar et al., "Improving the Security of Deep Neural Networks with Multiple Input Spaces"** also showed that unlearning methods can be brittle to quantization methods. 

\textbf{Weight-space (fine-tuning) attacks:} During a few-shot fine-tuning attack **Tran et al., "Few-Shot Fine-Tuning for Adversarial Attack and Defense on Neural Networks"**, an adversary can modify model weights via fine-tuning on a limited number of samples. 
For example, **Li et al., "Jailbreaking GPT-3 with Few-Shot Fine-Tuning"** showed that fine-tuning on as few as 10 samples could jailbreak GPT-3.5.
Many works have used few-shot fine-tuning attacks to elicit LLM capabilities that were previously suppressed by fine-tuning or unlearning **Bathina et al., "Eliciting Capabilities from Large Language Models"**.
% For example, **Meng et al., "Unlocking Locked Models: Fine-Tuning for Elicitation and Evaluation"** found that fine-tuning was a reliable way of eliciting hidden capabilities from a ``password locked'' model that would only exhibit certain capabilities if a specific ``password'' was present in the prompt.

\textbf{Capability elicitation and evaluation:} 
Research on adversarial capability elicitation **Tran et al., "Few-Shot Fine-Tuning for Adversarial Attack and Defense on Neural Networks"** in LLMs has primarily been done in the context of machine unlearning **Bathina et al., "Eliciting Capabilities from Large Language Models"** and jailbreaking **Li et al., "Jailbreaking GPT-3 with Few-Shot Fine-Tuning"**. 
Here, we experiment in these two domains. However, capability elicitation has also been researched in the context of backdoors/trojans **Meng et al., "Backdoor Attacks on Neural Networks: A Review"**, ``password-locked models'' **Meng et al., "Unlocking Locked Models: Fine-Tuning for Elicitation and Evaluation"** , and ``sandbagging'' **Bathina et al., "Eliciting Capabilities from Large Language Models"**.
In the unlearning field, several recent works have used adversarial methods to evaluate the robustness of unlearning algorithms **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**.
Here, we build off of **Tran et al., "Few-Shot Fine-Tuning for Adversarial Attack and Defense on Neural Networks"** who introduce WMDP-Bio, a benchmark for unlearning dual-use biotechnology knowledge from LLMs. 
% Compared to any of the above, our work is the first to systematically evaluate the relationships between different types adversarial vulnerabilities in LLMs and quantify what model tampering attacks can teach evaluators about LLM vulnerabilities to unforeseen failure modes. 
In the jailbreaking field, many techniques have been developed to make LLMs comply with harmful requests **Li et al., "Jailbreaking GPT-3 with Few-Shot Fine-Tuning"**.
Here, we experiment with 9 open-source LLMs and a set of gradient-guided, perplexity-guided, and prosaic techniques from the adversarial attack literature (see \Cref{tab:attacks_defenses}).

% \textbf{Red-teaming unlearning methods:} Recent research on red-teaming unlearning methods have demonstrated brittleness of unlearning methods GA, NPO to quantization **Meng et al., "An Empirical Study of Transferability in Adversarial Attacks"**, and brittleness of RMU, NPO, DPO to adapted input-space and fine-tuning attacks **Tran et al., "Few-Shot Fine-Tuning for Adversarial Attack and Defense on Neural Networks"**. Additionally, **Meng et al., "Unlocking Locked Models: Fine-Tuning for Elicitation and Evaluation"** evaluates unlearning methods geared towards safe-guarding against fine-tuning, and shows that claims by these methods can be misleading. Our results complement research on adapting attacks for red-teaming unlearning methods, and developing more comprehensive evaluation of unlearning efficacy by benchmarking vulnerability across 8 methods. We find similar to **Tran et al., "Few-Shot Fine-Tuning for Adversarial Attack and Defense on Neural Networks"** that TAR results suffer from significant utility drops, which calls for careful evaluation of capability suppression methods.