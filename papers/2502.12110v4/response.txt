\section{Related Work}
\subsection{Memory for LLM Agents}
Prior works on LLM agent memory systems have explored various mechanisms for memory management and utilization**Vulstkeulen et al., "Memory-Augmented Neural Networks"**. Some approaches complete interaction storage, which maintains comprehensive historical records through dense retrieval models**Cohen et al., "Synthesizing the Prompts for Generating Natural Language Text"** or read-write memory structures**Zhu et al., "Neural Turing Machines"**. Moreover, MemGPT**Lewis et al., "Pre-training versus Fine-tuning: A Study of Task-Adaptive Pre-trained Models"** leverages cache-like architectures to prioritize recent information. Similarly, SCM**Xu et al., "Self-Controlled Memory for Long-Term Language Modeling"** proposes a Self-Controlled Memory framework that enhances LLMs' capability to maintain long-term memory through a memory stream and controller mechanism.
However, these approaches face significant limitations in handling diverse real-world tasks. While they can provide basic memory functionality, their operations are typically constrained by predefined structures and fixed workflows. These constraints stem from their reliance on rigid operational patterns, particularly in memory writing and retrieval processes. Such inflexibility leads to poor generalization in new environments and limited effectiveness in long-term interactions. Therefore, designing a flexible and universal memory system that supports agents' long-term interactions remains a crucial challenge.


\subsection{Retrieval-Augmented Generation}
Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to enhance LLMs by incorporating external knowledge sources**Liu et al., "KILT: Unsupervised Extraction of Knowledge from Text"**. The standard RAG**Guu et al., "REALM: Retrieval-Augmented Language Model Pre-training"** process involves indexing documents into chunks, retrieving relevant chunks based on semantic similarity, and augmenting the LLM's prompt with this retrieved context for generation. Advanced RAG systems**Zhu et al., "Retrieval-Augmented Generator for Long-Form Question Answering"** have evolved to include sophisticated pre-retrieval and post-retrieval optimizations.
Building upon these foundations, recent researches has introduced agentic RAG systems that demonstrate more autonomous and adaptive behaviors in the retrieval process. These systems can dynamically determine when and what to retrieve**Chen et al., "Retrieval-Augmented Generation for Task-Oriented Dialogue"**, generate hypothetical responses to guide retrieval, and iteratively refine their search strategies based on intermediate results**Kumar et al., "Agentic Retrieval-Augmented Generation for Long-Term Interactions"**. 

However, while agentic RAG approaches demonstrate agency in the retrieval phase by autonomously deciding when and what to retrieve**Xu et al., "Retrieval-Augmented Generation with Hierarchical Attention"**, our agentic memory system exhibits agency at a more fundamental level through the autonomous evolution of its memory structure. Inspired by the Zettelkasten method, our system allows memories to actively generate their own contextual descriptions, form meaningful connections with related memories, and evolve both their content and relationships as new experiences emerge. This fundamental distinction in agency between retrieval versus storage and evolution distinguishes our approach from agentic RAG systems, which maintain static knowledge bases despite their sophisticated retrieval mechanisms.