\section{Related Work}
Representation theory applied to neural networks has been studied both theoretically **Bourbaki, "Elements of Mathematics"** and applied to a variety of groups, architectures and data type: CNNs **LeCun et al., "Backpropagation Applied to Adaptive Filtering"**, Graph Neural Networks **Kipf & Welling, "Semi-Supervised Classification with Graph Convolutional Networks"**, Transformers,  **Vaswani et al., "Attention Is All You Need"**, point clouds **Maturana & Scherer, " VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition"** , 
chemistry **Gilmer et al., "Neural Message Passing for Quantum Chemistry"**.
On the topic of disentanglement of group action and symmetry learning, **Bartolomé et al., "Group Equivariant Convolutional Networks"** factorize a Lie group from the orbits in data space,
while **Hewett & Osburn, "Equivariant Representations for Neural Networks"** learn through an autoencoder architecture invariant and equivariant representations of any group acting on the data. 
**Kim et al., "Learning Disentangled Representations with Non-Autonomous Systems"** learns disentangled representations solely from data pairs.
**Koch-Li & Ranzato, "Geometric Deep Learning: Grids, Groups, Graphs, Geometries"** propose an architecture based on Lie algebras that can automatically discover symmetries from data.
**Gómez-Bombarelli et al., "Automatic Discovery of the Parameters and Molecular Geometry through a Generalized Semiempirical Hamiltonian"** predict molecular conformations from molecular graphs
in an roto-translation invariant fashion with equivariant Markov kernels.

In the context of interpreting the latent space of diffusion models, **Hoogeboom et al., "Diffusion-based Generative Models of Discrete Data"** explores the local structure of the latent space (trajectory) of diffusion models using Riemannian geometry. Specifically, the authors assign a basis to each point in the tangent space through singular value decomposition (SVD), and show that these directions correspond to semantically meaningful features for image-based models. 
Similarly, **Song et al., "Diffusion Models of Nonlinear Transformations"** propose a method to uncover semantically meaningful directions in the semantic latent space ($h$-space)  **of denoising diffusion models (DDMs)** by PCA. **Nijkamp et al., "Understanding Discrete Diffusion Processes"** propose a method to  learn disentangled and interpretable latent representations of diffusion models in an unsupervised way. 
We note that the aforementioned works aim to extract meaningful latent factors  **in traditional DDMs, often restricting to human-interpretable semantic features and focusing on image generation**.

Related to our study is the field of diffusion on Riemannian manifolds. **Bhattacharya & Majumdar, "Diffusion Processes on Manifolds"** propose diffusion in a product space -- a condition which is not a necessary in our framework -- defined by the flow coordinates in the respective Riemannian sub-manifolds. When the Riemannian manifold is a Lie group, 
their method yields dynamics similar to ours, as illustrated in an example in Section \ref{ss:torsion}. 
In fact, our formalism could be combined with their approach to obtain Lie algebra-induced dynamics on Riemannian manifolds, yielding a unified framework for modeling diffusion processes on a broader class of manifolds.
In fact, our formalism could be integrated with their approach to create a unified framework for diffusion processes on the broader class of Riemannian manifolds admitting a Lie group action. 
These techniques has been applied in a variety of use cases  **Duan et al., "High-Performance Protein-Ligand Docking Using Spatial Autoregression"** for protein docking, ligand and protein generation.
The works  
**Cotter & Reich, "Hamiltonian Monte Carlo for Nonlinear Differential Equations"**
leverage trivialized momentum to perform diffusion on the Lie algebra (isomorphic to \(\mathbb{R}^n\)) instead of the Lie group, thereby eliminating curvature terms, although their approach is to date only feasible for Abelian groups. 
An interesting connection with our work is the work of  **Chen et al., "Diffusion-based Generative Models for Nonlinear Transformations"**: the authors propose a bijection to map a non-linear problem to a linear one, to approximate a bridge between two non-trivial distributions. Our case can be seen as a bijection between the (curved) Lie group manifold and the (flat) Euclidean data space.