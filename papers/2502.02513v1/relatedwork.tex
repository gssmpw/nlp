\section{Related Work}
Representation theory applied to neural networks has been studied both theoretically \citep{esteves2020theoretical, chughtai2023toy, puny2021frame, smidt2021euclidean} and applied to a variety of groups, architectures and data type: CNNs \citep{cohen2016group, romero2020attentive, liao2023lie, finzi2020generalizing,  weiler2019general, weiler20183d}, Graph Neural Networks \citep{satorras2021n}, Transformers,  \citep{geiger2022e3nn, romero2020group, hutchinson2021lietransformer}, point clouds \citep{thomas2018tensor}, 
chemistry \citep{schutt2021equivariant, le2022equivariant}.
On the topic of disentanglement of group action and symmetry learning, \citet{pfau2020disentangling} factorize a Lie group from the orbits in data space,
while \citet{winter2022unsupervised} learn through an autoencoder architecture invariant and equivariant representations of any group acting on the data. 
\citet{fumero2021learning} learns disentangled representations solely from data pairs.
\citet{dehmamy2021automatic} propose an architecture based on Lie algebras that can automatically discover symmetries from data.
\citet{xu2022geodiff} predict molecular conformations from molecular graphs
in an roto-translation invariant fashion with equivariant Markov kernels.

In the context of interpreting the latent space of diffusion models, \citet{park2023understanding} explores the local structure of the latent space (trajectory) of diffusion models using Riemannian geometry. Specifically, the authors assign a basis to each point in the tangent space through singular value decomposition (SVD), and show that these directions correspond to semantically meaningful features for image-based models. 
Similarly, \citet{haas2024discovering} propose a method to uncover semantically meaningful directions in the semantic latent space ($h$-space) \citep{wang2023infodiffusion} of denoising diffusion models (DDMs) by PCA. \citet{wang2023infodiffusion} propose a method to  learn disentangled and interpretable latent representations of diffusion models in an unsupervised way. 
We note that the aforementioned works aim to extract meaningful latent factors \cite{bertolini2023explaining, bertolini2024enhancing} in traditional DDMs, often restricting to human-interpretable semantic features and focusing on image generation.

Related to our study is the field of diffusion on Riemannian manifolds. \citet{de2022riemannian} propose diffusion in a product space -- a condition which is not a necessary in our framework -- defined by the flow coordinates in the respective Riemannian sub-manifolds. When the Riemannian manifold is a Lie group, 
their method yields dynamics similar to ours, as illustrated in an example in Section \ref{ss:torsion}. 
In fact, our formalism could be combined with their approach to obtain Lie algebra-induced dynamics on Riemannian manifolds, yielding a unified framework for modeling diffusion processes on a broader class of manifolds.
In fact, our formalism could be integrated with their approach to create a unified framework for diffusion processes on the broader class of Riemannian manifolds admitting a Lie group action. 
These techniques has been applied in a variety of use cases \citep{corso2022diffdock, ketata2023diffdock, pmlr-v202-yim23a, jing2022torsional} for protein docking, ligand and protein generation.
The works \citet{zhu2024trivialized, kong2024convergence}
leverage trivialized momentum to perform diffusion on the Lie algebra (isomorphic to \(\mathbb{R}^n\)) instead of the Lie group, thereby eliminating curvature terms, although their approach is to date only feasible for Abelian groups. 
An interesting connection with our work is the work of \citet{kim2022maximum}: the authors propose a bijection to map a non-linear problem to a linear one, to approximate a bridge between two non-trivial distributions. Our case can be seen as a bijection between the (curved) Lie group manifold and the (flat) Euclidean data space.