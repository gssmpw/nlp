@inproceedings{
leach2022denoising,
title={Denoising Diffusion Probabilistic Models on {SO}(3) for Rotational Alignment},
author={Adam Leach and Sebastian M Schmon and Matteo T. Degiacomi and Chris G. Willcocks},
booktitle={ICLR 2022 Workshop on Geometrical and Topological Representation Learning},
year={2022},
url={https://openreview.net/forum?id=BY88eBbkpe5}
}

@InProceedings{pmlr-v202-yim23a,
  title = 	 {{SE}(3) diffusion model with application to protein backbone generation},
  author =       {Yim, Jason and Trippe, Brian L. and De Bortoli, Valentin and Mathieu, Emile and Doucet, Arnaud and Barzilay, Regina and Jaakkola, Tommi},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {40001--40039},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/yim23a/yim23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/yim23a.html},
  abstract = 	 {The design of novel protein structures remains a challenge in protein engineering for applications across biomedicine and chemistry. In this line of work, a diffusion model over rigid bodies in 3D (referred to as frames) has shown success in generating novel, functional protein backbones that have not been observed in nature. However, there exists no principled methodological framework for diffusion on SE(3), the space of orientation preserving rigid motions in R3, that operates on frames and confers the group invariance. We address these shortcomings by developing theoretical foundations of SE(3) invariant diffusion models on multiple frames followed by a novel framework, FrameDiff, for estimating the SE(3) equivariant score over multiple frames. We apply FrameDiff on monomer backbone generation and find it can generate designable monomers up to 500 amino acids without relying on a pretrained protein structure prediction network that has been integral to previous methods. We find our samples are capable of generalizing beyond any known protein structure.}
}


@InProceedings{Li_2023_CVPR,
    author    = {Li, Bo and Xue, Kaitao and Liu, Bin and Lai, Yu-Kun},
    title     = {BBDM: Image-to-Image Translation With Brownian Bridge Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {1952-1961}
}

@inproceedings{
dhariwal_guidance_cosine,
title={Diffusion Models Beat {GAN}s on Image Synthesis},
author={Prafulla Dhariwal and Alexander Quinn Nichol},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=AAWuCvzaVt}
}




@article{uff_rappe_92,
author = {Rappe, A. K. and Casewit, C. J. and Colwell, K. S. and Goddard, W. A. III and Skiff, W. M.},
title = {UFF, a full periodic table force field for molecular mechanics and molecular dynamics simulations},
journal = {Journal of the American Chemical Society},
volume = {114},
number = {25},
pages = {10024-10035},
year = {1992},
doi = {10.1021/ja00051a040},
URL = { 
    
        https://doi.org/10.1021/ja00051a040
},
eprint = { 
    
        https://doi.org/10.1021/ja00051a040
}
}


@article{Rackers_2023,
doi = {10.1088/2632-2153/acb314},
url = {https://dx.doi.org/10.1088/2632-2153/acb314},
year = {2023},
month = {feb},
publisher = {IOP Publishing},
volume = {4},
number = {1},
pages = {015027},
author = {Joshua A Rackers and Lucas Tecot and Mario Geiger and Tess E Smidt},
title = {A recipe for cracking the quantum scaling limit with machine learned electron densities},
journal = {Machine Learning: Science and Technology},
abstract = {A long-standing goal of science is to accurately simulate large molecular systems using quantum mechanics. The poor scaling of current quantum chemistry algorithms on classical computers, however, imposes an effective limit of about a few dozen atoms on traditional electronic structure calculations. We present a machine learning (ML) method to break through this scaling limit for electron densities. We show that Euclidean neural networks can be trained to predict molecular electron densities from limited data. By learning the electron density, the model can be trained on small systems and make accurate predictions on large ones. In the context of water clusters, we show that an ML model trained on clusters of just 12 molecules contains all the information needed to make accurate electron density predictions on cluster sizes of 50 or more, beyond the scaling limit of current quantum chemistry methods.}
}

@Article{Ramakrishnan2014,
author={Ramakrishnan, Raghunathan
and Dral, Pavlo O.
and Rupp, Matthias
and von Lilienfeld, O. Anatole},
title={Quantum chemistry structures and properties of 134 kilo molecules},
journal={Scientific Data},
year={2014},
month={Aug},
day={05},
volume={1},
number={1},
pages={140022},
abstract={Computational de novo design of new drugs and materials requires rigorous and unbiased exploration of chemical compound space. However, large uncharted territories persist due to its size scaling combinatorially with molecular size. We report computed geometric, energetic, electronic, and thermodynamic properties for 134k stable small organic molecules made up of CHONF. These molecules correspond to the subset of all 133,885 species with up to nine heavy atoms (CONF) out of the GDB-17 chemical universe of 166 billion organic molecules. We report geometries minimal in energy, corresponding harmonic frequencies, dipole moments, polarizabilities, along with energies, enthalpies, and free energies of atomization. All properties were calculated at the B3LYP/6-31G(2df,p) level of quantum chemistry. Furthermore, for the predominant stoichiometry, C7H10O2, there are 6,095 constitutional isomers among the 134k molecules. We report energies, enthalpies, and free energies of atomization at the more accurate G4MP2 level of theory for all of them. As such, this data set provides quantum chemical properties for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships.},
issn={2052-4463},
doi={10.1038/sdata.2014.22},
url={https://doi.org/10.1038/sdata.2014.22}
}



@inproceedings{welling_11_ld,
author = {Welling, Max and Teh, Yee Whye},
title = {Bayesian learning via stochastic gradient langevin dynamics},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {681–688},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@article{neal_mcmc,
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis
algorithm, thereby avoiding the slow exploration of the state space that results from
the diffusive behaviour of simple random-walk proposals. Though originating in
physics, Hamiltonian dynamics can be applied to most problems with continuous
state spaces by simply introducing fictitious “momentum” variables. A key to
its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories
can thus be used to define complex mappings without the need to account for a
hard-to-compute Jacobian factor — a property that can be exactly maintained
even when the dynamics is approximated by discretizing time. In this review, I
discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present
some of its variations, including using windows of states for deciding on acceptance
or rejection, computing trajectories using fast approximations, tempering during
the course of a trajectory to handle isolated modes, and short-cut methods that
prevent useless trajectories from taking much computation time.
},
  added-at = {2013-11-20T16:22:16.000+0100},
  author = {Neal, Radford M.},
  biburl = {https://www.bibsonomy.org/bibtex/233fcb1d5a6d675e371170820f0b4dbcd/giacomo.fiumara},
  citeulike-article-id = {12118398},
  interhash = {f26b3ec7720399d06a5c271916a0ee92},
  intrahash = {33fcb1d5a6d675e371170820f0b4dbcd},
  journal = {Handbook of Markov Chain Monte Carlo},
  keywords = {Markov MonteCarlo},
  pages = {113--162},
  posted-at = {2013-03-06 23:35:00},
  priority = {2},
  timestamp = {2013-11-20T16:22:16.000+0100},
  title = {{MCMC} Using {Hamiltonian} Dynamics},
  volume = 54,
  year = 2010
}


@ARTICLE{vincent_dae,
  author={Vincent, Pascal},
  journal={Neural Computation}, 
  title={A Connection Between Score Matching and Denoising Autoencoders}, 
  year={2011},
  volume={23},
  number={7},
  pages={1661-1674},
  doi={10.1162/NECO_a_00142}}

@inproceedings{song_2019,
 author = {Song, Yang and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Modeling by Estimating Gradients of the Data Distribution},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Bordes2017LearningTG,
  title={Learning to Generate Samples from Noise through Infusion Training},
  author={Florian Bordes and Sina Honari and Pascal Vincent},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.06975},
  url={https://api.semanticscholar.org/CorpusID:3073252}
}

@inproceedings{Goyal2017VariationalWL,
  title={Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net},
  author={Anirudh Goyal and Nan Rosemary Ke and Surya Ganguli and Yoshua Bengio},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:27248616}
}

@inproceedings{Du2019ImplicitGA,
  title={Implicit Generation and Modeling with Energy Based Models},
  author={Yilun Du and Igor Mordatch},
  booktitle={Neural Information Processing Systems},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:202765664}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{song2020score,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{bertolini2024enhancing,
  title={Enhancing Interpretability in Molecular Property Prediction with Contextual Explanations of Molecular Graphical Depictions},
  author={Bertolini, Marco and Zhao, Linlin and Montanari, Floriane and Clevert, Djork-Arn{\'e}},
  booktitle={International Workshop on AI in Drug Discovery},
  pages={1--12},
  year={2024},
  organization={Springer}
}

@article{anderson1982reverse,
  title={Reverse-time diffusion equation models},
  author={Anderson, Brian DO},
  journal={Stochastic Processes and their Applications},
  volume={12},
  number={3},
  pages={313--326},
  year={1982},
  publisher={Elsevier}
}

@article{karpatne2018machine,
  title={Machine learning for the geosciences: Challenges and opportunities},
  author={Karpatne, Anuj and Ebert-Uphoff, Imme and Ravela, Sai and Babaie, Hassan Ali and Kumar, Vipin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={31},
  number={8},
  pages={1544--1554},
  year={2018},
  publisher={IEEE}
}

@article{zhang2024diffpack,
  title={Diffpack: A torsional diffusion model for autoregressive protein side-chain packing},
  author={Zhang, Yangtian and Zhang, Zuobai and Zhong, Bozitao and Misra, Sanchit and Tang, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{brehmer2020flows,
  title={Flows for simultaneous manifold learning and density estimation},
  author={Brehmer, Johann and Cranmer, Kyle},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={442--453},
  year={2020}
}

@article{klimovskaia2020poincare,
  title={Poincar{\'e} maps for analyzing complex hierarchies in single-cell data},
  author={Klimovskaia, Anna and Lopez-Paz, David and Bottou, L{\'e}on and Nickel, Maximilian},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={2966},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{esteves2020theoretical,
  title={Theoretical aspects of group equivariant neural networks},
  author={Esteves, Carlos},
  journal={arXiv preprint arXiv:2004.05154},
  year={2020}
}

@article{pfau2020disentangling,
  title={Disentangling by subspace diffusion},
  author={Pfau, David and Higgins, Irina and Botev, Alex and Racani{\`e}re, S{\'e}bastien},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17403--17415},
  year={2020}
}

@article{abramson2024accurate,
  title={Accurate structure prediction of biomolecular interactions with AlphaFold 3},
  author={Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick, Joshua and others},
  journal={Nature},
  pages={1--3},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{kim2022maximum,
  title={Maximum likelihood training of implicit nonlinear diffusion model},
  author={Kim, Dongjun and Na, Byeonghu and Kwon, Se Jung and Lee, Dongsoo and Kang, Wanmo and Moon, Il-Chul},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={32270--32284},
  year={2022}
}

@article{huang2022riemannian,
  title={Riemannian diffusion models},
  author={Huang, Chin-Wei and Aghajohari, Milad and Bose, Joey and Panangaden, Prakash and Courville, Aaron C},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2750--2761},
  year={2022}
}

@article{zhu2024trivialized,
  title={Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie Groups},
  author={Zhu, Yuchen and Chen, Tianrong and Kong, Lingkai and Theodorou, Evangelos A and Tao, Molei},
  journal={arXiv preprint arXiv:2405.16381},
  year={2024}
}

@article{blumenson1960derivation,
  title={A derivation of n-dimensional spherical coordinates},
  author={Blumenson, LE},
  journal={The American Mathematical Monthly},
  volume={67},
  number={1},
  pages={63--66},
  year={1960},
  publisher={JSTOR}
}

@article{xu2022geodiff,
  title={Geodiff: A geometric diffusion model for molecular conformation generation},
  author={Xu, Minkai and Yu, Lantao and Song, Yang and Shi, Chence and Ermon, Stefano and Tang, Jian},
  journal={arXiv preprint arXiv:2203.02923},
  year={2022}
}

@article{kong2024convergence,
  title={Convergence of kinetic langevin monte carlo on lie groups},
  author={Kong, Lingkai and Tao, Molei},
  journal={arXiv preprint arXiv:2403.12012},
  year={2024}
}

@article{yim2023se,
  title={SE (3) diffusion model with application to protein backbone generation},
  author={Yim, Jason and Trippe, Brian L and De Bortoli, Valentin and Mathieu, Emile and Doucet, Arnaud and Barzilay, Regina and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2302.02277},
  year={2023}
}

@article{park2023understanding,
  title={Understanding the latent space of diffusion models through the lens of riemannian geometry},
  author={Park, Yong-Hyun and Kwon, Mingi and Choi, Jaewoong and Jo, Junghyo and Uh, Youngjung},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={24129--24142},
  year={2023}
}

@inproceedings{bertolini2023explaining,
  title={Explaining, evaluating and enhancing neural networks’ learned representations},
  author={Bertolini, Marco and Clevert, Djork-Arn{\'e} and Montanari, Floriane},
  booktitle={International Conference on Artificial Neural Networks},
  pages={269--287},
  year={2023},
  organization={Springer}
}

@article{kwon2022diffusion,
  title={Diffusion models already have a semantic latent space},
  author={Kwon, Mingi and Jeong, Jaeseok and Uh, Youngjung},
  journal={arXiv preprint arXiv:2210.10960},
  year={2022}
}

@article{balabin2023disentanglement,
  title={Disentanglement learning via topology},
  author={Balabin, Nikita and Voronkova, Daria and Trofimov, Ilya and Burnaev, Evgeny and Barannikov, Serguei},
  journal={arXiv preprint arXiv:2308.12696},
  year={2023}
}

@article{puny2021frame,
  title={Frame averaging for invariant and equivariant network design},
  author={Puny, Omri and Atzmon, Matan and Ben-Hamu, Heli and Misra, Ishan and Grover, Aditya and Smith, Edward J and Lipman, Yaron},
  journal={arXiv preprint arXiv:2110.03336},
  year={2021}
}

@book{sarkka2019applied,
  title={Applied stochastic differential equations},
  author={S{\"a}rkk{\"a}, Simo and Solin, Arno},
  volume={10},
  year={2019},
  publisher={Cambridge University Press}
}

@article{brigo2008general,
  title={The general mixture-diffusion SDE and its relationship with an uncertain-volatility option model with volatility-asset decorrelation},
  author={Brigo, Damiano},
  journal={arXiv preprint arXiv:0812.4052},
  year={2008}
}

@inproceedings{cohen2016group,
  title={Group equivariant convolutional networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={International conference on machine learning},
  pages={2990--2999},
  year={2016},
  organization={PMLR}
}

@article{de2022riemannian,
  title={Riemannian score-based generative modelling},
  author={De Bortoli, Valentin and Mathieu, Emile and Hutchinson, Michael and Thornton, James and Teh, Yee Whye and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2406--2422},
  year={2022}
}

@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{kac1983invariant,
  title={The invariant bilinear form and the generalized Casimir operator},
  author={Kac, Victor G and Kac, Victor G},
  journal={Infinite Dimensional Lie Algebras: An Introduction},
  pages={14--24},
  year={1983},
  publisher={Springer}
}

@article{jing2022torsional,
  title={Torsional diffusion for molecular conformer generation},
  author={Jing, Bowen and Corso, Gabriele and Chang, Jeffrey and Barzilay, Regina and Jaakkola, Tommi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24240--24253},
  year={2022}
}

@inproceedings{song2020sliced,
  title={Sliced score matching: A scalable approach to density and score estimation},
  author={Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={574--584},
  year={2020},
  organization={PMLR}
}

@article{pang2020efficient,
  title={Efficient learning of generative models via finite-difference score matching},
  author={Pang, Tianyu and Xu, Kun and Li, Chongxuan and Song, Yang and Ermon, Stefano and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19175--19188},
  year={2020}
}

@article{peluchetti2023non,
  title={Non-denoising forward-time diffusions},
  author={Peluchetti, Stefano},
  journal={arXiv preprint arXiv:2312.14589},
  year={2023}
}

@inproceedings{
corso2022diffdock,
title={DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking},
author={Gabriele Corso and Hannes St{\"a}rk and Bowen Jing and Regina Barzilay and Tommi S. Jaakkola},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=kKF8_K-mBbS}
}

@article{ketata2023diffdock,
  title={Diffdock-pp: Rigid protein-protein docking with diffusion models},
  author={Ketata, Mohamed Amine and Laue, Cedrik and Mammadov, Ruslan and St{\"a}rk, Hannes and Wu, Menghua and Corso, Gabriele and Marquet, C{\'e}line and Barzilay, Regina and Jaakkola, Tommi S},
  journal={arXiv preprint arXiv:2304.03889},
  year={2023}
}

@article{le2022equivariant,
  title={Equivariant graph attention networks for molecular property prediction},
  author={Le, Tuan and No{\'e}, Frank and Clevert, Djork-Arn{\'e}},
  journal={arXiv preprint arXiv:2202.09891},
  year={2022}
}
@inproceedings{
le2022representation,
title={Representation Learning on Biomolecular Structures using Equivariant Graph Attention},
author={Tuan Le and Frank Noe and Djork-Arn{\'e} Clevert},
booktitle={The First Learning on Graphs Conference},
year={2022},
url={https://openreview.net/forum?id=kv4xUo5Pu6}
}

@inproceedings{hutchinson2021lietransformer,
  title={Lietransformer: Equivariant self-attention for lie groups},
  author={Hutchinson, Michael J and Le Lan, Charline and Zaidi, Sheheryar and Dupont, Emilien and Teh, Yee Whye and Kim, Hyunjik},
  booktitle={International Conference on Machine Learning},
  pages={4533--4543},
  year={2021},
  organization={PMLR}
}

@article{dehmamy2021automatic,
  title={Automatic symmetry discovery with lie algebra convolutional network},
  author={Dehmamy, Nima and Walters, Robin and Liu, Yanchen and Wang, Dashun and Yu, Rose},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2503--2515},
  year={2021}
}

@inproceedings{romero2020attentive,
  title={Attentive group equivariant convolutional networks},
  author={Romero, David and Bekkers, Erik and Tomczak, Jakub and Hoogendoorn, Mark},
  booktitle={International Conference on Machine Learning},
  pages={8188--8199},
  year={2020},
  organization={PMLR}
}

@inproceedings{satorras2021n,
  title={E (n) equivariant graph neural networks},
  author={Satorras, V{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle={International conference on machine learning},
  pages={9323--9332},
  year={2021},
  organization={PMLR}
}

@article{smidt2021euclidean,
  title={Euclidean symmetry and equivariance in machine learning},
  author={Smidt, Tess E},
  journal={Trends in Chemistry},
  volume={3},
  number={2},
  pages={82--85},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{schutt2021equivariant,
  title={Equivariant message passing for the prediction of tensorial properties and molecular spectra},
  author={Sch{\"u}tt, Kristof and Unke, Oliver and Gastegger, Michael},
  booktitle={International Conference on Machine Learning},
  pages={9377--9388},
  year={2021},
  organization={PMLR}
}

@article{weiler2019general,
  title={General e (2)-equivariant steerable cnns},
  author={Weiler, Maurice and Cesa, Gabriele},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{weiler20183d,
  title={3d steerable cnns: Learning rotationally equivariant features in volumetric data},
  author={Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco S},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{romero2020group,
  title={Group equivariant stand-alone self-attention for vision},
  author={Romero, David W and Cordonnier, Jean-Baptiste},
  journal={arXiv preprint arXiv:2010.00977},
  year={2020}
}

@inproceedings{wang2023infodiffusion,
  title={Infodiffusion: Representation learning using information maximizing diffusion models},
  author={Wang, Yingheng and Schiff, Yair and Gokaslan, Aaron and Pan, Weishen and Wang, Fei and De Sa, Christopher and Kuleshov, Volodymyr},
  booktitle={International Conference on Machine Learning},
  pages={36336--36354},
  year={2023},
  organization={PMLR}
}

@inproceedings{haas2024discovering,
  title={Discovering interpretable directions in the semantic latent space of diffusion models},
  author={Haas, Ren{\'e} and Huberman-Spiegelglas, Inbar and Mulayoff, Rotem and Gra{\ss}hof, Stella and Brandt, Sami S and Michaeli, Tomer},
  booktitle={2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)},
  pages={1--9},
  year={2024},
  organization={IEEE}
}

@inproceedings{fumero2021learning,
  title={Learning disentangled representations via product manifold projection},
  author={Fumero, Marco and Cosmo, Luca and Melzi, Simone and Rodol{\`a}, Emanuele},
  booktitle={International conference on machine learning},
  pages={3530--3540},
  year={2021},
  organization={PMLR}
}

@article{winter2022unsupervised,
  title={Unsupervised learning of group invariant and equivariant representations},
  author={Winter, Robin and Bertolini, Marco and Le, Tuan and No{\'e}, Frank and Clevert, Djork-Arn{\'e}},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31942--31956},
  year={2022}
}

@inproceedings{chughtai2023toy,
  title={A toy model of universality: Reverse engineering how networks learn group operations},
  author={Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
  booktitle={International Conference on Machine Learning},
  pages={6243--6267},
  year={2023},
  organization={PMLR}
}

@article{liao2023lie,
  title={Lie group equivariant convolutional neural network based on laplace distribution},
  author={Liao, Dengfeng and Liu, Guangzhong},
  journal={Remote Sensing},
  volume={15},
  number={15},
  pages={3758},
  year={2023},
  publisher={MDPI}
}

@inproceedings{finzi2020generalizing,
  title={Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data},
  author={Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  booktitle={International Conference on Machine Learning},
  pages={3165--3176},
  year={2020},
  organization={PMLR}
}

@inproceedings{lyu2009interpretation,
  title={Interpretation and generalization of score matching},
  author={Lyu, Siwei},
  booktitle={Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
  pages={359--366},
  year={2009}
}

@article{thomas2018tensor,
  title={Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds},
  author={Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  journal={arXiv preprint arXiv:1802.08219},
  year={2018}
}

@article{geiger2022e3nn,
  title={e3nn: Euclidean neural networks},
  author={Geiger, Mario and Smidt, Tess},
  journal={arXiv preprint arXiv:2207.09453},
  year={2022}
}

@inproceedings{niu2020permutation,
  title={Permutation invariant graph generation via score-based generative modeling},
  author={Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4474--4484},
  year={2020},
  organization={PMLR}
}

@article{le2023navigating,
  title={Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation},
  author={Le, Tuan and Cremer, Julian and No{\'e}, Frank and Clevert, Djork-Arn{\'e} and Sch{\"u}tt, Kristof},
  journal={arXiv preprint arXiv:2309.17296},
  year={2023}
}

@inproceedings{le2021parameterized,
  title={Parameterized hypercomplex graph neural networks for graph classification},
  author={Le, Tuan and Bertolini, Marco and No{\'e}, Frank and Clevert, Djork-Arn{\'e}},
  booktitle={International Conference on Artificial Neural Networks},
  pages={204--216},
  year={2021},
  organization={Springer}
}

@article{tang2024score,
  title={Score-based Diffusion Models via Stochastic Differential Equations--a Technical Tutorial},
  author={Tang, Wenpin and Zhao, Hanyang},
  journal={arXiv preprint arXiv:2402.07487},
  year={2024}
}

@inproceedings{Lu2022MaximumLT,
  title={Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching},
  author={Cheng Lu and Kaiwen Zheng and Fan Bao and Jianfei Chen and Chongxuan Li and Jun Zhu},
  booktitle={International Conference on Machine Learning},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:249712167}
}

@inproceedings{songscore,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{song2021maximum,
  title={Maximum likelihood training of score-based diffusion models},
  author={Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1415--1428},
  year={2021}
}

@article{huang2021variational,
  title={A variational perspective on diffusion-based generative models and score matching},
  author={Huang, Chin-Wei and Lim, Jae Hyun and Courville, Aaron C},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22863--22876},
  year={2021}
}

@article{hyvarinen2005estimation,
  title={Estimation of non-normalized statistical models by score matching.},
  author={Hyv{\"a}rinen, Aapo and Dayan, Peter},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={4},
  year={2005}
}

@article{lin2016estimation,
  title={Estimation of high-dimensional graphical models using regularized score matching},
  author={Lin, Lina and Drton, Mathias and Shojaie, Ali},
  journal={Electronic journal of statistics},
  volume={10},
  number={1},
  pages={806},
  year={2016},
  publisher={NIH Public Access}
}

@article{gardiner1985handbook,
  title={Handbook of stochastic methods for physics, chemistry and the natural sciences},
  author={Gardiner, Crispin W},
  journal={Springer series in synergetics},
  year={1985},
  publisher={Springer Berlin Heidelberg}
}