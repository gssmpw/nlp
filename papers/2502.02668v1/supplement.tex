\section{Additional Experiments}
\label{supplement}
\subsection{Synthetic Benchmark with Additional Methods}
In \autoref{fig:spectral_methods_comparison}, we repeat the experiment of Fig. 2 with a lower dimension $d = 100$ to include the PPcovMCD method introduced in \citet{POKOJOVY2022107475}, which is more computationally intensive and would otherwise be intractable for a benchmark with $d = 300$.
Here, we can observe that PPcovMCD recovers the signal direction well in the setting with an Imbalanced Clusters planted vector.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[scale = 0.5]{comparison_br_mean_small.eps}   
        \caption{Bernoulli-Rademacher}
        \label{br_comp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[scale = 0.5]{comparison_ic_mean_small.eps}
        \caption{Imbalanced Clusters}
        \label{ic_comp}
    \end{subfigure}
    \caption{
        Comparison of different methods in the planted vector setting.
        We plot the average inner product between the signal direction and the recovered direction by each algorithm over 30 datasets.
    }
    \label{fig:spectral_methods_comparison}
\end{figure}

\subsection{Differing Cluster Variance}
A setting not covered by the planted vector setting is the recovery of a Gaussian mixture with differing variances.
We will follow the experiments described in \citet{Cabana2021}.
We sample from $(1-p) \Normal(0, \bbI_d) + p \Normal(\delta \ustar, \lambda \bbI_d)$ with $p = 0.3$ and $d = 100$.
We will use our gradient-based projection pursuit algorithm to recover $\ustar$ from the \emph{whitened} data.
The results are plotted in \autoref{cabana_experiment} where we show the average of $|\inner{\uhat, \ustar}|$ over 30 datasets.
Here, we can observe a distinct behavior with $\lambda = 1$, which resembles the Imbalanced Clusters planted vector as $\delta \to \infty$.
In contrast, for any sufficiently large $|\lambda - 1|$, samples belonging to the cluster with a lower variance will have a lower norm.
It can be observed that for our algorithm, recovery is easier if $\lambda > 1$.
We hypothesize that a specialized algorithm can exploit this fact to outperform our algorithm in sample complexity in this setting.

\begin{figure}
    \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{heatmap_400.eps}
            \caption{$n = 400$}
        \end{subfigure}
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{heatmap_4000.eps}
            \caption{$n = 4000$}
        \end{subfigure}
    \caption{
        A plot of the average value of $|\inner{\uhat, \ustar}|$ of the predicted direction $\uhat$ and the signal direction $\ustar$.
        The experiment is repeated with $n = 400$ and $n = 4000$ samples.
        White pixels denote values close to 1, and black pixels denote values close to 0.
    }
    \label{cabana_experiment}
\end{figure}

\subsection{Anisotropic Gaussian Mixture}
We also demonstrate the efficacy of our approach by benchmarking on a Gaussian mixture where the Gaussians share a center, but one has an anisotropic variance.
For this, we choose a rank perturbation to the covariance matrix.
We obtain the distribution $(1-p) \Normal(0, \bbI_d) + p \Normal(0, \bbI_d + \ustar {\ustar}^\top (\lambda - 1))$ from which we sample $n = 700, 5000$ samples with $d = 200$, $p \in {0.01, ..., 0.4}$ and $\lambda = {0, ..., 10}$
We plot the results of this experiment in \autoref{anisotropic_experiment} in which we plot the average value of $|\inner{\uhat, \ustar}|$ over 10 runs of the gradient-based projection pursuit algorithm.
Here, we can observe that with smaller values for $p$ and larger values for $\lambda$, the problem becomes easier to recover the signal direction.

\begin{figure}[H]
    \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{heatmap_p_lambda_700.eps}
            \caption{$n = 700$}
        \end{subfigure}
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{heatmap_p_lambda_5000.eps}
            \caption{$n = 5000$}
        \end{subfigure}
    \caption{
        Plots of $|\inner{\uhat, \ustar}|$ where $\uhat$ was generated by the gradient-based algorithm with the \textbf{ReLU2} projection index. 
        White pixels denote values close to 1, and black pixels denote values close to 0.
    }
    \label{anisotropic_experiment}
\end{figure}

\subsection{Planted Vector in other Distributions than Gaussians}
The planted vector setting assumes that the orthogonal space to the signal direction is Gaussian.
Here, we will consider alternative distributions ($\inner{\bX, \ustar}$) for the signal and orthogonal directions.
Thus we will choose a random orthonormal basis $(\ustar, \be_2, ..., \be_d)$
For $\inner{X, \ustar}$ we will choose these distributions:
\begin{center}
\begin{tabular}{ c|c } 
\textbf{Abbreviation} & \textbf{Distribution} \\ 
\hline
GM & $0.9 \Normal(0, 1) + 0.1 \Normal(5, 1)$ \\ 
\hline
IC & Imbalanced Clusters with $p = 0.1$ \\ 
\hline
BR & Bernoulli Rademacher with $p = 0.1$ \\ 
\end{tabular}
\end{center}

For $\inner{X, \be_i}$ with $i \in \{ 2, ..., d\}$ we will choose the following distributions.

\begin{center}
\begin{tabular}{ c|c } 
\textbf{Abbreviation} & \textbf{Distribution} \\ 
\hline
Normal & $\Normal(0, 1)$ \\ 
\hline
Rademacher & Rademacher distribution\\ 
\hline
Heavy-Tailed & Student-t distribution with $\nu = 2$ degrees of freedom\\ 
\hline
Skewed & Skew normal distribution with $\alpha = 3$
\end{tabular}
\end{center}

The results of the experiment are plotted in \autoref{other_ambient} where we run the experiment for dimension $d = 100$ with different numbers of samples.
The data is whitened before applying the projection pursuit algorithm.
We test 2 different methods of choosing the best projection. 
First, we choose the algorithm as proposed in Algorithm 1.
Secondly, we just run gradient ascent once and choose the direction maximizing $\myabs{\inner{\uhat, \ustar}}$.
This is not implementable in practice but demonstrates that the gradient ascent algorithm still converges to the signal direction with a heavy-tailed distribution as the distribution orthogonal to the signal.
We believe this could be fixed using a more robust function for $\psi(\cdot)$.

We can observe that using Rademacher distributions instead of a Gaussian does not seem to change how easily the signal direction can be recovered.
It can also be observed that the heavy-tailed Gaussian Mixture makes recovery significantly more difficult.
We especially observe that our choice of projection index to detect converged projections fails for heavy-tailed distributions.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\linewidth]{other_abmient_distributions_new.eps}
        \caption{Proposed algorithm}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\linewidth]{other_abmient_distributions_new_oracle.eps}
        \caption{Best direction found}
    \end{subfigure}
    
    \caption{
        On the x-axis: number of samples $n = 600, 10000, 30000$ and projection index \{relu2, kurtosis\}.\\
        On the y-axis: distribution in the signal direction \{ Gaussian Mixture(GM), Imbalanced Clusters(IC), Bernoulli Rademacher(BR) \} and the distribution of the orthogonal space \{ Normal, Rademacher, Heavy Tailed, Skewed\}.\\
        We plot the inner product $\inner{\uhat, \ustar}$ of the recovered direction $\uhat$.
        White pixels denote values close to 1, and black pixels denote values close to 0.
    }
    \label{other_ambient}
\end{figure}