\documentclass[journal]{IEEEtran}
\usepackage{changepage, graphicx}
\usepackage{algorithm, multirow}
\usepackage{algorithmic, CJK}
\usepackage{amsmath, indentfirst}
\usepackage{amssymb}
\usepackage{float}
% \usepackage[colorlinks,linkcolor=blue,anchorcolor=blue,linktocpage=true,urlcolor=blue,citecolor=blue,citebordercolor={0  0  1}]{hyperref}
\usepackage{subfigure}
\usepackage{cite}
\usepackage{url}
\usepackage{color, soul}
\usepackage{color}
\usepackage{framed}
\usepackage{array}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage[colorlinks,urlcolor=black,linkcolor=black,citecolor=black,anchorcolor=black,filecolor=black,menucolor=black,runcolor=black]{hyperref} % è®¾ç½®é“¾æ¥é¢œè‰² 

\definecolor{shadecolor}{rgb}{1,0.94509804,0}
\hyphenation{op-tical net-works semi-conduc-tor}
\soulregister\cite7 % é’ˆå¯¹\citeå‘½ä»¤
\soulregister\citep7 % é’ˆå¯¹\citepå‘½ä»¤
\soulregister\citet7 % é’ˆå¯¹\citetå‘½ä»¤
\soulregister\ref7 % é’ˆå¯¹\refå‘½ä»¤
\soulregister\pageref7 % é’ˆå¯¹\pagerefå‘½ä»¤
\soulregister\math7 % é’ˆå¯¹\pagerefå‘½ä»¤
\soulregister\begin7 % é’ˆå¯¹\pagerefå‘½ä»¤

\makeatletter
\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
	\begin{center}
		\refstepcounter{algorithm}% New algorithm
		\hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
		\renewcommand{\caption}[2][\relax]{% Make a new \caption
			{\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
			\ifx\relax##1\relax % #1 is \relax
			\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
			\else % #1 is not \relax
			\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
			\fi
			\kern2pt\hrule\kern2pt
		}
	}{% \end{breakablealgorithm}
		\kern2pt\hrule\relax% \@fs@post for \@fs@ruled
	\end{center}
}

\begin{document}
\title{A Split-Window Transformer for Multi-Model Sequence Spammer Detection using Multi-Model Variational Autoencoder}
\author{
	Zhou~Yang, Yucai~Pang, Hongbo Yin, Yunpeng~Xiao 
	\thanks{This paper is partially supported by the National Natural Science Foundation of China (Grant No.62072066, 62006032), the Key Cooperation Project of Chongqing Municipal Education Commission(Grant No.HZ2021008) and Youth Innovation Group Support Program of ICE Discipline of CQUPT (Grant No.SCIE-QN-2022-05).\emph{(Corresponding author: Yucai Pang and Yunpeng Xiao.)}}
	
	\thanks{Z. Yang, Y. Pang, and Y. Xiao are with the School of Communications and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China (e-mail: yzhoul392@gmail.com;  pangyc@cqupt.edu.cn; xiaoyp@cqupt.edu.cn; ).}
	
	\thanks{H. Yin is with the School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China (e-mail: yinhub@yeah.net).}
}
\markboth{} %
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
\maketitle
\begin{abstract}
	This paper introduces a new Transformer, called MS$^2$Dformer, that can be used as a generalized backbone for \textbf{m}ulti-modal \textbf{s}equence \textbf{s}pammer \textbf{d}etection. Spammer detection is a complex multi-modal task, thus the challenges of applying Transformer are two-fold. Firstly, complex multi-modal noisy information about users can interfere with feature mining. Secondly, the long sequence of users' historical behaviors also puts a huge GPU memory pressure on the attention computation. To solve these problems, we first design a user behavior Tokenization algorithm based on the multi-modal variational autoencoder (MVAE). Subsequently, a hierarchical split-window multi-head attention (SW/W-MHA) mechanism is proposed. The split-window strategy transforms the ultra-long sequences hierarchically into a combination of intra-window short-term and inter-window overall attention. Pre-trained on the public datasets, MS$^2$Dformer's performance far exceeds the previous state of the art. The experiments demonstrate MS$^2$Dformer's\footnote{\url{https://github.com/yzhouli/MSDformer.}} ability to act as a backbone.
	
	% æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMSDformerçš„æ–°å‹Transformerï¼Œå®ƒå¯ä»¥ä½œä¸ºå¤šæ¨¡æ€åºåˆ—åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹çš„é€šç”¨éª¨å¹²ã€‚åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹æ˜¯ä¸€é¡¹å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå› æ­¤å°†Transformeråº”ç”¨åœ¨è¿™é¡¹ä»»åŠ¡æ‰€é¢ä¸´çš„æŒ‘æˆ˜æœ‰ä¸¤é¡¹ã€‚é¦–å…ˆï¼Œç”¨æˆ·å¤æ‚å¤šæ¨¡æ€å™ªå£°ä¿¡æ¯ä¼šå¹²æ‰°ç‰¹å¾æŒ–æ˜ã€‚å…¶æ¬¡ï¼Œè¶…é•¿ç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—ä¹Ÿä¸ºæ³¨æ„åŠ›è®¡ç®—é€ æˆå·¨å¤§æ˜¾å­˜å‹åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†åŸºäºå¤šæ¨¡æ€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMVAEï¼‰çš„ç”¨æˆ·è¡Œä¸ºTokenåŒ–ç®—æ³•ã€‚éšåï¼Œæå‡ºåˆ†å±‚åˆ†å‰²çª—å£çš„å¤šå¤´æ³¨æ„åŠ›ï¼ˆSW-MHAï¼‰è®¡ç®—æœºåˆ¶ã€‚åˆ†å‰²çª—å£ç­–ç•¥å°†è¶…é•¿åºåˆ—åˆ†å±‚è½¬åŒ–ä¸ºçª—å£å†…çŸ­æœŸå’Œçª—å£é—´æ€»ä½“æ³¨æ„åŠ›ç»“åˆè®¡ç®—ï¼Œæ˜¾è‘—é™ä½äº†MHAè®¡ç®—è¿‡ç¨‹çš„æ˜¾å­˜çˆ†ç‚¸éšæ‚£ï¼ŒåŒæ—¶åŠ é€Ÿæ¨ç†ã€‚åƒåœ¾é‚®ä»¶å‘é€è€…é€šå¸¸é‡‡ç”¨é•¿æœŸéšè—æˆ–çŸ­æœŸç›´æ¥å¼•å¯¼æ–¹å¼æ¨åŠ¨èˆ†è®ºå‘å±•ã€‚éšåï¼Œåœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ï¼ˆWeiboå’ŒTwitterï¼‰ä¸­è¿›è¡Œé¢„è®­ç»ƒã€‚MSDformerçš„æ€§èƒ½å¤§å¤§è¶…è¿‡äº†ä¹‹å‰çš„æŠ€æœ¯æ°´å¹³ï¼Œåœ¨Weiboä¸­è¾¾åˆ°äº†+9% Acc.å’Œ+7% AUCï¼Œåœ¨Twitterä¸­è¾¾åˆ°äº†+7% AUCã€‚å®éªŒè¯æ˜äº†MSDformerä½œä¸ºå¤šæ¨¡æ€åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹éª¨å¹²çš„èƒ½åŠ›ã€‚å…¬å¼€æ•°æ®é›†å’Œæ¨¡å‹ä»£ç åœ¨https://github.com/yzhouli/Spammer_Weiboã€‚ 
\end{abstract}
\begin{IEEEkeywords}
	Spammer Detection, Multi-modal Representation, Multi-modal Variational Autoencoder, Split-Window Attention Mechanism, Social Network Analysis
\end{IEEEkeywords}
\section{Introduction}
%åƒåœ¾é‚®ä»¶å‘é€è€…æ˜¯å¼•å¯¼ç¤¾äº¤èˆ†è®ºèµ°å‘çš„é‡è¦æ¨æ‰‹ã€‚é•¿æœŸä»¥æ¥ï¼Œåˆ¶é€ åƒåœ¾é‚®ä»¶æˆ–è™šå‡æ–°é—»çš„ç”¨æˆ·è¢«å®šä¹‰ä¸ºåƒåœ¾é‚®ä»¶å‘é€è€…ã€‚ä¿¡æ¯ä¼ æ’­é€šå¸¸ä»¥å›¾ç»“æ„ä¸ºè½½ä½“è¿›è¡Œä¼ æ’­ã€‚å› æ­¤ï¼Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æˆä¸ºè¯†åˆ«åƒåœ¾é‚®ä»¶å‘é€è€…çš„é€šç”¨éª¨å¹²ã€‚éšåï¼ŒGNNè¿›ä¸€æ­¥å‘å±•ï¼Œé€šè¿‡ç»“åˆå·ç§¯ï¼ˆGCNï¼‰å’Œæ³¨æ„åŠ›ï¼ˆGATã€Graph Transformerï¼‰æ€æƒ³ï¼ŒGNNæ€§èƒ½è¿›ä¸€æ­¥åŠ å¼ºã€‚
\IEEEPARstart{S}pammers are important promoters of directing social opinion. Users who create spam or fake news for a long time are defined as spammers. Information dissemination is usually carried out by graph structures. Therefore, the graph neural network (GNN) becomes a generalized backbone for identifying spammers. Subsequently, GNN has been further developed. By combining the ideas of Convolution (GCN\cite{li2019spam}), Attention (GAT\cite{zhang2023detecting, jiang2024learning} and Graph Transformer\cite{chen2024gnn}), and Sampling (Graph-SAGE\cite{zhang2024predicting}), the performance of GNN is further improved.
\begin{figure}[htbp]
	\center{\includegraphics[width=1\linewidth] {./image/inspire.pdf}} 
	\caption{Some examples of complex cross-modal feature mining challenges. (a) and (b) are both standard cases, i.e., the core argument can be identified in the text modal, and the multi-modal provides auxiliary features. (c) the text modal does not directly provide the point of view, so further alignment and mining of the core argument in conjunction with the image is required. (d) the situation is most common in social behavior. User behavior from the text aspect is often accompanied by noise features, i.e., emojis and non-common characters (@, \#, and //, etc.). In special cases, it is also mixed with URL linking to elaborate the argument.}
	\label{fig-inspire}
\end{figure}
\begin{figure*}[h]
	\center{\includegraphics[width=1\linewidth]  {./image/MHA.pdf}} 
	\caption{Inspiration for hierarchical attention mechanisms. (a) is the classical multi-head attention mechanism (MHA). (b-h) are sparse multi-head attention (SMHA) for solving ultra-long sequence modeling. (b) and (c) are SMHAs based on split windows. The core idea of them all is to limit the receptive field of an individual element, thus reducing the computational effort of $QK^\text{T}$. Among them, (c) expands the windowed receptive field similarly to the dilated convolution. Because the CPU dominates the windowing process, the full SMHA computation is slow (see Table \ref{table-ab-memory}). To solve this problem, the researchers adjusted the sliding distance to be consistent with the window length, thus proposing the block split-window mechanism (see (d) and (h)). Meanwhile, considering the importance of CLS tokens, Longformer (e\cite{beltagy2020longformer}) proposes global attention based on (d). Subsequently, random sampling is also added (see (f) and (g)).}
	\label{fig-MHAs}
\end{figure*}
%å¤æ‚è·¨æ¨¡æ€ç‰¹å¾æŒ–æ˜æŒ‘æˆ˜ä¸¾ä¾‹ã€‚(a)å’Œ(b)å‡ä¸ºæ ‡å‡†æƒ…å†µï¼Œå³åœ¨æ–‡æœ¬æ¨¡æ€å°±å¯ä»¥è¯†åˆ«æ ¸å¿ƒè®ºç‚¹ï¼Œå¤šæ¨¡æ€æä¾›è¾…åŠ©ç‰¹å¾ã€‚(c)ä¸­æ–‡æœ¬æ¨¡æ€å¹¶ä¸èƒ½ç›´æ¥æä¾›è§‚ç‚¹ï¼Œå› æ­¤éœ€è¦ç»“åˆå›¾åƒè¿›ä¸€æ­¥å¯¹é½å’ŒæŒ–æ˜æ ¸å¿ƒè®ºç‚¹ã€‚(d)æƒ…å†µåœ¨ç¤¾äº¤è¡Œä¸ºä¸­æœ€å¸¸è§ã€‚ç”¨æˆ·è¡Œä¸ºä»æ–‡æœ¬æ–¹é¢å¸¸å¸¸ä¼´éšå™ªå£°ç‰¹å¾ï¼Œå³è¡¨æƒ…åŒ…å’Œéå¸¸è§å­—ç¬¦ï¼ˆ@ã€#å’Œ\\ç­‰ï¼‰ã€‚ç‰¹æ®Šæƒ…å†µï¼Œè¿˜ä¼šæºæ‚URLé“¾æ¥æ–¹å¼é˜è¿°è®ºç‚¹ã€‚
%å¦ä¸€æ–¹é¢ï¼Œå­¦è€…è®¤ä¸ºåƒåœ¾é‚®ä»¶å‘é€è€…é€šå¸¸é‡‡ç”¨çªå‘æ€§å’ŒçŸ­æœŸè¿è´¯æ€§çš„è¡Œä¸ºå¼•å¯¼èˆ†è®ºã€‚éšåï¼Œä»–ä»¬åˆ™è¿›è¡Œå¿«é€Ÿä¼ªè£…ï¼Œå€ŸåŠ©ä¼ æ’­æ­£èƒ½é‡ç­‰è¡Œä¸ºèšé›†ç²‰ä¸ï¼Œä¸ºä¸‹ä¸€æ¬¡å¼•å¯¼åšå‡†å¤‡ã€‚å› æ­¤ï¼Œåºåˆ—å»ºæ¨¡ç­–ç•¥è¿‘æœŸä¹Ÿè¢«åº”ç”¨åœ¨å½“å‰ä»»åŠ¡ä¸­ã€‚ä¸GNNéª¨å¹²ä¸åŒï¼Œè¯¥ç­–ç•¥é‡ç‚¹å…³æ³¨ç”¨æˆ·å†å²è¡Œä¸ºé•¿æœŸæˆ–çŸ­æœŸä¹‹é—´çš„éšè—ç›¸å…³æ€§ã€‚å¦‚æœå°†å•ä¸ªå†å²è¡Œä¸ºè§†ä¸ºä¸€ä¸ªTokenï¼Œé‚£ä¹ˆæ•´ä¸ªå†å²è¡Œä¸ºåºåˆ—å»ºæ¨¡é—®é¢˜å°±å¯ä»¥è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é—®é¢˜ã€‚åœ¨NLPé¢†åŸŸä¸­ï¼ŒTransformerä¸“ä¸ºåºåˆ—å»ºæ¨¡å’Œè½¬è¯‘ä»»åŠ¡è€Œè®¾è®¡ï¼Œå…¶æ˜¾è‘—ç‰¹ç‚¹æ˜¯åˆ©ç”¨æ³¨æ„åŠ›å¯¹æ•°æ®ä¸­çš„é•¿ç¨‹æˆ–çŸ­ç¨‹ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚è¿™ä¸åºåˆ—å»ºæ¨¡éœ€æ±‚é«˜åº¦ä¸€è‡´ã€‚
\par On the other hand, scholars believe that spammers usually use sudden and short-term coherent behaviors to guide public opinion. Subsequently, they make quick disguises and gather followers with the help of behaviors such as spreading positive energy to prepare for the subsequent guidance. Meanwhile, they also use outdated information as a vehicle and add new claims to guide new public opinion. Moreover, the interval between outdated information and current activities is very long (see Fig. \ref{fig-ultra-long-term}). Therefore, sequence modeling strategies have also recently been applied to the current task. Unlike the GNN backbone, this strategy focuses on the hidden correlations between users' historical long-term and short-term behaviors. If a single historical behavior is considered a Token, the entire problem of modeling historical behavioral sequences can be transformed into the problem of natural language processing (NLP). In the field of NLP, Transformer is designed for sequence modeling and translation tasks, and its distinguishing feature is the use of attention to model long-term or short-term dependencies in the data. This is highly consistent with the sequence modeling needs of the task at hand.
%ç„¶è€Œï¼ŒNLPä»»åŠ¡ä¸åƒåœ¾é‚®ä»¶å‘é€è€…è¯†åˆ«ä»»åŠ¡å­˜åœ¨å·®å¼‚ã€‚å› æ­¤ï¼Œå°†NLPé¢†åŸŸä¸­æˆç†Ÿçš„Transformeréª¨å¹²åº”ç”¨åœ¨åƒåœ¾é‚®ä»¶å‘é€è€…è¯†åˆ«ä»»åŠ¡éœ€è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ã€‚1ï¼‰å¤šæ¨¡æ€ç‰¹å¾æŒ–æ˜é—®é¢˜ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œç¤¾äº¤ç½‘ç»œæ˜¯å…¸å‹çš„å¤æ‚ç½‘ç»œã€‚å› æ­¤ï¼Œç”¨æˆ·è¡Œä¸ºçš„è¡¨ç°è½½ä½“ä¹Ÿæ˜¯å¤šç§å¤šæ ·ï¼Œå³åŒ…æ‹¬æ–‡æœ¬å’Œå¤šæ¨¡æ€è¾…åŠ©ç‰¹å¾ã€‚åœ¨NLPä»»åŠ¡ä¸­ï¼Œç ”ç©¶äººå‘˜ä»…éœ€è¦è€ƒè™‘å¦‚ä½•æŒ–æ˜è¯­ä¹‰å’Œè¯­å¢ƒä¿¡æ¯å³å¯ã€‚ä½†æ˜¯ï¼Œåƒåœ¾é‚®ä»¶å‘é€è€…è¯†åˆ«ä»»åŠ¡ä¸­è¿˜éœ€è¦è€ƒè™‘å¤šæ¨¡æ€ç‰¹å¾æŒ–æ˜ä¸æ¨¡æ€å¯¹é½é—®é¢˜ã€‚2ï¼‰è¶…é•¿å†å²è¡Œä¸ºåºåˆ—è¡¨ç¤ºé—®é¢˜ã€‚åœ¨NLPé¢†åŸŸä¸­ï¼ŒåŸºäºTransformeræ¶æ„çš„æ¨¡å‹æ”¯æŒçš„Tokené•¿åº¦é€šå¸¸ä¸º512-2048ä¹‹é—´ï¼ˆBERTï¼ˆ512 tokensï¼‰ã€GPT-2ï¼ˆ1024 tokensï¼‰ã€GPT-3ï¼ˆ2048 tokensï¼‰ã€Longformerï¼ˆ4096 tokensï¼‰ã€GPT-4ï¼ˆ8192 tokensï¼‰ç­‰ï¼‰ã€‚ç„¶è€Œï¼Œç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—é•¿åº¦è¿œè¶…è¿™äº›é•¿åº¦ã€‚ç»è¿‡å®éªŒéªŒè¯ï¼ˆçœ‹è¡¨1ï¼‰ï¼Œåºåˆ—é•¿åº¦ä¸º36768æ—¶æ•ˆæœæœ€ä½³ã€‚æ­¤æ—¶ï¼Œéœ€è¦è€ƒè™‘çš„é—®é¢˜æ˜¯å¦‚ä½•è§£å†³å¤šå¤´æ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹ä¸­%QK^T%çŸ©é˜µé€ æˆçš„å†…å­˜æº¢å‡ºé—®é¢˜ã€‚åŒæ—¶ï¼Œç”±äºæ¯æ—¥ç¤¾äº¤ç”¨æˆ·è¿‡å¤šï¼Œå› æ­¤éœ€è¦è€ƒè™‘ä½¿ç”¨å°å‹åŒ–è®¾è®¡ï¼Œå› æ­¤ä¸èƒ½æ— é™åˆ¶å¢åŠ æ˜¾å¡å†…å­˜éœ€æ±‚ã€‚
\par However, there are differences between NLP and spammer detection tasks. Therefore, there are two issues that need to be addressed to apply the well-established Transformer backbone in the NLP domain for spammer detection tasks.

\begin{itemize}
	\setlength{\itemsep}{-1pt}
	\setlength{\parsep}{0pt}
	\setlength{\parskip}{0pt}
	\item[1)]
	The problem of multi-modal feature mining. Social networks are typically complex networks. Therefore, the representation carriers of user behaviors are also diverse, i.e., including text and multi-modal auxiliary features. In NLP tasks, researchers only need to consider how to mine semantic and contextual information. However, the problem of multi-modal feature mining and cross-modal alignment must also be considered in the spammer detection task (see Fig. \ref{fig-inspire}).
	\item[2)]
	The problem of representing ultra-long historical behavioral sequences. In the field of NLP, the classical MHA-based Transformer architecture supports token lengths typically between 512 and 8192 (BERT (512 tokens\cite{devlin2018bert}), GPT-2 (1024 tokens\cite{radford2019language}), GPT-3 (2048 tokens\cite{brown2020language}), GPT-4 (8192 tokens\cite{achiam2023gpt}), etc.). However, the length of historical user behavior sequences in the spammer detection task far exceeds this threshold. It has been experimentally verified (see Table 1) that the sequence length of 16, 384, or 36,768 is the most effective. At this point, the problem to be considered is solving the memory explosion caused by the $QK^T$ matrix during the multi-head attention computation. Meanwhile, miniaturized design must be considered because of the excessive number of daily social users. With the help of SMHA (see Fig. \ref{fig-MHAs}), the Transformer structure can handle ultra-long sequences. However, SMHA fails to meet the requirements at two levels of running efficiency and memory consumption (see Table \ref{table-ab-memory}).
\end{itemize}

%ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç§åä¸ºMS$^2$Dformerçš„æ–°å‹Transformerï¼Œå®ƒå¯ä»¥ä½œä¸ºå¤šæ¨¡æ€åºåˆ—åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹çš„é€šç”¨éª¨å¹²ã€‚åŒæ—¶ï¼Œç»“åˆå˜åˆ†è‡ªç¼–ç å™¨å’Œå±‚æ¬¡ç³»æ•°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è§£å†³å¤æ‚å¤šæ¨¡æ€ç‰¹å¾æŒ–æ˜ä¸è¶…é•¿åºåˆ—å»ºæ¨¡ã€‚éšåï¼Œå»ºç«‹Transformeréª¨å¹²ç½‘ç»œæ£€æµ‹åƒåœ¾é‚®ä»¶å‘é€è€…ã€‚æœ¬æ–‡è´¡çŒ®å¦‚ä¸‹æ‰€ç¤ºï¼š
\par To solve the above problems, we construct a novel Transformer, called MS$^2$Dformer, that can be used as a generalized backbone for \textbf{m}ulti-modal \textbf{s}equence \textbf{s}pammer \textbf{d}etection. Meanwhile, the variational autoencoder and hierarchical sparse attention mechanism are combined to solve complex multi-modal feature mining and ultra-long sequence modeling. Subsequently, the Transformer backbone is built to detect spammers. The contributions of this article are shown as follows:
%1ï¼‰æå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMVAEï¼‰çš„ç”¨æˆ·è¡Œä¸ºTokenåŒ–ç®—æ³•ã€‚é¦–å…ˆï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆBERTå’ŒViTï¼‰å¯¹å¤šæ¨¡æ€ç”¨æˆ·è¡Œä¸ºè¿›è¡ŒåµŒå…¥ã€‚å…¶æ¬¡ï¼Œæ„å»ºåŒé€šé“çš„å¤šæ¨¡æ€ç‰¹å¾ç¼–ç å’Œè·¨æ¨¡æ€ç‰¹å¾å¯¹é½ã€‚éšåï¼Œå»ºç«‹å…±äº«è·¨æ¨¡æ€ç‰¹å¾çš„åŒé€šé“è§£ç ç»„ä»¶ã€‚æœ€åï¼Œè·¨æ¨¡æ€ç¼–ç ç‰¹å¾åŒæ—¶å‚ä¸ç‰¹å¾é‡å»ºå’Œåºåˆ—åƒåœ¾é‚®ä»¶å‘é€è€…è¯†åˆ«ã€‚
%2ï¼‰æå‡ºåˆ†å±‚åˆ†å‰²çª—å£çš„å¤šå¤´æ³¨æ„åŠ›ï¼ˆSW-MHAï¼‰è®¡ç®—æœºåˆ¶ã€‚é¦–å…ˆï¼Œæ„å»ºåˆ†å‰²çª—å£ç­–ç•¥å°†è¶…é•¿åºåˆ—åˆ†å±‚è½¬åŒ–ä¸ºçª—å£å†…çŸ­æœŸæ³¨æ„åŠ›ç»“åˆè®¡ç®—ã€‚æ­¤ä¸¾æ—¢å¯ä»¥ä¿è¯æ¨¡å‹èƒ½å¤„ç†è¶…é•¿Tokenåºåˆ—ï¼ŒåŒæ—¶ä¹Ÿè¿›ä¸€æ­¥æŒ–æ˜çŸ­æœŸåƒåœ¾é‚®ä»¶å‘é€è€…çªå‘è¡Œä¸ºã€‚å¹¶ä¸”ï¼Œçª—å£é‡‡ç”¨é‡åˆæ»‘åŠ¨æ–¹å¼é˜²æ­¢é—æ¼é‡è¦çŸ­æœŸä¿¡æ¯ã€‚éšåï¼Œå»ºç«‹çª—å£é—´æ€»ä½“æ³¨æ„åŠ›ä»è€Œæ·±åº¦èšåˆé•¿æœŸæ½œä¼è¡Œä¸ºã€‚SW-MHAæ˜¾è‘—é™ä½äº†MHAè®¡ç®—è¿‡ç¨‹çš„æ˜¾å­˜çˆ†ç‚¸éšæ‚£ï¼ŒåŒæ—¶åŠ é€Ÿæ¨ç†ã€‚
%3ï¼‰åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œå³Weiboå’ŒTwitterï¼Œä¸­è¿›è¡Œé¢„è®­ç»ƒã€‚MS$^2$Dformerçš„æ€§èƒ½å¤§å¤§è¶…è¿‡äº†ä¹‹å‰çš„æŠ€æœ¯æ°´å¹³ï¼Œåœ¨Weiboä¸­è¾¾åˆ°äº†+9% Acc.å’Œ+7% AUCï¼Œåœ¨Twitterä¸­è¾¾åˆ°äº†+7% AUCã€‚å®éªŒè¯æ˜äº†MS$^2$Dformerä½œä¸ºå¤šæ¨¡æ€åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹éª¨å¹²çš„èƒ½åŠ›ã€‚

\begin{itemize}
	\setlength{\itemsep}{-1pt}
	\setlength{\parsep}{0pt}
	\setlength{\parskip}{0pt}
	\item[1)]
	A Tokenization algorithm is proposed for user behavior based on a multi-modal variational autoencoder (MVAE). Firstly, pre-trained models (BERT\cite{wang2024utilizing} and ViT\cite{liu2021swin, han2022survey}) are utilized to embed multi-modal user behaviors. Secondly, dual-channel multi-modal feature encoding and cross-modal feature alignment are constructed. Subsequently, a dual-channel decoding component that shares cross-modal features is built\cite{wang2024revisiting, fang2023unsupervised}. Finally, the cross-modal encoded features are involved in feature reconstruction and sequence spammer detection.
	\item[2)]
	A hierarchical split-window multi-head attention (SW/W-MHA, see Fig. \ref{fig-MHAs} (i)) computational mechanism is proposed. Firstly, the split-window strategy is constructed to transform the ultra-long sequences into short-term attention within the window by hierarchical transformation. This action ensures the model can handle ultra-long Token sequences while further mining short-term spammers' bursty behaviors. Moreover, the windows overlap and slide to prevent omitting important short-term information (SW-MHA). Subsequently, the overall attention between windows is built to aggregate the long-term latent behaviors (W-MHA) deeply. SW/W-MHA significantly reduces the explicit explosion potential of MHA computation and accelerates the inference.
	\item[3)]
	Pre-training is performed in two publicly available datasets. MS$^2$Dformer significantly outperforms the previous state of the art, achieving an accuracy improvement of +6.9/5.2\%. In the consumer-grade platform (RTX 4060 (16GB)), the model is built with more than 53M parameters and runs efficiently (see Table \ref{table-ab-memory}). The experiments demonstrate MS$^2$Dformer's ability to act as a multi-modal spammer detection backbone.
\end{itemize}

%æœ¬æ–‡å…¶ä½™ç« èŠ‚çš„ç»„ç»‡ç»“æ„å¦‚ä¸‹æ‰€ç¤ºã€‚ç¬¬äºŒèŠ‚ä»‹ç»äº†è¿‘æœŸä¸åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹ã€å¤šæ¨¡æ€å˜åˆ†è‡ªç¼–ç å™¨å’Œé•¿åºåˆ—Transformeræ¶æ„å»ºæ¨¡ä»»åŠ¡ç›¸å…³çš„å·¥ä½œã€‚ç¬¬ä¸‰èŠ‚ä»‹ç»æ„å»ºMS$^2$Dformeræ¨¡å‹æ‰€éœ€çš„ç›¸å…³å®šä¹‰ã€‚ç¬¬å››èŠ‚é‡ç‚¹æè¿°æ¨¡å‹çš„ç»†èŠ‚ã€‚ç¬¬äº”éƒ¨åˆ†æ¯”è¾ƒäº†MS$^2$Dformeræ¨¡å‹ä¸å½“å‰æœ€å…ˆè¿›ç®—æ³•åœ¨å…¬å¼€æ•°æ®é›†é¢„è®­ç»ƒçš„è®­ç»ƒå¯¹æ¯”ã€‚åŒæ—¶ï¼Œæ„å»ºæ¶ˆèç ”ç©¶éªŒè¯æ¨¡å‹ç»„ä»¶çš„åˆç†æ€§ã€‚ç¬¬å…­èŠ‚æ€»ç»“å…¨æ–‡å¹¶å±•æœ›æœªæ¥çš„ç ”ç©¶å·¥ä½œã€‚
\par The organization of the remaining sections of this article is shown as follows: Section II describes recent work related to the tasks of spammer detection, multi-modal variational autoencoder, and long sequence Transformer architecture. Section III describes the relevant definitions needed to construct the MS$^2$Dformer model. Section IV focuses on describing the details of the model. Section V compares the performance of the MS$^2$Dformer model with the current state-of-the-art algorithms pre-trained on a public dataset. Meanwhile, the ablation study is constructed to validate the rationality of the model components. Section VI summarizes the entire article and looks forward to future research work.

\section{Related Work}
%åƒåœ¾é‚®ä»¶å‘é€è€…é€šè¿‡å‘é€åƒåœ¾é‚®ä»¶ï¼ˆä¹Ÿæœ‰å­¦è€…ç§°ä¸ºè™šå‡æ–°é—»æˆ–è°£è¨€ï¼‰å¼•å¯¼èˆ†è®ºã€‚å› æ­¤ï¼Œå­¦æœ¯ç•Œå®šä¹‰åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹åŒ…å«ä¸¤ä¸ªå­ä»»åŠ¡ï¼šçŸ­æœŸçªå‘ç”¨æˆ·æ£€æµ‹å’Œé•¿æœŸéšè—ç”¨æˆ·æ£€æµ‹ã€‚å‰è€…ï¼Œç»„åˆç”¨æˆ·çŸ­æœŸå¤šä¸ªè¡Œä¸ºè¯†åˆ«ç”¨æˆ·æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶å‘é€è€…ã€‚è¿™é¡¹ä»»åŠ¡é€šå¸¸å®šä¹‰ä¸ºåƒåœ¾é‚®ä»¶ï¼ˆè™šå‡æ–°é—»æˆ–è°£è¨€ï¼‰æ£€æµ‹ã€‚æ­¤æ—¶ï¼Œç”±äºç”¨æˆ·çŸ­æœŸè¡Œä¸ºè¾ƒå°‘ï¼Œå› æ­¤é€šå¸¸ç»“åˆä¼ æ’­ç©ºé—´ä»¥åŠGNNä¸»å¹²æ„å»ºæ¨¡å‹ã€‚åè€…ï¼Œæ›´é€‚åˆè¯†åˆ«é•¿æœŸæ½œä¼çš„ç”¨æˆ·ã€‚åŒæ—¶ï¼Œä¹Ÿèƒ½å…¼é¡¾çªå‘ç”¨æˆ·æ£€æµ‹ä»»åŠ¡ã€‚å¹¶ä¸”ï¼Œä¼´éšç€ç®—åŠ›æå‡ï¼Œå¤šæ¨¡æ€ç‰¹å¾æŒ–æ˜ä¹Ÿè¢«ç»“åˆåœ¨è¿™ä¸¤é¡¹å­ä»»åŠ¡ä¸­ã€‚
\par Spammers guide public opinion by sending spam (also called fake news or rumors by some scholars). Therefore, the academic definition of spammer detection contains two sub-tasks: short-term burst and long-term hidden user detection. In the former, several short-term behaviors of a user are combined to identify whether the user is a spammer or not. This task is usually called spam (fake news or rumor) detection. In this case, the model is usually constructed by combining the spread space and the GNN backbone because the user has fewer short-term behaviors. The latter is more suitable for identifying long-term potential users. Meanwhile, it can also consider the task of sudden user detection. Moreover, with the arithmetic power improvement, multi-modal feature mining is combined in these two sub-tasks.
%åŸºäºå›¾ä¸»å¹²çš„æ¨¡å‹ï¼šç¤¾ä¼šä¼ æ’­è¿‡ç¨‹é€šå¸¸ä»¥å›¾ä¸ºè½½ä½“ã€‚å› æ­¤ï¼ŒåŸºäºå›¾çš„ç¥ç»ç½‘ç»œæ¨¡å‹è¢«å¹¿æ³›ç”¨äºå½“å‰ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼ŒBian ç­‰äººä½¿ç”¨è‡ªä¸Šè€Œä¸‹å’Œè‡ªä¸‹è€Œä¸Šçš„åŒå‘å›¾å·ç§¯æ¨¡å‹æ¥è§£å†³è™šå‡ä¿¡æ¯è¯†åˆ«ä»»åŠ¡ã€‚éšåï¼ŒWei ç­‰äººåœ¨ Bian ç­‰äººçš„åŸºç¡€ä¸Šå¼•å…¥äº†éšæœºåŒ–ç†è®ºã€‚ç»“æœï¼Œè¿™ä¸€ç­–ç•¥å¤§å¤§å¢å¼ºäº†åŸºäºå›¾ç¥ç»ç½‘ç»œçš„è™šå‡ä¿¡æ¯è¯†åˆ«æ¨¡å‹çš„æ™®é€‚æ€§ã€‚åŒæ—¶ï¼Œç»“åˆé©¬å°”å¯å¤«åœºçš„GNNä¸»å¹²ä¹Ÿè¢«åº”ç”¨åœ¨è™šå‡ä¿¡æ¯åˆ¶é€ è€…æ£€æµ‹ä»»åŠ¡ä¸­ã€‚å®æ—¶è¯æ˜ï¼ŒåŸºäºGNNçš„æ¨¡å‹åœ¨è¯†åˆ«ç²¾åº¦ä¸Šå·²ç»è¾¾åˆ°äº†å‡ ä¹ä¸å¯è¢«è¶…è¶Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œéšç€ç”¨æˆ·å†å²è¡Œä¸ºå¢åŠ ï¼Œå³è¯†åˆ«é•¿æœŸéšè—çš„åƒåœ¾é‚®ä»¶å‘é€è€…æ—¶ï¼ŒåŸºäºGNNçš„æ¨¡å‹éœ€è¦æ¶ˆè€—å¤§é‡GPUå†…å­˜å»æ¨å¯¼å¯ç–‘è¡Œä¸ºã€‚ç”±äºæ¯æ—¥ç¤¾äº¤è¡Œä¸ºæ•°é‡æ•°ä»¥äº¿è®¡ï¼Œæ‰€ä»¥æ„å»ºæ¨¡å‹æ˜¯ä¸å¾—ä¸è€ƒè™‘GPUå†…å­˜é—®é¢˜ã€‚æœ€å¥½æ˜¯å¯ä»¥åœ¨æ¶ˆè´¹çº§GPUä¸Šèƒ½å¤Ÿè¿è¡Œã€‚å› æ­¤ï¼Œåœ¨è¶…é•¿å†å²è¡Œä¸ºåºåˆ—ä¸ºèƒŒæ™¯çš„æ½œä¼å¼åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒGNNéª¨å¹²ç½‘ç»œä¸æ˜¯æœ€æœ‰æ•ˆçš„ã€‚
\par \textbf{GNN Backbone-Based Models:} Social diffusion processes are usually carried out in graphs. Therefore, GNN-based models are popularly used for the task at hand. For instance, Bian et al.\cite{Bian2020Rumor} used top-down and bottom-up bi-directional GCN models to solve the task of fake news recognition. Subsequently, Wei et al.\cite{wei2024modeling} introduced randomization theory based on Bian et al. Consequently, this strategy dramatically enhances the generalizability of the fake news recognition model. Meanwhile, the GNN backbone incorporating Markov fields\cite{deng2023markov} was also applied to the task of fake maker detection. It has been proven that the GNN-based model has reached an almost unsurpassable performance in terms of recognition accuracy. However, as the historical user behavior increases, i.e., when identifying long-hidden spammers, the GNN-based model consumes a large amount of GPU memory to derive suspicious behaviors. With hundreds of millions of social behaviors every day, models must be built with GPU memory in mind and ideally run on consumer GPUs. Therefore, the GNN backbone is not the most effective in latent spammer detection against the background of ultra-long historical behavioral sequences.
%åŸºäºåºåˆ—çš„æ¨¡å‹ï¼šç¤¾äº¤ç½‘ç»œä¼ æ’­ç©ºé—´åŒ…å«å›¾ç»“æ„å’Œæ—¶åºä¿¡æ¯ã€‚å› æ­¤ï¼Œéƒ¨åˆ†ç ”ç©¶äººå‘˜å°è¯•é‡‡ç”¨åºåˆ—å»ºæ¨¡ç­–ç•¥æ„å»ºæ£€æµ‹æ¨¡å‹ã€‚Yang ç­‰äººç»“åˆæ—¶é—´ç‰¹å¾æ„å»ºäº†åŸºäºæƒ…æ„Ÿç†µçš„ä¼ æ’­è¯­éŸ³ã€‚éšåï¼Œå€ŸåŠ©éŸ³é¢‘åˆ†ç±»æŠ€æœ¯è¯†åˆ«è™šå‡ä¿¡æ¯ã€‚åŒæ ·ï¼ŒMa ç­‰äººæå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´ç‰¹å¾çš„ä¼ æ’­å“åº”ä¿¡æ¯å»ºæ¨¡ç­–ç•¥ï¼Œå¹¶å€ŸåŠ© RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰è¯†åˆ«äº†è™šå‡ä¿¡æ¯ã€‚éšåï¼ŒMa ç­‰äººåœ¨ TD-RvNNciteçš„åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ª GANï¼ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰å¼æ¨¡å‹ã€‚å…¶ä¸­ï¼ŒTransformer æ¨¡å‹è¢«ç”¨æ¥å¯¹æ—¶é—´ä¼ æ’­åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†å…¶ç”¨ä½œç”Ÿæˆå™¨ã€‚éšåï¼Œå¼•å…¥ RNN æ¨¡å‹ä½œä¸ºåˆ¤åˆ«å™¨æ¥è¯†åˆ«è™šå‡ä¿¡æ¯ã€‚æ­¤æ—¶ï¼Œä½œè€…ä¸»è¦è§£å†³çŸ­æœŸè¡Œä¸ºè¡ç”Ÿçš„ä¼ æ’­ç©ºé—´å»ºæ¨¡ä»»åŠ¡ã€‚å› æ­¤ï¼Œä»–ä»¬ä½¿ç”¨äº†æ”¯æŒ512tokensçš„NLPé¢†åŸŸæˆç†ŸTransformeræ¶æ„ã€‚ä»–ä»¬çš„æ¨¡å‹åœ¨é¢ä¸´è¶…é•¿ç”¨æˆ·åºåˆ—æ—¶å°†è¶…è¿‡512tokensçš„è¡Œä¸ºåˆ é™¤ã€‚åŒæ—¶ï¼Œæ—¶åºä¼ æ’­å›¾ä¹Ÿè¢«åº”ç”¨ã€‚ä¾‹å¦‚ï¼ŒSun ç­‰äººåŸºäºå›¾å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹æŒ–æ˜äº†ä¼ æ’­å­ç©ºé—´çš„ç»“æ„ä¿¡æ¯ã€‚éšåï¼ŒåŸºäºæ—¶æ€ç‰¹å¾æ„å»ºäº†å¹³å±•å­ç©ºé—´ç»“æ„ç‰¹å¾çš„åŒå‘èåˆç­–ç•¥ã€‚ä»–ä»¬åŒæ ·æ²¡æœ‰è§£å†³æ¶ˆè´¹çº§GPUå¹³å°è®­ç»ƒå’Œéƒ¨ç½²æ—¶åºGNNä¸»å¹²ç½‘ç»œã€‚
\par \textbf{Sequence-Based Models:} Social network spread space contains graph structure and temporal sequence information. Therefore, some researchers try constructing a detection model using a sequence modeling strategy. By combining temporal features, Yang et al.\cite{Yang2024Topic} constructed a spreading audio based on emotional entropy. Subsequently, fake news is recognized with the help of audio classification techniques. Similarly, Ma et al.\cite{ma2016detecting} proposed a temporal feature-based modeling strategy for propagated response information and identified the fake news with the help of RNN (Recurrent Neural Network). Subsequently, Ma et al.\cite{ma2021improving} constructed a GAN (Generative Adversarial Network) style model based on TD-RvNN\cite{ma2016detecting}. In this case, the Transformer model is used to model time-propagated sequences, and it is used as a generator. Subsequently, the RNN model is introduced as a discriminator to identify fake news. At this point, the authors mainly address modeling the short-term behavior-derived spread space. Therefore, they used the well-established Transformer architecture of the NLP domain that supports 512 tokens. Their model removes behaviors exceeding 512 tokens when confronted with ultra-long user sequences. Meanwhile, temporal spreading graphs were also applied. For instance, Sun et al.\cite{Sun2022ddgcn} mined the structural information of the propagation subspace based on GCN models. Subsequently, a bidirectional fusion strategy for structural features of the spread subspace was constructed based on temporal features. They did not address the training and deployment of temporal GNN backbone networks on consumer GPU platforms.
%ç»“åˆå¤šæ¨¡æ€çš„æ¨¡å‹ï¼šéšç€è®¡ç®—æœºå’Œé€šä¿¡æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œåœ¨çº¿ç¤¾äº¤è¡Œä¸ºå·²ç»ä¸å±€é™äºæ–‡æœ¬æ¨¡å‹ã€‚å› æ­¤ï¼Œå¤šæ¨¡æ€æ•°æ®æŒ–æ˜ä¸è·¨æ¨¡æ€å¯¹é½æˆä¸ºå½“å‰ç ”ç©¶çš„çƒ­ç‚¹ã€‚åœ¨çŸ­æœŸçªå‘æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œç ”ç©¶äººå‘˜å·²ç»èšç„¦äºå¤šæ¨¡æ€é¢†åŸŸå±•å¼€ç ”ç©¶ã€‚ä¾‹å¦‚ï¼ŒWangç­‰äººå¼•å…¥å¯¹æŠ—å­¦ä¹ å¯¹é½è·¨æ¨¡æ€ç‰¹å¾ã€‚Zhangç­‰äººå¼•å…¥å¼ºåŒ–å­¦ä¹ ç­–ç•¥å­¦ä¹ è·¨æ¨¡æ€ç‰¹å¾ã€‚ç„¶è€Œï¼Œåœ¨é•¿æœŸéšè—ç”¨æˆ·æ£€æµ‹å­ä»»åŠ¡ä¸­ï¼Œå¤šæ¨¡æ€ç‰¹å¾å¹¶ç­æœ‰è¢«å¹¿æ³›ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼ŒQuç­‰äººè€ƒè™‘ä½¿ç”¨å¤šæ¨¡æ€å»ºæ¨¡ç­–ç•¥ã€‚ä½†æ˜¯ï¼Œä»–ä»¬ä»…ä»…æ˜¯å°†æ–‡æœ¬è½¬åŒ–ä¸ºä¸‰ä¸ªé€šé“çš„ç±»ä¼¼è§†è§‰ä¿¡æ¯ï¼Œè€Œå¹¶éçœŸæ­£çš„å¤šæ¨¡æ€æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»å­¦åˆ—å»ºæ¨¡è§’åº¦æ„å»ºäº†å¤šæ¨¡æ€çš„åƒåœ¾é‚®ä»¶å‘é€è€…å»ºæ¨¡é€šç”¨éª¨å¹²ï¼Œç§°ä¸ºMS$^2$Dformerã€‚é¦–å…ˆï¼Œä½¿ç”¨å¤šæ¨¡æ€å˜åˆ†è‡ªç¼–ç æ„å»ºåŒé€šé“ç‰¹å¾æŒ–æ˜ä¸å¯¹é½ã€‚éšåï¼ŒåŸºäºå±‚æ¬¡åˆ†å‰²çª—å£æ³¨æ„åŠ›æœºåˆ¶æ·±åº¦é‡åŒ–åºåˆ—ç‰¹å¾ã€‚ç»“åˆæ©ç æœºåˆ¶ï¼ŒMS$^2$Dformeråœ¨ä¸¤ä¸ªå­ä»»åŠ¡ä¸­å‡è¡¨ç°æä½³ï¼Œè¯æ˜äº†å®ƒä½œä¸ºåƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹ä»»åŠ¡é€šç”¨éª¨å¹²çš„æ½œåŠ›ã€‚
\par \textbf{Multi-Modal Models:} With the rapid development of computer and communication technologies, online social behaviors are no longer limited to text models. Therefore, multi-modal data mining and cross-modal alignment have become a hot topic in current research. In short-term burst detection, researchers have focused on the multi-modal domain. For instance, Wang et al.\cite{wang2023cross} introduced adversarial learning to align cross-modal features. Zhang et al.\cite{zhang2024reinforced} introduced a reinforcement learning strategy to learn cross-modal features. However, multi-modal features have not been widely used in the long-term hidden user detection sub-task. For instance, Qu et al.\cite{qu2024temporal} considered using a multi-modal modeling strategy. However, they only transformed the text into three channels of similar visual information, not accurate multi-modal data. To this end, we construct a generalized backbone for multi-modal spammer detection, called MS$^2$Dformer, from a sequence modeling perspective. Firstly, two-channel feature mining and alignment are constructed using multi-modal variational auto-encoding. Subsequently, sequence features are deeply quantified based on a hierarchical split-window attention mechanism.
\section{Problem Definition}
\subsection{Related Definitions}
% åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹æ˜¯ä¸€é¡¹å…¸å‹çš„å¤šæ¨¡æ€å¤æ‚ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæœ¬èŠ‚ä»å•ä¸ªå†å²è¡Œä¸ºçš„æ–‡æœ¬å’Œå›¾åƒå¤šæ¨¡æ€ç‰¹å¾æå–å‡ºå‘ï¼Œå®šä¹‰MS$^2$Dformeræ¨¡å‹çš„è¾“å…¥åºåˆ—æ•°æ®ã€‚
\par Spammer detection is a typically multi-modal and complex task. Therefore, the input sequence data for the MS$^2$Dformer model is defined in this section starting from multi-modal feature extraction of text and images.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par \textbf{Definition 1.} Text modal feature \begin{math} T^v \end{math}
% å¦‚å›¾1æ‰€ç¤ºï¼Œç”¨æˆ·å†å²è¡Œä¸ºä¸­æ–‡æœ¬æ¨¡å‹çš„æ•°æ®å¾€å¾€æä¾›äº†éå¸¸é‡è¦çš„æ•°æ®ã€‚å› æ­¤ï¼Œæœ¬èŠ‚é‡‡ç”¨é¢„è®­ç»ƒçš„ä»…ç¼–ç å™¨ç»“æ„çš„BERTæ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
%å…¶ä¸­ï¼Œ$T$è¡¨ç¤ºæ–‡æœ¬æ•°æ®ã€‚$T^v\in \mathbb{R}^{768}$è¡¨ç¤ºç»è¿‡é¢„è®­ç»ƒBERTæ¨¡å‹ç¼–ç çš„â€œCLSâ€tokençš„åµŒå…¥è¡¨ç¤ºã€‚ç‰¹åˆ«çš„ï¼ŒNLPé¢†åŸŸä¸­æ–‡æœ¬æ•°æ®åœ¨å¥é¦–è¿æ¥"CLS"tokenã€‚å› æ­¤ï¼Œ$CLS^{T}$è¡¨ç¤ºè¿™ä¸ªtokenã€‚
\par As shown in Fig. \ref{fig-inspire}, the data from the text model in the user's historical behavior often provides very important data. Therefore, in this section, a pre-trained BERT\footnote{\url{https://huggingface.co/google-bert/bert-base-uncased.}} model with only an encoder structure is used to embed the text. The equation is shown as follows:
\begin{equation}
	T^v = \text{BERT}(\text{CLS}^{\text{T}}+T) \in \mathbb{R}^{768}
	\label{eq-1}
\end{equation}
where $T$ represents the text data. In the NLP domain, the textual data is concatenated with the â€œCLSâ€ token at the beginning of the sentence. Thus, $\text{CLS}^{\text{T}}$ represents this token. $T^v\in \mathbb{R}^{768}$ represents the embedded representation of the â€œCLSâ€ token encoded by the pre-trained BERT model.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par \textbf{Definition 2.} Image modal feature \begin{math} I^v \end{math}
%ä»…ä»…é€šè¿‡æ–‡æœ¬æ¨¡æ€å¼•å¯¼èˆ†è®ºä¼šå¼•èµ·ç›®æ ‡ç¾¤ä½“ä¸ä¿¡ä»»ä»è€Œå¯¼è‡´æŒ‘æ‹¨å¤±è´¥ã€‚æ‰€ä»¥ï¼Œåƒåœ¾é‚®ä»¶å‘é€è€…å¯èƒ½ä¼šæä¾›å‰ªè¾‘åçš„å›¾åƒç­‰å¤šæ¨¡æ€ä¿¡æ¯ä½œè¯ä»–çš„è§‚ç‚¹ã€‚å› æ­¤ï¼Œåœ¨è€ƒè™‘æ–‡æœ¬ç‰¹å¾çš„åŒæ—¶ï¼Œå›¾åƒç‰¹å¾åŒæ ·ä¸å¯ç¼ºå°‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„ViTæ¨¡å‹æå–å›¾åƒç‰¹å¾ã€‚å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
%å…¶ä¸­ï¼Œ$I$è¡¨ç¤ºè¾“å…¥çš„å›¾åƒæ•°æ®ã€‚$I^v\in \mathbb{R}^{768}$è¡¨ç¤ºç»è¿‡é¢„è®­ç»ƒViTæ¨¡å‹ç¼–ç çš„â€œCLSâ€tokençš„åµŒå…¥è¡¨ç¤ºã€‚ç”±äºï¼ŒViTåœ¨æ¨¡å‹å†…éƒ¨æ·»åŠ â€œCLSâ€tokenï¼Œå› æ­¤ä¸åœ¨å…¬å¼ä¸­ä½“ç°ã€‚
\par Guiding public opinion through textual modalities alone can cause the target group to trigger a crisis of confidence. Ultimately, it leads to a failure of provocation. Therefore, a spammer may provide multi-modal information, such as edited images, to testify his viewpoint. Thus, while considering text features, image features are equally indispensable. For this purpose, a pre-trained ViT\footnote{\url{https://huggingface.co/google/vit-base-patch16-224.}} model is used to extract image features. The equation is shown as follows:
\begin{equation}
	I^v = \text{ViT}(I) \in \mathbb{R}^{768}
	\label{eq-2}
\end{equation}
where $I$ represents the input image data. $I^v\in \mathbb{R}^{768}$ represents the embedded representation of the â€œCLSâ€ token encoded by the pre-trained ViT model. Because ViT adds â€œCLSâ€ tokens inside the model, they are not represented in Eq. (\ref{eq-2}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par \textbf{Definition 3.} User Historical Behavior Sequence \begin{math}S=\{(T^v_0, I^v_0),..., (T^v_l, I^v_l)\}\end{math}
%æœ¬æ–‡æå‡ºä¸€ç§åºåˆ—åŒ–çš„åƒåœ¾é‚®ä»¶å‘é€è€…è¯†åˆ«æ¨¡å‹ã€‚å› æ­¤ï¼Œé¦–å…ˆéœ€è¦å°†ç”¨æˆ·$U_i$çš„å†å²è¡Œä¸ºåºåˆ—$U_i=\{b_0, ..., b_l\}$è½¬åŒ–ä¸ºæ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æ ‡å‡†è¡¨ç¤ºã€‚å¦‚å›¾1æ‰€ç¤ºï¼Œåºåˆ—ä¸­ä»»æ„è¡Œä¸ºåŒ…å«æ–‡æœ¬å’Œå›¾åƒä¸¤ç§æ¨¡æ€æ•°æ®ï¼Œå³$b_i=(T_i, I_i)$ï¼Œå¹¶ä¸”$i \in [0. l]$ã€‚ç‰¹åˆ«çš„ï¼Œå­˜åœ¨å†å²è¡Œä¸º$b_i$ä¸åŒ…å«å›¾åƒæ•°æ®æ—¶ï¼Œå¡«å……ä¸€ä¸ªç©ºå›¾åƒã€‚éšåï¼Œç»“åˆå…¬å¼1å’Œå…¬å¼2å°†å†å²è¡Œä¸ºåºåˆ—$\{b_0, ..., b_l\}$è½¬åŒ–ä¸ºæ ‡å‡†å½¢å¼ã€‚å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
%å…¶ä¸­ï¼Œ$S_i \in \mathbb{R}^{l\times2\times768}$è¡¨ç¤ºç”¨æˆ·$U_i$çš„æ ‡å‡†è¾“å…¥åºåˆ—ã€‚$l$è¡¨ç¤ºæ¨¡å‹æ”¯æŒçš„å†å²è¡Œä¸ºæ•°é‡ã€‚å½“ç”¨æˆ·çœŸå®è¡Œä¸ºæ•°é‡è¶…è¿‡$l$æ—¶å°†ä¼šè¢«æˆªæ–­ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸è¶³çš„åºåˆ—è¡¥å……ç©ºè¡Œä¸ºã€‚åœ¨æ¨¡å‹ä¸­é€šè¿‡MASKæœºåˆ¶æ¶ˆé™¤è¿™äº›ç©ºè¡Œä¸ºçš„å½±å“ã€‚åŒæ—¶ï¼Œä¸ºäº†åŠ é€Ÿæå–å¤šæ¨¡æ€ç‰¹å¾ï¼Œé‡‡ç”¨$\text{BERT}(\{T_0, ..., T_l\})$å’Œ$\text{ViT}(\{I_0, ..., I_l\})$ç­–ç•¥å¹¶è¡ŒåŒ–æå–ç‰¹å¾ã€‚
\par This paper proposes a serialized model for spammer detection. Therefore, it is first necessary to transform a sequence of historical user behaviors $U=\{b_0, ..., b_l\}$ into a standard representation that the model can process. As shown in Fig. \ref{fig-inspire}, any behavior in the sequence contains both text and image modal data, i.e., $b_i=(T_i, I_i)$, and $i \in [0, l]$. In particular, a history behavior $b_i$ exists that fills an empty image when it does not contain image data. Subsequently, the sequence of history behaviors $\{b_0, ..., b_l\}$ into standard form. The equation is shown as follows:	
\begin{equation}
	\begin{split}
	S&=\text{BERT}(\{T_0, ..., T_l\})\ \text{and}\ \text{ViT}(\{I_0, ..., I_l\})\\
	   &=\{(T^v_0, I^v_0),..., (T^v_l, I^v_l)\} \in \mathbb{R}^{l\times2\times768}
	\label{eq-3}
	\end{split}
\end{equation}
where $S \in \mathbb{R}^{l\times2\times768}$ represents the standard sequence of inputs for user $U$. $l$ represents the length of the sequence supported by the model. The user's real behavior sequence will be truncated when its length exceeds $l$. On the other hand, insufficient sequences supplement the empty behaviors. The effect of these empty behaviors is eliminated in the model by the MASK mechanism. Meanwhile, to accelerate the extraction of multi-modal features, $\text{BERT}(\{T_0, ... , T_l\})$ and $\text{ViT}(\{I_0, ... , I_l\})$ strategies are employed to parallelize the extraction of features.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„Transformerï¼Œåä¸ºMS$^2$Dformerï¼ˆçœ‹å›¾2ï¼‰ï¼Œä»¥æ•°å­¦æ–¹å¼è§£å†³åºåˆ—åŒ–çš„å¤šæ¨¡æ€åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹é—®é¢˜ã€‚è¯¥æ¨¡å‹å°†è¿™ä¸ªå¤æ‚é—®é¢˜åˆ†ä¸ºå››ä¸ªé˜¶æ®µåˆ†åˆ«å»ºæ¨¡ã€‚å› æ­¤ï¼Œæ¨¡å‹çš„æ€»ä½“è¡¨ç¤ºå¦‚ä¸‹ï¼š
\par To solve the problem of serialized multi-modal spammer detection mathematically, we propose a new Transformer, called MS$^2$Dformer. The model models this complex problem in four separate stages. Thus, the overall representation of the model is as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{eqnarray}\left. {\begin{array}{*{20}{l}}
			U=\{b_0, ... , b_l\}\\
			b_i=(T_i, I_i)\\
	\end{array}} \right\} \Rightarrow \text{MS}^2\text{Dformer} \Rightarrow {P\{s,n \mid user\}}
\end{eqnarray}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h]
	\center{\includegraphics[width=1\linewidth]  {./image/model1.pdf}} 
	\caption{The framework of the MS$^2$Dformer\_B model (MS$^2$Dformer Base Version, see Eq. (\ref{eq-base})).}
	\label{fig-2}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Model Input}\
% MS$^2$Dformeræ¨¡å‹çš„è¾“å…¥æ•°æ®å¦‚ä¸‹æ‰€ç¤ºï¼š
\par The input data for the MS$^2$Dformer model is shown as follows:
\begin{itemize}
	\setlength{\itemsep}{-1pt}
	\setlength{\parsep}{0pt}
	\setlength{\parskip}{0pt}
	\item
	The sequence of historical user behaviors $U=\{b_0, ... , b_l\}$. $l$ represents the length of the sequence supported by the model.
	\item
	Individual user behavior $b_i=(T_i, I_i)$. The $i$-th behavior of user $U$ contains text $T_i$ and image $I_i$, and $i \in [0, l]$.
\end{itemize}

\subsubsection{Model Output}\ 
%åŸºäºä¸Šä¸€èŠ‚çš„æ¨¡å‹è¾“å…¥æ•°æ®ï¼ŒMS$^2$Dformeræ¨¡å‹éœ€è¦åˆ†é˜¶æ®µè§£å†³ä»¥ä¸‹é—®é¢˜ã€‚
\par Based on the model input data from the previous section, the MS$^2$Dformer model (see Fig. \ref{fig-2}) needs to solve the following problems in stages.
\begin{itemize}
	\setlength{\itemsep}{-1pt}
	\setlength{\parsep}{0pt}
	\setlength{\parskip}{0pt}
	\item
	%å¤šæ¨¡æ€éšè—ç‰¹å¾$H_k \in \mathbb{R}^{h\times2\times768}$ã€‚é¦–å…ˆï¼Œæ¨¡å‹è¾“å…¥åºåˆ—$U_k=\{b_0, ... , b_l\}$ç»è¿‡å…¬å¼ï¼ˆ1ï¼‰åçš„åˆ°åµŒå…¥è¡¨ç¤º$S_k=\{(T^v_0, I^v_0),..., (T^v_l, I^v_l)\} \in \mathbb{R}^{l\times2\times768}$ã€‚å…¶æ¬¡ï¼Œåœ¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„åŸºç¡€ä¸Šæ„å»ºåŒé€šé“çš„å¤šæ¨¡æ€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMVAEï¼‰ã€‚éšåï¼Œç»è¿‡MVAEï¼Œ$S_k \in \mathbb{R}^{l\times2\times768}$è¢«è½¬åŒ–ä¸º$S_k \in \mathbb{R}^{l\times2D}$ã€‚$D$è¡¨ç¤ºMVAEåµŒå…¥éƒ¨åˆ†çš„ç»´åº¦ã€‚æœ€åï¼Œæ·»åŠ åˆ†ç±»tokenâ€œCLS$_{\text{S}}}$â€ã€‚å› æ­¤ï¼Œ$S_k \in \mathbb{R}^{l\times2\times768}$è¢«è½¬åŒ–ä¸º$S_k \in \mathbb{R}^{H\times2D}$ï¼Œ$H=l+1$ã€‚
	\textbf{Stage 1:} Multi-modal hidden features $S_1 \in \mathbb{R}^{H\times2D}$. Firstly, the model input sequence $U=\{b_0, ... , b_l\}$ after Eq. (\ref{eq-3}) to the embedding representation $S=\{(T^v_0, I^v_0),... , (T^v_l, I^v_l)\} \in \mathbb{R}^{l\times2\times768}$. Secondly, a two-channel multi-modal variational autoencoder (MVAE) is constructed from the variational autoencoder (VAE). After MVAE, $S \in \mathbb{R}^{l\times2\times768}$ is transformed into $\mathbb{R}^{l\times2D}$. $D$ represents the dimension of the MVAE embedding part. Finally, the classification token CLS$^{\text{S}}$ is added. Thus, $S \in \mathbb{R}^{l\times2\times768}$ is transformed into $S_1 \in \mathbb{R}^{H\times2D}$, and $H=l+1$.
	\item		
	% è§£å†³è¶…é•¿åºåˆ—å»ºæ¨¡é—®é¢˜ã€‚é¦–å…ˆï¼Œå»ºç«‹å±‚æ¬¡åˆ†å‰²çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSW-MHAï¼‰ã€‚éšåï¼ŒåŸºäºç¬¬ä¸€å±‚çª—å£æ³¨æ„åŠ›æœºåˆ¶å»ºç«‹SW-MHA Transformer blockã€‚æ­¤æ—¶ï¼Œ$S_k \in \mathbb{R}^{H\times2D}$è¢«SW-MHAè½¬åŒ–ä¸º$S_k \in \mathbb{R}^{$\frac{H}{W}$\times2WD}$ã€‚éšåï¼Œä¿®æ”¹åŸå§‹çº¿æ€§æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸ºSW-MLPã€‚å¹¶ä¸”ï¼Œ$S_k \in \mathbb{R}^{$\frac{H}{W}$\times2WD}$è¢«SW-MHAè½¬åŒ–ä¸º$S_k \in \mathbb{R}^{$\frac{H}{W}$\times4D}$ã€‚å…¶æ¬¡ï¼ŒåŸºäºç¬¬äºŒå±‚çª—é—´æ³¨æ„åŠ›æœºåˆ¶å»ºç«‹å¤šä¸ªW-MHA Transformer blockã€‚æ­¤æ—¶ï¼ŒW-MHA Transformer blockç”¨äºæ·±åº¦æŒ–æ˜çª—é—´åºåˆ—ç‰¹å¾ã€‚å› æ­¤ï¼Œä¸å¯¹è¾“å…¥æ•°æ®ç»´åº¦è¿›è¡Œæ”¹å˜ã€‚
	\textbf{Stage 2:} Solve the problem of modeling ultra-long sequences. Firstly, the hierarchical split-window attention mechanism (SW/W-MHA) is constructed. Subsequently, the SW-MHA Transformer block is constructed based on the intra-window attention mechanism. In this case, $S_1 \in \mathbb{R}^{H\times2D}$ is transformed by SW-MHA into $\mathbb{R}^{\frac{H}{\lambda_{1}}\times2W_{1}D}$. $\lambda_{1}$ represents the stride size of the window. $W_{1}$ represents the length of the window. Afterwards, the original multi-layer linear perceptron (MLP) is modified to SW-MLP. In addition, $S_1 \in \mathbb{R}^{\frac{H}{\lambda_{1}}\times2W_{1}D}$ is transformed by SW-MLP to $\in \mathbb{R}^{\frac{H}{\lambda_{1}}\times4D}$. Secondly, multiple W-MHA Transformer blocks are constructed based on the inter-window attention mechanism. In this case, the W-MHA Transformer block is used to deeply mine the inter-window sequence features. Therefore, no changes are made to the input data dimensions, i.e., $S_2 \in \mathbb{R}^{\frac{H}{\lambda_{1}}\times4D}$.
	\item	
	% æ·±åº¦åºåˆ—ç‰¹å¾æŒ–æ˜é—®é¢˜ã€‚ç›¸è¾ƒäºStage 2ï¼ŒStage 3å­˜åœ¨ä¸¤ç‚¹ä¸åŒã€‚é¦–å…ˆï¼ŒStage 3ä¸­SW-MHAç»„ä»¶è®¾ç½®çš„çª—å£æ»‘åŠ¨æ­¥é•¿W_{2}åº”è¯¥è¿œå°äºW_{1}ã€‚å…¶æ¬¡ï¼ŒW-MHAç»„ä»¶é‡‡ç”¨æ›´æ·±å±‚çš„Blockç»“æ„æŒ–æ˜åºåˆ—ç‰¹å¾ã€‚æ€»çš„æ¥çœ‹ï¼Œç»è¿‡Stage 3ï¼Œ$S_k \in \mathbb{R}^{$\frac{H}{W_{1}}$\times4D}$è¢«è½¬åŒ–ä¸º$S_k \in \mathbb{R}^{$\frac{H}{W_{1}W_{2}}$\times8D}$ã€‚
	\textbf{Stage 3:} Solve the problem of deep sequence feature mining. Compared with Stage 2, there are two differences in Stage 3. Firstly, the window sliding step $\lambda_{2}$ set by the SW-MHA component in Stage 3 should be much smaller than $\lambda_{1}$. Secondly, the W-MHA component uses a deeper Block structure to mine sequence features. Overall, $S_2 \in \mathbb{R}^{\frac{H}{\lambda_{1}}\times4D}$ is transformed into $\mathbb{R}^{\frac{H}{\lambda_{1}\lambda_{2}}\times8D}$, i.e., $S_3 \in \mathbb{R}^{\frac{H}{\lambda_{1}\lambda_{2}}\times8D}$.
	\item		
	% è§£å†³åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹é—®é¢˜ã€‚é¦–å…ˆï¼Œé€‰æ‹©$S_k \in \mathbb{R}^{\frac{H}{W_{1}W_{2}}\times8D}$çš„åˆ†ç±»token CLS$_{\text{S}}} \in \mathbb{R}^{8D}$ã€‚æœ€åï¼Œç»è¿‡ä¸¤å±‚çº¿æ€§å±‚å°†åˆ†ç±»tokenæ˜ å°„ä¸ºCLS$_{\text{S}}} \in \mathbb{R}^{2}$ã€‚å› æ­¤ï¼Œç»è¿‡softmaxå‡½æ•°è¯†åˆ«åƒåœ¾é‚®ä»¶å‘é€è€…ã€‚
	\textbf{Stage 4:} Solve the problem of spammer detection. Firstly, the classification token CLS$^{\text{S}} \in \mathbb{R}^{8D}$ in $S_3 \in \mathbb{R}^{\frac{H}{\lambda_{1}\lambda_{2}}\times8D}$ is selected. Lastly, the classification token is mapped to CLS$^{\text{S}} \in \mathbb{R}^{2}$ after two linear layers. Thus, the spammers are recognized by softmax function.
\end{itemize}

\section{MS$^2$Dformer Model}

\subsection{Overview}
% æˆ‘ä»¬æ„å»ºäº†ä¸€ç§åä¸ºMS$^2$Dformerçš„æ–°å‹Transformerï¼Œå®ƒå¯ä»¥ä½œä¸ºå¤šæ¨¡æ€åºåˆ—åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹çš„é€šç”¨éª¨å¹²ã€‚æ¨¡å‹åˆ†ä¸ºå››ä¸ªé˜¶æ®µè¯†åˆ«åƒåœ¾é‚®ä»¶å‘é€è€…ï¼ˆçœ‹å›¾2ï¼‰ã€‚é¦–å…ˆï¼Œé˜¶æ®µ1åŸºäºåŒé€šé“MVAEå®Œæˆç”¨æˆ·å†å²è¡Œä¸ºtokenåŒ–ã€‚å…¶æ¬¡ï¼Œç»“åˆé˜¶æ®µ2å’Œé˜¶æ®µ3å¯¹è¶…é•¿åºåˆ—å»ºæ¨¡ï¼Œå¹¶ä¸”æ·±åº¦æŒ–æ˜åºåˆ—ç‰¹å¾ã€‚æœ€å¥½ï¼Œé˜¶æ®µ4é‡‡ç”¨çº¿å½¢å±‚æŒ–æ˜åˆ†ç±»token CLS$^{\text{S}}ç‰¹å¾ï¼Œéšåè¯†åˆ«åƒåœ¾é‚®ä»¶å‘é€è€…ã€‚
\par A novel Transformer, called MS$^2$Dformer, is constructed that can be used as a generalized backbone for multi-modal sequence spammer detection. The model is divided into four stages to identify spammers (see Fig. \ref{fig-2}). Firstly, stage 1 completes tokenizing the user's historical behavior based on two-channel MVAE. Secondly, ultra-long sequences are modeled with stages 2 and 3, and sequence features are deeply mined. Finally, stage 4 employs a linear layer to mine classification token CLS$^{\text{S}}$ features and subsequently identifies spammers.
\begin{figure*}[h]
	\center{\includegraphics[width=1\linewidth]  {./image/SW_MHA.pdf}} 
	\caption{The case of SW/W-MHA.}
	\label{fig-SW-MHA}
\end{figure*}
\subsection{MVAE-based Historical Behavior Tokenization}
% åºåˆ—åŒ–åƒåœ¾é‚®ä»¶å‘é€è€…è¯†åˆ«æ¨¡å‹çš„é¦–è¦å¾…è§£å†³é—®é¢˜æ˜¯å¦‚ä½•å°†ç”¨æˆ·å¤šæ¨¡æ€å†å²è¡Œä¸ºtokenåŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†åŸºäºåŒé€šé“MVAEçš„ç”¨æˆ·å†å²è¡Œä¸ºtokenåŒ–ç­–ç•¥ã€‚éšåï¼Œè”åˆCLS$^{\text{S}} tokenå®Œæˆæ•´ä¸ªç”¨æˆ·å†å²è¡Œä¸ºçš„åºåˆ—åŒ–ã€‚å› æ­¤ï¼Œæœ¬èŠ‚åŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼šè¾“å…¥åµŒå…¥ã€MVAEå’Œè¡Œä¸ºtokenåŒ–ã€‚
\par The primary problem to be solved for the serialized spammer identification model is how to tokenize the user's multi-modal historical behavior. To this end, we construct a two-channel MVAE-based tokenization strategy for user history behaviors. Subsequently, the serialization of the entire user history behavior is completed by the joint CLS$^{\text{S}}$ token. Therefore, this section contains three steps: \textit{Input Embedding}, \textit{MVAE}, and \textit{Behavior Tokenization}.
% ç”±å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„è¾“å…¥åµŒå…¥èƒ½å¤Ÿç¼–ç ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨BERTï¼ˆçœ‹å…¬å¼1ï¼‰è·å¾—å•ä¸ªè¡Œä¸ºæ–‡æœ¬ç‰¹å¾åµŒå…¥$T^v\in \mathbb{R}^{768}$ï¼Œä½¿ç”¨ViTï¼ˆçœ‹å…¬å¼2ï¼‰è·å¾—å›¾åƒåµŒå…¥$I^v\in \mathbb{R}^{768}$ã€‚éšåï¼ŒåŸºäºåŸå§‹åºåˆ—æ„å»ºMVAEç»„ä»¶çš„è¾“å…¥åµŒå…¥åºåˆ—$S_k \in \mathbb{R}^{l\times2\times768}$ï¼ˆçœ‹å…¬å¼3ï¼‰ã€‚
\par \textbf{Input Embedding:} Input embeddings generated by large pre-trained models are capable of encoding rich contextual information. To this end, we use BERT (see Eq. \ref{eq-1}) to obtain a single behavioral text feature embedding $T^v\in \mathbb{R}^{768}$ , and use ViT (see Eq. \ref{eq-2}) to obtain the image embedding $I^v\in \mathbb{R}^{768}$. Subsequently, the input embedding sequence $S \in \mathbb{R}^{l\times2\times768}$ of the MVAE component is constructed based on the original sequence $U$ (see Eq. \ref{eq-3}).
% Since the embedding already contains abundant semantic information, there is little need to further extract contextual information with complex and deep layers in VAEâ€™s encoders. Thus, we decide to use one linear layer followed by batch normalization (BN) and dropout (ğ‘ = 0.2) as the multimodal VAEâ€™s encoder structure. BN is used to reduce internal covariance shift [15] by projecting the input to mean of zero and the variance of 1. We apply BN and dropout as they help overcome overfitting. Formally, the encoder is represented as:
% ç”±äºåµŒå…¥å·²ç»åŒ…å«äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå› æ­¤åœ¨ VAE çš„ç¼–ç å™¨ä¸­æ²¡æœ‰å¿…è¦å†é€šè¿‡å¤æ‚å’Œæ·±å±‚çš„å±‚æ¥è¿›ä¸€æ­¥æå–ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å†³å®šä½¿ç”¨ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç„¶åè¿›è¡Œæ‰¹é‡å½’ä¸€åŒ–ï¼ˆBNï¼‰å’Œå‰”é™¤ï¼ˆğ‘ = 0.2ï¼‰ï¼Œä½œä¸ºå¤šæ¨¡æ€ VAE çš„ç¼–ç å™¨ç»“æ„ã€‚BN ç”¨äºé€šè¿‡å°†è¾“å…¥æŠ•å½±ä¸ºå‡å€¼ä¸ºé›¶å’Œæ–¹å·®ä¸º 1 æ¥å‡å°‘å†…éƒ¨åæ–¹å·®åç§»ã€‚ç¼–ç å™¨çš„å½¢å¼è¡¨ç¤ºä¸º
\par \textbf{MVAE:} Because the embedding already contains rich semantic information, there is no need to go through complex and deep layers to further extract contextual information in the MVAE's encoder. Therefore, we use two linear layers followed by Batch Normalization (BN) and Dropout (rate = 0.2) as the encoder structure for MVAE. BN is used to reduce the internal covariance bias by projecting the inputs to have mean zero and variance one. Subsequently, the formal representation of the encoder is shown below:
\begin{equation}
	z = \mu + \sigma \odot \epsilon
	\label{eq-4}
\end{equation}
\begin{equation}
	\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}\left[\log p_\theta(x|z)\right] - \mathrm{D}_{\text{KL}}(q_\phi(z|x) \parallel p(z))
	\label{eq-5}
\end{equation}
where $\epsilon$ represents the noise sampled from the standard normal distribution $\mathcal{N}(0, I)$. $\odot$ represents the elemental product. $z$ denotes the latent variable. It is satisfying a Gaussian distribution. $\mu$ and $\sigma$ are the mean and variance of $z$, respectively. These parameters describe the conditional distribution $p_\theta(x|z)$ of the latent variable $z$. $\phi$ represents the parameters of the encoder. The objective of the decoder is to approximate the posterior distribution $q_\phi(z|x)$ to the posterior distribution $p_\theta(x|z)$.
% å…¶ä¸­$\epsilon$è¡¨ç¤ºä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ$\mathcal{N}(0, I)$ä¸­é‡‡æ ·çš„å™ªå£°ã€‚$\odot$è¡¨ç¤ºå…ƒç´ ä¹˜ç§¯ã€‚$z$è¡¨ç¤ºæ½œåœ¨å˜é‡ï¼Œå¹¶ä¸”æ»¡è¶³é«˜æ–¯åˆ†å¸ƒã€‚$\mu$å’Œ$\sigma$åˆ†åˆ«ä¸º$z$çš„å‡å€¼å’Œæ–¹å·®ã€‚è¿™äº›å‚æ•°æè¿°äº†æ½œåœ¨å˜é‡$z$çš„æ¡ä»¶åˆ†å¸ƒ$p_\theta(x|z)$ã€‚$\phi$è¡¨ç¤ºç¼–ç å™¨çš„å‚æ•°ã€‚è§£ç å™¨çš„ç›®æ ‡æ˜¯å°†åéªŒåˆ†å¸ƒ$q_\phi(z|x)$æ¥è¿‘å…ˆéªŒåˆ†å¸ƒ$p_\theta(x|z)$ã€‚
% åœ¨MVAEä¸­ï¼Œè¾“å…¥æ•°æ®$x$åŒ…å«æ–‡æœ¬åµŒå…¥$T^v$å’Œå›¾åƒåµŒå…¥$I^v$ï¼Œå³$x=(T^v, I^v)$ã€‚éšåï¼Œç»è¿‡åŒé€šé“çš„ç¼–ç ç»„ä»¶å¾—åˆ°æ½œåœ¨å˜é‡$z^\text{T}$å’Œ$z^\text{I}$ã€‚ç„¶åï¼Œç»„åˆæ–‡æœ¬å’Œå›¾åƒä¸¤ä¸ªé€šé“çš„å¤šæ¨¡æ€æ½œåœ¨ç‰¹å¾çš„åˆ°ç»¼åˆç‰¹å¾$z=\text{concat}(z^\text{T}, z^\text{I})$ã€‚
\par In MVAE, the input data $x$ contains text embedding $T^v$ and image embedding $I^v$, i.e., $x=(T^v, I^v)$. Subsequently, the potential variables $z^\text{T}$ and $z^\text{I}$ are obtained after the encoding component of the two channels. Then, the composite feature $z=\text{concat}(z^\text{T}, z^\text{I})$ is obtained by combining the two-channel multi-modal potential features.
% éšåï¼ŒMVAEéœ€è¦æ„å»ºä¸¤ä¸ªé€šé“çš„è§£ç å™¨è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾é‡å»ºã€‚ä¸¤ä¸ªç¼–ç å™¨å…±äº«ä¸€ä¸ªæ½œåœ¨å˜é‡$z$ã€‚å› æ­¤ï¼ŒMVAEçš„é‡å»ºæŸå¤±å‡½æ•°å¦‚ä¸‹æ‰€ç¤ºï¼š
\par Afterwards, MVAE constructs two channel decoders for multi-modal feature reconstruction. The two encoders share a latent variable $z$. Therefore, the reconstruction loss function for MVAE is shown as follows:
\begin{equation}
	\begin{split}
	\mathcal{L}^\text{T} = \mathbb{E}_{q_\phi(z|T^v)}\left[\log p_\theta(T^v|z)\right] -\mathrm{D}_{\text{KL}}(q_\phi(z|T^v) \parallel p(z))
	\end{split}
	\label{eq-7}
\end{equation}
\begin{equation}
	\mathcal{L}^\text{I} = \mathbb{E}_{q_\phi(z|I^v)}\left[\log p_\theta(I^v|z)\right] - \mathrm{D}_{\text{KL}}(q_\phi(z|I^v) \parallel p(z))
	\label{eq-8}
\end{equation}
where $\mathcal{L}^\text{T}$ and $\mathcal{L}^\text{I}$ represent the loss for the text and image channels. $\mathbb{E}_{q_\phi(z|T^v)}\left[\log p_\theta(T^v|z)\right]$ and $\mathbb{E}_{q_\phi(z|I^v)}\left[\log p_\theta(I^v|z)\right]$ represent the reconstruction loss for the two channel. $\mathrm{D}_{\text{KL}}(q_\phi(z|T^v) \parallel p(z))$ and $\mathrm{D}_{\text{KL}}(q_\phi(z|I^v) \parallel p(z))$ represent the KL divergence Loss. $p(z)$ represents the standard normal distribution, i.e., $p(z)=\mathcal{N}(0, I)$.
% å…¶ä¸­$\mathcal{L}^\text{T}$å’Œ$\mathcal{L}^\text{I}$è¡¨ç¤ºæ–‡æœ¬å’Œå›¾åƒé€šé“çš„æŸå¤±å‡½æ•°ã€‚$\mathbb{E}_{q_\phi(z|T^v)}\left[\log p_\theta(T^v|z)\right]$å’Œ$\mathbb{E}_{q_\phi(z|I^v)}\left[\log p_\theta(I^v|z)\right]$è¡¨ç¤ºä¸¤ä¸ªé€šé“çš„é‡æ„æŸå¤±ã€‚$\mathrm{D}_{\text{KL}}(q_\phi(z|T^v) \parallel p(z))$å’Œ$\mathrm{D}_{\text{KL}}(q_\phi(z|I^v) \parallel p(z))$è¡¨ç¤ºä¸¤ä¸ªé€šé“çš„KLæ•£åº¦æ­£åˆ™æŸå¤±ã€‚$p(z)$è¡¨ç¤ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå³$p(z)=\mathcal{N}(0, I)$ã€‚
\par \textbf{Behavior Tokenization:}
% åœ¨MVAEä¸­ï¼Œæ½œåœ¨å˜é‡$z$åŒ…å«è·¨å¤šæ¨¡æ€ç‰¹å¾ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥å™ªå£°$\epsilon$æ¨¡æ‹Ÿå›¾1å‡ºç°çš„å¤æ‚æƒ…å†µã€‚é€šè¿‡å¾ªç¯é‡å»ºåŒé€šé“ç‰¹å¾ï¼Œå¹²æ‰°ç‰¹å¾ä¼šè¢«æ’é™¤ã€‚å› æ­¤ï¼Œå°†å•ä¸ªè¡Œä¸ºå¯¹åº”æ½œåœ¨å˜é‡$z$ä½œä¸ºé˜¶æ®µ2è¾“å…¥åºåˆ—çš„tokenã€‚
In MVAE, the latent variable $z$ is included across multi-modal features. Meanwhile, the complexities appearing in Fig. \ref{fig-inspire} are simulated by introducing the noise $\epsilon$. Subsequently, the interference features are eliminated by cyclically reconstructing the two-channel features. Therefore, the individual behavior corresponding to the latent variable $z$ is used as a token for the input sequence of stage 2.
% éšåï¼Œè€ƒè™‘åˆ°è®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œé‡‡ç”¨å¹¶è¡ŒåŒ–ç­–ç•¥åŒæ­¥æå–æ•´ä¸ªè¾“å…¥åºåˆ—$S_k \in \mathbb{R}^{l\times2\times768}$çš„æ½œåœ¨å˜é‡$z \in \mathbb{R}^{l\times2D}$ã€‚å…¶ä¸­ï¼Œ$D$è¡¨ç¤ºMVAEç¼–ç å™¨ç»„ä»¶çš„æœ€ç»ˆè¡¨ç¤ºç»´åº¦ã€‚éšåï¼Œè€ƒè™‘åˆ°ä¼ ç»ŸTransformeræ¶æ„ä¸­åˆ†ç±»tokençš„é‡è¦æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨é›¶çŸ©é˜µCLS$^{\text{S}} \in \mathbb{R}^{1\times2D}$å……å½“åˆ†ç±»tokenã€‚æœ€åï¼Œæˆ‘ä»¬ç»„åˆCLS$^{\text{S}}$å’Œ$z$ä½œä¸ºé˜¶æ®µ2çš„è¾“å…¥åºåˆ—$S_k \in \mathbb{R}^{H\times2D}$ã€‚æ­¤æ—¶ï¼Œä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œæˆ‘ä»¬å®šä¹‰$H=l+1$ã€‚
\par Subsequently, a parallelization strategy is used to simultaneously extract the latent variables $z \in \mathbb{R}^{l\times2D}$ from the entire input sequence $S \in \mathbb{R}^{l\times2\times768}$. Where $D$ is a hyper-parameter indicating the final representation dimension of the MVAE encoder component. Later, the importance of classification tokens in the traditional Transformer architecture is taken into account. Therefore, we construct an all-zero matrix CLS$^{\text{S}} \in \mathbb{R}^{1\times2D}$ to act as the classification token. Lastly, we combine CLS$^{\text{S}}$ and $z$ to serve as the input sequence for stage 2, $S_1 \in \mathbb{R}^{H\times2D }$. In this case, to facilitate the computation, we define $H=l+1$.
\subsection{Ultra-long Behavior Sequence Mining}
% åºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­ï¼Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMHAï¼‰å±•ç°äº†æå…¶å¼ºå¤§çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºMHAçš„Transformeræ¶æ„ä½œä¸ºåºåˆ—ç‰¹å¾æŒ–æ˜çš„åŸºæœ¬éª¨å¹²ã€‚å…¶ä¸­ï¼Œç»å…¸MHAçš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š
% å…¶ä¸­ï¼Œ$X$è¡¨ç¤ºè¾“å‡ºåºåˆ—ã€‚$\widehat{X}$è¡¨ç¤ºç»è¿‡MHAè®¡ç®—åçš„æ³¨æ„åŠ›ã€‚$D$è¡¨ç¤ºMVAEç¼–ç å™¨ç»„ä»¶çš„æœ€ç»ˆè¡¨ç¤ºç»´åº¦ã€‚$H$è¡¨ç¤ºåºåˆ—é•¿åº¦ã€‚$Q$ã€$K$å’Œ$V$åˆ†åˆ«å¯¹åº”åºåˆ—$S_k$çš„æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚$\text{Attention}(Q, K, V)$è¡¨ç¤ºå•ä¸ªå¤´çš„æ³¨æ„åŠ›è®¡ç®—ã€‚$K^\text{T}$è¡¨ç¤º$K$çŸ©é˜µçš„è½¬ç½®ã€‚$d$è¡¨ç¤ºé”®çš„ç»´åº¦ã€‚$head_i$è¡¨ç¤ºç¬¬iä¸ªå¤´çš„æ³¨æ„åŠ›ã€‚$n$è¡¨ç¤ºMHAçš„å¤´æ•°é‡ã€‚$W^\text{Q}_{i}$ã€$W^\text{K}_{i}$ã€$W^\text{V}_{i}$å’Œ$W^\text{M}$è¡¨ç¤ºå¯è®­ç»ƒçš„å‚æ•°çŸ©é˜µã€‚
\par \textbf{MHA:} The multi-head attention mechanism (MHA) has shown to be extremely powerful in sequence modeling tasks. Therefore, the MHA-based Transformer architecture is used as the basic backbone for sequence feature mining. Among them, the computational process of classical MHA is shown as follows:
\begin{equation}
	Q=XW^\text{Q},\ K=XW^\text{K},\ V=XW^\text{V}
	\label{eq-9}
\end{equation}
\begin{equation}
	\text{Attention}(Q, K, V)=\text{softmax}(\frac{QK^\text{T}}{\sqrt{d}})V
	\label{eq-10}
\end{equation}
\begin{equation}
	head_i=\text{Attention}(QW^\text{Q}_{i}, KW^\text{K}_{i}, VW^\text{V}_{i})
	\label{eq-11}
\end{equation}
\begin{equation}
	\begin{split}
		\widehat{X}&=\text{MHA}(Q, K, V)\\
		&=\text{Concat}(head_1,...,head_n)W^\text{M}
		\label{eq-12}
	\end{split}
\end{equation}
where $X$ represents the input sequence. $\widehat{X}$ represents the sequence after MHA computation. $D$ represents the final representation dimension of the MVAE encoder component. $H$ represents the sequence length. $Q$, $K$, and $V$ represent the query, key, and value of the sequence $X$, respectively. $\text{Attention}(Q, K, V)$ represents the attention computation for a individual head. $K^\text{T}$ represents the transpose of the $K$ matrix. $d$ represents the dimension of the key. $head_i$ represents the attention of the $i$-th head. $n$ represents the number of heads in the MHA. $W^\text{Q}_{i}$, $W^\text{K}_{i}$, $W^\text{V}_{i}$, and $W^\text{M}$ represent the trainable parameter matrices.
% åœ¨åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—æ˜¯ä¸€ä¸ªè¶…é•¿åºåˆ—ã€‚å› æ­¤ï¼Œä¼ ç»ŸMHAè®¡ç®—ä¸­çš„$QK^\text{T}$ï¼ˆçœ‹å…¬å¼8ï¼‰ä¼šæ„å»ºä¸€ä¸ªè¶…å¤§çš„çŸ©é˜µä»è€Œé€ æˆæ˜¾å­˜å´©æºƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å±‚æ¬¡åˆ†å‰²çª—å£æ³¨æ„åŠ›ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨$QK^\text{T}$ä¸­æ„å»ºæ»‘åŠ¨çª—å£$\lambda$ã€‚éšåï¼Œçª—å£$\lambda$ä»¥æ­¥é•¿$W$æ»‘åŠ¨å–æ ·ã€‚ç»è¿‡SW-MHAè®¡ç®—ï¼Œè¶…é•¿åºåˆ—$H$ä¼šè¢«è½¬åŒ–ä¸º$\frac{H}{W}$ã€‚æ­¤æ—¶ï¼Œè¶…é•¿åºåˆ—å°±ä¼šè¢«è½¬åŒ–ä¸ºæ ‡å‡†åºåˆ—å»ºæ¨¡é—®é¢˜ã€‚æ­¤æ—¶ï¼Œåˆ†ä¸¤ä¸ªé˜¶æ®µå¯¹$QK^\text{T}$è¿›è¡Œåˆ†å—è®¡ç®—çª—å†…å’Œçª—å¤–æ³¨æ„åŠ›ï¼Œä»è€Œè¿‘ä¼¼è®¡ç®—æ•´ä¸ªåºåˆ—çš„æ³¨æ„åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤å±‚çª—å£æ³¨æ„åŠ›è®¡ç®—ï¼šSW-MHAå’ŒW-MHAã€‚
\par In the spammer detection task, the sequence of historical user behaviors is an ultra-long sequence. Therefore, $QK^\text{T}$ in traditional MHA computation (see Eq. 8) will construct an oversized matrix thus causing GPU memory crash (see Table \ref{table-ab-memory}). For this reason, we propose hierarchical split window attention. The core idea is to construct sliding windows $W$ in $QK^\text{T}$. Subsequently, the window $W$ is sliding sampled in steps $\lambda$. Subsequently, the length $H$ of the ultra-long sequence is transformed into $\frac{H}{\lambda}$. Thus, the problem of modeling ultra-long sequences is transformed into the problem of modeling standard sequences. In this case, the intra-window and inter-window attention is computed in chunks for $QK^\text{T}$ at two levels, thus approximating the attention of the entire sequence. To this end, a two-level windowed attention computation is proposed: \textit{SW-MHA} and \textit{W-MHA}.
% ä¸ºäº†é¿å…å‡ºç°$QK^\text{T}$è¶…å¤§çŸ©é˜µï¼Œå› æ­¤éœ€è¦åœ¨$Q$å’Œ$K$å‘é‡ä¸­è¿›è¡Œæ»‘åŠ¨åˆ†çª—ã€‚éšåï¼Œå¹¶è¡Œè®¡ç®—çª—å£åºåˆ—çš„çª—å†…æ³¨æ„åŠ›ã€‚å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
% å…¶ä¸­ï¼Œ$\text{SSW}$è¡¨ç¤ºçª—å£æ»‘åŠ¨åˆ†å‰²å‡½æ•°ã€‚å‡è®¾ï¼Œ$Q$ã€$K$å’Œ$V$å‡ä¸ºåºåˆ—é•¿åº¦$H$å’ŒåµŒå…¥ç»´åº¦$\eta$çš„çŸ©é˜µã€‚é‚£ä¹ˆï¼Œ$Q^{\text{SW}}$ã€$K^{\text{SW}}$å’Œ$V^{\text{SW}}$è¢«è½¬åŒ–ä¸ºåºåˆ—é•¿åº¦$\frac{H}{W}$å’ŒåµŒå…¥ç»´åº¦$W\times\eta$çš„çŸ©é˜µã€‚
\par \textbf{SW-MHA:} To avoid $QK^\text{T}$ oversized matrices, sliding window splitting is performed in the $Q$ and $K$ vectors. Subsequently, the intra-window attention of the window sequence is computed in parallel. The equation is shown as follows:
\begin{equation}
	\begin{split}
	\widehat{Q}=\text{SW}(XW^\text{Q}), \widehat{K}=\text{SW}(XW^\text{K}), \widehat{V}=\text{SW}(XW^\text{V})
	\end{split}
	\label{eq-13}
\end{equation}
\begin{equation}
	\text{SW-Att}_i=\text{softmax}(\frac{\widehat{Q}_{i}\widehat{K}_{i}^\text{T}}{\sqrt{d}})\widehat{V}_{i}, i\in [1, k]
	\label{eq-13-1}
\end{equation}
\begin{equation}
	\text{SW-Attention}(Q, K, V)=\text{concat}(\text{SW-Att}_1, ..., \text{SW-Att}_k)
	\label{eq-13-2}
\end{equation}
where $\text{SW}$ represents the sliding window splitting function, and the implementation process is shown in Fig. \ref{fig-SW-MHA} (a). $k$ represents the length of the sequence after the split window. It is assumed that $Q$, $K$ and $V$ are matrices of sequence length $H$ and embedding dimension $\eta$. Then, $Q$, $K$ and $V$ are all transformed into $\mathbb{R}^{\frac{H}{\lambda}\times W\times\eta}$ by the SW algorithm. $W$ and $\lambda$ represent the window length and sliding step, respectively.
% å…¶ä¸­ï¼Œ$\text{SW}$ è¡¨ç¤ºçª—å£æ»‘åŠ¨åˆ†å‰²å‡½æ•°ï¼Œå®ç°è¿‡ç¨‹å¦‚å›¾1ï¼ˆaæ‰€ç¤ºï¼‰ã€‚å‡è®¾ï¼Œ$Q$ã€$K$å’Œ$V$å‡ä¸ºåºåˆ—é•¿åº¦$H$å’ŒåµŒå…¥ç»´åº¦$\eta$çš„çŸ©é˜µã€‚é‚£ä¹ˆï¼Œ$Q$ã€$K$å’Œ$V$å‡è¢«SWç®—æ³•è½¬åŒ–ä¸º$mathbb{R}^{\frac{H}{\lambda}\times\lambda\times\eta}$ã€‚
% éšåï¼ŒåŸºäºå…¬å¼ï¼ˆ12ï¼‰è®¡ç®—å¤šå¤´æ³¨æ„åŠ›ã€‚æœ€åï¼Œä¸ºäº†æ»¡è¶³Transformeræ¶æ„è®¡ç®—ï¼Œæˆ‘ä»¬å°†$\widehat{X}$åœ¨æœ€åä¸¤ä¸ªç»´åº¦å±•å¼€ã€‚å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
% å…¶ä¸­$X^{\text{SW}} \in \mathbb{R}^{\frac{H}{W}\times W\eta}}$è¡¨ç¤ºç»è¿‡çª—å†…æ³¨æ„åŠ›è®¡ç®—åçš„ç‰¹å¾çŸ©é˜µã€‚
\par Subsequently, the multi-head attention is computed in combination with Eq. (12). Finally, to satisfy the Transformer architecture calculation, $\widehat{X}$ is expanded in the last two dimensions. The equation is shown as follows:
\begin{equation}
	X^{\text{SW}}=\text{Flatten}(\widehat{X})
	\label{eq-14}
\end{equation}
where $X^{\text{SW}} \in \mathbb{R}^{\frac{H}{\lambda}\times W\eta}$ represents the feature matrix after intra-window attention (SW-MHA).
\par \textbf{W-MHA:} The computation process for inter-window attention uses traditional MHA (see Eqs. (\ref{eq-9}-\ref{eq-12})). Unlike MHA, the input sequence for inter-window attention is the features obtained after the calculation of intra-window attention. Therefore, inter-window attention does not change the shape of the input features.
\par \textbf{Transformer Block:} Subsequently, a comprehensive approximate representation of the overall attention is made based on the intra-window attention (SW-MHA) and inter-window attention (W-MHA). Given the effectiveness of the Transformer architecture, SW-Transformer Block and W-Transformer Block are constructed based on SW-MHA and W-MHA. The specific equations are shown as follows:
\begin{equation}
	\begin{split}
		X^{\text{SW}}&=\text{SW-MHA}(\text{LN}(X)))\\
		\xi&=\text{SW-MLP}(\text{LN}(X^{\text{SW}})))
	\end{split}
	\label{eq-15}
\end{equation}
\begin{equation}
	\begin{split}
		\widehat{\xi}&=\xi+\text{W-MHA}(\text{LN}(\xi)))\\
		X^{\text{O}}&=\widehat{\xi}+\text{MLP}(\text{LN}(\widehat{\xi})))
	\end{split}
	\label{eq-16}
\end{equation}
where Eq. (\ref{eq-15}) describes the architecture of SW-Transformer Block and Eq. (\ref{eq-16}) describes the architecture of W-Transformer Block. LN represents Layer Normalization. MLP represents standard multilayer linear perceptron. SW-MLP represents the multilayer linear perceptron specifically for SW-Transformer Block. Compared to MLP, SW-MLP acts not to increase the hidden state space but to decrease the feature dimension. For instance, the output feature $X^{\text{SW}} \in \mathbb{R}^{\frac{H}{\lambda}\times W\eta}$ of SW-MHA has an embedding dimension of $W\eta$. MLP does it by raising the dimension first to $2W\eta$ and subsequently lowering it to $W\eta$. The very large dimension space of $2W\eta$ may also cause GPU memory crash. Therefore, SW-MLP reduces the dimensionality first to $4\eta$ and subsequently to $2\eta$. In this case, SW-MLP prevents GPU memory explosion and also constructs the hidden state space. $\widehat{\xi}$ and $\xi$ represent intermediate variables. $X^{\text{O}}$ represents the Block output. Let's assume that the dimension of the input feature $X$ is $\mathbb{R}^{H\times\eta}$, then the dimensions of $\xi$, $\widehat{\xi}$, and $X^{\text{O}}$ are $\mathbb{R}^{\frac{H}{\lambda}\times2\eta}$.
% Stage 2çš„è¾“å…¥åºåˆ—$X$ä¸º$S_k \in \mathbb{R}^{H\times2D}$ã€‚å› æ­¤ï¼ŒStage 2çš„ä¸»è¦ä»»åŠ¡æ˜¯è§£å†³è¶…é•¿åºåˆ—é€ æˆçš„GPUå†…å­˜å´©æºƒå¨èƒã€‚ä¸ºæ­¤ï¼ŒStage 2éœ€è¦ä¸€ä¸ªè¾ƒå¤§çš„çª—å£$\lambda$çš„æ»‘åŠ¨æ­¥é•¿$W_1$ç”¨äºçª—å†…ç‰¹å¾æŒ–æ˜ï¼ˆSW-Transformer Blockï¼‰ã€‚åŒæ—¶ï¼Œä¸ºäº†æŒ–æ˜çª—é—´å…³ç³»ç‰¹å¾ï¼Œè®¾å®š3å±‚W-Transformer Blockã€‚å¯¹æ¯”Stage 2ï¼ŒStage 3çš„é‡ç‚¹æ˜¯æŒ–æ˜åºåˆ—ç‰¹å¾ã€‚å› æ­¤ï¼ŒStage 2çš„SW-Transformer Blockçª—å£$\lambda$çš„æ»‘åŠ¨æ­¥é•¿$W_1$ä¸éœ€è¦è¿‡å¤§ã€‚åŒæ—¶ï¼Œéœ€è¦è®¾å®šæ›´å¤šçš„W-Transformer BlockæŒ–æ˜åºåˆ—ç‰¹å¾ã€‚
\par \textbf{Stage 2 and 3:} The input sequence $X$ for Stage 2 is $S_1 \in \mathbb{R}^{H\times2D}$. Therefore, the main task of Stage 2 is to address the threat of GPU memory crashes caused by ultra-long sequences. To this end, a larger sliding step $\lambda_1$ of the window $W_1$ for intra-window feature mining (SW-Transformer Block) is required in Stage 2. Meanwhile, to mine inter-window relationship features, 3 layers of W-Transformer Block are set. Comparing with Stage 2, Stage 3 focuses on mining sequence features. Therefore, the sliding step $\lambda_2$ of the SW-Transformer Block window $W_2$ in Stage 2 does not need to be too large. Meanwhile, more W-Transformer Block needs to be set to mine sequence features.

\subsection{Spammer Detection}
% å‰ä¸¤èŠ‚åˆ†åˆ«æè¿°äº†Stage 1-3çš„å·¥ä½œåŸç†ã€‚æ­¤æ—¶ï¼ŒStage 4çš„ç›®æ ‡æ˜¯è¯†åˆ«åƒåœ¾é‚®ä»¶å‘é€è€…ï¼Œè¾“å…¥åºåˆ—$S_k$ç»´åº¦ä¸º$\mathbb{R}^{\frac{H}{W_1W_2}\times8D}$ã€‚é¦–å…ˆï¼Œä»åºåˆ—$S_k$ä¸­é€‰æ‹©åˆ†ç±»token$\text{CLS}^{\text{S}_k} \in 8D$ã€‚éšåï¼Œå°†$\text{CLS}^{\text{S}_k}$è¾“å…¥ä¸¤å±‚çº¿æ€§å±‚ä¸­è¿›è¡Œç‰¹å¾å­¦ä¹ å’Œç»´åº¦è½¬æ¢ã€‚æœ€åï¼Œå€ŸåŠ©softmaxå‡½æ•°è¯†åˆ«åƒåœ¾é‚®ä»¶å‘é€è€…ã€‚å› æ­¤ï¼Œæ¨¡å‹çš„æœ€ç»ˆç›®æ ‡å‡½æ•°å¦‚ä¸‹æ‰€ç¤ºï¼š
% å…¶ä¸­ï¼Œ$\widehat{Y_k}$è¡¨ç¤ºæ¨¡å‹è¾“å‡ºã€‚Linerè¡¨ç¤ºçº¿æ€§å±‚ã€‚Dropoutè¡¨ç¤ºDropoutå±‚ï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†å……åˆ†è®­ç»ƒæ¨¡å‹å‚æ•°ã€‚éšåï¼Œæ¨¡å‹åˆ†ç±»æŸå¤±é‡‡ç”¨äº¤å‰ç†µå‡½æ•°ã€‚å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
% å…¶ä¸­ï¼Œ$N$è¡¨ç¤ºä¸€ä¸ªæ‰¹æ¬¡æ¨¡å‹è¾“å…¥æ ·æœ¬æ•°é‡ã€‚$Y_k$è¡¨ç¤ºæ ·æœ¬çœŸå®æ ‡ç­¾ã€‚$\widehat{Y_k}$è¡¨ç¤ºæ¨¡å‹é¢„æµ‹ã€‚éšåï¼Œç»“åˆMVAEåŒé€šé“æŸå¤±$\mathcal{L}^\text{I}$å’Œ$\mathcal{L}^\text{T}$å¾—åˆ°æ¨¡å‹æ€»æŸå¤±$\mathcal{L}^\text{Total}$ã€‚å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
% å…¶ä¸­ï¼Œ$\psi_1$ã€$\psi_2$å’Œ$\psi_3$åˆ†è¡¨è¡¨ç¤ºä¸‰ä¸ªæŸå¤±å‡½æ•°çš„è¡°å‡å› å­ã€‚
\par Stage 1-3 are described in the previous two sections, respectively. In this section, the objective of Stage 4 is to identify spammers with an input sequence $S_3$ of dimension $\mathbb{R}^{\frac{H}{\lambda_1\lambda_2}\times8D}$. Firstly, the classification token $\text{CLS}^{\text{S}} \in 8D$ is selected from the sequence $S_3$. Subsequently, $\text{CLS}^{\text{S}}$ is input into two linear layers for feature learning and dimension transformation. Finally, spammers are identified with the help of softmax function. Thus, the final objective function of the model is shown as follows:
\begin{equation}
	\widehat{Y}=\text{Liner}(\text{Dropout}(\text{Liner}(\text{CLS}^{\text{S}})))
	\label{eq-17}
\end{equation}
where $\widehat{Y}$ represents the model output. Liner represents the linear layer. Dropout represents the Dropout layer, which is intended to adequately train the model parameters. Subsequently, the model classification loss uses the cross-entropy function. The equation is shown as follows:
\begin{equation}
	\mathcal{L}=-\frac{1}{N}\sum^{N}_{k}Y_k\text{log}(\widehat{Y_k})
	\label{eq-18}
\end{equation}
where $N$ represents the number of input samples in a batch. $Y_k$ represents the true labels of the samples. $\widehat{Y_k}$ represents the model prediction. Subsequently, the model total loss $\mathcal{L}^\text{Total}$ is obtained by combining the MVAE two-channel loss $\mathcal{L}^\text{I}$ and $\mathcal{L}^\text{T}$. The equation is shown as follows:
\begin{equation}
	\mathcal{L}^\text{Total}=\psi_1\times\mathcal{L}+\psi_2\times\mathcal{L}^\text{I}+\psi_3\times\mathcal{L}^\text{T}
	\label{eq-19}
\end{equation}
where $\psi_1$, $\psi_2$, and $\psi_3$ represent the decay factors of the three losses, respectively.

\subsection{Learning Algorithm}% æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„Transformerï¼Œç§°ä¸ºMS$^2$Dformerã€‚æ¨¡å‹ä¸­ä¸»è¦åŒ…å«ä¸¤ç§æ–°çš„ç®—æ³•ã€‚é¦–å…ˆï¼ŒåŸºäºä¼ ç»ŸVAEå»ºç«‹äº†åŒé€šé“çš„MVAEç”¨äºå¤šæ¨¡æ€è¡Œä¸ºé‡åŒ–ï¼ˆçœ‹ç®—æ³•1ï¼‰ã€‚éšåï¼Œæå‡ºåŸºäºåˆ†å‰²çª—å£æ³¨æ„åŠ›æœºåˆ¶çš„SW-Transformer Blockï¼ˆçœ‹ç®—æ³•2ï¼‰ã€‚
\par A new Transformer, called MS$^2$Dformer, is proposed. Firstly, a two-channel MVAE for multi-modal behavior quantification is built based on the classical VAE (see Algorithm \ref{alg-1}). Subsequently, SW-Transformer Block based on the split-window attention mechanism is proposed (see Algorithm \ref{alg-2}).

\begin{algorithm}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{{User Historical Behavior Tokenization}}
	\label{alg-1}
	\begin{algorithmic}[1]
		\REQUIRE \
		\par The sequence of historical user behaviors $U=\{b_0, ... , b_l\}$;
		\par Individual user behavior $b_i=(T_i, I_i)$;
		\ENSURE \ 
		\par Behavior tokens: $S_1$, Dual-channel loss: $\mathcal{L}^\text{T}$, and $\mathcal{L}^\text{I}$;
		
		\STATE $U \to \{(T^v_0, I^v_l),..., (T^v_l, I^v_l)\}=(T^v, I^v)$ by Eqs. (\ref{eq-1}-\ref{eq-3});
		\STATE $T^v \to z^\text{T}$ and $I^v \to z^\text{I}$ by dual-channel encoder;
		\STATE $z=\mu + \sigma \odot \epsilon=\text{concat}(z^\text{T}, z^\text{I})$;
		\STATE $z \to T^v_d$ and $z \to I^v_d$ by dual-channel decoder;
		\STATE $(T^v_d \to T^v) \to \mathcal{L}^\text{T}$ and $(I^v_d \to I^v) \to \mathcal{L}^\text{I}$;
		\STATE $S_1=\text{concat}(\text{CLS}^{\text{S}}, z)$;
		\STATE \textbf{return} $S_1$, $\mathcal{L}^\text{T}$, and $\mathcal{L}^\text{I}$;
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{{SW-MHA}}
	\label{alg-2}
	\begin{algorithmic}[1]
		\REQUIRE \
		\par Input Feature Sequence $S^\text{I} \in \mathbb{R}^{H\times\eta}$;
		\ENSURE \ 
		\par Output Feature Sequence $S^\text{O} \in \mathbb{R}^{\frac{H}{\lambda}\times W\eta}$;
		
		\STATE $Q=S^\text{I}W^\text{Q},\ K=S^\text{I}W^\text{K},\ V=S^\text{I}W^\text{V}$;
		\FOR{$i$ from $0$ to $H$ step $\lambda$)}
			\STATE $\widehat{Q}_i=Q[i\ \text{to}\ W], \widehat{K}_i=K[i\ \text{to}\ W], \widehat{V}_i=V[i\ \text{to}\ W]$;
		\ENDFOR \\
		// $\widehat{Q} \in \mathbb{R}^{\frac{H}{\lambda}\times W\times\eta}, \widehat{K} \in \mathbb{R}^{\frac{H}{\lambda}\times W\times\eta}, \widehat{V} \in \mathbb{R}^{\frac{H}{\lambda}\times W\times\eta}$  by window $W$ and sliding step $\lambda$;\\
		// To prevent cases where $H$ is not a multiple of $\lambda$, link $\lambda$ empty elements at the end of the sequence $S^\text{I}$, similar to the padding process used to CNNs, i.e. $S^\text{I} \in \mathbb{R}^{(H+\lambda)\times\eta}$;
		\STATE $head_i=\text{Attention}(\widehat{Q}W^\text{Q}_{i}, \widehat{K}W^\text{K}_{i}, \widehat{V}W^\text{V}_{i})$;
		\STATE $\widehat{X}=\text{MHA}(\widehat{Q}, \widehat{K}, \widehat{V}) \in \mathbb{R}^{\frac{H}{\lambda}\times W\times\eta}$\\
		\ \ \ \ \ $=\text{Concat}(head_1,...,head_n)W^\text{M}$;
		\STATE $X^{\text{SW}}=\text{Flatten}(\widehat{X})$;
		\STATE \textbf{return} $S^\text{O} \in \mathbb{R}^{\frac{H}{\lambda}\times W\eta}$
	\end{algorithmic}
\end{algorithm}
\subsection{Time Complexity Analysis}
% å¦‚å›¾2ï¼ˆaï¼‰æ‰€ç¤ºï¼ŒMVAEé‡‡ç”¨åŒé€šé“çš„VAEç»“æ„ã€‚å› æ­¤ï¼ŒMVAEçš„æ—¶é—´å¤æ‚åº¦ä¸º$T_{\text{MVAE}}=T^{\txet{I}}_{\text{encoder}}+T^{\txet{I}}_{\text{decoder}}+T^{\txet{T}}_{\text{encoder}}+T^{\txet{T}}_{\text{decoder}}+T^{z}=O(4((H-1)\cdot 256(768+D)+D)\sim O(H-1)$ã€‚éšåï¼ŒSW-MHAçš„æ—¶é—´å¤æ‚åº¦ä¸º$O(k(W^2\cdot \eta))$ã€‚å…¶ä¸­ï¼Œ$k$è¡¨ç¤ºçª—å£åºåˆ—çš„é•¿åº¦ã€‚$W$è¡¨ç¤ºçª—å£é•¿åº¦ã€‚$\eta$è¡¨ç¤ºè¾“å…¥åºåˆ—ç‰¹å¾çš„ç»´åº¦ã€‚å› æ­¤ï¼ŒStage 2çš„æ—¶é—´å¤æ‚åº¦ä¸º$T_{\text{Satge 1}}=T_{\text{SW-Block}}+b\cdot T_{\text{W-Block}}$ã€‚å…¶ä¸­ï¼Œ$T_{\text{SW-Block}}=O(2D\cdot W_1^2+4D(2D+2DW_1))\sim O(D\cdot (W_1)^2+D^2W_1)$ï¼Œ$T_{\text{SW-Block}}=2DW_1\cdot (H/\lambda_1)^2+(4DW_1)^2\sim DW_1\cdot (H/\lambda_1)^2+(DW_1)^2$ã€‚å› æ­¤ï¼Œ$T_{\text{Satge 1}}=O(D\cdot (H/\lambda_1)^2+D^2W_1)+O(b(DW_1\cdot (H/\lambda_1)^2+(DW_1)^2))\sim O(H/\lambda_1)^2$ã€‚ç±»ä¼¼çš„ï¼ŒStage 3çš„æ—¶é—´å¤æ‚åº¦ä¸º$O(H/(\lambda_1\cdot \lambda_2))^2$ã€‚å› æ­¤ï¼ŒMS$^2$formerçš„æ€»ä½“æ—¶é—´å¤æ‚åº¦ä¸º$O(H-1)+O(H/\lambda_1)^2+O(H/(\lambda_1\cdot \lambda_2))^2\sim O(H/\lambda_1)^2$ã€‚ä¸ä¼ ç»ŸåŸºäºMHAçš„Transformeræ¶æ„ï¼ˆ$O(H^2)$ï¼‰ç›¸æ¯”,MS$^2$formeråœ¨è¿è¡Œæ•ˆç‡æ–¹é¢æ›´èƒœä¸€ç­¹ï¼ˆsee Table 4ï¼‰ã€‚
\par As shown in Fig. \ref{fig-2} (a), MVAE adopts a two-channel VAE structure. Therefore, the time complexity of MVAE is $T_{\text{MVAE}}=T^{\text{I}}_{\text{encoder}}+T^{\text{I}}_{\text{decoder}}+T^{\text{T}}_{\text{encoder}}+T^{\text{T}}_{\text{decoder}}+T^{z}=O(4((H-1)\cdot 256(768+D))+D)\sim O(H-1)$. Subsequently, the time complexity of SW-MHA is $O(k(W^2\cdot \eta))$. Where $k$ denotes the length of the window sequence. $W$ denotes the window length. $\eta$ denotes the dimension of the input sequence features. Therefore, the time complexity of Stage 2 is $T_{\text{Satge 1}}=T_{\text{SW-Block}}+b\cdot T_{\text{W-Block}}$. where $T_{\text{SW-Block}}=O(2D\cdot W_1^2+4D(2D+2DW_1))\sim O(D\cdot (W_1)^2+D^2W_1)$, and $T_{\text{SW-Block}}=2DW_1\cdot (W_1)^2+( 4DW_1)^2\sim DW_1\cdot (H/\lambda_1)^2+(DW_1)^2$. Thus, $T_{\text{Satge 1}}=O(D\cdot (W_1)^2+D^2W_1)+O(b(DW_1\cdot (H/\lambda_1)^2+(DW_1)^2))\sim O((H/\lambda_1)^2)$. Similarly, the time complexity of Stage 3 is $O((H/(\lambda_1\cdot \lambda_2))^2)$. Therefore, the overall time complexity of MS$^2$former is $O(H-1)+O((H/\lambda_1)^2)+O((H/(\lambda_1\cdot \lambda_2))^2)\sim O((H/\lambda_1)^2)$. Compared with the traditional MHA-based Transformer architecture ($O(H^2)$), MS$^2$former is superior in terms of operational efficiency (see Table \ref{table-ab-memory}).

\begin{figure*}[h]
	\center{\includegraphics[width=1.03\linewidth]  {./image/Paramters_3.pdf}} 
	\caption{Statistics from two publicly available datasets.}
	\label{fig-Parameters-windows}
\end{figure*}
\begin{figure*}[h]
	\center{\includegraphics[width=1.03\linewidth]  {./image/Paramters_1.pdf}} 
	\caption{The influence of different numbers of historical behaviors on model training.}
	\label{fig-behavior}
\end{figure*}
\section{Experiment and Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Data}	
% æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å¼€çš„è™šå‡ä¿¡æ¯æ£€æµ‹æ•°æ®é›†\cite{Yang2024model}ï¼Œå³Weibo V1å’ŒWeibo V2ï¼Œä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚åƒåœ¾é‚®ä»¶å‘é€è€…é€šå¸¸åœ¨ç»å¸¸å‘é€è™šå‡ä¿¡æ¯çš„ç”¨æˆ·ç¾¤ä½“ä¸­äº§ç”Ÿã€‚ä½†æ˜¯ï¼Œè¿™äº›ç”¨æˆ·å¯èƒ½å¹¶ä¸æ¸…æ¥šäº‹å®ä»…ä»…åªæ˜¯æ„Ÿå…´è¶£ä»è€Œè½¬å‘ä¿¡æ¯ï¼Œå› æ­¤ä¸æ˜¯ä»»ä½•å‘é€è¿‡è™šå‡ä¿¡æ¯çš„ç”¨æˆ·å°±æ˜¯çœŸæ­£çš„åƒåœ¾é‚®ä»¶å‘é€è€…ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è˜è¯·äº†å¤šä¸ªé¢†åŸŸçš„ç ”ç©¶ç”Ÿä¸“å®¶å¯¹Yangç­‰äººå…¬å¼€çš„ä¸¤ä¸ªæ•°æ®é›†ä¸­å‘å¸ƒè™šå‡ä¿¡æ¯çš„ç”¨æˆ·è¿›ä¸€æ­¥åˆ¤æ–­ï¼Œä»è€Œç­›é€‰å‡ºçœŸæ­£çš„åƒåœ¾é‚®ä»¶å‘é€è€…ã€‚éšåï¼Œæˆ‘ä»¬æ„å»ºå¹¶æ”¶é›†äº†Weibo 2023å’ŒWeibo 2024ä¸¤ä¸ªå…¬å¼€çš„åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹åŸºå‡†æ•°æ®é›†ã€‚å…¶ä¸­ï¼ŒWeibo V1åŒ…å«2022åˆ°2023å¹´å†…è¢«å¾®åšå¹³å°æ ‡è®°çš„1000æ¡è™šå‡ä¿¡æ¯å’Œ948ä¸ªåƒåœ¾é‚®ä»¶å‘é€è€…æ½œåœ¨ç”¨æˆ·ï¼Œå‘é€è™šå‡ä¿¡æ¯è¶…è¿‡2æ¡çš„ç”¨æˆ·ä»…æœ‰47ä¸ªã€‚å› æ­¤ï¼ŒWeibo 2023ä»…ç¡®å®š342ä¸ªåƒåœ¾é‚®ä»¶å‘é€è€…ã€‚ç±»ä¼¼çš„ï¼ŒWeibo V2åŒ…å«2022åˆ°2024å¹´å†…è¢«å¾®åšå¹³å°æ ‡è®°çš„5661æ¡è™šå‡ä¿¡æ¯å’Œ5653ä¸ªåƒåœ¾é‚®ä»¶å‘é€è€…æ½œåœ¨ç”¨æˆ·ã€‚å› æ­¤ï¼ŒWeibo 2023ä»…ç¡®å®š952ä¸ªåƒåœ¾é‚®ä»¶å‘é€è€…ã€‚ä¸¤ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡å¦‚è¡¨1æ‰€ç¤ºï¼š
\begin{table}[t]
	\renewcommand{\arraystretch}{1.3}
	\centering
	\caption{The statistics of the two datasets}
	\begin{tabular}{c|c c}
		\toprule[1.5pt]
		Statistic & Weibo 2023  & Weibo 2024 \\ \hline \hline
		$\#$ of Users& $684$ & $1971$ \\ \hline
		$\#$ of Normal& $343$ & $1019$ \\ \hline
		$\#$ of Spammer & $341$ & $952$ \\ \hline
		Avg. Behavior Length& $26,192$ & $25,310$ \\ \hline
		Max Behavior Length& $292,491$ & $308,798$ \\ \hline
		Min Behavior Length& $15$ & $1$ \\ \hline
		\bottomrule[1.5pt]
	\end{tabular}
	\label{table-datasets}
\end{table}
\par We conducted model training on two publicly available fake news detection datasets \cite{Yang2024model}, i.e., Weibo V1\footnote{\url{https://github.com/yzhouli/DDCA-Rumor-Detection/tree/main/MisDerdect.}} and Weibo V2\footnote{\url{https://github.com/yzhouli/SocialNet/tree/master/Weibo.}}. Spammers are usually created by users who frequently send out fake news. However, these users may not be aware of the facts or be interested in forwarding the news, so not every user who has sent fake news is a real spammer. Therefore, we hired graduate experts in various fields to determine further the users who posted fake news in the two datasets disclosed by \cite{Yang2024model} to filter out the real spammers. Subsequently, we constructed and collected two publicly available spammer detection benchmark datasets, Weibo 2023 and Weibo 2024. Weibo V1 contains 1,000 fake news and 948 potential users of spammers flagged by the Weibo platform between 2022 and 2023, and only 47 users who sent more than two fake news. Therefore, Weibo 2023 identifies only 342 spammers. Similarly, Weibo V2 contains 5,661 fake news and 5,653 potential users of spammers that were flagged by the Weibo platform between 2022 and 2024. Therefore, Weibo 2023 identifies only 952 spammers. The statistics of the two datasets are shown in Table \ref{table-datasets} and Fig.\ref{fig-Parameters-windows}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parameters Setting}
% æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªç‰ˆæœ¬çš„ç®—æ³•ï¼Œç§°ä¸ºMS$^2$Dformer\_Bï¼ŒMS$^2$Dformer\_Mï¼Œå’ŒMS$^2$Dformer\_Lã€‚ä»–ä»¬åˆ†åˆ«å¯¹åº”MS$^2$Dformeræ¶æ„çš„åŸºç¡€ã€ä¸­ç­‰å’Œå¤§å‹ç‰ˆæœ¬ã€‚å…·ä½“è®¾ç½®å¦‚ä¸‹ï¼š
\par \textbf{Model Variants:} Three variants of the MS$^2$Dformer model are constructed to cope with different usage environments:
\begin{equation}
	\begin{split}
		\text{MS$^2$Dformer}\_\text{B}:\{D&=16,W=\{64,64\},\\\lambda&=\{32, 4\},B=\{3, 3\}\}\\
	\end{split}
	\label{eq-base}
\end{equation}
\begin{equation}
	\begin{split}
		\text{MS$^2$Dformer}\_\text{M}:\{D&=16,W=\{128,64\},\\\lambda&=\{32, 4\},B=\{3, 11\}\}\\
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		\text{MS$^2$Dformer}\_\text{L}:\{D&=64,W=\{128 ,64\},\\\lambda&=\{32, 4\},B=\{7, 17\}\}
	\end{split}
\end{equation}
where $D$ represents the embedding dimension of the encoder in MVAE. $W$ represents the SW-MHA window size in Stage 2-3. $\lambda$ represents the window sliding step. $B$ represents the number of SW-MHA or W-MHA Transformer Block.
% æˆ‘ä»¬åœ¨Tensorflow 2.9.0å’ŒPython 3.8çš„è½¯ä»¶å¹³å°ä¸Šç¼–å†™æ¨¡å‹æºä»£ç ã€‚å¹¶ä¸”ï¼Œæ•°æ®é›†ä»¥7ï¼š2ï¼š1çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨Adamä½œä¸ºæ¨¡å‹çš„ä¼˜åŒ–å™¨ï¼Œå¹¶ä¸”è®¾ç½®å­¦ä¹ ç‡ä¸º$1 \times 10^{-4}$ã€‚æ¨¡å‹è®­ç»ƒæ—¶é•¿ä¸º60 epochsï¼ŒåŒæ—¶å¼•å…¥æ—©åœæœºåˆ¶ã€‚éšåï¼Œæˆ‘ä»¬æ ¹æ®ä¿¡æ¯ä¼ æ’­å¹³å‡æ—¶é•¿è®¾ç½®çª—å£é•¿åº¦$\lambda=64/128$ã€‚ä¹‹åï¼Œé‡‡ç”¨æ§åˆ¶å˜é‡æ³•éªŒè¯æŸå¤±å‡½æ•°å¤šæ¨¡æ€ä¿¡æ¯è¡°å‡å› æ­¤çš„å€¼ã€‚ç‰¹åˆ«çš„ï¼Œæœ¬å·¥ä½œçš„æ ¸å¿ƒæ˜¯æ£€æµ‹åƒåœ¾é‚®ä»¶å‘é€è€…ï¼Œå› æ­¤è®¾ç½®$\psi_1$ä¸º1ã€‚å¯ä»¥å‘ç°ï¼Œ$\psi_2=0.4\0.4$å’Œ$\psi_3=0.4\0.4$æ—¶æ•ˆæœæœ€ä½³ã€‚
\par \textbf{Hyper-parameter Settings:} We have written the model source code on the software platform of Tensorflow 2.9.0 and Python 3.8. Moreover, the dataset is divided into training, validation, and testing sets in the ratio of 7:2:1. Meanwhile, we used Adam as the optimizer of the model and set the learning rate to $1 \times 10^{-4}$. The model training time is 60 epochs, while the early stopping mechanism is introduced. Subsequently, we set the window size $W=64/128$ based on the average length of information dissemination (see Fig. \ref{fig-Parameters-windows} (a)). Afterward, the value of the multi-modal information decay factor of the loss function is verified using the control variable method (see Fig. \ref{fig-Parameters-decay}). In particular, the core of this work is to detect spammers, so $\psi_1$ is set to 1. It can be found that the best results are obtained when $\psi_2=0.3/0.3$ and $\psi_3=0.4/0.4$.
\begin{figure}[t]
	\center{\includegraphics[width=1.03\linewidth]  {./image/Paramters_2.pdf}} 
	\caption{The influence of different values of $\psi_2$ and $\psi_3$ ($\psi_1=1$).}
	\label{fig-Parameters-decay}
\end{figure}
% æˆ‘ä»¬ä»ä¸‰ä¸ªç»´åº¦é€‰æ‹©äº†å…ˆè¿›çš„åŸºçº¿ç®—æ³•ã€‚é¦–å…ˆï¼Œåœ¨ç®—æ³•æ–¹é¢æˆ‘ä»¬é€‰æ‹©äº†ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬GCNã€GATã€Graph-SAGEã€RNNã€LSTMã€GRUå’ŒMHAã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨GCNçš„åŸºç¡€ä¸Šé€‰æ‹©äº†Graph-U-Netsã€R-GCNå’ŒChebNetã€‚æœ€åï¼Œä»ä»»åŠ¡å±‚é¢å‡ºå‘ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸“ç”¨çš„åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹æ¨¡å‹ï¼Œå³MDGCNå’ŒAdver-GCNã€‚
\par \textbf{Baseline Settings:} We selected advanced baseline algorithms in three dimensions. Firstly, in terms of algorithms, we selected traditional deep learning algorithms, including GCN\cite{li2019spam}, GAT\cite{zhang2023detecting}, Graph-SAGE\cite{zhang2024predicting}, RNN\cite{zhang2023rumor}, LSTM\cite{babu2023efficient}, GRU\cite{GRU}, and MHA\cite{rao2023hybrid}. Secondly, we selected Graph-U-Nets\cite{gao2019graph}, R-GCN\cite{generale2022scaling}, and ChebNet\cite{he2022convolutional} on the basis of GCN. Finally, from the task dimension, we chose specialized spammer detection models, i.e., MDGCN\cite{deng2023markov}, Graph Transformer\cite{GraphTrans}, and Adver-GCN\cite{zhang2022detecting}.
\subsection{Validity Analysis of Historical Behavioral Length}
% åœ¨ç¤¾äº¤å¹³å°ä¸­ï¼Œç”¨æˆ·çš„å†å²è¡Œä¸ºæ•°é‡åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ã€‚å¦‚å›¾bæ‰€ç¤ºï¼Œå†å²è¡Œä¸ºæ•°é‡ä¸º32768æ˜¯è¯¥åˆ†å¸ƒçš„æ‹ç‚¹ï¼Œè¶…è¿‡70%çš„ç”¨æˆ·è¶…è¿‡è¿™ä¸ªé˜ˆå€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™ä¸ªèŒƒå›´å†…çš„ä¸åŒå†å²è¡Œä¸ºæ•°é‡å¯¹äºæ¨¡å‹æ€§èƒ½çš„å½±å“ï¼ˆå¦‚å›¾4æ‰€ç¤ºï¼‰ã€‚å…¶ä¸­ï¼ŒåŸºäºGNNçš„æ¨¡å‹ï¼Œå³GCNã€GATå’ŒGraph-SAGEåœ¨è¶…è¿‡2048è¿™ä¸ªè¡Œä¸ºæ•°é‡é˜ˆå€¼æ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚æ­¤æ—¶ï¼Œç”¨æˆ·å†å²è¡Œä¸ºæ„å»ºçš„ä¼ æ’­å›¾è¿‡å¤§ï¼Œä¼ ç»ŸGNNæ¨¡å‹ä¸èƒ½æœ‰æ•ˆæŒ–æ˜å…¨å±€å’Œå±€éƒ¨ç»“æ„ä¿¡æ¯ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäºåºåˆ—å»ºæ¨¡çš„æ¨¡å‹ï¼Œå³RNNã€LSTMå’ŒGRUï¼Œè¶…è¿‡128è¿™ä¸ªè¡Œä¸ºæ•°é‡é˜ˆå€¼æ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚è¿™æ˜¯å› ä¸ºè¿™äº›åºåˆ—æ¨¡å‹åœ¨é¢å¯¹è¶…é•¿åºåˆ—æ—¶å¹¶ä¸èƒ½æœ‰æ•ˆè¡¡é‡è¶…é•¿æœŸå’ŒçŸ­æœŸäº¤äº’å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬åŒæ ·è¿›è¡Œäº†ä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMHAï¼‰é¢å¯¹ä¸åŒå†å²è¡Œä¸ºåºåˆ—çš„æ€§èƒ½å˜åŒ–ã€‚å¯ä»¥å‘ç°ï¼ŒMHAå¯ä»¥æœ‰æ•ˆè¡¡é‡é•¿æœŸå’ŒçŸ­æœŸäº¤äº’å…³ç³»ã€‚ä½†æ˜¯ï¼Œå—åˆ¶äº$QK^{\text{T}}$çš„å­˜å‚¨å‹åŠ›ï¼Œé¢å¯¹è¶…é•¿åºåˆ—æ—¶MHAæ— æ³•è¿è¡Œã€‚ç‰¹åˆ«çš„ï¼Œå›¾4ä¸­$\circ$å’Œ$\triangle$åˆ†åˆ«è¡¨ç¤ºè®­ç»ƒå¹³å°ä¸ºPTX 3060 (16 GB)æˆ–A800 (80GB)ã€‚
\begin{table*}[htbp]
	\renewcommand{\arraystretch}{1.3}
	\caption{Comparison of the performance for different models on the Weibo 2023 dataset}
	\label{table_acc_v1}
	\centering	
	\begin{tabular}{cccccccc}
		\toprule[1.5pt]
		\multicolumn{1}{c|}{\multirow{2}*{Method}}&\multicolumn{1}{c|}{\multirow{2}*{Accuracy}}&\multicolumn{3}{c|}{Normal}&\multicolumn{3}{c}{Spammer}\\
		\cline{3-5} \cline{6-8}
		\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{}&\multicolumn{1}{c}{\multirow{1}*{Precision}}&\multicolumn{1}{c}{Recall}&\multicolumn{1}{c}{F1}&\multicolumn{1}{c}{\multirow{1}*{Precision}}&\multicolumn{1}{c}{Recall}&\multicolumn{1}{c}{F1}\\
		\hline \hline
		GAT & $0.816_{\pm 0.007}$ & $0.83_{\pm 0.003}$ & $0.794_{\pm 0.015}$ & $0.812_{\pm 0.009}$ & $0.803_{\pm 0.011}$ & $0.838_{\pm 0.001}$ & $0.820_{\pm 0.006}$\\
		Graph-SAGE & $0.828_{\pm 0.003}$ & $0.821_{\pm 0.003}$ & $0.838_{\pm 0.014}$ & $0.830_{\pm 0.005}$ & $0.835_{\pm 0.011}$ & $0.817_{\pm 0.008}$ & $0.826_{\pm 0.002}$\\
		GCN & $0.832_{\pm 0.007}$ & $0.798_{\pm 0.001}$ & $0.890_{\pm 0.022}$ & $0.842_{\pm 0.010}$ & $0.874_{\pm 0.020}$ & $0.773_{\pm 0.008}$ & $0.820_{\pm 0.005}$\\
		Graph-U-Nets & $0.758_{\pm 0.008}$ & $0.788_{\pm 0.056}$ & $0.717_{\pm 0.103}$ & $0.743_{\pm 0.030}$ & $0.749_{\pm 0.044}$ & $0.800_{\pm 0.110}$ & $0.767_{\pm 0.036}$\\ 
		R-GCN & $0.820_{\pm 0.004}$ & $0.778_{\pm 0.019}$ & $0.897_{\pm 0.029}$ & $0.832_{\pm 0.002}$ & $0.881_{\pm 0.026}$ & $0.742_{\pm 0.037}$ & $0.804_{\pm 0.011}$\\
		MDGCN & $0.842_{\pm 0.007}$ & $0.840_{\pm 0.034}$ & $0.849_{\pm 0.037}$ & $0.843_{\pm 0.001}$ & $0.848_{\pm 0.020}$ & $0.834_{\pm 0.051}$ & $0.84_{\pm 0.016}$\\  
		ChebNet & $0.834_{\pm 0.001}$ & $0.826_{\pm 0.005}$ & $0.849_{\pm 0.007}$ & $0.837_{\pm 0.001}$ & $0.844_{\pm 0.005}$ & $0.820_{\pm 0.008}$ & $0.832_{\pm 0.001}$\\
		Adver-GCN & $0.844_{\pm 0.007}$ & $0.832_{\pm 0.016}$ & $0.866_{\pm 0.007}$ & $0.848_{\pm 0.004}$ & $0.858_{\pm 0.002}$ & $0.822_{\pm 0.022}$ & $0.840_{\pm 0.010}$\\ 
		Graph Transformer & $0.864_{\pm 0.007}$ & $0.899_{\pm 0.008}$ & $0.825_{\pm 0.021}$ & $0.862_{\pm 0.008}$ & $0.832_{\pm 0.015}$ & $0.904_{\pm 0.007}$ & $0.867_{\pm 0.007}$\\\hline \hline
		
		RNN &  $0.832_{\pm 0.022}$ & $0.816_{\pm 0.005}$ & $0.852_{\pm 0.052}$ & $0.834_{\pm 0.027}$ & $0.850_{\pm 0.043}$ & $0.810_{\pm 0.008}$ & $0.829_{\pm 0.017}$\\
		GRU & $0.863_{\pm 0.011}$ & $0.865_{\pm 0.003}$ & $0.858_{\pm 0.023}$ & $0.861_{\pm 0.013}$ & $0.862_{\pm 0.019}$ & $0.868_{\pm 0.001}$ & $0.864_{\pm 0.010}$\\
		LSTM & $0.866_{\pm 0.008}$ & $0.884_{\pm 0.020}$ & $0.843_{\pm 0.008}$ & $0.862_{\pm 0.006}$ & $0.852_{\pm 0.003}$ & $0.890_{\pm 0.022}$ & $0.870_{\pm 0.009}$\\ 
		MHA & $0.872_{\pm 0.007}$ & $0.871_{\pm 0.012}$ & $0.871_{\pm 0.001}$ & $0.871_{\pm 0.006}$ & $0.874_{\pm 0.003}$ & $0.874_{\pm 0.015}$ & $0.874_{\pm 0.009}$\\ \hline \hline
		
		MS$^2$Dformer\_B & $0.923_{\pm 0.004}$ & $0.913_{\pm 0.001}$ & $0.932_{\pm 0.007}$ & $0.923_{\pm 0.004}$ & $0.932_{\pm 0.007}$ & $\underline{0.912}_{\pm 0.001}$ & $0.922_{\pm 0.003}$\\
		MS$^2$Dformer\_M & $\underline{0.931}_{\pm 0.004}$ & $\underline{0.914}_{\pm 0.011}$ & $\underline{0.948}_{\pm 0.022}$ & $\underline{0.931}_{\pm 0.005}$ & $\underline{0.947}_{\pm 0.021}$ & $0.912_{\pm 0.015}$ & $\underline{0.929}_{\pm 0.003}$\\
		MS$^2$Dformer\_L & $\textbf{0.941}_{\pm 0.004}$ & $\textbf{0.926}_{\pm 0.018}$ & $\textbf{0.959}_{\pm 0.015}$ & $\textbf{0.942}_{\pm 0.002}$ & $\textbf{0.958}_{\pm 0.014}$ & $\textbf{0.923}_{\pm 0.022}$ & $\textbf{0.940}_{\pm 0.005}$\\ \hline
		\bottomrule[1.5pt]
	\end{tabular}
\end{table*}
%\begin{table*}[htbp]
%	\renewcommand{\arraystretch}{1.3}
%	\centering	
%	\caption{Performance comparison of different models on two public datasets}
%	\begin{tabular}{ccccccc}
%		\toprule[1.5pt]
%		\multicolumn{1}{c|}{\multirow{3}*{Method}}&\multicolumn{3}{c|}{Weibo 2023}&\multicolumn{3}{c}{Weibo 2024}\\
%		\cline{2-4} \cline{5-7}
%		\multicolumn{1}{c|}{          }&\multicolumn{1}{c|}{\multirow{2}*{Accuracy}}&\multicolumn{2}{c|}{F1-Score}&\multicolumn{1}{c|}{\multirow{2}*{Accuracy}}&\multicolumn{2}{c}{F1-Score}\\
%		\cline{3-4} \cline{6-7}
%		\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{}&\multicolumn{1}{c}{\multirow{1}*{Normal}}&\multicolumn{1}{c|}{Spammer}&\multicolumn{1}{c|}{}&\multicolumn{1}{c}{\multirow{1}*{Normal}}&\multicolumn{1}{c}{Spammer}\\
%		\hline \hline
%		GAT & $0.816_{\pm 0.007}$ & $0.812_{\pm 0.009}$ & $0.820_{\pm 0.006}$ & $0.810_{\pm 0.003}$ & $0.820_{\pm 0.008}$ & $0.799_{\pm 0.006}$\\ 
%		Graph-SAGE & $0.828_{\pm 0.003}$ & $0.830_{\pm 0.005}$ & $0.826_{\pm 0.002}$& $0.842_{\pm 0.005}$ & $0.853_{\pm 0.009}$ & $0.828_{\pm 0.005}$\\ 
%		GCN & $0.832_{\pm 0.007}$ & $0.842_{\pm 0.010}$ & $0.820_{\pm 0.005}$& $0.806_{\pm 0.005}$ & $0.808_{\pm 0.010}$ & $0.802_{\pm 0.009}$\\ 
%		Graph-U-Nets & $0.758_{\pm 0.008}$ & $0.743_{\pm 0.030}$ & $0.767_{\pm 0.036}$& $0.744_{\pm 0.002}$ & $0.726_{\pm 0.011}$ & $0.761_{\pm 0.01}$\\ 
%		R-GCN & $0.820_{\pm 0.004}$ & $0.832_{\pm 0.002}$ & $0.804_{\pm 0.011}$& $0.842_{\pm 0.004}$ & $0.85_{\pm 0.006}$ & $0.832_{\pm 0.011}$\\ 
%		MDGCN & $0.842_{\pm 0.007}$ & $0.843_{\pm 0.001}$ & $0.84_{\pm 0.016}$& $0.841_{\pm 0.003}$ & $0.846_{\pm 0.001}$ & $0.835_{\pm 0.006}$\\ 
%		ChebNet & $0.834_{\pm 0.001}$ & $0.837_{\pm 0.001}$ & $0.832_{\pm 0.001}$& $0.812_{\pm 0.004}$ & $0.814_{\pm 0.010}$ & $0.808_{\pm 0.017}$\\ 
%		Adver-GCN & $0.844_{\pm 0.007}$ & $0.848_{\pm 0.004}$ & $0.840_{\pm 0.010}$& $0.851_{\pm 0.004}$ & $0.848_{\pm 0.002}$ & $0.854_{\pm 0.008}$\\ \hline \hline
%		
%		RNN &  $0.832_{\pm 0.022}$ & $0.834_{\pm 0.027}$ & $0.829_{\pm 0.017}$& $0.838_{\pm 0.004}$ & $0.837_{\pm 0.006}$ & $0.838_{\pm 0.013}$\\ 
%		GRU & $0.863_{\pm 0.011}$ & $0.861_{\pm 0.013}$ & $0.864_{\pm 0.010}$& $0.856_{\pm 0.008}$ & $0.853_{\pm 0.010}$ & $0.860_{\pm 0.005}$\\ 
%		LSTM & $0.866_{\pm 0.008}$ & $0.862_{\pm 0.006}$ & $0.870_{\pm 0.009}$& $0.875_{\pm 0.004}$ & $0.872_{\pm 0.008}$ & $0.878_{\pm 0.001}$\\ 
%		MHA & $0.872_{\pm 0.007}$ & $0.871_{\pm 0.006}$ & $0.874_{\pm 0.009}$& $0.883_{\pm 0.004}$ & $0.881_{\pm 0.003}$ & $0.884_{\pm 0.004}$\\ \hline \hline
%		
%		MS$^2$Dformer\_B & $0.923_{\pm 0.004}$ & $0.923_{\pm 0.004}$ & $0.922_{\pm 0.003}$& $0.917_{\pm 0.004}$ & $0.915_{\pm 0.006}$ & $0.919_{\pm 0.004}$\\
%		MS$^2$Dformer\_M & $0.931_{\pm 0.004}$ & $0.931_{\pm 0.005}$ & $0.929_{\pm 0.003}$& $0.926_{\pm 0.007}$ & $0.928_{\pm 0.007}$ & $0.924_{\pm 0.008}$\\
%		MS$^2$Dformer\_L & $0.941_{\pm 0.004}$ & $0.942_{\pm 0.002}$ & $0.940_{\pm 0.005}$& $0.935_{\pm 0.004}$ & $0.933_{\pm 0.004}$ & $0.935_{\pm 0.003}$\\ \hline
%		\bottomrule[1.5pt]
%	\end{tabular}
%	\label{table-acc}
%\end{table*}
\par In social platforms, the distribution of the number of historical behaviors of users varies widely. As shown in Fig. \ref{fig-Parameters-windows} (b), the number of historical behaviors of 16,384/32,768 is the inflection point of this distribution, with more than 60/70\% of users exceeding this threshold. Therefore, this range of historical behaviors is verified for the model performance (shown in Fig. \ref{fig-behavior}). In particular, the performance of the GNN-based models, i.e., GCN, GAT, and Graph-SAGE, decreases dramatically when the threshold of 2048, the number of behaviors, is exceeded. At this point, the spread graph constructed by the user's historical behaviors overlaps, and the traditional GNN models cannot effectively mine the global and local structural information. On the other hand, the performance of models based on sequence modeling, i.e., RNN, LSTM, and GRU, decreases dramatically when exceeding the threshold of 128 as the number of behaviors. This is because such sequence models do not effectively measure ultra-long-term and short-term interactions when confronted with ultra-long sequences. Finally, a similar exercise was conducted to examine the performance of the traditional Multi-head Attention Mechanism (MHA) when faced with different lengths of historical behavioral sequences. It can be found that MHA can effectively measure long- and short-term interactions. However, due to the memory pressure of $QK^{\text{T}}$, MHA cannot run when facing ultra-long sequences. Specifically, $\circ$ and $\triangle$ in Fig. \ref{fig-behavior} (a) and (c) represent that the training platform is RTX 4060 (16 GB) or A800 (80 GB).
\begin{table}[htbp]
	\renewcommand{\arraystretch}{1.3}
	\centering	
	\caption{Performance comparison of different multi-modal fusion strategies}
	\resizebox{0.48\textwidth}{!}{
		\begin{tabular}{ccccccc}
			\toprule[1.5pt]
			\multicolumn{1}{c|}{\multirow{2}*{Method}}&\multicolumn{3}{c|}{\multirow{1}*{Weibo 2023}}&\multicolumn{3}{c}{Weibo 2024}\\
			\cline{2-4} \cline{5-7}
			\multicolumn{1}{c|}{}&\multicolumn{1}{c}{Base}&\multicolumn{1}{c}{Middle}&\multicolumn{1}{c|}{Large}&\multicolumn{1}{c}{Base}&\multicolumn{1}{c}{Middle}&\multicolumn{1}{c}{Large}\\
			\hline \hline
			Our (MVAE \& (Bert+ViT)) & $\textbf{0.923}$ & $\textbf{0.931}$ & $\textbf{0.941}$ & $\underline{0.917}$ & $\textbf{0.926}$ & $\textbf{0.935}$ \\ \hline \hline
			-w Text \& w/o Image & $0.859$ & $0.863$ & $0.861$ & $0.863$ & $0.863$ & $0.881$ \\
			-w Image \& w/o Text & $0.904$ & $0.889$ & $0.897$ & $0.875$ & $0.884$ & $0.862$ \\
			-w Add Image \& Text & $0.889$ & $0.904$ & $0.924$ & $0.889$ & $0.897$ & $0.901$ \\ \hline \hline
			% Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese
			-w CLIP \& w/o (Bert+ViT) & $0.909$ & $0.914$ & $0.933$ & $0.913$ & $0.914$ & $0.928$ \\
			% Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision
			-w ALIGN \& w/o (Bert+ViT) & $0.915$ & $0.923$ & $\underline{0.937}$ & $\textbf{0.917}$ & $0.921$ & $0.928$ \\
			% Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models
			-w BLIP-2 \& w/o (Bert+ViT)  & $\underline{0.917}$ & $\underline{0.928}$ & $0.935$ & $0.916$ & $\underline{0.924}$ & $\underline{0.929}$ \\ 
			\hline
			\bottomrule[1.5pt]
	\end{tabular}}
	\label{table-ab-multi-model}
\end{table}
\begin{table*}[htbp]
	\renewcommand{\arraystretch}{1.3}
	\caption{Comparison of the performance for different models on the Weibo 2024 dataset}
	\label{table_acc_v2}
	\centering	
	\begin{tabular}{cccccccc}
		\toprule[1.5pt]
		\multicolumn{1}{c|}{\multirow{2}*{Method}}&\multicolumn{1}{c|}{\multirow{2}*{Accuracy}}&\multicolumn{3}{c|}{Normal}&\multicolumn{3}{c}{Spammer}\\
		\cline{3-5} \cline{6-8}
		\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{}&\multicolumn{1}{c}{\multirow{1}*{Precision}}&\multicolumn{1}{c}{Recall}&\multicolumn{1}{c}{F1}&\multicolumn{1}{c}{\multirow{1}*{Precision}}&\multicolumn{1}{c}{Recall}&\multicolumn{1}{c}{F1}\\
		\hline \hline
		GAT & $0.810_{\pm 0.003}$ & $0.803_{\pm 0.018}$ & $0.840_{\pm 0.034}$ & $0.820_{\pm 0.008}$ & $0.821_{\pm 0.024}$ & $0.779_{\pm 0.034}$ & $0.799_{\pm 0.006}$\\
		Graph-SAGE & $0.842_{\pm 0.005}$ & $0.817_{\pm 0.014}$ & $0.894_{\pm 0.037}$ & $0.853_{\pm 0.009}$ & $0.876_{\pm 0.035}$ & $0.786_{\pm 0.029}$ & $0.828_{\pm 0.005}$\\ 
		GCN & $0.806_{\pm 0.005}$ & $0.823_{\pm 0.029}$ & $0.796_{\pm 0.042}$ & $0.808_{\pm 0.010}$ & $0.790_{\pm 0.025}$ & $0.816_{\pm 0.044}$ & $0.802_{\pm 0.009}$\\
		Graph-U-Nets & $0.744_{\pm 0.002}$ & $0.804_{\pm 0.023}$ & $0.664_{\pm 0.035}$ & $0.726_{\pm 0.011}$ & $0.702_{\pm 0.011}$ & $0.831_{\pm 0.037}$ & $0.761_{\pm 0.01}$\\
		R-GCN & $0.842_{\pm 0.004}$ & $0.832_{\pm 0.031}$ & $0.871_{\pm 0.047}$ & $0.85_{\pm 0.006}$ & $0.856_{\pm 0.038}$ & $0.811_{\pm 0.052}$ & $0.832_{\pm 0.011}$\\
		MDGCN & $0.841_{\pm 0.003}$ & $0.85_{\pm 0.012}$ & $0.842_{\pm 0.012}$ & $0.846_{\pm 0.001}$ & $0.831_{\pm 0.007}$ & $0.838_{\pm 0.018}$ & $0.835_{\pm 0.006}$\\ 
		ChebNet & $0.812_{\pm 0.004}$ & $0.838_{\pm 0.048}$ & $0.798_{\pm 0.062}$ & $0.814_{\pm 0.010}$ & $0.798_{\pm 0.034}$ & $0.828_{\pm 0.074}$ & $0.808_{\pm 0.017}$\\
		Adver-GCN & $0.851_{\pm 0.004}$ & $0.864_{\pm 0.021}$ & $0.832_{\pm 0.022}$ & $0.848_{\pm 0.002}$ & $0.840_{\pm 0.012}$ & $0.870_{\pm 0.029}$ & $0.854_{\pm 0.008}$\\ 
		Graph Transformer & $0.865_{\pm 0.008}$ & $0.898_{\pm 0.006}$ & $0.827_{\pm 0.023}$ & $0.861_{\pm 0.009}$ & $0.838_{\pm 0.017}$ & $0.902_{\pm 0.008}$ & $0.869_{\pm 0.006}$\\ \hline \hline
		RNN & $0.838_{\pm 0.004}$ & $0.839_{\pm 0.04}$ & $0.840_{\pm 0.052}$ & $0.837_{\pm 0.006}$ & $0.846_{\pm 0.033}$ & $0.836_{\pm 0.058}$ & $0.838_{\pm 0.013}$\\ 
		GRU & $0.856_{\pm 0.008}$ & $0.866_{\pm 0.003}$ & $0.840_{\pm 0.022}$ & $0.853_{\pm 0.010}$ & $0.848_{\pm 0.017}$ & $0.872_{\pm 0.008}$ & $0.860_{\pm 0.005}$\\
		LSTM & $0.875_{\pm 0.004}$ & $0.883_{\pm 0.016}$ & $0.863_{\pm 0.03}$ & $0.872_{\pm 0.008}$ & $0.869_{\pm 0.022}$ & $0.887_{\pm 0.022}$ & $0.878_{\pm 0.001}$\\ 
		MHA & $0.883_{\pm 0.004}$ & $0.885_{\pm 0.007}$ & $0.878_{\pm 0.003}$ & $0.881_{\pm 0.003}$ & $0.881_{\pm 0.001}$ & $0.887_{\pm 0.007}$ & $0.884_{\pm 0.004}$\\ \hline \hline
		
		MS$^2$Dformer\_B & $0.917_{\pm 0.004}$ & $\underline{0.928}_{\pm 0.013}$ & $0.903_{\pm 0.022}$ & $0.915_{\pm 0.006}$ & $0.907_{\pm 0.018}$ & $\underline{0.930}_{\pm 0.014}$ & $0.919_{\pm 0.004}$\\
		MS$^2$Dformer\_M & $\underline{0.926}_{\pm 0.007}$ & $0.899_{\pm 0.012}$ & $\textbf{0.959}_{\pm 0.015}$ & $\underline{0.928}_{\pm 0.007}$ & $\underline{0.917}_{\pm 0.015}$ & $0.893_{\pm 0.015}$ & $\underline{0.924}_{\pm 0.008}$\\
		MS$^2$Dformer\_L & $\textbf{0.935}_{\pm 0.004}$ & $\textbf{0.937}_{\pm 0.001}$ & $\underline{0.930}_{\pm 0.008}$ & $\textbf{0.933}_{\pm 0.004}$ & $\textbf{0.932}_{\pm 0.007}$ & $\textbf{0.938}_{\pm 0.004}$ & $\textbf{0.935}_{\pm 0.003}$\\ \hline
		\bottomrule[1.5pt]
	\end{tabular}
\end{table*}
\begin{figure}[t]
	\center{\includegraphics[width=1\linewidth]  {./image/Paramters.pdf}} 
	\caption{Comparison of model training based on optimal parameter settings.}
	\label{fig-models}
\end{figure}
\begin{table*}[t]
	\renewcommand{\arraystretch}{1.3}
	\centering	
	\caption{GPU  platform operation with different MHA mechanisms (behavior size =16384). SPU represents the number of \textbf{s}econds that the model consumes to \textbf{p}rocess a individual \textbf{u}ser.}
		\begin{tabular}{ccccccccc}
			\toprule[1.5pt]
			\multicolumn{1}{c|}{\multirow{2}*{Method}}&\multicolumn{1}{c|}{\multirow{2}*{GPU Platform }}&\multicolumn{1}{c|}{\multirow{2}*{Run}}&\multicolumn{2}{c|}{\multirow{1}*{Base}}&\multicolumn{2}{c|}{\multirow{1}*{Middle}}&\multicolumn{2}{c}{\multirow{1}*{Large}}\\
			\cline{4-9}
			\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{}&\multicolumn{1}{c}{Params}&\multicolumn{1}{c|}{SPU}&\multicolumn{1}{c}{Params}&\multicolumn{1}{c|}{SPU}&\multicolumn{1}{c}{Params}&\multicolumn{1}{c}{SPU}\\
			\hline \hline
			Our (SW-MHA \& W-MHA) & RTX 4060 (16 GB) & $\surd$ & $2.21$ M & $0.29$ s & $3.66$ M & $0.32$ s & $53.8$ M & $0.35$ s \\ \hline \hline
			-w CNN \& w/o SW-MHA & RTX 4060 (16 GB) & $\surd$ & $1.97$ M & $0.20$ s & $2.53$ M & $0.23$ s & $20.9$ M & $0.27$ s \\ \hline \hline
			-w MHA \& w MVAE & A800 (80 GB) & $\times$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\ \hline \hline
			-w SW-SMHA \& w MVAE & V100 (32 GB) & $\surd$ & $1.73$ M & $77.2$ s & $1.82$ M & $172.6$ s & $6.41$ M & $181.3$ s \\
			-w BRSW-SMHA \& w MVAE & V100 (32 GB) & $\surd$ & $1.73$ M & $0.87$ s & $1.82$ M & $1.61$ s & $6.41$ M & $3.20$ s \\
			-w BSW-SMHA \& w MVAE & V100 (32 GB) & $\surd$ & $1.73$ M & $1.05$ s & $1.82$ M & $1.22$ s & $6.41$ M & $3.78$ s \\
			-w BDW-SMHA \& w MVAE & V100 (32 GB) & $\surd$ & $1.73$ M & $1.55$ s & $1.82$ M & $1.56$ s & $6.41$ M & $3.20$ s \\
			-w BGSW-SMHA \& w MVAE & V100 (32 GB) & $\surd$ & $1.73$ M & $0.89$ s & $1.82$ M & $1.74$ s & $6.41$ M & $3.82$ s \\ \hline
			\bottomrule[1.5pt]
	\end{tabular}
	\label{table-ab-memory}
\end{table*}
% åŸºäºå›¾4éªŒè¯çš„æœ€ä½³å†å²è¡Œä¸ºé•¿åº¦è®­ç»ƒMS$^2$Dformerå’ŒåŸºçº¿æ¨¡å‹ï¼Œè®­ç»ƒè¿‡ç¨‹å¦‚å›¾6æ‰€ç¤ºï¼Œè®­ç»ƒç»“æœå¦‚è¡¨2æ‰€ç¤ºã€‚å¯¹æ¯”GNNå˜ä½“ï¼ŒMS$^2$Dformer\_Læ¨¡å‹æ€§èƒ½æå‡$+0.097$å’Œ$+0.084$ï¼ŒMHAæ¨¡å‹æ€§èƒ½æå‡$+0.028$å’Œ$+0.032$ã€‚å¯ä»¥å‘ç°ï¼Œåºåˆ—å»ºæ¨¡ç­–ç•¥æ›´é€‚åˆå½“å‰ä»»åŠ¡ã€‚éšåï¼Œå¯¹æ¯”åºåˆ—æ¨¡å‹å˜ä½“ï¼ŒMS$^2$Dformer\_Læ¨¡å‹æ€§èƒ½æå‡$+0.069$å’Œ$+0.052$ã€‚MS$^2$Dformeræ¶æ„èƒ½å¤Ÿæœ‰æ•ˆæŒ–æ˜åºåˆ—ç‰¹å¾ï¼Œä»è€ŒéªŒè¯äº†å®ƒçš„æœ‰æ•ˆæ€§ã€‚
\subsection{Overall Performance Analysis}
\par The MS$^2$Dformer and baseline models are trained based on the optimal behavior length, the training process is shown in Fig. \ref{fig-models}, and the training results are shown in Table \ref{table_acc_v1}-\ref{table_acc_v2}. 
\par Comparing the GNN variants, the MS$^2$Dformer\_L model performance improves $+0.077$ and $+0.07$, and the MHA model performance improves $+0.008$ and $+0.018$. It can be found that the sequence modeling strategy is more suitable for the task at hand. Subsequently, comparing the sequence modeling variants, the MS$^2$Dformer\_L model performance is improved by $+0.069$ and $+0.052$. The MS$^2$Dformer architecture is able to efficiently mine the sequence features, thus validating its effectiveness.
\subsection{Ablation Study}
% ç›¸è¾ƒäºå•æ¨¡æ€å»ºæ¨¡ç­–ç•¥ï¼Œå³-w Text \& w/o Imageå’Œ-w Image \& w/o Textï¼Œèåˆç‰¹å¾ç­–ç•¥ï¼ˆMVAE \& (Bert+ViT)ï¼‰æ€§èƒ½æå‡$+3.7$\%åˆ°$+8.2$\%ã€‚å› æ­¤ï¼Œå¯ä»¥è¯æ˜å¤šæ¨¡æ€èåˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚éšåï¼Œå¯¹æ¯”ç›´æ¥èåˆç­–ç•¥ï¼ˆ-w Add Image \& Textï¼‰ï¼Œé‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„èåˆç­–ç•¥ï¼ˆ-w CLIP/ALIGN/BLIP-2 w/o (Bert+ViT)ï¼‰æ€§èƒ½ä¹Ÿå­˜åœ¨å¤§å¹…åº¦æå‡ã€‚ä¾‹å¦‚ï¼ŒCLIPæ¨¡å‹åŸºäºå¯¹æŠ—å­¦ä¹ ç­–ç•¥æ„å»ºæ¨¡å‹ã€‚éšåï¼Œåœ¨å¹¿æ³›çš„æ•°æ®é›†å­¦ä¹ åˆ°æ›´åŠ ç¹åçš„çŸ¥è¯†ã€‚å› æ­¤ï¼Œé‡‡ç”¨CLIP/ALIGN/BLIP-2ä¸‰ä¸ªé¢„è®­ç»ƒæ¨¡å‹çš„æ•ˆæœæ›´å¥½ã€‚ä½†æ˜¯ï¼Œä»–ä»¬æ— æ³•æ›´åŠ ä¸“æ³¨ä¸å½“å‰ä»»åŠ¡ã€‚å› æ­¤ï¼Œé‡‡ç”¨MVAE \& (Bert+ViT)çš„ç»„åˆç­–ç•¥ï¼Œå¹¶ä¸”è¿›è¡Œå…¨é‡è®­ç»ƒè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚
\par \textbf{Multi-Modal Fusion:} As shown in Table \ref{table-ab-multi-model}, compared to the uni-modal modeling strategies, i.e., -w Text \& w/o Image and -w Image \& w/o Text, the fusion feature strategy (MVAE \& (Bert+ViT)) improves the performance by $+3.7$\% to $+8.2$\%. Thus, the validity of the multi-modal fusion strategy can be demonstrated. Subsequently, comparing the direct fusion strategy (-w Add Image \& Text), there also exists a substantial performance improvement in the fusion strategy (-w CLIP\cite{yang2022chinese}/ALIGN\cite{jia2021scaling}/BLIP-2\cite{li2023blip} \& w/o (Bert+ViT)) using pre-trained models. For instance, the CLIP model constructs models based on an adversarial learning strategy. Subsequently, more generalized knowledge is learned over a wide range of datasets. Therefore, the three pre-trained CLIP/ALIGN/BLIP-2 models are used with better results. However, they could not focus more on the current task. Therefore, a combination strategy of MVAE \& (Bert+ViT) and full training was used to achieve the best performance.
% æœ¬å·¥ä½œçš„æå‡ºäº†ä¸€ç§å…¨æ–°çš„åŸºäºå±‚æ¬¡åˆ†å‰²çª—å£çš„MHAæœºåˆ¶ã€‚éšåï¼Œä¿®æ”¹åçš„MHAæœºåˆ¶å¯ä»¥åœ¨è¾ƒä½æ˜¾å­˜çš„GPUå¹³å°ä¸Šè¿è¡Œï¼ˆçœ‹è¡¨1ä¸Šï¼‰ã€‚å½“å‰ï¼Œå­¦æœ¯ç•Œæå‡ºäº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶è§£å†³è¶…é•¿åºåˆ—çš„æ³¨æ„åŠ›è®¡ç®—é—®é¢˜ã€‚ç¨€ç–æ³¨æ„åŠ›åœ¨ç¼“è§£GPUæ˜¾å­˜å‹åŠ›æ–¹é¢åšå‡ºäº†å“è¶Šè´¡çŒ®ï¼ˆçœ‹è¡¨1ä¸‹ï¼‰ã€‚ä½†æ˜¯ï¼Œç¨€ç–æ³¨æ„åŠ›æœ¬è´¨ä¸Šæ— æ³•æœ‰æ•ˆæŒ–æ˜è¶…é•¿åºåˆ—çš„é•¿æœŸäº¤äº’å…³ç³»ï¼Œå› æ­¤å®ƒåœ¨å½“å‰ä»»åŠ¡è¡¨ç°ä¸ä½³ã€‚ç‰¹åˆ«çš„ï¼Œä¸ºäº†ç¯èŠ‚è¶…é•¿åºåˆ—å»ºæ¨¡é—®é¢˜ï¼ŒCNNè¢«å¼•å…¥åœ¨Transformer å—ä¹‹å‰ï¼Œä»è€Œå°†è¾“å…¥åºåˆ—é™ä½åˆ°GPUå¹³å°æ”¯æŒçš„é•¿åº¦ã€‚æœ¬å·¥ä½œæå‡ºçš„SW-MHAæœºåˆ¶ä¸CNNæœ‰ç±»ä¼¼ä½œç”¨ï¼Œä½†æ˜¯CNNæ— æ³•æœ‰æ•ˆé‡åŒ–çŸ­æœŸäº¤äº’å…³ç³»ï¼Œå› æ­¤æ€§èƒ½è¾ƒä½ã€‚æœ€åï¼Œå¦‚è¡¨1æ‰€ç¤ºï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨æ˜¾å­˜æœ€å°çš„PTX 4060 ï¼ˆ16 GBï¼‰å¹³å°å®Œæˆäº†è¶…è¿‡53ç™¾ä¸‡å‚æ•°çš„æ¨¡å‹è®­ç»ƒã€‚å¹¶ä¸”ï¼Œæ¨¡å‹å¹³å‡å¤„ç†ç”¨æˆ·æ—¶é—´ï¼ˆFPSï¼‰è¾ƒä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹çš„æœ‰æ•ˆæ€§è¢«å†ä¸€æ¬¡éªŒè¯ã€‚
\begin{table}[t]
	\renewcommand{\arraystretch}{1.3}
	\centering	
	\caption{Performance comparison of different MHA mechanisms}
	\resizebox{0.48\textwidth}{!}{
		\begin{tabular}{ccccccc}
			\toprule[1.5pt]
			\multicolumn{1}{c|}{\multirow{2}*{Method}}&\multicolumn{3}{c|}{\multirow{1}*{Weibo 2023}}&\multicolumn{3}{c}{Weibo 2024}\\
			\cline{2-4} \cline{5-7}
			\multicolumn{1}{c|}{}&\multicolumn{1}{c}{Base}&\multicolumn{1}{c}{Middle}&\multicolumn{1}{c|}{Large}&\multicolumn{1}{c}{Base}&\multicolumn{1}{c}{Middle}&\multicolumn{1}{c}{Large}\\
			\hline \hline
			Our (SW-MHA \& W-MHA) & $\textbf{0.923}$ & $\textbf{0.931}$ & $\textbf{0.941}$ & $\textbf{0.917}$ & $\textbf{0.926}$ & $\textbf{0.935}$ \\ \hline \hline
			-w CNN \& w/o SW-MHA &  $0.911$ & $0.912$ & $0.919$ & $0.912$ & $0.910$ & $0.914$ \\ \hline \hline
			-w SW-SMHA \& w MVAE & $\underline{0.919}$ & $0.924$ & $0.927$ & $0.915$ & $\underline{0.921}$ & $\underline{0.928}$ \\
			-w BRSW-SMHA \& w MVAE & $0.907$ & $0.912$ & $0.913$ & $0.907$ & $0.913$ & $0.911$ \\
			-w BSW-SMHA \& w MVAE & $0.913$ & $0.918$ & $0.922$ & $0.913$ & $0.918$ & $0.918$ \\
			-w BDW-SMHA \& w MVAE & $0.912$ & $0.921$ & $0.923$ & $0.901$ & $0.919$ & $0.908$ \\
			-w BGSW-SMHA \& w MVAE & $0.918$ & $0.926$ & $\underline{0.929}$ & $\underline{0.916}$ & $0.917$ & $0.927$ \\ \hline
			\bottomrule[1.5pt]
	\end{tabular}}
	\label{table-ab-MHA}
\end{table}
\par \textbf{Sequence Modeling Variants:} This work proposes a new MHA mechanism based on hierarchical split windows. Subsequently, the modified MHA mechanism can run on GPU platforms with lower memory (see Table \ref{table-ab-memory} up). Currently, sparse attention mechanisms are proposed in academia to solve the problem of attention computation for ultra-long sequences. Sparse attention has made excellent contributions in relieving GPU memory pressure (see Table \ref{table-ab-memory} bottom). However, sparse attention inherently fails to effectively mine the long-term interactions of ultra-long sequences. Thus, it performs poorly in the current task (see Table \ref{table-ab-MHA}). In particular, to address the problem of modeling ultra-long sequences, CNN is introduced before the Transformer block, thus reducing the input sequence to a length supported by the GPU platform. The SW-MHA mechanism proposed in this work works similarly to the CNN. However, the CNN cannot quantify short-term interactions efficiently, thus performing less (see Table \ref{table-ab-MHA}). Finally, as shown in Table \ref{table-ab-memory}, our proposed model completes model training with more than 53 million parameters on the RTX 4060 (16 GB) platform with the smallest memory. Moreover, the model has a low SPU. Thus, the validity of our proposed model is validated once again.
\begin{table}[htbp]
	\renewcommand{\arraystretch}{1.3}
	\centering	
	\caption{Performance comparison of MS$^2$Dformer\_B models based on different position encoding}
	\resizebox{0.3\textwidth}{!}{
	\begin{tabular}{ccc}
		\toprule[1.5pt]
		Method & Weibo 2023 & Weibo 2024\\
		\hline \hline
		Our (APE \& APE) & $\textbf{0.923}$ & $\textbf{0.907}$\\ \hline \hline
		-w/o PE \& w/o PE & $0.896$ & $0.891$ \\
		-w/o PE \& w APE & $0.901$ & $0.897$ \\
		-w/o PE \& w TPE & $0.915$ & $0.899$ \\ \hline \hline
		-w APE \& w/o PE & $0.911$ & $0.901$ \\
		-w APE \& w TPE & $\underline{0.919}$ & $\underline{0.905}$ \\ \hline \hline
		-w TPE \& w/o PE & $0.896$ & $0.887$ \\
		-w TPE \& w APE & $0.909$ & $0.902$ \\
		-w TPE \& w TPE & $0.898$ & $0.903$ \\ \hline
		\bottomrule[1.5pt]
	\end{tabular}}
	\label{table-ab-PE}
\end{table}
% åŸºäºåºåˆ—å»ºæ¨¡çš„åƒåœ¾é‚®ä»¶å‘é€è€…æ£€æµ‹æ¨¡å‹é‡ç‚¹å…³æ³¨å†å²è¡Œä¸ºåºåˆ—åœ¨æ—¶åºç‰¹å¾ä¸Šé•¿æœŸå’ŒçŸ­æœŸäº¤äº’ç‰¹å¾ã€‚åœ¨MHAæœºåˆ¶ä¸­ï¼Œä½ç½®ç¼–ç çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹èƒ½å¤ŸçŸ¥é“åºåˆ—çš„ä½ç½®å…³ç³»ã€‚å› æ­¤ï¼Œæ¶ˆé™¤SW-MHAå’ŒW-MHAçš„ä½ç½®ç¼–ç ï¼ˆPEï¼‰æ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿæ€¥å‰§ä¸‹é™ï¼ˆçœ‹è¡¨1ï¼‰ã€‚ç»“åˆç»å¯¹ä½ç½®ç¼–ç ï¼ˆAPEï¼‰æ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ„ŸçŸ¥æ—¶åºç‰¹å¾ã€‚é‡‡ç”¨å¯è®­ç»ƒçš„ä½ç½®ç¼–ç æ—¶ï¼ˆTPEï¼‰æ¨¡å‹å¯èƒ½ä¼šè·å–æ›´æœ‰æ•ˆçš„ä½ç½®ä¿¡æ¯ã€‚ä½†æ˜¯ï¼ŒTPEä¹Ÿå¯èƒ½ç ´åæ—¶é—´ç»´åº¦çš„é•¿æœŸå’ŒçŸ­æœŸå…³ç³»ã€‚å› æ­¤ï¼Œé‡‡ç”¨TPEæœºåˆ¶çš„æ¨¡å‹æ€§èƒ½æ˜¯ä¸ç¨³å®šçš„ã€‚ç»¼ä¸Šï¼ŒSW-MHAå’ŒW-MHAå‡é‡‡ç”¨APEæ—¶æ¨¡å‹æ•ˆæœæœ€ä½³ã€‚
\par \textbf{Position Encoding:} The spammer detection model based on sequence modeling focuses on the long-term and short-term interaction features of historical behavioral sequences in terms of temporal features. In the MHA mechanism, position encoding aims to enable the model to know the positional relationship of the sequences. Therefore, the model can decline dramatically when removing the position encoding (PE) for SW-MHA and W-MHA (see Table \ref{table-ab-PE}). When combined with absolute position encoding (APE), the model can perceive temporal features effectively. The model may acquire more effective position information when using trainable position encoding (TPE). However, TPE may also disrupt the time dimension's long- and short-term relationships. Therefore, the performance of models using the TPE mechanism is unstable. In summary, the model works best when APE is used for the SW-MHA and W-MHA.
% åœ¨ç¤¾äº¤å¹³å°ä¸­ï¼Œç”¨æˆ·é€šå¸¸å‘å¸ƒå¸¦æœ‰å™ªå£°çš„ä¿¡æ¯å½±å“æ¨¡å‹æå–æœ‰æ„ä¹‰çš„ç‰¹å¾ï¼ˆçœ‹å›¾1ï¼‰ã€‚é‡‡ç”¨MVAEï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆç¯èŠ‚å™ªå£°å½±å“ï¼ˆçœ‹è¡¨2ï¼‰ã€‚åŒæ—¶ï¼Œç”¨æˆ·å­˜åœ¨çŸ­æœŸè‡ªèº«è¡Œä¸ºäº¤äº’å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†å‰²çª—å£çš„ç­–ç•¥æŒ–æ˜çŸ­æœŸäº¤äº’å…³ç³»ï¼ˆçœ‹å›¾2ï¼‰ã€‚ç„¶è€Œï¼Œç”±äºåœ¨çº¿ç¤¾äº¤æ–¹å¼å·²ç»ç»è¿‡å¤šå¹´å‘å±•ï¼Œå› æ­¤åƒåœ¾é‚®ä»¶å‘é€è€…ç»å¸¸å€ŸåŠ©å·²ç»å‘å¸ƒè¿‡çš„æˆ–è¿‡æ—¶çš„ä¿¡æ¯å†æ¬¡å¼•å¯¼èˆ†è®ºï¼ˆçœ‹å›¾3ï¼‰ã€‚è¿™ç§è¶…é•¿æœŸè¡Œä¸ºçš„äº¤äº’å…³ç³»å¯¹äºè¯†åˆ«æ½œä¼çš„åƒåœ¾é‚®ä»¶å‘é€è€…ååˆ†é‡è¦ã€‚ä¼ ç»ŸMHAå—åˆ°æ˜¾å­˜å½±å“ï¼Œå› æ­¤æ— æ³•å¤„ç†è¶…é•¿æœŸå†å²äº¤äº’å…³ç³»ï¼ˆå¦‚è¡¨3ä¸Šï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ç§é—®é¢˜ï¼Œå­¦è€…æå‡ºSMHAå¯¹è¶…é•¿åºåˆ—å»ºæ¨¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬å‡æ— æ³•é¿å…æ„å»º$QK^\text{T}$çŸ©é˜µï¼ˆçœ‹è¡¨3ä¸‹ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨MHAå’ŒSMHAçš„åŸºç¡€ä¸Šæå‡ºäº†åŸºäºåˆ†å‰²çª—å£çš„å±‚æ¬¡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒåœ¨æ€§èƒ½ã€ç®—åŠ›æ¶ˆè€—ã€SPUã€å‚æ•°é‡ç­‰æŒ‡æ ‡ä¸Šå‡å­˜åœ¨å·¨å¤§æå‡ã€‚
\begin{figure}[t]
	\center{\includegraphics[width=1\linewidth]  {./image/Case.pdf}} 
	\caption{The case of ultra-long history interactions with similar user behavior. (a) and (b) are selected from different historical behaviors of a individual user.}
	\label{fig-ultra-long-term}
\end{figure}
\subsection{Case Study}
\par In social platforms, users usually post information with noise affecting the model to extract meaningful features (see Fig. \ref{fig-inspire}). Using MVAE, our proposed model can effectively reduce the noise influence (see Table \ref{table-ab-multi-model}). Meanwhile, users have short-term self-behavioral interactions. Therefore, we adopt the strategy of split window to mine the short-term interaction relations (see Fig. \ref{fig-Parameters-windows}). However, because online social has been developed for many years, spammers often guide public opinion again with the help of already published or outdated information (see Fig. \ref{fig-ultra-long-term}). This interaction of ultra-long-term behavior is important for identifying latent spammers. Traditional MHA is affected by explicit memory, so it cannot handle ultra-long-term historical interactions (see Table \ref{table-ab-memory} up). To address this problem, scholars have proposed SMHA to model ultra-long sequences. However, none of them can avoid constructing the $QK^\text{T}$ matrix (see Fig. \ref{fig-MHAs}). Finally, we propose a hierarchical attention mechanism based on a split window according to MHA and SMHA, which exists a considerable performance improvement (see Table \ref{table_acc_v1}-\ref{table_acc_v2}), arithmetic consumption, SPU, and parameter size (see Table \ref{table-ab-memory}).
\section{Conclusions}
\par In this paper, we propose a model for multi-modal sequential spammer detection. To relieve the effect of multi-modal noise, a two-channel VAE mechanism is first constructed to complete the history behavior Tokenization. Subsequently, to model ultra-long historical behavior sequences, a hierarchical multi-head attention mechanism based on the split window is proposed for the first time. The MS$^2$Dformer architecture, with the largest parameters, lower processing time, and huge performance improvement, is trained in a low-computing power platform (RTX 4060 (16 GB)). In the future, the performance of the hierarchical multi-head attention mechanism will be tested for other ultra-long sequence representation tasks, such as recommendation systems.
\begin{thebibliography}{38}
\bibitem{li2019spam}
A. Li, Z. Qin, R. Liu, Y. Yang, and D. Li, â€œSpam review detection with graph convolutional networks,â€ in Proceedings of the 28th ACM international conference on information and knowledge management, 2019, pp. 2703â€“2711.

\bibitem{zhang2023detecting}
F. Zhang, J. Wu, P. Zhang, R. Ma, and H. Yu, â€œDetecting collusive spammers with heterogeneous graph attention network,â€ Information Processing \& Management, vol. 60, no. 3, p. 103282, 2023.

\bibitem{jiang2024learning}
B. Jiang, Z. Zhang, S. Ge, B. Wang, X. Wang, and J. Tang, â€œLearning graph attentions via replicator dynamics,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 7720â€“7727, 2024.

\bibitem{chen2024gnn}
C. Chen, Y. Wu, Q. Dai, H.-Y. Zhou, M. Xu, S. Yang, X. Han, and Y. Yu, â€œA survey on graph neural networks and graph transformers in computer vision: A task-oriented perspective,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 10297--10318, 2024.

\bibitem{zhang2024predicting}
X. Zhang and W. Gao, â€œPredicting viral rumors and vulnerable users with graph-based neural multi-task learning for infodemic surveillance,â€ Information Processing \& Management, vol. 61, no. 1, p. 103520, 2024.

\bibitem{devlin2018bert}
J.~Devlin, ``Bert: Pre-training of deep bidirectional transformers for language
  understanding,'' \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  vol.~1, no.~8, p.~9, 2019.

\bibitem{brown2020language}
T.~B. Brown, ``Language models are few-shot learners,'' \emph{arXiv preprint
  arXiv:2005.14165}, 2020.

\bibitem{beltagy2020longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan, ``Longformer: The long-document
  transformer,'' \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem{achiam2023gpt}
J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida,
  J.~Altenschmidt, S.~Altman, S.~Anadkat \emph{et~al.}, ``Gpt-4 technical
  report,'' \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{wang2024utilizing}
J.~Wang, J.~X. Huang, X.~Tu, J.~Wang, A.~J. Huang, M.~T.~R. Laskar, and
  A.~Bhuiyan, ``Utilizing bert for information retrieval: Survey, applications,
  resources, and challenges,'' \emph{ACM Computing Surveys}, vol.~56, no.~7,
  pp. 1--33, 2024.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in
  \emph{Proceedings of the IEEE/CVF international conference on computer
  vision}, 2021, pp. 10\,012--10\,022.

\bibitem{han2022survey}
K.~Han, Y.~Wang, H.~Chen, X.~Chen, J.~Guo, Z.~Liu, Y.~Tang, A.~Xiao, C.~Xu,
  Y.~Xu \emph{et~al.}, ``A survey on vision transformer,'' \emph{IEEE
  transactions on pattern analysis and machine intelligence}, vol.~45, no.~1,
  pp. 87--110, 2022.

\bibitem{wang2024revisiting}
Z.~Wang, C.~Pei, M.~Ma, X.~Wang, Z.~Li, D.~Pei, S.~Rajmohan, D.~Zhang, Q.~Lin,
  H.~Zhang \emph{et~al.}, ``Revisiting vae for unsupervised time series anomaly
  detection: A frequency perspective,'' in \emph{Proceedings of the ACM on Web
  Conference 2024}, 2024, pp. 3096--3105.

\bibitem{fang2023unsupervised}
L.~Fang, K.~Feng, K.~Zhao, A.~Hu, and T.~Li, ``Unsupervised rumor detection
  based on propagation tree vae,'' \emph{IEEE Transactions on Knowledge and
  Data Engineering}, vol.~35, no.~10, pp. 10\,309--10\,323, 2023.

\bibitem{Bian2020Rumor}
T.~Bian, X.~Xiao, T.~Xu, P.~Zhao, W.~Huang, Y.~Rong, and J.~Huang, ``Rumor
  detection on social media with bi-directional graph convolutional networks,''
  in \emph{Proceedings of the AAAI conference on artificial intelligence},
  2020, pp. 549--556.

\bibitem{wei2024modeling}
L.~Wei, D.~Hu, W.~Zhou, X.~Wang, and S.~Hu, ``Modeling the uncertainty of
  information propagation for rumor detection: A neuro-fuzzy approach,''
  \emph{IEEE Transactions on Neural Networks and Learning Systems}, vol.~35,
  no.~2, pp. 2522--2533, 2024.

\bibitem{deng2023markov}
L.~Deng, C.~Wu, D.~Lian, Y.~Wu, and E.~Chen, ``Markov-driven graph
  convolutional networks for social spammer detection,'' \emph{IEEE
  Transactions on Knowledge and Data Engineering}, vol.~35, no.~12, pp.
  12\,310--12\,322, 2023.

\bibitem{Yang2024Topic}
Z.~Yang, Y.~Pang, X.~Li, Q.~Li, S.~Wei, R.~Wang, and Y.~Xiao, ``Topic
  audiolization: A model for rumor detection inspired by lie detection
  technology,'' \emph{Information Processing \& Management}, vol.~61, no.~1, p.
  103563, 2024.

\bibitem{ma2016detecting}
J.~Ma, W.~Gao, P.~Mitra, S.~Kwon, B.~J. Jansen, K.-F. Wong, and M.~Cha,
  ``Detecting rumors from microblogs with recurrent neural networks,'' in
  \emph{Proceedings of the 25th International Joint Conference on Artificial
  Intelligence}, 2016, pp. 3818--3824.

\bibitem{ma2021improving}
J.~Ma, J.~Li, W.~Gao, Y.~Yang, and K.-F. Wong, ``Improving rumor detection by
  promoting information campaigns with transformer-based generative adversarial
  learning,'' \emph{IEEE Transactions on Knowledge and Data Engineering},
  vol.~35, no.~3, pp. 2657--2670, 2023.

\bibitem{Sun2022ddgcn}
M.~Sun, X.~Zhang, J.~Zheng, and G.~Ma, ``Ddgcn: Dual dynamic graph
  convolutional networks for rumor detection on social media,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 2022,
  pp. 4611--4619.

\bibitem{wang2023cross}
L.~Wang, C.~Zhang, H.~Xu, Y.~Xu, X.~Xu, and S.~Wang, ``Cross-modal contrastive
  learning for multimodal fake news detection,'' in \emph{Proceedings of the
  31st ACM international conference on multimedia}, 2023, pp. 5696--5704.

\bibitem{zhang2024reinforced}
L.~Zhang, X.~Zhang, Z.~Zhou, F.~Huang, and C.~Li, ``Reinforced adaptive
  knowledge learning for multimodal fake news detection,'' in \emph{Proceedings
  of the AAAI Conference on Artificial Intelligence}, vol.~38, no.~15, 2024,
  pp. 16\,777--16\,785.

\bibitem{qu2024temporal}
Z.~Qu, F.~Zhou, X.~Song, R.~Ding, L.~Yuan, and Q.~Wu, ``Temporal enhanced
  multimodal graph neural networks for fake news detection,'' \emph{IEEE
  Transactions on Computational Social Systems}, 2024.

\bibitem{Yang2024model}
Z.~Yang, Y.~Pang, Q.~Li, S.~Wei, R.~Wang, and Y.~Xiao, ``A model for early
  rumor detection base on topic-derived domain compensation and multi-user
  association,'' \emph{Expert Systems with Applications}, vol. 250, p. 123951,
  2024.

\bibitem{zhang2023rumor}
Q. Zhang, Y. Yang, C. Shi, A. Lao, L. Hu, S. Wang, and U. Naseem, â€œRumor detection with hierarchical representation on bipartite ad hocevent trees,â€ IEEE Transactions on Neural Networks and Learning Systems, vol. 35, no. 10, pp. 14x112â€“-14124, 2024.

\bibitem{babu2023efficient}
R. Babu, J. Kannappan, B. V. Krishna, and K. Vijay, â€œAn efficient spam detector model for accurate categorization of spam tweets using quantum chaotic optimization-based stacked recurrent network,â€ Nonlinear Dynamics, vol. 111, no. 19, pp. 18523â€“-18540, 2023.

\bibitem{GRU}
K. Yu, X. Zhu, Z. Guo, A. Tolba, J. J. P. C. Rodrigues, and V. C. Leung, â€œA cross-field deep learning-based fuzzy spamming detection approach via collaboration of behavior modeling and sentiment analysis,â€ IEEE Transactions on Fuzzy Systems, vol. 32, no. 12, pp. 7168â€“7182, 2024.

\bibitem{rao2023hybrid}
S. Rao, A. K. Verma, and T. Bhatia, â€œHybrid ensemble framework with self-attention mechanism for social spam detection on imbalanced data,â€ Expert Systems with Applications, vol. 217, p. 119594, 2023.

\bibitem{gao2019graph}
H. Gao and S. Ji, â€œGraph u-nets,â€ in international conference on machine learning. PMLR, 2019, pp. 2083â€“-2092.

\bibitem{generale2022scaling}
A. Generale, T. Blume, and M. Cochez, â€œScaling r-gcn training with graph summarization,â€ in Companion Proceedings of the Web Conference 2022, 2022, pp. 1073â€“-1082.

\bibitem{he2022convolutional}
M. He, Z. Wei, and J.-R. Wen, â€œConvolutional neural networks on graphs with chebyshev approximation, revisited,â€ Advances in neural information processing systems, vol. 35, pp. 7264â€“-7276, 2022.

\bibitem{GraphTrans}
Y. Cai, H. Wang, H. Cao, W. Wang, L. Zhang, and X. Chen, â€œDetecting spam movie review under coordinated attack with multi-view explicit and implicit relations semantics fusion,â€ IEEE Transactions on Information Forensics and Security, vol. 19, pp. 7588â€“-7603, 2024.

\bibitem{zhang2022detecting}
F. Zhang, S. Yuan, P. Zhang, J. Chao, and H. Yu, â€œDetecting review spammer groups based on generative adversarial networks,â€ Information Sciences, vol. 606, pp. 819â€“836, 2022.

\bibitem{yang2022chinese}
A. Yang, J. Pan, J. Lin, R. Men, Y. Zhang, J. Zhou, and C. Zhou, â€œChinese clip: Contrastive vision-language pretraining in chinese,â€ arXiv preprint arXiv:2211.01335, 2022.

\bibitem{jia2021scaling}
C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, â€œScaling up visual and vision-language representation learning with noisy text supervision,â€ in International conference on machine learning. PMLR, 2021, pp. 4904â€“4916.

\bibitem{li2023blip}
J. Li, D. Li, S. Savarese, and S. Hoi, â€œBlip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models,â€ in International conference on machine learning. PMLR, 2023, pp. 19 730â€“19 742.
\end{thebibliography}

%\bibliographystyle{IEEEtran}
%\bibliography{reference}
\vspace{-15mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.2in,clip,keepaspectratio]{./figures/author-ZhouY.pdf}}]{Zhou Yang}
	received the B.S degree in electronic Science and Technology from Zhengzhou University of Science and Technology, China, in 2021. He is currently working toward the M.S degree in information and communication engineering with Chongqing University of Posts and Telecommunications, China. His research interests include rumor detection, spammer detection, deep learning, and its application.
\end{IEEEbiography}
\vspace{-15mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.2in,clip,keepaspectratio]{./figures/author-YucaiP.pdf}}]{Yucai Pang}
	received the Ph.D. degree from the Harbin Engineering University, Harbin, China. He has been working as an Associate Professor with the School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications since 2017. His research interests include big data prediction, array signal processing, and social rumor detection.
\end{IEEEbiography}
\vspace{-15mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.2in,clip,keepaspectratio]{./figures/author-HongboY.png}}]{Hongbo Yin}
	is currently working toward his doctoral degree in the School of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China. He received the M.Sc. degree in Chongqing University of Posts and Telecommunications, Chongqing, China, in 2024. His main research interests are social networks, spammer detection, edge computing networks.
\end{IEEEbiography}
\vspace{-15mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.2in,clip,keepaspectratio]{./figures/author-XiaoYp.pdf}}]{Yunpeng Xiao}
	received the Ph.D. degree in computer science from Beijing University of Posts and Telecommunications, Beijing, China, in 2013. He is a professor and vice dean of the Institute of Electronic Information and Network Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China. He was a visiting scholar of Georgia Institute of Technology from 2018 to 2019. His research interests include social networks, e-commerce and intelligent system.
\end{IEEEbiography}

\end{document}


