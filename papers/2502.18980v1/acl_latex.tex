% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[most]{tcolorbox}
% \usepackage{xcolor}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{xspace}
\newcommand{\benchmark}{PEToolBench\xspace} % 加空格
\newcommand{\framework}{PEToolLLaMA\xspace}
% \newcommand{\benchmark}{LaMP}


\title{PEToolLLM: Towards Personalized Tool Learning in \\Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Qiancheng Xu$^{1}$, Yongqi Li$^{1\dagger}$, Heming Xia$^{1}$, Fan Liu$^{2}$, Min Yang$^{3}$, Wenjie Li$^{1}$ \\
% \thanks{Corresponding author.}
$^{1}$ The Hong Kong Polytechnic University \quad
$^{2}$ National University of Singapore \\
$^{3}$ Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences \\
\texttt{\{qiancheng.xu, he-ming.xia\}@connect.polyu.hk} \\
\texttt{liyongqi0@gmail.com} \quad
\texttt{cswjli@comp.polyu.edu.hk}
}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begingroup\def\thefootnote{$\dagger$}\footnotetext{Corresponding author.}\endgroup

\begin{abstract}
Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. 
However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences.
% meet personalized user needs.
To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. 
To fill the gap of missing benchmarks, we construct \benchmark, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios.
Moreover, we propose a framework \framework to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization.
Extensive experiments on \benchmark demonstrate the superiority of \framework over existing LLMs. 
We release our code and data at
% for review at \href{https://anonymous.4open.science/r/PEToolBench-952F/}{https://anonymous.4open.science/
% r/PEToolBench-952F/}.
\href{https://github.com/travis-xu/PEToolBench}{https://github.com/travis-xu/PEToolBench}.

\end{abstract}

\section{Introduction}
% Large Language Models (LLMs) have increasingly been regarded as a potential pathway toward developing general AI assistants for human users. 
Large Language Models (LLMs) possess extensive knowledge and have powerful instruction-following abilities, making them effective AI assistants for tasks such as text rewriting, question answering, and code writing~\cite{zhao2023survey}.
However, they often struggle in addressing user needs in scenarios such as checking weather and booking flights. 
To address this, tool learning~\cite{10.1145/3704435,qu2024tool} has emerged as a promising solution by enabling LLMs to utilize external tools, such as real-time weather APIs and booking systems.
% To bridge these gaps, tool learning has emerged as a promising approach, equipping LLMs with the ability to leverage external tools like real-time weather APIs and booking systems. 
% Tool learning not only overcomes current limitations of LLMs, but also extends their 
In this way, tool learning has extended LLMs' capabilities to tackle more complex tasks, enabling them to fulfill a wide range of user needs.

Current tool learning procedure typically begins with a user instruction, and then LLMs are required to use tools with appropriate functionalities for satisfying users' needs.
% use tools with appropriate functionalities to fulfill user’s requirements in the instruction. 
Existing tool learning methods can be categorized into in-context learning~\cite{wu-etal-2024-toolplanner,liu2025toolplanner} and fine-tuning approaches~\cite{NEURIPS2023_d842425e,wang2025toolgen}. The former approach allows LLMs to use tools by directly providing tool documentation in input but the performance is constrained by the input length.
% or demonstrations  % the limited input length 
The latter approach trains LLMs to internalize tool knowledge but struggles with tool generalization.
% specially
% Existing tool learning studies primarily focus on the \textit{general-purpose} tool-use capability, where
% % , i.e., utilizing various tools to address a wide range of user needs.
% % Specifically, given user instructions and tool documentation as inputs, 
% LLMs are required to understand two main aspects: 1) \textbf{explicit user requirements}, expressed in user instructions, and 2) \textbf{functionalities of tools}, derived from tool documentation. By understanding both aspects, LLMs can accurately utilize tools with appropriate functionalities to address user needs.


\begin{figure*}[!t]
    \centering
    % \centerline{\includegraphics[width=2.0\columnwidth]{MainFigure.pdf}}
    \includegraphics[width=1.0\textwidth]{fig_intro.pdf}
    \caption{Comparison between (a) tool learning and (b) personalized tool learning. Personalized tool learning facilitates implicit preference comprehension and customized tool usage for individual users.}
    \label{fig:intro}
\vspace{-1em}
\end{figure*}


Despite the advancement, existing tool learning methods primarily focus on the general-purpose tool-use capability but overlook the critical role of personalization. 
In tool learning, more personalized user needs are expected to be derived from the user's previous 
% interaction 
tool usage 
history as a supplement to user instructions, which can help LLMs provide more customized tool-usage assistance to enhance the user experience.
% This helps the LLM provide more customized tool assistance, thereby improving the user experience.
% requires LLMs to offer more customized tool-usage assistance for specific users. 
% uncover
As illustrated in Figure~\ref{fig:intro}, personalized tool learning is non-trivial due to the following aspects. 
% As tool-use LLMs are designed to serve human users, and considering that each user has unique needs, LLMs must use specific tools to meet user-specific needs, referred to as the \textit{personalized} tool-use capability.
% As user serving AI assistants, considering that each user has unique needs, LLMs must be capable of meeting these distinct requirements.
% Since tool learning aims at enabling LLMs to better serve human users, each of whom has unique needs.
% Unfortunately, they exhibit significant limitations due to the neglect of two critical aspects:
% catering for unique user needs,
% role of \textit{personalized} tool-use capabilities in LLMs, including two additional aspects
% 1) \textbf{implicit user preferences}, which are not explicitly stated in user instructions but can be inferred from personal user data (e.g., interaction history). 
1) \textbf{Implicit user preferences}. 
User preferences for tool usage are often implicitly conveyed through the user's history rather than explicitly stated in user instructions, making them difficult to understand.
% as they are implicitly concealed within the user's history instead of explicitly stated in user instructions.
For instance, when a user requests a search for articles, their preference for academic-related content needs to be inferred from previous interactions with academic tools like Google Scholar.
% For instance, when requesting a search for articles, a user may prefer academic-related results if their interaction history includes academic tools like Google Scholar. 
% may indicate a preference for academic-related search results.
% and are revealed in the user's interaction history.
% comprehension, which are not explicitly stated in user instructions but can be inferred from user's interaction history.
% For instance, when requesting a search for articles, a user may prefer academic-related results if their interaction history includes academic tools like Google Scholar. 
% For example, when requesting a search, a user's interaction history involving academic tools (e.g., Google Scholar) is likely to prefer academic-related search results;
% academically supportive
2) \textbf{Non-functional tool attributes}.
% In real-world scenarios, many tools have the same functionalities, 
% reflect the differences in 
Since many tools have the same functionalities, user preferences cannot be effectively distinguished based solely on tool functionalities. 
% which is challenging for LLMs to differentiate and align with different user preferences.
% To address this, it is crucial 
This underscores the need to consider non-functional tool attributes, such as usability, integrability, and accessibility, which can better reflect user preferences. 
% Due to the existence of multiple same-functionality tools, LLMs cannot determine the user's preferred tools based solely on their functionality. 
% In this paper, we refer to these as the \textit{non-functional attributes} of tools.
% to address specific personalized needs.
% discern which tool a user prefers based solely on its functionality.
% allowing LLMs to use the user's preferred tool from multiple tools with the same functionality.
% customization most
% This is possible because tools often have unique \textit{non-functional attributes} (e.g., usability, integrability, accessibility, etc.) that are preferred by different users.
% distinguish between same-functionality tools and use the most suitable one for individual user.
% beyond functionalities, which can significantly impact user preferences.
% \textbf{non-functional attributes of tools}, referring to attributes beyond functionalities (e.g., usability, integrability, etc.) that significantly influence user preferences.
% can differentiate same-functionality tools to align with different user preferences. 
As shown in Figure~\ref{fig:intro}, Google Search can be distinguished from other search tools by its integration into Google’s ecosystem with Google Scholar, making it more suitable for users with academic needs.
% These non-functional attributes are crucial for LLMs to call the most preferred tool for a specific user, especially when there exist multiple same-functionality tools. 
% Therefore, these aspects are essential for LLMs to form a holistic understanding of tools and user needs, facilitating customized tool-usage assistance.
% form a more comprehensive and deeper understanding of tools and user needs, advancing them to be a more personalized and user-centric tool-use assistant.

% To this end
To address the above issues, we formulate the task of personalized tool learning in LLMs, aiming at personalized tool usage for individual users.
Formally, given user instructions along with user's 
% user data (e.g. interaction history)
interaction history, LLMs are required to answer user instructions with tools by considering both explicit user requirements in instructions and implicit user preferences behind interaction history.
% must satisfy user needs by using tools which not only have appropriate functionalities to meet explicit user requirements, 
% % as well as with 
% but also possess suitable non-functional attributes to align with implicit user preferences.
% understand both the user's explicit requirements and implicit preferences, as well as functionalities and non-functional attributes of tools, then select and call the appropriate tool with corresponding parameters.
% , then select and call the appropriate tool with corresponding parameters to meet the user's needs.

% which takes personal user data and tool characteristics into consideration
% aiming for personalized user needs comprehension and tool utilization.
% , aiming to align with explicit user instructions and implicit user preferences derived from personalized data.
% by taking personalized user data and tool characteristics into consideration, aiming to facilitate more tailored tool usage that satisfies both the users' functional requirements and their individual preferences. 


Since there is no benchmark for this task currently, we fill this gap by introducing the first personalized tool learning benchmark (\benchmark). 
Specifically, the benchmark is created through three following steps.
1) Tool Preparation.
We collect a bunch of high-quality tools from RapidAPI and then
% as our seed tools. 
% And then we 
leverage LLM to understand the functionality and non-functional attributes of each tool. 
2) Preference Construction. 
% We leverage the obtained tool attributes to construct the user preferences on tools.
% We classify tools with the same functionalities into groups. 
Among same-functionality tools, we construct the user's tool preferences by assigning tools with distinct non-functional attributes to different users.
% construct tool preferences for users by assigning the user's preferred tool and non-preferred tools based on non-functional attributes.
% we identify the preferred and non-preferred tools of a specific user within the functionality group.
% by randomly assign a tool with distinct non-functional attributes as the user's preferred tool and other tools as non-preferred tools.
% for a specific user. 
% To enrich the user preference, we use the preferred tool to retrieve more tools with similar non-functional attributes.
% The preferred tool will also be used to retrieve more tools with similar non-functional attributes to be the user's preferred tool .
% In this way, we can obtain a bunch of preferred and non-preferred tools representing different user preferences.
% After multiple functionality groups, we can obtain a number of preferred and non-preferred tools representing .
% all serving as preferred tools for the specific user.  
3) Data Creation. 
Based on tool preference, we synthesize the user's interaction history into a sequence of tool-use interactions, each consisting of a user instruction and an LLM's tool call.
% The user's interaction history is generated into a sequence of tool-use interactions, each containing a user instruction and an LLM's tool call. 
% both preferred and non-preferred tools with binary ratings reflecting the user's preference; 
% Finally, we select tools 
We design three personalized tool-usage settings by generating the interaction history in three types, i.e., preferred-only, rating-integrated, and chronological. 
% representing different forms of user preferences.
% in  ways, we
And then we use tools not included in the interaction history to synthesize user instructions.
% as ground truth tools to generate user instructions for each data instance. 
% Each instruction is combined with the interaction history into a data instance. construct data instance 
% We generate interaction history into three types, i.e., preferred-only, rating-integrated, chronological, to represent different forms of user preferences.
% 1) \textit{preferred-only} history, which only includes the user's preferred tools ; 2) \textit{rating-integrated} history, integrating user's binary ratings on each tool call according to user preferences; and 3) \textit{chronological} history, where interactions are arranged in time order to capture changes in user preferences over time. 
After rigorous filtering, we obtain 12,000 user instructions with interaction histories reflecting diverse user preferences and cover a wide range of tool-use scenarios by encompassing 7454 tools across 46 categories.
 % of 1,703 users 

% .Overall, our benchmark sets itself apart through several distinctive features:
% structured through three primary phases:
% \begin{enumerate}
%     \item User's Preferred Tools Selection: 
%     We adopt real-world tools from ToolBench dataset, and identify their user-centric attributes beyond functionalities. Then we classify and select tools with similar user-centric attributes as the user's preferred tools. 
%     \item Interaction History Construction: 
%     We sample a bunch of user's preferred tools to synthesize a sequence of tool-use cases as the user's interaction history, constructed in three forms (i.e., preferred-only, rating-integrated and chronological), representing different manifestations of user preferences.
%     \item User Instructions Creation:
%     We utilize users' preferred tools not included in the interaction history as ground truth tools. Then we use them to synthesize user instructions and corresponding tool calling parameters.
% \end{enumerate}

% To equip LLMs with personalized tool-use capability, we propose \framework. Specifically, we train LLM on \benchmark through two stages: 
Based on the \benchmark dataset, we propose the personalized tool learning framework (\framework) to equip LLMs with personalized tool-use capability. The training process consists of two stages:
1) the supervised fine-tuning (SFT) stage, which equips LLM with foundational tool-use capability to address user needs; 2) the direct preference optimization (DPO) stage, which samples the user's preferred and non-preferred tool calls for pair-wise optimization to better align with user preferences.
 % (Dubey et al., 2024)
We evaluate 6 distinct open-source and closed-source LLMs including the latest GPT-4o on \benchmark. 
Experimental results demonstrate that our \framework significantly outperforms the best-performing LLM across all settings with improvements even more than 50\%, showcasing its superior personalized tool-use capabilities.
% to provide user-centric and customized assistance.
% Experimental results demonstrate that \framework consistently and significantly outperforms existing baselines, showcasing the potential of personalized tool-use LLMs to provide more user-centric and customized tool-usage assistance.


In summary, our contributions are as follows.
\begin{itemize}
\item 
We are the first to formulate the task of personalized tool learning in LLMs, 
which incorporates user's interaction history to achieve personalized tool-usage assistance.
% for individual users. 
% tool selection and calling, bridging users with customized tool-usage assistance.

\item 
We construct the first benchmark for personalized tool learning in LLMs, \benchmark,
featuring user instructions integrated with interaction history reflecting diverse user preferences and encompassing various tools.
% featuring diverse user instructions with interaction history across three types.
\item 
% We propose the first personalized tool-use LLM, \framework.
We propose a novel personalized tool learning framework \framework. 
% Extensive experiments show that \framework consistently exceeds existing baselines, effectively fulfilling both user requirements and preferences.
Extensive experiments demonstrate that \framework significantly surpass the best-performing LLM by more than 50\%, exibiting exceptional personalized tool-use capabilities.
\end{itemize}

\section{Related Work}
\subsection{Tool Learning in LLMs}
Tool learning aims at extending the capabilities of LLMs by equipping them with external tools to solve tasks like weather inquiry, car navigation, and restaurant reservation. Existing benchmarks primarily focus on evaluating the tool learning proficiency of LLMs in addressing user instructions, from aspects such as 
% tool usage awareness~\cite{huang2024metatool}, 
tool selection and calling accuracy~\cite{xu-etal-2024-enhancing-tool,NEURIPS2024_8a75ee6d,ye-etal-2024-rotbench,wang2025mtubench}, tool planning ability~\cite{basu-etal-2024-api,wang-etal-2024-appbench,NEURIPS2024_085185ea,liu2025toolace}, and complex workflow creation~\cite{shen2025shortcutsbench,qiao2025benchmarking,fan2025workflowllm}. To improve tool-use capabilities, various strategies have been introduced, including in-context learning which enables LLMs to use tools via documentation~\cite{yuan2024easytool,shi-etal-2024-learning,qu2025from},
% (Hsieh et al., 2023), 
and fine-tuning which trains LLMs on specialized tool-use datasets~\cite{zhuang2024toolchain,chen2024advancing,chen2025learning}. However, prior studies neglect the crucial role of personalized tool usage in LLMs. This paper addresses this gap by introducing personalized tool learning, developing a comprehensive benchmark for evaluation, and proposing an optimization strategy to enhance personalized tool-use capabilities in LLMs.

\subsection{Personalization in LLMs}
The goal of personalization in LLMs is to leverage personal user data, such as historical behaviors and background information, to generate outputs that better align with the user preferences~\cite{tseng-etal-2024-two}.
% (Chen et al., 2023e; Deshpande et al., 2024). 
Approaches such as fine-tuning~\cite{cai2025large} and prompt engineering~\cite{yuan-etal-2025-personalized}
% , and user-specific embeddings 
have been explored to adapt LLMs to individual or domain-specific tasks. These approaches have been applied across various fields, including recommendation systems~\cite{lyu-etal-2024-llm}, search engines~\cite{10.1145/3589334.3645482}, education~\cite{liu2024socraticlm}, 
% healthcare, 
and dialogue generation~\cite{wang-etal-2023-target}. However, previous research has not investigated LLMs' personalization in the area of tool learning. In this work, we bridge this gap by incorporating user's interaction history to assess and enhance the LLMs' capability in providing personalized tool-usage assistance for specific users.
% generating tool calls tailored to user-specific needs.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{fig_benchmark.pdf}
    % \caption{Illustration of our \benchmark.}
    \caption{Illustration of the process for constructing our \benchmark.}
    \label{fig:benchmark}
\vspace{-1em}
\end{figure*}

% Due to the lack of real personal data on tool-usage, we adopt a tool-driven approach to simulate user data by leveraging tool attributes to construct user preferences and interaction history. 
% real-world tools to simulate user data, distinguishing user preferences by tool attributes. 
% (1) Tool Preparation.
% We collect a bunch of high-quality tools from RapidAPI.
% % as our seed tools. 
% Then we leverage LLM to understand the functionality and non-functional attributes of each tool. 
% (2) Preference Construction. 
% % We leverage the obtained tool attributes to construct the user preferences on tools.
% We classify tools with the same functionalities into groups. 
% Within each functionality group, we construct the user preferences by
% % we identify the preferred and non-preferred tools of a specific user within the functionality group.
% randomly assign a tool with distinct non-functional attributes as the user's preferred tool and other tools as non-preferred tools.
% % for a specific user. 
% To enrich the user preference, we use the preferred tool to retrieve more tools with similar non-functional attributes.
% % The preferred tool will also be used to retrieve more tools with similar non-functional attributes to be the user's preferred tool .
% In this way, we can obtain a bunch of preferred and non-preferred tools representing different user preferences.
% % After multiple functionality groups, we can obtain a number of preferred and non-preferred tools representing .
% % all serving as preferred tools for the specific user.  
% (3) Data Creation. 
% Based on the user preference, we generate the user's interaction history as a sequence of tool-use interactions, each containing a user instruction and a corresponding tool call by LLM. The interaction history is constructed in three types: 1) \textit{preferred-only} history, which only includes the user's preferred tools ; 2) \textit{rating-integrated} history, integrating user's binary ratings on each tool call according to user preferences; and 3) \textit{chronological} history, where interactions are arranged in time order to capture changes in user preferences over time. 
% % both preferred and non-preferred tools with binary ratings reflecting the user's preference; 
% % Finally, we select tools 
% Finally, we generate user instructions based on the user's preferred tools not included in the interaction history.
% % as ground truth tools to generate user instructions for each data instance. 
% % Each instruction is combined with the interaction history into a data instance. construct data instance 


\section{Task and Benchmark}
\subsection{Task Formulation}
\paragraph{Tool Learning}
Given an instruction $q_u$ of the user $u$, tool learning aims to generate an appropriate tool call, including the selected tool and its corresponding parameters, from a set of candidate tools. Formally, let the candidate tool set be $\mathcal{T}=\{d(t_1), d(t_2), ..., d(t_N)\}$, where $d(t_i)$ represents the documentation of tool $t_i$ and $N$ is the total number of candidate tools. The LLM is then tasked with generating a tool call $c = (t, p)$, where \(t \in \mathcal{T}\) and \(p\) denotes its parameters: 
%The LLM $\rho_\theta$ needs to generate a tool call $c$ which consists of an appropriate tool \(t\) from the candidate tool set $\mathcal{D}$ and corresponding parameters \(p\), denoted as: 
\begin{equation}
% c = \rho_\theta(t,p|q_u,\mathcal{T})
(t,p) = LLM(q_u,\mathcal{T}).
% \leftarrow LLM()
\end{equation}

\paragraph{Personalized Tool Learning}
In personalized tool learning, we incorporate the users' interaction history alongside their instructions, enabling the LLM to generate tool calls that satisfy both the users' explicit requirements and implicit preferences.
For a user u, we define the interaction history as $\mathcal{H}_u = \{h_u^1, h_u^2, ..., h_u^M\}$, where each $h_u^i$ consists of a past user instruction $q_{u}^i$ and the corresponding tool call $c_u^i=(t_u^i,p_u^i)$, with $t_u^i$ representing the selected tool and $p_u^i$ denoting its associated parameters. Let $c_u = (t_u,p_u)$ represent the personalized tool call for user u, the personalized tool learning task can then be formulated as:
% The LLM $\rho_\theta$ needs to generate a tool call $c$ 
\begin{equation}
(t_u,p_u) = LLM(q_u,\mathcal{T},\mathcal{H}_u),
% c = \rho_\theta(t,p|q_u,\mathcal{T},\mathcal{H}_u)
\end{equation}

% requirement and preference. 
% Formally, for a user $u$, we define the user's instruction as $q_u$ and interaction history as $\mathcal{H_u}$ represented by a sequence $\{h_u^1, h_u^2, ..., h_u^M\}$. Each $h_u^i$ denotes an interaction, i.e., the past user instruction $q_{u}^i$ and the LLM's tool call $c^i=(t_i,p_i)$, containing the selected tool $t_i$ and corresponding parameters $p_i$.
% The candidate tool set is denoted by $\mathcal{T}=\{d(t_1), d(t_2), ..., d(t_N)\}$, where $d(t_i)$ represents the documentation of each candidate tool $t_i$ and $N$ is the total number of candidate tools.
% Based on the user's requirement from instruction $q_u$ and the user's preference from interaction history $\mathcal{H_u}$, the personalized tool-use LLM needs to generate a tool call $c$ which consists of an appropriate tool \(t\) from the candidate tool set $\mathcal{T}$ and corresponding parameters \(p\). 
% select a tool $t$ from the candidate tool set $D$ and call it with appropriate parameters $p$ to solve the task.
% and the toolset as $T=\{T_1, T_2, ..., T_n\}$,
% Based on $P_u$ and $q$, the model is expected to select a user-specific tool set $\hat{D}_u=\{d_i^u\}_{i=1}^k$ from $D$, and then output a user-specific response $r^u$ based on a tool-use trajectory
% $T=[t_j(d^u_i,c^u_i)|d^u_i\in \hat{D}_u]_{j=1}^S$, containing $S$ tool-use steps which sequentially uses a selected user-specific tool $d^u_i$ with corresponding user-specific tool calling parameters $c^u_i$.

\subsection{Benchmark Construction}
Due to the lack of real user interaction histories on tool-usage, we adopt a tool-driven approach to simulate interactions based on pre-constructed user's tool preferences. The whole process for constructing \benchmark, illustrated in Figure~\ref{fig:benchmark}, consists of three steps: tool preparation, preference construction, and data creation.
\subsubsection{Tool Preparation}

\paragraph{Tool Collection}
% We curate xx high-quality tools selected from real-world APIs in ToolBench dataset.
Following ToolBench~\cite{qin2024toolllm}, we adopt the tools from RapidAPI for our benchmark, since it offers a large-scale and diverse collection of real-world tools that can potentially address a wide range of user needs.
% Despite the numerous tools collected in ToolBench, the quality of tools is not guaranteed.
To ensure the quality of the collected tools, we perform strict filtering by removing: 1) outdated tools, which are marked as deprecated in RapidAPI; 2) tools with insufficient information, such as inadequate or missing tool documentation; and 3) duplicate tools, which have repeated tool names, descriptions, or category names.
% After filtering, we obtain xx high-quality tools.
% redundancies
% Complete Documentation. Despite the numerous tools collected in ToolBench (Qin et al., 2023b) from RapidAPI, the documentation quality is not guaranteed. To reduce the failure of tool calling cases caused by inadequate tool descriptions, which focus the evaluation attention on pure LLM abilities, we manually generate high-quality and detailed tool documentation for each tool.

\paragraph{Tool Understanding}
Since tool documentation often contains redundant and irrelevant information, directly extracting tool attributes from it can be challenging.
To address this, we 
% design a tool understanding process that leverages LLMs to comprehend both the functionality and non-functional attributes of each tool. Specifically, we 
first provide the documentation of each tool to LLM and prompt it to generate a tool-use example, including a simulated user instruction and parameters for calling the tool. 
Next, based on the tool documentation and tool-use example, the LLM is instructed to generate descriptions of the tool's functionality and non-functional attributes separately.
Besides, we include demonstrations in the prompt to help the LLM distinguish between these two attribute types.
By leveraging specific tool-use examples and demonstrations, the LLM can develop a more comprehensive understanding of each tool’s functionality and non-functional characteristics.
% plausible scenario for using the tool, relevant to the API
% We also add demonstrations into the instruction to enhance the instruction-following of LLMs in parsing tool documentation.
% tool documentation usually includes plenty of irrelevant information that makes it difficult to understand practical usage for LLMs
% To ensure a holistic understanding of tool attributes to align with diverse users' requirements and preferences, we leverage LLM to comprehend both the functionalities and non-functional attributes of each tool. Specifically, 
% we provide the tool documentation of each tool, and prompt LLM to understand 

\subsubsection{Preference Construction}

\paragraph{Tool Classification}
% that meet three main principles:
% tools with the same functionalities 
To identify potential tool-usage scenarios for users, we classify tools with the same functionalities into groups.
Specifically, we first employ the Ada Embedding model~\footnote{\url{https://platform.openai.com/docs/guides/embeddings/embedding-models}.} to compute embeddings for the functionality descriptions of all tools.
Then, we apply the DBSCAN algorithm~\cite{schubert2017dbscan} to cluster these tools into multiple groups based on the similarity of their embeddings.
Within each group, the tools share the same functionality and can be applied to a specific tool-usage scenario.
% Consequently, all tools within a group share the same functionality and can be applied to a specific tool-usage scenario.
To further ensure that tools within each group exhibit uniform functionality, we conduct rigorous filtering and only retain groups where tools 1) have the same input-output formats (i.e., required/optional parameters and response schema) and 2) belong to the same category (e.g., sports, music, finance).
% perform rigorous filtering 

% embeddings based on the semantic similarities between task descriptions and retain one instance for each cluster
% text embeddings for these explanations
% Due to the diversity in both the intent detection and slot-filling strategies, as well as the creation of specific tools based on LLM, the synthesized tool set can be highly redundant. 
% For example, tools like “search_movie” and “find_movie” may have different names but essentially perform the same function. 
% we cluster them based on the semantic similarities between task descriptions

\paragraph{Tool Preference Construction}
We leverage non-functional tool attributes to construct the user's tool preference. 
First, we randomly sample a functionality group for a user, representing a potential tool-usage scenario for interaction. Within this group, we choose a tool with specific non-functional attributes as the user's preferred tool, while the others are considered non-preferred. Using the preferred tool as a reference, we retrieve the top-$5$ tools with the most similar non-functional attributes. Similarity is computed based on the embeddings of the tools' non-functional descriptions, which are generated in the Tool Understanding phase. Through multiple iterations of sampling and retrieving, we obtain a diverse set of preferred and non-preferred tools that represent user preferences. After each iteration, we check for functionality overlap between newly retrieved tools and previously selected ones. If an overlap is detected, the tools are discarded, and the sampling process is restarted. This ensures that each tool-usage scenario is associated with only one preferred tool per user.
By following this approach, we construct diverse tool sets that align with different user preferences.
% Through this process, 
% Then, we utilize the Ada Embedding model to compute xx embeddings of all tools, based on descriptions of their non-functional attributes generated in the Tool Understanding phase.
% Then we use the chosen tool to retrieve top-$K$ tools based on the similarity scores between their embeddings.
% This ensures that each user has a unique preference for a single tool in each tool-usage scenario.
% from the whole tool collection 

\subsubsection{Data Creation}

\paragraph{Interaction History Generation}
Based on tool preference, we leverage the LLM to construct the user's interaction history.
Specifically, for each user, we provide LLM with the user's preferred and non-preferred tools, including tool attributes and tool-use examples generated in the Tool Understanding phase.
The LLM will generate a sequence of simulated user-LLM interactions, each consisting of a user instruction and an LLM's tool call, as the user's interaction history.
% , each consisting of a user instruction and an LLM's tool call.
% LLM is then prompted to leverage these tools to 
% between the user and the tool-usage LLM, as the user's interaction history.
% realistic and coherent user's interaction history, containing a sequence of interactions between the user and the tool-usage LLM. 
% Each interaction consists of a user instruction and a corresponding tool call, which can be reused from the provided tool-use examples or newly generated based on the history context.
% Next, we use the preferred tools to synthesize a sequence of interactions as the user's interaction history, each containing a user instruction and a corresponding tool call. 


We design three personalized tool-use settings by generating the interaction history in three types (illustrated in Figure~\ref{fig:history}):
1) \textit{preferred-only} history, where the tools involved in the interactions are all preferred by the user; 
2) \textit{rating-integrated} history, including both the user's preferred and non-preferred tools, with a user's binary rating for each tool-usage interaction representing the user preference, i.e., ``liked'' if the tool aligns with the user preferences, and ``disliked'' otherwise.
% i.e., the rating is set to 1 if the tool is preferred by the user, and 0 otherwise; 
3) \textit{chronological} history, which organizes interactions in time order to reflect changes in user preferences over time, i.e., the more recent tool-usage interactions are more preferred by the user, while earlier interactions are less preferred.
In this way, we can present different forms of user preferences.
% maintain the context length

\paragraph{Instruction Generation}
Next, we use LLM to generate user instructions based on the user's preferred tools that are not included in the user's interaction history. 
% do not appear
% We adopt tools that share the same functionality with other tools as ground truth tools for instruction generation.
We instruct the LLM to avoid directly generating the name of the tool in the instruction, ensuring that the user preference for the tool can only be inferred from the user's interaction history.
% to focus on the tool functionality and do not directly including the tool name in the instruction.
% identifying the preferred tool among xx based on user preferences.
% When generating the user instruction, we ask LLM to focus on the tool functionality and avoid including the tool name in the user instruction directly, in order to xx.
Each user instruction is combined with the user’s interaction history into a data instance. 
% where 

Finally, we obtain 12,000 data instances
% , simulating interaction history and 
encompassing 7,454 tools across 46 categories. We split all data into two parts: a training set comprising 9,000 instances for three personalized settings and a test set containing the rest instances.

\subsection{Benchmark Analysis}
% \paragraph{Statistic Analysis}
% We present the statistical information of our \benchmark in Figure \ref{statistics_length}, Table~\ref{dataset_statistics}. 
We present the statistical information of our \benchmark in Table~\ref{fig:statistics_instances_length}, including the statistics of data instances in three settings and under varying interaction history lengths.
We also present the distribution of tool categories in Figure~\ref{fig:statistics_category}. 
Statistical information demonstrates the diversity and complexity of our dataset.

% \input{tabs/statistics}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{statistics_instances_length.pdf}
    \caption{Statistics of data instances in three personalized settings (in the left figure) and distributions of interaction history length (in the right figure).}
    \label{fig:statistics_instances_length}%文中引用该图片代号
% \vspace{-1em}
\end{figure}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=1.0\linewidth]{statistics_category.pdf}
    \caption{Distributions of tool categories.}
    \label{fig:statistics_category}%文中引用该图片代号
\vspace{-1em}
\end{figure}



% \begin{figure}[htbp]
% \centering
% \begin{subfigure}{0.45\linewidth}
%     \centering
%     \includegraphics[width=0.9\linewidth]{statistics_category.pdf}
%     \caption{(a) Distributions of tool categories}
%     \label{statistics_category}%文中引用该图片代号
% \end{subfigure}
% \centering
% \begin{subfigure}{0.45\linewidth}
%     \centering
%     \includegraphics[width=0.9\linewidth]{statistics_length.pdf}
%     \caption{(b) Distribution of History Length}
%     \label{statistics_length}%文中引用该图片代号
% \end{subfigure}
% \end{figure}


% \paragraph{Consistency Analysis}
% % Since the user's interaction history are generated in our benchmark, 
% To verify the reliability of PersonalToolBench, we analyze the consistency of all three types of user’s interaction history in the data instance.  
% % by evaluating how well the users’ interaction history align with the ground truth tool call . 
% Specifically, we randomly sample xx data instances from PersonalToolBench, including all three types of user’s interaction history, and then ask LLM/human annotators to evaluate how well the users’ interaction history align with the ground truth tool call.
% % invite
% For each sampled instance, they should answer two yes/no questions: 1) whether the interaction history reflects the user preference, and 2) whether the ground-truth tool call matches that user preference.
% The results in Figure xx show that our constructed exhibit a xx consistency with the labeled tool call and xx alignment with user preference.


\subsection{Evaluation Metrics}
Given the user's instruction and interaction history, LLM is expected to select the appropriate tool from a candidate tool set, and then call the selected tool with corresponding parameters. Therefore, we define two metrics as follows.
\begin{itemize}
    \item Tool Accuracy (Tool Acc): The metric assesses the ability of LLM to select the appropriate tool to call. If the tool is correctly selected, the score is 1; otherwise, the score is 0.
    \item Parameter Accuracy (Param Acc): The metric assesses the ability of LLM to generate correct parameters for the tool call. If the input parameters are correctly generated, the score is 1; otherwise, the score is 0.
\end{itemize}

\input{tabs/main}


\section{Method: \framework}
To equip LLM with personalized tool-use capability, we conduct a two-stage training process: 1) personalized SFT, where LLM is fine-tuned on \benchmark to acquire fundamental proficiency in personalized tool usage, and 2) personalized DPO, where LLM is optimized on a preference dataset for better alignment with user preferences.

\paragraph{Personalized SFT.}
The first stage in our approach is Supervised Fine-Tuning (SFT), where we directly fine-tune LLM on the training set of \benchmark. Given the user's instruction $q_u$, interaction history $\mathcal{H}_u$, and the candidate tool set $\mathcal{T}$ as inputs, LLM is trained to generate the ground truth tool call \(c\). $\mathcal{H}_u$ uniformly covers all three types of user interactions to capture diverse user preferences. In this way, LLM can obtain basic personalized tool-usage experiences by understanding both the user needs and preferences.

\paragraph{Personalized DPO.}
In the second stage, we further enhance the LLM's performance through direct preference optimization (DPO)~\cite{NEURIPS2023_a85b405e}. 
Our goal is to guide the LLM to call the user's preferred tools instead of non-preferred ones.
Specifically, for each user instruction $q_u$, we collect multiple tool calls generated by LLM after the SFT stage. 
Then we select the user's preferred and non-preferred tool calls \(c_w\) and \(c_l\) based on the user's tool preference constructed in \benchmark.
\(c_w\) and \(c_l\) will be used to construct the preference dataset \(\mathcal{D}_{\text{DPO}} = \{ (x, c_w, c_l) \}\), where \(x\) denotes the input, including the user instruction $q_u$, interaction history $\mathcal{H}_u$, and the candidate tool set $\mathcal{T}$.
We then apply DPO to optimize the LLM by guiding it to generate the desired tool call \(c_w\) while avoid generating \(c_l\).
% generation direction, and a “rejected” pair to train the LLM to avoid specific outputs.
% prioritize
% encouraging it to generate function parameters similar to \(p_{i}^{\text{b}}\) and discouraging it from generating function parameters similar to \(p_{i}^{\text{w}}\). 
The loss function can be defined as:
\begin{equation} \label{dpo_loss}
\small
{
\mathcal{L} = -\mathbb{E} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(c_w \mid x)}{\pi_{\text{ref}}(c_w \mid x)} - \beta \log \frac{\pi_{\theta}(c_l \mid x)}{\pi_{\text{ref}}(c_l \mid x)} \right) \right],
}
\end{equation}
where \(\sigma\) is the logistic function and \(\beta\) is a weighting parameter that controls the deviation of the policy model $\pi_{\theta}$ (i.e., the LLM we need to optimize) from the reference model $\pi_{\text{ref}}$ (i.e., the LLM after SFT stage).
% the sensitivity of the model's preference to the log-ratio difference between the policy model \(\pi_{\theta}\) for optimization and reference model \(\pi_{\text{ref}}\) derived from the SFT stage.
In this way, LLM can focus on generating tool calls that are more aligned with individual user preferences.
% By directly optimizing for user preferences

\section{Experiments}
\subsection{Setup}
\paragraph{Baselines.} 
% following (Huang et al., 2024a; Zhuang et al., 2023). 
We adopt multiple LLMs from both closed-source and open-source models to ensure a comprehensive evaluation.
For closed-source LLMs, we select two representative models: GPT-4o and GPT-4o-mini from OpenAI.
For open-source LLMs, we include a wide spectrum of models, i.e., LLaMA-3.1-8B~\cite{dubey2024llama}, QWen-2.5-7B~\citep{yang2024qwen2}, Vicuna-7B-v1.5~\cite{chiang2023vicuna} and Mistral-7B-v0.3~\cite{jiang2023mistral}.

\paragraph{Implementation details.} 
% \footnote{\url{https://openai.com/index/introducing-chatgpt-and-whisper-apis/}.} 
% since it its the instruction following .
% The number of candidate tools $N$ is set to $10$, which 
In \benchmark construction, we employ gpt-4o-mini
as the LLM for tool understanding and generation of user instructions and interaction history. 
The candidate tool set consists of three parts: the ground-truth tool along with all other tools sharing the same functionality, five tools retrieved using ToolRetriever~\cite{qin2024toolllm}, and the remaining tools that were randomly sampled.
% The tools retrieved We adopt ToolRetriever~\cite{qin2024toolllm} as the dense retriever which is specifically finetuned on tool retrieval datasets. 


% highlighting several key insights.
\subsection{Main Results}
The detailed experimental results are shown in Table~\ref{main_results}. 
From the results, we can obtain the following key findings.
1) It can be observed that the performance of LLMs is generally unsatisfactory, particularly in tool accuracy with the majority failing to exceed 50\%. This indicates that current LLMs are severely limited in personalized tool-use capabilities. Additionally, the lower tool accuracy compared to parameter accuracy further suggests that personalized tool selection is more challenging than parameter configuration. This is because LLMs must account for both implicit user preferences and explicit user requirements when determining which tool to use.
2) Most LLMs perform worse in the rating-integrated and chronological settings. 
This is likely due to the inclusion of non-preferred interactions in the interaction history, which confuses LLMs and hinders their ability to accurately recognize user preferences. Notably, the chronological setting yields the lowest scores, suggesting that capturing evolving user preferences over time is even more challenging than interpreting explicit user ratings.
3) Our proposed \benchmark significantly outperforms all closed-source and open-source LLMs, demonstrating both effectiveness and robustness. It maintains strong performance, even in the two more challenging settings, by enabling the LLM to better understand diverse manifestations of user preferences and facilitate personalized tool usage.
% with superior
% It can be observed that existing open-source LLMs lay behind closed-source LLMs, i.e., GPT-4o and GPT-4o-mini, across all three scenarios. This is reasonable since GPT-4o is well known for its superior instruction-following and comprehension abilities compared with most open-source LLMs. 

\subsection{Ablation Study}
We conduct ablation studies to investigate the efficacy of the two-stage training process in our \framework.
First, we remove the second training stage (i.e., personalized DPO) to assess its contribution.
Then, we examine the impact of the SFT stage by directly conducting DPO training on the initial LLaMA3-8B model.
Table~\ref{main_results} reports the performance on the test set of \benchmark in all three settings.
The results indicate that the SFT stage is crucial for personalized tool learning performance, as it endows the model with fundamental tool usage and personalization capabilities. Removing the DPO stage results in a slight performance drop, suggesting that it can further refine the tool usage alignment with user preferences.

\subsection{In-depth Analysis}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=1.0\linewidth]{scores_wo_history.pdf}
    \caption{Performance comparison of tool accuracy when provided with and without interaction history.}
    \label{fig:scores_wo_history}
\vspace{-1em}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{scores_length.pdf}
    \caption{Performance comparison of tool accuracy on different interaction history length in the preferred-only setting.}
    \label{fig:scores_length}
\vspace{-1em}
\end{figure}



\input{tabs/error}

\paragraph{Analysis on the impact of interaction history.}
% Since interaction history plays a critical role
To investigate the impact of interaction history on LLM performance, we remove the interaction history from the inputs and provide only the user instructions with candidate tools set to conduct our experiments. The results are presented in Figure~\ref{fig:scores_wo_history}.
From the results, we can observe that both closed-source and open-source LLMs experience varying degrees of performance degradation without interaction history, compared to when provided with preferred-only history. This suggests that interaction history only containing the user's preferred tools can help the LLM effectively infer user preferences. On the other hand, we find that LLMs perform better in the absence of interaction history than with chronological history. This indicates that including both preferred and non-preferred tools can interfere with the LLM's understanding of user preferences, thus hindering its personalization capabilities.
In contrast, our \framework consistently improves performance across all three types of interaction history compared to the no-history setting. This demonstrates that our method enables LLM to effectively recognize different forms of user preferences from the interaction history.


\paragraph{Analysis on interaction history length.}
To evaluate the performance of LLMs under varying interaction history lengths, we break down the tool accuracy scores of LLMs based on the number of interactions in the history under the preferred-only setting.
As shown in Figure~\ref{fig:scores_length}, the performance of both closed-source and open-source LLMs deteriorates as interaction history length increases.
This is because a longer interaction history makes it more challenging for the LLM to identify the historical preferences relevant to identify relevant historical preferences in relation to the user’s current context.
In contrast, our \framework significantly outperforms all LLMs and maintains strong, consistent performance even as interaction history grows. This demonstrates that our method enables LLMs to effectively extract and utilize user preferences from complex historical data.


\subsection{Error Analysis}
We further conduct an error analysis to investigate the issues leading to incorrect tool calls in two personalized settings. 
We categorize the errors into six types:
1) Invalid Format. The tool call generated by the LLMs does not follow the expected JSON format. 
2) Tool Hallucination. The LLM generates a tool that does not exist in the given candidate tool set, which is a common hallucination issue in LLMs.
% ~\cite{huang2024survey}.
3) Tool Functionality Mismatch. The selected tool lacks the necessary functionality to fulfill the user’s requirements.
4) Tool Preference Mismatch. The selected tool has the correct functionality but is not preferred by the user.
5) Parameter Name Mismatch. The tool call contains missing or incorrect parameter names.
6) Parameter Value Mismatch. The parameter names are correctly generated, but the parameter values do not match the ground truth.

From the results in Table~\ref{error_results}, we observe that most LLMs perform worst in Tool Preference Mismatch, particularly in the chronological setting, where the error rate exceeds 50\%. This suggests that identifying user preferences from the interaction history is highly challenging, especially when preferences change over time, leading to significant model misinterpretation. In contrast, our \framework significantly reduces the error rate in Tool Preference Mismatch, demonstrating its effectiveness in capturing implicit user preferences. Additionally, the reduction in Tool Functionality Mismatch and Parameter Value Mismatch errors suggests that our method enhances LLMs' fundamental tool-usage ability, improving their handling of explicit user requirements.
Furthermore, \framework achieves low error rates in Invalid Format and Tool Hallucination, comparable to closed-source LLMs, highlighting its strong instruction-following capabilities.

\section{Conclusion and Future Work}
In this paper, we advanced general-purpose tool-use LLMs into personalized tool-use LLMs, aiming to provide users with customized tool-usage assistance. We formulate the task of personalized tool learning and identify the goal of leveraging user's interaction history to achieve implicit preference understanding and personalized tool calling. 
For training and evaluation, we construct the first \benchmark benchmark, featuring diverse users’ interaction history in three types. 
We also propose a novel personalized framework \framework conducted under a two-stage training process to endow LLMs with personalized tool-use capabilities.
Extensive experiments on \benchmark demonstrate that \framework consistently surpasses existing baselines, effectively meeting user requirements and preferences.
We believe that the task, benchmark, and framework for personalized tool learning will broaden the research scope, introduce new challenges and inspire novel methods. 

In the future, we aim to enhance this work from the following dimensions.
1) We plan to explore more heterogeneous personal user data beyond interaction history, such as user profiles or personas. This will allow us to reflect user preferences from multiple dimensions, providing a more comprehensive evaluation on the personalized tool-use capabilities of LLMs.
2) Currently, our work is limited to tool-usage scenarios involving a single tool. In the future, we intend to expand to more complex personalized tool-usage, such as multi-tool scenarios. These scenarios will require LLMs to perform personalized tool planning and engage in multi-round tool calling to address user needs effectively.

\section*{Limitations}
1) Due to the lack of real user interaction histories on tool usage, we utilize LLM to synthesize such data. However, this approach may compromise the authenticity and reliability of the data, which is a common challenge in data synthesis methods.
To mitigate this issue, we incorporate pre-constructed user preference information into the data generation process. This strategy helps guide LLM in generating contextually relevant outputs, thereby improving the quality and consistency of the synthesized data.
2) In real-world scenarios, tools have multiple dimensions of attributes. However, due to the limited information contained in tool documentation, it is difficult to identify and fully exploit all possible tool attributes. Fortunately, the attributes we have obtained are sufficient to differentiate between tools, enabling us to effectively construct user preferences.

\section*{Ethics Statement}
The dataset used in our work is derived from publicly available sources and generated through interactions with LLMs in English. Since the user interaction histories in our study are entirely simulated, user privacy is fully protected, and no real personal information is included in the dataset. Furthermore, all scientific artifacts used in this research are publicly accessible for academic purposes under permissive licenses, and their use in this paper complies with their intended purposes. Given these considerations, we believe our research adheres to the ethical standards of the conference.

% Taking a query $q$ and a user profile $U$ as input, the model is expected to select a user-specific tool set $D_u=\{d_i\}_{i=1}^k$ from 
% Formally, we design a number of user profiles $\{u_1, u_2, ..., u_m\}$, each of which represents a specific user.
% associate the query $q$ with $u_i$, where $u_i$ is a user-specific profile from a number of users.
% $\{u_1, u_2, ..., d_N\}$
% % ${u_i|i\in (1,M)}$ 
% where $M$ is the number of users.

% Given a user's instruction, tool learning aims to select a small number of tools, which could aid the LLM in answering the instruction, from a large-scale tool set and then .

% Given an input query $q$, tool learning model first selects from a large-scale tool set $D=\{d_1, d_2, ..., d_N\}$ a small number of tools $D(q)=\{d_1, d_2, ..., d_K\}$ which could help solve the query $q$, and then output a final response $r$ based on a tool-use trajectory $T=\{t(d_i)|d_i\in D(q)\}$ containing the tool execution results after calling each selected tools with parameters.


% In contrast, personalized tool learning aims to solve the query given by different users.
% It can be formulated as conditioning the tool learning process on $(q,u)$, where $u$ is a user-specific profile.
%  the model's output on a user

% Personalized tool learning can be formulated as conditioning the model's output on a user- $u$, represented by a user profile.



% In tool learning, a typical data entry consists of three components: an input query $q$, a tool set $D=\{d_1, d_2, ..., d_N\}$ containing $N$ ground-truth tools solving the query $q$,  
% % where $d_i$ represents the description of each tool and $N$ is the total number of tools, 
% and a tool-use trajectory $Traj=$.


% sequence $x$ that serves as the model's input, a target output $y$ that the model is expected to produce, and a profile $P_u$ that encapsulates any auxiliary information that can be used to personalize the model for the user.


% Given a user's query $q$, tool learning aims to select a small number of tools, which could aid the LLM in answering the instruction, from a large-scale tool set.
% Formally, we define the user instruction as $q$ and the tool set as $D=\{d_1, d_2, ..., d_N\}$, where $d_i$ represents the description of each tool and $N$ is the total number of tools.
% The retriever model $R$ needs to measure the relevance $R(q, d_i)$ 
% between the instruction $q$ and each tool description $d_i$, and return $K$ tools, denoted as $D=\{d_1, d_2, ..., d_K\}$.


% Generative language models often take an input $x$ and predict the most probable sequence tokens $y$ that follows $x$. 
% Tool learning can be formulated as xxx
% Personalized tool learning can be formulated as conditioning the model's output on a user $u$, represented by a user profile.



% \section{Introduction}
% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}

\appendix

\section{Details of Benchmark Construction}
We provide the illustration of three types of the user's interaction history in Figure~\ref{fig:history}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig_history.pdf}
    \caption{Illustration of three types of the user's interaction history.}
    \label{fig:history}
\vspace{-1em}
\end{figure}

\section{Implementation details}
To train \framework, we fine-tune the LLaMA-3.1-8B model with LoRA
% () 
and a warm-up ratio of $0.1$ in the SFT stage. 
The learning rate is set to $1e{-4}$ with a batch size of $16$ per GPU. 
In the DPO stage, the learning rate is set to $1e{-6}$ and the balancing factor $\beta$ is set to $0.1$ with a batch size of $32$.
We have trained the model several times to ensure the improvement is not randomly achieved and present the mid one. 
For evaluation, we set the number of candidate tools $N$ to $10$ and the temperature to 0.1 to reduce randomness. 
Since the maximum context length varies in different LLMs, we constrain the context window to 4000 tokens. The experiments on closed-source LLMs are fulfilled by APIs of OpenAI and those on open-source LLMs are conducted on NVIDIA A6000 GPUs with 48 GB of memory. 

\definecolor{lightgray}{RGB}{240, 240, 240}
% \definecolor{lightgray}{gray}{0.95}
\lstdefinestyle{prompt}{
    basicstyle=\ttfamily\fontsize{7pt}{8pt}\selectfont,
    frame=none,
    breaklines=true,
    backgroundcolor=\color{lightgray},
    breakatwhitespace=true,
    breakindent=0pt,
    escapeinside={(*@}{@*)},
    numbers=none,
    numbersep=5pt,
    xleftmargin=5pt,
}
\tcbset{
  aibox/.style={
    % width=220pt,
    % top=10pt,
    % colback=lightgray,
    % colframe=black,
    % colbacktitle=black,
    % enhanced,
    % center,
    % breakable,
    % attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    % boxed title style={boxrule=0pt,colframe=white,},
  }
}
\newtcolorbox{AIbox}[2][]{aibox, title=#2,#1}


\section{Prompt Details}
The prompt templates in for tool-use example generation and tool attributes understanding are shown in Figure~\ref{fig:prompt_example} and Figure~\ref{fig:prompt_attributes}. The prompt templates for interaction history generation across three types are shown in Figure~\ref{fig:prompt_history(p)}, Figure~\ref{fig:prompt_history(r)}, and Figure~\ref{fig:prompt_history(c)}.
The prompt template for instruction generation is shown in Figure~\ref{fig:prompt_instruction}.

\begin{figure*}[!ht] 
\vspace{-5mm}
\begin{AIbox}{Prompt for Tool-use Example Generation}
{\bf Prompt:} \\
{
Given a tool documentation as input, your task is to output an example for using this tool, including a simulated user instruction and parameters for calling the tool. The output example should be in JSON format: \{``instruction'': xx, ``parameters'': xx\}
\clearpage
Here is a demonstration:

Input:
\begin{lstlisting}[style=prompt]
{
    "tool_name": "<Text_Analysis>.<Spellout>.<Languages>",
    "tool_desciption": "List ISO 639 languages",
    "required_parameters": [],
    "optional_parameters": [
        {
            "name": "nameFilter",
            "type": "STRING",
            "description": "Filter as \"contains\" by language name",
            "default": ""
        }
    ]
}
\end{lstlisting}
Output:
\begin{lstlisting}[style=prompt]
{
    "instruction": "I want to filter the list of languages by English",
    "parameters": {
        "nameFilter": "English"
    }
}
\end{lstlisting}
Now you will be given the tool documentation, please generate the tool-use example. 

Begin!
}
\end{AIbox} 
\caption{The prompt for tool-use example generation.}
\label{fig:prompt_example}
\vspace{-5mm}
\end{figure*}



\begin{figure*}[!ht] 
\vspace{-5mm}
\begin{AIbox}{Prompt for Tool Attributes Understanding}
{\bf Prompt:} \\
{
Given a tool documentation and the corresponding tool-use example as input, your task is to understand the tool attributes thoroughly. Then generate two descriptions about the functionality and non-functional attributes of the tool respectively. 
% The functionality refers to the core function that the tool performs in order to fulfill its purpose. 
% Non-functional attributes refer to additional attributes beyond functionality that can reflect different characteristics of the tool and result in different user experience, such as usability, integrability, accessibility, and security.
\clearpage
Here is a demonstration:

Input:
\begin{lstlisting}[style=prompt]
Tool documentation:
{
    "tool_name": "<Commerce>.<Face Compare>.<GET Call>",
    "tool_desciption": "Used to fetch results using the request id received in responses.",
    "required_parameters": [
        {
            "name": "request_id",
            "type": "STRING",
            "description": "",
            "default": "76d1c748-51ed-435b-bcd8-3d9c9d3eb68a"
        }
    ],
Tool-use example:
{
    "instruction": "I want to use the request id '76d1c748-51ed-435b-bcd8-3d9c9d3eb68a' to fetch the result",
    "parameters": {
        "request_id": "76d1c748-51ed-435b-bcd8-3d9c9d3eb68a"
    }
}
\end{lstlisting}
Output:
\begin{lstlisting}[style=prompt]
Functionality: Fetches API results based on the request ID received in previous responses.
Non-functional attributes: Designed for commerce applications, used in face comparison scenarios.
\end{lstlisting}
Now you will be given the tool documentation and the tool-use example, generate two short phrases to describe the two types of attributes. 

Begin!
}
\end{AIbox} 
\caption{The prompt for tool attributes understanding.}
\label{fig:prompt_attributes}
\vspace{-5mm}
\end{figure*}


\begin{figure*}[!ht] 
\vspace{-5mm}
\begin{AIbox}{Prompt for Interaction History (Preferred-only) Generation}
{\bf Prompt:} \\
{
Given a list of tools preferred by a user as input, your task is to simulate the user's interaction history based on these tools. You should output a sequence of tool-usage interactions, each consisting of a simulated user instruction and a tool call to fulfill that instruction. The interaction sequence should be a list in JSON format: 
% \clearpage
% Here is a demonstration:
% Input:
\begin{lstlisting}[style=prompt]
[
    {
        "instruction": xx,
        "tool_call": {
            "tool_name": xx, 
            "parameters": xx
        }
    }, ...
]
\end{lstlisting}
Now you will be given the tools, please generate the interaction sequence. 

Begin!
}
\end{AIbox} 
\caption{The prompt for interaction history (preferred-only) generation.}
\label{fig:prompt_history(p)}
\vspace{-5mm}
\end{figure*}


\begin{figure*}[!ht] 
\vspace{-5mm}
\begin{AIbox}{Prompt for Interaction History (Rating-integrated) Generation}
{\bf Prompt:} \\
{
Given a list of tools preferred by a user and a list of tools not preferred as input, your task is to simulate the user's interaction history based on these two lists. You should output a sequence of tool-usage interactions, each consisting of a simulated user instruction, a tool call to fulfill that instruction, and a binary rating reflecting the user's satisfaction with the tool call. The interaction sequence should be a list in JSON format: 
\begin{lstlisting}[style=prompt]
[
    {
        "instruction": xx,
        "tool_call": {
            "tool_name": xx, 
            "parameters": xx
        },
        "rating": 1 or 0,
    }, ...
]
\end{lstlisting}
Now you will be given the two lists of tools, please generate the interaction sequence. 

Begin!
}
\end{AIbox} 
\caption{The prompt for interaction history (rating-integrated) generation.}
\label{fig:prompt_history(r)}
\vspace{-5mm}
\end{figure*}


\begin{figure*}[!ht] 
\vspace{-5mm}
\begin{AIbox}{Prompt for Interaction History (Chronological) Generation}
{\bf Prompt:} \\
{
Given a list of tools preferred by a user and a list of tools not preferred as input, your task is to simulate the user's interaction history based on these two lists. You should output a sequence of tool-usage interactions, each consisting of a simulated user instruction, a tool call to fulfill that instruction. The interactions should be organized in time order to reflect changes in user preferences over time, i.e., the more recent tool-usage interactions are more preferred by the user, while earlier interactions are less preferred. The interaction sequence should be a list in JSON format: 
\begin{lstlisting}[style=prompt]
[
    {
        "instruction": xx,
        "tool_call": {
            "tool_name": xx, 
            "parameters": xx
        }
    }, ...
]
\end{lstlisting}
Now you will be given the two lists of tools, please generate the interaction sequence. 

Begin!
}
\end{AIbox} 
\caption{The prompt for interaction history (chronological) generation.}
\label{fig:prompt_history(c)}
\vspace{-5mm}
\end{figure*}


\begin{figure*}[!ht] 
% \vspace{-2mm}
\begin{AIbox}{Prompt for Instruction Generation}
{\bf Prompt:} \\
{
Given a user's interaction history and a tool documentation as input, your task is to generate a simulated user instruction which can be fulfilled by calling the tool with parameters. The generated output should be in JSON format: 
\begin{lstlisting}[style=prompt]
{
    "instruction": xx,
    "parameters": xx
}

\end{lstlisting}
Remember, tool name is strictly prohibited from appearing in the generated instruction. 
Now you will be given the user's interaction history and tool documentation, please generate the output. 

Begin!
}
\end{AIbox} 
\caption{The prompt for instruction generation.}
\label{fig:prompt_instruction}
% \vspace{-5mm}
\end{figure*}


\label{sec:appendix}

% This is an appendix.

\end{document}
