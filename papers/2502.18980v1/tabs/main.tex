\begin{table*}[!t]
\centering
\caption{Evaluation results of different LLMs on \benchmark in terms of tool and parmater accuracy under settings including preferred-only, rating-integrated, chronological, and the whole data (\textit{All}). 
Bold highlights the best score among all LLMs and
% , and underline underscores the best score under the same model scale
\% improve represents the relative improvement achieved by our method over the previously best-performing LLM.}
% \small
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}l|cc|cc|cc|cc@{}}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{\textsc{Preferred-only}}} & \multicolumn{2}{c|}{\textbf{\textsc{Rating-integrated}}} & \multicolumn{2}{c|}{\textbf{\textsc{Chronological}}} & \multicolumn{2}{c}{\textbf{\textsc{All}}} \\ 
\cmidrule(lr){2-9}
&Tool Acc  &Param Acc  &Tool Acc &Param Acc  &Tool Acc  &Param Acc  &Tool Acc  &Param Acc  \\ \midrule
Vicuna-7B &$25.50$  &$44.80$  &$10.80$  &$57.40$  &$12.70$  &$56.00$  &$16.33$  &$52.73$  \\
Mistral-7B &$30.30$ &$55.70$  &$15.40$  &$63.20$  &$14.10$  &$64.90$  &$19.93$  &$61.27$ \\ 
Qwen2.5-7B &$40.40$  &$63.80$  &$24.80$  &$66.50$  &$24.80$  &$70.20$  &$30.00$  &$66.83$ \\ 
LLaMA3-8B &$48.10$  &$71.10$  &$26.90$  &$77.70$  &$26.60$  &$78.10$  &$33.87$  &$75.63$  \\
GPT-4o-mini &$51.80$  &$72.90$  &$38.40$  &$77.70$  &$31.20$  &$80.50$  &$40.47$  &$77.03$ \\ 
GPT-4o &$53.70$  &$77.60$  &$45.70$  &$79.60$  &$33.60$  &$81.80$  &$44.33$  &$79.67$ \\ 
\midrule
\textbf{\framework} &\bf 74.30  &\bf 87.90  &\bf 78.40  &\bf 89.70  &\bf 80.80  &\bf 91.30  &\bf 77.83  &\bf 89.63 \\ 
\% improve &38.36\% & 13.27\% &71.55\%  &12.69\%   &140.5\%  &11.61\%  &75.57\%  &12.50\%  \\ 
\textit{ w/o DPO} &71.50 &82.10 &74.20 &86.90 &77.30 &90.40 &74.33 &86.47  \\
\textit{ w/o SFT} &53.20 &61.80 &55.30 &62.40 &51.40 &61.10 &53.30 &61.77  \\
\bottomrule
\end{tabular}} 
\label{main_results}
\end{table*}