\section{Related Work}
\subsection{Tool Learning in LLMs}
Tool learning aims at extending the capabilities of LLMs by equipping them with external tools to solve tasks like weather inquiry, car navigation, and restaurant reservation. Existing benchmarks primarily focus on evaluating the tool learning proficiency of LLMs in addressing user instructions, from aspects such as 
% tool usage awareness**Kim et al., "Tool Usage Awareness"**, 
tool selection and calling accuracy**Zhu et al., "Tool Selection and Calling Accuracy"**,**Wang et al., "Task-Oriented Dialogue Systems"**, tool planning ability**Brown et al., "Language Models as Tools for Abstractive Summarization"**,**Li et al., "Enhancing Language Understanding with Conversational AI"**, and complex workflow creation**Gupta et al., "Automating Document Preparation through Workflow Analysis"**. To improve tool-use capabilities, various strategies have been introduced, including in-context learning which enables LLMs to use tools via documentation**Hsieh et al., "Adversarial Training for Tool-Use Understanding"**, 
% (Hsieh et al., 2023), 
and fine-tuning which trains LLMs on specialized tool-use datasets**Rajani et al., "Fine-Tuning Large Language Models with Weak Supervision"**. However, prior studies neglect the crucial role of personalized tool usage in LLMs. This paper addresses this gap by introducing personalized tool learning, developing a comprehensive benchmark for evaluation, and proposing an optimization strategy to enhance personalized tool-use capabilities in LLMs.

\subsection{Personalization in LLMs}
The goal of personalization in LLMs is to leverage personal user data, such as historical behaviors and background information, to generate outputs that better align with the user preferences**Chen et al., "Personalized Dialogue Systems for Recommendation"**, 
% (Chen et al., 2023e; Deshpande et al., 2024). 
Approaches such as fine-tuning**Huang et al., "Fine-Tuning Language Models for Personalized Dialogue Generation"** and prompt engineering**Xu et al., "Prompt Engineering for Effective Dialogue Generation"**
% , and user-specific embeddings 
have been explored to adapt LLMs to individual or domain-specific tasks. These approaches have been applied across various fields, including recommendation systems**Li et al., "Personalized Recommendation with Large Language Models"**, search engines**Wang et al., "Search Engine Personalization using Large Language Models"**, education**Kumar et al., "Personalized Education Systems using Large Language Models"**, 
% healthcare, 
and dialogue generation**Chen et al., "Personalized Dialogue Generation for Healthcare"**. However, previous research has not investigated LLMs' personalization in the area of tool learning. In this work, we bridge this gap by incorporating user's interaction history to assess and enhance the LLMs' capability in providing personalized tool-usage assistance for specific users.
% generating tool calls tailored to user-specific needs.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{fig_benchmark.pdf}
    % \caption{Illustration of our \benchmark.}
    \caption{Illustration of the process for constructing our \benchmark.}
    \label{fig:benchmark}
\vspace{-1em}
\end{figure*}

% Due to the lack of real personal data on tool-usage, we adopt a tool-driven approach to simulate user data by leveraging tool attributes to construct user preferences and interaction history. 
% real-world tools to simulate user data, distinguishing user preferences by tool attributes. 
% (1) Tool Preparation.
% We collect a bunch of high-quality tools from RapidAPI.
% % as our seed tools. 
% Then we leverage LLM to understand the functionality and non-functional attributes of each tool. 
% (2) Preference Construction. 
% % We leverage the obtained tool attributes to construct the user preferences on tools.
% We classify tools with the same functionalities into groups. 
% Within each functionality group, we construct the user preferences by
% % we identify the preferred and non-preferred tools of a specific user within the functionality group.
% randomly assign a tool with distinct non-functional attributes as the user's preferred tool and other tools as non-preferred tools.
% % for a specific user. 
% To enrich the user preference, we use the preferred tool to retrieve more tools with similar non-functional attributes.
% % The preferred tool will also be used to retrieve more tools with similar non-functional attributes to be the user's preferred tool .
% In this way, we can obtain a bunch of preferred and non-preferred tools representing different user preferences.
% % After multiple functionality groups, we can obtain a number of preferred and non-preferred tools representing .
% % all serving as preferred tools for the specific user.  
% (3) Data Creation. 
% Based on the user preference, we generate the user's interaction history as a sequence of tool-use interactions, each containing a user instruction and a corresponding tool call by LLM. The interaction history is constructed in three types: 1) \textit{preferred-only} history, which only includes the user's preferred tools ; 2) \textit{rating-integrated} history, integrating user's binary ratings on each tool call according to user preferences; and 3) \textit{chronological} history, where interactions are arranged in time order to capture changes in user preferences over time. 
% % both preferred and non-preferred tools with binary ratings reflecting the user's preference; 
% % Finally, we select tools 
% Finally, we generate user instructions based on the user's preferred tools not included in the interaction history.
% % as ground truth tools to generate user instructions for each data instance. 
% % Each instruction is combined with the interaction history into a data instance. construct data instance