\section{Related works}

Explicit 3D representations such as points~\cite{achlioptas2018learning}, voxels~\cite{tatarchenko2017octree}, meshes~\cite{fu2024lfs}, and polygonal surfaces~\cite{Nan_2017_ICCV} are commonly used for visualization and reconstruction~\cite{kazhdan2013screened}.
However, the discrete structures of these representations are challenging to adapt for learning-based approaches~\cite{takikawa2021neural,peng2020convoccnet} which rely on differentiability.
As a result, implicit 3D representations via neural networks have gained popularity ~\cite{park2019deepsdf} as they can be converted to an explicit model after training by techniques like Marching Cubes~\cite{lorensen1998marching}.
This paper addresses the task of reconstructing neural implicit surface representations from point clouds.


\paragraph{Neural implicit surface reconstruction}
Neural implicit methods utilize neural networks to model an occupancy field~\cite{mescheder2019occupancy, ouasfi2024unsupervised} or a distance field~\cite{park2019deepsdf, huang2022neural} for surface reconstruction.
Distance fields, including signed distance fields~(SDF)~\cite{park2019deepsdf,koneputugodage2024small,BaoruiTowards} and unsigned distance fields~(UDF)~\cite{chibane2020ndf,wang2022rangeudf}, are functions whose zero level set implicitly defines the object surface.
A learnt SDF predicts a query point's signed distance to the nearest surface, with a negative value indicating the point is inside the surface and a positive value indicating it is outside~\cite{takikawa2021neural}.
\quad
To encode unstructured point clouds into neural fields, various network architectures have been proposed, such as  
\input{figs/framework/item}
MLPs~\cite{Chen_2019_CVPR,mescheder2019occupancy,park2019deepsdf}, infinitely-wide-ReLU networks~\cite{williams2021neural}, PointNet~\cite{williams2022neural,tang2021SACon}, 3D-UNet~\cite{wang2023alto}, RandLA-Net~\cite{wang2022rangeudf}, sparse hierarchical networks~\cite{huang2023neural}, and MinkowskiNet~\cite{neuraludf}.
Compared to encoding the point cloud into a global feature~\cite{park2019deepsdf,Points2Surf}, organizing point clouds into regular or irregular grids or voxels for feature learning and spatial querying preserves more details~\cite{li2022learning,zhang20223dilg,zhong20243d,peng2020convoccnet,williams2022neural,li2024gridformer}.
For example, \cite{takikawa2021neural} uses a voxel octree to collect point-wise features, and retrieves the query point feature via trilinear interpolation at each tree level.
Alternatively, feature interpolation can be implemented with an attention-based~\cite{wang2023alto} or learning-based approach~\cite{boulch2022poco}.
Such data-driven methods often struggle to guarantee the accuracy of learned surfaces, and are difficult to scale and generalize~\cite{huang2022neural,williams2022neural}.
\citet{huang2023neural} addresses this problem by solving complex kernel functions on hierarchical voxels.
However, the quantization inherent to voxels or grids leads to information loss, and the solver increases the reconstruction time quadratically with the number of grid cells.
To solve these problems, we propose a point-based framework powered by a serialization encoding for implicit surface reconstruction.

\paragraph{Efficient point cloud networks}
Point-based networks~\cite{PointNet,hu2019randla} \at{achieve good performance on small datasets, but in applications to large point cloud data their message-passing strategy is not sufficiently computationally efficient.}
Sparse convolution networks~\cite{choy20194d,graham20183d} based on voxelization are fast but suffer from information loss. 
The recently proposed OctFormer~\cite{wang2023octformer} and PointTransformerV3~\cite{wu2024point} provide a superior combination of efficiency and encoding performance by leveraging a serialization-based strategy, and our method builds upon these approaches.

\paragraph{Point cloud serialization}
Backbone networks that rely on voxelization (e.g., MinkowskiNet~\cite{choy20194d} and sub-manifold sparse U-Net~\cite{graham20183d}) suffer of high computational cost, and from information loss due to quantization.
To avoid these limitations, point cloud serialization methods encode irregular point clouds into sequential structures with the use of space-filling curves.
This bijective encoding scheme excels at dimension reduction, preserving topology and retaining locality, making it a promising approach to address voxelization issues~\cite{wang2005space, wang2017cnn}.
\citet{chen2022efficient} leverages the Hilbert curve~\cite{hilbert1935stetige} to map voxels into an ordered sequence, enabling the use of 2D convolution and Transformers on 3D voxels.
\citet{wang2023octformer}, by employing z-order curves~\cite{morton1966computer} to sort octree nodes, achieves equal-sized point partitions and constructs an effective octree attention module for point clouds.
The idea of equal-sized sorting of windows is also adopted in~\cite{liu2023flatformer}.
To mitigate the computational overhead of K-Nearest Neighbor (KNN), \citet{wu2024point} integrates z-order and Hilbert curves to map 3D points into structured sequences and patches, upon which attention layers are constructed.
\citet{zhang2024voxel} introduces a Hilbert input layer for serializing 3D voxels, laying the foundation for a voxel-based state space model designed for 3D object detection.
This approach eliminates the need for 1D sequence grouping and padding.
\quad 
In our work, we present a point feature retrieval algorithm that operates directly on points based on point cloud serialization, and demonstrate its performance for neural surface reconstruction applications.











