\section{Introduction}
\label{sec:intro}

Reconstructing the surface sampled by a point cloud is a fundamental problem with many applications in robotics~\cite{tong2023scene}, autonomous driving~\cite{Autodriving}, and virtual reality~\cite{zhuang2024survey,guo2024fast}.
We tackle this task by predicting the \textit{signed distance field}~(SDF) associated with a given point cloud: a function that returns the signed distance to the nearest surface for any given 3D position.
Given the distance field, the surface can be extracted by finding the zero-crossings of the distance function.

State-of-the-art approaches such as NKSR~\cite{huang2023neural} and NeuralUDF~\cite{neuraludf} train a point cloud backbone to predict the distance value for any position in space.
Their backbones are trained on a collection of scenes, so as to capture the priors within the data that allows reconstruction to be performed even when the problem is \textit{ill-posed} (e.g., when the point cloud is sparse and/or incomplete).

\input{figs/teaser/teaser}


The core operation within these backbones is to predict a feature that \textit{aggregates} the information of input points near the query, which is then decoded to an SDF value.
In state-of-the-art models, these aggregation operations are realized by implementing multi-scale sparse convolutional neural networks~\cite{wang2023octformer,choy20194d}.
To be able to scale to large-scale point clouds, these backbones require voxelizing the input point cloud, and summarizing the information of points therein: a spatial \textit{quantization} operation that inevitably leads to information loss.
This quantization operation is detrimental when real-world point clouds are used, as the non-uniformity of sampling leads to performance degradation.


Rather than relying on spatial quantization and sparse-CNNs, we build upon PointTransformerV3~\cite{wu2024point}, and aggregate information by relying on locality-preserving serialization: we serialize the input point cloud to an ordered list, so that nearby points in the list are in close Euclidean proximity; see \Cref{fig:teaser}.
The serialization transformation \textit{does not} incur information loss due to quantization, and it offers superior computational efficiency in terms of feature aggregation compared to methods based on voxelization.

With serialization, retrieving the local neighbors to aggregate our features can result in \textit{false negatives}: points can be close in Euclidean space, but far in their serialized index.
To circumvent this problem, we retrieve features from approximate nearest neighbors \textit{across} several serialization levels, as provided by~\cite{wu2024point}, and then aggregate them with a PointNet architecture~\cite{PointNet} to predict the signed distance function.

Compared to state-of-the-art techniques, our framework requires neither \at{heavily engineered sparse processing backbones}~\cite{neuraludf}, nor differentiating through linear systems of equations~\cite{williams2022neural, huang2023neural}.
Nonetheless, this simple framework outperforms the state-of-the-art in \textit{both} time efficiency and reconstruction quality on \textit{multiple datasets} including ScanNet~\cite{dai2017scannet}, SceneNN~\cite{hua2016scenenn}, Carla~\cite{huang2023neural, dosovitskiy2017carla}, SyntheticRoom~\cite{peng2020convoccnet}.

Given the dominance of voxel-based data structures in surface reconstruction from point clouds, we demonstrate that carefully designed \textit{point-based} architectures can also be highly effective for this task, and we hope this will inspire \textit{renewed} interest in the research area.









