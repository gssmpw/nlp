\appendix
\clearpage
\setcounter{page}{1}
\maketitlesupplementary

This appendix provides additional ablation studies, experimental analyses and more qualitative results.
\section{Additional ablation studies}

\paragraph{Extending~\Cref{tab:agg_capacity} with more levels -- \Cref{tab:agg_capacity_supp}} 
A large non-linear aggregation module is essential to our method due to the false negatives in the fast approximate neighbors. However, more layers in the aggregation module degrades time efficiency. We show that $\aggregation$ with 2 non-linear layers suffices to achieve the best trade-off between accuracy and time efficiency.

\input{tbl/agg_capacity_supp}
\paragraph{Different ways to fuse per-scale features -- \Cref{tab:multi_level_agg}} 
We show the different ways to fuse the per-scale features and observe that they have similar accuracy. Attentive pooling achieves slightly better performance at the cost of degraded time efficiency. 
Note that we have an additional linear layer to predict the attention from the  concatenated features of all levels to perform the attentive pooling.
To realize a learnable gate where we multiply per-level weights with features before fusing levels, we train an additional learnable per-level weight followed by a Sigmoid function for the multiplication.
\input{tbl/multi_level_agg}
\section{More Experimental Analysis}
\input{tbl/training_speed_memory}
\paragraph{Overhead during training -- \Cref{tab:training_speed_memory}} 
We report our method's overhead during training in terms of GPU peak memory and latency required per each training iteration.   
Additionally, we profile training overhead (GPU peak memory and latency per iteration) on a single NVIDIA A6000 Ada with the PyTorch Lighting API. In all cases, we use a batch size of 1 for a fair comparison with the SOTA~\cite{huang2023neural} that has the batch size of 1 in one backward pass.  

\input{tbl/detailed_time}
\paragraph{Latency distribution in the steps of inference  -- \Cref{tab:detailed_time}} 
We show that our method achieves better time efficiency than the SOTA~\cite{huang2023neural} in all different steps whilst having better accuracy, even without the time-consuming decoder as in SOTA.

\input{figs/supplementary/time_efficiency_plot}
\paragraph{The impact of point cloud size on time efficiency of KNN vs serialization encoding -- \Cref{fig:knn_serialization_npts}}
We report how the point cloud size impacts the time efficiency of KNN and neighbors based the serialization encoding. Theoretically, serialization encoding should be more efficient. However, we observe that when the point cloud size is small such as \synthetic and \scannet, KNN is more efficient then serialization encoding. We suspect this is because KNN is highly engineered, with a CUDA implementation while the serialization encoding is purely implemented in python.  To estimate the time efficiency, we randomly generate 25000 query points and record the execution times of methods based on KNN and serialization encoding across varying numbers of input points.

\paragraph{More metrics: completeness and accuracy -- \Cref{tab:across_domain_supp} and \Cref{tab:indomain_supp}}
Following the state of the art method by~\citet{huang2023neural}, we further report additional metrics below.
We observe that the performance is consistent with other metrics we report in main paper.   

\input{tbl/segment_supp}
\paragraph{{Handling infinitely large scenes} --
\Cref{tab:segment_supp}}
\ws{We show that our method is capable of handling the infinitely large scenes. 
With serialization codes, we partition a large scene into segments and extract feature of segments individually, avoiding exploding the GPU memory. 
Note the partition stops message passing between segments, which harms the reconstruction accuracy. Even though, as shown in \Cref{tab:segment_supp}, our method achieves the good trade-off between reconstruction quality and the peak memory usage.}


\input{tbl/laplacian_supp}
\input{figs/supplementary/laplacian_carla_supp}
\paragraph{\ws{Smoother surfaces with Laplacian loss} --
\Cref{tab:laplacian_supp} and \Cref{fig:laplacian_carla_supp}}
\ws{We show that our method achieves smoother surfaces by regularizing the distance field $\distancefield$ with Laplacian loss from~\cite{benshabat2023digsdivergenceguided}. We define the loss as 
\begin{align}
    \loss{Laplacian} = \expect_{\x \sim \mathcal{Q}} \left[ \nabla^2 \distancefield(\x) \right].
\end{align}
As shown in \Cref{fig:laplacian_carla_supp}, the larger weight of $\loss{Laplacian}$ leads to the smoother surface. However, as a downside, the reconstruction accuracy is degraded as shown in \Cref{tab:laplacian_supp}. Nevertheless, with the weight of \(1 \times 10^{-4}\), our method achieves the better reconstruction accuracy than \nksr while having the similar surface smoothness. 
}

\input{tbl/shapenet_supp}
\paragraph{\ws{Performance on synthetic object-level dataset} --
\Cref{tab:shapenet_supp}}
\ws{We evaluate the reconstruction quality on ShapeNet\cite{chang2015shapenet}, a synthetic object-level dataset.
Note we use the data prepared by \nksr, and the smaller grid size (0.005) during serialization to avoid collisions.
As shown in \Cref{tab:shapenet_supp}, our method outperforms voxel-based methods, while performs worse than~\nksr.
We suspect that the ``voxel-growing'' strategy in \nksr is crucial to the synthetic object-level dataset, and we leave the integration of this strategy into our method for future work.}


\input{tbl/additional_metrics}
\section{More qualitative results}
We provide more qualitative results in \Cref{fig:carla_additional_1}, \Cref{fig:scannet_additional_1} and \Cref{fig:synthetic_additional_1}.
\input{figs/supplementary/carla_supp}
\input{figs/supplementary/scannet_supp}
\input{figs/supplementary/synthetic_supp}
















