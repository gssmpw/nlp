\section{Results}
\label{sec:results}
Following \nksr, we evaluate our method using metrics including the standard Chamfer-$L_1$ Distance~(CD-$L_1 \times 10^{-2}$, $\downarrow$) and F-score~($\uparrow$) with a threshold~($\delta{=}0.010$). 
We also report additional metrics proposed in \nksr~including Chamfer-$L_1$ Distance by Completeness (Comp.~$\times 10^{-2}$, $\downarrow$) and Accuracy (Acc.~$\times 10^{-2}$, $\downarrow$) in the \texttt{Supplementary Material}. 
We evaluate our method on multiple datasets, under two settings including in-domain evaluation for accuracy estimation -- training set and test set are from same dataset, and cross-domain evaluation for generalization ability estimation where training set and test set are from different datasets. 
Additionally, for cross-domain evaluation we use the following datasets prepared by the leading voxel-based baseline, \nksr, and one additional dataset from RangeUDF~\cite{wang2022rangeudf}:

\begin{itemize}
    \item \synthetic{}  is a synthetic dataset created from ShapeNet objects~\cite{chang2015shapenet}. Each scene contains 2-3 objects. 
    Following prior works~\cite{wang2022rangeudf,chibane2020ndf}, we re-scale the synthetic rooms to roughly match real-world scale.
    There are 3750 scenes as training set and \ws{995 scenes} as the test set. 
    \item \scannet{} is a real-world indoor scene dataset. We use the setting from previous work~\cite{wang2022rangeudf, tang2021SACon, peng2020convoccnet, boulch2022poco} where we train on 1201 rooms and test on 312 rooms. 
    \item \carla is a large-scale outdoor driving scene prepared by NKSR~\cite{huang2023neural} using the CARLA simulator~\cite{dosovitskiy2017carla}. 
    \ws{Following NSKR~\cite{huang2023neural}, we test on two subsets including the 'Original' subset (10 random drives simulated on 3 towns) and the 'Novel' subset (3 drives from an additional town only for testing).}
    To avoid exploding GPU memory during training, we follow NKSR~\cite{huang2023neural} to divide a large scene into patches. The resultant training set has {3757} patches. 
    \item \scenenn{}  is a real-world indoor dataset prepared by RangeUDF~\cite{wang2022rangeudf} which we used for cross-domain evaluation. We only use its test set which consists of 20 scenes.
\end{itemize}



\input{tbl/in_domain}

\paragraph{Evaluation pipeline}
To evaluate our method, we first extract the mesh with Dual Marching Cubes~\cite{schaefer2004dual} on the predicted SDF, and then compute the CD and F-score between 100k points sampled on the mesh, and 100k points sampled from the ground-truth dense point cloud.
We use the same approach as \nksr to prepare the input point clouds for training and evaluation from the ground-truth dense point clouds through downsampling.
Specifically, for indoor datasets (i.e., \synthetic, 
\scannet and \scenenn), we uniformly sample 10K points sampled from the ground truth dense point cloud. 
For outdoor driving scenes~(i.e., \carla), we follow the evaluation pipeline from \nksr.
We sample sparse input point clouds with a sparse 32-beam LiDAR with a ray distance noise of 0-5 cm and pose noise of $0-3^\circ$, and obtain the ground truth from a noise-free dense 256-beam LiDAR.

\input{visualizations/qual_results} 
\input{visualizations/scannet_results}  

\paragraph{Implementation details}
We base our feature backbone on PointTransformerV3~\cite{wu2024point} with 4-levels.
The PointNet-style network is a 2-layered residual connection MLP, with hidden dimension of $32$ and output feature dimension of $32$.    
The grid size used in neighborhood function is $0.01$ meters.
Following \nksr, we use the similar coefficients for loss terms -- i.e., $\lambda_{\text{SDF}}$ is $300$ and $\lambda_{\text{mask}}$ is $150$.
However, we empirically set $\lambda_{\text{Eikonal}}$ to $10$~(\nksr does not need this regularizer thanks to its specialized surface solver).
We train our model with a batch size of $4$ on either a single \texttt{NVIDIA RTX A6000 ADA} or an \texttt{NVIDIA L40S}, and a learning rate of $10^{-3}$.
We adopt the Adam optimizer with default parameters.
We set the maximum number of epochs to 200 and employ a cosine learning rate decay starting from epoch 120.


\input{tbl/across_domain}

\paragraph{Reconstruction latency}
For both our models and NKSR, we record the reconstruction latency for all indoor scenes on a single \texttt{NVIDIA RTX 3090}, and for large outdoor scenes on a single \texttt{NVIDIA L40s} given that more GPU memory is required.
We omit data loading time, and only record the average forward pass time. 

\subsection{In-domain evaluation}
We compare against \nksr~(the current state-of-the-art), RangeUDF~\cite{wang2022rangeudf},  SPSR~\cite{kazhdan2013screened}, NDF~\cite{chibane2020ndf}, ConvOcc~\cite{peng2020convoccnet} and SA-CONet~\cite{tang2021SACon}.     
We further include a baseline that replaces our backbone with MinkowskiNet~\cite{choy20194d} (i.e., Ours~(Minkowski)) to show the degraded performance due to the information loss caused by voxelization.

\paragraph{Quantitative results -- \Cref{tab:indomain}}
Across indoor and outdoor datasets, our method outperforms baselines in terms of accuracy and time efficiency. Especially in outdoor datasets, our method achieves the best surface reconstruction with the smallest latency -- nearly \textit{half} of the second best's latency.
In indoor datasets, which have relatively uniform sampling patterns, we achieve accuracy on par with the previous state-of-the-art, but with significantly improved time efficiency.
Note that we achieve this advantage even with KNN because, in smaller indoor point clouds, the highly engineered KNN implementation has similar time efficiency to that of our neighborhood function.
We further detail our analysis on this matter in the \texttt{Supplementary Material}. 
We also note that our approximate neighborhood function is still effective, as it outperforms the directly comparable baseline MinkowskiNet~\cite{choy20194d}, which shares the same structure except for the backbone and neighborhood function.


\paragraph{Qualitative results -- \Cref{fig:qual_results_carla_syn,fig:scannet_results}}
We show that our method tends to reconstruct surfaces of the best quality among the compared methods.
Especially, on the non-uniform large scale \carla, our method tends to preserve more details than the previous state-of-the-art~\cite{huang2023neural}, which voxelizes the point cloud.   

\subsection{Cross-domain evaluation -- \Cref{tab:across_domain}}
We further test the generalization ability of our method with a cross-domain evaluation.
We evaluate models trained with dataset A on other a different dataset B; we denote this as~A $\rightarrow$ B. 
As shown in \Cref{tab:across_domain}, there are three cases in total.
In two cases (i.e., \synthetic $\rightarrow$ \scannet and \scannet $\rightarrow$ \synthetic), our method achieves the best accuracy with the best time efficiency. 
In another case (\scannet $\rightarrow$ \scenenn), we achieve accuracy on par with SOTA~\cite{huang2023neural} with a much better time efficiency, i.e., less than a half of the latency required by the SOTA~\cite{huang2023neural}.

\subsection{Ablation studies}
Our ablations are executed on \scannet, as it is a real-world dataset, and is equipped with precise ground truth surface meshes.

\input{tbl/numpts_neighbor}
\paragraph{Impact of neighborhood size -- \Cref{tab:numpts_neighbor}}
We analyze the impact of neighborhood size on performance. Larger neighborhood size leads to increased computation overhead. 
We show that the 8-nearest neighboring points gives the best trade-off between accuracy and time efficiency.
Considering a large number (e.g., 16) of neighboring points degrades performance as the the aggregation module has limited capacity to predict the precise SDF from a large local point cloud.

\input{tbl/agg_capacity}
\paragraph{Impact of capacity of $\aggregation$ -- \Cref{tab:agg_capacity}} 
We report how the capacity of the aggregation module $\aggregation$ (i.e., different number of hidden layers) impacts the performance.
We observe that aggregation modules of higher capacity give better performance but degraded time efficiency. However, as shown in~\Cref{tab:agg_capacity}, a very large capacity (4 layers) for $\aggregation$ does not help.
We show that we we use 2 layers to have a good trade-off between accuracy and time efficiency. 
We supplement~\Cref{tab:agg_capacity} with an analysis across even more levels in the \texttt{Supplementary Material}.

\input{tbl/locality_neighbor}
\paragraph{Analysis of neighbors retrieved by~$\neighbor$ -- \Cref{tab:locality_neighbor}}
\at{We now investigate the quality of the point neighborhoods retrieved by various possible implementations for $\neighbor$.
In particular, we are interested to experimentally study whether our serialization indeed preserves locality.
To quantify this, we treat the neighborhood retrieved with KNN as the ground-truth.}
We report the recall rate of a local neighborhood by comparing it with this ground truth~(we ignore the precision rate because we remove false positives with a distance threshold).
We also report the recall rate of the one-ring neighborhood retrieved in Minkowski~\cite{choy20194d}.
We show that the recall rate of our Hilbert $\neighbor$ is the best across variants, and across all scales.

\input{tbl/nonuniform_scannet}
\paragraph{The impact of sampling pattern --~\Cref{tab:nonuniform_scannet}} 
We report the impact of sampling pattern on performance by evaluating models on ScanNet point clouds that are uniformly or non-uniformly sampled. 
{To non-uniformly sample the ScanNet point clouds, we first partitioned the scene into eight blocks and randomly sampled a different number of points from each block. The number of samples followed an arithmetic sequence with a common difference of 200. Finally, we padded the last block to ensure that the total number of points remained 10K.}
 
We show that our method achieves better robustness to non-uniform sampling than the baselines, highlighting the importance of avoiding quantization of the point cloud for high quality surface reconstruction. 

