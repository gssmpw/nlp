\begin{table}
\centering
\resizebox{.95\columnwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\makecell{\bf Method} & CD ($10^{-2}$) $\downarrow$ & Peak Memory $(GB)$ $\downarrow$ & Latency/Iter. (s) $\downarrow$ \\ \midrule
\nksr & 0.246 & 41.3 & 1.44\\
\rowcolor{1st}Ours & 0.257 & 4.6 & 0.59 \\
Ours(w/KNN) & 0.243 & 8.7 & 0.64 \\
Ours (Minkowski) & 0.301 & 3.4 & 0.27 \\
\bottomrule
\end{tabular}
}
\caption{
{\bf Overhead during training} We report the overhead during training in terms of GPU peak memory and latency required for each training iteration. We show that our method achieves more efficient training than the current SOTA~\cite{huang2023neural}.}
\label{tab:training_speed_memory}
\end{table}

