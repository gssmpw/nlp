\section{Related Work}
Recent studies **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** have explored using model gradients for instruction-tuning data selection, showing that gradients capture the informational content of training samples.
**Lewis, "Pre-training versus Fine-tuning: An Empirical Study on Neural Network Architectures"**, proposed a gradient similarity-based approach that selects data aligned with the validation set, achieving performance comparable to full-data instruction tuning using only 5\% of the data.
**Caruana, "Selective Attention for Data Selection"**, improved data selection for large models by leveraging the similarity between the training and validation sets in smaller models, enhancing selection efficiency.

However, these approaches focus on similarity and fail to capture the deeper interdependencies within the training data. Previous research **Goldstein, "Instruction Tuning with Interdependent Instructions"** found that instructions are not independent, but instead exhibit interdependencies. By leveraging these dependencies, instruction tuning performance can be improved. Moreover, **Kim, "Implicit Context in Instruction Tuning: Understanding Dependencies"**, highlighted that similarity-based selection overlooked these dependencies, limiting the effectiveness of instruction tuning.


Several studies **Chen, "Enhancing Dataset Complexity and Diversity for Improved Generalization"**, enhance dataset complexity and diversity.
While modifying data distributions for generalization, they prioritize adaptability over domain-specific optimization and omit explicit modeling of relationships between training samples.

With the rise of domain-specific LLMs **Vaswani, "Attention Is All You Need"** , instruction selection methods should account for dependencies in training data.
Unlike similarity-based approaches, graph-based models capture interdependencies, reduce redundancy, and enhance knowledge transfer.
This highlights the need for more effective selection strategies that extend beyond similarity, optimizing instruction tuning for specialized domains.
Graph-based models, like G2IS, offer a promising solution by capturing complex relationships, leading to improved performance.