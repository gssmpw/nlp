\section{Related Work}
Recent studies____ have explored using model gradients for instruction-tuning data selection, showing that gradients capture the informational content of training samples.
____ proposed a gradient similarity-based approach that selects data aligned with the validation set, achieving performance comparable to full-data instruction tuning using only 5\% of the data.
____ improved data selection for large models by leveraging the similarity between the training and validation sets in smaller models, enhancing selection efficiency.

However, these approaches focus on similarity and fail to capture the deeper interdependencies within the training data. Previous research____ found that instructions are not independent, but instead exhibit interdependencies. By leveraging these dependencies, instruction tuning performance can be improved. Moreover,____ highlighted that similarity-based selection overlooked these dependencies, limiting the effectiveness of instruction tuning.


Several studies____ enhance dataset complexity and diversity.
While modifying data distributions for generalization, they prioritize adaptability over domain-specific optimization and omit explicit modeling of relationships between training samples.

With the rise of domain-specific LLMs____, instruction selection methods should account for dependencies in training data.
Unlike similarity-based approaches, graph-based models capture interdependencies, reduce redundancy, and enhance knowledge transfer.
This highlights the need for more effective selection strategies that extend beyond similarity, optimizing instruction tuning for specialized domains.
Graph-based models, like G2IS, offer a promising solution by capturing complex relationships, leading to improved performance.