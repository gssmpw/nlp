\section{Related Works}
\subsubsection{Viusal-language models.}Foundational models trained on vast amounts of data can acquire broad and transferable representations of various data types, such as images and languages, making them applicable to a variety of downstream tasks ____. As type of foundation models, visual-language models (VLM) integrate linguistic and visual signals, playing a crucial role in fields that require processing of both modalities ____. There are diverse applications of VLM in reinforcement learning (RL). For instance, VLM can be used as reward functions ____, promptable representation learners ____, and for data augmentation based on hindsight relabeling ____. The applications demonstrate wide applicability and robust capabilities of VLM, providing effective tools and new perspectives for addressing complex RL challenges.

\subsubsection{VLM as RL rewards.}Currently, a promising application direction is using VLM to generate dense rewards for RL tasks, especially in scenarios with sparse rewards ____. Cui et al. ____ utilize the pretrained CLIP to provide image-based rewards for robotic manipulation tasks. Mahmoudieh et al. ____ successfully apply the fine-tuned CLIP as a language-described reward model for robotic tasks. Sontakke et al. ____ use VLM in a robotic environment to provide reward signals for RL agents, primarily defining rewards through video demonstrations. However, existing methods primarily focus on solving specific single-agent tasks, where the design of the reward function tends to be relatively simple and direct. Moreover, these methods are mostly implemented in vision-based environments. In contrast, multi-agent systems involve more complex interactions and collaboration mechanisms, making dense reward guidance more challenging.

\subsubsection{Potential-based rewards.}In MARL tasks with sparse rewards, the introduction of additional dense rewards to represent domain-specific expert knowledge has become a common practice to accelerate training ____. However, it soon became apparent that if used improperly, the dense rewards might alter the optimal response, which directly affects the performance of the policies ____.Ng et al. ____ demonstrate that the optimal policy do not change if the dense reward is designed as $F(s,s')=\gamma\phi(s')-\phi(s)$. Devlin et al. ____ further prove that time-varying potential-based rewards $F(s,t,s',t')=\gamma\phi(s',t')-\phi(s,t)$ do not alter the Nash equilibrium in multi-agent problems. Grzes ____ extend the proof to episodic tasks that terminate after a certain final time step $N$. Chen et al. ____ design potential-based rewards in GRF to be positively correlated with the distance between the ball and the goal. Zhang et al. ____ design the rewards that are proportionally linked to the remaining health of a unit in StarCraft II ____. However, existing methods are often based on simple formulas or rules, making it difficult to comprehensively represent human common sense.