\section{Related Works}
\subsubsection{Viusal-language models.}Foundational models trained on vast amounts of data can acquire broad and transferable representations of various data types, such as images and languages, making them applicable to a variety of downstream tasks \cite{wei2022emergent}. As type of foundation models, visual-language models (VLM) integrate linguistic and visual signals, playing a crucial role in fields that require processing of both modalities \cite{wang2021simvlm,ju2022prompting}. There are diverse applications of VLM in reinforcement learning (RL). For instance, VLM can be used as reward functions \cite{dang2023clip}, promptable representation learners \cite{chen2024vision}, and for data augmentation based on hindsight relabeling \cite{sumers2023distilling}. The applications demonstrate wide applicability and robust capabilities of VLM, providing effective tools and new perspectives for addressing complex RL challenges.

\subsubsection{VLM as RL rewards.}Currently, a promising application direction is using VLM to generate dense rewards for RL tasks, especially in scenarios with sparse rewards \cite{fu2024furl,rocamonde2023vision,wang2024rl}. Cui et al. \cite{cui2022can} utilize the pretrained CLIP to provide image-based rewards for robotic manipulation tasks. Mahmoudieh et al. \cite{mahmoudieh2022zero} successfully apply the fine-tuned CLIP as a language-described reward model for robotic tasks. Sontakke et al. \cite{sontakke2024roboclip} use VLM in a robotic environment to provide reward signals for RL agents, primarily defining rewards through video demonstrations. However, existing methods primarily focus on solving specific single-agent tasks, where the design of the reward function tends to be relatively simple and direct. Moreover, these methods are mostly implemented in vision-based environments. In contrast, multi-agent systems involve more complex interactions and collaboration mechanisms, making dense reward guidance more challenging.

\subsubsection{Potential-based rewards.}In MARL tasks with sparse rewards, the introduction of additional dense rewards to represent domain-specific expert knowledge has become a common practice to accelerate training \cite{mguni2021ligs,ma2022elign}. However, it soon became apparent that if used improperly, the dense rewards might alter the optimal response, which directly affects the performance of the policies \cite{csimcsek2006intrinsic}.Ng et al. \cite{ng1999policy} demonstrate that the optimal policy do not change if the dense reward is designed as $F(s,s')=\gamma\phi(s')-\phi(s)$. Devlin et al. \cite{devlin2012dynamic} further prove that time-varying potential-based rewards $F(s,t,s',t')=\gamma\phi(s',t')-\phi(s,t)$ do not alter the Nash equilibrium in multi-agent problems. Grzes \cite{grzes2017reward} extend the proof to episodic tasks that terminate after a certain final time step $N$. Chen et al. \cite{chen2022knowledge} design potential-based rewards in GRF to be positively correlated with the distance between the ball and the goal. Zhang et al. \cite{zhang2023multiexperience} design the rewards that are proportionally linked to the remaining health of a unit in StarCraft II \cite{samvelyan2019starcraft}. However, existing methods are often based on simple formulas or rules, making it difficult to comprehensively represent human common sense.