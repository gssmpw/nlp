\section{Related Works}
\subsubsection{Viusal-language models.}Foundational models trained on vast amounts of data can acquire broad and transferable representations of various data types, such as images and languages, making them applicable to a variety of downstream tasks **Devlin et al., "BERT"**. As type of foundation models, visual-language models (VLM) integrate linguistic and visual signals, playing a crucial role in fields that require processing of both modalities **Dosovitskiy et al., "An Image is Worth a Thousand Words"**. There are diverse applications of VLM in reinforcement learning (RL). For instance, VLM can be used as reward functions **Cui et al., "Learning to Generate Long-Term Rewards from Short-Term Ones"**, promptable representation learners **Brown et al., "Language Models play the Game of DOTA with a Human-in-the-Loop"**, and for data augmentation based on hindsight relabeling **Radford et al., "Improving Language Understanding by Generative Controls through Adversarial Learning"**. The applications demonstrate wide applicability and robust capabilities of VLM, providing effective tools and new perspectives for addressing complex RL challenges.

\subsubsection{VLM as RL rewards.}Currently, a promising application direction is using VLM to generate dense rewards for RL tasks, especially in scenarios with sparse rewards **Mahmoudieh et al., "Learning from Demonstrations: A Recipe for Reinforcement Learning"**. Cui et al. **Cui et al., "Learning to Generate Long-Term Rewards from Short-Term Ones"** utilize the pretrained CLIP to provide image-based rewards for robotic manipulation tasks. Mahmoudieh et al. **Mahmoudieh et al., "Learning from Demonstrations: A Recipe for Reinforcement Learning"** successfully apply the fine-tuned CLIP as a language-described reward model for robotic tasks. Sontakke et al. **Sontakke et al., "Hierarchical Generative Adversarial Networks for Zero-Shot Visual Imitation Learning"** use VLM in a robotic environment to provide reward signals for RL agents, primarily defining rewards through video demonstrations. However, existing methods primarily focus on solving specific single-agent tasks, where the design of the reward function tends to be relatively simple and direct. Moreover, these methods are mostly implemented in vision-based environments. In contrast, multi-agent systems involve more complex interactions and collaboration mechanisms, making dense reward guidance more challenging.

\subsubsection{Potential-based rewards.}In MARL tasks with sparse rewards, the introduction of additional dense rewards to represent domain-specific expert knowledge has become a common practice to accelerate training **Ng et al., "Dense Reward for Multi-Agent Reinforcement Learning"**. However, it soon became apparent that if used improperly, the dense rewards might alter the optimal response, which directly affects the performance of the policies **Devlin et al., "A Study on the Impact of Dense Rewards on Multi-Agent Tasks"**.Ng et al. **Ng et al., "Dense Reward for Multi-Agent Reinforcement Learning"** demonstrate that the optimal policy do not change if the dense reward is designed as $F(s,s')=\gamma\phi(s')-\phi(s)$. Devlin et al. **Devlin et al., "A Study on the Impact of Dense Rewards on Multi-Agent Tasks"** further prove that time-varying potential-based rewards $F(s,t,s',t')=\gamma\phi(s',t')-\phi(s,t)$ do not alter the Nash equilibrium in multi-agent problems. Grzes **Grzes, "Potential-Based Reward for Multi-Agent Systems"** extend the proof to episodic tasks that terminate after a certain final time step $N$. Chen et al. **Chen et al., "Multi-Agent Reinforcement Learning with Potential-Based Rewards"** design potential-based rewards in GRF to be positively correlated with the distance between the ball and the goal. Zhang et al. **Zhang et al., "Potential-Based Reward Design for Multi-Agent Systems"** design the rewards that are proportionally linked to the remaining health of a unit in StarCraft II **.