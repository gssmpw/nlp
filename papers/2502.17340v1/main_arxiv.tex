
\PassOptionsToPackage{capitalize}{cleveref}

\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\hypersetup{
        colorlinks   = true,
        urlcolor     = blue,
        linkcolor    = blue,
        citecolor   = blue
}
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %

\usepackage{changepage}
\usepackage{wrapfig}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} %

\usepackage{xcolor}
\usepackage[boxed]{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{comment}
\usepackage{authblk}
\usepackage{placeins}
\usepackage{caption}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{spverbatim}
\usepackage{csquotes}
\usepackage{float}
\usepackage{listings}

\usepackage{mathrsfs}


\usepackage[capitalize]{cleveref}
\usepackage[nolist,nohyperlinks]{acronym}

\usepackage{dsfont}

\usepackage{enumitem}

\usepackage{mathtools}

\usepackage{array}
\usepackage{tabularx}

\usepackage{fullpage}

\newcommand{\txtfont}{\fontfamily{qcr}\selectfont}


\usepackage{natbib}
\bibliographystyle{plainnat}
\setcitestyle{round,sectionbib}


\setlength{\parindent}{0pt}






\input{symbol}
\input{acronym.tex}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}

\newtheorem{assumption}{Assumption}




\usepackage{times}



\setlength{\marginparwidth}{14ex}


\begin{document}

\title{Low-rank bias, weight decay, and model merging in neural networks}
\author{Ilja Kuzborskij, Yasin Abbasi Yadkori\\
Google DeepMind}

\maketitle

\begin{abstract}
  We explore the low-rank structure of the weight matrices in neural networks
  originating from training with Gradient Descent (GD) and Gradient Flow (GF)
  with $L2$ regularization (also known as weight decay). We show several
  properties of GD-trained deep neural networks, induced by $L2$ regularization.
  In particular, for a stationary point of GD we show alignment of the
  parameters and the gradient, norm preservation across layers, and low-rank
  bias: properties previously known in the context of GF solutions.  Experiments
  show that the assumptions made in the analysis only mildly affect the
  observations.

  In addition, we investigate a multitask learning phenomenon enabled by $L2$
  regularization and low-rank bias. In particular, we show that if two networks
  are trained, such that the inputs in the training set of one network are
  approximately orthogonal to the inputs in the training set of the other
  network, the new network obtained by simply summing the weights of the two
  networks will perform as well on both training sets as the respective
  individual networks.  We demonstrate this for shallow ReLU neural networks
  trained by GD, as well as deep linear and deep ReLU networks trained by GF.
\end{abstract}


\section{Introduction}

First-order optimization algorithms, such as \ac{SGD} have emerged as a go-to
tool for training machine learning models.  Their popularity primarily stems
from good scalability and empirical performance, while their theoretical
properties and performance are reasonably well understood for learning problems
with sufficient structure, such as convexity or even weak forms of
non-convexity.  However, in the recent years with the advent of
overparameterized deep neural networks, there was a surge of interest in
understanding the behavior of these algorithms on non-convex and often
non-differentiable problems. Here, a combination of neural network architecture
choice, initialization, and hyperparameter tuning often achieves near-zero
training loss while enabling good test-time generalization ability.

Recently, some progress was made in attempt to explain this phenomenon by
looking for \emph{implicit biases} in the algorithm~\citep{vardi2023implicit}.
While implicit biases were known before in the context of simpler learning
problems such as overparameterized linear least-squares (solving this problem
through pseudo-inverse gives an interpolating solution
with the
\emph{minimal $L2$ norm}), observing the same phenomenon in neural network
learning is rather recent.  \citet{lyugradient,ji2020directional} showed that for
a certain family of neural networks (\emph{positive homogeneous} neural
networks, such as deep linear or deep ReLU neural networks) trained by an idealized version of \ac{GD} known as \ac{GF},
asymptotically converges to the predictor with the smallest parameter $L2$ norm.
In another influential line of work, known as the \ac{NTK}
approximation~\cite{jacot2018neural,du2018gradient,ji2019polylogarithmic,arora2019fine},
it was shown that a sufficiently wide shallow non-linear neural network behaves
similarly as a linear predictor on \ac{RKHS}.  This enabled reduction to
analysis of \ac{GD} on \ac{RKHS}, which is known to converge to the minimum $L2$
norm interpolating solution (or, which behaves as a regularized solution when
stopped early~\citep{yao2007early}).  %

While $L2$ norm minimization (or regularization) has a clear interpretation in
linear models, its role in deep neural network learning is less intuitive.  On
that front, several works showed that this form of regularization induces a
\emph{low-rank} structure in weight matrices of neural networks.
In fact, in deep linear neural networks weight matrices in such interpolants are
shown to be rank-$1$ matrices~\citep{ji2019gradient}.  Similarly,
\citet{timor2023implicit} showed that weight matrices in minimum $L2$ norm
interpolating deep ReLU networks will have a low \emph{stable rank} (ratio
between Frobenius and spectral norms) as long as the depth is sufficiently
large.
\citet{phuonginductive} show convergence of a hidden weight matrix in a shallow neural network to a rank-one matrix when it is trained by \ac{GF} on \emph{orthogonally-separable data} (in the context of classification all inputs with a matching label satisfy $\ip{x_i, x_j} > 0$, and otherwise for a non-matching one).
\citet{minearly} show convergence of stable rank to $2$ for shallow ReLU networks trained by \ac{GF} dynamics.
Under mild assumptions on the inputs \citet{freiimplicit} established that the hidden weight matrix of a shallow  neural network convergences to the low-rank matrix (at most $2$).

The presence of these biases was established for training without any form of
explicit regularization, however one might suspect that a similar message can be
said when we introduce explicit regularization.
Some recent works have indeed investigated whether a low-rank structure arises
in stochastic optimization~\citep{galanti2024sgd}, with the emphasis on the role
of the batch size.  Others have explored $L2$ regularization from a
generalization viewpoint, in the \ac{NTK} approximation setting, when neural
networks behave similarly to overparameterized linear models
\citep{wei2019regularization,hu2021regularization}.


Finally, while the appearance of low-rank weight matrix structure during
training is interesting on its own, here we might ask a related question whether
low-rank bias can explain other phenomena that we observe in neural network
learning.  In particular, in this paper we look at the connection between
low-rank bias and a form of a multi-task learning known as \emph{model merging}.
In model merging, weight matrices of two neural networks trained on different
tasks are summed to form a new neural network, which often performs well on both
original tasks.  This behaviour seems surprising at first glance, and the
reason why merging (or, related, parameter averaging) in neural networks might
be effective is not well understood.



\subsection{Our contributions}
In this paper we take a closer look at a low-rank bias and its effect on model merging.
Our contribution is two-fold yet at the core it is connected to low-rank bias induced
by $L2$ regularization. %
\paragraph{Model merging enabled by weight decay.} We show that low-rank bias in
neural networks enables effective \emph{model merging}: after neural networks
are trained on different datasets with mutually nearly-orthogonal inputs (but
not necessarily orthogonal within the task), simply summing their weight
matrices results in a predictor with combined weights that performs well
simultaneously on \emph{all tasks}.  We explain this phenomenon through the
low-rank bias.
Training biases different networks toward low-rank weight matrices that span
non-overlapping subspaces.  Consequently, summing their weight matrices results
in a neural network that behaves as the original ones on inputs from respective
tasks.
In other words, two neural networks can be made to `reside' in one parameter
set.

More formally, given input-label pairs
$(x_1, y_1), \ldots, (x_n, y_n) \in \mathbb{S}^{d-1} \times \{\pm 1\}$ we obtain
a predictor $f_{\th(t)}$, whose parameters $\th(t)$ are found by \ac{GD} (or
\ac{GF}) run for $t$ steps, while minimizing a regularized loss
$\th \mapsto \frac1n \sum_i \mathrm{loss}(f_\th(x_i), y_i) + \lambda \|\th\|^2$.
In a similar way, we obtain parameters $\th'(t)$ given another data
$(x_1', y_1'), \ldots, (x_n', y_n')$ belonging to the second task, such that
inputs between these tasks are approximately orthogonal in a sense that
$\max_{i,j} \abs{\ip{x_i, x_j'}} \leq \ve$.  Then, given
an input $x$ originating from the first task, we show that
\begin{align*}
  |f_{\th(t)}(x) - f_{\th(t) + \th'(t)}(x)| \leq f_{\th'(0)}(x) \, e^{-\lambda t} + C_2 \, \ve
\end{align*}
for several scenarios.
Here, exponentially vanishing term
that appears because of $L2$ regularization, is responsible for contribution of
initialization, which is typically non-zero in neural network training.  The
second, $\ve$-dependent term captures the length of projection of the input (or
activation vector in case of multilayer neural networks) from one task onto the
weight matrix of the network trained on another task.  Note that $L2$
regularization is essential here, since without it, the effect of initialization
(appearing through a constant term $f_{\th'(0)}(x)$) would not disappear.
Interestingly, none of these results require convergence to the local minimum.

In particular, we show the above in case of linear prediction and shallow ReLU
neural networks trained by \ac{GD}, and deep linear neural networks trained by
\ac{GF}.  In case of deep ReLU neural networks we look at a simplified case
where $\ve = 0$ and $t \to \infty$.  In that case we conclude that
$f_{\th(t)}(x) = f_{\th(t) + \th'(t)}(x)$.  In this case we only require that
\ac{GF} reaches a stationary point.
Finally, in \Cref{sec:experiments:merging} we provide empirical verification for these results
for fully-connected ReLU neural networks, which to some extent corroborate the above.

\paragraph{Low-rank bias and alignment through weight decay}
Model merging discussed earlier crucially relies on $L2$ regularization, and its success (good performance on both tasks) can be attributed to biases that arise in neural networks because of regularization. To this end, in \Cref{sec:stationary} we take a closer look at such biases.
In particular, this time we focus on homogeneous neural networks with
Lipschitz-gradient activations (such as, e.g.\ ReLU$^p$ with $p \geq 2$), and
consider running \ac{GD} with $L2$ regularization until convergence to the
stationary point.  We first establish asymptotic \emph{alignment}, meaning that
parameters converge in the direction of gradient of the
(unregularized) loss.  Although asymptotic alignment was known before in case of
\ac{GF} (without regularization)~\citep{ji2019gradient,ji2020directional}, here
we observe it in case of \ac{GD}.  Building upon this observation we prove
several other results that, which to the best of our knowledge, were only known
in case of \ac{GF}~\citep{du2018algorithmic,ji2019gradient,ji2020directional,timor2023implicit}:
\begin{itemize}
\item Norm-preservation: weight matrices in neural networks we consider have the same Frobenius norm throughout layers.
\item Deep linear neural networks asymptotically have all weight matrices of rank one.
\item Deep (possibly non-linear) neural networks have a low-rank weight matrix
  structure, in the following sense:
\end{itemize}
The harmonic mean of stable ranks\footnote{A stable rank of matrix is defined as
  a ratio of its Frobenius and spectral norms, while it is small whenever matrix
  has few dominant eigenvectors.} of weight matrices $(W_1, \ldots, W_K)$ is
controlled by regularization parameter $\lambda$:
\begin{align*}
  \frac{K}{\pr{\frac{\| W_1\|_F}{\|W_1 \|_2}}^{-1} + \dots + \pr{\frac{\| W_K\|_F}{\|W_K \|_2}}^{-1}}
  <
  \sqrt{ \frac{L(\theta)^{\frac12 - \frac1K}}{\lambda} }
  \leq
  C \, 
  \lambda^{\frac1K-1}
\end{align*}
Harmonic mean of stable ranks as a proxy for capturing the rank structure in
deep neural networks was proposed by \cite{timor2023implicit}. Since stable rank
cannot be smaller than one, the harmonic mean is small whenever majority of
weight matrices have small stable ranks.  In particular,
\cite{timor2023implicit} looked at the minimum $L2$-norm \emph{interpolating}
neural networks, in a sense that for all training examples $y_i \in \{\pm 1\}$,
$y_i f_\th(x_i) \geq 1$.
Their conclusion was that the harmonic mean $\to \sqrt{2}$ as depth
$K \to \infty$.  In contrast, we simply require \ac{GD} to achieve a stationary
point, in which case the harmonic mean is controlled by regularization parameter
and the training loss at stationarity, even at a finite depth (see
\Cref{lem:low-rank} and discussion therein).  Indeed, this seems to be supported
by some basic empirical evidence (see \Cref{sec:norm-rank-exps,fig:rank-l2}),
which suggests that that a low stable rank can be achieved even without
interpolation but with weight decay.

Finally, we look at the behavior of a stable rank and norm preservation
empirically for very deep neural networks (such as pre-trained large language
models) in \Cref{sec:norm-rank-exps}, and conclude that assumptions (such as
homogeneity and stationarity) only mildly affect the observations.


\section{Definitions}
\label{sec:def}
Throughout, $\|\cdot\|$ is understood as the Euclidean norm for vectors and the Frobenius norm matrices.
When, written explicitly for matrices, $\|\cdot\|_2$ is a spectral norm, and $\|\cdot\|_F$ is a Frobenius norm.
For some matrix $A$ its \emph{stable rank} is defined as a ratio $\|A\|_F / \|A\|_2$.
The Frobenius inner product between matrices $A,B$ is denoted by
$\ip{A, B} = \tr(A\tp B)$.
Throughout $e_j = (\ind(j=1), \ind(j=2), \ldots, \ind(j=m)) \in \{0,1\}^m$
and $a \wedge b = \min(a, b)$ and $a \vee b = \max(a,b)$.

In the following we denote \ac{ReLU} operation by $(x)_+ = \max(x, 0)$ for
$x \in \R$.  For vectors and matrices application of $(\cdot)_+$ is understood
elementwise.  \ac{ReLU} has several useful properties.  For $a,b \in \R$, we
have $|(a)_+ - (b)_+| \leq |a-b|$ and so for vectors $x,y \in \R^d$,
$\|(x)_+ - (y)_+\|_2^2 = \sum_i ((x_i)_+ - (y_i)_+)^2 \leq \sum_i (x_i - y_i)^2
= \|x - y\|_2^2$.

Function $f : \R^d \to \R$ is called \emph{positive homogeneous} of degree $K$ when $f(\alpha x) = \alpha^K f(x)$ for any $\alpha \geq 0$.
An important consequence for such functions is an Euler's homogeneous function theorem which states that, if the chain rule holds, then $K g(x) = \ip{x, \nabla f(x)}$.
Note that \ac{ReLU} is positive homogeneous, meaning that $(\alpha x)_+ = \alpha (x)_+$ for $\alpha \geq 0$.
In particular, this implies that for a $K$-layered \ac{ReLU} or linear neural network is positive homogeneous and so $f_{\th}(\alpha \, x) = \alpha^K f_{\th}(x)$ and so Euler's theorem holds.

For the logistic loss function $\ell(x) = \ln(1+e^{-x})$ we have
$\ell'(x) = - e^{-x} / (1 + e^{-x}) \in (-1, 0)$ and moreover
$|\ell'(x)| = -\ell'(x) \leq \ell(x)$ and $x \, \ell'(x) \leq \ell(x)$ for all $x$.

 
\paragraph{Differentiation}
We introduce some formalism for non-differentiable functions since occasionally
we will work with the \ac{ReLU} activation.  %
For some $F : \R^p \to \R$ that is locally Lipschitz\footnote{$F$ is locally Lipschitz if for every point $\th$, there exists a neighborhood $B \supseteq \{\th\}$ such that $F$ is Lipschitz on $B$.}, we denote by $\partial F$ its Clarke
differential:
\begin{align*}
  \partial F(\th) = \mathrm{conv}\pr{\cbr{
    g \in \R^p : \exists (\th_i)_i \to \th, \quad \nabla F(\th_i) \to g
  }}.
\end{align*}
Vectors in $\partial F$ are called subgradients, while $\bar \partial f(x)$ will
stand for a unique minimum-norm subgradient, that is
$ \bar\partial f(x) = \argmin_{g \in \partial f(x)} \|g\|~ $.  Throughout the
paper it is understood that $\nabla f = \bar\partial f$.
We will assume that the chain rule holds, that is
$\dot f(\th(t)) = \langle g(t), \dot \th(t) \rangle,$ for all
$g(t) \in \partial F(\th(t))$.  It is possible to formally establish that the
chain rule holds by assuming a technical notion of `definability' (not covered
here, see \citep{ji2019gradient}), which excludes functions that might result in
a badly behaved optimization.


\section{Preliminaries}
\label{sec:reliminaries}

In this paper we consider multilayer neural networks $f_{\th} : \R^d \to \R$ given by a recursive relationship
\begin{align*}
  f_{\th}(x) = \ip{w_K, h_{K-1}}, \quad h_k = \sigma(W_k h_{k-1}), \quad h_0 = x \qquad (k \in [K-1])
\end{align*}
where $x$ is the input,
$(W_k \in \R^{m \times m})_k$
is collection of weight matrices, $\th = (\vec(W_1), \ldots, \vec(W_K))$ is a parameter vector, and $\sigma : \R^m \to \R^m$ is activation function.
When we look at \emph{linear} neural networks $\sigma$ is identity, meanwhile in the \emph{non-linear} case or \ac{ReLU} networks $\sigma(x) = (x)_+$.
Activation vectors can alternatively be written as
\begin{align*}
  h_k = D_k W_k D_{k-1} W_{k-1} \cdots D_1 W_1 x
\end{align*}
where $D_k$ is a diagonal matrix $D_k = \diag(\sigma'(W_k h_{k-1}))$ while
$(D, h)$ implicitly depend on the input $x$.
When we want to state an explicit dependence on the inputs and parameters we will use notation $h_k(x; \, \th)$.
Then, differentiation with respect to a weight matrix has a convenient form
\begin{align*}
  \frac{\diff f_\th(x)}{\diff W_k}
  &=
    (W_K D_{K-1} \cdots W_{k+1} D_k)\tp (h_{k-1})\tp~.
\end{align*}

In practice parameters $\th$ are tuned based on the training data by minimizing some loss function.
In this paper we will focus on a binary classification problem, and so given a tuple of inputs and labels $(x_i, y_i)_{i=1}^n \in (\mathbb{B}^d \times \{-1,1\})^n$, the training loss (or empirical risk) is defined as
\begin{align*}
  L(\th) = \frac1n \sumin \ell\pr{y_i \, f_{\th}(x_i) }
\end{align*}
with respect to a logistic loss function $\ell(x) = \ln(1+e^{-x})$.
More generally we will look at the regularized objective of a form
\begin{align*}
  L_{\lambda}(\th) = L(\th) + \frac{\lambda}{2} \|\th\|_2^2 \qquad (\lambda \geq 0)~.
\end{align*}
\emph{\ac{GD}} algorithm approximately minimizing $\th \mapsto L_{\lambda}(\th)$ is given by \Cref{alg:gd}.
\begin{algorithm}[H]
  \caption{\acl{GD}}
\begin{algorithmic}[1]
  \STATE \textbf{Input}: step size $\eta > 0$, data $(x_i, y_i)_i$, initial parameter $\th_0 \in \R^p$.
  \FOR{$s=0,1,2,\ldots,t$}
  \STATE For all $k \in [K]$:
  \begin{align*}
      W_{k,s+1} = (1-\eta \lambda) W_{k,s}
    - \frac{\eta}{n} \sumin y_i \, \ell'(y_i \, f_{\th_s}(x_i)) \, \frac{\diff f_\th(x_i)}{\diff W_k}(\th_s)
  \end{align*}
  \ENDFOR
  \STATE Output $\th_t = (\vec(W_{1,t}), \ldots, \vec(W_{K,t}))$.
\end{algorithmic}
\label{alg:gd}
\end{algorithm}
When we consider \ac{GF} dynamics (we use $(t)$ time indexing instead of $\cdot_t$ as in the discrete case), the update rule is replaced by time derivative
\begin{align}
\label{eq:GF-W}
  \dot W_k(t) = -\lambda W_k(t) - \frac1n \sumin y_i \, \ell'(y_i \, f_{\th(t)}(x_i)) \, \frac{\diff f_\th(x)}{\diff W_k}(\th(t))~.
\end{align}




\section{Stationary points and low-rank bias}
\label{sec:stationary}

In this section, we study training deep neural networks with gradient descent and weight decay. We first consider the case of differentiable objective with Lipschitz gradient. Examples of such are neural networks with smooth activation functions (such as identity or smoothed versions of ReLU, e.g.\ powers of ReLU).
\begin{lemma}[Alignment]
\label{lem:alignment}
Assume $L_\lambda$ is differentiable and $H$-smooth with $H>0$ ($\nabla L_\lambda$ is $H$-Lipschitz w.r.t.\ $L2$ norm) and that step size satisfies $\eta \leq 1/H$. Then, the limiting point $\th = \th_{\infty}$ satisfies
$
\lambda \theta = -\nabla L(\theta)
$.
\hfill\hyperref[proof:sec:stationary]{[Proof]}
\end{lemma}
This type of alignment was first studied by \citep{ji2019gradient,ji2020directional} in the context of training with gradient flow, and under the extra condition that $L(\theta_0) < \ell(0)$.  Here, weight decay enables a simpler argument to establish alignment.
%
Next, we show several implications of this result.
\begin{lemma}[Deep linear networks]
\label{lem:deep-linear}
Let $f_{\theta}$ be a deep linear network, and $\theta$ be a parameter vector satisfying the alignment condition of~\Cref{lem:alignment}. Then all weight matrices $W_1,\dots,W_{K-1}$ are rank-1. \hfill\hyperref[proof:sec:stationary]{[Proof]}
\end{lemma}
\begin{lemma}[Norm preservation]
\label{lem:norm-preservation}
Let $\theta$ be a parameter vector satisfying the alignment condition of~\cref{lem:alignment}. Further, assume that $f_\theta$ is locally Lipschitz and positively homogeneous. Then weight matrices of all layers have the same Frobenius norm: 
\begin{align*}
\lambda \|W_k \|_F^2 = -\frac{1}{n} \sumin  \ell'(y_i f_{\theta}(x_i)) y_i f_{\theta}(x_i) \;.
\end{align*}
\hfill\hyperref[proof:sec:stationary]{[Proof]}
\end{lemma}
A deep neural network with non-linear smooth activations can satisfy the above conditions. A self-attention layer used in Transformer architecture~\citep{vaswaniattention} with a max-attention layer (instead of softmax) is also homogeneous (however, practical models typically include residual connections, layer norms, softmax attention, and so on, that deviate from the above conditions).
Yet, as we show empirically in \Cref{sec:norm-rank-exps}, the conclusion of the above result holds to some extent.
\begin{lemma}[Low-rank structure]
\label{lem:low-rank}
Let $\theta=\theta(\infty)$ be a stationary point of the $\lambda$-regularized loss function. For any $\lambda>0$, we have that
\begin{align}
\label{eq:low-rank}
    \frac{1}{K}\sum_{k=1}^K \frac{\| W_k\|_2}{\|W_k \|_F} &\ge \left( \frac{\lambda}{L(\theta)^{1/2 - 1/K}} \right)^{1/2} \;.
\end{align}
For exponential ($\ell(x)=e^{-x}$) and logistic ($\ell(x)=\ln(1+e^{-x})$) losses, we additionally have
\begin{align}
\label{eq:low-rank2}
    \frac{1}{K}\sum_{k=1}^K \frac{\| W_k\|_2}{\|W_k \|_F} &\ge \frac{\lambda e}{(\lambda e)^{1/K} e^{1/K}} \;.
\end{align}
\hfill\hyperref[proof:sec:stationary]{[Proof]}
\end{lemma}
The above result implies that the average stable rank decreases as $\lambda$ increases.   
\citet{timor2023implicit} show low-rank structures for the global optimum of the minimum-rank interpolating solution assuming existence of a smaller ``teacher'' network with $K'<K$ layers that interpolates data and Frobenius norm of its weight matrices are bounded by $C$. More specifically, they show that $\frac{1}{K}\sum_{k=1}^K \frac{\| W_k\|_2}{\|W_k \|_F} \ge \left(\frac{1}{C}\right)^{\frac{K'}{K}}$. We can have an intuitive understanding of their result by noting that given a smaller interpolating network, the last layer of the network tends to be low-rank due to the \textit{Neural Collapse} phenomena~\citep{papyan2020prevalence}. Then if we add more layers, the additional layers will remain low-rank as their activations will lie in a low-rank structure, and $\frac{1}{K}\sum_{k=1}^K \frac{\| W_k\|_2}{\|W_k \|_F}$ will approach one as we add more layers. 

Results in \Cref{lem:deep-linear} and \Cref{lem:low-rank} show a different phenomena. Remarkably, in both these results, irrespective of network capacity or performance, and as a consequence of weight decay, the weight matrices will become low-rank on average. Another interesting feature is that, unlike results of \citet{timor2023implicit}, depth plays a minor role in establishing low-rank structures in \Cref{lem:deep-linear} and \Cref{lem:low-rank}. In \Cref{sec:norm-rank-exps}, we present experiments that show that even if number of layers is small and network has a large number of classification mistakes, the average inverse stable-rank grows as weight decay parameter $\lambda$ increases.   




%


\section{Merging model parameters}
\label{sec:merging}

Throughout this section, in addition to the training tuple $(x_i, y_i)_{i=1}^n$ we introduce $(x_i', y_i')_{i=1}^n$, and in addition to the regularized objective $L_{\lambda}$ we will introduce $L_{\lambda}'$ defined with respect to the second training tuple.

As an instructive example first consider a linear prediction scenario where we
are looking at predictors of a form $f_\th(x) = \ip{\th, x}$ and suppose that
$\th_t$ is obtained by \ac{GD} minimizing $L_{\lambda}$, while
$\th_t'$ is obtained by minimizing $L_{\lambda}'$.  Now given a
test point $x$ that is sufficiently different from inputs $(x_i')_{i=1}^n$, or assuming that $\max_i |\ip{{x_i'}, x}| \leq \ve$ we have
\begin{align*}
  f_{\th_t + \th_t'}(x) \approx f_{\th_t}(x)
\end{align*}
and the same holds for some point $x'$ sufficiently different from $(x_i)_{i=1}^n$.
This is derived in a straightforward way based on \ac{GD} update rule
\begin{align*}
  \th_{t+1} = (1- \lambda \eta) \th_t - \eta \, \frac1n \sumin \ell'(y_i \ip{\th_t, x_i}) x_i y_i
\end{align*}
while unrolling the recursive relationship we get
\begin{align*}
  \th_{t}
   &= \th_0 (1- \eta \lambda)^t + \sumin \alpha_i x_i \quad
  \text{where} \quad \alpha_i = \frac1n \sum_{s=0}^{t-1} \eta (1-\eta \lambda)^{t-s-1} \ell'(y_i \ip{\th_s, x_i}) y_i~.
\end{align*}
The above identity tells us that the solution
$\th_t$ resides in the span of inputs.
So, it is easy to see that prediction on the inputs from the other task is close to $\ve$
  \begin{align*}
    \abs{\ip{\th_t, x'}}
    &\leq
      \abs{\ip{\th_0, x'}} (1- \eta \lambda)^t + \ve \, \frac{1 - (1-\eta \lambda)^t}{\lambda}
  \end{align*}
  for a large enough $t$.
  In particular, the above implies that the gap of interest $|f_{\th_t + \th_t'}(x) - f_{\th_t}(x)|$ is also controlled by the upper bound in the display above.
  Note that a possibly large term $\abs{\ip{\th_0, x'}}$ is attenuated
  exponentially quickly by the weight decay, so its effect is negligible at the
  end of optimization.  While in the linear case we could set $\th_0 = 0$, in
  case of neural network learning setting initialization at $0$ is
  atypical and therefore weight decay seems to have an important role in such
  scenarios.
  Another summand on the right hand side is $\ve$-dependent and captures similarity between inputs from different tasks. If inputs are orthogonal this term disappears.
  
  Next, we will demonstrate similar gaps for neural networks trained by \ac{GD}  and \ac{GF}.
\paragraph{Shallow \ac{ReLU} networks}
Consider a shallow \ac{ReLU} neural network
\begin{align*}
  f_\th(x) = \ip{u, (W x)_+} \qquad (x \in \R^d)
\end{align*}
where hidden weight matrix $W \in \R^{m \times d}$ is a tunable parameters, and
$u$ with $\|u\| \leq 1$ is fixed throughout training.
In this scenario we obtain a merged predictor by simply summing hidden weight
matrices of neural networks trained on different tasks.
The intuition behind the argument in this case is that for an input $x$, the
length $\|W_t' x\|$ must be small because rows of $W_t'$ lie in the span on
$(x_i')_i$ meanwhile each of these points is sufficiently different from $x$.
We formalize this in the following lemma which applies to \ac{GD} iterates:
\begin{lemma}
  \label{lem:hidden-null-space}
  Suppose that inputs $(x_i)_i$ and $x'$ are such that
  $
    \max_i\abs{\ip{x_i, x'}} \leq \ve
  $.
  Suppose that $W_t$ is a weight matrix of a shallow neural network obtained by running \ac{GD} for $t$ steps given $(x_i, y_i)_{i=1}^n$.
  Then, for any $j \in [m], t \in \mathbb{N}$,
  \begin{align*}
    \|W_t x'\| \leq
    \|W_0 x'\| \, (1-\eta \lambda)^t + \ve \, \frac{1-(1-\eta \lambda)^t}{\lambda}~.
  \end{align*}
  \hfill\hyperref[proof:lem:hidden-null-space]{[Proof]}
\end{lemma}
The lemma almost immediately implies
\begin{corollary}
  \label{thm:shallow}
  Suppose that inputs $(x_i)_i$ and $(x_i')_i$ satisfy
  $ \max_{i,j}\abs{\ip{x_i, x_j'}} \leq \ve $ and that $W_t$ and $W_t'$ are
  weight matrices of two shallow neural networks, obtained by running \ac{GD}
  given datasets $(x_i, y_i)_i$ and $(x_i', y_i')_i$ respectively.
  Then, for any $x$ such that $\max\abs{\ip{x, x_i'}} \leq \ve$ and any $t \geq 0$ we have
  \begin{align*}
      |f_{\th_t + \th_t'}(x) - f_{\th_t}(x)|
      \leq \|W_t' x\|
      \leq \|W_0' x\| \, (1-\eta \lambda)^t + \ve \, \frac{1-(1-\eta \lambda)^t}{\lambda}~.
  \end{align*}
    \hfill\hyperref[proof:thm:shallow]{[Proof]}  
\end{corollary}
\paragraph{Deep linear networks}
Next, we consider the predictor
\begin{align*}
  f_{\th}(x) = \ip{w_K, W_{K-1} \cdots W_1 x}
\end{align*}
where each weight matrix is trained by \ac{GF} dynamics as described in \cref{eq:GF-W}.
To show the desired result we exploit a technical result of \cite{arora2018optimization} which translates \ac{GF} dynamics for individual matrices into \emph{implicit} \ac{GF} dynamics for the \emph{end-to-end} vector
\begin{align*}
  w(t)\tp = w_K(t)\tp W_{K-1}(t) \cdots W_1(t)~.
\end{align*}
The proof exploits the fact that $w(t)$ indeed lies in the span of inputs.
\begin{theorem}
  \label{thm:deep-linear-multitask-cor}
  Assume that weight matrices are initialized such that
  for all $k \in [K-1]$
  \begin{align*}
    W_{k+1}(0)\tp W_{k+1}(0) = W_{k}(0) W_{k}(0)\tp~.
  \end{align*}
  Then, for any $t \geq 0$ and any input $x$ such that $\max_i \abs{\ip{x_i, x}} \leq \ve$ we have
  \begin{align*}
  |f_{\th(t)+\th'(t)}(x) - f_{\th(t)}(x)|
  \leq
  |f_{\th(0)}(x)| A_1 \, e^{- \lambda K  t}
  +
  A_2 \, \ve
\end{align*}
where terms $A_1, A_2, B$ depend only on initialization, $\lambda$, and $K$ (see
\Cref{thm:deep-linear-multitask} for precise expressions).
\hfill\hyperref[proof:thm:deep-linear-multitask-cor]{[Proof]}
\end{theorem}
\Cref{thm:deep-linear-multitask-cor} gives the bound on the gap of the same form as in the shallow and linear case, that is an exponentially decaying term that arises because of the weight decay, and an $\ve$-dependent term which captures similarity of inputs in different tasks.
Unlike the previous cases, the theorem assumes a particular initialization, which is inherited from \citet{arora2018optimization}: this assumptions is benign and is satisfied with high probability when entries of weight matrices are sampled from some symmetric distribution (e.g.\ isotropic Gaussian).
\paragraph{Deep \ac{ReLU} networks}
As our last result we show that parameter summation also works well in case of
deep \ac{ReLU} neural networks.  Here, we consider a simpler scenario
where such neural networks are trained by \ac{GF} until convergence, that is to
the zero-gradient of a regularized objective $L_{\lambda}$.\footnote{Although we believe that it is possible to state a similar result for a finite time, working with a stationary point allows to greatly simplify the proofs.}
  In
other words we look at the \emph{stationary points} of $L_{\lambda}$. Note that
convergence to the local minimum is \emph{not} required in our analysis.

However, we require two fairly mild technical assumptions.
The first, \Cref{asm:on-average-positive-margin} says that on-average margins are positive, and we will assume that this is satisfied at some time $t_0$.
Note that this implicitly assumes that the capacity of the predictor has to be large enough to correctly classify at least a fraction of training points.
This type of assumption is common in the literature, for instance \citep{lyugradient,ji2020directional} assume that $L(\th(0)) \leq \ell(0)$.

The second assumption, \Cref{asm:act-convergence}, captures scenarios where the set of positive activations converges to a non-empty set.
This prevents degenerate cases where all activations eventually vanish (which is possible if weight decay is too strong).
\begin{assumption}[On-average positive margin]
  \label{asm:on-average-positive-margin}
  We say that $\th$ satisfies on-average positive margin condition on
  training tuple $(x_1, y_i), \ldots, (x_n, y_n)$ when
  \begin{align*}
    -\frac1n \sumin \ell'(y_i \, f_{\th}(x_i)) \, y_i \, f_{\th}(x_i) \geq 0~.
  \end{align*}
\end{assumption}

\begin{definition}[Smallest positive normalized activation]
  \label{def:gamma}
  The smallest positive normalized activation of \ac{ReLU} neural network at layer $k$, provided input $x$, is defined as
  \begin{align*}
    \gamma_k(x; \th) = \min\cbr{ \frac{\ip{e_j, W_k h_{k-1}(x; \th)}}{\|h_{k-1}(x; \th)\|} ~:~ \ip{e_j, W_k h_{k-1}(x; \th)} > 0~, j \in [m]}~.
  \end{align*}
\end{definition}
\begin{assumption}[Convergence of positive activations]
\label{asm:act-convergence}
  We say that positive activations at layer $k$ converge given input $x$, if
  $\lim_{t \to \infty}\gamma_k(x; \th(t)) = c$ for $c > 0$.
\end{assumption}
The following is the main result of this section:
\begin{theorem}
  \label{thm:relu-merging}
  Suppose that parameters of two \ac{ReLU} neural networks
  $\th, \th'$ are limiting solutions of \ac{GF} minimizing $L_{\lambda}$ and $L'_{\lambda}$ respectively.
  \begin{itemize}
      \item Assume that there exists time $t_0$ such that both $\th(t_0), \th'(t_0)$ satisfy on-average
  positive margin condition of \Cref{asm:on-average-positive-margin}.
  \item Assume that activations at layer $k$ converge in a sense of \Cref{asm:act-convergence} for both networks given inputs $x, x'$ respectively.
  \item Finally, assume that inputs $x, x'$ satisfy  $\ip{x_i, x'} = 0$ and $\ip{x_i', x} = 0$.
  \end{itemize}
  Then, there exists a tuning of weight decay parameters $(\lambda_t)_t$ such that $\lambda_t \to 0$ as $t \to 0$ such that
  $
    h_k(x; \, \th + \th') = h_k(x; \, \th)
  $
  and similarly for $(x', \th')$.
  \hfill\hyperref[proof:thm:relu-merging]{[Proof]}
\end{theorem}

More precisely, the tuning of $\lambda_t$ mentioned in the theorem is of
order $\lambda_t = \Omega(\frac{1}{\gamma_k \sqrt{t}})$ where $\gamma_k < \min_{t \geq t_0} \gamma_k(x, \th(t))$, $ \min_{t \geq t_0} \gamma_k(x, \th'(t))$.

This result is somewhat stronger than the previous ones as it shows that merging is effective at any layer, and since we consider stationary points, we achieve an identity.

The proof of \Cref{thm:relu-merging} is conceptually similar to that of the shallow neural network case (\Cref{lem:hidden-null-space}).
In particular, in the shallow case we were relying on the fact that $\|W_k x'\|$ is small, meanwhile here we need to show that $\|W_k h_{k-1}(x'; \th')\| = 0$.
Alignment (\Cref{lem:alignment}) tells us that rows of $W_k$ will lie in the span of activation vectors $h_{k-1}(x_i; \th)$, and so the missing piece is to show that $h_{k-1}(x_i; \th)$ is orthogonal to $h_{k-1}(x'; \th')$.
This is largely the consequence of the weight decay: In \Cref{lem:almost-ortho-activations} we establish that dot product between activations vanishes exponentially quickly under appropriate conditions (\Cref{asm:on-average-positive-margin} and \ref{asm:act-convergence}).
\section{Experiments}
\label{sec:experiments}

\subsection{Norm and rank structure}
\label{sec:norm-rank-exps}
In this section we aim to investigate whether some of the biases discussed in
\Cref{sec:stationary} extend do very deep neural networks.  In particular, we
aim to verify whether norm preservation and low stable-ranks can be found in
some (smaller) LLMs.  Even though assumptions of \Cref{sec:stationary} might not
be satisfied, we still find that in several LLMs we observe low stable-ranks (drastically smaller than the dimension of the matrix), while norm preservation appears in most of the layers (MLP and attention ones). In these experiments we look at publicly available pre-trained BERT with
$\approx $ 110M parameters, GPT2-Large with $\approx$ 774M
, GPT2-XL with $\approx$ 1.5B , RoBERTa with $\approx$
125M , Phi-2 with $\approx$ 2.8B , GPT-J with $\approx$ 6B (we used Hugging Face ``Transformers'' library
  \citep{wolf-etal-2020-transformers}).
  
  Each of these `transformer' models consists of a sequence of a so-called \emph{encoder} layers, where each encoder layer consists of a
  \emph{self-attention layer} followed by a fully-connected neural network. Self-attention layer is given by a matrix-to-matrix function $Q \mapsto \text{softmax}\big( Q K^T / \sqrt{\text{columns}(K)} \big) V$ 
 where $(Q,V)$ are parameter matrices and softmax is taken row-wise.
 Each self-attention layer is followed by a feed-forward fully connected neural network consisting of two linear layers with a non-linear activation function (e.g., ReLU or a smooth activation function) in between. In the context of transformer architecture, $Q,K,V$ are known as query, key, and value matrices.
 In \Cref{sec:experiments-appendix} we provide a table
  \Cref{tab:transformers} that summarizes architectural details of these models.
  
  While some of the results are given in \Cref{sec:experiments-appendix}, here in \Cref{fig:phi-2} we provide results for two largest neural networks.   In these plots, Q, K, V, O, and QKV denote Query, Key, Value, Output, and the concatenation of Query, Key, Value matrices of the attention layer, respectively. Weight matrices of MLP layers are denoted by M1 and M2.





The plots show that the Frobenius norms of QKV, M1, and M2 matrices are generally in the same order. Although this might appear puzzling at first, it is in fact consistent with \Cref{lem:norm-preservation}: the lemma is a statement about norms of layer weights when the output of the network can be written as a product of those layers. In order to apply the lemma to a Transformer architecture, we should consider the QKV matrix as a layer weight instead of considering the individual attention matrices. All weight matrices generally have small stable-ranks, although the values can be different for different parameter type and layer. 

There are two observations for which we currently have no explanation: (1) The Frobenius norms of the same parameter type (even Q, K, V matrices) remain largely unchanged across layers. (2) Even though \Cref{lem:norm-preservation} suggests the O matrix should have similar Frobenius norms as QKV, M1, and M2 matrices, the plots show quite different values for norms of O matrices. 
\begin{figure}[H]
    \includegraphics[width=0.49\linewidth]{figures/phi-2-rank.png}
    \includegraphics[width=0.49\linewidth]{figures/phi-2-norm.png}
    \includegraphics[width=0.49\linewidth]{figures/GPT-J-rank.png}
    \includegraphics[width=0.49\linewidth]{figures/GPT-J-norm.png}
\caption{Stable ranks and Frobenius norms of different weight matrices in pretrained Phi-2 (first row) and GPT-J (second row) models.} 
\label{fig:phi-2}
\end{figure}
Finally, \Cref{fig:rank-l2} shows that even if number of layers is small and network has a large number of classification mistakes, the average inverse stable-rank grows as weight decay parameter $\lambda$ increases. In this experiment, 1000 inputs are drawn independently from $\cN(0,I_d)$ with $d=100$ and then projected onto a unit sphere. Given an input $x_i$, the label $y_i \in \{-1,+1\}$ is generated as
$y_i = 2 \, \ind\pr{1/(1+e^{-f^{\star}(x)}) \geq 1/2} - 1$ where the labeling
function is $f^{\star}(x) = \sin(10 W\tp x)$ (left plot) or $f^{\star}(x) = \sin(100 W\tp x)$ (right plot), and vector $W$ is
drawn from $\cN(0,I_d)$ and fixed throughout the experiment. For each value of $\lambda$, we use SGD with weight decay to train a network with two hidden layers (so, $K=3$), each of width 10. The number of epochs is 5000. \Cref{fig:rank-l2} shows the average inverse stable-rank of the final solution, along with its classification error, margin error (number of points with $y_i f_\theta(x_i) < 1$), and average loss. The plot shows that the average inverse stable-rank increases as $\lambda$ increases, even though the network might have large errors. 
\begin{figure}[H]
    \includegraphics[width=0.49\linewidth]{figures/rank-l2.png}
    \includegraphics[width=0.49\linewidth]{figures/rank-l2-2.png}
\caption{Low rank induced by weight decay.} 
\label{fig:rank-l2}
\end{figure}


\subsection{Model merging}
\label{sec:experiments:merging}

In our experiments we aim to verify four hypotheses:
\begin{itemize}
\item Training two neural networks on \emph{different tasks} (with nearly orthogonal
  inputs), and summing the weights results in a combined neural network that
  performs nearly as well on each of the tasks. Here the performance is
  understood in terms of the training loss.
\item In a contrast, training two neural networks on the \emph{same task} and
  performing the merging as discussed above leads to the combined predictor that
  performs poorly on both tasks.
\item This is behavior is enabled by the weight decay.
\item Using weight decay leads to a low stable rank of weight matrices.
\end{itemize}

\paragraph{Data.}
The first set of experiments is performed on synthetic data, constructed as follows:
In case of independent tasks, inputs for the first task are drawn from isotropic
Gaussian $\cN(0,\Sigma_1)$, while for the second task inputs are drawn from
$\cN(0,\Sigma_2)$ where $\ip{\Sigma_1, \Sigma_2} = 0$ (this corresponds to
scenario $\ve = 0$ in \Cref{sec:merging}).
Given an input $x_i$, the label $y_i \in \{-1,+1\}$ is generated as
$y_i = 2 \, \ind\pr{1/(1+e^{-f^{\star}(x)}) \geq 1/2} - 1$ where the labeling
function $f^{\star}(x) = \sin(W\tp x)$.  For each task, a vector $W$ is
drawn from $\cN(0,I_d)$ and fixed throughout the experiment.
In all experiments of this section inputs are projected onto a unit sphere.
We achieve similar observations also in case of a real data, see \Cref{sec:mlp-more-results}.
\paragraph{Model and training.}
In all the experiments Fully-connected \ac{ReLU} neural network with inputs
$d=100$, two hidden layers of sizes $(1000, 100)$, and a scalar output.%

In each experiment on synthetic data, models are trained by \ac{GD} with step size $\eta = 1$ over
$10^5$ steps.  In all the experiments excepts the one where weight decay varies,
weight decay parameter $\lambda$ is set to $10^{-4}$.
On Fashion MNIST dataset, step size is set as $\eta=0.1$ while $\lambda = 10^{-3}$.
All experiments are repeated on $10$ random draws of the sample, and we report standard deviations in all plots.
\begin{figure}
  \includegraphics[width=0.5\linewidth]{figures/fig1.pdf}
  \includegraphics[width=0.5\linewidth]{figures/fig2.pdf}
  \includegraphics[width=0.5\linewidth]{figures/fig3.pdf}
  \includegraphics[width=0.5\linewidth]{figures/fig4.pdf}
  \caption{First row: Training neural networks on different tasks (orthogonal inputs) (left) vs. the same task (right) and
    merging the parameters by adding weight matrices. The resulting network
    performs well on different tasks after sufficiently many iterations, while given the same task, it does not.
    Second row: this effect manifests
    when weight decay strength is sufficiently large (left).
    Stable rank of each weight matrix converges to a small value. Merged
    network matches the stable rank of individual networks (right).
    }
  \label{fig:t-vs-loss-different-tasks-synth}
\end{figure}
\paragraph{Discussion.}
On both datasets, we consistently observe that summing
weight matrices originating from different tasks, enables small logistic loss for merged models trained on all tasks after sufficiently long training (first row in \Cref{fig:t-vs-loss-different-tasks-synth}).
This seems to be in part enabled by orthogonality of inputs since, in contrast,
when inputs are not orthogonal and labels (or labeling functions) are different,
merged models do not get close in performance to distinct task-specific models.
Another
component that enables this gap is weight decay: In the second row of
\Cref{fig:t-vs-loss-different-tasks-synth} we observe that when
weight decay strength is not sufficient, the gap between losses of
merged and original models is substantial.  Finally, we observe that the stable
rank converges to a value much smaller than the actual rank at the stage when
performance of merged model actually gets close to the performance of original
model.  This is another observation in support of our hypothesis that low-rank bias is
crucial for model merging.


\section{Additional related work}
\label{sec:related}



\paragraph{Weight averaging}
Several prior works have considered building a final model by averaging weights of different models~\citep{Utans-1996,Izmailov-2018,Rame-2022,Wortsman-2022,Stojanovski-2022,Ilharco-2022,Ilharco-2023,chung2023parameter,Douillard-2023,Rame-2023,Rame2024WARMOT, rame2024warp}. To the best of our knowledge, prior works mostly consider settings where several models are trained on data from the same or similar tasks. Their main idea is that the randomization in data or parameter initialization leads to perturbations in the learned weights, and taking their average leads to more stable solutions. This is also known as ``linear mode connectivity"~\citep{frankle20a,Neyshabur2020WhatIB}. In contrast, we consider models trained on entirely different tasks, and we take the weights' summation instead of their average. Our explanation for the success of model merging is also very different and relies on the low-rank structures of the learned weights.  

\paragraph{Neural Collapse}
Low-rank bias is intimately related to a so-called \emph{Neural Collapse (NC)},
which is a form of clustering within the feature space of a neural network~\citep{papyan2020prevalence}.
NC hypothesis suggests that the network refines its representations so that inputs belonging to the same class (in context of classification) are pulled closer together, forming distinct clusters. Simultaneously, the network pushes the cluster centers (class means) away from each other, maximizing their separation.

Some of the
existing results are mostly in the Unconstrained Feature Model (UFM) where only the last-layer features and the output of linear classifier are learnable. This is in fact equivalent to learning a shallow linear network. %
Most studies show that the global optima of the loss function satisfies the conditions of neural collapse, without studying the dynamics of gradient descent.

\citet{yang2022inducing} demonstrated that in the Universal Feature Manifold (UFM) setting, fixing the linear output layer to satisfy a specific condition induces neural collapse in the feature vectors that minimize the loss, even with imbalanced data.  This fixed output layer approach allows them to extend typical neural collapse results, which often assume balanced data, to the more challenging imbalanced case.

\citet{RBF-2022} showed that for ReLU deep networks trained on balanced datasets using gradient flow with weight decay and a squared loss, critical points satisfying a "Symmetric Quasi-interpolation" assumption also exhibit neural collapse.  This assumption, which posits the existence of a classifier whose output depends only on the class label and not the specific data point, is presented as a key condition for their result.

Notably, model merging discussed here  differs from \emph{weight
averaging}~\citep{Utans-1996}: weight averaging relies on learned weights
converging to vicinity of each other, so taking their average leads to a more
stable solution. In our case, learned weights are far from each other, and the
average weights often do not perform well in practice. As our theory suggests,
we should sum the weights up, which in fact leads to good performance.

%



\section{Conclusions}
In this work we examined the role of $L2$ regularization in training of deep
neural networks with logistic loss.  We investigated a surprising phenomenon:
merging two neural networks trained on sufficiently different tasks by simply
adding their respective weight matrices results in a predictor that performs
well on both tasks simultaneously.  As we attributed the explanation of this to
the low-rank bias arising in weight matrices, we also established that $L2$
regularization leads to weight matrices of a low stable rank.

These observations open up some interesting possibilities, especially in
multitask learning with large models, such as large language models, and
distributed optimization.
We believe our proof technique for deep \ac{ReLU} neural networks (
\cref{thm:relu-merging}) can be refined, potentially removing some of the
assumptions, such as \Cref{asm:act-convergence}.


\bibliography{learning}



\appendix
\clearpage

\section{Proofs from \Cref{sec:stationary}}
\label{proof:sec:stationary}
\begin{proof}[Proof of \Cref{lem:alignment}]
If $L_\lambda$ is differentiable and $H$-smooth with $H>0$, by the descent lemma,
\[
\frac{1}{2H} \|\nabla L_\lambda(\theta_t) \|^2 \le L_\lambda(\theta_t) - L_\lambda(\theta_{t+1}) \;.
\]
By summing over all terms and the fact that losses are non-negative,
\[
\frac{1}{2H} \sum_{t=0}^\infty \|\nabla L_\lambda(\theta_t) \|^2 \le L_\lambda(\theta_0) \;.
\]
Boundedness of the above sum implies that $\|\nabla L_\lambda(\theta_t)\| \rightarrow 0$ as $t\rightarrow \infty$. %
Therefore, at the limit we have the alignment of the weight and the gradient vectors, $\lambda \theta = -\nabla L(\theta)$. 
\end{proof}

\begin{proof}[Proof of \Cref{lem:deep-linear}]
  Let $w^{(i)\top}=w_{K}\tp W_{K-1}\dots W_{i}$. Therefore, $f_{\theta}(x)= \ip{w^{(i)}, W_{i-1} \dots W_{1} x}$.
  Let $r_i(\th) := -y_i \ell'(y_i f_\th(x_i) / (n \lambda)$.
  By alignment, we have 
\begin{align*}
W_{1} &= w^{(2)}\sumin r_{i}(\theta) x_i\tp \,,\\
W_{2} &= w^{(3)}\sumin r_{i}(\theta) (W_{1} x_i)\tp \,,\\
\vdots \\
W_{K-1} &= w^{(K)}\sumin r_{i}(\theta) (W_{K-2}\dots W_{1} x_i)\tp \,,\\
w_{K} &= \sumin r_{i}(\theta) (W_{K-1}\dots W_{1} x_i)\tp \;.
\end{align*}
It's easy to see that all weight matrices above are rank-1. 
\end{proof}

\begin{proof}[Proof of \Cref{lem:norm-preservation}]
\Cref{lem:alignment} holds also for the parameters of layer $k$: $\lambda W_k = -\nabla_{W_k} L(\theta)$. Therefore,
\begin{align*}
\lambda \|W_k \|_F^2 &= \lambda\,\tr(W_k W_k\tp)\\ 
&= -\tr(\nabla_{W_k} L(\theta)W_k\tp) \\ 
&= -\frac{1}{n} \sumin \ell'(y_i f_{\theta}(x_i)) y_i \tr (\nabla_{W_k} f_{\theta}(x_i) W_k\tp)  \\
&= -\frac{1}{n} \sumin  \ell'(y_i f_{\theta}(x_i)) y_i f_{\theta}(x_i) \,,
\end{align*}
where the last step holds by the fact that for $f_\theta$ locally Lipschitz and positively homogeneous, $f_{\theta}(x) = \tr (\nabla_{W_k} f_{\theta}(x_i) W_k\tp)$ (see e.g. Lemma~9.2 of \citet{mjt_dlt}).
Given that the equation holds independently of layer index $k$, weight matrices all have the same Frobenius norm. 
\end{proof}

\begin{proof}[Proof of \Cref{lem:low-rank}]
By AM-GM inequality, 
\[
\lambda \| W_{k'}\|_F^2 \le \prod_{k=1}^K \|W_k\|_2 \le \left( \frac{1}{K}\sum_{k=1}^K \| W_k\|_2 \right)^K \;.
\]
Therefore,
\begin{align*}
    \frac{1}{K}\sum_{k=1}^K \frac{\| W_k\|_2}{\|W_k \|_F} &= \frac{1}{B}\cdot \frac{1}{K}\sum_{k=1}^K \| W_k\|_2  \\
    &\ge \frac{1}{B} \left(\lambda B^2 \right)^{1/K} \;.
\end{align*}
To get \Cref{eq:low-rank},  we note that by \Cref{lem:norm-preservation}, 
\begin{align*}
    \lambda \|W_k \|_F^2 &= -\frac{1}{n} \sumin  \ell'(y_i f_{\theta}(x_i)) y_i f_{\theta}(x_i) \\
    &\le \frac{1}{n} \sumin  \sqrt{\ell(y_i f_{\theta}(x_i))}\\
    &\le \sqrt{\frac{1}{n} \sumin \ell(y_i f_{\theta}(x_i))} \\
    &= \sqrt{L(\theta)} \;.
\end{align*}
To get \Cref{eq:low-rank2}, we again use \Cref{lem:norm-preservation} and get that for the exponential and logistic losses, we have $B \le 1/(e\lambda)$. 

\end{proof}

\clearpage

\section{Proofs from \Cref{sec:merging}}

\subsection{Proof of \Cref{lem:hidden-null-space}}
\label{proof:lem:hidden-null-space}
  Observe that \ac{GD} update rule is
  \begin{align*}
    W_{s+1} = (1-\eta \lambda) W_s - \eta \, \frac1n \sumin y_i \, \ell_i(y_i \, f_{\th_s}(x_i))  \, D_{s,i} \, u \, x_i\tp
  \end{align*}
  where $D_{s,i} = \diag(\ind(W_s x_i > 0))$.
  Then
  \begin{align*}
    W_{s+1} x' = (1-\eta \lambda) W_s x' - \eta \, \frac1n \sumin y_i \, \ell_i(y_i \, f_{\th_s}(x_i)) D_{s,i} \, u \ip{x_i, x'}
  \end{align*}
  which together with Cauchy-Schwartz inequality implies
  \begin{align*}
    \|W_{s+1} x'\|
    &\leq (1-\eta \lambda) \|W_s x'\| + \eta \, \frac1n \sumin |y_i \, \ell_i(y_i \, f_{\th_s}(x_i))| \|D_{s,i}\|_2 \, \|u\| \abs{\ip{x_i, x'}}\\
    &\leq (1-\eta \lambda) \|W_s x'\| + \ve \, \eta~.
  \end{align*}
  Observe that relation $x_{s+1} \leq a_s x_s + b_s$
unwinds from $t$ to $t_0$
as
$x_t \leq x_{t_0} \prod_{k=t_0}^{t-1} a_k + \sum_{s=t_0}^{t-1} b_s \prod_{k=s+1}^{t-1} a_k$.
So,
\begin{align*}
  \|W_t x'\|
  &\leq \|W_0 x'\| \, (1-\eta \lambda)^t + \ve \, \eta \, \sum_{s=0}^{t-1} \prod_{k=s+1}^{t-1} (1-\eta \lambda)\\
  &= \|W_0 x'\| \, (1-\eta \lambda)^t + \ve \, \eta \, \sum_{s=0}^{t-1} (1-\eta \lambda)^{t-s-1}~.
\end{align*}
\qed

\subsection{Proof of \Cref{thm:shallow}}
\label{proof:thm:shallow}
First observe that
\begin{align*}
    |f_{\th_t + \th_t'}(x) - f_{\th_t}(x)|
  &\leq \abs{ \ip{u, (W_t x + W_t' x)_+} - \ip{u, (W_t x)_+}}\\
  &\leq \|(W_t x + W_t' x)_+ - (W_t x)_+\| \tag{Cauchy-Schwartz inequality}\\
  &\leq \|W_t' x\|
\end{align*}
where the last inequality comes by a basic fact about \ac{ReLU}s (see
  \Cref{sec:def}).
  Now, since $\ell$ is $1$-Lipschitz,
  \begin{align*}
    \ell\pr{y  f_{\th_t + \th_t'}(x)} - \ell\pr{y  f_{\th_t}(x)}
    &=
      \ell\pr{y  \ip{u, (W_t x + W_t' x)_+}} - \ell\pr{y  \ip{u, (W_t x)_+}}\\
    &\leq |f_{\th_t + \th_t'}(x) - f_{\th_t}(x)|\\
    &\leq \|W_t' x\|~.
  \end{align*}
    At this point, we apply losses over $(x_i, y_i)_i$ and
  average to have
  \begin{align*}
    L(\th_t + \th_t') \leq L(\th_t) + \frac1n \sumin \|W_t' x_i\|~.
  \end{align*}
  Now we  use \Cref{lem:hidden-null-space} to control $\|W_t' x_i\|$.
  \qed


\subsection{Proof of \Cref{thm:deep-linear-multitask-cor}}
\label{proof:thm:deep-linear-multitask-cor}
\Cref{thm:deep-linear-multitask-cor} is a direct corollary of the following theorem which we show in this section:
\begin{theorem}
  \label{thm:deep-linear-multitask}
  Assume that weight matrices are initialized such that
  for all $k \in [K-1]$
  \begin{align*}
    W_{k+1}(0)\tp W_{k+1}(0) = W_{k}(0) W_{k}(0)\tp~.
  \end{align*}
  Let $\ell$ be a logistic loss and let $L(\th(0)) \leq C$.
  Then, for any $t \geq 0$ and any input $x'$ such that $\max_i \abs{\ip{x_i, x'}} \leq \ve$ we have
  \begin{align*}
  |\ip{w(t), x'}|
  \leq
  |\ip{w(0), x'}| A_1 \, e^{- \lambda K  t}
  +
  A_2 \, \ve
\end{align*}
where
\begin{align*}
  A_1 &= \exp\pr{\frac{B^{2 - \frac{2}{K}} (K-1) (1 +  \lambda K) C}{2 \lambda K}}~,\\
  A_2 &= 2  B^{2 - \frac{2}{K}} C \, A_1 \pr{\frac{1 - e^{- \lambda K t / 2}}{\lambda K}}~,\\
  B &= \|w(0)\|^2 + \frac{C}{\lambda}~.
\end{align*}
\hfill\hyperref[proof:thm:deep-linear-multitask]{[Proof]}
\end{theorem}
In the following for the end-to-end vector $w(t)\tp = w_K(t)\tp W_{K-1}(t) \cdots W_1(t)$ we use notation $L^1$ to denote its loss:
\begin{align*}
    L^1(w(t)) = \frac1n \sumin \ell(y_i  \ip{w(t), x_i})~.
\end{align*}
Note that $L^1(w(t)) =  L(\th(t))$.
Proof relies on the following crucial result connecting per-layer updates to
updates of the product matrix:
\begin{theorem}[{\citet[Theorem 1]{arora2018optimization}}]
  \label{thm:gf-linear-update}
  Assume that weight matrices are initialized (at time $t_0$) in such
  a way that they satisfy for all $k \in [K-1]$.
  \begin{align*}
    W_{k+1}(t_0)\tp W_{k+1}(t_0) = W_{k}(t_0) W_{k}(t_0)\tp
  \end{align*}
  Then, under dynamics with updates as in \cref{eq:GF-W} for the
  end-to-end matrix we have
  \begin{align*}
    \dot W(t) = -  \lambda K \cdot W(t)
    - \sum_{k=1}^K \pr{W(t) W(t)\tp}^{\frac{k-1}{K}}
    \cdot \nabla L^1(W) \cdot \pr{W(t)\tp W(t)}^{\frac{K-k}{K}}~.
  \end{align*}
\end{theorem}
\subsubsection{Proof of \Cref{thm:deep-linear-multitask}}
\label{proof:thm:deep-linear-multitask}
We start from adapting \Cref{thm:gf-linear-update} to our case to get $w(t)$ and then get an identity for $\ip{w(t), x'}$ by solving the resulting differential equation.
Once we get dependence on $\ve$ we must ensure that the remaining terms (arising from the solution to differential equation) are non-divergent, which we will do through the stationary point convergence argument.

\Cref{thm:gf-linear-update} gives us
\begin{align*}
  \dot w(t)\tp
  &= - \lambda K \cdot w(t)\tp
    -  \|w(t)\|^{\frac{2(K-1)}{K}} \cdot \nabla L^1(w(t))\tp\\
  &\qquad-  \sum_{k=1}^{K-1} \|w(t)\|^{\frac{2(k-1)}{K}} \cdot \nabla L^1(w(t))\tp \pr{w(t) w(t)\tp}^{\frac{K-k}{K}}\\
  &= - \lambda K \cdot w(t)\tp
    -  \|w(t)\|^{2 - \frac{2}{K}} \cdot \nabla L^1(w(t))\tp\\
  &\qquad-  \sum_{k=1}^{K-1} \|w(t)\|^{\frac{2(k-1)}{K} + \frac{2(K-k)}{K}} \cdot \nabla L^1(w(t))\tp \pr{\frac{w(t) w(t)\tp}{\|w(t)\|^2}}^{\frac{K-k}{K}}\\
  &= - \lambda K \cdot w(t)\tp
    -  \|w(t)\|^{2 - \frac{2}{K}} \cdot \nabla L^1(w(t))\tp\\
  &\qquad-  (K-1) \|w(t)\|^{2 - \frac{2}{K}} \cdot \ip{\nabla L^1(w(t)), w(t)} w(t)\tp~.
\end{align*}
Put another way,
\begin{align}
  \label{eq:linear-nets-upd}
  \dot w(t)
  = \pr{- \lambda K -  (K-1) \|w(t)\|^{2 - \frac{2}{K}} \, \ip{\nabla L^1(w(t)), w(t)}} \, w(t)
  -  \|w(t)\|^{2 - \frac{2}{K}} \, \nabla L^1(w(t))~.
\end{align}
Taking dot product with $x'$ gives
\begin{align}
  \label{eq:linear-nets-upd-x}
  \ip{\dot w(t), x'} = a(t) \, \ip{w(t), x'} + b(t)
\end{align}
where we introduce abbreviations
\begin{align*}
  a(t) &:= - \lambda K -  (K-1) \|w(t)\|^{2 - \frac{2}{K}} \ip{\nabla L^1(w(t)), w(t)}\\
  b(t) &:= - \|w(t)\|^{2 - \frac{2}{K}} \ip{\nabla L^1(w(t)), x'}~.
\end{align*}
Solving \cref{eq:linear-nets-upd-x} we get
\begin{align*}
  \ip{w(t), x'}
  &=
    \ip{w(t_0), x'} \, e^{\int_{t_0}^t a(s) \diff s}
    +
    \int_{t_0}^{t} b(s) \, e^{\int_s^t a(r) \diff r} \diff s~.
\end{align*}
Assuming for a moment that $\|w(t)\| \leq B$ (where $B$ will be determined
later) Cauchy-Schwartz inequality gives
\begin{align*}
  &a(t)
    \leq - \lambda K +  (K-1) B^{1 - \frac{1}{K}} \|w(t)\|^{1-\frac{1}{K}} |\ip{\nabla L^1(w(t)), w(t)}|~.
\end{align*}
On the other hand, using the fact that for logistic loss function $|\ell'(z)| \leq \ell(z)$,
\begin{align*}
  b(t)
  &=
     \|w(t)\|^{2 - \frac{2}{K}} \, \frac1n \sumin \ell'(y_i \ip{w(t), x_i}) y_i \ip{x_i, x'}\\
  &\leq
    \ve \cdot  B^{2 - \frac{2}{K}} \, \frac1n \sumin \ell(y_i \ip{w(t), x_i}) \nonumber\\
  &= \ve \cdot  B^{2 - \frac{2}{K}} \, L^1(w(t))~. \nonumber
\end{align*}
So, it is left to show that the term $\int_{t_0}^t a(s) \diff s$ does not diverge.
To this end, we show the following (with proof at the end of the section):
\begin{lemma}[Stationary point convergence for dynamics in \cref{eq:linear-nets-upd}]
  \label{lem:linear-nets-stationary}
  \begin{align*}
   (K-1) \int_s^t \|w(r)\|^{1-\frac{1}{K}} |\ip{\nabla L^1(w(r)), w(r)}| \diff r
  \leq
  \sqrt{ (K-1) (1 +  \lambda K) L^1(w(0)) \cdot (t-s)}~.
\end{align*}
\end{lemma}
%
Using this lemma to bound $\int a(s) \d s$ gives
\begin{align*}
  |\ip{w(t), x'}|
  &\leq
    |\ip{w(0), x'}| \, \underbrace{\exp\pr{- \lambda K \cdot t + B^{1 - \frac{1}{K}} \sqrt{ (K-1) (1 +  \lambda K) C \cdot t}}}_{(i)}\\
  &\qquad+
    \ve \cdot  B^{2 - \frac{2}{K}} C \, \underbrace{\int_{0}^{t} \exp\pr{- \lambda K \cdot (t-s) + B^{1 - \frac{1}{K}} \sqrt{ (K-1) (1 +  \lambda K) C \cdot (t-s)}} \diff s}_{(ii)}
\end{align*}
where we also assumed that $\sup_t L^1(w(t)) \leq C$.
At this point we will bound $(i)$ and $(ii)$ by using the fact that
\begin{align*}
  e^{-a t + b \sqrt{t}} \leq
  e^{\frac{b^2}{2 a}} \, e^{-a t/2} \qquad (a,b,t > 0)~.
\end{align*}
that comes from (choosing $p$ such that $\frac{b}{2 \sqrt{p}} = a/2$):
\begin{proposition}
  For any $a,b,t,p > 0$,
  \begin{align*}
    -a t + b \sqrt{t} \leq \pr{-a + \frac{b}{2 \sqrt{p}}} t  + \frac{b}{2} \sqrt{p}~.
  \end{align*}
\end{proposition}
In particular this gives
\begin{align*}
  (ii) \leq 2 \pr{\frac{1 - e^{- \lambda K t / 2}}{ \lambda K}} \exp\pr{\frac{B^{2 - \frac{2}{K}} (K-1) (1 +  \lambda K) C}{2 \lambda K}}
\end{align*}

As promised, the final bit is to give $B$.
Since objective is non-increasing
\begin{align*}
  \lambda \|w(t)\|^2 \leq \lambda \|w(t)\|^2 + L^1(w(t)) \leq \lambda \|w(0)\|^2 + L^1(w(0)) \qquad \implies \qquad
  B = \|w(0)\|^2 + \frac{L^1(w(0))}{\lambda}~.
\end{align*}

\begin{proof}[Proof of \Cref{lem:linear-nets-stationary}.]
Using the chain rule together with \cref{eq:linear-nets-upd} we have
\begin{align*}
  \frac{\d L^1(w(t))}{\d t}
  &= \ip{\nabla L^1(w(t)), \dot w(t)}\\
  &=
    - \lambda K \ip{\nabla L^1(w(t)), \dot w(t)} -  (K-1) \|w(t)\|^{2 - \frac{2}{K}} \, \ip{\nabla L^1(w(t)), w(t)}^2\\
    &-  \|w(t)\|^{2 - \frac{2}{K}} \, \|\nabla L^1(w(t))\|^2
\end{align*}
and so
\begin{align*}
  L^1(w(t)) - L^1(w(0))
  &=
  - \lambda K \int_0^t \ip{\nabla L^1(w(s)), \dot w(s)} \diff s\\
  &-  (K-1) \int_0^t \|w(s)\|^{2 - \frac{2}{K}} \, \ip{\nabla L^1(w(s)), w(s)}^2 \diff s\\
  &-  \int_0^t \|w(s)\|^{2 - \frac{2}{K}} \, \|\nabla L^1(w(s))\|^2 \diff s~.
\end{align*}
Note also that the chain rule gives
$
  \int_0^t \ip{\nabla L^1(w(s)), \dot w(s)} \diff s
  =
  L^1(w(t)) - L^1(w(0))
$
which gives
\begin{align*}
    & (K-1) \int_0^t \|w(s)\|^{2 - \frac{2}{K}} \, \ip{\nabla L^1(w(s)), w(s)}^2 \diff s
      +
       \int_0^t \|w(s)\|^{2 - \frac{2}{K}} \, \|\nabla L^1(w(s))\|^2 \diff s\\
    &\qquad= (1 +  \lambda K) \pr{L^1(w(0)) - L^1(w(t))}~.
  \end{align*}
  Applying Jensen's inequality completes the proof.
\end{proof}
\qed

\clearpage

\subsection{Proof of \Cref{thm:relu-merging}}
\label{proof:thm:relu-merging}
Before proving the theorem we will require few basic tools about \ac{GF}.
The first tool used in the proof is stationarity of \ac{GF} and its consequences:
\begin{proposition}
  \label{prop:gf-stationary}
  For $W(t)$ obtained through dynamics $\dot W(t) = - \nabla F(W(t))$,
  where $F$ is uniformly continuous possibly non-differentiable and
  satisfies assumptions of Clarke differentiation (see
  \Cref{sec:def}), for almost every $t \geq 0$ we have
  \begin{align*}
    \int_0^t \|\nabla F(W(s))\|_F^2 = F(W(0)) - F(W(t))~.
  \end{align*}
  In particular this implies that
  \begin{itemize}
  \item $t \mapsto F(W(t))$ is non-increasing.
  \item If $F(W(0)) < \infty$ we have $\|\nabla F(W(t))\|_F \to 0$ as $t \to \infty$. %
  \end{itemize}
\end{proposition}
\begin{proof}
  The first statement is
  \begin{align*}
    F(W(t)) - F(W(0)) = - \int_0^t \min\{\|g\|^2 ~:~ g \in \partial F(W(s))\} \diff s = - \int_0^t \|\nabla F(W(s))\|^2 \diff s~.
  \end{align*}
  The second statement is immediate as $F(W(0))$ is bounded.
\end{proof}
The above implies a stationarity result for \ac{GF} with weight decay:
\begin{lemma}
  \label{lem:loss-stationarity}
  For $W_k(s)$ obtained through dynamics of \cref{eq:GF-W},
  for any $k \in [K]$ any $t \geq t_0$,
  \begin{align*}
    \int_{t_0}^t \|\nabla_{W_k} L(\th(s))\|^2 \d s + \lambda \int_{t_0}^t \ip{\nabla_{W_k} L(\th(s)), W_k(s)} \d s = L(\th(t_0)) - L(\th(t))~.
  \end{align*}

  Moreover, assuming that $\th \mapsto f_\th$ is homogeneous and
  satisfies \Cref{asm:on-average-positive-margin} at initial time
  $t_0$, we have
  \begin{align*}
    \int_{t_0}^t \|\nabla_{W_k} L(\th(s))\|^2 \d s \leq L(\th(t_0)) - L(\th(t))~.
  \end{align*}
\end{lemma}
\begin{proof}
  From \Cref{prop:gf-stationary} we have
\begin{align*}
  \int_{t_0}^t \|\nabla_{W_k} L(\th(s)) + \lambda W_k(s)\|^2 \diff s = C:= L(\th(t_0)) + \frac{\lambda}{2} \|W_k(t_0)\|^2 - L(\th(t)) - \frac{\lambda}{2} \|W_k(t)\|^2
\end{align*}
or
\begin{align*}
  \int_{t_0}^t \|\nabla_{W_k} L(\th(s))\|^2 \diff s + 2 \lambda \int_{t_0}^t \ip{\nabla_{W_k} L(\th(s)), W_k(s)} \diff s +  \lambda^2 \int_{t_0}^t \| W_k(s)\|^2 \diff s = C
\end{align*}
On the other hand,
\begin{align*}
  \frac12 \|W_k(t)\|^2 - \frac12 \|W_k(t_0)\|^2
  &=
  \frac12 \int_{t_0}^t \frac{\d}{\d s} \|W_k(s)\|^2 \d s\\
  &=
  \int_{t_0}^t \ip{W_k(s), \dot W_k(s)} \d s\\
  &=
  - \lambda \int_{t_0}^t \|W_k(s)\|^2 \d s
  - \int_{t_0}^t \ip{W_k(s), \nabla_{W_k} L(\th(s))} \diff s
\end{align*}
which implies the first statement.

The second statement comes by observing that by homogeneity of $f$,
\begin{align*}
  \ip{\nabla_{W_k} L(\th(s)), W_k(s)}
  &=
  - \frac1n \sumin y_i \ell'(y_i f_{\th(s)}(x_i)) \ip{\nabla_{W_k} f_{\th(s)}(x_i), W(s)}\\
  &=
    - \frac1n \sumin y_i \ell'(y_i f_{\th(s)}(x_i)) f_{\th(s)}(x_i) \geq 0
\end{align*}
by assumption of the lemma.
\end{proof}

The second crucial component is to show that activations of two different neural
networks trained on different tasks are approximately orthogonal.
\begin{lemma}
  \label{lem:almost-ortho-activations}
  Suppose that parameters for two \ac{ReLU} neural networks
  $\th(t), \th'(t)$ are obtained by running \ac{GF} given training
  tuples $(x_i, y_i)_{i=1}^n$ and $(x_i', y_i')_{i=1}^n$ respectively.
  Suppose that at time $t_0$ both $\th(t), \th'(t)$ satisfy on-average
  positive margin condition of \Cref{asm:on-average-positive-margin}.
  Then, for any two inputs $x, x'$, assuming the smallest positive
  normalized activation of both networks satisfy (see
  \cref{def:gamma}) is no smaller than some $\gamma_k > 0$, that is
  \begin{align*}
    \gamma_k \leq \min_{t \geq t_0} \gamma_k(x; \th(t)) \wedge  \min_{t \geq t_0} \gamma_k(x'; \th'(t))
  \end{align*}
  we have
  \begin{align*}
    \ip{h_k(x;\, \th(t)), h_k(x' ;\, \th'(t))} \leq \ip{h_k(x;\, \th(t_0)), h_k(x' ;\, \th'(t_0))}
    \exp\pr{- 2 \lambda (t-t_0) + 2 \, \frac{\sqrt{C \, (t - t_0)}}{\gamma_k}}
  \end{align*}
  where
  $C$ is a constant such that $L(\th(t_0)) \vee L'(\th'(t_0')) \leq C$.
  \hfill\hyperref[sec:proof:lem:almost-ortho-activations]{[Proof]}
\end{lemma}
In particular, this implies
\begin{corollary}
\label{cor:orth-activations}
  Under conditions of \Cref{lem:almost-ortho-activations}
  \begin{align*}
    \lim_{t \to \infty} \inf_{\lambda \in \Lambda(t)} \ip{h_k(x;\, \th(t)), h_k(x' ;\, \th'(t))} = 0
  \end{align*}
  where $\Lambda(t) = \cbr{\lambda \in [0,1] ~:~ \lambda > \frac{\sqrt{C/t}}{\gamma_k}}$.
\end{corollary}





\begin{proof}[Proof of \Cref{thm:relu-merging}]
  For any given layer $k$, our first step is to show that the
  pre-activation of a merged model satisfies:
\begin{align*}
  (W_k + W_k') h_{k-1}(x'; \, \th + \th')
  =
  W_k' h_{k-1}(x'; \, \th + \th')~.
\end{align*}

Assume induction hypothesis on $k$:
\begin{align*}
  \ip{h_{k-1}(x_i'; \, \th'), h_{k-1}(x; \, \th + \th')} = 0
\end{align*}
that is that activations of different networks on point from
respective task, are orthogonal.

\Cref{prop:gf-stationary} (third statement) gives that for the stationary point
\begin{align*}
  W_k = - \frac{1}{\lambda} \nabla L(W_k)~.
\end{align*}
Then, using this fact we have
\begin{align*}
  &(W_k + W_k') h_{k-1}(x; \, \th + \th')\\
  &\quad= - \frac{1}{n\lambda} \sumin y_i \, \ell'(y_i f_{\th}(x_i))  \, p_{k,i}\ip{ h_{k-1}(x_i; \, \th), h_{k-1}(x; \, \th + \th')}\\
  &\qquad- \frac{1}{n\lambda} \sumin y_i' \, \ell'(y_i' f_{\th'}(x_i')) \, p_{k,i}' \ip{h_{k-1}(x_i'; \, \th'), h_{k-1}(x; \, \th + \th')}\\
  &\quad\stackrel{(a)}{=} - \frac{1}{n\lambda} \sumin y_i \, \ell'(y_i f_{\th}(x_i))  \, p_{k,i} \ip{ h_{k-1}(x_i; \, \th), h_{k-1}(x; \, \th + \th')}\\
  &\quad=  W_k h_{k-1}(x; \, \th + \th')
\end{align*}
where in $(a)$ we used in induction hypothesis.
Taking $(\cdot)_+$ on both sides we have
\begin{align*}
  h_k(x ; \, \th + \th') = (W_k h_{k-1}(x; \, \th + \th'))_+
\end{align*}
and expanding this recursion we have
\begin{align*}
  h_k(x ; \, \th + \th') = (W_k (W_{k-1} (\dots W_1 h_0(x; \, \th + \th'))_+)_+)_+ = h_k(x ; \, \th)
\end{align*}
since $h_0(x; \, \th + \th') = x$.

Now we show the step of induction, using the above combined with
\Cref{cor:orth-activations}, namely
\begin{align*}
  \ip{h_k(x_i'; \, \th'), h_k(x; \, \th + \th')}
  =
  \ip{h_k(x_i' ; \, \th'), h_k(x ; \, \th)}
  = 0~.
\end{align*}
Base case of induction is evident since
$\ip{h_0(x_i'; \, \th'), h_0(x; \, \th + \th')} = \ip{x_i', x} = 0$.
\end{proof}





\subsection{Proof of \Cref{lem:almost-ortho-activations}}
\label{sec:proof:lem:almost-ortho-activations}
Throughout the proof notation $(h)_j$ means that we are considering
$j$th coordinate of activation vector $h$, and similarly for $h'$.
We also use abbreviations
\begin{align*}
  h(t) &= h(x; \, W_k(t))\\
  h_{k-1}(t) &= h(x; \, W_{k-1}(t))\\
  h_i(t) &= h(x_i; \, W_k(t))\\
  h_{k-1,i}(t) &= h(x_i; \, W_{k-1}(t))\\
  \nabla (h)_j(W(t)) &= \frac{\diff}{\diff W} (h)_j(x; \, W) \Bigr|_{W=W(t)}
\end{align*}
and similarly for $h'$.
Observe that
\begin{align*}
  \nabla (h)_j(W(t)) = \ind\pr{(h)_j(t) > 0} \, e_j \, h_{k-1}(t)\tp~.
\end{align*}
In particular the above implies that $\ip{W(t), \nabla (h)_j(W(t))} = (h)_j(t)$.

Consider the following dynamics of the inner product between
activations, where by chain rule we have
\begin{align*}
  \frac{\diff}{\diff t} \ip{h(t), h'(t)}
  &=    
    \ip{\dot h(t), h'(t)}
    +
    \ip{h(t), \dot h'(t)}~.
\end{align*}
Focusing on one of the terms on the right hand side (another one is
handled similarly):
\begin{align*}
  \ip{\dot h(t), h'(t)}
  &=
    \sum_j \ip{\dot W(t), \nabla (h)_j(W(t))} \cdot (h')_j(t)\\
  &=
    - \lambda \sum_j \ip{W(t), \nabla (h)_j(W(t))} \cdot (h')_j(t)
    - \sum_j \ip{\nabla L_W(\th(t)), \nabla (h)_j(W(t))} \cdot (h')_j(t)\\
  &=
    - \lambda \ip{h(t), h'(t)}
    + a(t)
\end{align*}
where
\begin{align*}
  a(t) &:= - \sum_j \ip{\nabla L_W(\th(t)), \ind\pr{(h)_j(t) > 0} \cdot (h')_j(t) \, e_j \, h_{k-1}(t)\tp}\\
       &\leq \|\nabla L_W(\th(t))\| \, \|h_{k-1}(t)\|
         \sum_{j=1}^m \ind\pr{(h)_j(t) > 0} \cdot (h')_j(t)\\
       &= \|\nabla L_W(\th(t))\| \, \|h_{k-1}(t)\|
         \sum_{j ~:~ (h)_j(t) > 0} \frac{1}{(h)_j(t)} \, (h)_j(t) \cdot (h')_j(t)\\
       &\leq \frac{1}{\gamma_k} \, \|\nabla L_W(\th(t))\|
         \ip{h(t), h'(t)}
\end{align*}
where we recall that $\gamma_k \leq \min\cbr{ \ip{e_j, W(t) h_{k-1}(t)} / \|h_{k-1}(t)\| ~:~ \ip{e_j, W(t) h_{k-1}(t)} > 0~,  t \geq t_0~, j \in [m]}$.

Putting everything together, we have differential inequality
\begin{align*}
  \frac{\d}{\d t}\ip{\dot h(t), h'(t)}
  \leq
  - 2 \lambda \ip{h(t), h'(t)}
  +
  \pr{\frac{1}{\gamma_k} \,\|\nabla_W L(\th(t))\| + \frac{1}{\gamma'_k} \, \|\nabla_{W'} L'(\th'(t))\|}
  \ip{h(t), h'(t)}
\end{align*}
and so by Gr\"onwall's inequality
\begin{align*}
  \ip{h(t), h'(t)} \leq \ip{h(t_0), h'(t_0)}
  \exp\pr{- 2 \lambda (t-t_0) + \frac{1}{\gamma_k} \int_{t_0}^t \|\nabla_W L(\th(s))\| \diff s + \frac{1}{\gamma'_k} \int_{t_0}^t \|\nabla_{W'} L'(\th'(s))\| \d s}~.
\end{align*}
Here, the second result of \Cref{lem:loss-stationarity} combined with
Jensen's inequality gives
\begin{align*}
  \int_{t_0}^t \|\nabla_W L(\th(s))\| \d s
  \leq
  \sqrt{(t - t_0) (L(\th(t_0)) - L(\th(t)))}
\end{align*}
which completes the proof.


\qed
\clearpage


\section{Additional empirical results from \Cref{sec:experiments}}
\label{sec:experiments-appendix}
\subsection{Transformer results}
\Cref{fig:bert} summarizes more measurements in addition to those in \Cref{sec:experiments}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model name} & \textbf{number of layers} & \textbf{model dimension} & \textbf{FF dimension} \\ \hline
BERT                & 12                & 768           & 3072           \\ \hline
RoBERTa & 12 & 768 & 3072            \\ \hline
 GPT2-Large & 36 & 1280 & 5120 \\ \hline
 GPT2-XL & 48 & 1600 & 6400 \\ \hline
 Phi-2 & 32 & 2560 & 10240 \\ \hline
 GPT-J & 28 & 4096 & 16384 \\ \hline
\end{tabular}
\caption{Specification of Transformer models.}
\label{tab:transformers}
\end{table}
 In \Cref{tab:transformers} model dimension refers to dimensionality of self-attention weight matrices, while FF dimension refers to the size of the hidden layer of the feedforward network.  
\begin{figure}
    \includegraphics[width=0.49\linewidth]{figures/bert-rank.png}
    \includegraphics[width=0.49\linewidth]{figures/bert-norm.png}
    \includegraphics[width=0.49\linewidth]{figures/gpt2-l-rank.png}
    \includegraphics[width=0.49\linewidth]{figures/gpt2-l-norm.png}
    \includegraphics[width=0.49\linewidth]{figures/gpt2-xl-rank.png}
    \includegraphics[width=0.49\linewidth]{figures/gpt2-xl-norm.png}
    \includegraphics[width=0.49\linewidth]{figures/roberta-rank.png}
    \includegraphics[width=0.49\linewidth]{figures/roberta-norm.png}
\caption{Stable ranks and Frobenius norms of different weight matrices in pretrained BERT (first row), GPT2-Large (second row), RoBERTa (third row), and GPT2-XL (fourth row) model.} 
\label{fig:bert}
\end{figure}


\subsection{MLP results}
\label{sec:mlp-more-results}
The second set of experiments is performed on `Fashion MNIST'
dataset~\citep{xiao2017fashion}, which contains grayscale images of 28x28 pixels
each, representing clothing items from 10 different categories.  We adapt this
dataset with sample size $10$ for binary classification by grouping first 5 classes into class 0, and
remaining into class 1.
When we consider two different tasks, we append 784-dimensional zero vector for the first task, and prepend in case of the second task.
This way, inputs from different tasks remain orthogonal, while the length of each input is preserved.
Finally, in case of task one binary labels are preserved as is, while for the second task labels are inverted.
\begin{figure}[H]
  \includegraphics[width=0.5\linewidth]{figures/fig1_fashion_mnist.pdf}
  \includegraphics[width=0.5\linewidth]{figures/fig2_fashion_mnist.pdf}
  \includegraphics[width=0.5\linewidth]{figures/fig3_fashion_mnist.pdf}
  \includegraphics[width=0.5\linewidth]{figures/fig4_fashion_mnist.pdf}
  \caption{(Fashion MNIST dataset) First row: Training neural networks on different tasks (orthogonal inputs) (left) vs. the same task (right) and
    merging the parameters by adding weight matrices. The resulting network
    performs well on different tasks after sufficiently many iterations, while given the same task, it does not.
    Second row: this effect manifests
    when weight decay strength is sufficiently large (left).
    Stable rank of each weight matrix converges to a small value. Merged
    network matches the stable rank of individual networks (right).}
  \label{fig:t-vs-loss-different-tasks-fm}
\end{figure}


\begin{figure}[H]
\end{figure}


\end{document}
