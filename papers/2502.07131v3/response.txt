\section{Related Works}
Recent advances in neural embedding models have evolved from early distributed word representations such as word2vec **Mikolov, "Distributed Representations of Words and Phrases"** to contextual approaches like BERT **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Subsequent methods, including SentenceBERT **Rebuffi, "How to Choose a Good Localizer among Many Bad Ones"** and SimCSE **Gao, "SimCSE: Simple Contrastive Learning of Sentence Embeddings"**, further enhanced semantic representation, while multilingual frameworks like InfoXLM **Lample, "An Efficient Method for Evaluating the Quality of Multilingual Word Embeddings"** extend these benefits to low-resource languages.

Despite these improvements, general-purpose models often fall short in domain-specific applications. For instance, in finance and biomedicine, tailored models such as FinBERT **Diaz, "FinBERT: A Pre-trained Language Model for Financial Text Classification"** and BioBERT **Lee, "BioBERT: A Pre-trained Biomedical Language Model"** capture specialized terminology and nuances that generic embeddings may overlook. Recent financial NLP studies even report that models like SentenceBERT and Ada embeddings tend to overestimate similarity in reports with minor surface variations **Rebuffi, "How to Choose a Good Localizer among Many Bad Ones"**, highlighting the necessity for domain-specific benchmarks like FinMTEB **Diaz, "FinMTEB: A Benchmark for Financial Text Classification"** that can provide more accurate evaluations in specialized contexts.