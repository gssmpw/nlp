\section{Related Works}
Recent advances in neural embedding models have evolved from early distributed word representations such as word2vec \citep{mikolov2013efficientestimationwordrepresentations} to contextual approaches like BERT \citep{devlin2019bertpretrainingdeepbidirectional}. Subsequent methods, including SentenceBERT \citep{reimers2019sentencebertsentenceembeddingsusing} and SimCSE \citep{gao2022simcsesimplecontrastivelearning}, further enhanced semantic representation, while multilingual frameworks like InfoXLM \citep{chi2021infoxlminformationtheoreticframeworkcrosslingual} extend these benefits to low-resource languages.

Despite these improvements, general-purpose models often fall short in domain-specific applications. For instance, in finance and biomedicine, tailored models such as FinBERT \citep{araci2019finbertfinancialsentimentanalysis} and BioBERT \citep{Lee_2019} capture specialized terminology and nuances that generic embeddings may overlook. Recent financial NLP studies even report that models like SentenceBERT and Ada embeddings tend to overestimate similarity in reports with minor surface variations \citep{liu2024surfacesimilaritydetectingsubtle}, highlighting the necessity for domain-specific benchmarks like FinMTEB \citep{tang2024needdomainspecificembeddingmodels} that can provide more accurate evaluations in specialized contexts.