@misc{tang2024needdomainspecificembeddingmodels,
      title={Do We Need Domain-Specific Embedding Models? An Empirical Investigation}, 
      author={Yixuan Tang and Yi Yang},
      year={2024},
      eprint={2409.18511},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.18511}, 
}

@misc{muennighoff2023mtebmassivetextembedding,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07316}, 
}

@misc{son2024kmmlumeasuringmassivemultitask,
      title={KMMLU: Measuring Massive Multitask Language Understanding in Korean}, 
      author={Guijin Son and Hanwool Lee and Sungdong Kim and Seungone Kim and Niklas Muennighoff and Taekyoon Choi and Cheonbok Park and Kang Min Yoo and Stella Biderman},
      year={2024},
      eprint={2402.11548},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11548}, 
}

@misc{son2024haeraebenchevaluationkorean,
      title={HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models}, 
      author={Guijin Son and Hanwool Lee and Suwan Kim and Huiseo Kim and Jaecheol Lee and Je Won Yeom and Jihyu Jung and Jung Woo Kim and Songseong Kim},
      year={2024},
      eprint={2309.02706},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.02706}, 
}

@misc{liu2024surfacesimilaritydetectingsubtle,
      title={Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives}, 
      author={Jiaxin Liu and Yi Yang and Kar Yan Tam},
      year={2024},
      eprint={2403.14341},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.14341}, 
}

@article{Shahriar_Lund_Mannuru_Arshad_Hayawi_Bevara_Mannuru_Batool_2024, title={Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency}, volume={14}, DOI={10.3390/app14177782}, number={17}, journal={Applied Sciences}, publisher={Multidisciplinary Digital Publishing Institute}, author={Shahriar, Sakib and Lund, Brady and Mannuru, Nishith Reddy and Arshad, Muhammad and Hayawi, Kadhim and Bevara, Ravi Varma Kumar and Mannuru, Aashrith and Batool, Laiba}, year={2024}, month=sep, pages={7782} }

@misc{wu2023bloomberggptlargelanguagemodel,
      title={BloombergGPT: A Large Language Model for Finance}, 
      author={Shijie Wu and Ozan Irsoy and Steven Lu and Vadim Dabravolski and Mark Dredze and Sebastian Gehrmann and Prabhanjan Kambadur and David Rosenberg and Gideon Mann},
      year={2023},
      eprint={2303.17564},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.17564}, 
}

@misc{chang2023multilingualitycurselanguagemodeling,
      title={When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages}, 
      author={Tyler A. Chang and Catherine Arnett and Zhuowen Tu and Benjamin K. Bergen},
      year={2023},
      eprint={2311.09205},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09205}, 
}

@misc{reimers2019sentencebertsentenceembeddingsusing,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.10084}, 
}

@misc{gao2022simcsesimplecontrastivelearning,
      title={SimCSE: Simple Contrastive Learning of Sentence Embeddings}, 
      author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
      year={2022},
      eprint={2104.08821},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08821}, 
}

@misc{araci2019finbertfinancialsentimentanalysis,
      title={FinBERT: Financial Sentiment Analysis with Pre-trained Language Models}, 
      author={Dogu Araci},
      year={2019},
      eprint={1908.10063},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.10063}, 
}

@article{Lee_2019,
   title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
   volume={36},
   ISSN={1367-4811},
   url={http://dx.doi.org/10.1093/bioinformatics/btz682},
   DOI={10.1093/bioinformatics/btz682},
   number={4},
   journal={Bioinformatics},
   publisher={Oxford University Press (OUP)},
   author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
   editor={Wren, Jonathan},
   year={2019},
   month=sep, pages={1234–1240} }

@misc{behnamghader2024llm2veclargelanguagemodels,
      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}, 
      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},
      year={2024},
      eprint={2404.05961},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.05961}, 
}
@misc{muennighoff2022sgptgptsentenceembeddings,
      title={SGPT: GPT Sentence Embeddings for Semantic Search}, 
      author={Niklas Muennighoff},
      year={2022},
      eprint={2202.08904},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.08904}, 
}
@misc{chi2021infoxlminformationtheoreticframeworkcrosslingual,
      title={InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training}, 
      author={Zewen Chi and Li Dong and Furu Wei and Nan Yang and Saksham Singhal and Wenhui Wang and Xia Song and Xian-Ling Mao and Heyan Huang and Ming Zhou},
      year={2021},
      eprint={2007.07834},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.07834}, 
}

@misc{mikolov2013efficientestimationwordrepresentations,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}
@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{son2023removingnonstationaryknowledgepretrained,
      title={Removing Non-Stationary Knowledge From Pre-Trained Language Models for Entity-Level Sentiment Classification in Finance}, 
      author={Guijin Son and Hanwool Lee and Nahyeon Kang and Moonjeong Hahm},
      year={2023},
      eprint={2301.03136},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.03136}, 
}

@misc{li2024makingtextembeddersfewshot,
      title={Making Text Embedders Few-Shot Learners}, 
      author={Chaofan Li and MingHao Qin and Shitao Xiao and Jianlyu Chen and Kun Luo and Yingxia Shao and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2409.15700},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2409.15700}, 
}

@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}
@article{wang2023improving,
  title={Improving Text Embeddings with Large Language Models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}
@misc{su2023embeddertaskinstructionfinetunedtext,
      title={One Embedder, Any Task: Instruction-Finetuned Text Embeddings}, 
      author={Hongjin Su and Weijia Shi and Jungo Kasai and Yizhong Wang and Yushi Hu and Mari Ostendorf and Wen-tau Yih and Noah A. Smith and Luke Zettlemoyer and Tao Yu},
      year={2023},
      eprint={2212.09741},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09741}, 
}
@misc{wang2020minilmdeepselfattentiondistillation,
      title={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers}, 
      author={Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},
      year={2020},
      eprint={2002.10957},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.10957}, 
}
@misc{KURE,
  author = {Youngjoon Jang and Junyoung Son and Taemin Lee},
  year = {2024},
  url = {https://github.com/nlpai-lab/KURE}
},