\section{Related Works}
\label{sec:related}

% Many approaches have been proposed to address this problem, including gradient compression~\cite{10.1145/3452296.3472904,tang2020survey} and task scheduling by overlapping gradient communication with computation~\cite{203269,9488803}. Nevertheless, the limited bandwidth precludes the possibility of fully concealing communication through computation, even when employing the most advanced compression techniques, which offer a compression ratio of approximately $50\times$~\cite{agarwal2022utility,GCCharge,tang2020survey}. And the gradient compression may require extra potential computation time, which prevents it from deployment~\cite{agarwal2022utility}. Besides, the sparsity is reduced by the number of workers as different indexes of gradient are left, which severely limits the scalability of gradient compression.

\textbf{Overlapping Communication and Computation.}
In distributed training with S-SGD, the gradient of each layer can be communicated instantly after its BP and overlapped with BP of subsequent layers, thus saving total training time~\cite{203269}. Overlapping communication with feed-forward computing~\cite{10.1145/3341301.3359642} helps further reduce total training time. Some observe that merging small tensors for communication~\cite{9155269} can help improve communication efficiency. ASC-WFBP~\cite{9488803} further improves overlapping by simultaneous communications and scheduling. Different from them, we consider overlapping communication and computation in LSGD.

% Some work~\cite{wang2020overlap,9488803} also proposes to overlap the communication of LSGD with local training. However, ~\cite{wang2020overlap} requires an additional anchor model and is completely different from our core idea, i.e. decoupling layers for overlapping. 
% WFBP~\cite{203269} proposes to 

% Considering the communication topology, some works~\cite{9796688,GossipFL} considers decoupling the synchronization between all workers into group-wise all-reduce. Thus, only a part of workers independently conduct the collective communication.
\textbf{LSGD.}
% LSGD~\cite{stich2018localsgd,woodworth2020minibatch,AperiodcLocalSGD,sadiev2022communication} aims to reduce the communication time from the communication frequency perspective. 
LSGD~\cite{stich2018localsgd,woodworth2020minibatch,AperiodcLocalSGD,sadiev2022communication} reduces the communication time by periodically averaging model weights instead of gradients. The synchronization frequency strongly influences the convergence and communication time. Thus, some works vary synchronization periods to tradeoff convergence and communication~\cite{stich2018localsgd,AperiodcLocalSGD,wang2019adaptive}. The strong synchronization severely restricts the potential improvements of LSGD, preventing it from communication overlapping. To this end, we propose partial synchronization to decouple the whole model and synchronize layers at different training iterations.

% The LSGD with varied synchronization periods has been proved to have the same convergence rate of the S-SGD~\cite{stich2018localsgd,AperiodcLocalSGD}. Some work propose increasing communication period instead of decreasing~\cite{shen2021stl}. While some work finds the decreasing the period of LSGD is better~\cite{wang2019adaptive}. Different from focusing on varied synchronization periods, our method proposes decoupling the whole model and partially synchronizing layers at different training iterations.


The data distribution in different workers also influences the convergence rate of LSGD~\cite{woodworth2020minibatch}. In this work, we consider the IID data in centralized distributed training rather than federated Learning. Nevertheless, our method and the convergence analysis can be easily extended to the federated learning scenarios~\cite{kairouz2021advances,VHL}.


\textbf{Reducing Communication Costs.}
To reduce the communication time, some works propose compressing the communicated gradients~\cite{10.1145/3452296.3472904,tang2020survey} in S-SGD or model parameters~\cite{10229032,10229086,GossipFL} in LSGD. However, the compression may require large potential computation time and slow convergence, which prevents it from deployment~\cite{agarwal2022utility}. ~\cite{10229032,10229086} proposed compressing local results of Federated Learning~\cite{VHL,GossipFL}, in which LSGD is used. Orthogonal to this direction, our method aims to modify LSGD to improve system efficiency.


% Some works proposes compressing local updates to further reduce the communication costs~\cite{basu2019qsparse}.


% SWARM~\cite{ryabinin2023swarm}, Learning@home~\cite{DecentMOE}, CocktailSGD~\cite{cocktailSGD}, AQ-SGD~\cite{NEURIPS2022_7a43b8eb} and DT-FM~\cite{yuandecentralized} are the first to use low-bandwidth connected devices to train larger models such as GPT-2 and large Transformers.

% Some works including SETI@home~\cite{895191} and CoolStreaming~\cite{DONet} aggregate geo-distributed computing and bandwidth resources to complete tasks. These pioneering works bear a striking resemblance to our decentralized training concept for LLMs. In this paper, we propose a decentralized training system, 




\vspace{-0.1cm}