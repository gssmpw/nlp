@inproceedings{10.1145/3341301.3359642,
author = {Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong},
title = {A generic communication scheduler for distributed DNN training acceleration},
year = {2019},
booktitle = {SOSP},
pages = {16–29},
numpages = {14},
}

@inproceedings{10.1145/3452296.3472904,
author = {Fei, Jiawei and Ho, Chen-Yu and Sahu, Atal N. and Canini, Marco and Sapio, Amedeo},
title = {Efficient sparse collective communication and its application to accelerate distributed deep learning},
year = {2021},
booktitle = {ACM SIGCOMM},
pages = {676–691},
numpages = {16},
series = {SIGCOMM '21}
}

@INPROCEEDINGS{10229032,
  author={Guan, Yixuan and Liu, Xuefeng and Ren, Tao and Niu, Jianwei},
  booktitle={INFOCOM}, 
  title={Enabling Communication-Efficient Federated Learning via Distributed Compressed Sensing}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
}

@INPROCEEDINGS{10229086,
  author={Wang, Juncheng and Liang, Ben and Dong, Min and Boudreau, Gary and Afana, Ali},
  booktitle={INFOCOM}, 
  title={Online Distributed Optimization with Efficient Communication via Temporal Similarity}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
}

@inproceedings{203269,
author = {Hao Zhang and Zeyu Zheng and Shizhen Xu and Wei Dai and Qirong Ho and Xiaodan Liang and Zhiting Hu and Jinliang Wei and Pengtao Xie and Eric P. Xing},
title = {Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on {GPU} Clusters},
booktitle = {USENIX ATC},
year = {2017},
address = {Santa Clara, CA},
pages = {181--193},
month = jul
}

@ARTICLE{895191,
	author       = {Korpela, E. and Werthimer, D. and Anderson, D. and Cobb, J. and Leboisky, M.},
	journal      = {Computing in Science & Engineering}

@inproceedings{9155269,
	author       = {Shi, Shaohuai and Wang, Qiang and Chu, Xiaowen and Li, Bo and Qin, Yang and Liu, Ruihao and Zhao, Xinxiao},
	booktitle    = {IEEE INFOCOM},
	title        = {Communication-Efficient Distributed Deep Learning with Merged Gradient Sparsification on GPUs},
	year         = {2020}
}

@INPROCEEDINGS{9488803,
  author={Shi, Shaohuai and Chu, Xiaowen and Li, Bo},
  booktitle={IEEE INFOCOM}, 
  title={Exploiting Simultaneous Communications to Accelerate Data Parallel Distributed Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
}

@INPROCEEDINGS{9796688,
  author={Wang, Weiyan and Zhang, Cengguang and Yang, Liu and Chen, Kai and Tan, Kun},
  booktitle={INFOCOM}, 
  title={Addressing Network Bottlenecks with Divide-and-Shuffle Synchronization for Distributed DNN Training}, 
  year={2022},
  pages={320-329},
}

@inproceedings{AperiodcLocalSGD,
author = {Zhang, Hao and Wu, Tingting and Cheng, Siyao and Liu, Jie},
title = {Aperiodic Local SGD: Beyond Local SGD},
year = {2023},
booktitle = {ICPP},
articleno = {1},
numpages = {10},
location = {Bordeaux, France},
}

@INPROCEEDINGS{DONet,
	author       = {Xinyan Zhang and Jiangchuan Liu and Bo Li and Yum, Y.-S.P.},
	booktitle    = {Proceedings IEEE 24th Annual Joint Conference of the IEEE Computer and Communications Societies.},
	title        = {CoolStreaming/DONet: a data-driven overlay network for peer-to-peer live media streaming},
	year         = {2005},
	volume       = {3},
	number       = {},
	pages        = {2102--2111 vol. 3},
	doi          = {10.1109/INFCOM.2005.1498486}
}

@inproceedings{DecentMOE,
	author       = {Ryabinin, Max and Gusev, Anton},
	booktitle    = {NeurIPS},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages        = {3659--3672},
	publisher    = {Curran Associates, Inc.},
	title        = {Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts},
	volume       = {33},
	year         = {2020}
}

@inproceedings{GCCharge,
	author       = {Bai, Youhui and Li, Cheng and Zhou, Quan and Yi, Jun and Gong, Ping and Yan, Feng and Chen, Ruichuan and Xu, Yinlong},
	title        = {Gradient Compression Supercharged High-Performance Data Parallel DNN Training},
	year         = {2021},
	isbn         = {9781450387095},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	pages        = {359–375},
	numpages     = {17},
	location     = {Virtual Event, Germany},
	series       = {SOSP '21}
}

@ARTICLE{GossipFL,
	author       = {Tang, Zhenheng and Shi, Shaohuai and Li, Bo and Chu, Xiaowen},
	journal      = {IEEE Transactions on Parallel and Distributed Systems},
	title        = {GossipFL: A Decentralized Federated Learning Framework with Sparsified and Adaptive Communication},
	year         = {2022},
	volume       = {},
	number       = {},
	pages        = {1--13},
	doi          = {10.1109/TPDS.2022.3230938}
}

@inproceedings{NEURIPS2022_7a43b8eb,
	author       = {Wang, Jue and Yuan, Binhang and Rimanic, Luka and He, Yongjun and Dao, Tri and Chen, Beidi and R\'{e}, Christopher and Zhang, Ce},
	booktitle    = {NeurIPS},
	title        = {Fine-tuning Language Models over Slow Networks using Activation Quantization with Guarantees},
	year         = {2022}
}

@InProceedings{VHL,
	title        = {Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning},
	author       = {Tang, Zhenheng and Zhang, Yonggang and Shi, Shaohuai and He, Xin and Han, Bo and Chu, Xiaowen},
	booktitle    = {ICML},
	pages        = {21111--21132},
	year         = {2022},
	volume       = {162},
}

@article{agarwal2022utility,
  title={On the utility of gradient compression in distributed training systems},
  author={Agarwal, Saurabh and Wang, Hongyi and Venkataraman, Shivaram and Papailiopoulos, Dimitris},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={652--672},
  year={2022}
}

@article{basu2019qsparse,
  title={Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations},
  author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@InProceedings{cocktailSGD,
	title        = {{C}ocktail{SGD}: Fine-tuning Foundation Models over 500{M}bps Networks},
	author       = {Wang, Jue and Lu, Yucheng and Yuan, Binhang and Chen, Beidi and Liang, Percy and De Sa, Christopher and Re, Christopher and Zhang, Ce},
	booktitle    = {ICML},
	year         = {2023},
	publisher    = {PMLR}
}

@misc{kairouz2021advances,
	title        = {Advances and Open Problems in Federated Learning},
	author       = {Peter Kairouz et al},
	year         = {2021},
	eprint       = {1912.04977},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG}
}

@inproceedings{ryabinin2023swarm,
author = {Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
title = {SWARM parallelism: training large models can be surprisingly communication-efficient},
year = {2023},
booktitle = {ICML},
}

@article{sadiev2022communication,
  title={Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox},
  author={Sadiev, Abdurakhmon and Kovalev, Dmitry and Richt{\'a}rik, Peter},
  journal={NeurIPS},
  volume={35},
  pages={21777--21791},
  year={2022}
}

@inproceedings{shen2021stl,
  title={STL-SGD: Speeding up local SGD with stagewise communication period},
  author={Shen, Shuheng and Cheng, Yifei and Liu, Jingchang and Xu, Linli},
  booktitle={AAAI},
  volume={35},
  number={11},
  pages={9576--9584},
  year={2021}
}

@inproceedings{stich2018localsgd,
    title={Local {SGD} Converges Fast and Communicates Little},
    author={Sebastian U. Stich},
    booktitle={ICLR},
    year={2019},
}

@article{tang2020survey,
	title        = {Communication-efficient distributed deep learning: A comprehensive survey},
	author       = {Tang, Zhenheng and Shi, Shaohuai and Chu, Xiaowen and Wang, Wei and Li, Bo},
	journal      = {arXiv preprint arXiv:2003.06307},
	year         = {2020}
}

@inproceedings{wang2019adaptive,
	title        = {Adaptive communication strategies to achieve the best error-runtime trade-off in local-update SGD},
	author       = {Wang, Jianyu and Joshi, Gauri},
	booktitle      = {MLsys},
	year         = {2019}
}

@inproceedings{wang2020overlap,
  title={Overlap local-SGD: An algorithmic approach to hide communication delays in distributed SGD},
  author={Wang, Jianyu and Liang, Hao and Joshi, Gauri},
  booktitle={ICASSP},
  pages={8871--8875},
  year={2020},
}

@article{woodworth2020minibatch,
	title        = {Minibatch vs local sgd for heterogeneous distributed learning},
	author       = {Woodworth, Blake E and Patel, Kumar Kshitij and Srebro, Nati},
	journal      = {NeurIPS},
	volume       = {33},
	pages        = {6281--6292},
	year         = {2020}
}

@article{yuandecentralized,
  title={Decentralized training of foundation models in heterogeneous environments},
  author={Yuan, Binhang and He, Yongjun and Davis, Jared and Zhang, Tianyi and Dao, Tri and Chen, Beidi and Liang, Percy S and Re, Christopher and Zhang, Ce},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25464--25477},
  year={2022}
}

