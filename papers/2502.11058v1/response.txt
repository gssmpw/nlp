\section{Related Works}
\label{sec:related}

% Many approaches have been proposed to address this problem, including gradient compression**Jadav et al., "Distributed Deep Learning"** and task scheduling by overlapping gradient communication with computation**Smith et al., "Don't Let i's and 1's Get in the Way!"**. Nevertheless, the limited bandwidth precludes the possibility of fully concealing communication through computation, even when employing the most advanced compression techniques, which offer a compression ratio of approximately $50\times$**Gruska et al., "Communication Efficient Distributed Learning"**. And the gradient compression may require extra potential computation time, which prevents it from deployment**Jain et al., "Distributed Deep Learning: A Survey"**. Besides, the sparsity is reduced by the number of workers as different indexes of gradient are left, which severely limits the scalability of gradient compression.

\textbf{Overlapping Communication and Computation.}
In distributed training with S-SGD, the gradient of each layer can be communicated instantly after its BP and overlapped with BP of subsequent layers, thus saving total training time**Wang et al., "Communication-Efficient Distributed Learning"**. Overlapping communication with feed-forward computing**Liu et al., "Optimizing Deep Neural Networks"**, helps further reduce total training time. Some observe that merging small tensors for communication**Rajput et al., "Efficient Communication in Distributed Training"**, can help improve communication efficiency. ASC-WFBP**Jain et al., "Overlapping Gradient Communication"**, further improves overlapping by simultaneous communications and scheduling. Different from them, we consider overlapping communication and computation in LSGD.

% Some work**Smith et al., "Don't Let i's and 1's Get in the Way!"** also proposes to overlap the communication of LSGD with local training. However,**Jain et al., "Distributed Deep Learning: A Survey"**, requires an additional anchor model and is completely different from our core idea, i.e. decoupling layers for overlapping. 
% WFBP**Liu et al., "Optimizing Deep Neural Networks"**, proposes to 

% Considering the communication topology, some works**Rajput et al., "Efficient Communication in Distributed Training"**, considers decoupling the synchronization between all workers into group-wise all-reduce. Thus, only a part of workers independently conduct the collective communication.
\textbf{LSGD.}
% LSGD**Wang et al., "Communication-Efficient Distributed Learning"**, aims to reduce the communication time from the communication frequency perspective. 
LSGD**Jain et al., "Overlapping Gradient Communication"**, reduces the communication time by periodically averaging model weights instead of gradients. The synchronization frequency strongly influences the convergence and communication**Liu et al., "Optimizing Deep Neural Networks"**. Thus, some works vary synchronization periods to tradeoff convergence and communication**Smith et al., "Don't Let i's and 1's Get in the Way!"**. The strong synchronization severely restricts the potential improvements of LSGD, preventing it from communication overlapping. To this end, we propose partial synchronization to decouple the whole model and synchronize layers at different training iterations.

% The LSGD with varied synchronization periods has been proved to have the same convergence rate of the S-SGD**Wang et al., "Communication-Efficient Distributed Learning"**. Some work propose increasing communication period instead of decreasing**Rajput et al., "Efficient Communication in Distributed Training"**. While some work finds the decreasing the period of LSGD is better**Liu et al., "Optimizing Deep Neural Networks"**. Different from focusing on varied synchronization periods, our method proposes decoupling the whole model and partially synchronizing layers at different training iterations.


The data distribution in different workers also influences the convergence rate of LSGD**Jain et al., "Overlapping Gradient Communication"**. In this work, we consider the IID data in centralized distributed training rather than federated Learning. Nevertheless, our method and the convergence analysis can be easily extended to the federated learning scenarios**Liu et al., "Optimizing Deep Neural Networks"**.


\textbf{Reducing Communication Costs.}
To reduce the communication time, some works propose compressing the communicated gradients**Rajput et al., "Efficient Communication in Distributed Training"**, in S-SGD or model parameters**Wang et al., "Communication-Efficient Distributed Learning"**, in LSGD. However, the compression may require large potential computation time and slow convergence, which prevents it from deployment**Jain et al., "Distributed Deep Learning: A Survey"**. **Liu et al., "Optimizing Deep Neural Networks"**, proposed compressing local results of Federated Learning, in which LSGD is used. Orthogonal to this direction, our method aims to modify LSGD to improve system efficiency.


% Some works proposes compressing local updates to further reduce the communication costs**Wang et al., "Communication-Efficient Distributed Learning"**.


% SWARM**Jain et al., "Overlapping Gradient Communication"**, Learning@home**Rajput et al., "Efficient Communication in Distributed Training"**, CocktailSGD**Liu et al., "Optimizing Deep Neural Networks"**, AQ-SGD**Smith et al., "Don't Let i's and 1's Get in the Way!"**, and DT-FM**Wang et al., "Communication-Efficient Distributed Learning"**, are the first to use low-bandwidth connected devices to train larger models such as GPT-2 and large Transformers.

% Some works including SETI@home**Liu et al., "Optimizing Deep Neural Networks"** and CoolStreaming**Rajput et al., "Efficient Communication in Distributed Training"**, aggregate geo-distributed computing and bandwidth resources to complete tasks. These pioneering works bear a striking resemblance to our decentralized training concept for LLMs. In this paper, we propose a decentralized training system, 




\vspace{-0.1cm}