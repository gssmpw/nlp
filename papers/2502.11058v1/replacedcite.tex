\section{Related Works}
\label{sec:related}

% Many approaches have been proposed to address this problem, including gradient compression____ and task scheduling by overlapping gradient communication with computation____. Nevertheless, the limited bandwidth precludes the possibility of fully concealing communication through computation, even when employing the most advanced compression techniques, which offer a compression ratio of approximately $50\times$____. And the gradient compression may require extra potential computation time, which prevents it from deployment____. Besides, the sparsity is reduced by the number of workers as different indexes of gradient are left, which severely limits the scalability of gradient compression.

\textbf{Overlapping Communication and Computation.}
In distributed training with S-SGD, the gradient of each layer can be communicated instantly after its BP and overlapped with BP of subsequent layers, thus saving total training time____. Overlapping communication with feed-forward computing____ helps further reduce total training time. Some observe that merging small tensors for communication____ can help improve communication efficiency. ASC-WFBP____ further improves overlapping by simultaneous communications and scheduling. Different from them, we consider overlapping communication and computation in LSGD.

% Some work____ also proposes to overlap the communication of LSGD with local training. However, ____ requires an additional anchor model and is completely different from our core idea, i.e. decoupling layers for overlapping. 
% WFBP____ proposes to 

% Considering the communication topology, some works____ considers decoupling the synchronization between all workers into group-wise all-reduce. Thus, only a part of workers independently conduct the collective communication.
\textbf{LSGD.}
% LSGD____ aims to reduce the communication time from the communication frequency perspective. 
LSGD____ reduces the communication time by periodically averaging model weights instead of gradients. The synchronization frequency strongly influences the convergence and communication time. Thus, some works vary synchronization periods to tradeoff convergence and communication____. The strong synchronization severely restricts the potential improvements of LSGD, preventing it from communication overlapping. To this end, we propose partial synchronization to decouple the whole model and synchronize layers at different training iterations.

% The LSGD with varied synchronization periods has been proved to have the same convergence rate of the S-SGD____. Some work propose increasing communication period instead of decreasing____. While some work finds the decreasing the period of LSGD is better____. Different from focusing on varied synchronization periods, our method proposes decoupling the whole model and partially synchronizing layers at different training iterations.


The data distribution in different workers also influences the convergence rate of LSGD____. In this work, we consider the IID data in centralized distributed training rather than federated Learning. Nevertheless, our method and the convergence analysis can be easily extended to the federated learning scenarios____.


\textbf{Reducing Communication Costs.}
To reduce the communication time, some works propose compressing the communicated gradients____ in S-SGD or model parameters____ in LSGD. However, the compression may require large potential computation time and slow convergence, which prevents it from deployment____. ____ proposed compressing local results of Federated Learning____, in which LSGD is used. Orthogonal to this direction, our method aims to modify LSGD to improve system efficiency.


% Some works proposes compressing local updates to further reduce the communication costs____.


% SWARM____, Learning@home____, CocktailSGD____, AQ-SGD____ and DT-FM____ are the first to use low-bandwidth connected devices to train larger models such as GPT-2 and large Transformers.

% Some works including SETI@home____ and CoolStreaming____ aggregate geo-distributed computing and bandwidth resources to complete tasks. These pioneering works bear a striking resemblance to our decentralized training concept for LLMs. In this paper, we propose a decentralized training system, 




\vspace{-0.1cm}