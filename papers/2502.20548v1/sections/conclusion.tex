\section{Conclusion}
We introduce \ALG, a theoretically-grounded distributional RL algorithm for LLM post-training that provably converges to the optimal policy under mild assumptions. Through experiments on both synthetic and math reasoning tasks, we demonstrated that \ALG consistently outperforms prior value-based methods by achieving higher accuracy with lower KL divergence from the reference policy. Our approach's success in correcting pre-training shortcuts, combined with its practical advantages, establishes \ALG as a promising direction for enhancing LLM capabilities in post-training. A natural next-step is to apply \ALG for Pareto-optimal pluralistic alignment by interpolating classifiers \citep{mudgal2023controlled,wang2024conditioned}.
