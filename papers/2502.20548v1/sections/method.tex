\section{Method}

\subsection{Preliminaries}\label{sec:preliminaries}
The goal of this paper is to solve KL-regularized RL in deterministic Markov Decision Processes (MDPs), where LLM post-training is a special case and our main focus.
First, we introduce the formal setup.
The MDP consists of a state space $\Xcal$, action space $\Ycal$, horizon $H$, transition kernels $(P_1,\dots,P_H)$, where $P_h:\Xcal\times\Ycal\mapsto\Delta(\Xcal)$ and known reward functions $(r_1,\dots,r_H)$, where $r_h:\Xcal\times\Ycal\mapsto\RR$.
To interact with the MDP, we use policies of the form $\pi=(\pi_1,\dots,\pi_H)$ s.t. $\pi_h:\Xcal\mapsto\Delta(\Ycal)$, which takes actions given the current state at each step.
For a given $\eta > 0$, the soft value $V^{\pi,\eta}$ of a policy $\pi$ is the expected cumulative rewards subject to KL regularization, defined as,
\begin{equation}
    \textstyle\EE_\pi[\sum_{h=1}^H r_h(x_h,y_h)-\eta\,\KL(\pi_h(x_h)\Mid\piref_h(x_h))], \label{eq:soft-value-def}
\end{equation}
where recall the KL divergence is defined as $\KL(p\Mid q)=\EE_{z\sim p}[\ln(p(z)/q(z))]$ for distributions $p,q$.
In \cref{eq:soft-value-def}, the expectation with subscript $\pi$ denotes the randomness is coming from the random trajectory $(x_1,y_1,\dots,x_H,y_H)$ of $\pi$ interacting with the MDP where the initial state $x_1$ is sampled from a known distribution $d_1$.
Our goal is to learn the optimal policy $\pi^{\star,\eta} = \argmax_\pi V^{\pi,\eta}$.

A classical result is that KL-regularized RL can be solved using the soft Bellman equations \citep{ziebart2008maximum}.
Specifically, we can compute $\pi^{\star,\eta}_h$ by iterating the following equations from $h=H,H-1,\dots,1$:
\begin{align}
    &V^{\star,\eta}_{H+1}(x) = 0, \nonumber
    \\&Q_h^{\star,\eta}(x,y) = r_h(x,y)+\EE_{x'\sim P_h(x,y)}[V^{\star,\eta}_{h+1}(x')], \nonumber
    \\&\pi_h^{\star,\eta}(y\mid x)\propto \piref_h(y\mid x)\exp(\eta^{-1}Q^{\star,\eta}_h(x,y)), \label{eq:optimal-policy}
    \\&V_h^{\star,\eta}(x)=\eta\ln\EE_{y\sim\piref(x)}\exp(\eta^{-1}Q^{\star,\eta}_h(x,y)). \nonumber
\end{align}
In other words, the optimal action distribution $\pi^{\star,\eta}_h(\cdot\mid x)$ is simply the softmax of the $Q^{\star,\eta}_h$ function weighted by $\piref$'s action distribution.
Moreover, $Q^{\star,\eta}_h$ has a special interpretation: it is the highest achievable expected cumulative reward with KL regularization starting from state $x$, action $y$, at time $h$.
Specifically, if we define $Q^{\pi,\eta}_h(x,y)=\EE_\pi[\sum_{t\geq h}r_t(x_t,y_t)-\eta\I{t>h}\,\KL(\pi_t(x_t)\Mid\piref_t(x_t))\mid x_h=x,y_h=y]$, then $Q^{\star,\eta}_h(x,y)=\max_\pi Q^{\pi,\eta}_h(x,y)$ and the argmax is precisely the $\pi^{\star,\eta}_h$ defined in \cref{eq:optimal-policy}.
Similarly, if we define $V^{\pi,\eta}_h(x)=\EE_\pi[\sum_{t\geq h} r_t(x_t,y_t)-\eta\,\KL(\pi_t(x_t)\Mid\piref_t(x_t))\mid x_h=x]$, then $V^{\star,\eta}_h(x)=\max_\pi V^{\pi,\eta}_h(x)$.

In this paper, we focus on solving deterministic MDPs where the transitions $P_h$ are deterministic, which includes LLM post-training and many other problems (\eg, diffusion, \citealp{domingo2024adjoint}) as a special case.
\begin{assumption}\label{asm:deterministic-mdp}
The transitions $P_h$ are deterministic.
\end{assumption}
Under \cref{asm:deterministic-mdp}, we can greatly simplify the value function by shedding its recursive dependencies:
\begin{align}
&\exp(\eta^{-1}V^{\star,\eta}_h(x)) \nonumber
\\&=\EE_{y\sim\piref_h(x)}[\exp(\eta^{-1}r_h(x,y)+\eta^{-1}V^{\star,\eta}_{h+1}(x'))] \label{eq:deterministic-key-one-step}
\\&=\textstyle\EE_{\piref}[\exp(\eta^{-1}\sum_{t\geq h}r_t(x_t,y_t))\mid x_h=x], \label{eq:deterministic-key-unroll}
\end{align}
where \cref{eq:deterministic-key-one-step} is due to deterministic $P_h$, and \cref{eq:deterministic-key-unroll} is due to unrolling for multiple steps.
In sum, we have shown the following theorem which is a known result from \citet{piche2018probabilistic,li2024derivative,domingo2024adjoint}.
\begin{theorem}\label{thm:deterministic-mdp-v-star}
Under \cref{asm:deterministic-mdp}, we have
\begin{align*}
&\textstyle V^{\star,\eta}_h(x_h)=\eta\ln\EE_{\piref}[\exp(\eta^{-1}\sum_{t\geq h}r_t(x_t,y_t))\mid x_h],
\\&\textstyle Q^{\star,\eta}_h(x_h,y_h)=\eta\ln\EE_{\piref}[\exp(\eta^{-1}\sum_{t\geq h}r_t(x_t,y_t))\mid x_h,y_h].
\end{align*}
\end{theorem}
This shows that $V^{\star,\eta}$ and $Q^{\star,\eta}$ are simple functionals of $Z^\star$ -- the cumulative reward distribution of $\piref$ -- where the functional is $f(P)=\eta\ln\EE_P\exp(X/\eta)$.
In other words, if we learn the cumulative reward distribution of $\piref$, then we can directly compute $V^{\star,\eta}$ and $Q^{\star,\eta}$, without any dynamic programming. This has several benefits.

First, we do not require temporal difference (TD) learning (\ie, bootstrapping) which is notoriously unstable with deep networks \citep{van2018deep} and requires completeness-type assumptions to guarantee convergence in theory \citep{munos2008finite}.
Second, fitting the reward-to-go distribution $Z^\star$ or regressing $\EE_{\piref}[\exp(\eta^{-1}\sum_{t\geq h}r_t)]$ is a standard supervised learning task with a fixed target, which is much more stable in practice and well-understood in theory. Notably, there is no bootstrapping or changing targets which is what renders deep RL fragile. 
Third, we can leverage distributional RL (DistRL) \footnote{In this work, by distributional RL, we mean that we fit the distribution of the rewards of $\piref$ using supervised learning (\eg, maximum likelihood) and then directly use the learned distribution to compute $V^{\star,\eta}$ and $Q^{\star,\eta}$. Our distributional RL notably \emph{does not} involve distributional Bellman equation nor distributional TD update, which can be unstable in practice.} to fit the reward-to-go distribution, which has many benefits for representation learning \citep{bellemare2017distributional,lyle2019comparative}, lower variance updates \citep{rowland2023statistical}, and second-order bounds \citep{wang2024central,wang2024more}.
For DistRL, our ability to avoid TD is a significant advantage since the distributional Bellman operator is not even a contraction for certain metrics \citep{bellemare2017distributional}.

\paragraph{Applicability to LLMs.}
For our theoretical framework, we study the deterministic MDP model since it captures the LLM post-training problem as a special case \citep{ouyang2022training}. Specifically, the initial state $x_1$ is the prompt, the intermediate state $x_h$ is the current prefix, and the action $y_h$ is the next token or next block of tokens.
So, the policy is simply the LLM's autoregressive generation process.
The transition function simply concatenates the LLM's continuation to the prefix, \ie, $P_h(x_h,y_h)=x_hy_h$, which is a deterministic operation.
In many cases, the reward function is sparse, \ie, only $r_H$ is non-zero, in which case \cref{thm:deterministic-mdp-v-star} can be simplified to $Q^{\star,\eta}_h(x_h,y_h) = \eta\ln\EE_{\piref}[\exp(\eta^{-1}r(x_H,y_H))\mid x_h,y_h]$.
For example, in math tasks, it is the correctness of the solution; in chat, it is the preference of the response measured by a (learned) reward model.
In sum, our theoretical model is directly applicable to LLM post-training.


\paragraph{Inference with Cumulative Reward Distribution.}
Let $Z^\star$ denote the conditional distribution of cumulative rewards under roll-outs from $\piref$; that is,
\begin{equation*}
    \textstyle Z^\star_h(x,y)\overset{D}{=} \sum_{t\geq h}r_t(x_t,y_t)\mid x_h=x,y_h=y,
\end{equation*}
where $(x_h,y_h,\dots,x_H,y_H)$ is a random trajectory under $\piref$ and $\overset{D}{=}$ denotes that two random variables have equal probability laws.
Combining \cref{thm:deterministic-mdp-v-star} and \cref{eq:optimal-policy}, we have that $\pi^{\star,\eta}$ can be expressed in terms of $Z^\star$:
\begin{equation*}
    \pi^{\star,\eta}_h(y\mid x)\propto \piref_h(y\mid x)\EE_{z\sim Z^\star_{h}(x,y)}[\exp(z/\eta)]. %
\end{equation*}
This motivates us to define the policy induced by a given distribution $Z:\Xcal\times\Ycal\mapsto\Delta(\RR)$ as
\begin{equation}
    \pi^{Z,\eta}_h(y\mid x)\propto \piref_h(y\mid x)\EE_{z\sim Z_{h}(x,y)}[\exp(z/\eta)]. \label{eq:distrl-policy-form}
\end{equation}
Since $\pi^{\star,\eta} = \pi^{Z^\star,\eta}$, this naturally motivates learning a good estimate $\wh Z\approx Z^\star$ via distributional learning techniques such as maximum likelihood estimation (MLE)  and plugging back into \cref{eq:distrl-policy-form} to obtain a good policy, which is the crux of our \ALG algorithm.

\subsection{Algorithm \ALG}\label{sec:algorithm}
We propose Q-Sharp (\ALG), a distributional value-based algorithm for solving KL-regularized RL in deterministic MDPs.
\ALG is an iterative algorithm that collects data from progressively better policies to learn the target distribution $Z^\star$.
In this section, we describe the \ALG algorithm using practical notation for deep neural networks and LLMs; in \cref{sec:theory}, we will provide a theoretically grounded version using online learning oracles and prove convergence guarantees under the mild assumption of realizability.

Let $Z^\theta_h:\Xcal\times\Ycal\to\Delta(\RR)$ be a conditional distribution with parameters $\theta$.
For a label $R\in\RR$ (\eg, a sample from $Z^\star$) and an estimate $\hat Z$, let $L(R,\hat Z)$ be a distributional loss function for learning $\theta^\star$, the optimal parameter that minimizes the distance between $Z^\star$ and $Z^{\theta}$.
For example, if $Z^\star_h(x,y)\overset{D}{=}\op{Ber}(p^\star_h(x,y))$ is always Bernoulli, then $Z^\theta_h(x,y)$ can be parameterized by a neural network that outputs a single scalar estimate of $p^\star_h$.
Then the natural loss is binary cross-entropy (BCE), \ie,
\begin{equation*}
    L_{\op{bce}}(r, \hat p) = -r\ln\hat p-(1-r)\ln(1-\hat p).
\end{equation*}
This is useful in tasks like math or multiple choice questions where the reward is binary.
If there is no \emph{a priori} structure about the reward distribution, we can use a non-parametric estimator such as histogram models that discretize the reward space into bins and train with maximum likelihood (MLE) loss \citep{bellemare2017distributional}:
\begin{equation*}
    L_{\op{mle}}(r, \hat z) = -\ln\hat z[\op{idx}(r)],
\end{equation*}
where $\op{idx}(r)$ is the index of the bin that $r$ falls into and $\hat z[i]$ is the $i$-th bin's probability estimate.
\ALG is amenable to any distributional RL loss function \citep{bellemare2023distributional}.

Then, the key idea is to iteratively update the current parameters $\theta^k$ using new data collected from the current induced policy $\pi^k\gets\pi^{Z^{\theta^k},\eta}$. %
Specifically, the data collection process rolls-in $\pi^k$ for $h-1$ steps until $x_h$, finishes the trajectory with $\piref$ and collects the cumulative rewards $R_{h,k}$ henceforth, which is exactly a sample from $Z^\star_h(x_h)$.
These samples are added to the dataset and the parameters are updated via gradient descent on the distributional loss function.
This procedure is repeated until convergence.
The full algorithm is described in \cref{alg:practical-alg}.

Our iterative data collection process is similar in spirit to DAgger \citep{ross2011reduction},  AggreVaTe \citep{ross2014reinforcement,sun2017deeply},  and RLGF \citep{chang2023learning}, where the iterative training approach addresses the distribution shift problem and ensures that the learned estimator will be accurate during the test/inference time. Prior value-based work such as CD \citep{mudgal2023controlled} and entropy-regularized PRM \citep{zhang2024entropy} only learns estimators under the data generated by $\piref$. However, while the learned estimator can be accurate under $\piref$, there is no guarantee that it will be accurate under the data generated during the inference time when the estimator is used for steering $\piref$'s generation.  %

\begin{algorithm}[h!]
\caption{\ALG}
\label{alg:practical-alg}
\begin{algorithmic}[1]
    \State\textbf{Input:} reference policy $\piref$. %
    \State Initialize $\theta^1$ and dataset $\Dcal_h=\emptyset$ for all $h$.
    \For{$k=1,2,\dots$ until convergence}
        \State Let $\pi^k\gets\pi^{Z_{\theta^k},\eta}$ be policy induced by $Z_{\theta^k}$.
        \For{$i=1,2,\dots,N$}
            \State Sample a switching time $h\sim [H]$.
            \State Roll-in with $\pi^k$ for $h-1$ steps.
            \State Resume trajectory with $\piref$ from $x_{h}$.
            \State Let $R_t$ denote cumulative rewards after time $t$.
            \State Add $(x_t,y_t,R_{t})$ to $\Dcal_t$, $\forall t\geq h$.
        \EndFor
        \State Update $\theta^k$ by minimizing the distributional loss on the aggregated data:
        \begin{align*}
            \textstyle \theta^{k+1}\gets\argmin_{\theta}\sum_h\EE_{\Dcal_h}[\Lcal(R_{h},Z^\theta(x_h,y_h))].
        \end{align*}
    \EndFor
    \State\textbf{Output:} Final $\theta^k$.
\end{algorithmic}
\end{algorithm}

Once we have learned a good parameter $\theta^k$ with \cref{alg:practical-alg} such that $Z^{\theta^k}\approx Z^\star$, we can induce a near-optimal policy $\pi^{\theta^k,\eta}$ via \cref{eq:distrl-policy-form}.
In \cref{sec:theory}, we prove that this procedure indeed converges to the optimal policy under the mild realizability assumption.

We remark that CD \citep{mudgal2023controlled} and VAS \citep{han2024value} are related value-based algorithms for LLM post-training, but there are at least three shortcomings.
First, CD and VAS use $Q^{\piref,0}$, the non-regularized $Q$-function of $\piref$, to guide the generations, which \emph{does not solve KL-regularized RL in general} -- in \cref{sec:theory}, we indeed show there are simple MDPs where CD and VAS provably fail to maximize the reward and/or stay close to $\piref$.
In contrast, \ALG uses $Q^{\star,\eta}$ to guide $\piref$, which is principled and provably converges to $\pi^{\star,\eta}$ under a mild assumption of realizability.
Second, \emph{CD and VAS are offline algorithms} that operate on a fixed dataset, while \ALG is an online algorithm that interleaves data collection and parameter updates, which leads to more robust generalization \citep{ross2011reduction,ross2014reinforcement}.
Third, \emph{CD and VAS use squared loss regression} to learn $Q^{\piref,0}$, which implicitly assumes that the cumulative reward distributed as a homoskedastic gaussian. In contrast, \ALG uses the distributional RL framework which is provably more sample efficient \citep{wang2023benefits,wang2024more} and often learns better policies in practice \citep{bellemare2017distributional,lyle2019comparative}.

Our approach is also fundamentally different from standard actor-critic style RL algorithms. While we do learn a value function, our target value function, $V^{\star,\eta}$ or $Q^{\star,\eta}$, is  fixed during the entire training process. In contrast, in standard actor-critic RL methods (e.g., PPO), the target value function, $V^{\pi}$ or $Q^\pi$, keeps changing as $\pi$ being updated.  We emphasize again that our value function is learned via distributional supervised learning techniques (e.g., MLE) instead of the bootstrapping TD/Q-learning style update.

\paragraph{Inference with many $\eta$.}
The learned reward distribution $\wh Z^\theta$ does not depend on $\eta$, and thus a single distributional reward network can support a range of $\eta$ at inference time. 
