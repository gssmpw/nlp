
\section{Experiments}
\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.58\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/star-graph.png}
    \subcaption*{(a) The star-graph $G(5,5)$.}
    \label{fig:star-graph}
  \end{subfigure}
  \begin{subfigure}[t]{0.4\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/star-graph-accs.png}
    \subcaption*{(b) Generalization accuracies.}
    \label{fig:star-graph-accs}
  \end{subfigure}
  \caption{(a) The star-graph with degree $d=5$ and path length $\ell=5$. Next-token prediction, the standard pre-training loss, learns a sub-optimal shortcut that selects a random first node and follows it until the end.
  (b) Accuracies on held-out test graphs for various models. Policy-based post-training methods such as REINFORCE and RPO (a variant of DPO) still exhibit the shortcut and has test accuracy $1/d$, while our value-based \ALG fixes the shortcut and achieves near-perfect accuracy.}
  \label{fig:combined-star-graph}
\end{figure}


\subsection{Star-Graph}\label{sec:star-graph}
We start with the star-graph task from \citet{bachmann2024pitfalls}, which is illustrated in \cref{fig:combined-star-graph}(a). A star-graph $G(d,\ell)$ consists of $d$ paths of length $\ell$ emanating from a central node. The LM's task is to generate a path from a given start and goal node, and the set of edges describing the star-graph. 
While this task seems deceptively easy, \citet{bachmann2024pitfalls} showed that pre-training with next-token prediction actually learns a faulty shortcut where the model randomly picks the first node, which is correct $1/d$ fraction of the time, and follows the path until the final node. Hence, the shortcut policy only has a test-time accuracy of $1/d$, which shows that pre-training with next-token prediction can lead to sub-optimal shortcuts for planning-based tasks.

A natural question is: \textbf{can this shortcut be fixed during post-training?}
We evaluate popular post-training algorithms such as REINFORCE \citep{ahmadian2024back}, DPO \citep{rafailov2024direct} and RPO \citep{pang2024iterative}, as well as \ALG, and report the final test-set accuracies in \cref{fig:star-graph-accs}.
We find \ALG is consistently able to correct the shortcut, approaching perfect generalization accuracy. CD \citep{mudgal2023controlled} achieves similar performance as \ALG and is also able to fix the shortcut. However, we find that policy-based post-training algorithms, including REINFORCE and RPO, consistently fail to fix the shortcut and only achieves an accuracy of $1/d$ or lower. In particular, DPO consistently led to policy collapse by pushing down the probability of both chosen and rejected paths, achieving an accuracy of $0$, a failure mode also observed by RPO \citep{pang2024iterative}. The failure of policy-based post-training is likely due to the fact that once the transformer has learned the shortcut, it becomes difficult to unlearn \citep{hu2024learning}. Thus, these results demonstrate the value of value-based algorithms, such as \ALG and CD, for post-training LLMs.
Please see \cref{app:star-graph-details} for implementation details.

\subsection{Math Reasoning}\label{sec:math_reasoning}
\textbf{Datasets.} We also experiment on mathematical reasoning datasets, namely GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring}. GSM8K consists of grade school arithmetic word problems and MATH contains high school mathematical competition problems that are much more challenging. In addition, we create a 90\%-10\% split on the original training set to be our training set for learning and validation set for hyperparameter tuning. For reporting test set performance, we use the entire GSM8K test set and a random subset of MATH test set, MATH-500 which we follow from several previous work \citep{lightman2023let,wang2024math}.

\textbf{Models.} We use Llama 3 \citep{dubey2024llama} series of models since they are competitive in math reasoning and have a wide range of parameter sizes. Unless otherwise specified, the $Q^{\star,\eta}$ function in \ALG is parametrized and initialized with a Llama 3.2 1B model and $\eta = 0.1$ is used as we find it gives good and consistent performance. We run \ALG for two iterations and observe performance convergence. Additional details on models and \ALG training can be found in Appendix \ref{app:model_details} and \ref{app:training_details}.

\textbf{Evaluation metrics.} We evaluate performance with \textit{single sample} accuracy and \textit{majority voting} accuracy. For \textit{single sample} accuracy (denoted as \texttt{pass@1}), one generation is sampled per problem and the correctness is evaluated against the ground truth final answer. For \textit{majority voting} accuracy (denoted as \texttt{maj1@k}), $k$ generations are sampled and the most common final answer among them is checked for correctness. In our experiments, we use $k = 8$ and all generations are sampled with temperature $T = 0.8$ and nucleus sampling $p = 0.9$. The prompt template used for evaluation can be found in Appendix \ref{app:evaluation_details}.

\textbf{Main Results.} In Table \ref{tab:main_results}, we show the performance of \ALG on GSM8K (Left) and MATH (Right) when $\piref$ is either Lllama 3 or 3.1 8B. Note that although both Llama 3 and Llama 3.1 have 8B parameters, Llama 3.1 is significantly better than Llama 3 for both datasets. Across all settings, we observe that \ALG consistently outperforms $\piref$, improving \texttt{pass@1} accuracy as much as 9\% on GSM8K for Llama 3 8B, with just 1B additional parameters. In addition, we evaluate the CD baseline \citep{mudgal2023controlled,han2024value} which guides $\piref$ with the incorrect $Q^{\piref,0}$ function instead.
We find that \ALG consistently improves upon CD on two accuracy metrics while at the same time incurring lower KL divergence w.r.t. $\piref$. 
In sum, \ALG Pareto-dominates the baseline for the KL-regularized RL problem where reward should be higher and KL should be lower.


\begin{table}[t]
\centering
\caption{Comparison of \ALG with $\piref$ and CD baseline on GSM8K (Left) and MATH (Right). For both Llama 3 and Llama 3.1 8B, \ALG consistently improves both \texttt{pass@1} and majority voting accuracy upon baselines while incurring minimal KL deviation.}
\resizebox{0.48\linewidth}{!}{%
\begin{tabular}{@{}cccc|ccc@{}}
\toprule
$\piref$ & \multicolumn{3}{c|}{Llama 3 8B} & \multicolumn{3}{c}{Llama 3.1 8B} \\
\midrule
Methods & $\piref$ & CD & \ALG & $\piref$ & CD & \ALG  \\
\midrule
\texttt{pass@1} $\uparrow$ & 69.1 & 77.8 & \textbf{78.4} & 82.9 & 84.5 & \textbf{85.1} \\
\texttt{maj1@8} $\uparrow$ & 85.8 & 87.2 & \textbf{88.1} & 90.5 & 90.9 & \textbf{91.4} \\
KL-Divergence $\downarrow$ & - & 6.39 & \textbf{2.65} & - & 7.43 & \textbf{3.67} \\
\bottomrule
\end{tabular}
}
\resizebox{0.48\linewidth}{!}{%
\begin{tabular}{@{}cccc|ccc@{}}
\toprule
$\piref$ & \multicolumn{3}{c|}{Llama 3 8B} & \multicolumn{3}{c}{Llama 3.1 8B} \\
\midrule
Methods & $\piref$ & CD & \ALG & $\piref$ & CD & \ALG \\
\midrule
\texttt{pass@1} $\uparrow$ & 25.4 & 24.9 & \textbf{27.1} & 43.9 & 45.3 & \textbf{46.7} \\
\texttt{maj1@8} $\uparrow$ & 34.3 & 34.3 & \textbf{37.9} & 57.0 & 59.0 & \textbf{60.1} \\
KL-Divergence $\downarrow$ & - & 15.27 & \textbf{7.14} & - & 26.8 & \textbf{8.69} \\
\bottomrule
\end{tabular}
}
\label{tab:main_results}
\end{table}


\textbf{Larger $\piref$ and \ALG sizes.} We also investigate how the performance scales as we vary $\piref$ and \ALG model sizes. In Table \ref{tab:model_scaling}, we summarize the results of using 70B versions of Llama 3 and Llama 3.1 as $\piref$ on MATH. Compared to their 8B counterparts, both Llama 3 and Llama 3.1 are much stronger reasoning models, reaching \texttt{pass@1} accuracy of 45.6\% and 60.6\% for Llama 3 and 3.1 respectively. Given the significant performance improvement, \ALG of size 1B is still capable of guiding and improving the generation of 70B $\piref$, improving Llama 3.1 \texttt{pass@1} and \texttt{maj1@8} by 2.5\% and 3.5\% respectively. As we increase \ALG size to 3B, \texttt{pass@1} for both Llama 3 and Llama 3.1 continues to improve, which suggests the scalability of \ALG. As we compare with Table \ref{tab:main_results} (right) for Llama 3.1, we also highlight that with 9B parameters (8B $\piref$ + 1B \ALG), its majority voting accuracy already catches up with the \texttt{pass@1} accuracy of the 70B $\piref$ model in Table \ref{tab:model_scaling}, which could be a low-resource alternative. The \texttt{pass@1} accuracy increases but \texttt{maj1@8} accuracy decreases slightly for Llama 3 model. We hypothesize that this is because \ALG leads to more diverse generations on harder problems which boosts \texttt{pass@1} but less consistent correct generations for easier problems.


\begin{table}[t]
\centering
\caption{Performance of $\piref$ and \ALG on MATH with larger $\piref$ and \ALG model sizes. \ALG of size 1B is capable of guiding a 70B $\piref$ model. Increasing \ALG model sizes to 3B also leads to noticeably better performance for Llama 3.1 70B.}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{@{}cccc|ccc@{}}
\toprule
$\piref$ & \multicolumn{3}{c|}{Llama 3 70B} & \multicolumn{3}{c}{Llama 3.1 70B} \\
\midrule
\ALG Model & None & Llama 3.2 1B & Llama 3.2 3B & None & Llama 3.2 1B & Llama 3.2 3B  \\
\midrule
\texttt{pass@1} $\uparrow$ & 45.6 & 46.4 & \textbf{46.7} & 60.6 & 63.1 & \textbf{64.1} \\
\texttt{maj1@8} $\uparrow$ & \textbf{55.6} & 55.5 & 55.3 & 69.0 & 72.5 & \textbf{72.7} \\
KL-Divergence $\downarrow$ & - & \textbf{3.12} & 5.15 & - & \textbf{4.98} & 4.99 \\
\bottomrule
\end{tabular}
}
\label{tab:model_scaling}
\end{table}

\begin{table}[t]
\centering
\caption{Performance of $\piref$ and \ALG on GSM8K and MATH when using \ALG also as a reward model to evaluate complete generations. The reward model can determine the best generation among all generations for a problem and consistently improves \texttt{maj1@8} for $\piref$ and \ALG own generations.}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{@{}ccc|cc@{}}
\toprule
Setting & \multicolumn{2}{c|}{Llama 3 8B GSM8K} & \multicolumn{2}{c}{Llama 3.1 8B MATH} \\
\midrule
Methods & $\piref$ & \ALG & $\piref$ & \ALG \\
\midrule
\texttt{pass@1} & 69.1 & 78.4 & 43.9 & 46.7 \\
\texttt{maj1@8} & 85.8 & 88.1 & 57.0 & 60.1 \\
\ALG-RM Best of 8 & 85.9 & 86.0 & 54.0 & 54.0 \\
\ALG-RM \texttt{maj1@8} & \textbf{88.5} & \textbf{89.2} & \textbf{59.2} & \textbf{60.6} \\
\bottomrule
\end{tabular}
}
\label{tab:reward_scaling}
\end{table}

\textbf{\ALG as a reward model.} Since \ALG learns a token-level $Q$ function, besides \textit{guiding} $\piref$ generation, we can also re-purpose it as a reward model that efficiently \textit{evaluates} how good a complete generation is with just one forward pass. To calculate the reward / score of a generation, we compute $Q(\text{generation}, \texttt{EOS})$. Since Llama 3 8B and Llama 3.1 8B with \ALG shows a greater absolute improvement over $\piref$ on GSM8K and MATH respectively, we investigate if their performance can be further improved with \ALG reward model. In Table \ref{tab:reward_scaling}, we tabulate \ALG-RM Best of 8 and \ALG-RM \texttt{maj1@8} accuracy on the two settings for both $\piref$ and \ALG generations. Both \ALG-RM metrics use $k = 8$ generations where \ALG-RM Best of 8 selects the highest scored sample for evaluation and \ALG-RM \texttt{maj1@8} performs majority voting by aggregating the total score for each unique final answer. It can be seen that \ALG-RM \texttt{maj1@8} consistently enhances vanilla \texttt{maj1@8} for both GSM8K and MATH, indicating the general benefit of using \ALG as a reward model. We also see that the reward model can be used on both $\piref$ and \ALG own generations to further improve performance, which suggests the (same) reward model has generalizability for evaluating diverse generations. Lastly, \ALG-RM Best of 8 also significantly improves upon \texttt{pass@1} by more than 10\% for $\piref$ generations on both GSM8K and MATH. We note that although Best of 8 underperforms \texttt{maj1@8} or \ALG-RM \texttt{maj1@8} for GSM8K and MATH, majority voting is not a generally applicable approach for all reasoning tasks since for proof-based questions, how to aggregate final answers is not immediately obvious. Best of N using reward model, however, can still be readily applied. Therefore, a large improvement from \texttt{pass@1} for Best of 8 is meaningful and demonstrates the effectiveness of \ALG reward model.






\textbf{The effect of $\eta$.} In Figure \ref{fig:cd_ours_kl}, we show the performance and KL divergence tradeoff for CD and \ALG on the GSM8K validation set. The left figure shows that the \texttt{pass@1} accuracy can be improved by incurring more KL penalty for both CD and \ALG. \ALG dominates CD by achieving a better Pareto frontier than CD. In addition, we empirically find that CD performance is much more sensitive to a proper choice of $\eta$. As seen in the right figure, as $\eta^{-1}$ increases, CD incurs a very large KL and starts to significantly deviate from $\piref$, which leads to performance that is even worse than the original $\piref$. \ALG, however, is more stable and does not require extensive search for $\eta$, which provides another empirical benefit for \ALG. %

\begin{figure}[t]%
\centering
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/pass_1_kl.pdf}%
\end{subfigure}\hfill%
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[width=\linewidth]{figs/eta_kl.pdf}%
\end{subfigure}\hfill%
\caption{Performance tradeoff of CD and \ALG on the validation set of GSM8K. (Left) \texttt{pass@1} accuracy vs. KL divergence. \ALG dominates CD by achieving higher accuracy with a lower KL penalty. (Right) Different $\eta$ leads to different KL for CD and \ALG. The KL of CD blows up quickly and significantly deviates from $\piref$ whereas \ALG is more stable.}
\label{fig:cd_ours_kl}
\end{figure}

\textbf{Ablations.} We ablate on several of our design choices in Table \ref{tab:ablation} on the validation set of GSM8K and MATH for \texttt{pass@1} accuracy. The first column (Prefix) is whether we train on all the prefixes after switching to $\piref$. As seen in Algorithm \ref{alg:practical-alg} Line 10, our default is to train on all $t \geq h$ instead of just $t = h$. From a supervised learning perspective, this makes training samples no longer IID. However, we find out the additional large amount of training data helps \ALG learning significantly for as much as 4\%. We also experiment with two types of parametrization for learning $Q^{\star,\eta}$: Q-type and V-type. The Q-type takes input of $x$ and computes $Q^{\star,\eta} (x, y)$ for all $y$ in the vocabulary of the $\piref$ model. The V-type takes input of concatenated $x$ and a specific token $\hat{y}$ and outputs a single value that represents $Q^{\star,\eta} (x, \hat{y})$. By comparing the results on second row and the fourth row, we observe that V-type is better than Q-type. We hypothesize that this is because V-type has fewer number of parameters than Q-type but performs more computation per token. More details on Q-type and V-type can be found in Appendix \ref{app:model_details}. We also compare the distributional version of \ALG with direct MSE regression based. The regression based \ALG consistently underperforms and this is expected since for the math reasoning tasks, we know the underlying reward distribution is Bernoulli (either 0 or 1). Finally, we find that running Algorithm \ref{alg:practical-alg} for more than one iteration can further boosts the performance of \ALG slightly. We observe no major improvement beyond two iterations and therefore we by default run \ALG for two iterations.

\begin{table}[t]
\centering
\caption{Ablations of \ALG (last row) on \texttt{pass@1} with various configurations on the validation set of GSM8K and MATH. The improvement suggests that our design choices all contribute positively to the final performance.}
\resizebox{0.7\linewidth}{!}{%
\begin{tabular}{@{}lccc|cc@{}}
\toprule
Prefix & Type & Opt. & \# Iter. & Llama 3 8B GSM8K & Llama 3.1 8B MATH \\
\midrule
Single & V & Dist. & 1 & 80.5 & 64.5 \\
All & Q & Dist. & 1 & 81.4 & 66.4 \\
All & V & MSE & 1 & 81.4 & 65.4 \\
All & V & Dist. & 1 & 82.3 & 67.4 \\
All & V & Dist. & 2 & 83.5 & 68.5 \\
\bottomrule
\end{tabular}
}
\label{tab:ablation}
\end{table}

\textbf{Qualitative comparison.} In Figure \ref{fig:qualitative_example_main}, we show a few generations side by side from $\piref$ and \ALG to qualitatively visualize the effect of guidance on math reasoning questions. We observe that the generations from $\piref$ and \ALG usually start off with similar prefixes, which is another supporting evidence for the low KL deviation of \ALG. However, \ALG can successfully mitigate mistakes from $\piref$ and leads to generally better reasoning chain-of-thought than $\piref$. Full analysis on more examples can be found in Appendix \ref{app:qualitative}.
