\section{Theory}\label{sec:theory}
In this section, we provide theoretical analyses for \ALG and prior value-based post-training approaches, including CD \citep{mudgal2023controlled} and VAS \citep{han2024value}.

\subsection{CD \& VAS are sub-optimal for KL-regularized RL}\label{sec:theory-counterexample-CD}
First, CD and VAS both propose to reweight $\piref(\cdot\mid x)$ with the unregularized $Q$-function of $\piref$:
\begin{equation}
    \pi^{\textsf{CD},\eta}(y\mid x)\propto\piref(y\mid x)\exp(Q^{\piref}(x,y)/\eta),\label{eq:cd}
\end{equation}
where recall that $Q^{\piref}_h(x_h,y_h) = \EE_{\piref}[ \sum_{t\geq h}r_t\mid x_h,y_h ]$.
Comparing with \cref{eq:optimal-policy}, we can already see that $\pi^{\textsf{CD},\eta}$ does not match the optimal policy $\pi^{\star,\eta}$, as $Q^{\piref}$ can be arbitrarily far from $Q^{\star,\eta}$.
In particular, $\pi^{\textsf{CD}}$ may fail to optimize the KL-regularized RL objective and exhibit two failure cases, which we demonstrate with a simple MDP.

\begin{example}[!h]
\centering
\resizebox{0.4\textwidth}{!}{
\begin{tikzpicture}[
    ->, >=stealth,
    level distance=1.3cm,        %
    level 1/.style={sibling distance=3cm},  %
    level 2/.style={sibling distance=2cm},  %
    state/.style={draw, circle, minimum size=0.8cm},
    terminal/.style={draw, rectangle, minimum width=0.8cm, minimum height=0.5cm}
]
\node[state] (x1) {$x_1$}
  child { %
    node[state] (xL) {$x_L$}
      child {
        node[terminal] (r1L) {$r=0.1$}
        edge from parent node[left] {$1$}
      }
    edge from parent node[left] {$p_L$}
  }
  child { %
    node[state] (xR) {$x_R$}
      child {
        node[terminal] (r1R) {$r=1$}
        edge from parent node[left] {0.05}
      }
      child {
        node[terminal] (r0R) {$r=0$}
        edge from parent node[right] {0.95}
      }
    edge from parent node[right] {$p_R$}
  };

\node[above=0.3cm of xL, xshift=-0.5cm] (ZL) {$Z^\star_L = \delta(0.1)$};
\node[above=0.3cm of xR, xshift=1cm]  (ZR) {$Z^\star_R = \mathrm{Ber}(0.05)$};

\end{tikzpicture}
}
\caption{A tree MDP where edges are labeled with $\piref$'s action probability.
Specifically, $\piref$ goes to the left sub-tree w.p. $p_L$ and the right sub-tree w.p. $p_R$, where $p_L,p_R > 0$.
The left sub-tree gives $r=0.1$ w.p. $1$. In the right sub-tree, $\piref$ chooses reward $1$ w.p. $0.05$ and chooses reward $0$ w.p. $0.95$.}
\label{fig:small-subtree-mdp-A}
\end{example}

First, we show that CD fails to maximize expected reward in this MDP, even as the KL-regularizer $\eta$ decays to zero.
\begin{theorem}\label{thm:cd-suboptimal-reward}
Under \cref{fig:small-subtree-mdp-A},
CD learns to always select the left sub-tree as $\eta\to 0$, which gives a sub-optimal reward of $0.1$,
while $\pi^{\star,\eta}$ learns to always select the right sub-tree and chooses the path that gives reward $1$.
\end{theorem}
\begin{proof}
First, for CD, we have $Q^{\piref}(x_1,a_L)=0.1$ and $Q^{\piref}(x_1,a_R)=0.05$.
Hence, CD's probability of selecting the left sub-tree is $\frac{p_L\exp(0.1/\eta)}{p_L\exp(0.1/\eta)+p_R\exp(0.05/\eta)}$, which converges to $1$ as $\eta\to 0$.
Next, for \ALG, we have $Q^{\star,\eta}(x_1,a_L)=0.1$ and $Q^{\star,\eta}(x_1,a_R)=\eta\ln(0.05\exp(1/\eta)+0.95)$.
Hence, \ALG's probability of selecting the left sub-tree is $\frac{p_L\exp(0.1/\eta)}{p_L\exp(0.1/\eta)+p_R(0.05\exp(1/\eta)+0.95)}$, which converges to $0$ as $\eta\to 0$.
Thus, CD learns the sub-optimal path.
\end{proof}

Next, we show that CD also incurs a higher KL than \ALG.
\begin{theorem}
Under \cref{fig:small-subtree-mdp-A}, CD's KL converges to $\ln(1/p_L)$ while \ALG's KL converges to $\ln(1/p_R)$ as $\eta\to 0$. Thus if $p_L\ll p_R$, CD converges to a higher KL than \ALG.
\end{theorem}
\begin{proof}
As shown in \cref{thm:cd-suboptimal-reward}, CD learns to select the left sub-tree while \ALG learns to select the right sub-tree as $\eta\to 0$.
Then, the KLs simply follow by definition.
\end{proof}
In sum, we proved that \cref{fig:small-subtree-mdp-A}, CD both incurs a higher KL and achieves a lower sub-optimal reward compared to \ALG. Thus, \ALG generally Pareto-dominates CD in the reward-KL trade-off, which matches our empirical findings.

\subsection{Performance Guarantee for \ALG}
We prove that the learned policy by \ALG is guaranteed to converge to the optimal policy with enough samples.
This result holds in rich-observation MDPs where the size of the state space can be exponentially large or infinite, so long as the mild realizability assumption holds.

To setup, let $\Fcal$ be a distributional function class for modeling $Z^\star$, the reward-to-go distribution under $\piref$.
Each element of $\Fcal$ has type $f=(f_1,\dots,f_H)$ and $f_h:\Xcal\times\Ycal\mapsto\Delta([0,V^{\max}])$.\footnote{Suppose rewards-to-go under $\piref$ lie in $[0,V^{\max}]$ w.p. $1$.}
For purpose of analysis, we assume access to a no-regret online learning oracle for the maximum likelihood (MLE) loss, which proceeds as follows:
for each iteration $k=1,2,\dots,K$, given any $\{x_{h,k},y_{h,k},R_{h,k}\}_{h=1}^H$, the oracle outputs $\wh Z_k\in\Fcal$ s.t.
\begin{align*}
    \textstyle\sum_{k=1}^K\sum_{h=1}^H (\log Z^\star_h(R_{h,k}\mid x_{h,k},y_{h,k})
    -\log \wh Z_{h,k}(R_{h,k}\mid x_{h,k},y_{h,k}))\leq \OracleRegret(K).
\end{align*}
No-regret online learning is well-studied in the literature \citep{cesa2006prediction,orabona2019modern} and is a standard tool when reducing decision making to supervised learning \citep{ross2011reduction,foster2021efficient,wang2023benefits}.
For example, if $\Fcal$ is finite and satisfies realizability, then Vovk's aggregating algorithm ensures that $\OracleRegret(K)\lesssim\ln(|\Fcal|)$ \citep{vovk1995game}.\footnote{$a\lesssim b$ is short for $a\leq Cb$ for some universal constant $C$.}
\begin{assumption}[Realizability]\label{asm:realizability}
$Z^\star\in\Fcal$.
\end{assumption}
The following algorithm is a slightly modified version of \cref{alg:practical-alg} amenable for theoretical analysis.
The only differences with \cref{alg:practical-alg} are: (1) we use the MLE oracle to learn $\wh Z_k$, and (2) for purpose of local exploration, we play a random action at the switching time $h$ before following $\piref$ to the end of the trajectory \citep{ross2014reinforcement}.
\begin{algorithm}[h!]
\caption{\ALG (Theory Version)}
\label{alg:theory-alg}
\begin{algorithmic}[1]
    \State\textbf{Input:} reference $\piref$, iteration count $K$, regularizer $\eta$.
    \State Initialize $\wh Z_{1}$ randomly.
    \For{$k=1,2,\dots,K$}
        \State Let $\pi^k\gets\pi^{\wh Z_k,\eta}$.
        \For{step $h=1,2,\dots,H$}
            \State Roll-in with $\pi^k$ for $h-1$ steps and see $x_{h,k}$.
            \State Play random action $y_{h,k}$ and transit to $x_{h+1,k}$.
            \State Resume trajectory with $\piref$ from $x_{h+1,k}$.
            \State Let $R_{h,k}$ be cumulative rewards after time $h$.
        \EndFor
        \State Input $\{x_{h,k},y_{h,k},R_{h,k}\}_{h\in[H]}$ to MLE oracle.
        \State Receive $\wh Z_k$ from MLE oracle.
    \EndFor
    \State\textbf{Output:} $\wh Z_1,\dots,\wh Z_K$.
\end{algorithmic}
\end{algorithm}

We now state our main PAC bound for \ALG.
\begin{restatable}{theorem}{MainPacBound}\label{thm:main-pac-bound}
Fix any $\eta\in(0,V^{\max}]$ and $\delta\in(0,1)$.
Under \cref{asm:deterministic-mdp,asm:realizability}, \cref{alg:theory-alg} ensures w.p. at least $1-\delta$, setting $\beta=\ln(1/\delta)+\OracleRegret(K)$, we have
\begin{align*}
    \textstyle\sum_{k=1}^K (V^{\star,\eta}-V^{\pi^k,\eta})\lesssim
    \textstyle AV^{\max}(\sqrt{\sum_{h=1}^H\sum_{k=1}^{K}\CoV_{h,k}^2(x,y) \cdot \beta } + \max_{h\in[H]} E_h\cdot\beta),
\end{align*}
where $\CoV_{h,k}(x,y) := \EE_{x_h\sim\pi^k,y_h\sim\op{Unif}(\Acal)}\bracks{\frac{\sqrt{\Var(\exp(Z_h^\star(x_h,y_h)/\eta))}}{\EE[\exp(Z_h^\star(x_h,y_h)/\eta)]}}$ is the coefficient of variation of $\exp(Z_h^\star(x_h,y_h)/\eta)$, and $E_h:=\nm*{\exp((V^{\max}-Q^{\star,\eta}_h(x_h,y_h))/\eta)}_{L_\infty(\piref)}$ is the envelope of $\exp((V^{\max}-Q^{\star,\eta}_h(x_h,y_h))/\eta)$, both under $\piref$.
\end{restatable}
We highlight this applies to rich-observation MDPs where our only requirement for $\Fcal$ is realizability.
Our bound only scales with the function class's complexity, \ie, $\ln(|\Fcal|)$, and does not contain structural complexity measures.
In contrast, prior bounds in RL theory require stronger assumptions such as Bellman completeness \citep{chen2019information,wang2021exponential,foster2021offline,jin2021bellman,chang2022learning,ayoub2024switching,wang2024more}, even in deterministic MDPs \citep{wu2024computationally}, and/or scale with structural complexity measures such as coverability \citep{xie2022role,mhammedi2024the}, eluder dimension \citep{russo2013eluder,jin2021bellman}, and  certain rank related complexity measures \citep{jiang2017contextual,sun2019model,du2021bilinear}.

Also, we highlight that \cref{alg:theory-alg} is model-free and computationally efficient.
In contrast, prior model-free algorithms for rich-observation MDPs perform exploration with version spaces and are computationally hard \citep{jiang2017contextual,dann2018oracle,jin2021bellman,xie2022role,wang2024more}.
Thus, \cref{thm:main-pac-bound} shows that \cref{alg:theory-alg} achieves \emph{both statistical and computational} efficiency under mild assumptions by simply operating within the KL-regularized RL framework, which is of great relevance for post-training. We remark that \citet{uehara2024offline} observed similar benefits in offline RL while we study the harder online setting.

Moreover, \cref{thm:main-pac-bound} is a \emph{second-order bound}, thanks to distributional RL \citep{wang2024central,wang2024more}.
The leading term $\Ocal(\sqrt{\sum_{h=1}^H\sum_{k=1}^{K}\CoV_{h,k}^2(x,y)})$ is a sum of coefficient of variations, which in the worst case scales like $\Ocal(\sqrt{\sum_{h=1}^H E_h^2 K})$. However, in benign cases where $Z^\star_h$ has small or zero variance, this term vanishes and we are only left with the lower order term $\Ocal(\max_{h\in[H]}E_h\ln(K))$ which only grows logarithmically in $K$. In sum, when $Z^\star_h$ has small variance, the second-order bound adaptively becomes $\Ocal(\ln(K))$ instead of $\Ocal(\sqrt{K})$.
Interestingly, the envelope term $E_h$ is also instance-dependent as it involves the optimal policy's $Q$-function $Q^{\star,\eta}$.
In the best case when $Q^{\star,\eta}= V^{\max}$, the envelope term then becomes $1$ regardless of $\eta$. 
In general, we can tolerate an $\eta$ that is as small as the worst $V^{\max}-Q^{\star,\eta}$ under rollouts from $\piref$, which is reminiscent of the condition required for first-order or small-loss bounds \citep{foster2021efficient,wang2023benefits,ayoub2024switching}. 

Finally, we remark that our bound can be greatly simplified if the reward-to-go $Z^\star_h(x,y)$ is always distributed as a Bernoulli, say with parameter $p_h(x,y)$, which is exactly the case for closed-ended problems such as math or multiple choice. Specifically, the CV term can be bounded by $\CoV_{h,k}\leq \EE_{\pi^k\circ\op{Unif}}\sqrt{(1-p_h(x_h,y_h))/p_h(x_h,y_h)}$ and the envelope term becomes $\|1/p_h(x_h,y_h)\|_{L_\infty(\piref)}$, which notably does not have exponential dependence on $1/\eta$. Thus, as long as the reference model $\piref$ has sufficient probability of solving the math or multiple choice problem, our bound can be made independent of $\eta$. Finally, we note that the distributional-realizability condition can also be weakened to mean-realizability, since the only parameter of a Bernoulli distribution is its mean; also the MLE loss reduces to the binary cross-entropy loss \citep{foster2021efficient,ayoub2024switching}. We present the corollary below and the proof in \cref{app:bernoulli-proofs}.
\begin{corollary}
Suppose reward-to-gos are Bernoulli random variables, $Z^\star_h(x,y)\sim\op{Ber}(p_h(x,y))$.
Then, under the setup of \cref{thm:main-pac-bound} and replacing dist-realizability by mean-realizability, the bound can be simplified to:
\begin{equation*}
    \textstyle\sum_{k=1}^K (V^{\star,\eta}-V^{\pi^k,\eta})\lesssim
    \textstyle A(\sqrt{\sum_{h=1}^H\sum_{k=1}^{K}\EE_{x_h\sim\pi^k,y_h\sim\op{Unif}(\Acal)}\bracks{\frac{1-p_h(x_h,y_h)}{p_h(x_h,y_h)}} \cdot \beta } + \max_{h\in[H]}\|\frac1{p_h(x_h,y_h)}\|_{L_\infty(\piref)}\cdot\beta),
\end{equation*}
\end{corollary}


\paragraph{Remark: Modification for Regret Bound.}
It is possible to turn \cref{thm:main-pac-bound} into a regret bound by replacing random action in Line 7 of \cref{alg:theory-alg} with a no-regret contextual bandit oracle, where ``context'' is $x_h$, action is $y_h$ and ``reward'' is $R_h$. This is alike the steps needed to convert AggreVaTe's PAC bound into a regret bound \citep{ross2014reinforcement}. Our theory can be interpreted as a regret/PAC reduction from KL-regularized RL in deterministic MDPs to no-regret online learning, which mirrors the type of imitation learning guarantees obtained for AggreVaTe \citep{ross2014reinforcement}.
