\section{Related Works}\label{sec:related-work}
\textbf{From the empirical side, } the most relevant works are controlled decoding (CD; \citealp{mudgal2023controlled}) and value augmented sampling (VAS; \citealp{han2024value}). These two works both propose to guide the reference policy $\piref$ with $Q^{\piref,0}$, the expected reward-to-go under $\piref$ \emph{without} KL regularization.
As discussed in \cref{sec:theory-counterexample-CD}, guiding with $Q^{\piref,0}$ is not principled for the KL-regularized RL problem and can lead to both sub-optimal reward and large KL from $\piref$.
In contrast, we propose to guide $\piref$ with $Q^{\star,\eta}$, the expected reward-to-go under the optimal policy \emph{with} KL regularization, which is the correct closed-form of the optimal policy. 
A recent work \citet{zhang2024entropy} proposed a process reward model (PRM) of a similar form as our $Q^{\star,\eta}$, but their PRM is applied to steps instead of tokens, and they do not use distributional RL or iterative training (i.e., data aggregation).

In terms of reweighting $\piref$ with classifier scores, FUDGE \citep{yang-klein-2021-fudge} is another closely related work but their derivation is based on Bayes rule and FUDGE does not solve KL-regularized RL. 
Sequential Monte Carlo (SMC) methods \citep{piche2018probabilistic,zhao2024probabilistic} also reweight $\piref$'s distribution with a twist function, where the optimal twist function is analogous to our $Q^{\star,\eta}$. One key difference is that SMC performs resampling while we directly combine logits of $\piref$ and $\exp(Q^{\star,\eta})$ to avoid importance sampling, which has higher variance. Finally, none of these prior works apply distributional RL losses \citep{bellemare2017distributional,dabney2018distributional,farebrother2024stop,ayoub2024switching} or online data aggregation \citep{ross2011reduction} to learn $Q^{\star,\eta}$, which we showed to be beneficial in our ablations. Indeed, CD and VAS both use square loss regression over a fixed offline dataset. We also remark that risk-sensitive RL has been an important application of distributional RL \citep{dabney2018distributional,wang2024risk} and extending \ALG along those lines is a promising future direction.

We also cite some tangentially related works. Proxy tuning \citep{liu2024tuning} and speculative decoding \citep{leviathan2023fast} both use a small model to guide the logit distribution of a large $\piref$ model. Speculative decoding is focused on maximizing the large model's likelihood, which does not relate to any extrinsic rewards. In our framework, the classifier model can be any size relative to $\piref$, although deeper investigation into the computational benefits of using a small classifier is a promising direction for future work. We note that the star-graph problem can also be solved during pre-training by also predicting backwards via the belief state transformer \citep{hu2024learning}.

Finally we discuss previous post-training methods for LLMs. First, online iterative DPO \citep{xiong2023gibbs,pang2025iterative}, REBEL \citep{gao2025rebel}, PPO \citep{schulman2017proximal}, etc. are based on policy gradient and require a good reset distribution which only guarantees local optimality. XPO \citep{xie2024exploratory}, VPO \citep{cho2024vpo}, SELM \citep{zhang2024self}, etc.  treat this as an exploration setting but requires solving non-convex optimization oracles and relies on strong structure conditions such as coverability / eluder / linearity, similar to the theoretical works like \citep{jin2021bellman,xie2022role}. Instead, we approach post-training in a fundamentally different angle and solve it via simple computationally tractable regression and mle oracles, without any strong structural conditions or reset distribution assumptions.

\textbf{From the theoretical side, } KL-regularized RL is closely related to soft RL or maximum entropy RL which are well-studied \citep{ziebart2008maximum,fox2015taming,haarnoja2018soft,piche2018probabilistic}. The optimal policy decomposition in deterministic MDPs is also known in prior works \citep{li2024derivative,domingo2024adjoint}. Our contribution is an algorithm that provably learns $Q^{\star,\eta}$ using distributional RL \citep{bellemare2017distributional} and data aggregation \citep{ross2011reduction}. This enables us to prove a reduction of KL-regularized RL (in deterministic MDPs) to no-regret online learning, which ensures convergence to the optimal policy with realizability being the only assumption for function approximation. Notably we are able to avoid more stringent conditions such as completeness or structural MDP conditions which are ubiquitous in the current literature \citep{wang2021exponential,jin2021bellman,chang2022learning,wang2023benefits,wang2024more,ayoub2024switching,xie2022role}. 
\citet{uehara2024offline} observed similar benefits in offline RL, while we provide guarantees for the harder online RL setting.

We remark that our theoretical guarantees are quite similar in structure to that of AggreVaTe \citep{ross2014reinforcement,sun2017deeply}, which is a reduction of imitation learning to no-regret online learning. Besides the obvious difference in problem setting, another improvement from our work is using distributional RL theory to prove second-order bounds. Notably, we are able to prove second-order bounds without any completeness assumptions that were required in \citep{wang2023benefits,wang2024central,wang2024more}.

