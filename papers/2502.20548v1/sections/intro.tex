\section{Introduction}
Reinforcement learning (RL) post-training is a critical step in the training process of large language models (LLMs) that aligns their generations with human preferences \citep{christiano2017deep} and imbues them with reasoning abilities \citep{setlur2024rewarding,guo2025deepseek}.
Post-training with RL typically occurs after the supervised learning stage (next-token prediction) and the LLM is trained to maximize the expected cumulative reward while minimizing the KL divergence w.r.t. the reference model $\piref$ from supervised learning. This KL penalty is critical as it forces the LLM to stay close to $\piref$ during the optimization process and mitigates reward hacking and catastrophic forgetting.

Most state-of-the-art LLMs \citep{ouyang2022training,dubey2024llama,team2024gemma} are currently post-trained with \emph{policy-based} RL algorithms, which directly update the weights of the LLM with stochastic gradient descent via methods like RLOO \citep{kool2019buy}, PPO \citep{schulman2017proximal} and DPO \citep{rafailov2024direct}. However, these approaches face significant computational challenges: requiring full backpropagation through massive language models during training. Instead, in this paper, we propose a \emph{value-based} RL algorithm, where we guide the generations of the reference policy using a learned value function without modifying the weights of the reference policy model. This approach is particularly compelling because for many tasks, evaluation is easier than generation \citep{ouyang2022training,pang2023language} suggesting we can use much smaller models to learn value functions for guidance during inference. In our experiments (Section \ref{sec:math_reasoning}), this enables us to effectively control and improve a 70B parameter LLM while using only a 1B parameter model for the value function.



While there are existing value-based RL algorithms for LLM post-training, namely CD \citep{mudgal2023controlled} and VAS \citep{han2024value}, they all have a major issue which is that they do not faithfully optimize the KL-constrained RL objective. Specifically, they propose to guide $\piref$ using $Q^{\piref}$, the expected reward-to-go under $\piref$ without KL regularization, which is not guaranteed to converge to the optimal policy $\pi^{\star,\eta}$ of the KL-regularized RL objective. 
Instead, under the classical KL-regularized RL framework, we show that it is provably optimal to guide $\piref$ using $Q^{\star,\eta}$, the expected reward-to-go under $\pi^{\star,\eta}$ with KL-regularization, which is guaranteed to converge to $\pi^{\star,\eta}$.
Thus, prior approaches can suffer from sub-optimal reward and/or large KL deviations, as we illustrate in the paper, and \ALG can provably fix these issues.

By leveraging the special properties of $Q^{\star,\eta}$ in deterministic MDPs, our approach iteratively trains a model to estimate $Q^{\star,\eta}$ directly via distributional supervised learning (e.g., MLE). The iterative training procedure is motivated by the classic imitation learning algorithm DAgger \citep{ross2011reduction}, which addresses covariate shift issues and ensures that the learned $Q^{\star,\eta}$ estimator remains accurate during inference time when used for guiding $\pi_{ref}$. Our distributional learning approach not only improves practical performance but also enables second-order style regret bounds - instance-dependent bounds that adapt to the variance of the model's generation.

Our approach differs from traditional RL methods in two key aspects. First, we avoid complex temporal difference (TD) learning \citep{tesauro1991practical} or Q-learning techniques \citep{van2016deep,kumar2020conservative} in favor of direct supervised learning of a fixed critic. Second, while we leverage distributional learning, our method is conceptually simpler than traditional distributional RL approaches like C51 \citep{bellemare2017distributional} - we learn outcome distributions directly through supervised maximum likelihood estimation without needing to handle distributional Bellman equations.




\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/main_fig.png}
    \caption{(Left) A sketch of our post-training algorithm (\ALG) based on distributional RL. \ALG alternates between learning $Z^\star$ -- the reward-to-go distribution of $\piref$ -- and using the induced policy to collect new data and further improve the distributional estimate. (Right) Evaluation result on the GSM8K dataset \citep{cobbe2021training}. We see that \ALG achieves both higher accuracy and lower KL compared to prior value-based post-training algorithms \citep{mudgal2023controlled,han2024value}.}
    \label{fig:enter-label}
\end{figure*}

In summary, our contributions are as follows:
\begin{enumerate}
    \item We propose \ALG, a principled algorithm for KL-regularized RL in deterministic MDPs, which includes LLMs, based on guiding $\piref$ with the soft $Q^\star$ learned with \emph{distributional RL} 
    (\cref{sec:algorithm}).
    \item We prove variance-dependent PAC bounds for convergence to the optimal policy, which only requires realizability in the function class (\cref{sec:theory}).
    \item We show that value-based post-training, which includes \ALG, can fix biases and shortcuts in a star-graph environment \citep{bachmann2024pitfalls}, while popular policy-based methods cannot (\cref{sec:star-graph}).
    \item We provide extensive experiments on math reasoning tasks that validate the effectiveness of our method at maximizing reward while maintaining small KL deviations from the reference policy (\cref{sec:math_reasoning}).
\end{enumerate}



