\section{The Insufficiency of Unconditioned Single-Phase Testing}

A natural starting point for evaluating an AI system's understanding of student cognition would be to test its ability to generate plausible wrong answers that students might choose. The intuition is compelling: if an AI can anticipate how students will err, surely it understands how they think. This leads to a simple unconditioned single-phase test where both AI and human experts generate distractors for multiple choice questions without knowledge of individual student reasoning patterns. Let us formalize this design:

\begin{definition}[Unconditioned Single-Phase Test Design]
For any question $q$, let:
\begin{itemize}
   \item $C(q)$ be the correct answer
   \item $A(q)$ be the AI-generated distractor 
   \item $H(q)$ be the human expert-generated distractor
   \item $R(q)$ be a randomly generated incorrect answer
\end{itemize}

The test presents these options in random order to students. Let $S(q,s)$ represent student $s$'s selection for question $q$. Critically, both $A(q)$ and $H(q)$ are generated without observing any prior responses from student $s$.
\end{definition}

\begin{definition}[Performance Metrics]
The quality of AI-generated distractors would be evaluated by comparing selection rates:
\begin{align*}
p_A &= \mathbb{P}(S(q,s) = A(q)) \\
p_H &= \mathbb{P}(S(q,s) = H(q))
\end{align*}
with the AI system "passing" if $|p_A - p_H| \leq \epsilon$ for some small $\epsilon > 0$.
\end{definition}

However, this unconditioned approach has fundamental flaws that make it inadequate for validating true understanding of student cognition:

\begin{theorem}[Unconditioned Convergence]
Given a question $q$ with misconception set $M(q) = \{m_1,...,m_k\}$ where $P(m_i)$ represents the probability of misconception $m_i$ in the student population, both AI and human experts will rationally converge to targeting $m^* = \argmax_{m_i \in M(q)} P(m_i)$.
\end{theorem}

\begin{proof}
For any distractor generator (AI or human), the expected selection rate is maximized by targeting the most common misconception:
\[\mathbb{E}[p] = \sum_{i=1}^k P(m_i)\mathbb{I}(m_i \text{ targeted}) \leq \max_{i} P(m_i)\]
with equality achieved only when targeting $m^*$.
\end{proof}

This convergence creates two critical problems:

\begin{enumerate}
   \item \textbf{Population-Level Optimization}: Both AI and human experts are incentivized to target the single most common misconception for each question, regardless of individual student reasoning patterns. This reduces the test to measuring statistical pattern matching rather than understanding of student cognition.
   
   \item \textbf{False Equivalence}: An AI system could ``pass'' this test by simply learning population-level statistics about common wrong answers, without any actual understanding of how individual students reason about concepts or how their misconceptions evolve across related problems.
\end{enumerate}

These flaws demonstrate why a conditioned two-phase design is necessary - one that can validate an AI system's ability to model individual student reasoning rather than just aggregate statistics. To overcome the limitations of unconditioned testing, our two-phase design introduces crucial conditioning on individual student reasoning. In Phase 1, each student $s$ provides open-ended responses to questions $Q_s$, generating tuples $(s,q,A(s,q))$ where $A(s,q)$ is their incorrect answer. In Phase 2, given each Phase 1 tuple $(s,q,A(s,q))$, both the AI system and human experts observe this specific student mistake and use it to inform their predictions. The AI generates a new question $q'$ and predicts the wrong answer $A'(s,q')$ that this particular student would give, while human experts predict their own expected wrong answer $H'(s,q')$. Both these predictions are explicitly conditioned on the observed student mistake from Phase 1.


This conditioning provides several key advantages:

\begin{theorem}[Prediction Differentiation]
Given students $s_1, s_2$ with different misconceptions $m_1, m_2$ observed in Phase 1, optimal predictions for Phase 2 will differ:
\[A'(s_1,q') \neq A'(s_2,q')\]
even for the same question $q'$.
\end{theorem}

\begin{proof}
Unlike the unconditioned case where predictions converge to the population mode, conditioned predictions should maximize:
\[\mathbb{P}(A'(s,q') \text{ chosen} \mid A(s,q))\]
This probability differs based on the specific misconception demonstrated in Phase 1.
\end{proof}

This conditioning forces the AI to demonstrate three crucial capabilities:
\begin{enumerate}
    \item Understanding individual student reasoning patterns from Phase 1 responses
    \item Generating new questions that test the same conceptual understanding
    \item Predicting how specific misconceptions will manifest in new contexts
\end{enumerate}

Unlike the unconditioned design, success in this framework cannot be achieved through simple population-level statistics, as it requires modeling the cognitive processes of individual students. This deeper understanding of student reasoning pathways has direct implications for educational capabilities: an AI system that can accurately predict how a student's specific misconceptions will manifest across different problems is necessarily equipped to provide targeted tutoring interventions and personalized feedback. The ability to generate new questions that probe and predict individual misconceptions demonstrates the AI system's capacity to construct adaptive learning sequences and deliver feedback that addresses each student's particular cognitive challenges.