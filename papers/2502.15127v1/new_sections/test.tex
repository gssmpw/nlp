\section{Test Design}

Our proposed evaluation framework consists of two distinct phases designed to validate an AI system's understanding of student misconceptions. In the first phase, we collect unbiased samples of student misconceptions through open-ended responses. The second phase tests whether the AI system can accurately predict how these misconceptions will manifest in a newly generated question.

Let $S$ denote our set of students and $D$ our domain of questions. For any student $s \in S$ and question $q \in D$, we define $A(s,q)$ as the student's response and $C(q)$ as the correct answer. 

In Phase 1, each student $s$ responds to a set of questions $Q_s \subset D$ without multiple choice options. For each incorrect response, we record the tuple $(s,q,A(s,q))$ where $A(s,q) \neq C(q)$. These tuples provide unbiased samples of natural student misconceptions, uninfluenced by the presence of pre-selected answer choices.

For Phase 2, given each Phase 1 tuple $(s,q,A(s,q))$:

1. The AI system implements a function $f_{LLM}: D \times A \to D$ that generates a new question $q'$ based on the original question and incorrect answer.

2. Given $q'$, both our AI system and a human expert independently generate predicted wrong answers. The AI system implements $g_{AI}: D \times D \times A \to A$ that generates prediction $a'$, while the human expert implements $g_H: D \times D \times A \to A$ generating prediction $a''$. A random wrong answer $r$ is also generated to serve as a control.

3. The student $s$ then receives question $q'$ as a multiple choice question with four options presented in random order: the correct answer $C(q')$, the AI-predicted wrong answer $a'$, the expert-predicted wrong answer $a''$, and the random wrong answer $r$. The student's selection is denoted $P_2(s,q,q')$.
