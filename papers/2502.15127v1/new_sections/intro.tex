\section{Introduction}

A fundamental challenge in artificial intelligence for education is developing systems that truly understand how students think and reason \cite{chowdhury2024autotutormeetslargelanguage,corbett1994knowledge,klymkowsky2024endmultiplechoicetests,sleeman}. While recent advances have produced AI systems that can engage with students and curate educational content \cite{sonkar-etal-2023-class,code_class,Markel2023GPTeachIT,schmucker2023ruffle,shridhar2022automaticgenerationsocraticsubquestions,team2024learnlm}, these systems often lack deep understanding of student cognition - how students develop misconceptions, how they reason incorrectly, and why they make specific mistakes \cite{nlet,pedalign,klymkowsky2024endmultiplechoicetests,Shibani_2024}. This understanding is critical for providing meaningful educational support, whether through tutoring, feedback, or assessment. Currently, it's difficult to determine whether an AI system possesses this crucial understanding of student thinking \cite{eedi-mining-misconceptions-in-mathematics,zaphir2024criticallyaithinkframework,malalgoqa}.

\input{figures/image}

Traditional evaluation methods like measuring learning gains, while important, suffer from significant limitations. These studies not only require months or even years to conduct \cite{feng2024sampleefficienthumanevaluationlarge,pane2014effectiveness,roschelle2016online}, but their results are confounded by numerous variables - including changes in student motivation and external support, variations in classroom environments and teaching styles, and broader educational policy shifts that occur during the study period \cite{biderman2024lessonstrenchesreproducibleevaluation,laskar2024systematicsurveycriticalreview}. Such confounders make it difficult to isolate whether any observed improvements truly stem from the AI system's deep understanding of student cognition or from other factors entirely.

In contrast to traditional studies with their many confounding variables, we propose a two-phase evaluation methodology to directly test an AI system's understanding of student cognition. In Phase 1, students complete open-ended questions without multiple choice options, providing unbiased samples of natural misconceptions. In Phase 2, for each incorrect answer from Phase 1, the AI must generate a new but related question Q' and predict what wrong answer A' the same student would give to Q'. The key innovation is that these predictions are explicitly conditioned on each student's specific Phase 1 mistakes - without this conditioning, any approach would simply target the most common misconceptions rather than demonstrate understanding of individual reasoning. This creates a personalized test of the AI's ability to model individual student thinking. The student then receives Q' as a multiple choice question containing the correct answer, the AI's predicted wrong answer A', a human expert's predicted wrong answer A'', and a random incorrect answer. By analyzing whether students select AI-generated distractors at rates similar to human expert-generated ones, we can directly validate if the AI system genuinely models student cognition.

This approach mirrors the Turing test in a compelling way: just as the original test evaluates AI through its ability to produce human-like conversational responses, our framework evaluates AI through its ability to predict student mistakes as accurately as expert teachers do. Success requires two fundamental capabilities: understanding common patterns in student misconceptions and predicting how these misconceptions will manifest for individual students across different problems. An AI system that can match human experts in this prediction task demonstrates genuine understanding of student cognition - it's not just recognizing statistical patterns, but modeling the underlying reasoning processes that lead students to specific mistakes.

This understanding of student cognition, as demonstrated through distractor generation conditioned on a student's specific errors, is not just a narrow capability but underpins virtually every aspect of AI-supported education. An AI system that can model how a student reasons about a concept, based on observing their specific mistakes, can better adapt its tutoring, feedback, and assessments to that student's needs. Our research thus positions conditioned distractor generation not merely as an evaluation tool, but as a probe into an AI system's fundamental ability to model student thinking - a capability essential for meaningful educational support. While predicting wrong answers might seem a narrow technical challenge, we show it requires precisely the kind of deep understanding of student cognition that has been missing from current educational AI systems. This paper makes several key contributions:

\begin{enumerate}
    \item \textbf{A Novel Turing-like Test for Educational AI:}
    We introduce a practical and efficient approach to evaluating educational AI systems through a two-phase Turing-like test. Unlike traditional evaluation methods that require lengthy studies of learning outcomes, our test provides a direct measure of an AI system's ability to model student thinking by having it generate new questions and predict errors based on observed student mistakes. The test evaluates whether these AI-generated predictions are indistinguishable from those created by expert human educators, providing a concrete benchmark for measuring progress in educational AI systems' ability to understand individual student reasoning.

    \item \textbf{Positioning Conditioned Distractor Generation as a Core Test of Educational AI Capabilities:}
    Building on this testing methodology, we establish that the ability to generate distractors conditioned on a student's specific errors provides a fundamental test of an educational AI system's capabilities. Through our two-phase methodology - first observing natural student mistakes and then generating new questions with predicted errors - we show that success at this task requires deep modeling of individual student thinking. This provides both a theoretical framework for understanding what capabilities educational AI systems need and a practical method for evaluating these capabilities. Our analysis demonstrates that these same cognitive modeling abilities underpin various educational applications, from providing targeted feedback to designing adaptive assessments, making this test a powerful proxy for evaluating educational AI systems.
\end{enumerate}
