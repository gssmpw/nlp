\section{Limitations and Future Work}

While this paper establishes rigorous mathematical foundations for evaluating educational AI through distractor generation, empirical validation of this framework remains as important future work. The comprehensive theory developed here - from statistical sampling guarantees to proofs of prediction differentiation - provides critical insights into why conditioning on individual student responses is essential and establishes precise validation criteria. This formal foundation addresses fundamental questions about what constitutes genuine understanding of student cognition versus simple pattern matching, creating a principled basis for developing and evaluating AI tutoring systems. Crucially, our theoretical analysis reveals why unconditioned approaches fail to measure true understanding, demonstrating that validating AI systems' educational capabilities requires the type of mathematically grounded, two-phase framework presented here. This theoretical contribution establishes the minimal requirements any evaluation methodology must satisfy to meaningfully assess an AI system's comprehension of student reasoning.