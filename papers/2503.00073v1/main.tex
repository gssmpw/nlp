\documentclass[11pt, a4paper]{googledeepmind}

\usepackage[authoryear, sort&compress, round]{natbib}
\bibliographystyle{abbrvnat}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands}
\input{todos}

\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{siunitx}
\usepackage[capitalise,nameinlink,noabbrev]{cleveref}
\crefname{section}{Sec.}{Secs.}
\crefname{appendix}{App.}{Apps.}
\crefname{algorithm}{Alg.}{Algs.}
\creflabelformat{equation}{#2\textup{#1}#3}
\DeclareMathOperator{\seg}{seg}

\title{Forecasting Whole-Brain Neuronal Activity from Volumetric Video}

\author[1]{Alexander Immer}
\author[1]{Jan-Matthis Lueckmann}
\author[2]{Alex Bo-Yuan Chen}
\author[1]{Peter H. Li}
\author[2]{Mariela D. Petkova}
\author[3]{Nirmala A. Iyer}
\author[3]{Aparna Dev}
\author[3]{Gudrun Ihrke}
\author[3]{Woohyun Park}
\author[3]{Alyson Petruncio}
\author[3]{Aubrey Weigel}
\author[3]{Wyatt Korff}
\author[2]{Florian Engert}
\author[2]{Jeff W. Lichtman}
\author[3]{Misha B. Ahrens}
\author[1]{Viren Jain}
\author[1]{Micha≈Ç Januszewski}

\affil[1]{Google Research}
\affil[2]{Harvard University}
\affil[3]{HHMI Janelia}

\correspondingauthor{mjanusz@google.com, viren@google.com}

\newcommand{\link}[3][blue]{\texttt{\href{#2}{\color{#1}{#3}}}}
\newcommand{\linkrepo}{\link{https://github.com/google-research/zapbench/}{github.com/google-research/zapbench}}
\newcommand{\linkwebsite}{\link{https://google-research.github.io/zapbench/}{google-research.github.io/zapbench}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{abstract}
Large-scale neuronal activity recordings with fluorescent calcium indicators are increasingly common, yielding high-resolution 2D or 3D videos. Traditional analysis pipelines reduce this data to 1D traces by segmenting regions of interest, leading to inevitable information loss. Inspired by the success of deep learning on minimally processed data in other domains, we investigate the potential of forecasting neuronal activity directly from volumetric videos. To capture long-range dependencies in high-resolution volumetric whole-brain recordings, we design a model with large receptive fields, which allow it to integrate information from distant regions within the brain.
We explore the effects of pre-training and perform extensive model selection, analyzing spatio-temporal trade-offs for generating accurate forecasts. Our model outperforms trace-based forecasting approaches on ZAPBench, a recently proposed benchmark on whole-brain activity prediction in zebrafish, demonstrating the advantages of preserving the spatial structure of neuronal activity.
\end{abstract}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

Recent advances in imaging techniques have enabled the recording of neuronal activity at unprecedented resolution and scale. Light-sheet imaging allows recording of whole-brain activity for small animals, such as the larval zebrafish~\citep{hillman2019light}. Raw recordings are in the form of volumetric videos, with hundreds of millions voxels per time step, recorded over hours. Typically, heavy postprocessing is applied to reduce dimensionality of this data down to 1D time traces of activity for distinct regions of interest representing individual neurons or clusters of cells~\citep{abbas2022computational}. Inspired by the success of deep learning models in analyzing minimally processed data in other fields, such as weather and climate forecasting~\citep{rasp2020weatherbench,andrychowicz2023deep},  we explore the potential of building predictive models directly on such volumetric videos, avoiding any information loss.

The ability to predict future behavior based on past observations is a cornerstone of scientific modeling across a diverse range of domains, ranging from physics to social sciences. Until now, it has not been applied in the context of whole-brain activity in a vertebrate. The recently introduced Zebrafish Activity Prediction Benchmark (ZAPBench)~\citep[][\linkwebsite{}]{lueckmann2025zapbench} aims to change that, taking advantage of datasets that can now be acquired with modern microscopy techniques. ZAPBench provides a rigorous evaluation enabled by the comparison of future brain activity predicted from past brain activity to actual experimental recordings, thereby achieving an objective measure for evaluating predictive models of brain function. The dataset used in ZAPBench is a whole-brain recording from a larval zebrafish, collected using a light-sheet microscopy setup~\citep{vladimirov2014lightsheet}. The raw volume is made of ${\sim}1.5$ trillion voxels, which is reduced in size by three orders of magnitude to a trace matrix of time series by applying a neuron segmentation mask. ZAPBench is the first benchmark that poses the forecasting problem for a significant fraction of neurons in a single brain and provides the raw volumetric recordings for the experiments.

To test the viability of end-to-end forecasting on such data, we propose to use a video model based on techniques that have not been applied to this domain previously. Since processing in the brain is highly distributed~\citep{urai2022largescaleb,naumann2016whole}, we hypothesize that large receptive fields are important. Furthermore, in comparison to models applied to activity traces, we expect the following advantages. First, by utilizing the entire video as input, a video-based model is not reliant on the precision of neuron segmentation masks (masks are still applied to predicted frames for direct comparison to trace-based methods). Second, the inherent grid structure of the video preserves the spatial relationships between neurons, information that is otherwise lost during trace extraction. Finally, such a model can leverage potentially relevant visual cues present in the voxels between segmented cells or within the voxels of individual cell masks to enhance forecasting accuracy.

Building a model for this problem poses fundamental engineering challenges. 
Standard video models operate on 2D frames, and the presence of the additional \textsc{z} dimension naturally complicates scaling. In ZAPBench, a single \textsc{xy} slice of a 3D frame has a native size of 2048$\times$1328 pixels, and is thus comparable to a frame of a natural $1080$p video. Every 3D frame is composed of $72$ such slices, increasing the volume of the input data by up to two orders of magnitude relative to such videos and resulting in several hundreds of megabytes per frame.

For our model, we choose a variant of the UNet~\citep{ronneberger2015u} and adapt it to 4D data. 
We develop a pipeline where both input and model are spatially sharded across multiple hosts and accelerators.
To maintain a manageable size of the intermediate activations, we represent temporal input frames as channels. This approach allows us to explore the impact of varying spatial context by manipulating the receptive field while keeping computational cost (FLOPS) roughly constant.


We perform extensive experiments to construct an effective video model for neuronal activity forecasting on ZAPBench.
Despite the success of pre-training in other domains~\citep{devlin2018bert,bao2021beit}, we find it not to be a useful technique for improving forecast accuracy, even when using an order of magnitude more data recorded from other specimens of the same species.
Further, we investigate the effect of input resolution, spatial context, and temporal context of the model on the forecast accuracy.
Surprisingly, we find that lowering input resolution by up to 4x can be beneficial for performance and observe a clear trade-off between spatial and temporal context. 

Our models, which implicitly capture the spatial relationships within their field of view, can improve forecast accuracy beyond that achieved by trace-based models on ZAPBench, especially when only short temporal context is available.
On ZAPBench, multivariate trace-based models, which can in principle learn functional relationships between cells, do not perform significantly better than univariate models that treat all cells independently and identically.  
Our proposed model, which is included in ZAPBench \citep{lueckmann2025zapbench}, is therefore the only multivariate model that can consistently outperform univariate models on this benchmark.


\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth,trim=0 0.17in 0 0,clip=True]{fig_overview.pdf}
\caption{We propose to model light-sheet microscopy recordings of neural activity directly as volumetric video for forecasting instead of extracting and modelling neuron traces. Specifically, we train a model directly on the video and mask the output to optimize the per-neuron mean absolute error (MAE). We find that a UNet performs particularly well for small temporal context and can more effectively utilize spatial contextual information than trace-based time series models.}
\label{fig:overview}
\end{figure*}

In summary, our contributions are as follows:
\alex{Potentially reduce indentation of enum}
\begin{enumerate}
    \item We propose to forecast zebrafish neuronal activity recorded using light-sheet microscopy directly in the native domain as volumetric video (3D + time).
    \item We empirically show that the input resolution and pre-training on additional volumetric videos from similar specimens have negligible impact on the results.
    \item We perform exhaustive model selection to quantify the impact of spatial (\textsc{xyz}) and temporal context size for activity forecasting accuracy and find a clear trade-off.
    \item On ZAPBench, our proposed model is the only approach that consistently benefits from multivariate information, and therefore achieves leading performance for short temporal context.
\end{enumerate}

\section{Forecasting Neuronal Activity from Video}
\label{sec:setup}

We propose to forecast neuronal activity in the ZAPBench dataset~\citep{lueckmann2025zapbench} directly in the volumetric video domain. Specifically, we utilize a temporal context of $C$ video frames to predict the subsequent $H$ frames. Per-neuron forecasts and loss are then computed by applying the segmentation mask to the predicted video frames. This contrasts with the traditional approach, which applies the segmentation mask to the original video data to extract activity traces before performing any forecasting.  See Figure \ref{fig:overview} for a comparison of these two approaches.

The ZAPBench dataset comprises high-resolution, whole-brain activity recordings of a larval zebrafish engaged in various behavioral tasks.  Data was acquired using light-sheet fluorescence microscopy, enabling real-time imaging of neuronal activity at cellular resolution. This was made
possible by using an animal genetically modified to express GCaMP ~\citep{dana2019high}, a fluorescent calcium sensor, in the nuclei of its neurons.
ZAPBench provides both preprocessed activity traces for approximately 70,000 neurons and the corresponding raw volumetric video data.  This raw data, denoted as $\rmY$,  has dimensions of 2048$\times$1152$\times$72$\times$7879 (\textsc{xyzt}) and a resolution of \SI{406}{\nano\metre}$\times$\SI{406}{\nano\metre}$\times$\SI{4}{\micro\metre}$\times$\SI{914}{\milli\second}. We use a center crop of $1328$ voxels in \textsc{y} due to negligible cell activity in the border regions.  Models forecasting $H=32$ time steps are benchmarked using short ($C=4$) or long ($C=256$) temporal context.

\citet{lueckmann2025zapbench} preprocess the raw volumetric video by aligning each frame to a reference volume for stabilization so that the neuron segmentation masks can be statically applied throughout the experiment.
Further, a standard ``$\Delta F / F$'' normalization scheme is applied to the voxel intensities, with $F$ denoting a baseline value~\citep{mu2019glia,zhang2023fast}. The normalized signal is in the $[-0.25,1.5]$ range.

\newpage

\begin{wrapfigure}{r}{0.4\textwidth}
  \vspace{-1.5em}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{fig_segmentation_example.pdf}
  \end{center}
  \vspace{-1em}
  \caption{Illustration of potential loss of information when segmenting neurons. The colored objects are predicted segmentation masks. A fragment of a 2d slice of the activity video is shown in greyscale.}
  \vspace{-1em}
  \label{fig:neuron_mask}
\end{wrapfigure}
The neuron segmentation model is specifically trained for the dataset and yields 71,721 neurons.
Formally, the segmentation mask can be considered as a mapping $\seg \colon \sN \rightarrow 2^{\sS}$ from integer identity of a neuron $\sN = [71721]$ to a set of three-dimensional spatial indices, which is an element of the power set of index locations $\sS = [2048] \times [1328] \times [72]$.
The neuron activity at an arbitrary timestep $t$ is then given by \emph{averaging} the activity over spatial locations associated with each cell, i.e.:
\begin{equation}
    \label{eq:trace_extraction}
    \ervy_n(t) = \frac{1}{| \seg(n) |} \sum_{s \in \seg(n)} \rmY_s (t).
\end{equation}
While this is a natural choice, it loses information related to cell size, position and spatial distribution of intensities within it, and completely discards voxels that are not part of any segmentation mask or incorrectly segmented.
\cref{fig:neuron_mask} depicts these potential issues.

We instead apply a video model to the raw input frames and directly forecast volumetric frames while optimizing and measuring the mean absolute error (MAE) on the segmented neurons.
Prior work in neural response prediction~\citep{schoppe2016measuring,cadena2019deep} has proposed additional metrics that explicitly take  trial-to-trial variability into account. The experimental setting used in ZAPBench did not allow for sufficiently numerous trial repetitions to make these metrics applicable, but we note them as an interesting direction to explore in future work if calcium recordings made with increased number of trials become available.
In addition to MAE, we also report correlations between the predicted and actual activity in \cref{app:corr}.

One frame of the volumetric video can be described as $\rmY(t) \in \sV$ with $\sV = \R^{2024 \times 1152 \times 72}$.
A~video model with a $P$-dimensional weight vector $\rvw \in \R^P$ can then be denoted as $\rvf: \sV^C \times \R^P \rightarrow \sV^H$.
That means the video model $\rvf$ receives a 4D volumetric input with $C$ frames, outputs $H$ frames, and is parameterized with weights  $\rvw$.
We obtain the prediction of the $h$-th frame as
\begin{equation}
    \label{eq:model}
    \hat{\rmY}(t, h) = \rvf_h(\rmY(t), \ldots, \rmY(t+C), \rvw),
\end{equation}
and denote by $\hat{\rvy}(t,h)$ the corresponding 1D trace vector computed using Eq.~\ref{eq:trace_extraction}.
For a fair comparison with trace-based models in ZAPBench we optimize the trace-based MAE $\mathcal{L}$ over all training timesteps $T_\text{train}$ with respect to the model parameters $\rvw$
\begin{equation}
    \label{eq:trace_mae}
    \mathcal{L}(\rvw) = \frac{1}{|T_\text{train}| \, H \,|\sN|} \sum_{t \in T_{\text{train}}} \sum_{h \in [H]} \sum_{n \in \sN} \left\vert \rvy_n(t + h) - \hat{\rvy}_n(t, h) \right\vert.
\end{equation}
If we instead optimize the voxel-wise MAE, the models perform relatively worse when evaluated on the trace-based MAE because it corresponds to a different weighting of neurons by their size expressed in number of voxels.

\section{Scalable Volumetric Video Architecture}
\label{sec:model}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{fig_architecture.pdf}
\caption{Architecture and input sharding overview. \textbf{A}: We use a variation of the UNet architecture~\citep{ronneberger2015u} with 3D spatial input and treat the $C$ input frames as channels. 
Further, we use a fixed number of features at every resolution to improve scalability. 
The network is conditioned on the time horizon $H$ and outputs a single volumetric frame at a time, similar to MetNet-3~\citep{andrychowicz2023deep}. 
To control for spatial context at constant FLOPS, four blocks at the lowest resolution can be replaced by one block of higher resolution. 
\textbf{B}: Data loading and the network are spatially sharded and allow for flexible scaling to full resolution inputs.}
\label{fig:arch}
\end{figure*}

Efficiently training models consuming high-resolution volumetric video of varying input context sizes $C$ requires a scalable architecture and data loading system.
We achieve this by extending a standard UNet architecture~\citep{ronneberger2015u} to 4D by mapping temporal input context to features of the first convolutional layer, conditioning on lead-time to predict only single frames, and sharding both the model and the data loading process.
\cref{fig:arch} shows the intermediate resolutions and representation sizes.
The network comprises a series of pre-activation residual convolutional blocks~\citep{he2016identity} with fixed feature size $F=128$, each with two group normalization layers~\citep{wu2018group} using 16 groups, Swish activation~\citep{ramachandran2017searching}, and $3^3$ convolutions for \textsc{xyz} throughout.

\subsection{Temporal Input Context as Features}
Typically, video UNet variations use color channels as input features~\citep{gao2022simvp,ho2022imagen} and convolve over frames using a temporal convolution~\citep{ho2022video}. This approach is intractable in our case because of the additional \textsc{z} dimension. 
Instead, we treat the temporal input context of $C$ frames as input features to the UNet.
This confers the following advantages: 1) the temporal sizes of the input and output are decoupled, 2) the network parameter count is easily controlled, 3) representation sizes and computation requirements are reduced while using more features, and 4) early layers of the network have access to long-range temporal dependencies.
Our model is similar to architectures used in standard time series models, which often treat temporal context as features~\citep{zeng2023transformers,chen2023tsmixer}.

\subsection{Varying the Receptive Field}
We design a flexible UNet architecture that can adapt the receptive field while keeping the computational cost (FLOPS) fixed.
We find that full native resolution is not necessary for optimal prediction accuracy~(see \cref{sec:model_selection}), and thus downsample the input by a factor of 4 in \textsc{xy} using averaging.
The first resampling block then uses a factor 2 in \textsc{xy} to achieve roughly isotropic resolution in \textsc{xyz}, while the following ones downsample equally in all dimensions.
We always use four residual blocks at the lowest resolution, and three at all other resolutions.
This allows us to change the receptive field while keeping the FLOPS roughly fixed by removing the four lowest resolution blocks and instead adding one block to the respective next higher resolution.
This is because one block at the higher resolution requires as many FLOPS as four blocks after downsampling by a factor of two in \textsc{x} and~\textsc{y}.
In an ablation, we show that controlling for FLOPS is sensible because increasing the parameter count does not increase performance further~(see \cref{fig:param_ablation}).

The receptive field along a dimension depends on the cumulative product of the downsampling factors and the number of convolutions at the lowest resolution,
\begin{equation}
    \label{eq:receptive_field}
    \nonumber
    \texttt{receptive\_field}_{\texttt{dim}} = \texttt{cum\_downsampling\_factor}_{\texttt{dim}} \times \texttt{num\_blocks} \times \texttt{4},
\end{equation}
where the factor $4$ is because every block has two convolutions, each of which increase the receptive field by two. 
For a network that does not downsample at all, as for example used in \cref{sec:spatial_vs_temporal}, to account for the input and output convolutions and the center voxel, we have to increase the receptive field size by five.
Therefore, the architecture depicted in \cref{fig:arch} has a receptive field of $(1024, 1024, 128)$ in \textsc{xyz} comparable to the size of the complete frame. We tried to further enhance the receptive field to cover the whole frame using a multi-axis vision transformer~\citep{tu2022maxvit} at the lowest resolution, but did not observe any accuracy gains.
\alex{Refer to appendix for ablation on increasing parameters}
For the output, we upsample twice to obtain the original resolution, and use one residual block per resolution, but with a reduced feature dimension of $F'=32$ to keep hidden representations at a manageable size.

\subsection{Lead-time Conditioning}
\begin{wrapfigure}{r}{0.4\textwidth}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{fig_loss_ablation.pdf}
  \end{center}
  \vspace{-1.5em}
  \caption{Comparison of direct MAE and lead-time conditioned variants.}
  \label{fig:loss_ablation}
\end{wrapfigure}
Instead of forecasting autoregressively or predicting the complete horizon of $H$ frames in a one-shot way, we condition the network on an integer lead-time $h \in [H]$ and predict the corresponding single frame independently as proposed by~\citet{andrychowicz2023deep} for weather forecasting.
During data loading, we sample a lead time and the corresponding target frame uniformly at random from $[H]$.
This requires loading only a single target frame per sample and, in line with previous results from weather forecasting~\citep{rasp2020weatherbench}, performs better than frame-level autoregressive prediction on our problem.
Every convolutional block in the network is conditioned using a FiLM layer~\citep{perez2018film} on the lead time encoded using a 32-dimensional sinusoidal embedding~\citep{vaswani2018transformer}.
\cref{fig:loss_ablation} shows that directly predicting all $H$ frames tends to overfit while lead-time conditioning performs equally well with both MAE and HL-Gauss~\citep{farebrother2024stop}, a distributional regression objective that results in slightly faster model convergence.
However, in our experiments we use the conditioned MAE for its simplicity and because it does not require binning, which might complicate pre-training on datasets of slightly different scale.

\subsection{Sharded Data Loading and Model}
Despite the scalability features of the proposed UNet model for  volumetric video, in practice applying it requires distributing the input and hidden representations across accelerators and machines.
We train all models in \cref{sec:experiments} using a single sample per batch, noting that this can already correspond to several GBs of input data.
We use spatial sharding in \textsc{xy} using the \texttt{jax.Array} API~\citep{jax2018github} so that each box in \cref{fig:arch}B is handled by an individual accelerator. 
We also implement a custom data loader that distributes data loading across hosts so that each machine only loads the necessary subvolumes. 
To achieve this, we chunk our data in the \texttt{zarr3} format~\citep{zarr2023} and use the TensorStore API~\citep{google2020ts} to load and collate chunks.
Our data loader follows the jax sharding automatically.


\section{Experimental Results}
\label{sec:experiments}

We present experimental results evaluating the proposed volumetric video model on ZAPBench, a benchmark for whole-brain neuronal activity prediction for a larval zebrafish~\citep{lueckmann2025zapbench}. Uniquely, ZAPBench provides the raw volumetric recordings for most of the neurons in the brain enabling data-driven approaches like ours.
First, in \cref{sec:model_selection} we empirically select and validate the final architecture variant used for the benchmark. 
In particular, we investigate the trade-off between temporal context $C$ and spatial context in the form of the receptive field to assess the need for multivariate models.
Further, we evaluate the feasibility of pre-training on additional zebrafish specimens as well as the effect of input resolution.
We identify the model depicted in \cref{fig:arch} as a strong model for the short context size $C=4$, where we achieve the best performance across the benchmark, as presented in \cref{sec:benchmark}.
For the long temporal context $C=256$, we only see an improvement of forecast accuracy in specific cases.

\paragraph{Hyperparameters.}
Unless stated otherwise, we train every model for $250\text{k}$ to $500\text{k}$ steps by optimizing the trace-based MAE with a batch size of $1$ using the AdamW optimizer~\citep{loshchilov2017fixing} using an initial learning rate of $10^{-4}$ decayed using a cosine schedule~\citep{loshchilov2017sgdr} to $10^{-7}$ and a weight decay factor of $10^{-5}$.
Due to their tendency to overfit, we use a dropout rate of $0.1$ on the features for long-context models with $C=256$. 
These hyperparameters were optimized on the validation set during development.
We choose checkpoints based on the validation performance monitored during training.
We present experimental results in terms of mean performance and report two standard errors over three random seeds that control data loading and parameter initialization.
The only exception to this are the high resolution results presented in \cref{sec:input_resolution}, where we only report a single result because of their compute requirements.
Most individual training experiments use $16$ A100~40GB GPUs.

\subsection{Model Selection}
\label{sec:model_selection}

We compare between different methods and models to improve performance on the ZAPBench benchmark.
For \cref{sec:spatial_vs_temporal} and \ref{sec:pretraining}, we downsample the volumetric frames by a factor of four in \textsc{xy} using averaging to 512$\times$288$\times$72.
The segmentation mask is downsampled to the same shape using striding.
We investigate the effect of spatial and temporal context and the potential for pre-training on related datasets. 
In \cref{sec:input_resolution}, we use the full resolution ZAPBench targets and segmentation, and assess the importance of input resolution on performance.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{fig_spatial_temporal.pdf}
\caption{Validation and test performance for varying temporal context sizes $C$ as well as spatial context sizes $S$ with networks having comparable FLOPS. We find that there is a trade-off between spatial and temporal context with a cross-over point between $C=16$ and $C=64$, where spatial context stops being useful and leads to overfitting. The periodicity of many conditions is roughly $64$, which might explain spatial context becoming redundant. We report the mean and two standard errors.}
\label{fig:spatial_vs_temporal}
\end{figure*}

\subsubsection{Spatial vs. Temporal Context}
\label{sec:spatial_vs_temporal}
We use UNets with different numbers of downsampling blocks to vary the spatial context but keep the FLOPS fixed~(see \cref{sec:model}), and find that there is a trade-off between the spatial ($S$) and temporal ($C$) context.
We compare models without any downsampling blocks, with two downsampling blocks, and with four.
The models have spatial contexts $S$ of $21$, $64$, and $256$ in \textsc{xy}, respectively.
Details on the architecture and computation of the receptive field can be found in \cref{app:spatial_vs_temporal_arch}.
Also note that the spatial context at full resolution of these models would be 4$\times$ higher.
\cref{fig:spatial_vs_temporal} shows that a short temporal context requires larger spatial context to obtain optimal performance.
For long temporal context, however, the models with large spatial context start to overfit and underperform. The effect becomes apparent between a temporal context of $16$ and $64$.
This result suggests that video models are able to exploit multivariate information for short temporal context but provide little benefit for long context, where univariate models perform equally well~\citep{lueckmann2025zapbench}.

\subsubsection{Pre-Training on Other Specimens}
\label{sec:pretraining}

\begin{wraptable}{r}{0.42\textwidth}
\centering
\vspace{-1em}
\begin{tabular}{l|c}
\textsc{setting}                             & \textsc{test mae}        \\ \hline
Train                                    & $0.02573 \pm 0.00005$            \\
Pre-train +2       & $0.02590 \pm 0.00005$            \\
Pre-train +8    & $0.02591 \pm 0.00001$            \\
Train + Val     & $0.02534 \pm 0.00010$         \\
\end{tabular}
\caption{Training on more data from the same specimen (``Train + Val'') improves performance more than pre-training and finetuning on others. Results shown for $C=4$ and data 4$\times$ downsampled in \textsc{xy}.}
\vspace{-1em}
\label{tab:pretraining}
\end{wraptable}
We attempt to pre-train a model on other specimens recorded and preprocessed in a similar way to the zebrafish used for ZAPBench.
We pre-train the model either on two additional specimens recorded in the same experimental session, or on these two and six more from two other sessions.
Because there is no segmentation available for the other specimens, the model is pre-trained using voxel-based MAE for 800k steps, and then fine-tuned on the ZAPBench dataset for 200k steps using the trace-based MAE.
We use three different learning rates, $10^{-4}$, $10^{-5}$, and $10^{-7}$,  for fine-tuning, and select the best model by validation performance, which was obtained by fine-tuning with the lowest learning rate.
\cref{tab:pretraining} shows that pre-training with fine-tuning does not improve performance over standard training.
However, training on $\sim$14\% more data from the same specimen does improve performance significantly.
Confidence intervals shown are calculated as two standard errors.

\subsubsection{Effect of Input Resolution}
\label{sec:input_resolution}

We assess the relevance of input resolution when forecasting neuronal activity, and find that, surprisingly, predicting from a lower resolution performs best.
We compare three variants of our model:
a model that predicts from data 4$\times$ downsampled in \textsc{xy}, as depicted in \cref{fig:arch}, one that downsamples only by factor $2$, and one that is parameterized at full native resolution.
Therefore, the full-resolution model loads and processes $16\times$ more data. 
We achieve almost perfectly linear scaling by using proportionally more compute resources, maintaining the same throughput thanks to the sharded data input pipeline and model~(see \cref{sec:model}). In all cases, we scale the field of view of the network so that its size in physical units remains constant between experiments (see \cref{app:input_resolutions}).

\begin{wraptable}{r}{0.42\textwidth}
\centering
\begin{tabular}{l|l}
\textsc{Input}                             & \textsc{test mae}        \\ \hline
Downsample 4$\times$                                    & $0.0268 \pm 0.0002$            \\
Downsample 2$\times$       & $0.0268$            \\
Full resolution    & $0.0273$            \\
\end{tabular}
\caption{Increasing input resolution does not improve performance, and decreases it slightly at full resolution.}
\vspace{-0.5em}
\label{tab:input_resolution}
\end{wraptable}
For $C=4$, \cref{tab:input_resolution} shows that the model with the lowest input resolution obtains a trace-based test MAE that is statistically identical with that of the model using intermediate resolution inputs.
However, the full resolution model performs significantly worse.
This suggests that despite the short temporal context input resolution does not play a major role in improving performance, and that the intracellular voxel-to-voxel variations in the recorded images do not carry information useful for forecasting, which might have applications to the design of future zebrafish activity recording experiments. We suspect that the decreased performance of the full resolution model could be caused by the significantly increased input voxel-to-parameter ratio while keeping the number of training examples fixed.

\subsection{Performance on ZAPBench}
\label{sec:benchmark}

We evaluate the best-performing architectures on ZAPBench for both short and long context settings. In \cref{fig:trace_benchmark}, we report the trace-based MAE versus forecasting steps in comparison to the best-performing trace-based models ~\citep{lueckmann2025zapbench}. We average performance across test sets of different stimulus conditions.
For trace-based models, TSMixer~\citep{chen2023tsmixer} achieves the best overall performance. We use the video-based architecture depicted in \cref{fig:arch} for the short context that has a spatial context of 1024$\times$1024$\times$72 in \textsc{xyz}, which is global except in \textsc{x} where it covers half the voxels.
For the long temporal context, we use a model that does not downsample further than $(4, 4, 1)$ at the input, which we found in \cref{sec:spatial_vs_temporal} to work best for this case. 
This model has a spatial context of 64$\times$64$\times$21, which corresponds to \SI{26}{\micro\metre}$\times$\SI{26}{\micro\metre}$\times$\SI{84}{\micro\metre} in \textsc{xyz}.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{fig_trace_benchmark.pdf}
\caption{Comparison of volumetric video model with best-performing trace-based model for short (left) and long (right) context on the benchmark test set (averaged over eight conditions) and the experimental condition held out from the training data. We report the mean and two standard errors.}
\label{fig:trace_benchmark}
\end{figure*}

We find that the volumetric video models achieve the best performance in the short context $C=4$ setting. 
For $C=256$, there is no significant difference between the univariate trace-based model and the video model on the test set when evaluated with MAE, but the video model does improve correlation metrics (\cref{app:corr}).
This aligns with our observation in \cref{sec:spatial_vs_temporal}, where longer temporal context requires less spatial context for the same forecast accuracy.
ZAPBench also holds out one stimulus condition entirely from training. We find that video models generalize better on this holdout condition for one-step-ahead forecasts but not for longer horizons.
In \cref{app:exp}, we further show model performance separately for each experimental condition.
For the short context, we find that the video model performs better in five, equally well in one, and worse in three out of the nine conditions.

More precisely, when evaluated with a context length $C=4$ on both the test and holdout sets, the video model demonstrates a significant improvement in one-step-ahead forecasting accuracy, achieving about 8 (test) and 6 (holdout) percentage point reduction in error compared to the best performing trace-based model. With $C=256$, the video model exhibits marginally superior performance in the first few forecasting steps, achieving up to a 2 percentage point reduction in MAE at the first step.  Beyond the initial steps, both models demonstrate comparable accuracy on the test set.

What explains the improved performance of the video model relative to the trace-based approaches? In \cref{app:unsegment} we report results of an experiment in which we masked out all unsegmented voxels, which did not affect the grand test-MAE. This suggests that segmentation quality is not a significant limitation, that no significant information is contained in the unsegmented regions of the dataset, and that the accuracy gains can be attributed to better utilization of the spatial distribution of the recorded fluorescence signal.

Furthermore, the results in \cref{tab:input_resolution} and \cref{fig:spatial_vs_temporal} suggest that it is specifically the correlations between cells in the recorded fluorescence signals, rather than the distribution of signals within individual cells, that drives these improvements. To test this hypothesis, we also trained models on synthetic data with all voxels of every cell replaced with the value of the activity trace for that cell (see \cref{app:unsegment} for details), which did not significantly degrade forecasting accuracy. We then tried scrambling the signal by performing the same experiment but with the cell-trace association randomized, which \emph{did} reduce accuracy.

\section{Conclusion}
\label{sec:conclusion}

We presented an approach for forecasting of zebrafish neuronal activity based on utilizing the raw neural recording data as a volumetric video to make predictions. We find that this method has several advantages over traditional trace-based methods. In particular, video-based prediction leverages spatial relationships between neurons that are hard to exploit when reducing the data to 1D traces. This allows for more accurate predictions, especially when working with short temporal contexts. This advance comes at the expense of a significant increase in computational cost (2-3 orders of magnitude relative to trace-based models, see \cref{app:cost} for details).

We report several findings that were contrary to our expectations.
First, we surprisingly find that using higher resolution input frames does not improve performance.
Second, the commonly used paradigm of pre-training on a larger data set and fine-tuning only leads to reduced forecast accuracy in our experiments.
In contrast, we observe that more data from \emph{the same specimen} does improve performance, so we hypothesize that pre-training may be complicated by distribution shifts between specimens, such as differences in signal and noise levels.
Finally, increasing model capacity does not always translate to performance improvements but instead leads to overfitting for long temporal context.

Future work could explore the use of probabilistic models, latent space representations, and more sophisticated regularization methods and input augmentations to further improve the accuracy of video forecasting for neuronal activity.

\subsubsection*{Acknowledgments}

We thank Nal Kalchbrenner, Jeff Donahue, Stephan Hoyer, and the Connectomics team at Google Research for technical discussions, and Michael P. Brenner, Lizzie Dorfman,  Elise Kleeman, and John Platt, for project support. 

\bibliography{references}

\newpage
\appendix
\section{Appendix}

\subsection{Architectural Details}
\label{app:archs}
Every network has an embedding $3^3$ convolutional layer mapping from temporal context $C$ to $F$ features, and an output convolutional layer mapping from $F'$ (when upsampling) or $F$ to $1$ feature, giving the single lead-time conditioned forecast frame.
In the downsampling pathway, we apply one convolutional block at every resolution.
During symmetric upsampling, we use three convolutional blocks at the lowest resolution, and two for all higher resolutions.
Before upsampling to super-resolution (i.e., resolution that is higher than that of the input), we use a convolution to map from $F$ to $F'$ features to reduce the size of intermediate representations.
During super-resolution upsampling we use one convolutional block per resolution.
Each convolutional block has a pre-activation residual design with the following chained layers: group normalization, swish activation, $3^3$ convolution, group normalization, conditioning on lead time using a FiLM layer, swish activation, optional feature dropout (only used with rate $0.1$ for $C=256$), and lastly the second $3^3$ convolution.
The UNet-structure is realized by adding the representations obtained during sequential downsampling to the upsampled representation.
The number of features at every resolution is fixed to $F=128$, except for the super-resolution upsampling, where it is $F'=32$.

\subsubsection{Spatial vs. Temporal Models}
\label{app:spatial_vs_temporal_arch}
This study employs three distinct models based on the aforementioned design.

The first model, maintaining a consistent spatial dimension of 512$\times$288$\times$72, forgoes downsampling and upsampling blocks. It incorporates four processing blocks at this resolution, along with two convolutional layers at the input and output stages.  The receptive field, calculated as $S=1+(4\times2+2)\times2=21$, is determined by considering the central voxel and adding 2 for each $3^3$ convolution.

The second model, downsamples the input data to 64$\times$64$\times$32 and has a receptive field of $S = 64$. This is derived from the cumulative downsampling factors of (4, 4, 2) in the \textsc{x}, \textsc{y}, and \textsc{z} dimensions, respectively, and applying~\cref{eq:receptive_field}.

Similarly, the third model employs downsampling factors of $(16, 16, 8)$, resulting in a 256$\times$256$\times$128 receptive field. This translates to a global receptive field along the \textsc{z}-axis, a near-global receptive field along the \textsc{y}-axis, and a receptive field encompassing half of the total extent along the \textsc{x}-axis.

\subsubsection{Lead-Time Conditioning}
For the results shown in \cref{fig:loss_ablation}, we use three different losses: direct MAE, conditioned MAE, and conditioned HL-Gauss.
Apart from the FiLM layers to condition on lead-time, the architecture is the same in all cases, with the exception of the last layer which maps from $F$ to the output dimensionality.
The output dimensionality for the direct MAE is the number of forecast timesteps~$H$.
For the conditioned MAE, it is simply $1$, as also described in \cref{fig:arch}.
For the conditioned HL-Gauss loss, it is $32$, which is equal to $H$, and each output corresponds to a discretized bin of the data range.
The HL-Gauss loss transforms a real value by representing it as a weighted average of bin mean-values, for details see \citep{farebrother2024stop}.

\subsubsection{Models for Different Input Resolutions}
\label{app:input_resolutions}

To investigate the influence of input resolution on model performance, we conducted a comparative analysis.  We compared our primary model, which operates on data downsampled by a factor of $4$ in the \textsc{xy} plane, with two alternative configurations: one employing a downsampling factor of $2$, and another utilizing full-resolution input.

To ensure equitable comparison, the architectures of these models were kept broadly consistent, with necessary adjustments to accommodate the differing input resolutions, while maintaining a consistent full-resolution output frame. Specifically, for the model operating on 2$\times$ downsampled input, we augmented the architecture with three additional blocks at the input resolution and removed one upsampling block, relative to the architecture depicted in~\cref{fig:arch}.  In contrast, the model utilizing full-resolution input incorporated two initial downsampling blocks with factors $(2, 2, 1)$ and omitted any super-resolution components, resulting in a conventional UNet architecture.

\subsection{Impact of Unsegmented Voxels and Spatial Distribution of Calcium Signal}
\label{app:unsegment}
In contrast to video models which analyze all voxels of the calcium movie, the trace extraction process ignores voxels that do not correspond to segmented cells. This potentially discards information that could be useful in forecasting. To test to which degree this is indeed the case, we trained the $C=4$ video forecasting model with the unsegmented voxels set to constant value (0). The grand average test MAE for that model ($0.0267 \pm 0.0001$) was not significantly different from that of the video model processing the complete volume ($0.0267 \pm 0.0003$).
This indicates that the unsegmented voxels are unlikely to contain information that could improve forecasts and that any gains relative to the trace-based models can be attributed to the utilization of the spatial distribution of the underlying calcium signals within the segmented cells.

To further test this hypothesis we rendered two "synthetic calcium movies": one ("rendered traces") with the voxels of the segmented cells set to the corresponding trace value (uniformly throughout each cell), and one ("shuffled traces") with the traces randomly reassigned to different cells. Training video models on this data, we observed that the model using "rendered traces" performs equivalently to the ones using the full $\Delta F / F$ and the segment-masked $\Delta F / F$ volumes (test MAE of $0.0267 \pm 0.0001$). The "shuffled traces" variant however showed significantly worse test MAE ($0.0272 \pm 0.0001$).
This provides additional evidence that the distribution of fluorescent signal within individual cells has negligible impact on forecasting accuracy, and that the additional accuracy of the video model stems for the utilization of multivariate, cross-cell information -- precisely the type of information that trace-based models in ZAPBench have difficulty using.

\subsection{Additional Experimental Results}
\label{app:exp}
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-1em}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{fig_param_field_ablation.pdf}
  \end{center}
  \vspace{-1em}
  \caption{Ablation on increasing parameter count instead of receptive field.}
  \label{fig:param_ablation}
  \vspace{-1em}
\end{wrapfigure}

In \cref{fig:trace_benchmark_c=4} and \ref{fig:trace_benchmark_c=256}, we show a fine-grained version of the benchmark of the trace and video-based models in the main text~(\cref{fig:trace_benchmark}).
The figures show the performance per experimental condition the fish was exposed to.
For more details on these conditions, we refer to ZAPBench~\citep{lueckmann2025zapbench}.
For short context, $C=4$, we observe that the video-based model performs better on six experimental conditions, and worse for many steps ahead on the ``dots'', ``taxis'', and ``open loop'' conditions. 
For long context, the video-based model performs almost identical. As in the main text, we display two standard errors about the mean in with shaded regions.

\cref{fig:trace_benchmark_additional} reports performance relative to four trace-based models included in ZAPBench. \cref{fig:mae_illustration} illustrates MAE differences for a few example frames.

On the right in \cref{fig:param_ablation}, we further show an ablation to confirm that the improvement of multivariate video models is due to increased receptive field and not because of using more parameters. 
In particular, in our experimental setup in \cref{sec:spatial_vs_temporal} we keep FLOPS fixed instead of number of parameters.
In the example on the right, we instead increase the width by a factor of two leading to an increase in FLOPS by a factor of $4$ while keeping the receptive field fixed.
We observe that increasing FLOPS at the same spatial context leads statistically to the same performance.
Therefore, the performance improvement observed in \cref{fig:spatial_vs_temporal} is likely due to the increased receptive field, especially for short context.
The example on the right is for the case of $C=4$.


\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{fig_trace_benchmark_c=4.pdf}
\caption{Comparison of volumetric video to best trace-based model on all conditions \textbf{for short context, $C=4$}.}
\label{fig:trace_benchmark_c=4}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{fig_trace_benchmark_c=256.pdf}
\caption{Comparison of volumetric video to best trace-based model on all conditions \textbf{for long context, $C=256$}.}
\label{fig:trace_benchmark_c=256}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig_trace_benchmark_additional.pdf}
\caption{Comparison of volumetric video model with four trace-based models from ZAPBench~\citep{lueckmann2025zapbench} for short (left) and long (right) context on the benchmark test set (averaged over eight conditions) and the experimental condition held out from the training data. In remaining figures, we report performance relative to TSMixer. Note that MAEs of TiDE on the holdout are higher than the axis limits, which is due to its reliance on stimulus covariates. We report the mean and two standard errors.}
\label{fig:trace_benchmark_additional}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig_mae_illustration.png}
\caption{Illustration of MAE differences. Top row shows the MAE between target and predicted activity for a video model on five test set frames for the gain condition, $C=4$, at 32 steps predicted ahead, with brighter colors indicating higher error. Bottom row shows corresponding MAEs on these frames for a trace-based model. When MAEs are averaged across all test set frames and neurons for this condition, the MAE difference between these models is approximately 0.005.}
\label{fig:mae_illustration}
\end{figure*}

\clearpage
\subsection{Correlation metrics}
\label{app:corr}
\begin{table}[h]
\renewcommand{\arraystretch}{1.5}
\caption{Test set $\mathrm{Corr_H}$ $\pm$ 2 SE.}
\label{tab:corr_h}
\centering
\begin{tabular}{ccc}
\noalign{\global\arrayrulewidth=0.8pt}
\hline
\textbf{Context} & \textbf{Video} & \textbf{Trace} \\ \hline
\noalign{\global\arrayrulewidth=0.5pt}
$C=4$   & \textbf{0.1495 $\pm$ 0.0018} & 0.1155 $\pm$ 0.0056 \\ \hline
$C=256$ & \textbf{0.1853 $\pm$ 0.0003} & 0.1645 $\pm$ 0.0007 \\
\noalign{\global\arrayrulewidth=0.8pt} \hline
\end{tabular}
\end{table}

To better measure the quality of the temporal structures predicted by the model, we also computed two types of correlation metrics $\mathrm{Corr_W}$ and $\mathrm{Corr_H}$, which compare recorded and predicted activity over $H=32$ steps, with the predictions assembled at constant lead time $h$ or from a specific starting point $t$, respectively (see \cref{fig:xcorrs}).

The correlation metrics paint a picture broadly consistent with that shown by the MAE, except in the long-context regime $C=256$ where the video model outperfoms the trace-based models (see \cref{tab:corr_h}, \cref{fig:corr_w}).



\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig_xcorrs.pdf}
\caption{Illustration of the two types of correlation metrics (for a single neuron). Top: actually recorded activity (green) is aligned in experiment time~$t$ with predicted snippets (blue) of activity of length $H=32$ starting from various offsets. Bottom: correlations are always computed over~$32$ time steps between predicted activity and corresponding real recording. In $\mathrm{Corr_H}$, complete predicted snippets are correlated with the recordings, and then averaged over starting points. In $\mathrm{Corr_W}$, snippets are assembled from predictions at a specific lead time~$h$, and correlated with the corresponding recordings. Reported metrics are averaged over all neurons.}
\label{fig:xcorrs}
\end{figure*}



\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig_trace_benchmark_corr_w.pdf}
\caption{Comparison of volumetric video model with best-performing trace-based model in terms of $\mathrm{Corr_W}$ for short and long context on the benchmark test set (averaged over eight conditions) and holdout, higher is better.  We report the mean and two standard errors. }
\label{fig:corr_w}
\end{figure*}


\subsection{Computational cost estimates}
\label{app:cost}
The loss ablation in \cref{fig:loss_ablation} required around 5k GPU hours, pre-training and fine-tuning as shown in \cref{sec:pretraining} around 14k GPU hours, comparing spatial to temporal context in \cref{sec:spatial_vs_temporal} around 50k GPU hours, and the final results including the ablation on input resolution another 30k GPU hours.
This makes a total of roughly 100k GPU hours used for the experiments presented in the paper.

A single training run of the best performing video model for $C=4$ required \SI{36}{\hour}, whereas the model for $C=256$ required \SI{120}{\hour}, both using 16 A100 GPUs. This compute cost is two to three orders of magnitude higher than that incurred by training the baseline trace-based models, which require about \SI{2}{\hour} on a single A100 GPU.
However, video models require less raw data preprocessing relative to time series models, partially offsetting the increased cost.

\end{document}