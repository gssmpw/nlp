\section{Results}


\paragraph{Promising Results in Rating Correlation}
For all dimensions in \texttt{Creative-100}, the correlations between average human and VLM ratings are both high and statistically significant, with InternVL2-8B being the best-performing model in every dimension. However, the correlations are much lower in \texttt{Atypical-300}, although they are statistically significant. We believe this is due to the small annotation size (3 per ad) in \texttt{Atypical-300}, which can easily be biased by one annotation data point, leading to an overall more noisy distribution of ratings. Distribution Divergence also shows promising results, with the lowest divergence achieved by the LLaVA-13B model. Cross-dataset disparity is also much lower, where the KL divergence is similar for the atypicality in both datasets. Output examples and reasoning text word cloud in Appendix \ref{sec:appendix_distribution_output}.


\paragraph{Disagreement Prediction Remains Challenging} For all scenarios in Disagreement Prediction (except for Atypicality in \texttt{Creative-100}, with InternVL2-8B), disagreement predictions have no statistical correlation with human rating standard deviations. For LLaVA-13B, all outputs are ``middle'', making the correlation result \textit{nan}. This suggests that using VLM as a group-opinion synthesizer remains challenging \footnote{We also calculated the correlation between standard deviations of model predictions and that of human ratings, but the correlations are all near zero.}. 


\paragraph{Great Performance in Pairwise Preference} Results from the pairwise preference task are very impressive, with the best-performing GPT-4v achieving more than 0.9 F1 score. We also further analyze the performance by dividing image pairs into ``easy'' and ``hard'' subsets: compared to median human rating differences if a pair of ad images has a higher absolute difference, it is an easy image pair, and vice versa. As shown in Table \ref{table:mturk_results}, all VLMs perform better in easy tasks than hard ones, confirming the alignment with human preference differences.


\paragraph{Smaller Models' Superior Performance} Counterintuitively, LLaVA-7B and InternVL2-8B consistently outperform LLaVA-13B in Rating Correlation and Pairwise Preference, both of which are ranking-based evaluations. Based on the error analysis (more details in Appendix \ref{sec:appendix_error_analysis}), we believe this can be explained by ranking tasks requiring higher reasoning capability from the language part of the VLM. The language parts of those two smaller models are Mistral-7B and InternLM2.5-7B-Chat, both having a higher ranking on the HuggingFace Open LLM LeaderBoard \footnote{\href{https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard}{HuggingFace LLM Leaderboard}} in reasoning tasks compared to the language part (Vicuna-13B) of LLaVA-13B~\footnote{More output analyses are in Appendix \ref{sec:appendix_output_analysis}}.
