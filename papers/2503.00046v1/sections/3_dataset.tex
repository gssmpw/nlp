\section{Dataset}

\input{sections/table_questions}

We use the Pitt Ads Dataset (referred to as \texttt{Pitt-Ads}) as our basis, where each ad image is annotated with its topic, expected actions from viewers after seeing the ad, binary labels of atypical objects in it (when applicable), and the topic of the ad (10 topic groups in total) \cite{Hussain2017AutomaticUO,ye2019interpreting}. We sample 100 ads and collect fine-grained human creativity annotations (\texttt{Creative-100} in \ref{sec:fine_grained_creativity_data}); we also sample an additional 300 ads from the remaining data points for atypicality prediction(\texttt{Atypical-300} in \ref{sec:atypicality_data}).


\subsection{\texttt{Creative-100}}
\label{sec:fine_grained_creativity_data}
\texttt{Creative-100} consists of 100 ads, with 10 from each topic group: food, pet, drinks, automobile, electronics, service, education, beauty, healthcare, clothing, home, leisure, shopping, and non-commercial. To do quality creativity evaluation, we break down creativity into two dimensions: originality and atypicality, the two most influential dimensions for ads creativity according to \cite{modeling_determinants}. Human annotations are then collected via Amazon Mechanical Turk (Mturk) to represent fine-grained ratings in all three dimensions: \textbf{originality}, \textbf{atypicality}, and \textbf{creativity} (see Figure \ref{fig:mturk} for Mturk annotation interface). 

Due to the inherent subjectivity of the creativity judgment, we formulate the measurement of creativity as several multiple-choice questions with possible answers as a categorical distribution of those choices. In other words, the predictive target is not a single label (e.g., ``creative'') but a distribution of human ratings. This motivates us to collect 25 annotations per ad image to approximate the true rating distribution within certain error rate ~\cite{mchugh2012interrater}. 
Refer to Appendix \ref{sec:appendix_num_samples} for more details.


% \paragraph{Human Annotation}
For atypicality and originality, we follow \citet{modeling_determinants} and record responses about various statements (Table \ref{table:mturk_questions}). For creativity, we record an overall score from 1 to 5 and convert it to a 3-scale, aligning with other dimensions. We also include a quality check question by asking annotators to choose the action after seeing a given ad (e.g., ``I should go to Chick-fil-A'' for Ad A in Figure \ref{fig:intro}). Five actions are given, with one correct action and four randomly sampled from \texttt{Pitt-Ads}. Annotators get 96.88\% accuracy in this question, highlighting their accurate understanding of visual advertisements. More dataset construction details are in Appendix \ref{sec:data_collection}.





\subsection{\texttt{Atypical-300}}
\label{sec:atypicality_data}
We also randomly sampled 300 ads (\texttt{Atypical-300}) from \texttt{Pitt-Ads}, where 185(62\%) include atypical object(s). Different from \texttt{Creative-100}, each ad in this set only has three binary annotations on atypicality. Both ~\citet{modeling_determinants} and \texttt{Creative-100} (see Appendix \ref{sec:appendix_connection}) show that atypicality has a positive correlation with creativity  Thus, we include this dataset to gain further insight into VLM's ability to evaluate ad creativity.

\input{sections/table_results}