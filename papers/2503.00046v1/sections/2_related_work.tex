\section{Related Work}
\paragraph{Evaluation of Creativity}
Research in the evaluation of creativity 
includes cognitive science \cite{SaidMetwaly, QuantifyingcreativityDeanKeithSimonton, lloydcox2022}, marketing \cite{El-Murad188, HowAdvertisingCreativityWorks, doi:10.1080/13527266.2012.677464, modeling_determinants}, creative writing \cite{LiquidGoldDowntheDrainMeasuringPerceptionsofCreativityAssociatedwithFigurativeLanguageandPlay}, human computer interaction \cite{chakrabarty2024creativity}, and artificial intelligence \cite{chakrabarty-etal-2023-spy, Chakrabarty_art_artifice}. There are two common grounds. First, creativity is the balance between divergence and effectiveness; we ensure effectiveness through the quality check question in human annotation and mainly focus on modeling divergence through atypicality and originality. Second, the evaluation of creativity is subjective, making fine-grained human feedback critical. This motivates our distribution modeling task instead of a traditional, majority-label prediction task. Our work is mostly related to two works. \citet{modeling_determinants} focused on advertisement images and proposed five creativity dimensions, including atypicality and originality. We adapt their creativity decomposition. \citet{Chakrabarty_art_artifice} use LLMs to evaluate short stories; in contrast, we %conducted automatic evaluations with VLMs and 
analyzed the alignment between VLM outputs and human ratings.


\paragraph{Automatic Evaluation with Foundation Models}

GPTScore \cite{fu2023gptscore} and UniEval \cite{zhong-etal-2022-towards} decompose the evaluation of a complex task into simpler ones that can be accomplished by language models; whereas PandaLM \cite{wang2024pandalm} focuses on pairwise evaluation for free-form text quality. In the vision domain, ~\cite{Jayasumana_2024_CVPR, Otani_2023_CVPR} explore evaluating generated image content using CLIP embeddings. These prior works focus on single modality, instead of the image-text pair as we do. 
