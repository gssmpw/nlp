\section{Conclusion}
%\minisection{Conclusion} 
We present a case study of using SoTA VLMs to evaluate creativity in advertisements. Inspired by marketing research, we collect fine-grained human annotations. We show decent alignment between VLMs and humans in Distribution Modeling and Pairwise Preference, whereas Disagreement Prediction remains challenging. Our work opens the opportunity for automatic evaluation of advertisement creativity by providing a benchmark and metrics.

\section{Limitations}
One obvious limitation is the size of our dataset. The fine-grained creativity annotation only consists of 100 ad images. Two bottlenecks that lead to such a limited number is budget and annotation quality. Since we want to explore distribution modeling, we need more annotation than typical machine learning tasks, leading to a huge budget requirement. 

Another limitation is the natural biases contained in our annotation as a majority of our annotators are located in the U.S. We have plans to expand the annotation to other platforms (e.g., LabInTheWild) where a more diverse set of annotators is available. We would also suggest that researchers be cautious when applying our method to data from other countries or languages. Future work could also explore alternative prompting approaches to simulate group behavior or conduct a demographic analysis of human annotations, which could check whether VLM holds opinions comparable to those of particular groups.

Regarding the design of simulating ``group behavior'' by prompting a VLM 25 times with the same prompt, we recognize the simplicity of how we prompted the VLMs to make predictions. However, creating 25 different prompts for each ad could complicate the analysis and results. For example, certain prompts could disproportionately increase the likelihood of a ``creative'' label compared to others. One interesting approach to generating prompts more meaningfully could involve exploring persona prompts to simulate multiple annotators' behavior. However, we believe that starting with a straightforward approach using the sampling strategy is essential. Therefore, we leave the use of 25 distinct persona prompts for future work. 

Also, due to hardware constraints, we only experiment with VLMs in the 7B to 13B range when much larger models, such as LLaVA-34B, are available. We will leave more extensive prompt tuning and model selections to future work.

