\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{iccv2023AuthorKit/Figures/lol.pdf}
  \caption{
   Additional few-shot fine-tuning results on image highlighting.
  }
  \phantomsection
  \label{fig:lol}
\end{figure}



\section{Additional Results}
\label{appendix:additional_results}
\subsection{Additional Visualizations}
We present additional visualization results of our method across various tasks, as can be seen in Figures~\ref{fig:demo},~\ref{fig:fur},~\ref{fig:lol},~\ref{fig:medical},~\ref{fig:depth},~\ref{fig:normal},~\ref{fig:entity},~\ref{fig:sam},~\ref{fig:sam_1p},~\ref{fig:sam_5p},~\ref{fig:pose},~\ref{fig:seman}. For point-prompted segmentation, we further compared our approach with SAM. These results strongly demonstrate the potential of \ours. \ours\ is capable of achieving high-quality results, even in challenging scenarios. Furthermore, the few-shot fine-tuning of \ours, which requires minimal data and trainable parameters, strongly demonstrates the remarkable transferability of \ours to tackle new tasks. Our \ours\ is capable of further refining the segmentation of fine details, such as intricate hair structures.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{iccv2023AuthorKit/Figures/one_diffusion.pdf}
  \caption{
   Our segmentation not only separates semantically identical objects but also distinguishes different instances of the same category, achieving higher segmentation quality. Moreover, One Diffusion tends to generate an image similar to the input when performing image understanding tasks, as red-highlighted in the figure.
  }
  \label{fig:one_diffusion}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{iccv2023AuthorKit/Figures/medical.pdf}
  \caption{
   Additional few-shot fine-tuning results on lung segmentation and tumor segmentation.
  }
  \phantomsection
  \label{fig:medical}
\end{figure}


% CS: MOVED TO APPENDIX
\subsection{One-step Inference Does Not Work}
Genpercept~\cite{xu2024diffusion} demonstrates that one-step diffusion significantly enhances both the speed and accuracy of perceptual tasks. However, our experimental results reveal a notable increase in the proportion of failure cases when applying one-step diffusion in a multi-task setting, as illustrated in Figure~\ref{fig:1step}. We believe that this is due to the potential overlap of denoising trajectories for different tasks. These overlapping trajectories can interfere with each other, resulting in failure cases. In contrast, multi-step denoising helps to mitigate such interactions. In a single-task setting, since the denoising trajectories pertain to a single task, this approach is more effective and stable.








