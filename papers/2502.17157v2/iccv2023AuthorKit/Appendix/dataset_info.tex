\section{Dataset}
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{iccv2023AuthorKit/Figures/demo.pdf}
  \caption{
   Additional visualizations. Our one single model tackles multiple perception tasks.
  }
  \phantomsection
  \label{fig:demo}
\end{figure*}

\label{appendix:dataset}
We summarize the datasets used in our work in Table~\ref{tab:data}. The depth and normal data samples are obtained by randomly selecting 500K images from OpenImages~\cite{kuznetsova2020open} and labeling them using Depth Pro~\cite{bochkovskii2024depth} and StableNormal~\cite{ye2024stablenormal}, respectively. The 400K point segmentation data samples are obtained by randomly selecting images from the SA-1B dataset~\cite{kirillov2023segment}. For the synthesis of point segmentation data, we extract the foreground from P3M-10K~\cite{li2021privacy}, AIM500~\cite{li2021deep} and AM2K~\cite{li2022bridging}, randomly applying transformations such as rotation, resizing, and flipping. These transformed foregrounds are then pasted onto different background images, resulting in 200K synthetic images with fine-grained hair segmentation.

For the validation set, we evaluate depth using the same evaluation protocol as Genpercept~\cite{xu2024diffusion}, conducting tests on the NYUv2~\cite{nyu}, KITTI~\cite{kitti}, ScanNet~\cite{scannet}, DIODE~\cite{diode}, ETH3D~\cite{eth3d}. Similarly, for normal estimation, we followed the evaluation protocol of StableNormal~\cite{ye2024stablenormal} and performed evaluations on the NYUv2~\cite{nyu}, ScanNet~\cite{scannet}, DIODE~\cite{diode}. For point segmentation, we conducted extensive comparisons across 23 datasets. The remaining tasks, including Entity Segmentation, Semantic Segmentation, and Human Keypoints, were evaluated on the MS COCO 2017 dataset~\cite{lin2015microsoftcococommonobjects}. We believe the comprehensive experiments provide solid evidence of the remarkable performance of our method.
\input{iccv2023AuthorKit/Tables/data}