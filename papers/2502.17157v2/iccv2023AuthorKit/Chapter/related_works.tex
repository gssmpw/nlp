\section{Related Work}
% We review some relevant work here. 
\subsection{Vision Foundation Models}
Vision foundation models are models that are trained on large-scale datasets and demonstrate excellent performance within their trained domains.
Vision foundation models now exist for a broad range of vision tasks, including monocular depth estimation~\cite{yang2024depth, yang2024depth2, yang2024depthvideo, bochkovskii2024depth}, object detection~\cite{carion2020end}, segmentation~\cite{kirillov2023segment, ravi2024sam}, multimodal tasks~\cite{radford2021learning, liu2024visual}, image and video generation~\cite{rombach2022high, esser2024scaling, blattmann2023stable}, and more recently, emerging 3D models~\cite{wang2024dust3r, ma2024you}. While many works~\cite{wang2024task,khanna2024explora,li2024matching,rajivc2023segment,zhong2024convolution, zhu2024unleashing} have sought to leverage the prior knowledge embedded in these models to tackle other tasks, such efforts often require complex network designs and intricate training strategies, typically transferring only to a limited number of tasks.
Some foundation models~\cite{ren2024dino, he2022masked, oquab2023dinov2, caron2021emerging} emphasize representation learning, aiming to solve diverse downstream tasks by relying on generalized features. However, the results of these methods often fall short when compared with specialized foundation models. In contrast, our approach ensures consistent accuracy across multiple tasks while also enabling swift adaptation to new downstream tasks.


\subsection{Diffusion Models}
Diffusion models~\cite{esser2024scaling, rombach2022high, blattmann2023stable} have achieved remarkable success in image and video generation in recent years. The idea is to gradually add noise to the data and train a model to reverse this process, denoising step by step to generate the result. Recent diffusion models~\cite{esser2024scaling} utilize flow matching~\cite{lipman2022flow, albergo2022building, liu2022flow} and the DiT architecture~\cite{peebles2023scalable}, making them more scalable and efficient. Diffusion models have enabled a wide range of notable applications, including conditional image generation~\cite{zhang2023adding, li2024photomaker, ye2023ip, mou2024t2i, qin2023unicontrol}, image editing~\cite{brooks2023instructpix2pix, kawar2023imagic, yang2023paint}, story generation~\cite{wang2024autostory, zhou2024storydiffusion}, video generation~\cite{ho2022video, guo2023animatediff, zhao2024moviedreamer, yang2024cogvideox, blattmann2023stable, kong2024hunyuanvideo, wang2024framer}, and video editing~\cite{ceylan2023pix2video, liu2024video, chai2023stablevideo}. These successes underscore the substantial prior knowledge embedded in diffusion models.

Building on this insight, many studies~\cite{xu2024diffusion, he2024lotus, ye2024stablenormal, ke2024repurposing, zhu2024unleashing} leverage diffusion models for downstream image understanding tasks. However, these approaches typically require separate fine-tuning for each individual task. Recently, we find several concurrent works~\cite{wang2024lavin, le2024diffusiongenerate} also use diffusion models for multitask learning. Yet, these methods often involve complex network architectures and training procedures, and their evaluations tend to focus only on a very limited subset of image understanding results.
In contrast, our \ours\ offers a simpler solution. We not only conduct detailed evaluations of our method across a variety of tasks but also demonstrate that the simplicity, paired with the inherent strengths of diffusion models, can be sufficient to deliver strong results without relying on overly complicated setups.

\subsection{Multi-task Generalist Models}
Recently, there has been a surge of interest in exploring visual multitask learning. Some approaches~\cite{wang2023images, wang2023seggpt} draw inspiration from in-context learning in NLP, adapting it for the visual domain. Others~\cite{lu2022unified, lu2024unified, mizrahi20234m, bachmann20244m} have advocated for sequence modeling methods, utilizing a transformer encoder-decoder architecture. In these approaches, different encoders map various tasks into a shared representation space, and distinct decoders are employed to transform tokens into the outputs specific to each task. However, these methods face notable limitations: they need to train a separate encoder 
and decoder for every individual task and they usually rely on substantial amounts of data to attain optimal performance.

The recent success of high-quality Vision Language Models (VLMs)~\cite{liu2024visual} has also encouraged researchers to leverage them for building multitask models. Yet, these VLM-based methods~\cite{bai2023qwen, wang2024qwen2, chen2024internvl, lu2024deepseek, ren2024pixellm, li2025llamavid} typically focus on multimodal understanding tasks, such as image captioning, rather than general visual perception tasks. Meanwhile, some approaches~\cite{sun2024generative, zhao2024moviedreamer, pan2023kosmos} combine diffusion models with autoregressive models, focusing primarily on instruction-following image generation or editing tasks, rather than addressing image perception tasks. Although certain studies~\cite{lai2024lisa, jiang2024chatrex, cheng2024spatialrgpt, guo2024regiongpt} have tried to apply VLMs to more advanced semantic perception tasks, they struggle to establish a unified generalist visual model.



\subsection{Compared with One Diffusion}
\label{appendix:one_diffusion}
The concurrent work, One Diffusion~\cite{le2024diffusiongenerate}, addresses multi-task image generation, whereas our approach focuses on multi-task image understanding. We excel at performing a broader range of image understanding tasks with higher quality. While One Diffusion's strategy of treating different images as different views benefits generation tasks, their failure to distinguish between conditions and images introduces harmful degrees of freedom for perception tasks, as illustrated in the red-highlighted regions of Figure~\ref{fig:one_diffusion}. Specifically, when performing perception tasks, One Diffusion tends to generate an image similar to the original input, rather than the desired perceptual results.

Although One Diffusion suggests that more detailed text prompts can lead to better results, we argue that \textbf{performance in perception tasks should not overly depend on the quality of text prompts.} In contrast, our method uses only simple task prompts to distinguish between different tasks, rather than allowing the text prompts to dominate the results.

Crucially, while One Diffusion requires a massive amount of data (75 million samples) and computational resources for from-scratch training, we leverage the priors of pretrained models and demonstrate that, with significantly less data (1.8 million samples), we achieve performance on par with state-of-the-art results. In the image understanding tasks shared by both approaches, we consistently produce more stable and higher-quality results than One Diffusion.

