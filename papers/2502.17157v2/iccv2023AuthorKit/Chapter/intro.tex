\section{Introduction}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{iccv2023AuthorKit/Figures/teaser.pdf}
  \caption{
  \textbf{With one single model}, \oursbf\ 
  solves 
  multiple tasks 
  without relying on any task-specific modules (rows 1 to 3). The red dots in the figure indicate the input points used for point-prompted segmentation. \ours\ preserves fine details in segmentation, such as hair (row 4). \ours\ supports both human pose estimation and semantic segmentation (row 5, 6). \ours\ can quickly adapt to new tasks by fine-tuning less than 1\% of its parameters on as few as 50 images (row 7). For additional visualizations, please refer to Figures 
  \ref{fig:demo}, 
  \ref{fig:fur}, 
  \ref{fig:lol}, 
  \ref{fig:medical}, 
  \ref{fig:depth},
\ref{fig:normal},
\ref{fig:entity},
\ref{fig:sam},
\ref{fig:sam_1p},
\ref{fig:sam_5p},
\ref{fig:pose},
\ref{fig:seman} in the Appendix.
  }
  \label{fig:teaser}
\end{figure*}

Foundation models \cite{kirillov2023segment, ravi2024sam, yang2024depth, yang2024depth2, yang2024depthvideo, carion2020end, bochkovskii2024depth, radford2021learning, oquab2023dinov2, rombach2022high, blattmann2023stable, he2022masked}, typically requiring extensive training on billions of data samples, play a pivotal role in their respective domains. In natural language processing (NLP), current foundation models \cite{brown2020language, touvron2023llama, touvron2023llama2, dubey2024llama} have already demonstrated the potential to serve as versatile solutions, 
solving diverse fundamental 
tasks and with minimal fine-tuning needed for new tasks. This success can be attributed to the relatively small representational differences among various language tasks. However, in the domain of computer vision, task representations can differ substantially, and up to date, we %currently
still 
lack an effective approach to unify these distinct tasks.
Consequently, existing vision foundation models usually excel at one single specific task, such as image segmentation \cite{kirillov2023segment, ravi2024sam} or monocular depth estimation \cite{yang2024depth, yang2024depth2, yang2024depthvideo}, because they are trained on data tailored exclusively to that task.  
Owing to the pronounced disparity in visual representations across tasks, coupled with the single-task specialization that characterizes current vision foundation models, fine-tuning these models for new tasks remains a formidable challenge.
Although some efforts \cite{caron2021emerging, oquab2023dinov2, he2022masked, ren2024dino} have been made to learn universal visual representations for more generalized vision foundation models, their performance still falls noticeably short compared to specialized models.


Recent studies \cite{wang2023images, lu2022unified, lu2024unified, mizrahi20234m, bachmann20244m, xi2023dynamic} on visual generalist models are predominantly trained from scratch, often requiring substantial computational resources and large datasets to achieve good results. 
Unfortunately, 
the price 
of collecting a sufficiently large and high-quality multi-task dataset is substantial. 
Here, 
inspired by the success of diffusion models, we propose the hypothesis that leveraging their powerful priors 
can 
help mitigate the significant computational and data overhead for training \textbf{powerful
generalist models}. While some existing works \cite{ke2024repurposing, xu2024diffusion, he2024lotus, ye2024stablenormal, shao2024learning} have demonstrated 
that
this is feasible in single-task scenarios, the potential of diffusion model priors in multi-task settings remains largely under-explored.



In this paper, we successfully leverage the priors of diffusion models to achieve results on par or even better  with the state-of-the-art models on various tasks with only  minimal training data.
We name our powerful visual generalist model \oursbf.
For each task, we require substantially less data than specialized foundation models. 
For instance, compared to SAM segmentation, \textit{
\ours\  achieves comparable performance using a significantly smaller dataset of 600K samples}, without any selective sampling. This is in sharp contrast to SAM which relies on a massive dataset of 1 billion pixel-level annotated samples. This demonstrates the efficiency and robustness of \ours, as it requires only 0.06\% of the data used by SAM while still producing competitive results. 

The concurrent work One Diffusion~\cite{le2024diffusiongenerate} proposes a unified model for image generation by treating different images as different views. However, their approach fails to distinguish between conditions and images, greatly limiting its effectiveness in perception tasks. More critically, their from-scratch training demands substantial data and computational resources.
In contrast, our work focuses on image perception rather than generation, achieving on-par state-of-the-art performance in perception tasks.
See more details in Appendix~\ref{appendix:one_diffusion}.
\ours\  highlights that \textit{the generative 
image priors lead to surprisingly more efficient and effective 
pathways to 
generalist image understanding models.}
The entire training process is remarkably stable, eliminating the need to design a complex training recipe. Even more notably, \ours\ is capable of quickly adapting to new tasks using as few as 50 training images and fine-tuning less than 1\% of its parameters.
Moreover, we show that previously ineffective strategies in generalist models such as task control via prompts~\cite{xi2023dynamic} and random color assignments for segmentation masks~\cite{wang2023images} are viable in our framework. Additionally, we investigate the role of 1-step denoising in multi-task settings, a technique proven beneficial in single-task settings. \ours\ provides valuable insights for the 
design and training of strong
visual generalist models.

In summary, our main contributions are as follows.

\begin{itemize}
\itemsep 0cm
    \item We introduce \ours, a generalist model capable of performing multiple visual perception tasks. Extensive 
    experiments 
    demonstrate that \ours\ can effectively address various tasks,
    achieving or par accuracy with state-of-the-art specialist models.
    
    \item When trained on new tasks, \ours\ achieves high-quality results with minimal fine-tuning. Using only a small number of images (even with 50 images), the model can achieve exceptional performance by fine-tuning less than 1\% of its parameters.
    \item We conduct a thorough set of experiments exploring various aspects of multitask models, including network architectures, and 1-step diffusion. We believe that our experimental findings offer valuable insights and will significantly benefit future research.
\end{itemize}
