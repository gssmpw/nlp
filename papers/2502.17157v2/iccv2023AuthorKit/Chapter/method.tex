\section{Method}

\subsection{Preliminary}

Recent diffusion models predominantly build upon flow matching methodologies~\cite{lipman2022flow, albergo2022building, liu2022flow} and the DiT architecture~\cite{peebles2023scalable}. They aim to learn a velocity field that effectively maps samples from a source distribution to a target distribution. The training process involves minimizing the discrepancy between the model’s predicted velocity and the ground-truth velocity, which is typically formulated as:
\begin{equation}
    {\rm Loss} = {\mathbb E}_{\mathbf z_0,t, c} \Vert v_\theta(\mathbf z_t,t,c) - u(\mathbf z_t) \Vert^2_2,
\end{equation}
where $c$ is the condition, usually the text prompt. $\mathbf z_0$ is the clean image latent and $\mathbf z_t$ is the noisy one. The learned velocity field corresponds to an ordinary differential equation (ODE), such that during inference, a sample drawn from the source distribution can be transformed into the desired output by solving this ODE.

\subsection{Unifying Task Representation into RGB Space}
The decision to unify representations of diverse tasks in RGB space was motivated by two key factors: (1) It Maximally leverages the priors in text-to-image models, which have been extensively trained within the RGB domain. (2) RGB serves as a foundational representation in computer vision, providing a common visual framework through which a wide variety of tasks can be coherently and intuitively visualized.

We focus on several of the most fundamental tasks in computer vision: monocular depth estimation, normal estimation, and segmentation. Segmentation, in particular, encompasses point-prompted segmentation, entity segmentation, and semantic segmentation. All these tasks can be unified within an RGB space, with the difference being the number of channels. For single-channel representations, such as depth maps and segmentation masks, we align them with RGB by repeating the channel data three times. For inherently three-channel representations, such as normal maps, we treat them directly as RGB images.

Entity segmentation is to segment every instance in an image but with no category. We assign each mask within an image a random color and merge them into a three-channel RGB mask. 
Painter found that assigning color randomly makes the model hard to optimize. However, we find this approach has no adverse impact on the training and enables the model to effectively learn to distinguish different instances by painting them with different colors. Each instance’s mask can be extracted from the RGB mask using clustering algorithms during post-processing without significant performance degradation.
We also apply the random color assignment in semantic segmentation. Unlike traditional semantic segmentation, our method is capable of segmenting instances of the same semantic category.
By default, We use KMeans for mask extraction.

Let $\mathbf x_r$ denote the pre-unified raw representation for each task, and 
$\mathbf x$ represents the unified RGB-like output representation. We formalize this process as: $\mathbf x = \Psi(\mathbf x_r)$.





\subsection{\oursbf: A Unified Framework}
\paragraph{Architecture.}
Our model adopts the same architecture as SD3. We aim to keep the architecture as unchanged as possible, fully leveraging the pretrained prior knowledge. 
We use simple task prompts to direct the model to perform various tasks, such as “image2depth,” “image2normal,” and “image2segmentation.”

For point-prompted segmentation, a naive approach is directly painting points on the image. But this strategy is highly sensitive to the size of the points. If the painted points are too large, they can obscure small regions, causing segmentation to fail. Conversely, if the painted points are too small, the model may lose relevant point information after VAE downsampling and patchification. To address this, we introduce a minimal straightforward two-layer MLP 
$\Phi(\cdot)$
that enables the model to understand the point prompt.

Inspired by SAM, we apply sin-cos positional encoding to the point coordinates $p$, then pass them into the MLP $\Phi(\cdot)$  to produce point embeddings that match the dimension of the input hidden states. We use two learnable embeddings to indicate whether the embedding is a point embedding or not: $\xi_p$ for point embeddings and $\xi_{np}$ for non-point embeddings. The processed point embedding is summed with $\xi_p$. For other tasks, we simply use 
$\xi_{np}$ as the point embedding. During training, we randomly select 1–5 points to guide the segmentation. When the number of selected points is fewer than 5, we pad the point embeddings to a length of 5 with $\xi_{np}$. 
When performing tasks that do not require point input, the point embedding is simply a length-5 sequence, where each element is $\xi_{np}$.
By denoting the final point embedding as $\xi$,  this process is formulated as:
\begin{equation}
    \begin{aligned}
        \xi = 
            \begin{cases} 
            \operatorname{Concat}(\Phi(\operatorname{PE}(p))+\xi_{p}, \xi_{np}) & \text{if point segmentation} \\
            \xi_{np} & \text{else}
            \end{cases}
    \end{aligned}
\end{equation}
\paragraph{Input Formulation and Loss.}
\ours\ introduces two additional inputs based on SD3: the input image $\mathbf x'$ and point embedding $\xi$. For the input image, we first apply VAE to down-sample it by a factor of 8, after which it is $2\times2$ patchified into sequences. We denote this pre-processing as $\tau$. Subsequently, the task prompt token $\mathbf{e}$, point embedding $\xi$, noisy token $\mathbf{z}_t$, and input image token $\mathbf z'$ are concatenated to form the complete input.
We follow the flow matching loss in training SD3. During training, the loss is applied solely to the noisy tokens:
\begin{equation}
    \begin{aligned}
        \mathbf z_0 &= \tau(\mathbf x) \\
        \mathbf z' &= \tau(\mathbf x') \\
        {\rm Loss } = 
        {\mathbb E}_{\mathbf z_0,t} \Vert v_\theta&(\mathbf z_t, \mathbf z',t,\mathbf e, \xi) - u(\mathbf z_t) \Vert^2_2.
    \end{aligned}
\end{equation}

\subsection{Adapting to New Tasks}
In practical scenarios, it is often necessary to enable models to quickly adapt to new tasks using only a small amount of training data. Traditional specialized foundation models, however, are typically limited to tasks closely related to their domain and generally require substantial datasets and carefully designed network architectures for adaptation. 
Diffusion models, while powerful in many respects, cannot easily adapt to downstream tasks through fine-tuning only a few parameters with limited data.

\ours\ effectively addresses this limitation. We conducted experiments on lung segmentation, tumor segmentation, and image highlighting, which represent tasks with varying degrees of overlap with the model’s original domain.
We train fewer than 1\% of the model’s parameters using LoRA~\cite{hu2021lora} without any complex architectural modifications. Notably, despite the limited availability of training samples (50 per task), \ours\ consistently delivered successful and high-quality performance across all target tasks. These results provide compelling evidence for the potential of \ours\ as a truly unified foundation model.