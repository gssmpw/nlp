

\def\x{\ensuremath{\times}}


\section{Experiments}
\subsection{Implementation Details}

\paragraph{Data.}
We \textit{randomly} select 500k images from the OpenImages~\cite{kuznetsova2020open} dataset and use DepthPro~\cite{bochkovskii2024depth} and StableNormal~\cite{ye2024stablenormal} to generate depth and normal data. For point-prompted segmentation, we \textit{randomly} select 400k images from the SA-1B~\cite{kirillov2023segment} dataset, as well as 200k images with fine-grained hair masks synthesized from the AM2k~\cite{li2022bridging}, AIM500~\cite{li2021deep}, and P3M-10k~\cite{li2021privacy}. Entity segmentation data is from EntityV2~\cite{qi2022high}, while semantic segmentation data comes from the COCO-Rem~\cite{singh2024benchmarkingobjectdetectorscoco}, and human pose data is sourced from COCO~\cite{lin2015microsoftcococommonobjects}. For few-shot fine-tuning, we select 50 samples from the publicly labeled Chest X-Ray dataset~\cite{wang2017hospital}, LOL-v2~\cite{yang2020fidelity}, and Kaggle's Brain Tumor dataset as training samples. More details can be found in Appendix ~\ref{appendix:dataset}.

\paragraph{Training.}
%
Our training lasts for 24 days
using 
4 NVIDIA 
H800 
GPUs. We employ the AdamW optimizer with a constant learning rate of 2$e$$-5$ and a batch size of 28 per GPU. We found that the training process is highly stable. However, the convergence speed for segmentation tasks was slower compared to depth and normal tasks. Therefore, we increased the proportion of segmentation data in each batch.  Specifically, in each batch, depth and normal each account for 15\%, point-prompted segmentation, entity segmentation, and semantic segmentation each account for 20\%, and pose estimation accounts for 20\%. We observe that, by the end of training, despite the loss no longer significantly decreasing, the model's performance on segmentation tasks continues to improve.


During few-shot fine-tuning, we apply a
rank-128 
LoRA to all attention Q, K, and V layers in the network, which accounts for less than 1\% of the total network parameters. The task prompts for different tasks are ``im\-a\-ge-to-seg\-men\-ta\-ti\-on lung," ``im\-a\-ge-to-seg\-men\-ta\-ti\-on tumor," and ``ima\-ge-to-high\-li\-gh\-t." 
LoRA training is conducted on a single NVIDIA H100 GPU, with a constant learning rate of 2$e$$-5$ and a batch size of 8. Please refer to Appendix~\ref{appendix:additional_results} for more few-shot fine-tuning visualizations.

\vspace{-3mm}
\paragraph{Inference.}
We follow the settings of the pretrained model and perform 28 steps of denoising during inference. The inference can be run on a  GPU of 24GB memory with a batch size of 4.

\subsection{Comparisons with Existing Methods}
\input{iccv2023AuthorKit/Tables/depth}
\input{iccv2023AuthorKit/Tables/normal}
\input{iccv2023AuthorKit/Tables/entity}
\input{iccv2023AuthorKit/Tables/pose}
\input{iccv2023AuthorKit/Tables/semantic}
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{iccv2023AuthorKit/Figures/metric_sam.pdf}
  \caption{
   Comparisons of mIoU with SAM-vit-h. \textbf{We achieve results on par with SAM using only 0.06\% of their data (600K vs.\  1B).}
  }
  \label{fig:metric_sam}
\end{figure*}
We compare the performance of specialized models, existing multi-task models, and our \ours\ across various tasks. Specifically, we evaluate depth using the same protocol as Genpercept, normal estimation using the same method as StableNormal, point-prompted segmentation using the same approach as SAM, and human keypoints using the same method as Painter. We also assess semantic and entity segmentation on the MS COCO dataset. For entity segmentation, we assigned all predicted categories to the same label. 



As in Tables \ref{tab:depth} and \ref{tab:normal}, our \ours\ significantly outperforms existing multi-task models and achieves performance on par with state-of-the-art specialized models or demonstrates only an acceptable performance decrease. For point-prompted segmentation, as shown in Figure~\ref{fig:metric_sam}, \textbf{we achieve results on par with SAM-vit-h using only 0.06\% of their data.} SAM shows a clear advantage only on certain out-of-distribution datasets that are outside the scope of our model's training, such as WoodScape fisheye dataset. \textit{Notably, while most specialized models require extensive data or complex data pipelines, 
our method achieves excellent results with significantly less data, the majority of which is obtained through simple random selection.} 
For entity segmentation, we observe that our model performs poorly on small objects. We believe 
that 
this is due to the limited amount of data, which causes the model to overfit larger objects. This problem can be solved by introducing more data towards small objects.

We observe that, although our model generates high-quality visualizations for human pose and semantic segmentation, the corresponding evaluation metrics remain relatively low. 
For human keypoints, this is primarily due to two factors: Firstly, we utilize skeletal-form RGB images rather than heatmaps. While the former produces visually appealing results, the extraction of keypoints during post-processing introduces considerable errors. Secondly, our evaluation follows the 192\x256 top-down human keypoints protocol. The original 192\x256 images are resized to 768\x768 before being input into the model, resulting in extremely blurred inputs that likely contribute to the diminished performance.

Semantic segmentation also introduces considerable errors during post-processing. We offer a comprehensive explanation of the metric degradation in the Appendix~\ref{appendix:mask_degradation}.
For individual images, we can adjust hyperparameters during the post-processing of RGB mask to achieve the optimal masks. However, different results require different optimal hyperparameters, and manually adjusting each sample in the validation set during evaluation is impractical due to the excessive labor involved. This leads to remarkable semantic segmentation visualizations produced by our method but relatively low metrics.
We achieve excellent results by training solely on COCO-Rem. Furthermore, our semantic segmentation can distinguish different instances of the same semantic category. We believe we have strongly demonstrated the potential of our method for instance-level semantic segmentation with IDs.
Please check Appendix~\ref{appendix:post_processing} for more details about post-processing.


\subsection{Comparisons with Our Single-task Models}

For the training of single-task models, we ensure that the network architecture remains the same and the total amount of training data seen for each specific task is the same as that for the multi-task model. For example, if the multi-task model trains for 100 iterations with 4 depth data samples per batch, the single-task model will also be 
trained  for 100 iterations with 4 data samples per batch. In our current data setting (approximately 1.8 million samples), we have not observed a significant gap between the multi-task and single-task models, nor have we seen a trend of mutual promotion between different tasks, as shown by ``Ours-single" in Tables~\ref{tab:depth}, \ref{tab:normal}, \ref{tab:entity} and Figure \ref{fig:metric_sam}. 


We believe that it is more appropriate to explore with larger datasets in order to draw more solid conclusions.
We leave this as future work.

\subsection{Ablations}

\paragraph{Multi-point Prompted Segmentation.}
Ambiguity is a significant issue in point-prompted segmentation. For example, if a point is placed on a personâ€™s clothing, the model may segment the clothing, but the desired result is the person. Therefore, more points are needed to resolve this ambiguity. As illustrated in Figure~\ref{fig:ab_point}, additional points help the model better segment the desired results.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{iccv2023AuthorKit/Figures/ab_point.pdf}
  \caption{
   Comparisons between 1-point and 5-point segmentation of mIoU on all 23 validation datasets.
  }
  \label{fig:ab_point}
\end{figure}

\paragraph{Architecture of Diffusion Model.}
Before the advent of DiT~\cite{peebles2023scalable}, the UNet architecture was predominantly used in diffusion models. We also conduct multi-task experiments based on a UNet pretrained model SDXL~\cite{podell2023sdxl}. Specifically, we follow Marigold's~\cite{ke2024repurposing} approach by expanding the first convolution layer's input channels from 4 to 8 to accommodate image inputs, and similarly use task prompts to guide the model in solving different tasks. However, as shown in Figure~\ref{fig:unet}, we find that this approach is not sufficiently effective, even for a minimal multi-task scenario involving only depth and normal estimation. 
We attribute this limitation to the superior information aggregation capabilities of the Transformer architecture compared to UNet. While the UNet suffers from significant information loss during downsampling, the Transformer maintains more comprehensive representations, enabling it to perform better in multi-task scenarios.
