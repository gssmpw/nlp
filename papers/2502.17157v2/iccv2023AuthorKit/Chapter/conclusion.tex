\section{Conclusion}
We have introduced \ours, a multi-task visual generalist model based on the diffusion model. Our approach unifies different tasks in the RGB space, leveraging the prior knowledge of pre-trained image generation model to achieve results that are on par with or acceptably lower than the results of specialized foundation models.
We achieve good performance without carefully cherry-picking extremely high-quality data or by using an exceptionally large amount of data.

Furthermore, for segmentation tasks, we demonstrate that the strategy of assigning random colors to different instances is highly effective in our framework, enabling high-quality entity segmentation and semantic segmentation. 
In few-shot fine-tuning, we are able to achieve high-quality results with minimal data and minimal trainable parameters. 
We believe that \ours\ sheds light on how to effectively use generative model priors to build a strong visual generalist, enabling more efficient solutions to perception tasks.