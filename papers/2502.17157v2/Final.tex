\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{adjustbox}

\usepackage{caption}
\captionsetup{labelfont=bf,tableposition=top,font=small}

\usepackage{float}

\usepackage{xcolor}
\newcommand{\red}[1]{\textcolor{red}{#1}}

\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}

\usepackage{bold-extra}
\usepackage{algorithm}
\usepackage{algorithmic}



\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\newcommand{\ours}{{\sc Di\-Ce\-p\-ti\-on}}
\newcommand{\oursbf}{{\bfseries\scshape Di\-Ce\-p\-ti\-on}}



\title{\oursbf: A Generalist Diffusion Model for 
Visual
Perceptual Tasks
}


\author{%
  Canyu Zhao$^{\rm 1,*}$ ~~ Mingyu Liu$^{\rm 1,2,}$\thanks{Equal Contribution} ~~ Huanyi Zheng$^{\rm 1}$ ~~ Muzhi Zhu$^{\rm 1}$ ~~  
  \\ Zhiyue Zhao$^{\rm 1}$ ~~
  Hao Chen$^{\rm 1}$ ~~ Tong He$^{\rm 2}$ ~~ Chunhua Shen$^{\rm 1}$\\[0.25cm]
  \textsuperscript{\rm 1} Zhejiang University~~~~~
  \textsuperscript{\rm 2} Shanghai AI Laboratory\\
}


%CS: 
\input fig1.tex



\maketitle
% Remove page # from the first page of camera-ready.

% \ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}

Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images and introduce our visual generalist model: \oursbf. 
Our exhaustive evaluation metrics demonstrate that \ours\ effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models.
\textbf{We achieve results on par with SAM-vit-h using only 0.06\% of their data (\textit{e.g.}, 600K vs.\ 1B 
   pixel-level annotated 
   images)}.
Inspired by
Wang \textit{et al.}\ 
\cite{wang2023images}, 
   \ours\ formulates the outputs of various perception tasks using color encoding;
   and we show
   that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. 
   Unifying various perception tasks as 
   conditional 
   image generation
   enables us to fully leverage  
   pre-trained text-to-image models. Thus,
   \ours\ can be efficiently
   trained at a  cost of orders of magnitude lower,
   compared to conventional models that were trained from scratch.
   When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and $\sim$1\% of its parameters.
   \ours\ provides valuable insights and a more promising solution for visual generalist models. 
   \href{https://aim-uofa.github.io/Diception/}{Project webpage}, and 
   \href{https://huggingface.co/spaces/Canyu/Diception-Demo}{huggingface demo} are available. 
   
\end{abstract}

%%%%%%%%% BODY TEXT
\input{iccv2023AuthorKit/Chapter/intro}
\input{iccv2023AuthorKit/Chapter/related_works}
\input{iccv2023AuthorKit/Chapter/method}
\input{iccv2023AuthorKit/Chapter/experiments}
\input{iccv2023AuthorKit/Chapter/conclusion}

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{Main}
}




\clearpage
\appendix


\renewcommand\thesection{\Alph{section}}
\renewcommand\thefigure{S\arabic{figure}}
\renewcommand\thetable{S\arabic{table}}
\renewcommand\theequation{S\arabic{equation}}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}



\section*{Appendix}




\input{iccv2023AuthorKit/Appendix/dataset_info}
\input{iccv2023AuthorKit/Appendix/post_processing}
\input{iccv2023AuthorKit/Appendix/additional_results}
\input{iccv2023AuthorKit/Appendix/limitation}

\end{document}
