\section{Related Work}
\subsection{Logical Inference in Language Models}
\paragraph{Abductive Inference}
In the era of pre-trained language models, $\alpha$\textbf{-NLI} \citep{bhagavatula2020abductivecommonsensereasoning} introduced abductive reasoning to commonsense reasoning, where plausible explanations are inferred from observations. Subsequent works proposed various techniques to enhance this capability \citep{qin2021futureunsupervisedbackpropbaseddecoding, kadiÄ·is2022embarrassinglysimpleperformanceprediction, chan2023selfconsistentnarrativepromptsabductive}, including extensions to uncommon scenarios focusing on rare but logical explanations \citep{zhao2024uncommonsensereasoningabductivereasoning}. Unlike real-world data in commonsense reasoning, benchmarks like \textbf{ProofWriter} \citep{tafjord2021proofwritergeneratingimplicationsproofs} evaluate formal abductive reasoning within semi-structured texts with explicit logical relationships. Recent studies have explored LLMs in more challenging open-world reasoning contexts \citep{zhong2023chatablabductivelearningnatural, del2023truedetectivedeepabductive, thagard2024chatgptmakeexplanatoryinferences}. Beyond natural language inference, abductive reasoning has also been examined in graph-based modalities for commonsense and event knowledge \citep{du-etal-2021-learning, bai2024advancingabductivereasoningknowledge}.

\paragraph{Deductive and Inductive Inference}
Deductive inference is studied using benchmarks like \textbf{RuleTaker} \citep{clark2020transformerssoftreasonerslanguage}, where LMs perform rule-based reasoning on natural language. Further works evaluate deductive reasoning under generalization, emphasizing challenges with longer proofs \cite{saparov2023testinggeneraldeductivereasoning} and complex logic \cite{zheng2025enhancingtransformersgeneralizablefirstorder}. Inductive inference is explored through datasets like \textbf{EntailmentBank} \citep{dalvi2022explaininganswersentailmenttrees}, where models construct entailment trees to explain answers. While LLMs demonstrate emergent inductive abilities via few-shot learning \citep{wei2022emergentabilitieslargelanguage}, \citet{min2022rethinkingroledemonstrationsmakes} argue that structural cues often outweigh label correctness in induction.

\subsection{Analogical Reasoning} 
The study of analogical reasoning in AI has progressed from early symbolic systems, such as the \textbf{Structure-Mapping Engine} \citep{FALKENHAINER19891}, which used hand-crafted representations, to models like the \textbf{Latent Relation Mapping Engine} \citep{Turney_2008}, which integrated symbolic rules with statistical learning. The neural era introduced word embeddings for analogy evaluation \citep{mikolov2013distributedrepresentationswordsphrases}, emphasizing local semantic patterns. With LLMs, \citet{webb2023emergentanalogicalreasoninglarge} demonstrated emergent analogical reasoning, but challenges remain. \textbf{AnaloBench} \citep{ye2024analobenchbenchmarkingidentificationabstract} shows minimal scaling gains for long-context analogies, while \textbf{ANALOGICAL} \citep{wijesiriwardene2023analogicalnovelbenchmark} highlights struggles with complex metaphors. Story-level benchmarks like \textbf{StoryAnalogy} \citep{jiayang2023storyanalogyderivingstorylevelanalogies} and \textbf{ARN} \citep{sourati2024arnanalogicalreasoningnarratives} reveal difficulties in cross-domain narrative mapping without explicit prompts.