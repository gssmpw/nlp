% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Load xcolor explicitly with all required options
\usepackage[table,xcdraw]{xcolor}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable}
\newtcolorbox[auto counter, number within=section]{promptbox}[2][]{%
    colframe=blue!75!black,
    colback=blue!10,
    coltitle=white,
    fonttitle=\small\bfseries,
    title=Prompt Template~\thetcbcounter: #2,
    breakable, % Allows the box to break across pages
    enhanced,
    fontupper=\ttfamily,
    #1 % Pass additional options from the user
}

\usepackage{amsmath}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}


\title{\textsc{LogiDynamics}: Unraveling the Dynamics of Logical Inference \\ in Large Language Model Reasoning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Tianshi Zheng$^{\spadesuit}$,  Jiayang Cheng$^{\spadesuit}$, Chunyang Li$^{\spadesuit}$, Haochen Shi$^{\spadesuit}$, Zihao Wang$^{\spadesuit}$, Jiaxin Bai$^{\spadesuit}$\\ \textbf{Yangqiu Song}$^{\spadesuit}$, \textbf{Ginny Y. Wong}$^{\clubsuit}$, \textbf{Simon See}$^{\clubsuit}$ \\
  $^{\spadesuit}$Department of Computer Science and Engineering, HKUST, Hong Kong SAR, China\\
  $^{\clubsuit}$NVIDIA AI Technology Center (NVAITC), NVIDIA, Santa Clara, USA\\
  \texttt{tzhengad@connect.ust.hk}\\
}
%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMs' reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning—a fundamental cognitive task—that systematically parameterized across three dimensions: \textit{modality} (textual, visual, symbolic), \textit{difficulty} (easy, medium, hard), and \textit{task format} (multiple-choice or free-text generation). We analyze the \textbf{comparative dynamics} of \textit{inductive}, \textit{abductive}, and \textit{deductive} inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as \textit{hypothesis selection}, \textit{verification}, and \textit{refinement}, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies. Resources are available at \href{https://github.com/HKUST-KnowComp/LogiDynamics}{\texttt{github.com/HKUST-KnowComp/LogiDynamics}}.
\end{abstract}

\begin{quote}
\centering
\emph{"It is not enough to have a good mind;\\ the main thing is to use it well."} \\[1ex]
{\raggedleft\small\textemdash\ René Descartes\par}

\end{quote}

\begin{figure}[t]
\begin{center}
\includegraphics[clip,width=220pt]{images/logidynamics_raven3.pdf}
\end{center}

\caption{(a) An illustration of System 1 and System 2 logical inference pipelines in RAVEN's progressive matrix. (b) General \textbf{comparative dynamics} between System 1 and System 2 pipelines in all experiments.}

\label{fig:raven}
\end{figure}

\section{Introduction}
Logical Inference\footnote{The term `inference' encompasses multiple interpretations across different disciplines. This paper employs the term strictly within its logical trichotomy: deductive, inductive, and abductive inference, as defined in \cite{flach2000abduction}.} is the reasoning process of deriving conclusions from known premises \cite{copi1990introduction,deductivereasoningjohnson}. It primarily categorizes into \textit{deductive inference} --- where conclusions follow with logical necessity from premises, and \textit{inductive inference} --- where conclusions serves as general rules derived from specific instances \cite{salmon_1984}. While the introduction of \textit{abductive inference} \cite{peirce_1958, frankfurt1958peirce} serves as a third perspective, denoting the process of forming an explanatory hypothesis from an observation requiring explanation. Logical inference plays a crucial role in artificial intelligence, scientific research, and philosophy, where rational decision-making and hypothesis formation are foundational \cite{Hempel_Oppenheim_1948,684604fc-4b8d-3190-bf8c-948758999eb7,REITER198757}.

Different logical inference pipelines can be applied in solving the same reasoning task. Figure \ref{fig:raven}(a) illustrates an example of Raven's Progressive Matrices \cite{raven1938,zhang2019raven}, where the missing element in the 3×3 matrix is inferred through the common patterns among different rows. There are two approaches to solving this problem: 1) directly inferring the missing element from the observed elements in the matrix, and 2) explicitly identifying the common patterns across rows, then deductively applying these patterns to determine the missing element in the last row. The former is driven by inductive inference and features fast, intuitive, pattern-recognition guided reasoning. The latter consists of abductive and deductive inference, featuring slower but more deliberate analysis. These approaches correspond to System 1 and System 2 thinking, respectively \cite{kahneman2011thinking}.

Research on large language models (LLMs) has explored the logical inference pipelines employed by LLMs for solving a wide range of tasks. \citet{qiu2024phenomenal} and \citet{wang2024hypothesis} have demonstrated the effectiveness of the System 2 approach in various inductive reasoning datasets such as ARC \cite{chollet2019measure} and its variants \cite{kim2022playgrounds,xu2023llms}. \citet{he2024ideaenhancingrulelearning} highlighted the potential of System 2 logical inference in the reasoning workflow of LLM-based agents. While \citet{liu2024incompleteloopinstructioninference} compared both System 1 and System 2 approaches in several in-context learning tasks, pointing out the inconsistency of their relative performances across datasets. Nevertheless, all prior studies leave an open question: \textit{When} and \textit{how} can System 1 and System 2 logical inference pipelines be effectively leveraged to enhance LLM reasoning?

To unravel this intricate question, we systematically study the comparative dynamics of logical inference pipelines in LLM reasoning. First, we build a fully controllable evaluation environment using analogical reasoning tasks. The environment is controlled in three dimensions: 1) \textit{Modality}: The data covers textual (word/phrase), visual (images), and symbolic modalities. 2) \textit{Difficulty}: All tasks are labeled with relative difficulty levels (easy, medium, and hard). 3) \textit{Task Format}: For each question, we provide two task formats: multiple-choice questions (MCQ) or free-text generation (FTG) format.

With experiments in 10 modern LLMs (and MLLMs), \textbf{we discover several key findings}:
\begin{itemize}
    \item \textbf{Modality-dependent}: System 2 logical inference shows superior performance in \textit{visual} and \textit{symbolic} tasks, while System 1 performs comparably in \textit{textual} tasks.
    \item \textbf{Difficulty-dependent}: System 2 logical inference is more advantageous in \textit{harder} tasks, while System 1 achieve comparable performance in \textit{easier} tasks.
    \item \textbf{Task Format-dependent}: For tasks involving explicit rule execution, System 1 logical inference outperforms System 2 in \textit{FTG} format, but underperforms in \textit{MCQ} format.
\end{itemize}

To verify the generalizability of our findings, we conduct further experiments in the List Function dataset \cite{rule2020child} and SALT dataset (ours), where we observe similar comparative dynamics in difficulty and task format. We argue that \textbf{our findings can be generalized to broader in-context learning (ICL) tasks} where: 1) the few-shot demonstrations are presented in Input-Output format, and 2) the mapping function between input and output can be explicitly defined.

Furthermore, we explored the effects of more sophisticated System 2 logical inference pipelines, including hypothesis selection, hypothesis verification, and refinement. Using these paradigms, LLMs demonstrate significant performance improvements as the number of inference tokens increases. We show that, with sufficient computational resources, \textbf{LLMs under logical inference scaling achieve performance comparable to state-of-the-art Long-CoT reasoning models}. This highlights the potential of scaling inference through advanced System 2 logical inference pipelines.

This work makes several key contributions to understanding and improving LLM reasoning capabilities from a logical inference perspective:
\begin{enumerate}
\item We provide a \textbf{systematic evaluation environment} to compare logical inference paradigms across controlled dimensions.\,(\S\ref{sec:eval}) 
\item We present \textbf{rich findings as clear guidelines} for leveraging different inference approaches based on task characteristics.\,(\S\ref{sec:main}) 
\item We validate our findings' \textbf{generalizability to broader in-context learning tasks}.\,(\S\ref{sec:generalization}) 
\item We highlight the potential to \textbf{scale up LLM reasoning} using advanced System 2 logical inference paradigms.\,(\S\ref{sec:scaling}) 
\end{enumerate}
Collectively, these contributions establish a foundation for future research on enhancing LLM reasoning through optimized logical inference strategies.

\section{Preliminaries}

\subsection{Analogical Reasoning}

%What is analogical reasoning. Problem Definition. Why it is suitable for our case? (OOD, Hypothesis available)

Analogical reasoning is a fundamental aspect of cognitive intelligence \cite{gentner2001analogical}. It involves inferring a missing element in a target domain according to relational structures from a source domain. 
Formally, given a source pair \((A, A')\) and an incomplete target pair \((B, x)\), where \(A\) and \(A'\) have an implicit relational pattern \(P\), the goal is to infer \(x\) that have the same relational pattern \(P\) with \(B\). 
This task can be defined as:
\[
B' = \arg\max_{x \in \mathcal{X}} \text{sim}_P((A, A'), (B, x)),
\]
where \(\text{sim}_P\) measures the consistency of the relational pattern \(P\) between the source pair \((A, A')\) and the candidate target pair \((B, x)\), and \(\mathcal{X}\) represents the set of all possible candidates for \(B'\). The complete analogy is denoted as \(A : A' :: B : B'\).  For instance, given the source pair \((\textit{sun}, \textit{planet})\) and the incomplete target pair \((\textit{nucleus}, x)\), we can infer \(x = \textit{electron}\) by identifying the pattern \(P\) as \textit{orbital relationship}.


% The relational pattern \(P\) can encode structural, semantic, or relational transformations and is assumed to remain invariant across the source and target domains. 


The task of analogical reasoning is particularly well-suited for our investigation for several reasons: 1) it offers a well-defined task structure while encompassing diverse data modalities, 2) it is compatible with a variety of logical inference pipelines, and 3) it is considered out-of-distribution for the training data of LLMs, enabling a robust evaluation of their reasoning capabilities under generalization \cite{claire2024analogy}.


\subsection{Logical Inference Pipelines}
In the main experiment, we compare three logical inference pipelines: direct induction, abduction + deduction, and automate inference. More sophisticated pipelines involving hypothesis selection, verification, and refinement are discussed in the scaling experiments in Section \ref{sec:scaling}. Detailed prompt templates are provided in Appendix \ref{app:prompt}.
\paragraph{Direct Answering as Inductive Inference}
Inductive inference is often associated with fast, intuitive reasoning in cognition \cite{2af2fe0f-5f65-3dca-bfa3-cd96adbc1502}. Similar to \citet{liu2024incompleteloopinstructioninference}, we regard the direct answering of LLMs as a form of inductive inference, representing their System 1 logical inference pipeline.
\paragraph{Abductive and Deductive Inference} With this System 2 pipeline, task completion is decomposed into two steps. First, LLMs are required to abductively infer the hypothetical pattern \(P_h\) based on the source pair(s). Then, they deductively apply this pattern to the incomplete target pair as \( B \xrightarrow{P_h} B' \).
\paragraph{Zero-shot CoT as Automate Inference} The automate rationale in LLMs, or zero-shot CoT \cite{wei2023chainofthoughtpromptingelicitsreasoning}, demonstrates their inherent logical inference process, which is acquired during the instruction-tuning or alignment stages. Therefore, we included it in our comparison for reference.

\section{Evaluation Environment}
In this section, we introduce our evaluation environment of analogical reasoning, providing details on the settings for each control dimensions.

\label{sec:eval}
\begin{table}[]
\setlength{\tabcolsep}{3.5pt}
\centering
\scriptsize
\begin{tabular}{cllcccc}
\toprule
\multicolumn{3}{c}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{Difficulty}} & \multirow{2}{*}{\textbf{Total}} \\ \cmidrule(lr){1-3}\cmidrule(lr){4-6}
Task & \multicolumn{1}{c}{Modality} & \multicolumn{1}{c}{Benchmark} & Easy & Medium & Hard &  \\ \midrule
\multirow{3}{*}{Analogy} & Textual & E-KAR & 317 & 435 & 496 & 1248 \\
 & Visual & VASR & 455 & 572 & 320 & 1347 \\
 & Symbolic & RAVEN & 402 & 462 & 395 & 1259 \\ \midrule
 \multirow{2}{*}{General ICL} & Math/Code & List Function & 432 & 423 & 395 & 1250 \\
 & Textual & SALT & 400 & 400 & 400 & 1200 \\ \midrule
\multicolumn{3}{c}{\textbf{Total}} & 2006 & 2292 & 2006 & \textbf{6304} \\ \bottomrule
\end{tabular}
\caption{Dataset statistics across modalities and difficulty levels. Details of general in-context learning tasks (List Function and SALT) are introduced in Section \ref{sec:generalization}.}
\label{tab:dataset_stats}

\end{table}
\begin{figure*}[t]
\begin{center}
\includegraphics[clip,width=\linewidth]{images/logidynamics_main.pdf}
\end{center}
\caption{LLM performances (in Accuracy \%) in our evaluation environment under different reasoning pipelines.}
\vspace{-0.3cm}
\label{fig:main_result}
\end{figure*}
\subsection{Modality}
Exploring diverse data modalities is crucial for obtaining comprehensive insights. To this end, we selected three analogical reasoning tasks across different modalities. \textbf{E-KAR} \cite{Chen_2022} consists of human-curated analogy questions between word pairs (or sets), where analogies are determined by shared ontological relationships between words. \textbf{VASR} \cite{bitton2022vasrvisualanalogiessituation} comprises human-annotated analogical questions between image pairs, where analogies are determined by shared semantic transitions between images. \textbf{RAVEN} \cite{raven1938,zhang2019raven,hu2022stratifiedruleawarenetworkabstract} generates symbolic matrices using attributed stochastic image grammar (A-SIG), where analogies are determined by shared attribute shifts among rows. To enhance comprehension in large language models, we adopt the abstracted version proposed by \citet{hu-etal-2023-context}, which tokenizes the matrix images into symbolic vectors.

\subsection{Difficulty}
Task difficulty, while a key determinant of thinking styles \cite{phillips2016thinking}, is largely overlooked in research on reasoning paradigms in LLMs. To address this, we conducted difficulty annotations for all three datasets. In analogical reasoning involving real-world data, difficulty is often measured by the semantic distance between analogy pairs \cite{Vendetti2012,Jones2022}. For E-KAR, we compute the semantic distance between word pairs using \textbf{FastText} embeddings \cite{bojanowski2017enriching}, which are more suitable than Word2Vec \cite{mikolov2013efficientestimationwordrepresentations} or BERT \cite{devlin2019bertpretrainingdeepbidirectional}, as the word pairs exhibit morphological variations but lack contextual dependencies. For VASR, we calculate the distance between \textbf{VGG} encodings \cite{simonyan2015deepconvolutionalnetworkslargescale} to account for both semantic and graphical features. For RAVEN, task complexity is defined by the number of \textbf{attribute variations} across the columns. The statistics of our datasets across different modalities and difficulty levels are presented in Table \ref{tab:dataset_stats}. Further details about our difficulty annotation process are provided in Appendix \ref{app:difficulty}.



\subsection{Task Format}
The task format also serves as an important factor influencing reasoning performance \cite{Ribeiro2018SemanticallyEA,zong2024comparisonqaevaluatingfactualityrobustness}. We conducted experiments separately under two task formats\footnote{For the visual dataset, we evaluated only in the MCQ format for feasibility.}: multiple-choice questions (MCQ) and free-text generation (FTG), aiming to achieve a more comprehensive perspective in our exploration.


\begin{table}[t]
\centering
\small
(a) Modality (Task Format = MCQ)

\vspace{0.05cm}

\begin{tabular}{lccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Modality}} \\ \cmidrule(lr){2-4} 
\multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Pipeline}}} & Textual & Visual & Symbolic \\ \midrule
Induction & 55.70 & 38.88 & 28.58 \\
Automate & 58.05 & 51.52 & 34.99 \\
Abduction+Deduction & \textbf{59.13} & \textbf{53.93} & \textbf{37.69} \\ \midrule
System 2 Advantage & \cellcolor[HTML]{FFFFFF}{\color[HTML]{50BF50}+6.16\%} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{50BF50}+38.73\%} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{50BF50}+31.86\%} \\ \bottomrule
\vspace{0.01cm}
\end{tabular}

\setlength{\tabcolsep}{5.6pt}
(b) Difficulty (Task Format = MCQ)

\vspace{0.05cm}

\begin{tabular}{lccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Difficulty}} \\ \cmidrule(lr){2-4} 
\multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Pipeline}}} & Easy & Medium & Hard \\ \midrule
Induction & 51.48 & 41.93 & 31.48 \\
Automate & 58.12 & 48.23 & 40.02 \\
Abduction+Deduction & \textbf{59.68} & \textbf{49.76} & \textbf{43.20} \\ \midrule
System 2 Advantage & \multicolumn{1}{r}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{50BF50}+15.92\%}} & \multicolumn{1}{r}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{50BF50}+18.68\%}} & \multicolumn{1}{r}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{50BF50}+37.20\%}} \\ \bottomrule
\vspace{0.01cm}
\end{tabular}
\setlength{\tabcolsep}{2.4pt}
(c) Task Format

\vspace{0.05cm}

\begin{tabular}{lccc>{\columncolor[HTML]{ffe1e1}}c}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Textual} & \multicolumn{2}{c}{Symbolic} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
\multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Pipeline}}} & MCQ & FTG & MCQ & FTG \\ \midrule
Induction & 55.70 & 23.36 & 28.58 & \textbf{19.18} \\
Automate & 58.05 & 24.89 & 34.99 & 8.67 \\
Abduction+Deduction & \textbf{59.13} & \textbf{24.93} & \textbf{37.69} & 11.33 \\ \midrule
System 2 Advantage & {\color[HTML]{50BF50}+6.16\%} & {\color[HTML]{50BF50}+6.74\%} & {\color[HTML]{50BF50}+31.86\%} & {\color[HTML]{FF0000} -40.93\%} \\ \bottomrule
\end{tabular}

\caption{Comparative dynamics of different logical inference pipelines in our evaluation environment, controlled by \textit{modality}, \textit{difficulty}, and \textit{task format}. Performances (in Accuracy \%) are averaged across all LLMs. "System 2 Advantage" denotes the relative improvements of abduction + deduction pipeline over direct induction.}
\label{tab:dynamics_modality}
\vspace{-0.3cm}
\end{table}

\section{Main Experiment Results and Analysis}
\label{sec:main}
We evaluated 10 modern LLMs / MLLMs (details provided in Appendix \ref{app:llm}) within our exploration environment. The experimental results are presented in Figure \ref{fig:main_result}. Across the entire environment, the tested LLMs achieved an overall average performance of only \textbf{35.4\%}, demonstrating that our datasets effectively stress-test the real reasoning abilities of LLMs rather than simply retrieving from memorization. Furthermore, the significant performance gaps across difficulty levels validate the effectiveness of our difficulty annotations. Generally, the abduction + deduction pipeline outperforms direct induction, while automated inference falls between the two pipelines in most scenarios.

To better illustrate the comparative dynamics between different logical inference pipelines, we present the consolidated results controlled by each dimension in Table \ref{tab:dynamics_modality}. From these results, we observe the key findings as follows:

\paragraph{Findings 1: The comparative advantages of the System 2 logical inference pipeline are modality-dependent.} As shown in Table \ref{tab:dynamics_modality}(a), the abduction + deduction pipeline significantly outperforms direct induction in visual and symbolic tasks, with relative improvements of 38.73\% and 31.86\%, respectively. However, in textual tasks, direct induction achieves comparable performance, trailing behind by only 6.16\%.

\paragraph{Findings 2: The comparative advantages of the System 2 logical inference pipeline are difficulty-dependent.} Based on Table \ref{tab:dynamics_modality}(b), the abduction + deduction pipeline outperforms direct induction by 37.20\% on hard questions, while the performance gap reduces to 18.68\% and 15.92\% on medium and easy questions, respectively.

\paragraph{Findings 3: The System 2 logical inference pipeline falls short in free-text generation format when the task requires explicit rule execution.} Results from Table \ref{tab:dynamics_modality}(c) reveal a \textit{noteworthy inconsistency}: in textual tasks, the advantage of the System 2 pipeline remains the same across task formats. However, in symbolic tasks, the System 2 pipeline severely underperforms direct induction in the free-text generation format, which \textbf{sharply contrasts} with its advantage in the multiple-choice question format. We identified several explanations for this observation: 
\begin{enumerate}
    \item The precise execution of complex rules are challenging for most LLMs. The two-step inference process incurs a significant performance cost in such reasoning tasks.
    \item Implicit pattern matching may be more effective in this case, as employed by direct induction. However, in the System 2 pipeline, lengthy rationales disrupt the well-structured few-shot patterns essential for in-context learning, thereby rendering implicit learning ineffective.
    \item For multiple-choice questions, the System 2 pipeline can better infer patterns, as the answer space is reduced to a few candidates. It may also occasionally leverage reasoning shortcuts to improve performance \cite{geirhos2020shortcut,zong2024comparisonqaevaluatingfactualityrobustness} — an advantage that cannot be employed in direct induction.
\end{enumerate}

As a result, the abduction + deduction pipeline tends to favor the MCQ format when addressing problems that require explicit rule execution, whereas, under the FTG format, direct induction demonstrates a surprising advantage.

\begin{figure}[t]
\begin{center}
\includegraphics[clip,width=220pt]{images/logidynamics_gen.pdf}
\end{center}

\caption{LLM performances (in Accuracy \%) in List Function and SALT under different reasoning pipelines.}
\vspace{-0.3cm}
\label{fig:gen_result}
\end{figure}


\begin{table}[]
\setlength{\tabcolsep}{3.81pt}
\centering
\small
(a) List Function
\scriptsize

\begin{tabular}{lcccc>{\columncolor[HTML]{ffe1e1}}c}
\toprule
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Difficulty}} & \multicolumn{2}{c}{\textbf{Task Format}} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-6}
\multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Pipeline}}} & Easy & Medium & Hard & MCQ & FTG \\ \midrule
Induction & 65.26 & 42.53 & 24.18 & 44.96 & \textbf{43.92} \\
Automate & 64.42 & 42.16 & 26.35 & 52.35 & 37.09 \\
Abduction+Deduction & \textbf{68.55} & \textbf{43.21} & \textbf{28.06} & \textbf{52.93} & 40.85 \\ \midrule
System 2 Advantage & {\color[HTML]{50BF50}+5.04\%} & {\color[HTML]{50BF50} +1.60\%} & {\color[HTML]{50BF50}+16.06\%} & {\color[HTML]{50BF50}+17.73\%} & {\color[HTML]{FF0000} -6.98\%} \\ \bottomrule
\vspace{-0.05cm}
\end{tabular}
\setlength{\tabcolsep}{3.5pt}
\small
(b) SALT
\scriptsize

\begin{tabular}{lcccc>{\columncolor[HTML]{ffe1e1}}c}
\toprule
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Difficulty}} & \multicolumn{2}{c}{\textbf{Task Format}} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-6}
\multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Pipeline}}} & Easy & Medium & Hard & MCQ & FTG \\ \midrule
Induction & \textbf{49.75} & 33.58 & 23.42 & 41.44 & \textbf{29.72} \\
Automate & 41.88 & 36.17 & 29.46 & 45.83 & 25.83 \\
Abduction+Deduction & 43.71 & \textbf{39.17} & \textbf{33.58} & \textbf{50.53} & 27.11 \\ \midrule
System 2 Advantage & {\color[HTML]{FF0000}-12.14\%} & {\color[HTML]{50BF50} +16.63\%} & {\color[HTML]{50BF50}+43.42\%} & {\color[HTML]{50BF50}+21.92\%} & {\color[HTML]{FF0000} -8.79\%} \\ \bottomrule
\vspace{0.01cm}

\\\midrule
\multicolumn{1}{c}{\textbf{Average}}& {\color[HTML]{FF0000}-3.55\%} & {\color[HTML]{50BF50} +9.11\%} & {\color[HTML]{50BF50}+29.74\%} & {\color[HTML]{50BF50}+19.83\%} & {\color[HTML]{FF0000} -7.88\%} \\ \bottomrule
\end{tabular}

\caption{Comparative dynamics of different logical inference pipelines in List Function and SALT. Performances (in Accuracy \%) are averaged across all LLMs.}
\vspace{-0.3cm}
\label{tab:gen_dynamics}
\end{table}


\section{Generalization Experiment}
\label{sec:generalization}
To further assess the generalizability of our findings, we extend the scope from analogical reasoning to general in-context learning tasks. Specifically, we formally define our target task scope using the following constraints: 1) The task requires generating output \(y\) from input \(x\), based on \(n\)-shot demonstrations \(D = [(x_1, y_1), \dots, (x_n, y_n)]\). 2) The input-output function \(y = f(x)\) can be explicitly defined. We conduct generalization experiments on two in-context learning datasets, both of which require explicit rule execution. 

\vspace{0.2cm}
\noindent{\textbf{List Function}} \cite{rule2020child, li2025patternsprinciplesfragilityinductive} maps input lists of integers to output lists using 250 predefined transition functions. In this task, LLMs must infer the underlying function from provided demonstrations (input-output pairs) and apply it to new input lists. The difficulty of the task is determined by the complexity of the transition functions.

\vspace{0.2cm}
\noindent{\textbf{SALT}} (\textbf{S}yntax-aware \textbf{A}rtificial \textbf{L}anguage \textbf{T}rans- lation) is a machine translation benchmark that we developed to address key limitations in existing datasets. Unlike benchmarks such as SCAN \cite{higgins2018scanlearninghierarchicalcompositional} and Kalamang \cite{tanzer2024benchmarklearningtranslatenew}, SALT introduces diverse syntactic shifts (e.g., inversion of semantic unit order) while rigorously mitigating data leakage—a common issue in low-resource machine translation benchmarks. The task difficulty is determined by the complexity of the syntactic structures, enabling fine-grained evaluation of model performance across varying levels of linguistic challenge. Details of the SALT dataset are provided in Appendix \ref{app:salt}.

The results of the generalization experiments are illustrated in Figure \ref{fig:gen_result}, with the consolidated findings presented in Table \ref{tab:gen_dynamics}. Across both datasets,   we observed patterns similar to those in our evaluation environment in analogy: The advantage of the System 2 logical inference pipeline increases significantly as task difficulty rises. While the two pipelines exhibit contrasting task preferences between the MCQ and FTG format. Consequently, we demonstrate that \textbf{our findings are generalizable to broader in-context learning tasks} where the input-output function is explicitly defined.

\section{Scaling-up System 2 Logical Inference}
\label{sec:scaling}
Beyond the basic processes of abductive hypothesis generation and deductive execution, more sophisticated logical inference pipelines can be employed to tackle complex tasks. We introduce two inference methodologies in philosophy and connect them to the logical inference pipelines of LLMs.

\subsection{Liptonian and Holmesian Inference}

\vspace{0.1cm}
\noindent{\textbf{Liptonian Inference}} \cite{Lipton2000-LIPITT-8} provides a widely recognized modern account of IBE (Inference to the Best Explanation). It characterizes the process of selecting the most explanatory hypothesis from a set of candidates based on its capacity to best account for the observed evidence.  In LLM reasoning, this corresponds to the parallel sampling of multiple hypotheses, followed by hypothesis selection as a precursor to the final deductive execution. In our experiment, we evaluated the effectiveness of \textbf{hypothesis selection} across sampling sizes ranging from 1 to 10.

\vspace{0.1cm}  
\noindent{\textbf{Holmesian Inference}} \cite{Bird2005-BIRAKA} provides an alternative model to Liptonian, emphasizing hypothesis verification rather than selection. Inspired by Sherlock Holmes’s famous dictum, it involves systematically eliminating all but one hypothesis to ensure that the remaining one is necessarily true. In LLM reasoning, this can be simulated through iterative verification and refinement (regeneration) of hypotheses, where candidate outputs are repeatedly evaluated and improved. In our experiment, we investigated \textbf{hypothesis verification and refinement} across iteration rounds up to 5.
\begin{figure}[t]
\begin{center}
\includegraphics[clip,width=220pt]{images/logidynamics_scale.pdf}
\end{center}
\caption{Effect of hypothesis selection, verification and refinement on LLM performances (in Accuracy \%).}
\vspace{-0.3cm}
\label{fig:scale_result}
\end{figure}
\subsection{Scaling Performances}
The experimental results of hypothesis selection, verification and refinement are presented in Figure \ref{fig:scale_result} (a) and \ref{fig:scale_result} (b). In hypothesis selection, we observe clear improvements in sampling sizes from 1 to 5. However, the performance starts to decrease when the sampling size increases to 10, as the diversity of the sampled hypotheses begins to saturate, and the selection process also becomes less effective with a longer context. In terms of hypothesis verification and refinement, the saturation of improvements was reached after one round of verification, except for GPT-4o in the List Function, where positive improvements were observed in every additional round of verification. This interesting inconsistency can be explained as follows: \textbf{1) Stronger LLMs lead to better verification quality.} Compared to the consistent improvements observed in GPT-4o, GPT-4o-mini did not exhibit similar enhancements, as its ability to detect incorrect hypotheses is also weaker.
\textbf{2) Well-formed hypothesis formats make refinements easier.} The improvement seen in the List-Function dataset (where hypotheses are written in Python code) does not hold for the RAVEN dataset (where hypotheses are presented in free text). A better hypothesis format may also enhance the effectiveness of proofreading or maintaining the validity of existing hypotheses.

Figure \ref{fig:scale_result} (c) illustrates the combined effect of the two scaling strategies. In both datasets, GPT-4o demonstrates significant performance improvements as the number of inference tokens increases. For instance, performance of GPT-4o in the List-Function dataset improved from \textbf{46.8\%} to \textbf{61.6\%}, consuming \textbf{25×} more inference tokens compared to automated inference. \textbf{This underscores the potential of scaling up LLM reasoning through System 2 logical inference pipelines.}

\setlength{\tabcolsep}{3.2pt}


\subsection{Discussions on Large Reasoning Models}
Recent advances in large reasoning\,models\,(LRMs), such as o1 \cite{openai2024o1preview} and Deepseek-R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, have demonstrated impressive performance in mathematical and code reasoning tasks. LRMs emerge strong self-reflec- tion abilities during their reinforcement learning stage, driven by rule-based rewards. From\,our\,explor- ation, LRMs exhibit two noteworthy characteristics within our task domain (in-context learning with explicit input/output functions): 1) \textbf{LRMs emulate an "iterative holmesian inference"} by engaging in repeated cycles of hypothesis generation and verification. 2) \textbf{The number of inference tokens (rounds of iterative hypothesis generation) increases significantly as task difficulty rises}.

Nevertheless, can short-CoT LLMs achieve comparable performance by scaling up System 2 logical inference? To answer this question, we conducted experiments on Deepseek-V3 \cite{deepseekai2024deepseekv3technicalreport}, employing adaptive logical inference scaling under \textit{low} and \textit{high} computational consumptions, where the model autonomously determined the number of iteration within a set limit. As shown in the results in Table~\ref{tab:lrm}, under high consumptions, Deepseek-V3 \textbf{demonstrates a similar inference scaling effect across difficulty levels and achieves comparable performance to LRMs}.


\begin{table}[]
\centering
\scriptsize
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{3}{c}{\textbf{Inference Tokens (\# Rounds)}} & \multirow{2}{*}{\textbf{Accuracy}} \\ \cmidrule(lr){2-4}
\multicolumn{1}{c}{} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} &  \\ \midrule
Deepseek-R1 & 2174.5 (3.9) & 3353.1 (5.0) & 5935.9 (6.5) & 69.2 \\
o1-mini & 1345.5 (2.6) & 2229.8 (3.2) & 4188.0 (3.5) & 69.6 \\
o1 & 1949.1 (2.7) & 3233.0 (3.3) & 6995.7 (5.5) & \textbf{77.2} \\
o3-mini & 1184.3 (2.5) & 2126.3 (3.0) & 5328.7 (6.2) & \underline{76.8} \\ \midrule
\vspace{-0.15cm}

\\ \midrule
Deepseek-V3 & 989.0 & 1261.1 & 1260.9 & 57.2 \\
 +Sys2 Scaling (low) & 1758.0 (2.4) & 2124.4 (2.5) & 2618.9 (2.7) & 65.2 \\
 +Sys2 Scaling (high) & 2356.8 (2.7) & 2985.3 (2.9) & 4308.0 (3.8) & 69.6 \\\bottomrule
\end{tabular}
\caption{Performance of LRMs and LLMs with adaptive logical inference scaling on the List Function dataset.}
\vspace{-0.4cm}
\label{tab:lrm}
\end{table}


\section{Related Work}
\subsection{Logical Inference in Language Models}
\paragraph{Abductive Inference}
In the era of pre-trained language models, $\alpha$\textbf{-NLI} \citep{bhagavatula2020abductivecommonsensereasoning} introduced abductive reasoning to commonsense reasoning, where plausible explanations are inferred from observations. Subsequent works proposed various techniques to enhance this capability \citep{qin2021futureunsupervisedbackpropbaseddecoding, kadiķis2022embarrassinglysimpleperformanceprediction, chan2023selfconsistentnarrativepromptsabductive}, including extensions to uncommon scenarios focusing on rare but logical explanations \citep{zhao2024uncommonsensereasoningabductivereasoning}. Unlike real-world data in commonsense reasoning, benchmarks like \textbf{ProofWriter} \citep{tafjord2021proofwritergeneratingimplicationsproofs} evaluate formal abductive reasoning within semi-structured texts with explicit logical relationships. Recent studies have explored LLMs in more challenging open-world reasoning contexts \citep{zhong2023chatablabductivelearningnatural, del2023truedetectivedeepabductive, thagard2024chatgptmakeexplanatoryinferences}. Beyond natural language inference, abductive reasoning has also been examined in graph-based modalities for commonsense and event knowledge \citep{du-etal-2021-learning, bai2024advancingabductivereasoningknowledge}.

\paragraph{Deductive and Inductive Inference}
Deductive inference is studied using benchmarks like \textbf{RuleTaker} \citep{clark2020transformerssoftreasonerslanguage}, where LMs perform rule-based reasoning on natural language. Further works evaluate deductive reasoning under generalization, emphasizing challenges with longer proofs \cite{saparov2023testinggeneraldeductivereasoning} and complex logic \cite{zheng2025enhancingtransformersgeneralizablefirstorder}. Inductive inference is explored through datasets like \textbf{EntailmentBank} \citep{dalvi2022explaininganswersentailmenttrees}, where models construct entailment trees to explain answers. While LLMs demonstrate emergent inductive abilities via few-shot learning \citep{wei2022emergentabilitieslargelanguage}, \citet{min2022rethinkingroledemonstrationsmakes} argue that structural cues often outweigh label correctness in induction.

\subsection{Analogical Reasoning} 
The study of analogical reasoning in AI has progressed from early symbolic systems, such as the \textbf{Structure-Mapping Engine} \citep{FALKENHAINER19891}, which used hand-crafted representations, to models like the \textbf{Latent Relation Mapping Engine} \citep{Turney_2008}, which integrated symbolic rules with statistical learning. The neural era introduced word embeddings for analogy evaluation \citep{mikolov2013distributedrepresentationswordsphrases}, emphasizing local semantic patterns. With LLMs, \citet{webb2023emergentanalogicalreasoninglarge} demonstrated emergent analogical reasoning, but challenges remain. \textbf{AnaloBench} \citep{ye2024analobenchbenchmarkingidentificationabstract} shows minimal scaling gains for long-context analogies, while \textbf{ANALOGICAL} \citep{wijesiriwardene2023analogicalnovelbenchmark} highlights struggles with complex metaphors. Story-level benchmarks like \textbf{StoryAnalogy} \citep{jiayang2023storyanalogyderivingstorylevelanalogies} and \textbf{ARN} \citep{sourati2024arnanalogicalreasoningnarratives} reveal difficulties in cross-domain narrative mapping without explicit prompts.
\section{Conclusion}

This paper systematically investigates the dynamics of logical inference in large language models. Our comprehensive evaluation environment—spanning multiple modalities, difficulty levels, and task formats—reveals key patterns in the effectiveness of different logical inference pipelines. Our findings highlight that the System 2 approach (abduction + deduction) excels in visual/symbolic reasoning and complex problem-solving, while System 1 (direct induction) remains competitive in simpler and text-based tasks. However, we observe a critical dependency on task format, as System 2 struggles with free-text generation in rule-execution tasks. Generalization experiments confirm that these findings extend to broader in-context learning tasks involving explicit input-output functions. Furthermore, scaling System 2 paradigms through hypothesis selection, verification, and refinement yields significant performance gains, demonstrating the potential for enhancing logical inference in large language models. These insights provide actionable guidelines for selecting inference strategies based on task characteristics and emphasize the importance of adaptive reasoning systems that integrate multiple inference approaches. Overall, this work establishes a foundation for future research into optimizing and scaling logical inference capabilities in large language model reasoning.

\section*{Limitations}
While our extensive experiments and analyses yield rich findings, our exploration is limited to reasoning frameworks for static LLMs. Future research could build on this work by focusing on the tuning stage of LLMs, aiming to develop systems that dynamically balance different types of logical inference. For example, a system capable of automatically identifying the nature of a question and determining whether to apply System 1 or System 2 reasoning could not only maintain or enhance performance but also improve efficiency. Such adaptive reasoning closely mirrors the way humans naturally approach problem-solving.

\section*{Ethics Statement}

This work aims to advance the understanding of logical inference in LLMs through systematic experimentation and analysis. All LLMs used in this study are publicly available. We strictly prohibit harmful content in the selection, curation, and annotation process of our datasets, ensuring they are free from sensitive or biased material. Our work is conducted with a focus on advancing understanding while adhering to ethical research practices.
\bibliography{main}

\newpage

\appendix

\,

\newpage
\section{Model Details}
\label{app:llm}
In our experiments, we tested 15 modern LLM / MLLMs / LRMs with their detailed information as follows:
\begin{itemize}
\item \textbf{Qwen-2.5-7b / Qwen-2.5-72b} \cite{qwen2025qwen25technicalreport} is an open-source MoE LLM series, trained with 18 trillion tokens of pre-training corpus and 1 million fine-tuning examples.
\item \textbf{Llama-3.1-70b / Llama-3.1-405b} \cite{meta2024llama31} is an open-source dense LLM series, trained with 15 trillion tokens of pre-training corpus, and adopted DPO \cite{rafailov2024directpreferenceoptimizationlanguage} during its alignment stage.
\item \textbf{GPT-4o-mini / GPT-4o} \cite{openai2024gpt4o} is the latest proprietary LLM series by OpenAI prior to their reasoning models.
\item \textbf{Gemini-1.5-flash / Gemini-1.5-pro} \cite{google2024gemini} is a proprietary MoE LLM series featuring a long context window of 1 million tokens.
\item \textbf{Gemini-2.0-flash} \cite{google2024gemini2} is the latest Gemini series LLM, offering enhanced multimodal and reasoning performance.
\item \textbf{Pixtral-12b} \cite{agrawal2024pixtral12b} is a lightweight open-source multimodal LLM.
\item \textbf{Deepseek-V3} \cite{deepseekai2024deepseekv3technicalreport} is the state-of-the-art open-source LLM.
\item \textbf{Deepseek-R1} \cite{deepseekai2025deepseekr1incentivizingreasoningcapability} is the leading open-source LRM trained with reinforcement learning using a rule-based reward system.
\item \textbf{o1-mini / o1} \cite{openai2024o1preview} represents the state-of-the-art proprietary LRM series developed by OpenAI.
\item \textbf{o3-mini} \cite{openai2025openaiO3Mini} is the latest LRM by OpenAI, featured its cost-effectiveness.

\end{itemize}
The temperature for all LLMs is set to zero in our main experiments, while it is set to 0.4 during the hypothesis sampling in our scaling experiments.
\section{Difficulty Annotation}
\label{app:difficulty}
The detailed difficulty annotation standards are presented in Table \ref{tab:difficulty}. For \textbf{EKAR} and \textbf{VASR}, we set thresholds for semantic distances to categorize the difficulty into easy, medium, and hard, ensuring comparable sizes across categories. For \textbf{RAVEN}, we calculate the number of attributes in transition among rows, with fine-grained categorization applied within each question typology. For \textbf{List Function}, we use the predefined complexity ranking of mapping functions provided by \cite{rule2020child}. For \textbf{SALT}, we classify the syntax complexity of the translation examples into simple, medium, and complex categories.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\setlength{\tabcolsep}{8pt}

\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{cclccc}
\toprule
\textbf{Dataset} & \textbf{Determinator} & \multicolumn{1}{c}{\textbf{Category}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} \\ \midrule
E-KAR & FastText Distance & \multicolumn{1}{c}{-} & \textless{}0.70 & 0.70$\sim$0.80 & \textgreater{}0.80 \\ \midrule
VASR & VGG Distance & \multicolumn{1}{c}{-} & \textless{}0.70 & 0.70$\sim$0.76 & \textgreater{}0.76 \\ \midrule
\multirow{7}{*}{RAVEN} & \multirow{7}{*}{Number of Transitions} & center\_single & 1 & 2 & \textgreater{}=3 \\
 &  & distribute\_four & \textless{}=2 & 3 & \textgreater{}=4 \\
 &  & distribute\_nine & \textless{}=2 & 3 & \textgreater{}=4 \\
 &  & in\_center\_single\_out\_center\_single & \textless{}=3 & 4 & \textgreater{}=5 \\
 &  & in\_distribute\_four\_out\_center\_single & \textless{}=3 & 4 & \textgreater{}=5 \\
 &  & up\_center\_single\_down\_center\_single & \textless{}=3 & 4 & \textgreater{}=5 \\
 &  & left\_center\_single\_right\_center\_single & \textless{}=4 & 5 & \textgreater{}=6 \\ \midrule
List Function & Function Complexity Ranking & \multicolumn{1}{c}{-} & \textless{}=84 & 85$\sim$170 & \textgreater{}=170 \\ \midrule
SALT & Syntax Complexity & \multicolumn{1}{c}{-} & simple & intermediate & complex \\ \bottomrule
\end{tabular}
\caption{Difficulty classification standards for each datasets in our experiment.}
\label{tab:difficulty}
\end{table*}

\section{Syntax-aware Artificial Language Translation}
\label{app:salt}
Syntax-aware Artificial Language Translation (SALT) is a low-resource machine translation (MT) benchmark that we designed and developed to evaluate generalizable in-context learning in large language models. LLMs are required to infer vocabulary mappings as well as syntactic transitions from few-shot demonstrations and apply them to translate a compositionally crafted testing instance. SALT offers two key advantages over other low-resource MT benchmarks: 1) SALT synthesizes out-of-vocabulary strings for the artificial language, \textbf{preventing data leakage}, a common issue in other low-resource MT benchmarks.  2) SALT provides \textbf{detailed difficulty control} enabled by human-curated syntactic structures with compositional complexities.  

The creation of SALT involves two main stages:  
\begin{enumerate}
    \item \textbf{Syntax-aware Template Design} In the first stage, we design syntactic rules that involve the permutation or repetition of semantic units in the artificial language, as illustrated in Table \ref{tab:salt_example}. Next, we manually craft templates for few-shot demonstrations with considerations in compositional generalization. We ensure that all the necessary underlying word mappings and syntactic rules required for translating the testing instances can be inferred from the provided few-shot demonstrations.
    \item \textbf{Semantic-aware Data Synthesis} After acquiring the templates, we populate them with semantically appropriate English words using LLM-assisted selection. Next, we randomly assign out-of-vocabulary letter strings as the artificial language equivalents for each English word. Finally, a total of 1,200 questions are sampled—400 at each difficulty level—ensuring comparability in size with other datasets.
\end{enumerate}



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{ccc}
\toprule
\textbf{English Sentence} & \textit{I like beautiful house.} & \textit{Giant elephant runs quickly.} \\ \midrule
\textbf{Syntax Structure} & \textless{}pronoun - verb - adjective - noun\textgreater{} & \textless{}adjective - noun - verb - adverb\textgreater{} \\ \midrule
\textbf{Grammar Rule} & \textless{}noun-adjective inversion\textgreater{} & \textless{}predicate-subject inversion\textgreater{} \\ \midrule
\textbf{Transition Type} & Intra-Constituent & Inter-Constituent \\ \midrule
\textbf{Vocabulary} & I \(\rightarrow\) gkt, like \(\rightarrow\) ivo, beautiful \(\rightarrow\) prr, house \(\rightarrow\) cbi & giant \(\rightarrow\) rgd, elephant \(\rightarrow\) krt, runs \(\rightarrow\) uco, quickly \(\rightarrow\) xrk \\ \midrule
\textbf{Translation} & \textit{gkt ivo cbi prr.} & \textit{uco xrk rgd krt.} \\ \bottomrule
\end{tabular}
\caption{Examples of intra-constituent and inter-constituent syntactic transitions in the SALT dataset. }
\label{tab:salt_example}
\end{table*}





\newpage



%\section{Exploring Large Reasoning Models}
%\label{app:lrm}

%From our analysis of DeepSeek-R1's reasoning process, we observed that large reasoning models emulate an "iterative Holmesian inference" by engaging in repeated cycles of hypothesis generation and verification.
%To gain deeper insights into the inference dynamics of these models, we conducted an evaluation of four state-of-the-art reasoning models using the List Function dataset:


%In our experiments, in addition to measuring inference token consumption, we instructed the model to track the number of trials involved in hypothesis generation, verification, and refinement/regeneration. Since the reasoning process of the o-series models is not directly accessible, the reported numbers should be considered as reference points only.

%The experimental results are presented in Table \ref{tab:lrm}. We observe a noteworthy trend: \textbf{The number of inference tokens (as well as the number of iterations) increases sharply as task difficulty rises.} Notably, in the case of o3-mini, the inference tokens required for \textit{hard} questions are approximately \textbf{5×} greater than those for \textit{easy} questions. This demonstrates the model's adaptiveness in adjusting inference costs based on task complexity.

%Nevertheless, can short-CoT LLMs reach comparable inference by scaling-up System 2 logical inference? We conducted experiments on Deepseek-V3 \cite{deepseekai2024deepseekv3technicalreport}, with dynamic logical inference scaling under \textit{low} and \textit{high} consumptions. 

\newpage
\section{Prompt Templates}
\label{app:prompt}
\begin{promptbox}[colback=pink!10, colframe=pink!50!purple, title=Textual Analogy (Induction)]{}
\scriptsize
Below is an analogy question, where analogy x:y::x':y' exists between the two wordsets, your task is to finish the second wordset to complete the analogy. \\
\begin{verbatim}
Wordset1: <word_x>:<word_x'>
Wordset2: <word_y>:[missing_word]
  
Your response should strictly follow the JSON dict format:

{
    "answer": "missing word here"
}
\end{verbatim}
\end{promptbox}

\begin{promptbox}[colback=pink!10, colframe=pink!50!purple, title=Textual Analogy (Automate)]{}
\scriptsize
Below is an analogy question, where analogy x:y::x':y' exists between the two wordsets, your task is to finish the second wordset to complete the analogy. \\
\begin{verbatim}
Wordset1: <word_x>:<word_x'>
Wordset2: <word_y>:[missing_word]
  
Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "answer": "missing word here"
}
\end{verbatim}
\end{promptbox}
\newpage
\begin{promptbox}[colback=pink!10, colframe=pink!50!purple, title=Textual Analogy (Abduction)]{}
\scriptsize
Below is an analogy question, where analogy x:y::x':y' exists between the two wordsets, your task is to infer the relational pattern within wordsets. \\
\begin{verbatim}
Wordset1: <word_x>:<word_x'>
Wordset2: <word_y>:[missing_word]
  
Your response should strictly follow the JSON dict format:

{
    "reasoning": "reasoning steps here"
    "pattern": "relational pattern here"
}
\end{verbatim}
\end{promptbox}

\begin{promptbox}[colback=pink!10, colframe=pink!50!purple, title=Textual Analogy (Deduction)]{}
\scriptsize
Below is an analogy question, where analogy x:y::x':y' exists between the two wordsets, your task is to finish the second wordset to complete the analogy. Here's the relational pattern: <pattern> \\
\begin{verbatim}
Wordset1: <word_x>:<word_x'>
Wordset2: <word_y>:[missing_word]
  
Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "answer": "missing word here"
}
\end{verbatim}
\end{promptbox}



\newpage
\begin{promptbox}[colback=green!10, colframe=green!60!black, title=Visual Analogy (Induction)]{}
\scriptsize
Below is an analogy question, where analogy x:y::x':y' exists between the two image pairs, your task is to complete the second image pair to complete the analogy. \\
\begin{verbatim}
Image Pair 1: <img_x>:<img_x'>
Image Pair 2: <img_y>:[missing_img]

<Candidate Images>

Your response should strictly follow the JSON dict format:

{
    "answer": "missing image choice here"
}
\end{verbatim}
\end{promptbox}

\begin{promptbox}[colback=green!10, colframe=green!60!black, title=Visual Analogy (Automate)]{}
\scriptsize
Below is an analogy question, where analogy x:y::x':y' exists between the two image pairs, your task is to complete the second image pair to complete the analogy. \\
\begin{verbatim}
Image Pair 1: <img_x>:<img_x'>
Image Pair 2: <img_y>:[missing_img]

<Candidate Images>

Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "answer": "missing image choice here"
}
\end{verbatim}
\end{promptbox}

\begin{promptbox}[colback=green!10, colframe=green!60!black, title=Visual Analogy (Abduction)]{}
\scriptsize
Below is an analogy question, where analogy x:y::x':y' exists between the two image pairs, your task is to infer the relational pattern within image pairs. \\
\begin{verbatim}
Image Pair 1: <img_x>:<img_x'>
Image Pair 2: <img_y>:[missing_img]

<Candidate Images>

Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "pattern": "relational pattern here"
}
\end{verbatim}
\end{promptbox}


\begin{promptbox}[colback=green!10, colframe=green!60!black, title=Visual Analogy (Deduction)]{}
\scriptsize
Below is an analogy question, where analogy x:y::x':y' exists between the two image pairs, your task is to complete the second image pair to complete the analogy. Here's the relational pattern: <pattern> \\
\begin{verbatim}
Image Pair 1: <img_x>:<img_x'>
Image Pair 2: <img_y>:[missing_img]

<Candidate Images>

Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "answer": "missing image choice here"
}
\end{verbatim}
\end{promptbox}
\newpage
\begin{promptbox}[colback=blue!10, colframe=blue!60!black, title=Symbolic Analogy (Induction)]{}
\scriptsize
Below is a 3x3 matrix of abstracted symbols. The symbols follow a certain rule or pattern in rows. Your task is to infer the missing symbol. \\
\begin{verbatim}
Incomplete Matrix: <incomplete_matrix>

Your response should strictly follow the JSON dict format:

{
    "answer": "missing symbol here"
}
\end{verbatim}
\end{promptbox}



\begin{promptbox}[colback=blue!10, colframe=blue!60!black, title=Symbolic Analogy (Automate)]{}
\scriptsize
Below is a 3x3 matrix of abstracted symbols. The symbols follow a certain rule or pattern in rows. Your task is to infer the missing symbol. \\
\begin{verbatim}
Incomplete Matrix: <incomplete_matrix>

Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "answer": "missing symbol here"
}
\end{verbatim}
\end{promptbox}

\begin{promptbox}[colback=blue!10, colframe=blue!60!black, title=Symbolic Analogy (Abduction)]{}
\scriptsize
Below is a 3x3 matrix of abstracted symbols. The symbols follow a certain rule or pattern in rows. Your task is to infer the relational pattern. \\
\begin{verbatim}
Incomplete Matrix: <incomplete_matrix>

Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "pattern": "relational pattern here"
}
\end{verbatim}
\end{promptbox}


\begin{promptbox}[colback=blue!10, colframe=blue!60!black, title=Symbolic Analogy (Deduction)]{}
\scriptsize
Below is a 3x3 matrix of abstracted symbols. The symbols follow a certain rule or pattern in rows. Your task is to infer the missing symbol. Here's the relational pattern: <pattern>\\
\begin{verbatim}
Incomplete Matrix: <incomplete_matrix>

Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "answer": "missing symbol here"
}
\end{verbatim}
\end{promptbox}

\newpage



\begin{promptbox}[colback=yellow!10, colframe=yellow!40!brown, title=List Function ICL (Induction)]{}
\scriptsize
Below are several examples of input and output lists. There exists an unified function that maps the input list to the output list. \\
\begin{verbatim}

Input 1: <input_list1>, Output 1: <output_list1>
Input 2: <input_list2>, Output 2: <output_list2>
Input 3: <input_list3>, Output 3: <output_list3>

Please infer the output list for the new input list below:
New Input: <new_input_list>

Your response should strictly follow the JSON dict format:

{
    "answer": "output list here"
}
\end{verbatim}
\end{promptbox}


\begin{promptbox}[colback=yellow!10, colframe=yellow!40!brown, title=List Function ICL (Automate)]{}
\scriptsize
Below are several examples of input and output lists. There exists an unified function that maps the input list to the output list. \\
\begin{verbatim}

Input 1: <input_list1>, Output 1: <output_list1>
Input 2: <input_list2>, Output 2: <output_list2>
Input 3: <input_list3>, Output 3: <output_list3>

Please infer the output list for the new input list below:
New Input: <new_input_list>

Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "answer": "output list here"
}
\end{verbatim}
\end{promptbox}


\begin{promptbox}[colback=yellow!10, colframe=yellow!40!brown, title=List Function ICL (Abduction)]{}
\scriptsize
Below are several examples of input and output lists. There exists an unified function that maps the input list to the output list. \\
\begin{verbatim}

Input 1: <input_list1>, Output 1: <output_list1>
Input 2: <input_list2>, Output 2: <output_list2>
Input 3: <input_list3>, Output 3: <output_list3>

Please infer the mapping function in python.
Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "function": "python function here"
}
\end{verbatim}
\end{promptbox}


\begin{promptbox}[colback=yellow!10, colframe=yellow!40!brown, title=List Function ICL (Deduction)]{}
\scriptsize
Below are several examples of input and output lists. There exists an unified function that maps the input list to the output list. The python code for the function is: <function> \\
\begin{verbatim}

Input 1: <input_list1>, Output 1: <output_list1>
Input 2: <input_list2>, Output 2: <output_list2>
Input 3: <input_list3>, Output 3: <output_list3>

Please infer the output list for the new input list below:
New Input: <new_input_list>

Your response should strictly follow the JSON dict format:

{
    "reasoning":"reasoning steps here",
    "answer": "output list here"
}
\end{verbatim}
\end{promptbox}

\newpage



\begin{promptbox}[colback=blue!20!green!5, colframe=green!55!blue!80, title=SALT ICL (Induction)]{}
\scriptsize
You are required to translate english sentences to an artificial language.
The translation involves both vocabulary mapping and syntax rules transition. Below are translation examples:\\
\begin{verbatim}

English 1: <english_1>, Translation 1: <translation_1>
English 2: <english_2>, Translation 2: <translation_2>
English 3: <english_3>, Translation 3: <translation_3>
English 4: <english_4>, Translation 4: <translation_4>

Please translate this sentence: <english_new>
Your response should strictly follow the JSON dict format:
{
    "translation": "translated sentence here"
}
\end{verbatim}
\end{promptbox}

\begin{promptbox}[colback=blue!20!green!5, colframe=green!55!blue!80, title=SALT ICL (Automate)]{}
\scriptsize
You are required to translate english sentences to an artificial language.
The translation involves both vocabulary mapping and syntax rules transition. Below are translation examples:\\
\begin{verbatim}

English 1: <english_1>, Translation 1: <translation_1>
English 2: <english_2>, Translation 2: <translation_2>
English 3: <english_3>, Translation 3: <translation_3>
English 4: <english_4>, Translation 4: <translation_4>

Please translate this sentence: <english_new>
Your response should strictly follow the JSON dict format:
{
    "reasoning":"reasoning steps here",
    "translation": "translated sentence here"
}
\end{verbatim}
\end{promptbox}

\begin{promptbox}[colback=blue!20!green!5, colframe=green!55!blue!80, title=SALT ICL (Abduction)]{}
\scriptsize
You are required to study translations from english sentences to an artificial language.
The translation involves both vocabulary mapping and syntax rules transition. Below are translation examples:\\
\begin{verbatim}

English 1: <english_1>, Translation 1: <translation_1>
English 2: <english_2>, Translation 2: <translation_2>
English 3: <english_3>, Translation 3: <translation_3>
English 4: <english_4>, Translation 4: <translation_4>

Please infer the word mappings and syntax rules.
Your response should strictly follow the JSON dict format:
{
    "reasoning":"reasoning steps here",
    "vocabulary": "word mappings here",
    "grammar": "syntax rules here"
}
\end{verbatim}
\end{promptbox}

\begin{promptbox}[colback=blue!20!green!5, colframe=green!55!blue!80, title=SALT ICL (Deduction)]{}
\scriptsize
You are required to translate english sentences to an artificial language.
The translation involves both vocabulary mapping and syntax rules transition. Vocabulary mapping: <vocab>; Syntax rules: <grammar>. Below are translation examples:\\
\begin{verbatim}

English 1: <english_1>, Translation 1: <translation_1>
English 2: <english_2>, Translation 2: <translation_2>
English 3: <english_3>, Translation 3: <translation_3>
English 4: <english_4>, Translation 4: <translation_4>

Please translate this sentence: <english_new>
Your response should strictly follow the JSON dict format:
{
    "reasoning":"reasoning steps here",
    "translation": "translated sentence here"
}
\end{verbatim}
\end{promptbox}

\section{Full Results}
\label{app:full_result}
The detailed LLM performances in our analogy environement and in-context learning benchmarks is presented in tables below:
\begin{itemize}
    \item Table \ref{tab:textual-mcq}: Textual Analogy (E-KAR)-MCQ
    \item Table \ref{tab:visual-mcq}: Visual Analogy (VASR)-MCQ
    \item Table \ref{tab:symbolic-mcq}: Symbolic Analogy (RAVEN)-MCQ
    \item Table \ref{tab:textual-ftg}: Textual Analogy (E-KAR)-FTG
    \item Table \ref{tab:symbolic-ftg}: Symbolic Analogy (RAVEN)-FTG
    \item Table \ref{tab:list-mcq}: List Function ICL-MCQ
    \item Table \ref{tab:list-ftg}: List Function ICL-FTG
    \item Table \ref{tab:salt-mcq}: SALT ICL-MCQ
    \item Table \ref{tab:salt-ftg}: SALT ICL-FTG
\end{itemize}
\setlength{\tabcolsep}{12pt}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Qwen-2.5-7b} & Induction & 65.93 & 56.32 & 40.93 & 52.64 \\
 & Automate & 68.45 & 52.87 & 39.11 & 51.36 \\
 & Abduction+Deduction & 69.40 & 54.71 & 44.35 & 54.33 \\ \midrule
\multirow{3}{*}{Qwen-2.5-72b} & Induction & 76.03 & 68.74 & 46.77 & 61.86 \\
 & Automate & 75.39 & 67.13 & 49.60 & 62.26 \\
 & Abduction+Deduction & 76.97 & 70.34 & 51.01 & 64.34 \\ \midrule
\multirow{3}{*}{Llama-3.1-70b} & Induction & 64.67 & 56.32 & 37.90 & 51.12 \\
 & Automate & 73.19 & 64.14 & 46.37 & 59.38 \\
 & Abduction+Deduction & 73.19 & 62.53 & 46.17 & 58.73 \\ \midrule
\multirow{3}{*}{Llama-3.1-405b} & Induction & 74.76 & 64.83 & 43.95 & 59.05 \\
 & Automate & 77.92 & 68.97 & 52.62 & 64.74 \\
 & Abduction+Deduction & 73.50 & 67.13 & 50.60 & 62.18 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 66.88 & 54.94 & 36.49 & 50.64 \\
 & Automate & 63.72 & 55.40 & 40.32 & 51.52 \\
 & Abduction+Deduction & 63.41 & 56.78 & 40.73 & 52.08 \\ \midrule
\multirow{3}{*}{GPT-4o} & Induction & 73.82 & 64.83 & 44.15 & 58.89 \\
 & Automate & 69.72 & 63.22 & 48.59 & 59.05 \\
 & Abduction+Deduction & 73.50 & 68.74 & 51.61 & 63.14 \\ \bottomrule
\end{tabular}
\caption{LLM performances on textual analogy dataset (E-KAR) in MCQ task format.}
\label{tab:textual-mcq}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Gemini-1.5-flash} & Induction & 38.90 & 30.59 & 29.38 & 33.11 \\
 & Automate & 54.07 & 49.83 & 47.50 & 50.71 \\
 & Abduction+Deduction & 59.34 & 47.73 & 48.75 & 51.89 \\ \midrule
\multirow{3}{*}{Gemini-1.5-pro} & Induction & 50.55 & 45.28 & 43.13 & 46.55 \\
 & Automate & 65.49 & 54.37 & 59.06 & 59.24 \\
 & Abduction+Deduction & 65.71 & 57.34 & 59.38 & 60.65 \\ \midrule
\multirow{3}{*}{Gemini-2.0-flash} & Induction & 52.31 & 47.38 & 47.50 & 49.07 \\
 & Automate & 63.96 & 60.84 & 61.56 & 62.06 \\
 & Abduction+Deduction & 67.47 & 59.44 & 65.62 & 63.62 \\ \midrule
\multirow{3}{*}{Pixtral-12b} & Induction & 32.53 & 24.30 & 17.81 & 25.54 \\
 & Automate & 33.85 & 32.87 & 30.94 & 32.74 \\
 & Abduction+Deduction & 41.54 & 39.34 & 35.31 & 39.12 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 34.73 & 26.57 & 25.31 & 29.03 \\
 & Automate & 51.43 & 41.61 & 40.00 & 44.54 \\
 & Abduction+Deduction & 51.21 & 41.43 & 44.06 & 45.36 \\ \midrule
\multirow{3}{*}{GPT-4o} & Induction & 54.95 & 47.90 & 46.56 & 49.96 \\
 & Automate & 66.37 & 55.07 & 59.06 & 59.84 \\
 & Abduction+Deduction & 68.13 & 59.97 & 60.94 & 62.95 \\ \bottomrule
\end{tabular}
\caption{LLM performances on visual analogy dataset (VASR) in MCQ task format.}
\label{tab:visual-mcq}
\end{table*}

\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Qwen-2.5-7b} & Induction & 29.10 & 19.26 & 11.39 & 19.94 \\
 & Automate & 29.10 & 20.35 & 14.43 & 21.29 \\
 & Abduction+Deduction & 30.10 & 21.43 & 16.46 & 22.64 \\ \midrule
\multirow{3}{*}{Qwen-2.5-72b} & Induction & 40.55 & 28.57 & 18.99 & 29.39 \\
 & Automate & 51.24 & 38.74 & 26.08 & 38.76 \\
 & Abduction+Deduction & 54.48 & 43.72 & 36.20 & 44.80 \\ \midrule
\multirow{3}{*}{Llama-3.1-70b} & Induction & 38.06 & 28.35 & 18.73 & 28.44 \\
 & Automate & 52.99 & 36.58 & 28.35 & 39.24 \\
 & Abduction+Deduction & 49.50 & 36.36 & 34.43 & 39.95 \\ \midrule
\multirow{3}{*}{Llama-3.1-405b} & Induction & 54.23 & 38.10 & 25.06 & 39.16 \\
 & Automate & 53.48 & 38.53 & 28.35 & 40.11 \\
 & Abduction+Deduction & 64.93 & 47.62 & 37.72 & 50.04 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 36.82 & 22.51 & 15.44 & 24.86 \\
 & Automate & 37.56 & 26.41 & 12.91 & 25.73 \\
 & Abduction+Deduction & 36.32 & 25.11 & 22.78 & 27.96 \\ \midrule
\multirow{3}{*}{GPT-4o} & Induction & 41.79 & 29.87 & 17.22 & 29.71 \\
 & Automate & 58.21 & 41.13 & 35.44 & 44.80 \\
 & Abduction+Deduction & 55.47 & 35.93 & 31.39 & 40.75 \\ \bottomrule
\end{tabular}
\caption{LLM performances on symbolic analogy dataset (RAVEN) in MCQ task format.}
\label{tab:symbolic-mcq}
\end{table*}







% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Qwen-2.5-7b} & Induction & 28.08 & 22.99 & 16.33 & 21.63 \\
 & Automate & 28.71 & 25.98 & 19.56 & 24.12 \\
 & Abduction+Deduction & 27.76 & 23.68 & 17.74 & 22.36 \\ \midrule
\multirow{3}{*}{Qwen-2.5-72b} & Induction & 35.02 & 29.20 & 21.77 & 27.72 \\
 & Automate & 33.75 & 28.74 & 22.18 & 27.40 \\
 & Abduction+Deduction & 34.07 & 29.66 & 21.98 & 27.72 \\ \midrule
\multirow{3}{*}{Llama-3.1-70b} & Induction & 29.02 & 22.07 & 15.32 & 21.15 \\
 & Automate & 32.81 & 23.45 & 19.15 & 24.12 \\
 & Abduction+Deduction & 29.97 & 25.52 & 18.95 & 24.04 \\ \midrule
\multirow{3}{*}{Llama-3.1-405b} & Induction & 28.71 & 24.60 & 16.94 & 22.60 \\
 & Automate & 29.34 & 25.75 & 18.75 & 23.88 \\
 & Abduction+Deduction & 32.18 & 27.59 & 19.76 & 25.64 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 28.08 & 22.99 & 16.33 & 21.63 \\
 & Automate & 29.34 & 25.29 & 20.16 & 24.28 \\
 & Abduction+Deduction & 29.97 & 25.75 & 19.15 & 24.20 \\ \midrule
\multirow{3}{*}{GPT-4o} & Induction & 32.81 & 26.90 & 19.35 & 25.40 \\
 & Automate & 32.18 & 26.21 & 20.77 & 25.56 \\
 & Abduction+Deduction & 31.23 & 27.59 & 20.36 & 25.64 \\ \bottomrule
\end{tabular}
\caption{LLM performances on textual analogy dataset (E-KAR) in FTG task format.}
\label{tab:textual-ftg}
\vspace{3cm}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Qwen-2.5-7b} & Induction & 19.15 & 8.87 & 5.57 & 11.12 \\
 & Automate & 0.75 & 0.87 & 0.00 & 0.56 \\
 & Abduction+Deduction & 1.00 & 2.60 & 1.77 & 1.83 \\ \midrule
\multirow{3}{*}{Qwen-2.5-72b} & Induction & 37.81 & 20.13 & 13.42 & 23.67 \\
 & Automate & 17.91 & 5.41 & 1.52 & 8.18 \\
 & Abduction+Deduction & 25.37 & 13.85 & 8.86 & 15.97 \\ \midrule
\multirow{3}{*}{Llama-3.1-70b} & Induction & 30.35 & 13.20 & 8.10 & 17.08 \\
 & Automate & 18.16 & 7.14 & 4.81 & 9.93 \\
 & Abduction+Deduction & 9.45 & 7.58 & 6.08 & 7.70 \\ \midrule
\multirow{3}{*}{Llama-3.1-405b} & Induction & 42.29 & 20.78 & 13.42 & 25.34 \\
 & Automate & 30.85 & 12.99 & 7.34 & 16.92 \\
 & Abduction+Deduction & 28.61 & 16.45 & 12.15 & 18.98 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 26.37 & 12.34 & 8.61 & 15.65 \\
 & Automate & 11.19 & 4.76 & 2.53 & 6.12 \\
 & Abduction+Deduction & 11.69 & 6.93 & 3.54 & 7.39 \\ \midrule
\multirow{3}{*}{GPT-4o} & Induction & 37.81 & 18.40 & 10.89 & 22.24 \\
 & Automate & 16.17 & 9.09 & 5.82 & 10.33 \\
 & Abduction+Deduction & 25.12 & 14.07 & 9.37 & 16.12 \\ \bottomrule
\end{tabular}
\caption{LLM performances on symbolic analogy dataset (RAVEN) in FTG task format.}
\vspace{3cm}
\label{tab:symbolic-ftg}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Qwen-2.5-7b} & Induction & 47.69 & 33.57 & 29.87 & 37.28 \\
 & Automate & 60.42 & 44.21 & 39.24 & 48.24 \\
 & Abduction+Deduction & 64.81 & 32.97 & 38.23 & 49.36 \\ \midrule
\multirow{3}{*}{Qwen-2.5-72b} & Induction & 65.05 & 46.34 & 40.51 & 50.96 \\
 & Automate & 69.44 & 52.25 & 44.81 & 55.84 \\
 & Abduction+Deduction & 68.52 & 49.41 & 45.57 & 54.80 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 59.03 & 45.86 & 33.92 & 46.64 \\
 & Automate & 61.81 & 53.90 & 42.28 & 52.96 \\
 & Abduction+Deduction & 66.20 & 52.01 & 44.80 & 54.64 \\ \bottomrule
\end{tabular}
\caption{LLM performances on List Function dataset in MCQ task format.}
\label{tab:list-mcq}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Qwen-2.5-7b} & Induction & 65.18 & 36.24 & 9.75 & 37.60 \\
 & Automate & 54.59 & 24.71 & 7.50 & 29.36 \\
 & Abduction+Deduction & 57.88 & 24.71 & 8.00 & 30.64 \\ \midrule
\multirow{3}{*}{Qwen-2.5-72b} & Induction & 79.06 & 49.65 & 17.25 & 49.28 \\
 & Automate & 74.59 & 44.94 & 13.75 & 45.04 \\
 & Abduction+Deduction & 80.47 & 47.53 & 17.75 & 48.32 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 75.53 & 43.53 & 13.75 & 44.88 \\
 & Automate & 65.65 & 32.94 & 10.50 & 36.88 \\
 & Abduction+Deduction & 73.41 & 41.65 & 14.00 & 43.60 \\ \bottomrule
\end{tabular}
\caption{LLM performances on List Function dataset in FTG task format.}
\label{tab:list-ftg}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Qwen-2.5-7b} & Induction & 21.50 & 16.75 & 10.00 & 16.08 \\
 & Automate & 36.00 & 31.75 & 19.75 & 29.17 \\
 & Abduction+Deduction & 35.25 & 31.50 & 22.75 & 29.83 \\ \midrule
\multirow{3}{*}{Qwen-2.5-72b} & Induction & 58.50 & 54.25 & 52.00 & 54.92 \\
 & Automate & 61.25 & 60.00 & 60.00 & 60.42 \\
 & Abduction+Deduction & 60.75 & 62.00 & 63.25 & 62.00 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 63.50 & 56.75 & 39.75 & 53.33 \\
 & Automate & 53.00 & 47.25 & 43.50 & 47.92 \\
 & Abduction+Deduction & 64.75 & 61.25 & 53.25 & 59.75 \\ \bottomrule
\end{tabular}
\caption{LLM performances on SALT dataset in MCQ task format.}
\label{tab:salt-mcq}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[]
\centering
\scriptsize
\begin{tabular}{clcccc}
\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{Pipeline}} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Total} \\ \midrule
\multirow{3}{*}{Qwen-2.5-7b} & Induction & 37.50 & 15.25 & 2.00 & 18.25 \\
 & Automate & 29.00 & 17.25 & 6.75 & 17.67 \\
 & Abduction+Deduction & 29.25 & 16.00 & 6.50 & 17.25 \\ \midrule
\multirow{3}{*}{Qwen-2.5-72b} & Induction & 51.25 & 33.50 & 19.50 & 34.75 \\
 & Automate & 38.00 & 35.25 & 26.75 & 33.33 \\
 & Abduction+Deduction & 33.50 & 30.25 & 30.00 & 31.25 \\ \midrule
\multirow{3}{*}{GPT-4o-mini} & Induction & 66.25 & 25.00 & 17.25 & 36.17 \\
 & Automate & 34.00 & 25.50 & 20.00 & 26.50 \\
 & Abduction+Deduction & 38.75 & 34.00 & 25.75 & 32.83 \\ \bottomrule
\end{tabular}
\caption{LLM performances on SALT dataset in FTG task format.}
\label{tab:salt-ftg}
\end{table*}

\end{document}


