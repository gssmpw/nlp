% \section{Related Literature}\label{sec:background}
% \subsection{Path Planning \& Navigation of 2D Robots}
%Robots operating in GPS-denied environments rely on exteroceptive sensors such as IMU~\cite{north2012improved}, cameras~\cite{seok2020rovins}, LiDARs~\cite{qin2020lins,caspers2021robotic}, and range sensors~\cite{zhang2021rgb} for path planning and autonomous navigation. Researchers focus on two navigation objectives: \textbf{(i)} exploring and mapping unknown areas~\cite{wang2019autonomous} while simultaneously performing self-localization~\cite{tranzatto2022team}, and \textbf{(ii)} reaching a goal by avoiding obstacles~\cite{hassani2018robot}. For real-time exploration and trajectory generation in an unknown environment, For trajectory generation in an unknown environment, researchers utilize viewpoint planning~\cite{okada2015exploration} that aims to maximize information gain from each viewpoint, though it may not guarantee energy-efficient paths. Another approach is receding horizon~\cite{ryll2019efficient} or next-best-view~\cite{bircher2016receding} planner, which adaptively generates safe trajectories within the robot’s field of view. This strategy effectively decouples local collision avoidance from global navigation but may struggle in unstructured environments when the global planner fails~\cite{baxevani2022resilient}. The frontier approach~\cite{yamauchi1997frontier,gonzalez2002navigation}, though time intensive, is more effective for discovering full-space by iteratively expanding the horizon of explored space.



% \vspace{-1mm}
% Robots operating in 2D environments (UGVs/ASVs) rely on visual-inertial state estimates~\cite{north2012improved,seok2020rovins}, a local/partial map of the surroundings~\cite{qin2020lins,caspers2021robotic}, and/or range sensors~\cite{zhang2021rgb} for obstacle avoidance and autonomous navigation~\cite{wang2023maritime,hassani2018robot}.


\section{Background \& Related Work}\label{sec:background}
\subsection{Path Planning \& Exploration by Mobile Robots}
A substantial body of literature has explored various tree search and probabilistic sampling methods for \textit{source-to-goal} path planning and navigation by UGVs and ASVs~\cite{liu2023path,sanchez2021path}. Breadth-First Search (BFS) and Depth-First Search (DFS) are generally adopted for \textit{uninformed} search, whereas Dijkstra's algorithm and A* are widely used for \textit{informed} search~\cite{cormen2022introduction}. DFS is generally more memory-efficient than BFS and better suited for dynamic obstacles, whereas Dijkstra’s algorithm is ideal when finding the shortest path is essential. Nevertheless, these methods employ exhaustive search, hence can be computationally demanding in complex spaces. The family of A* and D* algorithms~\cite{hart1968formal,ferguson2006using,al2011d} improves upon the computational efficiency by incorporating heuristics to reduce unnecessary exploration while guaranteeing the shortest path.

Beyond a discretized grid-like environment, sampling-based algorithms are better suited for continuous high-dimensional spaces~\cite{cormen2022introduction}. Probabilistic Roadmaps (PRM)~\cite{kavraki1998analysis} and Rapidly-exploring Random Tree (RRT)~\cite{li2021adaptive} methods are classical methods that build a tree of random samples from the configuration space, starting from the initial position and incrementally growing toward the goal~\cite{xu2024recent}. PRM is best for multi-query planning in static environments, whereas RRT is fast and suitable for real-time planning but produces suboptimal paths. While some variants, such as RRT*, are asymptotically optimal, although at a higher computational cost~\cite{shi2014spark}.

% Tree search-based methods (\eg, BFS, DFS) discretize the map into a grid and plan a collision-free path~\cite{cormen2022introduction}.  %Dijkstra~\cite{dijkstra2022note} is another classical algorithm that provides optimal path between two nodes. The A* algorithm~\cite{hart1968formal} further improves upon the computational efficiency by incorporating a heuristics to reduce the number of nodes explored. Variants of A* such as LPA*~\cite{koenig2005fast}, D*~\cite{ferguson2006using}, D* Lite~\cite{al2011d}, Theta*~\cite{daniel2010theta}, ARA*~\cite{likhachev2003ara}, Hybrid A*~\cite{dolgov2010path}...
% Sampling-based methods such as Rapidly-Exploring Random Tree (RRT)... These algorithms have two limitations in general: (i) they do not take into account the robot dynamics when constructing global maps; and (ii) the computational demand limits their online implementation on mobile robotic platforms. In contrast, algorithms that rely on direct sensory feedback and local decision-making such as Bug algorithms, are more suitable for planning and navigation in unknown dynamic environment.

% \JI{talk about local and online path planners, with global and local maps}
% \JI{cite both UGV and ASV path planning papers}
% \JI{also where is A*, RRT, Disktra, etc!!!!!. We need to mention them and then converge to the second paragraph on Bug algorithms}

% If the robot operates in a known static environment \ie, a map is given, then a global motion planner can generate the complete trajectory from current location to destination~\cite{chaari2017design}. However, in absence of such a global map, the robot uses real-time sensory data to detect obstacles within its field of view (FOV) and navigates toward the goal by following the boundaries of detected obstacles. For goal-directed navigation, the problem is framed as \textit{point A to point B navigation}~\cite{bagnell2010learning}

%For \textit{source-to-goal} path planning, Bug algorithm~\cite{mcguire2019comparative} are a family of locally optimal techniques for autonomous navigation. The first Bug algorithm, developed by Lumelsky~\etal~\cite{lumelsky1986dynamic}, is referred to as Bug0 or common sense algorithm. While it provides a basic solution, it can get trapped in complex environments, such as overlapping U-shaped mazes, resulting in navigation failure. Bug1 overcomes these limitations by circumnavigating each obstacle; however, it is less intuitive and results in unnecessary long paths.

On the other hand, Bug0~\cite{lumelsky1986dynamic} and Bug1~\cite{lumelsky1987path} algorithms employ a locally optimal strategy of following the line-of-sight and avoiding obstacles along their boundaries. Bug2~\cite{lumelsky1987path} optimizes the path using an imaginary \emph{m-line} (main line) to avoid unnecessary loops. Besides, Alg1 and Alg2 approaches~\cite{sankaranarayanan1990new} leverage memory of the previous obstacle \emph{hit points} to search for alternative routes, while DistBug~\cite{kamon1997sensory} makes it memory efficient by retaining the most recent hit points only. M-Bug~\cite{mohsen2019new} minimizes the path length by dynamically adjusting the {m-line}. Integration of range sensors further allows the robot to sense nearby obstacles and choose a short-cut while maintaining minimum deviation from m-line, as demonstrated in TangentBug~\cite{kamon1996new}, WedgeBug~\cite{laubach1999autonomous}, and VisBug~\cite{lumelsky1990incorporating}.





In contrast, I-Bug~\cite{taylor2009bug} solely relies on signal intensity from the goal to guide navigation. EgressBug~\cite{guruprasad2012egressbug} offers an escape mechanism, allowing the robot to recover from deadlocks. PointBug~\cite{buniyamin2011simple} introduces the notion of \emph{sudden points}, \ie, turning points on the outer perimeter of obstacles, which is further improved in E-bug methods~\cite{Meddah2015ebug,mistri2022automated}. More sophisticated algorithms such as APF-Bug~\cite{wyrkabkiewicz2020local} assess ground quality to avoid immobilization in complex terrain obstacles and increase the success rate of reaching the goal.

% \JI{Missed} M-Bug,   APF-BUG - perhaps a few more.
% These algorithms address many shortcomings of the original approach, offering faster and more robust solutions for obstacle avoidance and navigation in unknown environments.

% Full width banner figure for Sec.III, floated up
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{res/Fig.Banner.pdf}%
    \vspace{-1mm}
    \caption{Our \textit{Rover Master} platform designed for ClipRover is shown: (\textbf{a}) Back view, showing the single board computer (SBC) and electronics stack including a brushless motor speed controller, a flight controller with IMU, and a power distribution board;
    (\textbf{b}) Front view, showing the camera for zero-shot navigation; (\textbf{c}) An alternative design, demonstrating the flexibility of this platform, configured with revised wheels, chassis, and additional manipulators for potential field robotics applications.}
    \label{fig:system_design}
    \vspace{-1mm}
\end{figure*}


\subsection{Zero-shot Learning \& Vision-Language Navigation}

\label{sec:related-work:VLN}

Zero-shot learning enables autonomous robots to identify unseen objects and make navigation decisions in unfamiliar environments~\cite{Guan2024LOCZSONLO}. Researchers use visual features~\cite{yang2018visual}, knowledge graphs~\cite{pal2021learning}, semantic embeddings~\cite{zhao2023zero}, and spatial appearance attributes~\cite{ma2024doze} to encode information of object classes into the search space. More recently, text-based descriptions or human instructions are combined with vision to improve servoing, as demonstrated in LM-Nav~\cite{shah2023lm} and InstructNav~\cite{long2024instructnav}. Among CLIP~\cite{radford2021learning}-based frameworks, CoW~\cite{gadre2022clip} presents a trajectory planning strategy using frontier-based exploration. ClipNav~\cite{dorbala2022clipnav} designs a \textit{costmap} to further improve obstacle avoidance during exploration. Other approaches such as ESC~\cite{zhou2023esc} and VLMaps~\cite{huang2023visual} use prompts to translate action commands into a sequence of open-vocabulary navigation tasks for planning. Besides, CorNav~\cite{liang2024cornav} includes environmental feedback in the zero-shot learning process to dynamically adjust navigation decisions on the fly.
% Besides, ESC~\cite{zhou2023esc} adopts a prompt-based grounding and an LLM for room and object reasoning and thus facilitates unknown scene understanding.

Contemporary works on vision-language navigation (VLN) focus on enabling robots to understand language instructions for interactive task planning. VLN algorithms integrates visual information (\eg, recognizing objects, obstacles) and language instructions to plan a trajectory or action sequence for task execution. This is achieved by first generating semantic tags into the SLAM pipeline~\cite{huang2023visual,Gadre2022CoWsOP, Guan2024LOCZSONLO} to generate a 3D map. Sbsequently, the resulting augmented map is used to support online navigation to perform zero-shot semantic tasks~\cite{yokoyama2023vlfmvisionlanguagefrontiermaps}. These works primarily focus on adding semantic information to existing mapping and exploration techniques~\cite{huang2023visual,krantz2020navgraph}, do not participate in the path planning process.
% Zero-shot learning does not participate in guiding the robot during the initial mapping run. The performance of task execution with pre-generated maps on a dynamically changing scene remains to be validated.

In contrast, \textit{node graph} based VLNs~\cite{anderson2018visionandlanguagenavigationinterpretingvisuallygrounded, kiran2022spatialrelationgraphgraph,Savarese-RSS-19} construct a graph of the active map where each node represents a free location and edges stand for directly navigable paths. Dynamic expansion of a node graph is then achieved with the help of precise localization, typically achieved by additional sensors. Experiments show that these algorithms suffer from miscorrelating existing nodes when revisiting an explored area, and also in dynamically changing scenes. To address this, researchers are exploring zero-shot learning aided observation of unknown spaces~\cite{yokoyama2023vlfmvisionlanguagefrontiermaps}. Techniques like odometry, stereo depth, and LiDAR sensors are used alongside a pretrained vision-language model to achieve simultaneous mapping and exploration -- demonstrating a clear advantage regarding the efficiency of reaching the prompted goal.

The existing zero-shot learning and VLN approaches rely on dedicated sensors to construct a global map on the first run. Subsequent motion decisions are generated by a traditional trajectory planner. Online planning and navigation in dynamic and unknown spaces without a prior map is still an open problem, which we attempt to address in this paper.

% \YZ{
%     Reviewers might expect us to mention autonomous driving as related work. We might need to clarify the differences to avoid their confusion.
% }


%\JI{Perhaps we can talk about the table here and mention where ClipRover Fits}