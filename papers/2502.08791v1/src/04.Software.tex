\section{ClipRover: Navigation Pipeline}\label{sec:arch}

Recent advancements in vision-language models (VLMs) have demonstrated their potential for spatial reasoning and awareness capabilities~\cite{chen2024spatialvlm,dorbala2022clipnav}. Different from traditional approaches,  this study explores the feasibility of incorporating VLMs directly into a robot's autonomy pipeline to facilitate real-time exploration and target identification. To this end, we introduce a modular pipeline that integrates stages for perception, planning, and navigation. The core computational components of the proposed ClipRover navigation pipeline are depicted in Fig.~\ref{fig:arch}.

The proposed pipeline comprises three main stages.
The \li{frontend} 
    processes raw input frames, divides them into tiles, and encodes these tiles into embeddingsâ€”numerical vectors representing semantic meanings; In our implementation, the CLIP vision encoder is employed as the frontend.
Then, \li{middlewares}
    components take the visual embeddings generated by the frontend as input and produce scores that carry specific semantic interpretations. These scores are typically derived by correlating the input embeddings with the middleware's internal database. The meanings of these scores can vary depending on the application's requirements. In this study, the middleware generates three types of scores: \textit{navigability}, \textit{familiarity}, and \textit{target confidence}.
Lastly, at the \li{backend},
    those scores from middlewares are used to make motion decisions. It is designed to be adaptable, allowing the integration of different algorithms tailored to various applications and environments. For this study, a minimal backend was implemented with three operational modes: basic navigation, look-around, and target lock. These modes are dynamically activated or deactivated based on the provided scores.

% In specific, the navigability score is derived from correlating against a set of
% \textit{text prompts} that describes areas that are clean and navigable (positive scores) or areas that are cluttered by obstacles (negative scores) and the target confidence is derived from correlating against a set of \textit{text prompts} that describe the target. Specially, the familiarity score is cross-correlated against a database of \textit{visual embeddings} which is accumulated in real time during the same task. The familiarity score is used to make the robot prefer unexplored areas which typically have a lower familiarity score.

% Our work is backed by the idea that vision-language model possesses certain level of spatial awareness, as reported in several researches

% Our system is divided into several components that can be chained in a pipeline. We define these components as follows:

% \begin{figure}
%     \centering
%     \includegraphics[width=0.49\columnwidth]{res/Visual-Perception.jpg}
%     \includegraphics[width=0.49\columnwidth]{res/Visual-LookAround.jpg}
%     \caption{
%         \textbf{Left}: Rendering of the perception results during a mission. Green tiles represents navigable space, blue tiles represents target (if identified), and red tiles represents non-navigable space.
%         \textbf{Right}: Rendering of a lookaround operation. Dashed lines are used indicate candidate directions to turn to \textit{afterwards}. Detailed explanation for color codings and symbols are provided in Fig.~\ref{fig:exp}.
%     }
%     \label{fig:visual}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{res/Visual.pdf}
%     \caption{
%         Perception results from the robot's own vision camera. The green tiles represent navigable space, the blue tile represents an identified target, and the red tiles represent non-navigable space.
%         \JI{Agree with Sanjeev's comments that you can add the clear picture and real robot's picture here}
%     }
%     \label{fig:visual}
% \end{figure}

\subsection{Visual Perception Frontend}
In the frontend, raw camera frames are sliced into six; the slicing strategies are discussed in Sec.~\ref{sec:impl/slicing}. Each tile represents a spatial location in robot's FOV, which are scaled and processed by the CLIP's visual encoder. As shown in Fig.~\ref{fig:arch}\,f, each frame is sliced into $N=6$ tiles and then rearranged into a tensor of shape $N\times 3 \times H \times W$, with $H$ and $W$ are the tile height and width in pixels. The encoder processes these inputs and generates an $N\times D$ embedding vector for each tile, where $D$ is the dimensionality of each prediction vector ($D=512$ in the CLIP model).

Additionally, the standard deviation for each tile is computed and combined with the model's predictions. This metric serves as an indicator of the \textit{amount of information} present in each tile, proving particularly useful in scenarios where the robot encounters feature-poor, uniformly colored objects such as walls, doors, or furniture. In such instances, an \textbf{abnormally} low standard deviation suggests that the vision encoder's output may lack reliability.

%\JI{you should refer to Fig5 and give some examples or explanations on how to interpret the embedding vectors}

\begin{figure}
    \centering
    \vspace{-2mm}
    \includegraphics[width=\columnwidth]{res/Dataflow.pdf}%
    \vspace{-2mm}
    \caption{
        An overview of ClipRover architecture; the \textit{primary}
        data flow and \textit{supplementary} sensory feedback are in black and red arrows, respectively. The camera frame-slicing strategy is shown in (\textbf{f}); each frame is sliced into two rows (NEAR, FAR) and three columns (\underline{L}EFT, \underline{C}ENTER, and \underline{R}IGHT). Each of the six \textit{tiles} is encoded into a separate visual embedding vector for perception.
    }
    %\label{fig:segmentation-multi}
    %\label{fig:slicing}
    \label{fig:arch}
    \vspace{-3mm}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{res/Correlation.pdf}%
    \vspace{-1mm}
    \caption{Detailed examples of the proposed \textit{correlation middlewares} are illustrated; the circled cross symbol denotes the inner product of broadcasted vectors. (\textbf{a}) A navigable clean floor is encoded and correlated with the \textit{navigability} database, where green rows (positive prompts) yield higher scores than red rows (negative prompts); the resulting positive final score indicates the space is navigable. (\textbf{b}) A toy bear (the target) is encoded and correlated with the target database, where blue rows (positive prompts) produce higher scores than red rows (negative prompts); the resulting positive final score confirms the target's presence. (\textbf{c}) A paper box is encoded and correlated with both databases; it corresponds to neither a navigable space nor the target, both scores are negative, indicating the space is not navigable and no target is present. [Best viewed digitally at $2\times$ zoom.]
        }
    \label{fig:correlation}
    \vspace{-2mm}
\end{figure*}


\subsection{Navigability Middleware: Vision-Language Correlation}
To distinguish navigable spaces from non-navigable ones, we designed a set of \textit{positive prompts} describing clean and navigable environments, such as: \Prompt{A photo of a \SttPrompt{flat\DIV open\DIV wide\DIV clear} \ObjPrompt{floor\DIV ground\DIV hallway}}, and a set of \textit{negative prompts} describing spaces that are cluttered by obstacles, such as: \Prompt{A \DscPrompt{cropped\DIV bad\DIV imcomplete} photo of a \SttPrompt{blocked\DIV messy\DIV cluttered} \ObjPrompt{scene\DIV space}} and \Prompt{A photo of a \SttPrompt{large\DIV way blocking} \ObjPrompt{object\DIV item}}. As shown in Fig.~\ref{fig:correlation}\,a, a clear floor is identified as navigable space, while the cluttered scene in Fig.~\ref{fig:correlation}c is accurately categorized as non-navigable.

For target discovery, a similar set of text prompts is used to define the \textit{target} of a task. In our experiments, we use a toy bear (see Fig.~\ref{fig:env}\,b) as the discovery target due to its uniqueness in the scene. We design a set of prompts that describe the target, \eg~\Prompt{A photo of a \SttPrompt{brown\DIV toy} \ObjPrompt{bear\DIV teddy bear}}. We also design a set of negative prompts that describe generic objects to filter out false positives, \eg~ \Prompt{A photo of an \SttPrompt{unknown} \ObjPrompt{item\DIV scene\DIV object}}. As shown in Fig.~\ref{fig:correlation}\,b, the \textit{target prompt} accurately identified the toy bear, while negative prompts effectively suppressed false positives on unrelated objects, such as the paper box in Fig.~\ref{fig:correlation}\,c.

The resulting scores for both navigability and target confidence were computed on a per-tile basis. As depicted in Fig.~\ref{fig:correlation}\,a-c, the CLIP visual encoder generated embeddings for each tile, which were then compared against a prompt database using inner products. The prompt database consisted of pre-encoded text prompts generated by the CLIP text encoder, organized into positive and negative categories. The resulting scores, ranging from $-1.0$ to $1.0$, were determined by the highest absolute score among the prompt matches.

By employing both positive and negative prompts in each database and selecting the final result based on their contrast, the correlation process becomes more robust against fluctuations in absolute correlation values. This approach is particularly beneficial in environments with varying lighting conditions or complex scenes, where all scores may drift upward or downward depending on the quality of the visual input. Moreover, this method eliminates the need for manually setting a fixed threshold, further enhancing its adaptability.

% \JI{
% reviewers would want to know how the {\tt Nav} correlation scores are calculated, some insights are missing here. I did not understand how the vision-language correlation happens}
% % \TODO{\YZ{Will add an algorithm block to explain this.}}
% \YZ{Added Fig.~\ref{fig:correlation}, please have a look!}
% \JI{Also, are we talking about Target, for target discovery cases only? How about the pure exploration case}


\subsection{Familiarity Middleware: Vision-Vision Correlation}\label{sec:arch/decision/familiarity}
In addition to navigability scores, a \textit{familiarity database} is accumulated in real-time to track previously explored spaces. It is constructed with visual embeddings ($512$ dimensional vectors) representing known spaces without storing or using actual images. An incoming visual embedding is considered ``known'' if its correlation score with an existing vector exceeds a predefined threshold. Each new embedding vector is incrementally merged into the familiarity database; we implement the following two strategies for this:
\begin{enumerate}[label={$\arabic*$.},nolistsep,leftmargin=*]
\item Averaging among all vectors that belong to a known point, this involves keeping track of the count of vectors already merged into a known spot ($s$):
$$
v_\text{next} = \frac{s}{s + 1} \cdot v_\text{prev} + \frac{1}{s + 1} \cdot v_\text{new}
$$

\item Performing a rolling average operation upon merging a new vector; this method does not need to keep track of the total count of already merged vectors. Therefore, the vector tends to lean towards newly inserted vectors and gradually ``forget'' older ones. The ``rate of forgetting'' can be controlled by a factor $\lambda$ (a.k.a decay factor):
$$
v_\text{next} = (1 - \lambda) \cdot v_\text{prev} + \lambda \cdot v_\text{new}
$$
\end{enumerate}
When no \textit{known vector} exists in the database, the incoming vector is inserted as a new data point. Eventually, a familiarity score is generated for each perception vector in the database. This score guides navigation by encouraging the robot to prioritize unexplored areas over revisiting familiar ones.


%\JI{dont understand this at all. seems very hand-wavy without actual informative content}
% This score is then used to assist navigation so the robot will prefer to navigate into unfamiliar spaces rather than familiar ones.


%% Commenting for now, don't think we will have space for it
% \begin{algorithm}[t]
%     \small
%     \caption{Motion mapping from confidence matrix}\label{alg:motion}
%     \begin{algorithmic}[1]
%     % \REQUIRE $\text{S} = \text{Matrix}(2, 3)$
%     \Statex \textbf{Input:} $S = \text{Matrix}(2, 3)$
%     \Statex \textbf{Output:} $V_x,~V_y,~V_r$ - Velocities of range $\big[-1,~1\big]$
%     \Statex \textbf{Parameter:} $T$ \codecomment{Confidence Threshold}
%     \Statex \textbf{Parameter:} $w$ \codecomment{Near v.s. Far Weight,~ $0 < w < 1$}
%     \Statex \codecomment{Using Python Style Spread Assignment}
%     \State $S_L,~S_C,~S_R = S\big[1\big] \times w + S\big[0\big] \times (1 - w)$
%     \State \codecomment{Distraction: Which side looks more navigable}
%     \State $d~\gets~S_L - S_R$
%     \If{$S_C \geq T$}
%         \State \codecomment{High confidence to go forward}
%         \State $V_x,~V_y,~V_r~\gets~S_C,~0.0,~0.0$
%     \ElsIf {$S_C > 0$}
%         \State \codecomment{Moderate forward confidence}
%         \State \codecomment{Use Distraction Term to Steer Sideways}
%         \State $V_x, V_y,~V_r~\gets~S_C,~\frac{d}{2},~\frac{d}{2}$
%     \ElsIf {$S_L > 0~\textbf{or}~S_R > 0$}
%         \State \codecomment{Cannot go forward but at least one side is clear}
%         \State \codecomment{Slowly turn to the clear side}
%         \State $V_x, V_y,~V_r~\gets~0.0,~0.0,~d$
%     \Else
%         \State \codecomment{Entire view is blocked, back off slowly}
%         \State $V_x, V_y,~V_r~\gets~-0.1,~0.0,~0.0$
%     \EndIf
%     \end{algorithmic}
% \end{algorithm}



%It overlooks the task performed and attempts to achieve the final goal.


\subsection{Navigation Decision Backend}\label{sec:arch/decision}
Lastly, the decision module generates motion commands according to the information provided by the perception and correlation systems. Specifically, a ``motion mixer'' is introduced as the baseline correlation-to-motion translator. It takes all aforementioned scores for each tile (\ie, \textit{navigability},
\textit{familiarity}, and \textit{standard deviation}) into consideration and makes an intelligent decision. The motion mixer generally prioritizes highly navigable yet less familiar locations while avoiding areas with minimal texture, indicated by low standard deviation values. To handle non-trivial scenarios, the decision module incorporates two additional functionalities: (1) \textit{trap detection}, enabling the robot to identify and escape from potential dead-ends, and (2) \textit{look-around}, allowing it to reorient itself in complex environments. The overall decision-making process and state transitions are illustrated in Fig.~\ref{fig:state-diagram}.



\begin{figure}
% no need for additional padding
    \centering
    \includegraphics[width=0.85\columnwidth]{res/StateDiagram.pdf}%
    \vspace{-1mm}
    \caption{
        A state diagram illustrating the decision-making process, providing a detailed view of Fig.~\ref{fig:arch}c; blocks with dashed borders are other computational blocks in Fig.~\ref{fig:arch}. Dashed arrows represent data/signal propagation, while solid arrows denote conditions and transitions between states.
    }%
    \label{fig:state-diagram}
    \vspace{-1mm}
\end{figure}
% \JI{
%     Need to end this section with a state machine diagram for the navigation decisions. It seems very high-level.
% }
% \YZ{
%     The state machine is actually a ``multiplexer'' shown in Fig.~\ref{fig:arch}c. Would you like to have it in a separate figure?
% }
% [DONE]

\subsubsection{Trap Detection}
In certain scenarios, the robot may encounter a ``dead end'', where no navigable path is visible within its FOV. In rare instances, the vision-language model may generate false positive \texttt{Nav scores} for scenes it does not adequately comprehend. For example, the lack of salient features can lead to incorrect positive scores when the camera is positioned too close to a plain white wall. Such situations are defined as ``trapped'' states, which can occur under two conditions: (a) the proximity switch asserts a halt signal for a specified duration, or (b) the cumulative travel distance, as measured by odometry, falls below a defined threshold over a given period. Empirically, the system flags the robot as trapped if it travels less than $0.2$ meters within the past $5$ seconds.


\subsubsection{Look Around}\label{sec:arch/look-around}
Due to the limited FOV from a single camera, the robot could only see objects in front of it. 
To address this, a ``look around'' mechanism (see Fig.~\ref{fig:visual-homo}) is introduced to enable the robot to gain situational awareness by performing a $360^\circ$ rotation while collecting \texttt{Nav scores} associated with different headings. A Gaussian convolution is then applied to these scores to identify the most navigable direction. When recovering from a ``trapped'' state, the \textit{look around} mechanism prioritizes a direction different from the original heading by a linear factor 
$k$, rewarding headings that deviate from the initial orientation. Additionally, a \textit{look around} behavior is triggered at the start of a new mission to ensure the robot identifies the most promising path for exploration. 



\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{res/LookAroundHomo.jpg}%
    \vspace{-1mm}
    \caption{Rendering of a \textit{look around} operation overlaid on a birds-eye view of the robot; it was able to identify navigable paths (blue bars) apart from obstacles (red bars) based on the proposed visual perception pipeline. The candidate headings are annotated as $C_i$. After the look-around operation, the robot selected $C_0$ as its next heading according to the area of free space. Details on the color codes and symbols are in Fig.~\ref{fig:exp}.
    }
    \vspace{-1mm}
    \label{fig:visual-homo}
\end{figure}

%Detailed examples will be shown in Sec. \ref{sec:exp/familiarity}.

% \JI{Have a flowchart showing the end-to-end computational pipeline}
% [DONE]