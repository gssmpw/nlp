\section{Conclusion}
In this paper, we introduced \textbf{ClipRover}, a novel navigation pipeline designed for simultaneous {exploration} and {target discovery} by autonomous ground robots in unknown environments, leveraging the power of VLMs. Unlike traditional approaches that decouple exploration from path planning and rely on inefficient algorithms, ClipRover enables \textbf{zero-shot inference} and efficient navigation using only monocular vision, without prior maps or target-specific information. To validate our approach, we developed a functional prototype UGV platform named \textbf{Rover Master}, optimized for general-purpose VLN tasks in real-world environments. Extensive evaluations demonstrated that ClipRover outperforms state-of-the-art map traversal algorithms in efficiency and achieves comparable performance to path-planning methods that rely on prior knowledge. The integration of a CLIP-based VLM into a real-time navigation system underscores the potential of ClipRover to advance intelligent robotic exploration. As a modular and flexible framework, ClipRover lays the groundwork for future research in applying VLMs to more complex and dynamic robotic applications such as autonomous warehousing, security patrolling, and smart home assistance.

%In this paper, we approached the problem of \textbf{Simultaneous exploration and target discovery} with a novel framework \textbf{ClipRover}, supported by a custom-designed platform \textbf{Rover Master}. We demonstrated the effectiveness of our system in a series of experiments, showing that our proposed system is capable of performing efficient exploration and target discovery tasks in real-world environments. We compared the performance of our system against state-of-the-art algorithms and showed that our system outperforms in terms of efficiency (against map traversal algorithms) and success rate (against path-finding algorithms). The value in our work extends far beyond the immediate results. As a modular and flexible framework, \textbf{ClipRover} will serve as a foundation for future research in embedding vision-language models into \textbf{realtime} robotic systems.


\section*{Acknowledgments}
This work is supported in part by the National Science Foundation (NSF) grants \#$2330416$, \#$19244$; the Office of Naval Research (ONR) grants \#N000142312429, \#N000142312363; and the University of Florida (UF) ROSF research grant \#$132763$.
