\section{Introduction}
% \JI{what is the problem, why is this important}
Autonomous exploration and target discovery is a fundamental problem in robotics, with applications ranging from search-and-rescue to environmental monitoring and surface mapping~\cite{wang2019autonomous,lluvia2021active}. In 2D exploration, notable implementations are seen in household robots, \eg, vacuum cleaning~\cite{hasan2014path,stachniss2009robotic} and warehouse robotics~\cite{gadd2015framework,keith2024review}. Traditionally, their navigation strategies have relied on various forms of \textit{Bug algorithms} (Bug0, Bug1, Bug2)~\cite{lumelsky1986dynamic,lumelsky1987path}, which are best suited when a robot has \textit{a priori} belief about the environment, and can sense obstacles locally. The principle is to use the \textit{line-of-sight} gradient towards the goal by applying simple obstacle avoidance schemes. Algorithms such as \textit{random walks} or predefined \textit{search patterns} are also utilized to explore an unknown workspace to develop an \textit{occupancy} grid map, on which standard path-planning algorithms~\cite{wang2017autonomous,amigoni2010information} can be deployed for mobile robot navigation. Tree search algorithms like Dijkstra and A*~\cite{candra2020dijkstra} have been popular as they are simple and efficient for limited onboard resources~\cite{xu2017mobile}. For large search spaces, sampling-based methods such as Probabilistic Roadmaps (PRM)~\cite{kavraki1998analysis} and Rapidly-exploring Random Tree (RRT)~\cite{li2021adaptive} and RRT*~\cite{karaman2011sampling} are generally adopted for source-to-target path planning in real-time.

In existing unmanned ground vehicle (UGV) systems, mapping and target discovery are typically conducted as separate processes, relying on different sensors. For instance, a 2D LiDAR is commonly used for obstacle avoidance and mapping~\cite{sun2023autonomous,peng2015obstacle}, while a separate camera is used for scene classification and target detection~\cite{kim2022autonomous}. In a more complicated 3D environment such as an underwater cave exploration, the robot may need to use a combination of sensors such as sonar, LiDAR, and cameras to navigate and explore the environment by generating a semantic representation of the surroundings~\cite{abdullah2023caveseg,rahman2022svin2}. However, an initial map traversal is not feasible in partially observable and dynamic environments, requiring frequent correction or re-initialization~\cite{hernandez2020real,bonnevie2021long}. 



More advanced UGV systems address this by using \textit{online} path planning algorithms~\cite{mistri2022automated,mohsen2019new} with dynamic obstacles and partially observable state space. The major challenge here is to adapt the path when new obstacles intersect the trajectory and/or dynamic changes occur in the environment~\cite{das2020modified}. To this end, visually-guided robots use the semantic understanding and spatio-temporal modeling of dynamic objects in the scene~\cite{humblot2022navigation,martins2020extending}. For visual semantic navigation, the path planner becomes an intermediate representation of a \textit{task planner}~\cite{rana2023sayplan} for application-specific higher-level tasks such as inspection~\cite{wyrkabkiewicz2020local} search-and-rescue~\cite{wang2023maritime}, exploration~\cite{liang2021sscnav}, \etc In recent years, more \textit{interactive} task planners are designed with vision-language navigation (VLN) models~\cite{huang2023visual,Gadre2022CoWsOP} to enable human-like understanding of the robot's tasks from natural language and visual (semantic) cues. These are particularly useful for human-in-the-loop systems~\cite{islam2018person}, \eg, robots operating in warehouses, healthcare, and social settings. VLNs enable robots to interpret the \textit{context} of the tasks from language description and map the corresponding visual \textit{cues} to make context-aware navigation decisions. However, integrating vision-language models (VLMs) into the early stages of map exploration -- remains unexplored. We hypothesize that this integration can enable robots to leverage high-level zero-shot visual information for \textbf{simultaneous exploration and target discovery} without a prior map.


% \draft
%However, to the best of our knowledge, very little research has attempted to integrate vision-language models (VLMs) into the early stages of map exploration tasks. We envision that the integration of VLMs would significantly enhance exploration efficiency while enabling robots to leverage high-level visual information for obstacle avoidance and decision-making. This approach would also eliminate the need for any additional sensors other than a general-purpose monocular RGB camera for visual perception.
 % not only improves the efficiency of the exploration process, but also enables the robot to make use of high-level visual information for obstacle avodiance and decision making, eliminating the need for any additional sensors other than a general-purpose monocular RGB camera.



 
% what do we propose to solve/address those limitations
%In this paper, we leverage the spatial context awareness of VLMs~\cite{chen2024spatialvlm} to guide robotic exploration and target discovery in unknown indoor environments.  The proposed ClipRover framework is designed to harness the power of a general-purpose VLM. It is structured modular around three key stages: \textit{perception}, \textit{correlation}, and \textit{decision}. The modular architecture splits a system-level problem into many fine-grained sub-problems, allowing for incremental improvements to each stage in the future. The proposed architecture also features configurable and flexible correlation middleware design, enabling easy adaptation of different tasks and environments.

This paper presents \textbf{ClipRover}, a novel framework that utilizes the spatial context awareness capabilities of general-purpose VLMs~\cite{chen2024spatialvlm} to guide robotic exploration and target discovery in unknown environments. The framework adopts a modular architecture organized into three key stages: \textit{perception}, \textit{correlation}, and \textit{decision}. This modular design enables the decomposition of complex system-level challenges into smaller, manageable sub-problems, allowing for iterative improvements to each stage. Furthermore, the framework features a highly configurable correlation middleware, offering flexibility to adapt to various tasks and environmental conditions without any prior map or knowledge about the target.

% \JI{mention CLIP, mention your capabilities in a better way. Seems very scattered}

% The architecture is divided into three major parts so they may be further optimized separately in future. The modular design also allows certain parts, \ie, the middlewares, to be deployed in a distributed fashion. This would not only reduce the computational load on the robot's onboard computer, but also enable low latency remote task monitoring under bandwidth-limited situations.

% \draft{
% In order to take our proposed architecture to real-world tests, we set off to develop a better UGV platform that meets the computational performance demands of the vision-language model and provides great maneuverability and mechanical stability to support pure-vision based operations.
% }

% highlight/list your contributions and impact / use cases
To validate the proposed framework in real-world environments, we develop a UGV platform that meets the computational demands of VLMs while offering superior maneuverability and mechanical stability during the task. We deployed the CLIP model~\cite{radford2021learning,ilharco_gabriel_2021_5143773} in our proposed architecture and performed extensive tests in a real-world environment. Overall, we make the following contributions to this paper:
\vspace{1mm}

\begin{enumerate}[label={$\arabic*$.},nolistsep,leftmargin=*]
\item \textbf{Science:} We propose a novel navigation pipeline named ClipRover for simultaneous exploration and target discovery by UGVs in unknown environments. It harnesses the power of VLMs into a modular architecture, consisting of a vision-encoder as \textit{frontend}, language correlation databases as \textit{middlewares}, and a decision-maker as the \textit{backend}.
%\item We proposed \textbf{a novel architecture} for simultaneous exploration and target discovery in unknown environments. The architecture consists of a vision-encoder as \textit{frontend}, various correlation databases as \textit{middlewares}, and a decision-making module as \textit{backend}.

\item \textbf{System:} We design a novel UGV platform to support the maneuverability and computational demands of VLM-based navigation systems like ClipRover. It is designed to be powerful and configurable for general-purpose robotics research; it will be open-sourced for academic use. 

%Notably, over $90\%$ of its components are 3D-printed, and the design will be open-sourced for public use.
%\item \textbf{System:} We design \textbf{a custom UGV platform} to support the maneuverability and computational demands of a VLM-based active navigation system like ClipRover. Our novel platform is designed to be powerful and highly configurable for general-purpose robotics research. The design features over $90\%$ 3D-printed parts, it will be open-sourced for public use.

\item \textbf{Integration:} We develop a fully functional system integrating the proposed navigation pipeline with the UGV platform. It is then optimized for real-time performance through comprehensive benchmarks. We present the key challenges and practicalities involved in deploying such systems in real-world application scenarios. 
%\item We developed a \textbf{fully functional system} using the proposed architecture on our custom-designed UGV platform, optimized the pipeline's performance to a usable level and demonstrated its effectiveness in a set of real-world experiments.

%\item \textbf{Evaluation:} 
% \JI{need specific: what percentage better based on what metric over what algorithm}.
%\item We performed \textbf{extensive experiments} with the system and compared its performance against map-traversal and path-finding algorithms. We measured the performance according to the distance traveled before reaching the desired target. The results showed that our system constantly outperforms map traversal algorithms and closely matches the performance of path-finding algorithms (which depends on prior knowledge of the map and target location, an unfair advantage over our system).
\end{enumerate}
 %\JI{Revise this last paragraph, refer to and discuss the figure 2.}
We conducted extensive experiments to evaluate the proposed system against both map-traversal and path-finding algorithms. A performance comparison based on efficiency and success rates is shown in Fig.~\ref{fig:overall}, identifying four equipotential lines: {\tt EP0}-{\tt EP3}. In our evaluation, we observe that algorithms that use a similar amount of information tend to reside on the same {\tt EP}. A higher equipotential line represents more information (\eg, prior map of the environment, target location, additional sensors) available to the algorithm. With zero prior information of the environment ({\tt EP0}), ClipRover offers performance margins comparable and often better than Bug2 ({\tt EP2}-{\tt EP3}). Note that Bug2 needs prior information on the target position and a precise localization to keep track of its {\tt m-line}. Overall, ClipRover delivers  \textbf{$\mathbf{33\%}$ more efficient} source-to-target paths compared to Bug2 and achieves a \textbf{$\mathbf{10\%}$ higher success} than the Bug1 algorithm. Importantly, none of the evaluated planner and traversal methods have a semantic understanding of the scene, limiting their ability to adapt to dynamic environments. 


With vision-language integration, SOTA systems such as Clip-Nav~\cite{dorbala2022clipnav}, Clip of Wheels~\cite{Gadre2022CoWsOP}, and GCN~\cite{kiran2022spatialrelationgraphgraph} - demonstrate superior navigation performance while providing semantic scene understanding. However, these systems rely on pre-captured images of candidate scenes, target locations, and pre-built knowledge graphs of the navigable space~\cite{Savarese-RSS-19}. Additionally, many are tested primarily in simulation or overly simplistic environments. Other contemporary methods, such as SEEK~\cite{Ginting2024Seek}, require high-sensing modalities involving multiple cameras, LiDAR, and onboard SLAM pipelines for navigation. ClipRover overcomes these limitations by enabling real-time active navigation using only a monocular camera, without relying on any prior information or pre-built dependencies.





%State-of-the-art vision-language integration systems, such as Clip-Nav~\cite{dorbala2022clipnav}, Clip of Wheels~\cite{Gadre2022CoWsOP}, and GCN~\cite{kiran2022spatialrelationgraphgraph}, demonstrate superior navigation performance while providing semantic scene understanding. However, these systems rely on pre-captured images of candidate scenes, pre-defined target locations, and pre-built scene or node graphs of the navigable space~\cite{Savarese-RSS-19}. Additionally, many are tested primarily in simulation or overly simplistic environments. Other contemporary methods, such as SEEK~\cite{Ginting2024Seek}, require high-sensing modalities involving multiple cameras, LiDAR, and onboard SLAM pipelines for effective navigation. In contrast, ClipRover overcomes these limitations by enabling real-time active navigation using only a monocular camera, without relying on any prior information or pre-built dependencies.

%ClipRover consistently outperforms map-traversal algorithms and achieves performance comparable to path-finding methods, despite the latter relying on prior knowledge of the map and target location. ClipRover achieves significantly shorter trajectory lengths, making it more efficient than map-traversal algorithms. It delivers  \textbf{$\mathbf{33\%}$ more efficient} source-to-target paths compared to Bug2 and achieves a \textbf{$\mathbf{10\%}$ higher success} than the Bug1 algorithm. Importantly, none of the evaluated planner and traversal methods have a semantic understanding of the scene, limiting their ability to adapt to dynamic environments without a prior map. ClipRover addresses these limitations, delivering superior performance while ensuring real-time active navigation on only a monocular camera-based robotic platform.
%\JI{Need to revise this last sentence...} yielding a $33\%$ advantage over the second-best option; it also outperforms path-finding algorithms in success rates by over $10\%$ margins than the second-best comparison with similar path lengths.


\begin{figure}[t]
    \centering
    %\vspace{-2mm}
    \includegraphics[width=\columnwidth]{res/summary.pdf}%
    \vspace{-1mm}
    \caption{Performance of \textbf{ClipRover} compared to map-traversal (\eg, wavefront, random walk, wall bounce)~\cite{pang2019randoomwalk} and path-finding (Bug) algorithms~\cite{lumelsky1986dynamic,lumelsky1987path} for UGV navigation. The equipotential lines labeled as {\tt EP*} are defined by Eq.~\ref{eq:exp/epsilon}. %Note that the \textit{relative efficiency} is calculated as an inverse metric to the normalized trajectory path length from baseline; details are provided later in Table.~\ref{tab:results}.
    % \JI{explain the relative efficienty}
    %ClipRover consistently outperforms map-traversal algorithms such as wavefront and random walk. It also achieves comparable and often better performance than the Bug algorithm~\cite{lumelsky1986dynamic,lumelsky1987path}-based path planners; note that they have an inherent advantage of relying on prior knowledge of the target location, with Bug1 further requiring precise localization.
    }
    \label{fig:overall}
    \vspace{-1mm}
\end{figure}

%The aforementioned systems generally require a detailed map of the environment prior to operation~\cite{lee2011smooth}. Otherwise, an initial map traversal phase must be conducted to obtain the necessary spatial information for path finding~\cite{amigoni2010information}. Such map generation tasks are performed with suboptimal algorithms designed to maximize coverage~\cite{jin2010optimal,tan2021comprehensive}. Additionally, due to insufficient information during the initial mapping stages, traditional algorithms can hardly optimize area exploration and target discovery tasks~\cite{luperto2020robot}. These systems are also susceptible to dynamic environments, where partially constructed maps can become invalid during operation, requiring frequent correction or re-initialization~\cite{hernandez2020real,bonnevie2021long}. Therefore, simultaneous exploration and target discovery remains an open problem in the field of robotics.