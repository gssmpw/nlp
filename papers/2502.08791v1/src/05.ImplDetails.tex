\section{Implementation Details}
\subsection{Slicing Strategies}\label{sec:impl/slicing}
% (1) Why use 2x3 tiles, what else has been considered and why they are not used.
To enhance the embedding of positional information within the pipeline, we preprocess the raw camera frames by slicing them into smaller \textit{tiles} before inputting them into the vision-language model. This slicing strategy is specifically designed to optimize the robot's ability to make informed navigation decisions based on its visual perceptions. As illustrated in Fig.~1, the slicer divides each frame into six ($2~\text{rows} \times 3~\text{columns}$) tiles, with the number of horizontal divisions aligned with the robot's control capabilities. Using smaller tiles instead of the full camera frame reduces visual distractions, thereby improving the accuracy of the correlation results.


% (2) Explain that each tile will be processed separately and independently by correlation middlewares, and the dimensions of the correlation scores will correspond to the number of tiles.
The slicing dimensions are consistently maintained across all stages of the pipeline. For instance, under the $2\times3$  slicing strategy, a frame divided into six tiles produces a corresponding 
$2\times3\times512$ matrix of visual embeddings. These embeddings are subsequently aggregated and passed to the correlation middleware, which generates a $2\times3$ matrix of correlation scores, with each element corresponding to a specific tile. The influence of individual tiles on one another is determined by the logic of the respective correlation middleware. For example, while the familiarity middleware disallows correlations between tiles within the same frame, it may permit correlations between the lower-left tile of a previous frame and the upper-right tile of the current frame.

%The slicing dimensions are preserved across all parts of the pipeline. Take the $2 \times 3$ slicing strategy as an example, a $2 \times 3$ matrix of sliced tiles will produce a $2 \times 3 \times 512$ matrix of visual embeddings. These embeddings are then packed together and passed down to the correlation middleware. Each correlation middleware will then produce a $2 \times 3$ matrix of correlation scores, where each element corresponds to a tile in the sliced frame. How different tiles affect each other is determined by each correlation middleware. For example, the familiarity middleware does not permit correlation between tiles within the same frame, but it might correlate the lower left tile from a previous frame to the upper right tile on the current frame.

% (3) Explain how each tiles are slightly "expanded" to create overlaps between them.
To preserve spatial context in each tile, sliced regions are slightly expanded beyond their original boundaries. This overlap introduces approximately $20\%$ shared area between adjacent tiles, mitigating the loss of contextual information. This adjustment is beneficial when the camera encounters texture-less surfaces, such as walls, paper boxes, or door panels -- where the absence of distinctive features could otherwise impair the robot's perception and navigation performance.


%The slicing dimensions are preserved across all parts of the pipeline. Take the $2 \times 3$ slicing strategy as an example, a $2 \times 3$ matrix of sliced tiles will produce a $2 \times 3 \times 512$ matrix of visual embeddings. These embeddings are then packed together and passed down to the correlation middleware. Each correlation middleware will then produce a $2 \times 3$ matrix of correlation scores, where each element corresponds to a tile in the sliced frame. How different tiles affect each other is determined by each correlation middleware. For example, the familiarity middleware does not permit correlation between tiles within the same frame, but it might correlate the lower left tile from a previous frame to the upper right tile on the current frame.


%To include more context in each sliced tile, the slicing regions are slightly expanded around the original tile boundaries. In our experiment, the neighboring tiles share approximately $20\%$ of area with each of their neighbors. This overlap is intended to counteract the loss of context in each tile, especially when the camera is up close against textureless surfaces such as walls, paper boxes, or door panels.


% Several image slicing strategies are employed to improve navigation decision-making using a vision-language model. Each slicing strategy is a combination of the following steps:
% (i) slicing the raw image, (ii) designing the prompts, (iii) calculating familiarity score, and finally (iv) making an informed navigation decision.
% % Our goal is to examine pure vision-language based navigation without any additional sensor. Since existing vision-language models require input images to be preprocessed into a square aspect ratio ($1:1$), our slicing strategies are designed to adhere to this limitation. The optimum slicing strategy used in all experiments is explained below.
% % \vspace{2mm}
% % \TODO{Experiments show a huge advantage for option (1). Option (2) does not work well for now. Hence I plan to remove it completely from this paper and try to rephrase this section accordingly.}
% % \vspace{1mm}
% % \noindent
% % \textbf{One set of prompts, many slices of tiles.}
% As shown in Fig.~\ref{fig:arch}f, each frame from the camera is sliced into 2 rows (\underline{F}ar, \underline{N}ear) and 3 columns (\underline{L}eft,
% \underline{C}enter, \underline{R}ight). For each tile, the vision encoder frontend encodes a separate embedding vector. The visual embeddings are then passed down to correlation middlewares. A universal set of text prompts is designed to correlate with the encoded vectors, containing positive prompts (\eg empty space, clear floor) and negative prompts (\eg blocked way, large object). For each tile, a cross-correlation is performed between the visual embedding and text embedding (prompts). The highest correlation among all prompts on a tile is used as its ``score'' (a.k.a confidence). A positive score is assigned if the winner is a positive prompt, and vice versa. After obtaining a $2 \times 3$ matrix of confidences, the motion of the robot can be mapped by a bare minimum decision tree for map traversal (shown in Alg. \ref{alg:motion}). Higher-level strategies, such as avoiding visiting the same area or following natural language instructions, may override the aforementioned decision tree.
% \vspace{1mm}
% \noindent
% \textbf{One tile per frame, many sets of prompts.}
% In this setup, only one slice will be extracted from each frame. This slice is centered on the raw frame and scaled to the maximum possible size that fits in the raw frame. Different sets of prompts are designed to map to different degrees of freedom for robot motion.



\subsection{Prompt Generation}
We also design a prompt system to compose text prompts in a hierarchical template. The proposed design is inspired by WinCLIP~\cite{Jeong2023CVPR,zhu2024toward}, an anomaly detection framework. Our prompt system consists of the following templates and notations, which we follow throughout the paper.
\begin{itemize}
\item \li{Top-level prompts} such as:
\begin{itemize}
    \item \Prompt{A \DscPrompt{desc.} photo of a \SttPrompt{state} \ObjPrompt{object}}
    \item \Prompt{A \DscPrompt{desc.} image of a \SttPrompt{state} \ObjPrompt{object}}
    \item \Prompt{A \SttPrompt{state} \ObjPrompt{object}}
\end{itemize}
\item \li{\DscPrompt{descriptions}} such as \ttt{clear}, \ttt{blurry}, \ttt{cropped}, \ttt{empty}, \ttt{corrupted}, \etc
\vspace{1mm}
\item \li{\SttPrompt{states}} such as \ttt{clean}, \ttt{clear}, \ttt{wide}, \ttt{narrow}, \ttt{cluttered}, \ttt{messy}, \ttt{way blocking}, \etc
\vspace{1mm}
\item \li{\ObjPrompt{objects}} such as \ttt{floor}, \ttt{wall}, \ttt{door}, \ttt{object}, \etc
\end{itemize}
%\noindent As is shown above, we use different types of brackets to indicate different types of template substitutions. All figures in this paper follows the same convention.

We utilize a YAML-based syntax to define the structure of the prompt databases. YAML, a widely adopted and human-readable markup language, has been extended in our implementation to incorporate additional syntactical features for enhanced flexibility and fine-grained control over the generation of text prompt combinations. Two key features introduced are: Selective integration and in-place expansion. 
\begin{enumerate}[label={$\arabic*$.},nolistsep,leftmargin=*]
\item  \li{Selective integration} enables a prompt to bypass certain levels of the template hierarchy, thereby allowing precise customization. For instance, a prompt such as \Prompt{A \SttPrompt{meaningless} photo} should not be followed by any \ObjPrompt{object}. This is achieved by terminating the prompt early, omitting the insertion marker (\ie~\ttt{\footnotesize{\{\}}}) in its definition. 

\item \li{In-place expansion} facilitates the collective definition of similar short prompts by using a vertical bar syntax (\ie~\ttt{|}) to separate terms. This feature is particularly effective when used in conjunction with Selective Integration. For example, the prompt \Prompt{A photo with no \ObjPrompt{context|texture|information}} is expanded into three distinct prompts, although not fitting into the hierarchical template structure, can be defined as top-level negative prompts, aiding in distinguishing meaningful content from irrelevant or meaningless information. 
\end{enumerate}
These enhancements to the YAML-based syntax provide an effective framework for defining and managing prompt databases with high precision and adaptability.


%We use a {\tt YAML} based syntax to define the \textit{prompt databases}. {\tt YAML} is a popular human-friendly mark-up language. Our implementation features additional syntaxes that allows for more flexible and find-grained control over the generated combinations of text prompts: \li{Selective Integration} allows a prompt to skip integrating certain levels of the template. For example, prompt \Prompt{A \SttPrompt{meaningless} photo} should not be followed by any \ObjPrompt{object}. A prompt can be terminated early by excluding the insertion mark (\ie~\ttt{\footnotesize{\{\}}}) from its definition. \li{In-place expansion} allows for similar short prompts to be defined collectively. The syntax applies to consecutive terms divided by vertical bars (\ie \ttt{|}). It is particularly useful when used alongside selective integration. For example, prompt \Prompt{A photo with no \ObjPrompt{context|texture|information}} will be expanded into three separate prompts. These prompts would not fit into the template hierarchy, but thanks to the aforementioned features, they can be defined as top-level negative prompt to help differentiate useful information from meaningless ones.

\vspace{-1mm}
\subsection{Performance Optimization}
Navigation decisions are a critical component of real-time autonomous robotic systems, requiring rapid execution to ensure safety and operational efficiency. However, most vision-language models are designed as extensions of LLMs, where processing delays and data throughput are less critical. To evaluate the suitability of VLMs for robotic navigation, we identify two key performance metrics:
%Navigation decision is a critical component of a real-time autonomous robotic system, requiring rapid execution. In contrast, most vision-language models are designed as extensions of large language models, where delays and data throughput are less critical. Therefore, we identify two key metrics that reflect the performance of a VLM-based navigation system:
\begin{enumerate}[label={$\arabic*$.},nolistsep,leftmargin=*]
\item  \li{Decision delay} measures the time elapsed between capturing a frame from the robot’s camera and issuing a corresponding motion command to the motors. This metric reflects the system's ability to promptly react to environmental changes, such as avoiding obstacles and maintaining safe navigation. 
\item \li{Throughput} quantifies the number of frames processed per second, directly influencing the smoothness of the robot’s motion. Limited computational resources necessitate dropping unprocessed frames, retaining only the latest frame for decision-making. Higher throughput minimizes inter-frame discrepancies, reducing abrupt changes in motion and ensuring smoother transitions. Mathematically, throughput is the inverse of the decision delay: \ie~$f=\frac{1}{T_{\text{delay}}}$.
\end{enumerate}

\vspace{1mm}
\noindent
On the other hand, most robotic systems rely on embedded computers featuring RISC architecture processors and power-limited batteries, prioritizing power efficiency over computational capability. Addressing these constraints, we developed an optimized architecture to maximize resource utilization while improving decision delay and throughput.

%\noindent\textbf{(I) Decision delay}: It refers to the time elapsed between capturing a frame from the camera and sending a motion command to the motors based on the information from that frame. The decision delay determines how promptly the robot can react to steer itself away from obstacles and navigate safely.


%\noindent\textbf{(II) Throughput}: Due to limited computing power, the system needs to drop frames between each iteration of the decision. \ie~it only keeps track of the latest frame and drops earlier frames unprocessed. The throughput affects the smoothness of the robot's motion. Higher throughput allows more frequent updates to the robot's motion and reduces the difference of input across each iteration, hence avoiding dramatic change to the outputs. In an unoptimized system, the throughput amounts to the inverse of the total time cost of each iteration. \ie~$f=\frac{1}{T_{\text{delay}}}$.

%On the other hand, mode robotic systems are operated by embedded computers. These computers typically feature a RISC architecture processor on a power-limited battery. These platforms are optimized for power efficiency rather than computing capabilities.

%We developed an architecture that utilizes as many resources as possible on such a platform to improve both decision delay and data throughput. Four major nodes (preprocess, inference, correlation and decision) are divided into separate threads and are executed in parallel. All computationally intensive tasks are offloaded to the GPU. With there optimizations in place, we achieved a data processing delay of $252.10$ ms and a throughput of $5.37$. As shown in Table. \ref{tab:perf}, that is $800\%$ improvement for the delay and $400\%$ improvement for the throughput compared to the sequential CPU implementation.

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{res/StrideDemo.pdf}
%     \caption{
%         Demonstration of the effect of \textit{stride} parameter for trajectory smoothing.
%         (a) Raw trajectory generated by the 2-pass process;
%         (b) Processed trajectory with \textit{stride} $= 0.20~\text{m}$.
%     }
%     \label{fig:trj-stride}
% \end{figure}

Focusing on reducing decision delay and increasing data throughput, our proposed framework divides the navigation pipeline into four major nodes: pre-processing, inference, correlation, and decision-making. Computationally intensive tasks are offloaded to the GPU to exploit parallel processing capabilities. These optimizations were implemented on a power-limited embedded system, balancing efficiency and performance. With the proposed optimizations, we achieve: (\textbf{i}) decision delay: $252.10$\,ms, an $900\%$ improvement; and (\textbf{ii}) throughput: $5.01$ FPS (frames per second), representing a $400\%$ improvement compared to the sequential CPU-based implementation. These results, detailed in Table \ref{tab:perf}, demonstrate the viability of our VLM-based navigation pipeline for real-time robotic applications under computational constraints. 

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \newcommand{\dg}{\textsuperscript{\textdagger}}
    \caption{Performance margins of the navigation pipeline of ClipRover at different optimization levels. The acronyms: \acronym{P} Preprocess,  \acronym{I} Inference, \acronym{C} Correlation, and \acronym{D} Decision.}%
    \vspace{-1mm}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \Xhline{2\arrayrulewidth}
    \multirow{2}{*}{\small{Config.}} & \multicolumn{5}{c|}{\small{Mean Data Processing Delay (ms)}} & \multirow{2}{*}{\Large{$^\text{Throughput}_\text{(mean FPS)}$}} \\ \cline{2-6}
                    & \acronym{P} & \acronym{I} & \acronym{C} & \acronym{D} & \small{Total} & \\ \hline
    CPU Seq.\,& $80.57$ & $ 2221.67$ & $\,3.13\,$ & $\,0.37\,$ & $ 2534.55$ & $0.41\, \pm 0.07$ \\ \hline
    CPU Para. & $73.16$ & $ 2201.84$ & $\,3.29\,$ & $\,0.40\,$ & $ 2463.26$ & $0.44\, \pm 0.02$ \\ \hline
    GPU Seq.\,& $65.93$ & $\z178.77$ & $\,2.94\,$ & $\,0.45\,$ & $\z327.57$ & $3.22\, \pm 0.25$ \\ \hline
    GPU Para. & $25.32$ & $\z171.08$ & $\,3.43\,$ & $\,0.48\,$ & $\z252.10$ & $\mathbf{5.01} \pm 0.36$ \\
    \Xhline{2\arrayrulewidth}
    \multicolumn{7}{l}{}\\[-4mm] % Add some space between the table and the footnote
    % \multicolumn{7}{l}{\small{
    %     \textbf{\textit{Acronyms}}:
    %     \acronym{P} Preprocess,  \acronym{I} Inference,
    %     \acronym{C} Correlation, \acronym{D} Decision.
    % }} \\
    % \multicolumn{7}{l}{\small{
    %     \dg~Showing typical (mean) values sampled across multiple iterations.
    % }}
    \end{tabular}
    }
    \label{tab:perf}
    \renewcommand{\arraystretch}{1.0}
    \vspace{-3mm}
\end{table}

%We developed an architecture that utilizes as many resources as possible on such a platform to improve both decision delay and data throughput. Four major nodes (preprocess, inference, correlation and decision) are divided into separate threads and are executed in parallel. All computationally intensive tasks are offloaded to the GPU. With there optimizations in place, we achieved a data processing delay of $252.10$ ms and a throughput of $5.37$. As shown in Table. \ref{tab:perf}, that is $800\%$ improvement for the delay and $400\%$ improvement for the throughput compared to the sequential CPU implementation.


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{res/LookAround.pdf}
    \includegraphics[width=\textwidth]{res/LookAroundLegends.pdf}\vspace{-2mm}
    \caption{Visualizations of look-around operations during a demonstrative task; candidate directions are depicted as dashed lines ($C_i$), where candidates with smaller indices are assigned a higher priority. The plots are generated from correlation scores without utilizing additional sensory data. (\textbf{a}) Initial look-around at the center of the map: the system does not incorporate familiarity scores since the familiarity database is uninitialized; thus, the navigability score predominantly influences the robot's decision. (\textbf{b}) The robot encountered a dead-end and was temporarily trapped; a look-around operation enabled it to identify a navigable path and resume exploration. (\textbf{c}) The robot was trapped due to a false positive navigability perception caused by a transparent object; with a look-around, the system successfully recovered from the false positive and continued its exploration. (\textbf{d}) The target was located nearby in this scenario; through the look-around, the robot was able to identify the target and assign it as the highest-priority candidate. [Best viewed digitally at $2\times$ zoom for clarity.]
    }
    \label{fig:exp-look-around}
    %\vspace{-2mm}
\end{figure*}


\subsection{Usage of 2D LiDAR}\label{sec:LiDAR}
The proposed ClipRover pipeline use only monocular RGB data for VLN navigation decisions. Nevertheless, we installed a 2D $360$\degree\ scanning LiDAR on the robot for experimental safety, map generation, and comparative analyses. The specific LiDAR functions are listed below.
\begin{enumerate}[label={$\arabic*$.},nolistsep,leftmargin=*]
\item  \li{Emulation of a proximity kill-switch.} The LiDAR serves as a virtual \textit{kill switch} during operation, detecting obstacles in the robot’s intended path. It monitors the motion commands sent to the wheels to infer the robot’s planned trajectory and checks for potential obstructions along that direction.  A {\textit{halt signal}} is asserted upon detection of a potential collision, thus strictly used for safety purposes without influencing the navigation algorithm. 

\item  \li{Visualization of map and robot trajectory.} Mapping is performed offline using recorded LiDAR and odometry data, with subsequent trajectory analysis to evaluate navigation efficiency and overall performance. We utilize the \textit{SLAM Toolbox}~\cite{Macenski2021} to generate maps and trajectories, implementing a two-pass process to enhance the quality of the results. In the first pass, the SLAM Toolbox operates in offline mapping mode, creating a high-resolution 2D map of the environment. In the second pass, the pre-constructed map is loaded and run in localization-only mode to generate accurate trajectories using LiDAR localization.
Despite this two-pass process, the trajectories generated often exhibit zig-zag patterns caused by a mismatch between SLAM drift correction frequency and the robot's odometry update rate. These patterns are not observed in the actual motion of the robot, still they affect our performance evaluation due to increased lengths of the projected paths. To mitigate this, we introduce a \textit{stride} parameter to smooth the trajectory. This smoothing parameter reduces the projected trajectory length and better matches the robot's actual travel distance.

%The mapping is performed offline using recorded data, and the resulting trajectory is analyzed to assess the robot's navigation efficiency and overall performance. We employ \textit{slam toolbox} to generate maps and trajectories from recorded LiDAR and odometry data. We designed a 2-pass process to help improve the quality of results. In the 1st pass, \textit{slam toolbox} was ran in offline mapping mode, creating a high-quality 2D map of the environment. In the 2nd pass, \textit{slam toolbox} was configured to load back the map constructed from the 1st pass and run itself in localization-only mode. This pass generates an accurate trajectory derived from LiDAR localization. All maps and trajectories shown in this paper were generated using the aforementioned process. It is worth noting that, even with the 2-pass process, the generated trajectories still contain zig-zag patterns due to mismatch between SLAM drift correction frequency and robot's odometry update frequency. As is shown in Fig.~\ref{fig:trj-stride}a, such patterns were not observed in the actual motion of the robot. These patterns negatively affect our evaluation of the robot's navigation performance as it increases the projected path length. To mitigate this issue, we introduce a \textit{stride} parameter to smooth the trajectory. The effect of this parameter is demonstrated in Fig.~\ref{fig:trj-stride}b. The total length (``travel'') of the sample trajectory reduced from $8.53~\text{m}$ to $5.11~\text{m}$, which is considered closer to the ground truth.
\item \li{Comparison with simulated algorithms.} A 2D map generated using LiDAR data is employed to simulate and evaluate traditional range sensor-based map traversal and path-planning algorithms for performance comparison. The LiDAR data collected during ClipRover experiments was used to reconstruct the exploration map, which is reused for simulations. This approach ensured that all comparisons were conducted on the same map, providing a fair and consistent evaluation of the different algorithms.
\end{enumerate}

\vspace{-1mm}
\subsection{Wave Front Simulation}\label{sec:impl/wavefront}
We further design a ``Wave Front'' simulation package to serve as a general baseline metric for comparison between different methods. It utilizes the concept of classic wave function~\cite{Maxwell1865} and interprets it as probability distribution, allowing simultaneous exploration of infinity many directions driven by the following equation:
\begin{equation}
\frac{\partial^2}{\partial t^2}\,\psi(x, y, t) = c^2 \nabla^2\psi(x, y, t)
\end{equation}
The simulation implements a 2D Laplacian equation at discretized time steps. Given wave speed $c$, time step $\Delta t$, and spatial step $\Delta x$, the wave equation is discretized as:
\begin{align*}
\psi(x, y, t_{n+1})
=~& 2\cdot \psi(x, y, t_{n}) - \psi(x, y, t_{n-1}) \\
+~& \z~ ~ \frac{c^2 \Delta t^2}{\Delta x^2} \nabla^2\psi(x, y, t) \\
s.t.~& \z~ ~ t_{n+1} - t_{n} = \Delta t
\end{align*}
Here, the discretized Laplacian operator $\nabla^2$~\cite{FDTD2000Wave} is defined as:
\begin{align*}
\nabla^2\psi(x, y)
& = \psi(x_{i+1},\,y_j)
  + \psi(x_{i-1},\,y_j) \\
& + \psi(x_i,\,y_{j+1})
  + \psi(x_i,\,y_{j-1}) \\
& - 4\cdot\psi(x, y) \\
s.t.~& ~ ~ ~ \, x_{i+1} - x_i = y_{j+1} - y_j = \Delta x
\end{align*}
Besides, map boundaries and obstacles are simulated by reflective conditions. That is, at boundary point $(x, y)$, we have: $\nabla\psi(x, y) \cdot \mathbf{\hat{n}}_{x, y} = 0$, where $\mathbf{\hat{n}}_{x, y}$ is the normal vector.
In a discretized 2D grid, the directions of this normal vector are simplified to four possibilities, \ie, $\mathbf{\hat{n}} \in \left\{ \pm \mathbf{\hat{x}}, \pm \mathbf{\hat{y}} \right\}$.

To implement the aforementioned boundary condition, the discretized laplacian operator can be rewritten as:
\begin{align*}
    \nabla^2\psi(x,\,y)
    & = k_{i+1,\,j} \cdot \psi_{i+1,\,j}
      + k_{i-1,\,j} \cdot \psi_{i-1,\,j} \\
    & + k_{i,\,j+1} \cdot \psi_{i,\,j+1}
      + k_{i,\,j-1} \cdot \psi_{i,\,j-1} \\
    & - (k_{i+1,\,j} + k_{i-1,\,j} + k_{i,\,j+1} + k_{i,\,j-1})\cdot\psi(x,\,y)
\end{align*}
{\small\begin{equation*}
s.t.~ ~ ~ ~ \, k_{i,j} = \left\{~\begin{matrix}
        1 & \text{if } (x_i,\,y_j) \text{ is free space} \\[2mm]
        0 & \text{if } (x_i,\,y_j) \text{ is obstacle ~} \\
    \end{matrix}\right.
\end{equation*}}

At time $T = 0$, a probability distribution is initialized as a Gaussian distribution centered at the robot's starting location, with a standard deviation set to half of the robot's size; the total probability is normalized to $1$. At each iteration, the wave function is multiplied by an inverse Gaussian function centered around the target location, effectively reducing the total probability on the map and simulating a draining effect. The simulation terminates when the total remaining probability falls below a defined threshold ($1e^{-4}$ in our experiments). The outcome is a 2D probability distribution $p(t)$ indicating the likelihood of the robot reaching the target at each time step. The mean and standard deviation of this distribution are used for comparison with other trajectory-based algorithms.

At a high level, this algorithm simulates the behavior of infinitely many wall-bouncing robots moving in all possible directions. Notably, the drain operation creates a uniform gradient toward the target, subtly guiding the robot. As a result, the algorithm performs slightly better than a purely randomized wall-bouncing simulation, which lacks any target information. The travel distance when the first non-zero probability is detected is used as the baseline distance for each origin-target combination. For relative performance comparisons, as shown earlier in Fig.~\ref{fig:overall}, the travel distances are normalized by this baseline distance before aggregating the results.

%At each iteration, the wave function is multiplied with an inverse gaussian function centered around the target location. This operation reduces the total probability on the entire map, simulating a draining effect. The simulation terminates when the total probability remaining on the map falls below a threshold (we use $0.0001$ in our experiments). The result is a 2D probability distribution (\,$t$ v.s. $p(t)$\,) indicating the probability of the robot reaching the target at each time step. The mean and standard deviation of the distribution is used to compare against other trajectory based algorithms.

%On the high level, this algorithm simulates the behavior of infinitely many wall-bouncing robots heading towards all possible directions. However, it's worth mentioning that the drain will create a uniform gradient towards the target. This will provide a hint on the direction of the target, causing the final result to be slightly better than randomized wall bouncing simulation, which does not have any information of the target.

%We use the travel distance when the first non-zero probability is detected as the distance metric as the baseline distance for each origin-target combination. In the relative performance comparison (Fig.~\ref{fig:overall}), we normalize the travel-distance using the baseline distance before aggregating all results.


% ========== Original ==========
% As the mission progresses, the robot's positions are recorded, generating a trajectory that captures the entire path. This trajectory is later analyzed to evaluate the robot's navigation efficiency and overall performance.
% ========== Modified by Yuxuan: emphasize that the algorithm is ran AFTER a mission, not during a mission. ==========
% ======== Modified #2 =========
% We employ HectorSLAM~\cite{kohlbrecher2011flexible}, a lightweight algorithm for simultaneous localization and mapping (SLAM) with LiDAR pointcloud data.
% ==== Modified by Yuxuan: I've updated this according to the actual process ====


% \input{results/table.tex}

% \input{results/table_overall_comparison}
