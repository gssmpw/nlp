%\section{Experimental Experiments}\label{sec:exp}

% \draft{
% Extensive experiments on the proposed ClipRover system were conducted to evaluate its performance in real-world environments.
% }

% \subsection{Basic Obstacle Avoidance - Proof of Effectiveness}

% As the first step, we tested the effectiveness of our proposed system in a simple and constrained environment. We show that the robot exhibits positive maneuvers to avoid running itself into obstacles. As is shown in Fig.~\ref{fig:basic-nav}, the robot was able to positively maneuver itself to avoid potential collisions as indicated by the dashed red lines.

% % \begin{figure}[H]
% %     \centering
% %     \includegraphics[width=0.8\columnwidth]{res/BasicNav.pdf}
% %     \caption{
% %         Obstacle avoidance demonstration. Black line is robot's real trajectory. Red dashed lines represent potential collisions, arrows represent robot's maneuver to avoid them.
% %     }
% %     \label{fig:basic-nav}
% % \end{figure}

% % To quantize the effectiveness of the visual navigation, we recorded the count of halt signal being triggered during the test. We ran the robot down the hall way for demonstration. As is shown in \TODO{Fig.}, our proposed system was able to maintain the desired path and intentionally steer away from the walls when necessary.

% \subsection{Familiarity Database - Maximizing Exploration Efficiency}\label{sec:exp/familiarity}

% As introduced in Sec. \ref{sec:arch/decision/familiarity}, the familiarity database is introduced to help the robot avoid known places. Instead of images, the database utilizes the visual embeddings generated by the visual encoder as a condensed representation of a scene. Visual embeddings are correlated with existing points and merged into the database using a real-time clustering process. This process also generates a "familiarity score" for each tile. The score ranges from $0.0$ to $1.0$, a higher score means higher familiarity. The \textit{familiarity score} will then be coupled with \textit{navigability score} and \textit{standard deviation score} (described in Sec. \ref{sec:arch/decision}) and passed to the navigation node for decision making. The \textit{familiarity score} is used in two scenarios for navigation:

% \noindent\textbf{Normal Navigation:}
% The robot operates under normal navigation mode when there is no target in its visual and it has not been trapped. In this mode, the familiarity scores are applied as a \textit{penalty} to the navigability score. This is designed to make the final motion decision prefer locations that are both navigable and unfamiliar.

% \noindent\textbf{Look Around:}
% As mentioned in Sec. \ref{sec:arch/look-around}, a look-around action will be triggered when the system determines itself to be ``trapped''. The look-around action helps the system to achieve awareness to its $360$ degree surroundings and decide which direction it would use to resume exploring. In this process, a ``look-around database'' will be accumulated during the action. The database contains both the navigability scores, and familiarity scores. Our algorithm will first derive all candidate directions which resembles navigable paths, and then apply the inverse familiarity as a \textit{bonus} to each candidates. The final score will be sorted to get the next heading. In this process, the familiarity score helps to make the system favor unexplored areas which typically have a lower familiarity score.

% To demonstrate the efficacy of the look-around subsystem, we presented sample results in Fig.~\ref{fig:exp}a-d. Each sub-figure consists of (1) Trajectory which the robot follows \textit{before} performing the look around. (2) A radar plot showing the data accumulated during the look around operation, and the rationale behind its final decision; and (3) Trajectory which the robot took \textit{after} finishing the look-around, with an arrow on its end.

% \subsection{Full Demo - Exploration and Target Discovery}

\section{Experimental Analyses}\label{sec:exp}

\subsection{Experiment Setup}
Real-world experiments were conducted in an indoor workspace, as depicted in Fig.~\ref{fig:env}\,a. This environment was selected for its cluttered layout and complex details, which include numerous obstacles, potential traps, and loops, providing a challenging setting for comprehensive analysis. As shown in Fig.~\ref{fig:env}, the robot was tasked with exploring the lab space while searching for a designated target--a toy bear approximately $20$\,cm tall and $10$\,cm wide, chosen for its distinctive appearance within the test scene. For each task, the source (robot's starting position) and destination (target location) were selected from five predefined regions (\eg, SW, C, NW, NE, SE), as labeled in Fig.~\ref{fig:env}\,c.

%As shown in Fig.~\ref{fig:env}, the robot is tasked to explore the lab space while looking for a specific target, which is a toy bear (approximately $20$\,cm tall and $10$\,cm wide); it is chosen to ensure its uniqueness in the test scene. In each task, the source (robot's starting position) and destination (target location) are chosen from the five regions (\eg, SW, C, NW, NE, SE) labeled in Fig.~\ref{fig:env}c.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{res/ExperimentSetup.pdf}%
  \vspace{-1mm}
  \caption{The environment used for real world exploration and discovery experiments: (\textbf{a}) An overview of the machine hall. (\textbf{b}) A photo of a toy bear that is used as the \textit{discovery target}. (\textbf{c}) A 2D map of the experimental space, with major corner locations annotated in blue; the area utilized for the experiments is enclosed within the dashed red line.
  % [DONE]
  % \JI{for (a), better take a top-view photo from the second floor to show the space, that resembles the map; the current photo is not informative.}
  }
  \label{fig:env}
  \vspace{-2mm}
\end{figure}

\subsection{Performance Evaluation}
\noindent 
\textbf{Evaluation criteria.} Our experimental trials are designed to evaluate \textit{efficiency} of exploring an area and \textit{success rates} of finding a target with no prior map or knowledge about the target. To quantify these, we use the total distance traveled (trajectory length) before approaching the target as the core metric. Instead of the total duration of the task, the travel distance metric is agnostic to the CPU/GPU performance of the onboard computer and the capability of the mechanical driving system, which are considered external factors that can be improved independently.




\vspace{1mm}
\noindent 
\textbf{Algorithms for comparison.} We compare the performance of the proposed \textbf{ClipRover} system with six widely used map-traversal and path-finding algorithms. For fair comparisons, a novel 2D simulation framework, \textbf{RoboSim2D}, is developed. It utilizes 2D LiDAR maps generated from recorded scans of the same environment used in real-world experiments. Additionally, the size of the simulated robot was configured to match the dimensions of the physical robot, ensuring consistency across simulations and the real platform. 


\vspace{1mm}
\noindent
For map traversal algorithms~\cite{pang2019randoomwalk}, we consider:
\begin{enumerate}[label={$\arabic*$.},nolistsep,leftmargin=*]
\item  \li{Random Walk:} Starts with a given heading, then randomly selects a new heading when an obstacle is encountered.
\item  \li{Wall Bounce:} Starts at a given heading, then bounces off obstacles based on the normal vector of the impact point.
\item  \li{Wave Front:} Starts as a Gaussian probability distribution, then spreads outwards and bounces off obstacles. A detailed explanation is in Sec.~\ref{sec:impl/wavefront}.
\end{enumerate}

\vspace{1mm}
\noindent
Additionally, we compare the following Bug Algorithms~\cite{lumelsky1986dynamic,lumelsky1987path}:
\begin{enumerate}[label={$\arabic*$.},nolistsep,leftmargin=*]
\item  \li{Bug0:} Moves straight toward the target until encountering an obstacle, then follows the obstacle boundary until it can resume a direct path to the target.
\item  \li{Bug1:} Heads towards the target when possible, otherwise, circumnavigates the obstacle. After looping an obstacle, travels to the point with the minimum distance on the loop.
\item  \li{Bug2:} Circumnavigates obstacles upon encountering until it crosses the direct line from the start to the target (the {\tt m-line}), then resumes a straight path toward the target.
\end{enumerate}


%\subsection{Criteria of Failure}
\vspace{1mm}
\noindent
\textbf{Criteria of failure.} Due to practical limitations, a mission is considered a failure if the distance traveled exceeds a predefined upper limit. For real-world experiments conducted in the test area (approximately $12 \times 16$ m), this limit is set to $100$ meters. In simulations, the limit is set to $1,000$ meters for random walk, wall-bouncing, and wave-front algorithms. For Bug algorithms, loop detection is employed to identify endless loops, which are classified as mission failures. Since failed missions yield infinite travel distances, they are excluded from the metrics presented in Table~\ref{tab:results}. Instead, they are reported separately in the overall performance Table~\ref{tab:overall} and visualized in the relative performance plot in Fig.~\ref{fig:overall}.

\input{res/table}

\input{res/table_overall_comparison}

% May need to check if this is complete (no loops) like Bug2, upper bound and lower bounds if possible to calculate. For reference, check \url{https://spacecraft.ssl.umd.edu/academics/788XF14/788XF14L14/788XF14L14.pathbugsmapsx.pdf}.

\newcommand{\EP}{\bm{\varepsilon}}


\vspace{1mm}
\noindent
\textbf{Evaluation metrics.} As observed in our simulation results, for a given amount of information, the success rate $R$ is inversely proportional to the normalized path length $L$. This relationship is expressed by a commonly adopted performance metric for VLN systems, known as \textit{Success weighted by Path Length} (SPL)~\cite{anderson2018evaluation}, defined as:
\begin{equation}
\text{SPL} = \frac{1}{N} \sum_{i=1}^N S_i \frac{l_i}{\max(p_i, l_i)}.
\label{eq:SPL}
\end{equation}

\noindent
Note that, for each source-target combination, the first contact distance of WaveFront simulation is used as the baseline distance $D_\text{baseline}$. With the choice of $l = D_\text{baseline}$, we have $p_i > l, ~ \forall p_i \in \{p\}$; hence, the above equation is simplified to:

\begin{equation}
\text{SPL}
  = \frac{1}{N} \sum_{i}^{S_i = 1} \frac{D_\text{baseline}}{p_i}
  = \frac{N_s}{N} \sum_{i}^{S_i = 1} \frac{L_i}{N_s}
  = \,R\, \cdot \,\overline{L}
\end{equation}
\begin{equation}
\text{s.t.}
\left\{\scalebox{.9}{$
    \setlength\arraycolsep{0pt}
    \begin{matrix*}[l]
        \,\,L_i & = \dfrac{D_\text{baseline}}{p_i}
        & ~ ~ - ~ ~ \text{Inverse Relative Distance} \\[8pt]
        \,\,N_s & = \sum_{i}^{S_i = 1} 1
        & ~ ~ - ~ ~ \text{Number of Success Runs} \\[4pt]
        ~ R & = \dfrac{N_s}{N}
        & ~ ~ - ~ ~ \text{Success Rate} \\[4pt]
        ~\, \overline{L} & = \sum_{i}^{S_i = 1} \dfrac{L_i}{N_s}
        & ~ ~ - ~ ~ \text{Mean Inverse Path Length} \\[4pt]
    \end{matrix*}
$}\right.
\label{eq:definitions}
\end{equation}

\noindent
As a consequence, the SPL metric equates to an inverse proportional relation between two quantities, namely the success rate $R$ and the mean inverse path length $\overline{L}$, as follows:

\begin{equation}
\overline{L} = \frac{\text{SPL}}{R}.
\end{equation}

\noindent
However, experimental data reveal that the curvature of the SPL function does not align well with the observed $R$--$\overline{L}$ curve shown in Fig.~\ref{fig:overall}. To better model the observed data, \textbf{we introduce a new metric $\EP$} to represent the amount of information available to a given algorithm. It has a range of $[0, 1]$. When $\EP=0$, the algorithm operates without environmental information and lacks sensing capabilities beyond collision detection, as seen in methods like random walk and wall bounce. As $\EP$ approaches $1$, it gains more information or has higher environment sensing capability, and the performance is expected to increase accordingly. Considering the impact of $\EP$, the revised relationship reduces to:

\begin{equation}
\overline{L} = \frac{\EP^\prime}{(kR + t) ^ \beta}
\label{eq:exp/epsilon}
\end{equation} \vspace{-2pt}
\begin{equation}
\text{s.t.} ~ ~ \beta = \alpha \cdot \EP^\prime ~ ~ \text{and} ~ ~ \EP^\prime = 1 + \omega\cdot\EP
\end{equation}

\noindent
where $k$, $t$, and $\alpha$ are hyper-parameters specific to the environment. They are derived using linear regression with $\EP=0$; additionally, $\omega$ is solvable by setting $\EP = \overline{L} = R = 1$.




\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{res/Samples.pdf}%
  \caption{
      Sample results of autonomous exploration and target discovery by ClipRover compared to Bug algorithms. %\draft{Showing $\mathbf{8}$ out of $\mathbf{20}$ combinations. A complete matrix of all experiments is attached in the Appendix.} 
      The trajectories of each algorithm are overlaid on a 2D map of the test environment. Circled numbers indicate the \textit{look around} operations of ClipRover and the corresponding waypoints sequence it followed. For Bug algorithms, the red, green, and blue trajectories represent paths traversed by \textit{Bug0}, \textit{Bug1}, and \textit{Bug2}, respectively. Best viewed digitally at $2\times$ zoom for clarity; more results are in the Appendix. A video demonstration is provided here: \url{https://youtu.be/84XLM-GbhS8}.
  }
  \label{fig:exp}
  \vspace{-2mm}
\end{figure*}


\subsection{Qualitative and Quantitative Analyses}
We conduct extensive real-world experiments with over $60$ trials based on various combinations of source-target locations shown in Fig.~\ref{fig:env}. For the proposed ClipRover system, three trials were performed for each origin-target pair. In comparison, $200$ trials were recorded for each random walk experiment, while $180$ evenly distributed headings were recorded for each combination of the wall bounce runs.

Samples of the trajectories traversed by ClipRover and other algorithms are presented in Fig.~\ref{fig:exp}, overlaid on a 2D map of the environment. Under \textit{normal navigation mode}, ClipRover exhibits a human-like motion strategy by navigating near the center of open spaces, enhancing exploration efficiency, and reducing the likelihood of becoming trapped by obstacles. In contrast, path-finding algorithms often follow the contours of obstacles due to their lack of semantic scene understanding. During \textit{look-around operations} (see Fig.~\ref{fig:visual-homo} and Fig.~\ref{fig:exp-look-around}), ClipRover accurately discriminates between navigable spaces and obstacles. Additionally, it demonstrates a preference for unexplored (unfamiliar) areas over previously visited (familiar) regions, contributing to improved efficiency compared to traditional map-traversal algorithms.

%Samples of trajectories traversed by the proposed ClipRover system and other algorithms in comparison are shown in Fig.~\ref{fig:exp}, overlaid on a 2D map of the environment. In \li{normal navigation mo, ClipRover is programmed to exhibit a human-like motion preference to navigate at the center of free spaces. This helps to improve the efficiency of exploration and reduce the chance of being trapped by obstacles. In contrast, the path-finding algorithms often travel along the contours of obstacles due to their lack of high-level understanding of the scene. In \li{look-around operations} (see Fig.~\ref{fig:visual-homo} and Fig.~\ref{fig:exp-look-around}), ClipRover accurately discriminates navigable spaces apart from obstacles. Besides, it exhibits a general preference towards unexplored (unfamiliar) areas over previously explored (familiar) areas, contributing to a higher efficiency over the map traversal algorithms.





Out of all $60$ trials, 3 failure cases were recorded, yielding $95\%$ overall success rate for ClipRover. Among them, two were caused by the trajectory length exceeding the upper limit, while the other one was caused by the robot getting jammed by a reflective metal tank, which was not detected by either the VLN pipeline or the LiDAR proximity switch.

The quantitative results categorized by source and target locations are presented in Table~\ref{tab:results}. To ensure comparability across different source-target pairs, the travel distance for each row is normalized by the baseline travel distance and then aggregated into Table~\ref{tab:overall}. For randomized algorithms, the success rate $R$ is a configurable hyperparameter. Sample results fo $R = 50\%$ and $R = 80\%$ are provided to represent their underlying performance in Table~\ref{tab:overall}, with the corresponding equipotential curves previously shown in Fig.~\ref{fig:overall}. In contrast, deterministic algorithms (such as Bug variants) have a fixed success rate. Their performances are reported directly in Table~\ref{tab:overall} and visualized in Fig.~\ref{fig:overall} for comparison.




As these results demonstrate, ClipRover averages $2.66 \times D_\text{baseline}$ travel distance before reaching the target, while the next best score is from Wall Bounce, with $9.58 \times D_\text{baseline}$ at a significantly lower success rate ($50\%$). Note that, it achieves this performance gains despite having the same prior information as the map traversal algorithms. On the other hand, although the Bug* algorithms uses additional information about the environment map and target location, ClipRover ($95\%$) still outperforms outperforms Bug1 ($6.34 \times D_\text{baseline}$) by trajectory lengths, and offers significantly higher success rates than Bug0 ($45\%$) and Bug2 ($80\%$) algorithms.


In summary, the proposed system outperforms traditional map traversal algorithms by significant margins in both success rate and trajectory efficiency, given the same amount of information. Despite the inherent disadvantage of operating without prior target knowledge or precise localization, the system achieves performance comparable to path-finding algorithms. Notably, in many scenarios, it surpasses path-finding algorithms in either trajectory efficiency or success rates. These results highlight the effectiveness of the proposed pipeline for simultaneous exploration and target discovery tasks.



\input{res/comparison_vln.tex}

\subsection{Comparative Analyses of SOTA VLNs}
We also compare ClipRover with state-of-the-art VLN systems based on their features and prerequisites. The characteristics of each system are summarized in Table~\ref{tab:vln-comparison}. Compared to existing systems, ClipRover uniquely requires zero prior knowledge of the task space and operates solely with monocular RGB perception. Additionally, contemporary VLN systems do not directly produce low-level motion commands but instead generate high-level instructions that depend on another traditional localization and navigation pipeline for the eventual motion execution. In contrast, ClipRover directly outputs low-level motion commands, eliminating the need for auxiliary control and localization systems. This feature makes ClipRover particularly suitable for rapid deployment on low-cost robots with limited computational and sensing resources.



It is important to note that, although the calculation of the bare SPL metric (Eq.~\ref{eq:SPL}) for ClipRover is possible, it is infeasible to conduct a fair quantitative comparison against the aforementioned systems. For example, SEEK \cite{Ginting2024Seek} reported $65${\small$\pm35$}$\%$ SPL for random walk, while our random walk simulation yielded $8.4${\small$\pm11$}$\%$ SPL, showing a magnitude of difference. Preliminary analysis suggests that this discrepancy stems from SEEK's reliance on a node-graph map structure, which reduces the problem space to a finite set of graph nodes and edges. In contrast, ClipRover operates continuously in an unknown environment, resulting in a substantially larger problem space facilitating active robot navigation with no prior information or pre-compiled knowledge graphs.


\vspace{1mm}
\section{Limitations, Failure Cases, and Potential Improvements of ClipRover}
\subsection{Texture-less or Transparent Surfaces}
As a vision-driven system, the proposed ClipRover pipeline encounters challenges in environments with texture-less or transparent surfaces. When presented with a blank or non-informative image, the inherent VLM struggles to generate meaningful responses, disrupting the entire pipeline. For instance, as demonstrated in Fig.~\ref{fig:exp-look-around}\,d (C1), the robot mistakenly classified a water tank with transparent walls as navigable space. This error occurred because the VLM perceived the interior of the tank, visible through its transparent walls, as accessible terrain. To mitigate this issue, a standard deviation threshold was introduced to filter out ambiguous tiles. While this approach improved performance, the system still exhibits a general tendency to misclassify transparent or uniformly colored surfaces, such as blank walls or white floors, as navigable space. These challenges persist, particularly when the robot is positioned close to such surfaces, where the lack of texture further confuses the VLM. Note that this issue with transparent and texture-less walls has been an open problem for 2D indoor navigation by mobile robots~\cite{zhou2017fast}. 

%As a pure vision-driven system, our proposed pipeline does not handle well in environments with texture-less or transparent surfaces. When a blank or meaningless image is presented to the VLM, the model will likely fail to generate a meaningful response. This will in turn disrupt the entire pipeline. As shown in Fig.~\ref{fig:exp-look-around}d (C1), the robot falsely classified a water tank with transparent walls as navigable space because it saw through the transparent wall and perceived it's inside as navigable space. The problem has been mitigated with the introduction of the standard deviation threshold on each tile. A general tendency to misclassify transparent surfaces as navigable spaces can still be observed across experiments. This also applies when the robot is too close against a uniformly colored surface, which makes little difference with a white floor to the VLM.

\subsection{Open Space and Artificially Constructed Mazes}
The system's reliance on the semantic understanding capabilities of the VLM limits its effectiveness in large, open spaces. Without sufficient visual cues or meaningful objects in the environment, the robot cannot optimize its exploration based on semantic information. In such cases, the robot defaults to moving straight ahead until it encounters visually significant objects, such as obstacles, at which point its correlation middleware resumes providing meaningful guidance. This limitation reduces the system's efficiency in environments devoid of distinguishing visual features.

%One of the core ideas in the design of our system is to leverage VLM's ability to understand the semantics of a scene. However, this also means that the system is not well equipped to handle open spaces. In an open space, the robot will not be able to optimize it's exploration based on the semantical information provided by the VLM due to lack of visual cues. In this case, the robot will always go straight ahead until it regains visual contact to meaningful objects, \eg~obstacles, and then it's correlation middlewares will start providing meaningful information.

Moreover, our experiments in artificially constructed mazes, such as those built from paper boxes, revealed limitations in the VLM's ability to differentiate between the maze walls such as monochrome cardboards, and floors. As a result, the robot either failed to navigate or attempted to collide with the maze walls. This behavior can be attributed to the lack of similar examples in the VLMâ€™s training data, which likely did not include artificially constructed environments of this nature. Consequently, the system is better suited to real-world scenarios where the VLM has been trained on relevant, diverse visual data. This observation suggests that our proposed system will work most effectively in real-world environments where VLMs generally work well.

%Before using the high-bay lab shown in Fig.~\ref{fig:env}, we attempted to build a maze out of paper boxes for testing. However, preliminary experiments turned out unsuccessful because the VLM cannot reliably tell the difference between brown paper box walls apart from white floor. The robot either did not go anywhere or attempted to crush into the boxes. This is understandable since the model's training data does not likely contain many samples of mazes built from paper boxes. Hence it does not generalize well to this scenario.

\subsection{Familiarity Saturation}
\vspace{-1mm}
The \textit{familiarity} middleware is designed to help the robot avoid revisiting already explored areas by accumulating memory of the environment. However, during extended missions, the familiarity database tends to saturate due to the repetitive nature of objects in the environment. Once all unique objects are recorded, the system struggles to prioritize unexplored areas effectively. Future iterations of the system could implement a memory decay mechanism, allowing older entries in the familiarity database to fade over time, thereby prioritizing new observations and maintaining exploration efficiency.

%The familiarity middleware was designed to accumulate memory of the environment in order to avoid revisiting the same place. However, we observed that the familiarity score tends to saturate in an extended mission. This is caused by the fact that many object in the environment are repetitive, and the familiarity database will eventually pick up all of them. In our current version of implementation, there lacks a mechanism for our familiarity database to gradually fade out older memory in favor of new ones.

\subsection{Adaptive Slicing Strategies}
\vspace{-1mm}
In our current implementation of ClipRover, a fixed slicing strategy is used based on the camera frame's aspect ratio, which generally performs well in general. However, in specific scenarios involving narrow navigable spaces (\eg, a $50$\,cm gap between two obstacles), the robot often fails to recognize these spaces as traversable. This is because the narrow passage does not occupy an entire tile, leaving obstacles visible in all tiles and leading the system to avoid the space rather than navigate through it. Future improvements could include the development of advanced slicing strategies that dynamically adapt the region of interest (ROI) based on the surrounding environment. This could be achieved using sparse object detection (bounding box) or pixel-wise segmentation models, enabling better handling of narrow spaces.

%In the current implementation, the slicing strategy is predetermined based on the aspect ratio of the camera frame. This worked well for simple test environments. However, we observed cases when the robot failed to recognize narrow navigable spaces (\eg a $50\,\text{cm}$ wide space between two paper boxes) because the space is not large enough to fill a tile, leaving visible obstacles in all tiles. This causes the system to avoid the space instead of navigating through it. In future work, a more advanced slicing strategy can be implemented to adaptively foveate its region of interest (ROI) depending on perceived surroundings. This can be achieved by either a bonding-box generation model, or a pixel-wise segmentation model.

\subsection{High-Level Reasoning and Task Narration}
\vspace{-1mm}
One of the most promising extensions of this work lies in incorporating high-level reasoning and task narration using an integrated LLM. With this addition, the robot could leverage common-sense reasoning to make more intelligent navigation decisions. For example, when prompted to locate an item in a different room, the robot could infer that heading toward a door is the most logical action. Moreover, in bandwidth-limited applications, the LLM could provide real-time task narration by interpreting visual embeddings to describe mission progress. This approach would significantly reduce the communication bandwidth required compared to streaming raw video. While the architecture lays the foundation for these enhancements, their implementation remains a future work. By addressing these limitations and pursuing the outlined improvements, ClipRover can evolve into a more robust, efficient, and versatile platform for VLN in complex real-world environments.

%One of the most exciting addition to our system is the ability to understand and reason about the environment at a higher level with the help from a large language model (LLM). For example, the robot can be use common sense to prefer going to a door when it's prompted to find an item in a different room. In addition, in bandwidth limited applications, a remote operator can understand the progress of a mission by using a LLM to narrate the stream of visual embeddings, which requires significantly lower bandwidth than the raw video stream. Our existing architecture already paved the way for such possibilities, but the implementation is left for future work.
