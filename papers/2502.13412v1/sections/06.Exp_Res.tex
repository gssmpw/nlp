\section{EXPERIMENTAL RESULTS}
This section delves into four RQs to evaluate and discuss our methodâ€™s performance.

\subsection{What is the optimal threshold in the KG filtering module?}\label{sec: RQ1}
\subsubsection{Motivation}
In the KG Filtering module, we apply the association rule to filter out invalid type triples.
The association rule involves three key metrics: support, confidence, and lift.
Setting appropriate thresholds for these metrics is crucial for ensuring the effectiveness of the KG filtering module and the reliability of the KG.
This RQ aims to explore the optimal thresholds for these metrics to balance the reliability of the KG with the richness of the API knowledge it contains.

\subsubsection{Methodology}
Due to the extremely large number of possible threshold combinations, it is unrealistic to enumerate all cases. Therefore, we select five representative cases.
In these cases, the support, confidence, and lift are gradually increased from low to high.
The specific details are as follows:
\begin{itemize}[leftmargin=*]
\item Case 1: support = 0.001; confidence = 0.005; lift = 0.6
\item Case 2: support = 0.003; confidence = 0.01;  lift = 0.8
\item Case 3: support = 0.005; confidence = 0.02;  lift = 1.0
\item Case 4: support = 0.007; confidence = 0.03;  lift = 1.2
\item Case 5: support = 0.009; confidence = 0.04;  lift = 1.4
\end{itemize}

We collected the KG schemas generated by the KG filtering module under different thresholds, as well as the KGs constructed based on these schemas.
Then, we collect the outputs corresponding to the texts in the test set, which are used to calculate the experimental metrics P-R-F1.
To assess the effectiveness of the type triples, we invite two PhD students (both with more than 4 years of Java development experience) to annotate the type triples output by the fully connected KG schema generation unit.
To resolve conflicts between them, we assign another PhD student, who does not participate in the annotation process.
Finally, we calculate the Cohen's Kappa coefficient~\cite{Cohen1960ACO}, which results in 0.86, indicating almost perfect agreement among the annotators.
Based on this, we calculate the accuracy of the type triples output by the KG filtering module in each case. 


\begin{table*}[t]
\centering
\caption{KG Construction Performance Across Different Thresholds.}
\label{tab: res12}
\begin{tabular}{c|ccc|ccc|ccc|ccl|lll}
\hline
\multirow{2}{*}{Threshold} & \multicolumn{3}{c|}{Case 1}                                  & \multicolumn{3}{c|}{Case 2}                                  & \multicolumn{3}{c|}{Case 3}                                           & \multicolumn{3}{c|}{Case 4}                                                     & \multicolumn{3}{c}{Case 5}                                                     \\ \cline{2-16} 
                           & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1            & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & \multicolumn{1}{c}{F1} \\ \hline
@0.90                      & \multicolumn{1}{c|}{0.53} & \multicolumn{1}{c|}{0.84} & 0.65 & \multicolumn{1}{c|}{0.54} & \multicolumn{1}{c|}{0.84} & 0.66 & \multicolumn{1}{c|}{0.67} & \multicolumn{1}{c|}{0.84} & \textbf{0.75} & \multicolumn{1}{c|}{0.68} & \multicolumn{1}{c|}{0.62} & 0.65                    & \multicolumn{1}{l|}{0.70} & \multicolumn{1}{l|}{0.55} & 0.62                   \\ \hline
@0.92                      & \multicolumn{1}{c|}{0.51} & \multicolumn{1}{c|}{0.82} & 0.63 & \multicolumn{1}{c|}{0.53} & \multicolumn{1}{c|}{0.82} & 0.64 & \multicolumn{1}{c|}{0.66} & \multicolumn{1}{c|}{0.82} & \textbf{0.73} & \multicolumn{1}{c|}{0.68} & \multicolumn{1}{c|}{0.60} & 0.64                    & \multicolumn{1}{l|}{0.69} & \multicolumn{1}{l|}{0.54} & 0.61                   \\ \hline
@0.94                      & \multicolumn{1}{c|}{0.49} & \multicolumn{1}{c|}{0.80} & 0.61 & \multicolumn{1}{c|}{0.51} & \multicolumn{1}{c|}{0.80} & 0.62 & \multicolumn{1}{c|}{0.64} & \multicolumn{1}{c|}{0.80} & \textbf{0.71} & \multicolumn{1}{c|}{0.66} & \multicolumn{1}{c|}{0.59} & 0.62                    & \multicolumn{1}{l|}{0.68} & \multicolumn{1}{l|}{0.53} & 0.60                   \\ \hline
\end{tabular}
\end{table*}

\begin{table}[t]
\centering
\caption{KG Schema Validity Across Different Thresholds.}
\label{tab: res11}
\begin{tabular}{c|c|c|c|c|c}
\hline
Metric & Case 1 & Case 2 & Case 3 & Case 4 & Case 5 \\ \hline
\#Total      & 79              & 48              & 34              & 23               & 27              \\ \hline
\#Correct    & \textbf{30}              & 29              & 26              & 18               & 13              \\ \hline
Accuracy        & 0.43            & 0.60            & 0.76            & \textbf{0.78}            & 0.82           \\ \hline
\end{tabular}
\begin{tablenotes}
\small
\item Note: \#Correct refers to the number of correct type triples among the valid type triples output by the KG filtering module.
\end{tablenotes}
\vspace{-4mm}
\end{table}


\subsubsection{Result}
The validity of the KG schema output by our method in various cases is shown in Table~\ref{tab: res11}.
As the threshold increases, the accuracy of the type triples gradually improves, reaching its highest value of 0.82 in Case 5.
However, the number of correct type triples significantly decreases, with only 14 correct type triples remaining in Case 5.
For our method, an excess of incorrect type triples leads to a large number of suspicious instance triples in the KG, reducing its richness.
On the other hand, too few incorrect type triples result in the loss of some instance triples, impacting the utility of the KG.
Therefore, the KG schema in Case 3 is the most balanced, as its accuracy (0.76) is close to the maximum, while the number of correct type triples (26) remains relatively high.

The KG construction performance of the method is shown in Table~\ref{tab: res12}.
In the three similarity measurement scenarios, as the threshold increases, the method's F1 score first rises and then falls, reaching its highest value in Case 3.
This is because a low threshold introduces incorrect type triples, leading to suspicious instance triples in the KG, which reduces its reliability.
For example, based on incorrect triples (\textit{class}, \textit{equivalence}, \textit{method}), a suspicious instance triple like (\textit{ArrayList}, \textit{similar to}, \textit{Collections.reverse}) can be extracted. 
On the other hand, a high threshold may remove correct type triples and their instance triples, decreasing the richness of the KG.
For example, the KG schema output in Case 5 does not contain the correct type triple (\textit{class}, \textit{difference}, \textit{class}).
Therefore, to balance the reliability and richness of the KG, we adopt the threshold used in Case 3.

\insightbox{
\textbf{Answer:}
Both too low and too high thresholds can impact the effectiveness of the KG.
The threshold in Case 3 is the optimal one, providing the most balanced KG schema.
}


\begin{table*}[t]
\centering
\caption{The Performance of Existing Methods for KG Construction.}
\label{tab: rq22res}
\begin{tabular}{c|ccc|lll|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Threshold} & \multicolumn{3}{c|}{MKC~\cite{Manual}}  & \multicolumn{3}{c|}{APIRI~\cite{yanbang2}} & \multicolumn{3}{c|}{GraphRAG~\cite{GraphRAG}} & \multicolumn{3}{c|}{EDC~\cite{EDC}} & \multicolumn{3}{c}{Our} \\ \cline{2-16} 
& \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   \\ \hline
@0.90                      & \multicolumn{1}{c|}{0.52} & \multicolumn{1}{c|}{0.12} & 0.19 & \multicolumn{1}{l|}{0.51} & \multicolumn{1}{l|}{0.28} & 0.36                    & \multicolumn{1}{c|}{0.31} & \multicolumn{1}{c|}{0.51} & 0.39 & \multicolumn{1}{c|}{0.56} & \multicolumn{1}{c|}{0.62} & 0.59 & \multicolumn{1}{c|}{0.67} & \multicolumn{1}{c|}{0.84} & \textbf{0.75} \\ \hline
@0.92                      & \multicolumn{1}{c|}{0.48} & \multicolumn{1}{c|}{0.11} & 0.18 & \multicolumn{1}{l|}{0.47} & \multicolumn{1}{l|}{0.24} & 0.32                    & \multicolumn{1}{c|}{0.22} & \multicolumn{1}{c|}{0.33} & 0.26 & \multicolumn{1}{c|}{0.54} & \multicolumn{1}{c|}{0.59} & 0.56 & \multicolumn{1}{c|}{0.66} & \multicolumn{1}{c|}{0.82} & \textbf{0.73} \\ \hline
@0.94                      & \multicolumn{1}{c|}{0.37} & \multicolumn{1}{c|}{0.09} & 0.14 & \multicolumn{1}{l|}{0.44} & \multicolumn{1}{l|}{0.20} & 0.28                    & \multicolumn{1}{c|}{0.14} & \multicolumn{1}{c|}{0.18} & 0.15 & \multicolumn{1}{c|}{0.50} & \multicolumn{1}{c|}{0.54} & 0.52 & \multicolumn{1}{c|}{0.64} & \multicolumn{1}{c|}{0.80} & \textbf{0.71} \\ \hline
\end{tabular}
\end{table*}

\begin{table}[t]
\centering
\caption{The Accuracy of AI Units}
\label{tab: rq21res}
\begin{tabular}{c|c}
\hline
AI Unit                           & Accuracy \\ \hline
Entity Extraction                 & 0.83 \\ \hline
Relation Extraction               & 0.78 \\ \hline
Entity Type Labeling              & 0.81 \\ \hline
Entity Type Fusion                & 0.93 \\ \hline
Relation Type Fusion              & 0.82 \\ \hline
Schema-Guided Entity Extraction   & 0.79 \\ \hline
Schema-Guided Relation Extraction & 0.74 \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}

\subsection{How well does our method perform in KG construction?}\label{sec: RQ2}
\subsubsection{Motivation}
In this paper, we design three main modules to achieve the automated construction of the API KG.
These three modules contain various AI units, such as entity extraction, entity type annotation, and so on.
This RQ aims to explore whether these AI units are effective and to investigate the performance of our method in constructing the API KG.

\subsubsection{Methodology}
We input the seed text into the KG exploration module and collect the outputs (e.g., entities, relations, etc.) of the AI units within it.
Subsequently, we input the test set into the KG construction module and gather the outputs of the AI units within it (e.g., entity types: entity instances, etc.).
Following this, we invite six PhD students (all with over four years of Java development experience) to annotate these results.
The annotation process is as described in Section~\ref{sec: data pre}.
We calculate the Cohen's Kappa coefficient~\cite{Cohen1960ACO} of the annotation results, which were 0.81 and 0.83 respectively, indicating almost perfect agreement among the annotations.
Based on the annotation result, we calculate the accuracy of each unit.
At the same time, we apply all the texts to be extracted to both the existing method and our method, and collect the output results corresponding to the texts in the test set.
We then calculate P-R-F1 score to compare the performance of the methods.

\subsubsection{Result}
The accuracy of the AI units is shown in Table~\ref{tab: rq21res}.
All AI units perform well, with the entity type fusion unit achieving the highest accuracy of 0.93.
It is observed that the more complex the task, the lower the accuracy of the unit.
Additionally, the accuracy of AI units related to relations is generally lower than that of units related to entities.
For example, the schema-guided relation extraction unit has an accuracy of 0.79, as it only needs to ensure the correctness of the identified entities and their types.
On the other hand, the schema-guided relation extraction unit has an accuracy of 0.74, because it not only needs to accurately identify instance triples but also ensure the correctness of their type triples.
Even so, the accuracy of both the schema-guided entity extraction and schema-guided relation extraction units exceeds 70\%, which effectively ensures the reliability of the constructed API KG.

The comparison of the number of entity types, relation types, and type triples in the KG schemas constructed by the schema-based methods (MKC~\cite{Manual} and our method) is shown in Fig.~\ref{fig: schema comparison}.
For entity types, MKC identifies three entity types, while our method identifies four, with the additional entity type ``interface'' (the specific details are shown in Table~\ref{tab: existing schemas}).
For relation types, MKC and our method share 8 semantically overlapping relation types, but MKC has one unique relation type (function-opposite), while our method has an additional 5 unique relation types (e.g., modification and containment, as detailed in Section~\ref{section: newrel}).
Finally, our method generates 26 correct type triples, of which 8 overlap with MKC's type triples, and the remaining 18 are unique.
MKC also has one unique type triple (\textit{method}, \textit{function opposite}, \textit{method}) due to its unique relation type.
Moreover, although EDC~\cite{EDC} can refine relation types through its strategy, many of these types have similar semantics (e.g., check and test) and could be further merged, as described in Section~\ref{section: newrel}.
In summary, our method can explore a more comprehensive KG schema, laying the foundation for KG construction.

Table~\ref{tab: rq22res} presents the experimental results of different methods for KG construction.
In all three similarity scenarios, our method outperforms existing methods.
Among them, MKC performs the worst.
First, due to the limited entity types and relation types in its KG schema, it is difficult to construct a knowledge-rich KG.
Second, MKC uses a rule-based extraction method, which ensures high precision, but the strict rules result in missing instance triples, leading to low recall and, consequently, poor overall performance.
APIRI~\cite{yanbang2} uses LLM to extract API knowledge, and although its performance improves, it is still limited by the small number of relation types in the KG schema, with a maximum F1 score of only 0.36.
For GraphRAG~\cite{GraphRAG}, the lack of KG schema guidance during the extraction process leads to noise in the results.
For example, GraphRAG incorrectly classifies the Java Virtual Machine (Java VM) as an API entity.
As a result, its highest F1 score is only 0.39.
Additionally, its performance fluctuates significantly across all three scenarios, indicating instability in the prompt design.
GraphRAG uses descriptive sentences to represent relations, failing to accurately capture the textual semantics, which leads to inaccurate relation instances.
The state-of-the-art method EDC performs better than the previous two methods, but its extraction process also lacks KG schema guidance, preventing it from focusing on API entity objects, which leads to noise.
For example, it extracts incorrect instance triples such as (\textit{FilterWriter}, \textit{is a}, \textit{class}).
Compared with EDC, our method improves the F1 score of KG construction by 25.2\%.
Our method extracts API knowledge based on the KG schema with diverse entity types and relation types, enabling the construction of both comprehensive and 
reliable API KG.

\begin{figure}[t]
    \centering
    \subfloat[]{%
        \includegraphics[width=0.31\linewidth]{materials/figs/experiment/newres21.pdf}%
    }
    \hfill
    \subfloat[]{%
        \includegraphics[width=0.31\linewidth]{materials/figs/experiment/newres22.pdf}%
    }
    \hfill
    \subfloat[]{%
        \includegraphics[width=0.31\linewidth]{materials/figs/experiment/newres23.pdf}%
    }
    \caption{The Number of Entity Types (a), Relation Types (b) and type triples (c) in Different KG Schemas.}
    \label{fig: schema comparison}
    \vspace{-4mm}
\end{figure}

\insightbox{
\textbf{Answer:}
The AI units we designed can efficiently complete various 
tasks, ensuring the construction of the KG.
Our method overcomes the limitations of existing method, discovering a rich variety of entity and relation types, thereby constructing a practical and reliable KG.}


\subsection{Is the Explore-Construct-Filter framework effective?}\label{sec: RQ3}
\subsubsection{Motivation}
To enhance the richness and reliability of the API KG, we propose the exploration-construct-filter framework and design three core modules: KG exploration, KG construction, and KG filtering. 
This RQ aims to verify whether this strategy can enhance the effectiveness of the KG.

\subsubsection{Methodology}
In this RQ, we design three variant methods, as described in Section~\ref{sec: baseline}.
For each variant method, we use the same data as in RQ2 (seed texts, texts to be extracted, and the test set) and collect their output results.
By comparing the extracted instance triples with the ground truth in the test set, we calculate precision, recall, and F1 scores to evaluate the performance of each method.

\begin{table*}[t]
\centering
\caption{KG Construction Performance of Variant Methods.}
\label{tab: res32}
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Threshold} & \multicolumn{3}{c|}{Our$_{\text{w/oKE}}$}                    & \multicolumn{3}{c|}{Our$_{\text{w/oKF}}$}                    & \multicolumn{3}{c|}{Our$_{\text{w/oFC}}$}                    & \multicolumn{3}{c}{Our}                                      \\ \cline{2-13} 
                           & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   \\ \hline
@0.90                      & \multicolumn{1}{c|}{0.59} & \multicolumn{1}{c|}{0.24} & 0.34 & \multicolumn{1}{c|}{0.47} & \multicolumn{1}{c|}{0.84} & 0.60 & \multicolumn{1}{c|}{0.64} & \multicolumn{1}{c|}{0.56} & 0.60 & \multicolumn{1}{c|}{0.67} & \multicolumn{1}{c|}{0.84} & \textbf{0.75} \\ \hline
@0.92                      & \multicolumn{1}{c|}{0.53} & \multicolumn{1}{c|}{0.22} & 0.31 & \multicolumn{1}{c|}{0.44} & \multicolumn{1}{c|}{0.82} & 0.57 & \multicolumn{1}{c|}{0.62} & \multicolumn{1}{c|}{0.55} & 0.58 & \multicolumn{1}{c|}{0.66} & \multicolumn{1}{c|}{0.82} & \textbf{0.73} \\ \hline
@0.94                      & \multicolumn{1}{c|}{0.47} & \multicolumn{1}{c|}{0.21} & 0.29 & \multicolumn{1}{c|}{0.43} & \multicolumn{1}{c|}{0.80} & 0.56 & \multicolumn{1}{c|}{0.60} & \multicolumn{1}{c|}{0.53} & 0.56 & \multicolumn{1}{c|}{0.64} & \multicolumn{1}{c|}{0.80} &\textbf{0.71} \\ \hline
\end{tabular}
\end{table*}

\begin{table}[t]
\centering
\caption{KG Schema Validity of Different Methods.}
\label{tab: res31}
\begin{tabular}{c|c|c|c|c}
\hline
Metric     &  Our$_{\text{w/oKE}}$  & Our$_{\text{w/oKF}}$   & Our$_{\text{w/oFC}}$     & Our \\ \hline
\#Total    &  8                     & 208                    & 20                     & 34   \\ \hline
\#Correct  &  8                     & \textbf{31}                     & 18                     & 26    \\ \hline
Accuracy   &  1.00                  & 0.15                     & \textbf{0.90}                   & 0.76   \\ \hline
\end{tabular}
\vspace{-2mm}
\end{table}

\subsubsection{Result}
The results of the KG schema validity experiment are shown in Table~\ref{tab: res31}.
We can observe that the type triple accuracy of Our$_{\text{w/oKE}}$ is also 1.00 due to it uses the KG schema from MKC.
However, some type triples (e.g., ($method$, $function\ opposite$ $method$)) have a low frequency of occurrence and are removed by the KG filtering module. 
Due to the lack of the KG filtering module, Our$_{\text{w/oKF}}$ retains all 208 type triples generated by the KG exploration module, but only 31 of them are correct, resulting in an accuracy of only 0.15.
On the other hand, Our$_{\text{w/oKF}}$, lacking the fully connected strategy, maintains relatively high accuracy due to the KG filtering module.
However, the number of correct type triples is much smaller than that in our KG schema, thus it cannot comprehensively extract instance triples.
In summary, by comparing these methods, it can be observed that the accuracy of the type triples and the number of correct type triples in our method are relatively high.

As shown in Table~\ref{tab: res32}, the experimental results demonstrate that our method surpasses all variant methods in KG construction.
Specifically, the comparison between Our$_{\text{w/oKE}}$ and our method reveals that the exploration module significantly enhances the richness of the KG, achieving an average improvement of 133.6\% across three similarity thresholds.
The absence of the KG exploration module in Our$_{\text{w/oKE}}$ leads to a lower recall rate for instance triple extraction, primarily because some valid type triples are lost, thereby hindering the extraction of diverse instance triples.
Conversely, Our$_{\text{w/oKF}}$ removes the KG filtering module and includes numerous suspicious instance triples, thereby reducing precision in instance triple extraction.
In contrast, our method incorporates the KG filtering module, which on average improves the richness of the KG by 26.6\%.
Moreover, when compared to Our$_{\text{w/oFC}}$, our method benefits significantly from the fully connected strategy, improving the comprehensiveness of the KG by an average of 33.5\%.
Due to the lack of a fully connected strategy, Our$_{\text{w/oFC}}$ overlooks some valid type triples, which leads to a lower recall rate.
Furthermore, the comparison between Our$_{\text{w/oKE}}$ and Our$_{\text{w/oFC}}$ highlights that the F1 score of Our$_{\text{w/oFC}}$ averages 0.58, significantly higher than the F1 score of Our$_{\text{w/oKE}}$, which further emphasizes the ability of the KG exploration module to uncover a wide array of entity and relation types.

\begin{table}[t]
\centering
\caption{KG Schema Validity across Different LLMs.}
\label{tab: res41}
\begin{tabular}{c|c|c|c}
\hline
Metric       &  Our$_{\text{Llama}}$  & Our$_{\text{Claude}}$        & Our     \\ \hline
\#Total      &  30                     &  32                          & 34      \\ \hline
\#Correct    &  20                    &  23                          & \textbf{26}      \\ \hline
Accuracy     &  0.67                  & 0.72                         & \textbf{0.76}     \\ \hline
\end{tabular}
\end{table}

\insightbox{
\textbf{Answer:}
The explore-construct-filter framework is effective and indispensable.
This framework can significantly improve the richness and reliability of the API KG.

}


\subsection{How generalizable is our method across different LLMs?}\label{sec: RQ5}
\subsubsection{Motivation}
In this paper, we propose an automated method based on the LLM (GPT-4) to construct API KGs.
For this RQ, our goal is to verify whether different LLMs impact our method.

\subsubsection{Methodology}
We select two other popular LLMs, Llama-3.1-70b~\cite{Llama-3.1} and Claude-3.5-Sonnet~\cite{claude-3-5-sonnet}, both of which have demonstrated excellent performance across various natural language processing tasks.
Furthermore, we use the same experimental methodology as before to calculate the metrics for the different methods.

\begin{table*}[t]
\centering
\caption{KG Construction Performance Across Different LLMs.}
\label{tab: rq42}
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Threshold} & \multicolumn{3}{c|}{Our$_{\text{Llama}}$}                                   & \multicolumn{3}{c|}{Our$_{\text{Claude}}$}                                  & \multicolumn{3}{c}{Our}                              \\ \cline{2-10} 
                           & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   & \multicolumn{1}{c|}{P}    & \multicolumn{1}{c|}{R}    & F1   \\ \hline
@0.90                      & \multicolumn{1}{c|}{0.66} & \multicolumn{1}{c|}{0.63} & 0.65 & \multicolumn{1}{c|}{0.68} & \multicolumn{1}{c|}{0.71} & 0.70 & \multicolumn{1}{c|}{0.67} & \multicolumn{1}{c|}{0.84} & \textbf{0.75} \\ \hline
@0.92                      & \multicolumn{1}{c|}{0.65} & \multicolumn{1}{c|}{0.62} & 0.64 & \multicolumn{1}{c|}{0.67} & \multicolumn{1}{c|}{0.70} & 0.69 & \multicolumn{1}{c|}{0.66} & \multicolumn{1}{c|}{0.82} & \textbf{0.73} \\ \hline
@0.94                      & \multicolumn{1}{c|}{0.64} & \multicolumn{1}{c|}{0.60} & 0.62 & \multicolumn{1}{c|}{0.65} & \multicolumn{1}{c|}{0.67} & 0.66 & \multicolumn{1}{c|}{0.64} & \multicolumn{1}{c|}{0.80} & \textbf{0.71} \\ \hline
\end{tabular}
\end{table*}


\subsubsection{Result}
As shown in Table~\ref{tab: schema3}, both variant methods' KG exploration modules output 4 entity types and 13 relation types, which are identical to the entity and relation types generated by our method.
This indicates that our method demonstrates high stability when exploring KG schema, with minimal influence from the base model.
Furthermore, the accuracy of the type triples output by the KG filtering module for each method is shown in Table~\ref{tab: res41}.
Our method performs the best, with a type triple accuracy of 0.76 and 26 correct type triples.
Our$_{\text{Claude}}$ follows closely, with an accuracy near 0.72, generating 23 correct type triples.
Our$_{\text{Llama}}$ performs slightly weaker, generating 20 correct type triples, but its accuracy still reaches 0.67.
Although the our method outperforms the others in both accuracy and the number of correct type triples, Our$_{\text{Claude}}$ and Our$_{\text{Llama}}$ are still capable of generating relatively accurate type triples to construct a reliable KG.

The performance of KG construction is shown in Table~\ref{tab: rq42}.
Compared to Our$_{\text{Claude}}$ and Our$_{\text{Llama}}$, our method performs better in KG construction.
This is attributed to GPT-4o's training on large-scale corpora and its powerful model parameters, which enhance its information extraction capabilities.
Furthermore, the number and accuracy of the type triples generated by our method outperform those of other methods, improving the performance of KG construction and ensuring that the filtered KG maintains both richness and reliability.
Nevertheless, the highest F1 scores of methods Our$_{\text{Claude}}$ and Our$_{\text{Llama}}$ are still 0.70 and 0.65, respectively, indicating that even when the base model in our method is replaced with a weaker model, it can still generate an effective and reliable KG.
In summary, our method demonstrates good generalizability across different models, and as model performance improves, the method's performance will also be enhanced accordingly.

\insightbox{
\textbf{Answer:}
Our method is universal across different models, and the more capable the model is, the better the performance of this method will be.
}
