\section{Background}\label{sec: motivation}
In this section, we first introduce API relations and API KG construction, and then discuss how to use LLMs to enhance the API KG construction process.

\subsection{API Relation}
API relations reflect the semantic connections and interactions between different APIs.
Liu et al.~\cite{Liu2020GeneratingCB} studied the subtle differences between APIs, comparing them from three aspects: categorization, functionality, and characteristics.
Based on this, Huang et al.~\cite{huang2022se} summarized nine types of API semantic relations, including behavior-difference, function-replace, and efficiency-comparison.
For example, the behavior difference relation describes how two similar APIs behave differently when performing the same task, whereas the efficiency comparison relation specifies the variance in efficiency between two APIs under particular conditions.
These API semantic relations are widely present in natural language texts (e.g., API documentation and Q\&A forums) and understanding them is crucial for the correct and efficient use of APIs.
In this paper, we focus more on the semantic relations between APIs rather than their structural relations.
The former requires careful mining from large amounts of textual data, while the latter can be easily obtained from development documentation.

\subsection{API KG Construction}\label{sec: kgcon}
The API KG is a complex network structure designed to represent APIs and their interrelationships.
In an API KG, each node corresponds to an API and includes essential attributes such as name, description, and functionality.
This information enables developers to quickly understand the API's purpose and usage.
The edges in the graph represent semantic relations between APIs, such as constraint and collaboration.
By analyzing these relations, developers can gain a clear understanding of how APIs interact, optimizing system integration.

API KG construction begins with the design of the KG schema, which defines the entity and relation types within the KG.
This schema uses type triples, such as (\textit{$ET_{A}$, $RT_{R}$, $ET_{B}$}), to represent connections between entity types A and B through relation type R.
Guided by this schema, instances are extracted from diverse data sources to form the API KG, represented by instance triples like (\textit{$e_{a}$, $r$, $e_{b}$}), where entity \textit{a} is related to entity \textit{b} via relation \textit{r}.
Therefore, an API KG with a rich set of instances requires a complete KG schema that includes as many type triples as possible.

Most API KG schemas are manually designed by annotators~\cite{huang2022se,Huang2018APIMR,Li2018ImprovingAC}.
These annotators determine the necessary entity and relation types based on domain knowledge and summarize type triples to construct the schema.
However, developing these schemas requires significant time from annotators, resulting in high labor costs.
In this paper, we propose an LLM-based automated method that simulates annotators in summarizing entity and relation types from a small set of texts to generate a complete schema.
Based on it, we further construct a knowledge-rich API KG.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{materials/figs/methodology/killing_example.pdf}    
    \caption{The Comparison of API KG Construction Methods.}
    \label{fig: killing_example}
\end{figure*}

\subsection{LLM for KG Construction}
LLMs, such as GPT-4~\cite{GPT4oAnalysis-1} and Claude~\cite{claude-3-5-sonnet}, are deep learning models trained on vast amounts of text data.
Recently, LLMs are widely applied to downstream tasks.
For instance, some researchers~\cite{yanbang1, yanbang2} attempt to utilize LLMs to extract API entities and relations, thereby constructing an API KG.
However, no work has yet attempted to use LLMs to design a KG schema.
In the schema design phase, LLMs need to reason about various components of the KG schema, such as how to categorize entities and how to define relations between them.
To improve reasoning accuracy, we use the Chain-of-Thought (CoT) method, which breaks down complex tasks into multiple simple tasks, and leverages the LLM to accomplish these simple tasks step by step.

To further enhance the effectiveness of task execution, we introduce the in-context learning method~\cite{Brown2020LanguageMA, Min2022RethinkingTR}.
By providing task descriptions and examples, this method helps LLMs capture patterns and rules within tasks.
% In-context learning is simple and efficient, making it the mainstream approach for transferring LLMs to downstream tasks, especially in scenarios with limited labeled data.
However, research~\cite{PCR, huang2024se} shows that the effectiveness of in-context learning largely depends on the design of prompts, including prompt style, example content, and example order.
To address it, we adopt structured prompts~\cite{xing2025when} to enhance the LLM's performance across various tasks.
By combining these strategies, LLMs can better simulate manual annotators to construct API KGs.
