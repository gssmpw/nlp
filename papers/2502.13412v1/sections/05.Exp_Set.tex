\section{EXPERIMENTAL SETUP}
This section starts with four research questions for evaluating our method's effectiveness, and then introduces baselines, data preparation, and evaluation metrics.

\subsection{Research Questions (RQs)}
\begin{itemize}[leftmargin=*]
\item What is the optimal threshold in the KG Filtering module?
\item How well does our method perform in KG construction?
\item Is the Explore-Construct-Filter framework effective?
\item How generalizable is our method across different LLMs?
\end{itemize}

\subsection{Baselines}\label{sec: baseline}
In the experiments, we implement a total of seven baseline methods to evaluate our method.
% All methods (ours and baselines) are based on the GPT-4o model~\cite{basellm}.
To explore the performance of our method, we propose three baseline methods, namely MKC, APIRI, GraphRAG, and EDC, with details as follows.
\begin{itemize}[leftmargin=*]
    \item Huang et al. \cite{Manual} propose a schema-based method MKC.
    By analyzing the text of API documentation and Stack Overflow posts, they summarized three entity types and nine semantic relation types, and designed a KG schema (see Table~\ref{tab: existing schemas}).
    Finally, they constructed an API KG based on this KG schema using a rule-based method. 
    We obtain its code from Github~\cite{yuangit} and measure the performance of this method by inputting the same test text.
    \item In order to improve the performance of knowledge extraction, Huang et al. propose APIRI~\cite{yanbang2} to extract instance triples based on the KG schema of MKC.
    This method extracts instance triples by querying the LLM for feature knowledge of two APIs (e.g., usage, performance).
    We obtained its code from Github~\cite{APIRICGIT} and tested its performance using the same testing method as MKC.
    \item GraphRAG \cite{GraphRAG} is used to improve the efficiency of retrieving information from the KG.
    It consists of two stages: the indexing stage, where structured data such as entities, relations, and statements are extracted from unstructured text using an LLM, and the querying stage, where information relevant to the user's query is retrieved from the KG.
    This paper focuses only on the first stage, with particular emphasis on entity and relation extraction. 
    Therefore, we obtain the relevant code from Github~\cite{graphraggit} and provide the entity types summarized in MKC to construct an API KG.
    \item Zhang et al.\cite{EDC} design EDC, an automated schema-free method for KG construction. 
    We use their GitHub code\cite{edcgit} to construct an API KG. 
    However, this method normalizes relation instances into relation types but ignores entity types. 
    As a result, the KG schema includes only relation types, and the entity instances lack specific entity types.
\end{itemize}


To evaluate the effectiveness of the Explore-Construct-Filter strategy, we implement three baselines, named Our$_{\text{w/oKE}}$, Our$_{\text{w/oKF}}$, and Our$_{\text{w/oFC}}$, with details as follows.

\begin{itemize}[leftmargin=*]
    \item Our$_{\text{w/oKE}}$ refers to removing the KG exploration module while retaining the KG construction module and the KG filtering module. 
    To ensure the normal execution of our method, we adopted the same KG schema as that of MKC.
    By comparing Our$_{\text{w/oKE}}$ and our method, we can verify whether the KG exploration module can discover more entity types and relation types, thereby enhancing the richness of the KG.
    \item Our$_{\text{w/oKF}}$ means removing the KG filtering module and taking the KG output by the KG construction module as the final result.
    By comparing Our$_{\text{w/oKF}}$ and our method, we can verify whether the KG filtering module can improve the reliability of the API KG.
    \item  Our$_{\text{w/oFC}}$ refers to removing the full combination strategy of entity types and relation types.
    In the KG schema, the type triples are those to which the instance triples in the KG exploration module belong. 
    Compared Our$_{\text{w/oFC}}$ with our method, we can verify if the full combination strategy can generate more reliable type triples to enhance the richness of the API KG, thereby improving the comprehensiveness of the API KG.
\end{itemize}

Note that all methods, including ours and the baselines, are based on the GPT-4o model~\cite{basellm}.
Please refer to the Appendix for the parameter settings of the model.

\subsection{Data Preparation} \label{sec: data pre}
To fairly compare the MKC method~\cite{Manual} with our method, we obtain texts from GitHub that summarize API entity types and relation types for constructing a KG schema. 
These texts are sourced from posts on Stack Overflow and are related to Java APIs, totaling 206 entries.
In this paper, they are treated as seed texts.
Previous work \cite{Manual} also collects 32,505 texts from the Java tutorial~\cite{javatutorial} documentations for constructing the API KG based on KG schema.
However, not all of these texts contain API entities and relations.
For instance, descriptions such as ``This text focuses on the two most common operations: Adding/removing elements...'' do not involve API entities or relations.
To filter out texts that contain API entities and relations, we design three filtering criteria inspired by the API text filtering rules proposed by Huang et al.~\cite{yanbang1}:
\begin{itemize}[leftmargin=*]
    \item Since the method entity usually ends with ``()'', if the text contains ``()'' and the text length is greater than 8 tokens, the text is retained.
    \item The API entity usually contains ``.'' to indicate a function call (e.g., iterator.remove); therefore, if the text contains ``.'' (both before and after are letters) and the text length is greater than 8 tokens, the text is retained.
    \item If the text contains the words ``method'', ``class'', ``package'' (e.g., remove method), and the text length is greater than 8 tokens, the text is retained.
\end{itemize}


\begin{table}[t]
\centering
\caption{The Quality of AI Units}
\label{tab: data_details}
\begin{tabular}{c|c}
\hline
Type Triples                           & Number    \\ \hline
(\textit{class}, \textit{containment}, \textit{method})           &  318      \\ \hline
(\textit{method}, \textit{dependency}, \textit{method})           &  184      \\ \hline
(\textit{class}, \textit{execution}, \textit{method})             &  128      \\ \hline
(\textit{class}, \textit{access}, \textit{method})                &  124      \\ \hline
(\textit{method}, \textit{dependency}, \textit{class})            &  120      \\ \hline
(\textit{method}, \textit{equivalence}, \textit{method})          &  119      \\ \hline
(\textit{method}, \textit{difference}, \textit{method})           &  106      \\ \hline
(\textit{method}, \textit{collaboration}, \textit{method})        &  76       \\ \hline
(\textit{package}, \textit{containment}, \textit{class})          &  75       \\ \hline
(\textit{class}, \textit{implementation}, \textit{interface})     &  72       \\ \hline
\end{tabular}
\end{table}

Using these criteria, we obtain 5,047 texts, which are used to extract API entities and relations for constructing an API KG.
Finally, we discovered 4 entity types and 13 relation types from the seed text, generating 26 verified type triples (see Table~\ref{tab: existing schemas}).
Based on this, we construct a KG containing 1,375 unique entities and 1,843 unique relation instances 
Table~\ref{tab: data_details} shows the ten most frequent type triples in the KG.



However, annotating standard answers for such a large number of texts is extremely labor-intensive.
Therefore, we adopt a sampling method~\cite{Singh1996ElementsOS} that is widely used in previous studies~\cite{Li2018ImprovingAC, Liu2020GeneratingCB}, to ensure that the observed metrics in the sample can be generalized to the entire population.
Thus, at a 95\% confidence level with a confidence interval of 5, we randomly select 384 texts as the test set.
We then invite four PhD students (who are not involved in this study) with over five years of Java development experience to annotate the instance triples in the test set.
During the annotation process, they are divided into two groups, each consisting of two annotators, who independently annotate 192 identical texts.
After the annotation, any conflicts are resolved by an annotator from the other group.
Finally, we calculate the Cohen's Kappa coefficient for the two groups, which are 0.78 and 0.82, indicating almost perfect agreement.
Therefore, we construct the ground truth for this test set, which contains 382 instance triples and 352 unique API entities.

It should be noted that we only annotate instance triples, as they are concrete and can be directly identified from the text, whereas type triples are more abstract, requiring reasoning and categorization, and lack a unified standard, making their annotation more challenging.
To ensure efficient and consistent annotation, we choose to focus solely on instance triples.
The accuracy of type triples can be calculated by annotating the output of the schema-guided entity extraction unit and schema-guided
relation extraction unit, as detailed in Section~\ref{sec: RQ2}.



\subsection{Evaluation Metrics}
In this paper, we employ precision (p), recall (r), and F1-score to evaluate the performance of each method for KG construction.
However, the instance triples extracted by each method may deviate from the ground truth.
For instance, given the sentence ``There is a little difference between forward() and include()...'', the extracted triple could be (\textit{forward()}, \textit{has difference between}, \textit{include()}), whereas the ground truth is (\textit{forward()}, \textit{is different from}, \textit{include()}).
Although the two triples are highly similar, a simple string comparison cannot judge the correctness of the extraction result.

Therefore, we assess the correctness of the extracted triples by calculating their similarity with the ground truth triples.
Specifically, we use a pre-trained BERT model~\cite{devlin2018bert} to generate semantic vectors for the triples and calculate their cosine similarity. It is important to note that we only compute the similarity when both the head entity and the tail entity of the two triples match.
In this study, we define three similarity thresholds: 0.9, 0.92, and 0.94, denoted as @0.9, @0.92, and @0.94, respectively.
Only when the similarity of the extracted triple exceeds the defined threshold is the extraction considered correct.
Based on this, we calculate the precision, recall, and F1 score for instance extraction.




% Inspired by the measurement method proposed in previous work~\cite{APIRI}, we also use accuracy to evaluate the quality of AI units (RQ1), where accuracy is defined as the ratio of the number of correct results (API entities, relations, etc.) to the total number of results (API entities, relations, etc.). 
% Note that, for entity type fusion and relation type fusion units, if the subtypes can be encompassed by the new fused type, the subtypes are marked as correct.

% For the remaining RQs, we adopt accuracy, precision (p), recall (r), and F1 as the experimental metrics.
% Specifically, due to the wide variety of API relations and the indistinct boundaries among them, it is difficult to construct the ground truths for type triples from the seed texts.
% Therefore, we calculate the accuracy of the type triples to measure the effectiveness of the generated KG schema.

% Precision, recall, and F1 score are used to measure the performance of the KG constructed based on the schema.
% However, the instance triples extracted by each method may deviate from the ground truth.
% For instance, given the sentence ``There is a little difference between forward() and include()...'', the extracted triple could be ($forward()$, $has\ difference \ between$, $include()$), whereas the ground truth is ($forward()$, $is\ different\ from$, $include()$).
% Although the two triples are highly similar, a simple string comparison cannot judge the correctness of the extraction result.



