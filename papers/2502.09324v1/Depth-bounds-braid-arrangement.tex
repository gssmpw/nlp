\documentclass[12pt]{amsart}
\pdfoutput=1
\usepackage[foot]{amsaddr}
\usepackage{times}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx} % Required for inserting images
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{url}            % simple URL typesetting
\usepackage{lmodern}
\usepackage{dsfont}
\usepackage{mathtools}
\usepackage{tikz}
\let\todo\relax
\setlength {\marginparwidth }{2cm}
\usepackage{todonotes}
\usepackage{natbib}
\usepackage{xfrac} 
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{xargs}
\usepackage{fullpage}
\usetikzlibrary{fit}
\usepackage{wrapfig}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newcommand{\relu}{\operatorname{ReLU}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\cpwl}{\operatorname{CPWL}}
\newcommand{\cone}{\operatorname{cone}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\FB}[1]{\mathcal{V}_{\mathcal{B}_{#1}}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\La}{\mathcal{L}}
\newcommand{\Sf}[1]{\mathcal{F}_{#1}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\HC}[1]{\mathcal{C}_{#1}}
\newcommand{\ind}{\mathds{1}}

\newcommand{\maxout}[2]{\mathcal{M}_{#1}^{\mathbf{#2}}}
\newcommand{\mtwo}[2]{\mathcal{M}^2_{#1}({#2})}
\newcommand{\asum}[2]{\inner{\al{#1}}{#2}}
\newcommand{\inner}[2]{\langle {#1},{#2} \rangle}
\newcommand{\al}[1]{\alpha_{#1}}


\newcommandx{\moritz}[3][1=, 2=]{\todo[color=teal!50, #1, caption={#2}]{Moritz: #3}}
\newcommandx{\georg}[3][1=, 2=]{\todo[color=purple!50, #1, caption={#2}]{Georg: #3}}
\newcommandx{\christoph}[3][1=, 2=]{\todo[color=orange!50, #1, caption={#2}]{Christoph: #3}}
\newcommandx{\generalfan}[3][1=, 2=]{\todo[color=yellow!50, #1, caption={#2}]{Idea general fan: #3}}

\title[Depth-Bounds for Neural Networks via the Braid Arrangement]{Depth-Bounds for Neural Networks \\via the Braid Arrangement}




\author{Moritz Grillo$^1$}
\address{$^1$Technische Universit\"at Berlin}
\email{grillo@math.tu-berlin.de}

\author{Christoph Hertrich$^2$}
\address{$^2$University of Technology Nuremberg} 
\email{christoph.hertrich@utn.de}

\author{Georg Loho$^{3,4}$}
\address{$^3$Freie Universit\"at Berlin} 
\address{$^4$University of Twente}
\email{georg.loho@math.fu-berlin.de}


\begin{document}

\maketitle

\begin{abstract}
    We contribute towards resolving the open question of how many hidden layers are required in ReLU networks for exactly representing all continuous and piecewise linear functions on~$\R^d$. 
    While the question has been resolved in special cases, the best known lower bound in general is still 2. 
    We focus on neural networks that are compatible with certain polyhedral complexes, more precisely with the braid fan.  
    For such neural networks, we prove a non-constant lower bound of $\Omega(\log\log d)$ hidden layers required to exactly represent the maximum of $d$ numbers. 
    Additionally, under our assumption, we provide a combinatorial proof that 3 hidden layers are necessary to compute the maximum of 5 numbers; this had only been verified with an excessive computation so far.
    Finally, we show that a natural generalization of the best known upper bound to maxout networks is not tight, by demonstrating that a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to represent the maximum of 7 numbers. 
\end{abstract}



\section{Introduction}

Among the various types of neural networks, ReLU networks have become particularly prominent \citep{pmlr-v15-glorot11a,GoodBengCour16}.
For a thorough theoretical understanding of such neural networks, it is important to analyze which classes of functions we can represent with which depth. Classical universal approximation theorems \citep{Cybenko1989ApproximationBS,Hornik1991ApproximationCO} ensure that just one hidden layer can approximate any continuous function on a bounded domain with arbitrary precision. However, establishing an analogous result for \emph{exact} representations remains an open question and is the subject of ongoing research \citep{arora2018understanding,hertrich2023towards,haase2023lower,valerdi2024minimal,averkov2025on}.

While in practical settings approximate representations are often sufficient, studying the exact piecewise linear structure of neural network representations enabled deep connections between neural networks and fields like tropical and polyhedral geometry \citep{huchette2023deep}. These connections, in turn, are important for algorithmic tasks like neural network training \citep{arora2018understanding,goel21training,khalife2022algorithms,froese2022computational,Froese23training,BertschingerHJM23} and verification \citep{li19symbolic,katz17reluplex,froese2024complexitydecidinginjectivitysurjectivity}, including understanding the computational complexity of the respective tasks.

\citet{arora2018understanding} initiate the study of exact representations by showing that the class of functions exactly representable by ReLU networks is the class of continuous piecewise linear (CPWL) functions. Specifically, they demonstrate that every CPWL function defined on $\R^d$ can be represented by a ReLU network with $ \lceil \log_2(d + 1) \rceil$ hidden layers. This result is based on \citet{wang2005generalization}, who reduce the representation of a general CPWL function to the representation of maxima of $d+1$ affine terms. By computing pairwise maxima in each layer, such a maximum of $d+1$ terms can be computed with logarithmic depth overall in the manner of a binary tree.


However, it remains open if there exists a CPWL function on $\R^d$ that really needs $\lceil \log_2(d + 1) \rceil$ hidden layers to be represented. In fact, so far nobody has been able to prove any lower bound that is better than two hidden layers for any CPWL function.

\citet{hertrich2023towards} conjecture that the logarithmic upper bound by \citet{arora2018understanding} is indeed tight and hence the set of representable functions increases with each layer up to logarithmic depth. Formally, if $\relu_d(\ell)$ denotes the set of CPWL functions on $\R^d$ that can be computed with $\ell$ hidden layers, the conjecture is that  $\relu_d(\ell-1) \subsetneq \relu_d(\ell)$ for all $\ell \leq \lceil \log_2(d + 1) \rceil$.

The conjecture has been confirmed in special cases \citep{hertrich2023towards,haase2023lower,valerdi2024minimal,averkov2025on}, but remains unproven in general.
Based on the result by \citet{wang2005generalization}, \citet{hertrich2023towards} deduced that it suffices to determine the minimum depth representation of the maximum function. 
More precisely, if the conjecture is true, then the maximum function is an example contained in $\relu_d(\ell) \setminus \relu_d(\ell-1)$, separating the different depths. Consequently, the conjecture can be \emph{equivalently} written as follows.
\begin{conjecture}[\citep{hertrich2023towards}]
\label{con:2}
    The function $x \mapsto \max\{0,x_1,\ldots,x_d\}$ is not contained in $\relu_d(\ell)$ for $d=2^\ell$. 
\end{conjecture}

While it is easy to show that $\max\{0,x_1,x_2\}$ cannot be represented with one hidden layer \citep{mukherjee2017lower}, it is already open whether two hidden layers are sufficient to represent $\max\{0,x_1,x_2,x_3,x_4\}$.

In order to identify tractable special cases of the conjecture, two approaches have been pursued so far. 
The first restricts the possible \emph{breakpoints} of all neurons in a network computing $x \mapsto \max\{0,x_1,\ldots,x_d\}$. 
A breakpoint of a neuron is an input for which the function computed by the neuron is non-differentiable. 

A neural network is called \emph{$\B^0_d$-conforming} if breakpoints only appear where the ordering of some pair of coordinates changes (i.e., all breakpoints lie on hyperplanes $x_i=x_j$ or $x_i=0$). \citet{hertrich2023towards} confirm  \Cref{con:2} for $\B^0_d$-conforming networks when $d=4$ with a computational proof via a mixed integer programming formulation of the problem. The second approach restricts the weights of the network. \citet{haase2023lower} confirm \Cref{con:2} for the case of integral weights. \citet{averkov2025on} extend the approach by showing that, if all weights are $N$-ary fractions, the max function can only be represented by neural network with depth $\Omega(\frac{\log N}{\log \log d})$. To the best of our knowledge, the two approaches of restricting either the breakpoints or the weights are incomparable.

\subsection{Our contributions}
We follow the approach from \citet{hertrich2023towards} and prove lower bounds on $\B^0_d$-conforming networks. The study of $\B^0_d$-conforming networks is not only motivated through \Cref{con:2}, it also appears in \citet{brandenburg2024decompositionpolyhedrapiecewiselinear} and \citet{froese2024complexitydecidinginjectivitysurjectivity} due to the connection to submodular functions and graphs. Furthermore, we believe that resolving \Cref{con:2} for $\B^0_d$-conforming networks might eventually lead towards resolving it in general, for example by studying different underlying fans instead of focussing on the braid fan as an intermediate step.

In \Cref{sec:loglog} we prove for $d=2^{2^\ell-1}$ that the function $x \mapsto \max\{0,x_1,\ldots,x_d\}$ is not representable with a $\B^0_d$-conforming ReLU network with $\ell$ hidden layers. This means that depth $\Omega(\log\log d)$ is necessary for computing all CPWL functions, yielding the first conditional non-constant lower bound without restricting the weights of the neural networks.

We also study maxout networks as a natural generalization of ReLU networks.
A rank-$r$-maxout unit computes the maximum of $r$ affine functions and can be regarded as a multi-argument generalization of the ReLU activation function. In particular, the set of functions computable with rank-$2$-maxout networks with $\ell$ hidden layers $\mtwo{d}{\ell}$ equals $\relu_d(\ell)$.  We extend the result by proving that for $d= 2^{R^{\ell}-1}$, the function $\max \{0,x_1,\ldots, x_{d}\}$ is not computable by a $\B^0_d$-conforming maxout network with $\ell$ hidden layers where each layer has rank $r_i$ and $R= 2\cdot \max \{r_1,\ldots, r_\ell\}$. 

To prove our results, the first observation is that the set of functions that are representable by a $\B_d^0$-conforming network forms a finite-dimensional vector space.
While one would like to identify subspaces of this vector space representable with a certain number of layers, taking the maximum of two functions does not behave well with the structure of linear subspaces.
To remedy this, we identify a suitable sequence of subspaces $\Sf{\La}(k)$ for $k=1,2,\dots$ that can be controlled through an inductive construction.
These auxiliary subspaces arise from the correspondence between $\B_d^0$-conforming functions and set functions.
This allows us to employ the combinatorial structure of the collection of all subsets of a finite ground set.
This is also reflected in the structure of the breakpoints of $\B_d^0$-conforming functions. 
Hence, we are able to show that applying a rank-$2$-maxout-layer to functions in $\Sf{\La}(k)$ yields a function in $\Sf{\La}(k^2+k)$. 
Iterating this argument yields the desired bounds. 

In \Cref{sec:d=4}, we focus on the case $d=4$. 
We provide a combinatorial proof of the result of \citet{hertrich2023towards} showing that the function $x \mapsto \max\{0,x_1,x_2,x_3,x_4\}$ is not representable by a $\B^0_d$-conforming ReLU network with two hidden layers. 



Finally, in \Cref{sec:3-2}, we study a natural generalization of \Cref{con:2} to maxout networks and show that it fails in this case. 
Given that ReLU layers are equivalent to rank-2-maxout layers, one could conjecture that a maxout network with ranks $r_i$ in the hidden layers $i=1,\dots,\ell$ cannot compute the maximum of $1+\prod_{i=1}^\ell r_i$ numbers. 
We disprove this natural generalization by showing that a maxout network with one rank-$3$ layer and one rank-$2$ layer can compute the maximum of 7 numbers, or equivalently, the function $x \mapsto \max \{0,x_1,\ldots,x_6\}$. This suggests that proving \Cref{con:2} requires leveraging the fact that the dimension is a power of two.

\subsection{Further Related Work}
In light of the prominent role of the max function for neural network expressivity, \citet{safran2024many} showed that the max function can be efficiently approximated with a depth of $\log \log d$.

In an extensive line of research, tradeoffs between depth and size of neural networks have been explored, demonstrating that deep networks can be exponentially more compact than shallow ones \citep{montufar14number,telgarsky16benefits,eldan16depth,arora2018understanding,ergen24topology}. While most of these works also involve lower bounds on the depth, they are usually proven under assumptions on the width. In contrast, we aim towards proving lower bounds on the depth for unrestricted width.
The opposite perspective, namely studying bounds on the size of neural networks irrespective of the depth, has been subject to some research using methods from combinatorial optimization \citep{hertrich2023provably,hertrich2024relu,hertrich2024neural}.

One of the crucial techniques in expressivity questions lies in connections to tropical geometry via Newton polytopes of functions computed by neural networks. This was initiated by \citet{zhang2018tropical}, see also \citet{maragos2021tropical}, and subsequently used to understand decision boundaries, bounds on the depth, size, or number of linear pieces, and approximation capabilities \citep{montufar2022sharp,misiakos2022neural,haase2023lower,brandenburg2024real,valerdi2024minimal,hertrich2024neural}.


\section{Preliminaries}
In \Cref{sec:notation}, the reader can find an overview of the notation used in the paper and in \Cref{sec:proofs} detailed proofs of all the statements. 
\vspace{-0.2cm}
\subsection{Polyhedra}
We review basic definitions from polyhedral geometry; see \cite{ilp_theory, ziegler_lecturespolytopes} for more details.

A \emph{polyhedron} $P$ is the intersection of finitely many closed halfspaces and a \emph{polytope} is a bounded polyhedron. A hyperplane \emph{supports} $P$ if it bounds a closed halfspace containing $P$, and
any intersection of $P$ with such a supporting hyperplane yields a \emph{face} $F$ of $P$. 
A face is a \emph{proper face} if $F \subsetneq P$ and $F \neq \emptyset$ and inclusion-maximal proper faces are referred to as \emph{facets}.
A \emph{(polyhedral) cone} $C \subseteq \R^n$ is a polyhedron such that $\lambda u + \mu v \in C$ for every $u,v \in C$ and  $\lambda, \mu \in \R_{\geq 0}$.
A cone is \emph{pointed} if it does not contain a line.  A cone $C$ is \emph{simplicial}, if there are linearly independent vectors $v_1,\ldots,v_k\in \R^n$ such that $C=\{\sum_{i=1}^k\lambda_i v_i \mid \lambda_i \geq 0\}$. 

A \emph{polyhedral complex} $\mathcal P$ is a finite collection of polyhedra such that (i) $\emptyset \in \mathcal P$, (ii) if $P \in \mathcal P$ then all faces of $P$ are in $\mathcal P$, and (iii) if $P, P' \in \mathcal P$, then $P \cap P'$ is a face both of $P$ and $P'$.
The \emph{lineality space} of a polyhedron $P$ is defined as $\{v \in \R^d \mid x +v \in P$ for all $x \in P\}$. The lineality space of a polyhedral complex $\PP$ is the lineality space of one (and therefore all) $P \in \PP$. 
\subsection{Neural networks and CPWL functions}
A continuous function $f\colon\R^n\to\R$ is called \emph{continuous and piecewise linear} (CPWL), if there exists a polyhedral complex $\PP$ such that the restriction of $f$ to each full-dimensional polyhedron $P\in\mathcal{P}^n$ is an affine function. If this condition is satisfied, we say that $f$ and $\PP$ are \emph{compatible} with each other.
We denote the set of all CPWL functions from $\R^d$ to $\R$ by $\cpwl_d$.


For a number of hidden layers $\ell\geq0$, a \emph{neural network} with \emph{rectified linear unit} (ReLU) activation is defined by a sequence of $\ell+1$ affine maps $T_i:\R^{n_{i-1}}\to\R^{n_i}$, $i\in[\ell+1]$. We assume that $n_0=d$ and $n_{\ell+1}=1$. If $\sigma$ denotes the function that computes the ReLU function $x\mapsto \max\{x,0\}$ in each component, the neural network is said to compute the CPWL function $f\colon\R^d\to \R$ given by $f=T_{\ell+1}\circ\sigma\circ T_\ell\circ\sigma\circ\dots\circ\sigma \circ T_1$. 

A \emph{rank-$r$-maxout layer} is defined by $r$ affine maps $T^{(q)} \colon \R^d \to \R^n$ for ${q \in [r]}$ and computes the function $x \mapsto (\max\{(T^{(1)}x)_j, \ldots, (T^{(r)}x)_j\})_{j\in[n]}$.
For a number of hidden layers $\ell \geq 0$ and a rank vector $\mathbf{r}=(r_1,\ldots,r_\ell) \in \N^\ell$, a \emph{rank-$\mathbf{r}$-maxout neural network} is defined by maxout layers $f_i:\R^{n_{i-1}}\to\R^{n_i}$ of rank $r_i$ for $i \in [\ell]$ respectively and an affine transformation $T_{out}\colon \R^{n_\ell} \to \R$. The rank-$\mathbf{r}$-maxout neural network computes the function $f\colon\R^d\to \R$ given by $f = T_{out} \circ f_\ell \circ \cdots \circ f_1$. 
Let $\maxout{d}{r}$ be the set of functions representable by a rank-$\mathbf{r}$-maxout neural network with input dimension $d$. 
 Moreover, let $\mtwo{d}{\ell}$ be the set of functions representable with  networks with $\ell$ rank-$2$-maxout layers.

\subsection{The braid arrangement and set functions}



\begin{definition}
    The \emph{braid arrangement} in $\R^d$ is the hyperplane arrangement consisting of the $\binom{d}{2}$ hyperplanes $x_i=x_j$, with $1\leq i<j \leq d$.
   The \emph{braid fan} $\B_d$ is the polyhedral fan induced by the braid arrangement.
\end{definition} 
Sometimes we will also refer to the fan given by the $\binom{d+1}{2}$ hyperplanes $x_i=x_j$ and $x_i=0$ for $1\leq i<j \leq d$, which we denote by $\B_d^0$. 
 
We summarize the properties of the braid fan that are relevant for this work. For more details see \citet{stanley07arrangement}.
The $k$-dimensional cones of $\B_d$ are given by \[\{\cone(\ind_{S_1}, \ldots, \ind_{S_k}) + \spn(\ind_{[d]})\mid \emptyset \subsetneq S_1 \subsetneq S_2 \subsetneq \cdots \subsetneq S_k \subsetneq [d]\},\]
where $\ind_S=\sum_{i\in S} e_i$.
The braid fan has $\spn(\ind_{[d]})$ as lineality space.  
Dividing out the lineality space of $\B_d$ yields $\B_{d-1}^0$. 
See \Cref{fig:braid_arrangement} for an illustration of $\B_d^0$.

Using the specific structure of the cones of $\B_d$ in terms of subsets of $[d]$
 allows to relate the vector space $\FB{d}$ of CPWL functions compatible with the braid fan $\B_d$ with the vector space of set functions $\Sf{d} \coloneqq  \R^{2^{[d]}}$: restricting to the values on $\{\ind_S\}_{S\subseteq [d]}$ yields a vector space isomorphism  $\Phi \colon \FB{d} \to \Sf{d}$ whose inverse map is given by interpolating the values on $\{\ind_S\}_{S\subseteq [d]}$ to the interior of the cones of the braid fan. Detailed proofs of all statements can be found in \Cref{sec:proofs}.
\begin{proposition}
\label{prop:iso_cpwl_to_setfunction}
    The linear map $\Phi \colon \FB{d} \to \Sf{d}$  given by $F(S) = f(\ind_S)$ is an isomorphism.
\end{proposition}

 This implies that $\FB{d}$  has dimension $2^{d}$. Another basis for $\FB{d}$ is given by \mbox{$\{\sigma_M \mid  M \in  2^{[d]} \}$}, where the function $\sigma_M \colon \R^d \to \R$ is defined by $\sigma_M(x) = \max_{i \in M} x_i$ \citep{DANILOV01games,jochemko22permutahedra}.
We have the following strict containment of linear subspaces:
 \[\FB{d}(0) \subsetneq \FB{d}(1) \subsetneq \ldots \subsetneq \FB{d}(d) =\FB{d}\]
 where $\FB{d}(k) \coloneqq \spn\{\sigma_M \mid M\subseteq [d], |M| \leq k\}$. 
In order to describe the linear subspaces $\Phi(\FB{d}(k))$, we now describe the isomorphism $\Phi$ with respect to the basis $\{\sigma_M \mid  M \in  2^{[d]} \}$. 

\begin{definition}
    Let $X$ and $Y$ be finite sets such that $X \subseteq Y$, then the interval $[X,Y] \coloneqq \{S \subseteq [Y] \mid X \subseteq S \}$ is a \emph{boolean lattice} with the partial order given by inclusion. 
    \end{definition}
    The \emph{rank} of $[X,Y]$ is given by $|Y\setminus X|$.
    Sometimes we also write $x_1 \cdots  x_n$ for the set $\{x_1,\ldots,x_n\} \in \La$ and $\overline{x_1 \cdots  x_n}$ for the set $X \cup (Y \setminus \{x_1,\ldots,x_n\}) $.

For a boolean lattice $\La = [X,Y]$ of rank $n$, the \emph{rank function} $r \colon \La \to [n]_0$ is given by $r(S) = |S|-|X|$  and $r(S)$ is called the \emph{rank} of $S$. Moreover, we define the \emph{levels} of a boolean lattice by  $\La_i\coloneqq r^{-1}(i)$ and introduce the notation $\La_{\leq i} \coloneqq \bigcup_{j \leq i} \La_{j}$ for the set of elements whose rank is bounded by $i$. For $S,T \in \La$ with $S \subseteq T$, we call $[S,T]$ a \emph{sublattice} of $\La$ and define the vector $\al{S,T} \in \R^\La$ by 
$\al{S,T} \coloneqq \sum_{S \subseteq Q \subseteq T} (-1)^{r(Q)-r(S)} \ind_Q $.
The set $\Sf{\La} \coloneqq  (\R^\La)^*$ of real-valued functions on $\La$ is a vector space, and for any fixed $S,T \in \La$, the map $F \mapsto \inner{\al{S,T}}{F}$  is a linear functional of $\Sf{\La}$. 
Furthermore, let \[\R^\La(k) = \spn \{\al{S,T} \mid S,T \in \La, S \subseteq T \text{ such that } r(T)-r(S) = k+1\}\]
and $\Sf{\La}(k) \coloneqq (\R^\La(k))^\perp = \{F \in \Sf{\La} \mid \asum{S,T}{F}= 0$ for all $\al{S,T} \in \R^\La(k)\}$ be a linear \mbox{subspace of $\Sf{\La}$.
To simplify notation, we also set $\Sf{d}(k) \coloneqq \Sf{2^{[d]}}(k)$}.
\begin{proposition}
\label{prop:mobius_inversion}
The isomorphism $\Phi \colon \FB{d} \to \Sf{d}$ maps the function 
$f=\sum_{M \subseteq [d]} \lambda_M \cdot
\sigma_M$ to the set function defined by 
\[
F(S) \coloneqq \sum\limits_{\substack{M \subseteq [d] \\ M \cap S \neq \emptyset}}\lambda_M \cdot \sigma_M .
\]
 The inverse $\Phi^{-1} \colon \Sf{d} \to \FB{d}$ of $\Phi$ is given by the M\"obius inversion formula  \[F \mapsto \sum_{M \subseteq [d]}-\asum{[d] \setminus M, [d]}{F} \cdot \sigma_M.\]   In particular, it holds that $\Phi(\FB{d}(k)) = \Sf{d}(k)$ for all $k \leq d$ and $\dim(\Sf{d}(k)) = \dim(\FB{d}(k)) =  \sum_{i=1}^k\binom{d}{i}$. See also \Cref{fig:coefficient} for an illustration of \Cref{prop:mobius_inversion}.
\end{proposition}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{.4\textwidth}
\label{fig:braid}
\centering
    \begin{tikzpicture}[scale=1.3]
    \definecolor{convex}{rgb}{0,0.3,0.5}
    \definecolor{concave}{rgb}{0.8,0.2,0}
        \draw[black] (0,0) -- (0,2) ;
        \draw[black] (0,0) -- (0,-2) ;
        \draw[black] (0,0) -- (2,0) ;
        \draw[black] (0,0) -- (-2,0) ;
        \draw[black] (0,0) -- (-2,-2) ;
        \draw[black] (0,0) -- (2,2) ;
        \node at (0.9,1.8) {\small $0\leq x_1 \leq x_2$};
        \node at (1.9,0.7) {\small $0\leq x_2 \leq x_1$};
        \node at (-1,1) {\small $x_1 \leq 0 \leq x_2$};
        \node at (1,-1) {\small $x_2 \leq 0 \leq x_1$};
        \node at (-0.9,-1.8) {\small $ x_2 \leq x_1 \leq 0$};
        \node at (-1.9,-0.7) {\small $x_1 \leq x_2 \leq 0$};
    \end{tikzpicture}
    \caption{The braid arrangement $\B_2^0$.}
    \label{fig:braid_arrangement}
    \end{subfigure}
      \begin{subfigure}[b]{0.4\textwidth}
\centering
    \begin{tikzpicture}[scale=1]
    \definecolor{convex}{rgb}{0,0.3,0.5}
    \definecolor{concave}{rgb}{0.8,0.2,0}
        \draw[black] (0,-2) -- (0,0);
        \draw[black] (-2,0) -- (0,0);
        \draw[black] (0,0) -- (2,2);
        \draw[red,dashed] (1,-2) -- (0,0); 
        \draw[red,dashed] (-2,-1) -- (0,0); 
        \node at (0,1) {$x_2$};
        \node at (1,0) {$x_1$};
        \node at (-1,-1) {$0$};
    \end{tikzpicture}
    \caption{The breakpoints of the function $\max\{0,x_1,x_2\}$ in black and breakpoints not supported on the braid arrangement in red and dashed.}
    \label{fig:breakpoints}
    \end{subfigure}
    \caption{}
    \end{figure}
\section{Neural networks conforming with the braid fan}
For a polyhedral complex $\PP$, we call a maxout neural network \emph{$\PP$-conforming}, if the functions at all neurons are compatible with $\PP$. 
By this we mean that for all $i \in [\ell]$ and all coordinates $j$ of the codomain of $f_{i}$,
the function $\pi_j \circ  f_i \circ \ldots \circ f_1$ is compatible with $\PP$,
where $\pi_j$ is the projection on the coordinate $j$. 
We denote by $\maxout{\PP}{r}$ the set of all functions representable by $\PP$-conforming rank-$\mathbf{r}$-maxout networks. 
For the remainder of this article, we only consider the cases 
$\maxout{\B_d}{r}$ and $\maxout{\B^0_d}{r}$
 The assumption of being $\B^0_d$-conforming to compute $\max \{0,x_1,\ldots,x_d\}$ appears natural, as any breakpoints of a neuron outside the braid arrangement must eventually be neutralized by subsequent layers. However, this intuition alone does not suffice for a rigorous proof, as neurons can influence the behavior of others in later layers.  
\begin{lemma}
\label{lem:equivalence_assumptions}
   The function $x \mapsto \max \{0,x_1,\ldots,x_{d-1}\}$ can be represented by a $\B_{d-1}^0$-conforming rank-$\mathbf{r}$-maxout network if and only if the function $x \mapsto \max \{x_1,\ldots,x_{d}\}$ can be represented by a $\B_{d}$-conforming rank-$\mathbf{r}$-maxout network.
\end{lemma}

By computing $r_i$ maxima in each layer, we can compute the basis functions of $\FB{d}(\prod_{i=1}^\ell r_i)$ with a $\B_d$-conforming rank-$\mathbf{r}$-maxout network.
 \begin{proposition}
 \label{prop:inclusion_of_conforming}
     For any rank vector $\mathbf{r} \in \N^\ell$, it holds that all functions in $\FB{d}(\prod_{i=1}^\ell r_i)$ are representable by a $\B_d$-conforming rank-$\mathbf{r}$-maxout network.
 \end{proposition}
Most of the paper is concerned with proving that $\maxout{\B_d}{r}$ is contained in certain subspaces of $\FB{d}$.
Let $\Sf{\La}^r = \bigoplus_{i \in [r]} \Sf{\La}$ be the $r$-fold direct sum of $\Sf{\La}$ with itself. In order to model the application of the rank-$r$-maxout activation function for a set function under the isomorphism $\Phi$, we define for $(F_1,\ldots,F_r) \in \Sf{\La}^r $ the function $\max\{F_1,\ldots, F_r\} \in \Sf{\La}$ given by $\max\{F_1,\ldots, F_r\}(S) = \max\{F_1(S),\ldots, F_r(S)\}$.

For $f_1,\ldots, f_r \in \FB{d}$, the function $\max\{f_1,\ldots, f_r\}$ is $\B_d$-compatible if taking the maximum does not create breakpoints that do not lie on the braid arrangement, that is, on every cone $C$ of the braid arrangement, it holds that $\max\{f_1,\ldots, f_r\}=f_q$ for a $q \in [r]$. 
Next, we aim to model the compatibility with the braid arrangement for set functions.  We call a tuple $(F_1,\ldots,F_r) \in \Sf{\La}^r$ \emph{conforming} if for every chain $\emptyset =S_0 \subsetneq S_1 \subsetneq \ldots \subsetneq S_n$ there is a $j \in [r]$ such that $F_j(S_i) = \max \{F_1,\ldots,F_r\}(S_i)$ for all $i \in [n]_0$. 
Then, the set $\HC{\La}^r\subseteq\Sf{\La}^r$ of conforming tuples are exactly those tuples of CPWL functions such that applying the maxout activation function yields a function that is still compatible with the braid fan as stated in the next lemma. 
Again, to simplify notation, we also set $\HC{d}^r \coloneqq \HC{2^{[d]}}^r$. 
\begin{lemma}
\label{lem:maxout_conforming_setfunc}
    For $(F_1,\ldots,F_r) \in (\Sf{d})^r$, the function $\max\{\Phi^{-1}(F_1),\ldots,\Phi^{-1}(F_r)\}$ is $\B_d$-conforming if and only if $(F_1,\ldots,F_r) \in \HC{d}^r$. In this case, \[\max\{\Phi^{-1}(F_1),\ldots,\Phi^{-1}(F_r)\} = \Phi^{-1}(\max\{F_1,\ldots,F_r\})\]
\end{lemma}
The statement ensures that taking the maximum of the set functions is the same as taking the maximum of the piecewise-linear functions exactly for compatible tuples. 
\begin{figure}
    \begin{subfigure}[b]{.4\textwidth}
\begin{tikzpicture}[scale=0.41]
% Define nodes symmetrically for each level
\node (abcd) at (0, 0) {$1234$};
% Level 3
\node (abc) at (-3, -2) {$\overline{4}$};
\node (abd) at (-1, -2) {$\overline{3}$};
\node (acd) at (1, -2) {$\overline{2}$};
\node (bcd) at (3, -2) {$\overline{1}$};
% Level 2
\node (ab) at (-5, -4) {$12$};
\node (ac) at (-3, -4) {$13$};
\node (ad) at (-1, -4) {$23$};
\node (bc) at (1, -4) {$23$};
\node (bd) at (3, -4) {$24$};
\node (cd) at (5, -4) {$34$};
% Level 1
\node (a) at (-3, -6) {$1$};
\node (b) at (-1, -6) {$2$};
\node (c) at (1, -6) {$3$};
\node (d) at (3, -6) {$4$};
% Level 0
\node (empty) at (0, -8) {$\emptyset$};
% Draw edges manually
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
% Level 2 to 3
\draw[gray!50, very thin] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[gray!50, very thin] (bcd) -- (cd);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[black] plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};
\draw[black] plot [smooth] coordinates {(-2,0)  (-2,-5.5) (-0.5,-5)( 0,-3.5) (1.5,-3) (2,-1.5) (2.5,-0.1)  };
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};
    % Fill the area below the curve
    \fill[teal, opacity=0.1] plot [smooth] coordinates {
      (-2,0)  (-2,-5.5) (-0.5,-5)( 0,-3.5) (1.5,-3) (2,-1.5) (2.5,-0.1) 
    } --  (0,6)--cycle;
\end{scope}
\end{tikzpicture}
\caption{Illustration of \Cref{prop:mobius_inversion}. The coefficient of the function $\sigma_{\{2,4\}}$ in the linear combination for $F$ is given by $-\asum{13,1234}{F}$. }
\label{fig:coefficient}
\end{subfigure}
\hspace{0.5cm}
        \begin{subfigure}[b]{.4\textwidth}
\begin{tikzpicture}[scale=0.4]
% Define nodes symmetrically for each level
\node (abcd) at (0, 0) {$abcd$};
% Level 3
\node (abc) at (-3, -2) {$abc$};
\node (abd) at (-1, -2) {$abd$};
\node (acd) at (1, -2) {$acd$};
\node (bcd) at (3, -2) {$bcd$};
% Level 2
\node (ab) at (-5, -4) {$ab$};
\node (ac) at (-3, -4) {$ac$};
\node (ad) at (-1, -4) {$ad$};
\node (bc) at (1, -4) {$bc$};
\node (bd) at (3, -4) {$bd$};
\node (cd) at (5, -4) {$cd$};
% Level 1
\node (a) at (-3, -6) {$a$};
\node (b) at (-1, -6) {$b$};
\node (c) at (1, -6) {$c$};
\node (d) at (3, -6) {$d$};
% Level 0
\node (empty) at (0, -8) {$\emptyset$};
% Draw edges manually
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
% Level 2 to 3
\draw[gray!50, very thin] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[gray!50, very thin] (bcd) -- (cd);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[black] plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};
\node[draw, circle, thick, red, fit=(bcd), inner sep=-1pt] {};
\node[draw, circle, dashed, thick, red, fit=(bd), inner sep=-2pt] {};
\node[draw, circle, dashed, thick, red, fit=(cd), inner sep=-2pt]{};
\node[draw, circle, dashed, thick, red, fit=(d), inner sep=-1pt]{};
\node[draw, circle, dashed, red, fit=(bc), inner sep=-2pt] {};
\node[draw, circle, dashed, thick, red, fit=(b), inner sep=-1pt] {};
\node[draw, circle, dashed, thick, red, fit=(c), inner sep=-0.5pt] {};
\node[draw, circle, dashed, thick, red, fit=(empty), inner sep=-1pt] {};
\draw[black] plot [smooth] coordinates {(-2.5,-7.9) (-2,-5.5) (-0.5,-5)( 0,-3.5) (1.5,-3) (2,-1.5) (2.5,-0.1)   };
%\draw[black,dashed] plot [smooth] coordinates {(-3.9,-6.8) (-4,-3.5) (-0.5,-3)( 0,-1.5) (1,-1)(1.7,0.4)   };
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};   
    % Fill the area below the curve
    \fill[teal, opacity=0.1] plot [smooth] coordinates {
        (-2.5,-7.9) (-2,-5.5) (-0.5,-5)( 0,-3.5) (1.5,-3) 
    } --  plot [smooth] coordinates {
       (2.5,-0.1) (1.75,-2.5)  (1.75,-8.4)  
    } -- (0,-10) --cycle;
      \fill[teal, opacity=0.1] plot [smooth] coordinates {
        (1.75,-8.4) (1.75,-2.5)  (2.5,-0.1)
    } -- (6,0) -- (6,-8) -- cycle;
\end{scope}
\end{tikzpicture} 
\caption{Illustration of \Cref{lem:lowsubsets}. If $F\in \Sf{\La}(2) \cap \HC{\La}$ and $F(\{b,c,d\}) < 0$, then there is a $S \subsetneq\{b,c,d\}$ with $F(S) < 0$ since $\asum{\emptyset,bcd}{F}=0$.}
\label{fig:pushing_down}
\end{subfigure}
\caption{}
\end{figure}
    \section{Doubly-logarithmic lower bound}
    \label{sec:loglog}
    In this section, we prove that for any number of layers $\ell \in \N$ and rank vector $\mathbf{r} \in \N^\ell$, the function $\max \{0,x_1,\ldots, x_{2^{R^{\ell}-1}}\}$ is not computable by a $\B^0_d$-conforming rank-$\mathbf{r}$-maxout network, where $R= 2\cdot \max \{r_1,\ldots, r_\ell\}$. 
    We first show that $\max \{0,x_1,\ldots, x_{2^{2^{\ell}-1}}\}$ is not computable by a $\B^0_d$-conforming rank-$2$-maxout neural network (or equivalently ReLU neural network) with $\ell$ hidden layers. Due to the equivalence of $\B_d$ and $\B^0_d$, we will prove that $\mtwo{\B_d}{\ell} \subseteq \FB{d}(2^{2^{\ell}-1})$ for $d \geq 2^{2^{\ell}-1}+1$ and then use the fact that a rank-$r$-maxout layer can be computed by  $\lceil \log_2 r \rceil$ many rank-$2$-maxout layer. 

    First, we define an operation $\mathcal{A}$ on subspaces of $\FB{d}$ that describes rank-$2$-maxout layers that maintain compatibility with $\B_d$. 
    For any subspace $U \subseteq \FB{d}$, let $\mathcal{A}(U) \subseteq \FB{d}$ be the subspace containing all the functions computable by a $\B_d$-conforming rank $2$-maxout layer that takes functions from $U$ as  input. 
    Formally, \[\mathcal{A}(U) = \spn \{\max\{f_1,f_2\} \mid  f_1,f_2 \in U,\max\{f_1,f_2\} \in \FB{d} \}.\]
Clearly,  $\mathcal{A}(U_1)$ is a subspace of $\mathcal{A}(U_2)$ whenever $U_1$ is a subspace of $U_2$.
    We recursively define $\mathcal{A}^\ell(U) = \mathcal{A}(\mathcal{A}^{\ell-1}(U))$. This recursive definition allows to describe the set of $\B_d$-conforming network with $\ell$ rank-$2$-maxout layers $\mtwo{\B_d}{\ell}$. 
        \begin{lemma}
        \label{lem:layer_description}It holds that 
        \begin{enumerate}
        \item \label{lem:layer_description_case1}$\mtwo{\B_d}{1}=\mathcal{A}(\FB{d}(1)) = \FB{d}(2)$ and 
        \item  \label{lem:layer_description_casel}$\mtwo{\B_d}{\ell}=\mathcal{A}(\mtwo{\B_d}{\ell-1})=\mathcal{A}^\ell(\FB{d}(1))$ for all $\ell \in \N$.
        \end{enumerate}
    \end{lemma}
    Since it holds that $\max\{f_1,f_2\}=\max\{0,f_1-f_2\}+f_2$, we can assume wlog that one of the functions is the zero map, as stated in the following lemma.
    \begin{lemma}
    \label{lem:wlog_zero_map}
    It holds that 
 $\mathcal{A}(U) = \spn \{\max\{0,f\} \mid  f \in U,\max\{0,f\} \in \FB{d} \}$.
    \end{lemma}
    To prove that $\mtwo{\B_d}{\ell}= \mathcal{A}^\ell(\FB{d}(1))$ is a proper subspace of $\FB{d}$ for $d \geq 2^{2^\ell-1}+1$, we perform a layerwise analysis and inductively bound $n_k$ depending on $k$ such that $\mathcal{A}(\FB{d}(k)) \subseteq \FB{d}(n_k)$ for all $k \in \N$. 
    In this attempt, we translate this task to the setting of set functions on boolean lattices using the isomorphism $\Phi$. Recall that the pairs $(F_1,F_2) \in \HC{\La}^2$ are precisely the functions such that the maximum of the corresponding CPWL functions $f_1$ and $f_2$ is still compatible with $\B_d$. Moreover, it is easy to observe, that the pair $(0,F)\in \Sf{\La}^2$ is conforming if and only if $F$ is contained in the set \[\HC{\La} \coloneqq \{F \in \Sf{\La} \mid F(S) \text{ and } F(T) \text{ do not have opposite signs for } S \subseteq T \}.\]  
Again, to simplify notation, we also set $\HC{d} \coloneqq \HC{2^{[d]}}$ and use the notation $F^+=\max\{0,F\}$. By slightly overloading notation, for any subspace $U \subseteq \Sf{\La}$, let $\mathcal{A}(U) = \spn \{F^+ \mid F \in U \cap \HC{\La}\}$. \Cref{lem:maxout_conforming_setfunc} justifies this notation and allows us to carry out the argumentation to the world of set functions on boolean lattices, as we conclude in the following lemma. 
\begin{lemma}
\label{lem:indBase_maxout}
   It holds that $\mathcal{A}(\Phi(U))= \Phi(\mathcal{A}(U))$ for all subspaces $U \subseteq \FB{d}$.
    In particular, for any lattice $\La=[X,Y]$, it holds that 
 $ \mathcal{A}(\Sf{\La}(1)) = \Sf{\La}(2)$.
  \end{lemma}
In the following, we prove that $\mathcal{A}(\Sf{\La}(k)) \subseteq \Sf{\La}(k^2+k)$ by an induction on $k$ and \Cref{lem:indBase_maxout} serves as the base case. 

Next, we describe properties of the vector space $\R^\La$ that will be useful for the induction step.
Every sublattice of $\La$ of rank $k+1$ is of the form $[S,S\cup T]$, where $S\cap T = \emptyset$ and $|T|=k+1$. 
For any $T \subseteq Y\setminus X$, one can decompose $\La=[X,Y]$ into the sublattices $[S,S\cup T]$ for all $S \subseteq Y\setminus T$, resulting in the following lemma.
\begin{figure}
    \centering
\begin{subfigure}[t]{.32\textwidth}
\begin{tikzpicture}[scale=0.4]
% Define nodes symmetrically for each level
\node (abcd) at (0, 0) {$abcd$};
% Level 3
\node (abc) at (-3, -2) {$abc$};
\node (abd) at (-1, -2) {$abd$};
\node (acd) at (1, -2) {$acd$};
\node (bcd) at (3, -2) {$bcd$};
% Level 2
\node (ab) at (-5, -4) {$ab$};
\node (ac) at (-3, -4) {$ac$};
\node (ad) at (-1, -4) {$ad$};
\node (bc) at (1, -4) {$bc$};
\node (bd) at (3, -4) {$bd$};
\node (cd) at (5, -4) {$cd$};
% Level 1
\node (a) at (-3, -6) {$a$};
\node (b) at (-1, -6) {$b$};
\node (c) at (1, -6) {$c$};
\node (d) at (3, -6) {$d$};
% Level 0
\node (empty) at (0, -8) {$\emptyset$};
% Draw edges manually
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
% Level 2 to 3
\draw[gray!50, very thin] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[gray!50, very thin] (bcd) -- (cd);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[black] plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};
\draw[black] plot [smooth] coordinates {(-2,0.2) (-2,-5) (-0.5,-5)( 0,-3.5) (1.75,-3)  (1.75,-8.4)  };
\draw[black,dashed] plot [smooth] coordinates {(-2.5,-7.9) (-2,-5)   };
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};
        \fill[teal, opacity=0.1] plot [smooth] coordinates {
        (-2.5,-7.9) (-2,-5) 
    } --  plot [smooth] coordinates {
       (-2,0.2) (-2,-5) (-0.5,-5)( 0,-3.5) (1.75,-3)  (1.75,-8.4)
    } -- (0,-10) --cycle;
    % Fill the area below the curve
    \fill[blue, opacity=0.1] plot [smooth] coordinates {
        (-2.5,-7.9) (-2,-5.5)  (-2,0.2)
    } -- (-6,0) -- (-6,-8) -- cycle;
\end{scope}
\end{tikzpicture}
\caption{$\al{\emptyset,abc}=\al{\emptyset,bc}-\al{a,abc}$}
\label{fig:solid_line}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
    \begin{tikzpicture}[scale=0.4]
% Define nodes symmetrically for each level
\node (abcd) at (0, 0) {$abcd$};
% Level 3
\node (abc) at (-3, -2) {$abc$};
\node (abd) at (-1, -2) {$abd$};
\node (acd) at (1, -2) {$acd$};
\node (bcd) at (3, -2) {$bcd$};
% Level 2
\node (ab) at (-5, -4) {$ab$};
\node (ac) at (-3, -4) {$ac$};
\node (ad) at (-1, -4) {$ad$};
\node (bc) at (1, -4) {$bc$};
\node (bd) at (3, -4) {$bd$};
\node (cd) at (5, -4) {$cd$};
% Level 1
\node (a) at (-3, -6) {$a$};
\node (b) at (-1, -6) {$b$};
\node (c) at (1, -6) {$c$};
\node (d) at (3, -6) {$d$};
% Level 0
\node (empty) at (0, -8) {$\emptyset$};
% Draw edges manually
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
% Level 2 to 3
\draw[gray!50, very thin] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[gray!50, very thin] (bcd) -- (cd);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[black] plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};
\draw[black] plot [smooth] coordinates {(-2.5,-7.9) (-2,-5.5) (-0.5,-5)( 0,-3.5) (1.5,-3) (2,-1.5) (2.5,-0.1)   };
\draw[black,dashed] plot [smooth] coordinates {(1.75,-8.4) (1.75,-2.5)   };
\draw[black,dashed] plot [smooth] coordinates {(-2,-5.5) (-2,0)   };
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};   
    % Fill the area below the curve
    \fill[teal, opacity=0.1] plot [smooth] coordinates {
        (-2.5,-7.9) (-2,-5.5) (-0.5,-5)( 0,-3.5) (1.5,-3) 
    } --  plot [smooth] coordinates {
       (2.5,-0.1) (1.75,-2.5)  (1.75,-8.4)  
    } -- (0,-10) --cycle;
      \fill[green, opacity=0.1] plot [smooth] coordinates {
        (1.75,-8.4) (1.75,-2.5)  (2.5,-0.1)
    } -- (6,0) -- (6,-8) -- cycle;
\end{scope}
\end{tikzpicture}
\caption{$\al{\emptyset,bcd}=\al{\emptyset,bc}- \al{d,bcd}$}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\begin{tikzpicture}[scale=0.4]
% Define nodes symmetrically for each level
\node (abcd) at (0, 0) {$abcd$};
% Level 3
\node (abc) at (-3, -2) {$abc$};
\node (abd) at (-1, -2) {$abd$};
\node (acd) at (1, -2) {$acd$};
\node (bcd) at (3, -2) {$bcd$};
% Level 2
\node (ab) at (-5, -4) {$ab$};
\node (ac) at (-3, -4) {$ac$};
\node (ad) at (-1, -4) {$ad$};
\node (bc) at (1, -4) {$bc$};
\node (bd) at (3, -4) {$bd$};
\node (cd) at (5, -4) {$cd$};
% Level 1
\node (a) at (-3, -6) {$a$};
\node (b) at (-1, -6) {$b$};
\node (c) at (1, -6) {$c$};
\node (d) at (3, -6) {$d$};
% Level 0
\node (empty) at (0, -8) {$\emptyset$};
% Draw edges manually
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
% Level 2 to 3
\draw[gray!50, very thin] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[gray!50, very thin] (bcd) -- (cd);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[black] plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};
\draw[black] plot [smooth] coordinates {(-2.5,-7.9) (-2,-5.5) (-0.5,-5)( 0,-3.5) (1.5,-3) (2,-1.5) (2.5,-0.1)   };
\draw[black,dashed] plot [smooth] coordinates {(1.75,-8.4) (1.75,-2.5)   };
\draw[black,dashed] plot [smooth] coordinates {(-2,-5.5) (-2,0)   };
% Clip and fill the desired region
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {(0,-9) (6,-4) (0,1) (-6,-4)};
    % Fill the area below the curve
    \fill[blue, opacity=0.1] plot [smooth] coordinates {
        (-2.5,-7.9) (-2,-5.5)  (-2,0)
    } -- (-6,-1) -- (-6,-8) -- cycle;
      \fill[green, opacity=0.1] plot [smooth] coordinates {
        (1.75,-8.4) (1.75,-2.5)  (2.5,-0.1)
    } -- (6,0) -- (6,-8) -- cycle;
\end{scope}
\end{tikzpicture}
\caption{$\al{d,bcd} - \al{a,abc} =\\\al{\emptyset,bcd} - \al{\emptyset,abc}$}
\end{subfigure}
\caption{Illustration of \Cref{lem:lattice_structure}. The solid line in \Cref{fig:solid_line}, decomposes the lattice in $[\emptyset, abc] \cup  [d,abcd]$, which implies that $\al{\emptyset,abcd}= \al{\emptyset,abc} - \al{d,abcd}$. The dashed line further decomposes $[\emptyset,abc] = [\emptyset,bc] \cup [a,abc]$. The 3 figures illustrate that $\al{S,S\cup \{b,c\}}-\al{S',S'\cup \{b,c\}}\in \R^\La(2)$ for all $S,S' \subseteq \{a,d\}$.}
\label{fig:spreading}
\end{figure}
\begin{lemma}
\label{lem:lattice_structure}
	 Let $\La = [X,Y]$ be a lattice of rank $n$. Then,
\begin{enumerate}
    \item \label{lem:lattice_decomposition} for every $T \subseteq Y \setminus X$, it holds that $\al{X,Y} \in \spn \{\al{S,S \cup T} \mid {S \subseteq Y \setminus T} \}$ and
    \item \label{lem:induction_step} for every $T \subseteq Y \setminus X$ with $|T|=k$, it holds that $\al{S,S \cup T}- \al{S',S' \cup T} \in \R^\La(k)$ for all $S,S' \in [X,Y \setminus T]$.
\end{enumerate}
\end{lemma}
See \Cref{fig:spreading} for a visualization  of \Cref{lem:lattice_structure}. \Cref{lem:lattice_structure} implies that  it suffices to find a  $T \subseteq Y$ such that $\asum{S,S \cup T}{F^+} = 0$ for all $S \subseteq Y \setminus T$, in order to prove that $\Sf{\La}(n-1)$. The idea of the induction step is to find a $T$ of cardinality at least $(k-1)^2+(k-1)+1$ such that $F \in \Sf{[S,S \cup T]}(k-1)$ for all $S \subseteq Y 
\setminus T$.
Then, applying the induction hypothesis to each sublattice $[S,S \cup T]$ yields $\asum{S,S \cup T}{F^+} = 0$ and hence $F^+ \in \Sf{\La}(n-1)$. 


If $F \in \Sf{\La}(k)$, \Cref{lem:lattice_structure}  implies that for any $T' \subseteq Y \setminus X$ of cardinality $k$, the value $\asum{S',S'\cup T'}{F}$ is independent of $S' \subseteq Y \setminus T'$.
Hence, in this case, it suffices to find a $T$ such that $F \in \Sf{[S,S\cup T]}(k-1)$ for only one $S \subseteq Y \setminus T$, since it is equivalent to $F \in \Sf{[S,S\cup T]}(k-1)$  for all $S \subseteq Y \setminus T$.
 
    Given $F \in \Sf{\La}(k) \cap \HC{\La}$, it remains to find such $S$ and $T$. We define the \emph{support} of $F \in \Sf{\La}$ by $\supp(F) = \{S \in \La \mid F(S) \neq 0\}$ and the \emph{positive and negative support} by $\supp^+(F) = \{S \in \La \mid F(S) >0\}$ respectively $\supp^-(F) = \{S \in \La \mid F(S) <0\}$.
In particular, $F \in \HC{\La}$ implies that for  $X^+ \in \supp^+(F)$ and $X^- \in \supp^-(F)$, it holds that $F(R)=0$ for all $R \supseteq X^+ \cup X^-$. 
 
\Cref{lem:lowsubsets} says that, given that the positive and negative support are not empty, we can always ``push the elements $X^+$ and $X^-$ in the support down in the lattice'', that is, we can find elements in the supports that are of relatively low  rank. See \Cref{fig:pushing_down}
for an illustration.
  \begin{lemma}
    	\label{lem:lowsubsets}
    	Let $\La = [X,Y]$ be a lattice of rank $n$.
    	Let $F \in \Sf{\La}(k) \cap \HC{\La}$ such that $F  \not\geq 0$ and $F \not\leq 0$. Then, there are $X^- \in \La_{\leq k} \cap \supp^-(F)$ and $ X^+ \in \La_{\leq k} \cap \supp^+$ as well as $Y^- \in \La_{\geq n-k} \cap \supp^-(F)$ and $
            Y^+ \in \La_{\geq n-k} \cap \supp^+(F)$.
    \end{lemma}  
\begin{figure}
    \centering
\begin{subfigure}[t]{0.46\textwidth}
\begin{tikzpicture}[scale=0.36][
  every node/.style={draw, circle, minimum size=0.8cm, inner sep=2pt},
  every edge/.style={draw, gray, very thin}]
% Define nodes symmetrically for each level
\node (abcde) at (0, 0) {abcde};
% Level 4
\node (abcd) at (-4, -2) {$\overline{e}$};
\node (abce) at (-2, -2) {$\overline{d}$};
\node (abde) at (0, -2) {$\overline{c}$};
\node (acde) at (2, -2) {$\overline{b}$};
\node (bcde) at (4, -2) {$\overline{a}$};
% Level 3
\node (abc) at (-9, -4) {abc};
\node (abd) at (-7, -4) {abd};
\node (abe) at (-5, -4) {abe};
\node (acd) at (-3, -4) {acd};
\node (ace) at (-1, -4) {ace};
\node (ade) at (1, -4) {ade};
\node (bcd) at (3, -4) {bcd};
\node (bce) at (5, -4) {bce};
\node (bde) at (7, -4) {bde};
\node (cde) at (9, -4) {cde};
% Level 2
\node (ab) at (-9, -6) {ab};
\node (ac) at (-7, -6) {ac};
\node (ad) at (-5, -6) {ad};
\node (ae) at (-3, -6) {ae};
\node (bc) at (-1, -6) {bc};
\node (bd) at (1, -6) {bd};
\node (be) at (3, -6) {be};
\node (cd) at (5, -6) {cd};
\node (ce) at (7, -6) {ce};
\node (de) at (9, -6) {de};
% Level 1
\node (a) at (-4, -8) {a};
\node (b) at (-2, -8) {b};
\node (c) at (0, -8) {c};
\node (d) at (2, -8) {d};
\node (e) at (4, -8) {e};
\node[draw, circle, thick, red, fit=(a), inner sep=0.5pt] {};
\node[draw, circle, thick, dashed, blue, fit=(b), inner sep=0pt] {};
% Level 0
\node (empty) at (0, -10) {$\emptyset$};
% Draw edges manually
% Level 4 to 5
\draw[gray!50, very thin] (abcde) -- (abcd);
\draw[gray!50, very thin] (abcde) -- (abce);
\draw[gray!50, very thin] (abcde) -- (abde);
\draw[gray!50, very thin] (abcde) -- (acde);
\draw[gray!50, very thin] (abcde) -- (bcde);
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
\draw[gray!50, very thin] (abce) -- (abc);
\draw[gray!50, very thin] (abce) -- (abe);
\draw[gray!50, very thin] (abce) -- (ace);
\draw[gray!50, very thin] (abce) -- (bce);
\draw[gray!50, very thin] (abde) -- (abd);
\draw[gray!50, very thin] (abde) -- (abe);
\draw[gray!50, very thin] (abde) -- (ade);
\draw[gray!50, very thin] (abde) -- (bde);
\draw[gray!50, very thin] (acde) -- (acd);
\draw[gray!50, very thin] (acde) -- (ace);
\draw[gray!50, very thin] (acde) -- (ade);
\draw[gray!50, very thin] (acde) -- (cde);
\draw[gray!50, very thin] (bcde) -- (bcd);
\draw[gray!50, very thin] (bcde) -- (bce);
\draw[gray!50, very thin] (bcde) -- (bde);
\draw[gray!50, very thin] (bcde) -- (cde);
% Level 2 to 3
\draw[gray!50, very thin] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (abe) -- (ab);
\draw[gray!50, very thin] (abe) -- (ae);
\draw[gray!50, very thin] (abe) -- (be);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (ace) -- (ac);
\draw[gray!50, very thin] (ace) -- (ae);
\draw[gray!50, very thin] (ace) -- (ce);
\draw[gray!50, very thin] (ade) -- (ad);
\draw[gray!50, very thin] (ade) -- (ae);
\draw[gray!50, very thin] (ade) -- (de);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[gray!50, very thin] (bcd) -- (cd);
\draw[gray!50, very thin] (bce) -- (bc);
\draw[gray!50, very thin] (bce) -- (be);
\draw[gray!50, very thin] (bce) -- (ce);
\draw[gray!50, very thin] (bde) -- (bd);
\draw[gray!50, very thin] (bde) -- (be);
\draw[gray!50, very thin] (bde) -- (de);
\draw[gray!50, very thin] (cde) -- (cd);
\draw[gray!50, very thin] (cde) -- (ce);
\draw[gray!50, very thin] (cde) -- (de);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (ae) -- (a);
\draw[gray!50, very thin] (ae) -- (e);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (be) -- (b);
\draw[gray!50, very thin] (be) -- (e);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
\draw[gray!50, very thin] (ce) -- (c);
\draw[gray!50, very thin] (ce) -- (e);
\draw[gray!50, very thin] (de) -- (d);
\draw[gray!50, very thin] (de) -- (e);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[gray!50, very thin] (e) -- (empty);
% Draw the cycle
\draw[black] plot [smooth cycle] coordinates {
    (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
};
% Draw the curve 3
\draw[black,] plot [smooth] coordinates {
    (-8,-7.6) (-8,-6) (-7,-5) (-5,-5) (-4,-4) (-3,-3) (0,-3) (1,-2) (2,0.6) 
}; 
% Fill the area inside the cycle and above the new curve
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    };
    % Fill the region above the curve
    \fill[green,opacity=0.1] plot [smooth] coordinates {
        (-8,-7.6) (-8,-6) (-7,-5) (-5,-5) (-4,-4) (-3,-3) (0,-3) (1,-2) (2,0.6)
    } -- (2,0.6) -- (0,1) -- (-10,1) -- (-10,-8) -- (-8,-7.6) -- cycle;
\end{scope}
\end{tikzpicture}
\caption{}
\label{fig:low_subsets} %Section
\end{subfigure}
\begin{subfigure}[t]{0.46\textwidth}
    \begin{tikzpicture}[scale=0.36]
% Define nodes symmetrically for each level
\node (abcde) at (0, 0) {abcde};
% Level 4
\node (abcd) at (-4, -2) {$\overline{e}$};
\node (abce) at (-2, -2) {$\overline{d}$};
\node (abde) at (0, -2) {$\overline{c}$};
\node (acde) at (2, -2) {$\overline{b}$};
\node (bcde) at (4, -2) {$\overline{a}$};
% Level 3
\node (abc) at (-9, -4) {abc};
\node (abd) at (-7, -4) {abd};
\node (abe) at (-5, -4) {abe};
\node (acd) at (-3, -4) {acd};
\node (ace) at (-1, -4) {ace};
\node (ade) at (1, -4) {ade};
\node (bcd) at (3, -4) {bcd};
\node (bce) at (5, -4) {bce};
\node (bde) at (7, -4) {bde};
\node (cde) at (9, -4) {cde};
% Level 2
\node (ab) at (-9, -6) {ab};
\node (ac) at (-7, -6) {ac};
\node (ad) at (-5, -6) {ad};
\node (ae) at (-3, -6) {ae};
\node (bc) at (-1, -6) {bc};
\node (bd) at (1, -6) {bd};
\node (be) at (3, -6) {be};
\node (cd) at (5, -6) {cd};
\node (ce) at (7, -6) {ce};
\node (de) at (9, -6) {de};
% Level 1
\node (a) at (-4, -8) {a};
\node (b) at (-2, -8) {b};
\node (c) at (0, -8) {c};
\node (d) at (2, -8) {d};
\node (e) at (4, -8) {e};
% Level 0
\node (empty) at (0, -10) {$\emptyset$};
% Draw edges manually
% Level 4 to 5
\draw[gray!50, very thin] (abcde) -- (abcd);
\draw[gray!50, very thin] (abcde) -- (abce);
\draw[gray!50, very thin] (abcde) -- (abde);
\draw[gray!50, very thin] (abcde) -- (acde);
\draw[gray!50, very thin] (abcde) -- (bcde);
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
\draw[gray!50, very thin] (abce) -- (abc);
\draw[gray!50, very thin] (abce) -- (abe);
\draw[gray!50, very thin] (abce) -- (ace);
\draw[gray!50, very thin] (abce) -- (bce);
\draw[gray!50, very thin] (abde) -- (abd);
\draw[gray!50, very thin] (abde) -- (abe);
\draw[gray!50, very thin] (abde) -- (ade);
\draw[gray!50, very thin] (abde) -- (bde);
\draw[gray!50, very thin] (acde) -- (acd);
\draw[gray!50, very thin] (acde) -- (ace);
\draw[gray!50, very thin] (acde) -- (ade);
\draw[gray!50, very thin] (acde) -- (cde);
\draw[gray!50, very thin] (bcde) -- (bcd);
\draw[gray!50, very thin] (bcde) -- (bce);
\draw[gray!50, very thin] (bcde) -- (bde);
\draw[gray!50, very thin] (bcde) -- (cde);
% Level 2 to 3
\draw[gray!50, very thin] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (abe) -- (ab);
\draw[gray!50, very thin] (abe) -- (ae);
\draw[gray!50, very thin] (abe) -- (be);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (ace) -- (ac);
\draw[gray!50, very thin] (ace) -- (ae);
\draw[gray!50, very thin] (ace) -- (ce);
\draw[gray!50, very thin] (ade) -- (ad);
\draw[gray!50, very thin] (ade) -- (ae);
\draw[gray!50, very thin] (ade) -- (de);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[gray!50, very thin] (bcd) -- (cd);
\draw[gray!50, very thin] (bce) -- (bc);
\draw[gray!50, very thin] (bce) -- (be);
\draw[gray!50, very thin] (bce) -- (ce);
\draw[gray!50, very thin] (bde) -- (bd);
\draw[gray!50, very thin] (bde) -- (be);
\draw[gray!50, very thin] (bde) -- (de);
\draw[gray!50, very thin] (cde) -- (cd);
\draw[gray!50, very thin] (cde) -- (ce);
\draw[gray!50, very thin] (cde) -- (de);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (ae) -- (a);
\draw[gray!50, very thin] (ae) -- (e);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (be) -- (b);
\draw[gray!50, very thin] (be) -- (e);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
\draw[gray!50, very thin] (ce) -- (c);
\draw[gray!50, very thin] (ce) -- (e);
\draw[gray!50, very thin] (de) -- (d);
\draw[gray!50, very thin] (de) -- (e);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[gray!50, very thin] (e) -- (empty);
% Draw the cycle
\draw[black] plot [smooth cycle] coordinates {
    (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
};
% Draw the curve 1
\draw[black,] plot [smooth] coordinates {
    (-1,-10.9) (-1,-8) (0,-7) (3,-7) (5,-5) (7,-5) (9,-3) 
};
% Clip and fill the desired region
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    };
    % Fill the area below the curve
    \fill[blue, opacity=0.1] plot [smooth] coordinates {
        (-1,-11) (-1,-8) (0,-7) (3,-7) (5,-5) (7,-5) (9,-3) (12,-2)
    } -- (9,-11) -- (-1,-11) -- cycle;
\end{scope}
% Draw the curve 2
\draw[black,] plot [smooth] coordinates {
    (-3,-10.25) (-3,-8) (-2,-6) (-1,-5) (1,-5) (2,-4) (3,-2) (4,-0.25) 
}; 
% Fill the area
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    };  
    % Fill the region below the new curve and above the old curve
    \fill[teal,opacity=0.1] plot [smooth] coordinates {
         (-3,-10.25) (-3,-8) (-2,-6) (-1,-5) (1,-5) (2,-4) (3,-2) (4,-0.25) (5,0.25)
    } -- plot [smooth] coordinates {
        (9,-3) (7,-5) (5,-5) (3,-7) (0,-7) (-1,-8) (-1,-11)
    } -- cycle;
\end{scope}
% Draw the curve 3
\draw[black,] plot [smooth] coordinates {
    (-8,-7.6) (-8,-6) (-7,-5) (-5,-5) (-4,-4) (-3,-3) (0,-3) (1,-2) (2,0.6) 
}; 
% Fill the area inside the cycle and above the new curve
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    }; 
    % Fill the region above the curve
    \fill[green,opacity=0.1] plot [smooth] coordinates {
        (-8,-7.6) (-8,-6) (-7,-5) (-5,-5) (-4,-4) (-3,-3) (0,-3) (1,-2) (2,0.6)
    } -- (2,0.6) -- (0,1) -- (-10,1) -- (-10,-8) -- (-8,-7.6) -- cycle;
\end{scope}
% Fill the area inside the cycle and above the new curve
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    };    
    % Fill the region above the curve
    \fill[yellow,opacity=0.1]  plot [smooth] coordinates {
    (-8,-7.6) (-8,-6) (-7,-5) (-5,-5) (-4,-4) (-3,-3) (0,-3) (1,-2) (2,0.6)}--(5,0) -- plot [smooth] coordinates { (4,-0.25) (3,-2)  (2,-4) (1,-5) (-1,-5)  (-2,-6) (-3,-8)
    (-3,-10.25) } --cycle;
\end{scope}
\end{tikzpicture}
\caption{}
\label{subfig:decomposition}
\label{fig:decomposition}
\end{subfigure}
    \caption{An illustration of the induction step. Let $Y= \{a,b,c,d,e\},X = \emptyset, \La =[X,Y]$ and $F \in \Sf{\La}(2) \cap \HC{\La}$. If $F(a) < 0$ and $F(b) > 0$, then it follows that $F(R)$ for all $R \in [S,S \cup T]$ for $S= ab$ and $T= cde$ (\Cref{fig:low_subsets}).  In particular, $F \in \Sf{S,S  \cup T}(1)$ and thus, by \Cref{lem:lattice_structure}, it holds that $F \in \Sf{S',S'  \cup T}(1)$  for all $S' \subseteq Y \setminus T$. \\ \Cref{subfig:decomposition} shows the decomposition of the lattice $\La=[X,Y]$ for $T= \{c,d,e\}$ into the sublattices $[S,S\cup T]$ for all $S \subseteq Y \setminus T$. For every such sublattice we have that $F \in \Sf{[S,S\cup T]}(1) \cap \HC{[S,S\cup T]}$ and thus by induction $\asum{S,S\cup T}{F^+}=0$.
     % Hence, we have that $\asum{X,Y}{F^+} = \sum_{S \subseteq \{a,b\}} (-1)^{r(S)}\asum{S,S\cup T}{F^+}=0$
     }
\end{figure}
Let $S = X^+ \cup X^-$, then $F \in \HC{\La}$ implies that for $T=Y \setminus S$, we have that $F(R) = 0$ for all $R \in [S,S\cup T]$. In particular, it holds that $F \in \Sf{[S,S\cup T]}(k-1)$. Thus, by \Cref{lem:lattice_structure}, if $F \in \Sf{\La}(k)$, it follows that $F \in \Sf{[S',S'\cup T]}(k-1)$ for all $S' \subseteq Y\setminus T'$. Since $|S|$ is at most $2k$ it follows by counting that if $n \geq (k^2+k+1)$, the cardinality of $T$ is at least $(k-1)^2+(k-1)+1$. This allows to apply the inductions hypothesis to all sublattices $[S',S'\cup T]$ for $S'\subseteq Y \setminus T$, resulting in the following proposition. See also \Cref{fig:decomposition} for an illustration of the induction step.  
\begin{proposition}
\label{prop:maxout_quadratic_bound}
For $k \in \N$, let $\La=[X,Y]$ be a lattice of rank $n \geq k^2+k+1$ and $F \in \Sf{\La}(k) \cap \HC{\La}$. Then it holds that $\asum{X,Y}{F^+}=0$  
\end{proposition}
Applying \Cref{prop:maxout_quadratic_bound} to every sublattice of rank $k^2+k+1$ allows to sharpen the bound.
    \begin{proposition}
    \label{prop:maxout_quadratic_bound2}
   Let $\La$ be a lattice and $k \in \N$, then it holds that $\mathcal{A}(\Sf{\La}(k)) \subseteq \Sf{\La}(k^2+k)$.
        \end{proposition}
   Translating this result back to the CPWL functions and applying the argument iteratively for a rank-$2$-maxout network, layer by layer, we obtain the following theorem.
    \begin{theorem}
    \label{thm:maxout_doubly_exponential}
        For a number of layers $\ell \in \N$, it holds that $\mtwo{\B_d}{\ell} \subseteq \FB{d}(2^{2^{\ell}-1})$.
    \end{theorem}
Any rank $r_i$-maxout layer can be computed by $\lceil \log_2 r_i \rceil$ many rank $2$-maxout layer and a ReLU network is a special case of a rank-$2$-maxout network, resulting in the two following corollaries.
    \begin{corollary}
          For a number of layers $\ell \in \N$ and the rank vector $\mathbf{r} \in \N^\ell$, it holds that  $\maxout{\B_d}{r} \subseteq \FB{d}(2^{R^{\ell}-1})$, where $R= 2\cdot \max \{r_1,\ldots, r_\ell\}$.
    \end{corollary}
    \begin{corollary}\label{cor:loglog}
       The function $x \mapsto \{0,x_1,\ldots,x_{2^{2^\ell-1}}\}$ is not computable by a $\B^0_d$-conforming ReLU neural network with $\ell$ hidden layers.
    \end{corollary}
\section{Combinatorial proof for dimension four}
\label{sec:d=4}
In this section, we prove that the function $\max \{0, x_1 , \ldots, x_4\}$ cannot be computed by a $\B^0_d$-conforming rank-$(2,2)$-maxout networks or equivalently ReLU neural networks with $2$ hidden layers. This completely classfies the set of functions computable by $\B_d$-conforming ReLU neural networks with $2$ hidden layers.

If $\La$ is a lattice of rank $5$ and $F\in \Sf{\La}(2) \cap \HC{\La}$, we know by \Cref{lem:lowsubsets}, given that the supports of $F$ are not empty, that there are $X^+ \in \La_2 \cap \supp^+(F)$ and $X^- \in \La_2 \cap \supp^-(F)$. We first argue that in the special case of rank $5$ we can even assume that there are $X^+ \in \La_1 \cap \supp^+(F)$ and $X^- \in \La_1 \cap \supp^-(F)$. Then, with analogous arguments as in \Cref{sec:loglog}, we prove that $F^+ \in \Sf{\La}(4)$, resulting in the sharp bound for rank-$(2,2)$-maxout networks.
 \begin{figure}
    \centering
\begin{subfigure}[t]{0.46\textwidth}
    \begin{tikzpicture}[scale=0.36][
  every node/.style={draw, circle, minimum size=0.8cm, inner sep=2pt},
  every edge/.style={draw, gray, very thin}
]
% Define nodes symmetrically for each level
\node (abcde) at (0, 0) {abcde};
% Level 4
\node (abcd) at (-4, -2) {$\overline{e}$};
\node (abce) at (-2, -2) {$\overline{d}$};
\node (abde) at (0, -2) {$\overline{c}$};
\node (acde) at (2, -2) {$\overline{b}$};
\node (bcde) at (4, -2) {$\overline{a}$};
% Level 3
\node (abc) at (-9, -4) {abc};
\node (abd) at (-7, -4) {abd};
\node (abe) at (-5, -4) {abe};
\node (acd) at (-3, -4) {acd};
\node (ace) at (-1, -4) {ace};
\node (ade) at (1, -4) {ade};
\node (bcd) at (3, -4) {bcd};
\node (bce) at (5, -4) {bce};
\node (bde) at (7, -4) {bde};
\node (cde) at (9, -4) {cde};
% Level 2
\node (ab) at (-9, -6) {ab};
\node (ac) at (-7, -6) {ac};
\node (ad) at (-5, -6) {ad};
\node (ae) at (-3, -6) {ae};
\node (bc) at (-1, -6) {bc};
\node (bd) at (1, -6) {bd};
\node (be) at (3, -6) {be};
\node (cd) at (5, -6) {cd};
\node (ce) at (7, -6) {ce};
\node (de) at (9, -6) {de};
% Level 1
\node (a) at (-4, -8) {a};
\node (b) at (-2, -8) {b};
\node (c) at (0, -8) {c};
\node (d) at (2, -8) {d};
\node (e) at (4, -8) {e};
\node[draw, circle, thick, red, fit=(ab), inner sep=-2.5pt] {};
\node[draw, circle, thick, red, fit=(abc), inner sep=-3.5pt] {};
\node[draw, circle, thick, red, fit=(ce), inner sep=-2.5pt] {};
\node[draw, circle, thick, red, fit=(bce), inner sep=-3.5pt] {};
\node[draw, circle, thick, dashed, blue, fit=(ae), inner sep=-2.5pt] {};
\node[draw, circle, thick, dashed, blue, fit=(ace), inner sep=-3.5pt] {};
\node[draw, circle, thick, dashed, blue, fit=(cd), inner sep=-2.5pt] {};
\node[draw, circle, thick, dashed, blue, fit=(bcd), inner sep=-3.5pt] {};
% Level 0
\node (empty) at (0, -10) {$\emptyset$};
% Draw edges manually
% Level 4 to 5
\draw[gray!50, very thin] (abcde) -- (abcd);
\draw[gray!50, very thin] (abcde) -- (abce);
\draw[gray!50, very thin] (abcde) -- (abde);
\draw[gray!50, very thin] (abcde) -- (acde);
\draw[gray!50, very thin] (abcde) -- (bcde);
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
\draw[gray!50, very thin] (abce) -- (abc);
\draw[gray!50, very thin] (abce) -- (abe);
\draw[gray!50, very thin] (abce) -- (ace);
\draw[gray!50, very thin] (abce) -- (bce);
\draw[gray!50, very thin] (abde) -- (abd);
\draw[gray!50, very thin] (abde) -- (abe);
\draw[gray!50, very thin] (abde) -- (ade);
\draw[gray!50, very thin] (abde) -- (bde);
\draw[gray!50, very thin] (acde) -- (acd);
\draw[gray!50, very thin] (acde) -- (ace);
\draw[gray!50, very thin] (acde) -- (ade);
\draw[gray!50, very thin] (acde) -- (cde);
\draw[gray!50, very thin] (bcde) -- (bcd);
\draw[gray!50, very thin] (bcde) -- (bce);
\draw[gray!50, very thin] (bcde) -- (bde);
\draw[gray!50, very thin] (bcde) -- (cde);
% Level 2 to 3
\draw[red] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (abe) -- (ab);
\draw[gray!50, very thin] (abe) -- (ae);
\draw[gray!50, very thin] (abe) -- (be);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (ace) -- (ac);
\draw[blue] (ace) -- (ae);
\draw[gray!50, very thin] (ace) -- (ce);
\draw[gray!50, very thin] (ade) -- (ad);
\draw[gray!50, very thin] (ade) -- (ae);
\draw[gray!50, very thin] (ade) -- (de);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[blue] (bcd) -- (cd);
\draw[gray!50, very thin] (bce) -- (bc);
\draw[gray!50, very thin] (bce) -- (be);
\draw[red] (bce) -- (ce);
\draw[gray!50, very thin] (bde) -- (bd);
\draw[gray!50, very thin] (bde) -- (be);
\draw[gray!50, very thin] (bde) -- (de);
\draw[gray!50, very thin] (cde) -- (cd);
\draw[gray!50, very thin] (cde) -- (ce);
\draw[gray!50, very thin] (cde) -- (de);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (ae) -- (a);
\draw[gray!50, very thin] (ae) -- (e);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (be) -- (b);
\draw[gray!50, very thin] (be) -- (e);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
\draw[gray!50, very thin] (ce) -- (c);
\draw[gray!50, very thin] (ce) -- (e);
\draw[gray!50, very thin] (de) -- (d);
\draw[gray!50, very thin] (de) -- (e);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[gray!50, very thin] (e) -- (empty);
% Draw the cycle
\draw[black] plot [smooth cycle] coordinates {
    (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
};
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}[t]{0.46\textwidth}
\begin{tikzpicture}[scale=0.36]
% Define nodes symmetrically for each level
\node (abcde) at (0, 0) {abcde};
% Level 4
\node (abcd) at (-4, -2) {$\overline{e}$};
\node (abce) at (-2, -2) {$\overline{d}$};
\node (abde) at (0, -2) {$\overline{c}$};
\node (acde) at (2, -2) {$\overline{b}$};
\node (bcde) at (4, -2) {$\overline{a}$};
% Level 3
\node (abc) at (-9, -4) {abc};
\node (abd) at (-7, -4) {abd};
\node (abe) at (-5, -4) {abe};
\node (acd) at (-3, -4) {acd};
\node (ace) at (-1, -4) {ace};
\node (ade) at (1, -4) {ade};
\node (bcd) at (3, -4) {bcd};
\node (bce) at (5, -4) {bce};
\node (bde) at (7, -4) {bde};
\node (cde) at (9, -4) {cde};
% Level 2
\node (ab) at (-9, -6) {ab};
\node (ac) at (-7, -6) {ac};
\node (ad) at (-5, -6) {ad};
\node (ae) at (-3, -6) {ae};
\node (bc) at (-1, -6) {bc};
\node (bd) at (1, -6) {bd};
\node (be) at (3, -6) {be};
\node (cd) at (5, -6) {cd};
\node (ce) at (7, -6) {ce};
\node (de) at (9, -6) {de};
% Level 1
\node (a) at (-4, -8) {a};
\node (b) at (-2, -8) {b};
\node (c) at (0, -8) {c};
\node (d) at (2, -8) {d};
\node (e) at (4, -8) {e};
% Level 0
\node (empty) at (0, -10) {$\emptyset$};
% Draw edges manually
% Level 4 to 5
\draw[gray!50, very thin] (abcde) -- (abcd);
\draw[gray!50, very thin] (abcde) -- (abce);
\draw[gray!50, very thin] (abcde) -- (abde);
\draw[gray!50, very thin] (abcde) -- (acde);
\draw[gray!50, very thin] (abcde) -- (bcde);
% Level 3 to 4
\draw[gray!50, very thin] (abcd) -- (abc);
\draw[gray!50, very thin] (abcd) -- (abd);
\draw[gray!50, very thin] (abcd) -- (acd);
\draw[gray!50, very thin] (abcd) -- (bcd);
\draw[gray!50, very thin] (abce) -- (abc);
\draw[gray!50, very thin] (abce) -- (abe);
\draw[gray!50, very thin] (abce) -- (ace);
\draw[gray!50, very thin] (abce) -- (bce);
\draw[gray!50, very thin] (abde) -- (abd);
\draw[gray!50, very thin] (abde) -- (abe);
\draw[gray!50, very thin] (abde) -- (ade);
\draw[gray!50, very thin] (abde) -- (bde);
\draw[gray!50, very thin] (acde) -- (acd);
\draw[gray!50, very thin] (acde) -- (ace);
\draw[gray!50, very thin] (acde) -- (ade);
\draw[gray!50, very thin] (acde) -- (cde);
\draw[gray!50, very thin] (bcde) -- (bcd);
\draw[gray!50, very thin] (bcde) -- (bce);
\draw[gray!50, very thin] (bcde) -- (bde);
\draw[gray!50, very thin] (bcde) -- (cde);
% Level 2 to 3
\draw[gray!50, very thin] (abc) -- (ab);
\draw[gray!50, very thin] (abc) -- (ac);
\draw[gray!50, very thin] (abc) -- (bc);
\draw[gray!50, very thin] (abd) -- (ab);
\draw[gray!50, very thin] (abd) -- (ad);
\draw[gray!50, very thin] (abd) -- (bd);
\draw[gray!50, very thin] (abe) -- (ab);
\draw[gray!50, very thin] (abe) -- (ae);
\draw[gray!50, very thin] (abe) -- (be);
\draw[gray!50, very thin] (acd) -- (ac);
\draw[gray!50, very thin] (acd) -- (ad);
\draw[gray!50, very thin] (acd) -- (cd);
\draw[gray!50, very thin] (ace) -- (ac);
\draw[gray!50, very thin] (ace) -- (ae);
\draw[gray!50, very thin] (ace) -- (ce);
\draw[gray!50, very thin] (ade) -- (ad);
\draw[gray!50, very thin] (ade) -- (ae);
\draw[gray!50, very thin] (ade) -- (de);
\draw[gray!50, very thin] (bcd) -- (bc);
\draw[gray!50, very thin] (bcd) -- (bd);
\draw[gray!50, very thin] (bcd) -- (cd);
\draw[gray!50, very thin] (bce) -- (bc);
\draw[gray!50, very thin] (bce) -- (be);
\draw[gray!50, very thin] (bce) -- (ce);
\draw[gray!50, very thin] (bde) -- (bd);
\draw[gray!50, very thin] (bde) -- (be);
\draw[gray!50, very thin] (bde) -- (de);
\draw[gray!50, very thin] (cde) -- (cd);
\draw[gray!50, very thin] (cde) -- (ce);
\draw[gray!50, very thin] (cde) -- (de);
% Level 1 to 2
\draw[gray!50, very thin] (ab) -- (a);
\draw[gray!50, very thin] (ab) -- (b);
\draw[gray!50, very thin] (ac) -- (a);
\draw[gray!50, very thin] (ac) -- (c);
\draw[gray!50, very thin] (ad) -- (a);
\draw[gray!50, very thin] (ad) -- (d);
\draw[gray!50, very thin] (ae) -- (a);
\draw[gray!50, very thin] (ae) -- (e);
\draw[gray!50, very thin] (bc) -- (b);
\draw[gray!50, very thin] (bc) -- (c);
\draw[gray!50, very thin] (bd) -- (b);
\draw[gray!50, very thin] (bd) -- (d);
\draw[gray!50, very thin] (be) -- (b);
\draw[gray!50, very thin] (be) -- (e);
\draw[gray!50, very thin] (cd) -- (c);
\draw[gray!50, very thin] (cd) -- (d);
\draw[gray!50, very thin] (ce) -- (c);
\draw[gray!50, very thin] (ce) -- (e);
\draw[gray!50, very thin] (de) -- (d);
\draw[gray!50, very thin] (de) -- (e);
% Level 0 to 1
\draw[gray!50, very thin] (a) -- (empty);
\draw[gray!50, very thin] (b) -- (empty);
\draw[gray!50, very thin] (c) -- (empty);
\draw[gray!50, very thin] (d) -- (empty);
\draw[gray!50, very thin] (e) -- (empty);
% Draw the cycle
\draw[black] plot [smooth cycle] coordinates {
    (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
};
\node[draw, circle, thick, red, fit=(a), inner sep=-0.5pt] {};
\node[draw, circle, thick, blue, dashed ,fit=(bcde), inner sep=-2pt] {};
% Clip and fill the desired region
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    };    
    % Fill the area below the curve
    \fill[blue, opacity=0.1] plot [smooth] coordinates {
        (-1,-11) (-1,-8) (0,-7) (3,-7) (5,-5) (7,-5) (9,-3) (12,-2)
    } -- (9,-11) -- (-1,-11) -- cycle;
\end{scope}
% Draw the curve 2
\draw[black,] plot [smooth] coordinates {
    (-3,-10.25) (-3,-8) (-2,-6) (-1,-5) (1,-5) (2,-4) (3,-2) (4,-0.25) 
}; 
% Fill the area
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    };   
    % Fill the region below the new curve and above the old curve
    \fill[blue,opacity=0.1] plot [smooth] coordinates {
         (-3,-10.25) (-3,-8) (-2,-6) (-1,-5) (1,-5) (2,-4) (3,-2) (4,-0.25) (5,0.25)
    } -- plot [smooth] coordinates {
        (9,-3) (7,-5) (5,-5) (3,-7) (0,-7) (-1,-8) (-1,-11)
    } -- cycle;
\end{scope}
% Fill the area inside the cycle and above the new curve
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    };   
    % Fill the region above the curve
    \fill[red,opacity=0.1] plot [smooth] coordinates {
        (-8,-7.6) (-8,-6) (-7,-5) (-5,-5) (-4,-4) (-3,-3) (0,-3) (1,-2) (2,0.6)
    } -- (2,0.6) -- (0,1) -- (-10,1) -- (-10,-8) -- (-8,-7.6) -- cycle;
\end{scope}
% Fill the area inside the cycle and above the new curve
\begin{scope}
    % Clip to the inside of the cycle
    \clip plot [smooth cycle] coordinates {
        (0,-11) (9,-7) (10,-5) (9,-3) (0,1) (-9,-3) (-10,-5) (-9,-7)
    };   
    % Fill the region above the curve
    \fill[red,opacity=0.1]  plot [smooth] coordinates {
    (-8,-7.6) (-8,-6) (-7,-5) (-5,-5) (-4,-4) (-3,-3) (0,-3) (1,-2) (2,0.6)}--(5,0) -- plot [smooth] coordinates { (4,-0.25) (3,-2)  (2,-4) (1,-5)(-1,-5)  (-2,-6) (-3,-8) (-3,-10.25) } --cycle;
\end{scope}
\end{tikzpicture}
\end{subfigure}
\caption{An illustration of \Cref{lem:pairing} (left) and \Cref{lem:case1-4} (right). If $\supp(F)\subseteq \La_2 \cup \La_3$, then we can match every $S \in \La_2$ with a $T \in \La_3$ such that $F(T)=F(S)$ which implies $\asum{\emptyset,abcde}{F^+}= \sum_{S \in \La_2}F^+(S) - \sum_{T \in \La_3} F^+(T)=0$. If $F(a) <0$ and $F(bcde) > 0$, then it holds that $\asum{\emptyset,abcde}{F}= \asum{\emptyset,bcde}{F}=0$.}
    \label{fig:4d}
\end{figure}   

If the positive support of a function $F 
\in \Sf{\La}(2) \cap \HC{\La}$ is contained in the levels $\La_2$ and $\La_3$, then for every $S \in \supp^+(F) \cap \La_2$ there must be a $T \in \supp^+(F) \cap \La_3$ such that $T \supseteq S$ and $F(S) \leq F(T)$ since $\asum{S,Y}{F}=0$. Applying the same argument to $T$, we conclude that $F(S) = F(T)$ and that there are no further subsets in $\supp^+(F)$ that are comparable to $S$ or $T$. Thus, we can match the subsets $S \in \La_2$ with the subsets $T \in \La_3$ such that $F(S) = F(T)$ and hence it follows that $\asum{X,Y}{F^+}= \sum_{S \in \La_2}F^+(S) - \sum_{T \in \La_3} F^+(T)=0$. By symmetry, the same holds if $\supp^-(F) \subseteq \La_2 \cup \La_3$.  See \Cref{fig:4d} for an illustration. Following this idea, we state the lemma for a more general case.
 \begin{lemma}
    	\label{lem:pairing}
    	Let $\La = [X,Y]$ be a lattice of rank $n$ and $F \in \Sf{\La}(k) \cap \HC{\La}$ with $n \geq 2k+1$. If there are   $i,j \in [n]_0$ such that $\supp^+(F) \subseteq \La_i \cup \La_j$ or $\supp^-(F) \subseteq \La_i \cup \La_j$, then it holds that $F^+ \in \Sf{\La}(n-1)$.
      \end{lemma} 
    If there is a $X^+ \in \La_1 \cap \supp^+(F)$ and a $X^- \in \La_4 \cap \supp^-(F)$, then it holds that $\asum{X,Y}{F^+}=\asum{X^+,Y}{F}=0$ (\Cref{fig:4d} and \Cref{lem:case1-4} in the appendix). Thus we can assume that there are $X^+ \in \La_1 \cap \supp^+(F)$ and $X^- \in \La_1 \cap \supp^-(F)$. By proceeding analogously as in \Cref{sec:loglog}, we prove the following theorem.
    \begin{theorem}
    \label{thm:maxout4d}
          It holds that $\mtwo{\B_d}{2} =\FB{d}(4)$. In particular, the function $x \mapsto \{0,x_1,\ldots,x_{4}\}$ is not computable by a $\B^0_d$-conforming ReLU neural network with $2$ hidden layers.
    \end{theorem}
\section{The unimaginable power of maxouts}
\label{sec:3-2}
By \Cref{prop:inclusion_of_conforming}, all functions in $\FB{d}(\prod_{i=1}^\ell r_i)$ are representable by a $\B_d$-conforming rank-$\mathbf{r}$-maxout network. In \Cref{sec:d=4}, we have seen that this bound is tight for the rank vector $(2,2)$. Moreover, \Cref{con:2} would imply that the bound is tight for all rank vectors that consist only of powers of $2$.
In this section, we prove that this bound in general is not tight by demonstrating that  the function $x \mapsto \{0,x_1,\ldots,x_6\}$ is computable by a $\B_d^0$-conforming rank-$(3,2)$-maxout network. 
    \begin{proposition}
    \label{prop:outof6space}
        Let $f_1,f_2 \in \FB{7}(3)$ be the functions given by \begin{align*}
            f_1 &= 2 \cdot \sigma_{\{1,2\}} + \sigma_{\{1,4,5\}} + \sigma_{\{1,6,7\}} + \sigma_{\{2,4,6\}} + \sigma_{\{2,5,7\}} \\
            f_2 &=  \sigma_{\{3,4,5\}} + \sigma_{\{3,6,7\}} + \sigma_{\{1,2,4\}} + \sigma_{\{1,2,5\}
        } + \sigma_{\{1,2,6\}} + \sigma_{\{1,2,7\}}
        \end{align*}
         Then it holds that $\max\{f_1,f_2\}  \in \FB{7}(7) \setminus \FB{7}(6)$.
    \end{proposition}
   \begin{proof}[Proof Sketch]
           Let $F_1 = \Phi(f_1)$ and $F_2 = \Phi(f_2)$.
           We write $i_1 \cdots i_n$ for $\{i_1, \ldots, i_n\}$  and $\overline{i_1 \cdots i_n}$ for $[7] \setminus \{i_1,\ldots, i_n\}$ and note that the sublattices 
           $[12,\overline{3}], [13,\overline{2}], [23,\overline{1}],  
           [3,\overline{12}], [2,\overline{13}],$ $ [1,\overline{23}], [\emptyset,\overline{123}],[123,[7]]$ form a partition of $[\emptyset,[7]]$.

           We first show that on any of the above sublattices except $[1,\overline{23}]$, either $F_1$ or $F_2$ attains the maximum on all elements of the sublattice and that for $F \coloneqq F_1 -F_2$ it holds that $\supp^+(F) \subseteq [1,\overline{23}] \cup 146 \cup 167$ and $\asum{[\emptyset,[7]]}{F^+} =\asum{12,\overline{3}}{F}-F(146) - F(167) = -2$ and thus $F^+ \in \Sf{\La} \setminus \Sf{\La}(6)$.
             Then by looking at the partition into sublattices, we argue that $F \in \HC{\La}$ and thus by \Cref{lem:maxout_conforming_setfunc}, we conclude that $\max \{f_1,f_2\} \in \FB{7}\setminus\FB{7}(6)$.
              \end{proof}
 Hence $\max \{f_1,f_2\} = \sum_{M \subseteq [7]} \lambda_m \sigma_M$ with $\lambda_{[7]} \neq 0$ and since all functions in $\FB{d}(6)$ are computable by a rank-$(3,2)$-maxout network, we conclude that $x \mapsto \{x_1,\ldots,x_7\}$ is computable by a rank-$(3,2)$-maxout network or equivalently:
    \begin{theorem}
        The function $x \mapsto \{0,x_1,\ldots,x_6\}$ is computable by a rank-$(3,2)$-maxout network.
    \end{theorem} 
    \begin{remark}
        One can check (e.g.,  with a computer) that $x \mapsto \{0,x_1,\ldots,x_6\}$ is computable by a rank-$(3,2)$-maxout network with integral weights. In light of \citet{haase2023lower}, this indicates that a proof of \Cref{con:2} must somehow exploit the fact that the number of entries in the max functions is one plus a power of two.
    \end{remark}
    \paragraph{\textbf{Acknowledgments}}
    Moritz Grillo is supported by the Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) under Germanys Excellence Strategy --- The Berlin Mathematics Research Center
MATH+ (EXC-2046/1, project ID: 390685689).
    Part of this work was completed while Christoph Hertrich was affiliated with Universit Libre de Bruxelles, Belgium, and received support by the European Union's Horizon Europe research and innovation program under the Marie Skodowska-Curie grant agreement No 101153187---NeurExCo.
\bibliographystyle{unsrtnat}
\bibliography{ref}
\newpage
\appendix
\crefalias{section}{appendix}
\section{List of notation}
\label{sec:notation}
\begin{tabular}{cp{0.9\textwidth}}
  $\PP$ & Polyhedral complex \\
  $\B_d$ & Braid fan with lineality space  \\
  $\B_d^0$ & Braid fan without lineality space \\
  $\ell$ & Number of layers \\
  $\mathbf{r} \in \N^\ell$ & Vector where each entry corresponds to the rank of a layer \\
  $\maxout{d}{r}$ & Functions computable by rank-$\mathbf{r}$-maxout networks \\
  $\maxout{\B_d}{r}$ & Functions computable by $\B_d$-conforming rank-$\mathbf{r}$-maxout networks  \\
    $\mtwo{\B_d}{\ell}$ & Functions computable by $\B_d$-conforming networks with $\ell$ rank-$2$-maxout layers  \\
    $\sigma_M$ & Function $x \mapsto \max_{i \in M} x_i$ \\
    $\FB{d}$ & CPWL functions compatible with $\B_d$  \\
    $\FB{d}(k)$ & Span of $\{\sigma_M \mid M \subseteq [d], |M| \leq k\}$  \\
    $\La$ & Boolean lattice \\
    $\Sf{\La}$ & Functions on $\La$ \\
    $\ind_S$ & $\sum_{i \in S} e_i$ \\
    $\al{S,T}$ &  $\sum_{S \subseteq Q \subseteq T} (-1)^{r(Q)-r(S)} \ind_Q$ \\
    $\Sf{\La}(k)$ & Functions on $\La$ that are orthogonal to $\spn \{\al{S,T} \mid r(T) -r(S) \leq k, S \subseteq T \}$  \\
    $\Phi$ & Isomorphism between $\FB{d}$ and $\Sf{d}$. \\
    $\mathcal{A}(U)$ & Span of $ \{\max\{f_1,f_2\} \mid  f_1,f_2 \in U,\max\{f_1,f_2\} \in \FB{d} \}$ \\
   $x_1 \cdots  x_n$ & $\{x_1,\ldots,x_n\} \in \La=[X,Y]$ \\
   $\overline{x_1 \cdots  x_n}$ &  $X \cup (Y \setminus \{x_1,\ldots,x_n\}) $\\
  $\HC{\La}^r\subseteq\Sf{\La}^r$ &  Set of conforming tuples \\
  $\HC{\La}$ &$\{F \in \Sf{\La} \mid F(S) \text{ and } F(T) \text{ do not have opposite signs for } S \subseteq T \}$
  
\end{tabular}\\

\section{Proofs}\label{sec:proofs}

\begin{proof}[Proof of \Cref{prop:iso_cpwl_to_setfunction}]
 The map $\Phi$ is clearly a linear map.
    To prove that $\Phi$ is an isomorphism, we show that a function $f\in \FB{d}$ is uniquely determined by its values on $\{\ind_S\}_{S\subseteq [d]}$ and any choice of real values $\{y_S\}_{S\subseteq [d]}$ give rise to a function $f\in \FB{d}$ such that $f(\ind_S)=y_S$.

    First, note that the maximal cones of $\B_d$ are of the form $C_{\pi}=\{x \in \R^d \mid x_{\pi(1)} \leq \ldots \leq  x_{\pi(d)}\}$ for a permutation $\pi \colon [d] \to [d]$. There are exactly the $d+1$ indicator vectors $\{\ind_{S_i}\}_{i=0,\ldots,d}$ contained in $C_{\pi}$, where $S_i \coloneqq\{\pi(d+1-i),\ldots,\pi(d)\}$ for $i \in [d]$ and $S_0 \coloneqq \emptyset$. Moreover, the vectors $\{\ind_{S_i}\}_{i=0,\ldots,d}$ are affinely independent and hence the values $\{f(\ind_{S_i})\}_{i=0,\ldots,d}$ uniquely determine the affine linear function $f|_{C_{\pi}}$. Therefore, $f$ is uniquely determined by $\{f(\ind_{S_i})\}_{S\subseteq[d]}$.
    
    Given any values $\{y_S\}_{S \subseteq [d]}$, by the discussion above, there are unique affine linear maps $f|_{C_{\pi}}$ yielding $f|_{C_{\pi}}(\ind_S)=y_S$ for all $S \subseteq [d]$ such that $\ind_S \in C_{\pi}$. It remains to show that the resulting function $f$ is well-defined on the facets. Any such facet is of the form \[C_{\pi,i}=\{x \in \R^d \mid x_{\pi(1)} \leq \ldots \leq x_{\pi(i)} = x_{\pi(i+1)} \leq \ldots \leq  x_{\pi(d)}\},\] which is the intersection of $C_{\pi}$ and $C_{\pi \circ (i,i+1)}$,  where $(i,i+1)$ denotes the transposition swapping $i$ and $i+1$. However, the indicator vectors $\{\ind_{S_i}\}_{i\in[d]\setminus \{i\}}$ contained in $C_{\pi,i}$ are a subset of the indicator vectors contained in $C_{\pi}$ and $C_{\pi \circ (i,i+1)}$. Therefore, it holds that $f|_{C_{\pi}}(x)=f|_{C_{\pi \circ (i,i+1)}}(x)$ for all $x \in C_{\pi,i}$ implying that $f$ is well-defined as a CPWL function.
\end{proof}

\begin{proof}[Proof of \Cref{prop:mobius_inversion}]
   For $S,M \subseteq [d]$, it holds that \[\sigma_M(\ind_S) =
   \begin{cases}
       1 &   M \cap S \neq \emptyset \\
       0 &  M \cap S =\emptyset
   \end{cases}\] and thus $\Phi(f)(S) = f(\ind_S) = \sum\limits_{\substack{M \subseteq [d] \\ M \cap S \neq \emptyset}}\lambda_M$.
   
   For the inverse, let $G \in \Sf{d}$ be the map given by $G(S) = \sum_{M \subseteq S} \lambda_M$. 
   We use the notation $S^c \coloneqq [d] \setminus S$ and observe that it holds that \[F(S) = \sum\limits_{\substack{M \subseteq [d] \\ M \cap S \neq \emptyset}}\lambda_M=\sum\limits_{\substack{M \subseteq [d] \\ M \nsubseteq S^c }}\lambda_M=\sum\limits_{M \subseteq [d]}\lambda_M-\sum\limits_{\substack{M \subseteq [d] \\ M \subseteq S^c }}\lambda_M = \sum\limits_{M \subseteq [d]}\lambda_M - G(S^c)\]
   Given a $F \in \Sf{d}$, we now compute the corresponding $\lambda_M$ using the M\"obius inversion formula. For a Boolean lattice the M\"obius function $\mu$ on sublattices satisfies  $\mu(S,M) = (-1)^{r(M)-r(S)}$ \cite{stanley07arrangement}. Hence the coefficient $\lambda_M$ yields
   \begin{align*}
       \lambda_M  &= \sum_{S \subseteq M} \mu(S,M)\cdot G(S)
       \\&= \sum_{S \subseteq M} \mu(S,M)\cdot \bigg(\Big(\sum\limits_{M \subseteq [d]}\lambda_M\Big)-F(S^c)\bigg)
       \\&= \sum\limits_{M \subseteq [d]}\lambda_M \Big(\sum_{S \subseteq M} \mu(S,M)\Big)-\sum_{S \subseteq M} \mu(S,M)\cdot F(S^c)
       \\&= -\sum_{S \subseteq M} (-1)^{r(M)-r(S)}\cdot F(S^c)
       \\&= -\sum_{S \supseteq M^c} (-1)^{r(M)-r(S^c)}\cdot F(S)
   \end{align*}
   It holds that $(-1)^{r(M)-r(S^c)}=(-1)^{r(S)-r(M^c)}$ since  the parity of $r(M)-r(S^c)$ and $r(S)-r(M^c)$ is the same. Thus, it follows that $\lambda_M = -\sum_{S \supseteq M^c} (-1)^{r(S)-r(M^c)}\cdot F(S) = -\asum{[d]\setminus M,[d]}{F}$, proving the claim.
  \end{proof}
   \begin{proof}[Proof of \Cref{lem:equivalence_assumptions}]
    Let $g_d \colon \R^{d-1} \to \R^{d}$ be the linear map given by $(x_1,\ldots,x_{d-1}) \mapsto  (x_1,\ldots,x_{d-1},0)$. Then $f_i \circ \ldots \circ f_1$ is $\B_d$-conforming if and only if $f_i \circ \ldots \circ f_1 \circ g_d$ is $\B^0_{d-1}$-conforming. In particular, if $x \mapsto \max \{x_1,\ldots,x_{d}\}$ can be represented with a $\B_{d}$-conforming  rank-$\mathbf{r}$-maxout network $f$, then $f \circ g_d$ is a $\B_{d-1}^0$-conforming rank-$\mathbf{r}$-maxout network computing $x \mapsto \max \{0,x_1,\ldots,x_{d-1}\}$.
    
    Conversely, let $h_d \colon \R^{d} \to \R^{d-1}$ be the linear map given by $(x_1,\ldots,x_{d}) \mapsto (x_1-x_{d},\ldots,x_{d-1}-x_{d})$. Then $f_i \circ \ldots \circ f_1$ is $\B^0_d$-conforming if and only if $f_i \circ \ldots \circ f_1 \circ h_d$ is $\B_{d}$-conforming.
    In particular, if $f$ is a $\B_{d-1}^0$-conforming rank-$\mathbf{r}$-maxout network computing $x \mapsto \max \{0,x_1,\ldots,x_{d-1}\}$, then $f \circ h_d + \sigma_{\{d\}}$ is a $\B_{d}$-conforming rank-$\mathbf{r}$-maxout network computing $x \mapsto \max \{x_1,\ldots,x_{d}\}$
   \end{proof}
\begin{proof}[Proof of \Cref{prop:inclusion_of_conforming}]
     By induction on the number of layer $\ell$, assume  that $\FB{d}(\prod_{i=1}^{\ell-1} r_i) \subseteq \maxout{d}{(r_1,\ldots,r_{\ell-1})}$, since the case $\ell=1$ is trivially satisfied.  Now, let $M \subseteq [d]$ be a subset of cardinality at most $|\prod_{i=1}^{\ell} r_i|$ and let $M_1 \cup \cdots M_{r_\ell}$ be a partition of $M$ into subsets of cardinality at most $|\prod_{i=1}^{\ell-1} r_i|$. Then $\sigma_{M_1}, \ldots, \sigma_{M_{r_\ell}} \in \FB{d}(\prod_{i=1}^{\ell-1} r_i) \subseteq \maxout{d}{(r_1,\ldots,r_{\ell-1})}$ and hence $\max\{\sigma_{M_1}, \ldots, \sigma_{M_{r_\ell}}\}$ is computable by a rank-$\mathbf{r}$-maxout network. Since all restriction to polyhedra of the braid fan of the CPWL functions $\sigma_{M_1}, \ldots, \sigma_{M_{r_\ell}}$ are of the form $x \mapsto x_i$, the function $\sigma_M = \max\{\sigma_{M_1}, \ldots, \sigma_{M_{r_\ell}}\}$ has only breakpoints on hyperplanes $x_i = x_j$ and thus is compatible with $\B_d$. Hence, it holds that $\sigma_M \in \maxout{d}{r}$ and since $\sigma_M$ is an arbitrary basis function of $\FB{d}(\prod_{i=1}^\ell r_i)$ and $\maxout{d}{r}$ is a vector space, the claim follows.
\end{proof}
   \begin{proof}[Proof of \Cref{lem:maxout_conforming_setfunc}]
Let  $(F_1,\ldots,F_r) \in \HC{d}^r$, $f_i \coloneqq \Phi^{-1}(F_i)$ the corresponding cpwl functions and $C$ be a cone of the braid arrangement. Then, $C= \cone(\ind_{S_1}, \ldots \ind_{S_{k-1}}) +\spn(\ind_{[k]})$ for a $k \in [d]$ and a chain $\emptyset \subsetneq S_1 \subsetneq S_2 \subsetneq \cdots \subsetneq S_k = [d]$. Since $(F_1,\ldots,F_r) \in \HC{d}^r$, there is a $i \in [r]$ such that $F_i(S_j) = \max\{F_1,\ldots,F_r\}(S_j)$ for all $j \in [k]$, which is equivalent to $f_i(\ind_{S_j}) = \max \{f_1(\ind_{S_j}),\ldots,f_r(\ind_{S_j})\}$ for all $j \in [k]$ and thus $f_{j} = \max \{f_1,\ldots, f_r\}$ on $C$. Since $f_j$ is compatible with $\B_d$, it holds that $f_j$ is linear on $C$ and therefore also \[\max \{f_1,\ldots,f_r\} = f_j = \Phi^{-1}(F_j) = \Phi^{-1}(\max \{F_1,\ldots,F_r\})\] is linear on $C$. Since $C$ was arbitrary, it holds that $\max \{f_1,\ldots,f_r\} = \Phi^{-1}(\max \{F_1,\ldots,F_r\})$ is compatible with $\B_d$.

Conversely, if $(F_1,\ldots,F_r) \notin \HC{d}^r$, then there are is a chain  $\emptyset =S_0 \subsetneq S_1 \subsetneq \ldots \subsetneq S_k=[d]$ with $\bigcap_{i=0}^n a(S_i) = \emptyset$, where $a(S_i) = \argmax \{F_1,\ldots, F_r\}(S_i)$. This chain corresponds to a cone $C$ of the braid arrangment and it means that there is no function $f_j$ that attains the maximum on all rays $\ind_{S_i}$ of the cone $C$. Note that $\max\{f_1,\ldots,f_r\}$ can just be affine linear on a cone of the braid arrangement if $\max\{f_1,\ldots,f_r\}=f_j$ for some $j \in [r]$. We excluded this possibility by the above discussion and hence the function $\max\{f_1,\ldots,f_r\}$ is not linear on $C$ and therefore not $\B_d$-conforming.

\end{proof}
\begin{proof}[Proof of \Cref{lem:layer_description}]
    For \ref{lem:layer_description_case1}, note that $\FB{d}(1)$ equals the set of affine linear functions. Now, $f \in \mtwo{\B_d}{1}$, if and only if there are $f^{(i)}_1, f^{(i)}_2 \in \FB{d}(1)$ for $i \in [m]$ for some   $m \in \N$ such that $f = \sum_{i \in [m]} \lambda_i \max \{f^{(i)}_1, f^{(i)}_2\}$ and all summands $\max \{f^{(i)}_1, f^{(i)}_2\}$ are compatible with $\B_d$, which implies that $\mtwo{\B_d}{1}=\mathcal{A}(\FB{d}(1))$. Since the functions are affine linear, we have that $\{x \in \R^d \mid (f^{(i)}_1- f^{(i)}_2)(x) = 0 \}$ is a hyperplane (or $\R^d$) and since $\max \{f^{(i)}_1, f^{(i)}_2\}$ is compatible with $\B_d$, it follows that $f^{(i)}_1- f^{(i)}_2 = \sigma_{\{j_1\}} - \sigma_{\{j_2\}}$ for some $j_1,j_2 \in [d]$. Thus $f^{(i)}_1 = \sigma_{\{j_1\}} + g_i$ and $f^{(i)}_2 = \sigma_{\{j_2\}} + g_i$ for some $g_i \in \FB{d}(1)$ and therefore $f = \sum_{i \in [m]} \lambda_i \max \{\sigma_{\{j_1\}},\sigma_{\{j_2\}}\}+g_i = \sum_{i \in [m]} \lambda_i \max \{x_{j_1},x_{j_2}\}+g_i \in \FB{d}(2)$. Since it clearly also holds that $\FB{d}(2) \subseteq \mathcal{A}(\FB{d}(1))$, the claim follows.

    
      For \ref{lem:layer_description_casel}, the function $f$ is in $\mtwo{\B_d}{\ell}$ if and only if there are $f^{(i)}_1, f^{(i)}_2 \in \mtwo{\B_d}{\ell-1}$ for $i \in [m]$ for some  $m \in \N$  such that  $f = \sum_{i \in [m]} \lambda_i \max \{f^{(i)}_1, f^{(i)}_2\}$ and all functions  $\max \{f^{(i)}_1, f^{(i)}_2\}$ are compatible with $\B_d$. This is the case if and only if $f \in \mathcal{A}(\mtwo{\B_d}{\ell-1})$. The claim then follows by induction. 

\end{proof}
    \begin{proof}[Proof of \Cref{lem:wlog_zero_map}]
    Let $A_0(U) \coloneqq \spn \{\max\{0,f\} \mid  f \in U,\max\{0,f\} \in \FB{d} \}.$
    It is clear that $A_0(U) \subseteq \mathcal{A}(U)$. For the opposite inclusion, first note that $U \subseteq \mathcal{A}(U)$ and that $\max\{f^{(i)}_1, f^{(i)}_2\}$ is compatible with $\B_d$ if and only if  $\max\{0, f^{(i)}_1 -f^{(i)}_2\}$ is. Moreover, since $f= \max\{0,f\} - \max \{0,-f\}$, it also holds that $U \subseteq A_0(U)$. Let $f = \sum_{i \in [m]} \lambda_i \max\{f^{(i)}_1, f^{(i)}_2\} \in \mathcal{A}(U)$. Then we have that $f= \sum_{i \in [m]} \lambda_i \max\{f^{(i)}_1- f^{(i)}_2,0\} +f^{(i)}_2 \in A_0(U)$, proving the claim.
\end{proof}
    \begin{proof}[Proof of \Cref{lem:indBase_maxout}]
  Follows from \Cref{lem:wlog_zero_map} and \Cref{lem:maxout_conforming_setfunc}:
    \begin{align*}
        \Phi(\mathcal{A}(U)) &\overset{\ref{lem:wlog_zero_map}}{=} \Phi(\spn \{\max\{0,f\} \mid  f \in U,\max\{0,f\} \in \FB{d} \}) 
        \\&= \spn \{\Phi(\max\{0,f\}) \mid  f \in U,\max\{0,f\} \in \FB{d} \}
        \\&\overset{\ref{lem:maxout_conforming_setfunc}}{=} \spn \{\max\{0,\Phi(f)\}) \mid  (0,\Phi(f)) \in \Phi(U)^2 \cap \HC{d}^2\}
        \\&= \spn \{\max\{0,F\}) \mid  F \in \Phi(U) \cap \HC{d}\}
        \\&= \mathcal{A}(\Phi(U))
    \end{align*} 
   \end{proof}
\begin{proof}[Proof of \Cref{lem:lattice_structure}]
 \ref{lem:lattice_decomposition} follows immediately from
	   \[
		\al{X,Y}
		= \sum_{X \subseteq Q \subseteq Y} (-1)^{r(Q)}\ind_Q
		 = \sum_{S \subseteq Y \setminus T}\sum_{S \subseteq Q \subseteq S  \cup T} (-1)^{r(Q)}\ind_Q 
		= \sum_{S \subseteq Y \setminus T}(-1)^{r(S)}\al{S,S \cup T}. \]

        For \ref{lem:induction_step},  first assume that $S=S' \cup \{a\}$, where $a \in Y \setminus (T\cup S)$. Then we have that $|(S' \cup T) \setminus S|=k+1$. Then \[[S,S \cup (T \cup \{a\})] = [S,S \cup T] \cup [S',S'\cup T]\] and hence $\al{S,S \cup (T \cup \{a\})} = \al{S,S \cup T} - \al{S',S'\cup T}$ by \ref{lem:lattice_decomposition}.  In general we have that $S \cup S' \setminus S= S' \cup S \setminus S'$ and applying the above argument iteratively proves the claim.
\end{proof}
 \begin{proof}[Proof of \Cref{lem:lowsubsets}]

    	Since $F \not\leq 0$, there is a $R \in \La$ such that $F(R) >0$. If $r(R) > k$, then it holds that $\asum{X,R}{F} = 0$ since $|R \setminus X| \geq k+1$ and $F \in \Sf{\La}(k)$. Moreover, $F(Q) \geq 0$ for all $Q \in [X,R]$, since $F \in \HC{\La}$. Hence, there is a $Q \in [X,R], Q \neq R$ such that $F(Q) > 0$. In particular, $r(Q) < r(R)$. This argument can be applied iteratively until we find a $X^+ \in \La_{\leq k}$ such that $F(X^+) >0.$ The proof for the existence of a $X^- \in \La_{\leq k}$ as well as $Y^+,Y^- \in \La_{\geq n-k}$ follows analogously.
\end{proof}
    \begin{proof}[Proof of \Cref{prop:maxout_quadratic_bound}]
We prove the statement by induction on $k$. For $k=1$, we have that $n \geq 3$ and hence the claim follows from \Cref{lem:indBase_maxout}. 



We can assume that there are $Y_1,Y_2$ such that $F(Y_1) > 0$ and $F(Y_2) > 0$. Otherwise, either $F^+=F$ or $F^+=0$ and in both cases it trivially holds that $F^+ \in \Sf{\La}(k) \subseteq \Sf{\La}(n-1)$. 

Hence, by \Cref{lem:lowsubsets}, we can find subsets of rank less or equal to $k$ that are in the positive respectively negative support of $F$. That is, there are $X^+,X^- \in \La_{\leq k}$ such that $F(X^+) > 0$ and $F(X^-) < 0$.
Let $S\coloneqq X^+ \cup X^-$ and $T \coloneqq Y \setminus S$. 
Then we have that $|S| \leq 2k$ and hence 
\[
|T| 
= n - |S|    
\geq (k^2+k+1) - 2k
= k^2-k+1
=(k-1)^2 +2k -1 -k +1
= (k-1)^2 +(k-1) +1
\]

Since $F\in \HC{\La}$ and hence $F$ cannot have opposite signs on comparabale subsets, 
we have that $F(R) = 0$ for all $R \in [S,S\cup T]$ due to $F(X^+) > 0$ and $F(X^-) < 0$.
In particular, it holds that $\asum{S,S\cup T'}{F} = 0$ for all $T' \subseteq T$ with $|T'|=k$.   

Let $S' \subseteq Y \setminus T$. We proceed by showing that the restriction of $F$ to $[S',S'\cup T]$ is in $\Sf{[S',S'\cup T]}(k-1)$. To see this, consider an arbitrary sublattice $[R, R \cup T'] \subseteq [S',S'\cup T]$ of rank $k$. Note that $R$ and $T'$ can be choosen such that $T' \subseteq T$ and $|T'|=k$. 
Hence, by \Cref{lem:lattice_structure}, 
it follows that \[0 = \asum{R, R \cup T'}{F} - \asum{S, S \cup T'}{F} = \asum{R, R \cup T'}{F}\] and therefore, $F \in \Sf{[S',S'\cup T]}(k-1)$ by definition. 
   
   
   Since the rank of the lattice $[S',S'\cup T]$ is at least $(k-1)^2 +(k-1) +1$, the induction hypothesis implies that $\asum{S',S'\cup T}{F^+}=0$ and therefore $\asum{X,Y}{F^+}=0$ by \Cref{lem:lattice_structure}.
   For an illustration of the induction step, see \Cref{fig:decomposition}.

\end{proof}
\begin{proof}[Proof of \Cref{prop:maxout_quadratic_bound2}]
        Let $F \in \Sf{\La}(k) \cap \HC{\La}$ and  $X,Y \in \La$ such that $|Y\setminus X|= k^2+k+1$. For the restriction of $F$ to $[X,Y]$, it still holds that $F \in \Sf{[X,Y]}(k) \cap \HC{[X,Y]}$. Then, by \Cref{prop:maxout_quadratic_bound}, we have that $\asum{X,Y}{F^+}=0$ and thus $F^+ \in \Sf{\La}(k^2+k)$. This concludes the proof, since $\Sf{\La}(k^2+k)$ is a vector space and therefore also every linear combination of such $F^+$ is contained in $\Sf{\La}(k^2+k)$.
\end{proof}
\begin{proof}[Proof of \Cref{thm:maxout_doubly_exponential}]
             Let the sequence $(k_\ell)_{\ell \in \N}$ be defined by $k_1 \coloneqq 2$ and $k_\ell \coloneqq k_{\ell-1}^2+k_{\ell-1}$.
             By \Cref{prop:maxout_quadratic_bound2} and \Cref{lem:indBase_maxout} it holds that $\mathcal{A}(\FB{d}(k_\ell)) \subseteq \FB{d}(k_\ell^2 +k_\ell)$ for all $\ell \in \N$.
             
             We first show that $\mtwo{\B_d}{\ell} \subseteq \FB{d}(k_\ell)$ by induction on $\ell$.
        For $\ell=1$, we have that $\maxout{\B_d}{2} = \FB{d}(2)$ , settling the induction base.
        For the induction step, assume that $\mtwo{\B_d}{\ell} \subseteq \FB{d}(k_{\ell})$. Then we have that \[\mtwo{\B_d}{\ell+1} = \mathcal{A}(\mtwo{\B_d}{\ell}) \subseteq \mathcal{A}(\FB{d}(k_\ell)) \subseteq \FB{d}(k_\ell^2 +k_\ell) =\FB{d}(k_{\ell+1})\]

        Moreover, by induction it holds that
        \[k_{\ell+1} = k_\ell^2+k_\ell
        \leq 2 \cdot k_\ell^2 \leq 2 \cdot (2^{2^\ell-1})^2 = 2 \cdot (2^{2 \cdot(2^\ell-1)}) = 2 \cdot (2^{2^{\ell+1}-2}) = 2^{2^{\ell+1}-1}, \]
        proving the statement.
\end{proof}
    \begin{proof}[Proof of \Cref{cor:loglog}]
    Any rank $r_i$-maxout layer can be computed by $\lceil \log_2 r_i \rceil$ many rank $2$-maxout layer since $\max\{f_1,\ldots,f_{r_i}\} =\max\{\max\{f_1,f_2\}\ldots,\max \{f_{r_{i-1}},f_{r_i}\}\}$. Then, a rank-$\mathbf{r}$-maxout network can be computed by a maxout network with $\sum_{i=1}^\ell \lceil \log_2 r_i \rceil \leq \ell \cdot \log_2 R$ many layers of rank $2$. Moreover, we have that $2^{2^{\ell \log_2 R}-1} = 2^{R^\ell-1}$, proving the claim.
   \end{proof}
    \begin{proof}[Proof of \Cref{lem:pairing}]
      	First, we observe that \Cref{lem:lowsubsets} implies that wlog $i \leq k$ and $j \geq n-k$.
      	Now, assuume that $\supp^+(F) \subseteq \La_i \cup \La_j$. 
      	Then, for every $R \in \La_i \cap \supp^+(F)$, there is a $Q \in \La_j \cap \supp^+(F)$ with $R \subset Q$ since $\asum{R,Y}{F}=0$ and $F \in \HC{\La}$. Note that this also implies that wlog $i$ is even and $j$ is odd.  Moreover, assume that there is a $Q' \in \La_j \cap \supp^+(F), Q' \neq Q$ such that $R \subset Q'$. Since $\La_l \cap \supp^+(F) = \emptyset$ for all $l \neq i,j$, it holds that $F(R) \geq F(Q) + F(Q') > F(Q)$. But then, if follows that $\asum{\emptyset,Q}{F} \leq F(R) - F(Q) < 0$, which is a contradiction to $F \in \Sf{\La}(k)$. Hence, for every $R \in \La_i \cap \supp^+(F)$ there is exactly one $Q \in \La_j \cap \supp^+(F)$ with $R \subset Q$. Moreover, $\asum{R,Y}{F}=0$ implies that $F(R)=F(Q)$. Therefore, it holds that 
      	\begin{align*}
      		\asum{X,Y}{F^+}&=\sum\limits_{S \in \supp^+(F)} (-1)^{r(S)}F(S)    
      		\\&= \left(\sum\limits_{R \in \supp^+(F) \cap \La_i}F(R)   -  \sum\limits_{Q \in \supp^+(F) \cap \La_j}F(Q) \right) 
      		\\&= 0.
      	\end{align*}
      	If $\supp^-(F) \subseteq \La_i \cup \La_j$, then the statement follows analogously due to the fact that \[\asum{X,Y}{F^+} = \asum{X,Y}{F} - \sum\limits_{S \in \supp^-(F)} (-1)^{r(S)}F(S).\]
   
            
\begin{lemma}
      \label{lem:case1-4}
      Let $\La = [X,Y]$ be a lattice of rank $5$ and $F \in \Sf{\La}(k) \cap \HC{\La}$. If there is a $X^+ \in \La_1 \cap \supp^+(F)$ and a $X^- \in \La_4 \cap \supp^-(F)$, then it holds that $F^+ \in \Sf{\La}(4)$
      \end{lemma}
           If there is a $X^+ \in \La_1 \cap \supp^+(F)$ and a $X^- \in \La_4 \cap \supp^-(F)$, it holds that $[X^+,Y] \cap \supp^-(F) = \emptyset$ and $[X, X^-] \cap \supp^+(F)=\emptyset$ since $F \in \HC{\La}$.
 In particular $X\not\in[X^+,Y]$ and thus $[X,Y]= [X^+,Y] \cup [X, X^-]$. Therefore, \[\asum{X,Y}{F^+} = \asum{X^+,Y}{F} = 0,\]
 since $F \in \Sf{\La}(2)$.
\end{proof}
    \begin{proposition}
\label{prop:lattice_case}
 Let $\La=[X,Y]$ be a Boolean lattice of rank $5$. If $F \in \Sf{\La}(2) \cap \HC{\La}$, then it holds that $F^+ \in \Sf{\La}(4)$.
\end{proposition}
    \begin{proof}
By \Cref{lem:pairing} and \Cref{lem:case1-4}, we can assume that there are $X^+\in \La_1 \cap \supp^+(F)$ and $X^-\in \La_1 \cap \supp^-(F)$. In this case, the proof proceeds analogously to the proof of \Cref{prop:maxout_quadratic_bound} taking advantage of the existence of $X^+$ and $X^-$ in $\La_1$.
 Let $S \coloneqq X^+ \cup X^- \in \La_2$.
   Since $F \in \HC{\La}$, it follows that $F(R)=0$ for all $R \in [S, Y]$. In particular, we have that $\asum{S,S\cup T}{F}=0$ for all $T \in Y \setminus S$ with $ |T| =2$. Hence, by \Cref{lem:lattice_structure}, it follows that $\asum{S',S'\cup T}{F}=0$ for all $S' \subseteq S$ and $T \in Y \setminus S$ with $|T| =2$ and therefore $F \in \Sf{[S', S' \cup T]}(1)$. Since $[S',S' \cup (Y\setminus S)]$ is a boolean lattice of rank $3$, by \Cref{lem:indBase_maxout}, it follows $\asum{S',S' \cup (Y\setminus R)}{F^+}=0$ and therefore $F^+ \in \Sf{\La}(4)$ due to  \Cref{lem:lattice_structure}.
\end{proof}
    \begin{proposition}
        It holds that $\mathcal{A}(\Sf{\La}(2)) \subseteq \Sf{\La}(4)$.
    \end{proposition}
              \begin{proof}
    The proof is analogously to the proof of \Cref{prop:maxout_quadratic_bound2}.
                Let $F \in \Sf{\La}(2) \cap \HC{\La}$ and  $X,Y \in \La$ such that $|Y\setminus X|= 5$. For the restriction of $F$ to $[X,Y]$, it still holds that $F \in \Sf{[X,Y]}(2) \cap \HC{[X,Y]}$. Then, by \Cref{prop:lattice_case}, we have that $\asum{X,Y}{F^+}=0$ and thus $F^+ \in \Sf{\La}(4)$. 
    \end{proof}

   \begin{proof}[Proof of \Cref{thm:maxout4d}]
   \[\mtwo{\B_d}{2} = \mathcal{A}(\FB{d}(2)) = \mathcal{A}(\Sf{d}(2)) \subseteq \Sf{d}(4) = \FB{d}(4) \]
\end{proof}
\begin{proof}[Proof of \Cref{prop:outof6space}]
 Let $F_1 = \Phi(f_1)$ and $F_2 = \Phi(f_2)$ and for every $M \subseteq [7]$, let $G_M = \Phi(\sigma_M)$, which means that \[G_M(S) = 
           \begin{cases}
               1 & M \cap S \neq \emptyset \\
               0 & M \cap S = \emptyset
           \end{cases}\] 
           We write $i_1 \cdots i_n$ for $\{i_1, \ldots, i_n\}$  and $\overline{i_1 \cdots i_n}$ for $[7] \setminus \{i_1,\ldots, i_n\}$ and note that  \begin{align*}
            F_1 &= 2 \cdot G_{12} + G_{145} + G_{167} + G_{246} + G_{257} \\
            F_2 &=  G_{345} + G_{367} + G_{124} + G_{125
        } + G_{126} + G_{127}
        \end{align*}
        Furthermore, the sublattices 
           \[[12,\overline{3}], 
           [13,\overline{2}], 
           [23,\overline{1}], 
           [3,\overline{12}], 
           [2,\overline{13}],
           [1,\overline{23}],[\emptyset,\overline{123}],[123,[7]]\] form a partition of $[\emptyset,[7]]$.

           We first show that on any of the above sublattices except $[1,\overline{23}]$, either $F_1$ or $F_2$ attains the maximum on all elements of the sublattice and that for $F \coloneqq F_1 -F_2$ it holds that \[\supp^+(F) \subseteq [1,\overline{23}] \cup 146 \cup 167\] and $\asum{[\emptyset,[7]]}{F^+} =\asum{12,\overline{3}}{F}-F(146) - F(167) = -2$ and thus $F^+ \in \Sf{\La} \setminus \Sf{\La}(6)$.

             Then by looking at the partition into sublattices, we argue that $F \in \HC{\La}$. Hence, by \Cref{lem:maxout_conforming_setfunc}, it holds that $F^+ = \max\{0,\Phi^{-1}(f_1-f_2)\} =\Phi^{-1}(\max\{0,f_1-f_2\})$ and $\max\{0,f_1-f_2\}$ is compatible with $\B_d$. Moreover, since $F^+ \in \Sf{7} \setminus \Sf{7}(6)$, it follows that $\max \{0,f_1-f_2\} \in \FB{7}\setminus\FB{7}(6)$ and therefore also $\max \{f_1,f_2\} \in \FB{7}\setminus\FB{7}(6)$.



            On the sublattice $[1,\overline{23}]$, we have that $F_1 =4+ G_{46} + G_{57}$ and $F_2 =4+ G_{45} + G_{67}$, which means that $F_1(145) > F_2(145)$, $F_1(167) > F_2(167)$, $F_2(146) > F_1(146)$ and $F_2(157) > F_1(157)$ and $F_1(S) = F_2(S)$ for all other $S \in [1,\overline{23}]$.
            
            First, note that $F_i(S) \leq 6$ for $i=1,2$ and all $S \subseteq [7]$ since there are only $6$ summands that attain the values $0$ or $1$. 
           
           On the sublattice $[12,\overline{3}]$ it holds that $F_1 = 6$  and thus $F_1(S) \geq F_2(S)$ for all $S \in [12,\overline{3}]$.
           
           On the sublattice $[13,\overline{2}]$ it holds that $F_2 = 6$  and thus $F_2(S) \geq F_1(S)$ for all $S \in [13,\overline{2}]$.
           
           On the sublattice $[23,\overline{1}]$ it holds that $F_2 = 6$  and thus $F_2(S) \geq F_1(S)$ for all $S \in [23,\overline{1}]$.

           On the sublattice $[3,\overline{12}]$ it holds that $F_1 = G_{45} + G_{67} + G_{46} + G_{57}$ and $F_2= 2 + G_{4} + G_{5} + G_{6} + G_{7}$. Since $G_{45} \leq G_{4} + G_{5}$ and $G_{67} \leq  G_{6} + G_{7}$, it follows that $F_2(S) \geq F_1(S)$ for all $S \in [3,\overline{12}]$.

         On the sublattice $[2,\overline{13}]$ it holds that $F_1 = G_{45}+G_{67}+4$ and $F_2 = G_{45}+G_{67} +4$ and thus $F_1(S) = F_2(S)$ for all $S \in [2,\overline{13}]$.

         On the sublattice $[\emptyset,\overline{123}]$, it holds that $F_1 = G_{45} + G_{67} + G_{46} + G_{57}$ and $F_2 = G_{45} + G_{67} + G_{4} + G_{5} + G_6 + G_7$. Since $G_{46} \leq G_4 + G_6$ and $G_{57} \leq G_5 + G_7$, it holds that $F_2(S) \geq F_1(S)$ for all $S \in [\emptyset,\overline{123}]$.

         On the sublattice $[123,[7]]$ it holds that $F_1 = F_2 = 6$. 

         On the sublattice $[1,\overline{23}]$, we have that $F_1 =4+ G_{46} + G_{57}$ and $F_2 =4+ G_{45} + G_{67}$, which means that $F_1(145) > F_2(145)$, $F_1(167) > F_2(167)$, $F_2(146) > F_1(146)$ and $F_2(157) > F_1(157)$ and $F_1(S) = F_2(S)$ for all other $S \in [1,\overline{23}]$.
         
         Let $F\coloneqq F_1-F_2$, then summarizing, it holds that \[F(S) 
         \begin{cases}
            \geq 0 & S \in  [12,\overline{3}] \cup 145 \cup 167 \\
             = 0  & S \in [2,\overline{13}] \cup [123,[7]] \cup [1,\overline{23}] \setminus (145\cup 146 \cup 157 \cup 167)  \\
             \leq 0 & S \in [13,\overline{2}] \cup [23,\overline{1}] \cup [3,\overline{12}] \cup [\emptyset,\overline{123}] \cup 146 \cup 157
         \end{cases}\]

         and thus $\asum{[\emptyset,[7]]}{F^+} =\asum{12,\overline{3}}{F}-F(146) - F(167) = -2$.


         It remains to prove that $F \in \HC{7}$. We prove this by showing that for any $S$ such that $F(S) >0$, there is no $T$ comparable with $S$ such that $F(T) <0$. On the sublattice $[12,\overline{3}]$ we have that $F_1=6$ and $F_2 = G_{45}+G_{67}+4$. Hence we have that $\supp^+(F) = \{12,124,125,126,127,1245,1267,145,167\}$. Also note that $F_1(S) = 6$ for all $S \in \supp^+(F)$ and thus there cannot be a $T$ with $T \supseteq S$ such that $S \in \supp^+(F)$ and $F(T) <0$ since $F_1$ is monotone. Thus it remains to check the possible subsets of $\supp^+(F)$, which are $1,14,15,16,17,2,24,25,26,27,245,267,4,5,45,6,7$ and $67$. For all but $4,5,45,6,7$ and $67$, we already know that $F$ attains the value $0$, since they are contained in $S \in [2,\overline{13}]  \cup [1,\overline{23}]$. The remaining elements are contained in $[\emptyset,\overline{123}]$ and on this sublattice we have that $F_1 = G_{45}+G_{67}+G_{46}+G_{57}$ and $F_2 = G_{45}+G_{67}+G_4 +G_5 + G_6 + G_7$ and thus $F = G_{46}+G_{57} - G_4 -G_5 - G_6 - G_7$. It now follows easily that $F(S)=0$ for all $S \in \{4,5,45,6,7,67\}$ and therefore $F \in \HC{7}$. 
\end{proof}
         \section{Conforming tuples form a polyhedral fan}
         In this section, we prove that for any boolean lattice $\La$, the set of conforming tuple $\HC{\La}^r$ is the support of a polyhdedral fan in $\Sf{\La}^r$.
For a lattice $\La$ of rank $n$ we call a map $a \colon \La \to 2^{[r]} \setminus \emptyset$  \emph{conforming} if for all chains $\emptyset =S_0 \subsetneq S_1 \subsetneq \ldots \subsetneq S_n$ it holds that $\bigcap_{i=0}^n a(S_i) \neq \emptyset$. For two such maps $a,b \colon \La \to 2^{[r]} \setminus \emptyset$, we denote $a \leq b$ if $b(S) \subseteq a(S)$ for all $S \in \La$. Furthermore, we define  the union of the maps as $a \cup b \colon \La \to 2^{[r]} \setminus \emptyset$ given by $(a\cup b)(S) = a(S) \cup b(S)$. We define the set \[C_a \coloneqq \{ (F_1,\ldots,F_r) \mid a(S) \subseteq \argmax_{i \in [r]} F_i(S) \text{ for all }S \in \La \}\]


\begin{lemma} The set $\{C_a \subseteq \Sf{\La}^r \mid a \colon \La \to 2^{[r]} \setminus \emptyset\}$ is a complete polyhedral fan. More precisely, for any $a,b \colon \La \to 2^{[r]} \setminus \emptyset$ it holds that
\begin{enumerate}
 \item $C_a$ is a polyhedral cone,
\item $C_a$ is a face of $C_b$ if and only if $a \leq b$ and
\item $C_a \cap C_b = C_{a \cup b}$
\end{enumerate}
\end{lemma}
\begin{proof}
    The condition  $a(S) \subseteq \argmax_{i \in [r]} F_i(S)$  is equivalent to $F_i(S) = F_j(S)$ for all $i,j \in a(S)$ and $F_i(S) \geq F_j(S)$ for all $i \in a(S), j \notin a(S)$. Hence, $C_a$ is the intersection of finitely many linear hyperplanes and linear halfspaces and therefore a polyhedral cone. 

    If $a \leq b$, then more of the above inequalities are tight and hence $C_a$ is a face of $C_b$.

    \begin{align*}
        C_a \cap C_b &= \{ (F_1,\ldots,F_r) \mid a(S),b(S) \subseteq \argmax_{i \in [r]} F_i(S) \text{ for all }S \in \La \}
        \\&=  \{ (F_1,\ldots,F_r) \mid a(S) \cup b(S) \subseteq \argmax_{i \in [r]} F_i(S) \text{ for all }S \in \La \}
        \\&=C_{a \cup b}
    \end{align*}


    

    
\end{proof}
\begin{lemma}
    The set $\Sigma_{\La,r} \coloneqq \{C_a \subseteq \Sf{\La}^r \mid a\colon \La \to 2^{[r]} \setminus \emptyset $ conforming $\}$ is a sub fan of $\{C_a \subseteq \Sf{\La}^r \mid a \colon \La \to 2^{[r]} \setminus \emptyset \}$.  
\end{lemma}
\begin{proof}
    Follows from the fact that if $b \colon \La \to 2^{[r]} \setminus \emptyset$ is conforming and $a \leq b$, then also $a$ is conforming.
\end{proof}
\end{document}

