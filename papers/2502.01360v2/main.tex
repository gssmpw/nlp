
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tikz-cd}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{tikz-cd}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Neural Networks as Gluing Machines, Relative Homology through the Overlap Decomposition}

\begin{document}

\twocolumn[
\icmltitle{A Relative Homology Theory of Representation in Neural Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kosio Beshkov}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Biosciences, University of Oslo, Oslo, Norway}

\icmlcorrespondingauthor{Kosio Beshkov}{kosio.neuro@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}
%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\begin{abstract}
Previous research has proven that the set of maps implemented by neural networks with a ReLU activation function is identical to the set of piecewise linear continuous maps. Furthermore, such networks induce a hyperplane arrangement splitting the input domain into convex polyhedra $G_J$ over which the network $\Phi$ operates in an affine manner. 

In this work, we leverage these properties to define the equivalence class of inputs $\sim_\Phi$, which can be split into two sets related to the local rank of $\Phi_J$ and the intersections $\cap \text{Im}\Phi_{J_i}$. We refer to the latter as the \textit{overlap decomposition} $\mathcal{O}_\Phi$ and prove that if the intersections between each polyhedron and the input manifold are convex, the homology groups of neural representations are isomorphic to relative homology groups $H_k(\Phi(\mathcal{M})) \simeq H_k(\mathcal{M},\mathcal{O}_\Phi)$. This lets us compute Betti numbers without the choice of an external metric. We develop methods to numerically compute the overlap decomposition through linear programming and a union-find algorithm.

Using this framework, we perform several experiments on toy datasets showing that, compared to standard persistent homology, our relative homology-based computation of Betti numbers tracks purely topological rather than geometric features. Finally, we study the evolution of the overlap decomposition during training on various classification problems while varying network width and depth and discuss some shortcomings of our method.
\end{abstract}

\section{Introduction}
\label{Introduction}

%Finding the topology of neural representations is important
%Polyhedral decompositions are important
%We combine them into one 
%
Deep learning has proven to be an incredibly powerful framework for learning and generalizing on highly complex tasks. Despite its widespread success, our theoretical understanding of this framework is still heavily lacking. What we do know is that deep neural networks are universal approximators \cite{hornik1991approximation} and therefore have unbounded expressive power. If we further restrict ourselves to neural networks with ReLU activation functions, we also know that they are exactly equal to the set of continuous piecewise-linear (CPWL) functions \cite{arora2016understanding}. 

A CPWL function applies a different affine function over different pieces of its domain. Naturally, this phenomenon is also observed in neural networks, where the structure of the weights splits the domain into different polyhedra called \textit{linear regions} \cite{pascanu2013number, montufar2014number}. Quantifying the number \cite{arora2016understanding, raghu2017expressive, serra2018bounding, hanin2019deep} as well as other properties \cite{fan2023deep} of the linear regions that a network generates is thought to reflect the expressivity of a neural network and is a rich subfield in its own right.

Another avenue of research that has shown a lot of promise in many fields of science is Topological Data Analysis (TDA), which attempts to quantify the topological features of data \cite{carlsson2009topology, wasserman2018topological}. In the context of deep learning, it has been used to relate the topology of training trajectories \cite{birdal2021intrinsic, dupuis2023generalization} and network weights \cite{rieck2018neural, gutierrez2021persistent, andreeva2023metric} to the generalization error of a neural network. In addition, while counting linear regions is one measure of expressivity, another way to quantify it is to look at the topology of the decision boundary of a neural network \cite{guss2018characterizing, petri2020topological, grigsby2022transversality} or the degree to which a network can change the topology of the input domain as it is propagated through its layers \cite{naitzat2020topology, wheeler2021activation}.

While the examples above are only a small subset of the plethora of novel ideas for how to apply topological tools to study neural networks, many, if not most, such ideas are somehow based on persistent homology \cite{zomorodian2004computing}. Despite the power of this method, it is known that it is sensitive to outliers, undersampling, and highly non-linear mappings. Furthermore, it is based on calculating distances between points and therefore requires the choice of a metric. This choice leads to the identification of not only topological features, but also geometric ones like curvature \cite{bubenik2020persistent} and convexity \cite{turkes2022effectiveness}. This might be a benefit in some cases, but when reasoning solely about topological features, we would like to avoid contamination from geometric sources.

These issues are especially relevant in the context of deep neural networks as they apply highly non-linear transformations of the input domain that increase curvature \cite{poole2016exponential} in the output space, making their analysis incredibly difficult. To avoid this, we study topological features in the input space through a purely topological lens. We do this using relative homology \cite{hatcher2005algebraic}, which except for a few works \cite{pokorny2016topological, blaser2022relative, beshkov2024rank}, has received surprisingly little attention in the deep learning literature.

\subsection{Our contributions}
Instead of having to deal with an external space, induced by a map, in which the choice of a metric is ambiguous, we can imagine the following. Take a dataset, determine which pieces are glued to each other by the map, and then count the holes in the manifold after this gluing. Under some simple assumptions, these three steps can be rigorously identified with a manifold $\mathcal{M}$, a quotient space $\sim_\Phi$ and the relative homology groups $H_k(\mathcal{M},\sim_\Phi)$ respectively (see Appendix \ref{Prerequisites} for more background on these terms). This procedure avoids the leakage of geometric information into our estimates of homology since it only depends on the quotient space $\sim_\Phi$. It does, however, require knowledge or at least a good estimate of the topology of the initial manifold $\mathcal{M}$ and a robust way to identify the quotient space $\sim_\Phi$.

In this work, we show how these steps can be carried out in the context of ReLU neural networks, further developing previous work relating polyhedral decompositions and homology \cite{liu2023relu}. There are two difficulties with making such an approach rigorous. Firstly, how can we actually calculate the quotient space $\sim_\Phi$? Secondly, given this quotient space, how and when can we calculate the relative homology groups $H_k(\mathcal{M},\sim_\Phi)$?

Our fundamental realization is that since ReLU neural networks split the input domain into convex polyhedra, we can separate the sources of gluing (also called non-injectivity) into the local rank of the map at each polyhedron and the non-trivial intersections between the images of different polyhedra. We refer to these as the rank and the overlap source, respectively, and show that the latter can be computed either approximately, using a distance threshold, or exactly, using feasibility checks through linear programming. Afterwards, we prove under which conditions the overlap decomposition is sufficient to calculate relative homology groups.

We then compare the performance of persistent and relative homology when it comes to determining the Betti numbers of toy datasets with a known topology. Our approach also gives a new perspective on how topological information propagates through the layers of a neural network. Using relative homology, we observe that topological simplification happens much more gradually than observed by previous persistent homology-based calculations \cite{naitzat2020topology}. Finally, we study the properties of the overlap decomposition in randomly initialized and trained neural networks and show that the volume of overlapping regions decreases after training, whereas the number of overlap regions tends to increase.

\section{Decompositions of Neural Networks} 
\label{decompositions}

Before diving deeper into the mathematics, let us first fix our terminology. We will assume that a dataset $\mathcal{D} = \{x_1,x_2,...,x_K\}$ is sampled from some compact manifold $\mathcal{M}$, with or without boundary, embedded in $\mathbb{R}^{n_0}$. A neural network is a function $\Phi: \mathbb{R}^{n_0} \to \mathbb{R}^{n_1} ... \to \mathbb{R}^{n_L}$, where $n_l$ is the $l$-th layer of the network. Between each pair of layers, we have \textit{preactivations} $T_l:\mathbb{R}^{n_{l-1}} \to \mathbb{R}^{n_l}$ given by the affine functions $T_l(z) = Wz+b$ and \textit{representations} given by $S_l(z) = \text{ReLU}(T_l(z))$. We will denote the output at layer $l$ by $\Phi^l:\mathbb{R}^{n_0} \to \mathbb{R}^{n_l}$ which is given by function composition $\Phi^l = S_l \circ S_{l-1} \circ ... \circ S_1 = \prod\limits_{k=1}\limits^{l} S_k$. We will use the convention that the last layer does not apply an activation function so $\Phi^L = T_L \circ \prod\limits_{k=1}\limits^{L-1} S_k$.

\subsection{ReLU networks and linear regions}
It is well known in the literature that every ReLU neural network is equivalent to a continuous piecewise-linear function and vice versa \cite{arora2016understanding} and that such a network decomposes input space into convex polyhedra \cite{montufar2014number, balestriero2019geometry, zhang2020empirical, liu2023relu}. This happens as a result of the fact that the affine functions $T_l$ can be interpreted as specifying conditions $CT_l$, where $C$ is some diagonal matrix with entries in $\{-1,1\}$ defining a polyhedron $P = \{x | CT_l(x) \leq 0 \}$. Each subsequent layer further decomposes the input space into more polyhedra, since it adds more conditions to the previous decomposition. 

It is also known that within these convex polyhedra, the neural network applies an affine map. For any input $x_k$ in a dataset, we can look at the neurons that get activated by it and write down the \textit{codeword vector} $c_l(x) = \text{sign}(\Phi^l(x))$ and the \textit{precodeword vector} $p_l(x) = \text{sign}(T_l \circ \Phi^{l-1}(x))$ at a layer $l$. Since this is defined at a single layer, we will call it a \textit{local layer codeword}. There are many inputs that activate the same neurons within a layer, so a single codeword corresponds to a whole region of input space. This leads to our first decomposition.

\begin{definition}
    For a codeword $J  \in \{0,1\}^{n_l}$ and a supporting set $R$ of input space under consideration (for example the dataset $\mathcal{D}$), there is an associated codeword set supported on $R$,
    \begin{equation}
        L^l_J|_R = \{x | c_l(x)=J, x \in R \subset \mathbb{R}^{n_0}\}.
    \end{equation}
    Then the \textit{local layer decomposition} supported on $R$ is defined by the set going over all possible $J$'s and $\mathcal{L}^l|_R = \{L^l_J|_R\}$.
\end{definition}

While the codewords within the same region of the local layer decomposition are constant, that does not mean that the network applies the same affine function over the region (see \ref{proof:loc_affine} in the Appendix). However, this property does hold in a different decomposition called the polyhedral decomposition \cite{liu2023relu}. Instead of looking at the codeword at layer $l$ we can stack together all codeword vectors from the previous layers $C_l(x) = [c_1(x),c_2(x),...,c_l(x)]$ forming a new \textit{global codeword vector}. This leads us to the next decomposition,

\begin{definition}
    For a codeword $J \in \{0,1\}^{\sum\limits_{k=1}\limits^{l} n_k}$  and a supporting set $R$ of input space under consideration, there is an associated codeword set supported on $R$,
    \begin{equation}
        G^l_J|_R = \{x | C_l(x)=J, x \in R \subset \mathbb{R}^{n_0}\}.
    \end{equation}
    Then the \textit{polyhedral decomposition} supported on $R$ is defined by the set going over all possible $J$'s and $\mathcal{G}^l|_R = \{G^l_J|_R\}$.
\end{definition}

As already mentioned, on each polyhedron $G^l_J$ the neural network applies an affine map $\Phi^l|_{G^l_J}: G^l_J \to \mathbb{R}^{n_l}$, which can be written explicitly as,
\begin{equation}
    \label{eq: Phi rank}
    \Phi^l|_{G^l_J} (\cdot) = (\prod\limits_{k=1}\limits^{l} Q_{J_k}W_k) (\cdot)+\sum\limits_{i=1}\limits^{l} \prod\limits_{j=i+1}\limits^{l} (Q_{J_j}W_j) Q_{J_i}b_i,
\end{equation}

where $Q_{J_k} = \text{diag}(J_k)$ and $J_k$ is given by the codeword on the polyhedra. From now on we shall simplify our notation by writing this map as $\Phi^l_J$.

\subsection{The overlap decomposition}
The polyhedral decomposition is widely known in the literature, here we define a new type of decomposition that can be leveraged to compute the homology groups of a neural network, which we will call the \textit{overlap decomposition}. Our approach is intuitively justified by the realization that if neural networks are equivalent to continuous piecewise-linear functions, then the only way that the topology of the input space can change is through non-injective transformations. Since the network operates piecewise, we can have non-injectivity from two sources; see Appendix \ref{Theorem: two sources} for a proof:

\begin{enumerate}
    \item The map $\Phi^l|_{G^l_J}$ is low-rank and projects the region $G^l_J$ to a lower-dimensional subspace. We will call this the \textit{rank source}.
    \item For a set of maps $\{\Phi^l|_{G^l_{J_1}},\Phi^l|_{G^l_{J_2}},...,\Phi^l|_{G^l_{J_N}}\}$ on different polyhedra, there is a non-trivial intersection of the images over their respective polyhedra $\bigcap\limits_{n} \text{Im}\Phi^l|_{G^l_{J_n}} \neq \emptyset$. We will call this the \textit{intersection source}.
\end{enumerate}

The first condition has been described in previous work \cite{beshkov2024rank}, but as we will see later, it can be ignored when the intersections between the data manifold and the regions $G^l_J$ are convex. Therefore, we will focus on defining the overlap decomposition based on the second condition, with the agreement that one should be careful when this condition fails to hold. With this in mind, let us define the overlap decomposition.

\begin{definition}
    \label{Overlap definition}
    A neural network $\Phi^l$ has an the equivalence class which describes all  non-injective regions of a network,
    \begin{equation}
        \sim_{\Phi^l} = \{[x] | \Phi^l(x)=\Phi^l(y)\}.
    \end{equation}
    The \textit{overlap decomposition} describes the part of the equivalence class coming from the intersection source and is given by,
    \begin{equation}
    \begin{split}
        & \mathcal{O}_{\Phi^l} = \{[x] | \Phi(y) \in \bigcap\limits_{i \in I} \Phi(G^l_{J_i}) \text{ for }\\
        &   I \subset \{1,2,...,\text{\# polyhedra}\} \text{ and } \Phi(y) \not\in \bigcap\limits_{i \in K} \Phi(G^l_{J_i})  \\
         & \text{ for any }  I \subsetneq K, \Phi(x)=\Phi(y), |I|>1 \}.
    \end{split}
    \end{equation}
\end{definition}

This decomposition can be fully computed given that we have knowledge of the polyhedral decomposition $\mathcal{G}^l$, by identifying the intersections between all pairs of different polyhedra and using a union-find structure on top \cite{kleinberg2006algorithm}. In the next section, we discuss how to approximate or exactly compute this decomposition for a neural network and a supporting dataset. We also discuss how it can be leveraged to compute the homology groups of a neural representation.



\section{Relating the Overlap Decomposition and Homology} % Show how homology can be calculated through quotiening out the overlap decomposition
\label{Relative Homology}


\subsection{Numerical Determination of the Overlap Decomposition}
We have stated that the overlap decomposition is a subset of the equivalence class $\sim_\Phi = \{[x] | \Phi(x)=\Phi(y)\}$. However, we have yet to show how to actually compute this set when working with finite data samples. Here we propose two methods of increasing precision which can be used to find the overlap decomposition. This comes at the cost of an increase in their algorithmic complexity.

\subsubsection{Distance threshold}
Even if parts of the underlying data manifold are glued together, when working with finite samples, it is highly unlikely that we will ever observe actual perfect equality between points. The distance threshold approach tries to avoid this issue by simply replacing the strict equivalence condition by $\leq_{\epsilon \Phi} = \{[x] | d(\Phi(x),\Phi(y))\leq \epsilon\}$. Pseudocode for this algorithm is shown in \ref{alg:d_thresh}.


\begin{algorithm}
    \caption{Distance threshold}
    \label{alg:d_thresh}
\begin{algorithmic}
    \STATE {\bfseries Input:} The output of a model $\Phi_l(x)$ and a threshold $\epsilon$
        \STATE $D \gets d(\Phi_l(x),\Phi_l(y))$ Compute  distances
        \STATE $\mathcal{O} \gets \arg [D \leq \epsilon]$ Identify close pairs
        \STATE $\mathcal{O} \gets UF(\mathcal{O})$ Union-Find to get partially overlapping pairs
    \STATE \textit{Return} $\mathcal{O}$
\end{algorithmic}
\end{algorithm}

This method is still highly data-dependent and non-exact. Since it is evaluated in the output space it carries all of the problems that come with sparse data sampling and highly non-linear maps. Its advantage is that it is easy to understand and has a low computational cost.

\subsubsection{Linear programming}
As we will show later, under the assumption that the intersection between a polyhedron from the polyhedral decomposition and the data manifold $P\cap \mathcal{M}$ is a convex set (this is automatically true if $\mathcal{M}$ is convex), we can use the polyhedral decomposition to exactly determine the overlaps between data samples. For two populated polyhedra, meaning there exist points in the dataset $\mathcal{D}$ that live in each of these polyhedra, $P_1, P_2 \in \mathcal{G}^l$ the overlaps $\Phi_l(P_1) \cap \Phi_l(P_2)$ can be determined through linear programming. This is done by checking if given a point $p \in P_1$ there is another point $p' \in P_2$ such that $\Phi_l(p)=\Phi_l(p')$. This second point $p'$ can be found through linear programming under the constraints given by the H-representation of $P_1$ and $P_2$. Thus, to check whether there is a point in $P_2$ that overlaps with $p \in P_1$, we get the following feasibility linear program,


\begin{align}
\begin{split}
    & \min\limits_{x}(0^Tx),\\
    & \text{Such that } A_2x \leq b_2, \\
    & \Phi^l_{J_2}(x) = \Phi^l_{J_1}(p).    
\end{split}
\end{align}

It is important to note that this approach looks for points within a polyhedron defined by the weights of a network, but this point does not need to be in the dataset $\mathcal{D}$. Therefore, this approach implicitly assumes that any point within $G^l_{J_2} \cap \mathbb{R}^{n_0}$ is a valid data point that \textit{could have existed} in the dataset. The pseudocode for this is shown in Algorithm \ref{alg:linprog}.

\begin{algorithm}
    \caption{Linear programming}
    \label{alg:linprog}
\begin{algorithmic}
    \STATE {\bfseries Input:} Points $X$ from $\mathcal{G}_l$
        \FOR{$P_i \neq P_j \in \mathcal{G}_l$}
            \STATE $\{A_i,b_i\},  \{A_j, b_j\}$ \text{ H-representations of } $P_i$ \text{ and } $P_j$
            \FOR{$\forall y \in P_i \text{ and } \forall z \in P_j$}
                \STATE $C \gets \{A_jx \leq b_j\} \text{ and } \{\Phi^l_{J_j}(x)= \Phi^l_{J_i}(y)\}$ Add the constraints
                \STATE LP $\gets \min\limits_{x}(0^Tx)|C$ Solve the linear program
                    \IF {LP is not empty}
                        \STATE $B_y \gets y$ \text{ or } $B_z \gets z$
                    \ENDIF
                \ENDFOR
            \STATE $\mathcal{O} \gets B_y\times B_z$ Add overlapping pairs
            \ENDFOR
        \STATE $\mathcal{O} \gets UF(\mathcal{O})$ Union-Find to get partially overlapping pairs
    \STATE \textit{Return} $\mathcal{O}$
\end{algorithmic}
\end{algorithm}


Looking at the pseudocode, it is worth noting that if either $B_y$ or $B_z$ are empty, then the Cartesian product is the empty set and if one of the polyhedra is not sampled well, we could potentially miss some overlaps. A workaround to this is to add the points that the non-empty set overlaps with, but this would lead to a larger dataset and therefore overcomplicate our analysis. We further examine the problems that can arise as a result of this in the discussion.

The advantage of this method is that it only identifies an overlap if there are two points on which the neural network generates the same output. In this sense, is an exact determination of the overlap decomposition (up to errors due to machine precision) and avoids all issues that come with choosing an external metric. The drawback is that it requires the determination of the H-representation of many polyhedra and for high-dimensional input spaces it is prohibitively expensive to represent all of them. This could be improved through the use of other polyhedral representations that scale much better \cite{kochdumper2019representation, sigl2023m}. A way to achieve a minor speed up is to precompute distances and skip looking for overlaps between points that are $\delta$ away from each other, in this work we always choose $\delta=1$, except for section \ref{naitz} where we set $\delta=10$ for a slightly more conservative estimate.


\subsection{Relative Homology through the Overlap Decomposition}

Previously we stated that the rank and the overlap conditions form an exhaustive set of sources of topological change that a network can induce on the input manifold. This statement is formulated as a theorem and proven in Appendix \ref{Theorem: two sources}.

\begin{theorem} %move to appendix with proof
\label{Homology decomp theorem}
    (informal) The homology groups $H_i(\Phi(\mathcal{M}))$ can be fully determined by knowing the rank and the overlap decompositions.
\end{theorem}


This is already an encouraging result, but we have only described how to calculate the overlap decomposition. We have yet to describe a method to determine the rank decomposition. One suggestion is that we can treat regions in $\mathcal{R}_\Phi$ as contractible as in \cite{beshkov2024rank}. As stated in that work, this is effective for regions of rank zero and one but can fail to produce an accurate result for regions of higher rank. Fortunately, it turns out that if the intersections $\mathcal{M} \cap G^l_J$ are convex, then homology is invariant to the presence of low-rank maps and the overlap decomposition is sufficient to compute relative homology, see \ref{Convex homology proof} for a proof.

\begin{theorem}
    \label{Convex homology theorem}
    Given a neural network $\Phi$ with a polyhedral decomposition $\mathcal{G}^l_J$ such that $\mathcal{M} \cap G^l_J$ is convex for any $G^l_J \in \mathcal{G}^l_J$, the homology groups $H_k(\Phi(\mathcal{M})) \simeq H_k(\mathcal{M}, \mathcal{O}_\Phi)$.
\end{theorem}


One might object that this convexity condition does not usually obtain. However, we note that given the highly overparameterized nature of neural networks and the increasing number of polyhedra within a given volume \cite{hanin2019deep} this condition is likely to obtain. It has also turned out to be empirically true in the simulations performed in this paper. Additionally, we can often choose to work with a convex input manifold, in which case this condition always obtains (intersections of convex sets are convex). As seen in \cite{beshkov2024rank} the cases in which this condition will likely not be satisfied is if one is working with highly non-convex, high-dimensional input domains and insufficiently wide networks.

\section{Results}
\subsection{Comparison between Overlap and Persistent Homology} %Show that persistent homology fails on data from uneven density and on highly non-linear maps from neural nets

Persistent homology is an essential tool for computing homology groups of data from many different domains of science \cite{carlsson2009topology, wasserman2018topological}, including machine learning \cite{papamarkou2024position}. For better or worse, it is not a purely topological method as it also tracks geometric features of the underlying point cloud \cite{bubenik2020persistent, turkes2022effectiveness}. In addition, it is highly sensitive to the sampling distribution and the chosen metric, and while there are ways to overcome these problems in specific situations, it is likely that geometric properties are sneaking into the discussion of topology in many works.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{knot_manifolds.png}
    \caption{Visualization of our method, the top row shows two example non-linear curves, while the bottom shows their estimated homology groups through persistent homology (green) and relative homology (black). Points that are glued through the overlap decomposition are given the same color and can be seen in the output space (top row) and the input space (the gray lines in the bottom row). As highlighted by the blue boxes in the second example, our method identifies the points that are glued by the neural network.}
    \label{fig:curves}
\end{figure}

To show the effectiveness of our method and compare it to persistent homology, we generated datasets of highly non-linear curves by the equation $f(\theta) = [\cos(a\theta)\cos(b\theta), \cos(a\theta)\sin(b\theta)]^T$. The parameters $a, b$ were randomly sampled from a uniform distribution on the interval $[-1,1]$. We sampled 500 equally spaced points $\theta \in [-\pi,\pi]$. Afterwards, we used a mean squared error function to optimize a neural network with three layers, each with a width of 50 neurons, to predict these functions given the input points $(\theta,0)$. All other parameters for all simulations are described in detail in Appendix \ref{Sim details}.

Following training we compute the first homology groups $H_1$ of the output of the final network layer. We do this in two ways: standard persistent homology with the Euclidean metric and relative homology using a quotient metric as described in Appendix \ref{Relative Homology on Data}.

In Figure \ref{fig:curves} we show two examples of learned curves and their associated homology groups given by the barcodes below. One can see that in the first example, standard persistent homology identifies a circular feature as the ends of the interval are close in the output despite the fact that the network does not map them onto each other. This error is avoided when using relative homology, as seen by the lack of bars in the barcode plot. In the second example, we see that both persistent homology as well as relative homology generate two persistent features. However, given that we have access to the overlap decomposition, we are also able to track exactly which points were glued together by the network and were therefore responsible for the generation of the two circles. Many other examples are shown in Appendix \ref{fig:supp figure many knots}.

\subsection{Revisiting manifold propagation through neural networks} %Generate Betti curves and compare them to those found in Naitzat 2020
\label{naitz}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{hom_propagation.png}
    \caption{Reproduced Betti curves from Naitzat et al. (green) and a relative homology calculation (black). In all three datasets we see a slower decay of all topological features when relative homology is used. The three plots in the botton right show distributions of the ranks generated by a neural network, indicating that the rank decomposition is unlikely to play a role in the estimated curves.}
    \label{fig:hom propagation}
\end{figure*}

Previous work by Naitzat et al. \cite{naitzat2020topology} has studied how the homology groups of different toy datasets evolve through the layers of an almost perfectly trained neural network. In order to estimate the distances between points, they generate a k-nearest neighbor graph, with k being optimized over the input data, and apply a shortest path algorithm on top of this graph. Following this, they calculate the homology groups at a particular scale for which they are stable and take that as an estimate of the homology groups of the neural representation at a layer.


They observe that the initial layers of a network rapidly reduce the betti number of the input manifold in all three datasets. Given that persistent homology also provides geometric, rather than purely topological information \cite{bubenik2020persistent}, we check whether our approach, which is only concerned with topological information, agrees with their results. Since working with the amount of samples used in the datasets generated by Naitzat et al. proved to be too computationally heavy for the computation of the polyhedral and overlap decompositions, we sampled 7800, 7500, and 8000 points for the three datasets, respectively and reran all parts of their analysis (see Appendix \ref{Sim details} for the full details of our implementation). Despite using smaller datasets, we managed to reproduce the quickly decaying Betti curves observed in their work.


As can be seen in Figure \ref{fig:hom propagation}, our method based on relative homology shows a very different behavior than the reproduced curves (one might also compare them to the original curves in their paper for a similar conclusion). Thus, our analysis shows that if we only consider purely topological transformations, then it seems that while neural networks might initially twist the manifolds to look like they have lost topological features, actual changes in the topology of such manifolds happen much more gradually. This naturally leads one to wonder whether topological changes in the strict sense of non-homeomorphism or rather strong geometric changes are more important for network performance. We focus more on this question in the discussion.

One might object that the datasets in Naitzat et al. are not convex, and therefore we cannot apply Theorem \ref{Convex homology theorem}, as then we would miss changes in topology induced by the rank source. To check if this was in fact an issue, we computed the rank of the maps over each polyhedron in the polyhedral decomposition for all models and datasets. The results can be seen in the bottom right plot of Figure \ref{fig:hom propagation} and show that regions of lower rank appear very rarely and are unlikely to have a severe impact on the curves that we estimate.
\subsection{Overlap decompositions in random and trained networks} 

\label{overlap trained random}
So far we have shown that by using a relative homology approach we can study the topology of neural representation in almost perfectly trained neural networks by only knowing the input space and the overlap decomposition. This has helped us separate the topological from the geometric features of a neural representation. Assuming that a network manages to learn a function almost perfectly, we would expect that the topology it generates is going to be similar to the topology that is induced by the function being learned. Furthermore, classification problems inherently have the property of simplifying the topology of the initial data (a large part of the data is sent to a single point representative of a particular class) \cite{papyan2020prevalence,rangamani2023feature}.

However, even before any learning occurs, we would like to have a good initialization of the network. Thus, it is important to understand the differences in the overlap decomposition of a neural network before and after training. The size of the overlap decomposition at initialization should reflect the ability of a network to glue different pieces of the input. Therefore, more overlap regions correspond to a measure of expressivity reflecting the degree to which a network can implement non-injective functions.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{init_trained_decomps.png}
    \caption{ \textbf{(Top row)} Visualization of the polyhedral decomposition at initialization (magenta) and after training (cyan) across layers. In the last layer we see the points in the overlap decomposition and note that some of them overlap with each other both before and after training. \textbf{(Bottom row)} (left) Polyhedron volume decreases across layers and dimensions before (circles) and after (stars) training. (center) Overlap volume decreases after training. Stars indicate Bonferroni corrected significance using a Kruskal-Wallis test \cite{kruskal1952use}. (right) The number of overlap regions increases after training.}
    \label{fig:overlap regions}
\end{figure*}


To do this, we create another toy problem in which we can compare overlap regions before and after training. We take four d-spheres with radii \{1, 1.5, 2, 2.5\} respectively and assign a class of 0 to the spheres with radius \{1, 2\} and a class of 1 to the other two. We do this for spheres of dimension \{1, 2, 3\} by sampling points i.i.d. from a normal distribution in $d+1$ and afterwards projecting them to the sphere of the desired radius through normalization. We sample 500 points from each such sphere and train 10 Kaiming-initialized \cite{he2015delving} networks for each dimension. Each network has an architecture of \{d+1, 25, 25, 25, 25, 2\} and is optimized for 1000 epochs with a learning rate of 2e-5.

Before and after training, we computed the polyhedral and overlap decompositions through linear programming. A plot of the regions generated by one network at each layer, before (magenta) and after (cyan) training, is shown in Figure \ref{fig:overlap regions}. As one can see in this toy problem, points fall in the overlap decomposition only in the last layer of the network. Another impression that this plot makes is that the polyhedra in the trained network seem to have a smaller volume. The plots below confirm that this is the case both for those that participate in the overlap decomposition as well as those that do not.

Finally, since classification problems are likely more injective than an arbitrary function, one might expect that the number of overlap regions increases as a result of training. Looking at the final plot in Figure \ref{fig:overlap regions}, this does seem to be the case, although this trend is only significant for the datasets using two- and three-dimensional spheres. We also achieve similar results when using an orthogonal initialization; see Figure \ref{fig:orthogonal} in the Appendix.


\section{Discussion}
While deep neural networks have proved to be extremely powerful and flexible, our theoretical understanding of their inner workings is still lacking. In this work we have related them to piecewise-linear functions over a set of convex polyhedra and described the areas of the input domain on which they act non-injectively. We further applied this description to define a relative homology theory, which we then used to compute homology groups that are sensitive purely to topological rather than geometric features. We presented several toy problems on which our method managed to avoid several pitfalls that plague persistent homology. However, every method has flaws, so it is worthwhile to discuss some situations in which our approach will fail.


\subsection{Homological type 1 and type 2 errors}

While in a perfect world we would like to identify all polyhedra that a network generates, due to combinatorial explosion, it is not feasible to do this for large networks. One way to do this would be to list all possible codewords that could specify a polyhedron in $\mathcal{G}^l$ and then find their H-representation. This means that we need to identify $2^{\sum\limits^{l}\limits_{i=0}n_i}$ polyhedra for each layer. While more efficient approaches are certainly possible, this exponential scaling is highly limiting. For this reason, we have focused on only considering polyhedra that are populated by at least one point in a dataset. This means that for a populated polyhedron $p \in P_i$ where the network implements a map $\Phi|_{P_i}$, there might be another unpopulated polyhedron $P'$ such that $\Phi|_{P_i}(p) = \Phi|_{P'}(z)$, where $z \in P'$. This would be a gluing that the network performs on the input space that is missed due to the limited dataset and is reminiscent of a homological type 2 error.

Another issue arises from the fact that if we think of neural networks as functions between vector spaces $f:\mathbb{R}^{n_0} \to \mathbb{R}^{n_L}$, then the polyhedral decomposition they define exists over all of input space regardless of the actual manifold that a dataset comes from. Since we tend to embed input data in these large vector spaces, there might be a point $z \notin \mathcal{M}$ within one of the populated polyhedra, such that $\Phi|_{P_i}(p) = \Phi|_{P_j}(z)$. This would imply that the network applies a gluing when it does not do that on the manifold $\mathcal{M}$ from which the data is sampled. This is reminiscent of a homological type 1 error.

\subsection{Topological versus geometric transformations}


We showed that previous calculations of Betti curves in neural networks have likely described geometric rather than topological features and that topological changes occur much more gradually in neural representations across the layers of a network. This naturally raises the question of which type of transformation we should focus our efforts on understanding further. As argued in \cite{petri2020topological}, from the perspective of a linear classifier, the topology of the two classes is irrelevant as long as they are linearly separable in the final layer. This supports the view that when it comes to topology, only that of the decision boundary carries significance.

It is important to note that, while this purely geometric view makes intuitive sense, it fails to capture the behavior of actual networks. A well-known phenomenon in deep networks trained with cross-entropy on classification tasks is neural collapse \cite{papyan2020prevalence, rangamani2023feature}, in which samples from different classes collapse to the class mean. This is an example of topological simplification which occurs despite it not being strictly necessary from the perspective of a classifier.

Furthermore, not all problems that we are interested in are classification problems. Certain tasks will require the implementation of topological changes (a trivial example is learning a constant function, which requires gluing all data points to one point), and in such cases, it is important to be able to describe them without interference from geometric sources. We are hopeful that the tools introduced in this paper help us get closer to this ability.


\subsection{Number of overlaps as a measurement of expressivity}
%Expressivity in terms of the overlap decomposition

Expressivity is a measure describing the complexity of the functions that a network architecture can implement and is often measured through the number of linear regions. Here we have argued that a related measure is the number of regions in the overlap decomposition. This notion of expressivity more closely describes the degree to which a network can implement non-injective maps.

We have not thoroughly evaluated overlap expressivity in this work. However, a simple counting of the number of overlap regions at initialization shows that compared to the number of polyhedra, they seem to scale in a non-strictly monotonic fashion; see Figure \ref{fig:overlap expressivity} in the Appendix. This is in contrast to the number of polyhedra, which keeps increasing as width and depth are increased.

In principle, it is possible that certain network architectures might have many linear regions but fail to be able to implement highly non-injective maps. The features that determine whether a network has the ability to generate such regions are still unknown. Understanding whether having more linear regions directly corresponds to the capacity of implementing more non-injective functions is an open question that will hopefully lead to a better understanding of neural networks and their inner workings.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

\section*{Code Availability}
All code is openly available and deposited at https://github.com/KBeshkov/network-relative-homology. The authors welcome any further questions regarding the reproducibility of this work.

\bibliography{main}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn
\section{Prerequisites}
\label{Prerequisites}

Here we provide some prerequisite knowledge which will hopefully come in handy to readers unfamiliar with some of the concepts that we have used.

\subsection{Polyhedral geometry}
The atomic elements of this work are convex polyhedra. There are two popular ways to represent such objects. The first is called the H-representation and is given by the intersections of half-spaces. The second is called the V-representation and is given by the convex hull of a set of points. Here we will work purely with the H-representation, although previous work has also used V-representations \cite{masden2022algorithmic}.

\begin{definition}
    A half-space is the set given by a linear equation,
    \begin{equation*}
        H_- = \{x | a^Tx \leq b\}.
    \end{equation*}
    In the H-representation, a polyhedron is described as the intersection of finitely many half-spaces. Then, a system of linear equations specified by a matrix $A$ and a vector $b$ form the H-representation of a polyhedron,
    \begin{equation*}
        P = \{x | Ax \leq b\}.
    \end{equation*}
\end{definition}

It is worth noting that if we replace the inequality in the half-space definition with a strict equality, we get hyperplanes defined by the equations $a_i^Tx=b$, where $a_i$ is the i-th row of the matrix $A$. Such hyperplanes are called the \textbf{supporting hyperplanes} of the polyhedron. Several hyperplanes also define a \textbf{hyperplane arrangement} which is in \textbf{general position} whenever $\dim (H_1\cap ... \cap H_n) = m-n$, where m is the dimension of the input space and $m \geq n$. In the case where $m < n$, the intersection should be empty for an arrangement to be in general position. In this work we will only concern ourselves with arrangements in general position, which are known to almost always occur in neural networks \cite{grigsby2022transversality}.

One property of such polyhedra that we will use later is the fact that they are always convex. This will come in handy when we proove the conditions under which only the overlap decomposition is necessary when computing relative homology. For all calculations using polyhedra we used the \textbf{polytope} package in python (https://github.com/tulip-control/polytope) and specified a bounding box of $[-100,100]$ around every input space.

\subsection{Linear Programming}
While the field of neural networks is still in its infancy, there are well established ways to study hyperplane arrangements. One such approach which we use to discover overlaps is linear programming. A linear program is a linear optimization technique that finds a solution $x$ under a set of linear constraints,

\begin{align*}
    & \max\limits_{x}(c^Tx),\\
    & \text{Such that } Ax \leq b.
\end{align*}

A special type of linear program which we mainly concern ourselves with in this work is the discovery of a feasible region. In this case we just want to find any point $x$ that satisfies the inequality constraints. This can be computed by substituting $c$ with a zero vector and thereby only looking for a point $x$ that satisfies the linear constraints. Linear programs can be solved through a variety of methods, in this work we have used the \textbf{scipy} wrapper of the HiGHS solver \cite{huangfu2018parallelizing}.

\subsection{Simplicial complexes and filtrations}
Data typically comes in the form of a point cloud in which there is no apriori knowledge of the underlying structure of the space from which it is sampled. In order to study the topology of neural representations across the layers of a network, one has to associate some structure to these points. Given a metric space $X$, a standard approach for this is the Vietoris-Rips complex, defined as the set $V_\epsilon(X)  = \{S : d(x_i,x_j) \leq \epsilon, \forall x_i,x_j \in S\}$. This forms an abstract simplicial complex in which every $S$ with $k$ elements is a $(k-1)$-simplex. Given this strucutre we can compute simplicial homology.

Very often we do not have knowledge of the right value of $\epsilon$ and therefore use the strategy of persistent homology. In this case one builds a filtration of simplicial complexes, known as the Vietoris-Rips filtration. This is done by progressively increasing the value of $\epsilon$ . Since we know that $V_\epsilon \subseteq V_{2\epsilon}$, this corresponds to adding new simplices to each previous simplicial complex.

\subsection{Quotient maps}

The main intuition behind our approach is that a network can change the topology of an input manifold by gluing different pieces of it together. This intuitive idea is studied in topology through the concept of a quotient space under an equivalence relation $\sim$. Quotient spaces are central in all following proofs and therefore deserve more attention. The first concept that needs clarification is that of an equivalence relation.

\begin{definition}
    A relation $\sim$ is an equivalence relation if and only if obeys the following properties.
    \begin{itemize}
        \item $x \sim x$ (reflexivity).
        \item $x \sim y \Leftrightarrow y\sim x$ (symmetry).
        \item If $x \sim y$ and $y \sim z$ then $x \sim z$ (reflexivity).
    \end{itemize}
\end{definition}

If we are given a map $f:X \to Y$, it is often the case that $f$ produces the same output on several values of $X$ and therefore splits $X$ into several equivalence classes denoted by $[x] = \{z \in X | f(z)=f(x)\}$ (note that points on which $f$ is unique generate an equivalence class with a single point $[x] = \{x\}$). Together these equivalence classes can be written as a set $X/\sim_f = \{[x]| x\in X\}$ and there is \textit{canonical quotient map} $q:X \to X/\sim_f$ sending any $x$ to its equivalence class $[x]$. This set is then equipped with the \textit{quotient topology}, meaning that any subset $U \subseteq X/\sim_f$ is open if and only if $q^{-1}(U)$ is open in $X$.

\subsection{Relative homology}
While standard homology calculations are at this point quite familiar to machine learning researchers, the concept of relative homology is still rather unexplored in the machine learning literature. Intuitively, one of the main motivations behind relative homology is to understand how the homology groups of a space change as we glue parts of it to each other. This is possible as long as $X$ and $S$ are compact Hausdorff and there is some neighborhood of $S$ which is a strong deformation retract to some closed neighborhood of $S$ (see \textbf{Theorem 2.14} and \textbf{Corollary 2.15} in \cite{vick2012homology}). Therefore, given this mild condition, we can use relative homology to study the homology groups of quotient spaces $X/S$.

The precise steps to calculate relative homology work as follows. If we have a simplicial complex of a space $V_\epsilon(X)$ we can associate a chain complex $C_k(X)$ containing all k-chains. Furthermore, we can associate a chain complex $C_k(S)$ to a subcomplex $S \subset X$. Finally, we also need to define a chain complex structure on the quotient $C_k(X,S) = C_k(X)/C_k(S)$. These objects fall inside of a short exact sequence,

\begin{equation*}
    \begin{tikzcd}
        0 \arrow[r, ""] & C_k(S) \arrow[r, "i"] & C_k(X) \arrow[r,"j"] & C_k(X,S) \arrow[r,""] & 0,
    \end{tikzcd}
\end{equation*}

where $i$ is the inclusion $S \xhookrightarrow{} X$, and $j$ is the quotient map $j: X \to X/S$.

What we would really like to have is a sequence between the quotients of the chain groups through some sort of boundary map like those used in standard homology calculations,

\begin{equation*}
    \begin{tikzcd}
        ... \arrow[r, ""] & C_k(X,S) \arrow[r, "\partial_k"] & C_{k-1}(X,S) \arrow[r,"\partial_{k-1}"] & C_{k-2}(X,S) \arrow[r,""] & ...
    \end{tikzcd}
\end{equation*}

Given such a map, we can define homology groups like usual,

\begin{equation*}
    H_k(X,S) = \ker\partial_{k-1} / \text{Im}{\partial_k}.
\end{equation*}

While we cannot get such a map directly, we can use the zig-zag lemma \cite{munkres2018elements} on the short exact sequence of chain groups to derive a long exact sequence of homology groups, which we can use to compute relative homology groups given knowledge of $H_k(S)$ and $H_k(X)$.

\begin{equation*}
    \begin{tikzcd}
        ... \arrow[r, ""] & H_k(S) \arrow[r, "i_*"] & H_k(X) \arrow[r,"j_*"] & H_k(X,S) \arrow[r,"\partial_*"] & H_{k-1}(S) \arrow[r,""] & ...
    \end{tikzcd}
\end{equation*}

The star subscript denotes that the maps are induced by the homology functor $H_k: f \to f_*$. This new boundary map connecting the homology groups $H_k(X,S)$ to $H_{k-1}(S)$ is called the \textit{connecting homeomorphism} (for details see \cite{hatcher2005algebraic} pages 115-119). 


\subsubsection{Computing relative homology in data}
\label{Relative Homology on Data}
The fact the computation of relative homology requires knowledge of the homology groups $H(X)$ and $H(S)$, brings both advantages and disadvantages. On the bright side, it is sometimes the case that we might know something about the structure of the input data, whereas studying the output of a highly non-linear function like those implemented by neural networks is much more difficult. In this way, relative homology avoids having to deal with the non-linear effects generated by neural networks.
The downside of this approach is that if we do not know the homology groups of the input data (which is often the case), we need a way to estimate them. So how should we approach this?

If we have a reasonable guess for the spatial scale at which the input data can be analyzed, we can fix some $\epsilon$ and use the Vietoris Rips complex at that value to calculate homology groups at a single point. Then to calculate relative homology we can append a single point to each simplex in $S$. This makes each simplex $\sigma \in S$ into a contractible cone and therefore homeomorphic to taking the quotient $X/S$. 

Alternativelly, we can still use persistent homology but set the distances between every two points $s_i, s_j \in \sigma$ to 0. After this we can define the quotient (pseudo)metric with $p_1 \in [x]$ and $q_n \in [y]$ by,

\begin{equation*}
    d([x],[y]) = \text{inf}\{d(p_1,q_1) + d(p_2,q_2)+...+d(p_n,q_n)\}.
\end{equation*}

This effectively computes the shortest path between two points given that the distances within each equivalence class $[x]$ and $[y]$ are set to 0. Computationally, we implement this by first setting the aforementioned distances to 0 and afterwards, computing the shortest path on the distance weighted adjacency matrix using Djikstras algorithm \cite{dijkstra1959note}. While we developed an implementation of the first approach, it proved much more efficient to use the second.

\newpage

\section{Proofs}

We start this section by showing that the only source of topological change that a network can implement is due to a failure to be injective. While such maps can fail to be surjective if we define $\Phi:\mathcal{M} \to \mathbb{R}^n$, since the codomain can fail to be covered, we are more interested in how the data manifold is transformed. Therefore, for the purposes of this goal we will define the codomain of the map to be $\text{Im}(\Phi)$, where $\text{Im}(\Phi) := \{\Phi(x)|x\in \mathcal{M}\} \subseteq \mathbb{R}^n$. This makes the mapping trivially surjective. Furthermore, we omit the layer index as our proofs apply to any stage of the network.

\begin{theorem}
    A ReLU neural network $\Phi: \mathcal{M} \to \text{Im}(\Phi)$, with $\mathcal{M}$ being a compact manifold, is not a homeomorphism iff $\Phi$ is not injective.
\end{theorem}

\begin{proof}
    It is useful to remind the reader of the properties necessary for a map to be a homeomorphism. 
    \begin{itemize}
        \item $\Phi$ is bijective,
        \item $\Phi$ is continuous,
        \item There is an inverse continuous function $\Phi^{-1}$.
    \end{itemize}

    From this we can see that since if $\Phi$ is not injective it is therefore not bijective, this is enough to prove one direction of the statement. Now we have to show that if $\Phi$ is not a homeomorphism then it is not injective. We will do that by showing that all other properties for a homeomorphism are guaranteed by the nature of the maps that neural networks can implement. For this we use \textbf{Theorem 2.1} in Arora et al. 2016., which states that every ReLU deep neural network represents a continuous piecewise linear function.

    Since such neural networks are continuous piecewise linear functions, the continuity of $\Phi$ follows. We have seen that surjectivity trivially follows from the way we define the codomain, so the only thing left to show is that $\Phi^{-1}$ is also continuous.

    It turns out that such a function need not be continuous, but that occurs solely when $\Phi$ is not injective. Let us assume that $\Phi$ is injective, then it is a continuous bijection (surjectivity applies everywhere). Since $\mathcal{M}$ is compact we can use \textbf{Theorem 26.6} from \cite{munkres200} to see that this implies that $\Phi^{-1}$ is continuous (the theorem in Munkres also requires that $\text{Im}(\Phi)$ is Hausdorff but this property follows from the fact that $\mathcal{M}$ is a compact manifold and $\Phi$ is continuous). This proves that injectivity implies that we have a homeomorphism and thus all topological changes occur as a result of the network failing to be injective.
\end{proof}

Next we show that the non-injectivity of neural networks can be described by two sources related to the local rank of a map and the overlap of regions in the polyhedral decomposition.

\begin{theorem}
    Denote the set of equivalence classes generated by $\Phi$ as $\sim_\Phi = \{[x] | \Phi(x)=\Phi(y)\}$. This set is equal to the union of $\mathcal{R}_\Phi = \{[x] | \text{rank}(\Phi|_x) < \dim(\mathcal{M}), y \in G_J, \Phi(x)=\Phi(y)\}$ and $\mathcal{O}_\Phi$ (defined in \ref{Overlap definition}).
    \label{Theorem: two sources}
\end{theorem}

\begin{proof}
    Any equivalence class $[x]$ in $\sim_\Phi$ contains a set of points in $\mathcal{M}$. The input manifold has a polyhedral decomposition and can be written as $\mathcal{M} = \bigcup\limits_J G_J$, what this means is that the set of points contained in $[x]$ live in either one or several regions $G_J$. Thus, depending on how many polyhedra are occupied by the points in $[x]$, we can consider two cases.

    \begin{itemize}
        \item \textbf{Case 1:} The points in $[x]$ all fall in the same region $G_J$. We know that within the same region the network applies an affine transformation and whenever such a transformation is full rank, the map is locally a homeomorphism. Therefore, for a map to be non-injective within a single region, it has to fail to be full rank. This is the exact set described by the rank decomposition $\mathcal{R}_\Phi$.
        \item \textbf{Case 2:} The points in $[x]$ fall in several different regions $G_J$. In this case we know that since for any two points in $[x]$, coming from different regions $G_J$, we have $\Phi(x)=\Phi(y)$ then $\Phi(y) \in \bigcap\limits_{i \in I} \Phi(G_{J_i})$ for any $y \in [x]$. This set exactly coincides with the overlap decomposition $\mathcal{O}_\Phi$.
    \end{itemize}
    Since these are the only two possibilities for where the points in any $[x]$ can come from, we have shown that $\sim_\Phi = \mathcal{R}_\Phi \cup \mathcal{O}_\Phi$. It is also worthwhile to note that the intersection $\mathcal{R}_\Phi \cap \mathcal{O}_\Phi$ is not necessarily trivial.
\end{proof}

Next we show that if we treat $\Phi$ as a canonical quotient map and we have knowledge of the rank and overlap decompositions, then we can compute homology groups through relative homology. This is a corrolary of the fact that the image of $\Phi$ is homeomorphic to the canonical quotient map induced by the equivalence relation $\sim_\Phi$.
\begin{theorem}
    $\Phi(\mathcal{M})$ is homeomorphic to $q(\mathcal{M})$. Where $q:\mathcal{M} \to \mathcal{M}/\sim_\Phi$ is the canonical quotient map.
\end{theorem}

\begin{proof}
    We essentially have the following diagram: 
    \begin{equation*}
        \begin{tikzcd}
        \mathcal{M} \arrow[r, "\Phi"] \arrow[d, "q"']
        & \text{Im}\Phi  \\
        \mathcal{M}/\sim_\Phi \arrow[ru, "\pi"]\\
    \end{tikzcd}
    \end{equation*}
    If we can find some $\pi$ that is a homeomorphism, then we can write $\Phi = \pi \circ q$ and the proof would be complete. We can construct such a map in the following way,

    \begin{equation*}
        \pi([x]) = 
        \begin{cases}
            \Phi(x) \text{ if } |[x]|=1, \\
            \Phi(y) \text{ for some } y \in [x].
        \end{cases}
    \end{equation*}
    This function is injective, since $\pi([x])=\pi([z])$ implies $\Phi(x)=\Phi(z)$ and therefore $[x]=[z]$. It is also surjective as for any $z \in \text{Im}\Phi$, there is an $x \in \mathcal{M}$ such that $\Phi(x) = z$ and $\pi([x]) = z$. The continuity of $\pi$ simply follows from the fact that $\Phi$ is continuous. Finally, the continuity of the inverse $\pi^{-1}$ again follows from \textbf{Theorem 26.6} in Munkres \cite{munkres200} as the quotient space of a compact space is compact,
    and the precise choice of $y$ is irrelevant.
\end{proof}

\begin{corollary}
    The homology groups $H_k(\Phi(\mathcal{M}))$ are equivalent to the relative homology groups $H_k(\mathcal{M}, \sim_\Phi)$.
\end{corollary}
\begin{proof}
    This follows simply from the fact that homeomorphic spaces have identical homology groups.
\end{proof}

It turns out that in cases for which $\mathcal{M} \cap G_J$ is convex for any $J$, we can ignore the rank decomposition and find the relative homology groups only given knowledge of the overlap decomposition which we have a way to compute. This is proven in the following theorem.

\begin{theorem}    
    \label{Convex homology proof}
    If $\mathcal{M}\cap G_J$ is convex (or more generally - contractible) $\forall G_J$, then $H_k(\Phi(\mathcal{M})) \simeq H_k(\mathcal{M},\mathcal{O}_\Phi)$.
\end{theorem}

\begin{proof}
    
    In the corrolary of the previous theorem we have already shown that $H_k(\Phi(\mathcal{M})) \simeq H_k(\mathcal{M},\sim_\Phi)$. We also know that $\sim_\Phi = \mathcal{R}_\Phi \cup \mathcal{O}_\Phi$, therefore our proof will be complete if we can show that quotiening out the part of the rank decomposition that does not intersect the overlap decomposition leaves the homology groups invariant.

    Since we know that homology groups are invariant under homotopy equivalence, we require that $\mathcal{M}/(\mathcal{R}_\Phi \cup \mathcal{O}_\Phi)$ is homotopy equivalent to $\mathcal{M}/(\mathcal{O}_\Phi)$. This is represented by the diagram:
    \begin{equation*}
        \begin{tikzcd}
        \mathcal{M} \arrow[r, "p"] \arrow[d, "q"']
        & \mathcal{M}/\mathcal{O}_\Phi \arrow[ld,shift left = 1, "\sigma"] \\
        \mathcal{M}/\sim_\Phi \arrow[ru, shift left = 1, "\pi"] \\
    \end{tikzcd}
    \end{equation*}
    The two arrows $p$ and $q$ coming out from $\mathcal{M}$ are canonical quotient maps. To show homotopy equivalence we now prove that $\pi \circ \sigma$ and $\sigma \circ \pi$ are homotopic to $\text{id}_{\mathcal{M}/\mathcal{O}_\Phi}$ and $\text{id}_{\mathcal{M}/\sim_\Phi}$ respectively. The two quotient maps split $\mathcal{M}$ into equivalence classes that we denote as $[x]_p$ and $[x]_q$. Since both $p$ and $q$ take a quotient with respect to $\mathcal{O}_\Phi$ the equivalence classes $[x]_q$ and $[x]_p$ will match on $x \in \mathcal{M} - (\mathcal{R}_\Phi-\mathcal{R}_\Phi\cap\mathcal{O}_\Phi)$. Therefore on this set $\pi$ and $\sigma$ map identical equivalence classes to each other. The interesting part of the proof is dealing with the remaining set that is generated purely by the rank decomposition.

    We can define the map $\sigma: \mathcal{M}/\mathcal{O}_\Phi \to \mathcal{M}/\sim_\Phi$ as another quotient canonical map such that $q = \sigma \circ p$, that collapses the points that were missed by $p$ or,
    
    \begin{equation*}
        \sigma([x]_p) = 
        \begin{cases}
            q(x) \text{ if } x \in \mathcal{M} - (\mathcal{R}_\Phi - \mathcal{R}_\Phi \cap \mathcal{O}_\Phi), \\
            q \circ p^{-1}([x]_p) \text{ if } x \in \mathcal{R}_\Phi - \mathcal{R}_\Phi \cap \mathcal{O}_\Phi.
        \end{cases}
    \end{equation*}
    
    Where we note that since $p$ is injective on points that are purely in $\mathcal{R}_\Phi$, it is invertible on this subdomain. The map $\pi: \mathcal{M}/\sim_\Phi \to \mathcal{M}/\mathcal{O}_\Phi$ is tricker, but we define it piecewise as,

    \begin{equation*}
    \pi([x]_q) = 
        \begin{cases}
            p(x) \text{ if } x \in \mathcal{M} - (\mathcal{R}_\Phi-\mathcal{R}_\Phi\cap\mathcal{O}_\Phi), \\
            [z]_p \text{ such that } \sigma([z]_p) = [x]_q \text{ for some } z \in \mathcal{R}_\Phi - \mathcal{R}_\Phi\cap\mathcal{O}_\Phi).
        \end{cases}
    \end{equation*}

    For the second condition we need to choose some representative $[z]_p$, since there are many $\sigma([z]_p) = [x]_q$, this particular choice does not really matter as in the end $\sigma \circ \pi ([x]_q) = \sigma([z]_p) = [x]_q$ and $\pi \circ \sigma([x]_p) = \pi([x]_q) =  [z]_p$. The first composition of maps is clearly homotopic to the identity, for the second we construct the homotopy.

    For the points in $\mathcal{M}$ representative of an equivalence class $[x]_q \in \mathcal{R}_\Phi- \mathcal{R}_\Phi\cap\mathcal{O}_\Phi$ we can define a strong deformation retraction as $F:\mathcal{M}/\mathcal{O}_\Phi\times [0,1] \to \mathcal{M}/\mathcal{O}_\Phi$, where we have $F(x,0) = \text{id}_{\mathcal{M}/\mathcal{O}_\Phi}([x]_p)$ and $F(x,1) = \pi \circ \sigma ([x]_p)$. 
    \begin{equation*}
        F([x]_p,t) = 
        \begin{cases}
            [x]_p \text{ if } x\notin \mathcal{R}_\Phi- \mathcal{R}_\Phi\cap\mathcal{O}_\Phi),\\
            \gamma_z(t) \text{ if } x \in \mathcal{R}_\Phi- \mathcal{R}_\Phi\cap\mathcal{O}_\Phi).
        \end{cases}
    \end{equation*}

    Where $\gamma_z(t)$ is a continuous path (which exists due to the equivalence classes being convex and therefore contractible) from any point $[x]_p \in p\circ q^{-1}([x]_q)$ to the representative $[z]_p$ point. This map starts at $\gamma_z(0)([x]_p) = [x]_p$ and ends at $\gamma_z(1)([x]_p) = [z]_p$ while following a continuous path. This proves that there is a homotopy equivalence between $\mathcal{M}/\sim_\Phi$ and $\mathcal{M}/\mathcal{O}$. Since homology groups are invariant under homotopy equivalence we get our final result,
    \begin{equation*}
        H_k(\mathcal{M},\sim_\Phi) \simeq H_k(\mathcal{M},\mathcal{O}_\Phi).
    \end{equation*}
\end{proof}

But how often is it true that $\mathcal{M} \cap G_J$ is contractible for any $J$? It turns out that this is always true if $\mathcal{M}$ is convex. Otherwise the probability of this property obtaining likely increases with the size of a network. This is expected to happen since as the number of polyhedra increases, the volume of each polyhedron shrinks and intersects a smaller area of the manifold. At this stage this is more of a heuristic argument and hopefully future research will clarify the extent to which it applies in practically useful neural networks.


\begin{proposition}
\label{proof:loc_affine}
    Given a region $L^l_J$ of the local decomposition $\mathcal{L}^l$, there can be two points $x,y \in L^i_J$ for which the maps $\Phi^l|x \neq \Phi^l|y$.
\end{proposition}

\begin{proof}
    We can prove this by a simple counterexample. Consider the interval $[-1,1]$, let's say that there is a piecewise-function,
    \begin{equation*}
        f(x) = 
        \begin{cases}
            x+1 \text{ if } x<0\\
            -x+1 \text{ if } x > 0.
        \end{cases}
    \end{equation*}
    Since this is a piecewise-linear function we know that there is an associated neural network $\Phi: \mathbb{R} \to \mathbb{R}$ with a single output neuron that implements it. We also know that the local decomposition is $\mathcal{L} = L_{\{1\}} = [-1,1]$. Clearly the two affine functions are not equal to each other.
\end{proof}

\newpage

\section{Simulation details}
\label{Sim details}

Here we present all parameters and details for all simulations presented in the results. Where no further explanation is needed, we shall present the parameters in tables.

\subsection{Non-linear curves simulations}
We show two example non-linear curves in Figure \ref{fig:curves} and ten more in a supplementary Figure \ref{fig:supp figure many knots}. The parameters described below apply for each individual example that we present. Since the point of these simulations is to compare persistent homology to our relative homology based approach, we only use a training dataset. All parameters are specified in the table below.

\begin{table}[ht]
\caption{Parameters for non-linear curve simulations}
\label{curves params}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
\# Samples & Criterion & Learning rate & Epochs & Stopping Criterion & Width & Depth & Sensitivity \\
\midrule
500 & MSE & 1e-4 & 1000 & MSE $<$ 0.00002 & 50 & 3 & 1 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Reproduction of the results of Naitzat et al.}
As mentioned before, we reproduced the results from \cite{naitzat2020topology} with fewer samples due to the high computational demand of computing the overlap decomposition. This meant that we also had to reproduce the pre-processing steps in their pipeline. In order to speed up the computation of persistent homology and attain a more robust measure of distance, Naitzat et al. fixed a k-nearest neighbors graph at a fixed persistence parameter $\epsilon$ for all datasets. Since the homology groups of the datasets were known a priori, they were able to choose these values on the dataset and carry them forward in the computation of homology groups in deeper layers. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{val_params.png}
    \caption{Parameter values that generate the same Betti numbers as the ground truth data. The chosen values were $\epsilon=2.5$ for all datasets, k=14 for D-I and k=19 for D-II and D-III. These values are highlighted in red.}
    \label{fig:enter-label}
\end{figure}


We follow in their steps, but due to the different number of samples we ended up using slightly different parameter values. Below we show the parameter values for which we achieved the ground truth topology (in green), the one we ended up choosing is highlighted in red.

\begin{table}[ht]
\caption{Parameters for Naitzat et al. reproduction results}
\label{naitzat params}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccccc}
\toprule
Dataset & Train/Test & Criterion & Learning rate & Epochs & Stopping Criterion & \# models & Width & Depth \\
\midrule
D-I & 7800/2200  & CSE & 2e-5 & 5000 & Accuracy $>$ 0.999 & 30 & 15 & 9  \\
D-II & 7500/2500 & CSE & 2e-5 & 5000 & Accuracy $>$ 0.999  & 30 & 15 & 9  \\
D-III & 8000/4000 & CSE & 2e-5 & 5000 & Accuracy $>$ 0.999 & 30 & 15 & 9  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

In addition we wanted to avoid issues in homology estimation due to missclassified points, so we excluded all such points from the test set before performing a homology calculation. Given that our networks had almost perfect accuracy, this procedure removed very few points ($7.5 \pm 12.9$). For the calculation of persistent homology we used the \textbf{Ripser} package \cite{ctralie2018ripser, Bauer2021Ripser}.


\subsection{Counting regions in the spheres dataset}
All experiments for spheres of different dimensions $S^1, S^2 \text{ and } S^3$ share the same parameters except for the dimension. Also the two classes were balanced, meaning that they contained the same number of samples. 

\begin{table}[ht]
\caption{Parameters for spheres datasets}
\label{spheres params}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
\# Samples & Criterion & Learning rate & Epochs & \# models & Width & Depth & Sensitivity \\
\midrule
2000 & CSE & 2e-5 & 1000 & 10 & 25 & 4 & 1 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\newpage


\section{Supplementary figures}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{knots_many.png}
    \caption{More examples of non-linear manifolds and their homology groups. The numbers in the top right of each manifold show the ground truth number of holes. While our relative homology approach typically performs better, notice that in the fourth and (arguably) eighth example we see a homological type 2 error.}
    \label{fig:supp figure many knots}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{orthogonal_decomps.png}
    \caption{Same plot as in Figure \ref{fig:overlap regions} but using an orthogonal initialization (in magenta).}
    \label{fig:orthogonal}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{depth_width_overlaps.png}
    \caption{(left) Number of populated polyhedra across 10 randomly initialized networks on the two dimensional data from \ref{overlap trained random} for different widths and depths. Annotations show the mean ± the standard deviations across models. (right) Number of overlap regions in the same models. Compared to the number of populated polyhedra, the number of overlap regions seems to scale non-monotonically.}
    \label{fig:overlap expressivity}
\end{figure}


\end{document}
