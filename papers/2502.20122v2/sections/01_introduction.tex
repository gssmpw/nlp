\section{Introduction}

Chain-of-thought (CoT) reasoning has significantly improved the ability of large language models (LLMs) to perform complex tasks \cite{wei2022chain}.
The effectiveness of CoT reasoning has been attributed to the additional computation allocated during inference, as each intermediate reasoning token enables the model to perform an additional forward pass through its parameters \cite{nye2021show, wei2022chain}.
On the other hand, this inherently incurs additional inference cost and latency, roughly proportional to the number of output tokens \cite{agrawal2024taming, ho2024block}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/main.pdf}
    \caption{\textbf{Potential of concise reasoning in current LLMs.} Examples of reasoning samples with different lengths and the normalized length distribution of correct reasoning paths from Llama-3.1-8B on GSM8K.
    Typical samples from the \textit{\textcolor{blue}{original model distribution} marked in blue}, as shown in the \textit{\textcolor{blue} {blue box}}, contain redundant context as marked in gray.
    We fine-tune the model with relatively concise reasoning paths within the \textit{\textcolor{orange}{subregion of the model output distribution} marked in orange}.
    This elicits the model to reason more efficiently, as shown in the \textit{\textcolor{orange}{orange box}}, retaining only the essential parts while still leading to the correct answer.
    }
    \label{fig:distribution}
\end{figure}


We posit that current models often generate more tokens than necessary to accomplish the task, incurring extraneous inference costs.
Typical reasoning chains sampled from the original model distribution, exemplified in the blue box of \autoref{fig:distribution}, includes many tokens that do not contribute to the solution, specifically verbose explanations and repetitive phrasing.
Similar observations have also been made in prior work~\cite{renze2024benefits, zhang2024verbosity, chiang2024over}.

% Namgyu TODO - improve flow of logic
We argue that the redundancy in reasoning chains of current models is not surprising.
LLMs have not been explicitly trained to utilize intermediate tokens for reasoning in the most efficient manner.
Rather,  CoT reasoning is an emergent ability of LLMs \cite{wei2022emergent} first \textit{uncovered} through novel few-shot prompting \cite{wei2022chain}.
Post-hoc analysis suggests that reasoning ability is derived from procedural knowledge in pretraining data \cite{ruis2024procedural}, which is not optimized for conciseness.
Therefore, it is natural that the default behavior of LLMs does not optimize for efficient use of tokens for reasoning.
% (see \autoref{discussion}).
However, this does not preclude the possibility that LLMs \textit{possess} the ability to reason more efficiently.

Indeed, the distribution of reasoning path lengths shown in~\autoref{fig:distribution} reveals a compelling insight: the presence of shorter, more efficient reasoning paths within the model's output distribution, marked in orange, suggests a latent capacity for concise reasoning.
% The orange box exemplifies concise reasoning paths within this `shorter' subregion, only containing essential logic, leading to the correct answer.
% In contrast to zero-shot prompting, which struggles to achieve both length reduction and maintained accuracy, particularly on task-specialized models,
This observation motivates our central hypothesis: that by selectively leveraging these existing concise examples, we can fine-tune the model to elicit this latent ability and shift its default output distribution towards more efficient reasoning.

In this paper, we propose a simple yet effective fine-tuning method to elicit efficient, concise reasoning in models for a given target task.
We note that existing zero-shot prompting methods fails to reliably elicit concise reasoning, exhibiting near negligible impact on task-specialized models (\autoref{zero_shot_prompting}).
On the other hand, we can leverage best-of-N (BoN) sampling and few-shot conditioning (FS) to reliably generate concise training data (\autoref{method}).
We can then apply standard fine-tuning to distill the length-reducing benefits of BoN sampling and few-shot prompting back into the model itself while avoiding their associated overheads at inference time.

Across GSM8K and MATH datasets and a wide variety of model families, our unified few-shot conditioned best-of-N sampling (FS-BoN) method achieves a substantial reduction in output length of 30\% on average, a 2.4x improvement over previous fine-tuning baselines \cite{de2024rational}, while preserving overall accuracy.
% Our analysis confirms that training data length reductions effectively transfer to model outputs. 
Notably, our analysis shows that trained models adaptively adjust output length based on question complexity, preserving detail for difficult problems while simplifying responses to easier ones. 
We confirm that these results remain consistent across various model scales.
These results strongly suggest that fine-tuning with carefully curated, self-generated data effectively can unlock latent concise reasoning abilities within LLMs, leading to significantly more cost-effective inference for complex tasks.



% To address the redundancy in reasoning paths and improve inference efficiency in \textit{task-specific} settings, we propose a simple yet effective fine-tuning (FT) method to reliably elicit concise reasoning while preserving accuracy. Our approach focuses on self-training, leveraging the model's own output to generate high-quality training data. Specifically, we introduce a best-of-N (BoN) sampling strategy to identify the most concise correct reasoning paths from a diverse set of model-generated solutions. Recognizing the sample inefficiency of naive BoN sampling, we further incorporate few-shot conditioning with exemplars derived from human annotation, powerful proprietary LLMs, and self-generation, to bootstrap the reduction in output length, significantly improving sample efficiency (\autoref{fig:bon_sample_efficiency}). Our combined FS-BoN fine-tuning method effectively distills the length-reducing benefits of few-shot prompting and BoN sampling directly into the model while avoiding the overhead associated with repeated sampling and few-shot prompting during inference.

% We demonstrate the effectiveness of our proposed task-specific fine-tuning method across a range of mathematical reasoning tasks. 
% We investigate zero-shot prompting techniques, revealing that simple prompt-based interventions often fail to elicit concise reasoning or maintain accuracy, especially in task-specialized models. In stark contrast to these zero-shot limitations, our proposed BoN sampling method, wherein the fine-tuned model achieves substantial length reduction, attains performance competitive with previous baselines. %such as Rational Metareasoning \cite{de2024rational}. 
% Further, we show that few-shot conditioning (FS) can significantly increase this reduction. 
% Critically, combining few-shot conditioning with BoN sampling (FS-BoN) achieves even greater reduction, attaining a 2.4x reduction compared to the previous fine-tuning baseline \cite{de2024rational} on average, while preserving overall accuracy. 
% Key analyses demonstrate that fine-tuning effectively transfers the length reduction observed in the training set to the model's output, that the method exhibits adaptive length reduction based on question complexity, and that it maintains consistent performance across different model scales. 
% Our results suggest that fine-tuning with carefully curated self-generated data can unlock latent concise reasoning abilities within LLMs, leading to more efficient and cost-effective inference.



% Recent advances in standard LLM training pipelines involve high quality math and code datasets and synthetic reasoning data derived from teacher models, but these do not explicitly promote conciseness or token efficiency.
% In fact, recent `thinking' models are reinforced to use additional tokens to improve reasoning performance, rather than save on token budget

% Previous work and limitagtion
% Recent and concurrent work~\citep{han2024token} explores various methods, based on prompting and fine-tuning, to adapt LMs to reason with fewer tokens.
% However, previous studies on adaptation methods have been limited to a small number of model families and sizes or to datasets that are too easy for the model. 
% Since each model family has been trained using distinct training datasets and post-training methods, the modelâ€™s behavior under shallow adaptation such as prompting or small-scale fine-tuning may not work.

% In this work, we aim to develop a simple and robust method to reduce reasoning tokens without compromising performance, by minimizing verbal redundancy.
% To this end, we first thoroughly evaluate existing prompting methods and our own hand-crafted prompts to validate their robustness and identify several shortcomings~(Section~\ref{sec:preliminary}).
% Then, we investigate various fine-tuning methods to reduce the reasoning length, starting from a naive approach using best-of-n~(BoN) supervised training~(SFT) which exploits the distribution of reasoning lengths, shown in Figure~\ref{fig:distribution} (b).
% Finally, by leveraging few-shot conditioning and mixing rationales from different prompting strategies along with BoN SFT, we can further reduce tokens while matching the original accuracy.
% Through extensive experiments, we validate the robustness of these methods across a wide range of model families and challenging datasets that benefit from CoT reasoning.

