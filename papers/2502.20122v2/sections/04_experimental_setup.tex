\section{Experimental setup}
\label{experimental_setup}


\paragraph{Models}
% main models
% variety of models with moderate model size: Llama 3.2 3B, Gemma 2 2B, Qwen2.5 3B
% \cite{dubey2024llama, team2024gemma, yang2024qwen2}
% math-specific models
% {yang2024qwen2math, shao2024deepseekmath}
% we use math specific models to verify the robustness of methods when applied to models that undergo extensive post-training.
% we also conduct scaling study on the llama 3 family of models, for 1B (3.2), 3B (3.2) and 8B (3.1).
% main models
% variety of models with moderate model size: Llama 3.2 3B, Gemma 2 2B, Qwen2.5 3B
% \cite{dubey2024llama, team2024gemma, yang2024qwen2}
% math-specific models
% {yang2024qwen2math, shao2024deepseekmath}
% we use math specific models to verify the robustness of methods when applied to models that undergo extensive post-training.
% we also conduct scaling study on the llama 3 family of models, for 1B (3.2), 3B (3.2) and 8B (3.1).
To account for realistic task-specific deployment settings, we select recent moderately sized post-trained models.
We also consider math-specialized models to evaluate on models that have been optimized for specific task domains.
We consider five main models for our key experiments: Llama-3.2-3B \small\cite{dubey2024llama}\normalsize, Gemma-2-2B \small\cite{team2024gemma}\normalsize, Qwen2.5-3B \small\cite{yang2024qwen2}\normalsize, Qwen2.5-Math-1.5B \small\cite{yang2024qwen2math}\normalsize, and DeepSeekMath-7B \small\cite{shao2024deepseekmath}\normalsize.
We investigate scaling on Llama-3.2-\{1B, 3B\} and Llama-3.1-8B.

\paragraph{Tasks and datasets}
% - focus on math reasoning tasks because they require shortform answers, and CoT reasoning is used to enhance the accuracy of answers. It is desirable to reduce reasoning in such task to reduce inference cost, whereas it may not be desirable to reduce output length in tasks that require longform answers such as creative writing.
% focus on sufficiently difficult reasoning tasks where CoT reasoning is beneficial for performance.
% we use GSM8K \cite{cobbe2021training} and MATH \cite{hendrycks2021measuring}, where our models achieve 40--90% and 20--70% accuracy, respectively.
We focus on challenging reasoning tasks where (1) CoT reasoning significantly improves model performance, (2) only the final answer is relevant, and (3) models achieve moderate performance.
Reasoning length reduction is desirable under the first and second conditions as it can reduce inference latency without affecting utility.
The third condition is necessary to assess accuracy preservation.
We consider two mathematical reasoning datasets: GSM8K \cite{cobbe2021training} and MATH \cite{hendrycks2021measuring}, where the models achieve accuracies of 40--90\% and 20--70\%, respectively.
For evaluation, we utilize the test set of the GSM8K and MATH500 dataset~\citep{lightman2023let}.
We explain details in \autoref{appx_datasets}.

\paragraph{Evaluation metrics}
We evaluate methods using two primary metrics: \textit{accuracy} and \textit{length}.
Accuracy is evaluated using Python-based parsing code, described in \autoref{appx_answer_parsing}.
Length is defined as the average number of output tokens in all reasoning paths, including incorrect ones, as output tokens incur inference costs in deployment scenarios regardless of their correctness.
We focus on number of \textit{output} tokens for clear comparison, as number of input tokens are similar across methods, and output tokens affect wall-clock latency to a higher degree  \cite{agrawal2024taming}.
% Namgyu - it seems the first reason is more important. Input token prefilling time is not insignificant, as shown by zero-shot vs few-shot generation time.
We further justify this choice in \autoref{appx_justification_length}.
% Consistent with our preliminary analysis in \autoref{sec:preliminary},
We also employ \textit{relative accuracy} and \textit{relative length} metrics to better evaluate how well each method elicits concise reasoning while maintaining accuracy.
% Concretely, relative accuracy refers to the given method's accuracy divided by the baseline accuracy. The same goes for relative length.
Specifically, relative accuracy is the ratio of the given method's accuracy to the baseline accuracy, and relative length is the ratio of the given method's average length to the baseline average length, using a strong baseline zero-shot prompt \cite{pang2024iterative}.
We use greedy decoding throughout evaluation to ensure reproducibility\footnote{
Note that this contrasts with \autoref{sec:preliminary} where we took the average length of \textit{correct} samples over \textit{stochastic} generations per question, for precise question-level length normalization.
}.
% We evaluate methods using two primary metrics: \textit{accuracy} and \textit{length}. We assess concise reasoning ability in line with our preliminary analysis in \autoref{sec:preliminary} and also employ \textit{relative accuracy} and \textit{relative length} metrics.
% \textit{Accuracy} is evaluated using Python-based parsing code, which is detailed in \autoref{appx_answer_parsing}.
% \textit{Relative accuracy} is the ratio of the method's accuracy to the baseline accuracy.
% \textit{Length} is defined as the average number of output tokens in reasoning paths obtained through greedy decoding across the entire evaluation set. 
% % This differs from the calculation in \autoref{sec:preliminary} as it includes incorrect reasoning paths.
% % In real-world deployments, the tokens in all reasoning paths contribute to inference costs, regardless of correctness.
% This differs from the calculation in \autoref{sec:preliminary} as it also includes incorrect reasoning paths, since in real-world deployments, tokens of all reasoning paths contribute to inference costs regardless of correctness.
% We focus on the number of \textit{output} tokens for a clearer comparison, as this correlates to real-world wall-clock latency, whereas input tokens are typically processed with significantly higher throughput \cite{agrawal2024taming} (\autoref{appx_justification_length}).
% \textit{Relative length} is the ratio of the method's average length to the baseline's average length.

\paragraph{Baseline methods}
For baselines, we consider zero-shot prompting (\autoref{zero_shot_prompting}) and fine-tuning directly on ground-truth answers, and with external supervision from human- and GPT-4o-generated concise reasoning paths.
For existing fine-tuning methods on concise reasoning, we reproduce Rational Metareasoning (RM) \cite{de2024rational}, which follows a similar approach to our naive BoN method but with two key differences: (1) a reward function that balances efficiency and utility (i.e., output length and accuracy), and (2) iterative training via expert iteration \cite{zelikman2022star}.
% In contrast, our naive BoN method selects the shortest samples and is trained in a single step.

\paragraph{Self-training budget allocation}  
% The cost structure of self-training methods consists of two components: generation and fine-tuning. We generate $N = 16$ paths per question for BoN methods. In our reproduction of RM \cite{de2024rational}, we generate $N = 4$ paths in each of their 4 iterations. For FS-BoN, our default setting includes 16 few-shot-conditioned and 16 naive BoN-conditioned paths. Additionally, we explore a `Budget-Matched' setting, where we use 8 paths from few-shot conditioning and 8 from naive BoN, as shown in Table \ref{tab:main_results}. We focus on the generation budget since selecting the most concise reasoning path for each question keeps the fine-tuning cost minimal across all methods. Detailed experimental settings are provided in~ \autoref{appx_experimental_details}.
The cost of self-training methods can be broken down into (1) generation and (2) fine-tuning.
We aim to match the budget across methods in terms of generation (number of paths generated per question) since fine-tuning costs are relatively negligible (see Table \ref{tab:generation_training_time}).
For naive BoN, we generate 16 paths per question.
For RM \cite{de2024rational}, we generate $N = 4$ paths per iteration across 4 iterations.
We generate a single path for standalone few-shot conditioning (FS) and 16 paths for FS-BoN, both augmented with 16 additional paths from the default output distribution (used for naive BoN).
Considering the large budget of FS-BoN with augmentation, we evaluate a \textit{Budget-Matched} setting with 8 paths each from the few-shot conditioned distribution and default distribution, as shown in Table \ref{tab:main_results}.
Details are provided in~\autoref{appx_generation_and_fine_tuning}.
% Namgyu - BoN conditioning is not a valid expression in our paper