% Chain-of-thought (CoT) reasoning has dramatically improved large language model performance on complex tasks.
Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks.
% Recent `thinking' models such as OpenAI's o1 models are reinforced to use 
% Recent ``thinking'' models such as OpenAI o1 use substantial inference budgets to further boost performance.
However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs.
% We hypothesize that LLMs have a latent capacity for more concise reasoning, evidenced by relatively short, correct reasoning paths in their outputs. 
Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, \textit{relative} to their default behavior.
To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings.
Our combined method achieves a 30\% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy.
By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training.
% Our few-shot conditioned best-of-N sampling (FS-BoN) method significantly reduces reasoning length by 30\%, or 2.4x more than previous baselines, while maintaining accuracy. This implies fine-tuning with curated self-generated data can unlock latent concise reasoning, enabling more efficient inference.