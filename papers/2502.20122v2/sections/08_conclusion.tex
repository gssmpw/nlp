\section{Conclusion}


We tackle redundancy in CoT reasoning, hypothesizing LLMs possess a latent capacity for concise reasoning, evidenced by shorter correct reasoning paths. 
We introduce fine-tuning methods, leveraging self-generated data from BoN sampling and few-shot conditioning, to elicit this capacity. 
Our FS-BoN method significantly reduces reasoning length by 30\% reduction while maintaining accuracy. 
This implies fine-tuning with curated self-generated data can reliably unlock latent concise reasoning, enabling more efficient inference.

