\input{tables/tab_main_results}

\section{Results}
\label{results}

\input{tables/tab_examples}


\subsection{Main results}

Our main results, presented in \autoref{tab:main_results} and \autoref{fig:main_methods_comparison}, demonstrate the performance of our self-training methods against baseline approaches.
% We highlight key observations from these results below.

\paragraph{Naive BoN fine-tuning is effective but sample inefficient.}
Naive BoN fine-tuning effectively reduces output length without significantly degrading model performance. 
This also holds true for Qwen2.5-Math-1.5B and DeepSeekMath-7B (\autoref{tab:main_results_full_gsm8k} and \autoref{tab:main_results_full_math}), which failed to achieve length reduction through zero-shot prompting.
% However, while naive BoN does reduce output length, the reduction is limited to 12\%.
However, the length reduction from naive BoN with $N=16$ is limited to 12\% on average.
Furthermore, as illustrated in Figure~\ref{fig:bon_sample_efficiency}, achieving more compression with BoN becomes progressively less efficient.

\paragraph{Iterative baseline yields similar results as naive BoN fine-tuning.}
% We compare our single-step naive BoN approach with Rational Metareasoning \cite{de2024rational}, an iterative approach using expert iteration \cite{zelikman2022star}  which incorporates an additional \textit{utility reward} to balance efficiency and accuracy in BoN sampling.
Rational Metareasoning, an iterative baseline, yields similar relative length reduction and relative accuracy to BoN fine-tuning. 
This suggests that the utility reward proposed by \citet{de2024rational} may not effectively achieve both accuracy gains and token length reduction.

\begin{figure}[t] % "h" places the figure roughly here
    \centering
    \includegraphics[width=\columnwidth]{figures/main_methods_comparison.pdf} % Adjust width as needed
    \caption{Tradeoff between relative accuracy and length reduction for main methods. Results are averaged over GSM8K and MATH across five main models. Matching colors and shapes indicate the same FS prompt. FS conditioning without augmentation (†) are marked with lighter colors. 
    Relative length reduction refers to 100 - relative length (\%).}
    \label{fig:main_methods_comparison} % Label for referencing in text
\end{figure}
% \red{TODO - shorten this}

\paragraph{Few-shot conditioning outperforms BoN in length reduction.}
The results demonstrate that few-shot conditioning achieves a greater relative length reduction compared to naive BoN, including math-specialized models (\autoref{tab:main_results_full_gsm8k} and \autoref{tab:main_results_full_math}).
% This reduction is attributed to the fact that the fine-tuning datasets generated through few-shot conditioning contain shorter reasoning paths compared to those generated by naive BoN, as illustrated in \autoref{fig:bon_sample_efficiency}.  % too long
This is in line with the superior length reduction of few-shot conditioning, compared to naive BoN as shown in \autoref{fig:bon_sample_efficiency}.
Notably, self-training on generations conditioned on human-annotated examples (FS-Human) achieves an average relative length of 67.96\% on GSM8K, compared to 87.17\% with naive BoN.  % good to have some specific numbers in the text
% We further analyze the effect of fine-tuning on length reduction in \autoref{analysis}.



\paragraph{Self-training better preserves accuracy than training with external data.} 
\autoref{tab:main_results} shows fine-tuning with external data (\textit{FT-External Data}) leads to a significant reduction in relative length but causes a severe drop in relative accuracy. 
% \autoref{fig:main_methods_comparison} further highlights that while fine-tuning with GPT-4o CoT (FT-GPT4o) achieves slightly greater reduction in relative length than fine-tuning with self-generated data using few-shots from GPT-4o (FS-GPT4o), it results in substantially lower relative accuracy.  % a bit complicated / not concrete (conrete evidence = one where we beat external FT in both accuracy and reduction)
\autoref{fig:main_methods_comparison} further highlights the accuracy preservation of self-training: fine-tuning with external concise reasoning supervision from GPT-4o (FT-GPT4o) lies below the Pareto-curve of relative accuracy and relative length reduction, established by our self-training methods.
% NAMGYU - TODO add some commentary

\paragraph{Few-shot conditioned BoN achieves best length reduction while maintaining accuracy.}
% Few-shot conditioned BoN enables substantial length reduction compared to all other BoN and few-shot methods while maintaining relative accuracy.
FS-BoN elicits the largest length reduction among our self-training methods, while maintaining relative accuracy, on average.
Notably, for math-specialized models, FS-GPT4o-BoN achieves the greatest reduction among all methods, except those fine-tuned on external data which greatly sacrifice the accuracy (\autoref{tab:main_results_full_gsm8k} and \autoref{tab:main_results_full_math}). 
% This result reflects how applying BoN to few-shot conditioning further reduces the relative length of the training data while also increasing the proportion of correct samples.  % unnecessary

\paragraph{Augmentation boosts accuracy for few-shot conditioning.}
\autoref{fig:main_methods_comparison} compares few-shot conditioning, i.e., FS and FS-BoN, with and without augmentation (†). 
Augmentation improves accuracy by providing solutions for previously unsolvable hard questions as discussed in \autoref{sample_augmentation}. 
While augmentation may slightly affect reduction rates, they remain superior to naive BoN and RM.
% Similar effect is observed for augmentation in FS-BoN.
% Even when matching the budget (\textit{Budget-Matched}) with other fine-tuning methods using self-generated data in \autoref{tab:main_results}, it achieves the greatest length reduction among them with minimal accuracy degradation.
Even when matching the budget (\textit{Budget-Matched}) with other self-training methods in \autoref{tab:main_results}, it achieves the greatest length reduction among them with minimal accuracy degradation.
The effect of augmentation on training data length is analyzed in \autoref{appx_augmentation_length}.
% Furthermore, as shown in Figure \ref{fig:main_methods_comparison}, augmentation on few-shot conditioned BoN enhances accuracy similar to naive BoN and Meta-Reasoning while achieving greater length reduction.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/length_by_difficulty.pdf} % Adjust width as needed
    \caption{\textbf{Tokens are reduced adaptively according to question difficulty.} 
    Token reduction rate for each difficulty level on MATH, for 4 models individually and averaged.
    % Higher difficulty levels show lower reduction rates.
    Relative length reduction refers to 100 - relative length (\%).
    }
    \label{fig:length_difficulty} % Label for referencing in text
\end{figure}

\subsection{Analysis}
\label{analysis}
% This section analyzes length reduction: transfer from generation to fine-tuning, reduction by question difficulty, qualitative analysis, and consistency across model sizes. DeepSeekMath-7B is excluded from quantitative analysis due to cost.
% let's keep this short
In this section, we analyze the length reduction effects in depth.
We exclude DeepSeekMath-7B from quantiative analysis due to cost.


% \paragraph{Analysis on sample efficiency}
% As shown in \autoref{fig:bon_sample_efficiency}, best-of-n (BoN) sampling requires a substantial number of samples to be generated to achieve a level of reasoning length reduction comparable to that achievable through few-shot conditioning.
% In other words, it is infeasible to reach the reasoning length reduction performance of few-shot conditioning using BoN alone, without generating a prohibitively large number of samples.
% However, our experiments consistently demonstrate that combining few-shot conditioning with BoN sampling is more effective in reducing reasoning length than using either technique in isolation.
% Specifically, few-shot conditioning helps to guide the model towards generating more concise reasoning paths, while BoN sampling allows us to select the shortest and most accurate path from a diverse set of candidates.
% This synergistic effect results in a more efficient and effective approach to concise reasoning.


% \paragraph{FT can reduce generation length effectively.}
% As shown in \autoref{fig:ft_length_scatter}, after fine-tuning, the models tend to follow the length of the training data, suggesting that reasoning length reduction can be achieved through simple supervised fine-tuning on short reasoning samples.
% Note that test generation length is relatively longer than the training data length, as the models can generate lengthy incorrect answers, while the training data consists of correct answers.
% Correctly generated answers align more closely with training data length as shown in (Appendix~\ref{appx_length_scatter_correct}).

% \paragraph{Length reduction through generation and fine-tuning}
% Our method reduces reasoning length in two stages: generation and fine-tuning.
% First, as shown in \autoref{fig:ft_length_scatter}, 
% % generation length for training data varies depending on the method. 
% few-shot conditioning methods produce shorter outputs than naive BoN, with few-shot conditioned BoN achieving the shortest. 
% Second, fine-tuning with shorter rationales results in shorter model outputs, showing a strong correlation between test and training lengths\footnote{Test generation lengths are generally longer than training data lengths due to the possibility of lengthy incorrect answers during testing. Test outputs that are correct align more closely with training data lengths, as shown in Appendix~\ref{appx_length_scatter_correct}.}.
% Overall, FS-GPT4o-BoN effectively generates and trains for shorter reasoning paths.
% Additionally, unlike zero-shot methods, our approach significantly reduces token length in math-tuned models like Qwen2.5-Math-1.5B with FS-GPT4o-BoN, achieving 54.7\% relative length after fine-tuning. (See \autoref{tab:main_results_full_gsm8k} and \autoref{tab:main_results_full_math}).

\paragraph{Tokens are reduced adaptively according to question complexity.} 
The MATH dataset's difficulty levels range from 1 (basic algebra) to 5 (advanced calculus and complex mathematical reasoning).
As shown in \autoref{fig:length_difficulty}, our method adaptively reduces tokens based on question difficulty, with higher difficulty leading to less reduction.
% Most models achieve their peak reduction (around 20\%--40\%) at difficulty levels 1-2, where simple concepts allow for more concise explanations.
% The reduction rate gradually declines at levels 3-5, indicating our method's ability to preserve necessary details for complex problems automatically.
%  -> not precise. simple concepts allow for more concise explanations *in absolute terms*, but this does not necessarily mean that length reduction *relative to the default* should be high. E.g., if the model already uses very few tokens for easy questions, then relative reduction would be low.
The higher reduction (20\%--40\%) at easier difficulty levels (1--2) suggests that the original model outputs for these easier questions contained unnecessary tokens.
This reveals a gap in current models' ability to tailor their inference budget to problem complexity.
Our method effectively closes this gap by reducing redundancy, allowing for more precise token allocation based on question difficulty.

\begin{figure}[t] % "h" places the figure roughly here
    \centering
    \includegraphics[width=\columnwidth]{figures/scaling_methods_comparison.pdf} % Adjust width as needed
    \caption{Scaling study on baseline and few-shot conditioned self-training methods. Results are averaged over GSM8K and MATH for Llama 1B, 3B, and 8B.
    % Accuracy tends to be maintained, with greater length reduction using our FS-GPT4o(-BoN) method.
    Relative length reduction refers to 100 - relative length (\%).
    }
    \label{fig:scaling_methods_comparison} % Label for referencing in text
\end{figure}

\paragraph{Self-training maintains consistency across model scales.}
We conduct a scaling study on Llama-3.2-1B, 3B, and Llama-3.1-8B to examine consistency across different model sizes (\autoref{fig:scaling_methods_comparison}). 
Overall, token reduction increases as the model size increases, while the maintenance of accuracy does not show a strong correlation with model size. 
RM exhibits lower reduction rates compared to our few-shot conditioned self-training methods across all models and shows a decrease in accuracy with increasing model size. 
% The few-shot method also shows a similar trend in length reduction, but it achieves the best relative accuracy in the 3B model.
Our standalone few-shot conditioning method (FS-GPT4o) also shows a similar trend in length reduction, but better preserves accuracy.
Our joint FS-GPT4o-BoN method achieves the greatest reduction across all models, maintaining relative accuracy across different model sizes, especially in the largest 8B model.



\paragraph{Sample study}
\autoref{tab:samples} presents qualitative examples of reasoning paths generated by the model before and after fine-tuning with our method. 
The original reasoning exhibits verbosity, containing redundant processes such as question confirmation and repeated instructions. 
In contrast, the reasoning generated by our method includes only the necessary steps, significantly reducing the number of tokens while still arriving at the correct answer. 
% These examples demonstrate the effectiveness of our method in reducing token count. 
More examples are provided in the \autoref{appx_sample_studies}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/both_length_scatter.pdf} % Adjust width as needed
    \caption{\textbf{Fine-tuning effectively transfers the length reduction to the model.} Correlation between the relative length of train data and test output averaged over GSM8K and MATH across 4 models. Training length includes only correct solutions. Solid points represent test lengths including all generated outputs, while lighter points indicate test lengths of correct solutions only.}
    \label{fig:ft_length_scatter} % Label for referencing in text
\end{figure}

\paragraph{Length reduction is transferred through fine-tuning.}
As shown in \autoref{fig:ft_length_scatter}, fine-tuning with shorter rationales results in shorter model outputs, showing a strong correlation between test and training lengths.
% Test generation lengths (solid datapoints) are generally longer than training data lengths due to the possibility of lengthy incorrect answers during testing.
% However, when comparing with test generation lengths that are correct (lighter datapoints), they align more closely with training data lengths.
We note that the length of test outputs (incorrect and correct) are longer than the length of training samples (only correct) on average.
This is mainly because incorrect paths are generally longer than correct ones.
We find a closer correspondence between train length and test length of correct samples only, indicated by the lighter datapoints.
This discrepancy suggests the need to terminate incorrect paths early to minimize redundant inference overhead.
We consider this for future work.
