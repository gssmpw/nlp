\section{Preliminary Investigation}
\label{sec:preliminary}
\label{preliminary}
\label{concise_reasoning}

\input{tables/tab_zero_shot_prompting_gsm8k}

In this section, we probe the potential of current models to perform \textit{concise reasoning} (\autoref{reasoning_length_distribution}) and investigate whether zero-shot prompting can reliably elicit such behavior (\autoref{zero_shot_prompting}).
We start with a definition of concise reasoning and key experimental setup.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/length_distribution_normalized_gsm8k.pdf}
    \caption{\textbf{Models already exhibit relatively concise reasoning in their output distributions to differing degrees}. Distribution of output token lengths in reasoning paths of GSM8K, normalized by the average length of correct reasoning paths for each question, across five main models. Reasoning paths to the left of the gray line are relatively shorter than the model's \textit{default output} (\autoref{concise_reasoning})
    }
    \label{fig:main_distribution_analysis}
\end{figure}

\paragraph{Concise reasoning}
We define concise reasoning in relative terms:
for a given model, concise reasoning is to correctly reason through a given problem using less \textit{output} tokens, compared to its default output.
\textit{What is the default output?}
For our preliminary investigation, we measure the default output length of a model on a given question by taking the average length of correct paths over multiple stochastic generations.
We use this measure instead of greedy decoding length, as the greedy path may be incorrect.
We also consider the greedy output length as a noisy estimate, as it is highly sensitive to superficial input perturbations and numerical errors~\citep{holtzman2019curious}.

\paragraph{Key experimental setup}
% Our primary objective is to determine if models can perform concise reasoning and if zero-shot prompting can reliably elicit concise reasoning across model families.
We analyze a wide range of moderately sized models, including both instruction-tuned language models and models specifically fine-tuned for mathematical reasoning, to assess robustness.
We consider two reasoning tasks of moderate difficulty to assess preservation of model performance: GSM8K and MATH.
We explain our full experimental setup in \autoref{experimental_setup}.

% \textbf{To assess redundancy in reasoning chains, it is essential to consider factors tied to the inherent capabilities of each pretrained model.}
% What appears redundant for one model may be essential for another, as the the reasoning capacity during each forward pass varies across models.
% Depending on the model dimension, each model has distinct computation capacity (FLOPs) per forward pass and storage size within the hidden space, i.e., dimension of KV states.
% Moreover, the ability to utilize this architectural capacity to perform reasoning is determined by the specific training pipeline used to optimize each model.
% To accurately assess redundancy in reasoning chains, it is therefore crucial to account for the unique reasoning capacities of the model in question.
% -> move to discussion?j

\subsection{Reasoning Length Distribution}
\label{reasoning_length_distribution}

To assess the potential of current models to elicit concise reasoning, we analyze the extent of concise reasoning present within their output paths.
Following our definition of concise reasoning in this context, we analyze the \textit{normalized} length of \textit{correct} reasoning paths, which is obtained by dividing the length (number of output tokens) of each path by the average length of correct paths for that question.
We stochastically sample 16 paths per question on the GSM8K training dataset and visualize the distribution of normalized lengths using kernel density estimation~\citep{scott2015multivariate}.
% with sampling temperature $T=0.7$ \cite{cobbe2021training}.
Alternative analysis on absolute token count and raw character count, as well as results on the MATH dataset, are present in Appendix~\autoref{appx_reasoning_length_distribution}.

\paragraph{Models can reason with fewer tokens than their default to varying degrees.}
In \autoref{fig:main_distribution_analysis}, the distribution of normalized lengths show substantial mass below 100\%, indicating frequent occurrences of solutions shorter than the default length.
This suggests that models have the capability to produce more concise solutions than their average output would suggest.
% Furthermore, the extent of this capability varies significantly across model families. 
Notably, DeepSeekMath-7B, which already uses the least tokens in absolute terms (see \autoref{fig:token_distribution}), produces correct solutions using \textit{less than half} of its average token count in 8.37\% of cases, demonstrating substantial potential for more concise reasoning.
We find similar patterns on the MATH dataset, detailed in \autoref{appx_reasoning_length_distribution}.

% In \autoref{fig:main_distribution_analysis}, we plot the distribution of output lengths normalized by the default output (mean length of correct solutions per question).
% Our analysis reveals that all models can produce valid solutions using significantly fewer tokens than usual, although the proportion of such concise solutions varies considerably by model family.
% Notably, DeepSeekMath-7B, despite having the most concise reasoning overall with a mean output length of 179 tokens, still produces correct solutions using less than half its average token count in 8.37\% of cases.
% We find similar patterns in the MATH dataset, though with generally longer output lengths due to the increased complexity of the problems (Appendix~\ref{appx_reasoning_length_distribution}).

\subsection{Efficacy of Zero-Shot Prompting}
\label{zero_shot_prompting}

%We evaluate the effectiveness of zero-shot prompting techniques aimed at eliciting concise reasoning.
% We revisit prompts from previous works and assess their robustness across a  wide range of model families.
% We evaluate the effectiveness of zero-shot prompting for concise reasoning across diverse model families.
% We revisit existing prompts and investigate: (1) `Be Concise' \citep{renze2024benefits}, which appends the phrase `Be concise.' to the baseline prompt; (2) `Estimated Budget' \citep{nayab2024concise}, which adds `Use less than XXX tokens,' where XXX is a dynamically determined by the model itself; (3) `Fixed Budget' \citep{han2024token}, similar to 'Estimated Budget' but with a fixed 100-token budget for all questions.
% For "Estimated Budget," the budget estimation's token count is included in the reported length.  We also introduce four new prompts, optimized on Llama-3.2-1B, to maximize zero-shot token reduction.  Full prompts and prompting template details are in Appendix~\ref{appx_prompts}.
While several zero-shot prompting methods have been proposed to elicit concise reasoning, evaluation has been limited to few models.
We reassess their impact on output length and reasoning performance across diverse model families, evaluating accuracy and output length, relative to that of a strong default prompt~\citep{pang2024iterative}.
\textit{Be Concise}~\citep{renze2024benefits} appends the phrase `be concise' to the baseline prompt.
\textit{Estimated Budget}~\citep{nayab2024concise} prompts the model to `use less than N tokens,' where N is determined by the model itself.
% We include the number of output tokens used during the budget estimation phase.
\textit{Fixed Budget} \citep{han2024token} prompts the model to `limit the answer length to 100 words.'
In addition, we introduce four novel prompts optimized on Llama-3.2-1B, as our best-effort attempt at prompt optimization.
Full prompts and prompting details are outlined in \autoref{appx_prompts}.

\paragraph{Output length reduction typically comes at the cost of accuracy.}  
As shown in \autoref{tab:zero_shot_prompting_gsm8k}, most zero-shot prompting methods reduce reasoning length but often lead to notable accuracy loss.
For example, the \textit{Fixed Budget} prompt shortens reasoning by 32.2\% but lowers accuracy by 10.1\% relative to the baseline, on average.
Our hand-crafted prompts confirm that greater length reduction generally results in higher accuracy degradation.


% \paragraph{Zero-shot prompting is less effective on heavily optimized task-specific models.}
% The performance of zero-shot prompting methods is particularly inconsistent across different model families.
% While some prompts may work reasonably well on general-purpose models like Llama-3.2-3B, they often fail to elicit concise reasoning or reduce accuracy on math-specialized models like Qwen2.5-Math-1.5B.
% This suggests that these models' internal representations are less amenable to simple prompt-based interventions.
% Similar trends are observed on the MATH dataset (\autoref{tab:zero_shot_prompting_math}).

\paragraph{Zero-shot prompting is less effective on task-specific models.}  
Zero-shot prompting methods show inconsistent performance across model families.
While some prompts are effective on general-purpose models like Llama-3.2, they fail to elicit concise reasoning on math-specialized models like Qwen2.5-Math (\autoref{tab:zero_shot_prompting_gsm8k}).
This suggests the internal representations of task-specific models are less responsive to zero-shot prompting.
Similar trends are observed on the MATH dataset in \autoref{tab:zero_shot_prompting_math}.
