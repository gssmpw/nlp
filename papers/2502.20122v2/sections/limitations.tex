\section*{Limitations and Future Work}

\subsection*{Advanced Training Schemes} While our focus was on standard fine-tuning with self-generated samples, exploring advanced reinforcement learning (RL) based training schemes could potentially maximize efficiency further.  Our preliminary experiments with expert iteration and BoN sampling (without a utility reward) showed promising results, boosting both accuracy and length reduction compared to non-iterative BoN.  Further investigation of iterative or more advanced RL methods is warranted.

\subsection*{Few-Shot Prompting Exploration} Although we leveraged few-shot prompting to bootstrap length reduction during data generation, we did not explore advanced few-shot prompting methods as a middle ground between zero-shot and fine-tuning.  While direct few-shot prompting with our prompts achieved comparable length reduction to fine-tuning on single-path few-shot generations, it incurred a slight accuracy loss.  Critically, fine-tuning enabled us to incorporate BoN sampling and sample augmentation, leading to significantly greater length reduction without sacrificing accuracy.  However, future work on integrating advanced exemplar selection \cite{fu2022complexity} and many-shot prompting \cite{agarwal2025many} could further enhance our fine-tuning approach.

\subsection*{Extended Scaling Studies} Our scaling study was limited to Llama 3.x models at 1B, 3B, and 8B parameters. While these model sizes are relevant for our task-specific setting, further empirical studies are needed to evaluate the effectiveness of our method on models exceeding 8B parameters.  Investigating scaling trends across a wider range of model sizes is crucial for understanding the full potential of our approach.

\subsection*{Concise Reasoning in General LLMs} This study focused on task-specific fine-tuning.  While existing zero-shot and fine-tuning methods often struggle with reliability and efficacy even in such settings, and our method proved effective, generalizing our approach to a broader range of tasks is an important direction.  Exploring techniques like multi-task training could enable efficient reasoning without task-specific tuning.  This could be particularly beneficial for `thinking' models \citep{guo2025deepseek} where reasoning enhances the final response, similar to our math reasoning tasks but in a more general context.  Furthermore, while prior work has explored reducing the number of reasoning stages \cite{chen2024not}, our method focuses on reducing verbal redundancy within sentences, offering a complementary approach.