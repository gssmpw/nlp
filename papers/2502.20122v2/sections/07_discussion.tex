\section{Discussion}
\label{discussion}

\paragraph{Default reasoning behavior of LLMs}
Previous research has shown that the CoT reasoning ability of LLMs originates from procedural knowledge in pretraining data \cite{ruis2024procedural}.
Modern LLM training pipelines utilize high-quality math, code, and synthetic reasoning data to enhance reasoning, but these do not promote conciseness \cite{dubey2024llama, yang2024qwen2}.
Furthermore, recent `thinking' models are reinforced to use additional tokens to improve reasoning performance, rather than save on token budget \cite{openai2024systemcard}.
Thus, current LLMs naturally exhibit redundant reasoning.
We believe that incorporating concise reasoning supervision or rewards in training pipelines can be beneficial for model efficiency, especially for `thinking' models with lengthy internal reasoning.

\paragraph{Lightweight fine-tuning for concise reasoning elicitation}
In this paper, we focused on standard fine-tuning based on self-generated samples.
Our analysis shows that LLMs \textit{already} possess the ability to correctly reason in a relatively concise manner.
Therefore, we posit that lightweight fine-tuning is sufficient to achieve significant reasoning length reduction, echoing the Superficial Alignment Hypothesis in the field of LLM alignment \cite{zhou2024lima}, 
Indeed, our post-hoc analysis in \autoref{fig:ft_length_scatter} shows that standard fine-tuning can reduce the output length of models in proportion to that of training samples.