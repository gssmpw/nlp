% \onecolumn

\input{sections/06_related_works}

\section{Methodological Details}
\label{appx_method_details}

\subsection{Self-Generation of Few-Shot Exemplars (FS-Self)}

We devise a systematic approach to obtain few-shot exemplars using the default prompt (Appendix \ref{appx_prompts_all_zero_shot}) of concise reasoning from the target model itself, motivated by BoN sampling, through a two-phase process: \textbf{sampling} and \textbf{selection}.

\paragraph{Sampling Phase}

In the sampling phase, 128 questions were randomly selected from the training set, and for each selected question, 128 diverse reasoning paths were generated with a sampling temperature of $T=0.7$. This process created a diverse set of candidate paths for both the GSM8K and MATH datasets.

\paragraph{Selection Phase}

The selection process was executed through the following steps:

\begin{enumerate}[leftmargin=0.5cm]
    \item Sorting candidate paths by token count in ascending order.
    \item Filtering only correct samples by comparing oracle labels with the models' answers using the parsing code (\autoref{appx_answer_parsing}).
    \item Sequential validation using GPT-4o with the following specialized prompt was performed until eight valid and concise examples were obtained. To ensure question uniqueness, alternative solutions from previously selected questions were excluded.
\end{enumerate}

\begin{quote}
You are a math reasoning validator. Given a math problem, its answer, and a proposed rationale:
First, determine if the problem requires step-by-step reasoning to solve.\\
Then, validate if the rationale contains explicit step-by-step reasoning that correctly solves the problem.\\
Question: [question here]\\
Expected Answer: [label here]\\
Proposed Rationale: [rationale here]\\
Respond with ONLY ACCEPT or REJECT:\\
ACCEPT - if the problem requires step-by-step reasoning AND the rationale contains explicit step-by-step reasoning that correctly leads to the answer.\\
REJECT - if the problem is too simple (can be solved in one step).\\
REJECT - if the rationale lacks explicit reasoning steps or contains incorrect logic.\\
REJECT - if the rationale only states the answer.
\end{quote}

\paragraph{Dataset-Specific Selection Criteria}

For GSM8K, eight valid examples were selected using the above selection phase. An example is shown in Table \ref{tab:fs_llama3_gsm8k}. For MATH, to avoid selecting only very easy questions, the selection was distributed across different categories: two examples from algebra (the largest category) and one example from each of the other categories (geometry, intermediate algebra, prealgebra, number theory, counting and probability, and precalculus). An example is shown in Table \ref{tab:fs_llama3_math}.

\subsection{Generation of Few-Shot Exemplars From GPT-4o (FS-GPT4o)}

To obtain few-shot exemplars of concise reasoning from GPT-4o, we use our zero-shot prompt \textbf{Hand Crafted 3} (Appendix \ref{appx_prompts_all_zero_shot}) to generate short reasoning paths. For GSM8K, we randomly sample eight questions from the training set. For MATH, we randomly select two questions from the largest category, Algebra and one from each of the other categories (Geometry, Intermediate Algebra, Prealgebra, Number Theory, Counting \& Probability, Precalculus), all from the training set.
We manually filter low-quality outputs to ensure the quality of few-shot examples. However, we found that GPT-4o reliably followed the zero-shot prompt in the majority of cases.
Generated examples of GPT-4o are in Tables \ref{tab:fs_gpt4o_ex_gsm8k} and \ref{tab:fs_gpt4o_ex_math}.


\section{Experimental Details}
\label{appx_experimental_details}

\subsection{Datasets}
\label{appx_datasets}

A summary of the datasets used in our experiments, along with their original licenses, is provided in \autoref{tab:math_gsm8k_statistics}. Both datasets are in English and focus on mathematical reasoning tasks. GSM8K contains grade school math word problems requiring multi-step reasoning, while MATH covers more advanced problems across various categories (algebra, geometry, precalculus, etc.) with different difficulty levels. Both datasets use straightforward language and standard mathematical notation. The original train/test splits are used for fine-tuning and evaluation. Results for MATH are reported on the MATH-500 subset, following the previous work \cite{lightman2023let}.

\input{tables/tab_appendix_dataset}

\subsection{Answer Parsing}
\label{appx_answer_parsing}
For accuracy, we extract and verify the final numerical answer using Python-based parsing code.
We build previous implementations for GSM8K \citep{kojima2022large} and MATH \citep{lai2024step}, with additional rules to ensure consistent parsing across all models. Our implementation additionally handles mathematical expressions by standardizing numerical formats, removing units and mathematical notation, and cleaning model-specific formatting to ensure consistent evaluation across models.

During our experiments, we observed that different models require different parsing approaches to extract answers correctly for the MATH dataset. We implemented a model-specific parsing strategy where Gemma-2-2b and Qwen2.5-3B models utilize an enhanced parser, while all other models (including Llama, DeepSeekMath, and Qwen2.5-Math) use our standard parser. This adjustment led to more accurate model performance evaluation. All experiments use the appropriate parser for each model to ensure consistent evaluation.

\subsection{Justification For Length Metric}
\label{appx_justification_length}
We define our length metric for model evaluation as the average number of \textit{output} tokens in model responses across the entire evaluation set.
For clear comparison, we do not include input tokens because (1) the number of input tokens do not differ significantly between zero-shot prompting and direct prompting on fine-tuned models, but more importantly (2) the real-world wall-clock latency incurred by each output token is significantly greater than that of each input token.
The greater latency of output tokens stems from the sequential nature of their processing (the decode stage).  Input tokens, in contrast, are processed in parallel (the prefill stage).
The decode stage is heavily memory-bound, requiring parameter fetching for every forward pass, which significantly limits compute utilization \cite{agrawal2024taming}.
This difference in processing is also reflected in the pricing structures of commercial LLM API providers, where input tokens are cheaper than output tokens\footnote{https://openai.com/api/pricing/}.

\subsection{Generation and Fine-Tuning}
\label{appx_generation_and_fine_tuning}
% - Greedy decoding for evaluation and temperature sampling with T=0.7, following \cite{cobbe2021training, wang2022self} for distribution analysis and training data generation
% - bf16 precision for generation and fine-tuning
% - update all model parameters for fine-tuning
We implemented all model fine-tuning using the HuggingFace Trainer library and conducted generation using vLLM for optimized inference. For evaluation, we use greedy decoding, while for distribution analysis and training data generation we use temperature sampling with $T$=0.7, following \citet{cobbe2021training, wang2022self}.
We generate up to 512 output tokens on the GSM8K dataset and up to 1024 output tokens on the MATH dataset, following the token limits established in prior works \cite{dubey2024llama, ren2024learn}.
For fine-tuning, we use a batch size of 16 and train for one epoch with a learning rate of 1e-5. The fine-tuning phase requires at maximum 469 training steps, making it computationally modest compared to the generation phase, as demonstrated in \autoref{tab:generation_training_time}.
For comparison with the iterative fine-tuning baseline MR \cite{de2024rational}, we limit to 4 iterations as the authors observe performance generally plateaus after iteration 4.
All experiments are conducted using bfloat16 precision for both generation and fine-tuning.
We conduct our experiments on 8 NVIDIA H100 GPUs.
Our total usage for main experiments is approximately 1,000 H100 GPU hours.

\section{Additional results}
\label{appx_additional_results}

\subsection{Full Analysis on Reasoning Length Distribution (\autoref{reasoning_length_distribution})}
\label{appx_reasoning_length_distribution}

We analyze the distribution of output lengths across different model families using three metrics. First, we examine the absolute output token count distribution, which directly reflects the computational cost during inference. Second, we analyze the raw character count distribution to verify our findings are consistent across different tokenization schemes. Third, we study the normalized output token count, where each solution's output length is divided by the mean output length of correct solutions for that specific question, allowing us to identify relatively concise solutions.

\autoref{fig:token_distribution} reveals significant variations in how different model families approach mathematical reasoning. On GSM8K, DeepSeekMath-7B demonstrates notably concise reasoning with a mean output length of 179 tokens, while Qwen2.5-3B uses substantially more tokens, averaging 298 tokens per solution. This pattern persists in the MATH dataset, though with generally longer output lengths due to the increased complexity of the problems.

\input{tables/tab_appendix_time_measurement}

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/length_distribution_tokens_gsm8k.pdf}
        \caption{GSM8K}
        \label{fig:token_distribution_gsm8k}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/length_distribution_tokens_math.pdf}
        \caption{MATH}
        \label{fig:token_distribution_math}
    \end{subfigure}
    \caption{Distribution of \textbf{absolute output token counts} across model families on GSM8K and MATH datasets. The distributions show significant variation in output token usage patterns between different models, with some models consistently using more tokens than others for correct solutions.}
    \label{fig:token_distribution}
\end{figure}

The presence of a small peak at 512 tokens in some distributions is an artifact of the maximum output length limit used during generation, following standard practices in the field.

\autoref{fig:char_distribution} confirms that our observations about output token usage patterns are not merely artifacts of different tokenization schemes. The relative patterns between models remain consistent, suggesting that the observed differences in output length are fundamental to how each model approaches problem-solving, rather than being an artifact of tokenization choices.

The character-level distributions also provide additional insight into the actual verbosity of solutions, as they measure the raw text length without the influence of tokenization strategies.


\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/length_distribution_characters_gsm8k.pdf}
        \caption{GSM8K}
        \label{fig:char_distribution_gsm8k}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/length_distribution_characters_math.pdf}
        \caption{MATH}
        \label{fig:char_distribution_math}
    \end{subfigure}
    \caption{Distribution of \textbf{character counts} across model families. The character-level analysis confirms that the observed patterns in output token usage are consistent when measured in raw UTF-8 characters, suggesting that the findings are not artifacts of different tokenization schemes.}
    \label{fig:char_distribution}
    \vspace{20pt}
\end{figure}

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/length_distribution_normalized_gsm8k.pdf}
        \caption{GSM8K}
        \label{fig:normalized_distribution_gsm8k}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/length_distribution_normalized_math.pdf}
        \caption{MATH}
        \label{fig:normalized_distribution_math}
    \end{subfigure}
    \caption{
    Distribution of \textbf{normalized output token counts}, where each solution's output length is divided by the mean output length of correct solutions for that question.
    The distributions reveal that models can produce valid solutions using significantly fewer tokens than their typical output length, though this capability varies across model families.
    }
    \label{fig:normalized_distribution}
    \vspace{10pt}
\end{figure}

\input{tables/tab_zero_shot_prompting_math}

\autoref{fig:normalized_distribution} shows a particularly interesting phenomenon. All models demonstrate the ability to solve problems with significantly fewer output tokens than their typical output length, though this capability varies across model families. The normalization process, which divides each solution's output length by the mean output length of correct solutions for that specific question, reveals that some models are more flexible in producing concise solutions than others.

The presence of significant probability mass below 100\% in the normalized distributions highlights an intriguing aspect of model behavior: while models may tend toward verbose outputs, they are not constrained to this verbosity and can successfully solve problems with more compact reasoning in some cases.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/correct_length_scatter.pdf}
%     \caption{\textbf{Alignment of correctly generated output length with fine-tuning data length.} Correlation between the relative length averaged over GSM8K and MATH across 4 models excluding DeepSeekMath-7B of train data and test output. Both train and test lengths are calculated after filtering out incorrect solutions.}
%     \label{fig:ft_length_scatter_correct}
% \end{figure}

\subsection{Full Analysis on Efficacy of Zero-Shot Prompting Methods (\autoref{zero_shot_prompting})}
\label{appx_zero_shot_prompting}

\autoref{tab:zero_shot_prompting_math} shows evaluation results for zero-shot prompting methods on MATH. Prompting methods exhibit similar limitations as those on the GSM8K dataset.

\subsection{Impact of Question-Wise Selection}
\label{appx_question_wise}
We compare the performance of FS-GPT4o-BoN when selecting the shortest samples per question versus selecting the shortest samples across all questions. Both methods are tested without augmentation. The average relative accuracy across all models and datasets is 99.03\% with question-wise selection and 96.03\% without it, while the average relative length is 70.54\% and 64.33\%, respectively.
Although selecting the shortest samples across all questions reduces relative length, it comes at the cost of accuracy. Notably, the accuracy of Gemma-2-2B drops significantly from 90.53\% with question-wise selection to 75.81\% without it. Therefore, we adopt question-wise selection to maintain question diversity and accuracy.



\subsection{Full Result Tables on Main Methods}
\label{appx_full_results}

We present full results on the accuracy and reasoning length elicited by main methods for GSM8K in \autoref{tab:main_results_full_gsm8k} and MATH in \autoref{tab:main_results_full_math}.
We show absolute accuracy and output token length for individual models and show average over relative accuracy and relative length for aggregates across models.



\subsection{Impact of Augmentation on Training Data Length}
\label{appx_augmentation_length}
\autoref{tab:augmentation_length} demonstrates the impact of augmentation on training data length.  Augmentation effectively expands the dataset by leveraging samples from zero-shot BoN. This is achieved by incorporating previously unsolvable long solutions, while also replacing solutions that can be solved with shorter solutions with zero-shot prompting. Consequently, the average solution length remains relatively stable, while the number of correct samples for fine-tuning increases, leading to improved model accuracy.

% \subsection{Analysis on Effectiveness of Fine-Tuning}
% \label{appx_length_scatter_correct}
% \autoref{fig:ft_length_scatter_correct} shows the correlation between the relative length of the training data and the relative length of the correctly generated test answers, closely following the identity line, indicating a strong alignment between the two.


\section{Sample Study}
\label{appx_sample_studies}

Tables~\ref{tab:appen_samples1}-\ref{tab:appen_samples4} present additional examples of generated reasoning paths from both the initial model and the model fine-tuned using FS-GPT4o-BoN. We primarily showcase results for the general model Llama-3.2-3b and the math-fine-tuned model Deepseek-7b, evaluated on the GSM8k and MATH500 datasets. One can observe a consistent reduction in token length in our fine-tuned models, achieved through decreased verbosity. This demonstrates the effectiveness of our method in improving inference efficiency.

\section{Prompts}
\label{appx_prompts}

\subsection{Zero-Shot Prompts}
\label{appx_prompts_all_zero_shot}

For each question, we use the following instructions as the system input to the language model if the model's chat template includes a system role. If no system role exists, we append the prompt to the user input instead.

\subsubsection{Prompting Template}
\label{appx_prompting_template}

We used the default chat template for all models except Llama. For Llama, we modified its default chat template by removing the automatically added date information:
\begin{verbatim}
Cutting Knowledge Date: December 2023
Today Date: 23 July 2024
\end{verbatim}
This modification was made to ensure fair comparison across all models by maintaining consistent system prompt formatting.

\subsubsection{System Inputs:}

\vspace{4pt}

\quad\textbf{Default \cite{pang2024iterative}}
\begin{quote}
Your task is to answer the question below. Give step by step reasoning before you answer, and when you’re ready to answer, please use the format \textquoteleft The answer is'.
\end{quote}

\textbf{Be Concise \cite{renze2024benefits}}  
\begin{quote}
Your task is to answer the question below. Give step by step reasoning before you answer, and when you’re ready to answer, please use the format \textquoteleft The answer is'. Be concise.
\end{quote}

\vspace{30pt}

\textbf{Est. Budget \cite{han2024token}}
\begin{quote}
\textbf{Budget Estimation:} Analyze the given question and estimate the minimum number of tokens required to generate a complete and accurate response. Please give the response by strictly following this format: [[budget]], for example, Budget: [[12]].
\end{quote}
\begin{quote}
\textbf{Token-budget-aware CoT:} Let's think step by step and use less than \{budget\_estimate\} tokens.
\end{quote}

\textbf{Fixed Budget \cite{nayab2024concise}}
\begin{quote}
Let’s think a bit step by step and limit the answer length to 100 words.
\end{quote}

\textbf{Hand Crafted 1 (ours)}  
\begin{quote}
Your task is to answer the question below. Give step by step reasoning before you answer, and when you’re ready to answer, please use the format \textquoteleft The answer is'. You don’t need unnecessary explanations; you can solve problems using only essential words and expressions. Summarize your thought process as simply as possible and provide your answer. Do not generate only the final answer.
\end{quote}

\textbf{Hand Crafted 2 (ours)}  
\begin{quote}
Your task is to answer the question below. Give step by step reasoning before you answer, and when you’re ready to answer, please use the format \textquoteleft The answer is'. Use less words and more compact expressions to be concise.
\end{quote}

\textbf{Hand Crafted 3 (ours)}  
\begin{quote}
Give concise step by step reasoning before you answer. Only retain key steps such as names, objects, numbers and mathematical operations. Use short plain words. Don't use any formatting such as emphasis, lists, or enumeration. Make sure that the intermediate results are presented in the order that they are calculated. After your reasoning, please use the format \textquoteleft The answer is' to answer.
\end{quote}

\vspace{50pt}

\textbf{Hand Crafted 4 (ours)}  
\begin{quote}
Your task is to answer the question below. Carefully solve the problem step by step, while using *as few words are possible*. Be careful about your choice of words. Use only necessary and essential steps. Avoid extra words. Avoid repetition. Avoid verbose statements. Avoid introductory remarks. When you’re ready to answer, please use the format \textquoteleft The answer is’.
\end{quote}

\subsubsection{User Input:}

\vspace{4pt}

\begin{quote}
Question: [question here]\\
Solution:
\end{quote}

\subsection{Few-Shot Prompts} 
\label{appxt_prompts_all_few_shot}
We provide 8-shot examples for each question, following the approach of \cite{wei2022chain}, without any system instructions given to the language model. Each example is formatted in a chat-based template that includes both user and assistant roles. The order of examples for each question is randomly shuffled. We present one representative example for each prompt type, while the full set of examples is available in our public repository. Table \ref{tab:fs_human_ex}-\ref{tab:fs_llama3_math} display one of eight examples used in FS-Human, FS-GPT4o, and FS-Self. 

\input{tables/tab_augmentation_length}
\input{tables/tab_main_results_full_gsm8k}
\input{tables/tab_main_results_full_math}

\input{tables/tab_appendix_examples}

\begin{table*}[t]
\caption{
    One of eight human-annotated examples of concise reasoning for solving GSM8K and MATH problems from \citet{wei2022chain}
    }
\begin{tabular}{p{0.95\textwidth}}
\toprule
    \textbf{Question:} There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\
    \textbf{Solution:} There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\
\bottomrule
\end{tabular}
\label{tab:fs_human_ex}
\end{table*}

\begin{table*}[t]
\caption{
    One of eight GPT-4o-generated examples of concise reasoning for GSM8K problems.
}
\begin{tabular}{p{0.95\textwidth}}
\toprule
    \textbf{Question:} Aaron pays his actuary membership fees each year. The membership fee increases yearly by \$10. If he pays \$80 in the first year, how much does his membership cost, in dollars, in the sixth year?\\[0.5em]
    \textbf{Solution:} 1. Start with the first year fee: \$80.\\
    2. The fee increases by \$10 each year.\\
    3. Calculate the fee for the second year: \$80 + \$10 = \$90.\\
    4. Calculate the fee for the third year: \$90 + \$10 = \$100.\\
    5. Calculate the fee for the fourth year: \$100 + \$10 = \$100.\\
    6. Calculate the fee for the fifth year: \$100 + \$10 = \$103.\\
    7. Calculate the fee for the sixth year: \$103 + \$10 = \$106.

    The answer is \$106.\\
\bottomrule
\end{tabular}
\label{tab:fs_gpt4o_ex_gsm8k}
\end{table*}

\begin{table*}[t]
\caption{One of eight GPT-4o-generated examples of concise reasoning for MATH problems.
}
\begin{tabular}{p{0.95\textwidth}}
\toprule
    \textbf{Question:} A convex polyhedron $P$ has $26$ vertices, $60$ edges, and $36$ faces, $24$ of which are triangular and $12$ of which are quadrilaterals. A space diagonal is a line segment connecting two non-adjacent vertices that do not belong to the same face. How many space diagonals does $P$ have?
    \\
    \textbf{Solution:} To find the number of space diagonals in the polyhedron, we first calculate the total number of line segments connecting any two vertices. This is given by the combination formula for choosing 2 vertices out of 26, which is:
    
    Total line segments = C(26, 2) = 26 * 25 / 2 = 325.
    
    Next, we subtract the number of edges and face diagonals from this total to find the number of space diagonals.
    
    The polyhedron has 60 edges, so we subtract these:
    
    325 - 60 = 265.
    
    Now, we need to subtract the diagonals that lie on the faces. 
    
    For the 24 triangular faces, there are no diagonals since a triangle has no diagonals.
    
    For the 12 quadrilateral faces, each quadrilateral has 2 diagonals. Therefore, the total number of face diagonals is:
    
    12 * 2 = 24.
    
    Subtract these face diagonals from the remaining line segments:
    
    265 - 24 = 241.
    
    Thus, the number of space diagonals is 241.
    
    The answer is 241.\\
\bottomrule
\end{tabular}
\label{tab:fs_gpt4o_ex_math}
\end{table*}

\begin{table*}[t]
\caption{
    One of eight self-generated examples of concise reasoning for GSM8K problems from Llama-3.2-3B.
}
\begin{tabular}{p{0.95\textwidth}}
\toprule
    \textbf{Question:} A pie shop charges \$5 for a slice of pie. They cut each whole pie into 4 slices. How much money will the pie shop make if they sell 9 pies?\\
    \textbf{Solution:} Here's the step-by-step reasoning:

    1. The pie shop cuts each whole pie into 4 slices, so they can sell 4 slices per pie.\\
    2. They sell 9 pies, so they can sell a total of 9 x 4 = 36 slices.\\
    3. Each slice costs \$5, so they will make 36 x \$5 = \$180.
    
    The answer is \$180.\\
\bottomrule
\end{tabular}
\label{tab:fs_llama3_gsm8k}
\end{table*}

\begin{table*}[t]
\caption{One of eight self-generated examples of concise reasoning for MATH problems from Llama-3.2-3B.
}
\begin{tabular}{p{0.95\textwidth}}
\toprule
    \textbf{Question:} When a polynomial is divided by $2x^2 - 7x + 18$, what are the possible degrees of the remainder?  Enter all the possible values, separated by commas.\\
    \textbf{Solution:} \#\# Step 1: Recall the division algorithm for polynomials.
    When a polynomial $f(x)$ is divided by another polynomial $g(x),$ the remainder must be of degree less than $g(x).$
    
    \#\# Step 2: Determine the degree of the divisor.
    The degree of $2x^2 - 7x + 18$ is $2.$
    
    \#\# Step 3: Determine the possible degrees of the remainder.
    Since the remainder must be of degree less than $2,$ the possible degrees of the remainder are $0$ and $1.$
    
    The final answer is: $boxed\{0,1\}$\\
\bottomrule
\end{tabular}
\label{tab:fs_llama3_math}
\end{table*}

\begin{table*}[b]
\vspace{320pt}
\end{table*}