\section{Method}
\label{method}

To address the limitations of zero-shot prompting, we propose a simple yet robust fine-tuning (FT) approach to reliably elicit concise reasoning in LLMs while preserving accuracy.
We focus on \textit{self-training} approaches, to unlock and refine the \textit{latent concise reasoning abilities} of current LLMs, observed in \autoref{reasoning_length_distribution}.
Self-training not only removes external dependencies, but we posit that it helps preserve reasoning capability, as the training data originates from the model's own distribution.

% Our aim is to unlock and refine the existing, but latent, concise reasoning abilities within large language models, both as a method for preserving existing capabilities and as a data generation strategy.
% To generate high-quality training data from the model itself,

% Specifically, we employ best-of-N (BoN) sampling to collect the most concise reasoning samples from the diverse output distribution of various models, corresponding to the probability mass toward the left side of the distributions shown in \autoref{fig:main_distribution_analysis}.
% We complement this with few-shot conditioning to bootstrap output length reduction for better sample efficiency.
% We elaborate on our methods below.

\subsection{Naive Best-of-N Sampling (\textit{BoN})}
\label{best_of_n}

We first consider naive BoN sampling to collect relatively concise reasoning samples from the original output distribution, corresponding to the probability mass toward the left side of the distribution in \autoref{fig:main_distribution_analysis}, for fine-tuning.
Specifically, we generate $N$ reasoning paths for each question in the original training dataset and select the shortest \textit{correct} reasoning path for each question\footnote{Note that questions without any correct reasoning paths are excluded from the fine-tuning dataset.}.
In contrast to selecting the shortest subset of correct reasoning paths from a combined pool of questions, our question-wise selection scheme ensures supervision across a wide range of difficulty levels, as difficult questions may require longer absolute reasoning lengths. Corresponding experimental results are in \autoref{appx_question_wise}.
% We apply BoN before fine-tuning because it is too computationally expensive to use directly during inference, where our aim is to maximize inference efficiency.

% Best-of-N  sampling has proven effective across diverse tasks by leveraging the power of search. 
% % SFT
% When combined with fine-tuning it can efficiently align models to maximize rewards by increasing the probability of successful cases.
% % Applied to our setting
% In our setting, we define the reward as strict correctness and minimal length, where the best sample is the shortest reasoning path that leads to a correct answer.
% % formulation
% Specifically, let $p$ be a language model and $x$ be the input question, and $S = \{s_i\}_0^{N}$ be $N$ possible reasoning paths for $x$, sampled through $s_i \sim 
%  p(\cdot|x)$.
% We define a correctness function $C(s_i, x)$ such that:
% \begin{equation}
% \label{eq:correctness}
%     C(s_i, x) = \begin{cases} 
%         1, & \text{if $s_i$ leads to a correct answer} \\
%         0, & \text{otherwise}
%     \end{cases}
% \end{equation}

% Let $L(s_i)$ denote the length of the reasoning path $s_i$. Our reward function $R(s_i, x)$ combines correctness and length:
% \begin{equation}
% \label{eq:correctness}
%     R(s_i, x) = C(s_i, x) \cdot \frac{1}{1 + len(s_i)},
% \end{equation}

% \noindent where the inverse of the length ensures that shorter paths yield higher rewards, but only if they are correct.

% The Best-of-N sampling process begins by generating $N$ reasoning paths to have $S$ and then select the one with highest reward, which can be formulated as below:
% \begin{equation}
% \label{eq:BoN}
%     s_{\text{best}} = \arg\max_{s_i} R(s_i, p)
% \end{equation}

% In SFT-BoN, after selecting $s_{\text{best}}$, the language model $L$ is fine-tuned to increase the probability $L(s_{\text{best}}|p)$.
% % meta reasoning
% % Effect
% In this way, without any human supervision, we can encourage models to prefer shorter reasoning paths while maintaining their ability to generate correct solutions.

\subsection{Few-Shot Conditioning For Effective Reduction}
\label{few_shot}
While naive BoN sampling is a straightforward approach, its sample inefficiency \cite{xiang2025towards} makes it infeasible to achieve significant length reduction beyond a certain point.
\autoref{fig:bon_sample_efficiency} shows a log-linear relationship between $N$ and output length reduction, indicating that length reduction through BoN incurs exponential generation costs.
% that increasing the number of samples does not significantly reduce the number of tokens in the selected shortest samples.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/sample_efficiency_plot.pdf} % Adjust width as needed
    \caption{Sample efficiency of generation methods for self-training. Average relative length of reasoning paths collected by each method from 64 random questions in the training set. Relative length here is measured compared to the first path generated for naive BoN (only correct paths). Results are averaged over GSM8K and MATH across 4 models excluding DeepSeekMath-7B.}
    \label{fig:bon_sample_efficiency} % Label for referencing in text
\end{figure}

\paragraph{Few-shot conditioned sampling (\textit{FS})}
% Alternatively, we can consider in-context learning to bootstrap the reduction in output lengths, as it has proven effective for guiding the generation behavior of LLMs according to specific styles and formats \cite{brown2020language}, based on given few-shot examples.
To mitigate the sample inefficiency of naive BoN, we can leverage few-shot prompting to bootstrap the reduction in output length.
We consider three sources for few-shot exemplars: human-annotation (\textit{FS-Human}), proprietary frontier LLMs (\textit{FS-GPT4o}), and self-generated samples (\textit{FS-Self}).
We use human annotated examples from \citet{wei2022chain}, as they are readily available and demonstrate very concise reasoning.
We provide additional details on few-shot example acquisition in \autoref{appx_method_details}.

% Consequently, providing short reasoning examples in-context can improve sample efficiency in obtaining correct, concise reasoning paths. 
% Specifically, instead of naive sampling $s_i \sim p(\cdot | x)$, we sample using $s_i \sim p(\cdot | x, E)$, where $E$ represents the in-context short reasoning examples.  
% We then apply Eq.~\ref{eq:BoN} to generate SFT-BoN data.

We find that few-shot prompting reliably elicits concise generations across all models considered in our study.
As few-shot learning is a fundamental capability of LLMs \citep{brown2020language}, we expect this to further generalize to other models.
Horizontal lines in \autoref{fig:bon_sample_efficiency} shows that sampling with 8-shot conditioning reduces the length of reasoning paths significantly.
Notably, FS-Human elicits more reduction than that achieved by BoN sampling with $N=256$.
% Directly applying few-shot conditioning during inference is possible, but it slightly reduces performance and restricts the use of additional FS-BoN and augmentation methods discussed later.

\paragraph{Few-shot conditioned BoN sampling (\textit{FS-BoN})}
We can maximize length reduction by jointly applying BoN sampling on top of few-shot conditioning.
We consider GPT-4o generated examples for this approach (\textit{FS-GPT4o-BoN}), based on their strong accuracy preservation and length reduction in standalone few-shot conditioned self-training (see \textit{FS-GPT4o} in \autoref{tab:main_results}).
\autoref{fig:bon_sample_efficiency} shows that the improvements from few-shot conditioning and BoN sampling are largely independent and additive, resulting in significant length reduction.

\paragraph{Why do we need (self-)training?}
Directly applying BoN and few-shot prompting at test time, without self-training, incurs prohibitive inference overhead due to repeated sampling and long prompts (\autoref{tab:generation_training_time}).
This defeats our ultimate goal of mitigating the inference costs of reasoning.
On the other hand, self-training internalizes the length reduction benefits, allowing the model to reason concisely without additional inference-time overhead. % directly without the need for costly sampling or length prompts during inference.

\subsection{Sample Augmentation for Accuracy Boost}
\label{sample_augmentation}

% While few-shot conditioning elicits short reasoning, it may sturggle with complex questions requiring longer reasoning paths.
% Also, providing few-shot examples for all difficulty levels is impractical. 
% This approach retains concise few-shot conditioned samples for simpler cases while incorporating zero-shot solutions for harder ones, preserving accuracy with significant length reduction. 
% This approach retains shorter samples for simpler cases while incorporating longer solutions for harder ones, preserving accuracy with significant length reduction. 
While few-shot prompting elicits concise reasoning, its adaptability is limited due to the small number of given examples.
% While few-shot conditioning elicits short reasoning across all question, it is difficult to provide appropriate examples across all difficulty levels, thus limiting adaptability.
It may (1) prohibit generation of correct paths for very complex questions that require longer reasoning paths, while (2) eliciting extraneous steps that are not required for very easy questions.
To address this, for each question, we augment the set of \{1, $N$\} sample(s) generated for FS and FS-BoN, respectively, with $N$ samples generated for naive BoN, and select the shortest correct path from the combined set.
We find that this retains the length reduction from FS and FS-BoN while better preserving accuracy, likely due to better coverage of difficult questions.
We apply this augmentation by default to few-shot conditioned methods, with its effectiveness ablated in \autoref{fig:main_methods_comparison}.