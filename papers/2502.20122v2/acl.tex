% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt,dvipsnames]{article}
% namgyu - dvipsnames added for xcolor

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint ]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% namgyu - Colored box thing
\usepackage{mdframed}


\usepackage{enumitem}  % namgyu - itemize margins
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{amssymb}    % tergel - math symbol
\usepackage{enumitem}  % namgyu - enumerate margin

\newcommand{\red}[1]{\textcolor{BrickRed}{#1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% following few-shot CoT paper
% \title{Self-Training Elicits Concise Chain-of-Thought Reasoning\\in Large Language Models}
\title{
Self-Training Elicits Concise Reasoning in Large Language Models
% \vspace{-8pt}
}
% following zero-shot CoT paper
% \title{Large Language Models Are Concise Reasoners}
% \title{Large Language Models Can Teach Themselves to Reason Concisely}
% \title{Large Language Models Can Self-Teach\\Concise Chain-of-Thought Reasoning}
% \title{Large Language Models Can Reason Concisely}

% \title{Chain-of-Thought Compression:\\Self-Taught Concise Reasoning in Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\makeatletter
\renewcommand{\@fnsymbol}[1]{\ifcase#1\or *\else \@ctrerr\fi}
\makeatother
\author{
  Tergel Munkhbat\thanks{Equal contribution.\quad${}^\dagger$Corresponding author.}
  \hspace{4pt}
  Namgyu Ho${}^\text{*}$
  \hspace{4pt}
  Seo Hyun Kim${}^\text{*}$
  \hspace{4pt}
  \textbf{Yongjin Yang
  \hspace{6pt}
  Yujin Kim
  \hspace{6pt}
  Se-Young Yun${}^\dagger$}\hspace{4pt}
  \vspace{2pt} \\
  KAIST AI
  \vspace{2pt} \\
  \texttt{\{tergelmunkhbat, itsnamgyu, shkimsally, yunseyoung\}@kaist.ac.kr} \\
  % \vspace{-2pt} \\
  \url{https://github.com/TergelMunkhbat/concise-reasoning}
  % \\\And
  % Second Author \\
  % Affiliation / Address line 1 \\
  % Affiliation / Address line 2 \\
  % Affiliation / Address line 3 \\
  % \texttt{email@domain} \\
  }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

% namgyu - uppercase autoref!
\renewcommand{\figureautorefname}{Figure}
\renewcommand{\tableautorefname}{Table}
\renewcommand{\partautorefname}{Part}
\renewcommand{\appendixautorefname}{Appendix}
\renewcommand{\chapterautorefname}{Chapter}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}

\begin{document}
\maketitle

\begin{abstract}
\input{sections/00_abstract}
\end{abstract}

\input{sections/01_introduction}
\input{sections/02_preliminary}
\input{sections/03_method}
\input{sections/04_experimental_setup}
\input{sections/05_results}
% \input{sections/06_related_works}
\input{sections/07_discussion}
\input{sections/08_conclusion}

\input{sections/limitations}

\input{sections/ack}

\clearpage

\bibliography{custom}

\clearpage

\appendix
\input{sections/appendix}







\clearpage

% \section{OLD DRAFTS}

% % \textbf{To assess redundancy in reasoning chains, it is essential to consider factors tied to the inherent capabilities of each pretrained model.}
% % What appears redundant for one model may be essential for another, as the the reasoning capacity during each forward pass varies across models.
% % Depending on the model dimension, each model has distinct computation capacity (FLOPs) per forward pass and storage size within the hidden space, i.e., dimension of KV states.
% % Moreover, the ability to utilize this architectural capacity to perform reasoning is determined by the specific training pipeline used to optimize each model.
% % To accurately assess redundancy in reasoning chains, it is therefore crucial to account for the unique reasoning capacities of the model in question.
% % -> move to discussion?

% \textbf{To this end, we perform a preliminary analysis on the unmodified output distribution of pretrained LMs, to identify redundancy while accounting for the models' inherent capabilities (Section 3).}
% Our analysis on the distribution of reasoning chain lengths and task performance show that \textbf{current models can maintain performance while using relatively few intermediate tokens, compared to the mean distribution.}
% This is, the minimum length is significantly shorter than the average length of valid reasoning chains.
% This suggests that there exists significant redundancy in the default reasoning chains of current models, which can be avoided.
% Therefore, it may be possible to elicit this behavior through lightweight adaptation, such as prompting or fine-tuning.

% (Later parts to be revised)

% We investigate zero-shot prompting \cite{renze2024benefits}, few-shot prompting and fine-tuning. For the latter, we fine-tune the model on concise reasoning chains generated by the model itself. To control for performance benefits of fine-tuning, we start with the self-distillation baseline, also known as rejection sampling fine-tuning (RFT) \cite{yuan2023scaling}.

% We consider two methods to obtain data on concise reasoning chains for few-shot prompting and fine-tuning (\autoref{sec:data}).
% First, we generation multiple reasoning chains for each question, and subsample the shortest chains.
% Second, we rephrase the generated reasoning chains into concise ones, via few-shot prompting using few manually constructed examples.

% We focus on tasks such as mathematical reasoning, where the answer is short and easily verifiable, in line with many previous work on LM reasoning \cite{wei2022chain}.
% In these tasks, intermediate CoT reasoning is mainly used as an instrument to obtain the final response, trading inference cost (extra tokens) for performance.
% We aim to reduce the cost of this instrument, without sacrificing performance.
% In contrast, we do not consider tasks where the response itself is long-form, such as novel storytelling, where conciseness may not always be desirable.

% \section{Preliminary Analysis}

% \newmdenv[
%   backgroundcolor=red!5,
%   topline=false,
%   bottomline=false,
%   rightline=false,
%   leftline=true,
%   linecolor=red!75!black,
%   linewidth=2pt,
%   skipabove=10pt,
%   skipbelow=10pt,
%   innerleftmargin=10pt,
%   innerrightmargin=10pt,
%   innertopmargin=10pt,
%   innerbottommargin=10pt,
% ]{custombox}

% \begin{custombox}
% \textbf{\textcolor{red!75!black}{Insights}}
% \end{custombox}

% Analysis on sampled reasoning chains from unmodified output distributions of current LMs.
% \textbf{Models: one variant of Llama and Gemma, respectively.}


\end{document}
