\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
\begin{tabular}{l|rrllrrll}
\toprule
\textbf{Dataset} & \multicolumn{4}{c}{\textbf{GSM8K}} & \multicolumn{4}{c}{\textbf{MATH}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-5} \cmidrule(lr){6-9}
\textbf{Method} & Acc & Len & Rel. Acc & Rel. Len & Acc & Len & Rel. Acc & Rel. Len \\
\midrule
\multicolumn{9}{l}{\textit{Zero-Shot Prompting}} \\
\midrule
\hspace{12pt}Baseline & 78.06 & 241.87 & 100.00 \small{(0.00)} & 100.00 \small{(0.00)} & 46.40 & 480.37 & 100.00 \small{(0.00)} & 100.00 \small{(0.00)} \\
\hspace{12pt}Be Concise & 77.98 & 214.87 & 99.85 \small{(1.18)} & 88.46 \small{(10.37)} & 47.76 & 446.09 & 102.71 \small{(7.59)} & 92.66 \small{(7.46)} \\
\hspace{12pt}Hand Crafted 2 (ours) & 76.72 & 184.13 & 98.27 \small{(3.67)} & 77.10 \small{(22.27)} & 46.84 & 404.85 & 101.62 \small{(4.79)} & 85.26 \small{(15.97)} \\
\midrule
\multicolumn{9}{l}{\textit{FT - External Data}} \\
\midrule
\hspace{12pt}Direct Answer & 19.70 & 3.17 & 24.88 \small{(5.03)} & 1.36 \small{(0.40)} & 15.08 & 6.98 & 35.16 \small{(10.34)} & 1.44 \small{(0.73)} \\
\hspace{12pt}Human CoT & 65.73 & 127.85 & 83.82 \small{(7.28)} & 54.95 \small{(13.17)} & 33.88 & 243.54 & 75.61 \small{(13.56)} & 53.14 \small{(13.87)} \\
\hspace{12pt}GPT4o CoT & 76.36 & 156.24 & 97.65 \small{(3.63)} & 67.60 \small{(16.70)} & 40.44 & 399.80 & 90.52 \small{(15.07)} & 87.21 \small{(22.22)} \\
\midrule
\multicolumn{9}{l}{\textit{FT - Best-of-N Self-Generation}} \\
\midrule
\hspace{12pt}Naive BoN & 77.12 & 214.22 & 98.79 \small{(1.64)} & 87.17 \small{(8.79)} & 47.64 & 433.26 & 101.74 \small{(7.04)} & 89.89 \small{(3.99)} \\
\hspace{12pt}Rational Metareasoning & 76.15 & 207.49 & 97.21 \small{(5.74)} & 84.93 \small{(5.09)} & 47.56 & 432.56 & 103.02 \small{(6.56)} & 90.56 \small{(5.25)} \\
\midrule
\multicolumn{9}{l}{\textit{FT - Few-Shot Conditioned Self-Generation (ours)}} \\
\midrule
\hspace{12pt}FS-Human & 76.66 & 161.72 & 98.06 \small{(3.28)} & 67.96 \small{(16.62)} & 46.44 & 421.54 & 99.69 \small{(6.97)} & 87.78 \small{(5.98)} \\
\hspace{12pt}FS-GPT4o & 78.07 & 175.54 & 99.94 \small{(1.69)} & 73.15 \small{(13.49)} & 47.36 & 421.21 & 101.87 \small{(5.33)} & 87.58 \small{(6.60)} \\
\hspace{12pt}FS-Self & 77.27 & 190.03 & 98.86 \small{(2.51)} & 77.51 \small{(9.18)} & 48.00 & 426.67 & 102.67 \small{(5.24)} & 88.50 \small{(4.49)} \\
\midrule
\multicolumn{9}{l}{\textit{FT - Few-Shot Conditioned Best-of-N Self-Generation (ours)}} \\
\midrule
% GPT4o Best-of-16 (Naive) & 75.48 & 153.51 & 96.56 \small{(3.79)} & 64.12 \small{(16.35)} & 47.28 & 367.49 & 101.50 \small{(9.81)} & 76.96 \small{(11.42)} \\
\hspace{12pt}FS-GPT4o-BoN & 75.88 & 153.38 & 97.00 \small{(4.11)} & 64.25 \small{(16.66)} & 47.36 & 364.33 & 102.56 \small{(6.24)} & 76.30 \small{(10.56)} \\
\hspace{24pt}\raisebox{0.5ex}{$\llcorner$}\hspace{4pt}\textit{Budget-Matched} & 76.24 & 160.59 & 97.44 \small{(3.67)} & 67.15 \small{(16.41)} & 47.52 & 384.43 & 101.58 \small{(9.53)} & 80.43 \small{(9.04)} \\

\bottomrule
\end{tabular}
    }
    \caption{
        Evaluation of zero-shot prompting, fine-tuning (FT), and our proposed methods on GSM8K and MATH datasets, averaged across five main models.
        We report absolute accuracy (\%) and length (tokens), along with relative accuracy (\%) and length (\%) compared to baseline prompting. Values in parentheses show standard deviations.
    }
    \label{tab:main_results}
\end{table*}