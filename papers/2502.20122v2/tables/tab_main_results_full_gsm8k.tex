\begin{table*}[h!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|rr|rrrrrrrrrr}
    \toprule
    & \multicolumn{2}{c}{\textbf{Avg. Relative}} & \multicolumn{2}{c}{\textbf{Ll-3B}} & \multicolumn{2}{c}{\textbf{Ge-2B}} & \multicolumn{2}{c}{\textbf{Qw-3B}} & \multicolumn{2}{c}{\textbf{QM-1.5B}} & \multicolumn{2}{c}{\textbf{DM-7B}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
    \textbf{Method} & Acc & Len & Acc & Len & Acc & Len & Acc & Len & Acc & Len & Acc & Len \\
    \midrule
    \multicolumn{13}{l}{\textit{Zero-Shot Prompting}} \\
    \midrule
    \hspace{12pt}Baseline & 100.00 & 100.00 & 77.26 & 220.00 & 65.43 & 200.09 & 84.38 & 301.05 & 82.79 & 305.91 & 80.44 & 182.29 \\
    \hspace{12pt}Be Concise & 99.85 & 88.46 & 78.24 & 168.35 & 64.22 & 163.14 & 83.78 & 258.16 & 82.94 & 304.14 & 80.74 & 180.58 \\
    \hspace{12pt}Hand Crafted 2 & 98.27 & 77.10 & 79.15 & 143.58 & 62.62 & 144.94 & 78.77 & 146.42 & 82.49 & 303.50 & 80.59 & 182.21 \\
    \midrule
    \multicolumn{13}{l}{\textit{FT - External Data}} \\
    \midrule
    \hspace{12pt}Direct Answer & 24.88 & 1.36 & 17.36 & 2.12 & 11.30 & 3.42 & 22.90 & 3.43 & 22.97 & 3.46 & 23.96 & 3.42 \\
    \hspace{12pt}Human CoT & 83.82 & 54.95 & 65.73 & 102.01 & 47.99 & 130.58 & 69.83 & 136.65 & 69.67 & 137.21 & 75.44 & 132.81 \\
    \hspace{12pt}GPT4o CoT & 97.65 & 67.60 & 78.17 & 141.88 & 60.27 & 156.94 & 80.97 & 156.76 & 82.34 & 160.79 & 80.06 & 164.84 \\
    \midrule
    \multicolumn{13}{l}{\textit{FT - Best-of-N Self-Generation}} \\
    \midrule
    \hspace{12pt}Naive BoN & 98.79 & 87.17 & 77.94 & 182.08 & 63.91 & 179.22 & 81.96 & 287.58 & 81.20 & 287.06 & 80.59 & 135.15 \\
    \hspace{12pt}Metareasoning & 97.21 & 84.93 & 79.00 & 185.98 & 57.24 & 158.45 & 82.34 & 261.19 & 81.35 & 282.86 & 80.82 & 148.96 \\
    \midrule
    \multicolumn{13}{l}{\textit{FS - Few-Shot Prompting}} \\
    \midrule
    \hspace{12pt}FS-Human & 96.18 & 64.37 & 77.71 & 115.14 & 59.74 & 155.41 & 78.09 & 233.80 & 79.00 & 111.19 & 81.27 & 141.84 \\
    \hspace{12pt}FS-GPT4o & 97.52 & 77.88 & 77.03 & 146.81 & 60.35 & 168.01 & 79.15 & 296.32 & 83.02 & 163.29 & 81.73 & 158.39 \\
    \hspace{12pt}FS-Self & 96.49 & 77.42 & 74.98 & 144.65 & 61.26 & 142.22 & 80.44 & 293.61 & 80.14 & 232.30 & 80.14 & 140.00 \\
    \midrule
    \multicolumn{13}{l}{\textit{FT - Few-Shot Conditioned Self-Generation (ours)}} \\
    \midrule
    \hspace{12pt}FS-Human & 98.06 & 67.96 & 78.85 & 131.68 & 60.96 & 164.60 & 82.41 & 249.17 & 80.89 & 132.78 & 80.21 & 130.37 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}No Aug & 96.19 & 64.93 & 77.71 & 113.61 & 59.44 & 154.48 & 78.92 & 236.44 & 79.38 & 119.79 & 80.52 & 142.40 \\
    \midrule
    \hspace{12pt}FS-GPT4o & 99.94 & 73.15 & 78.92 & 150.51 & 63.76 & 167.37 & 84.31 & 263.97 & 82.56 & 163.85 & 80.82 & 132.02 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}No Aug & 96.58 & 79.07 & 77.48 & 153.16 & 59.06 & 169.85 & 77.56 & 299.60 & 82.11 & 162.22 & 81.43 & 160.94 \\
    \midrule
    \hspace{12pt}FS-Self & 98.86 & 77.51 & 78.47 & 142.74 & 62.09 & 159.44 & 82.94 & 270.71 & 82.34 & 243.01 & 80.52 & 134.24 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}No Aug & 95.59 & 77.28 & 76.95 & 142.21 & 55.57 & 138.69 & 79.15 & 294.31 & 82.41 & 233.47 & 80.52 & 142.82 \\
    \midrule
    \multicolumn{13}{l}{\textit{FT - Few-Shot Conditioned Best-of-N Self-Generation (ours)}} \\
    \midrule
    \hspace{12pt}FS-GPT4o-BoN & 97.00 & 64.25 & 77.79 & 119.89 & 59.29 & 152.92 & 80.82 & 245.40 & 80.74 & 124.56 & 80.74 & 124.14 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}Budget-Matched & 97.44 & 67.15 & 76.95 & 127.93 & 59.67 & 157.78 & 82.64 & 255.19 & 81.05 & 134.31 & 80.89 & 127.76 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}No Aug & 96.56 & 64.12 & 76.42 & 119.92 & 60.12 & 147.41 & 78.70 & 249.33 & 81.20 & 126.43 & 80.97 & 124.44 \\
    \bottomrule
    \end{tabular}
    }
    \caption{
    Full results for main methods on GSM8K.
    Relative accuracy (\%) and length (\%) compared to default prompting are shown, averaged over five main models: Llama-3.2-3B, Gemma-2-2B, Qwen2.5-3B, Qwen2.5-Math-1.5B, and DeepSeekMath-7B.  
    Absolute accuracy (\%) and length (tokens) are reported for each individual model.  
    }
    \label{tab:main_results_full_gsm8k}
\end{table*}