\begin{table*}[h!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|rr|rrrrrrrrrr}
    \toprule
    & \multicolumn{2}{c}{\textbf{Avg. Relative}} & \multicolumn{2}{c}{\textbf{Ll-3B}} & \multicolumn{2}{c}{\textbf{Ge-2B}} & \multicolumn{2}{c}{\textbf{Qw-3B}} & \multicolumn{2}{c}{\textbf{QM-1.5B}} & \multicolumn{2}{c}{\textbf{DM-7B}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
    \textbf{Method} & Acc & Len & Acc & Len & Acc & Len & Acc & Len & Acc & Len & Acc & Len \\
    \midrule
    \multicolumn{13}{l}{\textit{Zero-Shot Prompting}} \\
    \midrule
    \hspace{12pt}Baseline & 100.00 & 100.00 & 44.40 & 511.75 & 24.00 & 375.99 & 56.80 & 605.45 & 69.80 & 551.36 & 37.00 & 357.32 \\
    \hspace{12pt}Be Concise & 102.71 & 92.66 & 47.20 & 477.47 & 22.20 & 307.06 & 60.20 & 541.92 & 68.00 & 550.23 & 41.20 & 353.77 \\
    \hspace{12pt}Hand Crafted 2 & 101.62 & 85.26 & 46.60 & 445.85 & 24.00 & 275.69 & 54.80 & 393.80 & 68.80 & 543.72 & 40.00 & 365.17 \\
    \midrule
    \multicolumn{13}{l}{\textit{FT - External Data}} \\
    \midrule
    \hspace{12pt}Direct Answer & 35.16 & 1.44 & 11.80 & 5.85 & 11.40 & 4.46 & 17.80 & 4.86 & 17.80 & 14.81 & 16.60 & 4.91 \\
    \hspace{12pt}Human CoT & 75.61 & 53.14 & 26.80 & 233.95 & 21.20 & 240.38 & 38.20 & 218.88 & 49.40 & 273.08 & 33.80 & 251.39 \\
    \hspace{12pt}GPT4o CoT & 90.52 & 87.21 & 35.80 & 381.65 & 26.00 & 410.97 & 56.60 & 395.13 & 49.20 & 407.29 & 34.60 & 403.96 \\
    \midrule
    \multicolumn{13}{l}{\textit{FT - Best-of-N Self-Generation}} \\
    \midrule
    \hspace{12pt}Naive BoN & 101.74 & 89.89 & 47.20 & 433.42 & 21.60 & 341.47 & 60.00 & 559.09 & 70.00 & 521.26 & 39.40 & 311.08 \\
    \hspace{12pt}Metareasoning & 103.02 & 90.56 & 46.20 & 421.56 & 23.60 & 348.31 & 56.20 & 539.82 & 69.60 & 508.67 & 42.20 & 344.43 \\
    \midrule
    \multicolumn{13}{l}{\textit{FS - Few-Shot Prompting}} \\
    \midrule
    \hspace{12pt}FS-Human & 93.70 & 81.35 & 30.00 & 349.32 & 21.80 & 317.33 & 62.40 & 531.56 & 65.80 & 444.21 & 39.20 & 306.41 \\
    \hspace{12pt}FS-GPT4o & 100.52 & 86.76 & 42.60 & 393.43 & 23.00 & 345.45 & 62.20 & 580.58 & 66.20 & 418.44 & 39.40 & 333.16 \\
    \hspace{12pt}FS-Self & 103.28 & 85.30 & 46.20 & 407.70 & 20.60 & 323.20 & 64.40 & 572.48 & 71.80 & 473.90 & 40.80 & 287.19 \\
    \midrule
    \multicolumn{13}{l}{\textit{FT - Few-Shot Conditioned Self-Generation (ours)}} \\
    \midrule
    \hspace{12pt}FS-Human & 99.69 & 87.78 & 42.00 & 411.29 & 22.20 & 357.87 & 59.80 & 538.92 & 68.00 & 502.23 & 40.20 & 297.41 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}No Aug & 95.29 & 83.09 & 31.80 & 329.92 & 22.20 & 315.34 & 63.00 & 547.94 & 70.40 & 489.79 & 37.20 & 313.66 \\
    \midrule
    \hspace{12pt}FS-GPT4o & 101.87 & 87.58 & 45.20 & 397.24 & 22.80 & 351.49 & 59.20 & 560.48 & 69.20 & 495.32 & 40.40 & 301.53 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}No Aug & 102.88 & 86.90 & 41.60 & 379.03 & 23.40 & 342.62 & 63.40 & 580.10 & 69.20 & 456.91 & 41.60 & 323.86 \\
    \midrule
    \hspace{12pt}FS-Self & 102.67 & 88.50 & 45.60 & 429.66 & 22.60 & 346.29 & 60.40 & 561.20 & 71.60 & 498.16 & 39.80 & 298.05 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}No Aug & 103.06 & 85.88 & 46.40 & 426.78 & 21.00 & 317.65 & 64.00 & 577.18 & 70.80 & 474.14 & 40.40 & 286.59 \\
    \midrule
    \multicolumn{13}{l}{\textit{FT - Few-Shot Conditioned Best-of-N Self-Generation (ours)}} \\
    \midrule
    \hspace{12pt}FS-GPT4o-BoN & 102.56 & 76.30 & 43.60 & 316.12 & 24.00 & 313.52 & 60.60 & 523.00 & 67.40 & 378.64 & 41.20 & 290.39 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}Budget-Matched & 101.58 & 80.43 & 43.40 & 350.71 & 21.40 & 332.53 & 62.00 & 536.95 & 69.00 & 405.66 & 41.80 & 296.30 \\
    \hspace{12pt}\raisebox{0.5ex}{$\llcorner$}\hspace{8pt}No Aug & 101.50 & 76.96 & 44.40 & 321.27 & 21.40 & 331.72 & 60.40 & 529.90 & 67.60 & 374.96 & 42.60 & 279.59 \\ 
    \bottomrule
    \end{tabular}
    }
    \caption{
    Full results for main methods on MATH.
    Relative accuracy (\%) and length (\%) compared to default prompting are shown, averaged over five main models: Llama-3.2-3B, Gemma-2-2B, Qwen2.5-3B, Qwen2.5-Math-1.5B, and DeepSeekMath-7B.  
    Absolute accuracy (\%) and length (tokens) are reported for each individual model.  
    }
    \label{tab:main_results_full_math}
\end{table*}