


\begin{comment}
    



Our simulation framework consists of the following.
\begin{itemize}
    \item \texttt{QKeras:} to train 2-bit quantized deep learning models.
    \item \texttt{PyCARL\cite{pycarl}:} to generate spiking inference models.
    \item \texttt{Brian 2~\cite{stimberg2019modeling}:} for \astro{} modeling.
    \item \texttt{ARES~\cite{reagen2018ares}:} for fault simulations.
    \item \texttt{Xilinx Vivado:} for FPGA synthesis.
\end{itemize}
%Figure~\ref{fig:framework} illustrates the framework we use to evaluate the proposed design methodology. A model is trained using the training data. The trained model is partitioned into clusters. Clusters are analyzed for built-in error resilience using the \texttt{ARES}-based fault injection framework, biological model of \astro{}, and test images. The \astro{}-enabled model is implemented on the proposed \ftn{} hardware, which is designed using \astro{} \ckts{} for two baseline designs (crossbar-based and \mubrain{}-based). Faults are injected randomly (using \texttt{ARES}) to verify the \ftn{} implementation.

Evaluated models are described in Table~\ref{tab:apps}. We also report the number of clusters generated using the software framework for \mubrain{} and crossbar-based designs. The difference between the number of clusters for these two designs is because of the difference in their neuron and synaptic capacity (see Table~\ref{tab:area}).

\begin{figure*}[t!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=2.19\columnwidth]{images/astro_blocks.pdf}}
	%\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Block designs of an astrocyte.}
	%\vspace{-15pt}
	\label{fig:astro_blocks}
\end{figure*}

%\vspace{-10pt}
\begin{table}[h!]
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{2pt}
	\caption{Models used to evaluate the proposed design.}
	\label{tab:apps}
	%\vspace{-10pt}
	\centering
	\begin{threeparttable}
	{\fontsize{6}{10}\selectfont
	    %\vspace{-10pt}
		\begin{tabular}{cc|ccc|c}
			\hline
			 & & & \multicolumn{2}{c|}{\textbf{Culsters for}} & \\ \cline{4-5}
			\textbf{SDCNN} &
			\textbf{Dataset} &
			\textbf{Parameters} & \textbf{\mubrain{}-based design} & \textbf{Crossbar-based design} & \textbf{Accuracy}\\
			\hline
			LeNet & CIFAR-10 & 505,942 & 30 & 31 & 86.3\%\\
			AlexNet & CIFAR-10 & 41,241,906 & 2,370 & 2,518 & 66.4\%\\
			VGGNet & CIFAR-10 & 32,850,250 & 1,888 & 2,006 & 81.4 \%\\
			ResNet & CIFAR-10 & 575,298 & 34 & 36 & 57.4\%\\
			DenseNet & CIFAR-10 & 7,047,754 & 405 & 431 & 46.3\%\\
			MobileNet & CIFAR-10 & 2,280,586 & 132 & 140 & 47.3\%\\
			Xception & CIFAR-10 & 20,881,970 & 1,200 & 1,275 & 47.5\%\\
			\hline
	\end{tabular}}
	\end{threeparttable}
\end{table}
%\vspace{-10pt}

%We simulate the self-repair properties of \astro{} using the Brian 2 simulator~\cite{stimberg2019modeling}. \texttt{QKeras} is used to train and test deep learning inference models
%with 2-bit quantized synaptic weights. 

%All parameters used in our evaluation and extra simulation results are provided in this anonymous GitHub link \href{https://anonymous.4open.science/r/dac326}{https://anon\-ymous.4open.science/r/dac326}. 
%These can be viewed as supplemental, but we include them for reviewers. %Upon acceptance, the author note, \astro{} simulation code, and the VHDL code will be released to foster future research.

Table~\ref{tab:astro_params} reports the simulation parameters obtained from~\cite{liu2018exploring}. We model these parameters in Brian 2 simulator~\cite{stimberg2019modeling} to explore the self-repair property of an \astro{}.

\begin{table}[h!]
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{2pt}
	\caption{Astrocyte simulation parameters~\cite{liu2018exploring}.}
	\label{tab:astro_params}
	%\vspace{-10pt}
	\centering
	\begin{threeparttable}
	{\fontsize{6}{10}\selectfont
	    %\vspace{-10pt}
		\begin{tabular}{c|c||c|c||c|c||c|c}
			\hline
			\textbf{Parameter} & \textbf{Value} & \textbf{Parameter} & \textbf{Value} & \textbf{Parameter} & \textbf{Value} & 
			\textbf{Parameter} & \textbf{Value} \\
			\hline
			$\tau_{AG}$ & $10 s$ & $r_{AG}$ & $0.8\mu Ms^{-1}$ & $\tau_{Glu}$ & $100ms$ & $r_{Glu}$ & $10\mu Ms^{-1}$\\
			$t_{Ca}$ & $0.3\mu M$ & $\tau_{eSP}$ & $40s$ & $m_{eSP}$ & $55,000$ & $K_{AG}$ & $-4000$\\
			\hline
	\end{tabular}}
	\end{threeparttable}
\end{table}

Table~\ref{tab:fault_params} reports the fault modeling parameters obtained from~\cite{espine}.
\begin{table}[h!]
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{1pt}
	\caption{Fault modeling parameters~\cite{espine}.}
	\label{tab:fault_params}
	%\vspace{-10pt}
	\centering
	\begin{threeparttable}
	{\fontsize{6}{10}\selectfont
	    %\vspace{-10pt}
		\begin{tabular}{c|c||c|c||c|c||c|c}
			\hline
			\textbf{Parameter} & \textbf{Value} & \textbf{Parameter} & \textbf{Value} & \textbf{Parameter} & \textbf{Value} & 
			\textbf{Parameter} & \textbf{Value} \\
			\hline
			$I_{cell}$ & $100\mu A$ & $R_{cell}$ & $10K\Omega$ & $l$ & $120nm$ & $V$ & $V = 4\times 10^{-14} cm^3$\\
			$T_{amb}$ & $300K$ & $\gamma$ & $1000$ & $m_{eSP}$ & $55,000$ & $C$ & $1.25 JK^{-1} cm^{-3}$\\
			\hline
	\end{tabular}}
	\end{threeparttable}
\end{table}

\subsection{Design Implementation}\label{sec:implementation}
Figure~\ref{fig:astro_blocks} shows the block diagram of a digital \astro{}~\cite{haghiri2020digital} implemented using a reduced fixed-point representation while still maintaining physiologically realistic precision.
The designed structure is a parallel architecture that takes less processing time.
The output provider unit (OPU) generates the retrograde feedback signal that facilitates frequency restoration at a faulty synaptic site.
The frequency reconstruction error can be controlled by choosing the right \astro{} coupling coefficients.
We use the piece-wise linear model to create efficient FPGA-based architecture.
The number of bits used for particular variables is determined by the required precision, processing performance, and resource use.
We use a 42-bit fixed point with 2 bit used for sign, 20 bits for integer, and 20 bits for fractional components.

The role of an astrocyte in regulating and modulating the excitation current of a neurons can be seen in Fig.~\ref{fig:memory_potential}.

\begin{figure}[h!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{images/memory_potential.pdf}}
	%\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Memory potential.}
	%\vspace{-15pt}
	\label{fig:memory_potential}
\end{figure}

A key element of the design is the piece-wise linear approximation function, which is illustrated in Figure~\ref{fig:pwl}.

FPGA illustrated acceptable performance with a very low error value between the proposed hardware and simulation results.

\begin{figure}[h!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{images/pwla.pdf}}
	%\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Piece-Wise Linear Approximation Function.}
	%\vspace{-15pt}
	\label{fig:pwl}
\end{figure}

We implemented the \astro{} design, the baseline \mubrain{} and crossbar designs on Xilinx VCU128 development board (see Table~\ref{tab:area_astro}). 
We observe that although an \astro{} \ckt{} is smaller than the size of a \mubrain{} (336 neurons) and crossbar (256 neurons), it is in fact, significantly larger and consumes significantly higher power than a single neuron \ckt{}. Furthermore, an \astro{} \ckt{} uses more flip flops (FF), slices, and lookup tables (LUTs) than the two baseline designs. The higher area of the two baseline designs are due to the use of more block RAMs (BRAMs). 
The power consumption of an \astro{} design is shown in Figure~\ref{fig:astro_power}, distributed into clocks, signals, logic, DSP, BRAM, MMCM, and I/O. 

\begin{figure}[t!]
		\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=1.00\columnwidth]{images/astro_power.pdf}}
	%\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Power consumption of astrocyte, distributed into clocks, signals,BRAMs,DSPs,MMCM, and I/Os.}
	%\vspace{-15pt}
	\label{fig:astro_power}
\end{figure}

\subsection{Model Area}
Table~\ref{tab:area} reports the design area for each of the evaluated deep learning inference models using 1) model replication technique~\cite{ponzina2021e2cnns}, 2) redundant mapping technique~\cite{yuan2021improving}, and 3) the proposed design methodology (both co-design and platform-based design). Design areas are reported for both the \mubrain{}-based core~\cite{sentryos} and the crossbar-based core~\cite{dynapse}. All results are normalized to the \mubrain{}-based design implementing the LeNet model using the model replication technique. We make the following three key observations.

%\vspace{-5pt}
\begin{table}[h!]
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{2pt}
	\caption{Implementation of an \astro{} and the baseline \mubrain{}~\cite{sentryos} and crossbar~\cite{dynapse} designs on Xilinx VCU128.}
	\label{tab:area_astro}
	%\vspace{-10pt}
	\centering
	\begin{threeparttable}
	{\fontsize{6}{10}\selectfont
	    %\vspace{-10pt}
		\begin{tabular}{c|ccc}
		    \hline
		    & \textbf{\mubrain{}~\cite{sentryos}} & \textbf{Crossbar~\cite{dynapse}} & \textbf{Astrocyte}\\
		    \hline
		    Neurons & 336 & 256 & --\\
		    Synapses & 17,408 & 16,384 & --\\
		    \hline
		    Operating Frequency & 100MHz & 100MHz & 100MHz \\
		    BRAM & 48 & 32 & 4 \\
		    DSP & 0 & 0 & 4 \\
		    FF & 129 & 86 & 2,368 \\
		    Slice & 117 & 78 & 670 \\
		    LUT & 114 & 76 & 1,345 \\
		    \hline
		    FPGA Utilization & 49\% & 40\% & 12\%\\
		    \hline
		    Power & 4.64W & 4.53W & 0.538 W \\
			\hline
	    \end{tabular}}
	\end{threeparttable}
	%\vspace{12pt}
	%\vspace{-10pt}
\end{table}
%\vspace{-10pt}

%\vspace{-10pt}
\begin{table}[h!]
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{2pt}
	\caption{Design area compared to model replication~\cite{ponzina2021e2cnns} and redundant mapping~\cite{yuan2021improving}.}
	\label{tab:area}
	%\vspace{-10pt}
	\centering
	\begin{threeparttable}
	{\fontsize{6}{10}\selectfont
	    %\vspace{-10pt}
		\begin{tabular}{c|cc|cc|cc|cc}
		    \hline
		    & \multicolumn{2}{|c|}{\textbf{Model}} & \multicolumn{2}{|c|}{\textbf{Redundant}} & \multicolumn{4}{|c}{\textbf{Proposed Design}}\\\cline{6-9}
		    & \multicolumn{2}{|c|}{\textbf{Replication~\cite{ponzina2021e2cnns}}} & \multicolumn{2}{|c|}{\textbf{Mapping~\cite{yuan2021improving}}} & \multicolumn{2}{|c|}{\textbf{Co-Design}} & \multicolumn{2}{|c}{\textbf{Platform-based}}\\
			\cline{2-9}
			& \textbf{\mubrain{}} & \textbf{crossbar} & \textbf{\mubrain{}} & \textbf{crossbar} & \textbf{\mubrain{}} & \textbf{crossbar} & \textbf{\mubrain{}} & \textbf{crossbar} \\
			\hline
			LeNet & 1.0 & 0.8 & -- & 0.7 & 0.5 & 0.4 & 0.6 & 0.4 \\
			AlexNet & 79.0 & 68.5 & -- & 54.8 & 39.2 & 33.1 & 45.6 & 36.5 \\
			VGGNet & 62.9 & 54.6 & -- & 43.7 & 31.2 & 26.4 & 36.4 & 29.1 \\
			ResNet & 1.1 & 0.9 & -- & 0.8 & 0.6 & 0.5 & 0.6 & 0.5 \\
			DenseNet & 13.5 & 11.7 & -- & 9.4 & 6.7 & 5.7 & 7.8 & 6.2 \\
			MobileNet & 4.4 & 3.8 & -- & 3.0 & 2.2 & 1.8 & 2.5 & 2.0 \\
			Xception & 40 & 34.7 & -- & 27.7 & 19.9 & 16.8 & 23.1 & 18.5 \\
			\hline
	    \end{tabular}}
	\end{threeparttable}
	%\vspace{12pt}
	%\vspace{-10pt}
\end{table}
%\vspace{-10pt}

First, design area is larger for models with higher number of parameters (see Table~\ref{tab:apps}). This is because models with more parameters require more clusters (cores), which increases the design area. 
Second, the redundant mapping technique is only applicable to crossbar-based designs.
%only and therefore, . Therefore, results for the \mubrain{}-based design are not provided.
Third, for the \mubrain{}-based design, the proposed co-design (platform-based design) approach results in 50\% (42.1\%) lower area than the replication technique. For the crossbar-based design, the proposed co-design (platform-based design) approach results in 51.6\% (46.7\%) lower area than the replication technique and 39.5\% (33.3\%) lower area than the redundant mapping technique. These improvements are because implementing a few \astro{s} in a baseline \mubrain{} and crossbar designs is area-efficient than 1) replicating model clusters, which requires more cores to implement a model, and 2) redundant mapping technique, which requires larger crossbars to implement each cluster.

\subsection{Model Power}
Figure~\ref{fig:power} reports the power for each evaluated model on a crossbar-based design using the four evaluated approaches. Power numbers for each core is calculated based on the static power of the design and the activation of the synaptic weights in the core. We make two key observations. First, power is higher for models such as AlexNet, VGGNet, and Xception due to higher number of model parameters. Second, on average, power using the proposed co-design (platform-based) approach is 60\% (50\%) lower than replication technique and 50\% (40\%) lower than redundant mapping technique. For \mubrain{}-based design (not shown here for space limitations), power using the proposed co-design (platform-based) approach is 60\% (44\%) lower than the replication technique.
\begin{figure}[h!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{images/power.pdf}}
	%\vspace{-15pt}
	%\caption{An example of spiking neural network.}
	\caption{Power consumption.}
	%\vspace{-10pt}
	\label{fig:power}
\end{figure}

Figure~\ref{fig:disable_astro} shows the power reduction on disabling unused astro\-cytes in \mubrain{} and crossbar-based designs with platform-based design approach. Results are normalized to our baseline implementation where a \mubrain{} (crossbar) core has 6 (4) \astro{s}. We observe that disabling unused \astro{s} results in an average 17\% (12\%) lower power for the \mubrain{} (crossbar) based design. Alexnet, which has very little built-in error resilience, end up using all \astro{s} of the hardware. So there is only marginal power reduction. 

\begin{figure}[h!]
	\centering
	%\vspace{-5pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{images/disable_astro.pdf}}
	%\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Power reduction due to disabling \astro{s}.}
	%\vspace{-15pt}
	\label{fig:disable_astro}
\end{figure}


\subsection{Fault Tolerance}
Figure~\ref{fig:accuracy_final} plots the accuracy, normalized to the replication technique, of each evaluated model for 10\%, 20\%, and 50\% of parameters with error. These errors are injected randomly using the \texttt{ARES} framework and the reported results are average of 10 runs.
With 10\% error rate, there are only a few errors per cluster. Therefore, most errors can be masked by \astro{s} that are inserted into each model cluster. So, we see no accuracy drop. With higher error rates, the accuracy is lower. This is because of the increase in parameter errors in each cluster. 
%Since each \astro{} encloses multiple neurons, 
Errors in multiple neurons of an enclosed \astro{} impact its ability to restore the spike frequency, causing a significant amount of accuracy drop. On average, the accuracy is 23\% and 54\% lower than the model replication technique for error rate of 20\% and 50\%, respectively.

\begin{figure}[h!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{images/accuracy.pdf}}
	%\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Normalized accuracy for different error rates.}
	%\vspace{-20pt}
	\label{fig:accuracy_final}
\end{figure}

\subsection{Design Tradeoffs}
Figure~\ref{fig:area_scalability} shows the area of a \mubrain{}-based design normalized to the replication technique for three error rates -- 10\%, 20\%, and 30\%. 
The accuracy constraint is set as the accuracy without error.
This accuracy constraint is achieved for 10\% error rate using our baseline design (sec.~\ref{sec:pdm}). So there is no area overhead. For 20\% and 50\% error rates, more \astro{s} are needed to achieve the accuracy constraint. On average, the proposed design requires 28\% and 49\% higher area for 20\% and 50\% error rate, respectively. 
%This increase is due to the increase in the number of \astro{s} needed to achieve the desired degree of \ftc{}.

\begin{figure}[h!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{images/area_scalability.pdf}}
	%\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Normalized area for different error rates.}
	%\vspace{-15pt}
	\label{fig:area_scalability}
\end{figure}

\end{comment}