%Table~\ref{tab:apps} reports the number of neurons and synapses of the evaluated deep learning models. 
Deep learning models may have thousands of neurons and synapses.
A key research question is: \textbf{are all these neurons and synapses susceptible to errors introduced due to hardware failures?} 
To analyze the built-in error resilience of a model, we use the \texttt{ARES} framework to inject a random synaptic error in a model and evaluate its accuracy drop on a test set~\cite{reagen2018ares}.\footnote{In our model, a synaptic (neuron) error is introduced by memory (logic) failures.}
%%, while a neuron error by logic failures.
%\footnote{In our evaluation, we use quantized deep learning models. An error is defined as the change of a synaptic weight from its trained value to another quantized value.} 
The experiment is repeated 100 times.
%To evaluate the impact of error,%See Section~\ref{sec:failures} for other error types we evaluate.
%To evaluate the accuracy impact of introducing errors in a trained machine learning model, 
%we conduct an experiment where a single error is injected to a randomly selected synaptic weight 
%parametter of the model 
%resulting in a change from its trained value.\footnote{In our evaluation, we use quantized deep learning models. A synaptic error is defined as the change of a synaptic weight from its trained value to another quantized value. See Section~\ref{sec:failures} for other error types we evaluate.} The model with the changed weight is evaluated on a test set and the experiment is repeated 100 times.
Figure~\ref{fig:app_accuracy_drop} plots these results for ResNet and MobileNet, two example models. We make two key observations.

\begin{figure}[h!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{images/app_accuracy_drop.pdf}}
	%\vspace{-15pt}
	%\caption{An example of spiking neural network.}
	\caption{Accuracy drop of ResNet and MobileNet with errors injected using the \texttt{ARES} framework~\cite{reagen2018ares}.}
	%\vspace{-10pt}
	\label{fig:app_accuracy_drop}
\end{figure}

First, not all errors lead to accuracy drop. This means that deep learning models are to a certain extent resilient to errors.
Second, between ResNet and MobileNet, MobileNet has fewer synaptic weights that are error-resilient.
%(see Figure~\ref{fig:accuracy_degradation}). 
Therefore, we see a significant accuracy drop for most errors. For ResNet, we see an accuracy drop only when an error affects a non-resilient synaptic weight.
To further expand on this,
%We observe that not all errors lead to accuracy degradation.
%This means that some neurons and synaptic weights are resilient to errors.
%we consider 2-bit quantized version of these models. 
Figure~\ref{fig:accuracy_degradation} reports the fraction of trained synaptic weights 
%(i.e., model parameters) 
in the first and last (dense) layers of seven models that leads to no accuracy drop when a single random error is injected. 
% We use the Aris framework to 
% %parameter values are altered due to hardware failures.
% % of less than 1\%, between 1\% and 10\%, between 10\% and 20\%, and more than 20\%.
% We simulated this behavior by inducing a change in each model parameter in the specific layer and analyzing the accuracy of the model by testing it on a test set (see Section~\ref{sec:evaluation}).

\begin{figure}[h!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{dac2022/images/accuracy_drop.pdf}}
	%\vspace{-15pt}
	%\caption{An example of spiking neural network.}
	\caption{Fraction of the trained weights that leads to no accuracy drop due to error injection.}
	%\vspace{-10pt}
	\label{fig:accuracy_degradation}
\end{figure}

We observe that an average 37\% of synaptic weights in the first layer and 30\% in the last layer are resilient to errors.
%, i.e., they do not lead to accuracy drop when their values are altered. 
Therefore, these synaptic weights do not need \ftc{}.

To provide more insight to this error-resilient behavior, Figure~\ref{fig:error_tolerance} illustrates the operation of 4 input leaky-integrate-and-fire (LIF) neurons connected to an output LIF neuron via four synapses. Input spikes raise the membrane voltage of the output neuron, which fires an output spike when its membrane voltage crosses a threshold. We illustrate the impact of a change in the synaptic weight \ineq{w_4}, connecting input \ineq{i_4} with output \ineq{o}. In this example, we show three quantized states of the synaptic weight \ineq{w_4} --- \ineq{w_x, w_y}, and \ineq{w_z}.%, and one virtual state \ineq{w_r}, which represents the condition where \ineq{w_4} is removed during pruning.

We observe that an output spike is generated for all three quantized states of \ineq{w_4} (shown as outputs \ineq{o_x}, \ineq{o_y}, and \ineq{o_z}). This means that the synaptic weight \ineq{w_4} is tolerant to memory failures. On the other hand, an output spike is generated even when the input neuron \ineq{i_3} fails to generate a spike. The corresponding membrane voltage is shown with the dashed line and the output as \ineq{o_r} in the figure. This means that the LIF neuron \ineq{i_3} is tolerant to logic failures.
%If the connection \ineq{w_4} is removed, no spike is generated. However, if there is a memory failure, 

\begin{figure}[h!]
	\centering
	%\vspace{-10pt}
	\centerline{\includegraphics[width=0.99\columnwidth]{images/motivation_error_tolerance_2.pdf}}
	%\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Built-in error tolerance of a model.}
	%\vspace{-10pt}
	\label{fig:error_tolerance}
\end{figure}

In our design methodology (Sec.~\ref{sec:design_methodology}), we exploit 
this built-in error resilience to insert the minimum number of \astro{s} in a model. %.of deep learning inference models that are needed for fault tolerance.
%in implementing them on the proposed \ftn{} \nm{} design.
%To identify precisely which neurons and synapses in a machine learning model are resilient to errors, an {exhaustive search} is necessary. The process involves assessing the criticality of each neuron (synapse) by injecting an error and determining the accuracy of the model using a test set. This process is of high time complexity. We propose a more scalable solution in Section~\ref{sec:design_methodology}.