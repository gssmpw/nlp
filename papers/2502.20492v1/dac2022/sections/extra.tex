To evaluate the impact of error,
%To evaluate the accuracy impact of introducing errors in a trained machine learning model, 
we conduct an experiment where a single error is injected to a randomly selected synaptic weight 
%parametter of the model 
resulting in a change from its trained value.\footnote{In our evaluation, we use quantized deep learning models. A synaptic error is defined as the change of a synaptic weight from its trained value to another quantized value. See Section~\ref{sec:failures} for other error types we evaluate.} The model with the changed weight is evaluated on a test set and the experiment is repeated 100 times.
Figure~\ref{fig:app_accuracy_drop} plots these results for ResNet and MobileNet. We make the following two key observations.

\begin{figure}[h!]
	\centering
	\vspace{-10pt}
	\centerline{\includegraphics[width=1.09\columnwidth]{images/app_accuracy_drop.pdf}}
	\vspace{-10pt}
	%\caption{An example of spiking neural network.}
	\caption{Accuracy drop of ResNet and MobileNet with randomly injected errors.}
	\vspace{-5pt}
	\label{fig:app_accuracy_drop}
\end{figure}

First, between ResNet and MobileNet, MobileNet has fewer synaptic weights that are error resilient (see Figure~\ref{fig:accuracy_degradation}). Therefore, we see a significant accuracy drop for most experiments. For ResNet, we see a large accuracy drop only when an error hits a non resilient synaptic weight.