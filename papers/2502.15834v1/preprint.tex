
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}



\title{Challenges of Multi-Modal Coreset Selection for Depth Prediction}

% Challenges of Multi-Modal Coreset Selection with Depth Prediction
% Coreset Selection Does Not Go Multi-Modal

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Viktor Moskvoretskii \\
Skoltech, HSE University\\
\texttt{vvmoskvoretskii@gmail.com} \\
\And
Narek Alvandian \\
Independent Researcher
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Coreset selection methods are effective in accelerating training and reducing memory requirements but remain largely unexplored in applied multimodal settings. We adapt a state-of-the-art (SoTA) coreset selection technique for multimodal data, focusing on the depth prediction task.
Our experiments with embedding aggregation and dimensionality reduction approaches reveal the challenges of extending unimodal algorithms to multimodal scenarios, highlighting the need for specialized methods to better capture inter-modal relationships.
\end{abstract}

\section{Introduction}

Modern deep learning systems require massive datasets demanding hundreds of gigabytes and even terabytes of storage \cite{imagenet} \cite{laion} as well as substantial computational resources for training. To address these computational challenges, researchers have developed coreset selection methods\cite{coreset1}\cite{coreset2}\cite{coreset3} â€” techniques for identifying the minimal subsets of training data that maintains model performance on a level of a model trained on the full dataset. 

However, many real-world applications, from medical diagnosis \cite{SALVI2024102134} to autonomous vehicles \cite{autonomous1} \cite{autonomous3} \cite{autonomous4} to Multi-Modal Foundation Models \cite{bachmann20244m21anytoanyvisionmodel}, require processing multiple modalities of data simultaneously. These multimodal scenarios not only amplify the computational demands but also introduce new challenges, as traditional coreset selection methods cannot be directly applied.
In this work, we extend SoTA coreset selection techniques to handle multimodal data, specifically investigating the adaptation of \citet{zhou2023datasetquantization}'s approach. 
Through extensive experimentation on depth prediction tasks, we demonstrate the limitations of current approaches and the need for specialized multimodal coreset selection methods for better modeling inter-modal relationships. We provide a code for reproducing experiments.\footnote{ \url{https://github.com/VityaVitalich/MultiModalCoreset}}

\section{Method}

We adapt the coreset selection method from previous studies \citep{zhou2023datasetquantization}, successfully applied to unimodal data, to the multimodal setting. The goal is to select a representative subset \( S \) that retains the diversity and informativeness of the original dataset \( D \). Let \( D = \{(\{x^m_i\}_{m=1}^M, y_i)\}_{i=1}^N \) denote a dataset with \( N \) samples and \( M \) modalities, where \( x^m_i \) represents the features of the \( i \)-th sample for the \( m \)-th modality, and \( y_i \) is the corresponding output. For the ease of notation, further $x_i$ will denote multimodal object $\{x^m_i\}_{m=1}^M$.

The objective is to select a coreset \( S \subseteq D \) of size \( |S| = M \ll N \) that minimizes the downstream task loss:
\[
S = \underset{S \subseteq D, |S| = M}{\text{argmin}} \ \mathcal{L}_{\text{downstream}}(S),
\]
where \( \mathcal{L}_{\text{downstream}}(S) \) is the task-specific loss incurred when training on \( S \).

Following previous approaches, we employ a submodular gain function~\citep{iyer2021submodular} generalized to multimodal data, denoted as \( P(x_i) \), to measure the importance of a multimodal sample \( x_i \) in maximizing the retained information. The gain for adding \( x_i \) to the current subset \( S_{i-1} \) is:
\[
P(x_i) = \sum_{p \in S_{i-1}} ||f(p) - f(x_i)||^2 - \sum_{p \in D \setminus S_{i-1}} ||f(p) - f(x_i)||^2,
\]
where \( f(x) \) is the embedding of a \textbf{multimodal} sample \( x \) in feature space, \( S_{i-1} \) is the current subset of size \( i-1 \), and \( D \setminus S_{i-1} \) represents the remaining samples.

The dataset \( D \) is divided into non-overlapping bins \( \{S_1, S_2, \ldots, S_N\} \) through recursive selection by maximizing submodular gain $x_k \gets {\text{argmax}} \ P(x)$ with ${x \in D \setminus \bigcup_{j=1}^{n-1} S_j}$

To enhance diversity, we follow previous studies~\citep{zhou2023datasetquantization} and uniformly sample from these bins, ensuring that even the most recently selected bin contributes equally to the final coreset.

\section{Experimental Procedure}

\textbf{Dataset:} We use the CLEVR dataset~\citep{johnson2016clevrdiagnosticdatasetcompositional}, where multimodal inputs consist of RGB image and semantic mask, the target is a depth map from Omnidata~\citep{eftekhar2021omnidata}.

\textbf{Model:} We employ MultiMAE backbone, with input and output adapters trained following the original paper~\citep{bachmann2022multimaemultimodalmultitaskmasked} and DPT output adapter~\citep{Ranftl_2021_ICCV}, trained for 40 epochs with batch size 128, with best checkpoint selected. Other technical details could be found in Appendix~\ref{sec:appendix_technical}.

\textbf{Coreset Selection:} We extract embeddings $f(x)$ from the MultiMAE transformer, as the DPT output adapter's feature map dimension is too large. All coresets are 20\% of the original dataset, obtained with $N=20$. We evaluate the following baselines: \textbf{Full}: Complete dataset used for training as a reference. \textbf{Random Coreset}: A random 20\% subset. \textbf{Token Aggregation}: Concatenation, mean, or sum of embeddings. \textbf{Dimensionality Reduction}: Applying PCA or UMAP~\citep{mcinnes2020umapuniformmanifoldapproximation} to concatenation of tokens before coreset selection.

\begin{table}[t!]
\centering
\caption{Percentage of quality retained relative to the Full Dataset for Validation RMSE, Validation Loss, and Training Loss, evaluated after training with coresets selected using each method.}
\label{tab:percentage_retained}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Aggregation}  & \textbf{Dimension} & \textbf{Val RMSE, \%} & \textbf{Val Loss, \%} & \textbf{Train Loss, \%} \\ 
\midrule
Full Dataset & - & - & 100.00 & 100.00 & 100.00 \\

\midrule
Random Coreset  & - & - & 50.23  & 46.33  & 55.32 \\

\midrule
\multirow{3}{*}{Coreset} & Concat & 301.824 & 49.08  & 47.41  & 51.81 \\
 & Mean & 768 & 51.54  & 47.90  & 52.63 \\
  & Sum & 768 & 51.11  & 52.12  & 54.47 \\

\midrule
\multirow{4}{*}{Coreset w/ PCA} & Concat & 512 & 47.73  & 45.43  & 45.37 \\
 & Concat & 1024 & 55.93  & 50.00  & 54.55 \\
 & Concat & 2048 & 47.90  & 43.23  & 52.46 \\
 & Concat & 4096 & 44.00  & 36.50  & 51.77 \\
\midrule

\multirow{2}{*}{Coreset w/ UMAP} & Concat & 512 & 49.29  & 49.23  & 47.44 \\
 & Concat & 1024 & 50.53  & 45.27  & 48.31 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\section{Results \& Discussion}

Our results in Table~\ref{tab:percentage_retained} show that coreset selection methods lead to a 50\% performance drop compared to the full dataset, with minimal improvement over random coreset. The best performance is achieved with PCA (1024 features), but the improvement is incremental, and UMAP shows no consistent gain.

We observe worse convergence relative to the full dataset, likely due to reduced data representativeness, making coresets behave similarly to random selection. Attempts to use a linear output adapter for bottleneck embeddings failed, confirming the necessity of the DPT adapter for depth prediction.

\section{Conclusion}

We address the challenge of multimodal coreset selection, essential for modern applications, and present an adaptation of SoTA coreset selection method to the multimodal setting. Testing on depth prediction reveals close to random selection, highlighting the need for further exploration of multimodal coreset selection techniques.






\bibliography{iclr2025}
\bibliographystyle{iclr2025}

\newpage 

\appendix
\section{Technical Details} \label{sec:appendix_technical}


Training was performed using the Adam optimizer with learning rate \( 5 \times 10^{-5} \), \(\beta_1 = 0.9\), \(\beta_2 = 0.99\), no weight decay and cosine annealing scheduler. The training was conducted on an NVIDIA A100 GPU.


% \begin{table}[t!]
% \centering
% \caption{Comparison of RMSE, Validation Loss, and Training Loss across methods.}
% \label{tab:results}
% \begin{adjustbox}{max width=\textwidth}
% \begin{tabular}{lccc}
% \toprule
% \textbf{Method} & \textbf{RMSE}    & \textbf{Validation Loss} & \textbf{Training Loss} \\ 
% \midrule
% Full Dataset    & 0.0033& 0.0015 & 0.0030\\
% Random Coreset  & 0.0065& 0.0033 & 0.0055\\
% Coreset, Full L2& 0.0066& 0.0032 & 0.0058\\
% Coreset, Mean Cosine & 0.0063& 0.0032 & 0.0057\\
% Coreset, Sum Cosine  & 0.0064& 0.0029 & 0.0055\\
% PCA (512)& 0.0068& 0.0033 & 0.0066\\
% UMAP (1024)     & 0.0065& 0.0033 & 0.0062\\
% UMAP (512)& 0.0066& 0.0031 & 0.0064\\
% PCA (1024) & 0.0059 & 0.0030 & 0.0055 \\
% PCA (2048)& 0.0068& 0.0035 & 0.0057\\
% PCA (4096)& 0.0074& 0.0042 & 0.0058\\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table}



\end{document}
