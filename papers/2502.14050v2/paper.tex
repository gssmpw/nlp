\documentclass[]{fairmeta}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm,algorithmic}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{multirow}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{setspace}
\usepackage{xcolor, colortbl}
\definecolor{Gray}{gray}{0.9}
\usepackage{inconsolata}
\usepackage{url}
\usepackage{amsfonts,amssymb}
\usepackage{mathrsfs}
\usepackage{newfloat}
\usepackage{listings}
%\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{array}
% \usepackage[hypcap=false]{caption}
\usepackage[justification=centering]{caption}
\usepackage{nicefrac} % compact symbols for 1/2, etc.
\usepackage{comment}
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{textcomp} 
\usepackage{scalerel}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{longtable}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}
\definecolor{MAEblue}{RGB}{47 112 182}
\definecolor{mydarkgreen}{RGB}{0, 139, 69}


\title{Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder}

\author[]{Xianjun Yang}
\author[]{Shaoliang Nie}
\author[]{Lijuan Liu}
\author[]{Suchin Gururangan}
\author[]{Ujjwal Karn}
\author[]{Rui Hou}
\author[]{Madian Khabsa}
\author[]{Yuning Mao}

\affiliation[]{Meta GenAI}

\abstract{
Instruction tuning data are often quantity-saturated due to the large volume of data collection and fast model iteration, leaving data selection important but underexplored. Existing quality-driven data selection methods, such as LIMA (NeurIPS 2023 \citep{zhou2024lima}) and AlpaGasus (ICLR 2024 \citep{chenalpagasus}) generally ignore the equal importance of data diversity and complexity. In this work, we aim to design a diversity-aware data selection strategy and creatively propose using sparse autoencoders (SAEs) to tackle the challenge of data diversity measure. 
In addition, SAEs can also provide more interpretability of model behavior and explain, e.g., the surprising effectiveness of selecting the longest response (ICML 2024 \citep{zhaolong}). Using effective data selection, we experimentally prove that models trained on our selected data can outperform other methods in terms of model capabilities, reduce training cost, and potentially gain more control over model behaviors.
We prove that SAEs can serve as a good alternative to diversity measure and design our method to be scalable for potential industrial large-scale pruning, and we will also release our trained SAEs for use by the broader community.}

\date{\today}
\correspondence{\email{xianjunyang@meta.com}, \email{yuningm@meta.com}}


\newcommand{\one}{SAE-GreedSelect }
\newcommand{\two}{SAE-SimScale }

\begin{document}

\maketitle

\input{sections/1_intro}
\input{sections/2_related_work}
\input{sections/3_method}
\input{sections/4_experiment}
\input{sections/5_results_analysis}
\input{sections/6_more}
\input{sections/7_conclusion}


\clearpage
\newpage
\bibliographystyle{assets/plainnat}
\bibliography{example_paper}


\clearpage
\newpage
\beginappendix

\input{sections/8_appendix.tex}

\end{document}