\section{Sparse Autoencoder}\label{sec:method}

\textbf{Formulation.}
Let $x^j$ denotes the $d$-dimension residual stream of the $j$th-layer of the transformer layer before the final layer of logits (we discard $j$ hereafter and only use $x$ for simplicity). A typical SAE can be decomposed into two parts: the encoder and decoder, where the encoder is used for representation decomposition and the decoder is only used during training for loss reconstruction and discarded during inference. A ReLU-SAE can be written as 
\begin{equation}
\begin{aligned}
   \textit{Encoder: } z &= \text{ReLU}(W_{\text{encoder}}(x - b_{\text{norm}}) + b_{\text{encoder}}) \\
   \textit{Decoder: }  \hat{x} &= W_{\text{decoder}}z + b_{\text{norm}}
\end{aligned}
\end{equation}
, where $b_{\text{norm}}$ means all inputs are normalized to the unit norm before passing to the autoencoder and computing the reconstruction errors. 
Notice that $W_{\text{encoder}}$ $\in$ $\mathbb{R}^{n \times d}$, $b_{\text{encoder}}$ $\in$ $\mathbb{R}^n$, where $n$ is number of pre-defined latents in SAE.

Similarly, a k-sparse autoencoder \citep{makhzani2013k} regulates the count of active latent units by employing the TopK activation function, which retains only the k largest latent values, setting all others to zero. Thus,
\begin{equation}
\begin{aligned}
   \textit{Encoder: } z &= \text{TopK}(W_{\text{encoder}}(x - b_{\text{norm}}) )
\end{aligned}
\end{equation}
and the decoder is the same. The model is trained by gradient descent through training loss $\mathcal{L} = \|x - \hat{x}\|_2^2$. The TopK-SAE has already been verified by OpenAI \citep{gao2024scaling} for its superior performance in explaining GPT-4, thus we also follow their training tricks, such as forcing dead neuron activation if it has not activated in 10 million tokens (with an auxiliary loss coefficient of $1/32$), normalizing the decoder weights to have unit norm, and setting $W_{\text{decoder}}$ to be the transpose of $W_{\text{encoder}}$ for improving the training process.

\textbf{SAE Training.}
We pick the last residual stream from the final layer of Llama-3.1-8b-instruct \footnote{For better visualization, we summarize all the used open-source links in App. \ref{app:links} hereafter.} for training our TopK-SAE. And we set the number of latents $n$ to $131,072$,  and $K$ to \{16, 32, 64, 128, 256\}. We use Top-$128$-SAE for our main experiments for its moderate size. 
We use the 10B tokens from RedPajama-Data-V2 \citep{together2023redpajama} for SAE training considering its high quality and diverse source. We tried various batch sizes from \{$4,096$, $8,192$, $12,288$\} and found $4,096$ to be the optimal, and more can be found in Appendix \ref{app: sae-bs}. 
For a total batch size of $4,096$, the batch size is 32 per device, and grad\_acc\_steps and micro\_acc\_steps are 4 and 2, respectively.
For all training, we use 4 nodes with 8 Nvidia A100 80G GPUs per node through model parallel. The lr\_warmup\_ratio is $0.5$ and the learning rate is $7e-5$. 
We set epoch to 4 and we do not find additional benefits with more epochs. 
Since the Llama-3 tokenizer contains a $BOS$ token which is useless and even detrimental to SAE training, we discard all $BOS$ tokens for SAE training. We preprocess all the data to the same length through concatenating and chunking passages.

Figure \ref{fig: loss_sae31} shows the training loss over different TopKs under the same configurations. It is generally observed that the training loss almost saturates after $4,000$ steps, and a larger k leads to a better final loss. However, a smaller loss does not necessarily correlate with better-decomposed features since more activated features will be more difficult to interpret. 

\textbf{SAE Inference}
After training the SAEs, inspired by the significant performance boost achieved with JumpReLU-SAE \citep{rajamanoharan2024jumping}, we further use JumpReLU during inference to rectify the activations and only treat the activation value larger than the threshold as true activation. 

\begin{equation}
\text{JumpReLU}(x) =
\begin{cases} 
x, & \text{if } x > \theta, \\
0, & \text{otherwise.}
\end{cases}
\end{equation}
, where $\theta$ is the jump threshold.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\linewidth]{figures/loss_vs_step.pdf}
\caption{The training loss of TopK-SAE on the layer 31 of Llama-3.1-8b-instruct.}\label{fig: loss_sae31}
\end{figure} 

\input{tables/overall}

\begin{algorithm}[h]
\caption{Two Diversity-driven Data Selection Methods}\label{algorithm:merged}
\begin{algorithmic}[1]
\REQUIRE The Whole Dataset $\mathcal{D}$, Sub-Dataset Size $N$, Sampling Mode (\textit{Greedy} or \textit{Similarity-based}), Threshold $\theta$ (if \textit{Similarity-based})
\ENSURE The Sampled Sub-Dataset $\mathcal{D}_s$
\STATE Initialize Empty $\mathcal{D}_s$
\STATE Sort Queries in $\mathcal{D}$ by instruction length in descending order
\WHILE{$|\mathcal{D}_s| < N$}
    \STATE Set $\mathcal{T}_s^B \leftarrow \emptyset$  \#$\mathcal{T}$ is the set of activated features
    \FOR{each Query $q \in \mathcal{D}$}
        \IF{\textcolor{blue}{$|\mathcal{T}_s^B \cup \mathcal{T}_q| > |\mathcal{T}_s^B|$ \textbf{(if \textit{Greedy: \one})} } \textbf{or} 
        \textcolor{red}{ $|\mathcal{T}_s^B \cap \mathcal{T}_q| / |\mathcal{T}_s^B| < \theta$ \textbf{(if \textit{Similarity-based: \two})}}}
            \STATE $\mathcal{D}_s \leftarrow \mathcal{D}_s \cup \{q\}$
            \STATE $\mathcal{T}_s^B \leftarrow \mathcal{T}_s^B \cup \mathcal{T}_q$
            \STATE $\mathcal{D} \leftarrow \mathcal{D} \setminus \{q\}$
            \IF{$|\mathcal{D}_s| = N$}
                \STATE break
            \ENDIF
        \ENDIF
    \ENDFOR
\ENDWHILE
\STATE \textbf{return} $\mathcal{D}_s$
\end{algorithmic}
\end{algorithm}


 
