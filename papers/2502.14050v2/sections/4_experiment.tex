\section{Methods and Experiments} 

\subsection{Diversity-driven Data Selection through Sparse Autoencoder}
We only focus on \textbf{diversity} for the selection to prove the effectiveness of SAEs against existing diversity measures.
Based on the extracted features from our trained SAEs, we design two data selection methods: 
1) when we only want to select a limited number of data as presented in \citep{chenalpagasus, zhaolong}, we propose greedy sampling using features from SAE for limited data (\textcolor{blue}{\textbf{SAE-GreedSelect}}) to maximize the utilization of features, and 
2) when we want to scale up the selected data rather than only picking a fixed number of data, we propose similarity-based sampling using features from SAE for scaling up the data selection (\textcolor{red}{\textbf{SAE-SimScale}}). For example, the original \#INSTAG \citep{lu2023instag} method uses a greedy search but can not scale to $5$k data since no data can bring new intention tags. 
The details of the two methods can be found in Algorithm \ref{algorithm:merged}.
Compared with previous methods used in the industrial model training pipeline \citep{lu2023instag}, our method is designed to be \textbf{simple and scalable so that it can also be used for large-scale training by the industry}.

As we mentioned above that we actually use JumpRelu during inference, and we empirically set the threshold to $10$ across all experiments in our work. For method \textbf{SAE-SimScale}, there is one additional parameter of similarity ratio, and we set it to $0.8$ for all experiments. Thus, there are only $1$ and $2$ tunable parameters for \textbf{SAE-GreedSelect} and \textbf{SAE-SimScale}, respectively. This way, we can maximize the simplicity.
Additional experiments regarding the threshold can be found in Section \ref{sec: threshold}, and some even witness better results.

\subsection{Experimental Settings}

\textbf{Models.} To validate our method for data selection for supervised instruction fine-tuning (SFT), we use Llama-2-13b-base for our main experiments as the foundation base model. In addition, we also use Gemma-2-9b and Llama-2-7b-base to verify the method at different model scales.

\textbf{Datasets.} We use Alpaca-52k and WizardLM\_evol\_instruct\_70k as the target instruction tuning datasets because they are widely used and contain large enough data points for data selection.
 
\textbf{Baselines.} We choose Longest-response \citep{zhaolong}, \textit{\#Instag} \citep{lu2023instag} and Repr Filter \citep{liumakes}  as the baselines. 
\textit{\#Instag} \citep{lu2023instag} and Repr Filter \citep{liumakes} share similar ideas of gradually selecting dadapoints that add additional diversity, measured by intention tags and Cos similarity (we follow the similarity threshold of $0.9$ in their paper but replace the sentence embedding with Llama-3-8b), respectively.
Additionally, we also use the longest instruction as a baseline for comparison with our methods. 
Note that the best performance in longest-response \citep{zhaolong} was achieved by additional tricks such as refining the instructions or NEFTune \citep{jainneftune}, but we did not follow this for a simple and fair comparison with other baselines.

\textbf{Configurations.} For all SFT experiments, we use 8 Nvidia A100 80G GPUs and the stanford alpaca code base. Following similar configurations in \citep{zhaolong}, we set the maximum length to be $1,024$ for data selected from Alpaca-52k and $2,048$ from WizardLM\_evol\_instruct\_70k. We always set the batch size to 128, learning\_rate to 1e-5, weight\_decay to 0.1, and warmup\_ratio to 0.03. We set epochs to 15 for $1$k and $3$k data, $5$ for $5$k data, and $3$ for full data. 



\textbf{Evaluation.}
The selection of SFT data aims to elicit superior instruction-following abilities, but the evaluation lacks standardization.
For a comprehensive evaluation, we use IFEval \citep{zhou2023instruction} for strict evaluation since it can accurately measure the response that adheres to the complex instructions through verifiable instructions, such as  "mention the keyword of AI at least 3 times".
We also use LLM- and Human-as-a-Judge for head-to-head evaluation and report the results on the AlpacaEval 2.0 leadboard as additional metric.
Besides, it is expected that the models trained on small instruction datasets also behave well in other knowledge-intensive tasks, such as MMLU \citep{hendrycksmeasuring}, TruthfulQA \citep{lin2022truthfulqa}, Winogrande \citep{sakaguchi2020winogrande}, Arc \citep{clark2018think} and Gsm8k \citep{cobbe2021training}. So we also use those five benchmarks for measuring the finetuned model's knowledge.
For all the evaluations on those five benchmarks, we utilize the lm-evaluation-harness under their default setting with the same number of $3$ in-context examples for all models.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/scatter_fit.pdf}
\caption{The correlation between text length and number of activations in SAEs.}\label{fig: correlation}
\end{figure} 

\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\linewidth]{figures/benchmark_alpaca-1k-llama2-13b.pdf}
\caption{The benchmark performance between different methods: Llama 2 (13B) trained from corresponding $1$k selected data from Alpaca.}\label{fig: benchmark_wizardlm-llama2-13b}
\end{figure*} 




