\section{Related Work}

\textbf{Sparse Autoencoders} (SAEs) are powerful for understanding neural representations \citep{gao2024scaling, paulo2024automatically, braun2024identifying}, as well as in scaling and enhancing the interpretability of LLMs \citep{cunningham2023sparse, foote2023neuron}.
The foundational work in sparse coding, also known as dictionary learning, dates back to an overcomplete basis set proposed around 30 years ago \citep{olshausen1997sparse}. Based on this, K-SVD \citep{aharon2006k} was developed as an algorithm for designing overcomplete dictionaries, facilitating sparse representations in signal processing. Furthermore, K-SAEs \citep{makhzani2013k} introduced a K-sparse constraint to enforce sparsity, leading to more accurate data representations.
\cite{bau2020understanding} highlights the importance of understanding unit-specific functions for the interpretability of the model. Furthermore, \cite{tonolini2020variational} proposes that the visual cortex can employ such strategies to efficiently represent natural images.
Recently, sparse dictionary learning has been applied to visualize transformer models \citep{elhage2022toy, henighan2023superposition}, revealing that contextualized embeddings can be expressed as linear combinations of transformer factors \citep{yun2021transformer}. Research has also investigated polysemanticity and capacity in neural networks, finding that neurons often encode multiple concepts, which poses challenges for interpretability \citep{scherlis2022polysemanticity, lieberum2024gemma}. Moreover, engineering monosemanticity has been explored to design neurons that represent single concepts, thus enhancing interpretability \citep{jermyn2022engineering}.
A structured mathematical framework has been proposed for analyzing transformer circuits \citep{elhage2021mathematical}. It has been demonstrated that LLMs can elucidate the functions of neurons within themselves, offering a novel perspective on model introspection \citep{bills2023language}.
Sparse probing techniques have identified significant neurons, providing case studies that underscore the utility of sparsity in interpretability \citep{gurneefinding}. Improvements in dictionary learning with Gated-SAE \citep{rajamanoharan2024improving} and JumpReLU-SAE \citep{rajamanoharan2024jumping} further enhanced the quality of learned representations.

\textbf{Data Selection} \citep{albalak2024survey, wang2024survey}, the process of selecting a representative subset of data to achieve efficient training without compromising model performance, is important in both pre-training \citep{brandfonbrener2024color, tirumala2023d4} and post-training \citep{chenalpagasus, li2024superfiltering}.
Previous studies \citep{mindermann2022prioritized, paul2021deep} have focused on optimizing this selection for various training objectives, specifically targeting model performance constraints \citep{xia2024refined}, the value of individual data points \citep{covertscaling}, and addressing bias \citep{jain2024data}.
Recently, the emphasis on data curation \citep{taori2023stanford, chiang2023vicuna, cui2024ultrafeedback, wang2023self} and selection \citep{zhou2024lima} for LLMs suggests that the main capabilities of LLMs come from pre-training, and a small amount of well-crafted instruction data can enable excellent instruction following.
As a result, various data selection methods \citep{du2023mods, chen2023maybe, xia2024rethinking, ge2024clustering, lee2024concept, liu2024selectit} have been proposed. For example, AlpaGasus \citep{chenalpagasus} uses ChatGPT to score data quality and selects only the top 1000 highest-scoring data points. \cite{zhaolong} proposes a simple yet effective baseline of selecting the longest responses, while \cite{xialess, zhang2024tagcos, pan2024g} employ gradient-based clustering for task-agnostic coreset selection.
Further research \citep{liumakes} indicates that data quality \citep{li2024quantity, ding2023enhancing, li2024selective, li2023reflection}, diversity \citep{ge2024clustering}, and complexity \citep{xu2023wizardlm, sun2024conifer, ivison2023camels} are all crucial to the success of IFT, especially under complex constraints. However, accurately measuring these dimensions is a nontrivial task.
\textbf{In this work, we focus on data diversity}. Data diversity in instruction-tuning is increasingly recognized as crucial for building robust models \citep{bukharin2023data}. \textit{\#InsTag} \citep{lu2023instag} measures data diversity through intention tags of instructions, a method also adopted in the official technical reports of advanced LLMs, including Llama-3 \citep{dubey2024llama} and Qwen2-72B \citep{bai2023qwen, yang2024qwen2}. In this work, we prove that SAEs offer a better diversity measure.
 
