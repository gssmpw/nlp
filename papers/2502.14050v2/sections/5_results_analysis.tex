\section{Results and Analysis}
\subsection{Why $1,000$ Longest Responses \citep{zhaolong} Lead to Strong Performance?}
Picking the $1,000$ longest responses \citep{zhaolong} serves as a simple but tough-to-beat baseline for SFT and they hypothesize that this is because the longest responses intuitively contain more learnable information, but do not provide quantitative evidence to this assumption. Thus, we provide a quantitative explanation of the activated features through SAE by examining the correlation between activated features and text length.
Figure \ref{fig: correlation} shows a strong positive correlation (r = $0.92$) between text length and feature richness in an SAE extracted from text in the Alpaca dataset, supporting their hypothesis that longer responses encode more learnable information. This simple strategy provides a competitive baseline for fine-tuning LLMs efficiently but is not always better, as shown later.



\subsection{Comparison with Baselines}

Table \ref{tab: overall} presents a comparative analysis of different data selection methods applied to Alpaca-52k and WizardLM\_evol\_instruct-70k datasets, evaluated under "Strict prompt-level," "Strict instruction-level," "Loose prompt-level," and "Loose instruction-level" on the IFEval.
As the data size increases from $1$k to $5$k, the performance of all methods improves across both datasets.
Our proposed methods \one and \two exhibit more significant gains than baseline approaches, indicating their robustness and scalability. For instance, in the WizardLM\_evol\_instruct-70k dataset at the 3k data scale, \two achieves a "Loose instruction-level" score of 50.96, significantly surpassing the \#instag baseline's 46.16 and other baselines. And it even matches the upper bound of using the entire $70$k data.
\two achieves the best results overall, particularly with higher data sizes ($3$k and $5$k), with significant performance improvements in "Strict instruction-level" and "Loose instruction-level" evaluations. Performance differences are more pronounced in the WizardLM\_evol\_instruct-70k dataset, highlighting the challenges and opportunities in leveraging larger and more complex instruction-tuning datasets.

The results suggest that the \two method is the most effective approach for data selection through diversity-driven frameworks.
By outperforming baselines across all metrics and datasets, \two highlights its potential for optimizing \textbf{scalable data selection}. 

 

\begin{figure*}[ht]
\centering
\includegraphics[width=0.7\linewidth]{figures/merged_threshold_wizard.pdf}
\caption{The comparison of \one (\textbf{Top}) and \two (\textbf{Bottom}) under different SAE thresholds.}\label{fig: threshold_wizard}
\end{figure*} 



\input{tables/different_sae}

\subsection{Preference Evaluation}
In addition to the above evaluation, we also focus on assessing human or AI preference evaluation. The head-to-head comparison is shown in Figure \ref{fig: headtohead1}, showing that our methods again consistently outperform the baselines. More details can be found in the Appendix \ref{app: comparison}.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{figures/alpaca_eval.pdf}
\caption{The head-to-head comparison between Llama-2-13b-base trained on different data selected from Alpaca.}\label{fig: headtohead1}
\end{figure} 

Besides, In Table \ref{tab: leadboard} we report the results on the AlpacaEval 2.0 benchmark with some baselines copied from their public leaderboard and \citep{zhaolong}. The bottom four models are all trained on Llama-2-13b-base with $5$k data selected from the WizardLM\_evol\_instruct-70k dataset. Notably, our method achieves superior performance compared to some commercial models, including ChatGPT and Claude, outperforms the baseline approaches by a large margin. %This shows our methods are surprisingly effective for data selection. 

\begin{table}[h]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{lcc}
        \toprule
        \small{Model} & \small{Win Rate}  & \small{Length Controlled Win Rate} \\
        \midrule
        %\small{gpt4\_0613} & 15.75   &  30.18  \\
        \small{Wizardlm-70b} & 14.38   &  17.57  \\
        \small{AlpaGasus-1k} &   4.87 & -  \\
        \small{LIMA-1k} &  5.64  &  -  \\
        \small{gpt-3.5-turbo-1106} &  9.18  &  19.3  \\
        \small{claude-2.1\_concise} &  9.23  & 18.21   \\
        \midrule
        \small{\#INSTAG-5k} &  3.42 &  6.08   \\
        \small{Longest-response-5k} & 5.86 &  7.04  \\
        \small{Repr Filter} &  4.91&  9.55   \\
        \small{\textbf{SAE-GreedSelect-5k}} &  \textbf{19.61} &  \textbf{22.81}   \\
        \bottomrule
    \end{tabular}
    }
    \caption{Preference evaluation results on AlpacaEval 2.0. '-' represents not available.}\label{tab: leadboard}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/IFEval_Scores_Radar.pdf}
\caption{Radar plot comparing the results of using SAEs on different layers.}\label{fig: different_layer}
\end{figure} 

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/case_study1.pdf}
\caption{Case study: the
instruction-following performance of Llama-2-13B model finetuned
on $3$k data selected from Alpaca with our \one and \textit{\#INSTAG} baseline.}\label{fig: case_study}
\end{figure} 


\subsection{Different SAE as Feature Extractor}

In Table \ref{tab: different_sae}, we show the result using different SAE as the feature extractor.
This table compares the performance of the same algorithm \one using two SAEs as the backbone, Gemma-SAE and Ours-SAE, across four evaluation metrics on subsets of the Alpaca-52k dataset with $1$k and $3$k data points. 
For the $1$k subset, Ours-SAE achieves higher accuracy scores than Gemma-SAE, such as 24.21\% vs. 23.66\% on strict prompt-level and 37.53\% vs. 35.25\% on loose instruction-level metrics. For the $3$k subset, Gemma-SAE slightly outperforms Ours-SAE in strict prompt-level (31.61\% vs. 29.76\%) and instruction-level (41.96\% vs. 40.89\%), though both methods are comparable in loose-level evaluations.
This confirms the universal effectiveness of our proposed data selection method when using different SAEs as the backbone, showing the flexibility and compatibility of our data selection strategy.

\subsection{Results on Base Model with Different Size}
\input{tables/different_model}
In Table \ref{tab: different_model}, we show the results using base models at different sizes to explore the impact of model parameters.
The results demonstrate that our proposed data selection method consistently outperforms baselines across all data scales ($1$k, $3$k, and $5$k) for both Gemma-2-9b and Llama-2-7b models.
And \two achieves the highest scores across almost all metrics, showcasing its effectiveness in improving performance over other methods.
The improvements with the proposed method are more evident in the Gemma-2-9b model compared to the Llama-2-7b, and the same trend is also observed in previous results on Llama-2-13b. 
The significance of the improvements achieved by the proposed data selection method diminishes when evaluated on smaller models, highlighting the dependence on the model scale for pronounced benefits. But this demonstrates significant importance of our method since it meets the scaling trend of current LLMs.

\subsection{Results of Using SAEs on Different Layers.}
The previous results come from picking the data using the SAE trained on the final ($31st$) layer. However, it is also possible to use other layers. Thus, we also pick the SAE trained on the $29th$ layer as the backbone and perform a similar pipeline of selecting the data and training the model.
Figure \ref{fig: different_layer} shows the radar plot to compare the four IFEval scores on models trained on $1$k, $3$k, and $5$k data selected using different SAE layers. 
For the 1k dataset, Layer-31-SAE scores ranged from $24.21$ to $37.53$, while Layer-$29$-SAE ranged from $21.44$ to $35.25$. With larger datasets ($3$k and $5$k), scores improved, with Layer-$31$-SAE reaching up to $43.65$ ($3$k) and $42.33$ ($5$k), consistently outperforming Layer-$29$-SAE, which peaked at $41.72$ ($3$k) and $41.01$ ($5$k).



\subsection{Impact of Inference Threshold of SAE}\label{sec: threshold}
In previous experiments, we always set the threshold to $10.0$ during the SAE inference. Here we show the results of using different threshold.
The figure \ref{fig: threshold_wizard} compares accuracy levels across varying thresholds for two versions of our methods ("\one" and "\two") under different data selection conditions ($1$k, $3$k, $5$k). Both methods exhibit an upward trend in accuracy as thresholds increase, although the rate of improvement varies. At lower thresholds, accuracy differences between the versions are minimal, but \two consistently outperforms \one at higher thresholds across all data sizes. Data size influences the trends: larger data size (e.g., $5$k) yields higher overall accuracy for both versions. Strict and loose prompt-level and instruction-level conditions impact the algorithms, with stricter conditions showing relatively lower accuracy compared to looser configurations. This figure demonstrates that an x-axis threshold of $10$ generally achieves the best accuracy results across all metrics and settings.

\subsection{Case Study}
Here we show some examples in Figure \ref{fig: case_study} as a case study, and the instructions are picked from the IFEval dataset. Our model outperforms the baseline in instruction adherence and generation correctness, with additional cases in App. \ref{app: case_study}.