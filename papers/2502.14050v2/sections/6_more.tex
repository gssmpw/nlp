\section{Factual Knowledge benchmarks}\label{sec:mitigation}



Despite the above instruction-following abilities, we look at the model performance on knowledge-intensive benchmarks, as shown in Figure \ref{fig: benchmark_wizardlm-llama2-13b}.
This figure presents a comparative analysis of model performance across five standard benchmarks and the average performance. It highlights that \two generally outperforms other models across most datasets, particularly in Winogrande and the overall score. GSM8k witnesses the lowest results among all datasets for all models, while Winogrande shows the highest accuracy, showcasing the variability in task difficulty.
However, there are \textbf{marginal differences between different selection methods}, suggesting that factual knowledge is almost maintained across five models, which is also proven in \citep{zhaolong} that small scale SFT does not impact abilities on knowledge benchmarks.
More can be found in Appendix \ref{tab: benchmark}.
 
