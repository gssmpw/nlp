
\section{Models and Datasets Links}\label{app:links}
In Table \ref{tab:models-links}, we summarize all the open-source links for the models, datasets and codebases used in our work.
\begin{table}[h]
    \centering
    \begin{tabular}{l|l}
        \hline
        Models, Datasets, Codebase & Links \\ \hline
        Llama-2-13b-base &  https://huggingface.co/meta-llama/Llama-2-13b \\ \hline
        Gemma-2-9b &  https://huggingface.co/google/gemma-2-9b \\ \hline
        Llama-2-7b-base &  https://huggingface.co/meta-llama/Llama-2-7b \\ \hline
        Alpaca &  https://huggingface.co/datasets/tatsu-lab/alpaca \\ \hline
        stanford\_alpaca &  https://github.com/tatsu-lab/stanford\_alpaca \\ \hline
        Llama-3.1-8B-Instruct &  https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct \\ \hline
        lm-evaluation-harness &  https://github.com/EleutherAI/lm-evaluation-harness \\ \hline
        Gemma-SAE &  https://huggingface.co/google/gemma-scope-9b-pt-res \\ \hline
        WizardLM\_evol\_instruct\_70k &  
        https://huggingface.co/datasets/WizardLMTeam/WizardLM\_evol\_instruct\_70k \\ \hline
    \end{tabular}
    \caption{Models and Datasets Links}
    \label{tab:models-links}
\end{table}



\section{Benchmark}
Here we show the complete results of all model performance on the benchmarks.

\input{tables/benchmark}

\section{Additional Case Study}\label{app: case_study}
Here we show additional examples in Figure \ref{fig: case_study2} as case studies, and the instructions are picked from the IFEval dataset. Our model outperforms both baselines in instruction adherence and generation correctness.

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/case_study2.pdf}
\caption{Case study to illustrate the
instruction-following performance of Llama-2-13B model finetuned
on $3$k data selected from Alpaca with our \one method, \textit{\#INSTAG} (Baseline1) and Longest-response (Baseline2).}\label{fig: case_study2}
\end{figure*} 

Besides, we also show additional results in Figures \ref{fig: case_study3} and \ref{fig: case_study4} with instructions from the AlpacaEval 2.0 benchmark to show the general instruction following abilities. 

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/case_study3.pdf}
\caption{Case study to illustrate the instruction-following performance of Llama-2-13B model fine-tuned on $5$k data selected from WizardLM\_evol\_instruct-70k with our \one method, and \textit{Repr Filter} (Baseline), with GPT4o-as-the-Judge.}\label{fig: case_study3}
\end{figure*} 


\begin{figure*}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/case_study4.pdf}
\caption{Case study to illustrate the instruction-following performance of Llama-2-13B model fine-tuned on $5$k data selected from WizardLM\_evol\_instruct-70k with our \one method, and \textit{Repr Filter} (Baseline), with GPT4o-as-the-Judge.}\label{fig: case_study4}
\end{figure*} 




\section{Additional SAE Training Loss VS. Batch Size}\label{app: sae-bs}
Here in figure \ref{fig: loss_bs} we the SAE training loss for layer 29 when we change the batch size from $4,096$, $8,192$ to $16,384$. We can see that setting batch size to $4,096$ makes the SAE training convergence faster and leads to smaller final loss.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\linewidth]{figures/loss_vs_relative_step.pdf}
\caption{The training loss of TopK-SAE on the layer 31 of Llama-3.1-8b-instruct.}\label{fig: loss_bs}
\end{figure} 

\section{Instruction Following Evaluation}\label{app: comparison}
The head-to-head comparison was conducted on the 805 instructions in the AlpacaEval dataset.
We utilize the same evaluation prompt template for GPT-4o, as employed by AlpaGasus \citep{chenalpagasus}, Longest \citep{zhaolong} and originally used in Vicuna work \citep{chiang2023vicuna}
. In Figure \ref{fig: evaluation_prompt}, we show the evaluation prompt for GPT-4o. In our preliminary experiments, we find that the the response length has a significant correlation with model judge, aligning with the findings in \citep{dubois2024length}. Thus, we first truncate both responses to the same length and then continue the LLM-as-a-judge for head-to-head comparison. For human evaluators, we hire PhD volunteers to only evaulate 200 of the 805 instructions and only ask for a score, with no explanation for simplicity. On the other hand, for the AlpacaEval 2.0 leadboard evaluation, we keep the original response length since they also offer the length controlled win rate.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{figures/prompt_template.pdf}
\caption{The prompt template used for GPT-4o as the judge.}\label{fig: evaluation_prompt}
\end{figure} 