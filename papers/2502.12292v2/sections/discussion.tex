\section{Related \& Future Work}\label{sec:related-work}

We develop methods for testing whether two models are independently trained given their weights that do not require intervening on the training of either model.

A related line of work known as model fingerprinting \citep{xu2024instructionalfingerprintinglargelanguage, zhang2024reefrepresentationencodingfingerprints, jin2024proflingofingerprintingbasedintellectualproperty, yang2024fingerprintlargelanguagemodels}
plants a secret signal in the weights of a model so that anyone who knows the key can detect the fingerprint from query access to the model (or fine-tunes of the model).
For example, \citet{xu2024instructionalfingerprintinglargelanguage} propose fingerprinting a model by fine-tuning on a secret random string; fingerprint detection then resolves to prompting a putative fingerprinted model with a prefix of the string.
Unlike \citet{xu2024instructionalfingerprintinglargelanguage}, we do not intervene on the training process of the models we test; however, we do require access to model weights in order to run our tests.

A separate line of work on text watermarking aims to attribute model-generated text by planting a watermark when sampling text from the model \citep{christ2023undetectablewatermarkslanguagemodels, kirchenbauer2024watermarklargelanguagemodels, kuditipudi2024robustdistortionfreewatermarkslanguage, aaronson2023watermarking, zhao2023provablerobustwatermarkingaigenerated}. 
Because it intervenes on sampling, text watermarking is inapplicable to open-weight models, the focus of both model fingerprinting and our setting. Recent work demonstrates that models can directly learn to generate watermarked text but also finds the learned watermark is not robust to further fine-tuning \citep{gu2024learnabilitywatermarkslanguagemodels}.

Finally, we show that the methods of the most related work \citep{zeng2024humanreadablefingerprintlargelanguage} (who investigate the same question of model provenance) are not robust to our transformations 
and also do not provide p-values for independence testing.
Other works like \citet{jin2024proflingofingerprintingbasedintellectualproperty} propose crafting specific queries that are likely to produce different responses among independently trained models and \cite{nikolic2025modelprovenancetestinglarge} identify similarities in non-independent model outputs; these methods do not require access to weights but also have fewer theoretical guarantees and do not produce exact p-values.

One limitation of our work is that we do not distinguish between different cases of dependent models, such as whether two models share a common ancestor versus one being a finetune of the other.
One direction for future work is to develop methods capable of differentiating between these cases to reconstruct a complete ``family tree'' of model lineage \citep{yax2024phylolminferringphylogeny}.
Another open question is whether it is possible to obtain exact guarantees (i.e., p-values) for our unconstrained setting.

\newpage 

\section*{Acknowledgments}
We gratefully acknowledge the support of this work by an NSF Frontier Award (NSF Grant no. 1805310) and Omidyar. Sally Zhu was supported by a Stanford CURIS Fellowship. Ahmed Ahmed is grateful to be supported by an NSF Graduate Research Fellowship and a Knight-Hennessy Fellowship.