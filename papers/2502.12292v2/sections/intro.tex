\section{Introduction}

Consider the ways in which two models could be related: one model may be a finetune of the other; one could be spliced and pruned from certain parts of the other; both models could be separately fine-tuned from a common ancestor; finally, they could be independently trained from each other. 
We consider the problem of determining whether two models are independently trained versus not from their weights, which we formalize as a hypothesis testing problem in which the null hypothesis is that the weights of the two models are independent. We concretely treat only the weight initialization as random and thus consider two models with different random initial seeds as independent, even if both models were trained on the same data, or one model was distilled from the outputs of the other.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/fig1_new.png}
    \caption{Given the weights of two language models, what relationships can we derive? They could be two models trained from scratch (left). Or, one model could be derived from the other: the dependent model could be a fine-tune, a pruned model, or a partially pruned model (right). We present tests to identify such relationships. 
    }
    \label{fig:figure1}
\end{figure}

A solution to this independence testing problem would enable independent auditors to track provenance of open-weight models. This is pertinent because while open-weight models enable broader access and customization, they also pose potential risks for misuse as they cannot be easily monitored or moderated \citep{kapoor2024openmodels}.
Model developers would also gain an enhanced ability to protect their intellectual property (IP) \citep{2024miquscandal, Peng2023dnnip} and enforce custom model licenses \citep{dubey2024llama3herdmodels,deepseekai2024deepseekv3technicalreport}. 

We consider two settings of the independence testing problem (Table \ref{tab:two_settings}).
In the $\constr$ setting, we make assumptions on training and initialization (essentially, that the training algorithm is equivariant to permuting the hidden units of the random initialization)
that enable us to obtain provably valid p-values. 
The main idea is that under these assumptions we can cheaply simulate many exchangeable copies of each model's weights and compare the value of some test statistic (e.g., cosine similarity of model weights) on each of these copies with the original model pair.
The assumptions generally hold in practice but preclude robustness to adversarial evasion attacks and architectural changes.

For the $\constr$ setting, we evaluate various test statistics on 21 models of the Llama 2 architecture \citep{touvron2023llama2openfoundation}, including 12 fine-tunes of Llama 2 and nine independently trained models, obtaining extremely small p-values for all 69 non-independent model pairs. Notably, our tests retain significant (small) p-values over different fine-tuning methods (e.g., different optimizers) and on models fine-tuned for many tokens from the base model such as Llemma \citep{azerbayev2024llemmaopenlanguagemodel}, which was fine-tuned on an additional 750B tokens from Llama 2 (37.5\% of the Llama 2 training budget). We are also able to confirm that the leaked Miqu-70B model from Mistral is derived from Llama 2-70B.

Next, we consider the unconstrained setting. While the constrained setting is useful for studying the existing ecosystem of open-weight models, simple modifications to model weights and architecture such as permuting hidden units can violate the assumptions of the $\constr$ setting if an adversary applies them after fine-tuning a model.
We address this limitation in the $\unconstr$ setting, wherein we do not make any assumptions on training.
Though we are not able to obtain provably exact p-values in the $\unconstr$ setting, we derive a test whose output empirically behaves like a p-value and reliably distinguishes non-independent models from independent models.
In particular, we first align the hidden units of two models---which may each have different activation types and hidden dimensions---and then compute some measure of similarity between the aligned models. Because of the alignment step, the test is robust to changes in model architecture and various adversarial evasion attacks (including those that break prior work). Moreover, it can localize the dependence: we can identify specific components or weights that are not independent between two models, even when they have different architectures.

\begin{table}[]
    \centering
    \begin{tabular}{|p{7cm}|p{7cm}|}
    \hline 
        \textbf{$\constr$ setting} & \textbf{$\unconstr$ setting} \\ \hline 
        gives exact p-values & does not give exact p-values \\ \hline 
        not robust to permutation & robust to permutations and other adversarial transformations \\ \hline 
        only applies to models of fixed shared architecture & works for models of different architectures and gives localized testing of shared weights \\ \hline 
    \end{tabular}
    \caption{Features of the $\constr$ versus $\unconstr$ problem settings.
    }
    \label{tab:two_settings}
\end{table}

In the unconstrained setting, we evaluate our test on 141 independent model pairs and find that its output empirically behaves like a p-value in the sense that it is close to uniformly distributed in $(0,1]$ over these pairs.
In contrast, it is almost zero for all dependent pairs we test (including those for which we simulate a somewhat strong adversary by retraining entire layers from scratch).
We also employ our test to identify \textit{pruned} model pairs, which occur when a model is compressed through dimension reduction techniques, such as reducing the number of layers or decreasing the hidden dimension by retaining select activations and weights; for example, we identified the precise layers of Llama 3.1-8B from which each of the layers of Llama 3.2-3B and Llama 3.2-1B derive.

We defer a full discussion of related work to Section~\ref{sec:related-work}. The work most closely related to ours is due to \citet{zeng2024humanreadablefingerprintlargelanguage}, who develop various tests to determine whether one model
is independent of another by computing the cosine similarity of the products of certain weight matrices in both models.
They show that their tests are robust to simple adversarial transformations of model weights that preserve model output;
however, we detail in Appendix~\ref{app:breakhuref} other transformations to perturb dependent models that evade detection by their tests, but not by our unconstrained setting tests. 
Additionally, unlike \citet{zeng2024humanreadablefingerprintlargelanguage}, in the $\constr$ setting we obtain exact p-values from our tests. 
