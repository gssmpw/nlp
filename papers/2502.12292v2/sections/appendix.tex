\section{Randomized Learning Algorithms}\label{sec:randomized-alg}
\begin{definition}\label{defn:perm-equiv}
    Let $\Pi \subset \Theta \to \Theta$. Let $\pi \in \Pi$ and $\init \in \Theta$, with $\bar\theta \sim A(\init), \theta = \pi(\bar\theta)$ and $\theta' \sim A(\pi(\theta_0))$. A randomized learning algorithm $A : \Theta \to \mathcal{P}(\Theta)$ is $\Pi$-\textit{equivariant} if and only if $\theta \stackrel{d}{=} \theta'$.
\end{definition}

\begin{theorem}\label{thm:main-randomized}
    Let $\phi: \Theta \times \Theta \to \R$ be a test statistic and $\Pi \subset \Theta \to \Theta$ be finite. 
    Let $A : \Theta \to \mc{P}(\Theta)$ satisfy Definition~\ref{defn:perm-equiv} and let $P \in \mc{P}(\Theta)$ be $\Pi$-invariant. 
    For $\theta_1^0 \sim P$, let $\theta_1 \sim A(\theta_1^0)$. Let $\theta_2 \in \Theta$ be independent of $\theta_1$. 
    Then $\est{p} = \permtest(\firstparam,\secondparam)$ is uniformly distributed on $\{\frac{i}{T+1}\}_{i=1}^T$.
\end{theorem}
\begin{proof}
    The proof is identical to that of Theorem \ref{thm:main}.
\end{proof}

\section{Transformer Architecture and Notation}
\label{app:llamaarchitecture}

We consider models with the Llama Transformer architecture and define the notation henceforth, although this can easily be extended to other Transformer architectures. 

Following the definition of $\fmlp$ in Example \ref{example:glu-mlp} and the definition of a submodel in Definition \ref{defn:submodel}, we can define an abstraction of the full Llama language model architecture consisting of $L$ Transformer blocks sandwiched between an input and output layer. For the sequel, we will abuse notation in applying $\fmlp$ to multi-dimensional tensors by broadcasting along the last axis.
We use $d,n \in \N$ to respectively denote the model dimension and sequence length, where $\Thetalm = \Thetain \times \Thetablock^{\times L} \times \Thetaout$ with $\Thetablock$ denoting the parameter space of each Transformer block and $\Thetain,\Thetaout$ denoting the parameter spaces the input and output layers.
We decompose $\Thetablock = \Thetapre \times \Thetamlp$ and use $\frest : \Thetapre \times \R^{n \times d} \to \R^{n \times d}$ to denote all remaining parts of the Transformer besides the MLP. The inputs to $\frest$ are the input and output of the MLP, and the output of $\frest$ is fed directly to the MLP of the next layer. In particular, $\frest$ 
takes the input and output to the MLP of layer $i$, and first performs the residual connection following the MLP of layer $i$, then the self-attention and normalization components of layer $i+1$, and returns the input to the MLP of layer $i+1$.
We use $\fin : \mc{X} \times \Thetain \to \R^{n \times d}$ and $\fout : \R^{n \times d} \times \Thetablock^{(L)} \to \mc{Y}$ to respectively denote the input and output layers, i.e. the elements before the first MLP and after the last MLP.
Putting everything together gives the following definition of the model; we introduce the notation $\activation{\theta}{i}$ in the definition as a matter of convenience to track intermediate activations.
\begin{definition}\label{defn:lm}
    (GLU Transformer model)
    Let $\theta = (\thetain, \{ \thetablock^{(i)} \}_{i=1}^L, \thetaout) \in \Thetalm$ and $X \in \mc{X}$, with $\thetablock^{(i)} = (\thetapre^{(i)},\thetamlp^{(i)})$.
    Then $\flm(X;\theta) = \fout(\activation{\theta}{L} ; \thetaout)$ for $\activation{\theta}{0}=\fin(X;\thetain)$
    and
    \begin{align} \label{eqn:Hix}
        \activation{\theta}{i} = \frest(\activation{\theta}{i-1}, \fmlp(\activation{\theta}{i-1})).
    \end{align}
\end{definition}

For a Llama model, Table \ref{tab:transformer_table} describes the shapes of the model weight matrices for $i = 1, \dots, L$, for $V$ (vocab size), $d_\text{emb}$ (the hidden dimension), and $d_\text{mlp}$ (MLP hidden dimension). Following Definition \ref{defn:lm}, we have $\thetain = (E), \thetablock^{(i)}= (\thetapre^{(i)}, \thetamlp^{(i)})$ where $\thetapre^{(i)} = (\gamma_{\text{input},i}, W_{Q,i}, W_{K,i}, W_{V,i}, W_{O,i}, \gamma_{\text{post-attn},i})$, $\thetamlp^{(i)} = (G_i, U_i, D_i)$, and $\thetaout = (\gamma_\text{final}, O)$. We now describe a forward pass of the model. 

\begin{table}
    \centering
    \begin{tabular}{c|c}
    \hline 
        \textbf{Parameter name} & \textbf{Notation} \\ \hline
        embedding & $E \in \R^{V \times d_{\text{emb}}}$ \\ \hline 
        input layernorm & $\gamma_{\text{input},i} \in \R^{1 \times d_{\text{emb}}}$ \\ 
        attention query matrix & $W_{Q,i} \in \R^{d_{\text{emb}} \times d_{\text{emb}}}$ \\ 
        attention key matrix & $W_{K,i} \in \R^{d_{\text{emb}} \times d_{\text{emb}}}$ \\ 
        attention value matrix & $W_{V,i} \in \R^{d_{\text{emb}} \times d_{\text{emb}}}$ \\ 
        attention output matrix & $W_{O,i} \in \R^{d_{\text{emb}} \times d_{\text{emb}}}$ \\ 
        post-attention layernorm & $\gamma_{\text{post-attn, } i} \in \R^{1 \times d_{\text{emb}}}$\\ \hline 
        MLP gate projection & $G_i \in \R^{d_{\text{mlp}} \times d_{\text{emb}}}$ \\ 
        MLP up projection & $U_i \in \R^{d_{\text{mlp}} \times d_{\text{emb}}}$ \\ 
        MLP down projection & $D_i \in \R^{d_{\text{emb}} \times d_{\text{mlp}}}$ \\ \hline 
        final layernorm & $\gamma_\text{final} \in \R^{1 \times d_{\text{emb}}}$ \\ 
        linear output & $O \in \R^{d_{\text{emb}} \times V}$ \\ \hline 
    \end{tabular}
    \caption{We describe our notation and the dimensions of parameters of the Llama model architecture. Here, $i$ ranges over the number of Transformer blocks.}
    \label{tab:transformer_table}
\end{table}

We define the softmax function on a vector $v = (v_1, \dots, v_n)$, softmax$(v)$, as
\begin{equation*}
    \text{softmax}(v)_i = \frac{e^{v_i}}{\sum_{k=1}^n e^{v_k}}.
\end{equation*}
On batched input $X \in \R^{N \times n \times m}$ where each $X^{(b)} = [w_1 | \dots | w_m] \in \R^{n\times m}$ with column vectors $w_i$, we define the softmax as 
\begin{equation*}
    \text{softmax}(X^{(b)}) = [ \text{softmax}(w_1)| \dots | \text{softmax}(w_m) ],
\end{equation*}
\begin{equation*}
    \text{softmax}(X) = [ \text{softmax}(X^{(1)})| \dots | \text{softmax}(X^{(N)}) ].
\end{equation*}

For a forward pass of the model $\flm(X;\theta)$, consider an input sequence of tokens $X \in \{ 0, 1 \}^{N \times V}$ as one-hot vectors where $n$ is sequence length. Then 

We feed the input through: 
\begin{enumerate}
     \item ($\fin$) Embedding layer: 
    \begin{equation*}
        \activation{\theta}{0}= \fin(X; \thetain) = X E \in \R^{N \times d_\text{emb}}
    \end{equation*}
    \item ($\fpre, \fmlp, \fpost$) For each Transformer block $i = 0, 1, \dots, L$, through $\fpre$, $\fmlp$, and $\fpost$:
    \begin{enumerate}
        \item Input layernorm: 
        \begin{equation*}
            X_{\text{LN}_1}^{(i)} = \frac{\activation{\theta}{i}}{\sqrt{\text{Var}(\activation{\theta}{i}) + \varepsilon}} \odot \gamma_{\text{input}, i}
        \end{equation*}
        (with variance over the last axis) for some offset $\varepsilon$ (typically 1e-6). 
        \item Causal multi-head self-attention: Split $X_{\text{LN}_1}^{(i)}$ on the first axis into nheads $X_{\text{LN}_1, j}^{(i)}, \dots, X_{\text{LN}_1, \text{nheads}}^{(i)}$. On each head $X_{\text{LN}_1, j}^{(i)}$, 
        \begin{equation*}
            X_{\text{SA}, j}^{(i)} = \text{self-attn}(X_{\text{LN}_1, j}^{(i)}) = \text{softmax}\left( \frac{X_{\text{LN}_1, j}^{(i)} W_{Q,i}^T (X_{\text{LN}_1, j}^{(i)} W_{K,i}^T)^T}{\sqrt{d_\text{emb}}} \right) X_{\text{LN}_1, j}^{(i)} W_{V,i}^T W_{O,i}^T
        \end{equation*}
        and concatenate $X_{\text{SA}, j}^{(i)}$ along the first axis again as $X_{\text{SA}}^{(i)}$.
        \item Dropout and residual connection: $X_{\text{DR}_1}^{(i)} = X_{\text{LN}_1}^{(i)} + \text{Dropout}(X_{\text{SA}}^{(i)})$
        \item Post-attention layernorm: 
        \begin{equation*}
            X_{\text{LN}_2}^{(i)} = \frac{X_{\text{DR}_1}^{(i)}}{\sqrt{\text{Var}(X_{\text{DR}_1}^{(i)}) + \varepsilon}} \odot \gamma_{\text{post-attn}, i}
        \end{equation*}
        (with variance over the last axis) for some offset $\varepsilon$. Then we have 
        \begin{equation*}
            \fpre(\activation{\theta}{i-1}; \thetapre^{(i)})=X_{\text{LN}_2}^{(i)}. 
        \end{equation*}
        \item Next, we feed through $\fmlp$, the multi-layer perceptron:
        \begin{equation*}
            \fmlp(X_{\text{LN}_2}^{(i)}; \thetamlp^{(i)}) = X_i^{\text{MLP}} = [ \sigma(X_i^{\text{LN}_2} G_i^T) \odot (X_i^{\text{LN}_2} U_i^T) ] D_i^T
        \end{equation*}
        for some activation $\sigma$ (e.g., SiLU). 
        \item Finally, we feed through $\fpost$, dropout and the residual connection: 
        \begin{equation*}
        \fpost(\thetamlp^{(i)})= \activation{\theta}{i+1} = X_i^{\text{DR}_1} + \text{Dropout}(X_i^{\text{MLP}})
        \end{equation*}
    \end{enumerate}
    \item ($\fout$) Final layernorm on the output $\activation{\theta}{N+1}$ from the final Transformer block:
    \begin{equation*}
        X_{\text{LN}}^{(L)} = \frac{\activation{\theta}{L}}{\sqrt{\text{Var}(\activation{\theta}{L}) + \varepsilon}} \odot \gamma_\text{final}
    \end{equation*}
    (with variance over the last axis) for some offset $\varepsilon$. Then, linear output embedding and softmax mapping to output probabilities: 
    \begin{equation*}
        \fout(\activation{\theta}{L}) = \text{softmax}(X_{\text{LN}}^{(L)}O^T), 
    \end{equation*}
\end{enumerate}
which defines the entire forward pass $\flm(X;\theta)$.

\section{Model Transformation Class}
\label{app:llama_permutation}

We describe two sets of equivariant transformations $\Pi$ on a Transformer model as described in Appendix \ref{app:llamaarchitecture}. (Abusing notation), the first set, $\Pi_\text{emb}$, consists of elements ${\pi_\text{emb}}$ where $\pi_\text{emb} \in \R^{d_\text{emb} \times d_\text{emb}}$ is a permutation matrix. The second set, $\Pi_\text{mlp}$, consists of elements $\pi_{\text{mlp}}$ where $\pi_\text{mlp} \in \R^{d_\text{mlp} \times d_\text{mlp}}$ is a permutation matrix. 

\begin{enumerate}
    \item $\pi_\text{emb}(\theta)$: Applying an embedding permutation $\pi_\text{emb} \in \mathbb{R}^{d_\text{emb} \times d_\text{emb}}$ by left or right multiplying all relevant matrices by $\xi_\text{embed}$ (permuting rows or columns).
    \item $\pi_\text{mlp}(\theta)$: Applying MLP permutations $\pi_{\text{mlp},i} \in \mathbb{R}^{d_\text{mlp} \times d_\text{mlp}}$ to MLP layers.
\end{enumerate}
These permutations are applied such that the outputs of the original model $\theta$ and the permuted model $\pi(\theta)$ remain aligned. We describe the details in Table \ref{tab:permute_model}. 
\begin{table}
    \centering
    \begin{tabular}{c|c|c|c}
    \hline 
        \textbf{Parameter name} & $\theta$ & $\pi_\text{emb}(\theta)$ & $\pi_\text{mlp}(\theta)$ \\ \hline
        embedding & $E$ & $E \pi_\text{emb}$ & $E$ \\ \hline 
        input layernorm & $\gamma_{\text{input},i}$ & $\gamma_{\text{input},i} \pi_\text{emb}$ & $\gamma_{\text{input},i}$ \\ 
        attention query matrix & \textbf{$W_{Q,i}$} & \textbf{$W_{Q,i}$}$\pi_\text{emb}$ & \textbf{$W_{Q,i}$} \\ 
        attention key matrix & \textbf{$W_{K,i}$} & \textbf{$W_{K,i}$}$\pi_\text{emb}$ & \textbf{$W_{K,i}$} \\ 
        attention value matrix & \textbf{$W_{V,i}$} & \textbf{$W_{V,i}$}$\pi_\text{emb}$ & \textbf{$W_{V,i}$} \\ 
        attention output matrix & \textbf{$W_{O,i}$} & $\pi_\text{emb}^T$\textbf{$W_{O,i}$} & \textbf{$W_{O,i}$} \\  
        post-attention layernorm & $\gamma_{\text{post-attn, } i}$ & $\gamma_{\text{post-attn, } i} \pi_\text{emb}$ & $\gamma_{\text{post-attn, } i}$ \\ \hline
        MLP gate projection & $G_i$ & $G_i \pi_\text{emb}$ & $\pi_{\text{mlp}, i} G_i$ \\ 
        MLP up projection & $U_i$ & $U_i \pi_\text{emb}$ & $\pi_{\text{mlp}, i} U_i$ \\ 
        MLP down projection & $D_i$ & $\pi_\text{emb}^T D_i$ & $D_i \pi_{\text{mlp}, i}^T$ \\ \hline 
        final layernorm & $\gamma_\text{final}$ & $\gamma_\text{final} \pi_\text{emb}$ & $\gamma_\text{final}$ \\ 
        linear output & $O$ & $\pi_\text{emb}^T O$ & $O$ \\ \hline 
    \end{tabular}
    \caption{We describe the equivariant transformation classes $\pi_\text{emb}$ and $\pi_\text{mlp}$ that we use in Llama-architecture model experiments. When running $\permtest$, we compose both $\pi_\text{emb}$ and $\pi_\text{mlp}$, with random permutations $\pi$ selected.}
    \label{tab:permute_model}
\end{table}

\section{Additional Constrained Setting Experimental Results}
\label{app:exp_results}
\label{app:nonrobust_results}

We report p-values from the constrained setting experiments --- statistics $\ltwo, \csw$, and $\csh$ on all 210 model pairs (from 21 Llama 2-architecture models) in Figures \ref{fig:ell2_pvalues}, \ref{fig:csw_pvalues}, and \ref{fig:csh_pvalues}, where the model names are colored by base model (ground truth). For all statistics, the p-values on independent model pairs are uniformly distributed, while they are all significant at $0.01$ and smaller for $\csw$ and $\csh$ (at $\varepsilon = $ 2.2e-308) for fine-tuned model pairs. 

As described in Section \ref{sec:constrained-results}, we compute $\ltwo$ with $\permtest$ with $T=99$; and for $\csw$ and $\csh$ we aggregate p-values across the 32 MLP submodels with $\fisher$. For $\csh$, activations are generated using inputs of sequences of tokens sampled uniformly from the models' vocabulary.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/l2-groundtruth.png}
    \caption{We report p-values from $\ltwo$ on Llama-7B model pairs, where $\ltwo$ is computed with $\permtest$ and $T=99$.}
    \label{fig:ell2_pvalues}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/csw-groundtruth.png}
    \caption{We report p-values from $\csw$ on Llama-7B model pairs, where $\varepsilon =$ 2.2e-308. We use $\speartest$ on $\csw$ and aggregate with $\fisher$ across the 32 MLPs.}
    \label{fig:csw_pvalues}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/csh-groundtruth.png}
    \caption{We report p-values from $\csh$ on Llama-7B model pairs, where $\varepsilon =$ 2.2e-308. We use $\speartest$ on $\csh$ and aggregate with $\fisher$ across the 32 MLPs.}
    \label{fig:csh_pvalues}
\end{figure}

\section{Additional Unconstrained Setting Experimental Results}
\label{app:robust_results}

We report values of $\rob$ on all model pairs in Figure \ref{fig:robust_stat}. The statistic is low ($< \varepsilon =$ 2.2e-308) for all non-independent model pairs, and uniformly distributed for independent model pairs, empirically acting as a p-value. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/robust-groundtruth.png}
    \caption{We report values of $\rob$ on Llama-7B model pairs, where $\varepsilon =$ 2.2e-308. We use $\speartest$ on $\rob$ and aggregate with $\fisher$ across the 32 MLPs.}
    \label{fig:robust_stat}
\end{figure}

\subsection{Striped Hyena Experiments}
\label{app:stripedhyena}
We report $\csw$ on specific parameters from \texttt{StripedHyena-Nous-7B} and \texttt{Mistral-7B-v0.1} shown in Table \ref{tab:stripedhyena}. We no longer only evaluate $\csw$ on MLP up projection matrices, so that we can investigate similarity in other parameters as well. These p-values no longer satisfy the independence requirement of Theorem \ref{thm:fisher}, so we do not aggregate them with $\fisher$.
\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
    \hline 
        Parameter name & Notation & $\csw$ \\ \hline 
        embedding & $E$ & 1.61e-16 \\ 
        attention query matrix & $W_Q^{(1)}$ & 6.17e-190 \\ 
        attention key matrix & $W_K^{(1)}$ & 1.47e-7 \\ 
        attention value matrix & $W_V^{(1)}$ & 1.56e-114 \\ 
        attention query matrix & $W_Q^{(1)}$ & 6.17e-190 \\ 
        attention output matrix & $W_O^{(1)}$ & 0.010 \\ 
        MLP gate projection & $G^{(1)}$ & 0.517 \\ 
        MLP up projection & $U^{(1)}$ & 0.716 \\ 
        MLP down projection & $D^{(1)}$ & 6.03e-80 \\ \hline 
    \end{tabular}
    \caption{We report $\csw$ on embedding and Transformer Block 1 parameters from \texttt{StripedHyena-Nous-7B} and \texttt{Mistral-7B-v0.1}, using $\csw$ on the specific parameters rather than only up-projection matrices. We find low p-values between some layers, including the embedding  matrix and attention matrices.}
    \label{tab:stripedhyena}
\end{table}


\subsection{MLP Retraining Experiments}
\label{app:retrain}

We retrain each of the 32 MLP layers by feeding in random inputs through the original MLP (gate, up, and down projection matrices.) We train for 10000 gradient steps using MSE loss and an Adam Optimizer with a learning rate of 0.001 and batch size of 5000. A sample learning curve is in Figure \ref{fig:mlpretrain_curve}. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/traincurve.png}
    \caption{We show a learning curve for the MLP retraining experiments used in Section \ref{sec:mlp-retrain} (retraining one MLP).}
    \label{fig:mlpretrain_curve}
\end{figure}

The MLP retraining results for all 32 MLP layers of \texttt{vicuna-7b-v1.5}, compared with \texttt{Llama-2-7b-hf} are in Table \ref{tab:mlp_retrain}, showing that the statistic is robust to retraining of all layers. 

\begin{table}[]
  \begin{minipage}{0.32\linewidth}
    \centering
    \begin{tabular}{c|c|c|}
    \hline 
        MLP & Loss & $\log_{10}(\rob^{(i)})$ \\ \hline
        1 & 0.0048 & ${-479}$ \\ 
        2 & 0.012 & ${-485}$ \\ 
        3 & 0.0026 & ${-614}$ \\ 
        4 & 0.0034 & ${-580}$ \\ 
        5 & 0.0030 & ${-523}$ \\ 
        6 & 0.0035 & ${-513}$ \\ 
        7 & 0.0041 & ${-533}$ \\ 
        8 & 0.0042 & ${-464}$ \\ 
        9 & 0.0050 & ${-439}$ \\ 
        10 & 0.0050 & ${-377}$ \\ 
        11 & 0.0060 & ${-365}$ \\ \hline 
    \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}{0.32\linewidth}
    \centering
\begin{tabular}{c|c|c|}
\hline 
        MLP & Loss & $\log_{10}(\rob^{(i)})$ \\ \hline
        12 & 0.0060 & ${-342}$ \\ 
        13 & 0.0058 & ${-330}$ \\ 
        14 & 0.0066 & ${-323}$ \\ 
        15 & 0.0063 & ${-414}$ \\ 
        16 & 0.0061 & ${-394}$ \\ 
        17 & 0.0063 & ${-445}$ \\ 
        18 & 0.0055 & ${-515}$ \\ 
        19 & 0.0045 & ${-571}$ \\ 
        20 & 0.0045 & ${-512}$ \\ 
        21 & 0.0047 & ${-595}$ \\ 
        22 & 0.0043 & ${-555}$ \\ \hline 
    \end{tabular}
  \end{minipage}
    \hfill
  \begin{minipage}{0.32\linewidth}
    \centering
\begin{tabular}{c|c|c}
\hline 
        MLP & Loss & $\log_{10}(\rob^{(i)})$ \\ \hline
        23 & 0.0043 & ${-593}$ \\ 
        24 & 0.0047 & ${-542}$ \\ 
        25 & 0.0050 & ${-497}$ \\ 
        26 & 0.0051 & ${-534}$ \\
        27 & 0.0052 & ${-482}$ \\ 
        28 & 0.0061 & ${-477}$ \\ 
        29 & 0.0065 & ${-433}$ \\
        30 & 0.0098 & ${-361}$ \\ 
        31 & 2.313 & ${-26.4}$ \\ 
        32 & 0.0114 & ${-174}$ \\ 
         & & \\ \hline 
    \end{tabular}
  \end{minipage}
  \caption{We retrain individual MLP blocks of \texttt{Llama-2-7b-hf} and \texttt{vicuna-7b-v1.5} then compute $\rob^{(i)}$ using Algorithm \ref{algorithm:general-test}. Even after retraining MLP layers, $\rob$ remains small (with log values reported here).
    }
    \label{tab:mlp_retrain}
\end{table}

\subsection{Fine-grained Forensics and Localized Testing}
\label{app:modelblockmatching}

As described in Section \ref{sec:localized-testing}, we can run $\rob$ on all pairs of Transformer blocks between two models (of different architecture), as long as they share the GLU structure. In addition to the Llama 3 results, we report results of matched blocks on the Sheared-LLaMa and Nvidia-Minitron models, which are both pruned from Llama models. 

In particular, we were able to identify the specific Transformer blocks of $\theta_{8B}= \texttt{Llama-3.1-8B}$ whose weights were likely used in initializing $\theta_{3B}=\texttt{Llama-3.2-3B}$ and $\theta_{1B}=\texttt{Llama-3.2-1B}$, as Meta reported that the \texttt{Llama-3.2-3B} and \texttt{Llama-3.2-1B} models were pruned from \texttt{Llama-3.1-8B} \citep{llamablog}. We use $\rob$ on all pairs of MLP blocks, where $(d_{\theta_{8B}},h_{\theta_{8B}},N_{\theta_{8B}})=(4096,14336,32)$,$(d_{\theta_{3B}},h_{\theta_{3B}},N_{\theta_{3B}})=(3072,8192,28)$, and $(d_{\theta_{1B}},h_{\theta_{1B}},N_{\theta_{1B}})=(2048,8192,16)$. We match blocks when the statistic $\rob^{(i,j)}$ from block $i$ of model 1 and block $j$ of model 2 is less than 1e-4, reported in Tables \ref{tab:llama3b} and \ref{tab:llama1b} (with the same for the other matchings in this section). 

\begin{table}[H]
    \centering
    \small 
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline 
        $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 \\ \hline
        $j:\rob^{(i,j)}(\theta_{8B},\theta_{3B})<1$e-4 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & & 11 & 12 & 13 & 14 & 15 \\ \hline
        \multicolumn{17}{c}{} \\ \hline 
        $i$ & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 & 32 \\ \hline
        $j:\rob^{(i,j)}(\theta_{8B},\theta_{3B})<1$e-4 &  & 16 & 17 & 18 & 19 & 20 &  & 21 & 22 & 23 & 24 & 25 &  & 26 & 27 & 28 \\ \hline 
    \end{tabular}
    \caption{We compute $\rob^{(i,j)}$ on all MLP blocks $i \in \{1,\dots,32 \}$ of $\theta_{8B}= \texttt{Llama-3.1-8B}$ and MLP blocks $j \in \{1,\dots, 28 \}$ of  $\theta_{3B}=\texttt{Llama-3.2-3B}$. We report pairs $(i,j)$ such that $\rob^{(i,j)} <1$e-4.  We find our test can identify the matching blocks even after pruning.}
    \label{tab:llama3b}
\end{table}

\begin{table}[H]
    \centering
    \small 
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline 
        $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 \\ \hline
        $j:\rob^{(i,j)}(\theta_{8B},\theta_{1B})<1$e-4 & 1 & 2 & 3 & 4 & 5 & 6 & & & 7 & & & 8 & & & 9 & \\ \hline 
        \multicolumn{17}{c}{} \\ \hline 
        $i$ & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 & 32 \\ \hline
        $j:\rob^{(i,j)}(\theta_{8B},\theta_{1B})<1$e-4 & & 10 & & & 11 & & & & & & & & & & 15 & 16 \\ \hline 
    \end{tabular}
    \caption{We compute $\rob^{(i,j)}$ on all MLP blocks $i \in \{1,\dots,32 \}$ of $\theta_{8B}= \texttt{Llama-3.1-8B}$ and MLP blocks $j \in \{1,\dots, 16 \}$ of  $\theta_{1B}=\texttt{Llama-3.2-1B}$. We report pairs $(i,j)$ such that $\rob^{(i,j)} <1$e-4.}
    \label{tab:llama1b}
\end{table}

Next, we test \texttt{Sheared-LLaMa-2.7B}, with 32 Transformer blocks, hidden dimension 2560 and MLP dimension 6912. All 32 blocks align with the 32 blocks of Llama 2-7B, although both hidden and MLP dimensions have been reduced through pruning. 
\begin{table}[H]
    \centering
    \small 
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline 
        $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 \\ \hline
        $j:\rob^{(i,j)}(\theta_{1},\theta_{2})<1$e-90 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 \\ \hline 
        \multicolumn{17}{c}{} \\ \hline 
        $i$ & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 & 32 \\ \hline
        $j:\rob^{(i,j)}(\theta_{1},\theta_{2})<1$e-90 &  17 & 18 & 19 & 20 &  21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 & 32 \\ \hline 
    \end{tabular}
    \caption{We compute $\rob^{(i,j)}$ on all MLP blocks $i \in \{1,\dots, 32\}$ of $\theta_{1}= \texttt{Sheared-LLaMa-2.7B}$ and MLP blocks $j \in \{1,\dots, 32 \}$ of  $\theta_{2}=\texttt{Llama-2-7B}$. We report pairs $(i,j)$ such that $\rob^{(i,j)} <1$e-90.}
    \label{tab:shearedllama2-7b}
\end{table}

Next, we test \texttt{Sheared-LLaMa-1.3B}, with 24 Transformer blocks, hidden dimension 2048 and MLP dimension 5504.

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline 
        $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\ \hline
        $j:\rob^{(i,j)}(\theta_{1},\theta_{2})<1$e-5 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 10 & 12 & & 16   \\ \hline 
        \multicolumn{13}{c}{} \\ \hline 
        $i$ & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 \\ \hline
        $j:\rob^{(i,j)}(\theta_{1},\theta_{2})<1$e-5 & 17 & 18 & 19 & 20 & 21 & 22 & 25 & 27 & 28 & 29 & 31 & 32 \\ \hline 
    \end{tabular}
    \caption{We compute $\rob^{(i,j)}$ on all MLP blocks $i \in \{1,\dots, 24\}$ of $\theta_{1}= \texttt{Sheared-LLaMa-1.3B}$ and MLP blocks $j \in \{1,\dots, 32 \}$ of  $\theta_{2}=\texttt{Llama-2-7B}$. We report pairs $(i,j)$ such that $\rob^{(i,j)} <1$e-5.}
    \label{tab:shearedllama1-3b}
\end{table}

Finally, we compare Llama 3.1-8B with $\texttt{Llama-3.1-Minitron-4B-Depth-Base}$, a pruned model by reducing from 32 to 16 Transformer blocks and are able to identify the likely shared blocks. 
\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline 
        $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 \\ \hline
        $j:\rob^{(i,j)}(\theta_{1},\theta_{2})<1$e-90 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 32 \\ \hline 
    \end{tabular}
    \caption{We compute $\rob^{(i,j)}$ on all MLP blocks $i \in \{1,\dots, 32\}$ of $\theta_{1}= \texttt{Llama-3.1-Minitron-4B-Depth-Base}$ and MLP blocks $j \in \{1,\dots, 32 \}$ of  $\theta_{2}=\texttt{Llama-3.1-8B}$. We report pairs $(i,j)$ such that $\rob^{(i,j)} <1$e-90.}
    \label{tab:nvidia-minitron}
\end{table}

\section{Output-Preserving Transformations}
\label{app:robust}

An adversary could apply a particular rotation scheme by multiplying weight matrices by an orthogonal rotation matrix $U$ that will also preserve outputs. We describe such a transformation which breaks the invariants proposed by \cite{zeng2024humanreadablefingerprintlargelanguage} by manipulating layernorms. While this list may not be exhaustive, the following six transformations (with the first two described previously) ``camouflage'' the language model while preserving outputs: 
\begin{enumerate}[
        label={T\arabic*.},
        ref={T\arabic*.}]
    \item Permuting the rows of the embedding matrix (and subsequent matrices due to residual connections) by a permutation $\xi_{\text{emb}} \in \R^{d_{\text{emb}} \times d_{\text{emb}}}$
    \item Permuting the MLP matrices ($N$ different permutations for each Transformer block) by permutations $\xi_1, \dots, \xi_{N} \in \R^{d_{\text{mlp}} \times d_{\text{mlp}}}$
    \item Rotating the embedding matrix (and subsequent matrices due to residual connections) by an orthogonal rotation matrix $R_{\text{emb}} \in \R^{d_{\text{emb}} \times d_{\text{emb}}}$
    \item Rotating the query and key attention matrices ($N$ different rotations for each Transformer block) by orthogonal rotation matrices $R_1, \dots, R_{N} \in \R^{d_{\text{emb}} \times d_{\text{emb}}}$
    \item Replacing all layernorms (input, post-attention, final) with vectors in $\R^{1\times d_{\text{emb}}}$ with non-zero elements
    \item Scaling the MLP matrices by a constant non-zero factor
\end{enumerate}

Consider a model $\theta$ of Llama architecture (Appendix \ref{app:llamaarchitecture}). Consider orthogonal matrices $R_{\text{emb}}, R_1, \dots R_{32}$ as described, as well as new layernorms $\gamma'_{\text{input}, 1}, \dots, \gamma'_{\text{input}, 32}, \gamma'_{\text{post-attn}, 1}, \dots, \gamma'_{\text{post-attn}, 32}$ in $\R^{1 \times d_{\text{emb}}}$ with non-zero elements. 
Finally, consider non-zero constants $c_1, \dots, c_{32}$, which we use to transform the layernorms. We apply the rotation with these parameters to $\theta$, to get a new ``rotated'' model, Rot$(\theta)$. We generalize the set of transformations above as applying Rot$(\theta)$ to a model $\theta$. 

We transform all the original matrices of $\theta$ as in Table \ref{tab:rotation_scheme} (for $i = 1, \dots, 32$). 
Note that the transformations T1 and T2 are elements of the classes $\pi_\text{emb}$ and $\pi_\text{mlp}$, respectively, and the remaining transformations T3 to T6 are described in Table \ref{tab:rotation_scheme}. Importantly, T5 is the transformation that \cite{zeng2024humanreadablefingerprintlargelanguage}'s invariants are not robust to; our unconstrained setting test $\rob$ is robust to all 6 transformations, which we show in Table \ref{tab:huref_invariants}.

\subsection{Breaking HuREF Invariants}
\label{app:breakhuref}

Only transformations T3 and T5 is required to break the invariants from \cite{zeng2024humanreadablefingerprintlargelanguage}. Their first invariant is $M_a =  E (W_{Q,i})^T W_{K,i}) E^T$ at layer $i$, and for $M'$ with an embedding matrix rotation $R_\text{emb}$ where the layernorms $\gamma_{\text{input},i}$ are replaced with $\gamma'_{\text{input},i}$, we have the invariant is
\begin{align*}
    M'_a &= (E R_\text{emb}) \left( \text{diag}( \frac{1}{\gamma'_{\text{input}, i}}) R_\text{emb}^T  \text{diag}(\gamma_{\text{input}, i}) W_{Q,i}^T \right) \left( W_{K,i} \text{diag}(\gamma_{\text{input}, i})  R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{input}, i}}) \right) (R_\text{emb}^T E^T) 
\end{align*}
and in general $M_a \neq M'_a$ unless the layernorm weights are equal constants. Similarly, we have the second invariant is 
\begin{align*}
    M'_b &= (E R_\text{emb}) \left( \text{diag}( \frac{1}{\gamma'_{\text{input}, i}}) R_\text{emb}^T  \text{diag}(\gamma_{\text{input}, i}) W_{V,i}^T \right) \left( W_{O,i} \text{diag}(\gamma_{\text{input}, i})  R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{input}, i}}) \right) (R_\text{emb}^T E) \\ &\neq E W_{V,i}^T W_{O,i} E^T = M_b
\end{align*}
in general, also failing due to the layernorm and rotation $R_\text{emb}$ (note that our notation for Transformers is different than theirs). Finally, assuming for invariant $M_f$ that $W_1$ and $W_2$ are the gate and down projection matrices of an MLP (this is not stated explicitly in the paper but can be inferred from experiments), the remaining invariants do not hold either: 
\begin{align*}
    M'_f &= (E R_\text{emb}) \left( \text{diag}( \frac{1}{\gamma'_{\text{input}, i}}) R_\text{emb}^T  \text{diag}(\gamma_{\text{input}, i}) G_i^T \right) \left( D_i \text{diag}(\gamma_{\text{input}, i})  R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{input}, i}}) \right) (R_\text{emb}^T E) \\ &\neq E G_i^T D_i E^T = M_f,
\end{align*}
so all their proposed invariants can be bypassed by these two transformations. 

Empirically, we we show this by computing all the invariants between \texttt{Llama-2-7b-hf} and independently trained models and between \texttt{Llama-2-7b-hf} and rotated finetuned models (including \texttt{Llama-2-7b-hf} itself) in Table \ref{tab:huref_invariants}. We can see there is little distinction between the independent vs. non-independent model pairs. 

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{c c | c c c | c c c c}
    \hline 
        $\theta_1 = \texttt{Llama-2-7b-hf}, \theta_2=$ & Indep.? & $M_a$ & $M_b$ & $M_f$ & $\rob$ & $\csw$ & $\csh$ & $\jsd$ \\ \hline 
        \texttt{vicuna-7b-v1.5}& \ding{55} & 1.0 & 0.9883 & 0.9922 & $\varepsilon$ & $\varepsilon$ & $\varepsilon$ & -10.874 \\
        \texttt{Nous-Hermes-llama-2-7b}& \ding{55} & 1.0 & 1.0 & 1.0 & $\varepsilon$ & $\varepsilon$ & $\varepsilon$ & -12.101 \\ \hline 
         \texttt{llama-7b-hf} & \ding{51} & 0.0884 & 0.0250 & 0.0400 & 0.049 & 0.595 & 0.253 & -11.102 \\
         \texttt{AmberChat} & \ding{51} & 0.1289 & -0.0093 & 0.0198 & 0.941 & 0.460 & 0.279 & -10.281 \\
         \texttt{Openllama-v1} & \ding{51} & 0.1084 & 0.0076 & 0.0057 & 0.286 & 0.357 & 0.703 & -8.381 \\ \hline 
         Rotated \texttt{Llama-2-7b-hf} & \ding{55} & 0.0767 & 0.0908 & 0.1011  & $\varepsilon$ & 0.517 & 0.323 & $-\infty$\\ 
         Rotated \texttt{vicuna-7b-v1.5} & \ding{55} & 0.1553 & 0.0933 & 0.0977 & $\varepsilon$ & 0.688 & 0.857 & -10.874 \\
         Rotated \texttt{Nous-Hermes-llama-2-7b} & \ding{55} & 0.0332 & 0.0718 & 0.1060 & $\varepsilon$ & 0.772 & 0.240 & -12.101 \\ \hline 
    \end{tabular}
    \caption{Results for the three invariants $M_a, M_b, M_c$ from \cite{zeng2024humanreadablefingerprintlargelanguage} between \texttt{Llama-2-7b-hf} and independent and non-independent models.}
    \label{tab:huref_invariants}
\end{table}

Notably, we see that $\rob$ remains $\varepsilon$ even for rotated dependent model pairs, so our test is robust to these transformations. 

\begin{table}
    \centering
    \begin{tabular}{c|c|c}
    \hline 
        \textbf{Parameter name} & $\theta$ & $\text{Rot}(\theta)=\theta'$ \\ \hline
        embedding & $E$ & $ER_\text{emb}$ \\ 
        input layernorm & $\gamma_{\text{input, } i}$ & $\gamma'_{\text{input, } i}$ \\ 
        attention query matrix & \textbf{$W_{Q,i}$} & $R_i$ \textbf{$W_{Q,i}$} diag($\gamma_{\text{input, } i}$) $R_\text{emb}$ diag( $\frac{1}{\gamma'_{\text{input, } i}}$) \\ 
        attention key matrix & \textbf{$W_{K,i}$} & $R_i$ \textbf{$W_{K,i}$} diag($\gamma_{\text{input, } i}$) $R_\text{emb}$ diag( $\frac{1}{\gamma'_{\text{input, } i}}$) \\ 
        attention value matrix & \textbf{$W_{V,i}$} & \textbf{$W_{V,i}$} diag($\gamma_{\text{input, } i}$) $R_\text{emb}$ diag( $\frac{1}{\gamma'_{\text{input, } i}}$) \\ 
        attention output matrix & \textbf{$W_{O,i}$} & $R_\text{emb}^T$ \textbf{$W_{O,i}$} \\ 
        post-attention layernorm & $\gamma_{\text{post-attn, } i}$ & $\gamma'_{\text{post-attn, } i}$ \\
        MLP gate projection & $G_i$ & $G_i$ diag($\gamma_{\text{post-attn}, i}$) $R_\text{emb}$ diag( $\frac{1}{\gamma'_{\text{post-attn},i}}$) \\ 
        MLP up projection & $U_i$ & $c_i U_i$ diag($\gamma_{\text{post-attn}, i}$) $R_\text{emb}$ diag( $\frac{1}{\gamma'_{\text{post-attn},i}}$) \\ 
        MLP down projection & $D_i$ & $\frac{1}{c_i} $ $R_\text{emb}^T$ $D_i$ \\ 
        final layernorm & $\gamma_\text{final}$ & $\gamma'_\text{final}$ \\ 
        linear output & $O$ & $O$ diag($\gamma_\text{final}$) $R_\text{emb}$ diag($\frac{1}{\gamma'_\text{final}}$) \\ \hline 
    \end{tabular}
    \caption{We describe the output-preserving rotation applied to the parameters of a Llama-architecture model.}
    \label{tab:rotation_scheme}
\end{table}

\subsection{Invariance of Outputs under Rotation}

These transformations are particularly important because they preserve outputs as we show in Theorem \ref{thm:output-preserving}, and hence generally can go undetected, though $\rob$ is robust to them.

\begin{theorem}
\label{thm:output-preserving}
For any input sequence $X \in \{ 0, 1 \}^{n \times V}$, the outputs of models $\theta$ and $\text{Rot}(\theta)=\theta'$ are aligned, i.e. $\flm(X;\theta) = \flm(X;\theta')$.
\end{theorem}

\begin{proof}

First, note that an element-wise product of two one-dimensional vectors is equivalent to multiplying by the diagonal matrix of the second vector, i.e. for $v, \gamma \in R^{1 \times m}$, 
\begin{equation*}
    v \ast \gamma = v \text{diag}(\gamma).
\end{equation*}
We use this in our layernorm calculations. 

Let the output from the unrotated embedding layer be $y = \fin(X, E)= EX$ (for $X \in \{ 0, 1 \}^{n \times V}$). Then the output from the rotated embedding layer is $ y' = \fin(X,E')=(ER_\text{emb})(x) = y R_\text{emb}$. Now consider Transformer block $i$ with input $y$ and the rotated Transformer block with input $y R_\text{emb}$. $y$ is passed into the input layernorm, which returns 
\begin{equation*}
    z = LN_i(y) = \frac{y}{\sqrt{\var(y) + \varepsilon}} \odot \gamma_{\text{input}, i} = \frac{y}{\sqrt{\var(y) + \varepsilon}} \text{diag}(\gamma_{\text{input}, i}). 
\end{equation*}
The rotated input layernorm on $y'$ returns

\begin{align*}
    z' = LN'_i(y') &= \frac{y'}{\sqrt{\var(y') + \varepsilon}} \odot \gamma'_{\text{input}, i}  = \frac{y R_\text{emb}}{\sqrt{\var(y R_\text{emb}) + \varepsilon}} \odot \gamma'_{\text{input}, i} \\ &= \frac{y }{\sqrt{\var(y) + \varepsilon}} R_\text{emb} \text{diag}(\gamma'_{\text{input}, i}) = z \text{ diag}( \frac{1}{\gamma_{\text{input}, i}}) R_\text{emb} \text{diag}(\gamma'_{\text{input}, i}), 
\end{align*}
which follows from $R_\text{emb}$ being orthogonal. Then we have the output from the unrotated self-attention is 
\begin{align*}
    w = \text{softmax} \left( \frac{zW_{Q,i}^T (zW_{K,i}^T)^T}{\sqrt{d_\text{key}}} \right) zW_{V,i}^T W_{O,i}^T,
\end{align*}
and the output from the rotated self-attention with input $z'$ is 
\begin{align*}
    &\text{softmax} \left( \frac{z' (R_iW_{Q,i} \text{diag}(\gamma_{\text{input, } i}) R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{input, } i}}) )^T (z' (R_i W_{K,i} \text{diag}(\gamma_{\text{input, } i}) R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{input, } i}}))^T )^T }{\sqrt{d_\text{key}}} \right) \\ &z' (W_{V,i} \text{diag}(\gamma_{\text{input, } i}) R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{input, } i}}))^T (R_\text{emb}^T W_{O,i})^T \\ &= 
    \text{softmax} \left( \frac{z' \text{diag}( \frac{1}{\gamma'_{\text{input, } i}}) R_\text{emb}^T \text{diag}(\gamma_{\text{input, } i})  W_{Q,i}^T  R_i^T (z' \text{diag}( \frac{1}{\gamma'_{\text{input, } i}}) R_\text{emb}^T  \text{diag}(\gamma_{\text{input, } i})   W_{K,i}^T R_i^T )^T }{\sqrt{d_\text{key}}} \right) \\ &z' \text{diag}( \frac{1}{\gamma'_{\text{input, } i}}) R_\text{emb}^T \text{diag}(\gamma_{\text{input, } i})  W_{V,i}^T  W_{O,i}^T R_\text{emb} \\ 
    &=
    \text{softmax} \left( \frac{z' \text{diag}( \frac{1}{\gamma'_{\text{input, } i}}) R_\text{emb}^T \text{diag}(\gamma_{\text{input, } i})   W_{Q,i}^T W_{K,i} \text{diag}(\gamma_{\text{input, } i}) R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{input, } i}})  (z')^T }{\sqrt{d_\text{key}}}\right) z W^T_{V,i} W^T_{O,i} R_\text{emb} \\ 
    &= \text{softmax} \left( \frac{z W_{Q,i} W^T_{K,i}  z^T}{\sqrt{d_\text{key}}} \right) z W^T_{V,i} W^T_{O,i} R_\text{emb} 
    \\ &= w R_\text{emb} = w'. 
\end{align*}
Then $y$ and $y'$ respectively from before the layernorm are added as residual connections as $v = y+w$ and $v' = y'+w' = v R_\text{emb}$. $v$ is passed into the post-attention layernorm, which returns 
\begin{equation*}
    u = LN_i(v) = \frac{v}{\sqrt{\var(v) + \varepsilon}} \odot \gamma_{\text{post-attn}, i} = \frac{v}{\sqrt{\var(v) + \varepsilon}} \text{diag}(\gamma_{\text{post-attn}, i}). 
\end{equation*}
Similar to the input layernorm, the rotated post-attention layernorm on $v'$ returns 
\begin{align*}
    u' = LN'_i(v') &= \frac{v'}{\sqrt{\var(v') + \varepsilon}} \odot \gamma'_{\text{post-attn}, i}  = \frac{v R_\text{emb}}{\sqrt{\var(v R_\text{emb}) + \varepsilon}} \odot \gamma'_{\text{post-attn}, i} \\ &= \frac{v}{\sqrt{\var(v) + \varepsilon}} R_\text{emb} \text{diag}(\gamma'_{\text{post-attn}, i}) = u \text{ diag}( \frac{1}{\gamma_{\text{post-attn}, i}}) R_\text{emb} \text{diag}(\gamma'_{\text{post-attn}, i}).
\end{align*}

Then the output from the unrotated MLP layer on $u$ is 
\begin{align*}
    t &= [ \sigma(u G_i^T) \odot (u U_i^T) ] D_i^T
\end{align*}
and the output from the rotated MLP on $u'$ is 
\begin{align*}
    t' &= [ \sigma(u' (G_i \text{diag}(\gamma_{\text{post-attn}, i}) R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{post-attn},i}}))^T \odot (u' (c_i U_i \text{diag}(\gamma_{\text{post-attn}, i}) R_\text{emb} \text{diag}( \frac{1}{\gamma'_{\text{post-attn},i}}))^T) ] (\frac{1}{c_i} R_\text{emb}^T D_i)^T \\ 
    &= [ \sigma(u \text{ diag}( \frac{1}{\gamma_{\text{post-attn}, i}}) R_\text{emb} \text{diag}(\gamma'_{\text{post-attn}, i}) \text{diag}( \frac{1}{\gamma'_{\text{post-attn},i}})  R_\text{emb}^T  \text{diag}(\gamma_{\text{post-attn}, i}) G_i^T) \odot \\ & (c_i u \text{ diag}( \frac{1}{\gamma_{\text{post-attn}, i}}) R_\text{emb} \text{diag}(\gamma'_{\text{post-attn}, i}) \text{diag}( \frac{1}{\gamma'_{\text{post-attn},i}})  R_\text{emb}^T  \text{diag}(\gamma_{\text{post-attn}, i})) U_i^T ] \frac{1}{c_i} D_i^T R_\text{emb} \\ 
    &= [ c_i \sigma(u G_i^T) \odot (u U_i^T) ] \frac{1}{c_i} D_i^T R_\text{emb} = t R_\text{emb}. 
\end{align*}
Then the output from the self-attention is added as a residual connection, and the final output from the unrotated Transformer block is $s = t+v$, and the output from the rotated Transformer block is $s' = t' + v' = s R_\text{emb}$. 

Suppose $a$ is the output after all Transformer layers in $\theta$ and $a'$ is the output after all Transformer layers in $\theta'$. Then the outputs after the final layernorms are
\begin{equation*}
    b = \frac{v}{\sqrt{\var(a) + \varepsilon}} \text{diag}(\gamma_\text{final})
\end{equation*}
\begin{equation*}
    b' = b \text{ diag}( \frac{1}{\gamma_{\text{final}}})  R_\text{emb} \text{diag}(\gamma'_{\text{final}}), 
\end{equation*}
and the logits from the linear output layer are 
\begin{align*}
    b O^T &= b \text{ diag}( \frac{1}{\gamma_{\text{final}}})  R_\text{emb} \text{diag}(\gamma'_{\text{final}}) \text{diag}(\gamma_\text{final})R_\text{emb}^T  \text{diag}( \frac{1}{\gamma'_\text{final}}) O^T \\ &= b' (O')^T,
\end{align*}
which are the same for both models.
\end{proof}

We attempted to undo such a transformation that an adversary may apply by solving the least squares problem: We solve for a rotation $A$ that minimizes $\vert AX - Y \vert $ where $X$ is a weight matrix of the first model and $Y$ is the corresponding weight matrix of the second model. Although this will provide a potential rotation to undo this transformation, we find that this solution will also find a matrix $A$ that aligns two independent model pairs as well. This makes undo-ing the rotation this way unreliable. 
The same holds for $X$ and $Y$ that are activations over multiple inputs. 