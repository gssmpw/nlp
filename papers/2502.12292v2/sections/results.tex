\section{Experimental Results}

\subsection{Constrained setting}
\label{sec:constrained-results}

We first validate the effectiveness of our tests in the constrained setting on open-weight language models --- 21 models trained with the Llama-7B architecture with public documentation on ground truth model independence. These models all contain $L = 32$ GLU MLPs, each part of its own Transformer block.

We run experiments with three different tests. Each test comprises two elements: a test statistic along with a method for computing p-values from the statistic.
For the first test, we use $\ltwo$ and compute p-values via $\permtest$ with $T = 99$.
The equivariant transformation class $\Pi$ is the set of permutations over both the hidden units of each MLP (see Example~\ref{example:permuting-hidden-units}) and the embedding dimension of the model (i.e., the inputs passed to the both the MLP and self-attention layers in each block); we defer the precise definition of $\Pi$ in this case to Appendix \ref{app:llama_permutation}.
For the other two tests, we compute p-values by directly aggregating the outputs of (respectively) $\csw$ and $\csh$ over $\ell \in [L]$ using $\fisher$.
We sample sequences of 4096 tokens uniformly at random from the models' vocabulary, then compute a forward pass through the full model while storing the MLP hidden layer activations, and use the input activations to the GLU MLP in the $\ell$-th layer as the activations to compute $\csh$.

In addition to these three tests, we report the Jensen-Shannon divergence between next token output distributions $\jsd$ \citep{jsd}.
Since $\jsd$ is (by definition) invariant to any transformation of weights that does not affect model output, we cannot compute meaningful p-values using $\permtest$; instead, in our experiments we report the raw test statistic value as a baseline reference.

\subsubsection{Results for Llama model tree}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figs/llama_lineage_diagram.png}
    \caption{We enumerate the public Llama-7B models and delineate the sets of dependent model pairs by color.}
    \label{fig:model-lineage-forest}
\end{figure}

The 21 models we evaluated, shown in Figure \ref{fig:model-lineage-forest}, include 6 base (trained from scratch) models of Llama-7B architecture and a variety of their finetunes, which form six disjoint sets of models stemming from a diverse mix of industry labs and non-profits \citep{azerbayev2024llemmaopenlanguagemodel, sudalairaj2024lablargescalealignmentchatbots, liu2023llm360, li2023camelcommunicativeagentsmind}.
We consider any pair of models in the same tree as dependent and all other pairs as independent.
We include examples of further fine-tunes (e.g., \texttt{llemma\_7b}) of fine-tunes (e.g., \texttt{CodeLlama-7b-hf}) among the models we test.
We will mostly refer to models using by their Hugging Face identifiers, without the organization names for brevity.
We report results for a subset of these pairs involving base model \texttt{Llama-2-7b-hf} in Table~\ref{tab:basic_statistics_llama2} while deferring the rest and the full experimental setup details to Appendix \ref{app:exp_results}.

\begin{table}[h]
    \centering
    \begin{tabular}{c c|c|cc c c}
    \hline 
         & &  & & & p-values & \\ 
        %\textbf{Model} 
        $\theta_1$ & $\theta_2 $ & Indep.? & $\jsd$ (log) & $\ltwo$ & $\csw$ & $\csh$ \\ \hline 
         \texttt{Llama-2-7b-hf} & \texttt{llama-7b-hf} & \ding{51} & -11.10 & 0.98 & 0.60 & 0.25 \\
         \texttt{Llama-2-7b-hf} & \texttt{vicuna-7b-v1.1} & \ding{51} & -10.40 & 0.63 & 0.16& 0.64 \\
         \texttt{Llama-2-7b-hf} & \texttt{Amber} & \ding{51} & -10.69 & 0.75 & 0.36 & 0.88 \\
         \texttt{Llama-2-7b-hf} & \texttt{open-llama-7b} & \ding{51} & -8.38 & 0.26 & 0.36 & 0.71 \\
         %\texttt{xgen-7b-4k-base} & \ding{51} & -8.42 & 0.12 & 0.001 & 0.01 \\ 
         \hline 
         \texttt{Llama-2-7b-hf} & \texttt{vicuna-7b-v1.5} & \ding{55} & -10.87 & 0.01 & $\varepsilon$ & $\varepsilon$ \\ 
         \texttt{Llama-2-7b-hf} & \texttt{CodeLlama-7b-hf} & \ding{55} & -10.62 & 0.01 & $\varepsilon$ & $\varepsilon$\\
         \texttt{Llama-2-7b-hf} & \texttt{llemma-7b} & \ding{55} & -10.24 & 0.01 & $\varepsilon$ & $\varepsilon$ \\
         \texttt{Llama-2-7b-hf} & \texttt{Orca-2-7b} & \ding{55} & -10.34 & 0.01 & $\varepsilon$ & $\varepsilon$ \\ \hline 
    \end{tabular}
    \caption{We report various constrained setting test statistics with $\firstparam$ as \texttt{Llama-2-7b-hf} and $\secondparam$ ranging over the listed models. The ``independent'' column is the ground truth. Here, $\varepsilon =$ 2.2e-308 (numerical underflow for a 64-bit float). We find our proposed tests $\csw$ and $\csh$ distinguish independent versus non-independent model pairs with high statistical power.
    }
    \label{tab:basic_statistics_llama2}
\end{table}

Consistent with prior work \cite{xu2024instructionalfingerprintinglargelanguage}, 
we find that $\jsd$ does not reliably distinguish independent versus dependent model pairs. For example, \texttt{CodeLlama-7b-hf} exhibits a larger divergence with \texttt{Llama-2-7b-hf} than the independently-trained models \texttt{llama-7b-hf} and \texttt{Amber}.

All other test statistics reliably distinguish independent versus dependent pairs; in particular, the p-values we obtain using the other test statistics are negligible for all dependent pairs (for $\ltwo$, because we run $\permtest$ with $T = 99$ for computational reasons, we cannot obtain a p-value less than $0.01$).
Notably, in contrast to our findings, prior work \citep{xu2024instructionalfingerprintinglargelanguage} argued that the $\ell_2$ distance between model parameters is not a reliable indicator of independence, in the sense that the $\ell_2$ distance between dependent pairs is sometimes larger than that of independent pairs (similar to the case of $\jsd$); the key difference is that \citet{xu2024instructionalfingerprintinglargelanguage} report the raw $\ell_2$ distance whereas we obtain p-values from the raw distances using $\permtest$.
We hypothesize that $\permtest$ effectively standardizes the raw distances.

Finally, we evaluated our tests on models of different architectures besides the Llama-7B architecture. We ran $\csw$ on four 70B parameter models, each with the same Llama 2-70B architecture, with results shown in Table \ref{tab:70b}. Notably, we verify that Miqu-70B from MistralAI is not independent from Llama 2-70B \citep{2024miquscandal}. 

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
    \hline 
        $\theta_1$ & $\theta_2$ & $\csw$ \\ \hline
        \texttt{Llama-2-70b-hf} & \texttt{miqu-1-70b-pytorch} & $\varepsilon$ \\ 
        \texttt{Llama-2-70b-hf} & \texttt{Llama-3.1-70B} & 0.571 \\ 
        \texttt{Llama-2-70b-hf} & \texttt{Palmyra-Fin-70B-32K} & 0.539 \\ \hline 
    \end{tabular}
    \caption{We evaluate the constrained setting test involving up-projection weights $\csw$ (aggregated with $\fisher$) with $\theta_1$ as \texttt{Llama-2-70b-hf} and $\theta_2$ ranging over the listed models. Here, $\varepsilon = $ 2.2e-308, notably suggesting that \texttt{Llama-2-70b-hf} and the leaked Mistral model \texttt{miqu-1-70b-pytorch} are not independent.}
    \label{tab:70b}
\end{table}

\subsection{Unconstrained setting}
\label{sec:adversarial-experiments}

Next, we evaluate our robust test $\rob$ in the unconstrained setting, which encompasses varying model architecture and adversarial evasion attacks. We also examine the internal values within $\rob$ from the $\speartest$ algorithm to perform localized testing.

We first assess the previous 21 models of the Llama-7B architecture. We compute $\rob$ with the gate and up-projection matrices $M = H_\text{gate}^{\ell}$ and $M' = H_\text{up}^{\ell}$ of each MLP in block $\ell \in [L]$, and aggregate them with $\fisher$. We obtain the activations in the MLPs by using input sequences sampled from WikiText-103 and computing a forward pass through the full model, 
with results on all model pairs in Appendix \ref{app:robust_results}. 

We find that the distribution of $\rob$ on independent model pairs is close to uniform 
(Figure \ref{fig:phimatchlines}), whereas across all non-independent model pairs the statistic is at most $\eps$. 
Unlike the $\constr$ setting, where the p-values are valid by construction, the output of the robust test does not enjoy such theoretical guarantees; however, Figure~\ref{fig:phimatchlines} suggests that even in the $\unconstr$ setting our statistic $\rob$ behaves like a p-value, i.e. that it is uniformly distributed on $[0,1)$ under the null hypothesis. 

\begin{figure}[h]
\centering
  \begin{subfigure}[]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figs/phi_MATCH_all.png}
    \caption{Plot of $x \in [0,1)$ vs. the fraction of $\rob^{(i)}$ (across all MLP blocks) of independent model pairs less than $x$.}
    \label{fig:robust_stat_all}
  \end{subfigure}
  %\hfill
  \begin{subfigure}[]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figs/phi_MATCH_agg.png}
    \caption{Plot of $x \in [0,1)$ vs. the fraction of $\rob$ ($\rob^{(i)}$ aggregated with $\fisher$ across all MLP blocks) of independent model pairs less than $x$.}
    \label{fig:robust_stat_agg}
  \end{subfigure}
  \caption{We plot the fraction of the statistic $\rob$ less than $x \in [0,1)$, aggregated with $\fisher$ and not for independent model pairs. Both plots roughly follow the line $y=x$, i.e. a uniform distribution in $[0,1)$ under the null, meaning $\rob$ empirically acts as a p-value.}
  \label{fig:phimatchlines}
\end{figure}

We next validate our tests on other model pairs with differing architectures: we compare the weights of the hybrid model \texttt{StripedHyena-Nous-7B} \citep{stripedhyena} with \texttt{Mistral-7B-v0.1} (where some layers of \texttt{StripedHyena-Nous-7B} are taken from \texttt{Mistral-7B-v0.1} and others are not) and find non-independent parameters via $\csw$. We compute $\csw$ on all parameters, which allows us to identify non-independence between specific parameters of the models, such as the self-attention matrices, rather than as models as a whole. We report values of $\csw$ among parameters of the embedding layer and first Transformer block in Table \ref{tab:stripedhyena} in Appendix \ref{app:stripedhyena}. From the small p-values we infer that some parameters, including the embedding layer and some self-attention matrices, were likely shared between the two models. 

\subsubsection{Testing identically distributed but independent models}
\label{sec:independent-similar-models}

We further evaluate the validity of our robust test through ablations by training two near-identical models that only differ on select sources of randomness. Specifically, we would like to check that $\rob$ does not incorrectly detect two similar (trained using the same learning algorithm) but independent (randomly initialized) models, as non-independent. 

To do this, we randomly initialize a model with the OLMo (7B) architecture \citep{groeneveld2024olmoacceleratingsciencelanguage} and train it on the \texttt{Dolma v1\_7} dataset \citep{soldaini2024dolmaopencorpustrillion}. We train a second model with independently chosen initialization and data ordering, but on the same dataset.
By only changing initialization and data ordering, we have two very similar models (trained with essentially the same learning algorithm $A$), yet are independent due to their random initializations. 

We keep checkpoints for both seeds after 100M, 1B, 10B, and 18B train tokens and evaluate the statistics $\csw, \csh$, and $\rob$ on the two models at each training checkpoint, reported in Table \ref{tab:olmo_indp}. We highlight that the p-values are broadly distributed, indicating that $\rob$ is valid even on two similarly-trained but independent models (the other tests are valid by construction but we report their results anyways just for reference).
We find that all test statistics work well, and there is also little difference in the results at different training checkpoints.

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c|c|c}
    \hline 
        \# train tokens & $\csw$ & $\csh$ & $\ltwo$ & $\rob$ & $\jsd$ (log) \\ \hline 
        100M & 0.641 & 0.119 & 0.07 & 0.809 & -11.81 \\ 
        1B & 0.789 & 0.483 & 0.06 & 0.443 & -11.05 \\ 
        10B & 0.707 & 0.277 & 0.93 & 0.343 & -11.28 \\ 
        18B & 0.819 & 0.141 & 0.64 & 0.027 & -11.03 \\ \hline 
    \end{tabular}
    \caption{We evaluate the statistics $\csw, \csh$, $\ltwo$, $\rob$, and $\jsd$ on four training checkpoints between two similar but independently-trained OLMo models (weights initialized independently). We find that our proposed statistics are uniformly distributed, meaning no false positives even for two similarly-trained models.}
    \label{tab:olmo_indp}
\end{table}

\subsubsection{Simulating strong-ish adversaries}
\label{sec:mlp-retrain}

A significant difficulty in evaluating the robustness of our test $\rob$ to adversarial transformations---in particular, transformations of weights that preserve model output---is that we cannot exhaustively enumerate all such transformations.
Recalling that $\rob$ specifically considers the MLP layers contained within two models, we attempt to fool it by randomly reinitializing and retraining these MLP layers individually from scratch.
Our motivation is to simulate a somewhat strong adversary. We also conjecture that if there exist transformations of the weights of either MLP that preserve model output but fool our test, then retraining from scratch will be as likely to find these transformed weights versus something close to the original; thus, if our test is robust to retraining MLP layers from scratch then this suggests it may be robust to a broad variety of such transformations.

We reinitialize the first GLU MLP submodel of a model $\theta_1$ with an MLP with double the width and train it to minimize mean squared error with respect to the outputs of the original MLP, over an isotropic Gaussian input distribution.
We retrain each of the 32 MLP layers (keeping other layers fixed) of \texttt{vicuna-7b-v1.5} (a finetune of \texttt{Llama-2-7b-hf}) for 10k gradient steps (until the loss curve plateus). (Additional hyperparameters and a learning curve are in Appendix \ref{app:retrain}.) For all 32 runs, we compute $\rob$ for the retrained model with the original \texttt{Llama-2-7b-hf} and find $\rob$ remains very small between two non-independent models even if one model's MLP blocks have been retrained from scratch.
For example, retraining the first MLP layer (with a final train loss of 0.0048), the value of the statistic $\rob^{(1)}$ on the first MLP was less than $\varepsilon = $ 2.2e-308, indicating that the two models are not independent. We find the same is true for the other MLP layers as well (i.e. $\rob^{(\ell)}$ when evaluated on retrained layer $\ell$), with full results in Table \ref{tab:mlp_retrain} of Appendix \ref{app:retrain}.

\subsubsection{Generalizing to different architectures}
\label{app:distill-glu}
As we describe in Section~\ref{sec:generalized}, we can also apply our test to model architectures which do not contain GLU MLP submodels.
For example, the GPT-2 architecture uses a standard 2-layer MLP rather than a GLU MLP.
We apply our test (Algorithm~\ref{algorithm:general-test}) to \texttt{GPT2\_PMC} and \texttt{gpt2},
where the former is a finetune of the latter \citep{radford2019language}.
We use 30k training steps with an isotropic Gaussian input distribution to learn the GLU MLP parameters with which we replace the original MLP submodels in each model.
The test yields a value of 3.034e-61, thus distinguishing the two models as dependent. We show additional results on independent and non-independent models in Table \ref{tab:distill_mlp}.

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c}
    \hline 
        $\theta_1$ & $\theta_2$ & Indep.? & $\rob^{(1)}$ \\ \hline
        \texttt{gpt2} & \texttt{GPT2\_PMC} & \ding{55} & 3.034e-61 \\ 
        \texttt{gpt2} & \texttt{artgpt2tox} & \ding{55} & 1.049e-75 \\
        \texttt{gpt2} & \texttt{distilgpt2} & \ding{55} & 1.079e-63 \\ 
        \texttt{Llama-3.2-1B} & \texttt{Llama-3.2-3B} & \ding{55} & 2.011e-70 \\ \hline 
        \texttt{openai-gpt} & \texttt{gpt2} & \ding{51} & 0.359 \\
        \texttt{openai-gpt} & \texttt{distilgpt2} & \ding{51} & 0.770 \\
        \texttt{gpt} & \texttt{Llama-3.2-1B} & \ding{51} & 0.481 \\ \hline 
    \end{tabular}
    \caption{We evaluate $\rob^{(1)}$ on models distilled with a GLU MLP. For GPT models, we reinitialize the feedforward-network with a GLU MLP and use Algorithm \ref{algorithm:general-test}. We find that even after training a GLU MLP, our unconstrained statistic $\rob$ detects non-independent models with low values, but not for independent models (indicated by the ground truth ``Indep.?'' column).}
    \label{tab:distill_mlp}
\end{table}

\subsection{Localized testing}
\label{sec:localized-testing}

Finally, we use $\rob$ on models pairs with different hidden dimensions, specifically on 
pruned model pairs, when model dimensions are reduced by preserving only select weights. 

In particular, we identify the specific Transformer blocks of $\texttt{Llama-3.1-8B}$ whose weights were likely used in initializing $\texttt{Llama-3.2-3B}$ and $\texttt{Llama-3.2-1B}$, as Meta reported that the first two models were pruned from the third \citep{llamablog}. We use $\rob$ on all pairs of MLP blocks (i.e. $\rob^{(i,j)}$ in equation (\ref{eqn:matching-blocks})), 
and match blocks by identifying pairs, $i$ from $\firstparam$ and $j$ from $\secondparam$, such that $\rob^{(i,j)}$ is less than 1e-4. We report the matched layers between the Llama 3.1 and Llama 3.2 models in Figure \ref{fig:llama-match} and in Appendix \ref{app:modelblockmatching}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/llama3.png}
    \caption{We evaluate $\rob^{(i,j)}$, the unconstrained setting statistic, between all pairs of GLU MLPs in Transformer block $i \in \{1, 2, \dots, 32 \}$ of Llama 3.1-8B and Transformer block $j \in \{1, 2, \dots, 28 \}$ of Llama 3.2-3B. Arrows indicate if $\rob^{(i,j)} < $ 1e-4 and suggest which Transformer blocks of Llama 3.1-8B were kept in the pruning process to initialize Llama 3.2-3B.}
    \label{fig:llama-match}
\end{figure}

We also identify which hidden units were most likely shared between the blocks when MLP dimension is reduced (from 14336 to 8192) during pruning, from the permutation $\pi$ returned from $\speartest(H_\text{up}^{(\ell)}(\theta_1),H_\text{up}^{(\ell)}(\theta_2))$---as $\pi$ describes the $h_2$ rows of $H_\text{up}^{(\ell)}(\theta_1)$ that are most similar with the rows of $H_\text{up}^{(\ell)}(\theta_2)$, which potentially describe the hidden units preserved in $\firstparam$ as it is pruned to $\secondparam$. The plot in Figure \ref{fig:matched-activations-llama} shows the activation rows from the up projection matrix $U^{(1)}$ of the first MLP of Llama 3.2-3B (8192 rows) (on the x-axis) matched with the rows from the up projection matrix of the first MLP of Llama 3.1-8B (out of 14336 rows) (on the y-axis). In particular, we can see that the activations are not simply the first the first 8192 rows pruned from the 14336-dimensional MLP, rather they appear to be distributed across all 14336 rows. 


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/matched_activations_llama.png}
    \caption{We align up-projection hidden activations from the first MLPs of Llama 3.1-8B and Llama 3.2-3B using $\speartest(H_\text{up}^{(\ell)}(\theta_1),H_\text{up}^{(\ell)}(\theta_2))$ and plot the activation row from Llama 3.2-3B on the x-axis and the matched activation row from Llama 3.1-8B on the y-axis. We see that the weights and activations of Llama 3.2-3B pruned from Llama 3.1-8B were likely uniformly selected.}
    \label{fig:matched-activations-llama}
\end{figure}

We also run this pairwise layer matching on the ShearedLlama \citep{xia2024shearedllamaacceleratinglanguage} models, which were the Llama 2-7B models pruned down to 1.3B and 2.7B parameters and find matching blocks, as well as on the pruned \texttt{Llama-3.1-Minitron-4B-Depth-Base} model \citep{muralidharan2024compactlanguagemodelspruning} from Llama 3.1, which we report in Appendix \ref{app:modelblockmatching}. 