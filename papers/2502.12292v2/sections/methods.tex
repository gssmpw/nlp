\section{Methods}

\subsection{Problem formulation}\label{sec:basics}

Let $f : \Theta \times \mc{X} \to \mc{Y}$ denote a \textit{model} mapping parameters $\theta \in \Theta$ and an input $X \in \mc{X}$ to an output $f(X ; \theta) \in \mc{Y}$. 
We represent a model training or fine-tuning process as a \textit{learning algorithm} $A: \Theta \to \Theta$ that takes as input a set of initial parameters corresponding to either a random initialization or, in the case of fine-tuning, base model parameters. 
Specifically, $A$ includes the choice of training data, ordering of minibatches, and all other design decisions and even the randomness used during training---everything other than the initial model weights. 

Given two models $\firstparam,\secondparam \sim P$ for some joint distribution $P \in \mc{P}(\Theta_1 \times \Theta_2)$,
our goal is to test the null hypothesis
\begin{equation}
    H_0: \firstparam \perp \secondparam,
\end{equation}
where $\perp$ denotes independence of two random variables.
One example of a case where $\firstparam$ and $\secondparam$ might not be independent is if $\secondparam$ is fine-tuned from $\firstparam$, i.e. $\Theta_1 = \Theta_2$ (meaning the two models share the same architecture) and
$\secondparam = A(\firstparam)$ for some learning algorithm $A$.
We treat learning algorithms as deterministic functions.
Thus for $\firstparam = A_1(\theta_1^0)$ and $\secondparam = A_2(\theta_2^0)$, then $\theta_1^0 \perp \theta_2^0$, i.e., the two models having independent random initializations, implies our null hypothesis.

Deep learning models are often nested in nature. 
For example, Transformer models contain self-attention layers and multilayer perceptron (MLP) layers as submodels.
We formalize the notion of a model containing another via the following definition of a submodel. We consider a projection operator that capture the subset of the full model's parameters that are relevant to the submodel.\footnote{For example, in the case of a full Transformer model $\theta$ containing an MLP in a Transformer block, 
$\text{proj}(\theta)$ would return only the weights of that MLP $(G,U,D)$ to pass to $g = f_\text{mlp}$ (see Example \ref{example:glu-mlp}).}

\begin{definition}\label{defn:submodel}
    A model $f : \mc{X} \times \Theta \to \mc{Y}$ \textit{contains} a submodel $g : \mc{X}' \times \Theta' \to \mc{Y}'$ if there exists a projection operator $\txt{proj} : \Theta \to \Theta'$ such that for all $\theta \in \Theta$ we have
\begin{align*}
    f(x;\theta) = f_\txt{out}(g(f_\txt{in}(x);\txt{proj}(\theta)))
\end{align*}
for some functions $f_\txt{in} : \mc{X} \to \mc{X}'$
and $f_\txt{out} : \mc{Y}' \to \mc{Y}$ (which may depend on $\theta$).
\end{definition}

Many of our experiments will specifically involve Transformer models containing MLP layers with Gated Linear Unit (GLU) activations, which are widely used among language models.
We will thus specifically define this type of MLP using the following example.

\begin{example}\label{example:glu-mlp}
    (GLU MLP)
    Let $\gateproj,\upproj \in \R^{h \times d}$ and $\downproj \in \R^{d \times h}$. Let $\act : \R \to \R$ be an element-wise activation function. For $x \in \R^d$ and $\theta = (\gateproj,\upproj,\downproj) \in \Thetamlp^h$, let $\fmlp(x; \theta) \defeq D ( \sigma(Gx) \odot (Ux) )$. Also, for $X \in \R^{s \times d}$ let $\fmlp(X ; \theta) \in \R^{s \times d}$ denote the result of broadcasting $\fmlp$ over the rows of $X$.
\end{example}

In addition to the basic independence testing problem above, we also consider the problem of \textit{localized testing}: testing whether various pairs of submodels among two overall models are independent or not. A prototypical example of a localized testing problem is identifying which layers of a larger model (e.g., Llama 3.1-8B) were used to initialize a smaller model (e.g., Llama 3.2-3B) (in this case, we treat the layers as different submodels).

\subsection{Constrained Setting}

\subsubsection{Testing Framework}

Algorithm~\ref{alg:permtest} ($\permtest$) encapsulates our framework for computing p-values against the null hypothesis in the constrained setting, wherein we simulate $T$ exchangeable copies of the first model $\theta_1$ by applying transformations to its weights. 
The exchangeability of these copies holds under some assumptions on the learning algorithm and random initialization that produced the original model.
We capture these assumptions in the following definitions; together, they define the constrained setting.

\begin{algorithm}[h]\label{algorithm:permtest}
    \DontPrintSemicolon
    \caption{Test for computing p-values ($\permtest$)} \label{alg:permtest}
    \KwIn{Model weights $\theta_1, \theta_2$}
    \SetKwInOut{Parameter}{Parameters}
    \Parameter{test statistic $\phi$; discrete transformation class $\Pi$; sample size $T$}
    \KwOut{p-value $\hat{p} \in (0,1]$}
    $\mathtt{n\_ties} \gets 0$\;
    \For{$t \in 1, \dots, T$}{
        $\pi_t \sim \txt{Unif}(\Pi)$\;
        $\phi_t \gets \phi(\pi_t(\theta_1), \theta_2)$\;
        $\mathtt{n\_ties} \gets \mathtt{n\_ties} + \textbf{1} \{ \phi_t = \phi(\theta_1, \theta_2) \}$\footnotemark\;
    }
    $\xi \sim \txt{Unif}\left(\{0,...,\mathtt{n\_ties}\}\right)$ \tcp{break ties randomly}
    $\hat{p} \gets \frac{1}{T+1}(1 + \xi + \sum_{t=1}^T \textbf{1} \{ \phi_t < \phi(\theta_1, \theta_2) \})$\;
    \Return{$1-\hat{p}$}
\end{algorithm}
\footnotetext{We keep track of the number of ties (when the permuted and original statistic have the same value). To ensure uniform distribution of the p-value, we add a uniformly distributed number in $\{0, \dots, s \}$ to the count before yielding the p-value (line 6), essentially breaking times randomly.}

\begin{definition}[$\Pi$-invariance]\label{defn:perm-invar} 
    Let $\Pi \subset \Theta \to \Theta$. A distribution $P \in \mc{P}(\Theta)$
    is $\Pi$-\textit{invariant} if for $\theta \sim P$ and any $\pi \in \Pi$, the parameters $\theta$ and $\pi(\theta)$ are identically distributed.
\end{definition}

\begin{definition}[$\Pi$-equivariance]\label{defn:perm-equiv-det}
    Let $\Pi \subset \Theta \to \Theta$, $\pi \in \Pi$, and $\init \in \Theta$.
    A learning algorithm $A$ is $\Pi$-\textit{equivariant} if and only if $\pi(A(\theta^0)) = A(\pi(\theta^0))$.
\end{definition}

The main idea underlying $\permtest$ is that so long as $\theta_1 = A(\theta_1^0)$ and $\theta_1^0 \sim P$
for some $\Pi$-equivariant learning algorithm $A$ and $\Pi$-invariant distribution $P$, we can simulate $T$
exchangeable (but not independent) copies $\{\pi_t(\firstparam)\}_{t=1}^T$ of $\firstparam$ by sampling $\pi_t \iid \txt{Unif}(\Pi)$. 
This allows us to efficiently compute an exact p-value without actually repeating the training process of $\firstparam$.
In effect, Definitions~\ref{defn:perm-invar} and \ref{defn:perm-equiv-det} imply that $\pi$ commutes with $A$---i.e., $\pi(A(\firstinit)) = A(\pi(\firstinit))$.
Under exchangeability, the p-value output by $\permtest$ will be uniformly distributed over $\{(i+1)/(T+1)\}_{i=0}^T$.

Standard initialization schemes for feedforward networks exhibit symmetry over their hidden units. This symmetry means that permuting hidden units represents one class of transformations under which any such initialization remains invariant.
Moreover, the gradient of the model's output with respect to the hidden units is permutation equivariant; thus, any learning algorithm whose update rule is itself a permutation equivariant function of gradients (e.g., SGD, Adam, etc.) satisfies Definition~\ref{defn:perm-equiv-det} with respect to these transformations.
An
example of a learning algorithm that is not permutation equivariant is one that uses different learning rates for each hidden unit depending on the index of the hidden unit.

\begin{example}[Permuting hidden units]\label{example:permuting-hidden-units} 
    Let $\theta = (\gateproj,\upproj,\downproj) \in \Thetamlp^h$ parameterize a GLU MLP. Recall $\fmlp(x ; \theta) \defeq D ( \sigma(Gx) \odot (Ux) )$ for some element-wise activation function $\sigma : \R \to \R$. 
    Abusing notation, let $\Pi$ be the set of $h \times h$ permutation matrices such that for $\pi \in \Pi$ we define $\pi(\theta) = (\pi \gateproj, \pi \upproj, \downproj \pi^T )$ (permuting the rows of $G, U$ and the columns of $D$). Observe $\fmlp(x ; \theta) = \fmlp(x ; \pi(\theta))$ and $\pi(\nabla_\theta \fmlp(x;\theta)) = \nabla_{\pi(\theta)} f(x;\pi(\theta))$ for all inputs $x$.
\end{example}

The assumptions we make in the constrained setting suffice for $\permtest$ to produce a valid $p$-value, as we show in the following theorem.
Importantly, the result of the theorem holds (under the null hypothesis) without any assumptions on $\secondparam$. Therefore, a model developer of $\theta_1$ testing other models with our methods can have confidence in the validity of our test without trusting the provider of $\secondparam$.
Of course, if $\secondparam$ does not satisfy the equivariance assumption on training (as in the unconstrained setting), then $\permtest$ is unlikely to produce a low p-value even in cases where $\firstparam$ and $\secondparam$ are not independent (e.g. if an adversary finetunes $\secondparam$ from $\firstparam$ but then afterwards randomly permutes its hidden units).

\begin{theorem}\label{thm:main}
    Let $\phi: \Theta \times \Theta \to \R$ be a test statistic and $\Pi \subset \Theta \to \Theta$ be finite. 
    Let $A : \Theta \to \Theta$ be $\Pi$-equivariant and let $P \in \mc{P}(\Theta)$ be $\Pi$-invariant. For $\theta_1^0 \sim P$, let $\theta_1 = A(\theta_1^0)$. Let $\theta_2 \in \Theta$ be independent of $\theta_1$. 
    Then $\est{p} = \permtest(\firstparam,\secondparam)$ is uniformly distributed on $\{\frac{i+1}{T+1}\}_{i=0}^T$.
\end{theorem}
\begin{proof}
    We assume $\Pi$ is finite so that $\txt{Unif}(\Pi)$ is well-defined.
    From our assumptions on $A$ and $P$ and the fact that $\{\pi_t\}_{t=1}^T$ are independently drawn, it follows that the collection $\{\pi_t(\firstparam)\}_{t=1}^T$ comprises $T$ exchangeable copies of $\firstparam$.
    The independence of $\firstparam$ and $\secondparam$ thus implies $\{(\pi_t(\firstparam),\secondparam)\}_{t=1}^T$ comprises $T$ exchangeable copies of $(\firstparam,\secondparam)$. Because we break ties randomly, by symmetry it follows that $\phi(\theta_1, \theta_2)$ will have uniform rank among $\{\phi_t\}_{t=1}^T$.
\end{proof}

One notable (non-contrived) category of deep learning algorithms that are \textit{not} permutation equivariant are those that employ 
dropout masks to hidden units during training. In our framework, the dropout masks are specified in the deterministic learning algorithm $A$. Once we fix a specific setting of mask values in $A$, this algorithm will not be permutation equivariant unless the individual dropout masks are all permutation invariant (which is highly unlikely).
For completeness, we generalize the result of Theorem~\ref{thm:main} to apply to randomized learning algorithms that satisfy a notion of equivariance in distribution (which includes algorithms that use dropout) in Appendix~\ref{sec:randomized-alg}.
However, throughout the main text we will continue to treat learning algorithms as deterministic for the sake of simplicity, and also since dropout typically is no longer used in training language models \citep{chowdhery2022palmscalinglanguagemodeling}.

\subsubsection{Test Statistics}
\label{sec:test-stats}

We have shown $\permtest$ produces a valid p-value regardless of the test statistic $\phi$ we use.
The sole objective then in designing a test statistic is to achieve high statistical power: we would like $\est{p} = \permtest(\firstparam,\secondparam)$ to be small when $\firstparam$ and $\secondparam$ are not independent.
The test statistics we introduce in this section apply to any model pair sharing the same architecture. 

Prior work \citep{xu2024instructionalfingerprintinglargelanguage} proposed testing whether two models are independent or not based on the $\ell_2$ distance between their weights, summed over layers. Specifically
for a model with $L$ layers parameterized by $\Theta = \Theta_1 \times ... \times \Theta_L$, with $\firstparam = (\firstparam^{(\ell)})_{\ell=1}^L$ and $\secondparam = (\secondparam^{(\ell)})_{\ell=1}^L$,
let $\ltwo(\firstparam,\secondparam) \defeq - \sum_{i=1}^L \ell_2(\theta_1^{(i)},\theta_2^{(i)})$.
We can obtain p-values from $\ltwo$ by using it within $\permtest$.
However, a major limitation is that in order to obtain a p-value less than $1/(T+1)$ we must recompute $\ltwo$ at least $T$ times; the effective statistical power of our test using $\ltwo$ is therefore bottlenecked by computation.

To address this limitation, we propose a family of test statistics whose distribution under the null is identical for \textit{any} model pair. Consider 
$m, n \in \N$ and some function $M : \Theta \to \R^{n \times m}$ that maps model weights to a matrix, such as returning a specific layer's weight matrix.
The proposed test statistics all share the following general form based on Algorithm~\ref{alg:speartest} ($\speartest$) for varying $M$:
\begin{align}\label{eqn:phi-m}
    \phi_M(\firstparam,\secondparam) \defeq \spearman(\speartest(M(\firstparam),M(\secondparam)),[1,...,n]),
\end{align}
where $\spearman$ is the Spearman rank correlation (Algorithm~\ref{algorithm:spearman-pvalue}).

\begin{algorithm}[h]\label{algorithm:speartest}
    \DontPrintSemicolon
    \caption{Cosine similarity matching ($\speartest$)}
    \label{alg:speartest}
    \KwIn{Matrices $\firstM, \secondM$ with $h$ rows}
    \SetKwInOut{Parameter}{Parameters}
    \KwOut{Permutation $\pi : [h] \to [h]$}
    \For{$i \in 1, \dots, h$}{
        \For{$j \in 1, \dots, h$}{
            $C_{i,j} \gets \dcos((\firstM)_i,(\secondM)_j)$\;
        }
    }
    $\pi \gets \match(C)$\;
    \Return{$\pi$}
\end{algorithm}

Equation~\eqref{eqn:phi-m} is applicable to any model architecture $\Theta$ for which we can define a suitable matrix valued function $M$ of model parameters. For example, $M$ could directly extract a weight matrix or activation matrix from a model layer (based on some set of inputs), with each row corresponding to a hidden unit. We use $\speartest$ to align the rows of the two extracted matrices and then pass this alignment to $\spearman$ to compute the Spearman rank correlation \citep{spearmanrank} of this alignment with the identity map between rows.
We describe matching in Algorithm~\ref{alg:speartest}, wherein $\dcos$ denotes cosine similarity function and $\match$ denotes the algorithm of \citet{Ramshaw2012OnMA} we use to solve the matching problem. 

\begin{algorithm}[h]\label{alg:spearman-pvalue}
    \DontPrintSemicolon
    \caption{Deriving p-values from Spearman correlation ($\spearman$) from \cite{spearmanrank}}
    \label{algorithm:spearman-pvalue}
    \KwIn{Permutations $\pi_1, \pi_2: [h] \to [h]$}
    \SetKwInOut{Parameter}{Parameters}
    \KwOut{p-value $\hat p \in (0,1]$}
    $r \gets 1 - \frac{6 \sum (\pi_1[i] - \pi_2[i])^2}{h(h^2-1)}$\; 
    $t \gets r \sqrt{\frac{h-2}{1-r^2}}$\;
    $\hat p \gets \mathbb{P}(T_{n-2} > t)$ \; 
    \Return{$\hat p$}
\end{algorithm}

The idea of the test is that for two dependent models, each row of $M(\firstparam)$ should be similar to its counterpart in $M(\secondparam)$; thus, the alignment found by $\spearman$ will be close to the identity map.
Meanwhile, so long as $M$ is a $\Pi$-equivariant map (Definition~\ref{defn:equivariant-map}), then $\phi_M(\theta_1,\theta_2)$ under the null yields valid p-values (see Theorem 2), 
so we can use the more computationally-efficient Algorithm~\ref{algorithm:spearman-pvalue} to convert statistics to p-values instead of running $\permtest$.

\begin{definition}\label{defn:equivariant-map}
    (equivariant map) A matrix-valued function $M : \Theta \to \R^{n \times m}$ is $\Pi$-equivariant with respect to a class of transformations $\Pi : \Theta \to \Theta$ if there exists a bijection between $\Pi$ and the set of $n \times n$ permutation matrices such that $M(\pi(\theta)) = \pi M(\theta)$ for all $\theta \in \Theta$ and $\pi \in \Pi$.
\end{definition}

\begin{theorem}
Let $M: \Theta \to \R^{n \times m}$ be a $\Pi$ equivariant map and let $P \in \mc{P}(\Theta)$ be $\Pi$-invariant. Let $\firstparam,\secondparam \in \Theta$ be independent random variables, with $\firstparam = A(\firstinit)$ for $\firstinit \sim \firstP$. Then $\phi_M(\theta_1,\theta_2)$ is uniformly distributed on $(0,1]$.
\end{theorem}
\begin{proof}
    As $M$ is a $\Pi$-equivariant map, if $\firstparam \perp \secondparam$ then letting $\pi = \match(C)$ in $\speartest$ is equivalent in distribution to sampling $\pi \sim \txt{Unif}(\Pi)$. Then the output of $\speartest$ is identical in distribution for \textit{any} pair of independent models, and can be converted to a p-value using $\spearman$ and the distribution for the Spearman correlation coefficient (t-distribution with $h-2$ degrees of freedom).
\end{proof}

We can choose various different functions for $M$, with each yielding a valid test statistic.
We focus our experiments on Transformer models consisting of a series of $L$ Transformer blocks. Each block contains a GLU MLP submodel, where $M(\theta)$ represents either the  up projection weights or the hidden-layer activations of these submodels.
In particular, let $U^{(\ell)}(\theta) \in \R^{h \times d}$ denote the first layer up projection weights of the MLP in the $\ell$-th block, where $h$ is the hidden dimension and $d$ is the input dimension, and let $H^{(\ell)}(\theta) \in \R^{h \times (N \cdot s)}$ denote the (flattened) hidden activations obtained from passing $N$ length $s$ input sequences $X \in \R^{N \times s \times d}$ to the same MLP module (the test is valid for any $X$; we will specify later how we choose $X$ in our experiments). 
The two main test statistics we will employ in our experiments are $\csw$ and $\csh$.

Both $U^{(\ell)}$ and $H^{(\ell)}$ are equivariant with respect to permuting the hidden units of the corresponding MLP, so we can directly interpret the outputs of $\csw$ and $\csh$ as p-values.
{Moreover, we can separately permute the hidden units of the MLP in the $\ell$-th block without changing the inputs or outputs of the other blocks.
Thus, as we show in Theorem~\ref{thm:fisher},
we can aggregate the p-values from $\csw$ and $\csh$ across blocks using Fisher's method (\cite{fisher_qa}) to obtain a more powerful test in Algorithm~\ref{alg:fisher} ($\fisher$). Specifically, Fisher's method of aggregating p-values requires independent tests from the same null hypothesis.

\begin{algorithm}[h]\label{algorithm:fisher}
    \DontPrintSemicolon
    \caption{Aggregating p-values ($\fisher$)}
    \label{alg:fisher}
    \KwIn{p-values $\{\est{p}^{(i)}\}_{i=1}^L$}
    \KwOut{p-value $\hat{p} \in (0,1]$}
    $\xi \gets \sum_{i=1}^L \log \est{p}^{(i)}$\;
    $\hat{p} \gets 1-\mathbb{P}(\chi^2_{2L} < -2\xi)$\;
    \Return{$\hat p$}
\end{algorithm}

\begin{theorem}
    \label{thm:fisher}
    Consider block indices $i,j \in [L]$ with $i \neq j$ for models with $L$ blocks. Suppose for $\ell \in \{i,j\}$ that
    \begin{itemize}
        \item[1.] $M^{(\ell)} : \Theta \to \R^{h \times N}$ is equivariant with respect to $\Pi^{(\ell)}$, i.e.,
              for any $\theta \in \Theta$ and $\pi^{(\ell)} \in \Pi^{(\ell)}$ we have
              \begin{align*}
                  M(\pi^{(\ell)}(\theta)) = \pi^{(\ell)} M(\theta).
              \end{align*} 
        \item[2.] $A$ is a $\Pi^{(\ell)}$-equivariant learning algorithm and $P \in \mc{P}(\Theta)$ is a $\Pi^{(\ell)}$-invariant distribution.
    \end{itemize}
    Let $\firstparam,\secondparam \in \Theta$. If $\firstparam \perp \secondparam$ for $\firstparam = A(\firstinit)$ with $\firstinit \sim P$,
    then
    \begin{align*}
        \speartest(M^{(i)}(\firstparam),M^{(i)}(\secondparam)) \perp \speartest(M^{(j)}(\firstparam),M^{(j)}(\secondparam)).
    \end{align*}
\end{theorem}
\begin{proof}
    Let $\firstparam' \sim A(\pi_1^{(i)} \circ \pi_2^{(j)} (\firstinit))$ for $\pi_1,\pi_2 \iid \txt{Unif}(\Pi)$. Then
    $\firstparam'$ is an independent copy of $\firstparam$ since taking the composition $\pi_1^{(i)} \circ \pi_2^{(j)} (\firstparam)$
    yields an independent copy of $\firstparam$ for any $\pi_1,\pi_2 \in \Pi$.
    From $\firstparam \perp \secondparam$, it follows for $\ell \in \{i,j\}$
    that $\speartest(M^{(\ell)}(\firstparam'),M^{(\ell)}(\secondparam))$ is identically distributed to
    $\speartest(M^{(\ell)}(\firstparam),M^{(\ell)}(\secondparam))$.
    The result then follows from the fact $\speartest$ is equivariant with respect to permuting the rows of its arguments: in particular,
    for any $\pi \in \Pi$ we have $\speartest(\pi \firstM,\secondM) = \pi \speartest(\firstM,\secondM)$.
\end{proof}


Recall $\csw$ and $\csh$ are functions of $\speartest(M^{(\ell)}(\firstparam),M^{(\ell)}(\secondparam))$ respectively for
$M^{(\ell)} = U^{(\ell)}$ and $M^{(\ell)} = H^{(\ell)}$, both of which satisfy the assumptions of the theorem.
Thus, the result of the theorem applies to both these test statistics, and the independence of the p-values from these test statistics across blocks follows directly from the independence of the statistics themselves. Hence, we can use Fisher's method and Algorithm \ref{alg:fisher} to aggregate p-values from $\csw$ or $\csh$ values to obtain a more powerful test. 

\subsection{Unconstrained Setting}
\label{robustness}

For the $\unconstr$ setting, our goal is to design a test that applies to models of different architectures and is robust to output-preserving transformations of model weights.
Recall our tests for the constrained setting satisfy neither of these desiderata: these tests assume both models have the same number of hidden units, and it is easy to fool them without changing the output of a model by permuting the order of the hidden units in the model.

Our robust test reposes on the design of $\phi_M$ in equation~\eqref{eqn:phi-m}. The goal is to identify two matrix valued functions of model parameters $M,M' : \Theta \to \R^{n \times m}$ that jointly satisfy the following condition: any output-preserving transformation of model parameters must transform both $M$ and $M'$ in the same way. Then, whereas previously we would correlate $\speartest(M(\theta_1),M(\theta_2))$ with the identity permutation, we instead define
\begin{align}\label{eqn:robust-test}
    \phi_{M,M'} \defeq \spearman(\speartest(M(\theta_1),M(\theta_2)),\speartest(M'(\theta_1),M'(\theta_2)).
\end{align}

The above goal is aspirational in the sense that for any nontrivial deep learning model we are not able to fully enumerate the set of transformations of model parameters to which model output is invariant; 
nonetheless, it will serve as a useful guiding principle for designing our robust test under the framework of equation~\eqref{eqn:robust-test}.
We organize the description of our full robust test---which is generally applicable to a variety of model architectures---into two parts: first, in Section \ref{sec:gluarchitecture} we instantiate equation~\eqref{eqn:robust-test} to obtain a test for GLU MLP models. Then, in Section \ref{sec:generalized} we use our GLU MLP test as a primitive for designing a test that applies to general deep learning models (including those which do not contain any GLU MLP submodels).


\subsubsection{Testing GLU models}
\label{sec:gluarchitecture}


Recalling our definition of a GLU MLP model in Example~\ref{example:glu-mlp}, for $k \in \{1,2\}$ let $\theta_k = (\gateproj_k,\upproj_k,\downproj_k) \in \Thetamlp^{h_k}$, and with inputs $X \in \R^{d \times N}$ let $H_\txt{up}(\theta_k) = U_k X \in \R^{\max\{h_1,h_2\} \times N}$ be the output of the up projection operation and let $H_\txt{gate}(\theta_k) = G_k X \in \R^{\max\{h_1,h_2\} \times N}$ be the output of the gate projection operation (with appropriate zero-padding when $h_1 \neq h_2$).
Due to the element-wise product operation, we conjecture that in general it is not possible to permute the rows of $\gateproj_k$ while preserving the output of $\theta_k$ without permuting the rows $\upproj_k$ in the same way, and so we use 
$\phi_{M,M'}$ with $M = H_\txt{gate}$ and $M' = H_\txt{up}$ for our GLU MLP test. Henceforth, we will shorthand this test as $\rob$.

As with the constrained setting, we focus much of our experiments on Transformer models, which recall consist of a series of $L$ Transformer blocks that each contain a GLU MLP submodel. Adopting the notational conventions of Section~\ref{sec:test-stats}, we can apply our GLU MLP test to the $\ell$-th block by taking $M = H_\txt{gate}^{(\ell)}$ and $M' = H_\txt{up}^{(\ell)}$, where like before (in the case of $\csh$) we obtain the activation inputs for each block by computing a forward pass through the full model over a set of length $s$ sequences of input tokens.

We can aggregate the results of these tests over blocks using $\fisher$, like we do for $\csw$ and $\csh$ in the constrained setting.
Alternatively, to perform localized testing we can apply the test to all possible $O(L^2)$ pairs of blocks between two Transformer models if we suspect that certain blocks from one model served as the initializations for different blocks in the other model. Specifically, we can test the $i$-th block of $\firstparam$ and the $j$-th block of $\secondparam$ using
\begin{align}
\label{eqn:matching-blocks}
    \rob^{(i,j)} \defeq \spearman(\speartest(H_\text{gate}^{(i)}(\theta_1),H_\text{gate}^{(j)}(\theta_2)),\speartest(H_\text{up}^{(i)}(\theta_1),H_\text{up}^{(j)}(\theta_2))).
\end{align}

\subsubsection{Beyond GLU Models}
\label{sec:generalized}

Thus far we have focused on models $f : \mc{X} \times \Theta \to \mc{Y}$ containing a GLU MLP submodel.
In particular, recalling Definition~\ref{defn:submodel},
we have assumed for some $\txt{proj}_\txt{mlp} : \Theta \to \Thetamlp^h$ that
\begin{align}\label{eqn:og-model}
    f(x;\theta) = f_\txt{out}(\fmlp(f_\txt{in}(x);\txt{proj}_\txt{mlp}(\theta))).
\end{align}
Now, our goal will be to test more general types of models.
In particular, we generalize to an arbitrary alternative submodel $f_\txt{alt} : \R^d \times \Theta_\txt{alt} \to \R^d$ with $\txt{proj}_\txt{alt} : \Theta \to \Theta_\txt{alt}$ such that
\begin{align}\label{eqn:alt-model}
    f(x;\theta) = f_\txt{out}(f_\txt{alt}(f_\txt{in}(x);\txt{proj}_\txt{alt}(\theta))).
\end{align}

In order to test whether two models $\firstparam,\secondparam \in \Theta$ of the more general form in equation~\eqref{eqn:alt-model} are independent, we will first construct proxy models of the form in equation~\eqref{eqn:og-model} and then apply our previous test $\rob$ to these proxy models.
We construct these proxy models by leveraging
the fact that $f_\txt{alt}$ shares the same input and output space with $\fmlp$.
Specifically, for $k \in \{1,2\}$ we first learn parameters $\est{\theta}_k \in \Thetamlp^h$ so that $\fmlp(\cdot \ ;\est{\theta}_k)$ approximates $f_\txt{alt}(\cdot \ ;\txt{proj}_\txt{alt}(\theta_k))$. We then return $\rob(\est{\theta}_1,\est{\theta}_2)$.
We capture this two-stage process in Algorithm~\ref{algorithm:general-test}.

\begin{algorithm}[H]\label{algorithm:general-test}
    \DontPrintSemicolon
    \caption{Generalized robust test}
    \KwIn{Model parameters $\firstparam, \secondparam \in \Theta$}
    \SetKwInOut{Parameter}{Parameters}
    \Parameter{distribution $P$ over $\R^d$}
    \KwOut{$\est{p} \in [0,1]$}
    \For{$k \in \{1,2\}$}{
        $\est{\theta}_k \gets \arg\min_{\est{\theta}} \Ep_{x \sim P} \left[ \norm{f_\txt{alt}(x;\txt{proj}_\txt{alt}(\theta_k)) - f_\txt{mlp}(x;\est{\theta}_k)}^2\right]$
    }
    \Return{$\est{p} \gets \rob(\est{\theta}_1,\est{\theta}_2)$}
\end{algorithm}

Perhaps surprisingly, we show that Algorithm~\ref{algorithm:general-test} is effective in practice at distinguishing independent versus non-independent models. The hidden dimension $h$ and input distribution $P$ with which we learn the GLU MLP are hyperparameters of the test. See Section \ref{sec:adversarial-experiments} for details. 