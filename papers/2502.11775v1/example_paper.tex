%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{bbm}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{tikz}
% \usepackage[table, dvipsnames]{xcolor}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}
\newcommand{\tikzxmark}{%
\tikz[scale=0.23] {
    \draw[line width=0.7,line cap=round] (0,0) to [bend left=6] (1,1);
    \draw[line width=0.7,line cap=round] (0.2,0.95) to [bend right=3] (0.8,0.05);
}}
\newcommand{\tikzcmark}{%
\tikz[scale=0.23] {
    \draw[line width=0.7,line cap=round] (0.25,0) to [bend left=10] (1,1);
    \draw[line width=0.8,line cap=round] (0,0.35) to [bend right=1] (0.23,0);
}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}


% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model}

\begin{document}

\twocolumn[
\icmltitle{video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Guangzhi Sun}{cam,bytedance}
\icmlauthor{Yudong Yang}{bytedance,tsinghua}
\icmlauthor{Jimin Zhuang}{bytedance,tsinghua}
\icmlauthor{Changli Tang}{bytedance,tsinghua}
\icmlauthor{Yixuan Li}{bytedance,tsinghua}
\icmlauthor{Wei Li}{bytedance}
\icmlauthor{Zejun MA}{bytedance}
\icmlauthor{Chao Zhang}{tsinghua}
\end{icmlauthorlist}

\icmlaffiliation{cam}{Univeristy of Cambridge}
\icmlaffiliation{bytedance}{ByteDance}
\icmlaffiliation{tsinghua}{Tsinghua university}

\icmlcorrespondingauthor{Chao Zhang}{cz277@tsinghua.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.
This paper proposes \emph{video-SALMONN-o1}, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. 
Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over \textbf{4,000} high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves \textbf{3-8}\% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves \textbf{6-8}\% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities. Demo Page: \url{https://github.com/BriansIDP/video-SALMONN-o1}
% \footnote{Demo Page: \url{https://github.com/BriansIDP/video-SALMONN-o1}.}
\end{abstract}

\section{Introduction}
\label{submission}
%The recent advancements in optimizing the reasoning process have further boosted text-based large language models (LLMs) \cite{o1,deepseek,qwq,macroo1,eurus} performance in answering complex logical questions, such as math problems \cite{qwen25math,mathshepherd,easytohard,internmath} and coding tasks \cite{o1coder}. These methods usually first split the solution into multiple simpler \textit{steps} to form a reasoning path ending with the final solution, as demonstrated in chain-of-thought (CoT) \cite{cot}. Advanced optimization algorithms have been developed such as the outcome reward model (ORM) \cite{orm0,orm1,selfverify} that optimizes the entire reasoning path based on the final solution, and the process reward model (PRM) \cite{ormprm,prm0,prm1,llamaberry} that optimizes each reasoning step based on how likely each step would lead to a correct answer.

The recent advancements in optimizing the reasoning process have further boosted text-based large language models (LLMs) \cite{o1,deepseek,qwq,macroo1,eurus} performance in answering complex logical questions, such as math problems \cite{qwen25math,mathshepherd,easytohard,internmath} and coding tasks \cite{o1coder}. These methods usually first split the solution into multiple simpler \textit{steps} to form a reasoning path ending with the final solution, as demonstrated in chain-of-thought (CoT) \cite{cot}. Advanced training approaches have been developed such as the outcome reward model (ORM) \cite{orm0,orm1,selfverify} that optimizes the entire reasoning path based on the final solution, and the process reward model (PRM) \cite{ormprm,prm0,prm1,llamaberry} that optimizes each reasoning step based on how likely each step would lead to a correct answer.


%In addition to text-based questions, reasoning also plays an indispensable role in understanding the multimodal world, such as comprehending concepts in an academic presentation, interpreting complex interactions among people or even detecting artificial anomalies. Thus, improving reasoning ability is also critical for multimodal LLMs \cite{tang2024extending,salmonn,videosalmonn,videollama2,llavavideo,videollava,gemini,qwen2vl,videosalmonn2} that process audio and visual inputs in addition to text, as the interaction with other modalities can largely increase the difficulty of the task. To this end, investigations have been performed on optimizing the reasoning process with multimodal inputs \cite{cotst}, and on particularly visual LLMs \cite{qvq,llavacot,virgo}. However, current research on enhancing reasoning capabilities for multimodal LLMs has predominantly focused on solving mathematical problems and image inputs. This overlooks the importance of reasoning in general video understanding and the interaction of audio, visual and text modalities, largely limiting their scopes of applications.

In addition to text-based questions, reasoning also plays an indispensable role in understanding the physical world, such as comprehending concepts in an academic presentation, interpreting complex interactions among people or even detecting artificial anomalies. Thus, improving reasoning ability is also critical for multimodal LLMs \cite{tang2024extending,salmonn,videosalmonn,videollama2,llavavideo,videollava,gemini,qwen2vl,videosalmonn2} that process audio and visual inputs in addition to text, as the interactions among multiple modalities can largely increase the difficulty of the task. To this end, investigations have been performed on optimizing the reasoning process with multimodal inputs \cite{cotst}, and on particularly visual LLMs \cite{qvq,llavacot,virgo}. However, current research on enhancing reasoning capabilities for multimodal LLMs has predominantly focused on solving mathematical problems and image inputs. This overlooks the importance of reasoning in general video understanding and the interactions among audio, visual and text modalities, largely limiting their scopes of applications.


%This paper proposes \textit{video-SALMONN-o1}, the first open-source reasoning-enhanced audio-visual LLM with improved reasoning abilities in general video understanding tasks. The audio-visual reasoning capability of video-SALMONN-o1 is first enhanced by creating a new dataset with challenging questions and step-by-step solutions for supervised fine-tuning (SFT), and then further boosted by the proposed process direct preference optimization (DPO) method \cite{dpo,dpovideo}. pDPO achieves step-level pairwise reward modeling via an efficient contrastive step selection approach tailored for multimodal inputs. While being more effective than the standard PRMs in general video understanding, the pDPO and the step selection make audio-visual reasoning more efficient without the need for an external reward model or a two-pass re-ranking pipeline.

%This paper proposes \textit{video-SALMONN-o1}, the first open-source reasoning-enhanced audio-visual LLM with improved reasoning abilities in general video understanding tasks. The audio-visual reasoning capability of video-SALMONN-o1 is first enhanced by creating a new dataset with challenging questions and step-by-step solutions for supervised fine-tuning (SFT), and then further boosted by the proposed variant of direct preference optimization (DPO), pDPO (pDPO) \cite{dpo,dpovideo}. pDPO achieves step-level pairwise reward modelling via an efficient contrastive step selection approach tailored for multimodal inputs. While being more effective than the standard PRMs in general video understanding, pDPO and the step selection make audio-visual reasoning more efficient without the need for an external reward model or a two-pass re-ranking pipeline.

This paper proposes \textit{video-SALMONN-o1}, the first open-source reasoning-enhanced audio-visual LLM with improved reasoning abilities in general video understanding tasks. The audio-visual reasoning capability of video-SALMONN-o1 is first enhanced by creating a new dataset with challenging questions and step-by-step solutions for supervised fine-tuning (SFT), and then further boosted by the proposed variant of direct preference optimization (DPO), process DPO (pDPO) \cite{dpo,dpovideo}. pDPO achieves step-level pairwise reward modelling via an efficient contrastive step selection approach tailored for multimodal inputs. While being more effective than the standard PRMs in general video understanding, pDPO and the step selection make audio-visual reasoning more efficient without the need for an external reward model or a two-pass re-ranking pipeline.


%Moreover, to better reflect the importance of reasoning in general video understanding, we propose the first \textbf{R}easoning-\textbf{i}ntensive \textbf{V}ideo understanding benchmark (RivaBench). RiVBench primarily focuses on three representative scenarios, including standup comedy, academic presentation and synthetic video detection. In particular, RiVBench contains over 4k high-quality question-answer pairs that are carefully crafted by human experts (e.g. medical doctors). Key contributions are summarized as follows:

To evaluate the performance on multimodal reasoning for general video understanding, we propose the first \textbf{r}easoning-\textbf{i}ntensive \textbf{v}ideo with \textbf{a}udio understanding benchmark (RivaBench). RivaBench primarily focuses on three representative scenarios, including standup comedy, academic presentation and synthetic video detection. In particular, RivaBench contains over 4k high-quality question-answer pairs that are carefully crafted by human experts (\textit{e.g.} medical doctors). Our key contributions are summarized as follows:


\begin{itemize}[itemsep=0pt, leftmargin=*]
    \item We propose video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM for general video understanding tasks. % Plus performance gains
%    \item video-SALMONN-o1 is the first to explore RL-based reasoning optimization for general video understanding. The proposed process DPO method with efficient contrastive step selection further enhances reasoning abilities. 
    \item video-SALMONN-o1 is the first to explore RL-based reasoning optimization for general video understanding. The proposed pDPO method with efficient contrastive step selection further enhances reasoning abilities. 
%    \item We propose RiVBench, the first general video understanding benchmark focusing on challenging audio-visual reasoning scenarios with human expert annotations.
    \item We propose RivaBench, the first general video understanding benchmark focusing on challenging audio-visual reasoning scenarios with human expert annotations.
%    \item video-SALMONN-o1 consistently outperforms the strong LLaVA-OneVision visual baseline on VideoMME, NExT-QA and RiVBench, with \textbf{3-8}\%  absolute accuracy improvements. The process DPO training achieved \textbf{6-8}\% improvements on RiVBench over the SFT model. Moreover, video-SALMONN-o1 is the first open-source model that showed zero-shot synthetic video detection ability.
    \item video-SALMONN-o1 consistently outperforms the strong LLaVA-OneVision visual baseline on VideoMME, NExT-QA and RivaBench, with \textbf{3-8}\%  absolute accuracy improvements. The pDPO training achieved \textbf{6-8}\% improvements on RivaBench over the SFT model. Moreover, video-SALMONN-o1 is also the first open-source model that showed zero-shot synthetic video detection ability.
\end{itemize}

\section{Related Work}
\subsection{CoT Reasoning}
CoT reasoning is one of the remarkable abilities of LLMs when solving difficult and complex problems. Earlier investigations employed prompt tuning and various search algorithms such as the Monte-Carlo tree search during inference time \cite{treesearch1,treesearch2,treesearch3,treesearch4,treesearch5}. Later on, training stage approaches using reinforcement learning (RL) were developed to further and more radically boost the reasoning capabilities of LLMs. PRMs which estimate the value function of each reasoning step have emerged as one of the most prevalent approaches in reasoning optimization tasks \cite{ormprm,prm0,prm1,llamaberry,prm2}. 

However, constructing step-level annotations for PRM training can be expensive and difficult to scale up. As mitigation, \citet{mathshepherd} and \citet{prm1} proposed automatic step annotation using \textit{rollout}, which approximated the expected correctness of each step by sampling multiple paths till the end with the same prefix solution. In particular, \citet{prm1} treats the first wrong step as the critical step to perform rollout which was found by binary search. 

\subsection{Reasoning in Multimodal LLMs}

%Researchers have been investigating optimizing CoT reasoning for multimodal LLMs to tackle increasingly challenging tasks. Most of them focus on extracting graphical or text information from an image and solving mathematical tasks based on the extracted information. Specifically, LLaVA-CoT \cite{llavacot} investigated better sampling and search algorithms to find a better reasoning path for math questions with image inputs. Virgo, on the other hand, explores the fine-tuning data organization and transferability of text-based reasoning tasks to image-based reasoning tasks \cite{virgo}. Very recently, MAmmoTH-VL \cite{mammothvl} reported improvements in general video benchmarks by SFT on a new dataset with reasoning steps. Different from these tasks, video-SALMONN-o1 particularly focuses on general video understanding scenarios, where different parts of the audio-visual information are constantly referred to during the reasoning process.

Researchers have been investigating optimizing CoT reasoning for multimodal LLMs to tackle increasingly challenging tasks. Most of them focus on extracting graphical or text information from an image and solving mathematical tasks based on the extracted information. Specifically, LLaVA-CoT \cite{llavacot} investigated better sampling and search algorithms to find a better reasoning path for math questions with image inputs. Virgo, on the other hand, explores the fine-tuning data organization and transferability of text-based reasoning tasks to image-based reasoning tasks \cite{virgo}. Recently, MAmmoTH-VL \cite{mammothvl} built a large-scale multimodal instruction-tuning dataset that can improve the question-answering performance on diverse modalities including video. Different from these works, video-SALMONN-o1 particularly focuses on general video understanding scenarios, where different parts of the audio-visual information are constantly referred to during the reasoning process.



\subsection{Benchmarks for Audio-visual LLMs}

%The fast-paced development of multimodal LLMs has boosted the creation of more and more challenging benchmarks for video understanding. Benchmarks are also evolving from video description and perception abilities \cite{2021value, AVSD2019, valor2023, musicAVQA2022, 2023vast, 2023videobench, 2023egoschema,pano-AVQA2021,avhallubench}, to video reasoning abilities such as inference about temporal and causal relation \cite{2021nextqa,2024mvbench,2023vitatecs,2024videomme,2024tempcompass,fang2024mmbench}. In particular, NExT-QA \cite{2021nextqa} focuses on causal relation reasoning such as why a certain action is performed, and Video-MME \cite{2024videomme} contains questions that require the combination of both audio and visual information to perform reasoning. RiVBench takes a step further into the reasoning by incorporating more challenging questions that require \textit{longer} thinking steps, \textit{broader} world knowledge and a \textit{tighter} combination of audio-visual information.

The fast-paced development of multimodal LLMs has boosted the creation of more challenging video understanding benchmarks. Benchmark focus evolves from video description and perception abilities \cite{2021value, AVSD2019, valor2023, musicAVQA2022, 2023vast, 2023videobench, 2023egoschema,pano-AVQA2021,avhallubench}, to video reasoning abilities such as inference about temporal and causal relations \cite{2021nextqa,2024mvbench,2023vitatecs,2024videomme,2024tempcompass,fang2024mmbench}. In particular, NExT-QA \cite{2021nextqa} focuses on causal relation reasoning such as why a certain action is performed, and Video-MME \cite{2024videomme} contains questions that require the combination of both audio and visual information to perform reasoning. Our proposed RivaBench has more challenging questions that require \textit{longer} thinking steps, \textit{broader} world knowledge and a \textit{tighter} combination of audio-visual information.


\vspace{-0.3cm}
\section{video-SALMONN-o1}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{structure.pdf}
    \caption{video-SALMONN-o1 model structure. The input video is processed by the visual and audio branches, generating encodings from the visual and audio frame sequences respectively. Two encoding streams are combined in an interleaved fashion to synchronize across time before sending to LLM.}
    \label{fig:structure}
\end{figure}

\subsection{Model Structure}
We adopt the same model structure as video-SALMONN 2, as shown in Fig. \ref{fig:structure}. As video-SALMONN 2 \cite{videosalmonn2}, the model is built based on a pre-trained visual LLM by adding the audio encoder branch. The input video and audio streams are processed separately by the audio encoder and visual encoder and are then separately mapped to the dimension of the LLM input via individual modality aligners. To combine the audio and visual encodings, the interleaved synchronization module is employed as illustrated in Fig. \ref{fig:structure}. The groups of encodings per visual frame are equally spaced across time, and the audio encodings corresponding to the time between two visual frames $t_1$ and $t_2$ are inserted between the two groups of visual encodings. The process is summarized as in Eqn. \eqref{eq:model1}:
\begin{equation}
    \mathbf{H}^\text{AV} = \text{Concat}(\dots, \mathbf{H}^V_{t_1}, \mathbf{H}^A_{t_1:t_2},\mathbf{H}^V_{t_2}, \dots)
    \label{eq:model1}
\end{equation}
where $\mathbf{H}^A\in\mathbb{R}^{m\times d}$ and $\mathbf{H}^V\in\mathbb{R}^{n\times d}$ represent groups of audio and visual encodings, and $m$ and $n$ are the number of encodings in each group. 

A multi-stage SFT pipeline with the cross-entropy loss on reference response is adopted to train video-SALMONN-o1 before optimizing the reasoning process with RL. Starting from the pre-trained visual model, the audio aligner is trained from scratch keeping other parts of the model frozen. Then, using paired audio-video data, the modality aligners and the low-rank adaptation (LoRA) module \cite{hu2022lora} are trained with other parts frozen. 

\subsection{Reasoning-intensive SFT Data}

We empirically discovered that video understanding models \cite{videollama2,llavavideo,videollava} generally lose the ability to perform step-by-step reasoning when a video is given, and always directly generate the final answer. To re-obtain the reasoning ability during the SFT stage, we create a set of more challenging question-answering pairs based on the same training set videos using proprietary LLMs, and the pipeline is shown in Fig. \ref{fig:sftdata}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{sftdata.pdf}
    \caption{Acquisition pipeline of reasoning-intensive SFT data. The question, answer and reasoning paths are generated by Gemini-1.5-pro taking the video with paired audio as inputs. GPT4o is employed for quality checks to ensure the QA-pair and the reasoning steps are valid and require logical thinking.}
    \label{fig:sftdata}
\end{figure}

For each video with paired audio, we use Gemini-1.5-pro to generate a question-answer pair with the reasoning steps. Then, to avoid bias in Gemini models and ensure the quality of the questions and reasoning steps, a quality check stage is employed using GPT-4o. Questions with poor quality will be discarded and a new question-answer pair will be generated again. In addition to the newly created question, we augment the original training set by generating reasoning paths with Gemini-1.5-pro and checking by GPT-4o following the pipeline to avoid network learning two distinct mechanisms for reasoning and direct answer. This turned out to be important to yield competitive reasoning performance from SFT in our empirical study.

%\section{Process DPO}
\section{Training to Enhance Reasoning Abilities}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{contrastiverollout.pdf}
    % \vspace{-0.3cm}
    \caption{Illustration of the contrastive step selection (top) and pairwise rollout (bottom) to construct per-step expected correctness score for pDPO. Contrastive step selection: Top 2 steps, $s_2$ and $s_5$ are selected in this example, and for $s_2$, an alternative step, $s'_2$, is sampled to form the preference pair. Pairwise rollout: Three rollouts are shown for each step and $s_2$ and $s'_2$ are step pairs with the same prefix solution. The answer correctness is checked using GPT-4o by comparing it against the reference answer.}
    % \vspace{-0.3cm}
    \label{fig:contrastive}
\end{figure*}

\subsection{Preliminary}

The reasoning process refers to the LLM generating the $Q\rightarrow \{s_1, s_2, ..., s_K\}\rightarrow A$ sequence, where $Q$ is the question, $A$ is the answer and $s_k$ are reasoning steps that logically connect the question $Q$ to the final answer $A$. By treating this as a Markov decision process (MDP) and the LLM as the policy model, PRM is to provide feedback for each step $s_k$ that guides the LLM in making accurate reasoning by optimising the policy to maximise the reward.

Following \citet{mathshepherd}, the PRM is to estimate the expected answer correctness, $p_{s_k}$, of a prefix solution $\{s_{1:k}\}$. The expected correctness score can be approximated with Monte Carlo sampling of multiple paths from the prefix solution to an answer $A_n$ as shown in Eqn \eqref{eq:prmtarget}.
\begin{equation}
    p_{s_k} \approx \frac{1}{N}\sum\nolimits^N_{n=1}\mathbbm{1}(A_n=A_\text{ref})
    \label{eq:prmtarget}
\end{equation}
where $A_\text{ref}$ is the reference answer and $A_n$ is one sampled answer. The sampled path $\{s_{k+1,n},s_{k+2,n},\dots, s_{K(n), n}\}$ that leads to $A_n$ is referred to as a \emph{rollout}. The PRM training loss is then shown as
\begin{equation}
    \mathcal{L}_\text{PRM} = \sum\nolimits_{k=1}^K p_{s_k}\log r_{s_k} + (1-p_{s_k})\log (1- r_{s_k})
    \label{eq:prm}
\end{equation}
where $r_{s_k}\in[0, 1]$ is the PRM prediction which can be derived from the LLM output at the last token of each step with a fully connected layer with a sigmoid function $\sigma(\cdot)$.

\subsection{Process DPO}
As pointed out by \citet{llamaberry}, predicting an absolute score fails to exploit the instruction-following capabilities of LLMs as well as influenced by ambiguities in score standards. Both problems are more severe in audio-visual LLMs. Therefore, we propose pDPO for video-SALMONN-o1, which is a pairwise preference modelling approach by training the model to select the better reasoning path rather than giving absolute scores to the paths. Different from the pairwise preference reward model (PPRM) in \cite{llamaberry} that leverages the partial ordering of entire reasoning paths, pDPO models the preference for a specific reasoning step given the same prefix solution. Specifically, the reward function for each step of interest can be written as
\begin{equation}
\small
    r(s_k) = \beta \log\frac{\pi_\theta(s_k|s_{<k},\mathbf{H}^\text{AV})}{\pi_\text{ref}(s_k|s_{<k},\mathbf{H}^\text{AV})} + \beta \log Z(s_{<k},\mathbf{H}^\text{AV})
\end{equation}
where $\pi_\theta$, $\pi_\text{ref}$, $\beta$ and $Z(\cdot)$ are the LLM policy, reference policy, a parameter controlling the deviation from $\pi_\text{ref}$, and the partition function as in \citet{dpo} respectively. $Z(s_{<k},\mathbf{H}^\text{AV})=\sum_{s_k}\pi_\text{ref}(s_k|s_{<k},\mathbf{H}^\text{AV})\exp(\frac{1}{\beta}r(s_k))$. For each step, an alternative step $s'_k$ is generated, and pairwise rollout is performed for both steps as shown in Fig. \ref{fig:contrastive}. The probability of $s_k$ being better than $s'_k$ is then defined using the Bradley-Terry model as
%where $\pi_\theta$ and $\pi_\text{ref}$, $\beta$ and $Z(\cdot)$ follow the same definition in \citet{dpo}. For each step, an alternative step $s'_k$ is generated, and pairwise rollout is performed for both steps as shown in Fig. \ref{fig:contrastive}. The probability of $s_k$ being better than $s'_k$ is then defined using the Bradley-Terry model as
\begin{equation}
    p(s_k\succ s'_k) = \sigma(r(s_k) - r(s'_k)).
\end{equation}
Then, the pDPO loss can be written as: 
\begin{equation}
\small
    \mathcal{L} = - \mathbb{E}\Big{[}\alpha_k \log p(s_k\succ s'_k) + (1 - \alpha_k) \log p(s'_k\succ s_k) \Big{]}
\end{equation}
where $\alpha_k=\mathbbm{1}(p_{s_k}>p_{s'_k})$. Alternatively, $\alpha_k=\sigma((p_{s_k}-p_{s'_k})/\mu)$ can be used as soft labels for DPO to accommodate the estimation noise introduced by the limited number of rollouts in $p_{s_k}$, where $\mu$ is the calibration hyper-parameter determining how much we believe the process annotations. As a result, pDPO retains the advantages of PPRM while offering finer modelling granularity at each step. In practice, pDPO is integrated with PPRM to construct complete reasoning paths, enhancing overall performance. While PPRM enables full-solution-level preference training, ensuring the generation of entire solutions, pDPO complements it by providing fine-grained, step-level preference guidance.
%As a result, pDPO exhibits the advantage of PPRM while having a finer modelling granularity down to each step. In practice, pDPO is combined with PPRM for complete reasoning paths for better performance. PPRM provides full-solution-level preference training which also guarantees the full solution to be generated, while pDPO provides the complementary fine-grained step-level preference information.

\subsection{Contrastive Step Selection}
While rollouts allow automatic process annotation, the computational cost can be high when the numbers of rollouts and steps grow. However, in pDPO, certain steps are more error-prone and hence more valuable to be optimized than others. For general video understanding, by examining a held-out validation set for reasoning paths with wrong answers, we found that over 70\% of the reasoning errors occur at steps where the model misinterprets or hallucinates the video content. Therefore, we choose to particularly focus the pDPO on optimizing those steps. 

%While rollouts allow automatic process annotation, the computational cost can be high when the numbers of rollouts and steps grow. However, in pDPO, certain steps are more error-prone and hence more valuable to be optimized than others. For general video understanding, by examining a held-out validation set for reasoning paths with wrong answers, we found that over 70\% of reasoning, errors occur at steps where the model misinterprets or hallucinates about the video content. Therefore, we choose to particularly focus the pDPO on optimizing those steps. 

%To locate those steps, we quantify the susceptibility of each reasoning step to the input video by applying a tiny perturbation to the input video and measuring the length-normalized per-token KL divergence. Specifically, as shown in the top part of Fig. \ref{fig:contrastive}, for each step $s_k$ we compute the length-normalized KL-divergence using Eqn. \eqref{eq:kld}

To locate those steps, we quantify the susceptibility of each reasoning step to the input video by applying a tiny perturbation to the input video and measuring the length-normalized per-token KL divergence. Specifically, as shown in the top part of Fig. \ref{fig:contrastive}, for each step $s_k$ we compute the length-normalized KL-divergence by 
\begin{equation*}
\small
    d_{s_k} = \frac{1}{|s_k|}\sum_{y_i\in s_k}D_\text{KL}\Big{(}P(y_i|y_{<i}, \mathbf{H}^\text{AV})||P(y_i|y_{<i}, \tilde{\mathbf{H}}^\text{AV})\Big{)},
    \label{eq:kld}
\end{equation*}
where $D_\text{KL}(\cdot)$ computes the KL-divergence between the output distributions with the original inputs $\mathbf{H}^\text{AV}$ and perturbed inputs $\tilde{\mathbf{H}}^\text{AV}$. A higher $d_{s_k}$ indicates that the reasoning step $s_k$ is more susceptible to small input change, and this high susceptibility is likely to yield more diverged subsequent steps. We select the top $T$ steps with the highest $d_{s_k}$ to perform pairwise rollout. While this selection biases pDPO training towards video-dependent errors, the other text-based logic errors can be accommodated by PPRM with entire reasoning paths.

\section{Audio-visual Reasoning Benchmark}

The RivaBench is proposed to extend the scope of complex video understanding with three new reasoning-intensive application scenarios, including academic presentation (Academic), stand-up comedy (StandUp) and synthetic video detection (SynthDec). The statistics of videos for each scenario partition are shown in Table \ref{tab:rivbench}.

\begin{table}[t]
\footnotesize
    \centering
    \caption{RivaBench basic statistics. The duration is given by mean $\pm$ standard deviation. The SynthDec split contains 100 synthetic videos and 100 real videos that human annotators search to have similar content as synthetic videos. MCQ stands for multiple-choice questions. Video sources are all from YouTube.}
    \vspace{0.1in}
    \begin{tabular}{lccc}
    \toprule
    Attribute     &  Academic & StandUp & SynthDec  \\
    \midrule
    Num. of QA  & 1,912   & 2,128 & 200 \\
    Duration (s) & 47.2$\pm$ 66.1 & 43.2$\pm$ 15.1 & 8.1$\pm$3.2 \\
    Format & 5-way MCQ & 5-way MCQ & Yes/No \\
    % Evaluation Metric & Accuracy & Accuracy & F1 score \\
    \bottomrule
    \end{tabular}
    \label{tab:rivbench}
\end{table}

The \textbf{Academic partition} is based on the M3AV \cite{m3av} test set containing recordings of conference or lecture presentations spanning five different domains. Human experts with mathematical, engineering and medical backgrounds are recruited to provide questions, answers and detailed explanations based on the video clips. Example annotations are shown in Figs. \ref{fig:academic_eg1} and \ref{fig:academic_eg2} in Appendix \ref{sec:academicexp}.

%While humour in videos has been explored \cite{smile,fun1,fun2}, the \textbf{StandUp partition} of RivaBench explores from an audio-visual reasoning perspective while most of them are from a descriptive perspective. Specifically, instead of prompting the model to list all funny elements, we particularly focus on understanding why a certain punchline is interesting and task the human annotators to set questions that require reasoning about the comedian's gestures, facial expression and speech content. Human annotators provide questions, answers and explanations (with automatically generated confusing choices), as shown in Fig. \ref{fig:standup_eg1} and \ref{fig:standup_eg2} in Appendix \ref{sec:standupexp}. 

While humour in videos has been explored from a descriptive perspective \cite{smile,fun1,fun2}, the \textbf{StandUp partition} of RivaBench explores from an audio-visual reasoning perspective. Specifically, instead of prompting the model to list all funny elements in the video, we particularly focus on understanding why a certain punchline is interesting and task the human annotators to set questions that require reasoning about the comedian's gestures, facial expression and speech content. Human annotators provide questions, answers and explanations (with automatically generated confusing choices), as shown in Figs. \ref{fig:standup_eg1} and \ref{fig:standup_eg2} in Appendix \ref{sec:standupexp}. 


This paper proposes the \textbf{SynthDec partition} for synthetic video detection, which has great potential since video generation models are becoming increasingly powerful. This task requires LLM to classify whether a given video clip is real or synthetic by finding clues in the video such as motions violating physics rules or objects being distorted. Videos are generated using the Hunyuan-large model \cite{hunyuan} (see examples in Figs. \ref{fig:antispoof1} and \ref{fig:antispoof2}) This is a challenging task that requires both logical reasoning and accurate perception of video content. The SynthDec partition can also serve as the performance indicator for reward models used to train video generators in the future.



\section{Experimental Setup}

\subsection{Model and Training Specifications}

video-SALMONN-o1 is built based on the SigLIP \citep{zhai2023sigmoid} visual encoder and Qwen 2 with 7B parameters backbone LLM. Two linear layers with GELU activation function are used \citep{hendrycks2016gaussian} as the visual aligner. The model processes videos at a 2-frame-per-second rate with a maximum of 60 frames.

The Whisper-Large-v3 encoder \citep{radford2023robust} is used as the audio encoder, and the window-level Q-Former \citep{tang2024extending} with a window length of 0.2 seconds is used as the audio aligner, producing 150 audio tokens for every 30 seconds.
We set LoRA hyper-parameters $r=64$ and $\alpha=256$ for the backbone LLM for both SFT and pDPO. During training, the visual encoder and aligner, audio encoder, and LLM remain frozen. SFT is performed on 16$\times$A100 GPUs for 48 hours and pDPO is trained with 8$\times$A100 GPUs for 24 hours. Prompts used for reasoning are shown in Appendix \ref{sec:prompts}. 
The code, SFT data, pDPO data and model checkpoints will be released.
% \footnote{Demo page: \url{https://anonymous.4open.science/r/video-SALMONN-o1-4B0B}}

\subsection{Data}
\begin{figure}[t]
    \centering
    % \hspace{-0.1cm}
    \includegraphics[width=1.0\linewidth]{reasoningsteps.pdf}
    \vspace{-0.6cm}
    \caption{Distributions of the numbers of reasoning steps in SFT data. Left: Distribution of the entire SFT data. Right: Distribution on the reasoning-intensive subset of SFT data. Due to the difficulty of the reasoning-intensive subset, more reasoning steps are required in general for samples in this set.}
    \vspace{-0.3cm}
    \label{fig:steps}
\end{figure}

Following \citet{videosalmonn2}, the audio modality alignment stage employs LibriSpeech-960h \citep{panayotov2015librispeech} ASR data and AudioCaps \citep{audiocaps} audio caption data to train the audio aligner. During the audio-visual SFT stage, 13k videos with rich audio information are selected with high-quality audio-visual captions. Around 150k normal question-answer (QA) pairs are directly generated using GPT-4o by providing detailed audio-visual captions, and an additional subset of 30k reasoning-intensive SFT QA pairs are generated with the proposed data generation pipeline. Each QA, regardless of the difficulty, is associated with reasoning steps, and the distributions of the numbers of reasoning steps for the QA pairs used for SFT are shown in Fig. \ref{fig:steps}. Both captions and QA pairs are used for SFT.

The reasoning-intensive subset is used to collect the data for pDPO training by sampling 10 paths for each QA. The QA pairs where the SFT model generates incorrect solutions are retained to perform rollouts and others that only contain correct solutions are discarded. For complete solutions, instead of directly comparing the paths \cite{llamaberry}, we compare each pair of solutions against the reference answer using GPT-4o and choose the one closer to the reference as the preferred solution. For intermediate steps, we choose the top 3 steps based on contrastive step selection, and 6 rollouts are performed for each chosen step. As a result, $\sim$100k pairs of complete solutions from 5k video clips are selected, and an extra 100k pairs of step-level partial solution pairs from these complete solutions are used for pDPO.


Besides RivaBench, video-SALMONN-o1 is also evaluated on Video-MME \cite{2024videomme} and NExT-QA \cite{2021nextqa} benchmarks with challenging reasoning questions where the former is an audio-visual task and the latter focuses on visual information only. For consistency, paired audios are also provided for NExT-QA videos if they exist. Note that the synthetic video detection task is \textbf{never seen} in model training, and hence is a zero-shot emergent ability.

%%%%%%%%%%% Result tables

\begin{table*}[t]
    %\small
    \setlength{\tabcolsep}{4pt}
    \centering
    \caption{Main results of video-SALMONN-o1 compared against other visual (V) and audio-visual (A+V) LLMs. SFT refers to the model after SFT with reasoning data and pDPO refers to the model obtained after training with pDPO based on the same SFT model. F1-score (Precision/Recall) is reported for SynthDec and accuracy is reported for others. Results with $\dagger$ are directly taken from the corresponding papers. video-SALMONN-o1 performs reasoning during inference and other open-source models give answers directly.}
    \vspace{0.1in}
    \begin{tabular}{lccccccccc}
    \toprule
    Model     & Modality & VideoMME & NExT-QA & \multicolumn{3}{c}{RivaBench}  \\
    & &  & & StandUp & Academic & SynthDec (P/R) \\
    \midrule
    \rowcolor{Gray}
    Proprietary models & & & & & &\\
    Gemini-1.5-pro \cite{gemini} & A+V & 75.0\%$\dagger$ & 79.2\% & 75.8\% & 67.1\% & 23.6\% (55\%/15\%)\\
    Gemini-1.5-pro+reasoning & A+V & 75.1\% & 79.5\% & 81.8\% & 69.5\% & 40.0\% (49\%/34\%) \\
    % \rowcolor{Gray}
    GPT-4o \cite{gpt4o} & V & 71.9\%$\dagger$ & 81.7\% & 63.3\% & 60.0\%  & 34.1\%(90\%/21\%) \\
    GPT-4o+reasoning & V & 72.1\% & 81.9\% & 69.6\% & 61.0\% & 25.8\%(53\%/17\%) \\
    \midrule
    \rowcolor{Gray}
    Open-source baselines & & & & & &\\
    LLaVA-OneVision \cite{llavaonevision} & V & 58.2\%$\dagger$ & 79.4\%$\dagger$ & 67.2\% & 45.8\% & 0.0\%(97\%/0\%) \\
    video-SALMONN \cite{videosalmonn} & A+V & 43.3\% & 49.2\% & 47.8\% & 33.6\% & 0.0\%(100\%/0\%) \\
    Video-LLaMA 2.1 \cite{videollama2} & A+V & 54.9\%$\dagger$ & 75.6\% & 53.7\%  & 34.3\% & 0.0\%(99\%/0\%)\\
    \midrule
    video-SALMONN-o1 (ours, SFT) & A+V & 62.9\% & 78.2\% & 68.6\% & 42.5\%  & 5.8\%(97\%/5\%) \\
    video-SALMONN-o1 (ours, pDPO) & A+V & \textbf{65.6}\% & \textbf{82.3}\% & \textbf{76.7}\% & \textbf{48.3}\% & \textbf{17.8}\%(87\%/13\%)\\
    \bottomrule
    \end{tabular}
    \label{tab:main}
\end{table*}

\begin{table*}[t]
    % \footnotesize
    \caption{Effect of different parts of the audio-visual SFT data on VideoMME, Academic and StandUp test sets. Underscore for second-best results. ``w/o reasoning-intensive part'' means removing the reasoning-intensive SFT data, and ``w/o any reasoning" always directly outputting answers during SFT. ``Reasoning-intensive part only" always performs reasoning for QA.}
    \vspace{0.1in}
    \centering
    \begin{tabular}{lccccc}
    \toprule
    Training Data    & Inference Reasoning & VideoMME & NExT-QA & Academic & StandUp \\
    \midrule
    Full SFT data & $\tikzxmark$ & \underline{63.7\%} & {80.7\%} & \underline{45.2\%} & \underline{72.3\%} \\
    Full SFT data & $\checkmark$  & 62.9\% & 78.2\% & 42.5\% & 68.6\% \\
    w/o any reasoning & $\tikzxmark$ & 63.2\% & \underline{81.0\%} & 44.1\% & 71.1\% \\
    w/o reasoning-intensive part & $\tikzxmark$ & 62.7\% & 78.9\% & 44.7\% & 71.5\% \\
    w/o reasoning-intensive part & $\checkmark$ & 61.6\% & 76.6\% & 42.3\% & 67.5\% \\
    Reasoning-intensive part only & $\checkmark$ & 58.8\% & 75.2\% & 40.1\% & 63.5\% \\
    \midrule
    Full SFT data + pDPO & $\checkmark$  & \textbf{65.6}\% & \textbf{82.3}\% & \textbf{48.3}\% & \textbf{76.7}\% \\
    \bottomrule
    \end{tabular}
    \label{tab:cot}
    \vspace{-0.2cm}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{Effect of different reward modelling methods on VideoMME, NExT-QA, the StandUp and Academic split of RivaBench. Major@20 and RM@20 are evaluated following \citet{llamaberry}, where Major@20 refers to the accuracy under majority voting with 20 sampled paths, and RM@20 is the best-of-n with 20 samples. Samples are all generated from the model after SFT. pDPO with full paths only uses preference pairs of complete reasoning paths.}
    \vspace{0.1in}
    \begin{tabular}{lccccc}
    \toprule
    Training Configuration & Inference & VideoMME & NExT-QA & StandUp & Academic \\
    \midrule
    % \rowcolor{Gray}
    % SFT models &&&&&\\
    SFT & 1-best & 62.9\% & 78.2\% & 68.6\% & 42.5\% \\
    SFT & Major@20 & 63.5\% & 81.5\% & 73.5\% & 45.3\% \\
    % \rowcolor{Gray}
    % Reward Models &&&&&\\
    SFT + ORM & RM@20 & 62.7\% & 78.5\% & 69.0\% & 42.6\% \\
    SFT + PRM & RM@20 & 63.5\% & 79.3\% & 72.1\% & 43.9\% \\
    % \rowcolor{Gray}
    % Pairwise Preference Methods &&&&&\\
    % SFT + pDPO (full paths only) & 1-best & 64.3\% & 80.6\% & 76.3\% & 45.6\%\\
    SFT + pDPO & 1-best & \textbf{65.6}\% & \textbf{82.3}\% & \textbf{76.7}\% & \textbf{48.3}\% \\
    \bottomrule
    \end{tabular}
    \label{tab:rm}
    \vspace{-0.15in}
\end{table*}
%%%%%%%%%%%

\section{Results}

\subsection{Main Results}


The main results on VideoMME, NExT-QA and the RivaBench are shown in Table \ref{tab:main}. No subtitles are given to any of the models under test for VideoMME. As performance references, we include GPT-4o (checkpoint at 2024-08-06) and Gemini-1.5-pro, with their results on VideoMME as reported in \citet{2024videomme}. When testing GPT-4o with videos, each video is split into images at a frame rate of 2 fps with a maximum of 30 frames due to token limitation, and the sequence of images is sent as the input. For open-source models, we compare video-SALMONN-o1 to LLaVA-OneVision \cite{llavaonevision} (same visual encoder and LLM backbone), together with video-SALMONN \cite{videosalmonn} and Video-LLaMA 2 \cite{videollama2} as the two most recent audio-visual LLMs.

\textbf{Proprietary LLM performance on RivaBench}: For the two proprietary LLMs, GPT-4o underperforms Gemini-1.5-pro on StandUp and Academic test sets due to the lack of audio information. This indicates that RivaBench provides challenging questions that require more audio-visual joint understanding compared to VideoMME. On the SynthDec set, since only the visual part is synthesized, GPT-4o demonstrated a stronger ability. Moreover, by performing reasoning with GPT-4o and Gemini-1.5-pro, larger improvements are found on StandUp and Academic test sets than VideoMME and NExT-QA, indicating the necessity of reasoning on RivaBench.

\textbf{Open-source LLM performance comparison}: Audio-visual SFT on video-SALMONN-o1 already yields better performance than LLaVA-OneVision on VideoMME due to the ability to comprehend speech and audio information, whereas no obvious improvements are found on the other benchmarks. The main improvements on other benchmarks come from pDPO, which achieved 4.1\%, 8.1\% and 5.8\% absolute accuracy improvements on NExT-QA, StandUp and Academic test sets respectively compared to the SFT model. Larger improvements are found on the RivaBench with 6-8\% absolute accuracy improvements obtained compared to LLaVA-OneVision, and video-SALMONN-o1 even performs better on the StandUp test set than Gemini-1.5-pro without reasoning. Besides, compared to other audio-visual LLMs, video-SALMONN-o1 exhibits better interpretability of the model output, and the cause of mistakes can be located by analyzing the reasoning process.
% However, the world knowledge of the backbone LLM limits the improvement of the Academic set.

\textbf{Zero-shot synthetic video detection}: video-SALMONN-o1 achieves zero-shot synthetic video detection ability while other open-source models output ``real" all the time, which also benefit from a better explanation with examples of anomalies in synthesized videos in the prompt. However, even for the videos where the motions obviously violate physics rules, current state-of-the-art video LLMs still fail to detect most of the time. 

In addition, two qualitative examples are shown in Figs. \ref{fig:synthdec1} and \ref{fig:synthdec2} in Appendix \ref{sec:synthdec}, where LlaVA-OneVison (and also other audio-visual models) are unable to provide the reasoning steps and the final answer is completely biased to ``Real". On the other hand, video-SALMONN-o1 can look for distortions in the video as part of its reasoning process, leading to the correct identification of synthesized videos. 

% Case studies are provided in Appendix \ref{cases}. 


\subsection{Effect of SFT Data}

The audio-visual SFT data is crucial for video-SALMONN-o1 to gain the initial audio-visual reasoning ability, and the effect of different data partitions is shown in Table \ref{tab:cot}.

\textbf{Direct answer outperforms reasoning after SFT}: Directly outputting a short answer or an option has been the dominating output mode for audio-visual LLMs on general video understanding, a major difference to math questions. Comparing row 2 to row 1 in Table \ref{tab:cot}, when using all the SFT data including the reasoning-intensive part, the model after SFT is still better at directly generating the answer than performing reasoning. This is due to the exposure bias in teacher forcing which has a much higher impact on the reasoning paths as they are much longer sequences. By learning on its own samples, pDPO mitigates this exposure bias and achieves consistently better performance than the SFT model.
Next, comparing row 1 to row 3 and row 4 in Table \ref{tab:cot}, when directly outputting the answer during inference, incorporating reasoning steps in SFT does not always yield an improvement on videoMME and NExT-QA, despite being slightly helpful on RivaBench. 

\textbf{The reasoning-intensive part is important}: When excluding the reasoning-intensive part, there is a clear degradation in model performance with reasoning during inference, showing the importance of this part of data to enable a better reasoning performance. However, when only using the reasoning-intensive part for SFT, the model struggles to acquire the fundamental audio-visual perception abilities, yielding sub-optimal performance.


\subsection{Effect of pDPO Training}
We then analyse different reward modelling techniques for the model performance in Table \ref{tab:rm}. In addition to pairwise preference models, we include ORM and PRM as proposed in \citet{prm0} as follows:

\textbf{ORM}: A projection layer is added to LLM output states and projects the last output state to a scalar. which is then passed through a sigmoid activation function to predict 1 if the final answer is correct, and 0 otherwise. 

\textbf{PRM}: A projection layer is added to LLM output states and projects the state at the end of each step to a scalar with a sigmoid to predict $\mathbbm{1}(p_{s_k}>0)$. The score of each solution is the lowest score among all steps \cite{mathshepherd}.

Both ORM and PRM are initialized with video-SALMONN-o1 after the SFT stage. Best-of-n is used for ORM and PRM where 20 sampled solutions are generated from the SFT model and the top one with the highest score is selected. Moreover, majority voting among the 20 samples is used as a baseline which is consistently marginally better than the 1-best solution across all test sets. 

While ORM showed mixed results compared to the 1-best solution from the SFT model, PRM showed consistent but marginal improvements and is on par with majority voting. The training loss of PRM and ORM only dropped about 5\%, which reflects the difficulty of learning raw scores for general video understanding tasks. Last, comparing the models above against the pDPO model, the use of pairwise preference models is much more effective compared to predicting the raw score, showing the difficulty of direct raw score modelling in general video QAs. Qualitative examples comparing answers between SFT and pDPO are provided in Figs. \ref{fig:standup_1} to \ref{fig:videomme_2} in Appendix \ref{sec:othercase}.

% Using intermediate steps in pDPO achieved further consistently improves model performance compared to only using the full solutions which is essentially PPRM \cite{llamaberry}, especially on videoMME and Academic test sets that require frequent reference to the video midway through the reasoning process. Qualitative examples comparing answers between SFT and pDPO are provided in Fig. 

\textbf{Effect of Contrastive Step Selection}. To analyze the effect of the number of steps selected for pairwise training, we conducted experiments without intermediate pairs of steps and with all intermediate pairs, in addition to using the top three steps from the contrastive step selection. The comparisons are given in Fig. \ref{fig:topk}. Using intermediate steps in pDPO achieved further consistently improves model performance compared to only using the full solutions, especially on questions that require frequent reference to the video or audio information at intermediate reasoning steps. A case study qualitatively showing the effect of contrastive step selection is included in Appendix \ref{sec:case_sel}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{processDPO_plot.pdf}
    \vspace{-0.3cm}
    \caption{Comparison between different top T steps selected for pDPO. Pairs of full solution paths are always used in addition to pairs of intermediate steps.}
    \vspace{-0.3cm}
    \label{fig:topk}
\end{figure}

% \subsection{Effect of Soft DPO Labels}

% \subsection{Case Studies}

% The case studies showed that video-SALMONN-o1 trained with pDPO is less likely to make unnecessary reasoning steps than SFT and PPRM. 

\section{Conclusions}

We propose video-SALMONN-o1, the first open-source audio-visual LLM with enhanced reasoning abilities. video-SALMONN-o1 is the first to explore reasoning process optimization for general video understanding and proposes the pDPO method with an efficient contrastive step selection algorithm. To further evaluate the reasoning abilities of audio-visual LLMs, the RivaBench is introduced with innovative and challenging tasks and over {4000} high-quality human expert annotations. video-SALMONN-o1 consistently outperforms the strong LLaVA-OneVision baseline with 3-8\% absolute accuracy improvements. pDPO training consistently outperformed the SFT model. Moreover, video-SALMONN-o1 showed zero-shot synthetic video detection abilities as a result of the enhanced reasoning abilities.


\section*{Impact Statement}

By enhancing reasoning abilities in general video understanding, video-SALMONN-o1 provides a more transparent and interpretable interface that is compatible with general videos to access and explain model responses and behaviours. This is indispensable to ensure the reliability of LLMs when applied to different video understanding scenarios and will be largely beneficial for pinpointing the specific causes or errors when the model generates dubious or toxic contents, thus enhancing AI safety.

The approaches in this paper do not give rise to any additional potential biases beyond the ones directly inherited
from the pre-trained model checkpoints used. The audio encoder and visual encoder might work worse for people from
particular demographics. The framework also inherits biases
from all the LLMs used in this paper. To mitigate potential
biases, we clearly describe the nature of each dataset and
provide clear and adequate references to all the resources
we used for video-SALMONN-o1.

The ability of video-SALMONN-o1 to understand speech in
videos could lead to potential technology abuses like surveillance and eavesdropping. To counter this, we’ve consulted with legal experts to establish clear usage guidelines, reducing risks and addressing concerns, highlighting our dedication to responsible research sharing.




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Reasoning SFT Data Example}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{sftexample1.pdf}
    \caption{Example of reasoning SFT data}
    \label{fig:sft_eg1}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{StandUp Data Examples}
\label{sec:standupexp}
Two examples of the StandUp part of RivaBench are shown in Fig. \ref{fig:standup_eg1} and \ref{fig:standup_eg2} respectively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{standup_eg1.pdf}
    \caption{Example of StandUp part of the RivaBench.}
    \label{fig:standup_eg1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{standup_eg2.pdf}
    \caption{Example of StandUp part of the RivaBench.}
    \label{fig:standup_eg2}
\end{figure}

\newpage
\section{Academic Data Examples}
\label{sec:academicexp}
Two examples of the Academic part of RivaBench are shown in Fig. \ref{fig:academic_eg1} and \ref{fig:academic_eg2} respectively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{academic_eg1.pdf}
    \caption{Example of Academic part of the RivaBench.}
    \label{fig:academic_eg1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{academic_eg2.pdf}
    \caption{Example of Academic part of the RivaBench.}
    \label{fig:academic_eg2}
\end{figure}

\section{Synthetic Video Detection Data Examples}
\label{sec:antispoofingexp}
Two synthetic video examples in the SynthDec partition of RivaBench are shown in Fig. \ref{fig:antispoof1} and \ref{fig:antispoof2} respectively.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{antispoofing1.pdf}
    \caption{Example video clip of the SynthDec part of RivaBench.}
    \label{fig:antispoof1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{antispoofing2.pdf}
    \caption{Example video clip of the SynthDec part of RivaBench.}
    \label{fig:antispoof2}
\end{figure}


\section{Prompt Templates}

Prompt templates for video-SALMONN-o1 are shown in Table \ref{tab:prompts}. When trained with the reasoning prompt template, the same template is used during inference. Changing templates may cause small perturbations in performance.

\label{sec:prompts}
\begin{table}[H]
    \centering
    \caption{Prompt used for different types of tasks.}
    \vspace{0.1in}
    \begin{tabular}{lp{12cm}}
    \toprule
    Type & Prompt content \\
    \midrule
    Direct answer     & $<$VIDEO$>$Select the best answer to the following question based on the video. Respond with only the letter of the correct option. \\
    & \text{\{Question\}}\\
    & \text{Choose from: A. \{Option A\}, B, \{Option B\}...}\\
    Reasoning  &  $<$VIDEO$>$ Question:\\
    & \text{\{Question\}}\\
    & \text{Choose from: A. \{Option A\}, B, \{Option B\}...}\\
    & Answer the question step by step. Output each thinking step. Mark the end of each step with $<$end\_of\_step$>$ token. \\
    SynthDec & An AI-generated video contains unnatural distorted things, such as distorted hands or faces. Is the given video AI generated? Answer YES or NO. Answer step by step and output each step clearly. \\
    \bottomrule
    \end{tabular}
    \label{tab:prompts}
\end{table}

\section{Case Studies: Solution with Reasoning Examples}
\label{sec:othercase}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{standup_1.pdf}
    \caption{Example video and solutions from the StandUp test set.}
    \label{fig:standup_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{videomme_1.pdf}
    \caption{Example video and solutions from videoMME test set.}
    \label{fig:videomme_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{videomme_2.pdf}
    \caption{Example video and solutions from videoMME test set.}
    \label{fig:videomme_2}
\end{figure}


\section{Case Studies: Zero-shot Synthetic Video Detection}
\label{sec:synthdec}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{synthdec_1.pdf}
    \caption{Example output from video-SALMONN-o1, GPT-4o and Gemini-1.5-pro for synthetic video detection.}
    \label{fig:synthdec1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{synthdec_2.pdf}
    \caption{Example output from video-SALMONN-o1, GPT-4o and Gemini-1.5-pro for synthetic video detection.}
    \label{fig:synthdec2}
\end{figure}

\newpage
\section{Examples of Contrastive Step Selection Process}
\label{sec:case_sel}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{stepselection_1.pdf}
    \caption{Example of the contrastive step selection process where two sampled paths are shown and the scores $d_{s_k}$ are given for each reasoning steps. The 3rd step in the first solution is wrong due to visual hallucination, and as a result, a very high score is assigned to that step and that step will be used to perform rollout.}
    \label{fig:stepsel_1}
\end{figure}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
