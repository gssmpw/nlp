@inproceedings{2021nextqa,
    title={{Next-QA}: {N}ext phase of question-answering to explaining temporal actions},
    author={Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
    booktitle={Proc. CVPR},
    year={2021}
}

@article{2021value,
    title={Value: {A} multi-task benchmark for video-and-language understanding evaluation},
    author={Li, Linjie and Lei, Jie and Gan, Zhe and Yu, Licheng and Chen, Yen-Chun and Pillai, Rohit and Cheng, Yu and Zhou, Luowei and Wang, Xin Eric and Wang, William Yang and others},
    journal={arXiv preprint arXiv:2106.04632},
    year={2021}
}

@inproceedings{2023egoschema,
    title={Egoschema: {A} diagnostic benchmark for very long-form video language understanding},
    author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
    booktitle={Proc. NeurIPS},
    year={2023}
}

@inproceedings{2023vast,
    title={Vast: {A} vision-audio-subtitle-text omni-modality foundation model and dataset},
    author={Chen, Sihan and Li, Handong and Wang, Qunbo and Zhao, Zijia and Sun, Mingzhen and Zhu, Xinxin and Liu, Jing},
    journal={Proc. NeurIPS},
    year={2023}
}

@article{2023videobench,
    title={Video-bench: {A} comprehensive benchmark and toolkit for evaluating video-based large language models},
    author={Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li},
    journal={arXiv preprint arXiv:2311.16103},
    year={2023}
}

@article{2023vitatecs,
    title={Vitatecs: {A} diagnostic dataset for temporal concept understanding of video-language models},
    author={Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu},
    journal={arXiv preprint arXiv:2311.17404},
    year={2023}
}

@inproceedings{2024mvbench,
    title={Mvbench: {A} comprehensive multi-modal video understanding benchmark},
    author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
    booktitle={Proc. CVPR},
    year={2024}
}

@article{2024tempcompass,
    title={Tempcompass: {D}o video {LLMs} really understand videos?},
    author={Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu},
    journal={arXiv preprint arXiv:2403.00476},
    year={2024}
}

@article{2024videomme,
    title={Video-{MME}: {T}he first-ever comprehensive evaluation benchmark of multi-modal {LLMs} in video analysis},
    author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
    journal={arXiv preprint arXiv:2405.21075},
    year={2024}
}

@inproceedings{AVSD2019,
    title={Audio Visual Scene-Aware Dialog},
    author={Alamri, Huda and Cartillier, Vincent and Das, Abhishek and Wang, Jue and Cherian, Anoop and Essa, Irfan and Batra, Dhruv and Marks, Tim K. and Hori, Chiori and Anderson, Peter},
    booktitle={Proc. CVPR},
    year={2019}
}

@article{avhallubench,
  title={{CrossCheckGPT}: {U}niversal Hallucination Ranking for Multimodal Foundation Models},
  author={Sun, Guangzhi and Manakul, Potsawee and Liusie, Adian and Pipatanakul, Kunat and Zhang, Chao and Woodland, Phil and Gales, Mark},
  journal={arXiv preprint arXiv:2405.13684},
  year={2024}
}

@article{fang2024mmbench,
  title={{MMBench-Video}: {A} Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  author={Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2406.14515},
  year={2024}
}

@article{llamaberry,
      title={{LLaMA-Berry}: {P}airwise Optimization for O1-like {O}lympiad-Level Mathematical Reasoning}, 
      author={Di Zhang and Jianbo Wu and Jingdi Lei and Tong Che and Jiatong Li and Tong Xie and Xiaoshui Huang and Shufei Zhang and Marco Pavone and Yuqiang Li and Wanli Ouyang and Dongzhan Zhou},
      year={2024},
      journal={arXiv:2410.02884},
}

@article{llavacot,
      title={{LLaVA-CoT}: {L}et Vision Language Models Reason Step-by-Step},
      author={Guowei Xu and Peng Jin and Hao Li and Yibing Song and Lichao Sun and Li Yuan},
      year={2024},
      journal={arXiv:2411.10440},
}

@article{mammothvl,
      title={{MAmmoTH-VL}: {E}liciting Multimodal Reasoning with Instruction Tuning at Scale}, 
      author={Jarvis Guo and Tuney Zheng and Yuelin Bai and Bo Li and Yubo Wang and King Zhu and Yizhi Li and Graham Neubig and Wenhu Chen and Xiang Yue},
      year={2024},
      journal={arXiv:2412.05237},
}

@inproceedings{mathshepherd,
    title = "{Math-Shepherd}: {V}erify and Reinforce {LLM}s Step-by-step without Human Annotations",
    author = "Wang, Peiyi  and
      Li, Lei  and
      Shao, Zhihong  and
      Xu, Runxin  and
      Dai, Damai  and
      Li, Yifei  and
      Chen, Deli  and
      Wu, Yu  and
      Sui, Zhifang",
    booktitle = "Proc. ACL",
    year = "2024",
}

@inproceedings{musicAVQA2022,
    title={Learning to answer questions in dynamic audio-visual scenarios},
    author={Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di},
    booktitle={Proc. CVPR},
    year={2022}
}

@article{ormprm,
      title={Solving math word problems with process- and outcome-based feedback}, 
      author={Jonathan Uesato and Nate Kushman and Ramana Kumar and Francis Song and Noah Siegel and Lisa Wang and Antonia Creswell and Geoffrey Irving and Irina Higgins},
      year={2022},
      journal={arXiv:2211.14275},
}

@inproceedings{pano-AVQA2021,
    title={Pano-{AVQA}: {G}rounded audio-visual question answering on 360deg videos},
    author={Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee},
    booktitle={Proc. ICCV},
    year={2021}
}

@article{prm0,
      title={Let's Verify Step by Step}, 
      author={Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
      year={2023},
      journal={arXiv:2305.20050},
}

@article{prm1,
      title={Improve Mathematical Reasoning in Language Models by Automated Process Supervision}, 
      author={Liangchen Luo and Yinxiao Liu and Rosanne Liu and Samrat Phatale and Meiqi Guo and Harsh Lara and Yunxuan Li and Lei Shu and Yun Zhu and Lei Meng and Jiao Sun and Abhinav Rastogi},
      year={2024},
      journal={arXiv:2406.06592},
}

@inproceedings{prm2,
    title = {Making Language Models Better Reasoners with Step-Aware Verifier},
    author = "Li, Yifei  and
      Lin, Zeqi  and
      Zhang, Shizhuo  and
      Fu, Qiang  and
      Chen, Bei  and
      Lou, Jian-Guang  and
      Chen, Weizhu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proc. ACL",
    year = "2023",
}

@INPROCEEDINGS{treesearch1,
      title={Reasoning with Language Model is Planning with World Model}, 
      author={Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu},
      year={2023},
      booktitle={Proc. EMNLP},
}

@article{treesearch2,
      title={Scaling {LLM} Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      journal={arXiv:2408.03314},
}

@INPROCEEDINGS{treesearch3,
  title={Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training},
  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun},
  booktitle={Proc. ICML},
  year={2024}
}

@INPROCEEDINGS{treesearch4,
      title={{Tree of Thoughts}: {D}eliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      booktitle={Proc. NeurIPS},
}

@INPROCEEDINGS{treesearch5,
      title={Think before you speak: {T}raining Language Models With Pause Tokens}, 
      author={Sachin Goyal and Ziwei Ji and Ankit Singh Rawat and Aditya Krishna Menon and Sanjiv Kumar and Vaishnavh Nagarajan},
      year={2024},
      booktitle={Proc. ICLR},
}

@article{valor2023,
    title={{VALOR}: {V}ision-Audio-Language Omni-Perception Pretraining Model and Dataset},
    author={Chen, Sihan and He, Xingjian and Guo, Longteng and Zhu, Xinxin and Wang, Weining and Tang, Jinhui and Liu, Jing},
    journal={arXiv preprint arXiv:2304.08345},
    year={2023}
}

@article{virgo,
      title={Virgo: {A} Preliminary Exploration on Reproducing o1-like MLLM}, 
      author={Yifan Du and Zikang Liu and Yifan Li and Wayne Xin Zhao and Yuqi Huo and Bingning Wang and Weipeng Chen and Zheng Liu and Zhongyuan Wang and Ji-Rong Wen},
      year={2025},
      journal={arXiv:2501.01904},
}

