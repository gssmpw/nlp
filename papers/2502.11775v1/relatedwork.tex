\section{Related Work}
\subsection{CoT Reasoning}
CoT reasoning is one of the remarkable abilities of LLMs when solving difficult and complex problems. Earlier investigations employed prompt tuning and various search algorithms such as the Monte-Carlo tree search during inference time \cite{treesearch1,treesearch2,treesearch3,treesearch4,treesearch5}. Later on, training stage approaches using reinforcement learning (RL) were developed to further and more radically boost the reasoning capabilities of LLMs. PRMs which estimate the value function of each reasoning step have emerged as one of the most prevalent approaches in reasoning optimization tasks \cite{ormprm,prm0,prm1,llamaberry,prm2}. 

However, constructing step-level annotations for PRM training can be expensive and difficult to scale up. As mitigation, \citet{mathshepherd} and \citet{prm1} proposed automatic step annotation using \textit{rollout}, which approximated the expected correctness of each step by sampling multiple paths till the end with the same prefix solution. In particular, \citet{prm1} treats the first wrong step as the critical step to perform rollout which was found by binary search. 

\subsection{Reasoning in Multimodal LLMs}

%Researchers have been investigating optimizing CoT reasoning for multimodal LLMs to tackle increasingly challenging tasks. Most of them focus on extracting graphical or text information from an image and solving mathematical tasks based on the extracted information. Specifically, LLaVA-CoT \cite{llavacot} investigated better sampling and search algorithms to find a better reasoning path for math questions with image inputs. Virgo, on the other hand, explores the fine-tuning data organization and transferability of text-based reasoning tasks to image-based reasoning tasks \cite{virgo}. Very recently, MAmmoTH-VL \cite{mammothvl} reported improvements in general video benchmarks by SFT on a new dataset with reasoning steps. Different from these tasks, video-SALMONN-o1 particularly focuses on general video understanding scenarios, where different parts of the audio-visual information are constantly referred to during the reasoning process.

Researchers have been investigating optimizing CoT reasoning for multimodal LLMs to tackle increasingly challenging tasks. Most of them focus on extracting graphical or text information from an image and solving mathematical tasks based on the extracted information. Specifically, LLaVA-CoT \cite{llavacot} investigated better sampling and search algorithms to find a better reasoning path for math questions with image inputs. Virgo, on the other hand, explores the fine-tuning data organization and transferability of text-based reasoning tasks to image-based reasoning tasks \cite{virgo}. Recently, MAmmoTH-VL \cite{mammothvl} built a large-scale multimodal instruction-tuning dataset that can improve the question-answering performance on diverse modalities including video. Different from these works, video-SALMONN-o1 particularly focuses on general video understanding scenarios, where different parts of the audio-visual information are constantly referred to during the reasoning process.



\subsection{Benchmarks for Audio-visual LLMs}

%The fast-paced development of multimodal LLMs has boosted the creation of more and more challenging benchmarks for video understanding. Benchmarks are also evolving from video description and perception abilities \cite{2021value, AVSD2019, valor2023, musicAVQA2022, 2023vast, 2023videobench, 2023egoschema,pano-AVQA2021,avhallubench}, to video reasoning abilities such as inference about temporal and causal relation \cite{2021nextqa,2024mvbench,2023vitatecs,2024videomme,2024tempcompass,fang2024mmbench}. In particular, NExT-QA \cite{2021nextqa} focuses on causal relation reasoning such as why a certain action is performed, and Video-MME \cite{2024videomme} contains questions that require the combination of both audio and visual information to perform reasoning. RiVBench takes a step further into the reasoning by incorporating more challenging questions that require \textit{longer} thinking steps, \textit{broader} world knowledge and a \textit{tighter} combination of audio-visual information.

The fast-paced development of multimodal LLMs has boosted the creation of more challenging video understanding benchmarks. Benchmark focus evolves from video description and perception abilities \cite{2021value, AVSD2019, valor2023, musicAVQA2022, 2023vast, 2023videobench, 2023egoschema,pano-AVQA2021,avhallubench}, to video reasoning abilities such as inference about temporal and causal relations \cite{2021nextqa,2024mvbench,2023vitatecs,2024videomme,2024tempcompass,fang2024mmbench}. In particular, NExT-QA \cite{2021nextqa} focuses on causal relation reasoning such as why a certain action is performed, and Video-MME \cite{2024videomme} contains questions that require the combination of both audio and visual information to perform reasoning. Our proposed RivaBench has more challenging questions that require \textit{longer} thinking steps, \textit{broader} world knowledge and a \textit{tighter} combination of audio-visual information.


\vspace{-0.3cm}