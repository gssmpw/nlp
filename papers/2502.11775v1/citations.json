[
  {
    "index": 0,
    "papers": [
      {
        "key": "treesearch1",
        "author": "Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu",
        "title": "Reasoning with Language Model is Planning with World Model"
      },
      {
        "key": "treesearch2",
        "author": "Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar",
        "title": "Scaling {LLM} Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"
      },
      {
        "key": "treesearch3",
        "author": "Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun",
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"
      },
      {
        "key": "treesearch4",
        "author": "Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan",
        "title": "{Tree of Thoughts}: {D}eliberate Problem Solving with Large Language Models"
      },
      {
        "key": "treesearch5",
        "author": "Sachin Goyal and Ziwei Ji and Ankit Singh Rawat and Aditya Krishna Menon and Sanjiv Kumar and Vaishnavh Nagarajan",
        "title": "Think before you speak: {T}raining Language Models With Pause Tokens"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ormprm",
        "author": "Jonathan Uesato and Nate Kushman and Ramana Kumar and Francis Song and Noah Siegel and Lisa Wang and Antonia Creswell and Geoffrey Irving and Irina Higgins",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "key": "prm0",
        "author": "Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe",
        "title": "Let's Verify Step by Step"
      },
      {
        "key": "prm1",
        "author": "Liangchen Luo and Yinxiao Liu and Rosanne Liu and Samrat Phatale and Meiqi Guo and Harsh Lara and Yunxuan Li and Lei Shu and Yun Zhu and Lei Meng and Jiao Sun and Abhinav Rastogi",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      },
      {
        "key": "llamaberry",
        "author": "Di Zhang and Jianbo Wu and Jingdi Lei and Tong Che and Jiatong Li and Tong Xie and Xiaoshui Huang and Shufei Zhang and Marco Pavone and Yuqiang Li and Wanli Ouyang and Dongzhan Zhou",
        "title": "{LLaMA-Berry}: {P}airwise Optimization for O1-like {O}lympiad-Level Mathematical Reasoning"
      },
      {
        "key": "prm2",
        "author": "Li, Yifei  and\nLin, Zeqi  and\nZhang, Shizhuo  and\nFu, Qiang  and\nChen, Bei  and\nLou, Jian-Guang  and\nChen, Weizhu",
        "title": "Making Language Models Better Reasoners with Step-Aware Verifier"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "mathshepherd",
        "author": "Wang, Peiyi  and\nLi, Lei  and\nShao, Zhihong  and\nXu, Runxin  and\nDai, Damai  and\nLi, Yifei  and\nChen, Deli  and\nWu, Yu  and\nSui, Zhifang",
        "title": "{Math-Shepherd}: {V}erify and Reinforce {LLM}s Step-by-step without Human Annotations"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "prm1",
        "author": "Liangchen Luo and Yinxiao Liu and Rosanne Liu and Samrat Phatale and Meiqi Guo and Harsh Lara and Yunxuan Li and Lei Shu and Yun Zhu and Lei Meng and Jiao Sun and Abhinav Rastogi",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "prm1",
        "author": "Liangchen Luo and Yinxiao Liu and Rosanne Liu and Samrat Phatale and Meiqi Guo and Harsh Lara and Yunxuan Li and Lei Shu and Yun Zhu and Lei Meng and Jiao Sun and Abhinav Rastogi",
        "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "llavacot",
        "author": "Guowei Xu and Peng Jin and Hao Li and Yibing Song and Lichao Sun and Li Yuan",
        "title": "{LLaVA-CoT}: {L}et Vision Language Models Reason Step-by-Step"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "virgo",
        "author": "Yifan Du and Zikang Liu and Yifan Li and Wayne Xin Zhao and Yuqi Huo and Bingning Wang and Weipeng Chen and Zheng Liu and Zhongyuan Wang and Ji-Rong Wen",
        "title": "Virgo: {A} Preliminary Exploration on Reproducing o1-like MLLM"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "mammothvl",
        "author": "Jarvis Guo and Tuney Zheng and Yuelin Bai and Bo Li and Yubo Wang and King Zhu and Yizhi Li and Graham Neubig and Wenhu Chen and Xiang Yue",
        "title": "{MAmmoTH-VL}: {E}liciting Multimodal Reasoning with Instruction Tuning at Scale"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "llavacot",
        "author": "Guowei Xu and Peng Jin and Hao Li and Yibing Song and Lichao Sun and Li Yuan",
        "title": "{LLaVA-CoT}: {L}et Vision Language Models Reason Step-by-Step"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "virgo",
        "author": "Yifan Du and Zikang Liu and Yifan Li and Wayne Xin Zhao and Yuqi Huo and Bingning Wang and Weipeng Chen and Zheng Liu and Zhongyuan Wang and Ji-Rong Wen",
        "title": "Virgo: {A} Preliminary Exploration on Reproducing o1-like MLLM"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "mammothvl",
        "author": "Jarvis Guo and Tuney Zheng and Yuelin Bai and Bo Li and Yubo Wang and King Zhu and Yizhi Li and Graham Neubig and Wenhu Chen and Xiang Yue",
        "title": "{MAmmoTH-VL}: {E}liciting Multimodal Reasoning with Instruction Tuning at Scale"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "2021value",
        "author": "Li, Linjie and Lei, Jie and Gan, Zhe and Yu, Licheng and Chen, Yen-Chun and Pillai, Rohit and Cheng, Yu and Zhou, Luowei and Wang, Xin Eric and Wang, William Yang and others",
        "title": "Value: {A} multi-task benchmark for video-and-language understanding evaluation"
      },
      {
        "key": "AVSD2019",
        "author": "Alamri, Huda and Cartillier, Vincent and Das, Abhishek and Wang, Jue and Cherian, Anoop and Essa, Irfan and Batra, Dhruv and Marks, Tim K. and Hori, Chiori and Anderson, Peter",
        "title": "Audio Visual Scene-Aware Dialog"
      },
      {
        "key": "valor2023",
        "author": "Chen, Sihan and He, Xingjian and Guo, Longteng and Zhu, Xinxin and Wang, Weining and Tang, Jinhui and Liu, Jing",
        "title": "{VALOR}: {V}ision-Audio-Language Omni-Perception Pretraining Model and Dataset"
      },
      {
        "key": "musicAVQA2022",
        "author": "Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di",
        "title": "Learning to answer questions in dynamic audio-visual scenarios"
      },
      {
        "key": "2023vast",
        "author": "Chen, Sihan and Li, Handong and Wang, Qunbo and Zhao, Zijia and Sun, Mingzhen and Zhu, Xinxin and Liu, Jing",
        "title": "Vast: {A} vision-audio-subtitle-text omni-modality foundation model and dataset"
      },
      {
        "key": "2023videobench",
        "author": "Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li",
        "title": "Video-bench: {A} comprehensive benchmark and toolkit for evaluating video-based large language models"
      },
      {
        "key": "2023egoschema",
        "author": "Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra",
        "title": "Egoschema: {A} diagnostic benchmark for very long-form video language understanding"
      },
      {
        "key": "pano-AVQA2021",
        "author": "Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee",
        "title": "Pano-{AVQA}: {G}rounded audio-visual question answering on 360deg videos"
      },
      {
        "key": "avhallubench",
        "author": "Sun, Guangzhi and Manakul, Potsawee and Liusie, Adian and Pipatanakul, Kunat and Zhang, Chao and Woodland, Phil and Gales, Mark",
        "title": "{CrossCheckGPT}: {U}niversal Hallucination Ranking for Multimodal Foundation Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "2021nextqa",
        "author": "Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng",
        "title": "{Next-QA}: {N}ext phase of question-answering to explaining temporal actions"
      },
      {
        "key": "2024mvbench",
        "author": "Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others",
        "title": "Mvbench: {A} comprehensive multi-modal video understanding benchmark"
      },
      {
        "key": "2023vitatecs",
        "author": "Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu",
        "title": "Vitatecs: {A} diagnostic dataset for temporal concept understanding of video-language models"
      },
      {
        "key": "2024videomme",
        "author": "Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",
        "title": "Video-{MME}: {T}he first-ever comprehensive evaluation benchmark of multi-modal {LLMs} in video analysis"
      },
      {
        "key": "2024tempcompass",
        "author": "Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu",
        "title": "Tempcompass: {D}o video {LLMs} really understand videos?"
      },
      {
        "key": "fang2024mmbench",
        "author": "Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai",
        "title": "{MMBench-Video}: {A} Long-Form Multi-Shot Benchmark for Holistic Video Understanding"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "2021nextqa",
        "author": "Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng",
        "title": "{Next-QA}: {N}ext phase of question-answering to explaining temporal actions"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "2024videomme",
        "author": "Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",
        "title": "Video-{MME}: {T}he first-ever comprehensive evaluation benchmark of multi-modal {LLMs} in video analysis"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "2021value",
        "author": "Li, Linjie and Lei, Jie and Gan, Zhe and Yu, Licheng and Chen, Yen-Chun and Pillai, Rohit and Cheng, Yu and Zhou, Luowei and Wang, Xin Eric and Wang, William Yang and others",
        "title": "Value: {A} multi-task benchmark for video-and-language understanding evaluation"
      },
      {
        "key": "AVSD2019",
        "author": "Alamri, Huda and Cartillier, Vincent and Das, Abhishek and Wang, Jue and Cherian, Anoop and Essa, Irfan and Batra, Dhruv and Marks, Tim K. and Hori, Chiori and Anderson, Peter",
        "title": "Audio Visual Scene-Aware Dialog"
      },
      {
        "key": "valor2023",
        "author": "Chen, Sihan and He, Xingjian and Guo, Longteng and Zhu, Xinxin and Wang, Weining and Tang, Jinhui and Liu, Jing",
        "title": "{VALOR}: {V}ision-Audio-Language Omni-Perception Pretraining Model and Dataset"
      },
      {
        "key": "musicAVQA2022",
        "author": "Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di",
        "title": "Learning to answer questions in dynamic audio-visual scenarios"
      },
      {
        "key": "2023vast",
        "author": "Chen, Sihan and Li, Handong and Wang, Qunbo and Zhao, Zijia and Sun, Mingzhen and Zhu, Xinxin and Liu, Jing",
        "title": "Vast: {A} vision-audio-subtitle-text omni-modality foundation model and dataset"
      },
      {
        "key": "2023videobench",
        "author": "Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li",
        "title": "Video-bench: {A} comprehensive benchmark and toolkit for evaluating video-based large language models"
      },
      {
        "key": "2023egoschema",
        "author": "Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra",
        "title": "Egoschema: {A} diagnostic benchmark for very long-form video language understanding"
      },
      {
        "key": "pano-AVQA2021",
        "author": "Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee",
        "title": "Pano-{AVQA}: {G}rounded audio-visual question answering on 360deg videos"
      },
      {
        "key": "avhallubench",
        "author": "Sun, Guangzhi and Manakul, Potsawee and Liusie, Adian and Pipatanakul, Kunat and Zhang, Chao and Woodland, Phil and Gales, Mark",
        "title": "{CrossCheckGPT}: {U}niversal Hallucination Ranking for Multimodal Foundation Models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "2021nextqa",
        "author": "Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng",
        "title": "{Next-QA}: {N}ext phase of question-answering to explaining temporal actions"
      },
      {
        "key": "2024mvbench",
        "author": "Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others",
        "title": "Mvbench: {A} comprehensive multi-modal video understanding benchmark"
      },
      {
        "key": "2023vitatecs",
        "author": "Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu",
        "title": "Vitatecs: {A} diagnostic dataset for temporal concept understanding of video-language models"
      },
      {
        "key": "2024videomme",
        "author": "Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",
        "title": "Video-{MME}: {T}he first-ever comprehensive evaluation benchmark of multi-modal {LLMs} in video analysis"
      },
      {
        "key": "2024tempcompass",
        "author": "Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu",
        "title": "Tempcompass: {D}o video {LLMs} really understand videos?"
      },
      {
        "key": "fang2024mmbench",
        "author": "Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai",
        "title": "{MMBench-Video}: {A} Long-Form Multi-Shot Benchmark for Holistic Video Understanding"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "2021nextqa",
        "author": "Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng",
        "title": "{Next-QA}: {N}ext phase of question-answering to explaining temporal actions"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "2024videomme",
        "author": "Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",
        "title": "Video-{MME}: {T}he first-ever comprehensive evaluation benchmark of multi-modal {LLMs} in video analysis"
      }
    ]
  }
]