\section{Related Work}
\label{sec:relatedwork}

\myPara{Diffusion models for customization.} In the realm of diffusion models~\cite{rombach2022high} for customized tasks, customization refers to the process by which the model learns to interpret new definitions provided by the user.
%
Techniques such as Textual Inversion~\cite{zhang2023inversion,alaluf2023neural,voynov2023p+}, DreamBooth~\cite{ruiz2023dreambooth}, and Custom Diffusion~\cite{kumari2023multi} enable the model to capture target concepts with only a limited number of images through token-based optimization. 
%
Specifically, Textual Inversion fine-tunes embeddings to reconstruct the target, DreamBooth uses less common class-specific terms to expand object categories, and Custom Diffusion focuses on fine-tuning the cross-attention layers within the diffusion model to learn new concepts. 
%
Additionally, there are methods that do not require training 
when inferring~\cite{xie2023smartbrush,xiao2024fastcomposer,avrahami2023break,shi2024instantbooth}, but their approaches to utilize pre-trained modules may perform suboptimally for certain specialized tasks.
%
LoRA~\cite{LoRA} and its variants~\cite{hayou2024lora+,zhang2023lora,zhou2024lora,zhou2024lora,kopiczko2023vera,zi2023delta,ren2024melora} are well-known for their ability to fine-tune large models and deliver high-quality results, making them an good choice for practitioners.


\myPara{LoRA combination in image generation.} In the field of image generation, research on LoRA combinations has primarily been advanced in two directions, including the integration of multiple objects and the fusion of contents with styles.
%
For object integration, studies have mainly focused on enabling models to integrate diverse object concepts encapsulated within multiple LoRAs~\cite{gu2024mix,LoRA-Composer,jiang2024mc,dong2024continually,liu2023cones}. 
%
By fine-tuning the subject LoRAs, these models can assimilate various new concepts and manage object layouts through masking techniques. 
%
Regarding content-style fusion, several works, such as MergingLoRA~\cite{Mergingloras}, Mixture-of-Subspaces~\cite{wu2024mixture}, and ZipLoRA~\cite{ZipLoRA}, have proposed approaches involving hyperparameter tuning and learning fusion matrices to merge pre-trained LoRA weight layers.
%
However, these methods may face challenges, such as concept dilution, blurring of fine details, and specific training requirements. 
%
Recently, B-LoRA~\cite{B-LoRA} has identified distinct roles for attention modules in the generative process, thereby achieving object-style decoupling within LoRA by training only two core attention modules. 
%
Additionally, LoRA Composition~\cite{Multi-LoRAComposition} uses a cyclic update of the model's LoRA modules to allow multiple LoRAs to collaboratively guide the model, allowing a variety of cross-concept fusion. 
%
Despite these advancements, existing methods continue to face challenges, including insufficient control precision, loss of object style, and high training requirements.