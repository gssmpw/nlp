\section{Related Work}
\label{h:related-work}

The evaluation of explanations remains a critical challenge in XAI research. To address this, functional evaluation techniques have emerged as key computational methods for assessing the quality of explanations without human intervention**Samek et al., "Evaluating the Interpretability of Deep Neural Networks"**.

We focus on perturbation analysis as a computational method to measure the correctness and compactness of explanations, properties identified as key quality criteria for XAI methods**Friedman, "Multivariate Adaptive Regression Splines"**. This approach was first introduced by Samek et al.**Samek et al., "Evaluating the Interpretability of Deep Neural Networks"**, to evaluate feature attribution methods in the image domain. It involves sequentially perturbing pixels in order of the most relevant features first by replacing them with noninformative values and observing the impact on model predictions. This process generates a perturbation curve that tracks these prediction changes, allowing the calculation of metrics such as the area under or over the curve to jointly measure the correctness and compactness of explanations.

In this context, correctness refers to whether an explanation faithfully identifies features that truly influence the model's prediction. Compactness captures how concisely the explanation represents the model's behavior. A compact explanation would show a quick degradation of predictions when perturbing just a few highly-ranked features, indicating that the model relies on a small subset of input features. The fundamental assumption underlying this approach is that perturbing important features should degrade model predictions proportionally to their attributed importance, while perturbing irrelevant features should have minimal effects on the model output.

Within time series classification, there are various explanation methods, categorized into approaches based on time points, subsequences, instances, and others**Huang et al., "Feature selection for multivariate time series"**. Our work focuses on feature attribution at the level of time points, examining how each point within a time series contributes to model predictions. As illustrated in Figure~\ref{fig:example_attributions}, these explanations identify the most influential parts of a time series for a prediction, visualized as a heatmap where darker regions indicate minimal contribution and lighter regions indicate stronger contribution to the prediction. The figure also demonstrates how different attribution methods can yield varying explanations for the same instance, highlighting the need for robust evaluation methods that answer the question of which explanation is correct.

\input{fig_example_attributions}

Figure~\ref{fig:perturbation-illustration} demonstrates how feature attributions can guide the perturbation process, where important time points or segments of a time series (indicated by darker red) are replaced with non-informative values like zero. This approach forms the basis of perturbation-based evaluation methods discussed below.
**Schlegel et al., "Evaluating and improving explainability in time series classification"** were the first to apply perturbation analysis to time series classification, evaluating the quality of attributions by measuring the average change in accuracy over the perturbed samples with four perturbation strategies, including zero and mean value replacement.
**Mercier et al., "A framework for evaluating feature attribution methods"** expanded on this framework by evaluating attributions with additional metrics from image explanations, such as sensitivity and infidelity, revealing that no single attribution method consistently outperforms others in all aspects of evaluation.

\input{fig_perturbation_illustration}

Another methodological advance came from **Šimić et al., "Evaluating feature attribution methods for time series classification"**, who introduced a metric that compares perturbations of features ordered by their attributed importance. Their approach addressed a key limitation: perturbation strategies can affect predictions regardless of the relevance and location of the feature. By measuring the difference between most and least relevant feature perturbations, they provided a more robust assessment of attribution quality. Their metric builds upon the degradation score introduced for image explanations**Samek et al., "Evaluating the Interpretability of Deep Neural Networks"**, adapting it with cubic weighting to emphasize the early divergence between perturbation orders.

Further developments focused on understanding the effectiveness of perturbations. 
**Schlegel et al., "Improving explainability in time series classification through visualization and evaluation"** introduced a novel visualization method to qualitatively assess the effectiveness of perturbations by showing class distribution histograms and distances between the original and perturbed time series, among others. They also benchmarked the effectiveness of 16 perturbation strategies by recording the number of flipped class labels.
Building on this, **Schlegel et al., "Evaluating and improving explainability in time series classification"** introduced the AttributionStabilityIndicator, which incorporates the correlation between original and perturbed time series to ensure minimal data perturbations while maintaining a significant prediction impact.

Recent work explored additional methodological refinements. **Turbé et al., "Perturbation-based evaluation of feature attribution methods"** incorporated perturbations into model training to mitigate distribution shifts, while Nguyen et al.**Nguyen et al., "A framework for evaluating and recommending feature attribution methods"** developed a framework to recommend optimal explanation methods based on aggregate accuracy loss across perturbed samples.
Furthermore, **Serramazza et al., "Evaluating and extending InterpretTime for multivariate time series classification"** evaluated and extended InterpretTime**Huang et al., "Feature selection for multivariate time series"**, on various multivariate time series classification tasks by averaging different perturbation strategies and applying said strategies in chunks.

However, an important question has remained unexplored: how do the characteristics of different classes affect perturbation-based evaluation methods? Perturbation strategies may be influenced by classifier biases. For example, if a classifier learned to associate certain perturbation values (such as zero) with specific classes, the effectiveness of perturbation-based evaluation could vary between different predicted classes. 
This phenomenon occurs when perturbation values inadvertently match features the model has associated with a specific class. As a result, substituting ``important'' features with these values might paradoxically reinforce rather than disrupt the prediction, regardless of attribution correctness.

This consideration may help explain some findings in the literature. For example, **Schlegel et al., "Evaluating and improving explainability in time series classification"** evaluated 16 different perturbation approaches and found that for most datasets, only up to 60\% of the samples changed their predicted label under perturbation, regardless of the strategy employed. These results suggest that the effectiveness of perturbations might be influenced by factors beyond the perturbation strategy itself. Our work investigates whether class-dependent effects could explain these observed patterns, examining how the relationship between perturbation strategies and learned class representations might affect evaluation outcomes.