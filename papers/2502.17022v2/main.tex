\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{microtype}
\usepackage[unicode,
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue,
    bookmarks=true,
    bookmarksnumbered=true]{hyperref}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\urlstyle{rm}

\begin{document}

\title{Class-Dependent Perturbation Effects in Evaluating Time Series Attributions}
\titlerunning{Class-Dependent Perturbation Effects}
\authorrunning{Baer et al.}

\author{Gregor Baer\inst{1, 2}\orcidID{0009-0002-9918-1376} \and
Isel Grau\inst{1, 2}\orcidID{0000-0002-8035-2887} \and
Chao Zhang\inst{2, 3}\orcidID{0000-0001-9811-1881} \and
Pieter Van Gorp \inst{1,2}\orcidID{0000-0001-5197-3986}}

\institute{Information Systems, Eindhoven University of Technology, Eindhoven, The Netherlands \and Eindhoven Artificial Intelligence Systems Institute, Eindhoven University of Technology, Eindhoven, The Netherlands \and Human-Technology Interaction, Eindhoven University of Technology, Eindhoven, The Netherlands
}

\maketitle

\begin{abstract}
As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contribute the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through systematic empirical analysis across multiple datasets, model architectures, and perturbation strategies, we reveal previously overlooked class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences.  Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions, offering particular value for class-imbalanced datasets. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common.\footnote{Code and results are available at~\url{https://github.com/gregorbaer/class-perturbation-effects}.}
\keywords{Feature attribution \and Perturbation analysis \and XAI evaluation \and Time series classification}.
\end{abstract}

\section{Introduction}

Explainable Artificial Intelligence (XAI) has emerged as a critical paradigm for understanding complex machine learning models, particularly in domains where trust and explainability are essential, such as finance or healthcare. Within XAI, feature attribution methods quantify how input features contribute to model predictions, with their often model-agnostic nature enabling application across different architectures and data types. These methods are increasingly being applied to structured data domains, such as time series, where temporal dependencies pose unique challenges. In such contexts, ensuring reliable evaluation of attribution quality becomes crucial~\cite{theissler.etal_2022_explainable}.

The evaluation of feature attribution methods faces a fundamental methodological challenge: the absence of a ground truth for explanations. Although human-centered evaluation offers a direct assessment of the utility of explanations~\cite{rong.etal_2024_humancentered}, it suffers from scalability limitations and potential domain-specific biases. Consequently, functional evaluation approaches have emerged as primary validation frameworks, with the aim of computationally verifying whether attribution methods satisfy certain desirable properties~\cite{doshi-velez.kim_2018_considerations,nauta.etal_2023_anecdotal}. Perturbation analysis represents one such framework that evaluates attribution correctness by measuring how modifying features impacts model predictions. This approach rests on a key assumption: perturbing important features should yield proportional changes in model output. 

Although perturbation analysis has gained traction for evaluating attribution methods in structured data domains like time series, previous work has mainly focused on aggregate performance metrics. Studies note that perturbation effectiveness can vary substantially with data characteristics, leading to recommendations to evaluate multiple ways of perturbing features~\cite{schlegel.keim_2023_deep,serramazza.etal_2024_improving}. However, how this effectiveness varies with specific data characteristics remains largely unexplored. A closer examination of reported results reveals an intriguing pattern: substantial portions of datasets can remain unaffected by perturbation when using a single strategy uniformly across all instances~\cite{schlegel.keim_2023_deep,simic.etal_2022_perturbation}. This observation suggests underlying methodological challenges that have not yet been systematically investigated.

Our analysis of these empirical patterns points to an important methodological limitation: the effectiveness of perturbation-based evaluation can vary substantially across different predicted classes, which we refer to as \emph{class-dependent perturbation effects}. These effects manifest when perturbation strategies effectively validate feature attributions for some classes while showing limited or no sensitivity for others. We hypothesize that such behavior emerges from classifier biases, where models learn to associate certain perturbation values with specific classes, potentially compromising the reliability of current evaluation practices.

Our research examines how class-dependent effects influence perturbation-based evaluation of attributions. Through extensive empirical analysis, we show that these effects appear more pronounced with perturbation strategies that show strong aggregate performance, and persist across different perturbation strategies, model architectures, and attribution methods. This asymmetry in perturbation effectiveness has important implications: data set imbalance may influence evaluation results, and evaluation metrics might reflect specific model behaviors rather than attribution quality. Although our evidence stems from time series classification, similar considerations may extend to other structured data domains such as computer vision.

This paper makes several contributions to existing XAI evaluation methodologies. First, we identify and characterize class-dependent effects in the evaluation of feature attributions with perturbation, supported by comprehensive empirical evidence across four commonly used benchmark datasets. Second, we introduce a penalty term that can be applied to any aggregate XAI evaluation metric to investigate the extent of class-dependent effects. Third, we provide recommendations for evaluation protocols that consider these effects, including how to assess whether attribution methods are affected by class-specific perturbation behaviors.

The remainder of this paper is organized as follows. Section~\ref{h:related-work} discusses related work on perturbation analysis for time series classification.
Section~\ref{h:method} introduces the notation and a formal definition of perturbation analysis, as well as the metrics used to measure explanation correctness and class differences.
Section~\ref{h:experiment-set-up} presents our experimental setup to investigate class-dependent perturbation effects. 
Section~\ref{h:results-discussion} analyzes our results and discusses their implications for XAI evaluation. 
Finally, Section~\ref{h:conclusion} concludes with recommendations for future research directions.

 
\section{Related Work}\label{h:related-work}

The evaluation of explanations remains a critical challenge in XAI research. To address this, functional evaluation techniques have emerged as key computational methods for assessing the quality of explanations without human intervention~\cite{doshi-velez.kim_2018_considerations,nauta.etal_2023_anecdotal}. 

We focus on perturbation analysis as a computational method to measure the correctness and compactness of explanations, properties identified as key quality criteria for XAI methods~\cite{nauta.etal_2023_anecdotal}. This approach was first introduced by Samek et al.~\cite{samek.etal_2017_evaluating} to evaluate feature attribution methods in the image domain. It involves sequentially perturbing pixels in order of the most relevant features first by replacing them with noninformative values and observing the impact on model predictions. This process generates a perturbation curve that tracks these prediction changes, allowing the calculation of metrics such as the area under or over the curve to jointly measure the correctness and compactness of explanations.

In this context, correctness refers to whether an explanation faithfully identifies features that truly influence the model's prediction. Compactness captures how concisely the explanation represents the model's behavior. A compact explanation would show a quick degradation of predictions when perturbing just a few highly-ranked features, indicating that the model relies on a small subset of input features. The fundamental assumption underlying this approach is that perturbing important features should degrade model predictions proportionally to their attributed importance, while perturbing irrelevant features should have minimal effects on the model output.

Within time series classification, there are various explanation methods, categorized into approaches based on time points, subsequences, instances, and others~\cite{theissler.etal_2022_explainable}. Our work focuses on feature attribution at the level of time points, examining how each point within a time series contributes to model predictions. As illustrated in Figure~\ref{fig:example_attributions}, these explanations identify the most influential parts of a time series for a prediction, visualized as a heatmap where darker regions indicate minimal contribution and lighter regions indicate stronger contribution to the prediction. The figure also demonstrates how different attribution methods can yield varying explanations for the same instance, highlighting the need for robust evaluation methods that answer the question of which explanation is correct.

\input{fig_example_attributions}

Figure~\ref{fig:perturbation-illustration} demonstrates how feature attributions can guide the perturbation process, where important time points or segments of a time series (indicated by darker red) are replaced with non-informative values like zero. This approach forms the basis of perturbation-based evaluation methods discussed below.
Schlegel et al.~\cite{schlegel.etal_2019_rigorous} were the first to apply perturbation analysis to time series classification, evaluating the quality of attributions by measuring the average change in accuracy over the perturbed samples with four perturbation strategies, including zero and mean value replacement.
Mercier et al.~\cite{mercier.etal_2022_time} expanded on this framework by evaluating attributions with additional metrics from image explanations, such as sensitivity and infidelity, revealing that no single attribution method consistently outperforms others in all aspects of evaluation.

\input{fig_perturbation_illustration}

Another methodological advance came from Šimić et al.~\cite{simic.etal_2022_perturbation}, who introduced a metric that compares perturbations of features ordered by their attributed importance. Their approach addressed a key limitation: perturbation strategies can affect predictions regardless of the relevance and location of the feature. By measuring the difference between most and least relevant feature perturbations, they provided a more robust assessment of attribution quality. Their metric builds upon the degradation score introduced for image explanations~\cite{schulz.etal_2020_restricting}, adapting it with cubic weighting to emphasize the early divergence between perturbation orders.

Further developments focused on understanding the effectiveness of perturbations. 
Schlegel et al.~\cite{schlegel.keim_2023_deep} introduced a novel visualization method to qualitatively assess the effectiveness of perturbations by showing class distribution histograms and distances between the original and perturbed time series, among others. They also benchmarked the effectiveness of 16 perturbation strategies by recording the number of flipped class labels.
Building on this, Schlegel et al.~\cite{schlegel.keim_2023_introducing} introduced the AttributionStabilityIndicator, which incorporates the correlation between original and perturbed time series to ensure minimal data perturbations while maintaining a significant prediction impact.

Recent work explored additional methodological refinements. Turbé et al.~\cite{turbe.etal_2023_evaluation} incorporated perturbations into model training to mitigate distribution shifts, while Nguyen et al.~\cite{nguyen.etal_2024_robust} developed a framework to recommend optimal explanation methods based on aggregate accuracy loss across perturbed samples.
Furthermore, Serramazza et al.~\cite{serramazza.etal_2024_improving} evaluated and extended InterpretTime~\cite{turbe.etal_2023_evaluation} on various multivariate time series classification tasks by averaging different perturbation strategies and applying said strategies in chunks.

However, an important question has remained unexplored: how do the characteristics of different classes affect perturbation-based evaluation methods? Perturbation strategies may be influenced by classifier biases. For example, if a classifier learned to associate certain perturbation values (such as zero) with specific classes, the effectiveness of perturbation-based evaluation could vary between different predicted classes. 
This phenomenon occurs when perturbation values inadvertently match features the model has associated with a specific class. As a result, substituting ``important'' features with these values might paradoxically reinforce rather than disrupt the prediction, regardless of attribution correctness.

This consideration may help explain some findings in the literature. For example, Schlegel et al.~\cite{schlegel.keim_2023_deep} evaluated 16 different perturbation approaches and found that for most datasets, only up to 60\% of the samples changed their predicted label under perturbation, regardless of the strategy employed. These results suggest that the effectiveness of perturbations might be influenced by factors beyond the perturbation strategy itself. Our work investigates whether class-dependent effects could explain these observed patterns, examining how the relationship between perturbation strategies and learned class representations might affect evaluation outcomes.

 
\section{Class-Adjusted Perturbation Analysis}\label{h:method}

Feature attribution methods for time series classification identify the time points that influence a model's predictions.
Since there is usually no ground truth for evaluating attributions, perturbation analysis is commonly used to assess attribution quality by modifying input features and observing the impact on model predictions.
The assumption is that destroying information at important time points should cause the predictions to change, while perturbing irrelevant time points should have minimal impact.

Let $\mathbf{x} = [x_1, \dots, x_N]$ represent a univariate time series of length $N$. A classifier $f(\mathbf{x})$ outputs predicted probabilities over $C$ classes, where $q_c$ denotes the probability of class $c$. An attribution method produces relevance scores $\mathbf{r} = [r_1, \dots, r_N]$ of the same length as $\mathbf{x}$, where $r_i$ quantifies the importance of time point $i$ to the model's prediction.
To evaluate whether attributions correctly identify relevant features, we perturb the time series $\mathbf{x}$ using a perturbation strategy $p$. The perturbed value at time point $i$ is denoted as $x'_i$.  Common perturbation strategies include replacing values with constants ($x'_i = 0$), statistical aggregates ($x'_i = \text{mean}(\mathbf{x})$), or transformations based on local statistics.

We evaluate attribution quality using the degradation score (DS)~\cite{simic.etal_2022_perturbation,schulz.etal_2020_restricting}. This metric compares two perturbation sequences: most relevant features first (MoRF), where features are perturbed in descending order of attributed importance, and least relevant features first (LeRF), where features are perturbed in ascending order. An effective attribution method should show strong prediction changes under MoRF perturbation but minimal impact under LeRF perturbation. This aligns with the intuition that modifying truly important features should significantly disrupt the model's decision-making process, while perturbing irrelevant features should leave the core signal intact. Figure~\ref{fig:ds_method_example} illustrates these expected behaviors.

\input{fig_ds_method_example}

Formally, for a given instance $\mathbf{x}$ belonging to class $c$, a perturbation strategy $p$, and a number of perturbed features $m$, we track the prediction changes through the perturbation curve, which is obtained from interpolating the vector:
\begin{equation}
    \label{eq:pc}
    \text{PC}_{\mathbf{x}}(c,p,m) =  [q_{cp1}, \dots, q_{cpm}]\,,
\end{equation}
where $q_{cpi}$ represents the predicted probability of class $c$ after perturbing $i$ features. If all time points are perturbed, then $m = N$. Without loss of generality, we denote the vectors $\text{PC}_{\mathbf{x}}(c,p,m)$ describing the perturbation curves in the MoRF and LeRF order as $\text{PC}_{\text{MoRF}}$ and $\text{PC}_{\text{LeRF}}$, respectively. Then, the DS measures the sum of the signed difference between the LeRF and MoRF perturbation curves at each perturbation step:
\begin{equation}
    \label{eq:ds}
    \text{DS} = \frac{1}{m} \sum_i^m (\text{PC}_{\text{LeRF}_i} - \text{PC}_{\text{MoRF}_i})\,.
\end{equation}
The DS ranges from -1 to 1, where positive values suggest correct feature attributions (MoRF perturbations impact predictions more than LeRF), zero indicates non-discriminative attributions (equal impact regardless of perturbation order), and negative values suggest flawed attributions (LeRF perturbations have a stronger impact than MoRF).

To evaluate attribution methods for multiple instances, we usually compute the mean degradation score $\overline{\text{DS}}$. However, this aggregate metric can mask important class-specific behaviors. Therefore, we extend this evaluation framework by introducing the class-adjusted degradation score $\text{DS}_c$ that balances overall attribution accuracy with consistent performance across classes. This metric rewards attribution methods that achieve both high aggregate performance and uniform behavior across different classes. Formally, we define:
\begin{equation}
\label{eq:ds_c}
\text{DS}_c(\alpha) = \overline{\text{DS}} - \alpha \cdot \Delta\,,
\end{equation}
where $\alpha$ is a parameter in $[0,1]$ that controls the penalty strength. 
The penalty term $\Delta$ quantifies the performance differences between classes. For example, for the trivial case of binary classification, it measures the absolute difference in attribution performance between the two classes:
\begin{equation}
\label{eq:penalty_bin}
\Delta = \frac{1}{2} |\overline{\text{DS}}_1 - \overline{\text{DS}}_0|\,.
\end{equation}
For multi-class problems, we extend this concept by computing the mean absolute difference across all possible class pairs:
\begin{equation}
\label{eq:penalty_multi}
\Delta = \frac{1}{2} \frac{1}{\binom{C}{2}} \sum_{i<j} |\overline{\text{DS}}_i - \overline{\text{DS}}_j| = \frac{1}{C(C-1)} \sum_{i<j} |\overline{\text{DS}}_i - \overline{\text{DS}}_j|\,,
\end{equation}
where $i,j$ are class indices and $C$ is the number of classes.
Since the maximum possible value of the mean absolute difference in all pairs is 2, we introduce a factor $\frac{1}{2}$ that normalizes the penalty to the interval [0,1]. This ensures that the class-adjusted degradation score $\text{DS}_c$ remains on the [-1,1] scale, conserving its interpretation. Setting $\alpha=1$ assigns equal importance to overall attribution correctness ($\overline{\text{DS}}$) and consistency between classes ($\Delta$), ensuring that both aspects are weighted equally in the final evaluation metric.
For example, in a binary classification task where $\overline{\text{DS}}_1=1$ and $\overline{\text{DS}}_0=0$, then $\overline{\text{DS}}=0.5$, which can be interpreted as a moderately good degradation score value. However, the MoRF and LeRF curves behave perfectly for all instances of class 1, while for class 0, no differences are observed in the curves on average. Assuming $\alpha=1$, the penalty $\Delta=0.5$ allows us to account for this difference in class behavior, resulting in a more strict class-adjusted degradation score $\text{DS}_c=0$.

Although we apply this penalty framework to the mean DS, it generalizes to any evaluation metric. This allows a systematic investigation of class-dependent effects in attribution evaluation by comparing base metrics against their class-adjusted variants while requiring minimal additional computation, as it utilizes class information already collected during the standard perturbation process. We use this framework to analyze how perturbation-based evaluation methods can exhibit different behaviors across classes.


\section{Experiment Set-up}\label{h:experiment-set-up}

We design our experiments to systematically investigate the effectiveness of attribution methods for time series data, with a particular focus on class-specific differences in perturbation behavior. Our investigation addresses three key aspects: (1) the general effectiveness of attribution methods across architectures and datasets, (2) the presence and characteristics of class-dependent perturbation effects, and (3) the relationship between perturbation strategy selection and these effects.

We use four univariate time series datasets from the UCR Time Series Classification Archive~\cite{dau.etal_2019_ucr}: \textit{FordA}, \textit{FordB}, \textit{ElectricDevices} (ElecDev) and \textit{Wafer}. Table~\ref{tab:dataset_performance} presents the characteristics of these datasets along with the achieved accuracy of the classifiers used in this study.
For visualizations of representative samples and detailed class distributions, see Appendix~\ref{appendix:datasets}.
These datasets are among the largest in the UCR Archive and are commonly used to evaluate feature attributions with perturbations~\cite{schlegel.etal_2019_rigorous,simic.etal_2022_perturbation,mercier.etal_2022_time,schlegel.keim_2023_deep,schlegel.keim_2023_introducing,turbe.etal_2023_evaluation,mercier.etal_2022_time}, making our results comparable to previous work.

\input{tab_dataset_performance} 

For model training, we select two distinct and widely-adopted deep learning architectures: ResNet~\cite{wang.etal_2017_time} and InceptionTime~\cite{ismailfawaz.etal_2020_inceptiontime} for their strong performance baselines~\cite{fawaz.etal_2019_deep} and different feature extraction approaches.
Following the predefined UCR splits, we train each model with a batch size of 256 using the AdamW optimizer with cosine annealing learning rate scheduling for up to 500 epochs, implementing early stopping with patience of 25 epochs to ensure stable model convergence. The validation set for early stopping consists of 20\% of randomly selected observations from the train set, stratified by the class label.
As shown in Table~\ref{tab:dataset_performance}, both architectures achieve performance comparable to previous perturbation studies~\cite{schlegel.keim_2023_deep,simic.etal_2022_perturbation} and approach the performance of state-of-the-art deep learning models~\cite{fawaz.etal_2019_deep}. 
Although we observe some performance disparity between training and test sets, indicating potential model capacity for further optimization through hyperparameter tuning, the achieved performance levels are sufficient for our objective of evaluating feature attributions.

We evaluate five widely adopted attribution methods.\footnote{We implement all attribution methods using the TSInterpret package~\cite{hollig.etal_2023_tsinterpret}.} These include four gradient-based methods: Gradients (GR)~\cite{simonyan.etal_2014_deep}, Integrated Gradients (IG)~\cite{sundararajan.etal_2017_axiomatic}, SmoothGrad (SG)~\cite{smilkov.etal_2017_smoothgrad}, and Gradient SHAP (GS)~\cite{lundberg.lee_2017_unified}. We also include a method based on perturbation, Feature Occlusion (FO)~\cite{fong.vedaldi_2017_interpretable}.
While gradient-based methods compute feature importance by analyzing how changes in inputs affect model outputs through gradient calculations, perturbation-based methods systematically modify input features and observe the resulting changes in predictions.
To ensure balanced class representation while maintaining computational feasibility, we compute attributions on a stratified sample of 300 instances per class from the test set.
Attributions always explain the predicted class label.

Our experimental design incorporates six established perturbation strategies from previous work~\cite{simic.etal_2022_perturbation,schlegel.keim_2023_deep} and extends them with a systematic framework of constant-value perturbations. The details of the different strategies are described in Table~\ref{tab:perturbation_strategies}. Although previous studies primarily focus on mean and zero value substitution among other more sophisticated strategies, we also evaluate a comprehensive grid of constant perturbation values ranging between $-2$ and $2$. This extension provides an interpretable baseline for understanding class-dependent perturbation effects, especially given the normalization of the UCR datasets to zero mean and unit standard deviation.

\input{tab_perturbation_strategies}

We apply these perturbation strategies systematically following the MoRF and LeRF orders, as determined by the feature attributions. For both orders, features are perturbed incrementally in steps of 2\% of the time series length (rounded up) until 50\% perturbation coverage is reached, recording the predicted probabilities at each perturbation step. This bounded perturbation approach ensures computational efficiency by excluding the latter half of features, which typically exhibit minimal discriminative power. 

To analyze both overall attribution quality and potential class-dependent effects, we employ two evaluation approaches. First, we compute the DS (Equation~\ref{eq:ds}) from these perturbation curves, providing a normalized measure between -1 and 1 for each sample that enables consistent comparison across datasets with varying time series lengths and perturbation step sizes. We establish overall performance by averaging DS metrics across all experimental conditions, allowing comparison with previous perturbation studies. To capture class-dependent effects, we extend this analysis using class-adjusted penalties (Equations~\ref{eq:penalty_bin} and~\ref{eq:penalty_multi}) to calculate the class-adjusted metric, $\text{DS}_c$ (Equation~\ref{eq:ds_c}) with $\alpha=1$.


\section{Results and Discussion}\label{h:results-discussion}

\subsection{Evaluating Attribution Quality}

To establish baseline performance and enable comparison with previous work, we begin by evaluating the overall effectiveness of different attribution methods and perturbation strategies. Table~\ref{tab:avg_ds} presents the mean DS metrics aggregated across all experimental conditions. The observed DS ranges align with previous findings by Šimić et al.~\cite{simic.etal_2022_perturbation}, suggesting consistent behavior in different experimental settings.

\input{tab_ds}

Examining these results in detail, we find that the perturbation strategies exhibit varying levels of performance, with notable dependencies on model architectures and datasets. Zero and SubMean perturbations often achieve the highest DS scores across the different experimental configurations, though Gauss occasionally outperforms them in specific contexts. Importantly, optimal perturbation selection appears highly contingent on model architecture, even when controlling for the dataset. For instance, on the FordB and Wafer datasets, Gauss perturbations perform relatively well for InceptionTime on several attribution methods, but for ResNet, SubMean or Zero yield better results.

We also observe substantial performance variations among the attribution methods. FO, IG and GS tend to achieve the highest DS values across datasets and perturbation strategies, indicating better feature importance identification. While FO frequently demonstrates marginally superior performance compared to IG and GS, this advantage is not consistent across all experimental settings. In contrast, GR shows poor discriminative ability, frequently yielding negative or near-zero DS values, suggesting that its feature importance assignments are often not better than random ordering. Between these extremes, GS typically outperforms GR but falls short of the effectiveness demonstrated by IG, GS and FO, depending on the experimental condition.

Although these aggregate metrics provide valuable insights into overall method effectiveness, they potentially mask important variations in performance distributions. To better understand these underlying patterns, we examine the distribution of DS metrics through a more detailed case study.

\subsection{Distribution Patterns in Attribution Quality}

\input{fig_ds_distributions}

To understand how attribution performance varies between individual instances, we analyze the distributional characteristics of DS scores. For this analysis, we focus on the FordB dataset, InceptionTime architecture, and SubMean perturbation strategy, as these demonstrate patterns typical of our broader findings. Figure~\ref{fig:ds_distributions} presents these distributions, revealing both overall performance patterns and, crucially, class-specific effects.

The aggregate distributions shown in Figure~\ref{fig:ds_distributions_a} reveal notable variation in the effectiveness of attribution methods. IG, GS, and FO exhibit positively skewed distributions with extended tails toward higher DS values, indicating better attribution quality. In contrast, GR and SG demonstrate negative skewness with tails extending toward lower DS values, suggesting less reliable feature identification. Critically, we observe a concentration of scores around zero, implying limited discriminative power in feature ordering for many instances, even among the better-performing methods.

Figure~\ref{fig:ds_distributions_b} presents a class-stratified analysis that uncovers substantial heterogeneity in attribution quality across classes.  For IG, GS, and FO, instances from class 1 consistently achieve higher DS scores with pronounced positive tails and minimal negative values. The same methods show markedly different behavior for class 0, where DS scores cluster tightly around zero, indicating minimal perturbation impact. GR and SG show an inverse pattern, with class 1 showing predominantly negative scores while class 0 maintains a more balanced distribution centered near zero.

These pronounced class-dependent variations in attribution performance raise questions about the generalizability of perturbation-based evaluation methods.  To determine whether this phenomenon extends beyond our case study, we now examine class-dependent behavior across all experimental conditions.


\subsection{Class-Dependent Effects in Perturbation Analysis}

We apply the proposed class-adjusted DS ($\text{DS}_c$) metric to all experimental conditions to investigate class-dependent behaviors. Table~\ref{tab:adjusted_ds} presents these adjusted scores, which balance average attribution correctness with consistency between classes. Our results show that incorporating class consistency penalties substantially reduces performance metrics across most experimental conditions, particularly for previously high-performing perturbation strategies.

\input{tab_adj_ds}

Zero, SubMean and Gauss perturbations, which demonstrated superior performance before, show marked degradation under class-adjusted evaluation, with most DS scores shifting toward zero or even becoming negative.
The extent of this impact differs among experimental conditions, particularly across various datasets. Wafer and ElecDev generally show more resilience to class adjustment compared to FordA and FordB, though we still see reduced scores. For ElecDev, the interpretation of these results requires additional context: as the only multiclass dataset in our evaluation, the pairwise averaging of class differences may underestimate class-specific effects due to the higher dimensionality of the classification space.

To examine the mechanisms underlying these class-specific effects, Figure~\ref{fig:perturbations_by_class} presents a detailed analysis of perturbation impacts across classes on the FordB dataset. Figure~\ref{fig:perturbations_by_class_a}, which focuses on named perturbation strategies with InceptionTime and FO attribution, reveals systematic class-dependent behavior. 
Class 0 instances exhibit minimal response to perturbation across all strategies, with DS scores tightly clustered around zero, while Class 1 instances demonstrate substantial variability in perturbation response. For Class 1, Gauss is the most effective, followed by Zero and SubMean strategies.
This asymmetric response pattern may explain the earlier observed degradation in class-adjusted metrics: perturbation strategies succeed primarily by exploiting class-specific model behaviors.

Analyzing the constant-value perturbations in Figure~\ref{fig:perturbations_by_class_b} reinforces these findings while providing additional information. The evaluation of perturbation values between -2 and 2 reveals optimal effectiveness at moderate positive values, particularly around 0.5. We now extend the investigation of the constant perturbation strategies to all experimental conditions.

\input{fig_perturbation_strategies}


\subsection{Evaluating Constant Perturbation Values}

Table~\ref{tab:adjusted_ds_constant} shows the class-adjusted DS for the constant perturbation strategies, revealing significant domain-specific variations. The effectiveness of constant perturbations varies significantly by dataset and classifier architecture. FordA and FordB often show the best performance with 0.5 perturbation. Wafer demonstrates inconsistent responses, where optimal perturbation constants vary across attribution methods. ElecDev exhibits the best performance with a perturbation constant of 0.5 for ResNet, and different constants between -0.5 and 0.5 for InceptionTime, though its multiclass nature introduces additional complexity in interpreting class-adjusted metrics. 

Interestingly, negative perturbation values generally yield negative class-adjusted scores for most datasets, indicating asymmetric class responses to the perturbation direction. However, this effect varies by attribution method, classifier, and dataset. For example, GR, SG and GS on ElecDev exclusively show positive scores for negative perturbation constants. These results suggest that the effectiveness of perturbation-based evaluation depends on the alignment between perturbation values and the learned class representations in the feature space.

\input{tab_constant_adj_ds}


\subsection{Synthesis and Implications}

Our investigation reveals three key findings on perturbation-based evaluation of feature attributions. First, the effectiveness of perturbation strategies can vary substantially between classes, with performance often showing asymmetric impacts. This pattern appears particularly in binary classification tasks, where perturbation strategies may validate attributions effectively for one class while showing limited sensitivity to the other.

Second, we observe that perturbation strategies that show strong aggregate performance often demonstrate the most pronounced class-dependent effects. When examining these strategies using our class-adjusted framework, we find that much of their effectiveness stems from strong performance in specific classes rather than consistent behavior across all classes. This observation suggests the importance of examining class-specific responses when evaluating perturbation strategies.

Third, our analysis of constant perturbation strategies shows that optimal perturbation values can vary across datasets and architectures, indicating that perturbation effectiveness may be influenced by the specific characteristics of learned model representations. This finding suggests that the choice of perturbation values warrants careful consideration for each specific application context.


\section{Conclusion and Future Work}\label{h:conclusion}

This paper presents a systematic investigation of class-dependent effects in perturbation-based evaluation of feature attributions for time series classification. Through empirical evaluation across four datasets, five attribution methods, and multiple perturbation strategies, we demonstrate that perturbation-based evaluation methods can exhibit class-specific behaviors that warrant careful consideration in validation procedures.
Namely, we show that: (1) perturbation effectiveness can vary substantially between classes; (2) strategies with stronger aggregate performance often exhibit more pronounced class-dependent effects; and (3) optimal perturbation strategies can vary considerably across datasets and model architectures, suggesting that perturbation effectiveness is influenced by specific characteristics of learned model representations and that domain-specific calibration may be necessary.

We recommend several evaluation approaches to address potential class-dependent effects. First, supplementing aggregate metrics with more detailed class-stratified analysis can help identify when dominant classes disproportionately influence results. Second, applying penalty frameworks like the one proposed in this study quantifies class-dependent effects without additional computational burden, allowing researchers to systematically compare attribution methods while accounting for class biases. Third, evaluating individual instances with multiple perturbation strategies targeting different class predictions may help validate attribution robustness beyond the limitations of any single approach.

The generalizability of our findings faces two primary constraints. Our investigation encompasses only four datasets and two classifier architectures, potentially limiting broader applicability. Additionally, the observed perturbation response patterns may reflect specific characteristics of our experimental configuration rather than fundamental properties of the evaluation approach. Nevertheless, the alignment with previous literature suggests wider relevance of our methodology.

Future research directions emerge from these findings. One promising avenue involves developing perturbation strategies that systematically account for class-specific model behaviors, particularly focusing on methods that can push predictions toward different classes. Additionally, investigating methods to adaptively select perturbation strategies for individual instances could improve evaluation effectiveness, as our results suggest that different instances may require different perturbation approaches to effectively validate their attributions.

\appendix

\section{Dataset Visualizations and Class Distributions} \label{appendix:datasets} 
\input{tab_class_distributions}
\input{fig_data_samples_binary}
\input{fig_data_samples_multiclass}
\clearpage

\begin{credits}
\subsubsection{\ackname} This paper is supported by the European Union’s HORIZON Research and Innovation Program under grant agreement No. 101120657, project ENFIELD (European Lighthouse to Manifest Trustworthy and Green AI). 

\subsubsection{\discintname}
All authors declare that they have no conflicts of interest.
\end{credits}

\bibliographystyle{splncs04}
\bibliography{references}

\end{document}