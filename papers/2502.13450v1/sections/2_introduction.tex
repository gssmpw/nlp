\section{Introduction}
Autoregressive models have been highly successful at modeling languages in a token by token fashion. While finetuned autoregressive (AR) models can produce realistic texts and maintain lengthy human-like conversations, they are known to fail at simple planning and reasoning tasks. One hypothesis is that AR generation is not suited for generating tokens where non-trivial constraints have to be satisfied. There have been efforts such as Chain-of-Thought prompting \cite{wei2022chain} and O1 \cite{o1model} which force the model to ``think over'' the solution in many steps before answering.

Diffusion models, another class of generative models, start with pure noise and slowly denoise to obtain a sample from the desired distribution \cite{ho2020denoising,song2020score}. While its outstanding applications have been in the context of generating images (i.e, continuous data) \cite{saharia2022photorealistic,rombach2022high}, it has been successfully extended to discrete data \cite{austin2021structured,lou2023discrete}. This model has shown promising results in planning and constrained generation in a wide range of tasks, such as layout generation, molecule generation, 3SAT, SuDoKu \cite{ye2024autoregressiondiscretediffusioncomplex} and traveling salesman problem \cite{zhang2024symmetricdiffusers}, outperforming AR models. This is attributed to diffusion models being able to parse the entire set of generated tokens multiple times during denoising. 

Algorithms based on D3PM \cite{austin2021structured} as presented in prior works \cite{inoue2023layoutdm,ye2024autoregressiondiscretediffusioncomplex} and mixed mode diffusion based works such as \cite{hua2024mudiff} assume that the denoising process samples from a product distribution of the tokens, which seems unreasonable in cases of constrained generation where the tokens can be highly dependent. It would be desirable if partial denoising of a token (continuous or discrete) is dependent on current denoised status of all other tokens. Alternative proposals such as Concrete Score Matching \cite{meng2022concrete}, SEDD \cite{lou2023discrete}, symmetric diffusion (\cite{zhang2024symmetricdiffusers}) and Glauber Generative Model (GGM) \cite{varma2024glauber} do not assume such a factorization. Symmetric diffusion considers the special case of generating permutations using riffle shuffle as the noising process and derives algorithms for denoising it exactly. This demonstrates gains in a variety of planning problems. GGM is a discrete diffusion model which denoises a lazy random walk exactly by learning to solve a class of binary classification problems.

Gibbs Sampler is a Markov chain which samples jointly distributed random variables by resampling one co-ordinate at a time from the accurate conditional distribution. This has been studied widely in Theoretical Computer Science, Statistical Physics, Bayesian Inference and Probability Theory \cite{geman1984stochastic,turchin1971computation,gelfand1990sampling,martinelli1999lectures,levin2017markov}. While the original form gives a Markov Chain Monte Carlo (MCMC) algorithm, \cite{varma2024glauber} considered a learned, time dependent, systematic scan variant of the Gibbs sampler for generative modeling over discrete spaces. 

In this work, we extend the principle of time dependent Gibbs sampler to mixed mode data - sequences with both discrete tokens and continuous vectors.
Such problems arise naturally in applications like Layout Generation \cite{levi2023dlt} and Molecule Generation \cite{hua2024mudiff}.



\paragraph{Our Contributions:}
We introduce an effective method to train a diffusion based model to solve planning problems and constrained generation problems where the sequence being generated could involve both discrete and continuous tokens. The key contributions include:
\begin{enumerate}
\item The Interleaved Gibbs Diffusion (IGD) framework for \textbf{sampling from mixed distributions} (mix of continuous and discrete variables), by performing Gibbs sampling type denoising, one element at a time. This does \textit{not assume factorizability} of the denoising process.




\item Theoretical justification for the proposed denoising process and a \textbf{novel adaptation} of Tweedie's formula to the IGD setting where we require learn conditional score function by estimating the the cumulative noise over multiple round robins despite the conditioning chain during the process. 

\item A framework for \textbf{conditional sampling} when some elements are fixed via state space doubling inspired by DLT \cite{levi2023dlt} and an \textbf{inference time} algorithm called \redenoise~inspired by SDEdit \cite{meng2021sdedit}. \redenoise~can potentially boost the accuracy of generation at the cost of additional compute.
\item \textbf{State-of-the-art performance} in constrained generation problems such as 3-SAT, molecule generation and layout generation. In molecule generation and layout generation, we outperform existing discrete-continuous frameworks and achieve SoTA results without relying on specialized diffusion processes or domain-specific architectures. In 3-SAT, we outperform the SoTA diffusion model out of the box and study how accuracy improves with the model size and dataset size.

\end{enumerate}

\textbf{Organization:}
  Preliminaries are given in Section~\ref{sec:prelim}. The IGD framework, with continuous and discrete denoisers as black boxes, is described in Section~\ref{sec:igd}. The inference time scaling \redenoise~algorithm is given in Section~\ref{sec:redenoise} and the conditional sampling algorithm is given in Section~\ref{sec:cond_generation}. Multiple recipes to design and train the black box denoisers are given in Section~\ref{sec:training}. Experimental results are presented in Section~\ref{sec:experiments}, Conclusion and Future Work in Section~\ref{sec:conclusion}.
