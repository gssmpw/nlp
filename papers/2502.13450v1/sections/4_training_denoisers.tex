\section{Training the Denoisers}
\label{sec:training}



Having established the IGD framework, we now describe strategies to train the discrete and continuous denoisers, which have been black boxes in our discussion so far.  

\subsection{Training the Discrete Denoiser}

 Throughout this subsection, we use $g_{\theta}$ to denote a parameterised neural network which is trained to be the discrete denoiser. $g_{\theta}$ takes input from the space $\mathcal{S}_{L} \times \{0, 1, \dots, T-1 \}$ and outputs logits in the space $[0, 1]^{\abs{\mathcal{X}}}$.  We now describe two strategies to train $g_{\theta}$:

\subsubsection{$\abs{\mathcal{X}}$-ary classification}
In this approach, the objective is to learn $\mathbb{P}\left({s}^{(t)}_{i_t} = \cdot | {s}^{(t+1)} = \hat{s}^{(t)} \right)$. So, we directly train the model to predict ${s}^{(t)}_{i_t}$ given ${s}^{(t+1)}$. Since there are $\abs{\mathcal{X}}$ discrete tokens in the vocabulary, this is a $\abs{\mathcal{X}}$-ary classification problem, where the input is ${s}^{(t+1)}$ and the corresponding label is ${s}^{(t)}_{i_t}$. Hence, we minimize the cross-entropy loss: $ \mathcal{L}_{CE}\left(\theta; {s}^{(t+1)}, t \right) = -\log \left( g_{\theta} ^{s_{i_t}} \left({s}^{(t+1)}, t \right) \right)  $
where $g^{s_{i_t}}_{\theta}(\cdot) $ denotes the logit corresponding to token ${s}^{(t)}_{i_t}$.

\subsubsection{Binary classification}
\label{subsec:model_train_binary}
In this approach, the objective is to learn $\mathbb{P}\left({s}^{(t)}_{i_t} = \cdot | {s}^{(t+1)}_{-i_t} = \hat{s}^{(t)}_{-i_t} \right)$. We adapt Lemma 3.1 from \cite{varma2024glauber} to simplify this objective:

\begin{lemma}
Let $s \in \mathcal{S}^L$. Then, for $x \in \mathcal{X}$ and discrete ${s}^{(t)}_{i_t}$, we can write $\mathbb{P}\left({s}^{(t)}_{i_t} = x | {s}^{(t+1)}_{-i_t} = {s}_{-i_t} \right)$ as :
\small
\begin{equation*}
     \frac{\mathbb{P}(z_t = x)}{\mathbb{P}(z_t = \phi)} \left( \frac{1}{\mathbb{P}\left({z_t} = x | {s}^{(t+1)}_{-i_t} = {s}_{-i_t}, {s}^{(t+1)}_{i_t} = x \right)}  - 1 \right)
\end{equation*}
\normalsize
 where $\left(s^{(0)},  \dots s^{(T)}, \right)$ is obtained from forward process.
\end{lemma}

Hence, it is sufficient for the model to learn $\mathbb{P}\left({z_t} = x | {s}^{(t+1)}_{-i_t} = {s}_{-i_t}, {s}^{(t+1)}_{i_t} = x \right)$ for all $x \in \mathcal{X}$. This can be formulated as a binary classification task: Given ${s}^{(t+1)}_{-i_t}$ and ${s}^{(t+1)}_{i_t} = x$ as the input, predict whether $z_t = x$ or $z_t = \phi$. Hence, we minimize the binary cross-entropy loss: $
    \mathcal{L}_{BCE}\left(\theta; {s}^{(t+1)}_{-i_t}, t \right) = -  \mathbf{1}_{z_t \neq \phi}  \log \left( g_{\theta} ^{x} \left({s}^{(t+1)}_{-i_t}, t \right) \right) 
    - \mathbf{1}_{z_t = \phi}  \log \left(1 - g_{\theta} ^{x} \left({s}^{(t+1)}_{-i_t}, t \right) \right)  $
\normalsize
where $g^{x}_{\theta}(\cdot) $ denotes the logit corresponding to token $x$.

Preliminary experiments (Appendix \ref{app:par_xary_binary}) gave better results with the binary classification loss; hence we use binary classification for training the discrete denoiser.


\subsection{Training the Continuous Denoiser}
\label{subsec:con_den}

In continuous diffusion, the noising (and denoising) process follows an SDE; the entire process happens in an uninterrupted fashion. However, in IGD, the noising and denoising happen with interruptions, because of the sequential nature. Thus, in the reverse process, the conditioning surrounding a continuous element changes every time it is picked for denoising. By an adaptation of the standard Tweedie's formula and exploiting the fact that forward noising process for every element is independent of other elements, we show that using the current conditioning and estimating the cumulative noise added from the beginning (across interruptions) still reverses the continuous elements in an interleaved manner. This is the novelty behind Lemma \ref{lemma:score_noise}.


Suppose we are given a sample $s^{(t)}$ from the distribution at time $t$. Let $\stackrel{d}{=}$ denote equality in distribution. Suppose $x_0 = s_{i_t}^{(t)} \in \mathbb{R}^{d_{i_t}}$ and consider the Orstein-Uhlenbeck Process $dx_\tau = -x_\tau d\tau + \sqrt{2}dB_\tau$ with standard Brownian motion $B_\tau$. Then $x_{\tau_0}|s^{(t)} \stackrel{d}{=} s_{i_t}^{(t+1)}|s^{(t)}$ whenever $\tau_0 = \frac{1}{2}\log(\tfrac{1}{1-\beta_{m_{i_t}^t}})$. Based on the observations in \cite{song2020score,ho2020denoising}, the reverse SDE given by 
\begin{equation}\label{eq:rev_sde}
    x_\tau^{\mathsf{rev}} = x_\tau^{\mathsf{rev}}d\tau + 2 \nabla \log q_{\tau_0-\tau}(x^{\rev}_{\tau}|s_{-i_t}^{(t)})\tau + \sqrt{2}dB_\tau
\end{equation} is such that if $x^{\rev}_0 = s_{i_t}^{(t+1)}$ then $x^{\rev}_{\tau_0}|s^{(t+1)} \stackrel{d}{=} s_{i_t}^{(t)}|s^{(t+1)}$ where
$q_{\tau}(\cdot|s^{(t+1)})$ is the conditional density function of $x_{\tau}$. We use DDPM \cite{ho2020denoising} to sample from $\mathbb{P}(s_{i_t}^{(t)} = \cdot|s^{(t+1)})$ by learning the score function $\nabla \log q_{\tau}(\cdot|s_{-i_t}^{(t+1)})$ and then discretizing the reverse SDE in Equation~\eqref{eq:rev_sde}.

To obtain a more precise discretization, we divide the noising at sequence timestep $t$ into $K_{i_t}^t$ \textit{element timesteps} (whenever $s_{i_t}$ is a continuous vector). We define $s_{i_t}^{(t, 0)} = s_{i_t}^{(t)}$, $s_{i_t}^{(t, K_{i_t}^{t})} = s_{i_t}^{(t+1)}$, and for $k \in [0, 1, \dots, K_{i_t}^{t}-1]$:
\small
\begin{align*}
    s^{(t, k+1)}_{i_t} \sim \mathcal{N}\left(\left(\sqrt{1 - {\beta}(t, k)}\right) s^{(t, k)}_{i_t}, \left({{\beta}(t, k)}\right) \mathbf{I}  \right)
\end{align*}
\normalsize
where ${\beta}$ is a continuous noise schedule which outputs a scalar given $(t, k)$ as input. Following the popular DDPM \cite{ho2020denoising} framework, we rewrite the noising process as:
\small
\begin{align}\label{eq:diff_relation}
    s^{(t, k+1)}_{i_t} = \left(\sqrt{\bar{\alpha}(t, k)}\right) s^{(0)}_{i_t} + \left(\sqrt{1 - \bar{\alpha}(t, k)}\right) \epsilon
\end{align}
\normalsize
where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ and $\bar{\alpha}$ is a cumulative noise schedule obtained from $\hat{\beta}$. The exact relations between $\tilde{\beta}$ (defined in \ref{subsec:fnp}), ${\beta}$ and $\bar{\alpha}$ are given in Appendix \ref{app:sec:beta_connection}. With this discretization, the reverse process becomes:
\small
\begin{align*}
    \hat{s}^{(t, k)}_{i_t} = \frac{\left(\hat{s}^{(t, k+1)}_{i_t} - {\beta}(t, k+1)p({s}^{(t, k+1)}) \right)}{\sqrt{1 - {\beta}(t, k+1)}} 
    + \sqrt{{\beta}(t, k+1)} \epsilon'
\end{align*}
\normalsize
where $\epsilon' \in \mathcal{N}(0, \mathbf{I})$ and  $p({s}^{(t, k+1)})$ is the  score function $\nabla_{{s}_{i_t}^{(t, k+1)}} \log q({s}_{i_t}^{(t, k+1)} | {s}_{-i_t}^{(t, k+1)} )$. Now to learn the score function, we use the following Lemma:
\begin{lemma}
Under the considered forward process where noising occurs independently, we have:
 \begin{align*}
     \nabla_{{s}_{i_t}^{(t, k+1)}} \log q({s}_{i_t}^{(t, k+1)} | {s}_{-i_t}^{(t, k+1)} )  = -\frac{1}{\sqrt{1-\bar{\alpha}}} \mathbb{E} \left[ \epsilon | {s}^{(t,k+1)} \right]
 \end{align*}
\label{lemma:score_noise}
\end{lemma}
Hence, if we can learn $\mathbb{E} \left[ \epsilon | {s}^{(t,k+1)} \right]$ exactly, the forward process can be reversed exactly assuming you start from the stationary distribution. Hence, we minimize the regression loss: $\norm{\epsilon - g\left({s}^{(t, k+1)}, t, k \right)}_2^2$
where $g(\cdot)$ is a neural network which is trained to predict  $\epsilon $ given $\left(\hat{s}^{(t, k+1)}, t, k \right)$. 

Apart from DDPM sampling, we also evaluated DDIM, which is an ODE based method. However, preliminary results (reported in Appendix \ref{app:par:ddpm_ddim}) indicated that DDPM performs better. A detailed description of the exact training and inference algorithms we use is given in Appendix \ref{app:sec:model_train_pseudo}.
