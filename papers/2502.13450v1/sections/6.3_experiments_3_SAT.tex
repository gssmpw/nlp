\subsection{Boolean Satisfiability Problem}

\subsubsection{Background} The Boolean Satisfiability (SAT) problem is the task of determining whether there exists a binary assignment to the variables of a given Boolean expression (in Conjunctive Normal Form (CNF)) that makes it evaluate to \textit{True}. SAT is a canonical NP-Complete problem~\cite{cook1971complexity} and underlies a broad range of real-world applications in formal hardware/software verification, resource scheduling, and other constraint satisfaction tasks~\cite{clarke2001modelchecking, gomes2008satisfiability, vizel2015satmodelchecking}.

Our goal is to find a valid assignment for the Boolean variables, when the given CNF formula is satisfiable. Let $n$ be the number of variables and $m$ the number of clauses. In Random $k$-SAT, a well-studied variation of SAT, the relative difficulty of an instance is determined by the clause density $\frac{m}{n}$. There is a sharp transition between satisfiable and unsatisfiable instances of random 3-SAT at the critical clause density $\alpha_{\mathrm{sat}}(k=3)$, when m is set close to $m=4.258n + 58.26 n^{-\frac{2}{3}}$ \cite{ding2015satisfiabilityconjecture}. Following the setup of \cite{ye2024autoregressiondiscretediffusioncomplex}, we choose $m$ close to this threshold to focus on relatively hard random 3-SAT instances.

\subsubsection{Experimental Setup}
\textbf{Datasets:}
We consider two experimental setups:
% the first to compare with prior works and the second for exploration of this problem.

\underline{Setup 1} (Single $n$): We follow the train and test partitions from \cite{ye2024autoregressiondiscretediffusioncomplex}, which provide separate datasets for $n \!= 5, 7,$ and $9$, for direct comparison. Specifically, $n=5$ and $n=7$, use a training set of 50K samples each, while for $n=9$, the training set consists of 100K samples.

\underline{Setup 2} (Combined $n$): We then move to a more challenging, large-scale setting by extending the range of $n$ up to 20. Following the same generation procedure, for each $n$ in ${6,7,\dots,20}$, we generate 1M training samples, resulting in a combined dataset of 15M instances. In this setup, we train a single model on the aggregated data covering all $n$ from 6 to 20. Figure \ref{fig:sat_n20_accuracy_plot} illustrates how the model's accuracy varies with $n$ under different model sizes.

More details on data generation and model configuration are provided in Appendix \ref{app:3sat}.

\textbf{Baselines:}
We compare against two types of baseline models: 1) Autoregressive Models with a GPT-2 architecture \cite{radford2019language} trained from scratch and 2) Discrete diffusion models in \cite{ye2024autoregressiondiscretediffusioncomplex} (MDM) that applies adaptive sequence- and token-level reweighting to emphasize difficult subgoals in planning and reasoning. MDM has demonstrated strong performance on tasks such as Sudoku and Boolean Satisfiability compared to standard autoregressive and discrete diffusion approaches.


\input{tables/3sat_results}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{images/sat_n20_accuracy_plot.pdf}
  \caption{\textbf{SAT:} Accuracy for different number of variables across model sizes trained on a combined dataset for n in $6,7\dots,20$.}
  \label{fig:sat_n20_accuracy_plot}
\end{figure}

\subsubsection{Results}
In Table~\ref{tab:sat_n_5_7_9_accuracy}, we see that our method consistently outperforms the autoregressive (GPT-2) and diffusion-based (MDM) baselines across different choices for $n$. This performance gap is more pronounced for larger $n$: at $n=9$, our model achieves 94.5\% accuracy, compared to 87.0\% for MDM and 73.3\% for GPT-2. Scaling the model to 85M parameters further reaches near-perfect accuracy (99.9\%) for $n=7$ and $n=9$, thus highlighting the crucial role of model capacity in handling complex SAT instances.

For \emph{Setup 2}, Figure \ref{fig:sat_n20_accuracy_plot} reveals a steep accuracy drop for the 6M-parameter model; it starts declining around $n = 8$ and approaches zero by $n = 12$. In contrast, the 85M-parameter model remains robust up to $n = 18$, and an even larger 185M-parameter model sustains high accuracy near $n = 19$. This degradation trend aligns with the theoretical hardness of random 3-SAT, where solution spaces become exponentially sparse as $n$ increases. Larger models postpone this accuracy drop underscoring a direct relationship between parameter count and combinatorial reasoning capacity.