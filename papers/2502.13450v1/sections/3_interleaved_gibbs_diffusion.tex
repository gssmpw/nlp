\section{Interleaved Gibbs Diffusion}
\label{sec:igd}
We now describe the Interleaved Gibbs Diffusion (IGD) framework for sampling from a target distribtuion $\pi$ over $ \mathcal{S}_L$, given access to discrete and continuous denoisers which satisfy certain properties. We first describe the forward noising process and then the reverse denoising process using the given denoisers. In IGD, both the forward noising and reverse denoising processes operate one element at a time. 
Our noising process is illustrated in Figure \ref{fig:noising_process}.

\begin{figure}
    %\centering
    \includegraphics[width = 0.5 \textwidth]{images/figure_noising_process.pdf}
    \caption{\textbf{Interleaved Noising Process: } Sequential noising of discrete tokens ($D_s$) and continuous vectors ($C_s$). Noising occurs one element at a time, keeping other elements unchanged.}
    \label{fig:noising_process}
\end{figure}

\subsection{Forward Noising Process} \label{subsec:fnp}
The forward noising process takes a sample $s$ from the target distribution $\pi$ and applies a discrete time Markov chain to obtain the trajectory $s^{(0)},s^{(1)},\dots,s^{(T)}$, where $T$ is the total number of timesteps. We refer to $t$ as the \textit{sequence time}. Note that $s^{(0)} = s$. For each $t$, we choose a position $i_t \in \{1, 2, \dots, L \}$ to be noised at sequence time $t$. In this work, we choose $i_t$ in a round-robin fashion from some permutation of $\{1, 2, \dots, L \}$ so that all positions are noised exactly once after every $L$ sequence timesteps; we call this permutation the \textit{interleaving pattern}. Given $i_t$, the corresponding sequence element $s_{i_t}$ can either be discrete or continuous, based on which we either perform either discrete noising or continuous noising.

\paragraph{Discrete Noising}
If $s_{i_t}$ is discrete (i.e, $i_t \leq L_1$), following \cite{varma2024glauber}, we consider token $\phi \notin \mathcal{X}$ and define a probability distribution $\Pi_t$ over $\mathcal{X} \cup \{\phi\}$. Note that $\Pi_t$ depends on the sequence time $t$. We refer to $\Pi_t$ as the discrete noise schedule. Then the discrete noising process is as follows:

Sample $z_t \sim \Pi_t$ independent of $s^{(t)}$. Then we have:
\begin{align*}
    s^{(t+1)}_{j} = 
    \begin{cases}
    % s^{(t)}_{j},& \text{if } j \neq  i_t\\
    % s^{(t)}_{j},& \text{if } j =  i_t \text{  and  } z_t = \phi\\
    z_t,& \text{if } j =  i_t \text{  and  } z_t \neq \phi \\
    s^{(t)}_{j},& \text{otherwise }
    \end{cases}
\end{align*}

\paragraph{Continuous Noising}
If $s_{i_t}$ is continuous (i.e, $L_1 < i_t \leq L_2$), we use $m_{i_t}^t$ to denote the number of times position $i_t$ has been visited by sequence time $t$ (including the visit at $t$). Let $m = \max_{i_t} m_{i_t}^T $. Define $[\tilde{\beta}_j]_{j = 1}^{m}$ to be a monotonically increasing sequence, which we refer to as the continuous noise schedule.
Then, the continuous noising process is given by:
$
    s^{(t+1)}_{i_t} = \left(\sqrt{1 - \tilde{\beta}_{m_{i_t}^t}}\right) s^{(t)}_{i_t} + \left(\sqrt{\tilde{\beta}_{m_{i_t}^t}}\right) \epsilon^{(t)}
$
where $\epsilon^{(t)} \sim \mathcal{N}(0, \mathbf{I})$. Note that $s^{(t+1)}_{j} = s^{(t)}_{j} \quad \forall j \neq i_t$.

\begin{lemma}[Mild extension of Lemma $1$ in \cite{varma2024glauber}]
Denote the distribution of $s^{(t)}$ by $P_t$. Suppose $\Pi_t\left(\cdot | \mathcal{X} \right) = \Pi\left(\cdot | \mathcal{X} \right)$ for all $t$, $\Pi_t(\phi) \leq 1 - \epsilon$ for some $\epsilon > 0$ and $\lim_{T \rightarrow \infty} \sum_{j} \log \left(1- \tilde{\beta}_j\right) = -\infty$. As $T \rightarrow \infty$, $P_T$ converges to the product distribution: $\Pi\left( \cdot| \mathcal{X} \right)^{L_1} \times_{i=L_1 + 1}^{L} \mathcal{N}\left(0, \mathbf{I}_{d_i} \right)$ .
\label{lemma:for_process}
\end{lemma}

\paragraph{Co-ordinate wise independent noising:}

The noising process of any element $s_{i_t}^{(t)}$ at any time $t$ is independent of other elements; this allows us to sample $s^{(t)}$ at any time $t$ directly from $s^{(0)}$ without having to compute $s^{(1)}, s^{(2)}, ..., s^{(t)}$ sequentially (Algorithm given in Appendix \ref{app:sec:fwd_prcs}).

\subsection{Reverse Denoising Process}

The reverse denoising process takes a sample $\hat{s}^{(T)}$ from $P_T$ 
% $\Pi\left( \cdot| \mathcal{X} \right)^{\otimes L_1} \otimes \prod_{i = 1}^{L_2} \mathcal{N}\left(0, \mathbf{I}_{d_i} \right)$
as the input and applies a discrete time Markov chain to obtain the trajectory $\hat{s}^{(T)},\hat{s}^{(T-1)},\dots,\hat{s}^{(0)}$, where $T$ is the total number of sequence timesteps.  Recall that $i_t$ denotes the position which was noised at time $t$ during the forward process.\\

Given $\hat{s}^{(t+1)}$, we set $\hat{s}^{(t)}_{-i_t} = \hat{s}^{(t+1)}_{-i_t}$. Depending on whether $\hat{s}^{(t+1)}_{i_t}$ is discrete (resp. continuous) we use the discrete denoiser (resp. continuous denoiser) to sample $\hat{s}^{(t)}_{i_t}$ (${s}^{(t+1)}$ is the sample from the forward process at time $t+1$):

\textbf{Discrete Denoiser} is a (learned) sampling algorithm which can sample from $\hat{P}_{t,i_t}(\cdot|s)$, a probability distribution over $\mathcal{X}$ given $s$ as the input. $\hat{P}_{t,i_t}(\cdot|s = \hat{s}^{(t+1)})$ approximates one of the following:

$\mathbb{P}(s^{(t)}_{i_t} = \cdot| s^{(t+1)}_{-i_t} = \hat{s}^{(t+1)}_{-i_t})$ or $\mathbb{P}(s^{(t)}_{i_t} = \cdot| s^{(t+1)} = \hat{s}^{(t+1)})$

\textbf{Discrete Denoising Step:} $\texttt{DiscDen}(\hat{s}^{(t)},i_t,t)$ outputs a sample $\hat{s}^{(t)}_{i_t} \sim \hat{P}_{t,i_t}\left( \cdot | s = \hat{s}^{(t+1)} \right)$.


\textbf{Continuous Denoiser} is a (learned) sampling algorithm which can sample from the distribution $\hat{P}_{t,i_t}(\cdot|s)$ over $\mathbb{R}^{d_{i_t}}$ given $\hat{s}^{(t+1)}$ as the input. $\hat{P}_{t,i_t}(\cdot|s = \hat{s}^{(t+1)})$ approximates the conditional distribution $\mathbb{P}(s^{(t)}_{i_t} = \cdot| s^{(t+1)} = \hat{s}^{(t+1)})$.

\textbf{Continuous Denoising Step:} $\texttt{ContDen}(\hat{s}^{(t)}_{i_t},i_t,t)$ outputs a sample $\hat{s}^{(t)}_{i_t} \sim \hat{P}_{t,i_t}\left( \cdot |s =  \hat{s}^{(t+1)} \right)$.


\begin{lemma}
Assume $\hat{s}^{(T)} \sim P_T$ and assume we have access to ideal discrete and continuous denoisers. Then, $\hat{s}^{(0)}$ obtained after $T$ steps of reverse denoising process, will be such that $\hat{s}^{(0)} \sim \pi$.
\label{lemma:rev_process}
\end{lemma}

From the definition of the discrete and continuous denoisers, it is clear that unlike the forward process, the reverse process is \textit{not factorizable}. However, by sacrificing factorizability, we are able to achieve \textit{exact reversal of the forward process}, provided we have access to ideal denoisers. The denoising algorithm is detailed in Algorithm \ref{alg:framework}.

\begin{algorithm}[ht]
\begin{algorithmic}
\INPUT { $\hat{s}^{T} \sim P_T$, {discrete denoiser} \texttt{DiscDen} , {continuous denoiser} \texttt{ContDen}, noise positions $\{i_t\}$}
\OUTPUT {$\hat{s}^{0} \sim {\pi}$}

\FOR{$t \in [T, T-1, \dots, 1]$}
 \STATE $\hat{s}^{(t-1)}_{-i_t} = \hat{s}^{(t)}_{-i_t}$
 \IF {$\hat{s}^{(t)}_{i_t}$ is discrete}
    \STATE  $\hat{s}^{(t-1)}_{i_t} = \texttt{DiscDen}(\hat{s}^{(t)},i_t,t)$
 \ELSE
    \STATE  $\hat{s}^{(t-1)}_{i_t} = \texttt{ContDen}(\hat{s}^{(t)},i_t,t)$ 
  \ENDIF
\ENDFOR
\end{algorithmic}
\caption{Interleaved Gibbs Diffusion: Ideal Denoising}
\label{alg:framework}
\end{algorithm}

\subsection{\redenoise~Algorithm}
\label{sec:redenoise}
Inspired by \cite{meng2021sdedit}, we now propose a simple but effective mechanism for quality improvements at inference time. Given a sample obtained through complete reverse process $\hat{s}^{(0)}$, we now repeat the following two steps $N_R$ times: (1) Noise $\hat{s}^{(0)}$ for $T_R$ rounds to obtain $\hat{s}^{(T_R)}$. (2) Denoise $\hat{s}^{(T_R)}$ back to $\hat{s}^{(0)}$.
While $N_R$ decides the number of times the noise-denoise process is repeated, $T_R$ decides how much noising is done each time. These are hyperparameters which can be tuned to suit the task at hand.

\subsection{Conditional Generation}
\label{sec:cond_generation}
We train the model for conditional generation - i.e, generate a subset of the co-ordinates conditioned on the rest. We adopt the state-space doubling strategy, inspired by \cite{levi2023dlt}. A binary mask vector is created indicating whether each element in the sequence is part of the conditioning or not; for vectors in $\mathbb{R}^d$, a mask is created for each element in the vector. The mask is now embedded/projected and added to the discrete/continuous embedding and fed into the model while training. Further, during the forward and reverse processes, the conditioned elements are not noised/denoised.
