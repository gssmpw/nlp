\section{Model Architecture}
Inspired by \cite{peebles2023scalable}, we use a transformer-based architecture closely resembling Diffusion Transformers (DiTs) for the model. Since DiT has been designed for handling discrete tokens, we modify the architecture slightly to accommodate continuous vectors as well. However, we keep modifications to a minimum, so that the proposed architecture can still benefit from DiT design principles. Our proposed architecture, which we refer to as Discrete-Continuous (Dis-Co) DiT, is illustrated in Figure \ref{fig:gen_dit}.

Figure \ref{fig:disco_network} gives a high-level overview of the model with $N$ Dis-Co DiT blocks stacked on top of each other. Discrete embeddings, continuous projections and their corresponding time embeddings are passed into the Dis-Co DiT blocks. Figure \ref{fig:disco_block} details the structure of a single Dis-Co DiT block. The discrete embeddings and continuous vectors are processed as in a regular transformer block; however the discrete and continuous time information ($(t,k)$ variables) is incorporated using adaptive layer normalization \cite{xu2019adaptivelayernormalization}. Exact details are given in Appendix \ref{app:sec:model_arch}. 
\sloppy
\begin{figure*}
    \begin{tabular}[c]{lr}
    \begin{subfigure}[c]{0.45\textwidth}
      \includegraphics[width=1.1\textwidth]{images/DisCo_DiT.pdf}
      \caption{}
      \label{fig:disco_network}
    \end{subfigure}&
    \begin{subfigure}[c]{0.45\textwidth}
      \includegraphics[width=1.1\textwidth]{images/DisCo_DiT_Block.pdf}
      \caption{}
      \label{fig:disco_block}
    \end{subfigure}
  \end{tabular} 
  \caption{\textbf{Dis-Co DiT Architecture:} (a) illustrates overall architecture, with both discrete and continuous inputs and outputs (b) shows detailed architecture of a single block, where time information is incorporated through adaptive layer normalization.}
  \label{fig:gen_dit}
\end{figure*}