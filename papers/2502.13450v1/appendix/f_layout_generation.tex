\section{Layout Generation}
\label{app:layout_gen}

\subsection{Additional Results}
\label{app:layout_gen_full_results}

\begin{table*}[h]
    \centering
    \caption{\textbf{Layout Generation:} Additional metrics on the RICO and PubLayNet datasets.}
    \label{tab:layout_results_additional}
    \resizebox{1.00\columnwidth}{!}{
    \begin{tabular}{lrrrrrrrr} 
        \toprule
        \multicolumn{9}{c}{RICO} \\
        \midrule
         &  \multicolumn{2}{c}{\shortstack{Unconditioned}} && \multicolumn{2}{c}{\shortstack{Category\\Conditioned}} && \multicolumn{2}{c}{\shortstack{Category$+$Size\\Conditioned}} \\ 
      Method                                    &  \abbalignment               & \abboverlap          & $\quad$ & \abbalignment        & \abboverlap &$\quad$ & \abbalignment & \abboverlap \\
        \midrule
       LayoutTransformer   & 0.037                       & 0.542             && -         & -  && - & - \\
         LayoutFormer\texttt{\char`+\char`+}           & 0.051                    & {0.546}     && \bftab{0.124}               & 0.537 && - & - \\
         NDN-none & - & - && 0.560 & {0.550} && - & -  \\
         LayoutDM                                     & 0.143                        & 0.584             && 0.222                & 0.598  && {0.175} &    0.606   \\
         DLT                                          & 0.271                     & 0.571             &&  0.303                       &  0.616 && 0.332 &  0.609 \\
         LayoutDiffusion                               & 0.069  & 0.502 && \bftab{0.124}    &  0.491   && - & - \\
         LayoutFlow                              & \bftab{0.150}           & 0.498             && 0.176      &  0.517 &&   0.283        & 0.523 \\ 
         \midrule
         Ours                              & {0.198}        & \bftab{0.443}            &&  {0.215}      &  \bftab{0.461} &&  \bftab{0.204}       & \bftab{0.490}  \\ 
         \midrule
        & \multicolumn{4}{c}{Alignment} &  \multicolumn{4}{c}{Overlap}\\
        \midrule\\
        Validation Data & \multicolumn{4}{c}{0.093} & \multicolumn{4}{c}{0.466} \\
        \bottomrule
    \end{tabular}
    % }
    % %\label{tab:layout_results_rico}
    
    %     \resizebox{0.9\columnwidth}{!}{
\quad
\begin{tabular}{lrrrrrrrr}
        \toprule
        \multicolumn{9}{c}{PubLayNet} \\
        \midrule
        &  \multicolumn{2}{c}{\shortstack[c]{Unconditioned}} && \multicolumn{2}{c}{\shortstack[c]{Category\\Conditioned}} && \multicolumn{2}{c}{\shortstack[c]{Category$+$Size\\Conditioned}} \\  
      Method                                    &  \abbalignment               & \abboverlap          & $\quad$ & \abbalignment        & \abboverlap &$\quad$ & \abbalignment & \abboverlap \\
        \midrule
       LayoutTransformer   & 0.067                       & 0.005             && -         & -  && - & - \\
         LayoutFormer\texttt{\char`+\char`+}           & 0.228                    & {0.001}     && \bftab{0.025}               & 0.009 && - & - \\
         NDN-none & - & - && 0.350 & 0.170 && - & -  \\
         LayoutDM                                     & 0.180                        & 0.132             && 0.267                & 0.139  && 0.246 &    0.160   \\
         DLT            & 0.117         & {0.036}             &&  0.097                       &  0.040 && 0.130 &  0.053 \\
         LayoutDiffusion                               & 0.065  & \bftab{0.003} && 0.029    &  \bftab{0.005}   && - & - \\
         LayoutFlow                              & \bftab{0.057}           &      0.009         &&       {0.037} & 0.011 &&   \bftab{0.041}        & 0.031 \\ 
         \midrule
         Ours                              & {0.094}        & 0.008            &&  0.088      &  {0.013} &&  {0.081}       & \bftab{0.027}  \\ 
        \midrule
        & \multicolumn{4}{c}{Alignment} &  \multicolumn{4}{c}{Overlap}\\
        \midrule\\
        Validation Data & \multicolumn{4}{c}{0.022} & \multicolumn{4}{c}{0.003} \\
        \bottomrule
    \end{tabular}
    }
    % \label{tab:layout_results}
\end{table*}

Alignment and Overlap capture the geometric aspects of the generations. As per \cite{guerreiro2025layoutflow}, we judge both metrics with respect to a reference dataset, which in our case is the validation dataset. We see that there is no consistent trend with respect to these metrics among models. Further, note that most of the reported models use specialized losses to ensure better performance with respect to these metrics; our model achieves comparable performance despite not using any specialized losses. Our framework can be used in tandem with domain-specific losses to improve the performance on these geometric metrics.

\subsection{Generated Examples}
Table \ref{tab:layout_gen_examples} shows generated samples on PubLayNet dataset on the three tasks of Unconditioned, Category-conditioned and Category+Size conditioned.

\begin{table}[h!]
    \centering
    \begin{tabular}{ccc}
        \multicolumn{1}{c}{\textbf{Unconditioned Generation}} &  \multicolumn{1}{c}{\textbf{Category-conditioned Generation}} & \multicolumn{1}{c}{\textbf{Category+Size-conditioned Generation}}\\
        % First Column
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth]{images/appendix_pub_layout_uncond1.pdf}
            \vspace{0.5cm}
            \includegraphics[width=0.5\textwidth]{images/appendix_pub_layout_uncond2.pdf}
        \end{minipage} &
        % Second Column
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth]{images/appendix_pub_layout_cond_c_1.pdf}
            \vspace{0.5cm}
            \includegraphics[width=0.5\textwidth]{images/appendix_pub_layout_cond_c_2.pdf}
        \end{minipage} &
        % Third Column
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth]{images/appendix_pub_layout_cond_cs_1.pdf}
        
            \vspace{0.5cm}
            \includegraphics[width=0.5\textwidth]{images/appendix_pub_layout_cond_cs_2.pdf}
        \end{minipage} \\
    \end{tabular}
    \caption{Generated Layouts on PubLayNet Dataset}
    \label{tab:layout_gen_examples}
\end{table}

\subsection{Training Details}
We train a Dis-Co DiT model with the configuration in Table \ref{tab:layout_arch}.

\begin{table}[h]
    \centering
    \begin{tabular}{c c}
    \toprule
    % \multicolumn{2}{c}{} \\
    % \midrule
   Number of Generalized DiT Blocks  & 6  \\
   Number of Heads  & 8 \\
   Model Dimension & 512 \\
   MLP Dimension & 2048 \\
   Time Embedding Input Dimension & 256 \\
   Time Embedding Output Dimension & 128 \\
   \bottomrule
\end{tabular}
    \caption{Model configuration for Layout Generation}
    \label{tab:layout_arch}
\end{table}

We use the AdamW optimizer \cite{loshchilov2018decoupled} (with $\beta_1 = 0.9$, $\beta_2 = 0.999$ and $\epsilon = 10^{-8}$) with no weight decay and with no dropout.  We use EMA with decay $0.9999$.  We set the initial learning rate to 0 and warm it up linearly for 8000 iterations to a peak learning rate of $10^{-4}$; a cosine decay schedule is then applied to decay it to $10^{-6}$ over the training steps. For PubLayNet, we train for $4$ Million iterations with a batch size of $4096$, whereas for RICO, we train for $1.1$ Million iterations with a batch size of $4096$. By default, the sequence is noised for $4$ rounds $(T = 120)$; each continuous vector is noised $200$ times per round. We use pad tokens to pad the number of elements to 20 if a layout has fewer elements.

\paragraph{Data sampling and pre-processing:}
Since we train a single model for all three tasks (unconditional, class conditioned, class and size conditioned), we randomly sample layouts for each task by applying the appropriate binary mask required for the state-space doubling strategy. We begin training by equally sampling for all three tasks; during later stages of training, it may help to increase the fraction of samples for harder tasks to speed up training. For instance, we found that for the RICO dataset, doubling the fraction of samples for unconditional generation after $700$k iterations results in better performance in unconditional generation (while maintaining good performance in the other two tasks) when training for $1.1$ Million iterations. Further, each bounding box is described as $[x_i, y_i, l_i, w_i]$, where $(x_i, y_i)$ denotes the positions of the upper-left corner of the bounding box and $(l_i, w_i)$ denotes the length and width of the bounding box respectively. Note that $0 \leq x_i,y_i,l_i,w_i \leq 1 $ since the dataset is normalized. We further re-parameterize these quantities using the following transformation:
$$ g(x) = \log\left(\frac{x}{1-x} \right) $$
Note that we clip $x$ to $[10^{-5}, 1-10^{-5}]$ so that $g(x)$ is defined throughout. We then use this re-parameterized version as the dataset to train the diffusion model. While inference, the predicted vectors are transformed back using the inverse transformation:
$$ h(x) = g^{-1}(x) = \left(\frac{e^{x}}{1+e^{x}} \right) $$

\subsection{Ablations}
\label{app:subsec:abl_layout}

Unless specified otherwise, all the results reported in ablations use top-$p$ sampling with $p = 0.99$ and do not use the ReDeNoise algorithm at inference. From preliminary experiments, we found top-p sampling and ReDeNoise to only have marginal effects on the FID score; hence, we did not tune this further. For all layout generation experiments, we noise the sequence in a round-robin fashion, and in each round, $\Pi(\phi)$ is constant for discrete tokens across all positions. Similarly, $K_{i_t}^{t}$ which is the number of continuous noising steps per round, is constant across all positions per round. Hence, from here on, we use sequences of length $r$. where $r$ is the total number of noising rounds to denote $\Pi(\phi)$ and $K_{i_t}^{t}$ values for that particular round. By default, we choose $\Pi(\phi)$ to be $[0.5, 0.5, 0.5, 0.5]$, where the $4$ element sequence, which we refer to as the discrete noise schedule, denotes noising for 4 rounds with $\Pi(\phi)$ for the round chosen from the sequence. Similarly, the default value of $K_{i_t}^{t}$ is chosen to be $[200, 200, 200, 200]$, and we refer to this sequence as the continuous noising steps. Let us denote $\sum_{t}K_{i_t}^{t} $ as $K$. Note that $K$ is same across positions since we assume same number of continuous noising steps across positions per round. Given $K$, we define the following as the cosine schedule for $\beta$ (denoted by $\text{cosine}(a, b)$):
$$ \beta(j) = b + 0.5(a - b)(1 + \cos(\left(\frac{j}{K} \right)\pi)) $$
where $j$ is the total number of continuous noising steps at sequence time $t$ and element time $k$. We use $\text{cosine}(0.0001, 0.03)$ as the default schedule. We also define a linear noise schedule for $\beta$ ($\beta$ (denoted by $\text{lin}(a, b)$)):
$$ \beta(j) = a + (b - a)(1 + (\left(\frac{j}{K} \right))) $$
Further, we report only the unconditional FID for PubLayNet/RICO in the ablations as this is the most general setting.
\paragraph{Interleaving pattern:}
We broadly considered two interleaving patterns. In the first pattern, the bounding box vectors of each item was treated as a separate vector to form the interleaving pattern $[t_1, p_1, t_2, p_2, \dots, t_n, p_n ]$, where $t_i \in \mathbb{N}$ is the discrete item type and $p_i \in \mathbb{R}^{4}$ is its corresponding bounding box description ($p_i = [x_i, y_i, l_i, w_i]^\top$).  This interleaving pattern leads to $20$ discrete elements and $20$ continuous vectors per layout, resulting in a sequence of length $40$. In the second pattern, the bounding box vectors of all the $n$ items were bunched together as a single vector to form the interleaving pattern $[t_1, t_2, \dots, t_n, p^c ]$, where $p^c \in \mathbb{R}^{4n}$ is a single vector which is formed by concatenating the bounding box vectors of all $n$ items. This interleaving pattern leads to $20$ discrete elements and $1$ continuous vector per layout, resulting in a sequence of length $21$. We compare FID scores on unconditional generation on PubLayNet with these two interleaving patterns in Table \ref{tab:abl_layout_interleaving}.

\begin{table}[h]
    \centering
    \begin{tabular}{c c c c c}
    \toprule
   Interleaving Pattern & Disc. Noise Schedule & Cont. Noise Schedule & Cont. Noise Steps & FID \\
   \midrule
   Positions separate  &  $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[200, 200, 200, 200]$ & 8.76 \\
   Positions together &  $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[200, 200, 200, 200]$ & 14.21 \\
   Positions together &  $[0.35, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[200, 200, 200, 200]$ & 13.59 \\
   Positions together &  $[0.75, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[200, 200, 200, 200]$ & 13.99 \\
   Positions together &  $[0.99, 0.9, 0.8, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[150, 150, 150, 150, 150, 150]$ & 25.38 \\
   Positions together &  $[0.9, 0.75, 0.5, 0.5, 0.25]$ & $\text{cosine}(0.0001, 0.015)$ & $[500, 500, 500, 500, 500]$ & 17.86 \\
   \bottomrule
\end{tabular}
    \caption{Ablation on Interleaving Pattern}
    \label{tab:abl_layout_interleaving}
\end{table}

We see that despite tuning multiple hyperparameters for noise schedules, having the positions together leads to worse results than having the positions separate. Hence, we use the interleaving pattern of having the positions separate for all further experiments.

\paragraph{$\abs{\mathcal{X}}$-ary classification v/s Binary classification:} \label{app:par_xary_binary}
We compare the two strategies for training the discrete denoiser, $\abs{\mathcal{X}}$-ary classification and Binary classification (as described in \ref{sec:training}), on the unconditional generation task in the RICO dataset. The results are given in Table \ref{tab:abl_layout_loss}.

\begin{table}[h]
    \centering
    \begin{tabular}{c c}
    \toprule
   Discrete Loss Considered & FID \\
   \midrule
    $\abs{\mathcal{X}}$-ary Cross Entropy &  3.51 \\
   Binary Cross Entropy  & 2.62 \\
   \bottomrule
\end{tabular}
    \caption{Ablation on choice of discrete loss function}
    \label{tab:abl_layout_loss}
\end{table}


\paragraph{Discrete and continuous noise schedules:}
We evaluate the unconditional FID scores on PubLayNet and RICO for multiple configurations of discrete and continuous noise schedules. We report the results in Tables \ref{tab:abl_publaynet_noising} and \ref{tab:abl_rico_noising}.

\begin{table}[h]
    \centering
    \begin{tabular}{c c c c c}
    \toprule
   Disc. Noise Schedule & Cont. Noise Schedule & Cont. Noise Steps & FID \\
   \midrule
   $[0.5, 0.5, 0.5, 0.5]$ & $\text{lin}(0.0001, 0.02)$ & $[200, 200, 200, 200]$ & 13.19 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{lin}(0.0001, 0.035)$ & $[200, 200, 200, 200]$ & 10.62 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[200, 200, 200, 200]$ & 8.86 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[100, 100, 300, 300]$ & 8.32 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[25, 25, 50, 700]$ & 8.68 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.06)$ & $[10, 10, 10, 370]$ & 12.78 \\
    $[0.75, 0.5, 0.25, 0.25]$ & $\text{cosine}(0.0001, 0.03)$ & $[10, 10, 10, 770]$ & 10.06 \\ 
     $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.025)$ & $[10, 10, 10, 970]$ & 9.67 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.02)$ & $[10, 10, 10, 1170]$ & 10.83 \\
    $[0.9, 0.75, 0.5, 0.5, 0.25]$ & $\text{cosine}(0.0001, 0.06)$ & $[50, 50, 50, 50, 50, 50]$ & 9.10 \\
    $[0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[10, 10, 10, 10, 10, 850]$ & 10.42 \\
    $[0.99, 0.9, 0.8, 0.5, 0.25, 0.05]$ & $\text{cosine}(0.0001, 0.03)$ & $[400, 400, 70, 10, 10, 10]$ & 17.69 \\
   
    
   \bottomrule
\end{tabular}
    \caption{Ablation on Discrete and Continuous Noise Schedules - PubLayNet}
    \label{tab:abl_publaynet_noising}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c c c c c}
    \toprule
   Disc. Noise Schedule & Cont. Noise Schedule & Cont. Noise Steps & FID \\
   \midrule
   $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[10, 10, 10, 770]$ & 2.54 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.06)$ & $[10, 10, 10, 370]$ & 3.67 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.05)$ & $[10, 10, 10, 570]$ & 3.35 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[300, 300, 100, 100]$ & 5.13 \\
    $[0.5, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[100, 100, 300, 300]$ & 4.33 \\
    $[0.9, 0.8, 0.7, 0.5, 0.5, 0.5]$ & $\text{cosine}(0.0001, 0.03)$ & $[10, 10, 10, 10, 380, 380]$ & 3.88 \\
    
   \bottomrule
\end{tabular}
    \caption{Ablation on Discrete and Continuous Noise Schedules - RICO}
    \label{tab:abl_rico_noising}
\end{table}

From the ablations, it seems like for layout generation, noising the discrete tokens faster than the continuous vectors gives better performance. This could be because denoising the bounding boxes faster allows the model to make the element type predictions better. 

\newpage

\paragraph{Best configuration:} We obtain the best results with the configuration in Table \ref{app:tab:layout_config_best}.

\begin{table}[!h]
    \centering
    \begin{tabular}{c c c}
    \toprule
    Hyperparameter & PubLayNet & RICO\\
    \midrule
    % \multicolumn{2}{c}{} \\
    % \midrule
   Interleaving Pattern  & Positions separate & Positions separate  \\
   Discrete Noise Schedule & $[0.5, 0.5, 0.5, 0.5]$ & $[0.5, 0.5, 0.5, 0.5]$ \\
   Continuous Noising Steps & $[100, 100, 300, 300]$ &  $[10, 10, 10, 770]$ \\
   Continuous Noise Schedule &$\text{cosine}(0.0001, 0.03)$  \\
   Top-p & 0.99\\
   \bottomrule
\end{tabular}
    \caption{Best configuration for Layout Generation}
    \label{app:tab:layout_config_best}
\end{table}


\newpage
