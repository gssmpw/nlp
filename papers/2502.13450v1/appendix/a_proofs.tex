\section{Proofs}

\subsection{Lemma \ref{lemma:for_process}}

\paragraph{Statement:\\}

Denote the distribution of $s^{(t)}$ by $P_t$. Suppose $\Pi_t\left(\cdot | \mathcal{X} \right) = \Pi\left(\cdot | \mathcal{X} \right)$ for all $t$, $\Pi_t(\phi) \leq 1 - \epsilon$ for some $\epsilon > 0$ and $\lim_{T \rightarrow \infty} \sum_{j} \log \left(1- \tilde{\beta}_j\right) = -\infty$. As $T \rightarrow \infty$, $P_T$ converges to the product distribution: $\Pi\left( \cdot| \mathcal{X} \right)^{L_1} \times_{i=L_1 + 1}^{L} \mathcal{N}\left(0, \mathbf{I}_{d_i} \right)$ .

\paragraph{Proof:\\}

We closely follow the proof of Lemma 1 in \cite{varma2024glauber}.

Note that the forward noising for each element is independent of all other elements.  Hence, it suffices to consider the noising of each element separately.

Consider a discrete element. By assumption, the probability of not choosing $\phi$:
$$ 1 - \Pi(\phi) \geq \epsilon  $$
where $\epsilon>0$ for all. Further, when $\phi$ is not chosen at time $t$, then the distribution of the discrete token is $\Pi( \cdot|\mathcal{X})$ for all time $\geq t$ independent of other tokens. The probability of choosing only $\phi$ until time $t$ is at most $(1-\epsilon)^t$ and this goes to $0$ as $t \rightarrow \infty $. Therefore with probability $1$, asymptotically, every discrete element is noised to the distribution $\Pi\left( \cdot| \mathcal{X} \right)$.

Consider a continuous vector at position $i$. From the definition of the forward process, we have:
\begin{align}
    s^{(t+1)}_{i} = \left(\sqrt{1 - \tilde{\beta}_{m_{i}^t}}\right) s^{(t)}_{i} + \left(\sqrt{\tilde{\beta}_{m_{i}^t}}\right) \epsilon
\end{align}
where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$. Merging the Gaussians, we have:
\begin{align*}
    s^{(t+1)}_{i} = \left(\sqrt{\tilde{\alpha}_{m_{i}^t}}\right) s^{(0)}_{i} + \left(\sqrt{1 - \tilde{\alpha}_{m_{i}^t}}\right) \epsilon
\end{align*}
where:
\begin{align*}
    \tilde{\alpha}_{m_{i}^t} = \prod_{j = 0}^{m_{i}^t} (1 - \tilde{\beta}_j)
\end{align*}
Since $m_i^{t}$ denotes the number of times the position $i$ is visited by sequence time $t$,  $m_i^{t} \rightarrow \infty$ as $t \rightarrow \infty$. Hence, from the assumption $\lim_{T \rightarrow \infty} \sum_{j} \log \left(1- \tilde{\beta}_j\right) = -\infty$, we have $\lim_{t \rightarrow \infty}\tilde{\alpha}_{m_{i}^t} = 0$ and hence the continuous vector will converge to an independent Gaussian with variance $1$ per continuous dimension.

\newpage

\subsection{Lemma \ref{lemma:rev_process}}

\paragraph{Statement:\\}
Assume $\hat{s}^{(T)} \sim P_T$ and assume we have access to ideal discrete and continuous denoisers. Then, $\hat{s}^{(0)}$ obtained after $T$ steps of reverse denoising process, will be such that $\hat{s}^{(0)} \sim \pi$.

\paragraph{Proof:\\}

Recall that $s^{(t)} \in \mathcal{S}_{L}$ denotes the the sequence at sequence time $t$ of the forward process. Further, $P_{t}$ denotes the probability measure of $s^{(t)}$ over $\mathcal{S}_{L}$. $\hat{s}^{(t)} \in \mathcal{S}_{L}$ denotes the the sequence at sequence time $t$ of the reverse process and let $\hat{P}_{t}$ denote the probability measure of $\hat{s}^{(t)}$ over $\mathcal{S}_{L}$.

We now prove the lemma by induction. Assume that $\hat{s}^{(t+1)} \overset{d}{=} {s}^{(t+1)} $, i.e., ${P}_{t+1} = \hat{P}_{t+1}$. Consider a measurable set $\mathcal{A}$ such that $\mathcal{A} \subseteq \mathcal{S}_L$ . Let $y \sim \hat{P}_{t+1}$. From the measure decomposition theorem, we have:
\begin{align*}
    \mathbb{P}(\hat{s}^{(t)} \in \mathcal{A}) = \int_{y} \mathbb{P}\left(\hat{s}^{(t)} \in \mathcal{A}|\hat{s}^{(t+1)} = y \right) d\hat{P}_{t+1}(y)
\end{align*}
From the induction assumption, we can rewrite this as:
\begin{align*}
    \mathbb{P}(\hat{s}^{(t)} \in \mathcal{A}) = \int_{y} \mathbb{P}\left(\hat{s}^{(t)} \in \mathcal{A}| \hat{s}^{(t+1)} = y \right) d{P}_{t+1}(y)
\end{align*}
From the definition of the reverse process, we know that $\hat{s}^{(t)}_{-i_t} = \hat{s}^{(t+1)}_{-i_t} $. Therefore, we have:
\begin{align*}
     \mathbb{P}\left(\hat{s}^{(t)} \in \mathcal{A}|\hat{s}^{(t+1)} = y \right) =  \mathbb{P}\left(\hat{s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)| \hat{s}^{(t+1)} = y \right) 
\end{align*}
where $\mathcal{A}_{-i_t}\left(y_{-i_t} \right) = \{x_{i_t}: x \in \mathcal{A}, x_{-i_t} = y_{-i_t} \}$. Depending on the reverse process chosen, we have:
\begin{align*}
    \mathbb{P}\left(\hat{s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|\hat{s}^{(t+1)} =  y \right) &= \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)} = y \right) \quad \text{or} \\
    \mathbb{P}\left(\hat{s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|\hat{s}^{(t+1)} =  y \right) &= \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)}_{-i_t} = y_{-i_t} \right)
\end{align*}

\textbf{Case 1:} $ \mathbb{P}\left(\hat{s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|\hat{s}^{(t+1)} =  y \right) = \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)} = y \right)  $

We have:
\begin{align*}
    \mathbb{P}\left(\hat{s}^{(t)} \in \mathcal{A}|\hat{s}^{(t+1)} = y \right) =  \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)} = y \right)
\end{align*}
And hence:
\begin{align*}
    \mathbb{P}(\hat{s}^{(t)} \in \mathcal{A}) &= \int_y  \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)} = y \right) d{P}_{t+1}(y) \\
    &= \mathbb{P}({s}^{(t)} \in \mathcal{A})
\end{align*}

\textbf{Case 2:} $ \mathbb{P}\left(\hat{s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|\hat{s}^{(t+1)} =  y \right) = \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)}_{-i_t} = y_{-i_t} \right)  $

We have:
\begin{align*}
    \mathbb{P}\left(\hat{s}^{(t)} \in \mathcal{A}|\hat{s}^{(t+1)} = y \right) = \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)}_{-i_t} = y_{-i_t} \right)
\end{align*}
And hence:
\begin{align*}
    \mathbb{P}(\hat{s}^{(t)} \in \mathcal{A}) &= \int_y  \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)}_{-i_t} = y_{-i_t} \right) d{P}_{t+1}(y) \\
\end{align*}
By measure decomposition theorem ${P}_{t+1}(y)$ is factorizable as:
\begin{align*}
    {P}_{t+1}(y) = {P}_{t+1, -i_t}(y_{-i_t}){P}_{t+1, i_t}(y_{i_t}|y_{-i_t})
\end{align*}
Therefore:
\begin{align*}
    \mathbb{P}(\hat{s}^{(t)} \in \mathcal{A}) &= \int_y  \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)}_{-i_t} = y_{-i_t} \right) \left(d{P}_{t+1, -i_t}(y_{-i_t})\right) \left(d{P}_{t+1, i_t}(y_{i_t}|y_{-i_t})\right) \\
    &= \int_{y_{-i_t}} \mathbb{P}\left({s}^{(t)}_{i_t} \in \mathcal{A}_{-i_t}\left(y_{-i_t} \right)|s^{(t+1)}_{-i_t} = y_{-i_t} \right) \left(d{P}_{t+1, i_t}(y_{i_t}|y_{-i_t}))\right) \int_{y_{i_t}} \left(d{P}_{t+1, i_t}(y_{i_t})\right) \\
     &= \mathbb{P}({s}^{(t)} \in \mathcal{A})
\end{align*}

Hence, we have  $\hat{s}^{(t)} \overset{d}{=} {s}^{(t)} $, i.e. ${P}_{t} = \hat{P}_{t}$. Therefore, by induction $\hat{P}_{0} = \pi$, provided $\hat{P}_{T} = {P}_{T}$.

\newpage
\subsection{Lemma \ref{lemma:score_noise}}

\paragraph{Statement:}
Under the considered forward process where noising occurs independently, we have:
 \begin{align*}
     \nabla_{{s}_{i_t}^{(t, k+1)}} \log q({s}_{i_t}^{(t, k+1)} | {s}_{-i_t}^{(t, k+1)} )  = -\frac{1}{\sqrt{1-\bar{\alpha}}} \mathbb{E} \left[ \epsilon | {s}^{(t,k+1)} \right]
 \end{align*}

\paragraph{Proof:}

Let us split $\hat{s}^{(t, k+1)} = \left[\hat{s}_{i_t}^{(t, k+1)} \; \hat{s}_{-i_t}^{(t, k+1)}  \right]$. Note that, $\hat{s}_{i_t}^{(t, k+1)}$ is the continuous part that is being de noised.

\begin{align} \label{eq:gradlog}
    \nabla_{\hat{s}_{i_t}^{(t, k+1)}} \log q(\hat{s}_{i_t}^{(t, k+1)} | \hat{s}_{-i_t}^{(t, k+1)} )  
  \hfill & =\frac{\nabla_{\hat{s}_{i_t}^{(t, k+1)}}  q(\hat{s}_{i_t}^{(t, k+1)} | \hat{s}_{-i_t}^{(t, k+1)} )}{ q(\hat{s}_{i_t}^{(t, k+1)} | \hat{s}_{-i_t}^{(t, k+1)} )} \nonumber \\
   \hfill & = \frac{\nabla_{\hat{s}_{i_t}^{(t, k+1)}}  q(\hat{s}_{i_t}^{(t, k+1)} | \hat{s}_{-i_t}^{(t, k+1)} ) q(\hat{s}_{-i_t}^{(t, k+1)})}{ q(\hat{s}_{i_t}^{(t, k+1)} | \hat{s}_{-i_t}^{(t, k+1)} ) q(\hat{s}_{-i_t}^{(t, k+1)})} \nonumber \\
   \hfill & = \frac{\nabla_{\hat{s}_{i_t}^{(t, k+1)}}  q(\hat{s}_{i_t}^{(t, k+1)}, \hat{s}_{-i_t}^{(t, k+1)} )}{ q(\hat{s}_{i_t}^{(t, k+1)} , \hat{s}_{-i_t}^{(t, k+1)} )} 
    \nonumber \\
    \hfill & = \frac{\nabla_{\hat{s}_{i_t}^{(t, k+1)}}  \int q(\hat{s}_{i_t}^{(t, k+1)}, \hat{s}_{-i_t}^{(t, k+1)} | s_{i_t}^{(0)} ) q(s_{i_t}^{(0)}) ds_{i_t}^{(0)} } { q(\hat{s}_{i_t}^{(t, k+1)} , \hat{s}_{-i_t}^{(t, k+1)} )} \nonumber \\
    \hfill & \overset{(a)}{=} \frac{\nabla_{\hat{s}_{i_t}^{(t, k+1)}}  \int q(\hat{s}_{i_t}^{(t, k+1)}| s_{i_t}^{(0)} ) q( \hat{s}_{-i_t}^{(t, k+1)} |  s_{i_t}^{(0)}) q(s_{i_t}^{(0)}) ds_{i_t}^{(0)} } { q(\hat{s}_{i_t}^{(t, k+1)} , \hat{s}_{-i_t}^{(t, k+1)} )} \nonumber \\
    \hfill &\overset{(b)}{=} \frac{\int \frac{-\epsilon}{\sqrt{1-\bar{\alpha}}} q(\hat{s}_{i_t}^{(t, k+1)}| s_{i_t}^{(0)} ) q( \hat{s}_{-i_t}^{(t, k+1)} |  s_{i_t}^{(0)}) q(s_{i_t}^{(0)}) ds_{i_t}^{(0)} } { q(\hat{s}_{i_t}^{(t, k+1)} , \hat{s}_{-i_t}^{(t, k+1)} )}  \nonumber \\
    \hfill &\overset{(a)}{=} \frac{\int \frac{-\epsilon}{\sqrt{1-\bar{\alpha}}} q(\hat{s}_{i_t}^{(t, k+1)}, \hat{s}_{-i_t}^{(t, k+1)},s_{i_t}^{(0)})  ds_{i_t}^{(0)} } { q(\hat{s}_{i_t}^{(t, k+1)} , \hat{s}_{-i_t}^{(t, k+1)} )} \nonumber \\
    \hfill & = \int \frac{-\epsilon}{\sqrt{1-\bar{\alpha}}} q(s_{i_t}^{(0)} |\hat{s}_{i_t}^{(t, k+1)}, \hat{s}_{-i_t}^{(t, k+1)})  ds_{i_t}^{(0)}  
\end{align}
In the RHS of the chain in \eqref{eq:gradlog}, we observe that $\hat{s}^{t,k+1}$ is being conditioned on and given $\hat{s}^{t,k+1}$, $\epsilon$ is a function only of $s_{i_t}^{(0)}$ from \eqref{eq:diff_relation}. Therefore, the above chain yields:
 \begin{align}
     \nabla_{\hat{s}_{i_t}^{(t, k+1)}} \log q(\hat{s}_{i_t}^{(t, k+1)} | \hat{s}_{-i_t}^{(t, k+1)} )  = \frac{1}{\sqrt{1-\bar{\alpha}}} \mathbb{E} \left[ -\epsilon | \hat{s}^{(t,k+1)} \right]
 \end{align}
 
 This is the exactly $\frac{g_{\theta}(\cdot)}{\sqrt{1-\bar{\alpha}}}$ if the estimator was a perfect MMSE estimator.


Justifications:- (a) observe that conditioned on $s_{i_t}^{(0)}$, how $i_t$-th element is noised in the forward process is independent of all other elements. This gives rise to the conditional independence.(b) We exchange the integral and the $\nabla$ operator. Let $q(x|y)$ be conditionally Gaussian, i.e.  $x|y \sim \mathcal{N}(\sqrt{\bar{\alpha}} y ; (1-\bar{\alpha}))$, then it is a property of the conditional Gaussian random variable that $\nabla_x q(x|y) = -\left(\frac{x - \sqrt{\bar{\alpha}}y}{1-\bar{\alpha}} \right) * q(x|y)$. Taking $x = \hat{s}_{i_t}^{t,k}$ and $y = s_{i_t}^{(0)}$ from \eqref{eq:diff_relation}, we see that: $\nabla_{\hat{s}_{i_t}^{(t, k+1)}}  q(\hat{s}_{i_t}^{(t, k+1)}| s_{i_t}^{(0)} ) = \frac{-\epsilon}{\sqrt{1-\bar{\alpha}}}* q(\hat{s}_{i_t}^{(t, k+1)}| s_{i_t}^{(0)} ) $.

\newpage
