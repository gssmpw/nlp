\section{Boolean Satisfiability Problem}
\label{app:3sat}

\subsection{Training Details} 
\label{app:sat_train_details}

We trained models of three different sizes (6M, 85M, and 185M parameters), whose configurations are summarized in Table \ref{tab:sat_model_configuration}. Each model was trained for 1M steps on the combined dataset with $n\in {6,\dots,20}$. For the experiments where a separate model was trained for each $n$ (corresponding to Table~\ref{tab:sat_n_5_7_9_accuracy}), the batch size was increased from 8192 to 16384 and trained for 200K steps.
A gradual noising schedule of $[0.99, 0.9, 0.8, 0.5, 0.5, 0.25]$ was used for the discrete noising process in all SAT experiments.
\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Parameter} & \textbf{6M} & \textbf{85M} & \textbf{185M} \\
\midrule
Number of DiT Blocks & 4 & 12 & 24 \\
Number of Heads & 8 & 12 & 16 \\
Model Dimension & 336 & 744 & 768 \\
MLP Dimension & 1344 & 2976 & 3072 \\
Time Embedding Input Dim & 256 & 256 & 256 \\
Time Embedding Output Dim & 128 & 128 & 128 \\
Learning Rate & 2e-4 & 7.5e-5 & 5e-5 \\
Batch Size & 8192 & 8192 & 4096 \\
\bottomrule
\end{tabular}
\caption{Model Configurations for Different Parameter Sizes for Boolean Satisfiability Problem}
\label{tab:sat_model_configuration}
\end{table}

Here DiT Block \cite{peebles2023scalable} is a modified transformer block designed to process conditional inputs in diffusion models. For Boolean Satisfiability (SAT), these blocks evolve variable assignments and clause states while incorporating diffusion timestep information through specialized conditioning mechanisms.\\

Adaptive Layer Norm (adaLN-Zero) \cite{xu2019adaptivelayernormalization}: Dynamically adjusts normalization parameters using timestep embeddings:
\begin{equation}
    \text{AdaLN}(h,t) = t_s \cdot \text{LayerNorm}(h) + t_b
\end{equation}
where $t_s$, $t_b$ are learned projections from timestep $t$. The \textit{adaLN-Zero} variant initializes residual weights ($\alpha$) to zero, preserving identity initialization for stable training.\\

Time-conditioned MLP: Processes normalized features with gated linear units (GLU), scaled by the diffusion timestep.

We use the AdamW optimizer \cite{loshchilov2018decoupled} (with $\beta_1 = 0.9$, $\beta_2 = 0.999$ and $\epsilon = 10^{-8}$) with no weight decay and with no dropout.  We use EMA with decay $0.9999$.

\subsection{Data Generation}
We follow the procedure of \citet{ye2024autoregressiondiscretediffusioncomplex} to create a large dataset of 15M satisfiable 3-SAT instances covering $n \in {6,\dots,20}$. Each instance is generated by: 
\begin{enumerate}
\item Sampling clauses where each clause has exactly three variables, chosen uniformly at random from the $n$ available. 
\item Randomly deciding whether each variable in the clause appears in complemented or non-complemented form. 
\end{enumerate} 
After generating the clauses, we run a standard SAT solver to ensure each instance is satisfiable, discarding any unsatisfiable cases. Finally, the data is split into training and test sets, with multiple checks to prevent overlap.


\subsection{Accuracy Trend During Training}
\label{app:sat_accuracy_trend}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\columnwidth]{images/appendix_sat_n20_clauses_accuracy_trend.pdf}
  \caption{Evolution of the modelâ€™s SAT accuracy and number of satisfied clauses over training for random 3-SAT instances with $n=18$ on 185M model.}
  \label{fig:sat_n_18_accuracy_trend}
\end{figure}

Figure~\ref{fig:sat_n_18_accuracy_trend} illustrates how the SAT accuracy evolves over training for a model trained on instances, showing for $n=18$ as a representative example. In the early stages (roughly the first half of training), the accuracy remains near zero, even as the model steadily improves in satisfying individual clauses. This indicates that the model initially learns partial solutions that satisfy a growing fraction of the clauses. Once the model begins consistently satisfying nearly all clauses in an instance, accuracy jumps sharply, reflecting that the assignments finally meet all the constraints simultaneously.
