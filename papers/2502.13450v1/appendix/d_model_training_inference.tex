\section{Model Training and Inference: Pseudocode}
\label{app:sec:model_train_pseudo}

We use the binary classification based loss for describing the training of the model to do discrete denoising since this leads to better results. Note that for this, from , the input to the model should be $s_{-i_t}^{(t+1)}$ and the model should predict $\mathbb{P}\left({z_t} = x | {s}^{(t+1)}_{-i_t} = {s}_{-i_t}, {s}^{(t+1)}_{i_t} = x \right)$ for all $x \in \mathcal{X}$. To do this efficiently, we adapt the masking strategy from \cite{varma2024glauber}. Define a token $\omega \notin \mathcal{X}$. Let $\mathcal{\tilde{X}} = \mathcal{X} \cup \omega$. Let $\tilde{s}^{(t+1)} \in \mathcal{\tilde{S}}_{L}$, where $\mathcal{\tilde{S}}_L = \mathcal{\tilde{X}}^{L_1}\times_{i=L_1+1}^{L}\mathbb{R}^{d_i}$, be defined as: $\tilde{s}^{t+1}_{-i_t} = s^{t+1}_{-i_t}$ and $\tilde{s}^{t+1}_{i_t} = \omega$. The neural network $f_{\theta}$ then takes as input: time tuple $(t, k)$, noising position $i_t$, sequence $\tilde{s}^{t+1}$ (or $\tilde{s}^{t, k+1}$ if $i_t$ corresponds to a continuous vector). The time tuple $(t, k)$ is $(t, 0)$ if the element under consideration is discrete since discrete tokens only have one noising step. The model has $\abs{\mathcal{X}}$ logits corresponding to \textit{each} discrete token (and hence a total of $L_1\abs{\mathcal{X}}$ logits) and $\mathbb{R}^{d_i}$ dimensional vectors corresponding to \textit{each} continuous vector (and hence a total of $L_2$ continuous vectors). $i_t$ is necessary for the model to decide which output needs to be sliced out: we use $f_{\theta}^{i_t}$ to denote the output of the model corresponding to the element at position $i_t$ (which could either be discrete or continuous). Further, we use $f_{\theta}^{(i_t, s_{i_t}^{t+1})}$ to denote the logit corresponding to position $i_t$ and token $s_{i_t}^{t+1}$, provided $i_t$ corresponds to a discrete token.

We can then write the pseudocode for training as follows:

\begin{algorithm}[ht]
\begin{algorithmic}
\INPUT { Dataset $\mathcal{D}$, {model} $f_{\theta}$ , {forward process block} \texttt{FwdPrcs}, optimizer \texttt{opt}, total sequence timesteps $T$, noise positions $\{i_t\}_{t = 0}^{T-1}$, discrete noise schedule $\{\Pi_{t}\}_{t = 0}^{T-1}$, continuous noise schedule $\{\beta_{j}\}_{j = L_1+1}^{L_2}$, continuous noising steps  $\{K_{j}^t\}_{j = L_11+1, t = 0}^{j = L_2, t=T-1}$}
\OUTPUT {trained model parameters $\theta$}

\FOR{each iteration:}
 \STATE sample $s^{(0)}$ from $\mathcal{D}$
 \STATE sample $t$ from $[0, 1, \dots, T-1]$
 \IF {$\hat{s}^{(0)}_{i_t}$ is discrete}
    \STATE  $(s^{(t)}, z_{t}, s^{(t+1)}) = \texttt{FwdPrcs}(s^{(0)}, t, \{i_{\tau}\}_{\tau = 0}^t, \{\Pi_{\tau}\}_{\tau = 0}^t)$
    \STATE construct $\tilde{s}^{t+1}$ from ${s}^{t+1}$
    \STATE compute the BCE loss:
    $$ \mathcal{L} =  -  \mathbf{1}_{z_t \neq \phi}  \log \left( f_{\theta} ^{(i_t, s^{(t+1)}_{i_t})} \left(\tilde{s}^{(t+1)}, t, i_{t} \right) \right)
    - \mathbf{1}_{z_t = \phi}  \log \left(1 - f_{\theta} ^{(i_t, s^{(t+1)}_{i_t})} \left(\tilde{s}^{(t+1)}, t, 0, i_{t} \right) \right)  $$
 \ELSE
    \STATE  sample $k$ from $[0, 1, \dots, K_{i_t}^t - 1]$
    \STATE $(s^{(t, k+1)}, \epsilon) = \texttt{FwdPrcs}(s^{(0)}, t, k, \{i_{\tau}\}_{\tau = 0}^t, \{\beta_{j}\}_{j = L_1+1}^{L_2}) $
    \STATE compute the MSE loss:
    $$ \mathcal{L} = \norm{\epsilon - f^{i_t}_{\theta}\left({s}^{(t, k+1)}, t, k, i_t \right)}_2^2 $$
    
 \ENDIF
 \STATE $\theta \leftarrow \texttt{opt.update}(\theta, \nabla_{\theta}\mathcal{L})$
\ENDFOR
\end{algorithmic}
\caption{Model Training}
\label{app:alg:training}
\end{algorithm}

\newpage

Recall that $\hat{s}^{(t)}$ represents the sequence from the reverse process at time $t$ and $P_T = \Pi\left( \cdot| \mathcal{X} \right)^{L_1} \times_{i=L_1 + 1}^{L} \mathcal{N}\left(0, \mathbf{I}_{d_i} \right)$ denotes the stationary distribution of the forward process. If the training of the model is perfect, we will have $\hat{s}^{(0)} \sim \pi$. Then the pseudocode for inference:

\begin{algorithm}[ht]

\begin{algorithmic}
\INPUT {total sequence timesteps $T$, noise positions $\{i_t\}_{t = 0}^{T-1}$, discrete noise schedule $\{\Pi_{t}\}_{t = 0}^{T-1}$, continuous noise schedule $\{\beta_{j}\}_{j = L_1+1}^{L_2}$, continuous noising steps  $\{K_{j}^t\}_{j = L_1+1, t = 0}^{j = L_2, t=T-1}$}
\OUTPUT{$\hat{s}^{(0)}$}

\STATE sample $\hat{s}^{(T)} \sim P_T$
\FOR{ $t$ in $[T-1, T-2, \cdots, 0]$}

\IF{$\hat{s}^{(t+1)}_{i_t}$ is discrete}
\STATE construct $\tilde{s}^{(t+1)}$ from $\hat{s}^{(t+1)}$
\STATE get $\hat{y} = f_{\theta} ^{i_t} \left(\tilde{s}^{(t+1)}, t, i_{t} \right)$ \COMMENT{ $\hat{y}$ denotes the vector of $\abs{\mathcal{X}}$ logits corresponding to position $i_t$}
\STATE compute $\hat{\mathbb{P}}\left(s^{(t)}_{i_t} = a | s^{(t+1)}_{-i_t} \right) = \frac{\Pi_t(a)}{\Pi_t(\phi)} \left( \frac{1}{\hat{y}^{(a)}} - 1 \right)$ for all $a \in
\mathcal{X}$ \COMMENT{$\hat{y}^{(a)}$ denotes logit corresponding to token $a$}
\STATE sample $\hat{s}^{(t)}_{i_t} \sim \hat{\mathbb{P}}\left(s^{(t)}_{i_t} = a | s^{(t+1)}_{-i_t} \right)$
\STATE set $\hat{s}^{(t)}_{-i_t} = \hat{s}^{(t+1)}_{-i_t}$
\ELSE
\STATE set $\hat{s}^{(t, K_{i_t}^t)} = \hat{s}^{(t+1)}$
\FOR{$k$ in $[K_{i_t}^t-1, K_{i_t}^t-2, \cdots, 0]$}
\STATE get $\epsilon_{\theta} = f_{\theta} ^{i_t} \left(\hat{s}^{(t, k+1)}, t, k, i_t \right)$ \COMMENT{ $f_{\theta} ^{i_t}$ denotes the continuous vector corresponding to position $i_t$}
\IF{t = k = 0}
\STATE get $\epsilon = 0$
\ELSE
\STATE get $\epsilon \sim \mathcal{N}(0, \mathbf{I})$
\ENDIF
\STATE set $\hat{s}^{(t, k)}_{i_t}  = \frac{\left(\hat{s}^{(t, k+1)}_{i_t} - {\beta_{i_t}}(t, k+1)\epsilon_{\theta} \right)}{\sqrt{1 - {\beta_{i_t}}(t, k+1)}} 
    + \left(\sqrt{{\beta_{i_t}}(t, k+1)}\right) \epsilon$
\STATE set $\hat{s}^{(t, k)}_{-i_t} = \hat{s}^{(t, k+1)}_{-i_t}$
\ENDFOR
\STATE set $\hat{s}^{(t)} = \hat{s}^{(t, 0)}$

\ENDIF

\ENDFOR

\end{algorithmic}

\end{algorithm}

\newpage
