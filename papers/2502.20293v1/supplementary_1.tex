%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%
\documentclass[preprint,3p,4pt,times,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
% \documentclass[final,3p,times]{elsarticle}
% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

% additional packages
\usepackage{array}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{lineno}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage{enumitem}

% Change hyperlink colors
\hypersetup{
    colorlinks=true,
    urlcolor=magenta,      % Color for URLs
    linkcolor=magenta,     % Color for internal links
    citecolor=magenta,     % Color for citations
    % pagebackref=true,    % Show page numbers of where citations were used
    % hypertexnames=true,
}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
\usepackage{lineno}
\usepackage[table]{xcolor}

\journal{AI Open}

\begin{document}

% \begin{frontmatter}

% %% Title, authors and addresses

% %% use the tnoteref command within \title for footnotes;
% %% use the tnotetext command for theassociated footnote;
% %% use the fnref command within \author or \affiliation for footnotes;
% %% use the fntext command for theassociated footnote;
% %% use the corref command within \author for corresponding author footnotes;
% %% use the cortext command for theassociated footnote;
% %% use the ead command for the email address,
% %% and the form \ead[url] for the home page:
% %% \title{Title\tnoteref{label1}}
% %% \tnotetext[label1]{}
% %% \author{Name\corref{cor1}\fnref{label2}}
% %% \ead{email address}
% %% \ead[url]{home page}
% %% \fntext[label2]{}
% %% \cortext[cor1]{}
% %% \affiliation{organization={},
% %%             addressline={},
% %%             city={},
% %%             postcode={},
% %%             state={},
% %%             country={}}
% %% \fntext[label3]{}

% \title{\emph{Supplementary File 1} for\\Scalable Graph Attention‐Based Instance Selection:\\A Mini‐Batch Sampling and Hierarchical Hashing Approach}

% \end{frontmatter}

\begin{center}
\Large{\textbf{\emph{Supplementary File 1} for}}\\[0.5em]
\Large{Scalable Graph Attention‐Based Instance Selection:\\A Mini‐Batch Sampling and Hierarchical Hashing Approach}\\[1em]
\end{center}

\vspace{1em} % Add some space after the title

%-------------------------------------------
% Paper Body
%-------------------------------------------
\section{Hyperparameter Optimization Details}

This section provides detailed information about the hyperparameters optimized in our experiments using Bayesian optimization with the Optuna framework. For each dataset and method, we conducted 50 primary optimization trials to tune the graph construction parameters and GAT architectural components, followed by 20 nested optimization trials for instance selection parameters.

\begin{table*}[htpb]
\caption{Distance-based Mini-Batching (DM) Method Parameters}
\label{tab:dm_params}
\scriptsize
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}
    >{\raggedright\arraybackslash}p{0.1\textwidth}|
    >{\raggedright\arraybackslash}p{0.3\textwidth}|
    >{\raggedright\arraybackslash}p{0.15\textwidth}|
    >{\raggedright\arraybackslash}p{0.4\textwidth}}
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Range/Options} & \textbf{Notes} \\
\midrule
$K$ (train) & Number of mini-batches for training data & 1-6 (integer) & Controls granularity of graph construction \\
$K$ (validation) & Number of mini-batches for validation data & 1-2 (integer) & Controls granularity of validation graph \\
$w_{\text{max}}$ & Maximum instances per mini-batch & 7000 (fixed) & Cap on mini-batch size to control memory usage \\
$d(x_u, x_v)$ & Distance metric used & ['euclidean', 'cosine'] & Determines how instance similarity is calculated \\
$\tau_k$ & Similarity threshold for edge creation & 0.8-0.99 (float) & Controls graph sparsity; higher values create sparser graphs \\
$f_{\text{original}}$ & Original sampling fraction & 0.7-0.999 (float) & Controls dynamic sampling fraction \\
\bottomrule
\end{tabular*}
\end{table*}

\begin{table*}[htpb]
\caption{Locality-Sensitive Hashing (LSH) Method Parameters}
\label{tab:lsh_params}
\scriptsize
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}
    >{\raggedright\arraybackslash}p{0.07\textwidth}|
    >{\raggedright\arraybackslash}p{0.2\textwidth}|
    >{\raggedright\arraybackslash}p{0.14\textwidth}|
    >{\raggedright\arraybackslash}p{0.14\textwidth}|
    >{\raggedright\arraybackslash}p{0.35\textwidth}}
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Range/Options} & \textbf{Applicable Methods} & \textbf{Notes} \\
\midrule
$L$ & Number of hash tables & \{2, 3, 4, 5, 6\} & All LSH variants & Base number of hash tables that increases exponentially with level in ML variants \\
$k$ & Number of projection vectors & \{2, 3, 4, 5, 6\} & All LSH variants & Base number of projections that increases exponentially with level in ML variants \\
$\theta$ & Bucket size threshold & \{100, 150, 200\} & All LSH variants & Controls adaptive bucket splitting threshold \\
LSH type & Hashing method & ['euclidean', 'angular'] & All LSH variants & Determines projection and quantization approach \\
$M$ & Number of hierarchical levels & \{1, 2, 3\} & ML- \& MVML-LSH & Not applicable to SL-LSH; defines number of edge sets \\
$V$ & Number of views & 2 (fixed) & MVML-LSH & Only relevant for multi-view variant \\
\bottomrule
\end{tabular*}
\end{table*}

\begin{table*}[htpb]
\caption{Cross-Attention Parameters}
\label{tab:cross_attn_params}
\scriptsize
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}
    >{\raggedright\arraybackslash}p{0.1\textwidth}|
    >{\raggedright\arraybackslash}p{0.25\textwidth}|
    >{\raggedright\arraybackslash}p{0.11\textwidth}|
    >{\raggedright\arraybackslash}p{0.15\textwidth}|
    >{\raggedright\arraybackslash}p{0.3\textwidth}}
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Range/Options} & \textbf{Applicable Methods} & \textbf{Notes} \\
\midrule
Attention heads & Number of attention heads for cross-attention & \{2, 4\} & CML- \& CMVML-LSH & Fixed options for cross-attention variants \\
$m$ & Number of landmark nodes for Nyström approximation & \{64, 128, 256\} & CML- \& CMVML-LSH & Controls approximation granularity for cross-level attention \\
$\gamma$ & Degree ratio parameter & 0.0-1.0 (step 0.1) & CML- \& CMVML-LSH & Balance between high-degree and random landmarks \\
Gating weight & Weight for cross-level attention contribution & \{0.3, 0.5, 0.7\} & CML- \& CMVML-LSH & Controls influence of cross-level/view information \\
\bottomrule
\end{tabular*}
\end{table*}

\begin{table*}[htpb]
\caption{Graph Attention Network (GAT) Parameters (Common to All Methods)}
\label{tab:gat_params}
\scriptsize
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}
    >{\raggedright\arraybackslash}p{0.13\textwidth}|
    >{\raggedright\arraybackslash}p{0.25\textwidth}|
    >{\raggedright\arraybackslash}p{0.14\textwidth}|
    >{\raggedright\arraybackslash}p{0.4\textwidth}}
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Range/Options} & \textbf{Notes} \\
\midrule
Number of layers & Number of hidden layers in GAT & 1-3 (integer) & Controls model depth \\
Hidden dimensions & Hidden dimension size & 4-32 (step 4) & Controls model capacity \\
$K$ (input heads) & Number of attention heads in input layer & 1-8 (integer) & More heads capture diverse patterns \\
$K$ (output heads) & Number of attention heads in output layer & 1-4 (integer) & Controls output representation diversity \\
Batch normalization & Whether to use batch normalization & \{True, False\} & Stabilizes training \\
Diversity mode & Mode of diversity-promoting attention & ['per\_head', 'single'] & Controls how diversity is encouraged across heads \\
$\lambda_k$ initialization & Initial diversity factor & 0.01-0.3 (step 0.01) & Controls strength of diversity promotion \\
Feature dropout & Feature dropout rate & 0.0-0.7 (step 0.05) & Prevents overfitting \\
Attention dropout & Attention dropout rate & 0.0-0.7 (step 0.05) & Prevents attention overfitting \\
Learning rate & Learning rate for optimizer & 1e-5 to 1e-1 (log scale) & Controls optimization speed \\
Weight decay & L2 regularization & 0.0-1e-3 (float) & Controls model complexity \\
Optimizer & Optimization algorithm & Adam & Fixed to Adam optimizer \\
Scheduler & Learning rate scheduler & ReduceLROnPlateau & Adaptive learning rate based on validation metrics \\
Class weighting & Whether to use class weights in loss & \{True, False\} & Helps with imbalanced datasets \\
\bottomrule
\end{tabular*}
\end{table*}

\begin{table*}[htpb]
\caption{Instance Selection Parameters (Nested Optimization)}
\label{tab:selection_params}
\scriptsize
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}
    >{\raggedright\arraybackslash}p{0.12\textwidth}|
    >{\raggedright\arraybackslash}p{0.2\textwidth}|
    >{\raggedright\arraybackslash}p{0.25\textwidth}|
    >{\raggedright\arraybackslash}p{0.3\textwidth}}
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Range/Options} & \textbf{Notes} \\
\midrule
$\rho$ & Desired retention rate & 0.001-0.3 (float) & Controls percentage of instances to select \\
Selection strategy & Method for instance selection & ['global threshold-based', 'class-aware'] & Global vs class-specific selection approaches \\
Class-aware variant & Type of class-aware selection & ['balanced', 'proportional'] & Only used when selection strategy='class-aware' \\
\bottomrule
\end{tabular*}
\end{table*}

\newpage

For each dataset and method combination, we employed a hierarchical optimization approach:

\begin{enumerate}
    \item \textbf{Primary Optimization (50 trials)}: Optimizes graph construction and GAT parameters to maximize validation set performance. This includes optimizing the graph structure parameters (whether distance-based or LSH-based) and the GAT architecture parameters.
   
    \item \textbf{Nested Optimization (20 trials per primary trial)}: For each primary trial configuration, optimizes instance selection parameters to maximize the effectiveness metric ($E = AC \times R$, where $AC$ is accuracy and $R$ is reduction rate) on the validation set. This step determines both the optimal selection strategy and retention rate $\rho$.

    \item \textbf{Final Evaluation}: The best configuration from the nested optimization is evaluated on the test set to report the final performance metrics.
\end{enumerate}

% This hierarchical approach ensures that the instance selection strategy is tailored to the specific characteristics of the graph representation and importance scores produced by each configuration, capturing the interdependencies between graph construction, node importance computation, and instance selection.


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
% \appendix
% \section{Example Appendix Section}
% \label{app1}

% Appendix text.

% \bibliographystyle{elsarticle-harv} 
% \bibliography{references}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
