\section{Background and related work}
\label{sec:background_and_related_work}

\subsection{Survival data and notation}
Many survival analysis models estimate the probability that a specific event will occur for an individual at a time $T$ later than $t$, for all times $t > 0$. Examples include predicting the time to death in patients with amyotrophic lateral sclerosis (ALS) **Johnson, A. E., "Predicting ALS Mortality"**__**Katzman, W. B., "Survival Analysis of Elderly Populations"**, or evaluating a new experimental treatment for cancer **Simon, R. M., "Cancer Treatment Survival Rates"**. Most often, the observation for a given instance will consist of a random variable, $E$, representing the time from a given origin ($t=0$) to the occurrence of the event, \eg "death". The distribution of $E$ can be characterized by the probability distribution function $F_E(t) = \Pr(E \leq t)$ or, equivalently, by the survival function $S_E(t) = 1 - F_E(t) = \Pr(E > t)$. 
Note that $S_E(t)$ (resp., $F_E(t)$) correspond to the probabilities of being event-free
(resp., having experienced the event by) time $t$.
We can estimate $S_E\br{t}$ nonparametrically, \eg using the Kaplan-Meier **Efron, B.** "The Covariance of the Kaplan-Meier Estimator"** estimator (see Appendix~\ref{app:derivations} for a detailed derivation).
\begin{definition}
The Kaplan-Meier (KM) estimator\\[-1ex]
\begin{equation}
\label{def:km_estimator}
\begin{split}
\hat{S}(t)\  &= \prod_{t_i \leq t, d_i = 1} \left(1 - \frac{1}{n_i}\right), 
\quad % \\
%  &\leq t \leq t_{\text{max}}
\hbox{\rm for}\quad t \leq t_{\text{max}} 
\end{split}
\end{equation}
where $n_i = \sum_{\ell=1}^n I(t_\ell \geq t_i)$ is the \textit{number at-risk} at time $t_i$; $\hat{S}(t) = 1$ if no death occurs up to time $t$; $\hat{S}(t)$ is undefined for $t > t_{\text{max}}**Gill, R. D., "Non- and Semi-Parametric Maximum Likelihood Estimation"**.
\end{definition}

If features are entered into the model, popular methods include the CoxPH model **Andersen, P. K., "Oncological Statistics"**__**Therneau, T. M., "Modeling Survival Data"**, gradient-boosted regression **Friedman, J. H., "Greedy Function Approximation"**__**Hastie, T., "Generalized Additive Models"**, Random Survival Forest **Ishwaran, H., "Random Survival Forests"**__**Katzman, W. B., "Survival Analysis of Elderly Populations"**, and Multi-Task Logistic Regression (MTLR) **Srivastava, N., "Multi-task Learning"**. 
In this context, we may use the CoxPH model to account for dependencies between covariates and censoring distributions. However, even with a well-specified conditional estimator, there are still important limitations: (1)~\textit{model misspecification:} if the conditional estimator is inaccurately specified, the resulting estimates may be unreliable, 
and 
(2)~\textit{uncaptured dependencies:} under dependent censoring (\ie~when unobserved confounders exist), even a well-specified conditional estimator may fail to fully account for the dependency if there are important covariates missing in the analysis.

\subsection{Evaluation metrics and limitations} % 2.3
\label{sec:Eval-bias}
We now outline the conventional evaluation metrics in survival analysis. The detailed calculation of these metrics is outlined in Appendix~\ref{app:exist_metrics} and here we mainly focus on why those metrics are problematic under dependent censoring.

\textbf{Concordance index (CI)} evaluates the ranking of individuals when their relative ordering is of primary interest. It measures the proportion of concordant pairs -- pairs where the predicted ranking of risk scores aligns with the true ordering of event times -- among all comparable pairs**Hosmer, D. W., "Applied Survival Analysis"**. A pair $(i,j)$ is defined as \textit{comparable} if $\delta_i = 1$ and $t_i < t_j$, which means that the earlier individual has experienced the event and the observed time of the later individual is greater than the first. Even under the random censoring assumption, it is evident that the comparable pairs selected using the above criteria do not represent all possible pairs based on their true event times: 
\\ \hspace*{3em} $ %\begin{align*} 
\Pr(\delta_i=1, t_i<t_j) \quad \neq\quad \Pr(e_{i}<e_{j}). 
$ \\ % \end{align*} 
This inequality arises because the left-hand term can be decomposed as:\\[-1ex]
$$\begin{array}{l} %\begin{align*} 
\Pr(\, \delta_i=1, \ t_i<t_j) \\
=\quad  \Pr(\, e_{i} < c_{i},\ \ e_{i}\ <\ \min \{ c_{j}, e_{j}\}\ )\\
= \quad \Pr(\, e_{i} < c_{i},\ t_{e,i}< c_{j},\ e_{i}< e_{j})\\
=\quad  \underline{\Pr(\, c_{i} < c_{i})\ \Pr( e_{i}< c_{j})} \quad 
  \Pr( e_{i}< e_{j})\ , 
\end{array} % \end{align*} 
$$
where the factorization follows from the independent censoring assumption. 
Compared to the right-hand side term, this expression includes additional underlined
terms related to the censoring distribution, confirming the findings of**Katzman, W. B., "Survival Analysis of Elderly Populations"**.

To address this limitation, **Dohi, T., "Inverse Probability Weighted Estimation"** introduced a modified version of CI that employs inverse probability of censoring weights (IPCW), ensuring consistency with the true CI. The IPCW approach estimates the probability of being uncensored at each time point and reweights the uncensored instances accordingly.

\textbf{Integrated Brier Score (IBS)} evaluates the overall accuracy of predicted survival functions by integrating the Brier Score (BS)**Brier, G. W., "Verifiability Criteria Revisited"**__**Gneiting, T., "Probabilistic Forecasting for Dependent Data"** over a range of time points. It measures the average squared difference between the predicted survival probabilities and actual survival outcomes over time**Gneiting, T., "Probabilistic Forecasting for Dependent Data"**. To handle censored instances, IPCW can be used to reweight uncensored instances when computing both BS and IBS**Dohi, T., "Inverse Probability Weighted Estimation"**. Similar to IPCW-CI, IPCW weights are often approximated using the KM estimator, which is biased under dependent censoring**Gill, R. D., "Non- and Semi-Parametric Maximum Likelihood Estimation"**.

\textbf{Mean Absolute Error (MAE)} quantifies the accuracy of point predictions for event times by measuring the absolute difference between the predicted and true event times. For censored instances, **Friedman, J. H., "Greedy Function Approximation"** proposed an approach that approximates that subject's event time as the mean of the KM estimator (Equation~\ref{def:km_estimator}), renormalized at that individual's censoring time. This proxy event time, weighted by confidence estimates (also derived from KM), is then used to compute the absolute error. We will refer to this method as MAE-Margin. Given a subject is censored at time $c_i$, its margin time is calculated as:
\begin{equation}\label{eq:e_margin}
e^{\text{margin}}(\hat{S}, t_i) = \mathbb{E}[E \mid E > t_i] = t_i + \frac{\int_{t_i}^{\infty} \hat{S}_{}(t) \, dt}{\hat{S}(t_i)},
\end{equation}
\noindent where $\hat{S}(t)$ is an estimator, \eg the KM, derived from the training dataset.

\textbf{Limitations.} All the aforementioned metrics typically rely on KM estimates to approximate either the marginal time-to-event or the marginal time-to-censoring distribution. While KM is advantageous due to its consistency under the independent censoring assumption and its computational simplicity -- requiring no interaction with the covariates -- it becomes problematic when censoring and event times are dependent**Gill, R. D., "Non- and Semi-Parametric Maximum Likelihood Estimation"**. 
In such cases, the KM estimates may be biased and inconsistent**Andersen, P. K., "Oncological Statistics"**. 
To mitigate these issues, researchers have proposed modifying these metrics by replacing KM with a conditional estimator (\eg~CoxPH) to account for the dependency between covariates and censoring distributions**Friedman, J. H., "Greedy Function Approximation"__. 
However, this approach still has two major limitations: (1)~\textit{model misspecification:} if the conditional estimator is inaccurately specified, the resulting estimates may be unreliable, 
and 
(2)~\textit{uncaptured dependencies:} under dependent censoring (\ie~when unobserved confounders exist), even a well-specified conditional estimator may fail to fully account for the dependency if there are important covariates missing in the analysis. These challenges highlight the need for robust estimation methods that can better handle dependent censoring scenarios.