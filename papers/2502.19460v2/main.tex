%\documentclass{uai2025} % for initial submission
\documentclass[accepted]{uai2025}

\usepackage[american]{babel}

\usepackage{natbib} % has a nice set of citation styles and commands
\bibliographystyle{plainnat}
\renewcommand{\bibsection}{\subsubsection*{References}}

\usepackage{mathtools} % amsmath with fixes and additions
%\usepackage{siunitx} % for proper typesetting of numbers and units
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amssymb,amsfonts,mathtools}
\usepackage[autostyle, english=american]{csquotes}
\usepackage{multirow}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{dsfont}
\usepackage{arydshln}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{bbm}
\numberwithin{equation}{section}

\MakeOuterQuote{"}

\usepackage{bm}
\usepackage{adjustbox}
\usepackage{makecell}
\DeclareMathOperator{\EX}{\mathbb{E}}%

\DeclarePairedDelimiter\autobracket{(}{)}
\newcommand{\br}[1]{\autobracket*{#1}}
\DeclarePairedDelimiter\sparen{[}{]}
\newcommand{\sbr}[1]{\sparen*{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bth}{\bm{\theta}}
\newcommand{\brth}{\br{\bm{\theta}}}
\newcommand{\KL}{\text{KL}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\bz}{{\bm{\zeta}}}

\renewcommand{\Pr}{\text{Pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\setlength\intextsep{\glueexpr\intextsep/2\relax}
\usepackage{dcolumn}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\DeclareUnicodeCharacter{2212}{-}

% http://trackchanges.sourceforge.net/
\usepackage[inline]{trackchanges}
\addeditor{RG}
\addeditor{CL}
\addeditor{SQ}

\def\ie{{\em i.e.},\ }
\def\eg{{\em e.g.},\ }
\def\CIDep{{\bf CI-Dep}} % \textbf{Dep. CI}
\def\IBSDep{{\bf IBS-Dep}} % \textbf{Dep. IBS}
\def\MAEDep{{\bf MAE-Dep}} % \textbf{Dep. MAE} 

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\title{Practical Evaluation of Copula-based Survival Metrics:\\ Beyond the Independent Censoring Assumption}

\author[12]{\href{mailto:clillelund@ualberta.ca}{Christian Marius Lillelund}{}}
\author[1]{Shi-ang Qi}
\author[1,3]{Russell Greiner}
% Add affiliations after the authors
\affil[1]{%
    Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada
}
\affil[2]{%
    Department of Electrical and Computer Engineering, Aarhus University, Aarhus, Denmark
}
\affil[3]{%
    Alberta Machine Intelligence Institute, University of Alberta, Edmonton, Alberta, Canada
}

\begin{document}
\maketitle

\begin{abstract}
Conventional survival metrics, such as Harrell's concordance index and the Brier Score, rely on the independent censoring assumption for valid inference in the presence of right-censored data. However, when instances are censored for reasons related to the event of interest, this assumption no longer holds, as this kind of dependent censoring biases the marginal survival estimates of popular nonparametric estimators. In this paper, we propose three copula-based metrics to evaluate survival models in the presence of dependent censoring, and design a framework to create realistic, semi-synthetic datasets with dependent censoring to facilitate the evaluation of the metrics. Our empirical analyses in synthetic and semi-synthetic datasets show that our metrics can give error estimates that are closer to the true error, mainly in terms of prediction accuracy.
\end{abstract}

\section{Introduction}\label{sec:Intro}

Learning survival prediction models resembles learning regression models, as both are essentially trying to learn, from a labeled dataset, a model that maps a description of an instance to a real value. However, survival prediction  differs as some of the training instances are \hbox{"censored"} -- that is, we only know a lower bound of the true time. This includes, for example, individuals who are still alive at the end of the study, as well as others who left the study and were \hbox{“lost to follow-up”}. 
The {\em independent censoring assumption} states that, within any subgroup of interest, we assume that the patients (\ie instances) who are censored at time $t$ are representative of all the patients in that subgroup who remained at risk at time $t$ with respect to their survival experience~\hbox{\citep[Ch. 1]{Emura2018}}. That is, censoring is \textit{random} within the subgroups. Let $E$ be the event distribution and $C$ be the censoring distribution, then we have $E \perp C$ -- \ie $E$ provides no information about the distribution of ${C}$, and vice versa~\hbox{\citep[Ch. 1]{kleinbaum2012survival}}. If there are features (covariates) $\bm{X}$, this assumption becomes the \emph{conditional independent} censoring assumption, \hbox{\ie} $E \perp C \mid \bm{X}$~\citep[Ch. 1]{kleinbaum2012survival}. (See Appendix~\ref{app:censoring_assumptions} for details.) Note that many standard survival tools -- \eg the Kaplan-Meier (KM)~\citep{kaplan_nonparametric_1958} and Nelson-Aalen~\citep{nelson_1969_hazard, aalen_1978_nonparametric} estimators, and the semiparametric Cox Proportional Hazards (CoxPH) model~\citep{cox_regression_1972}, rely on an assumption of independent censoring for valid inference in the presence of right-censored data~\citep[Ch. 1]{kleinbaum2012survival}.

\begin{figure*}[!ht]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth, trim=15 15 15 10, clip]{Figures/dependence1.pdf}
    \caption{}
    \label{fig:dependence1}
  \end{subfigure}
  \begin{subfigure}[b]{0.36\textwidth}
    \centering
    \includegraphics[width=\textwidth, trim=15 20 20 10, clip]{Figures/dependence2.pdf}
    \caption{}
    \label{fig:dependence2}
  \end{subfigure}
  \begin{subfigure}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth, trim=25 25 20 10, clip]{Figures/dependence3.pdf}
    \caption{}
    \label{fig:dependence3}
  \end{subfigure}
  \caption{(a)~Residual dependence between event time $E$ (cancer relapse) and censoring time $C$ (cancer death) denoted by the diamond `Dep', due to an unobserved confounding covariate -- here `Tumor grade'. 
  (b)~Evaluation under dependent censoring, where Patient A is censored immediately before the event for reasons related to the event. This leads to a bias estimation on the marginal survival distribution using the Kaplan-Meier (KM)~\citep{kaplan_nonparametric_1958} estimator. Note that the two curves are coincide between range $[0, c_{i}]$.
  (c)~Scatter plot generated from a standard exponential distribution under a Clayton~\citep{clayton1978model} copula model.}
  \label{fig:dependence}
\end{figure*}

Figure~\ref{fig:dependence1} shows how the independent censoring assumption fails to hold by omitting a confounding covariate -- \eg "tumor grade". Since "tumor grade" relates to both $E$ (cancer relapse) and $C$ (cancer death), the variation in the covariate induces changes in both $E$ and $C$. That is, we want a model that predicts the time until relapse, but death is censoring. As "tumor grade" is predictive of tumor progression, an advanced (resp., early) stage is linked to shorter (longer) values for both cancer relapse $E$ and cancer death $C$, and consequently, 
$E$ and $C$ 
are not independent.
This shows
that the conditional independent censoring assumption might not hold if any confounding covariates are omitted or ignored in a regression model~\citep[Ch. 1]{Emura2018}. 
This phenomenon is called \textit{residual dependence}
-- \ie %, that is, 
\textit{dependent censoring} not adjusted by the covariates.

Figure~\ref{fig:dependence2} demonstrates how dependent censoring biases the marginal survival estimates from the KM estimator~\citep{kaplan_nonparametric_1958}. Consider patient A, censored (due to cancer death) exactly one year after study entry, who likely would have experienced cancer relapse shortly thereafter. However, the KM estimator may overestimate this patient's survival time by assuming that they are like those remaining in the study, who tend to live longer, because it does not consider that these censored patients tend to have shorter event times. Indeed, the estimator relies on a stepwise construction of the survival probability that depends on knowing how many individuals are at risk and the event probabilities at each time point~\citep{Bland1998}. 

To address such dependency, Figure~\ref{fig:dependence3} presents a copula-based modeling approach. A copula function $C_\theta(u,v)$ links random variables by specifying their dependence structure~\citep[Ch. 1]{Emura2018} and have recently gained interest in survival analysis~\citep{foomani_copula-based_2023, zhang2024deep, liu2024hacsurv}. Applied here, it captures the dependency between event and censoring distributions by sampling the marginals from a (survival) copula. In this example, we see lower tail dependence from the copula -- describing the probability that small (early) values of one variable (\eg event) are associated with small values of another variable (\eg censoring).

In the absence of features, if any confounder related to the event censors the event time, we refer to this phenomenon as \textit{dependent censoring}. However, it is typically impossible to verify this in practice because we only observe one outcome (either event or censoring) per instance, but not both~\citep{Tsiatis1975}. Although we can reduce dependent censoring simply by collecting more data (\ie features)~\citep[Ch. 1]{Emura2018}, we would still have to evaluate our model under dependent censoring, independent of the learned model, preferably. 
Popular metrics for evaluating predicted survival probabilities (or risk scores)
are Harrell's concordance index (CI)~\citep{Harrell1982}, the Brier Score (BS)~\citep{Brier1950}, and the mean absolute error (MAE) using 
one of % either
hinge~\citep{haider_effective_2020}, margin~\citep{haider_effective_2020} or pseudo times~\citep{qi_effective_2023}. 
Unfortunately,
dependent censoring is both common~\citep[Ch. 1]{Emura2018} and also
introduces a bias that can affect the estimation of survival probabilities, and none of these metrics account for this.
Alternative methods have been proposed to handle dependent censoring, such as
Uno's CI~\citep{uno2011} using the inverse probability of censoring weighting (IPCW)~\citep{robins2000ipcw}, but this approach still relies on the KM estimator. Motivated by these limitations, we introduce three new metrics for evaluating survival models under dependent censoring. To our knowledge, no prior work has proposed evaluation metrics that explicitly model the degree of dependency between $E$ and $C$. Our main contributions are:
%\vspace*{-0.5em}
\begin{itemize}
    \item We show that conventional survival metrics, such as Harrell's CI, Uno's CI, the IPCW-integrated Brier Score, and the marginal MAE, are biased under dependent censoring, 
    meaning they can give inaccurate
    estimates of a model's performance.
    \item We propose three novel metrics -- \CIDep, \IBSDep, and \MAEDep\ 
    -- to evaluate survival models in the presence of dependent censoring. These metrics can evaluate the concordance (\ie ranking), the accuracy of the survival curve, and the accuracy of the predicted time-to-event estimates, respectively, by considering the dependency between event and censoring.
    \item We design a framework to create realistic, semi-synthetic datasets with various degrees of dependent censoring to determine if the proposed metrics can (1)~report an unbiased error closest to the true error, and (2)~rank the survival models by the true error.
    \item We provide empirical evidence that our proposed metrics can provide more accurate error measures for survival models, mainly in terms of prediction accuracy.
\end{itemize}

Our code is publicly available at \href{https://github.com/thecml/DependentEVAL}{this GitHub repository}.

\section{Background and related work}
\label{sec:background_and_related_work}

\subsection{Survival data and notation}
Many survival analysis models estimate the probability that a specific event will occur for an individual at a time $T$ later than $t$, for all times $t > 0$. Examples include predicting the time to death in patients with amyotrophic lateral sclerosis (ALS)~\citep{kuan2023accurate}, predicting the time to a fall in older adults~\citep{lillelund_prognosis_2024}, or evaluating a new experimental treatment for cancer~\citep{Zhang2011}. Most often, the observation for a given instance will consist of a random variable, $E$, representing the time from a given origin ($t=0$) to the occurrence of the event, \eg "death". The distribution of $E$ can be characterized by the probability distribution function $F_E(t) = \Pr(E \leq t)$ or, equivalently, by the survival function $S_E(t) = 1 - F_E(t) = \Pr(E > t)$. 
Note that $S_E(t)$ (resp., $F_E(t)$) correspond to the probabilities of being event-free
(resp., having experienced the event by) time $t$.
We can estimate $S_E\br{t}$ nonparametrically, \eg using the Kaplan-Meier~\citep{kaplan_nonparametric_1958} estimator (see Appendix~\ref{app:derivations} for a detailed derivation).
\begin{definition}
The Kaplan-Meier (KM) estimator\\[-1ex]
\begin{equation}
\label{def:km_estimator}
\begin{split}
\hat{S}(t)\  &= \prod_{t_i \leq t, d_i = 1} \left(1 - \frac{1}{n_i}\right), 
\quad % \\
%  &\leq t \leq t_{\text{max}}
\hbox{\rm for}\quad t \leq t_{\text{max}} 
\end{split}
\end{equation}
where $n_i = \sum_{\ell=1}^n I(t_\ell \geq t_i)$ is the \textit{number at-risk} at time $t_i$; $\hat{S}(t) = 1$ if no death occurs up to time $t$; $\hat{S}(t)$ is undefined for $t > t_{\text{max}}$~\citep{kaplan_nonparametric_1958}. 
\end{definition}

If features are entered into the model, popular methods include the CoxPH model~\citep{cox_regression_1972, katzman_deepsurv_2018, lillelund_efficient_2024}, gradient-boosted regression~\citep{Ridgeway1999}, Random Survival Forest~\citep{ishwaran_random_2008} and Multi-Task Logistic Regression (MTLR)~\citep{NIPS2011_1019c809}. 
In this context, we may define our survival dataset $\mathcal{D}$ as a set of observations, 
\ie we have $\mathcal{D} = \left\lbrace \br{\bm{x}_{i}, t_i, \delta_i} \right\rbrace_{i=1}^N$, where $\bm{x}_i \in \mathbb{R}^{d}$ is a feature (covariate) vector for instance $i$, $t_i \in \mathbb{R}^{\geq 0}$ is the time of either censoring or event, depending on which occurred first, and $\delta_i \in \{0, 1\}$ is a event indicator where $\delta_i = 0$ means the event has not happened by $t_i$ (right-censored) and $\delta_i = 1$ means the event occurred at $t_i$ (uncensored). We consider each instance to have a potential event time $e_i$ and a right-censoring time $c_i$, where we observe only the earlier of the two. Therefore, we define the observed time and event indicator as:
\\ \hspace*{3em} $ %\begin{align*} 
t_i \ := \ \min\{e_i, c_i\}, \quad \text{and} \quad \delta_i \ := \ \mathbbm{1}[e_i \leq c_i],
$ \\ % \end{align*} 
\noindent where $\mathbbm{1}[\cdot]$ represents the indicator function.

Given survival data $\mathcal{D}$, let $f_{E\mid \bm{X}}(\cdot)$ and $F_{E\mid \bm{X}}(\cdot)$ represent the conditional density and cumulative distribution functions, respectively, over the event horizon (\ie from $0$ to $t_{\text{max}}$). With these two functions, we then have the following useful definitions:

\begin{definition}
The survival function\\[-2ex]
\begin{equation}
\label{def:survival_function}
S_{E\mid \bm{X}}(t\mid \bm{X}) = \Pr(E > t \mid  \bm{X}) = 1 - F_{E\mid \bm{X}}(t \mid  \bm{X}),
\end{equation}
represents the probability the event occurs at time $t$~\cite[Ch. 11]{gareth_introduction_2021}.
\end{definition}
\begin{definition}

The hazard function\\[-2ex]
\begin{eqnarray}
% \begin{equation}
% \label{def:hazard_function}
% \begin{split}
h_{E\mid \bm{X}}(t\mid \bm{X})& = \ & \lim_{\epsilon \to 0} \Pr(E \in [t, t+\epsilon) \mid E \geq t, \bm{X})
 \nonumber\\
 &=\ & \frac{f_{E\mid \bm{X}}(t\mid \bm{X})}{S_{E\mid \bm{X}}(t\mid \bm{X})}
   \label{def:hazard_function}
% \end{split}
\end{eqnarray} % \end{equation}
represents the instantaneous event rate at given time $t$, conditional on surviving $t$~\cite[Ch. 11]{gareth_introduction_2021}.
\end{definition}

\subsection{Copula-Based Estimation} % 2.2
\label{sec:depCensor}
A \textit{copula} is a function that links two or more random variables by specifying their dependence structure~\citep[Ch. 1]{Emura2018}. 
Among the various types of copulas, the Archimedean copula family is the most used, and enables modeling of dependencies in arbitrarily high dimensions using a single parameter, $\theta$. This family of copulas is defined by a generator function, $\varphi_\theta(t): [0, 1] \to [0, \infty]$, which is continuous, strictly decreasing, and satisfies the boundary conditions $\varphi(0) = \infty$ and $\varphi(1) = 0$~\cite[Ch. 3]{Emura2018}. Here, we will focus on two common Archimedean copulas: Clayton~\hbox{\citep{clayton1978model}} and Frank~\hbox{\citep{frank1979simultaneous}}.

\citet{Zheng1995} generalized the KM estimator under independent censoring to the Copula-Graphic (CG) estimator under dependent censoring, which has become an important tool for analyzing survival data~\citep{Emura2016, Emura2017, Moradian2019}. 
\citet{Rivest2001} obtained the first explicit expression of the CG estimator under an assumed copula from the Archimedean family (\eg Clayton or Frank). We can estimate the survival function using the CG estimator (see Appendix~\ref{app:derivations} for a detailed derivation):
\begin{definition}
The Copula-Graphic (CG) estimator
\begin{equation} \small
\label{def:cg:estimator}
\begin{split}
\hat{S}_E(t) &= \varphi^{-1} \left(\sum_{
% \add[RG]
{i:\,}t_i \leq t,\delta_i=1} 
\varphi\left(\frac{n_i - 1}{n}\right) - \varphi\left(\frac{n_i}{n}\right) \right), \\
\hbox{for} & \quad t\ \leq\ t_{\text{max}} 
   % \in\ [0, \,t_{\text{max}}]\ ,
\end{split}
\end{equation}
where $n_i = \sum_{\ell=1}^n I(t_\ell \geq t_i)$ is the number at risk at time $t_i$; $\hat{S}_E(t) = 1$ if no death occurs up to time $t$; $\hat{S}_E(t)$ is undefined for $t > t_{\text{max}}$~\citep[Ch. 4]{Emura2018}.
\end{definition}
Note that when $\varphi(\cdot)$ is uniform, this reduces to the standard KM estimator. See Appendix~\ref{app:copulas_and_definitions} for more details.

\subsection{Evaluation metrics and limitations} % 2.3
\label{sec:Eval-bias}
We now outline the conventional evaluation metrics in survival analysis. The detailed calculation of these metrics is outlined in Appendix~\ref{app:exist_metrics} and here we mainly focus on why those metrics are problematic under dependent censoring.

\textbf{Concordance index (CI)} evaluates the ranking of individuals when their relative ordering is of primary interest. It measures the proportion of concordant pairs -- pairs where the predicted ranking of risk scores aligns with the true ordering of event times -- among all comparable pairs~\citep{harrell1996multivariable}. A pair $(i,j)$ is defined as \textit{comparable} if $\delta_i = 1$ and $t_i < t_j$, which means that the earlier individual has experienced the event and the observed time of the later individual is greater than the first. Even under the random censoring assumption, it is evident that the comparable pairs selected using the above criteria do not represent all possible pairs based on their true event times: 
\\ \hspace*{3em} $ %\begin{align*} 
\Pr(\delta_i=1, t_i<t_j) \quad \neq\quad \Pr(e_{i}<e_{j}). 
$ \\ % \end{align*} 
This inequality arises because the left-hand term can be decomposed as:\\[-1ex]
$$\begin{array}{l} %\begin{align*} 
\Pr(\, \delta_i=1, \ t_i<t_j) \\
=\quad  \Pr(\, e_{i} < c_{i},\ \ e_{i}\ <\ \min \{ c_{j}, e_{j}\}\ )\\
= \quad \Pr(\, e_{i} < c_{i},\ t_{e,i}< c_{j},\ e_{i}< e_{j})\\
=\quad  \underline{\Pr(\, c_{i} < c_{i})\ \Pr( e_{i}< c_{j})} \quad 
  \Pr( e_{i}< e_{j})\ , 
\end{array} % \end{align*} 
$$
where the factorization follows from the independent censoring assumption. 
Compared to the right-hand side term, this expression includes additional underlined
terms related to the censoring distribution, confirming the findings of~\citet{uno2011}.

To address this limitation, \citet{uno2011} introduced a modified version of CI that employs inverse probability of censoring weights (IPCW), ensuring consistency with the true CI. The IPCW approach estimates the probability of being uncensored at each time point and reweights the uncensored instances accordingly.

\textbf{Integrated Brier Score (IBS)} evaluates the overall accuracy of predicted survival functions by integrating the Brier Score (BS)~\citep{Brier1950} over a range of time points. It measures the average squared difference between the predicted survival probabilities and actual survival outcomes over time~\citep{gerds2006consistent}. To handle censored instances, IPCW can be used to reweight uncensored instances when computing both BS and IBS~\citep{graf1999assessment}. Similar to IPCW-CI, IPCW weights are often approximated using the KM estimator, which is biased under dependent censoring~\citep[Ch. 1]{Emura2018}.

\textbf{Mean Absolute Error (MAE)} quantifies the accuracy of point predictions for event times by measuring the absolute difference between the predicted and true event times. For censored instances, \citet{haider_effective_2020} proposed an approach that approximates that subject's event time as the mean of the KM estimator (Equation~\ref{def:km_estimator}), renormalized at that individual's censoring time. This proxy event time, weighted by confidence estimates (also derived from KM), is then used to compute the absolute error. We will refer to this method as MAE-Margin. Given a subject is censored at time $c_i$, its margin time is calculated as:
\begin{equation}\label{eq:e_margin}
e^{\text{margin}}(\hat{S}, t_i) = \mathbb{E}[E \mid E > t_i] = t_i + \frac{\int_{t_i}^{\infty} \hat{S}_{}(t) \, dt}{\hat{S}(t_i)},
\end{equation}
\noindent where $\hat{S}(t)$ is an estimator, \eg the KM, derived from the training dataset.

\textbf{Limitations.} All the aforementioned metrics typically rely on KM estimates to approximate either the marginal time-to-event or the marginal time-to-censoring distribution. While KM is advantageous due to its consistency under the independent censoring assumption and its computational simplicity -- requiring no interaction with the covariates -- it becomes problematic when censoring and event times are dependent~\citep[Ch. 1]{Emura2018}. 
In such cases, the KM estimates may be biased and inconsistent~\citep{campigotto2014impact}. 
To mitigate these issues, researchers have proposed modifying these metrics by replacing KM with a conditional estimator (\eg~CoxPH) to account for the dependency between covariates and censoring distributions~\citep{gerds2006consistent}. 
However, this approach still has two major limitations: (1)~\textit{model misspecification:} if the conditional estimator is inaccurately specified, the resulting estimates may be unreliable, 
and 
(2)~\textit{uncaptured dependencies:} under dependent censoring (\ie~when unobserved confounders exist), even a well-specified conditional estimator may fail to fully account for the dependency if there are important covariates missing in the analysis. These challenges highlight the need for robust estimation methods that can better handle dependent censoring scenarios.

\section{The Proposed Metrics}

\CIDep:
We propose a modified version of Harrell's CI~\citep{Harrell1982} that employs inverse probability weighting (\ie IPCW), but the marginal survival probability for censoring at time $t$ is estimated using the CG estimator under an approximated Archimedean copula, parameterized by $\theta$. 
When $\theta \approx 0$, it yields the independence copula, which is equivalent to using the KM estimator as in~\citet{uno2011}.
Our proposed concordance index thus requires a known Archimedean copula, that models the dependency between marginals $E$ and $C$, but does not require interaction with the covariates. Given an estimated risk function $\hat{\eta}(\cdot)$ and corresponding labels, the traditional CI is computed as:
\begin{equation}\small
\label{eq:ci_harrell}
\begin{split}
\text{CI} (\mathcal{D}; \, \hat{\eta}(\cdot))= \frac{\sum_{i,j}  \delta_i  \cdot \mathbbm{1}[t_i < t_j] \cdot 
\mathbbm{1}[\hat{\eta} (\bm{x}_i) > \hat{\eta} (\bm{x}_j)] }{
\sum_{i,j} \delta_i \cdot \mathbbm{1}[t_i < t_j] }.
\end{split}
\end{equation}
Now, employing the IPCW technique used by~\citet{uno2011}, but under dependent censoring, let $\hat{G}_{\text{CG}}(t) = \Pr(C > t)$ represent the marginal survival probability for censoring at $t$ estimated by the CG estimator. We can then reweight Equation \ref{eq:ci_harrell} using the CG estimator as:
\begin{equation}\small
\begin{split}
&\text{CI}_{\text{Dep}} (\mathcal{D}; \, \hat{\eta}(\cdot)) \ = \\ 
&\frac{\sum_{i,j}  \delta_i  \cdot \{\hat{G}_{\text{CG}}(t_i)\}^{-2}\cdot\mathbbm{1}[t_i < t_j] \cdot \mathbbm{1}[
\hat{\eta} (\bm{x}_i) > \hat{\eta} (\bm{x}_j)] }{\sum_{i,j} \delta_i \cdot \{\hat{G}_{\text{CG}}(t_i)\}^{-2}\cdot \mathbbm{1}[t_i < t_j] } ,
\end{split}
\end{equation}
which denotes the proposed \hbox{\CIDep}. Here, the weight is given by $1/\hat{G}_{\text{CG}}(t)$.

\IBSDep: We propose a modified version of the Brier Score~\citep{graf1999assessment} under dependent censoring using margin time estimates as the censoring weights. The margin or "best-guess" time can be interpreted as a conditional expectation of the event time given the event time is greater than the censoring time. Given an instance is censored at time $t_i$, we can calculate its margin time using Equation \ref{eq:e_margin}, but with the CG estimator instead of the KM, \ie $e^{\text{margin}}(t_i) = e^{\text{margin}}(\hat{S}_{\text{CG}}, t_i)$, where $\hat{S}_{\text{CG}}$ is the marginal survival probability estimated using the CG estimator. We then formulate the dependent Brier Score as:
\begin{equation}
\begin{array}{l}
\text{BS}_{\text{Dep}} \left( \mathcal{D}; \, \hat{S}(\cdot), t^* \right) \\
= \frac{1}{N} \sum_{i=1}^N \Bigg( \mathbbm{1} [\tilde{e}_i > t^*]  -  \hat{S}(t^* \mid \bm{x}_{i}) \Bigg)^2.
\end{array}
\label{eq:dep_bs}
\end{equation}
\begin{gather*}
\text{where} \quad
\tilde{e}_i =
\begin{cases}
e_i & \text{if}\quad \delta_i=1\\    
e_i^{\text{margin}} (t_i)  & \text{elsewhere.}
\end{cases}
\end{gather*}
To calculate the expectation of 
Equation~\ref{eq:dep_bs} over all times, 
we simply take the integral from $t=0$ to $t=t_{\text{max}}$
% Don't need as not used: \label{eq:dep_ibs}
\begin{equation}
\begin{array}{l} % {split}
\text{IBS}_{\text{Dep}} \left(\mathcal{D}; \, \hat{S}(\cdot) \right)
 \ = \ \frac{1}{t_{\text{max}}}  \cdot \int_0^{t_{\text{max}}} \text{BS}_{\text{Dep}}\left(\mathcal{D}; \, \hat{S}(\cdot),  \tau \right) d\tau, 
\end{array} % {split}
\end{equation} 
where $t_{\text{max}}$ is usually the maximum observed time.

\MAEDep: We propose a modified version of the MAE-Margin~\citep{haider_effective_2020} under dependent censoring. Again, we make use of the CG estimator to calculate margin times for the censored instances (Equation \ref{eq:e_margin}) and replace the censored event times with their "best-guess"-estimates. Furthermore, for each censored instance, we adopt the approach from \citet{haider_effective_2020} and assign a confidence weight $\omega_i = 1 - \hat{S}_{\text{\text{CG}}}(t_{i})$ to the error based on the margin value estimated using the CG method. For uncensored subjects, we set $w_i = 1$. 
The dependent MAE is thus:
% \begingroup\small \begin{equation} \begin{split}
\begin{eqnarray}\small
&&   \hspace*{-3em} \text{MAE}_{\text{Dep}} \left(\mathcal{D}; \, \hat{t}_i \right) = 
    \frac{1}{\sum_{i=1}^{N} \omega_i} 
    \sum_{i =1}^{N} \omega_i  
    \Big| \big[(1 - \delta_i) \cdot e^{\text{margin (CG)}}(t_i) \notag  \\
   &&\qquad +\quad  \delta_i \cdot t_i \big] - \hat{t}_i \Big| \ .
\end{eqnarray} 
% \end{split} \end{equation} \endgroup

\section{Experiments and results} % 4
\label{sec:Exp-Res}
For our empirical analyses, we evaluate the effectiveness of the proposed metrics under dependent censoring in synthetic datasets under a known copula and in semi-synthetic datasets by learning the copula based on the data.

\subsection{Experimental Setup}

\begin{figure*}[t] % !htbp]
\centering
\includegraphics[width=1\textwidth]{Figures/evaluation_pipeline.pdf}
\vspace*{-2em}
\caption{Outline of the proposed evaluation pipeline. Based on some raw, survival dataset $\mathcal{D}$, we (1) form $\tilde{\mathcal{D}}$ by flipping the event bit of $\mathcal{D}$ and then estimate the feature-dependent censoring distribution, $G_{\text{CoxPH}(\tilde{\mathcal{D}})}$. We then (2) select a subset of features from $\mathcal{D}$ using some strategy (\eg Top-K features), and (3) generate a ground-truth dataset, $\mathcal{D}'$, by removing the censored instances from $\mathcal{D}$. We then (4) create $\mathcal{D}''$ based on the ground-truth dataset, the learned censoring distribution, and the selected features. Using $\mathcal{D}''$, we (5) train two copula models, \ie Clayton and Frank, and use AIC to select the copula $C_\theta$ that best suits the data. At the same time, we (6) train 5 survival learners independent of the copula on $\mathcal{D}''$ to produce survival predictions. Lastly, we calculate the (censored) evaluation error and (7) compare this to the true error.}
\label{fig:evaluation_pipeline}
\end{figure*}

\textbf{Synthetic datasets.} 
Following the approach of~\citet{foomani_copula-based_2023}, we create a synthetic survival dataset from a data generation process (DGP). The event and censoring distributions are specified by linear Weibull marginal distributions. Specifically, we sample R.V.s $E$ and $C$ from a known copula given Kendall's $\tau$; if $\tau = 0$, the marginals are independent, otherwise if $\tau > 0$, the marginals are from an Archimedean copula, \eg Clayton or Frank, known at evaluation time. See Appendix~\ref{app:data_generation_and_processing} for more details.

\textbf{Semi-synthetic datasets.} To evaluate our proposed metrics, we need to know the true metric, which requires precise knowledge of when each instance experiences the event. Due to censoring, event times are not available in real-world datasets for all instances, so we create a realistic, semi-synthetic dataset, where the covariates, event distribution, and censoring distribution align with some real-world datasets. This approach is inspired by \citet{qi_effective_2023}, who proposed a semi-synthetic framework to generate different kinds of censoring distributions. Figure~\ref{fig:evaluation_pipeline} shows the pipeline to the create semi-synthetic datasets.

In summary, based on some real-world survival dataset $\mathcal{D}$, we calculate the censoring distribution $G_{\text{CoxPH}(\tilde{\mathcal{D}})}$, create $\mathcal{D'}$ by removing the censored instances from $\mathcal{D}$, and then apply the censoring distribution to $\mathcal{D'}$, so that for each instance, that instance is \textit{synthetically} censored if the censoring time is earlier than the event time ($\tilde{c}_{i} < t_i$), and otherwise is left uncensored. We perform a feature selection step on $\mathcal{D}$ to induce dependent censoring using one of three strategies:

\begin{itemize}
    \item Original, where the original feature set is kept intact.
    \item Top-$k$, where we select the $k$ most important features from $\mathcal{D}$ based on their predictive power. We train a CoxPH model on $\mathcal{D}$ and then use permutation feature importances~\citep{Breiman2001} to select the $k$ best features. Feature importances are calculated by shuffling feature values and observing how much the model's performance degrades, using Harrell's CI.
    \item Random-\%, where we select a random proportion of \%-pct features from the original feature set. In a small dataset, randomly selecting 25\% of the features can introduce significant residual dependence if key features are excluded. In contrast, in a high-dimensional dataset, this risk is lower as many important features are likely to remain.
\end{itemize}

Finally, we have $\mathcal{D}''$: a dataset with a reasonable, but synthetic censoring distribution with a restricted feature set and where the ground-truth event times are known. This will allow us to compare the proposed and existing (censored) metrics with the ground-truth metrics.

\textbf{Copula model.} To validate our dependent metrics in semi-synthetic datasets, we train two copula models, Clayton and Frank, and two nonlinear Weibull proportional-hazards models for the marginal distributions, \(E\) and \(C\), respectively, on $\mathcal{D}''$. The Weibull models use a multilayer perceptron (MLP) as the backbone network architecture with a single hidden layer and the nonlinear ReLU activation function. For each copula model, we train the marginals and the copula $C_\theta$ jointly using maximum likelihood estimation (MLE) (see Appendix~\ref{app:implementation_details} for details). After training the two candidate models, we choose the model that minimizes the Akaike Information Criterion (AIC), given its loss on the validation set and number of parameters. This offers a practical, data-driven approach to copula selection~\citep[Ch. 4]{Emura2018} by selecting the copula model with the lowest validation loss. We then use the best copula $C_\theta$ for the evaluation of the dependent metrics.

\textbf{Datasets.} We apply the evaluation pipeline in Figure \ref{fig:evaluation_pipeline} to 6 large, real-world datasets: METABRIC~\citep{Curtis2012}, 2 from MIMIC-IV (all causes and hospital death)~\citep{johnson_mimic_2023}, and 3 from SEER (death from brain, liver and stomach cancer)~\citep{ries2007seer}. They differ in their number of instances, features, and censoring rates. Appendix~\ref{app:data_generation_and_processing} gives further details on the datasets and describes how we preprocessed them.

\textbf{Survival learners.}
We compare the prediction results using 5 survival learners produced by the CoxPH~\citep{cox_regression_1972}, gradient boosting (GBSA)~\citep{Ridgeway1999}, Random Survival Forest (RSF)~\cite{ishwaran_random_2008}, DeepSurv~\cite{katzman_deepsurv_2018} and Multi-Task Logistic Regression (MTLR)~\citep{NIPS2011_1019c809}. These learners predict individual survival distributions (ISDs) for each instance $i$. See Appendix~\ref{app:survival_learners} for further details, including implementation details and hyperparameters.

We train the five survival learners across 5 experiments given a random seed (0-4). In each experiment, we follow~\citet{Sechidis2011} and first split the dataset into stratified training (70\%), validation (10\%), and test (20\%) sets. After splitting, we impute missing values using sample mean for real-valued covariates or mode for categorical covariates based on the training set only. We then normalize numerical values and encode categorical features using a one-hot encoding strategy. All results are reported in the test sets.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=1\linewidth]{Figures/metric_error_over_ktau_10000_10.pdf}
\caption{Plot of the bias ($Y$-axis) -- \ie the proximity to the true error -- of established baseline metrics and the proposed dependent metrics as a function of the dependency ($X$-axis) under a known copula. We plot the mean biases ($\pm$ SD.) in the test sets across 100 experiments in a synthetic dataset ($N=10,000$, $d=10$). Overall, our dependent metrics show lower biases when dependency is notable ($\tau > 0.25$) than the independent metrics. See Appendix~\ref{app:data_generation_and_processing} for details.}
\label{fig:metric_error_over_ktau}
\end{figure*}

\subsection{Results and Takeaways} % 4.2
\label{sec:Res-Take}
\textbf{Synthetic data.} 
Figure~\ref{fig:metric_error_over_ktau} shows the evaluation bias
-- \ie the proximity to true errorc
-- for four baseline metrics (Harrell's CI, Uno's CI, IPCW-integrated Brier Score, and MAE-Margin) and their dependent variants as a function of Kendall's $\tau$. Assuming a known ground-truth copula (Archimedean family and dependency strength), we estimate $\hat{S}(t)$ using a CoxPH model to predict $N$ ISDs, where the risk scores are the negative predicted survival times.
Overall, our dependent metrics show lower bias
-- \ie closer proximity to the true error --
when dependency is notable ($\tau > 0.25$). 
For the ranking metrics (Harrell's CI, Uno's CI), biases are similar under low dependency ($\tau < 0.25$) but improve significantly under high dependency ($\tau > 0.5$). For accuracy metrics (IPCW-integrated Brier Score, MAE-Margin), our dependent variants consistently provide better error estimates regardless of dependency or copula family.

As a practical example, consider a cancer study, with high dependent censoring ($\tau = 0.75$) from a Clayton copula between event time $E$ (cancer relapse) and censoring time ${C}$ (cancer death). Based on Figure~\ref{fig:metric_error_over_ktau}, if the true CI of a CoxPH model is 0.65, then, 
if we evaluate the model using Harrell's CI, we anticipate the CI to be 0.68 ($\pm$ 0.03). If we use \hbox{\CIDep}, we anticipate the CI to be 0.67 ($\pm$ 0.02).

\textbf{Semi-synthetic data.} 
Table~\ref{tab:semi_synthetic_experiments} presents the results in 3 semi-synthetic datasets: METABRIC, MIMIC-IV and SEER. The complete results for all 6 datasets are in Appendix \ref{app:complete_synthetic_data}. In the small-sized METABRIC dataset, Harrell's gives superior estimates of the true concordance index, and there is a performance degradation in using the dependent variant. We see that \hbox{\CIDep} degrades the performance compared to Uno's CI, in this case, while \hbox{\IBSDep} improves on the IPCW version, and MAE-Margin and \hbox{\MAEDep} give similar results. In the medium-sized MIMIC-IV dataset, Harrell's CI is again better, but our {\IBSDep} and \hbox{\MAEDep} variants improve on the IBS-IPCW and margin MAE, respectively, in all cases, however, the pseudo MAE is the best (\ie it has the lowest error). In the large-sized SEER dataset, there are few differences in discriminative performances for the three metrics, but our \hbox{\IBSDep} improves on IPCW, while our \hbox{\MAEDep} improves on pseudo and margin. More often than not, Harrell's and Uno's CI (IPCW) give better estimates than our \hbox{\CIDep} in these datasets, while our \hbox{\IBSDep} and \hbox{\MAEDep} give notable improvements to their independent counterparts. See Appendix~\ref{app:complete_synthetic_data} for additional results.

\begin{table*}[tb] % [!htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{clcccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Strategy}} & \multicolumn{3}{c}{\textbf{Discrimination (CI)}} & \multicolumn{2}{c}{\textbf{Accuracy (IBS)}} & \multicolumn{5}{c}{\textbf{Accuracy (MAE)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-7} \cmidrule(lr){8-12}
& & Harrell's & Uno's & Dependent & IPCW & Dependent & Uncens. & Hinge & Pseudo & Margin & Dependent \\
\midrule
\multirow{4}{*}{\makecell{METABRIC \\ ($N$=1,102, $C$=3.8\%)}}
& Original & \textbf{0.10$\pm$\scriptsize{0.08}} & 0.39$\pm$\scriptsize{0.14} & 0.48$\pm$\scriptsize{0.25} & 1.23$\pm$\scriptsize{0.85} & \textbf{1.17$\pm$\scriptsize{0.85}} & 3.24$\pm$\scriptsize{0.55} & 1.40$\pm$\scriptsize{0.21} & \textbf{0.25$\pm$\scriptsize{0.21}} & 0.31$\pm$\scriptsize{0.23} & 0.33$\pm$\scriptsize{0.22} \\
& Top 5 & \textbf{0.11$\pm$\scriptsize{0.08}} & 0.51$\pm$\scriptsize{0.17} & 0.58$\pm$\scriptsize{0.23} & 1.22$\pm$\scriptsize{0.85} & \textbf{1.17$\pm$\scriptsize{0.85}} & 3.21$\pm$\scriptsize{0.55} & 1.40$\pm$\scriptsize{0.21} & \textbf{0.25$\pm$\scriptsize{0.20}} & 0.31$\pm$\scriptsize{0.23} & 0.30$\pm$\scriptsize{0.23} \\
& Top 10 & \textbf{0.10$\pm$\scriptsize{0.08}} & 0.43$\pm$\scriptsize{0.11} & 0.49$\pm$\scriptsize{0.16} & 1.23$\pm$\scriptsize{0.85} & \textbf{1.17$\pm$\scriptsize{0.85}} & 3.26$\pm$\scriptsize{0.54} & 1.40$\pm$\scriptsize{0.21} & \textbf{0.25$\pm$\scriptsize{0.21}} & 0.32$\pm$\scriptsize{0.23} & 0.31$\pm$\scriptsize{0.23} \\
& Rand. 25\% & \textbf{0.10$\pm$\scriptsize{0.08}} & 2.34$\pm$\scriptsize{0.68} & 2.31$\pm$\scriptsize{0.70} & 1.25$\pm$\scriptsize{0.88} & \textbf{1.18$\pm$\scriptsize{0.88}} & 3.45$\pm$\scriptsize{0.48} & 1.40$\pm$\scriptsize{0.21} & \textbf{0.27$\pm$\scriptsize{0.22}} & 0.34$\pm$\scriptsize{0.23} & 0.33$\pm$\scriptsize{0.23} \\
\cmidrule(lr){1-1}
\multirow{4}{*}{\makecell{MIMIC-IV (hospital) \\ ($N$=6,780, $C$=74.3\%)}}
& Original & \textbf{0.74$\pm$\scriptsize{0.43}} & 1.27$\pm$\scriptsize{0.83} & 2.10$\pm$\scriptsize{1.54} & 8.75$\pm$\scriptsize{1.50} & \textbf{6.57$\pm$\scriptsize{1.44}} & 3.48$\pm$\scriptsize{0.25} & 6.44$\pm$\scriptsize{0.27} & \textbf{3.11$\pm$\scriptsize{0.27}} & 4.54$\pm$\scriptsize{0.26} & 4.36$\pm$\scriptsize{0.24} \\
& Top 5 & \textbf{0.79$\pm$\scriptsize{0.55}} & 1.16$\pm$\scriptsize{0.56} & 3.36$\pm$\scriptsize{2.74} & 8.80$\pm$\scriptsize{1.51} & \textbf{6.42$\pm$\scriptsize{1.45}} & 3.51$\pm$\scriptsize{0.24} & 6.45$\pm$\scriptsize{0.26} & \textbf{3.10$\pm$\scriptsize{0.26}} & 4.65$\pm$\scriptsize{0.25} & 4.33$\pm$\scriptsize{0.18} \\
& Top 10 & \textbf{0.71$\pm$\scriptsize{0.47}} & 1.10$\pm$\scriptsize{0.65} & 2.46$\pm$\scriptsize{1.69} & 8.73$\pm$\scriptsize{1.50} & \textbf{6.57$\pm$\scriptsize{1.44}} & 3.50$\pm$\scriptsize{0.27} & 6.45$\pm$\scriptsize{0.27} & \textbf{3.11$\pm$\scriptsize{0.27}} & 4.60$\pm$\scriptsize{0.28} & 4.40$\pm$\scriptsize{0.25} \\
& Rand. 25\% & \textbf{1.44$\pm$\scriptsize{0.58}} & 1.77$\pm$\scriptsize{1.02} & 2.70$\pm$\scriptsize{1.44} & 10.38$\pm$\scriptsize{0.76} & \textbf{7.91$\pm$\scriptsize{0.90}} & 3.57$\pm$\scriptsize{0.20} & 6.51$\pm$\scriptsize{0.19} & \textbf{3.06$\pm$\scriptsize{0.19}} & 4.79$\pm$\scriptsize{0.19} & 4.62$\pm$\scriptsize{0.20} \\
\cmidrule(lr){1-1}
\multirow{4}{*}{\makecell{SEER (liver) \\ ($N$=51,704, $C$=20.5\%)}}
& Original & 0.77$\pm$\scriptsize{0.10} & \textbf{0.57$\pm$\scriptsize{0.14}} & 0.76$\pm$\scriptsize{0.13} & 0.33$\pm$\scriptsize{0.15} & \textbf{0.25$\pm$\scriptsize{0.11}} & 2.59$\pm$\scriptsize{0.07} & 2.50$\pm$\scriptsize{0.04} & 2.47$\pm$\scriptsize{0.07} & 1.48$\pm$\scriptsize{0.06} & \textbf{1.44$\pm$\scriptsize{0.05}} \\
& Top 5 & 1.86$\pm$\scriptsize{0.12} & \textbf{1.70$\pm$\scriptsize{0.15}} & 1.76$\pm$\scriptsize{0.13} & \textbf{0.28$\pm$\scriptsize{0.14}} & 0.47$\pm$\scriptsize{0.21} & 2.80$\pm$\scriptsize{0.05} & 2.69$\pm$\scriptsize{0.06} & 2.25$\pm$\scriptsize{0.07} & 1.25$\pm$\scriptsize{0.06} & \textbf{0.85$\pm$\scriptsize{0.47}} \\
& Top 10 & \textbf{0.81$\pm$\scriptsize{0.08}} & 0.90$\pm$\scriptsize{0.09} & 1.10$\pm$\scriptsize{0.07} & 0.33$\pm$\scriptsize{0.15} & \textbf{0.25$\pm$\scriptsize{0.11}} & 2.61$\pm$\scriptsize{0.08} & 2.51$\pm$\scriptsize{0.04} & 2.46$\pm$\scriptsize{0.06} & 1.47$\pm$\scriptsize{0.05} & \textbf{1.43$\pm$\scriptsize{0.05}} \\
& Rand. 25\% & \textbf{0.89$\pm$\scriptsize{0.52}} & 1.80$\pm$\scriptsize{1.23} & 1.68$\pm$\scriptsize{1.21} & 0.45$\pm$\scriptsize{0.21} & \textbf{0.35$\pm$\scriptsize{0.17}} & 2.75$\pm$\scriptsize{0.11} & 2.49$\pm$\scriptsize{0.08} & 2.50$\pm$\scriptsize{0.14} & 1.49$\pm$\scriptsize{0.09} & \textbf{1.47$\pm$\scriptsize{0.11}} \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of established survival metrics and the proposed dependent metrics in terms of the mean ($\pm$ SD.) bias in the semi-synthetic test sets, averaged over 5 learners and 5 experiments. CI and IBS results are multiplied by 100.}
\label{tab:semi_synthetic_experiments}
\end{table*}

In general, a (censored) metric should be representative of the true metric, \ie it should tell us which model makes the best survival predictions. Therefore, we create a new experiment as follows:
\begin{itemize}
    \item[1.] In our semi-synthetic datasets, we are aware of the ground-truth (the actual event times) of all instances. For each model, we use this information to calculate the \textbf{True CI}, \textbf{True IBS} and \textbf{True MAE} results – that is, how good are the survival predictions using each metric, if all the instances were observed (not censored).
    \item[2.] For each true metric, we then rank the top-3 survival models – e.g., CoxPH, RSF and DeepSurv – by their true metric. In this example, CoxPH has the highest \textbf{True CI}, then RSF and then DeepSurv.
    \item[3.] For each censored metric (proposed and existing), we check if the metric gives the same ordering of survival models as its corresponding true metric – \eg if \textbf{True CI} indicates that CoxPH, RSF and DeepSurv are the three best models, does \hbox{\CIDep} indicate the same? This experiment is repeated 5 times with different data splits.
\end{itemize}
Figure~\ref{fig:ranking_error_metabric} shows the number of times the resp. metric correctly (green dot) or incorrectly (red dot) identified the ordering of the top-3 survival learners in the METABRIC test set. See Appendix \ref{app:complete_results} for results in all semi-synthetic datasets.

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\linewidth]{Figures/ranking_error_metabric.pdf}
\caption{Plot the ranking accuracy, \ie the number of times the resp. metric correctly (green dot) or incorrectly (red dot) identified the true ordering of the top-3 survival learners, out of 5 experiments in the METABRIC test sets.}
\label{fig:ranking_error_metabric}
\end{figure*}

\section{Discussion}

\textbf{How do the Metrics Work?}
As we do not have a formal proof of identifiability -- \ie we do not know the true relationship between $E$ and $C$ -- we therefore study this question empirically by approximating $C_\theta$ and making a comparison between conventional metrics and our dependent variants. While the advantages to adopting our dependent metrics are evident in synthetic data, where we know the ground-truth copula and dependency level, these advantages vary with the dataset and its underlying distribution in the semi-synthetic datasets. Based on our observations, the most consistent trend is seen in the proposed \hbox{\MAEDep} –- which is directly comparable to the MAE-Margin. Specifically, the \hbox{\MAEDep} always outperforms its independent counterpart when censoring exceeds 20\%. In these scenarios, \hbox{\MAEDep} yields lower mean biases across five repeated experiments on all four dataset variants with over 20\% censoring. This improvement is encouraging, as it holds across datasets with possibly varying characteristics and censoring mechanisms. However, when the censoring rate falls between 0\% and 20\%, the trend reverses slightly: the \hbox{\MAEDep} tends to be marginally higher than the MAE-Margin.

\textbf{What are the Limitations?}
Our metrics rely on a known or approximate copula to adjust for dependent censoring, but estimating the copula is inherently difficult due to the non-identifiability of competing-risks data~\citep{Tsiatis1975}. This poses a key limitation for applying our method to real-world datasets. However, there exist several strategies to approximate the copula using observed data: \cite{Emura2016} proposed to divide the $N$ instances into $K$ groups of approximately equal sample sizes, and for each instance $i \in K$, define a prognostic index, $\text{PI}_{i}(C_\theta, \bm{x}_i, W)$, which predicts the outcome for $i$, where $W$ are the model parameters. Using cross-validation, we pick the copula $\hat{\theta}$ which maximizes the CI over possible values of Kendall's $\tau$. \cite{parikh2022validating} and \cite{mahajan2024empirical} proposed generating counterfactual data with characteristics similar to the observed data and evaluate the model performance on the simulated data using counterfactual metrics (such as the Survival-$\ell_1$ by \cite{foomani_copula-based_2023}). \cite{foomani_copula-based_2023} proposed to first get an intuition about the underlying copula (\eg Clayton, Frank) by plotting the event and censoring distributions to visually inspect the curves and then go on to approximate $C_\theta$ using MLE.

\section{Conclusion}
\label{sec:conclusion}

In this paper, we have proposed three copula-based metrics for evaluating survival models under dependent censoring. We empirically compared them to existing, independent variants in both synthetic and semi-synthetic datasets. We found that in datasets with adequate instances and censoring, the dependent metrics gave error estimates that were closer to the true error for prediction accuracy, \ie Brier Score and MAE. We anticipate that others will be able to use these metrics and improve upon our framework.

\clearpage

\begin{acknowledgements}

This research received support from the Natural Science and Engineering Research Council of Canada (NSERC), the Canadian Institute for Advanced Research (CIFAR), and the Alberta Machine Intelligence Institute (Amii).

\end{acknowledgements}

% References
\bibliography{references}

\onecolumn

\title{Practical Evaluation of Copula-based Survival Metrics:\\ Beyond the Independent Censoring Assumption \\(Supplementary Material)}

\maketitle

\appendix
\section{Overview of the appendices}\label{app:overview}

Appendix~\ref{app:notation} provides an overview of the notation. 
Appendix~\ref{app:censoring_assumptions} describes the main censoring assumptions. 
Appendix~\ref{app:copulas_and_definitions} describes the theory behind copulas. 
Appendix~\ref{app:exist_metrics} describes the existing survival metrics. 
Appendix~\ref{app:experimental_details} gives details on data generation, preprocessing, survival learners and implementation. Appendix~\ref{app:complete_results} contains the complete results for the datasets. Appendix~\ref{app:derivations} provides derivations of used estimators.

\section{Notation}\label{app:notation}
Table~\ref{tab:notation} provides a list of notations used in this work. 

\begin{table}[!h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Symbol/Abbr.} & \textbf{Definition} \\
\midrule
$c_i$ & Censoring time of instance $i$ \\
$e_i$ & Event time of instance $i$ \\
$t_{i}$ & Observed time of instance $i$ \\
$\hat{t}_{i}$ & Predicted time of instance $i$ \\
$\delta_{i}$ & Event bit/indicator, $\delta_{i} = \mathbbm{1} [e_{i} > c_{i}]$ \\
$f_E$ & Probability density function, representing $\Pr(E = t)$ \\
$F_E$ & Cumulative density function, representing $\Pr(E < t)$ \\
$S_E$ & Survival function, $S: \mathbb{R} \rightarrow [0, 1]$ \\
$\mathcal{D}$ & Raw dataset\\
$\mathcal{D'}$ & Raw dataset with only uncensored subjects\\
$\mathcal{D''}$ & Semi-synthetic dataset with synthetic censoring on $\mathcal{D'}$\\
$\tilde{\mathcal{D}}$ & Raw dataset with flipped event bit\\
$N$ & Number of observations in the dataset \\
$d$ & Number of features/covariates in the dataset \\
$C$ & Censoring rate (\%) in the dataset \\
$\bx_{i}$ & Covariates of instance $i$ \\
$G(t)$ & Censoring distribution \\
$G_{CoxPH}(t\mid\bm{X})$ & Feature-dependent censoring distribution, estimated using CoxPH \\
$h_0(t)$ & Baseline hazard function \\
$h(\cdot \mid \bx_{i})$ & Hazard function given the covariates $\bx_{i}$ \\
$\mathcal{L}$ & Objective/loss function \\
$C_{\theta}$ & Copula parameterized by $\theta$ \\
$u_1, u_2$ & Inputs to a copula function \\
\bottomrule
\end{tabular}
\caption{Table of notation.}
\label{tab:notation}
\end{table}

\section{Censoring assumptions}\label{app:censoring_assumptions}

\textbf{Random censoring.} We can define random censoring as follows: "Subjects who are censored at time t should be representative of all the study subjects who remained at risk at time t with respect to their survival experience."~\citep[Ch. 1]{kleinbaum2012survival}. In other words, we assume that the death rate (or failure rate) of censored subjects is the same as that of uncensored subjects who remain in the risk set. Random censoring is a stronger assumption and more restrictive than what is called \textit{independent censoring}, which states that censoring is independent within a subgroup.

\begin{definition}
Random censoring
\begin{equation}
\label{def:random_censoring}
h_{\text{CE}}(t) = h_{\text{NCE}}(t)
\end{equation}
where $h_{\text{CE}}$ is the hazard rate for the censored subjects, and $h_{\text{NCE}}$ is the hazard rate for the uncensored subjects.
\end{definition}
\textbf{Independent censoring.} In the absence of competing risks, we can define independent censoring as follows: "Within any subgroup of interest, subjects who are censored at time $t$ should be representative of all subjects in that subgroup who remained at risk at time $t$ with respect to their survival experience"~\citep[Ch. 1]{kleinbaum2012survival}. In other words, censoring is independent provided that it is \textit{random} within any subgroup given the covariates.

\begin{definition}
Independent censoring
\begin{equation}
\label{def:independent_censoring}
\begin{aligned}
h_{A,\text{CE}}(t) &= h_{A,\text{NCE}}(t) \\
h_{B,\text{CE}}(t) &= h_{B,\text{NCE}}(t)
\end{aligned}
\end{equation}
where $A$ and $B$ are subgroups, \eg patients given treatment A and patients given treatment B.
\end{definition}

The idea behind random and independent censoring is that, even though the censored subjects were not randomly selected, their survival experience would be expected to be the same as if they had been randomly selected from the risk set at time $t$. Consider a clinical trial studying the recovery time of patients after surgery. The patients' recovery times are measured, but some individuals leave the study early for reasons unrelated to their recovery, such as relocating to another city, personal commitments, or withdrawal for reasons not related to their health (\eg family obligations). These individuals are censored, but their reason for leaving the study has nothing to do with the outcome (recovery time). Many of the analytic techniques in survival analysis, \eg the Kaplan-Meier~\citep{kaplan_nonparametric_1958} and Nelson-Aalen~\citep{nelson_1969_hazard, aalen_1978_nonparametric} population estimators, and the semiparametric Cox Proportional Hazards (CoxPH) model~\citep{cox_regression_1972}, rely on the assumption of independent censoring for valid inference in the presence of right-censored data~\citep[Ch. 1]{kleinbaum2012survival}. When censoring is not independent, however, this can introduce bias in survival analysis. Bias arises when censored individuals have a higher likelihood of experiencing the event compared to those who are not censored. Consequently, if a significant portion of censored individuals actually had the event (\eg died), the estimated survival probability at any time $t$ may overestimate the true survival probability at that time. By example of \citet[Ch. 1]{kleinbaum2012survival}, consider a drug study where some patients are censored due to the occurrence of side effects. The unobserved survival outcomes of those censored because of side effects may not be representative of those who remain in the study. If patients who experience side effects are more susceptible to the health outcome, \eg death, assuming independent censoring could lead to an overestimation of their survival rate.

\textbf{Non-informative censoring.} Censoring can be non-informative if the distribution of survival times ($E$) provides no information about the distribution of censoring times ($C$), and vice versa. Otherwise, the censoring is informative. However, we would still need to know which subjects are censored or not censored. The assumption of non-informative censoring is often justifiable when censoring is independent and/or random~\citep[Ch. 1]{kleinbaum2012survival}, but cases exist where the censoring is informative but also random and independent: Consider a cohort study, where each time a patient experiences an event (\eg death), another patient in the study is randomly selected to be censored. This could be a family member who decides to leave the study after an event occurs. If those who are censored are representative of those who remain in the risk set, then the censoring would be random and independent. However, in this case, the censoring would also be informative, as the censoring mechanism would be linked to the time-to-event distribution (since events trigger censoring). In fact, if this were the only mechanism by which individuals were censored, the distribution of survival times would entirely determine the distribution of censoring times, making it highly informative.

\section{Copulas and Definitions}\label{app:copulas_and_definitions}

We can view bivariate survival analysis as a \textit{competing risks} problem, where the occurrence of an event (\eg death) and censoring (\eg dropout) are mutually-exclusive incidents. \citet{Tsiatis1975} called this the \textit{identifiability issue}; competing risks data do not allow one to observe two event times simultaneously (one event censors the other), and hence, the data may not identify the dependence structure between event times. \citet{Zheng1995} gave a partial solution to the identifiability problem of dependent censoring by an assumed \textit{copula} between two event times. A copula is a function that links two or more random variables by specifying their dependence structure~\citep[Ch. 1]{Emura2018}. A mathematician, Abe Sklar, first used the word "copula" in his study of probabilistic metric space~\citep{Sklar1959}. In his paper, he gave a mathematical definition of copulas and established the most fundamental theorem about copulas, known as Sklar’s theorem~\citep{Sklar1959}. It demonstrated that any joint cumulative density can be written in terms of a copula over the quantiles of its marginal cumulative densities. Among the various types of copulas, the Archimedean copula is the most used, and enables the modeling of dependencies in arbitrarily high dimensions using a single parameter. This family of copulas is defined by a convex generator function, $\varphi: [0, 1] \to [0, \infty]$, which is continuous, strictly decreasing, and satisfies the boundary conditions $\varphi(0) = \infty$ and $\varphi(1) = 0$~\cite[Ch. 3]{Emura2018}. Given event time $E$ and censoring time $C$, we can define an Archimedean copula as:
\begin{equation}\label{eq:archimedeanc_copula}
\begin{split}
P(E > t, C > c) = \varphi^{-1} \big[ \varphi\{S_E(t)\} + \varphi\{S_C(t)\} \big],
\end{split}
\end{equation}
\noindent where $\varphi$ is the generator function, and $S_E(t) = P(E > t)$ and $S_C(t) = P(C > t)$ are the marginal survival functions. In the bivariate case, $\varphi$ describes the dependency structure (\ie correlation) between two random variables, \eg $E$ and $C$, and the (rank) correlation can described with Kendall's tau ($\tau$):
\begin{equation}
\begin{split}
\tau = &\quad \Pr\{\ (e_2 - e_1)(c_2 - c_1) \geq 0 \} - \Pr\{\ (e_2 - e_1)(c_2 - c_1) < 0 \},
\end{split}    
\end{equation}
\noindent where ($e_1$, $c_1$) and ($e_2$, $c_2$) are sampled from a (survival) copula. The Clayton~\citep{clayton1978model} and Frank~\citep{frank1979simultaneous} copulas are two common Archimedean copulas in survival literature. Within these families, the copula $C_{\theta}$ is parameterized by $\theta$, interpreted as the degree of dependence between ${E}$ and ${C}$ in the bivariate case. When $\theta \approx 0$, it yields the independence copula, thus a larger value of $\theta$ implies greater dependency between the marginal distributions. Formally, we write $C(u_1, \ldots, u_d) : [0, 1]^d \rightarrow [0, 1]$, which is a $d$-dimensional copula (\ie where $d$ is the number of variables) with $u_1, \ldots, u_d$ uniform marginal probabilities and the following properties~\cite{Nelsen2006}:

1. \textbf{Groundedness}: If there exists an \( i \in \{1, \dots, m\} \) such that \( u_i = 0 \), then  
   \[
   C(u_1, \dots, u_m) = 0.
   \]
2. \textbf{Uniform Margins}: For all \( i \in \{1, \dots, m\} \), if \( \forall j \neq i \Rightarrow u_j = 1 \), then  
   \[
   C(u_1, \dots, u_m) = u_i.
   \]
3. \textbf{$d$-Increasingness}: For all \( u = (u_1, \dots, u_m) \) and \( v = (v_1, \dots, v_m) \) where \( u_i < v_i \) for all \( i = 1, \dots, m \), the following holds:  
   \[
   \sum_{\ell \in \{0,1\}^m} (-1)^{\ell_1 + \dots + \ell_m} C\left( u_1^{\ell_1} v_1^{1-\ell_1}, \dots, u_m^{\ell_m} v_m^{1-\ell_m} \right) \geq 0.
   \]
Let ${E}$ be the event distribution, ${C}$ the censoring distribution, $\bm{X}$ a vector of covariates, and $S_{E\mid \bm{X}}(t\mid\bm{X}) = \Pr(E > t\mid\bm{X})$ and $S_{C\mid \bm{X}}(t\mid\bm{X}) = \Pr(C > t\mid\bm{X})$ the marginal survival functions given $\bm{X}$. We can then define a bivariate (survival) copula $C_{\theta}$~\citep{Nelsen2006} that describes the degree of dependence between $E$ and $C$ as follows:
\begin{equation}\label{eq:survival_copula}
\Pr(E > t, C > t\mid\bm{X})\ =\ C_{\theta}\{\,S_{E\mid \bm{X}}(t\mid\bm{X}),\, S_{C\mid \bm{X}}(t\mid\bm{X})\,\}.
\end{equation}
Kendall's tau ($\tau$) can be similarly defined as:
\begin{equation}
\begin{split}
\tau = &\quad \Pr\{\ (e_2 - e_1)(c_2 - c_1) \geq 0\mid \bm{X}\ \} - \Pr\{\ (e_2 - e_1)(c_2 - c_1) < 0 \mid \bm{X}\ \},
\end{split}    
\end{equation}
\noindent where ($e_1$, $c_1$) and ($e_2$, $c_2$) are sampled from the model~\eqref{eq:survival_copula}. Kendall’s $\tau$ can be solely expressed as a function of the copula $C_\theta$ by~\citep[Ch. 3]{Emura2018}:
\begin{equation}
\tau_{\theta} = 4\int_0^1 \int_0^1 C_\theta(u, v) \, C_\theta(du, dv),
\end{equation}
\noindent which implies that Kendall's $\tau$ does not depend on the marginal survival functions. An Archimedean copula is a type of copula that is defined using a generator function, which allows for a flexible way to model dependence structures between random variables, \eg $E$ and $C$. Such a copula is defined as~\cite[Ch. 3]{Emura2018}:
\begin{equation}
C_{\theta}(u, v) \ = \ 
\varphi_\theta^{-1}\left(\, \varphi_\theta(u) + \varphi_\theta(v)
  \,  \right),
\end{equation}
\noindent where the function $\varphi_\theta : [0, 1] \rightarrow [0, \infty]$ is called a generator of the copula, which is continuous and strictly decreasing from $\varphi_\theta(0) \geq 0$ to $\varphi_\theta(1) = 0$. The following Archimedean copulas are considered in this work:

\begin{itemize}
\item \textbf{The Independence copula:} \( C(u, v) = uv \).
\item \textbf{The Clayton copula~\citep{clayton1978model}:} 
\[
C_\theta(u, v) = \left( u^{-\theta} + v^{-\theta} - 1 \right)^{-1/\theta}, \quad \theta \geq 0.
\]

\item \textbf{The Frank copula~\citep{frank1979simultaneous}:} 
\[
C_\theta(u, v) = -\frac{1}{\theta} \log \left[ 1 + \frac{(\exp(-\theta u) - 1)(\exp(-\theta v) - 1)}{e^{-\theta} - 1} \right].
\]
\[
\quad \theta \neq 0.
\]
\end{itemize}

\section{Existing Evaluation Metrics}
\label{app:exist_metrics}

\subsection{Concordance Index}

The Harrell's concordance index (CI) quantifies the proportion of concordant pairs among all comparable pairs~\citep{harrell1996multivariable}. Given an estimated risk function $\hat{\eta}(\cdot)$ and corresponding labels, CI is computed as:
\begin{align*}
\label{eq:ci}
    \text{CI} (\{ \bm{x}_i, t_i, \delta_i \}_{i=1}^N; \, \hat{\eta}(\cdot)) \ = \ \frac{\sum_{i,j: i\neq j}  \delta_i  \cdot \mathbbm{1}[t_i < t_j] \cdot \mathbbm{1}[
    \hat{\eta} (\bm{x}_i) > \hat{\eta} (\bm{x}_j)] }{\sum_{i,j: i\neq j} \delta_i \cdot \mathbbm{1}[t_i < t_j] } .
    % &= \mathbb{P} \ (\eta_i > \eta_j \mid t_i < t_j, \delta_i = 1)
\end{align*}

Some definitions of CI~\citep{hartman2023pitfalls} incorporate tie-breaking rules for cases where risk scores are identical. Specifically, a score of 0.5 may be assigned to tied pairs in the numerator. However, for simplicity, we omit ties in our discussion.

As shown in~\cite{uno2011}, this calculation converges to a censoring-dependent probability:
\begin{align*}
    P(\ \hat{\eta}(\bm{x}_i) > \hat{\eta}(\bm{x}_j) \mid  e_i< \min(e_{ j}, c_{j}) \ ),
\end{align*}
which differs from the intended probability:
\begin{align*}
    P(\ \hat{\eta}(\bm{x}_i) > \hat{\eta}(\bm{x}_j) \mid e_i < e_j \ ),
\end{align*}
To address this bias, \citet{uno2011} proposed the IPCW-weighted CI, computed as\footnote{The proposed IPCW-CI also comes with a truncation time $\tau$ as a parameter -- which means we only focus on the accuracy of ranking within $[0, \tau]$. Here, for consistency with other CIs, we do not consider such a truncation time.}:
\begin{equation}
\label{eq:ci_ipcw}
\begin{aligned}
    \text{CI}_{\text{IPCW}} (\{ \bm{x}_i, t_i, \delta_i \}_{i=1}^N; \, \hat{\eta}(\cdot)) \ = \ \frac{\sum_{i,j: i\neq j}  \delta_i  \cdot \{\hat{G}(t_i)\}^{-2}\cdot\mathbbm{1}[t_i < t_j] \cdot \mathbbm{1}[
    \hat{\eta} (\bm{x}_i) > \hat{\eta} (\bm{x}_j)] }{\sum_{i,j: i\neq j} \delta_i \cdot \{\hat{G}(t_i)\}^{-2}\cdot \mathbbm{1}[t_i < t_j] } ,
\end{aligned}
\end{equation}
where $\hat{G}(t)$ represents the marginal survival probability for censoring at time $t$, typically estimated using the Kaplan-Meier (KM) estimator. The inverse probability censoring weight (IPCW) is given by $1/\hat{G}(t)$. In cases where censoring probability depends on features (i.e., $\hat{G}(t \mid \bm{x}_i)$), a survival model such as CoxPH can be used for estimation. These two metrics are implemented in package \texttt{SurvivalEVAL}~\citep{qi_survivaleval_2024}.

\subsection{Integrated Brier Score}
The Brier Score (BS) quantifies the accuracy of probabilistic survival predictions at a specific time $t^*$~\citep{graf1999assessment}. It measures the squared error between the predicted survival probability and the observed survival status at $t^*$:

To incorporate the censored subjects in the calculation, BS also requires estimating the IPCW to uniformly transfer a censored subject's weight (originally 1) to subjects with known status at that time~\citep{vock2016adapting}.
Formally:
\begin{align*}
    \text{BS} \left(\{ \bm{x}_i, t_i, \delta_i \}_{i=1}^N; \, \hat{S}(\cdot), \hat{G}(\cdot), t^* \right) \ = \ \frac{1}{N} \sum_{i}  \left( \frac{\delta_i \cdot \mathbbm{1} [t_i \leq t^*] \cdot \hat{S}(t^* \mid \bm{x}_{i})^2 }{\hat{G}(t_i)} + \frac{\mathbbm{1} [t_i > t^*] \cdot (1 - \hat{S}(t^*\mid \bm{x}_{i}) )^2 }{G(t^*)}  \right),
\end{align*}

The integrated Brier score (IBS) is the expectation of BS over all time, defined as: 
% IBS for survival prediction is typically defined as:
\begin{align*}
    \text{IBS} \left(\{ \bm{x}_i, t_i, \delta_i \}_{i=1}^N; \, \hat{S}(\cdot), G(\cdot) \right)
     \ = \ \frac{1}{t_{\text{max}}}  \cdot \int_0^{t_{\text{max}}} \text{BS}\left(\{ \bm{x}_i, t_i, \delta_i \}_{i=1}^N; \, \hat{S}(\cdot), G(\cdot), \tau \right) d\tau, 
\end{align*}
where $t_{\text{max}}$ is usually the maximum event time of the combined training and validation datasets. This metric is implemented in the \texttt{SurvivalEVAL} package~\citep{qi_survivaleval_2024}.

\subsection{Mean Absolute Error}

Survival prediction shares similarities with regression, as it estimates a continuous event time given a feature set $\bm{x}_{i}$. Therefore, mean absolute error (MAE) serves as a useful evaluation metric.

MAE-Margin~\citep{haider_effective_2020} estimates a "best guess" event time for censored subjects using the KM estimator~\citep{kaplan_nonparametric_1958}. For a subject censored at time $t_i$, the margin event time is computed as:
\begin{equation*}
    e^{\text{margin}}(\ t_i, \delta_i=0, S_{\text{KM}}(\cdot) \ ) \ = \ \mathbb{E}[E \mid E > t_{i}] = t_{i} + \frac{\int_{t_{i}}^\infty S_{\text{KM}} (\tau) d\tau}{S_{\text{KM}}(t_{i})} \ ,
\end{equation*}
where $S_{\text{KM}}(t)$ is the KM estimation that is typically derived from the training dataset. 

Additionally, for each censored subject, \citet{haider_effective_2020} suggested we use a confidence weight $\omega_i = 1 - S_{\text{KM}}(t_{i})$ for the error calculated based on the margin value. 
This weight $\omega_i$ yields lower confidence for early censoring subjects and higher confidence for late censoring data. 
Of course, we set the weights for uncensored subjects $i$ to $w_i = 1$ as we have full confidence in those error calculations.
The overall MAE margin after a re-weighting scheme is:
\begin{align*}
    \text{MAE}_{\text{margin}} \left(\{ \bm{x}_i, t_i, \delta_i \}_{i=1}^N; \, \hat{t}_i \right) =   \frac{1}{\sum_{i=1}^{N} \omega_i} \sum_{i =1}^{N} \omega_i  \left| [(1 - \delta_i) \cdot e^{\text{margin}}(t_i) + \delta_i \cdot t_i ] - \hat{t}_i \right| \ . 
\end{align*}

This metric is implemented in \texttt{SurvivalEVAL} package~\citep{qi_survivaleval_2024}.

\section{Experimental Details}\label{app:experimental_details}

\subsection{Data generation and preprocessing}
\label{app:data_generation_and_processing}

\begin{table*}[!htbp]
\centering
\begin{tabular}{lcccccc}
\toprule
Dataset & \#Instances & \#Features$^\dagger$ & \%Censored & \#Event & Max Event $t$ \\
\midrule
METABRIC & 1,902 & 9 (9) & 42.1\% & 1102 & 355 \\ \cdashline{1-6}
MIMIC-IV (hospital death) & 293,907 & 10 (14) & 97.7\% & 6780 & 248 \\ 
MIMIC-IV (all causes) & 38,520 & 91 (103) & 66.7\% & 12845 & 4404 \\  \cdashline{1-6}
SEER (brain) & 73,703 & 10 (10) & 40.1\% & 44137 & 224 \\
SEER (liver) & 82,841 & 14 (14) & 37.6\% & 51704 & 225 \\    
SEER (stomach) & 100,360 & 14 (14) & 43.4\% & 56807 & 226 \\ 
\bottomrule
\end{tabular}
\caption{Overview of the raw datasets. The datasets are sorted by name. $^\dagger$The number of features before performing one-hot encoding, with brackets (the number of features after one-hot encoding).}
\label{tab:datasets}
\end{table*}

In this section, we describe how the synthetic dataset is generated and how we preprocessed the raw survival datasets. Table \ref{tab:datasets} shows an overview of the raw datasets.

\textbf{Synthetic:} This paper utilizes a linear synthetic dataset with two competing events (\ie event and censoring), both based on the parametric Weibull distribution as discussed in~\citet{foomani_copula-based_2023}. The datasets are characterized by the conditional hazard function and conditional survival probability of an event or censoring as follows:
\begin{align}
    h_{E\mid \bm{X}}(t\mid \bm{X}) &= \left(\frac{v}{\rho}\right) \left(\frac{t}{\rho}\right)^{v- 1} \exp \left(g_{\Psi}(\bm{X})\right), \\
    \label{eq:dgp}
    S_{E\mid \bm{X}}(t\mid \bm{X}) &= \exp \left( - \left(\frac{t}{\rho}\right)^{v- 1} \exp \left(g_{\Psi}(\bm{X})\right)\right),
\end{align}
where $v$ and $\rho$ represent the shape and scale parameters of the Weibull distribution, respectively. We use $v_E=4$, $v_C=6$, $\rho_E=17$, $\rho_C=19$. $g_{\Psi}(\bm{X})$ represents the risk function, which is a linear function in this work. The data generation process begins by initializing a copula with a predefined parameter $\theta$. We then sample $N$ sets of CDF observations from the copula, each containing two CDF probabilities (corresponding to each competing event, \ie event or censoring). From these probabilities, we then calculate the corresponding times using the inverse function of~\eqref{eq:dgp}. The data generation process is further described in Algorithm 2 in~\citet{foomani_copula-based_2023}.

\textbf{METABRIC:} The Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset \citep{Curtis2012} contains survival information for breast cancer patients. This dataset includes a diverse range of feature sets that encompass clinical traits, expression profiles, copy number variation (CNV) profiles, and single nucleotide polymorphism (SNP) genotypes. All these features are derived from breast tumor samples collected during the METABRIC trial. The dataset can be downloaded from (\url{https://www.cbioportal.org/study/summary?id=brca_metabric}), and it does not have any missing values.

\textbf{MIMIC-IV:} The Medical Information Mart for Intensive Care (MIMIC)-IV \citep{johnson_mimic_2023} contains critical care data from patients admitted to hospital and intensive care units (ICU). We create a version using the MIMIC-IV database which contains patients that are alive at least 24 hours after being admitted to ICU. Their date of death is derived from hospital records or state records, which means, the cause of mortality is not limited to the reason for ICU admission. We follow the instructions in \citep{qi_effective_2023} to process the dataset and the code is available in their GitHub repository: \url{https://github.com/shi-ang/CensoredMAE}.

\textbf{SEER:} The SEER Program dataset~\citep{ries2007seer} is a comprehensive collection of cancer patient data from approximately 49\% of the U.S. population. Our study focuses on three subsets: SEER-brain, SEER-liver, and SEER-stomach, containing data on patients diagnosed with brain, liver, and stomach cancers, respectively. The goal is to model the time from diagnosis to failure events, such as death or disease progression. We preprocess the dataset as in \citep{qi2024conformalized}. Features analyzed include age, sex, cancer stage, grade, and various clinical factors. We excluded features with over 70\% missing data, patients without follow-up times, duplicates, and those with a survival time of zero. The SEER cohort is available for download at \url{www.seer.cancer.gov}.

\subsection{Survival learners}
\label{app:survival_learners}

In this section, we describe the implementation and hyperparameters of the survival learners.

\textbf{CoxPH:} The Cox Proportional Hazards model (CoxPH) is a semi-parametric survival analysis model \citep{cox_regression_1972}, consisting of a non-parametric baseline hazard function $h_0(t)$ and a parametric partial hazard function $f(\bm{\theta}, \bx_i)$, typically estimated using the Breslow method~\citep{breslow_analysis_1975}. The hazard function is $h(t\mid\bx_i) = h_0(t) \exp(f(\bm{\theta}, \bx_i))$, where $f$ is linear. The MLE of $\hat{\bm{\theta}}$ maximizes the Cox partial log-likelihood \citep{cox_regression_1972}. CoxPH is implemented from the \texttt{scikit-survival} package version 0.21.0.

\textbf{GBSA:} GBSA \citep{Ridgeway1999} is a likelihood-based boosting model that optimizes an $\ell_2$-norm penalized Cox partial log-likelihood \citep{cox_regression_1972}. It updates candidate variables flexibly at each boosting step using an offset-based gradient boosting method, unlike traditional gradient boosting, which updates either a single component or all features for fitting the gradient. GBSA is implemented from the \texttt{scikit-survival} package version 0.21.0.

\textbf{RSF:} Random Survival Forests (RSF)~\citep{ishwaran_random_2008} extend decision trees for survival analysis by constructing multiple trees on bootstrapped samples. Each tree predicts an individual's survival function, with splits based on criteria like the log-rank test. RSF handles censored data as right-censored and accounts for it during tree construction, with final predictions as the average of all trees' survival functions. RSF is implemented from the \texttt{scikit-survival} package version 0.21.0.

\textbf{DeepSurv:} DeepSurv~\citep{katzman_deepsurv_2018} is a neural network implementation of the CoxPH model of the form $h\br{t \mid \bx_i} = h_0\br{t} \exp \br{f\br{\bth,\bx_i}}$, where $f\br{\bth,\bx_i}$ denotes a risk score as a nonlinear function of the features, \ie $f\br{\bth,\bx_i} = \sigma(\bx_i\bth)$, where $\sigma$ is a nonlinear activation function. Popular functions include the hyperbolic tangent (Tanh) function or the Rectified Linear Unit (ReLU). The MLE for $\hat{\bth}$ is derived by numerically maximizing the partial Cox log-likelihood. DeepSurv is implemented from \url{https://github.com/shi-ang/BNN-ISD/}.

\textbf{MTLR:} Multi-Task Logistic Regression (MTLR)~\cite{NIPS2011_1019c809} is a neural network implementation of a discrete-time model that predicts the survival distribution using a sequence of dependent logistic regressors. The number of discrete times is determined by the square root of the number of uncensored patients, and we use quantiles to divide those uncensored instances evenly into each time interval. MTLR is implemented from \url{https://github.com/shi-ang/BNN-ISD/}.

Table \ref{tab:baseline_hyperparameters} reports the selected hyperparameters for the survival learners.

\begin{table}[!htbp]
\centering
\resizebox{0.27\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Parameter} & \textbf{Value} \\ 
\midrule
\textbf{CoxPH} & alpha & 0.0001 \\
 & ties & 'Breslow' \\
 & n\_iter & 100 \\
 & tol & 1e-9 \\ \midrule

 \textbf{GBSA} & n\_estimators & 100 \\
 & max\_depth & 1 \\
 & min\_samples\_split & 2 \\
 & min\_samples\_leaf & 1 \\
 & max\_features & 'sqrt' \\
 & sub\_sample & 0.8 \\
 & random\_state & 0 \\ \midrule

\textbf{RSF} & n\_estimators & 100 \\
 & max\_depth & 1 \\
 & min\_samples\_split & 2 \\
 & min\_samples\_leaf & 1 \\
 & max\_features & 'sqrt' \\
 & random\_state & 0 \\ \midrule

\textbf{DeepSurv} & hidden\_size & 100 \\
 & verbose & False \\
 & lr & 0.001 \\
 & c1 & 0.01 \\
 & num\_epochs & 1000 \\
 & batch\_size & 32 \\
 & dropout & 0.25 \\
 & early\_stop & True \\
 & patience & 10 \\ \midrule

\textbf{MTLR} 
 & verbose & False \\
 & lr & 0.001 \\
 & c1 & 0.01 \\
 & num\_epochs & 1000 \\
 & batch\_size & 32 \\
 & early\_stop & True \\
 & patience & 10 \\

\bottomrule
\end{tabular}%
}
\caption{Hyperparameters for the survival learners.}
\label{tab:baseline_hyperparameters}
\end{table}

\subsection{Implementation Details}
\label{app:implementation_details}

\textbf{Copula model.} To validate our approach in semi-synthetic datasets, we train two copula models, Clayton and Frank, alongside two nonlinear Weibull proportional-hazards models for the marginal distributions \(E\) and \(C\), respectively. The Weibull models use a multilayer perceptron (MLP) as the backbone network architecture. We use a single layer with 32 hidden nodes and the ReLU activation function. For all experiments, we train the models for 10000 epochs using the Adam optimizer~\citep{KingBa15} with a learning rate of 1e-3, batch normalization and a batch size of 1024. We use early stopping to terminate the training process if the validation loss does not improve for 100 consecutive epochs.

\textbf{Loss function.} We estimate the model parameters using maximum likelihood estimation (MLE) under dependent censoring, as described in \citet{foomani_copula-based_2023}. Given survival data $\mathcal{D}$, let $f_{T\mid \bm{X}}(\cdot)$ and $F_{T\mid \bm{X}}(\cdot)$ represent the conditional density and cumulative distribution functions, respectively, over the event horizon, with the event distribution $E$ and the censoring distribution ${C}$. Under independence censoring, the likelihood of the $i$-th instance experiencing the event is:
\begin{equation}
\label{eq:likelihood_function}
\mathcal{L}(\mathcal{D}) = \prod_{i=1}^{N} \left[ \Pr(E = t_i, C \geq t_i \mid \bm{X} = \bm{x}_{i})^{\delta_{i}} \times \Pr(E \geq t_i, {C} = t_i \mid \bm{X} = \bm{x}_{i})^{1 - \delta_{i}} \right]
\end{equation}
However, when $E$ and $C$ are no longer independent, we can no longer rely on this clean decomposition of the likelihood. Instead, we introduce a copula $C(u_1, u_2)$ to Equation~\ref{eq:likelihood_function} to adjust for the dependency between $E$ and $C$ by applying Lemma 2 from~\citet{foomani_copula-based_2023}. Now, as a function of $C(u_1, u_2)$ and taking the sum of logs, the (dependent) likelihood has the following form:
\begin{align}
\label{eq:dependent_likelihood_function}
\mathcal{L}(\mathcal{D}) = \sum_{i=1}^N \delta^{(i)} \; \text{log} \left[f_{{E} \mid \bm{X}}(t_i \mid \bm{x}_{i})\right] \; 
+ \; \delta_{i} \; \text{log} \left[\frac{\partial}{\partial u_1}C(u_1, u_2)\rvert^{u_1 = S_{E \mid  \bm{X}}(t_i \mid \bm{x}_{i})}_{u_2 = S_{C\mid\bm{X}}(t_i \rvert \bm{x}_{i})}\right] \\
+ \; (1 - \delta_{i}) \; \text{log} \left[f_{{C} \mid \bm{X}}(t_i \mid \bm{x}_{i})\right] + (1 - \delta_{i}) \; \text{log} \left[\frac{\partial}{\partial u_1}C(u_1, u_2)\rvert^{u_1 = S_{E\rvert\bm{X}}(t_i \rvert \bm{x}_{i})}_{u_2 = S_{{C}\rvert\bm{X}}(t_i \rvert \bm{x}_{i})}\right] \notag
\end{align}
In this expression, the first term represents the log-likelihood of observing the event at time $t_i$. The second term captures the conditional probability of observing the censoring time after the event time, given that the event occurred at $t_i$. The third and fourth terms, by symmetry, represent the same quantities for the censoring time. Given $C(u_1, u_2)$ is an Archimedean copula (\eg Clayton or Frank), the partial derivatives in Equation \ref{eq:dependent_likelihood_function} have closed-form solutions, allowing the log-likelihood to be expressed in a closed form that can be optimized using gradient-based methods. To estimate \ref{eq:dependent_likelihood_function}, we adopt the approach as described in Algorithm 1 in~\citet{foomani_copula-based_2023}.

\textbf{Copula selection.} After training, for each model (marginals and copula), we choose the model that minimizes the Akaike Information Criterion (AIC) given the model's negative log-likelihood (NLL) and its number of parameters. The AIC is a measure used to compare models, taking into account both the goodness of fit (via the likelihood) and model complexity (via the number of parameters). This offers a practical, data-driven approach to copula selection, as minimizing the NLL provides an objective criterion for which model best captures the dependency structure explicitly \citep[Ch. 4]{Emura2018}. We then extract $C_\theta$ from the chosen copula model and use that to calculate the dependent evaluation metrics.

\textbf{Reproducibility.} All experiments were implemented and executed in Python 3.9 with PyTorch 1.13.1, NumPy 1.24.3, and Pandas 1.5.3. The Copula-Graphic estimator was implemented using the \texttt{compound.Cox} package version 3.32. Tensor computations were carried out in double precision (fp64). The experiments were conducted on a single workstation equipped with an Intel Core i9-10980XE 3.00GHz CPU, 64GB of RAM, and an NVIDIA GeForce RTX 3090 GPU. Model training was performed on the GPU using CUDA 11.7. All datasets utilized in this study are publicly available. Detailed instructions for reproducing the results are included in the supplementary code.

\section{Complete results}\label{app:complete_results}

\subsection{Synthetic data}\label{app:complete_synthetic_data}

Figures \ref{fig:metric_error_over_ktau_1000_10}-\ref{fig:metric_error_over_ktau_10000_10} demonstrate the bias, \ie the proximity to the true error, of established baseline metrics and the proposed dependent metrics as a function of the dependence (Kendall's $\tau$) under a known copula. We plot the mean biases ($\pm$ SD.) across 100 experiments with different random seeds. Data is generated from a DGP with sample sizes $N=\{1000, 5000, 10000\}$ and $d=10$. Across all metrics, we generally see larger errors with high dependence ($\tau$). Concerning the small dataset ($N=1,000$), the \hbox{\CIDep} variant exhibits higher error than Harrell’s CI for both copulas under low to medium dependency ($\tau \leq 0.5)$, however, the \hbox{\CIDep} improves on both Harrell's and Uno's under high dependency ($\tau > 0.5)$). The same is the case for the IBS and MAE results, especially under low dependency. In the medium dataset (\(N=5,000\)), the dependent metrics consistently outperform their independent counterparts across all levels of \(\tau\). This trend becomes even more pronounced in the large dataset (\(N=10,000\)), where the dependent metrics show a significant improvement over the independent ones. These results highlight the increasing impact of dependent censoring as dataset size grows, further demonstrating the limitations of the independent censoring assumption in larger datasets.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=1\linewidth]{Figures/metric_error_over_ktau_1000_10.pdf}
\caption{Plot of the bias of established baseline metrics and the proposed dependent metrics. $N=1000$, $d=10$.}
\label{fig:metric_error_over_ktau_1000_10}
\end{figure*}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=1\linewidth]{Figures/metric_error_over_ktau_5000_10.pdf}
\caption{Plot of the bias of established baseline metrics and the proposed dependent metrics. $N=5000$, $d=10$.}
\label{fig:metric_error_over_ktau_5000_10}
\end{figure*}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=1\linewidth]{Figures/metric_error_over_ktau_10000_10.pdf}
\caption{Plot of the bias of established baseline metrics and the proposed dependent metrics. $N=10000$, $d=10$.}
\label{fig:metric_error_over_ktau_10000_10}
\end{figure*}

\subsection{Semi-synthetic data}\label{app:complete_semi_synthetic_data}

Table \ref{tab:app_semisynthetic_results} shows the results in the 6 semi-synthetic datasets. We see that \hbox{\CIDep} can improve on Uno's CI (\ie give lower bias) in cases with large sample size, \eg the SEER datasets, but Harrell's CI is generally the better choice among the two. Concerning the IBS results, the \hbox{\IBSDep} offers a solid alternative to the IPCW method, as it gives notable improvements in several datasets. Concerning the MAE results, the \hbox{\MAEDep} is aligned with the MAE-Margin in the smaller datasets, \ie METABRIC and MIMIC-IV, but gives notable improvements in the larger SEER datasets.

Figures \ref{fig:app_ranking_error_metabric}-\ref{fig:app_ranking_error_seer_stomach} shows the ranking accuracy, \ie the number of times the resp. metric correctly identified the ordering of the top-3 survival learners by the true metric, over 5 experiments in 6 datasets. We see general alignment between the metrics, but \hbox{\IBSDep} and \hbox{\MAEDep} give notable improvements over their independent variants in the SEER (liver) dataset.

\begin{table*}[!htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{clcccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Strategy}} & \multicolumn{3}{c}{\textbf{Discrimination (CI)}} & \multicolumn{2}{c}{\textbf{Accuracy (IBS)}} & \multicolumn{5}{c}{\textbf{Accuracy (MAE)}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-7} \cmidrule(lr){8-12}
& & Harrell's & Uno's & Dependent & IPCW & Dependent & Uncens. & Hinge & Pseudo & Margin & Dependent \\
\midrule
\multirow{4}{*}{\makecell{METABRIC \\ ($N$=1,102, $C$=3.8\%)}}
& Original & 0.10$\pm$\scriptsize{0.08} & 0.39$\pm$\scriptsize{0.14} & 0.48$\pm$\scriptsize{0.25} & 1.23$\pm$\scriptsize{0.85} & 1.17$\pm$\scriptsize{0.85} & 3.24$\pm$\scriptsize{0.55} & 1.40$\pm$\scriptsize{0.21} & 0.25$\pm$\scriptsize{0.21} & 0.31$\pm$\scriptsize{0.23} & 0.33$\pm$\scriptsize{0.22} \\
& Top 5 & 0.11$\pm$\scriptsize{0.08} & 0.51$\pm$\scriptsize{0.17} & 0.58$\pm$\scriptsize{0.23} & 1.22$\pm$\scriptsize{0.85} & 1.17$\pm$\scriptsize{0.85} & 3.21$\pm$\scriptsize{0.55} & 1.40$\pm$\scriptsize{0.21} & 0.25$\pm$\scriptsize{0.20} & 0.31$\pm$\scriptsize{0.23} & 0.30$\pm$\scriptsize{0.23} \\
& Top 10 & 0.10$\pm$\scriptsize{0.08} & 0.43$\pm$\scriptsize{0.11} & 0.49$\pm$\scriptsize{0.16} & 1.23$\pm$\scriptsize{0.85} & 1.17$\pm$\scriptsize{0.85} & 3.26$\pm$\scriptsize{0.54} & 1.40$\pm$\scriptsize{0.21} & 0.25$\pm$\scriptsize{0.21} & 0.32$\pm$\scriptsize{0.23} & 0.31$\pm$\scriptsize{0.23} \\
& Rand. 25\% & 0.10$\pm$\scriptsize{0.08} & 2.34$\pm$\scriptsize{0.68} & 2.31$\pm$\scriptsize{0.70} & 1.25$\pm$\scriptsize{0.88} & 1.18$\pm$\scriptsize{0.88} & 3.45$\pm$\scriptsize{0.48} & 1.40$\pm$\scriptsize{0.21} & 0.27$\pm$\scriptsize{0.22} & 0.34$\pm$\scriptsize{0.23} & 0.33$\pm$\scriptsize{0.23} \\
\cmidrule(lr){1-1}
\multirow{4}{*}{\makecell{MIMIC-IV (all) \\ ($N$=12,845, $C$=10.6\%)}}
& Original & 0.65$\pm$\scriptsize{0.21} & 0.66$\pm$\scriptsize{0.21} & 0.66$\pm$\scriptsize{0.21} & 0.76$\pm$\scriptsize{0.17} & 0.92$\pm$\scriptsize{0.17} & 36.73$\pm$\scriptsize{4.98} & 62.28$\pm$\scriptsize{5.12} & 37.18$\pm$\scriptsize{4.98} & 37.18$\pm$\scriptsize{4.98} & 37.34$\pm$\scriptsize{4.96} \\
& Top 5 & 1.11$\pm$\scriptsize{0.19} & 1.60$\pm$\scriptsize{0.23} & 1.60$\pm$\scriptsize{0.23} & 0.82$\pm$\scriptsize{0.19} & 1.12$\pm$\scriptsize{0.20} & 39.35$\pm$\scriptsize{5.88} & 65.63$\pm$\scriptsize{5.97} & 39.56$\pm$\scriptsize{5.87} & 39.56$\pm$\scriptsize{5.87} & 39.72$\pm$\scriptsize{5.87} \\
& Top 10 & 1.02$\pm$\scriptsize{0.17} & 1.29$\pm$\scriptsize{0.21} & 1.29$\pm$\scriptsize{0.21} & 0.79$\pm$\scriptsize{0.19} & 1.02$\pm$\scriptsize{0.20} & 37.71$\pm$\scriptsize{5.57} & 63.72$\pm$\scriptsize{5.69} & 38.13$\pm$\scriptsize{5.60} & 38.13$\pm$\scriptsize{5.60} & 38.33$\pm$\scriptsize{5.59} \\
& Rand. 25\% & 0.45$\pm$\scriptsize{0.17} & 0.48$\pm$\scriptsize{0.18} & 0.48$\pm$\scriptsize{0.18} & 0.81$\pm$\scriptsize{0.18} & 1.07$\pm$\scriptsize{0.20} & 36.95$\pm$\scriptsize{3.93} & 62.79$\pm$\scriptsize{4.10} & 37.25$\pm$\scriptsize{3.94} & 37.25$\pm$\scriptsize{3.94} & 37.46$\pm$\scriptsize{3.95} \\
\cmidrule(lr){1-1}
\multirow{4}{*}{\makecell{MIMIC-IV (hospital) \\ ($N$=6,780, $C$=74.3\%)}}
& Original & 0.74$\pm$\scriptsize{0.43} & 1.27$\pm$\scriptsize{0.83} & 2.10$\pm$\scriptsize{1.54} & 8.75$\pm$\scriptsize{1.50} & 6.57$\pm$\scriptsize{1.44} & 3.48$\pm$\scriptsize{0.25} & 6.44$\pm$\scriptsize{0.27} & 3.11$\pm$\scriptsize{0.27} & 4.54$\pm$\scriptsize{0.26} & 4.36$\pm$\scriptsize{0.24} \\
& Top 5 & 0.79$\pm$\scriptsize{0.55} & 1.16$\pm$\scriptsize{0.56} & 3.36$\pm$\scriptsize{2.74} & 8.80$\pm$\scriptsize{1.51} & 6.42$\pm$\scriptsize{1.45} & 3.51$\pm$\scriptsize{0.24} & 6.45$\pm$\scriptsize{0.26} & 3.10$\pm$\scriptsize{0.26} & 4.65$\pm$\scriptsize{0.25} & 4.33$\pm$\scriptsize{0.18} \\
& Top 10 & 0.71$\pm$\scriptsize{0.47} & 1.10$\pm$\scriptsize{0.65} & 2.46$\pm$\scriptsize{1.69} & 8.73$\pm$\scriptsize{1.50} & 6.57$\pm$\scriptsize{1.44} & 3.50$\pm$\scriptsize{0.27} & 6.45$\pm$\scriptsize{0.27} & 3.11$\pm$\scriptsize{0.27} & 4.60$\pm$\scriptsize{0.28} & 4.40$\pm$\scriptsize{0.25} \\
& Rand. 25\% & 1.44$\pm$\scriptsize{0.58} & 1.77$\pm$\scriptsize{1.02} & 2.70$\pm$\scriptsize{1.44} & 10.38$\pm$\scriptsize{0.76} & 7.91$\pm$\scriptsize{0.90} & 3.57$\pm$\scriptsize{0.20} & 6.51$\pm$\scriptsize{0.19} & 3.06$\pm$\scriptsize{0.19} & 4.79$\pm$\scriptsize{0.19} & 4.62$\pm$\scriptsize{0.20} \\
\cmidrule(lr){1-1}
\multirow{4}{*}{\makecell{SEER (brain) \\ ($N$=44,137, $C$=9.0\%)}}
& Original & 0.45$\pm$\scriptsize{0.08} & 0.86$\pm$\scriptsize{0.10} & 0.54$\pm$\scriptsize{0.11} & 0.44$\pm$\scriptsize{0.14} & 0.42$\pm$\scriptsize{0.34} & 1.79$\pm$\scriptsize{0.06} & 1.21$\pm$\scriptsize{0.04} & 1.50$\pm$\scriptsize{0.06} & 1.11$\pm$\scriptsize{0.04} & 1.13$\pm$\scriptsize{0.04} \\
& Top 5 & 0.25$\pm$\scriptsize{0.06} & 0.87$\pm$\scriptsize{0.13} & 0.63$\pm$\scriptsize{0.14} & 0.44$\pm$\scriptsize{0.15} & 0.37$\pm$\scriptsize{0.33} & 1.78$\pm$\scriptsize{0.06} & 1.22$\pm$\scriptsize{0.04} & 1.50$\pm$\scriptsize{0.06} & 1.11$\pm$\scriptsize{0.04} & 1.13$\pm$\scriptsize{0.04} \\
& Top 10 & 0.49$\pm$\scriptsize{0.07} & 0.88$\pm$\scriptsize{0.11} & 0.57$\pm$\scriptsize{0.12} & 0.44$\pm$\scriptsize{0.15} & 0.42$\pm$\scriptsize{0.34} & 1.80$\pm$\scriptsize{0.06} & 1.21$\pm$\scriptsize{0.04} & 1.50$\pm$\scriptsize{0.06} & 1.11$\pm$\scriptsize{0.04} & 1.13$\pm$\scriptsize{0.04} \\
& Rand. 25\% & 2.61$\pm$\scriptsize{2.38} & 2.83$\pm$\scriptsize{1.72} & 2.72$\pm$\scriptsize{1.75} & 0.48$\pm$\scriptsize{0.12} & 0.24$\pm$\scriptsize{0.09} & 1.89$\pm$\scriptsize{0.09} & 1.30$\pm$\scriptsize{0.10} & 1.45$\pm$\scriptsize{0.11} & 1.05$\pm$\scriptsize{0.10} & 1.07$\pm$\scriptsize{0.10} \\
\cmidrule(lr){1-1}
\multirow{4}{*}{\makecell{SEER (liver) \\ ($N$=51,704, $C$=20.5\%)}}
& Original & 0.77$\pm$\scriptsize{0.10} & 0.57$\pm$\scriptsize{0.14} & 0.76$\pm$\scriptsize{0.13} & 0.33$\pm$\scriptsize{0.15} & 0.25$\pm$\scriptsize{0.11} & 2.59$\pm$\scriptsize{0.07} & 2.50$\pm$\scriptsize{0.04} & 2.47$\pm$\scriptsize{0.07} & 1.48$\pm$\scriptsize{0.06} & 1.44$\pm$\scriptsize{0.05} \\
& Top 5 & 1.86$\pm$\scriptsize{0.12} & 1.70$\pm$\scriptsize{0.15} & 1.76$\pm$\scriptsize{0.13} & 0.28$\pm$\scriptsize{0.14} & 0.47$\pm$\scriptsize{0.21} & 2.80$\pm$\scriptsize{0.05} & 2.69$\pm$\scriptsize{0.06} & 2.25$\pm$\scriptsize{0.07} & 1.25$\pm$\scriptsize{0.06} & 0.85$\pm$\scriptsize{0.47} \\
& Top 10 & 0.81$\pm$\scriptsize{0.08} & 0.90$\pm$\scriptsize{0.09} & 1.10$\pm$\scriptsize{0.07} & 0.33$\pm$\scriptsize{0.15} & 0.25$\pm$\scriptsize{0.11} & 2.61$\pm$\scriptsize{0.08} & 2.51$\pm$\scriptsize{0.04} & 2.46$\pm$\scriptsize{0.06} & 1.47$\pm$\scriptsize{0.05} & 1.43$\pm$\scriptsize{0.05} \\
& Rand. 25\% & 0.89$\pm$\scriptsize{0.52} & 1.80$\pm$\scriptsize{1.23} & 1.68$\pm$\scriptsize{1.21} & 0.45$\pm$\scriptsize{0.21} & 0.35$\pm$\scriptsize{0.17} & 2.75$\pm$\scriptsize{0.11} & 2.49$\pm$\scriptsize{0.08} & 2.50$\pm$\scriptsize{0.14} & 1.49$\pm$\scriptsize{0.09} & 1.47$\pm$\scriptsize{0.11} \\
\cmidrule(lr){1-1}
\multirow{4}{*}{\makecell{SEER (stomach) \\ ($N$=56,807, $C$=21.9\%)}}
& Original & 0.43$\pm$\scriptsize{0.09} & 0.30$\pm$\scriptsize{0.10} & 0.49$\pm$\scriptsize{0.13} & 0.27$\pm$\scriptsize{0.08} & 0.22$\pm$\scriptsize{0.14} & 1.21$\pm$\scriptsize{0.05} & 2.12$\pm$\scriptsize{0.03} & 2.19$\pm$\scriptsize{0.10} & 1.24$\pm$\scriptsize{0.06} & 1.15$\pm$\scriptsize{0.18} \\
& Top 5 & 0.88$\pm$\scriptsize{0.05} & 0.62$\pm$\scriptsize{0.08} & 0.71$\pm$\scriptsize{0.05} & 0.26$\pm$\scriptsize{0.08} & 0.17$\pm$\scriptsize{0.11} & 1.17$\pm$\scriptsize{0.06} & 2.14$\pm$\scriptsize{0.04} & 2.17$\pm$\scriptsize{0.10} & 1.22$\pm$\scriptsize{0.06} & 1.21$\pm$\scriptsize{0.07} \\
& Top 10 & 0.37$\pm$\scriptsize{0.09} & 0.29$\pm$\scriptsize{0.08} & 0.43$\pm$\scriptsize{0.14} & 0.28$\pm$\scriptsize{0.08} & 0.22$\pm$\scriptsize{0.17} & 1.18$\pm$\scriptsize{0.06} & 2.11$\pm$\scriptsize{0.04} & 2.21$\pm$\scriptsize{0.10} & 1.26$\pm$\scriptsize{0.06} & 1.15$\pm$\scriptsize{0.18} \\
& Rand. 25\% & 0.86$\pm$\scriptsize{0.51} & 1.47$\pm$\scriptsize{1.01} & 1.37$\pm$\scriptsize{0.97} & 0.27$\pm$\scriptsize{0.09} & 0.16$\pm$\scriptsize{0.04} & 1.13$\pm$\scriptsize{0.11} & 2.12$\pm$\scriptsize{0.10} & 2.20$\pm$\scriptsize{0.13} & 1.28$\pm$\scriptsize{0.11} & 1.25$\pm$\scriptsize{0.10} \\
\bottomrule
\end{tabular}%
}
\caption{Full comparison of established survival metrics and the proposed dependent metrics in terms of the mean ($\pm$ SD.) bias, \ie the proximity to the true error, averaged over 5 survival learners and 5 experiments. $N$ and $C$ are the number of instances and censoring rate, respectively, after preprocessing. CI and IBS results are multiplied by 100.}
\label{tab:app_semisynthetic_results}
\end{table*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\linewidth]{Figures/ranking_error_metabric.pdf}
\caption{Plot the ranking accuracy over 5 experiments in the METABRIC dataset.}
\label{fig:app_ranking_error_metabric}
\end{figure*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\linewidth]{Figures/ranking_error_mimic_all.pdf}
\caption{Plot the ranking accuracy over 5 experiments in the MIMIC-IV (all) dataset.}
\label{fig:app_ranking_error_mimic_all}
\end{figure*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\linewidth]{Figures/ranking_error_mimic_hospital.pdf}
\caption{Plot the ranking accuracy over 5 experiments in the MIMIC-IV (hospital) dataset.}
\label{fig:app_ranking_error_mimic_hospital}
\end{figure*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\linewidth]{Figures/ranking_error_seer_brain.pdf}
\caption{Plot the ranking accuracy over 5 experiments in the SEER (brain) dataset.}
\label{fig:app_ranking_error_seer_brain}
\end{figure*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\linewidth]{Figures/ranking_error_seer_liver.pdf}
\caption{Plot the ranking accuracy over 5 experiments in the SEER (liver) dataset.}
\label{fig:app_ranking_error_seer_liver}
\end{figure*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\linewidth]{Figures/ranking_error_seer_stomach.pdf}
\caption{Plot the ranking accuracy over 5 experiments in the SEER (stomach) dataset.}
\label{fig:app_ranking_error_seer_stomach}
\end{figure*}

\clearpage

\section{Derivations}\label{app:derivations}

\subsection{The Kaplan-Meier estimator}

This derivation is reproduced from \citep[Ch. 2]{Emura2018}.

Recall that $(t_i, \delta_i), \; i = 1, \ldots, n$ represents survival data without covariates, where $t_i = \min(e_i, c_i)$, $\delta_i = \mathbbm{1}[e_i \leq c_i]$. We can then consider the survival function $S(t)$ as a function that decreases in steps, with discontinuities occurring only at the observed times of death. In this case, the survival function for the event can be expressed as
\begin{align}\label{eq:km1}
S_E(t) = \Pr(E \geq t) = \prod_{t_i \leq t, \, d_i = 1} \left( 1 - \Pr(E = t_i) \right) \Pr(E \geq t_i)
\end{align}
Second, suppose that $E$ and $C$ are independent. Then, we can write
\begin{align}\label{eq:k2}
S_E(t) &= \prod_{t_i \leq t, \, d_i = 1} \left( 1 - \Pr(E = t_i, C \geq t_i) \right) \Pr(E \geq t_i, C \geq t_i) \\
     &= \prod_{t_i \leq t, \, d_i = 1} \left( 1 - \Pr(\min(E, C) = t_i, E \leq C) \right) \Pr(\min(E, C) \geq t_i)
\end{align}
Finally, we replace the probability ratio of the last expression by its estimate to obtain
\begin{align}\label{eq:km3}
S_E(t) &= \prod_{t_i \leq t, \, d_i = 1} \left(1 - \frac{\sum_{\ell=1}^{N} I(t_\ell = t_i, \delta_\ell = 1) / n}{\sum_{\ell=1}^{N} I(t_\ell \geq t_i)/n} \right) \\
     &= \prod_{t_i \leq t, \, d_i = 1} \left(1 - \frac{1}{n_i} \right)
\end{align}


\subsection{The Copula-Graphic estimator}

This derivation is reproduced from \citep[Ch. 4]{Emura2018}.

Consider an Archimedean copula model
\begin{equation}\label{eq:cg1}
P(E > t, C > c) = \phi_\theta^{-1} \big[ \phi_\theta\{S_E(t)\} + \phi_\theta\{S_C(t)\} \big],
\end{equation}
\noindent where $\phi_\theta:[0, 1]^d \rightarrow [0, \infty]$ is a generator function, which is continuous, strictly decreasing, and satisfies the boundary conditions $\varphi(0) = \infty$ and $\varphi(1) = 0$~\cite[Ch. 3]{Emura2018}, and $S_E(t) = \Pr(E > t)$ and $S_C(t) = \Pr(C > t)$ are the marginal survival functions. Assume that $S_E(t)$ is a decreasing step function with jumps at event times. Thus, \( \delta_i = 1 \) implies \( S_E(t_i) \neq S_E(t_i - dt) \) and \( S_C(t_i) = S_C(t_i - dt) \). Setting \( t = t_i \) in Equation \ref{eq:cg1}, we have
\begin{equation}\label{eq:cg2}
\Pr(E \geq t_i, C \geq t_i) = \phi_\theta\{S_E(t_i)\} + \phi_\theta\{S_C(t_i)\}
\end{equation}
In the left-side of the preceding equation, we estimate $\Pr(E > t_i, C > t_i)$ by ($n_i - 1)/n$, where $n_i - 1 = \sum_{\ell=1}^n \mathbbm{1}[t_\ell > t_i]$ is the number of event-free instances at time $t_i$. Then we have
\begin{equation}\label{eq:cg3}
\phi_\theta(\frac{n_i-1}{n}) = \phi_\theta\{S_E(t_i)\} + \phi_\theta\{S_C(t_i)\}.
\end{equation}
We set $t=c = t_i - dt$ in Equation \ref{eq:cg1} and then estimate $\Pr(E > t_i - dt, C > t_i - dt)$ by $n_i/n$. Then,
\begin{equation}\label{eq:cg4}
\phi_\theta(\frac{n_i}{n}) = \phi_\theta\{S_E(t_i - dt)\} + \phi_\theta\{S_C(t_i)\}, \delta_i = 1
\end{equation}
Equations \ref{eq:cg3} and \ref{eq:cg4} result in the system of difference equations
\begin{equation}\label{eq:cg5}
\phi_\theta(\frac{n_i - 1}{n}) - \phi_\theta(\frac{n_i}{n})  = \phi_\theta\{S_E(t_i)\} + \phi_\theta\{S_C(t_i - dt)\}, \delta_i = 1
\end{equation}
We apply the standard constraint that \( S_E(t_i - dt) = 1 \) when \( t_i \) is the earliest event time. Under this condition, the solution to the differential equations is
\begin{align}\label{eq:cg6}
\phi_\theta\{S_E(t)\} &= \sum_{t_i \leq t, \delta_i = 1}[\phi_\theta\{S_E(t_i)\} - \phi_\delta\{S_E(t_i - dt)\}] \\
&= \sum_{t_i \leq t, \delta_i = 1} \phi_\delta\big(\frac{n_i - 1}{n}\big) - \phi_\delta(\frac{n_i}{n})
\end{align}
which is equivalent to the CG estimator. Under the independence copula, given by $\phi_\delta(t) = -\log(t)$, the CG estimator is equivalent to the Kaplan-Meier estimator. Substituting for the Clayton copula~\citep{clayton1978model}, the CG estimator is written as
\begin{equation}\label{eq:cg7}
S_E(t) = \Bigl[1 + \sum_{t_i \leq t, \delta_i = 1} \Bigl\{(\frac{n_i-1}{n})^{-\theta} - (\frac{n_i}{n})^{-\theta}\Bigr\}\Bigr]^{-1/\theta}
\end{equation}

\end{document}
