\section{Related Work}
\label{sec:related-work}

%\rl{We need to be a bit careful w this sentence bc the main contributions of the Carlini paper sort of boil down to a confidence score model -- still different from us as it works by checking how similar a given extracted candidate is to other candidates and giving confidence scores.} 

% To our knowledge, our work is the first to address ways for prompt owners to detect if their proprietary system prompt has been stolen. We contextualize our analysis and methods within the scope of prior work on topics such as the value of prompt engineering, feasibility of prompt theft, and the framing of membership inference techniques.
% \subsection{Prompt Engineering}
% Prompt engineering has emerged as an accessible approach to adapt LLMs for specific user needs **Rae et al., "Zero-Shot Learning with Deep Neural Networks"**.
% % leveraging their remarkable in-context learning capabilities **Brown et al., "Language Models are Few-Shot Learners"**.
% In-context learning **Lake and Baroni, "Hierarchical Concept Learning from a Principled Theory of Composition"** allows LLMs to acquire new skills by providing exemplars within the prompt, without retraining. A prominent technique is few-shot prompting **Bengio et al., "Advances in Pre-Trained Language Models"**, where the design of exemplars, such as their selection, ordering, and formatting, significantly impacts output quality **Stoller et al., "On the Use of Few-Shot Learning for Natural Language Processing"**. Another line of work focuses on chain-of-thought prompting  **Lake and Baroni, "Hierarchical Concept Learning from a Principled Theory of Composition"**, which encourages LLMs to express their thought process before delivering the final answer, often leading to improved performance on reasoning tasks **Stoller et al., "On the Use of Few-Shot Learning for Natural Language Processing"**. 
% % Numerous variations of chain-of-thought prompting have been proposed, including zero-shot  **Bengio et al., "Advances in Pre-Trained Language Models"**, automatic  **Lake and Baroni, "Hierarchical Concept Learning from a Principled Theory of Composition"**, uncertainty-routed  **Stoller et al., "On the Use of Few-Shot Learning for Natural Language Processing"**, and others  **Rae et al., "Zero-Shot Learning with Deep Neural Networks"**.
% Similarly, self-criticism techniques improve language models by encouraging them to criticize and refine their own outputs  **Brown et al., "Language Models are Few-Shot Learners"**.

% % such as self-calibration  **Rae et al., "Zero-Shot Learning with Deep Neural Networks"**, self-refinement  **Lake and Baroni, "Hierarchical Concept Learning from a Principled Theory of Composition"**, reverse chain-of-thought  **Stoller et al., "On the Use of Few-Shot Learning for Natural Language Processing"**, self-verification  **Bengio et al., "Advances in Pre-Trained Language Models"**, and chain-of-verification  **Brown et al., "Language Models are Few-Shot Learners"**.

% Zero-shot prompting techniques, closely related to system prompts, include role prompting  **Rae et al., "Zero-Shot Learning with Deep Neural Networks"**, emotion prompting  **Lake and Baroni, "Hierarchical Concept Learning from a Principled Theory of Composition"**, rephrase and respond  **Bengio et al., "Advances in Pre-Trained Language Models"**, and self-ask  **Stoller et al., "On the Use of Few-Shot Learning for Natural Language Processing"**. System prompts play a crucial role in shaping LLM outputs and driving performance across application domains  **Brown et al., "Language Models are Few-Shot Learners"**.

% with tuned system prompts often being valuable enough to be sold at online marketplaces.\footnote{See \url{https://prompti.ai/chatgpt-prompt/}, \url{https://promptbase.com/.}}

\subsection{Prompt extraction attacks}
\looseness=-1
Prompt engineering has emerged as an accessible approach to adapt LLMs for specific user needs  **Rae et al., "Zero-Shot Learning with Deep Neural Networks"**, with system prompts playing a crucial role in shaping LLM outputs and driving performance across application domains  **Brown et al., "Language Models are Few-Shot Learners"**.
% Given the value of prompts, we can reasonably expect efforts to steal them. 
Prior work has proposed several prompt extraction attacks, which deduce the content of a proprietary system prompt by interacting with a model, both for language models  **Carlini et al., "Hidden-Space Attacks on Language Models"** and for image generation models  **Xu et al., "Prompt Engineering: A Survey and Open Challenges"**. 
____ frame the problem as model inversion, where they deduce the prompt given next token probabilities.
Similarly, ____ propose a method to extract prompts from sampled generative model outputs. 
Furthermore, ____ describe a way to uncover system prompts using context and response pairs.
Additionally, ____ present an evaluation of prompt extraction attacks for a variety of modern LLMs. 
In contrast to the works on inversion style methods, one can also find adversarial inputs that jailbreak LLMs  **Elsayed et al., "Adversarial Attacks against Language Models"** and even lead them to eliciting the system prompt in the response. Both ____ and ____ use optimization over prompt tokens to provoke LLMs to respond by quoting their own system prompts. Prompt reconstruction methods can also be adapted to solve the problem of prompt verification through comparing the reconstructed prompt to the reference prompt, however, their high computational cost  **Carlini et al., "Hidden-Space Attacks on Language Models"**, the need to access model gradients  **Xu et al., "Prompt Engineering: A Survey and Open Challenges"**, and imperfect reconstruction success rate  **Rae et al., "Zero-Shot Learning with Deep Neural Networks"** motivate the development of methods specifically tailored to the problem of prompt reuse verification.
% This body of work convinces us that prompts can be stolen, and those developing valuable prompts should be interested in methods to detect such theft. 
% \valeriia{position our paper here}

\subsection{Data membership inference and extraction attacks on language models}

In the evolving discussion on data privacy, a significant topic is membership inference, which involves determining whether a particular data point is part of a model's training set  **Shokri et al., "Membership Inference Attacks Against Machine Learning Models"**. 
% Nonetheless, we are motivated by research on training data membership inference as methods that work in that setting may be interesting directions for those concerned with prompt membership inference.
____ and ____ both propose methods to determine membership in the training data based on the idea that models tend to behave differently on their training data than on other data. ____ further propose a more effective method and alleviate the need to know the target model's architecture, while ____ propose perturbing the query data to improve accuracy of their attack. 
____ consider the setting where the system includes an ensemble of models that may be updated over time. Other works explore training data membership inference in image generation models  **Wang et al., "Membership Inference Attacks on Image Generators"**. Additionally, dataset inference techniques explore settings where the whole training set is considered rather than single data points ____.
Compared to the standard membership inference setting, our work addresses a related but distinct question: whether a given text is part of the LLM input context, thus exploring prompt membership inference. 
% While the goal differs between training data membership and prompt membership attacks, the problem formulation remains similar.
%Finally, while we focus on system prompt verification, statistical methods have been widely applied to verify LLM behaviors across various contexts ____