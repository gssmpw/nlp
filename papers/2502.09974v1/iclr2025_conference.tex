
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{url}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage[bottom]{footmisc}
% \usepackage{algorithmic}
\usepackage[noEnd=True]{algpseudocodex}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


\title{Has My System Prompt Been Used? Large Language Model Prompt Membership Inference}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Roman Levin \thanks{Equal Contribution. Correspondence to: romlevin@amazon.com, cherepv@amazon.com. Code is available on GitHub: \url{github.com/amazon-science/prompt-membership-inference}} \\
Amazon\\
\And
Valeriia Cherepanova $^*$\\
Amazon \\
\And
Abhimanyu Hans $^*$\\
University of Maryland \\
\AND 
Avi Schwarzschild \\
Carnegie Mellon University\\
\And
Tom Goldstein \\ 
University of Maryland \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Prompt engineering has emerged as a powerful technique for optimizing large language models (LLMs) for specific applications, enabling faster prototyping and improved performance, and giving rise to the interest of the community in protecting proprietary system prompts. In this work, we explore a novel perspective on prompt privacy through the lens of membership inference. We develop Prompt Detective, a statistical method to reliably determine whether a given system prompt was used by a third-party language model. Our approach relies on a statistical test comparing the distributions of two groups of model outputs corresponding to different system prompts. Through extensive experiments with a variety of language models, we demonstrate the effectiveness of Prompt Detective for prompt membership inference. %in both standard and challenging scenarios, including black-box settings. 
Our work reveals that even minor changes in system prompts manifest in distinct response distributions, enabling us to verify prompt usage with statistical significance.
\end{abstract}

\begin{figure*}[b!]
\begin{center}
\includegraphics[width=0.999\textwidth]{figures/diagram-prompt-detective.drawio.png.pdf}
\caption{\textbf{Prompt Detective} verifies if a third-party chat bot uses a given proprietary system prompt by querying the system and comparing distribution of outputs with outputs obtained using proprietary system prompt.}
\label{fig:prompt-detective-idea}
\end{center}
\end{figure*}

\section{Introduction}
Prompt engineering offers a powerful, flexible, and fast way to optimize large language models (LLMs) for specific applications, enabling faster and cheaper customization than finetuning while delivering strong specialized performance.
Large language model providers, such as Anthropic and \mbox{OpenAI}, release detailed prompt engineering guides on prompting strategies allowing their customers to reduce hallucination rates and optimize business performance \citep{openai_prompt_engineering, anthropic_prompt_library}. The use of system prompts also provides specialized capabilities such as taking on a character which is often leveraged by startups in their products\footnote{\url{https://character.ai/}}. Developers put significant effort into prompt engineering and prompts optimized for specific use-cases are even sold at online marketplaces\footnote{\url{https://prompti.ai/chatgpt-prompt/}, \url{https://promptbase.com/}.}.





% In fact, legal experts suggest that language model prompts may potentially be patentable \footnote{https://ipwatchdog.com/2024/01/02/can-ai-prompts-patented-dont-quick-dismiss-question/id=171338/}.
\looseness=-1
The importance and promise of prompt engineering gave rise to the interest of the community in protecting proprietary prompts and a growing body of academic literature explores prompt reconstruction attacks \citep{hui2024pleak, zhang2024effective, morris2023language, geiping2024coercing} which attempt to recover a prompt used in a language model to produce particular generations. These methods achieve impressive results in approximate prompt reconstruction, however their reconstruction success rate is not high enough to be able to confidently verify the prompt reuse, they are computationally expensive usually relying on GCG-style optimization \citep{zou2023universal}, and some of these methods require access to model gradients \citep{geiping2024coercing}. Additionally, while some reconstruction methods provide confidence scores \citep{zhang2024effective}, they do not offer statistical guarantees for prompt usage verification.  

% As prompt reconstruction is inherently a sophisticated problem, it is challenging for these methods to consistently recover prompts with high accuracy. Additionally, while some reconstruction methods provide confidence scores \citep{zhang2024effective}, they do not offer statistical guarantees for prompt usage verification.  


% prompt reconstruction approaches usually do not offer certifiable ways to verify if the recovered prompt was indeed used.

% by a language model.

% In LLM-based chatbots, control over part of the input is given to the end user. Consider a customer service chatbot that employs a general LLM to help customers get the answers they need. These systems typically add the user input into a longer template that includes a system prompt with application-specific instructions to help ensure that the backend large general purpose language model returns useful content to the user. Note the value in an expertly written system prompt --- it could be critical in getting quality responses from large back end LLMs like GPT-4. Our threat model relates to someone who is suspicious that the closed system prompt in use here might be stolen from their own chatbot.

In this work, we specifically focus on the problem of verifying if a particular system prompt was used in a large language model. This problem can be viewed through the lens of an adversarial setup: an attacker may have reused someone else's proprietary system prompt and deployed an LLM-based chat bot with it. In LLM-based chatbots, control over part of the input is given to the end user. Consider a customer service chatbot that employs a general LLM to help customers get the answers they need. These systems typically add the user input into a longer template that includes a system prompt with application-specific instructions to help ensure that the back end large general purpose language model returns useful content to the user. Note the value in an expertly written system prompt -- it could be critical in getting quality responses from large back end LLMs. Assuming access to querying this chatbot, can we verify with statistical significance if the proprietary system prompt has not been used? In other words, we develop a method for system prompt membership inference. 
% which verify whether a particular prompt was used in a large language model.
Our contributions are as follows:
\begin{itemize}
\item We develop Prompt Detective, a training-free statistical method to reliably verify whether a given system prompt was used by a third-party language model, assuming query access to it.
\item We extensively evaluate the effectiveness of Prompt Detective across a variety of language models, including Llama, Mistral, Claude, and GPT families including challenging scenarios such as distinguishing similar system prompts and black-box settings.
\item Our work reveals that even minor changes in system prompts manifest in distinct response distributions of LLMs, enabling Prompt Detective to verify prompt usage with statistical significance. This highlights that LLMs take specific trajectories when generating responses based on the provided system prompt.

\end{itemize}

\section{Related Work}
\label{sec:related-work}

%\rl{We need to be a bit careful w this sentence bc the main contributions of the Carlini paper sort of boil down to a confidence score model -- still different from us as it works by checking how similar a given extracted candidate is to other candidates and giving confidence scores.} 

% To our knowledge, our work is the first to address ways for prompt owners to detect if their proprietary system prompt has been stolen. We contextualize our analysis and methods within the scope of prior work on topics such as the value of prompt engineering, feasibility of prompt theft, and the framing of membership inference techniques.
% \subsection{Prompt Engineering}
% Prompt engineering has emerged as an accessible approach to adapt LLMs for specific user needs \citep{liu2023pre}.
% % leveraging their remarkable in-context learning capabilities \citep{brown2020language, radford2019language}.
% In-context learning \citep{brown2020language, radford2019language} allows LLMs to acquire new skills by providing exemplars within the prompt, without retraining. A prominent technique is few-shot prompting \citep{brown2020language}, where the design of exemplars, such as their selection, ordering, and formatting, significantly impacts output quality \citep{zhao2021calibrate, lu2021fantastically, ye2023explanation}, and many-shot prompting can even match the power of fine-tuning \citep{scao2021many, agarwal2024many}. Another line of work focuses on chain-of-thought prompting \citep{wei2022chain, chu2023survey} which encourages LLMs to express their thought process before delivering the final answer, often leading to improved performance on reasoning tasks \citep{kojima2022large, zhang2022automatic, team2023gemini, zheng2023take, yasunaga2023large, zhou2023thread}. 
% % Numerous variations of chain-of-thought prompting have been proposed, including zero-shot \citep{kojima2022large}, automatic \citep{zhang2022automatic}, uncertainty-routed \citep{team2023gemini}, and others \citep{zheng2023take, yasunaga2023large, zhou2023thread}.
% Similarly, self-criticism techniques improve language models by encouraging them to criticize and refine their own outputs \citep{kadavath2022language, madaan2024self, xue2023rcot, weng2022large, dhuliawala2023chain}.

% % such as self-calibration \citep{kadavath2022language}, self-refinement \citep{madaan2024self}, reverse chain-of-thought \citep{xue2023rcot}, self-verification \citep{weng2022large}, and chain-of-verification \citep{dhuliawala2023chain}.

% Zero-shot prompting techniques, closely related to system prompts, include role prompting \citep{wang2023rolellm, zheng2023helpful}, emotion prompting \citep{li2023large}, rephrase and respond \citep{deng2023rephrase}, and self-ask \citep{press2022measuring}. System prompts play a crucial role in shaping LLM outputs and driving performance in application domains \citep{ng2023neurips}.

% with tuned system prompts often being valuable enough to be sold at online marketplaces.\footnote{See \url{https://prompti.ai/chatgpt-prompt/}, \url{https://promptbase.com/}.}

\subsection{Prompt extraction attacks}
\looseness=-1
Prompt engineering has emerged as an accessible approach to adapt LLMs for specific user needs \citep{liu2023pre}, with system prompts playing a crucial role in shaping LLM outputs and driving performance across application domains \citep{ng2023neurips}.
% Given the value of prompts, we can reasonably expect efforts to steal them. 
Prior work has proposed several prompt extraction attacks, which deduce the content of a proprietary system prompt by interacting with a model, both for language models \citep{morris2023language, zhang2024effective, sha2024prompt, yang2024prsa} and for image generation models \citep{wen2024hard}. 
\citet{morris2023language} frame the problem as model inversion, where they deduce the prompt given next token probabilities.
Similarly, \citet{sha2024prompt} propose a method to extract prompts from sampled generative model outputs. 
Furthermore, \citet{yang2024prsa} describe a way to uncover system prompts using context and response pairs.
Additionally, \citet{zhang2024effective} present an evaluation of prompt extraction attacks for a variety of modern LLMs. 
In contrast to the works on inversion style methods, one can also find adversarial inputs that jailbreak LLMs \citep{zou2023universal, cherepanova2024talking, geiping2024coercing} and even lead them to eliciting the system prompt in the response. Both \citet{hui2024pleak} and \citet{geiping2024coercing} use optimization over prompt tokens to provoke LLMs to respond by quoting their own system prompts. Prompt reconstruction methods can also be adapted to solve the problem of prompt verification through comparing the reconstructed prompt to the reference prompt, however, their high computational cost \citep{hui2024pleak, geiping2024coercing}, the need to access model gradients \citep{geiping2024coercing}, and imperfect reconstruction success rate \citep{hui2024pleak, zhang2024effective, geiping2024coercing} motivate the development of methods specifically tailored to the problem of prompt reuse verification.
% This body of work convinces us that prompts can be stolen, and those developing valuable prompts should be interested in methods to detect such theft. 
% \valeriia{position our paper here}

\subsection{Data membership inference and extraction attacks on language models}

In the evolving discussion on data privacy, a significant topic is membership inference, which involves determining whether a particular data point is part of a model's training set \citep[e.g.][]{yeom2018privacy, sablayrolles2019white, salem2018ml, song2021systematic, hu2022membership}. 
% Nonetheless, we are motivated by research on training data membership inference as methods that work in that setting may be interesting directions for those concerned with prompt membership inference.
\citet{shokri2017membership} and \citet{carlini2022membership} both propose methods to determine membership in the training data based on the idea that models tend to behave differently on their training data than on other data. \citet{bertran2024scalable} further propose a more effective method and alleviate the need to know the target model's architecture, while \citet{wen2022canary} propose perturbing the query data to improve accuracy of their attack. 
\citet{jagielski2023combine} consider the setting where the system includes an ensemble of models that may be updated over time. Other works explore training data membership inference in image generation models \citep{duan2023diffusion, matsumoto2023membership}. Additionally, dataset inference techniques explore settings where the whole training set is considered rather than single data points \citep{maini2021dataset, maini2024llm}.
Compared to the standard membership inference setting, our work addresses a related but distinct question: whether a given text is part of the LLM input context, thus exploring prompt membership inference. 
% While the goal differs between training data membership and prompt membership attacks, the problem formulation remains similar.
%Finally, while we focus on system prompt verification, statistical methods have been widely applied to verify LLM behaviors across various contexts \citep{chaudhary2024quantitative, kumar2024certifying, kang2024c}.



\section{Prompt Detective}
\subsection{Setup}
Prompt Detective aims to verify whether a particular known system prompt is used by a third-party chat bot as shown in Figure \ref{fig:prompt-detective-idea}. In our setup, we assume an API or online chat access to the model, that is, we can query the chat bot with different task prompts and we have control over choosing these task prompts. We also assume the knowledge about which model is employed by the service in most of our experiments, and we explore the black-box scenario in section \ref{sec: black_box}. 

This setup can be applied when a user, who may have spent significant effort developing the system prompt for their product such as an LLM character or a domain-specific application, suspects that their proprietary system prompt has been utilized by a third-party chat service effectively replicating the behavior of their product, and wants to verify if that was in fact the case while only having online chat window access to that service.
% . While the user may have spent significant effort developing and refining their system prompt for their product such as an LLM character or an application to a particular domain, the adversary gets to effectively replicate the behavior of such product by reusing the system prompt. 
% However, if the adversary makes their chat bot publicly available giving the user -- the original prompt owner -- a chat or API access, the user is then able to employ Prompt Detective to verify with statistical significance whether their proprietary prompt is used in the adversary's chat bot. 
We note that prompt engineering is a much less resource-intensive task than developing or fine-tuning a custom language model, therefore, it is reasonable to assume that such chat bots which reuse system prompts are based on one of the publicly available language models such as API-based GPT models \citep{achiam2023gpt}, Claude models \citep{Claude3}, or open source models like Llama or Mistral \citep{touvron2023llama, jiang2023mistral}.


% This setup can be applied when a user suspects that their proprietary system prompt has been stolen -- utilized by a third-party chat service without authorization. While the user may have spent significant effort developing and refining their system prompt for their product such as an LLM character or an application to a particular domain, the adversary gets to effectively replicate the behavior of such product by stealing the system prompt. However, if the adversary makes their chat bot publicly available giving the user -- the original prompt owner -- a chat or API access, the user is then able to employ Prompt Detective to verify with statistical significance whether their proprietary prompt is used in the adversary's chat bot. We note that prompt engineering is a much less resource-intensive task than developing or fine-tuning a custom language model, therefore, it is reasonable to assume that chat bots which utilize stolen system prompts are based on one of the publicly available language models such as API-based GPT models \citep{achiam2023gpt}, Claude models \citep{Claude3}, or open source models like LLAMA or Mistral \citep{touvron2023llama, jiang2023mistral}.

Moreover, this adversarial setup can be seen through the lens of membership inference attacks, where instead of verifying membership of a given data sample in the training data of a language model, we verify membership of a particular system prompt in the context window of a language model. We therefore refer to our adversarial setting as {\it prompt membership inference}.

\subsection{How does it work?}

\begin{algorithm*}[t] \caption{Prompt Detective} \label{alg:prompt_detective}
\begin{algorithmic}
\Require Third-party language model $f_p$, \\
Known (proprietary) system prompt $\bar{p}$, \\
Model $\bar{f}_{\bar{p}}$,\\
Task prompts $q_1, \ldots, q_n$,\\
Number of responses per task prompt $k$,\\
Significance level $\alpha$
\State $G_1 \gets \{\{f_p(q_1)^1...f_p(q_1)^k\}, \ldots, \{f_p(q_n)^1...f_p(q_n)^k\}\}$ \Comment{Generations from third-party model}
\State $G_2 \gets \{\{\bar{f}_{\bar{p}}(q_1)^1...\bar{f}_{\bar{p}}(q_1)^k\}, \ldots, \{\bar{f}_{\bar{p}}(q_n)^1...\bar{f}_{\bar{p}}(q_n)^k\}\}$ \Comment{Generations from known prompt}
\State $V_1 \gets \text{BERT}(G_1)$ \Comment{BERT embeddings of $G_1$}
\State $V_2 \gets \text{BERT}(G_2)$ \Comment{BERT embeddings of $G_2$}
\State $\mu_1 \gets \text{Mean}(V_1)$, $\mu_2 \gets \text{Mean}(V_2)$ \Comment{Mean vectors}
\State $s_{\text{obs}} \gets \text{CosineSimilarity}(\mu_1, \mu_2)$ \Comment{Observed cosine similarity}
\State $c \gets 0$ \Comment{Counter for extreme cosine similarities}
\For{$i = 1$ to $N_{\text{permutations}}$} \Comment{Permutation test loop}
    \State $V_1^* \gets V_1$, $V_2^* \gets V_2$ \Comment{Initialize permuted groups}
    \For{$j = 1$ to $n$} \Comment{Shuffle preserving the task prompt structure}
        \State $V_{\text{combined}} \gets V_1^*[(j-1)k:jk] \cup V_2^*[(j-1)k:jk]$ \Comment{Concatenate responses}
        \State $V_{\text{combined}} \gets \text{Shuffle}(V_{\text{combined}})$ \Comment{Permute combined responses}
        \State $V_1^*[(j-1)k:jk] \gets V_{\text{combined}}[:k]$ \Comment{Assign first part to $V_1^*$}
        \State $V_2^*[(j-1)k:jk] \gets V_{\text{combined}}[k:]$ \Comment{Assign second part to $V_2^*$}
    \EndFor
    \State $\mu_1^* \gets \text{Mean}(V_1^*)$, $\mu_2^* \gets \text{Mean}(V_2^*)$
    \State $s^* \gets \text{CosineSimilarity}(\mu_1^*, \mu_2^*)$
    \If{$s^* \leq s_{\text{obs}}$} \Comment{Check if new similarity is as extreme}
        \State $c \gets c + 1$ \Comment{Increment counter for extreme similarities}
    \EndIf
\EndFor
\State $p \gets c / N_{\text{permutations}}$ \If{$p < \alpha$} \State \textbf{return} "Prompts are distinct" \Else \State \textbf{return} "Insufficient evidence to claim prompts are distinct" \EndIf
\end{algorithmic}
\end{algorithm*}

%We assume that a third-party generative language model $f_p$ is prompted with an unknown system prompt $p$, and that we can query the service with task prompts $q$ to get generations $f_p(q)$. We also assume access to a similar model prompted with our known proprietary system prompt $\bar{p}$, that is $\bar{f}{\bar{p}}$. Our goal is to determine whether $p$ and $\bar{p}$ are distinct.
\vspace{-1pt}

\looseness=-1
Let $f$ denote a language model, let $p$ be a system prompt, and let $q$ be a task prompt.
Together, we denote the full output as $f_p(q)$.
For example, a system prompt could look like ``You are a helpful assistant'' and a task-specific query might be like ``Can you help me with a billing issue?''
If we applied the appropriate chat template, the full string we pass to the model's tokenizer would look as follows:

\begin{verbatim}
    [SYS] You are a helpful assistant [\SYS]
    
    [USER] Can you help me with a billing issue?[\USER]
\end{verbatim}

We assume the model owner uses an unknown system prompt $p$, and that we can query the service with task prompts $q$ to get output $f_p(q)$. 
We also assume access to a similar model prompted with our known proprietary system prompt $\bar{p}$, whose output is denoted by $\bar{f}{\bar{p}}$. 
Our goal is to determine whether $p$ and $\bar{p}$ are distinct.

\paragraph{Core idea.} %Prompt Detective is a training-free statistical method designed for this purpose. 
Prompt Detective is a training-free statistical method designed specifically for determining if a system prompt used in an LLM-based service matches a known string. The core idea is to compare the distributions of two groups of generations corresponding to different system prompts and apply a statistical test to assess if the distributions are significantly different, which would indicate that the system prompts are distinct. That is, Prompt Detective compares the distributions of high-dimensional vector representations of two groups of generations
$$ \{f_p(q_i)^j\}_{i\in [1, ..., n], j\in[1,...,k]} \;\;  \text{and} \;\ \{\bar{f}_{\bar{p}}(q_i)^j\}_{i\in [1, ..., n], j\in[1,...,k]}, $$
where the first set of generations is obtained from the third-party service $f_p$ prompted with task queries $q_i$ (with $k$ responses sampled for each task query), and the second set of generations is obtained from the $\bar{f}{\bar{p}}$ model prompted with the proprietary prompt $\bar{p}$ and the same task queries.


% $f_p(q_1)^1, ... , f_p(q_1)^k, \ldots, f_p(q_n)^1, ... ,f_p(q_n)^k$ obtained from the third-party service $f_p$ prompted with task queries $q_1,...,q_n$ (with $k$ responses sampled for each task query) and generations  $\bar{f}_{\bar{p}}(q_1)^1, ... ,\bar{f}_{\bar{p}}(q_1)^k,\ldots,\bar{f}_{\bar{p}}(q_n)^1, ... , \bar{f}_{\bar{p}}(q_n)^k$ from the $\bar{f}{\bar{p}}$ model prompted with the proprietary prompt $\bar{p}$ and the same task queries.


% The list of possible ways to convert chat bot responses into high-dimensional vector representations is truly endless and the text representation method is ultimately a design choice in Prompt Detective. 
\paragraph{Text representations.} %We simply utilized BERT \citep{reimers2019sentence} embeddings in our experiments. 
We use BERT embedding \citep{reimers2019sentence} to map strings to representation vectors. We compute the BERT embeddings for both 
$ \{f_p(q_i)^j\}_{i\in [1, ..., n], j\in[1,...,k]}$ and $\{\bar{f}_{\bar{p}}(q_i)^j\}_{i\in [1, ..., n], j\in[1,...,k]}, $
%$f_p(q_1)^1, ... , f_p(q_1)^k, \ldots, f_p(q_n)^1, ... ,f_p(q_n)^k$ and $\bar{f}_{\bar{p}}(q_1)^1, ... ,\bar{f}_{\bar{p}}(q_1)^k,\ldots,\bar{f}_{\bar{p}}(q_n)^1, ... , \bar{f}_{\bar{p}}(q_n)^k$ 
yielding two groups of high-dimensional vector representations of generations corresponding to the two system prompts under comparison. We include results for ablation study on embedding models in Appendix \ref{sec:additional_results} Table \ref{tab: embeddings}.

%\textcolor{red}{As a variation of Prompt Detective, we also experimented with computing BERT embeddings with the system prompt $\bar{p}$ prepended to chat bot responses for context enhancement.} \rl{This is a little tricky given we don't mention a validation set. While we did experiment with these methodological improvements on toy datasets prior to running any experiments, we have to be careful on how we phrase this, maybe just have an ablation} \valeriia{I don't think we need it for now?}




\paragraph{Statistical test of the equality of representation distributions.} To compare the distributions of these two groups, we employ a permutation test \citep{good2013permutation} with the cosine similarity between the mean vectors of the groups used as the test statistic. The permutation test is a non-parametric approach that does not make assumptions about the underlying distribution of the data, making it a suitable choice for Prompt Detective. Intuitively, the permutation test assesses whether the observed difference between the two groups of generations 
% (as measured by the cosine similarity between their mean vectors) 
is significantly larger than what would be expected by chance if the generations were not influenced by the underlying system prompts. By randomly permuting the responses within each task prompt across the two groups, the test generates a null distribution of cosine similarities between their mean vectors under the assumption that the system prompts are identical, while preserving the task prompt structure. The observed cosine similarity is then compared against this null distribution to determine its statistical significance. Algorithm \ref{alg:prompt_detective} outlines all of the steps of Prompt Detective in detail.

% \rl{Similarly, this is a design choice in Prompt Detective, maybe we can mention that in discussion or here} \valeriia{Intuitively, the test rejects blah blah blah}



% The key idea behind Prompt Detective is that if two system prompts are distinct, they will induce different distributions over the generated text, which will manifest in the high-dimensional BERT embeddings. By comparing the distributions using a permutation test based on the cosine similarity between mean vectors, we can reliably detect whether the system prompts are the same or different, even in challenging scenarios such as black-box settings.

\begin{figure*}[!t]
\begin{center}
\includegraphics[width=0.99\textwidth]{figures/hard-examples-zoom.drawio.pdf}
\caption{\textbf{Hard Examples} illustrate varying degrees of similarity between the original prompts and their rephrased versions. Similarity Level 1 is highly similar, while Level 5 is completely different.}
\label{fig:hard-examples}
\end{center}
\end{figure*}

\subsection{Task queries}
The selection of task prompts $q_1, \ldots, q_n$ is an important component of Prompt Detective, as these prompts serve as probes to elicit responses that are influenced by the underlying system prompt. Since we assume control over the task prompts provided to the third-party chat bot, we can strategically choose them to reveal differences in the response distributions induced by distinct system prompts.

We consider a task prompt a good probe for a given system prompt if it elicits responses that are directly influenced by and related to the system prompt. For example, if the system prompt is designed for a particular LLM persona or role, task prompts that encourage the model to express its personality, opinions, or decision-making processes would be effective probes. 
% Similarly, if the system prompt is tailored for a specific domain or task, prompts that require domain-specific knowledge or task-specific reasoning would be suitable choices. 
A diverse set of task prompts can be employed to increase the robustness of Prompt Detective.
% and mitigate the risk of relying on a single prompt that may not effectively distinguish between the system prompts.
In practice, we generated task queries for each of the system prompts $\bar{p}$ in our experiments with the Claude 3 Sonnet \citep{Claude3} language model unless otherwise noted (see Appendix \ref{sec: appendix Labels}).

% It is worth noting that while carefully curated task prompts can enhance the effectiveness of Prompt Detective, the method is designed to be general and does not inherently rely on any specific set of task queries. As long as the task prompts elicit responses that are influenced by the system prompt, Prompt Detective can reliably detect differences in the underlying distributions, even in challenging scenarios where the system prompts may be similar or the task prompts are not optimally chosen.


\section{Experimental Setup}

\subsection{System prompt sources}
\label{sec: system-prompt-sources}
% We conduct experiments with the following sources of system prompts.
% : Awesome-ChatGPT-Prompts and Anthropic's Prompt Library.

{\bf Awesome-ChatGPT-Prompts \footnote{https://github.com/f/awesome-chatgpt-prompts}} is a curated collection of 153 system prompts that enable users to tailor LLMs for specific roles. This dataset includes prompts for creative writing, programming, productivity, etc. Prompts are designed for various functions, such as acting as a Startup Idea Generator, Python Interpreter, or Personal Chef. 
% The accompanying task prompts can be found in Appendix \ref{sec: appendix A}.
The accompanying task prompts were generated with Claude 3 Sonnet (see Appendix \ref{sec: appendix Labels}).
For the 153 system prompts in Awesome-ChatGPT, we generated overall 50 task prompts. In these experiments, while a given task prompt is not necessarily a good probe for every system prompt, these 50 task prompts include at least one good probe for each of the system prompts.

{\bf Anthropic's Prompt Library \footnote{https://docs.anthropic.com/en/prompt-library/library}} provides detailed prompts that guide models into specific characters and use cases. For our experiments, we select all of the personal prompts from the library that include system prompts giving us 20 examples. Personal prompts include roles such as Dream Interpreter or Emoji Encoder. As the accompanying task prompts, we used 20 of the corresponding user prompts provided in the library.
% task prompts due to their broader applicability and increased detection difficulty compared to business task prompts. Personal task prompts include roles such as Dream Interpreter and Emoji Encoder.

{\bf Hard Examples:} To evaluate the robustness of Prompt Detective in challenging scenarios, we create a set of hard examples by generating variations of prompts from Anthropic's Prompt Library. These variations are designed to have different levels of similarity to the original prompts, ranging from minimal rephrasing to significant conceptual changes, producing varying levels of difficulty for distinguishing them from the original prompts.

For each system prompt from Anthropic's Prompt Library, we generate five variations with the following similarity levels (see Figure \ref{fig:hard-examples} for examples):

\begin{enumerate} \item \textbf{Same Prompt, Minimal Rephrasing}: The same prompt, slightly rephrased with minor changes in a few words. \item \textbf{Same Prompt, Minor Rephrasing}: Very similar in spirit, but somewhat rephrased. \item \textbf{Same Prompt, Significant Rephrasing}: Very similar in spirit, but significantly rephrased. \item \textbf{Different Prompt, Remote Similarities}: A different prompt for the same role with some remote similarities to the original prompt. \item \textbf{Different Prompt, Significant Conceptual Changes}: A completely different prompt for the same role with significant conceptual changes. \end{enumerate}
\looseness=-1
This process results in a total of 120 system prompts for hard examples. 
% These hard examples are used to evaluate the robustness of Prompt Detective in distinguishing between similar prompts. 
The system prompt variations and the accompanying task prompts were generated with the Claude 3 Sonnet model. For the hard example experiments, we generated 10 specific probe task queries per each of the original system prompts (see Appendices \ref{sec:appendix_data},\ref{sec: appendix Labels}).% giving us 200 task queries in total. 
%\valeriia{Mention temperature}




\subsection{Models}
We conduct our experiments with a variety of open-source and API-based models, including Llama2 13B \citep{touvron2023llama}, Llama3 70B \footnote{https://ai.meta.com/blog/meta-llama-3/}, Mistral 7B \citep{jiang2023mistral}, Mixtral 8x7B \citep{jiang2024mixtral}, Claude 3 Haiku \citep{Claude3}, and GPT-3.5 \citep{achiam2023gpt}.

\subsection{Evaluation: standard and hard examples} In the standard setup, to evaluate Prompt Detective, we construct pairs of system prompts representing two scenarios: (1) where the known system prompt $\bar{p}$ is indeed used by the language model (positive case), and (2) where the known system prompt $\bar{p}$ differs from the system prompt $p$ used by the model (negative case). The positive case simulates a situation where the proprietary prompt has been reused, while the negative case represents no prompt reuse. 

\looseness=-1
We construct a positive pair $(\bar{p}, \bar{p})$ for each of the system prompts and randomly sample the same number of negative pairs $(\bar{p}, p), \bar{p}\not=p$. The negative pairs may not represent similar system prompts, and we refer to this setting as the standard setup.

For the hard example setup, we construct prompt pairs using the variations of the Anthropic Prompt Library prompts with different levels of similarity, as described in section \ref{sec: system-prompt-sources}. The first prompt in each pair is the original prompt from the library, while the second prompt is one of the five variations, ranging from minimal rephrasing to significant conceptual changes. That is, while in this setup there are no positive pairs using identical prompts, some of the pairs represent extremely similar prompts differing by only very few words replaced with synonyms.


\section{Results}

\subsection{Prompt Detective can distinguish system prompts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%RESULTS TABLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/standard_setup}

Table \ref{tab: standard-setup-results} shows the effectiveness of Prompt Detective in distinguishing between system prompts in the standard setup across different models and prompt sources. We report the false positive rate (FPR) and false negative rate (FNR) at a standard $p$-value threshold of 0.05, along with the average $p$-value for both positive and negative prompt pairs. In all models except for Claude on AwesomeChatGPT dataset, Prompt Detective consistently achieves a zero false positive rate, and the false negative rate remains approximately 0.05. This rate corresponds to the selected significance level, indicating the probability of Type I error -- rejecting the null hypothesis that system prompts are identical when they are indeed the same.
%most models and datasets, Prompt Detective achieves low false positive and false negative rates.
% For instance, with the Llama2-13B model on the Awesome-ChatGPT-Prompts dataset, we observe an FPR of 0.01 and an FNR of 0.00, indicating that Prompt Detective correctly identifies the vast majority of positive and negative cases.
% The Claude 3 Haiku model exhibits slightly higher false positive rates, particularly on the Awesome-ChatGPT-Prompts dataset (FPR = 0.42). However, it maintains a perfect false negative rate, ensuring that it does not miss any instances where the proprietary prompt has been stolen.
% \looseness=-1
Figure \ref{fig:more-samples-help} shows how the average $p$-value changes in negative cases (where the prompts differ) as the number of task queries increases. As expected, the $p$-value decreases with more queries, providing stronger evidence for rejecting the null hypothesis of equal distributions. Consequently, increasing the number of queries further improves the statistical test's power, allowing for the use of lower significance levels and thus ensuring a reduced false negative rate, while maintaining a low false positive rate.

While there are no existing prompt membership inference baselines, prompt reconstruction methods can be adapted to the prompt membership inference setting by comparing recovered system prompts to the reference system prompts. We compare PLeak \citep{hui2024pleak} -- one of the most high performing of the existing prompt reconstruction approaches to Prompt Detective in the prompt membership setting. We find Prompt Detective to be significantly more effective in the prompt membership inference setting and report the results in Table \ref{tab: prompt-reconstruction-baselines} of Appendix \ref{sec: comparison-to-prompt-extraction-baselines}.

\begin{figure}[b!]
    \centering
    \includegraphics[width=\linewidth]{figures/awesomegpt_ngens_with_stolen_1x4_pale.png}
    \label{fig:image2}
    \caption{{\bf Average $p$-value computed for different number of task queries. Left: Awesome-ChatGPT-Prompts. Right: Anthropic Library.} Increasing the number of generations leads to decreasing $p$-value in negative cases, but the average $p$-value for positive cases remains close to 0.5.}
    \label{fig:more-samples-help}
\end{figure}





%maybe add what happens in positive cases
%\rl{Could mention here how we operationalize p-value using the 0.05 threshold}





% \begin{figure*}[h!]
%     \centering
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/awesomegpt_ngens.png}
%         \label{fig:image2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures/anthropic_ngens.png}
%     \label{fig:image1}
% `\end{subfigure}
%     \caption{{\bf Average $p$-value computed for different number of task queries. Left: Awesome-ChatGPT-Prompts. Right: Anthropic Library.} Increasing the number of generations leads to decreasing $p$-value in negative cases, and increasing $p$-value in positive cases.}
%     \label{fig:more-samples-help}
% \end{figure*}


\subsection{Hard examples: similar system prompts}
\label{sec: hard-examples}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%RESULTS TABLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/hard_examples}

Table \ref{tab: hard-examples-results} presents the results for the challenging hard example setup, where we evaluate Prompt Detective's performance on system prompts with varying degrees of similarity to the proprietary prompt. We conduct this experiment with Claude 3 Haiku and GPT-3.5 models, testing Prompt Detective in two scenarios. First, we use 2 generations per task prompt, resulting in 20 generations for each system prompt, as in the standard setup Anthropic Library experiments. Second, we use 50 generations for each task query, resulting in 500 generations per system prompt in total. We observe that when only 2 generations are used, the false positive rate is high reaching 65\% for GPT 3.5 and Claude models in Similarity Level 1 setup, indicating the challenge of distinguishing the response distributions for two very similar system prompts. However, increasing the number of generations for each probe to 50 leads to Prompt Detective being able to almost perfectly separate between system prompts even in the highest similarity category. 

We further explore the effect of including more generations and more task prompts on Prompt Detective's performance. In Figure \ref{fig:hard_p_value}, we display the average $p$-value for Prompt Detective on Similarity Level 1 pairs versus the number of generations, the number of task prompts, and the number of tokens in the generations. We ask the following question: for a fixed budget in terms of the total number of tokens generated, is it more beneficial to include more different task prompts, more generations per task prompt, or longer responses from the model? Our observations suggest that while having more task prompts is comparable to having more generations per task prompt, it is important to have at least a few different task prompts for improved robustness of the method. However, having particularly long generations exceeding 64 tokens is not as useful, indicating that the optimal setup includes generating shorter responses to more task prompts and including more generations per task prompt.
\begin{figure}[b!]
\begin{center}
    \includegraphics[width=0.8\textwidth]{figures/hard-p-value-wide.pdf}
    \caption{{\bf Effect of the number of task prompts, generations, and tokens on the performance of Prompt Detective.} Average $p$-value for GPT-3.5 model on system prompts of Similarity Level 1. The left panel shows the average $p$-value vs. the number of generations used in Prompt Detective. The blue line represents results with 10 task prompts and 5–50 generations (512 tokens long) per prompt. The red line represents results with 1–10 task prompts, each with 50 generations (512 tokens long). The right panel plots \( p^n_{avg} \) against the total number of tokens generated, with the green line showing results using 10 task prompts and 50 shorter generations (16–512 tokens long)}
    \label{fig:hard_p_value}
\end{center}
\end{figure}

We additionally find that Prompt Detective successfully distinguishes prompts in two case studies of special interest: (1) variations of the generic {\it``You are a helpful and harmless AI assistant''} common in chat applications, and (2) system prompts that differ only by a typo as an example of extreme similarity (see Appendix \ref{sec:case_study} for details).  
% To explore the feasibility of separation in even harder settings, we explore case studies of extreme prompt similarity 

% \valeriia{Mention Case Studies here}
% \rl{It would be nice to say "in general about 300 queries is sufficient to distinguish system prompts at similarity level 1", maybe in discussion}

\section{Black Box Setup}
\label{sec: black_box}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%RESULTS TABLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/black_box.tex}
So far we assumed the knowledge of the third-party model used to produce generations, and in this section we explore the black-box setup where the exact model is unknown. As mentioned previously, it is reasonable to assume that chat bots which reuse system prompts likely rely on one of the widely used language model families. To simulate such scenario, we now say that all the information Prompt Detective has is that the third party model $f_p$ is one of the six models used in our previous experiments. We then compare the generations of $f_p$ against each model $\{\bar{f}^i_{\bar{p}}\}_{i=1}^6$ used as reference and take the maximum $p$-value.
% Therefore, to evaluate effectiveness of Prompt Detective in the black-box setup, we run Prompt Detective against each model used in the experiment. 
Because of the multiple-comparison problem in this setup, we apply the Bonferroni correction to the $p$-value threshold to maintain the overall significance level of $0.05$. Table \ref{tab: black-box-results} displays the results for Prompt Detective in the black-box setup. We observe that, while false positive rates are slightly higher compared to the standard setup, Prompt Detective maintains its effectiveness, which demonstrates its applicability in realistic scenarios where the adversary's model is not known. 



\section{Discussion}
We introduce Prompt Detective, a method for verifying with statistical significance whether a given system prompt was used by a language model and we demonstrate its effectiveness in experiments across various models and setups. 

The robustness of Prompt Detective is highlighted by its performance on hard examples of highly similar system prompts and even prompts that differ only by a typo. The number of task queries and their strategic selection play a crucial role in achieving statistical significance, and in practice we find that generally 300 responses are enough to separate prompts of the highest similarity. Interestingly, we find that for a fixed budget of generated tokens having a larger number of shorter responses is most useful for effective separation.

% Importantly, our approach is effective even when the language model is explicitly instructed not to reveal its system prompt, as demonstrated with Mistral models. This is because Prompt Detective relies solely on the generated responses, which inherently encode the underlying prompt's influence. Furthermore, our black-box experiments (Section \ref{sec: black_box}) highlight the applicability of Prompt Detective in realistic scenarios where the adversary's language model is unknown.

A key finding of our work is that even minor changes in system prompts manifest in distinct response distributions, suggesting that large language models take distinct low-dimensional ``role trajectories'' even though the content may be similar and indistinguishable by eye when generating responses based on similar system prompts. This phenomenon is visualized in Appendix Figure \ref{fig:hard-examples-similarity-plots}, where generations from even quite similar prompts tend to cluster separately in a low-dimensional embedding space.

%TODO: 
% ethics statement, reproducibility statement
% move result tables to where more appropriate
% make the figures fit the wide format better
% NOTE: the risks, limitations, and broader impact were not included

% \section{Ethics Statement}
% Regarding potential risks, we acknowledge that Prompt Detective may be leveraged as a verification step in prompt extraction attacks and therefore we encourage the readers of this paper and the users of Prompt Detective to adhere to responsible AI practices. We emphasize that our method should only be used for legitimate purposes, such as protecting intellectual property rights and academic research, and not for malicious intent or violating privacy.

% \section{Reproducibility Statement}
% To ensure the reproducibility of our work, we provided detailed descriptions of our experimental setup, including the sources of system prompts, the language models used, and the procedures for generating task prompts and hard examples. We also included pseudocode for the Prompt Detective algorithm (Algorithm \ref{alg:prompt_detective}) and provided the code of complete implementation of Prompt Detective in supplementary materials.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Additional details on System Prompt Sources}
\label{sec:appendix_data}
\textbf{AwesomeChatGPT Prompts} is licensed under the CC0-1.0 license. The dataset contains 153 role system prompts, for which we constructed 50 universal task prompts used to produce generations. In the default experiments, we produce a single generation per system prompt - task prompt pair. Additionally, we conduct ablations by varying the number of task prompts used, as shown in Figure \ref{fig:more-samples-help}.\\

\textbf{Anthropic Prompt Library} is available on Anthropic's website and follows Anthropic's Terms of Use.\footnote{https://www.anthropic.com/legal/consumer-terms} We experiment with 20 personal system prompts, for which we construct 20 universal task prompts used to produce generations. In the default experiments, we produce a single generation per system prompt - task prompt pair. Additionally, we conduct ablations by varying the number of task prompts used, as shown in Figure \ref{fig:more-samples-help}. \\

\textbf{Anthropic Prompt Library -- Hard Examples} are variations of Anthropic Prompt Library personal system prompts constructed using strategies described in Section \ref{sec: system-prompt-sources}. We craft 10 unique task prompts for each of the 20 original system prompts, as detailed in Table \ref{table:task_prompt_generation}. In our experiments, we vary the number of generations per system-task prompt pair from 2 to 50.\\

% In Table \ref{} we include 5 example generations to the same task prompt based on two different system prompts to demonstrate that it is indeed challenging to distinguish between the generations by eye. However, Prompt Detective is able to reliably identify the difference between these system prompts based on 

%move to appendix
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/umap_ethical_fashion.png}
    \caption{%{\bf Left: False positive rate versus BERT similarity} of third-party system prompt to the given proprietary prompt. Distinguishing between two system prompts gets harder with increasing similarity between them. 
    {\bf UMAP projection of generations} of language model across 5 system prompts of varying similarity for one task prompt. It can be seen that generations from different, although conceptually similar system prompts, cluster together. }
    \label{fig:hard-examples-similarity-plots}
\end{figure}

\section{Additional Results}
\label{sec:additional_results}
% move to appendix, refer in main body briefly
Figure \ref{fig:hard-examples-similarity-plots} provides a visual representation of the generation distributions for one task prompt across five system prompts of varying similarity levels for Claude. Despite conceptual similarities, the generations from different prompts form distinct clusters in the low-dimensional UMAP projection, aligning with our finding that even minor changes in system prompts manifest in distinct response distributions.

In Figure \ref{fig:roc_curve} we illustrate the ROC-curves for Prompt Detective computed by varying the sifnificance level $\alpha$ in the standard setup for both Awesome ChatGPT Prompts and Anthropic Library datasets across all models. We observe that Prompt Detective achieves ROC-AUC of 1.0 in all setups except for the Claude model on AwesomeChatGPT prompts.

In Table \ref{tab: embeddings} we report results for Prompt Detective on Awesome ChatGPT Prompts dataset in a standard setup with various encoding models used in place of BERT embeddings. In particular, we experimented with smaller models from the \hyperlink{https://huggingface.co/spaces/mteb/leaderboard}{MTEB Leaderboard}, such as \hyperlink{https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct}{gte-Qwen2-1.5B-instruct} from Alibaba, \hyperlink{https://huggingface.co/jinaai/jina-embeddings-v3}{jina-embeddings-v3} from Jina AI and \hyperlink{https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1}{mxbai-embed-large-v1} from Mixedbread. We observe no significant difference in the results compared to the BERT embeddings. Therefore, we opt for using the cheaper BERT encoding model in Prompt Detective for obtaining multi-dimensional presentations of the generations.





\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/roc_curve_sklearn.png}
    \caption{{\bf ROC-Curves computed by varying the significance level $\alpha$ for Prompt Detective.} The markers correspond to the significance level of 0.05.}
    \label{fig:roc_curve}
\end{figure}

\input{tables/embeddings}

\subsection{Comparison to prompt extraction baselines}
\label{sec: comparison-to-prompt-extraction-baselines}
Prompt reconstruction methods can be adapted to the prompt membership inference setting by comparing recovered system prompts to the reference system prompts. We compared PLeak \citep{hui2024pleak} -- one of the most high performing of the existing prompt reconstruction approaches to Prompt Detective in the prompt membership setting. We used the optimal recommended setup for real-world chatbots from section 5.2 of the original PLeak paper \citep{hui2024pleak} –- we computed 4 Adversarial Queries with PLeak and Llama2 13B as the shadow model as recommended, and we used ChatGPT-Roles as the shadow domain dataset to minimize domain shift for PLeak. We observed that PLeak sometimes recovers large parts of target prompts even when there is no exact substring match, and that using the edit distance below the threshold of 0.2 to find matches maximizes PLeak’s performance in the prompt membership inference setting. To further maximize the performance of the PLeak method, we also aggregate the reconstructions across the 4 Adversarial Queries (AQs) by taking the best reconstruction match (this aggregation approach is infeasible in prompt reconstruction setting where the target prompt is unknown but can be used to obtain best results in prompt membership inference setting where we know the reference prompt). We then applied these adversarial prompt extraction queries to Llama2 13B as the target model with system prompts from Awesome-ChatGPT-Prompts and computed False Positive and False Negative rates for direct comparison with the results of Prompt Detective reported in Table \ref{tab: standard-setup-results} of our paper. We report the results in Table \ref{tab: prompt-reconstruction-baselines}.

\input{tables/prompt-reconstruction-baselines}

We see that Prompt Detective significantly outperforms PLeak in the prompt membership inference setting which is expected since Prompt Detective is specifically tailored to work in the verification setup while PLeak is geared towards solving a different problem of prompt reconstruction.
\section{Case Studies}
\label{sec:case_study}

We additionally explore two challenging scenarios for distinguishing between system prompts. 
\subsection{You are helpful and harmless AI assistant}

We explore the effect of the common system prompt first introduced in Llama models, which instructs the model to behave as a helpful, harmless, and honest assistant. Specifically, we consider two versions of this prompt: the standard prompt, \textit{"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."}, and a rephrased version, \textit{"Your role is to provide helpful, respectful, and truthful responses to users. Prioritize being as assistive as possible while operating within ethical boundaries. Avoid generating any content that could be considered harmful, unethical, biased, dangerous, or illegal. Maintain an impartial and positive tone in your outputs. If a query is nonsensical or lacks factual coherence, explain why instead of attempting an incorrect answer. Admit if you lack the knowledge to properly address a question rather than speculating or providing misinformation."} 

We then prompt the GPT-3.5 model with these system prompts and 50 task prompts from AwesomeChatGPT Prompts experiments. We generate 5 generations for each task prompt. 
We consider this a more challenging scenario because neither prompt installs a particular character on the model, and instead asks it to behave in a generically helpful way. Nevertheless, Prompt Detective can separate between these two system prompts with a $p$-value of 0.0001.


\subsection{System Prompt with a typo}

Next, we investigate whether introducing a couple of typos in the prompt leads to a changed "generation trajectory." For this experiment, we take one of the prompts from the Anthropic Library, namely the Dream Interpreter system prompt, and introduce two typos as follows: \textit{You are an AI assistant with a deep understanding of dream \textbf{interpretaion} and symbolism. Your task is to provide users with insightful and meaningful analyses of the symbols, emotions, and narratives present in their dreams. Offer potential interpretations while encouraging the user to reflect on their own \textbf{experiencs} and emotions.}. We then use the GPT-3.5 model to generate responses to 20 task prompts used in experiments with Anthropic Library prompts. Prompt Detective can separate the system prompt with typos from the original system prompt with a $p$-value of 0.02 when using 50 generations for each task prompt. This experiment highlights that even minor changes, such as small typos, can alter the generation trajectory, making it detectable for a prompt membership inference attack.

\newpage
\section{Prompt Detective: Detailed Explanation of the Algorithm}

\textbf{Inputs and Notations}
\begin{itemize}
    \item Third-party language model: $f_p$, prompted with an unknown system prompt $p$.
    \item Known proprietary system prompt: $\bar{p}$, used with a reference model $f_{\bar{p}}$.
    \item Task prompts: $q_1, q_2, \dots, q_n$, used to query both $f_p$ and $f_{\bar{p}}$.
    \item Number of generations per task prompt: $k$, the number of responses sampled for each task prompt.
    \item Significance level: $\alpha$, threshold for hypothesis testing.
    \item Number of permutations: $N_{\text{permutations}}$, the number of iterations for the permutation test.
\end{itemize}

\textbf{Algorithm Description}

Step 1: Generation of Responses. 

For each task prompt $q_i$ ($i \in [1, n]$), generate $k$ responses:
\[
G_1 = \{f_p(q_1)^1, \dots, f_p(q_1)^k, \dots, f_p(q_n)^1, \dots, f_p(q_n)^k\},
\]
\[
G_2 = \{f_{\bar{p}}(q_1)^1, \dots, f_{\bar{p}}(q_1)^k, \dots, f_{\bar{p}}(q_n)^1, \dots, f_{\bar{p}}(q_n)^k\}.
\]

Step 2: Encoding Generations

Convert text responses into high-dimensional vectors using a BERT embedding function $\phi(\cdot)$:
\[
V_1 = \{\phi(f_p(q_1)^1), \dots, \phi(f_p(q_1)^k), \dots, \phi(f_p(q_n)^1), \dots, \phi(f_p(q_n)^k)\},
\]
\[
V_2 = \{\phi(f_{\bar{p}}(q_1)^1), \dots, \phi(f_{\bar{p}}(q_1)^k), \dots, \phi(f_{\bar{p}}(q_n)^1), \dots, \phi(f_{\bar{p}}(q_n)^k)\}.
\]

Step 3: Mean Vector Computation

Compute the mean vectors for $V_1$ and $V_2$:
\[
\mu_1 = \frac{1}{|V_1|} \sum_{v \in V_1} v, \quad \mu_2 = \frac{1}{|V_2|} \sum_{v \in V_2} v.
\]

Step 4: Observed Cosine Similarity

Calculate the observed cosine similarity between $\mu_1$ and $\mu_2$:
\[
s_{\text{obs}} = \cos(\mu_1, \mu_2).
\]

Step 5: Permutation Test 

The goal of this step is to test whether the observed similarity $s_{\text{obs}}$ is significantly different from what would be expected if $V_1$ and $V_2$ were drawn from the same distribution.

\paragraph{Procedure:}
\begin{enumerate}
    \item Combine Responses: Merge all embeddings into a single set:
    \[
    V_{\text{combined}} = V_1 \cup V_2.
    \]
    \item Shuffle the Combined Embeddings: For each task prompt $q_i$, shuffle the embeddings associated with that prompt:
    \[
    V_{\text{combined}}[i] = \{v_{i,1}, \dots, v_{i,k}, u_{i,1}, \dots, u_{i,k}\},
    \]
    where $v_{i,j} \in V_1$ and $u_{i,j} \in V_2$. After shuffling, the embeddings are randomly reordered, eliminating any inherent grouping.
    \item Split into Two Groups: Divide the shuffled embeddings back into two groups, each containing $k$ embeddings per task prompt:
    \[
    V_1^*[i] = \{v'_{i,1}, \dots, v'_{i,k}\}, \quad V_2^*[i] = \{u'_{i,1}, \dots, u'_{i,k}\}.
    \]
    \item Compute Mean Vectors for Permuted Groups: Calculate the mean vectors for $V_1^*$ and $V_2^*$:
    \[
    \mu_1^* = \frac{1}{|V_1^*|} \sum_{v \in V_1^*} v, \quad \mu_2^* = \frac{1}{|V_2^*|} \sum_{v \in V_2^*} v.
    \]
    \item Calculate Permuted Cosine Similarity: Compute the cosine similarity for the permuted groups:
    \[
    s^* = \cos(\mu_1^*, \mu_2^*).
    \]
    \item Repeat for Null Distribution: Repeat the shuffle-split process $N_{\text{permutations}}$ times to generate a null distribution of permuted cosine similarities.
    \item Compute P-Value: Count the number of permuted similarities as extreme as $s_{\text{obs}}$:
    \[
    p = \frac{\sum_{i=1}^{N_{\text{permutations}}} \mathbb{I}(s^* \leq s_{\text{obs}})}{N_{\text{permutations}}}.
    \]
\end{enumerate}

Step 6: Hypothesis Testing

If $p < \alpha$, reject the null hypothesis and conclude that the system prompts $p$ and $\bar{p}$ produce distinct distributions of responses. Otherwise, there is insufficient evidence to distinguish the prompts.




\section{Hardware}

Our experiments were conducted using NVIDIA A10G 24GB GPUs. Although a single run of Prompt Detective for a given system prompt takes only minutes, even with a large number of generations, the total number of GPU hours required to produce the results presented in this paper amounted to approximately 150 GPU hours. These experiments involved three different system prompt sources, black-box experiments, and thorough ablation studies to evaluate the test's performance under varying numbers of task prompts, generations, and generation lengths. We also utilized the corresponding APIs for the commercial models.

\section{Prompt Templates and Examples}
\label{sec: appendix Labels}

Table \ref{table:task_prompt_generation} presents the instructions used with Claude 3 Sonnet for generating task queries and hard examples. Table \ref{tab:dream_interpreter_examples} presents an example of prompts used in experiments with hard examples. 

\include{tables/prompt_templates_appendix}

\include{tables/dream_example_appendix}

\section{LLM Selection for the experiments}
In our general experiments in Table \ref{tab: standard-setup-results}, we report Prompt Detective performance across a variety of language model families and sizes – including both larger and smaller models, multiple models of the various open source families, and closed-source models. We observed minor variations in performance across these settings and therefore we decided to focus on the efficient variants of models powering popular real-world chatbots in our exploration of highly similar system prompts in Section \ref{sec: hard-examples}, following the similar logic of responsible use of compute resources.

\end{document}
