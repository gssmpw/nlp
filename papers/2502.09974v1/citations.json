[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2023pre",
        "author": "Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham",
        "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhao2021calibrate",
        "author": "Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer",
        "title": "Calibrate before use: Improving few-shot performance of language models"
      },
      {
        "key": "lu2021fantastically",
        "author": "Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus",
        "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity"
      },
      {
        "key": "ye2023explanation",
        "author": "Ye, Xi and Durrett, Greg",
        "title": "Explanation selection using unlabeled data for chain-of-thought prompting"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "scao2021many",
        "author": "Scao, Teven Le and Rush, Alexander M",
        "title": "How many data points is a prompt worth?"
      },
      {
        "key": "agarwal2024many",
        "author": "Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Chan, Stephanie and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and Co-Reyes, John D and Chu, Eric and others",
        "title": "Many-Shot In-Context Learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "chu2023survey",
        "author": "Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting",
        "title": "A survey of chain of thought reasoning: Advances, frontiers and future"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      },
      {
        "key": "zhang2022automatic",
        "author": "Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex",
        "title": "Automatic chain of thought prompting in large language models"
      },
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "key": "zheng2023take",
        "author": "Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny",
        "title": "Take a step back: Evoking reasoning via abstraction in large language models"
      },
      {
        "key": "yasunaga2023large",
        "author": "Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H and Zhou, Denny",
        "title": "Large language models as analogical reasoners"
      },
      {
        "key": "zhou2023thread",
        "author": "Zhou, Yucheng and Geng, Xiubo and Shen, Tao and Tao, Chongyang and Long, Guodong and Lou, Jian-Guang and Shen, Jianbing",
        "title": "Thread of thought unraveling chaotic contexts"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2022automatic",
        "author": "Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex",
        "title": "Automatic chain of thought prompting in large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zheng2023take",
        "author": "Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny",
        "title": "Take a step back: Evoking reasoning via abstraction in large language models"
      },
      {
        "key": "yasunaga2023large",
        "author": "Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H and Zhou, Denny",
        "title": "Large language models as analogical reasoners"
      },
      {
        "key": "zhou2023thread",
        "author": "Zhou, Yucheng and Geng, Xiubo and Shen, Tao and Tao, Chongyang and Long, Guodong and Lou, Jian-Guang and Shen, Jianbing",
        "title": "Thread of thought unraveling chaotic contexts"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kadavath2022language",
        "author": "Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others",
        "title": "Language models (mostly) know what they know"
      },
      {
        "key": "madaan2024self",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      },
      {
        "key": "xue2023rcot",
        "author": "Xue, Tianci and Wang, Ziqi and Wang, Zhenhailong and Han, Chi and Yu, Pengfei and Ji, Heng",
        "title": "Rcot: Detecting and rectifying factual inconsistency in reasoning by reversing chain-of-thought"
      },
      {
        "key": "weng2022large",
        "author": "Weng, Yixuan and Zhu, Minjun and Xia, Fei and Li, Bin and He, Shizhu and Liu, Shengping and Sun, Bin and Liu, Kang and Zhao, Jun",
        "title": "Large language models are better reasoners with self-verification"
      },
      {
        "key": "dhuliawala2023chain",
        "author": "Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason",
        "title": "Chain-of-verification reduces hallucination in large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "kadavath2022language",
        "author": "Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others",
        "title": "Language models (mostly) know what they know"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "madaan2024self",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "xue2023rcot",
        "author": "Xue, Tianci and Wang, Ziqi and Wang, Zhenhailong and Han, Chi and Yu, Pengfei and Ji, Heng",
        "title": "Rcot: Detecting and rectifying factual inconsistency in reasoning by reversing chain-of-thought"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "weng2022large",
        "author": "Weng, Yixuan and Zhu, Minjun and Xia, Fei and Li, Bin and He, Shizhu and Liu, Shengping and Sun, Bin and Liu, Kang and Zhao, Jun",
        "title": "Large language models are better reasoners with self-verification"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "dhuliawala2023chain",
        "author": "Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason",
        "title": "Chain-of-verification reduces hallucination in large language models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wang2023rolellm",
        "author": "Wang, Zekun Moore and Peng, Zhongyuan and Que, Haoran and Liu, Jiaheng and Zhou, Wangchunshu and Wu, Yuhan and Guo, Hongcheng and Gan, Ruitong and Ni, Zehao and Zhang, Man and others",
        "title": "Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models"
      },
      {
        "key": "zheng2023helpful",
        "author": "Zheng, Mingqian and Pei, Jiaxin and Jurgens, David",
        "title": "Is\" A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "li2023large",
        "author": "Li, Cheng and Wang, Jindong and Zhang, Yixuan and Zhu, Kaijie and Hou, Wenxin and Lian, Jianxun and Luo, Fang and Yang, Qiang and Xie, Xing",
        "title": "Large language models understand and can be enhanced by emotional stimuli"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "deng2023rephrase",
        "author": "Deng, Yihe and Zhang, Weitong and Chen, Zixiang and Gu, Quanquan",
        "title": "Rephrase and respond: Let large language models ask better questions for themselves"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "press2022measuring",
        "author": "Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A and Lewis, Mike",
        "title": "Measuring and narrowing the compositionality gap in language models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "ng2023neurips",
        "author": "Andrew Ng and Isa Fulford",
        "title": "Application Development using Large Language Models"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "liu2023pre",
        "author": "Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham",
        "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "ng2023neurips",
        "author": "Andrew Ng and Isa Fulford",
        "title": "Application Development using Large Language Models"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "morris2023language",
        "author": "Morris, John X and Zhao, Wenting and Chiu, Justin T and Shmatikov, Vitaly and Rush, Alexander M",
        "title": "Language model inversion"
      },
      {
        "key": "zhang2024effective",
        "author": "Yiming Zhang and Nicholas Carlini and Daphne Ippolito",
        "title": "Effective Prompt Extraction from Language Models"
      },
      {
        "key": "sha2024prompt",
        "author": "Sha, Zeyang and Zhang, Yang",
        "title": "Prompt Stealing Attacks Against Large Language Models"
      },
      {
        "key": "yang2024prsa",
        "author": "Yang, Yong and Zhang, Xuhong and Jiang, Yi and Chen, Xi and Wang, Haoyu and Ji, Shouling and Wang, Zonghui",
        "title": "PRSA: Prompt Reverse Stealing Attacks against Large Language Models"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "wen2024hard",
        "author": "Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom",
        "title": "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "morris2023language",
        "author": "Morris, John X and Zhao, Wenting and Chiu, Justin T and Shmatikov, Vitaly and Rush, Alexander M",
        "title": "Language model inversion"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "sha2024prompt",
        "author": "Sha, Zeyang and Zhang, Yang",
        "title": "Prompt Stealing Attacks Against Large Language Models"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "yang2024prsa",
        "author": "Yang, Yong and Zhang, Xuhong and Jiang, Yi and Chen, Xi and Wang, Haoyu and Ji, Shouling and Wang, Zonghui",
        "title": "PRSA: Prompt Reverse Stealing Attacks against Large Language Models"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "zhang2024effective",
        "author": "Yiming Zhang and Nicholas Carlini and Daphne Ippolito",
        "title": "Effective Prompt Extraction from Language Models"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      },
      {
        "key": "cherepanova2024talking",
        "author": "Cherepanova, Valeriia and Zou, James",
        "title": "Talking Nonsense: Probing Large Language Models' Understanding of Adversarial Gibberish Inputs"
      },
      {
        "key": "geiping2024coercing",
        "author": "Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom",
        "title": "Coercing LLMs to do and reveal (almost) anything"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "hui2024pleak",
        "author": "Hui, Bo and Yuan, Haolin and Gong, Neil and Burlina, Philippe and Cao, Yinzhi",
        "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "geiping2024coercing",
        "author": "Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom",
        "title": "Coercing LLMs to do and reveal (almost) anything"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "hui2024pleak",
        "author": "Hui, Bo and Yuan, Haolin and Gong, Neil and Burlina, Philippe and Cao, Yinzhi",
        "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications"
      },
      {
        "key": "geiping2024coercing",
        "author": "Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom",
        "title": "Coercing LLMs to do and reveal (almost) anything"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "geiping2024coercing",
        "author": "Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom",
        "title": "Coercing LLMs to do and reveal (almost) anything"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "hui2024pleak",
        "author": "Hui, Bo and Yuan, Haolin and Gong, Neil and Burlina, Philippe and Cao, Yinzhi",
        "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications"
      },
      {
        "key": "zhang2024effective",
        "author": "Yiming Zhang and Nicholas Carlini and Daphne Ippolito",
        "title": "Effective Prompt Extraction from Language Models"
      },
      {
        "key": "geiping2024coercing",
        "author": "Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom",
        "title": "Coercing LLMs to do and reveal (almost) anything"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "yeom2018privacy",
        "author": "Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh",
        "title": "Privacy risk in machine learning: Analyzing the connection to overfitting"
      },
      {
        "key": "sablayrolles2019white",
        "author": "Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and Ollivier, Yann and J{\\'e}gou, Herv{\\'e}",
        "title": "White-box vs black-box: Bayes optimal strategies for membership inference"
      },
      {
        "key": "salem2018ml",
        "author": "Salem, Ahmed and Zhang, Yang and Humbert, Mathias and Berrang, Pascal and Fritz, Mario and Backes, Michael",
        "title": "Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models"
      },
      {
        "key": "song2021systematic",
        "author": "Song, Liwei and Mittal, Prateek",
        "title": "Systematic evaluation of privacy risks of machine learning models"
      },
      {
        "key": "hu2022membership",
        "author": "Hu, Hongsheng and Salcic, Zoran and Sun, Lichao and Dobbie, Gillian and Yu, Philip S and Zhang, Xuyun",
        "title": "Membership inference attacks on machine learning: A survey"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "shokri2017membership",
        "author": "Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly",
        "title": "Membership inference attacks against machine learning models"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "carlini2022membership",
        "author": "Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian",
        "title": "Membership inference attacks from first principles"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "bertran2024scalable",
        "author": "Bertran, Martin and Tang, Shuai and Roth, Aaron and Kearns, Michael and Morgenstern, Jamie H and Wu, Steven Z",
        "title": "Scalable membership inference attacks via quantile regression"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "wen2022canary",
        "author": "Wen, Yuxin and Bansal, Arpit and Kazemi, Hamid and Borgnia, Eitan and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom",
        "title": "Canary in a coalmine: Better membership inference with ensembled adversarial queries"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "jagielski2023combine",
        "author": "Jagielski, Matthew and Wu, Stanley and Oprea, Alina and Ullman, Jonathan and Geambasu, Roxana",
        "title": "How to combine membership-inference attacks on multiple updated machine learning models"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "duan2023diffusion",
        "author": "Duan, Jinhao and Kong, Fei and Wang, Shiqi and Shi, Xiaoshuang and Xu, Kaidi",
        "title": "Are diffusion models vulnerable to membership inference attacks?"
      },
      {
        "key": "matsumoto2023membership",
        "author": "Matsumoto, Tomoya and Miura, Takayuki and Yanai, Naoto",
        "title": "Membership inference attacks against diffusion models"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "maini2021dataset",
        "author": "Maini, Pratyush and Yaghini, Mohammad and Papernot, Nicolas",
        "title": "Dataset inference: Ownership resolution in machine learning"
      },
      {
        "key": "maini2024llm",
        "author": "Pratyush Maini and Hengrui Jia and Nicolas Papernot and Adam Dziedzic",
        "title": "LLM Dataset Inference: Did you train on my dataset?"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "chaudhary2024quantitative",
        "author": "Chaudhary, Isha and Hu, Qian and Kumar, Manoj and Ziyadi, Morteza and Gupta, Rahul and Singh, Gagandeep",
        "title": "Quantitative Certification of Bias in Large Language Models"
      },
      {
        "key": "kumar2024certifying",
        "author": "Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, AJ and Feizi, S and Lakkaraju, H",
        "title": "Certifying LLM Safety against Adversarial Prompting. arXiv 2024"
      },
      {
        "key": "kang2024c",
        "author": "Kang, Mintong and G{\\\"u}rel, Nezihe Merve and Yu, Ning and Song, Dawn and Li, Bo",
        "title": "C-rag: Certified generation risks for retrieval-augmented language models"
      }
    ]
  }
]