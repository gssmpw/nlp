\section{Related Work}
\label{sec:related-work}

%\rl{We need to be a bit careful w this sentence bc the main contributions of the Carlini paper sort of boil down to a confidence score model -- still different from us as it works by checking how similar a given extracted candidate is to other candidates and giving confidence scores.} 

% To our knowledge, our work is the first to address ways for prompt owners to detect if their proprietary system prompt has been stolen. We contextualize our analysis and methods within the scope of prior work on topics such as the value of prompt engineering, feasibility of prompt theft, and the framing of membership inference techniques.
% \subsection{Prompt Engineering}
% Prompt engineering has emerged as an accessible approach to adapt LLMs for specific user needs \citep{liu2023pre}.
% % leveraging their remarkable in-context learning capabilities \citep{brown2020language, radford2019language}.
% In-context learning \citep{brown2020language, radford2019language} allows LLMs to acquire new skills by providing exemplars within the prompt, without retraining. A prominent technique is few-shot prompting \citep{brown2020language}, where the design of exemplars, such as their selection, ordering, and formatting, significantly impacts output quality \citep{zhao2021calibrate, lu2021fantastically, ye2023explanation}, and many-shot prompting can even match the power of fine-tuning \citep{scao2021many, agarwal2024many}. Another line of work focuses on chain-of-thought prompting \citep{wei2022chain, chu2023survey} which encourages LLMs to express their thought process before delivering the final answer, often leading to improved performance on reasoning tasks \citep{kojima2022large, zhang2022automatic, team2023gemini, zheng2023take, yasunaga2023large, zhou2023thread}. 
% % Numerous variations of chain-of-thought prompting have been proposed, including zero-shot \citep{kojima2022large}, automatic \citep{zhang2022automatic}, uncertainty-routed \citep{team2023gemini}, and others \citep{zheng2023take, yasunaga2023large, zhou2023thread}.
% Similarly, self-criticism techniques improve language models by encouraging them to criticize and refine their own outputs \citep{kadavath2022language, madaan2024self, xue2023rcot, weng2022large, dhuliawala2023chain}.

% % such as self-calibration \citep{kadavath2022language}, self-refinement \citep{madaan2024self}, reverse chain-of-thought \citep{xue2023rcot}, self-verification \citep{weng2022large}, and chain-of-verification \citep{dhuliawala2023chain}.

% Zero-shot prompting techniques, closely related to system prompts, include role prompting \citep{wang2023rolellm, zheng2023helpful}, emotion prompting \citep{li2023large}, rephrase and respond \citep{deng2023rephrase}, and self-ask \citep{press2022measuring}. System prompts play a crucial role in shaping LLM outputs and driving performance in application domains \citep{ng2023neurips}.

% with tuned system prompts often being valuable enough to be sold at online marketplaces.\footnote{See \url{https://prompti.ai/chatgpt-prompt/}, \url{https://promptbase.com/}.}

\subsection{Prompt extraction attacks}
\looseness=-1
Prompt engineering has emerged as an accessible approach to adapt LLMs for specific user needs \citep{liu2023pre}, with system prompts playing a crucial role in shaping LLM outputs and driving performance across application domains \citep{ng2023neurips}.
% Given the value of prompts, we can reasonably expect efforts to steal them. 
Prior work has proposed several prompt extraction attacks, which deduce the content of a proprietary system prompt by interacting with a model, both for language models \citep{morris2023language, zhang2024effective, sha2024prompt, yang2024prsa} and for image generation models \citep{wen2024hard}. 
\citet{morris2023language} frame the problem as model inversion, where they deduce the prompt given next token probabilities.
Similarly, \citet{sha2024prompt} propose a method to extract prompts from sampled generative model outputs. 
Furthermore, \citet{yang2024prsa} describe a way to uncover system prompts using context and response pairs.
Additionally, \citet{zhang2024effective} present an evaluation of prompt extraction attacks for a variety of modern LLMs. 
In contrast to the works on inversion style methods, one can also find adversarial inputs that jailbreak LLMs \citep{zou2023universal, cherepanova2024talking, geiping2024coercing} and even lead them to eliciting the system prompt in the response. Both \citet{hui2024pleak} and \citet{geiping2024coercing} use optimization over prompt tokens to provoke LLMs to respond by quoting their own system prompts. Prompt reconstruction methods can also be adapted to solve the problem of prompt verification through comparing the reconstructed prompt to the reference prompt, however, their high computational cost \citep{hui2024pleak, geiping2024coercing}, the need to access model gradients \citep{geiping2024coercing}, and imperfect reconstruction success rate \citep{hui2024pleak, zhang2024effective, geiping2024coercing} motivate the development of methods specifically tailored to the problem of prompt reuse verification.
% This body of work convinces us that prompts can be stolen, and those developing valuable prompts should be interested in methods to detect such theft. 
% \valeriia{position our paper here}

\subsection{Data membership inference and extraction attacks on language models}

In the evolving discussion on data privacy, a significant topic is membership inference, which involves determining whether a particular data point is part of a model's training set \citep[e.g.][]{yeom2018privacy, sablayrolles2019white, salem2018ml, song2021systematic, hu2022membership}. 
% Nonetheless, we are motivated by research on training data membership inference as methods that work in that setting may be interesting directions for those concerned with prompt membership inference.
\citet{shokri2017membership} and \citet{carlini2022membership} both propose methods to determine membership in the training data based on the idea that models tend to behave differently on their training data than on other data. \citet{bertran2024scalable} further propose a more effective method and alleviate the need to know the target model's architecture, while \citet{wen2022canary} propose perturbing the query data to improve accuracy of their attack. 
\citet{jagielski2023combine} consider the setting where the system includes an ensemble of models that may be updated over time. Other works explore training data membership inference in image generation models \citep{duan2023diffusion, matsumoto2023membership}. Additionally, dataset inference techniques explore settings where the whole training set is considered rather than single data points \citep{maini2021dataset, maini2024llm}.
Compared to the standard membership inference setting, our work addresses a related but distinct question: whether a given text is part of the LLM input context, thus exploring prompt membership inference. 
% While the goal differs between training data membership and prompt membership attacks, the problem formulation remains similar.
%Finally, while we focus on system prompt verification, statistical methods have been widely applied to verify LLM behaviors across various contexts \citep{chaudhary2024quantitative, kumar2024certifying, kang2024c}.