\section{Related Work}
% 随着大模型的迅速发展，其在方方面面的应用展现了其卓越的能力以及巨大的潜力。如何准确、公平、全面地评价大模型成为了重要的挑战。现有的主流评估方式主要包括：（1）LLMs-as-a-judge. Benchmarks like AlpacaEval, PandaLM, MT-Bench and  C-Eval leverage large language models to evaluate a set of predefined questions. This method is not only efficient and affordable.（2）Humans-as-a-judge. Human evaluation involves assessing the quality and accuracy of model outputs through human involvement, offering more comprehensive and precise feedback~\citep{chang2024survey,ribeiro2022adaptive,gao2023adaptive}. In evaluating large language models, experts, researchers, or users are invited to review model outputs.（3）Other benchmarks. Several traditional benchmarks employ a series of standardized tests and static datasets to quantitatively assess the performance of models across various tasks. For instance, HELM and MMLU are primarily utilized to evaluate general knowledge and reasoning capabilities and MATH focuses on assessing mathematical skills. ToolBench concentrates on evaluating the ability to use tools. The performance of LLMs is measured by their ability to accurately complete these tasks.
% 


\paragraph{Evaluating Large Language Models.}

% With the rapid development of large models, their applications across various domains demonstrate exceptional capabilities and immense potential~\citep{chang2024survey}. Accurately, fairly, and comprehensively evaluating large models becomes a significant challenge. The current mainstream evaluation methods primarily include: \textbf{(1) LLMs-as-a-judge}: Benchmarks such as AlpacaEval~\citep{li2023alpacaeval}, PandaLM~\citep{wang2023pandalm}, MT-Bench~\citep{zheng2023judging}, and C-Eval~\citep{huang2024c} utilize large language models to evaluate a set of predefined questions. This approach is not only efficient but also cost-effective.
% \textbf{(2) Humans-as-a-judge}: Human evaluation involves assessing the quality and accuracy of model outputs through human involvement, providing more comprehensive and precise feedback \citep{ribeiro2022adaptive,gao2023adaptive}. Experts, researchers, or users are invited to review model outputs when evaluating large language models.
% \textbf{(3) Other benchmarks}: Several traditional benchmarks employ a series of standardized tests and static datasets to quantitatively assess the performance of models across various tasks. For example, HELM~\citep{liang2022holistic} and MMLU~\citep{hendrycks2020measuring} are primarily used to evaluate general knowledge and reasoning capabilities, while MATH~\citep{hendrycks2021measuring} focuses on assessing mathematical skills. ToolBench~\citep{xu2023tool} concentrates on evaluating the ability to use tools. The performance of large language models is measured by their ability to complete these tasks accurately.

% Given the widespread use and cross-industry applications of LLMs~\citep{el2021automatic, hao2022recent, cheng2023ml, gao2024fact}, a dynamic robustness evaluation of their reasoning performance is essential~\citep{glazer2024frontiermath}. While numerous benchmarks have been developed to assess LLMs' capabilities~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}, these largely focus on train-time compute models, with limited attention given to the emerging test-time compute models like o1. Moreover, test-time models often require substantial computational resources, presenting unique challenges for establishing a unified robustness evaluation benchmark. Current evaluations of o1-preview have mainly focused on specific tasks, such as planning, with few comprehensive analyses of robustness~\citep{gui2024logicgame, wang2024planning, zhong2024evaluation}.

% With rapid improvement of LLMs, their applications across domains show exceptional capabilities~\citep{chang2024survey}. 
Evaluating LLMs accurately and fairly poses a significant challenge~\citep{chang2024survey}. Mainstream evaluation methods include: \textbf{(1) LLMs-as-a-judge}: Benchmarks like AlpacaEval~\citep{li2023alpacaeval}, PandaLM~\citep{wang2023pandalm}, MT-Bench~\citep{zheng2023judging}, and C-Eval~\citep{huang2024c} use large language models for predefined questions.
% , offering efficiency and cost-effectiveness. 
\textbf{(2) Humans-as-a-judge}: Human evaluation provides comprehensive feedback through expert reviews~\citep{ribeiro2022adaptive,gao2023adaptive}. \textbf{(3) Other benchmarks}: Several traditional benchmarks employ static datasets to assess models across various tasks~\citep{liang2022holistic,hendrycks2020measuring,hendrycks2021measuring}. 
% Traditional benchmarks like HELM~\citep{liang2022holistic}, MMLU~\citep{hendrycks2020measuring}, and MATH~\citep{hendrycks2021measuring} assess various skills, while ToolBench~\citep{xu2023tool} evaluates tool usage. 
Our work falls into the third category. However, rather than using static data, we generate test sets dynamically.

% Given LLMs' widespread use~\citep{el2021automatic, hao2022recent, cheng2023ml, gao2024fact}, dynamic evaluation is crucial~\citep{glazer2024frontiermath}. Although many benchmarks exist~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}, they often focus on train-time compute models, with limited attention to test-time models like o1. These require significant computational resources, posing challenges for unified robustness evaluation. Current evaluations of o1-preview focus on specific tasks, such as planning, with limited comprehensive robustness analyses~\citep{gui2024logicgame, wang2024planning, zhong2024evaluation}.

\paragraph{Robustness of Large Language Models.}
%In order to apply LLMs across various scenarios, improving LLMs' robustness is critical. Previous studies, such as OOD-GLUE~\citep{yuan2023revisiting}, GLUE-X~\citep{yang2023glue}, and ZebraLogic~\citep{zebralogic2024}, focus on the robustness of models and propose corresponding evaluation benchmarks. These works primarily involve robustness evaluations of train-time compute models. \citet{wang2023robustness} present a pilot study on assessing the robustness of ChatGPT, following with~\citep{li2024gsm} which pays particular attention on evaluating the robustness of LLMs as mathematical problem solvers. Additionally, \citet{yang2022factmix} focuses on generating out-of-distribution data by employing counterfactual and semi-factual data construction methods. Recently, \citet{hosseini2024not} reveal a significant reasoning gap in most LLMs by evaluating LLMs' performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. The performance gap between solving compositional pairs and solving each question independently is more pronounced in smaller, more cost-efficient, and math-specialized models.
% Different from previous benchmarks that focused on the robustness of train-time compute models, our work is the first to analyze the robustness of both train-time and test-time compute models collectively.



Evaluating the robustness of LLMs is crucial~\cite{muennighoff2025s1,guo2025deepseek} for their applications across diverse scenarios~\cite{wang2023robustness,glazer2024frontiermath,li2024gsm}. Previous studies~\cite{li2024openai}, such as OOD-GLUE~\citep{yuan2023revisiting}, GLUE-X~\citep{yang2023glue}, and ZebraLogic~\citep{zebralogic2024}, focus on robustness of non-reasoning models. 
% \citet{wang2023robustness} assess ChatGPT's robustness, 
% while \citet{li2024gsm} evaluate LLMs as problem solvers. 
Additionally, \citet{yang2022factmix} focus on generating OOD data by employing semi-fact data augmentation methods. Recently, \citet{hosseini2024not} identify reasoning gaps in LLMs by evaluating math problem pairs, revealing performance disparities in smaller, math-specialized models. \citet{wu2024mrke} introduce cofQA, which targets text-based inference tasks using counterfactual data perturbations. 
Our work is similar in assessing general robustness but differs from the literature in focusing on reasoning tasks, for which OOD tests are more necessary as compared to general tasks.
% However, for mathematical and coding reasoning tasks, creating counterfactuals makes it challenging to determine the correct answer, often necessitating expert annotation. In contrast, constructing semi-factual data does not require additional human annotation. Therefore, to construct OOD data suitable for reasoning tasks and to alleviate the issue of data leakage in evaluations, we try to dynamically construct semi-factual data.

% \citet{wu2024mrke}提出的cofQA是专注文本的推理任务，利用的是反事实数据干扰。但是，对于数学等推理任务来说，构建反事实的话golden answer难以确认，通常需要专家标注，构建半事实数据则无需额外的人工标注。

%Unlike previous benchmarks that focus on the robustness of train-time compute models, our work is the first to collectively analyze the robustness of both train-time and test-time compute models.

% Given the widespread use of LLMs~\citep{el2021automatic, hao2022recent, cheng2023ml, gao2024fact}, evaluation becomes more and more essential~\citep{glazer2024frontiermath}. 
% Although numerous benchmarks exist~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}, they 
In addition, existing benchmarks predominantly emphasize non-reasoning models~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}.
% They cannot be used for assessing test-time computing models because t
% They often neglect the evaluation under test-time scaling.
% neglecting test-time models like o1, which demand substantial computational resources for unified robustness evaluation. Test-time computing models, such as o1-preview, have demonstrated impressive performance, necessitating thorough evaluation to better understand them. 
For reasoning models, current evaluations often target specific tasks, such as planning~\citep{wang2024planning} and rule execution~\citep{gui2024logicgame}, with limited comprehensive robustness analyses~\citep{zhong2024evaluation}. Unlike these work, our benchmark focuses on robustness and reasoning with practical applications, offering statistically significant insights.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Graph/o1-graph.pdf}
    \caption{Overview of ThinkBench framework. Based on the original data, ThinkBench dynamically generates scenario-level Semi-fact Data (a) and Attack-level Semi-fact Data (b), which can be used to evaluate the robustness of reasoning models and non-reasoning models. ThinkBench can also serve as a useful tool for Test-time Scaling Evaluation(c). 
    %Within the limited computation budget, the selected process reward model used for test-time compute model usually should consider the trade-off between the efficiency and the accuracy performance.
    }
    \label{fig:framework}
\end{figure*}
% Given LLMs' widespread use~\citep{el2021automatic, hao2022recent, cheng2023ml, gao2024fact}, dynamic evaluation is crucial~\citep{glazer2024frontiermath}. Although many benchmarks exist~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}, they often focus on train-time compute models, with limited attention to test-time models like o1. These require significant computational resources, posing challenges for unified robustness evaluation. Current evaluations of o1-preview focus on specific tasks, such as planning, with limited comprehensive robustness analyses~\citep{gui2024logicgame, wang2024planning, zhong2024evaluation}.

% Recently, test-time compute models like o1-preview have shown impressive performance. Evaluating these models is crucial for better understanding them. Efforts include LOGICGAME~\citep{gui2024logicgame}, assessing rule understanding and execution, ~\citet{wang2024planning} evaluating o1-preview's planning capabilities, and ~\cite{zhong2024evaluation} conducting case studies across domains. Unlike these works, our benchmark focuses on robustness and reasoning with practical applications, offering statistically significant insights.

% 此外，会有一些工作通过反事实、半事实数据构造的方式来生成OOD数据，从而检测模型的robustness。

% Recently, test-time compute models, such as o1-preview, have been introduced and demonstrate impressive performance. Evaluating these test-time compute models is important, as it helps in better analyzing and understanding them. Recent evaluation efforts for test-time compute models include LOGICGAME~\citep{gui2024logicgame}, which assesses the model's abilities in rule understanding, execution, and planning. ~\citet{wang2024planning} evaluate the planning capabilities of o1-preview across a series of planning benchmarks and ~\cite{zhong2024evaluation} conduct some case studies of o1-preview across different domains. Different from recent analytical works on o1, which emphasize planning evaluations or case-by-case analyses, our proposed benchmark is statistically significant and places greater emphasis on robustness and reasoning that have high practical application value.



% 半事实 反事实

 

%The exploration and optimization of PRM show its superior performance in developing high-performance models. However, suitable benchmarks for PRM and test-compute models are lacking. We propose a benchmark for test-time computing models to evaluate PRM's capabilities, unlocking its potential.


% In the past, the training of language models primarily relied on Outcome-based Reinforcement Models (ORM)~\citep{wang2024openr}. A model proposed by ~\citet{cobbe2021gsm8k} serves as a foundational example of ORM-based models, where the focus is on training evaluators to assess the final correctness of generated answers. This approach provides feedback for model training and plays a significant role. However, ORM has certain limitations, as it concentrates solely on the final outcome, neglecting the various steps involved in the reasoning process.




% To address ORM's oversight of the reasoning process, a few works on Process Reward Model (PRM) have emerged in recent years, offering more granular supervision. For instance, DeepMind~\citep{uesato2022solving} emphasizes supervising both intermediate reasoning steps and the final result, thereby providing more detailed feedback. OpenAI~\citep{lightman2023let} further advances PRM by introducing a high-quality human-annotated process supervision dataset, PRM800K, highlighting the importance of verifying each intermediate step.

% \citet{li2022making} demonstrate the combination of evaluator models with majority voting schemes to enhance the reliability of results. ~\citet{yu2024ovm} improve the reasoning process in large language models through reinforcement learning, integrating both outcome and process supervision. The recently proposed Generative Reward Model (GenRM)~\citep{zhang2024generative} gets tons of attention, allowing evaluators to interact with generators in an information-rich manner. This shift reflects a widespread demand for more sophisticated process supervision methods. The recent work~\citep{zheng2024processbench} contributes a benchmark designed to evaluate language models' ability to identify errors in step-by-step mathematical reasoning, focusing on challenging competition-level problems, using a diverse set of annotated solutions to foster research in scalable oversight for reasoning assessment.

% An increasing number of researchers turn to the exploration and optimization of PRM, whose superior performance marks a significant step towards the development of high-performance test-compute models. However, there is still a lack of suitable benchmarks for PRM and test-compute models. We propose a benchmark tailored for test-compute models to measure their performance and robustness, enabling a detailed and comprehensive evaluation of PRM's capabilities, thereby further exploring and unlocking the vast potential of PRM.

% \usepackage{booktabs}


\begin{table}
\centering
% \small
\caption{Statistics of reconstructed reasoning datasets based on three original test datasets, including AIME-500, AIME 2024, and GPQA Diamond.}
\label{tab:data_statistics}
 \begin{adjustbox}{width=0.99\columnwidth}{
\begin{tabular}{lccc} 
\toprule
                             & AIME-500 & AIME 2024 & GPQA Diamond \\ 
\midrule
% \# Class                     & -        & -         & 3                        \\
% \# Choices   & -        & -         & 4                        \\ 
% \midrule
\# Samples of original  & 500      & 30        & 198                      \\
Questions' Avg Len       & 51.1     & 60.1      & 67.7                     \\
Choices' Avg Len         & -        & -         & 27.8                     \\ 
\hline
\# Samples of OOD & 2,000      & 120        & 792                      \\
Questions' Avg Len       & 61.2     & 70.1      & 85.2                     \\
Choices' Avg Len         & -        & -         & 25.7                     \\
\bottomrule
\end{tabular}}    \end{adjustbox}
\end{table}