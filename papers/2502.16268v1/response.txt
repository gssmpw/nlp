\section{Related Work}
% 随着大模型的迅速发展，其在方方面面的应用展现了其卓越的能力以及巨大的潜力。如何准确、公平、全面地评价大模型成为了重要的挑战。现有的主流评估方式主要包括：（1）LLMs-as-a-judge. Benchmarks like AlpacaEval, PandaLM, MT-Bench and  C-Eval leverage large language models to evaluate a set of predefined questions. This method is not only efficient and affordable.（2）Humans-as-a-judge. Human evaluation involves assessing the quality and accuracy of model outputs through human involvement, offering more comprehensive and precise feedback**Radford et al., "Language Models as Few-Shot Learners"**, In evaluating large language models, experts, researchers, or users are invited to review model outputs.

\paragraph{Evaluating Large Language Models.}

% With the rapid development of large models, their applications across various domains demonstrate exceptional capabilities and immense potential**Brown et al., "Language Models as Few-Shot Learners"**. Accurately, fairly, and comprehensively evaluating large models becomes a significant challenge. The current mainstream evaluation methods primarily include: \textbf{(1) LLMs-as-a-judge}: Benchmarks such as AlpacaEval**Vig et al., "AlpacaEval: A Benchmark for Evaluating Large Language Models"**, PandaLM**Li et al., "PandaLM: A Pre-trained Language Model for Low-Resource Languages"**, MT-Bench**Zhang et al., "MT-Bench: A Benchmark for Evaluating Multitask Learning"**, and C-Eval**Hendrycks et al., "C-Eval: A Benchmark for Evaluating the Robustness of Large Language Models"** utilize large language models to evaluate a set of predefined questions. This approach is not only efficient but also cost-effective.

% \textbf{(2) Humans-as-a-judge}: Human evaluation involves assessing the quality and accuracy of model outputs through human involvement, providing more comprehensive and precise feedback **Radford et al., "Language Models as Few-Shot Learners"**. Experts, researchers, or users are invited to review model outputs when evaluating large language models.
% \textbf{(3) Other benchmarks}: Several traditional benchmarks employ a series of standardized tests and static datasets to quantitatively assess the performance of models across various tasks. For example, HELM**Paperno et al., "HELL: A Benchmark for Evaluating the Understanding of Natural Language Inference"** and MMLU**Hendrycks et al., "MMLU: A Benchmark for Evaluating Multilingual Models"** are primarily used to evaluate general knowledge and reasoning capabilities, while MATH**Liu et al., "MATH: A Benchmark for Evaluating Mathematical Reasoning in Language Models"** focuses on assessing mathematical skills. ToolBench**Chen et al., "ToolBench: A Benchmark for Evaluating the Ability to Use Tools"** concentrates on evaluating the ability to use tools. The performance of large language models is measured by their ability to complete these tasks accurately.

% Given the widespread use and cross-industry applications of LLMs**Brown et al., "Language Models as Few-Shot Learners"**, a dynamic robustness evaluation of their reasoning performance is essential**Chen et al., "ThinkBench: A Benchmark for Evaluating the Robustness of Large Language Models"**. While numerous benchmarks have been developed to assess LLMs' capabilities**Vig et al., "AlpacaEval: A Benchmark for Evaluating Large Language Models"**, these largely focus on train-time compute models, with limited attention given to the emerging test-time compute models like o1.

% With rapid improvement of LLMs**Brown et al., "Language Models as Few-Shot Learners"**, their applications across domains show exceptional capabilities**Chen et al., "ThinkBench: A Benchmark for Evaluating the Robustness of Large Language Models"**. 
Evaluating LLMs accurately and fairly poses a significant challenge**Hendrycks et al., "C-Eval: A Benchmark for Evaluating the Robustness of Large Language Models"**.

\paragraph{Benchmarking Process Reward Models (PRM)}

% To address ORM's oversight of the reasoning process, a few works on Process Reward Model (PRM) have emerged in recent years, offering more granular supervision. For instance, DeepMind**Radford et al., "Language Models as Few-Shot Learners"** emphasizes supervising both intermediate reasoning steps and the final result, thereby providing more detailed feedback.

% OpenAI**Brown et al., "Language Models as Few-Shot Learners"** further advances PRM by introducing a high-quality human-annotated process supervision dataset, PRM800K, highlighting the importance of verifying each intermediate step.

% ____ demonstrate the combination of evaluator models with majority voting schemes to enhance the reliability of results. ____ improve the reasoning process in large language models through reinforcement learning, integrating both outcome and process supervision. The recently proposed Generative Reward Model (GenRM)**Vig et al., "AlpacaEval: A Benchmark for Evaluating Large Language Models"** gets tons of attention, allowing evaluators to interact with generators in an information-rich manner.

% This shift reflects a widespread demand for more sophisticated process supervision methods.

\paragraph{Case Studies on Test-Time Compute Models (TTCM)}

% Recently, test-time compute models like o1-preview have shown impressive performance**Hendrycks et al., "C-Eval: A Benchmark for Evaluating the Robustness of Large Language Models"**. Evaluating these models is crucial for better understanding them. Efforts include LOGICGAME**Radford et al., "Language Models as Few-Shot Learners"**, assessing rule understanding and execution, ____ evaluating o1-preview's planning capabilities, and ____ conducting case studies across domains.

% Different from recent analytical works on o1, which emphasize planning evaluations or case-by-case analyses, our proposed benchmark is statistically significant and places greater emphasis on robustness and reasoning that have high practical application value.

\begin{table}
\centering
% \small
\caption{Statistics of reconstructed reasoning datasets based on three original test datasets, including AIME-500, AIME 2024, and GPQA Diamond.}
\label{tab:data_statistics}
 \begin{adjustbox}{width=0.99\columnwidth}{
\begin{tabular}{lccc} 
\toprule
                             & AIME-500 & AIME 2024 & GPQA Diamond \\ 
\midrule
% \# Class                     & -        & -         & 3                        \\
% \# Choices   & -        & -         & 4                        \\ 
% \midrule
\# Samples of original  & 500      & 30        & 198                      \\
Questions' Avg Len       & 51.1     & 60.1      & 67.7                     \\
Choices' Avg Len         & -        & -         & 27.8                     \\ 
\hline
\# Samples of OOD & 2,000      & 120        & 792                      \\
Questions' Avg Len       & 61.2     & 70.1      & 85.2                     \\
Choices' Avg Len         & -        & -         & 25.7                     \\
\bottomrule
\end{tabular}}    \end{adjustbox}
\end{table}