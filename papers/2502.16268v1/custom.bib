@misc{o1,
  author = {OpenAI},
  title = {Learning to Reason with LLMs},
  howpublished = "\url{https://openai.com/index/learning-to-reason-with-llms/
}",
  year = {2024}
}

@misc{o3,
  author = {OpenAI},
  title = {OpenAI o3-mini},
  howpublished = "\url{https://openai.com/index/openai-o3-mini/
}",
  year = {2025}
}

@article{glazer2024frontiermath,
  title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},
  author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and Chicharro, Diego and Chen, Evan and Gunning, Alex and Falkman Olsson, Caroline and Denain, Jean-Stanislas and Ho, Anson and de Oliveira Santos, Emily and others},
  journal={arXiv e-prints},
  pages={arXiv--2411},
  year={2024}
}

@book{kahneman2011thinking,
  title={Thinking, Fast and Slow},
  author={Kahneman, Daniel},
  year={2011},
  publisher={Farrar, Straus and Giroux},
  address={New York}
}

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}

@misc{deepscaler2025,
  title={DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL},
  author={Michael Luo and Sijun Tan and Justin Wong and Xiaoxiang Shi and William Tang and Manan Roongta and Colin Cai and Jeffrey Luo and Tianjun Zhang and Erran Li and Raluca Ada Popa and Ion Stoica},
  year={2025},
  note={Notion Blog}
}

@inproceedings{Liu2025Can1L,
  title={Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling},
  author={Runze Liu and Junqi Gao and Jian Zhao and Kaiyan Zhang and Xiu Li and Biqing Qi and Wanli Ouyang and Bowen Zhou},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:276249339}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{zhang2025lessons,
  title={The lessons of developing process reward models in mathematical reasoning},
  author={Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2501.07301},
  year={2025}
}

@misc{skyworkopeno12024,
  title={Skywork-o1 Open Series},
  author={Skywork-o1 Team},
  year={2024},
  month={November},
  howpublished={\url{https://huggingface.co/Skywork}},
  url={https://huggingface.co/Skywork},
}



@article{li2024openai,
  title={OpenAI-o1 AB Testing: Does the o1 model really do good reasoning in math problem solving?},
  author={Li, Leo and Luo, Ye and Pan, Tingyou},
  journal={arXiv preprint arXiv:2411.06198},
  year={2024}
}

@article{wu2024mrke,
  title={MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition},
  author={Wu, Jian and Yang, Linyi and Okumura, Manabu and Zhang, Yue},
  journal={arXiv preprint arXiv:2402.11924},
  year={2024}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@inproceedings{yang2022factmix,
  title={FactMix: Using a Few Labeled In-domain Examples to Generalize to Cross-domain Named Entity Recognition},
  author={Yang, Linyi and Yuan, Lifan and Cui, Leyang and Gao, Wenyang and Zhang, Yue},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={5360--5371},
  year={2022}
}


```bibtex
@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders J and Gur-Ari, Guy and Michalewski, Henryk and Dohan, David and Jiang, Jackie and Schulman, John and Fedus, William and Sutton, Charles},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}


@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{silver2017mastering,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@inproceedings{feng2023alphazero,
  title={Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training},
  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun},
  booktitle={ICML 2024},
year={2024}
}

@article{christianos2023pangu,
  title={Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning},
  author={Christianos, Filippos and Papoudakis, Georgios and Zimmer, Matthieu and Coste, Thomas and Wu, Zhihao and Chen, Jingxuan and Khandelwal, Khyati and Doran, James and Feng, Xidong and Liu, Jiacheng and others},
  journal={arXiv e-prints},
  pages={arXiv--2312},
  year={2023}
}


@book{elo1978rating,
  title={The rating of chessplayers, past and present},
  author={Elo, Arpad E},
  year={1978},
  publisher={Arco Pub.}
}


@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}


@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{bayo2023stepwise,
  title={Stepwise Verification: A New Paradigm for Language Model Safety},
  author={Bayo, Paul and Hall, Lauren and Jain, Arnav and Sadde, Sairaj and Shen, Caroline and Zhang, Jeffrey and Kasturi, Aditya},
  journal={arXiv preprint arXiv:2309.14975},
  year={2023}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}


@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{li2022making,
  title={Making large language models better reasoners with step-aware verifier},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2206.02336},
  year={2022}
}

@article{luo2024improve,
  title={Improve Mathematical Reasoning in Language Models by Automated Process Supervision},
  author={Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others},
  journal={arXiv preprint arXiv:2406.06592},
  year={2024}
}

@article{lin2020limitations,
  title={Limitations of autoregressive models and their alternatives},
  author={Lin, Chu-Cheng and Jaech, Aaron and Li, Xin and Gormley, Matthew R and Eisner, Jason},
  journal={arXiv preprint arXiv:2010.11939},
  year={2020}
}

@article{van2010unconscious,
  title={Unconscious activation of the prefrontal no-go network},
  author={Van Gaal, Simon and Ridderinkhof, K Richard and Scholte, H Steven and Lamme, Victor AF},
  journal={Journal of neuroscience},
  volume={30},
  number={11},
  pages={4143--4150},
  year={2010},
  publisher={Soc Neuroscience}
}

@article{bellman1958dynamic,
  title={Dynamic programming and stochastic control processes},
  author={Bellman, Richard},
  journal={Information and control},
  volume={1},
  number={3},
  pages={228--239},
  year={1958},
  publisher={Elsevier}
}

@article{wu2024empirical,
  title={An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal={arXiv preprint arXiv:2408.00724},
  year={2024}
}


@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}

@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}

@article{goyal2023think,
  title={Think before you speak: Training language models with pause tokens},
  author={Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  journal={arXiv preprint arXiv:2310.02226},
  year={2023}
}

@article{cobbe2021gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@inproceedings{yu2024ovm,
  title={OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning},
  author={Yu, Fei and Gao, Anningzhe and Wang, Benyou},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={858--875},
  year={2024}
}

@article{zhang2024generative,
  title={Generative Verifiers: Reward Modeling as Next-Token Prediction},
  author={Zhang, Lunjun and Hosseini, Arian and Bansal, Hritik and Kazemi, Mehran and Kumar, Aviral and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2408.15240},
  year={2024}
}

@article{zelikman2024quiet,
  title={Quiet-star: Language models can teach themselves to think before speaking},
  author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2403.09629},
  year={2024}
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={9426--9439},
  year={2024}
}

@article{wang2024multi,
  title={Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision},
  author={Wang, Zihan and Li, Yunxuan and Wu, Yuexin and Luo, Liangchen and Hou, Le and Yu, Hongkun and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.02658},
  year={2024}
}

@incollection{van2012reinforcement,
  title={Reinforcement learning and markov decision processes},
  author={Van Otterlo, Martijn and Wiering, Marco},
  booktitle={Reinforcement learning: State-of-the-art},
  pages={3--42},
  year={2012},
  publisher={Springer}
}

@inproceedings{carta2023grounding,
  title={Grounding large language models in interactive environments with online reinforcement learning},
  author={Carta, Thomas and Romac, Cl{\'e}ment and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  booktitle={International Conference on Machine Learning},
  pages={3676--3713},
  year={2023},
  organization={PMLR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Yu and Guo, Daya},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}


@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}


@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@article{wen2024reinforcing,
  title={Reinforcing Language Agents via Policy Optimization with Action Decomposition},
  author={Wen, Muning and Wan, Ziyu and Zhang, Weinan and Wang, Jun and Wen, Ying},
  journal={arXiv preprint arXiv:2405.15821},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{luo2024improve,
  title={Improve Mathematical Reasoning in Language Models by Automated Process Supervision},
  author={Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others},
  journal={arXiv preprint arXiv:2406.06592},
  year={2024}
}

@article{zhang2024llama,
  title={Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning},
  author={Zhang, Di and Wu, Jianbo and Lei, Jingdi and Che, Tong and Li, Jiatong and Xie, Tong and Huang, Xiaoshui and Zhang, Shufei and Pavone, Marco and Li, Yuqiang and others},
  journal={arXiv preprint arXiv:2410.02884},
  year={2024}
}

@article{kirchner2024prover,
  title={Prover-verifier games improve legibility of llm outputs},
  author={Kirchner, Jan Hendrik and Chen, Yining and Edwards, Harri and Leike, Jan and McAleese, Nat and Burda, Yuri},
  journal={arXiv preprint arXiv:2407.13692},
  year={2024}
}

@article{wang2024openr,
  title={OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models},
  author={Wang, Jun and Fang, Meng and Wan, Ziyu and Wen, Muning and Zhu, Jiachen and Liu, Anjie and Gong, Ziqin and Song, Yan and Chen, Lei and Ni, Lionel M and others},
  journal={arXiv preprint arXiv:2410.09671},
  year={2024}
}

@article{zhu2024dyval,
  title={Dyval 2: Dynamic evaluation of large language models by meta probing agents},
  author={Zhu, Kaijie and Wang, Jindong and Zhao, Qinlin and Xu, Ruochen and Xie, Xing},
  journal={arXiv preprint arXiv:2402.14865},
  year={2024}
}


% o1 Planning
@article{wang2024planning,
  title={On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability},
  author={Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2409.19924},
  year={2024}
}

% o1 Logic Reasoning
@article{gui2024logicgame,
  title={Logicgame: Benchmarking rule-based reasoning abilities of large language models},
  author={Gui, Jiayi and Liu, Yiming and Cheng, Jiale and Gu, Xiaotao and Liu, Xiao and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie},
  journal={arXiv preprint arXiv:2408.15778},
  year={2024}
}

% o1 Case Study
@article{zhong2024evaluation,
  title={Evaluation of openai o1: Opportunities and challenges of agi},
  author={Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others},
  journal={arXiv preprint arXiv:2409.18486},
  year={2024}
}

@article{el2021automatic,
  title={Automatic text summarization: A comprehensive survey},
  author={El-Kassas, Wafaa S and Salama, Cherif R and Rafea, Ahmed A and Mohamed, Hoda K},
  journal={Expert systems with applications},
  volume={165},
  pages={113679},
  year={2021},
  publisher={Elsevier}
}

@article{hao2022recent,
  title={Recent progress in leveraging deep learning methods for question answering},
  author={Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying},
  journal={Neural Computing and Applications},
  volume={34},
  number={4},
  pages={2765--2783},
  year={2022},
  publisher={Springer London London}
}

@inproceedings{cheng2023ml,
  title={ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding},
  author={Cheng, Xuxin and Cao, Bowen and Ye, Qichen and Zhu, Zhihong and Li, Hongxiang and Zou, Yuexian},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={6492--6505},
  year={2023}
}

@inproceedings{huang2024lateval,
  title={LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles},
  author={Huang, Shulin and Ma, Shirong and Li, Yinghui and Huang, Mengzuo and Zou, Wuhe and Zhang, Weidong and Zheng, Haitao},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={10186--10197},
  year={2024}
}

@article{li2024llms,
  title={When llms meet cunning questions: A fallacy understanding benchmark for large language models},
  author={Li, Yinghui and Zhou, Qingyu and Luo, Yuanzhen and Ma, Shirong and Li, Yangning and Zheng, Hai-Tao and Hu, Xuming and Yu, Philip S},
  journal={arXiv preprint arXiv:2402.11100},
  year={2024}
}

@article{zhu2023promptbench,
  title={Promptbench: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Zhenqiang Gong, Neil and others},
  journal={arXiv e-prints},
  pages={arXiv--2306},
  year={2023}
}

@inproceedings{zhu2023dyval,
  title={Dyval: Dynamic evaluation of large language models for reasoning tasks},
  author={Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

%OOD GLUE
@article{yuan2023revisiting,
  title={Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis, and LLMs evaluations},
  author={Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, Fangyuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={58478--58507},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{wang2023robustness,
  title={On the robustness of chatgpt: An adversarial and out-of-distribution perspective},
  author={Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others},
  journal={arXiv preprint arXiv:2302.12095},
  year={2023}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{ribeiro2022adaptive,
  title={Adaptive testing and debugging of nlp models},
  author={Ribeiro, Marco Tulio and Lundberg, Scott},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3253--3267},
  year={2022}
}

@inproceedings{gao2023adaptive,
  title={Adaptive testing of computer vision models},
  author={Gao, Irena and Ilharco, Gabriel and Lundberg, Scott and Ribeiro, Marco Tulio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4003--4014},
  year={2023}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{xu2023tool,
  title={On the tool manipulation capability of open-source large language models},
  author={Xu, Qiantong and Hong, Fenglu and Li, Bo and Hu, Changran and Chen, Zhengyu and Zhang, Jian},
  journal={arXiv preprint arXiv:2305.16504},
  year={2023}
}

@misc{li2023alpacaeval,
  title={Alpacaeval: An automatic evaluator of instruction-following models},
  author={Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
## Optional References

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Schärli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@inproceedings{petrenko2020sample,
  title={Sample factory: Egocentric 3d control from pixels at 100000 fps with asynchronous reinforcement learning},
  author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen},
  booktitle={International Conference on Machine Learning},
  pages={7652--7662},
  year={2020},
  organization={PMLR}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@misc{zebralogic2024,
    title={ZebraLogic: Benchmarking the Logical Reasoning Ability of Language Models},
    author={Bill Yuchen Lin and Ronan Le Bras and Yejin Choi},
    url={https://huggingface.co/spaces/allenai/ZebraLogic},
    year={2024}
}

@inproceedings{yang2023glue,
  title={GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective},
  author={Yang, Linyi and Zhang, Shuibai and Qin, Libo and Li, Yafu and Wang, Yidong and Liu, Hanmeng and Wang, Jindong and Xie, Xing and Zhang, Yue},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={12731--12750},
  year={2023}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}


@inproceedings{gao2024fact,
  title={Fact: Teaching MLLMs with Faithful, Concise and Transferable Rationales},
  author={Gao, Minghe and Chen, Shuang and Pang, Liang and Yao, Yuan and Dang, Jisheng and Zhang, Wenqiao and Li, Juncheng and Tang, Siliang and Zhuang, Yueting and Chua, Tat-Seng},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={846--855},
  year={2024}
}


@misc{o1journey,
  author = {Yiwei Qin and Xuefeng Li and Haoyang Zou and Yixiu Liu and Shijie Xia and Zhen Huang and Yixin Ye and Weizhe Yuan and Zhengzhong Liu and Yuanzhi Li and Pengfei Liu},
  title = {O1 Replication Journey: A Strategic Progress Report – Part 1},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/GAIR-NLP/O1-Journey}},
}

@article{wang2023pandalm,
  title={Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization},
  author={Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others},
  journal={arXiv preprint arXiv:2306.05087},
  year={2023}
}

@article{xu2024large,
  title={Large language models for generative information extraction: A survey},
  author={Xu, Derong and Chen, Wei and Peng, Wenjun and Zhang, Chao and Xu, Tong and Zhao, Xiangyu and Wu, Xian and Zheng, Yefeng and Wang, Yang and Chen, Enhong},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186357},
  year={2024},
  publisher={Springer}
}

@inproceedings{karanikolas2023large,
  title={Large language models versus natural language understanding and generation},
  author={Karanikolas, Nikitas and Manga, Eirini and Samaridi, Nikoletta and Tousidou, Eleni and Vassilakopoulos, Michael},
  booktitle={Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics},
  pages={278--290},
  year={2023}
}

@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@inproceedings{lu23mathvista,
  title={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  booktitle={The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23},
  year={2023}
}

@inproceedings{wu2024autogen,
  title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
  year={2024}
}

@article{opedal2024mathgap,
  title={MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs},
  author={Opedal, Andreas and Shirakami, Haruki and Sch{\"o}lkopf, Bernhard and Saparov, Abulhair and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2410.13502},
  year={2024}
}

@inproceedings{azerbayevllemma,
  title={Llemma: An Open Language Model for Mathematics},
  author={Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Dos Santos, Marco and McAleer, Stephen Marcus and Jiang, Albert Q and Deng, Jia and Biderman, Stella and Welleck, Sean},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{rein2023gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}
@inproceedings{zhengminif2f,
  title={miniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
  author={Zheng, Kunhao and Han, Jesse Michael and Polu, Stanislas},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{li2019textbugger,
  title={TextBugger: Generating Adversarial Text Against Real-world Applications},
  author={Li, J and Ji, S and Du, T and Li, B and Wang, T},
  booktitle={26th Annual Network and Distributed System Security Symposium},
  year={2019}
}

@inproceedings{naik2018stress,
  title={Stress Test Evaluation for Natural Language Inference},
  author={Naik, Aakanksha and Ravichander, Abhilasha and Sadeh, Norman and Rose, Carolyn and Neubig, Graham},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={2340--2353},
  year={2018}
}

@inproceedings{ribeiro2020beyond,
  title={Beyond Accuracy: Behavioral Testing of NLP Models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4902--4912},
  year={2020}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{riviere2024gemma,
  title={Gemma 2: Improving Open Language Models at a Practical Size},
  author={Rivi{\`e}re, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and Ferret, Johan and others},
  journal={CoRR},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{gao2018black,
  title={Black-box generation of adversarial text sequences to evade deep learning classifiers},
  author={Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  pages={50--56},
  year={2018},
  organization={IEEE}
}

@article{huang2024o1,
  title={O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?},
  author={Huang, Zhen and Zou, Haoyang and Li, Xuefeng and Liu, Yixiu and Zheng, Yuxiang and Chern, Ethan and Xia, Shijie and Qin, Yiwei and Yuan, Weizhe and Liu, Pengfei},
  journal={arXiv preprint arXiv:2411.16489},
  year={2024}
}

@article{li2024gsm,
  title={GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers},
  author={Li, Qintong and Cui, Leyang and Zhao, Xueliang and Kong, Lingpeng and Bi, Wei},
  journal={arXiv preprint arXiv:2402.19255},
  year={2024}
}

@article{hosseini2024not,
  title={Not All LLM Reasoners Are Created Equal},
  author={Hosseini, Arian and Sordoni, Alessandro and Toyama, Daniel and Courville, Aaron and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2410.01748},
  year={2024}
}

@misc{zheng2024processbench,
      title={ProcessBench: Identifying Process Errors in Mathematical Reasoning}, 
      author={Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2412.06559},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.06559}, 
}
@article{delaney2021uncertainty,
  title={Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations: Pitfalls and Solutions},
  author={Delaney, Eoin and Greene, Derek and Keane, Mark T},
  journal={arXiv preprint arXiv:2107.09734},
  year={2021}
}
@article{kenny2021generating,
  title={On generating plausible counterfactual and semi-factual explanations for deep learning},
  author={Kenny, Eoin M and Keane, Mark T},
  year={2021}
}