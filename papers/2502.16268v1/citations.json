[
  {
    "index": 0,
    "papers": [
      {
        "key": "chang2024survey",
        "author": "Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others",
        "title": "A survey on evaluation of large language models"
      },
      {
        "key": "ribeiro2022adaptive",
        "author": "Ribeiro, Marco Tulio and Lundberg, Scott",
        "title": "Adaptive testing and debugging of nlp models"
      },
      {
        "key": "gao2023adaptive",
        "author": "Gao, Irena and Ilharco, Gabriel and Lundberg, Scott and Ribeiro, Marco Tulio",
        "title": "Adaptive testing of computer vision models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chang2024survey",
        "author": "Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others",
        "title": "A survey on evaluation of large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2023alpacaeval",
        "author": "Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2023pandalm",
        "author": "Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others",
        "title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zheng2023judging",
        "author": "Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "huang2024c",
        "author": "Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others",
        "title": "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ribeiro2022adaptive",
        "author": "Ribeiro, Marco Tulio and Lundberg, Scott",
        "title": "Adaptive testing and debugging of nlp models"
      },
      {
        "key": "gao2023adaptive",
        "author": "Gao, Irena and Ilharco, Gabriel and Lundberg, Scott and Ribeiro, Marco Tulio",
        "title": "Adaptive testing of computer vision models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liang2022holistic",
        "author": "Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others",
        "title": "Holistic evaluation of language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hendrycks2021measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring mathematical problem solving with the math dataset"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xu2023tool",
        "author": "Xu, Qiantong and Hong, Fenglu and Li, Bo and Hu, Changran and Chen, Zhengyu and Zhang, Jian",
        "title": "On the tool manipulation capability of open-source large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "el2021automatic",
        "author": "El-Kassas, Wafaa S and Salama, Cherif R and Rafea, Ahmed A and Mohamed, Hoda K",
        "title": "Automatic text summarization: A comprehensive survey"
      },
      {
        "key": "hao2022recent",
        "author": "Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying",
        "title": "Recent progress in leveraging deep learning methods for question answering"
      },
      {
        "key": "cheng2023ml",
        "author": "Cheng, Xuxin and Cao, Bowen and Ye, Qichen and Zhu, Zhihong and Li, Hongxiang and Zou, Yuexian",
        "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding"
      },
      {
        "key": "gao2024fact",
        "author": "Gao, Minghe and Chen, Shuang and Pang, Liang and Yao, Yuan and Dang, Jisheng and Zhang, Wenqiao and Li, Juncheng and Tang, Siliang and Zhuang, Yueting and Chua, Tat-Seng",
        "title": "Fact: Teaching MLLMs with Faithful, Concise and Transferable Rationales"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "glazer2024frontiermath",
        "author": "Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and Chicharro, Diego and Chen, Evan and Gunning, Alex and Falkman Olsson, Caroline and Denain, Jean-Stanislas and Ho, Anson and de Oliveira Santos, Emily and others",
        "title": "FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "li2023alpacaeval",
        "author": "Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      },
      {
        "key": "huang2024lateval",
        "author": "Huang, Shulin and Ma, Shirong and Li, Yinghui and Huang, Mengzuo and Zou, Wuhe and Zhang, Weidong and Zheng, Haitao",
        "title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles"
      },
      {
        "key": "li2024llms",
        "author": "Li, Yinghui and Zhou, Qingyu and Luo, Yuanzhen and Ma, Shirong and Li, Yangning and Zheng, Hai-Tao and Hu, Xuming and Yu, Philip S",
        "title": "When llms meet cunning questions: A fallacy understanding benchmark for large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "gui2024logicgame",
        "author": "Gui, Jiayi and Liu, Yiming and Cheng, Jiale and Gu, Xiaotao and Liu, Xiao and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie",
        "title": "Logicgame: Benchmarking rule-based reasoning abilities of large language models"
      },
      {
        "key": "wang2024planning",
        "author": "Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang",
        "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability"
      },
      {
        "key": "zhong2024evaluation",
        "author": "Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others",
        "title": "Evaluation of openai o1: Opportunities and challenges of agi"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "chang2024survey",
        "author": "Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others",
        "title": "A survey on evaluation of large language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "chang2024survey",
        "author": "Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others",
        "title": "A survey on evaluation of large language models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2023alpacaeval",
        "author": "Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wang2023pandalm",
        "author": "Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others",
        "title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zheng2023judging",
        "author": "Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "huang2024c",
        "author": "Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others",
        "title": "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "ribeiro2022adaptive",
        "author": "Ribeiro, Marco Tulio and Lundberg, Scott",
        "title": "Adaptive testing and debugging of nlp models"
      },
      {
        "key": "gao2023adaptive",
        "author": "Gao, Irena and Ilharco, Gabriel and Lundberg, Scott and Ribeiro, Marco Tulio",
        "title": "Adaptive testing of computer vision models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "liang2022holistic",
        "author": "Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others",
        "title": "Holistic evaluation of language models"
      },
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      },
      {
        "key": "hendrycks2021measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring mathematical problem solving with the math dataset"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "liang2022holistic",
        "author": "Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others",
        "title": "Holistic evaluation of language models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "hendrycks2021measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring mathematical problem solving with the math dataset"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "xu2023tool",
        "author": "Xu, Qiantong and Hong, Fenglu and Li, Bo and Hu, Changran and Chen, Zhengyu and Zhang, Jian",
        "title": "On the tool manipulation capability of open-source large language models"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "el2021automatic",
        "author": "El-Kassas, Wafaa S and Salama, Cherif R and Rafea, Ahmed A and Mohamed, Hoda K",
        "title": "Automatic text summarization: A comprehensive survey"
      },
      {
        "key": "hao2022recent",
        "author": "Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying",
        "title": "Recent progress in leveraging deep learning methods for question answering"
      },
      {
        "key": "cheng2023ml",
        "author": "Cheng, Xuxin and Cao, Bowen and Ye, Qichen and Zhu, Zhihong and Li, Hongxiang and Zou, Yuexian",
        "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding"
      },
      {
        "key": "gao2024fact",
        "author": "Gao, Minghe and Chen, Shuang and Pang, Liang and Yao, Yuan and Dang, Jisheng and Zhang, Wenqiao and Li, Juncheng and Tang, Siliang and Zhuang, Yueting and Chua, Tat-Seng",
        "title": "Fact: Teaching MLLMs with Faithful, Concise and Transferable Rationales"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "glazer2024frontiermath",
        "author": "Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and Chicharro, Diego and Chen, Evan and Gunning, Alex and Falkman Olsson, Caroline and Denain, Jean-Stanislas and Ho, Anson and de Oliveira Santos, Emily and others",
        "title": "FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "li2023alpacaeval",
        "author": "Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      },
      {
        "key": "huang2024lateval",
        "author": "Huang, Shulin and Ma, Shirong and Li, Yinghui and Huang, Mengzuo and Zou, Wuhe and Zhang, Weidong and Zheng, Haitao",
        "title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles"
      },
      {
        "key": "li2024llms",
        "author": "Li, Yinghui and Zhou, Qingyu and Luo, Yuanzhen and Ma, Shirong and Li, Yangning and Zheng, Hai-Tao and Hu, Xuming and Yu, Philip S",
        "title": "When llms meet cunning questions: A fallacy understanding benchmark for large language models"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "gui2024logicgame",
        "author": "Gui, Jiayi and Liu, Yiming and Cheng, Jiale and Gu, Xiaotao and Liu, Xiao and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie",
        "title": "Logicgame: Benchmarking rule-based reasoning abilities of large language models"
      },
      {
        "key": "wang2024planning",
        "author": "Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang",
        "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability"
      },
      {
        "key": "zhong2024evaluation",
        "author": "Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others",
        "title": "Evaluation of openai o1: Opportunities and challenges of agi"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "yuan2023revisiting",
        "author": "Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, Fangyuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong",
        "title": "Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis, and LLMs evaluations"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "yang2023glue",
        "author": "Yang, Linyi and Zhang, Shuibai and Qin, Libo and Li, Yafu and Wang, Yidong and Liu, Hanmeng and Wang, Jindong and Xie, Xing and Zhang, Yue",
        "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "zebralogic2024",
        "author": "Bill Yuchen Lin and Ronan Le Bras and Yejin Choi",
        "title": "ZebraLogic: Benchmarking the Logical Reasoning Ability of Language Models"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "wang2023robustness",
        "author": "Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others",
        "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "li2024gsm",
        "author": "Li, Qintong and Cui, Leyang and Zhao, Xueliang and Kong, Lingpeng and Bi, Wei",
        "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "yang2022factmix",
        "author": "Yang, Linyi and Yuan, Lifan and Cui, Leyang and Gao, Wenyang and Zhang, Yue",
        "title": "FactMix: Using a Few Labeled In-domain Examples to Generalize to Cross-domain Named Entity Recognition"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "hosseini2024not",
        "author": "Hosseini, Arian and Sordoni, Alessandro and Toyama, Daniel and Courville, Aaron and Agarwal, Rishabh",
        "title": "Not All LLM Reasoners Are Created Equal"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "muennighoff2025s1",
        "author": "Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\\`e}s, Emmanuel and Hashimoto, Tatsunori",
        "title": "s1: Simple test-time scaling"
      },
      {
        "key": "guo2025deepseek",
        "author": "Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",
        "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "wang2023robustness",
        "author": "Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others",
        "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective"
      },
      {
        "key": "glazer2024frontiermath",
        "author": "Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and Chicharro, Diego and Chen, Evan and Gunning, Alex and Falkman Olsson, Caroline and Denain, Jean-Stanislas and Ho, Anson and de Oliveira Santos, Emily and others",
        "title": "FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI"
      },
      {
        "key": "li2024gsm",
        "author": "Li, Qintong and Cui, Leyang and Zhao, Xueliang and Kong, Lingpeng and Bi, Wei",
        "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "li2024openai",
        "author": "Li, Leo and Luo, Ye and Pan, Tingyou",
        "title": "OpenAI-o1 AB Testing: Does the o1 model really do good reasoning in math problem solving?"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "yuan2023revisiting",
        "author": "Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, Fangyuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong",
        "title": "Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis, and LLMs evaluations"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "yang2023glue",
        "author": "Yang, Linyi and Zhang, Shuibai and Qin, Libo and Li, Yafu and Wang, Yidong and Liu, Hanmeng and Wang, Jindong and Xie, Xing and Zhang, Yue",
        "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "zebralogic2024",
        "author": "Bill Yuchen Lin and Ronan Le Bras and Yejin Choi",
        "title": "ZebraLogic: Benchmarking the Logical Reasoning Ability of Language Models"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "wang2023robustness",
        "author": "Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others",
        "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "li2024gsm",
        "author": "Li, Qintong and Cui, Leyang and Zhao, Xueliang and Kong, Lingpeng and Bi, Wei",
        "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers"
      }
    ]
  },
  {
    "index": 46,
    "papers": [
      {
        "key": "yang2022factmix",
        "author": "Yang, Linyi and Yuan, Lifan and Cui, Leyang and Gao, Wenyang and Zhang, Yue",
        "title": "FactMix: Using a Few Labeled In-domain Examples to Generalize to Cross-domain Named Entity Recognition"
      }
    ]
  },
  {
    "index": 47,
    "papers": [
      {
        "key": "hosseini2024not",
        "author": "Hosseini, Arian and Sordoni, Alessandro and Toyama, Daniel and Courville, Aaron and Agarwal, Rishabh",
        "title": "Not All LLM Reasoners Are Created Equal"
      }
    ]
  },
  {
    "index": 48,
    "papers": [
      {
        "key": "wu2024mrke",
        "author": "Wu, Jian and Yang, Linyi and Okumura, Manabu and Zhang, Yue",
        "title": "MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition"
      }
    ]
  },
  {
    "index": 49,
    "papers": [
      {
        "key": "wu2024mrke",
        "author": "Wu, Jian and Yang, Linyi and Okumura, Manabu and Zhang, Yue",
        "title": "MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition"
      }
    ]
  },
  {
    "index": 50,
    "papers": [
      {
        "key": "el2021automatic",
        "author": "El-Kassas, Wafaa S and Salama, Cherif R and Rafea, Ahmed A and Mohamed, Hoda K",
        "title": "Automatic text summarization: A comprehensive survey"
      },
      {
        "key": "hao2022recent",
        "author": "Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying",
        "title": "Recent progress in leveraging deep learning methods for question answering"
      },
      {
        "key": "cheng2023ml",
        "author": "Cheng, Xuxin and Cao, Bowen and Ye, Qichen and Zhu, Zhihong and Li, Hongxiang and Zou, Yuexian",
        "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding"
      },
      {
        "key": "gao2024fact",
        "author": "Gao, Minghe and Chen, Shuang and Pang, Liang and Yao, Yuan and Dang, Jisheng and Zhang, Wenqiao and Li, Juncheng and Tang, Siliang and Zhuang, Yueting and Chua, Tat-Seng",
        "title": "Fact: Teaching MLLMs with Faithful, Concise and Transferable Rationales"
      }
    ]
  },
  {
    "index": 51,
    "papers": [
      {
        "key": "glazer2024frontiermath",
        "author": "Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and Chicharro, Diego and Chen, Evan and Gunning, Alex and Falkman Olsson, Caroline and Denain, Jean-Stanislas and Ho, Anson and de Oliveira Santos, Emily and others",
        "title": "FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI"
      }
    ]
  },
  {
    "index": 52,
    "papers": [
      {
        "key": "li2023alpacaeval",
        "author": "Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      },
      {
        "key": "huang2024lateval",
        "author": "Huang, Shulin and Ma, Shirong and Li, Yinghui and Huang, Mengzuo and Zou, Wuhe and Zhang, Weidong and Zheng, Haitao",
        "title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles"
      },
      {
        "key": "li2024llms",
        "author": "Li, Yinghui and Zhou, Qingyu and Luo, Yuanzhen and Ma, Shirong and Li, Yangning and Zheng, Hai-Tao and Hu, Xuming and Yu, Philip S",
        "title": "When llms meet cunning questions: A fallacy understanding benchmark for large language models"
      }
    ]
  },
  {
    "index": 53,
    "papers": [
      {
        "key": "li2023alpacaeval",
        "author": "Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      },
      {
        "key": "huang2024lateval",
        "author": "Huang, Shulin and Ma, Shirong and Li, Yinghui and Huang, Mengzuo and Zou, Wuhe and Zhang, Weidong and Zheng, Haitao",
        "title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles"
      },
      {
        "key": "li2024llms",
        "author": "Li, Yinghui and Zhou, Qingyu and Luo, Yuanzhen and Ma, Shirong and Li, Yangning and Zheng, Hai-Tao and Hu, Xuming and Yu, Philip S",
        "title": "When llms meet cunning questions: A fallacy understanding benchmark for large language models"
      }
    ]
  },
  {
    "index": 54,
    "papers": [
      {
        "key": "wang2024planning",
        "author": "Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang",
        "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability"
      }
    ]
  },
  {
    "index": 55,
    "papers": [
      {
        "key": "gui2024logicgame",
        "author": "Gui, Jiayi and Liu, Yiming and Cheng, Jiale and Gu, Xiaotao and Liu, Xiao and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie",
        "title": "Logicgame: Benchmarking rule-based reasoning abilities of large language models"
      }
    ]
  },
  {
    "index": 56,
    "papers": [
      {
        "key": "zhong2024evaluation",
        "author": "Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others",
        "title": "Evaluation of openai o1: Opportunities and challenges of agi"
      }
    ]
  },
  {
    "index": 57,
    "papers": [
      {
        "key": "el2021automatic",
        "author": "El-Kassas, Wafaa S and Salama, Cherif R and Rafea, Ahmed A and Mohamed, Hoda K",
        "title": "Automatic text summarization: A comprehensive survey"
      },
      {
        "key": "hao2022recent",
        "author": "Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying",
        "title": "Recent progress in leveraging deep learning methods for question answering"
      },
      {
        "key": "cheng2023ml",
        "author": "Cheng, Xuxin and Cao, Bowen and Ye, Qichen and Zhu, Zhihong and Li, Hongxiang and Zou, Yuexian",
        "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding"
      },
      {
        "key": "gao2024fact",
        "author": "Gao, Minghe and Chen, Shuang and Pang, Liang and Yao, Yuan and Dang, Jisheng and Zhang, Wenqiao and Li, Juncheng and Tang, Siliang and Zhuang, Yueting and Chua, Tat-Seng",
        "title": "Fact: Teaching MLLMs with Faithful, Concise and Transferable Rationales"
      }
    ]
  },
  {
    "index": 58,
    "papers": [
      {
        "key": "glazer2024frontiermath",
        "author": "Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and Chicharro, Diego and Chen, Evan and Gunning, Alex and Falkman Olsson, Caroline and Denain, Jean-Stanislas and Ho, Anson and de Oliveira Santos, Emily and others",
        "title": "FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI"
      }
    ]
  },
  {
    "index": 59,
    "papers": [
      {
        "key": "li2023alpacaeval",
        "author": "Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Alpacaeval: An automatic evaluator of instruction-following models"
      },
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      },
      {
        "key": "huang2024lateval",
        "author": "Huang, Shulin and Ma, Shirong and Li, Yinghui and Huang, Mengzuo and Zou, Wuhe and Zhang, Weidong and Zheng, Haitao",
        "title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles"
      },
      {
        "key": "li2024llms",
        "author": "Li, Yinghui and Zhou, Qingyu and Luo, Yuanzhen and Ma, Shirong and Li, Yangning and Zheng, Hai-Tao and Hu, Xuming and Yu, Philip S",
        "title": "When llms meet cunning questions: A fallacy understanding benchmark for large language models"
      }
    ]
  },
  {
    "index": 60,
    "papers": [
      {
        "key": "gui2024logicgame",
        "author": "Gui, Jiayi and Liu, Yiming and Cheng, Jiale and Gu, Xiaotao and Liu, Xiao and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie",
        "title": "Logicgame: Benchmarking rule-based reasoning abilities of large language models"
      },
      {
        "key": "wang2024planning",
        "author": "Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang",
        "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability"
      },
      {
        "key": "zhong2024evaluation",
        "author": "Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others",
        "title": "Evaluation of openai o1: Opportunities and challenges of agi"
      }
    ]
  },
  {
    "index": 61,
    "papers": [
      {
        "key": "gui2024logicgame",
        "author": "Gui, Jiayi and Liu, Yiming and Cheng, Jiale and Gu, Xiaotao and Liu, Xiao and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie",
        "title": "Logicgame: Benchmarking rule-based reasoning abilities of large language models"
      }
    ]
  },
  {
    "index": 62,
    "papers": [
      {
        "key": "wang2024planning",
        "author": "Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang",
        "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability"
      }
    ]
  },
  {
    "index": 63,
    "papers": [
      {
        "key": "zhong2024evaluation",
        "author": "Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others",
        "title": "Evaluation of openai o1: Opportunities and challenges of agi"
      }
    ]
  },
  {
    "index": 64,
    "papers": [
      {
        "key": "gui2024logicgame",
        "author": "Gui, Jiayi and Liu, Yiming and Cheng, Jiale and Gu, Xiaotao and Liu, Xiao and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie",
        "title": "Logicgame: Benchmarking rule-based reasoning abilities of large language models"
      }
    ]
  },
  {
    "index": 65,
    "papers": [
      {
        "key": "wang2024planning",
        "author": "Wang, Kevin and Li, Junbo and Bhatt, Neel P and Xi, Yihan and Liu, Qiang and Topcu, Ufuk and Wang, Zhangyang",
        "title": "On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability"
      }
    ]
  },
  {
    "index": 66,
    "papers": [
      {
        "key": "zhong2024evaluation",
        "author": "Zhong, Tianyang and Liu, Zhengliang and Pan, Yi and Zhang, Yutong and Zhou, Yifan and Liang, Shizhe and Wu, Zihao and Lyu, Yanjun and Shu, Peng and Yu, Xiaowei and others",
        "title": "Evaluation of openai o1: Opportunities and challenges of agi"
      }
    ]
  },
  {
    "index": 67,
    "papers": [
      {
        "key": "wang2024openr",
        "author": "Wang, Jun and Fang, Meng and Wan, Ziyu and Wen, Muning and Zhu, Jiachen and Liu, Anjie and Gong, Ziqin and Song, Yan and Chen, Lei and Ni, Lionel M and others",
        "title": "OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models"
      }
    ]
  },
  {
    "index": 68,
    "papers": [
      {
        "key": "cobbe2021gsm8k",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others",
        "title": "Training verifiers to solve math word problems"
      }
    ]
  },
  {
    "index": 69,
    "papers": [
      {
        "key": "uesato2022solving",
        "author": "Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina",
        "title": "Solving math word problems with process-and outcome-based feedback"
      }
    ]
  },
  {
    "index": 70,
    "papers": [
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      }
    ]
  },
  {
    "index": 71,
    "papers": [
      {
        "key": "li2022making",
        "author": "Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu",
        "title": "Making large language models better reasoners with step-aware verifier"
      }
    ]
  },
  {
    "index": 72,
    "papers": [
      {
        "key": "yu2024ovm",
        "author": "Yu, Fei and Gao, Anningzhe and Wang, Benyou",
        "title": "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning"
      }
    ]
  },
  {
    "index": 73,
    "papers": [
      {
        "key": "zhang2024generative",
        "author": "Zhang, Lunjun and Hosseini, Arian and Bansal, Hritik and Kazemi, Mehran and Kumar, Aviral and Agarwal, Rishabh",
        "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction"
      }
    ]
  },
  {
    "index": 74,
    "papers": [
      {
        "key": "zheng2024processbench",
        "author": "Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin",
        "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning"
      }
    ]
  }
]