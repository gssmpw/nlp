% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
\usepackage{soul, color, xcolor}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{subfig}
\usepackage{natbib}
\newcommand{\neal}[1]{{\color{blue}{leyang: #1}}}
\newcommand{\shulin}[2][]{{\color{red}{#2}}}
\newcommand{\ys}[1]{{\color{cyan} [YanSong: #1]}}

\usepackage[english]{babel}
% \usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{ThinkBench: Dynamic Out-of-Distribution Evaluation \\ for Robust LLM Reasoning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Shulin Huang \\
%   Westlake University 
%   % / Address line 1 \\
%   % Affiliation / Address line 2 \\
%   % Affiliation / Address line 3 \\
%   \texttt{huangshulin@westlake.edu.cn} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\author{
 \textbf{Shulin Huang\textsuperscript{1,2}},
 \textbf{Linyi Yang\textsuperscript{3}\thanks{Correspondence to: (yanglinyiucd@gmail.com, zhangyue@westlake.edu.cn)}},
 \textbf{Yan Song\textsuperscript{3}},
 \textbf{Shuang Chen\textsuperscript{1}}, 
 \textbf{Leyang Cui\textsuperscript{2}},
 \textbf{Ziyu Wan\textsuperscript{4}}, \\
 \textbf{Qingcheng Zeng\textsuperscript{5}},
  \textbf{Ying Wen\textsuperscript{4}}, 
 \textbf{Kun Shao\textsuperscript{6}},
  \textbf{Weinan Zhang\textsuperscript{4}},
    \textbf{Jun Wang\textsuperscript{3}},
    \textbf{Yue Zhang\textsuperscript{2$*$}} \\
 \textsuperscript{1}Zhejiang University,
 \textsuperscript{2}Westlake University,
 \textsuperscript{3}University College London,\\
 \textsuperscript{4}Shanghai Jiao Tong University,
 % \textsuperscript{5}Tencent AI Lab\\
 \textsuperscript{5}Northwestern University,
 \textsuperscript{6}Huawei Noah’s Ark Lab\\
  \texttt{huangshulin@westlake.edu.cn}
}

\begin{document}
\maketitle
\begin{abstract}
Evaluating large language models (LLMs) poses significant challenges, particularly due to issues of data contamination and the leakage of correct answers. To address these challenges, we introduce \emph{ThinkBench}, a novel evaluation framework designed to evaluate LLMs' reasoning capability robustly. 
ThinkBench proposes a dynamic data generation method for constructing out-of-distribution (OOD) datasets and offers an OOD dataset that contains 2,912 samples drawn from reasoning tasks.
ThinkBench unifies the evaluation of reasoning models and non-reasoning models. 
We evaluate 16 LLMs and 4 PRMs under identical experimental conditions and show that most of the LLMs' performance are far from robust and they face a certain level of data leakage. By dynamically generating OOD datasets, ThinkBench effectively provides a reliable evaluation of LLMs and reduces the impact of data contamination. 


\end{abstract}


\section{Introduction}

Large Language Models (LLMs) have made significant advancements across a range of application domains, including language understanding~\citep{karanikolas2023large,xu2024large}, language generation~\citep{wu2024autogen, naveed2023comprehensive}, and complex reasoning~\citep{hao2023reasoning, lu23mathvista, azerbayevllemma, wang2024openr}. Reasoning models, such as o1~\citep{o1}, have further extended the capabilities by leveraging the Process Reward Model (PRM) and advanced search strategies during decoding. Notably, models, such as OpenAI o1, o3~\citep{o1,o3} and Deepseek-R1~\citep{guo2025deepseek}, have shown performance that rivals that of a Ph.D.-educated individual, outperforming GPT-4 in complex reasoning tasks, thus revealing substantial potential for future developments in LLMs.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Graph/graph.pdf}
    \caption{Example of ThinkBench datasets containing Scenario-level and Attack-level semi-fact data.
    %Within the limited computation budget, the selected process reward model used for test-time compute model usually should consider the trade-off between the efficiency and the accuracy performance.
    }
    \label{fig:data example}
\end{figure}

An interesting finding from the OpenAI report~\citep{o1} is that the performance of the o1-series models on the Advanced International Mathematics Exam (AIME) significantly declined in 2024,
% \footnote{The last data collection date of OpenAI o1 model was before the publication date of AIME 2024 exam}
compared to previous years (1983-2023: 0.74 vs. 2024: 0.50). Historical statistics, however, indicate that the median scores of human participants on the AIME exam in previous years are consistent with 2024 exam (4.81 vs. 5.0)\footnote{Sourced from https://artofproblemsolving.com.}. Since o1 was trained on data by 2023, before the AIME 2024 Exam, this discrepancy suggests a possible data contamination issue, raising an important question about how to assess the generalization abilities of LLMs, rather than memorization.

\begin{figure*}[t]
\centering
\subfloat[OOD performance vs. ID performance for several reasoning models on AIME-500.]
{ \label{fig:aime500_line} 
\includegraphics[width=0.45\textwidth]{Graph/intro.pdf} 
}
\qquad
\subfloat[OOD performance vs. ID performance for several reasoning models on AIME 2024.] 
{ \label{fig:aime2024_line} 
\includegraphics[width=0.45\textwidth]{Graph/aime2024_line.png} 
} 
\caption{ \textbf{Math Reasoning Gap}: Most models demonstrate a visible performance gap between their math reasoning performance on ID and OOD, including open-source models and commercial models. 
%Out-of-distribution Gap. We report the performance gap between ID and OOD test on MMLU and GPQA Diamond, respectively.
}
\label{fig:comparison}
\end{figure*}

To address this problem, we propose a novel robust evaluation framework, namely ThinkBench. Building on causal theory and semi-factual causality~\citep{delaney2021uncertainty,kenny2021generating}, we introduce Out-Of-Distribution (OOD) data generation designed to test reasoning capabilities. As shown in Figure~\ref{fig:data example}, we introduce scenario-level and attack-level semi-fact data generation methods, differing in the specific elements of text they alter, enabling the creation of evaluation datasets that are both robust and challenging. By decoupling reasoning from memorization, dynamic evaluation allows us to more effectively test how well LLMs can generalize to unseen reasoning scenarios. 

We take AIME-500 (500 AIME questions from 1983 to 2023) and AIME 2024 (30 AIME questions in 2024) for math reasoning tasks, and GPQA Diamond for scientific questions, dynamically generating an
 OOD dataset of 2,912 samples, provides a diverse set of challenges that test both the generalization and reasoning capabilities of LLMs. % Need to change the observation.
% Based on dynamic evaluation datasets, our experiments reveal that both o1-preview and o1 show significant improvements in performance on these OOD datasets, particularly in math reasoning tasks, 
As illustrated in Figure~\ref{fig:comparison}, compared to the original datasets, our OOD evaluation set proves to be more difficult, resulting in an average performance decay of 24.9\% and 11.8\% across all models on AIME-500, and AIME 2024, respectively.
This indicates that there was indeed some data leakage in AIME questions before 2024, highlighting the importance of mitigating data contamination for reasoning evaluations. The difference in performance decay between AIME-500 and AIME 2024 demonstrates that our dynamically constructed OOD data construction is a convenient and effective method to reduce the impact of data contamination.


% 这说明AIME的题目在2024年之前确实存在一定的数据泄露问题，减轻数据污染对于reasoning评测而言非常重要，The difference of the decay in  AIME-500, AIME 2024说明了我们动态构建的OOD数据是一个方便、快捷且有效的方法，来减轻数据污染的影响。
% demonstrating its ability to handle unseen data and additional perturbations. This reinforces the model's stability and effectiveness in addressing robustness challenges.
Figure~\ref{fig:comparison} also shows that o1~\cite{o1}, o3~\cite{o3}, Deepseek-R1~\cite{guo2025deepseek} and s1~\cite{muennighoff2025s1} maintain the strongest accuracy among all models.
As representatives of reasoning models~\cite{deepscaler2025,Liu2025Can1L}, o1, o3, and s1 enhance inference performance by increasing computational resources during testing, contrasting with non-reasoning models.
ThinkBench provides a reasoning benchmark to evaluate both reasoning models and non-reasoning models.

In addition to the overall model accuracy, we also explore the impact of various PRMs and their performance under the best-of-\emph{n} search during decoding. Fine-grained evaluations show how different data generation strategies, such as Math-shepherd~\citep{wang2024math}, 
% the tree search techniques of o1-journey~\citep{o1journey}, and OmegaPRM~\citep{snell2024scaling}, 
influence model outcomes. The performance improves with an increased test-time computation budget, further highlighting the discriminative power of our benchmark and the quality of the data.

To our knowledge, we are the first to present a robust dynamic evaluation benchmark for testing reasoning capability in LLMs~\citep{o1journey,huang2024o1,wang2024openr}. In ThinkBench, we provide a convenient and effective OOD data construction method and a high-quality dataset to reduce data contamination impact, effectively evaluating the reasoning ability in both reasoning models and non-reasoning models. Notably, we verify the validation of the test-time scaling law using the dynamic evaluation without data contamination based on ThinkBench.

%offer a comprehensive, fine-grained, dynamic analysis of test time by evaluating the influence of different post-training datasets and PRMs using best-of-\emph{n} search during decoding.

% TO BE ADDED


%(1) We propose a dynamic data generation framework to create a testbed for the robustness evaluation of LLMs, resulting in high-quality robustness evaluation datasets adapted from AIME and GPQA datasets, following the o1's report \citep{o1}. 

%(2) We provide a unified robustness evaluation of 14 LLMs and 4 PRMs, including the recently released o1 model, under identical experimental conditions.

%(3) We offer a comprehensive, fine-grained, dynamic analysis of test time by evaluating the influence of different post-training datasets and PRMs using best-of-n search during decoding.

% As shown in Figure ~\ref{fig:intro},

% 然而，仅仅评价test-time compute models的性能及鲁棒性，是一种对其是否能直接完成任务的评价，粒度较粗。这种评价

% 作为研究test-time compute models的关键的一步，Process reward model发挥着巨大的作用。如今大多数PRM的评价依赖于静态的评测数据集Math、PRM800K。这些based on是否可以直接完成已有的数学题来对PRM进行测评粒度较粗，比如拿AIME 2024数据来测评其性能，却需要等待2025年的AIME竞赛产生新题来进一步测试，这样的静态测评对PRM的发展来说无疑是致命的，但我们的benchmark却为PRM的测评设计了动态生成新数据的方式，且为PRM提供了细粒度的测评方式，评测其Best-of-N Performance。We find that the model's performance improves with an increase in the test-time computation budget, indicating the high quality of our data and the discriminative capability of our benchmark。同时，我们分析了the influence of different post-training datasets in constructing process-reward models and their performance using the best-of-n search during the decoding phase. 对PRMs的细粒度且动态的测评，有助于我们进一步深入分析影响test-time compute models性能的因素，从而构建强性能的open test-time compute models。

%挪去related work更合适
%Process reward models (PRMs) plays a significant role in exploring test-time compute models. Currently, most evaluations of PRMs rely on static datasets such as Math~\citep{hendrycks2021measuring} and PRM800K~\citep{lightman2023let}. For example, PRMs use AIME 2024 for performance evaluation but need to wait for the 2025 American Invitational Mathematics Examination to get new problems to continue testing. Such static evaluations are undoubtedly detrimental to the development of PRMs. However, our benchmark introduces a method that can dynamically generate new data for PRM evaluation and provides a fine-grained assessment approach by measuring their Best-of-N Performance.
% For instance, assessing performance using AIME 2024 dataset requires waiting for new problems from the 2025 AIME competition for further testing. 

% Our benchmark not only evaluates the robustness of test-time compute models and train-time compute models, but also provides a high discriminative evaluation for Process Reward Models (PRMs).
% Since o1-preview is not open-sourced at the time of writing, we develop several specific test-time compute models to facilitate our analysis and experimentation. In particular, we construct several test-time compute models based on Qwen-2.5 post-training on different process-supervision datasets, allowing for a deeper analysis of change in performance during the decoding phase. We find that the model's performance improves with an increase in the test-time computation budget, indicating the high quality of our data and the discriminative capability of our benchmark.
% Test-time compute models represented by o1-preview not only demonstrate outstanding performance but also exhibit remarkable robustness against out-of-distribution (OOD) and adversarial (ADV) examples. According to the report on o1-preview~\citep{o1}, a notable distinction between o1-preview and GPT-4o lies in their respective roles as test-time compute models and train-time compute models. We need to conduct further analysis to understand how and why test-time compute models can yield superior performance and robustness.
% Since o1-preview is not open-sourced, we develop several specific test-time compute models to facilitate our analysis and experimentation. In particular, we construct several test-time compute models based on Qwen-2.5 post-training on different process-supervision datasets, allowing for a deeper analysis of change in performance during the decoding phase. 

% Through the experimental results, we observe that XX.
% 以o1为代表的test-time compute models不仅在性能上表现优异，OOD与ADV鲁棒性方面同样让人惊艳。根据o1的报告中我们可以得知，其与4o有着相当大的区别在于分别是test-time compute model和train-time compute models，我们希望进一步对test-time compute model是怎么以及为什么可以导致较好的性能及鲁棒性进行分析。但o1是未被开源的，为了便于分析及实验，我们自己搭建了几种（稍微具体）的test-time compute models。通过分析实验，我们发现XX
% 在我们自己自己搭建的test-time compute models的分析中（待填充


%Our benchmark allows for dynamic, comprehensive, and fair analysis of model performance and robustness, particularly for test-time compute models represented by o1. This benchmark addresses the issue of unfair comparisons caused by data leakage.

%(3) We provide a dynamic high-quality evaluation benchmark for Process Reward Models, which offers high discriminative.
% （1）我们构建了一个可以动态生成数据的benchmark，并构建了AIME 500，GPQA Diamond Robustness和MMLU 570数据集。
% （2）我们的benchmark可以动态、全面、公平地分析模型的性能和其鲁棒性。尤其面向以o1为代表的test-time compute models。我们的benchamrk可以缓解数据泄露带来的不公平比较问题。
% （3）我们为Process Reward Models提供了动态且具备区分度的高质量评测benchmark。


% (1) We introduce a novel robustness testing framework specifically for test-time compute models, providing a detailed analysis and comparison of closed-source test-time compute models, represented by o1, against other mainstream train-time compute models in adversarial (ADV) and out-of-distribution (OOD) scenarios.

% (2) We construct our own open-source test-time compute models and try to analyze how and why test-time compute models can yield superior performance and robustness. This effort provides a foundational approach for developing robust, generalizable models that can be effectively applied across various domains in the future.


% Based on the original data, ThinkBench 动态地生成Out-of-distribution数据(a)和adversarial数据(b)用于模型的鲁棒性测评(c),ThinkBench同时可以作为Process Reward Models Evaluation。

% 我们在这篇工作中的贡献如下：
%（1）首次提出了针对test-time compute models进行的鲁棒性测试，并细致分析、对比了了以o1为代表的闭源test-time compute models和其他主流的train-time compute models在ADv和OOD场景下的鲁棒性。

% (2) 自己构建了开源的test-time compute models并进行鲁棒性测试，这为未来提出稳健、泛化性强、易于用在各领域中的强大模型提供思路。

%(6) 贡献 进一步地，我们分析了两种不同的reward model范式的结果，趋势情况，以及和鲁棒性的联系，这有助于我们后续XX

\section{Related Work}

% 随着大模型的迅速发展，其在方方面面的应用展现了其卓越的能力以及巨大的潜力。如何准确、公平、全面地评价大模型成为了重要的挑战。现有的主流评估方式主要包括：（1）LLMs-as-a-judge. Benchmarks like AlpacaEval, PandaLM, MT-Bench and  C-Eval leverage large language models to evaluate a set of predefined questions. This method is not only efficient and affordable.（2）Humans-as-a-judge. Human evaluation involves assessing the quality and accuracy of model outputs through human involvement, offering more comprehensive and precise feedback~\citep{chang2024survey,ribeiro2022adaptive,gao2023adaptive}. In evaluating large language models, experts, researchers, or users are invited to review model outputs.（3）Other benchmarks. Several traditional benchmarks employ a series of standardized tests and static datasets to quantitatively assess the performance of models across various tasks. For instance, HELM and MMLU are primarily utilized to evaluate general knowledge and reasoning capabilities and MATH focuses on assessing mathematical skills. ToolBench concentrates on evaluating the ability to use tools. The performance of LLMs is measured by their ability to accurately complete these tasks.
% 


\paragraph{Evaluating Large Language Models.}

% With the rapid development of large models, their applications across various domains demonstrate exceptional capabilities and immense potential~\citep{chang2024survey}. Accurately, fairly, and comprehensively evaluating large models becomes a significant challenge. The current mainstream evaluation methods primarily include: \textbf{(1) LLMs-as-a-judge}: Benchmarks such as AlpacaEval~\citep{li2023alpacaeval}, PandaLM~\citep{wang2023pandalm}, MT-Bench~\citep{zheng2023judging}, and C-Eval~\citep{huang2024c} utilize large language models to evaluate a set of predefined questions. This approach is not only efficient but also cost-effective.
% \textbf{(2) Humans-as-a-judge}: Human evaluation involves assessing the quality and accuracy of model outputs through human involvement, providing more comprehensive and precise feedback \citep{ribeiro2022adaptive,gao2023adaptive}. Experts, researchers, or users are invited to review model outputs when evaluating large language models.
% \textbf{(3) Other benchmarks}: Several traditional benchmarks employ a series of standardized tests and static datasets to quantitatively assess the performance of models across various tasks. For example, HELM~\citep{liang2022holistic} and MMLU~\citep{hendrycks2020measuring} are primarily used to evaluate general knowledge and reasoning capabilities, while MATH~\citep{hendrycks2021measuring} focuses on assessing mathematical skills. ToolBench~\citep{xu2023tool} concentrates on evaluating the ability to use tools. The performance of large language models is measured by their ability to complete these tasks accurately.

% Given the widespread use and cross-industry applications of LLMs~\citep{el2021automatic, hao2022recent, cheng2023ml, gao2024fact}, a dynamic robustness evaluation of their reasoning performance is essential~\citep{glazer2024frontiermath}. While numerous benchmarks have been developed to assess LLMs' capabilities~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}, these largely focus on train-time compute models, with limited attention given to the emerging test-time compute models like o1. Moreover, test-time models often require substantial computational resources, presenting unique challenges for establishing a unified robustness evaluation benchmark. Current evaluations of o1-preview have mainly focused on specific tasks, such as planning, with few comprehensive analyses of robustness~\citep{gui2024logicgame, wang2024planning, zhong2024evaluation}.

% With rapid improvement of LLMs, their applications across domains show exceptional capabilities~\citep{chang2024survey}. 
Evaluating LLMs accurately and fairly poses a significant challenge~\citep{chang2024survey}. Mainstream evaluation methods include: \textbf{(1) LLMs-as-a-judge}: Benchmarks like AlpacaEval~\citep{li2023alpacaeval}, PandaLM~\citep{wang2023pandalm}, MT-Bench~\citep{zheng2023judging}, and C-Eval~\citep{huang2024c} use large language models for predefined questions.
% , offering efficiency and cost-effectiveness. 
\textbf{(2) Humans-as-a-judge}: Human evaluation provides comprehensive feedback through expert reviews~\citep{ribeiro2022adaptive,gao2023adaptive}. \textbf{(3) Other benchmarks}: Several traditional benchmarks employ static datasets to assess models across various tasks~\citep{liang2022holistic,hendrycks2020measuring,hendrycks2021measuring}. 
% Traditional benchmarks like HELM~\citep{liang2022holistic}, MMLU~\citep{hendrycks2020measuring}, and MATH~\citep{hendrycks2021measuring} assess various skills, while ToolBench~\citep{xu2023tool} evaluates tool usage. 
Our work falls into the third category. However, rather than using static data, we generate test sets dynamically.

% Given LLMs' widespread use~\citep{el2021automatic, hao2022recent, cheng2023ml, gao2024fact}, dynamic evaluation is crucial~\citep{glazer2024frontiermath}. Although many benchmarks exist~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}, they often focus on train-time compute models, with limited attention to test-time models like o1. These require significant computational resources, posing challenges for unified robustness evaluation. Current evaluations of o1-preview focus on specific tasks, such as planning, with limited comprehensive robustness analyses~\citep{gui2024logicgame, wang2024planning, zhong2024evaluation}.

\paragraph{Robustness of Large Language Models.}
%In order to apply LLMs across various scenarios, improving LLMs' robustness is critical. Previous studies, such as OOD-GLUE~\citep{yuan2023revisiting}, GLUE-X~\citep{yang2023glue}, and ZebraLogic~\citep{zebralogic2024}, focus on the robustness of models and propose corresponding evaluation benchmarks. These works primarily involve robustness evaluations of train-time compute models. \citet{wang2023robustness} present a pilot study on assessing the robustness of ChatGPT, following with~\citep{li2024gsm} which pays particular attention on evaluating the robustness of LLMs as mathematical problem solvers. Additionally, \citet{yang2022factmix} focuses on generating out-of-distribution data by employing counterfactual and semi-factual data construction methods. Recently, \citet{hosseini2024not} reveal a significant reasoning gap in most LLMs by evaluating LLMs' performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. The performance gap between solving compositional pairs and solving each question independently is more pronounced in smaller, more cost-efficient, and math-specialized models.
% Different from previous benchmarks that focused on the robustness of train-time compute models, our work is the first to analyze the robustness of both train-time and test-time compute models collectively.



Evaluating the robustness of LLMs is crucial~\cite{muennighoff2025s1,guo2025deepseek} for their applications across diverse scenarios~\cite{wang2023robustness,glazer2024frontiermath,li2024gsm}. Previous studies~\cite{li2024openai}, such as OOD-GLUE~\citep{yuan2023revisiting}, GLUE-X~\citep{yang2023glue}, and ZebraLogic~\citep{zebralogic2024}, focus on robustness of non-reasoning models. 
% \citet{wang2023robustness} assess ChatGPT's robustness, 
% while \citet{li2024gsm} evaluate LLMs as problem solvers. 
Additionally, \citet{yang2022factmix} focus on generating OOD data by employing semi-fact data augmentation methods. Recently, \citet{hosseini2024not} identify reasoning gaps in LLMs by evaluating math problem pairs, revealing performance disparities in smaller, math-specialized models. \citet{wu2024mrke} introduce cofQA, which targets text-based inference tasks using counterfactual data perturbations. 
Our work is similar in assessing general robustness but differs from the literature in focusing on reasoning tasks, for which OOD tests are more necessary as compared to general tasks.
% However, for mathematical and coding reasoning tasks, creating counterfactuals makes it challenging to determine the correct answer, often necessitating expert annotation. In contrast, constructing semi-factual data does not require additional human annotation. Therefore, to construct OOD data suitable for reasoning tasks and to alleviate the issue of data leakage in evaluations, we try to dynamically construct semi-factual data.

% \citet{wu2024mrke}提出的cofQA是专注文本的推理任务，利用的是反事实数据干扰。但是，对于数学等推理任务来说，构建反事实的话golden answer难以确认，通常需要专家标注，构建半事实数据则无需额外的人工标注。

%Unlike previous benchmarks that focus on the robustness of train-time compute models, our work is the first to collectively analyze the robustness of both train-time and test-time compute models.

% Given the widespread use of LLMs~\citep{el2021automatic, hao2022recent, cheng2023ml, gao2024fact}, evaluation becomes more and more essential~\citep{glazer2024frontiermath}. 
% Although numerous benchmarks exist~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}, they 
In addition, existing benchmarks predominantly emphasize non-reasoning models~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}.
% They cannot be used for assessing test-time computing models because t
% They often neglect the evaluation under test-time scaling.
% neglecting test-time models like o1, which demand substantial computational resources for unified robustness evaluation. Test-time computing models, such as o1-preview, have demonstrated impressive performance, necessitating thorough evaluation to better understand them. 
For reasoning models, current evaluations often target specific tasks, such as planning~\citep{wang2024planning} and rule execution~\citep{gui2024logicgame}, with limited comprehensive robustness analyses~\citep{zhong2024evaluation}. Unlike these work, our benchmark focuses on robustness and reasoning with practical applications, offering statistically significant insights.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Graph/o1-graph.pdf}
    \caption{Overview of ThinkBench framework. Based on the original data, ThinkBench dynamically generates scenario-level Semi-fact Data (a) and Attack-level Semi-fact Data (b), which can be used to evaluate the robustness of reasoning models and non-reasoning models. ThinkBench can also serve as a useful tool for Test-time Scaling Evaluation(c). 
    %Within the limited computation budget, the selected process reward model used for test-time compute model usually should consider the trade-off between the efficiency and the accuracy performance.
    }
    \label{fig:framework}
\end{figure*}
% Given LLMs' widespread use~\citep{el2021automatic, hao2022recent, cheng2023ml, gao2024fact}, dynamic evaluation is crucial~\citep{glazer2024frontiermath}. Although many benchmarks exist~\citep{li2023alpacaeval, hendrycks2020measuring, huang2024lateval, li2024llms}, they often focus on train-time compute models, with limited attention to test-time models like o1. These require significant computational resources, posing challenges for unified robustness evaluation. Current evaluations of o1-preview focus on specific tasks, such as planning, with limited comprehensive robustness analyses~\citep{gui2024logicgame, wang2024planning, zhong2024evaluation}.

% Recently, test-time compute models like o1-preview have shown impressive performance. Evaluating these models is crucial for better understanding them. Efforts include LOGICGAME~\citep{gui2024logicgame}, assessing rule understanding and execution, ~\citet{wang2024planning} evaluating o1-preview's planning capabilities, and ~\cite{zhong2024evaluation} conducting case studies across domains. Unlike these works, our benchmark focuses on robustness and reasoning with practical applications, offering statistically significant insights.

% 此外，会有一些工作通过反事实、半事实数据构造的方式来生成OOD数据，从而检测模型的robustness。

% Recently, test-time compute models, such as o1-preview, have been introduced and demonstrate impressive performance. Evaluating these test-time compute models is important, as it helps in better analyzing and understanding them. Recent evaluation efforts for test-time compute models include LOGICGAME~\citep{gui2024logicgame}, which assesses the model's abilities in rule understanding, execution, and planning. ~\citet{wang2024planning} evaluate the planning capabilities of o1-preview across a series of planning benchmarks and ~\cite{zhong2024evaluation} conduct some case studies of o1-preview across different domains. Different from recent analytical works on o1, which emphasize planning evaluations or case-by-case analyses, our proposed benchmark is statistically significant and places greater emphasis on robustness and reasoning that have high practical application value.



% 半事实 反事实

 

%The exploration and optimization of PRM show its superior performance in developing high-performance models. However, suitable benchmarks for PRM and test-compute models are lacking. We propose a benchmark for test-time computing models to evaluate PRM's capabilities, unlocking its potential.


% In the past, the training of language models primarily relied on Outcome-based Reinforcement Models (ORM)~\citep{wang2024openr}. A model proposed by ~\citet{cobbe2021gsm8k} serves as a foundational example of ORM-based models, where the focus is on training evaluators to assess the final correctness of generated answers. This approach provides feedback for model training and plays a significant role. However, ORM has certain limitations, as it concentrates solely on the final outcome, neglecting the various steps involved in the reasoning process.




% To address ORM's oversight of the reasoning process, a few works on Process Reward Model (PRM) have emerged in recent years, offering more granular supervision. For instance, DeepMind~\citep{uesato2022solving} emphasizes supervising both intermediate reasoning steps and the final result, thereby providing more detailed feedback. OpenAI~\citep{lightman2023let} further advances PRM by introducing a high-quality human-annotated process supervision dataset, PRM800K, highlighting the importance of verifying each intermediate step.

% \citet{li2022making} demonstrate the combination of evaluator models with majority voting schemes to enhance the reliability of results. ~\citet{yu2024ovm} improve the reasoning process in large language models through reinforcement learning, integrating both outcome and process supervision. The recently proposed Generative Reward Model (GenRM)~\citep{zhang2024generative} gets tons of attention, allowing evaluators to interact with generators in an information-rich manner. This shift reflects a widespread demand for more sophisticated process supervision methods. The recent work~\citep{zheng2024processbench} contributes a benchmark designed to evaluate language models' ability to identify errors in step-by-step mathematical reasoning, focusing on challenging competition-level problems, using a diverse set of annotated solutions to foster research in scalable oversight for reasoning assessment.

% An increasing number of researchers turn to the exploration and optimization of PRM, whose superior performance marks a significant step towards the development of high-performance test-compute models. However, there is still a lack of suitable benchmarks for PRM and test-compute models. We propose a benchmark tailored for test-compute models to measure their performance and robustness, enabling a detailed and comprehensive evaluation of PRM's capabilities, thereby further exploring and unlocking the vast potential of PRM.

% \usepackage{booktabs}


\begin{table}
\centering
% \small
\caption{Statistics of reconstructed reasoning datasets based on three original test datasets, including AIME-500, AIME 2024, and GPQA Diamond.}
\label{tab:data_statistics}
 \begin{adjustbox}{width=0.99\columnwidth}{
\begin{tabular}{lccc} 
\toprule
                             & AIME-500 & AIME 2024 & GPQA Diamond \\ 
\midrule
% \# Class                     & -        & -         & 3                        \\
% \# Choices   & -        & -         & 4                        \\ 
% \midrule
\# Samples of original  & 500      & 30        & 198                      \\
Questions' Avg Len       & 51.1     & 60.1      & 67.7                     \\
Choices' Avg Len         & -        & -         & 27.8                     \\ 
\hline
\# Samples of OOD & 2,000      & 120        & 792                      \\
Questions' Avg Len       & 61.2     & 70.1      & 85.2                     \\
Choices' Avg Len         & -        & -         & 25.7                     \\
\bottomrule
\end{tabular}}    \end{adjustbox}
\end{table}



\section{Dynamic Evaluation Benchmark}
% 我们通过对Original Test Set进行改写，
%对于数学和代码这类推理任务来说，构建反事实的话golden answer不好确认，需要专家标注，不像常识可以通过维基百科趣儿，而半事实不需要答案，所以ThinkBench采用了半事实数据构建。
For math reasoning tasks, constructing counterfactual data presents challenges in changing a golden answer, which contrasts with commonsense tasks, where reliable sources like Wikipedia can be utilized. In contrast, semi-fact data does not need to change a specific answer. As shown in Figure~\ref{fig:framework}, ThinkBench contains two dynamic semi-fact data generation methods, aiming to assess the real reasoning ability of LLMs: (a) Scenario-level Semi-fact Data~\citep{yang2023glue, zhu2023dyval, zhu2024dyval, opedal2024mathgap}, which changes the scenario for the original reasoning data; (b) Attack-level Semi-fact Data~\citep{zhu2023promptbench}, which uses three attack methods to rephrase the original data. 

We use generated OOD data to perform dynamic reasoning evaluation on both reasoning models and non-reasoning models. Maintaining core knowledge while altering scenarios or expressions, we evaluate whether models can consistently apply learned knowledge across contextual variations. Finally, we can leverage our OOD data to conduct (c) test-time evaluation based on PRMs.

%在动态构造完OOD数据后，我们可以使用OOD数据来对Train-time compute models和Test-time compute models做Dynamic Reasoning Evaluation。进一步地，我们可以utilize我们地OOD数据做对Process-reward model做Test-time Scaling Evaluation。


% We conduct statistical analysis on three reasoning datasets. 
% For each dataset, we provide corresponding information regarding the quantity and length of the original test, Scenario-level Semi-fact test data, and Attack-level Semi-fact test data. The data statistics of these datasets are shown in Table~\ref{tab:data_statistics}. Attack-level test for each dataset is constructed using TextBugger, CheckList, and StressTest, respectively, resulting in a sample that is three times the size of the original test.

\subsection{OOD Data Generation} 

% Inspired by Dyval2~\citep{zhu2024dyval},
% 为了解耦性地测出模型的reasoning能力水平，而不是与知识混杂的通用水平，我们对数据改写的要求是高且严格的。我们设计了4种构造半事实数据的方式，都是可以保证题目的方向、核心考察不变，即保证题目所考察的知识不变，改变的部分是题目设置的场景、表述、一些表达方面的扰动等。通过这样半事实数据的构造，我们可以将能力测试从知识与能力混杂的测试种解耦出来。理想的情况下，一个鲁棒、真正学会了运用知识进行reasoning的模型，在对题目考察知识不变的情况下，表述、场景的改变并不应该带来显著的表现下降。


% We construct the dynamic OOD Semi-fact data to ensure that the assessment focuses on the model's inherent ability rather than a mixture of ability -- corresponding to whether LLMs can solve the question in the same core knowledge -- and knowledge -- corresponding to the core of the question.
% This means that the knowledge being tested by the questions remains constant, while changes are made to the scenario, phrasing, and certain expressions of the questions. By constructing semi-factual data in this manner, we can decouple the ability assessment from tests that conflate knowledge and reasoning skills. Ideally, a robust model that has genuinely learned to apply knowledge for reasoning should not exhibit significant performance degradation when the expression and scenario of a question change, provided that the core knowledge being assessed remains the same.


% We propose dynamic OOD Semi-fact data construction to isolate model reasoning ability assessment (whether LLMs can solve the question within the same core knowledge) from knowledge dependency (the core of the question). 


\textbf{Scenario-level Semi-fact Data Generation.}  
The process primarily involves two types of agents: 
the Rephrasing Agent, which is responsible for transforming the original questions and generating new ones, while the Verifier Agent, which is responsible for confirming at each step whether the rephrasing meets the current requirements and whether the rephrasing is valid.

There are two Rephrasing Agents. The first generates suitable scenarios for reasoning problems, ensuring that the new scenarios are appropriately transferable concerning the core of the original problem. The second rewrites each part (often at the sentence level) to fit the new scenario while preserving the original meaning. This step-by-step process uses each newly generated part as a reference for subsequent parts.

There are three Verifier Agents. The first checks if the core content of the original problem can be effectively transferred to the new scenario. The second evaluates each newly generated part, ensuring it meets three criteria: it conveys the same core meaning as the original, is consistent with previously revised parts, and is correctly adapted to the new scenario. Additionally, an overall Verifier Agent assesses the overall new problem, ensuring it maintains essential consistency with the original questions, preserving informational content, and is appropriately constructed within the new scenario.

At every step, outputs from Rephrasing Agents need to pass the corresponding Verifier Agent’s checks. If invalid, the process reverts to regeneration. The final new problem also needs to be approved by the Verifier Agent or be regenerated.

For the datasets that contain Choices, similarly, Rephrase Agent modifies the expressions of certain options and randomly rearranges their order. Rephrasing of choices must also pass the Verifier Agent's check to ensure the data is valid.
% Throughout the generation and verification process, validation of outputs is crucial. Each piece generated by a Rephrasing Agent is validated by the corresponding Verifier Agent. If deemed invalid, the process reverts to the previous step for regeneration. The final output is assessed by the overall Verifier Agent, and if invalid, the problem is regenerated. This ensures that the new questions maintain the core content and intent of the original items within the new scenario.

% \textbf{Scenario-level Semi-fact 
%   Data Generation.} To evaluate a model's reasoning capabilities independently of its general knowledge proficiency, we impose high and stringent requirements on data rewriting. In particular, we rephrase the original test components of each dataset to dynamically generate new evaluation samples. 
% This dynamic data generation approach primarily involves two kinds of agents: the Rephrasing Agent is responsible for transforming the original questions and generating new ones, while the Verifier Agent is responsible for confirming at each step whether the rephrasing meets the current requirements and whether the rephrasing is valid.

% The overall objective of Rephrasing Agents is to receive each original test entry and rewrite it according to specified principles. There are two types of Rephrasing Agents involved in this process. The first type is responsible for generating suitable scenarios for reasoning problems, ensuring that these newly created scenarios are appropriately transferable concerning the core of the original problem. The second type of Rephrasing Agent focuses on each part of the original problem, typically treating a sentence as a part. It generates content that aligns with the newly created scenario while maintaining consistency with the core meaning of the corresponding part of the original problem. This process is executed step by step, with each newly generated part serving as a reference in the subsequent steps. Through the collaboration of these two types of Rephrasing Agents, a suitable new scenario is first generated, followed by the step-by-step rewriting of the original problem according to this new scenario.

% Correspondingly, there are two types of Verifier Agents. For the newly generated scenario, the first type of Verifier Agent needs to determine whether the core content of the original problem can be effectively transferred to the new scenario. For each newly generated part in every step, another Verifier Agent must assess whether the newly generated part satisfies three criteria: 1) whether the original and revised parts effectively convey the same core meaning, 2) whether the revised part is consistent with the previously revised parts, and 3) whether the revised part has been correctly adapted to the new scenario. In addition to these two Verifier Agents that correspond to the two Rephrasing Agents, there is another Verifier Agent tasked with evaluating the overall new problem generated after corresponding parts are created for the new scenario. This Verifier Agent assesses whether the new questions maintain essential consistency with the original questions, specifically whether they accurately preserve the intent and informational content of the original items, and whether the newly generated questions are constructed appropriately within the new scenario.

% Throughout the generation and verification process involving the Rephrasing Agents and Verifier Agents, it is crucial to validate the outputs. Whether it is the Rephrasing Agent for the new scenario or the Rephrasing Agent for the new parts, the generated content is validated by the corresponding Verifier Agent. If deemed invalid, the process needs to revert to the previous step for regeneration. The final output of the new problem is assessed by the overall Verifier Agent, and if found to be invalid, the problem needs to be regenerated.
%while the Verifier Agent evaluates the legality of such rephrasing and generations.

% The Rephrasing Agent receives each original test entry and rewrites it according to specified principles. The transformation process may include altering the phrasing of the question, rearranging the order of answer options, or adding supplementary contextual information. It is crucial that the core essence of the question is preserved during this transformation and the new question must logically align with the original question. 

% Rephrasing Agents的整体目标是receives each original test entry and rewrites it according to specified principles. 这里存在着两种Rephrasing Agent，第一种Rephrasing Agent负责的是针对reasoning问题生成出合适的场景，这里新生成的场景需要保证对于原问题的核心而言迁移过去是合适的。第二种Rephrasing Agent则需要针对原问题的每一个part，我们通常采用句子作为一个part，根据之前新生成的场景生成出既符合新场景、又与原问题种对应的part核心含义一致的内容。这个过程是一个step by step过程，上一步中新生成的part同样作为这一步中生成过程中的参考内容。通过这两种Rephrasing Agent相互配合，即先生成合适的新场景，再根据新的场景step by step将原问题进行了改写。

% 与上述的Rephrasing Agents相对应的存在着两种Verifier Agents，针对新生成的场景，第一种Verifier Agent需要判断原问题的核心内容是否可以有效迁移至新场景中。而对于每一步对应生成的新parts，Verifier Agent需要判断在每一个step中生成的新part是否满足三个要求：1. If the original and revised parts effectively convey the same core meaning. 2. If the revised part is consistent with the previous revised parts. 3. If the revised part has been correctly adapted to the new scenario. 除了与Rephrasing Agents对应的两种Verifier Agents，在根据原来的parts对应生成新场景的parts后，有一个对整体的新问题进行判断的Verifier Agents。This verifier agent is tasked with evaluating whether the new questions generated by Rephrasing Agent maintain essential consistency with the original questions, specifically whether they accurately preserve the intent and informational content of the original items.且这个verifier agent需要判断新生成的问题是否在新生成的场景下构建的。

% 在上述Rephrasing Agents与Rephrasing Agents的生成与Verification的过程中，无论是为了新生成场景的Rephrasing Agent，还是为了生成新part的Rephrasing Agent，在生成后需要根据对应的Verifier Agent判断是否valid，如果不valid，需要回退到上一个step重新生成，最后得到的新问题的结果由总体的verifier agent进行判断，如果不合法，则重新生成。

\textbf{Attack-level Semi-fact Data Generation.} 
% To evaluate the reasoning capabilities of LLMs, we also consider the introduction of Attack perturbations to create semi-fact data.
% Previous methods like DeepWordBug are often impractical, as they create sentences that are unrecognizable and nonsensical. Instead,
We focus on realistic errors using three methods: 

\begin{enumerate}
    \item TextBugger (character-level)~\citep{li2019textbugger}: This method simulates user input errors by introducing mistakes or typos within words.
    \item CheckList (sentence-level)~\citep{ribeiro2020beyond}: This approach assesses model robustness by adding irrelevant or redundant sentences to the original text.
    \item StressTest (sentence-level)~\citep{naik2018stress}: Similar to CheckList, StressTest evaluates model robustness by incorporating unrelated or redundant sentences.
\end{enumerate}

% \textbf{(1) TextBugger (character-level)}~\citep{li2019textbugger}: This method simulates user input errors by introducing mistakes or typos within words. \textbf{(2) CheckList (sentence-level)}~\citep{ribeiro2020beyond}: This approach assesses model robustness by adding irrelevant or redundant sentences to the original text. \textbf{(3) StressTest (sentence-level)}~\citep{naik2018stress}: Similar to CheckList, StressTest evaluates model robustness by incorporating unrelated or redundant sentences.

These methods reflect common errors such as typos and extraneous information. They serve as three Attack Agents. We apply only a single iteration of perturbation, avoiding unrealistic error densities. Attack-level Semi-fact data construction involves these three Attack Agents and a Verifier Agent. The Verifier Agent is used to check if it is consistent with the core aspects of the original data and ensures that any errors introduced by the perturbation do not impede overall comprehension.
% if the perturbed data remains coherent and consistent with the original, validating it as Attack-level Semi-fact Data.

% \textbf{Attack-level Semi-fact Data Generation.}
% To evaluate the reasoning capabilities of Large Language Models, in addition to constructing semi-fact data by altering scenarios, we also consider the introduction of adversarial perturbations that often occur in real-world applications to create semi-fact data.

% For the previous methods to construct adversarial perturbations, we find certain approaches are impractical and unreasonable to some extent. For instance, DeepWordBug~\citep{gao2018black} constructs sentences that are unrecognizable from the original through repeated iterative modifications. Such forced rewrites result in sentences that are entirely disconnected from their original samples and may even become nonsensical or unreadable to humans. Furthermore, such constructed error samples are unlikely to occur in real-world applications. To better reflect the types of errors that humans are likely to make in practical scenarios, we primarily focus on three types of error scenarios and employ the following three construction methods to generate the corresponding adversarial perturbation:

% \noindent\textbf{(1) TextBugger (character-level)}~\citep{li2019textbugger}: This method simulates user input errors by introducing mistakes or typos within words.

% \noindent\textbf{(2) CheckList (sentence-level)}~\citep{ribeiro2020beyond}: This approach assesses model robustness by adding irrelevant or redundant sentences to the original text.

% \noindent\textbf{(3) StressTest (sentence-level)}~\citep{naik2018stress}: Similar to CheckList, StressTest evaluates model robustness by incorporating unrelated or redundant sentences.

% These perturbations correspond to common real-world errors, such as typographical mistakes and the inclusion of extraneous information due to limited language proficiency. 
% It is noteworthy that for each type of adversarial perturbation method, we employ a single iteration of perturbation rather than multiple iterations. This decision is based on the premise that, in real-world scenarios, it is uncommon for every word in a sentence to exhibit spelling errors, omissions, or superfluous additions simultaneously.
% Once we obtain the perturbed data, we similarly employ the Verifier Agent to verify whether this perturbed sentence is valid. This involves checking if it is consistent with the core aspects of the original data and ensuring that any errors introduced by the perturbation do not impede overall comprehension. Only perturbed data deemed legitimate by the Verifier Agent are considered valid Attack-level Semi-fact Data.
% 得到扰动后的句子后，我们同样使用Verifier Agent去确认这个扰动后的句子是valid，即与原句子是否核心考点一致，且扰动带来的错误不影响整体理解，通过Verifier Agent判断合法的句子才会被作为有效的新合成的Attack-level Semi-fact Data。

% 对于一道原始的reasoning题目，通过构建对应的一条Scenario-level Semi-fact Data和3种Attack-level Semi-fact Data组成了我们的Semi-fact数据。使用这个Semi-fact数据来评测模型的鲁棒性方式如下：
For an original reasoning problem, we construct our OOD test by creating one scenario-level semi-fact data instance and three attack-level semi-fact data instances. The method for evaluating the model's OOD Accuracy for the original data \(i\) is as follows:
\begin{equation}
  \label{eq:acc}
\text{Acc(OOD)} = \frac{1}{2} \left( \min_{j=1}^{3} \text{Acc}(A_{ij}) + \text{Acc}(S_i) \right),
\end{equation}
where \(\text{Acc}(A_{ij})\) denotes the accuracy of performance for the \(j\)-th attack-level semi-data, with \(j = 1, 2, 3\).
\(\text{Acc}(S)\) represents the accuracy of performance for the scenario-level semi-data \(S\).

% To ensure the diversity of the generated new questions, each new version should examine the core concepts of the original question from different perspectives, thereby providing a more comprehensive assessment of the model's capabilities. If the Verifier determines that the newly generated question lacks consistency (i.e., answers ``No''), Rephrasing Agent is prompted to produce another version of the question. Conversely, if the verifier concludes that the new question is consistent with the original, it is deemed to have passed the consistency check.

% The verifier agent is tasked with evaluating whether the new questions generated by Rephrasing Agent maintain essential consistency with the original questions, specifically whether they accurately preserve the intent and informational content of the original items. The assessment of whether Rephrasing Agent has succeeded is conducted through a simple binary response system (``Yes'' or ``No''), indicating whether the newly generated question has passed the evaluation. This response is based on a thorough analysis of both the original and the rephrased questions. The verifier not only identifies superficial similarities or differences in wording but also delves into whether the two questions are conceptually or contextually aligned. 
% Through this methodology, the verifier ensures the quality and validity of the test samples generated by the Rephrasing Agent, making them reliable for evaluating the performance and robustness of LLMs.

% 来自于MMLU original set的一条关于治疗的数据。通过Rephrasing Agent针对问题的改写与增加上下文，由原来的"In what situation are closed pouches applied?"改写为了"under what circumstances would one use sealed pouches?"，并增加了一些上下文，如"In a hospital setting, where maintaining sterility of medical instruments is crucial to prevent infections"，改写后的问题通过了Verifier Agent，确保增加的上下文不会影响问题，修改前后整个的问题核心topic也未改变。 对于Choices，Rephrasing Agent对一些选项的表达进行了改写，并重新随机排列了选项顺序。同样地，对于choices的改写也通过了Verifier Agent的check，保证问题的合法有效。

%这样的改写的问题，保证了问题的有效性的同时，对问题的表达、format进行了改写，从而一定程度上减少了数据泄露带来的影响。





\textbf{Semi-fact Data Construction.} As shown in Figure~\ref{fig:framework}, Scenario-level semi-fact data is constructed as follows: The original query, ``There exist real numbers $x$ and $y$, both greater than 1, such that $\log_x(y^x) = \log_y(x^{4y}) = 10$. Find $xy$.'' is transformed step-by-step within a new scenario ``The concepts of growth rates of two different species of plants'' into ``Consider two species of plants with growth rates represented by real numbers $x$ and $y$, both exceeding 1, such that the growth rate of one species raised to the power of the other results in the equation $\log_x(y^x) = \log_y(x^{4y}) = 10$. Determine the product of their growth rates, $xy$.''The Verifier Agent then checks the rephrased question to ensure that the rephrased question is in a reasonable scenario and the rephrased expression does not alter the core topic of the question. 

For Attack-level semi-fact data, TextBugger injects character-level noise (e.g., replacing ``There'' with ``Ix''), while CheckList and StressTest append syntactically valid but irrelevant suffixes (e.g., ``5XeflW1ZJc'' and ``true is true'') to the problem statement. The Verifier Agent also needs to ensure the validity of this rephrased version.

\begin{table*}[t]
 \centering
\caption{Reasoning performance. We report performance on OOD data of different models on three datasets that we dynamically construct. Each performance value represents the accuracy in solving problems from the dataset. The accuracy of performance on OOD is calculated through Equation~\ref{eq:acc}. The term ``Gap'' denotes the percentage decrease in performance on Semi-fact data tests compared to the original test performance.}
 \begin{adjustbox}{width=.98\textwidth}{
\begin{tabular}{lcccccccccc} 
\toprule
\multirow{2}{*}{}    & \multicolumn{3}{c}{AIME 2024}    & \multicolumn{3}{c}{AIME-500}   & \multicolumn{3}{c}{GPQA Diamond} & \multirow{2}{*}{AVG Gap (\%)}  \\ 
\cmidrule(l){2-10}
                     & Original & OOD & Gap (\%) & Original & OOD & Gap (\%) & Original & OOD & Gap (\%)    &                                \\ 
\midrule

o1-preview           & 0.500    & 0.484     & -3.3     & 0.742    & 0.659     & -11.2    & 0.684    & 0.638 & -6.8          & -7.1                           \\
o1-mini              & 0.567    & 0.600     & 5.8      & 0.864    & 0.753     & -12.8    & 0.592    & 0.564 & -4.7          & -3.9                           \\
o3-mini              & 0.767    & 0.717     & -6.5     & 0.922    & 0.834     & -9.5     & 0.727    & 0.710 & -2.4          & -6.1                           \\
Deepseek-R1          & 0.800    & 0.750     & -6.3     & 0.920    & 0.828     & -10.0    & 0.747    & 0.682 & -8.7          & -8.3                           \\
GPT-4o               & 0.133    & 0.100     & -24.8    & 0.278    & 0.197     & -29.1    & 0.495    & 0.447 & -9.8          & -21.2                          \\
Deepseek-V3          & 0.367    & 0.333     & -9.3     & 0.528    & 0.429     & -18.8    & 0.581    & 0.530 & -8.8          & -12.3                          \\ 

% Yi-Lightning         & 0.000~   & 0.017~    & -        & 0.114~   & 0.116~    & 1.8~     & 0.337~   & 0.408~ & 21.1~        & 7.6~                           \\ 
\midrule

Mixtral-8x7B-IT-v0.1 & 0.000    & 0.000     & -        & 0.012    & 0.006     & -50.0    & 0.168    & 0.163 & -3.0          & -17.7                          \\
Qwen2.5-72B-IT       & 0.200    & 0.184     & -8.3     & 0.432    & 0.341     & -21.1    & 0.536    & 0.487 & -9.1          & -12.8                          \\
Qwen2.5-Math-72B-IT  & 0.267    & 0.217     & -18.9    & 0.536    & 0.430     & -19.8    & 0.449    & 0.416 & -7.5          & -15.4                          \\
LLAMA3.1-70B-IT      & 0.200    & 0.167     & -16.5    & 0.424    & 0.318     & -25.0    & 0.388    & 0.383 & -1.3          & -14.3                          \\ 
\midrule

s1.1-32B             & 0.533    & 0.517     & -3.0     & 0.752    & 0.574     & -23.7    & 0.525    & 0.508 & -3.3          & -10.0                          \\
Gemma-2-27B-IT       & 0.033    & 0.017     & -50.0    & 0.062    & 0.045     & -27.4    & 0.291    & 0.281 & -3.6          & -27.0                          \\
Gemma-2-9B-IT        & 0.000    & 0.000     & -        & 0.032    & 0.021     & -34.4    & 0.214    & 0.189 & -11.9         & -15.4                          \\
LLAMA3.1-8B-IT       & 0.000    & 0.017     & -      & 0.132    & 0.087     & -34.1    & 0.204    & 0.179 & -12.5         & -15.5                          \\
% Qwen2.5-Math-7B      & 0.067    & 0.100     & 49.3     & 0.426    & 0.358     & -16.0    & 0.230    & 0.263 & 14.3          & 15.9                           \\
Phi-3-mini-4k-IT     & 0.000    & 0.000     & -        & 0.046    & 0.026     & -43.5    & 0.224    & 0.192 & -14.5         & -19.3                          \\
LLAMA3.2-3B-IT       & 0.033    & 0.033     & 0.0        & 0.122    & 0.087     & -28.7    & 0.153    & 0.161 & 4.9           & -7.9                           \\
\bottomrule
\end{tabular}
}
    \end{adjustbox}
        \label{tab:main table1}
\end{table*}

\subsection{Data Analysis}
\label{Sec:data}

%\neal{shall we add a table/figure including the examples of OOD and ADV (https://arxiv.org/pdf/2402.19255 table 2). }

% We utilize AIME-500, AIME 2024, and GPQA Diamond, aimed at evaluating the model's reasoning ability in a decoupled manner to construct dynamic OOD semi-fact datasets.
% % 



% \textbf{AIME-500 (extracted from Year of 1983 - 2023) and AIME 2024 (Year of 2024).} The AIME dataset is designed to challenge the most talented high school mathematics students in the United States. The original test component of AIME-500 consists of 500 questions extracted from the original dataset~\citep{zhengminif2f} while AIME 2024 contains 30 examples included in the exam for 2024 representing the newest version of math problems. 

% \textbf{GPQA Diamond.} It refers to the GPQA Diamond dataset in this work, which originates from the GPQA benchmark~\citep{rein2023gpqa}, a rigorous intelligence assessment that evaluates expertise in the fields of chemistry, physics, and biology. The original test component of GPQA Diamond encompasses the entirety of the GPQA Diamond dataset. 

We construct three OOD semi-fact datasets from the following datasets to evaluate the model's reasoning ability: \textbf{AIME-500 (extracted from Year of 1983 - 2023) and AIME 2024 (Year of 2024).} The AIME dataset is designed to challenge the most talented high school mathematics students in the United States. The original test component of AIME-500 consists of 500 questions extracted from the original dataset~\citep{zhengminif2f} while AIME 2024 contains 30 examples included in the exam for 2024. \textbf{GPQA Diamond.} Originating from the GPQA Diamond datasetk~\citep{rein2023gpqa} which tests scientific questions. These original datasets are in English, publicly available, and permitted for research.




% Table~\ref{tab:data_statistics}展示了我们通过动态构建的四个数据集的statistics分析，值得注意的是，通过OOD数据的构造，问题的长度相较于original test有了将近两倍的提升，这说明我们在动态构建OOD test 数据的过程中，通过模型动态引入了一些合法的语义信息，这些信息并不会对问题的答案产生改变，但可以达到改写的目的，将通过记忆而非推理的模型根据OOD与ID的差距区分出来。

% We conduct statistical analysis on three reasoning datasets. 
% For each dataset, we provide corresponding information regarding the quantity and length of the original test, Scenario-level Semi-fact test data, and Attack-level Semi-fact test data. The data statistics of these datasets are shown in Table~\ref{tab:data_statistics}. Attack-level test for each dataset is constructed using TextBugger, CheckList, and StressTest, respectively, resulting in a sample that is three times the size of the original test.
\textbf{Statistics Analysis.}  Table~\ref{tab:data_statistics} shows the analysis of three dynamically constructed datasets. For Out-Of-Distribution (OOD) test, the sample size is four times as the original due to each original instance being transformed into one Scenario-level and three Attack-level Semi-fact Data entries. This augmentation allows for a thorough OOD evaluation of model performance. The OOD test data exhibits a length comparable to that of the original test data, indicating that both Scenario-level and Attack-level Semi-fact data, derived through part-by-part or attack-based modifications respectively, preserve the model's comprehension of the question's core elements. This construction effectively incorporates legitimate semantic information that rephrases the question without altering its answer.
% Table~\ref{tab:data_statistics} presents the statistical analysis of the three dynamically constructed datasets.
% For Out-Of-Distribution (OOD) test, the sample size is quadrupled compared to the original test dataset. This increase is attributed to the transformation of each original data instance into semi-fact data, which is composed of one Scenario-level Semi-fact Data and three distinct Attack-level Semi-fact Data entries. 
% This structured augmentation facilitates a comprehensive evaluation of model performance under varied conditions.
% The OOD test data exhibits a length comparable to that of the original test data, indicating that both Scenario-level and Attack-level Semi-fact data, derived through part-by-part or attack-based modifications respectively, preserve the model's comprehension of the question's core elements. This construction effectively incorporates legitimate semantic information that rephrases the question without altering its answer.


% \shulin{Notably, the construction of the Scenario-level Semi-fact data results in nearly a twofold increase in question length compared to the original test data. It indicates that during the dynamic construction of the OOD test data, the model introduces legitimate semantic information that does not alter the answer to the question but serves the purpose of rephrasing.} 
This approach helps distinguish models that rely on memorization rather than reasoning by highlighting the differences between OOD and ID data.

\textbf{Human Evaluation.} 
% 为了确认动态修改后的数据在经过模型确认合法后，是否与答案对应，即是否在人类视角中也依旧正确合法，我们进一步设计了human evaluation。具体来说，我们选择了AIME 2024这30条数据作为评估对象，并provide a detailed explanation of 标注的规则给擅长数学的 undergraduate students,and hire them for manual annotation. 人类评估的结果为，30条OOD test data和90条ADV test data的问题与结果是100%对应的，这说明构造的OOD test与ADV test在抽样结果中是100%合法的。
To verify whether the dynamically constructed data, deemed legitimate by the model, also corresponds to the correct answers from a human perspective, we further design a human evaluation. Specifically, we use all samples from AIME 2024 for evaluation which has 30 samples. We provide a detailed explanation of the annotation guidelines to 3 undergraduate students proficient in mathematics and pay them \$2.5 per entry for manual annotation. The result of the human evaluation shows that the questions and answers for the 30 corresponding Scenario-level Semi-fact data and Attack-level Semi-fact data were 100\% aligned. This indicates that the constructed Semi-fact test is 100\% legitimate in the sampled results.


\begin{figure*}[t]
\centering
\subfloat[OOD performance vs. ID performance for AIME-500.]
{ \label{fig:aime500_bar} 
\includegraphics[width=0.45\textwidth]{Graph/exp1.pdf} 
}
\qquad
\subfloat[OOD performance vs. ID performance for AIME 2024.] 
{ \label{fig:aime2024_bar} 
\includegraphics[width=0.45\textwidth]{Graph/aime2024_bar.pdf} 
} 
\caption{ The performance gap between ID and OOD test on AIME-500 and AIME 2024. ``ID performance'' and ``OOD performance'' represent the accuracy of LLMs in solving problems on the AIME-500 and AIME 2024's original test and OOD test, respectively.}
\label{fig:bar}
\end{figure*}


\section{Experiments}
We conduct experiments to verify the effectiveness of our proposed dynamic OOD data construction method and analyze the differentiated performance and robustness of various LLMs.
\subsection{Setup}
We use the datasets in Section~\ref{Sec:data} for our experiment
across two test sets: (1) the original test set, (2) the OOD test set. 
% For each of the two datasets, the number of test sets remains consistent.
We conduct evaluations on various models, including o1-preview~\citep{o1}, o1-mini~\citep{o1}, o3-mini~\citep{o3}, GPT-4o~\citep{achiam2023gpt}, Deepseek-V3~\citep{liu2024deepseek}, Deepseek-R1~\citep{guo2025deepseek}, S1.1-32B~\citep{muennighoff2025s1}, LLAMA3.1-70B and 8B~\citep{dubey2024llama}, Gemma2 (both 9B and 27B)~\citep{riviere2024gemma}, Mistral-7B~\citep{jiang2023mistral}, Mixtral-8x7B~\citep{jiang2024mixtral}, Qwen2.5-72B-IT~\citep{yang2024qwen2}, and Qwen2.5-Math-72B-IT~\citep{yang2024qwen2}. All models were configured with a temperature setting of 0.7 and used the pass@1 metric in a single test run.

% In Table~\ref{tab:main table1}, we present the comparisons between the OOD test sets and the original test sets across the three datasets. 

% Table~\ref{tab:main table2} shows the comparisons between the ADV test sets and the original test sets for the same datasets. 



\subsection{Results}

% 通过

% 对比 
\textbf{ThinkBench.} The overall results  of ThinkBench are shown in Table~\ref{tab:main table1}. We observe that all models exhibit a certain degree of performance decline when evaluated on the original dataset versus the OOD dataset. This decay in performance can be attributed to the fact that the models have encountered, to some extent, the original dataset. Notably, the performance gap between ID (Original) and OOD in AIME 2024 for most models is much smaller than the performance gap in AIME-500, which demonstrates the data leakage in AIME data before 2024. The existing dataset~\citep{li2024openai, glazer2024frontiermath} is insufficient to reveal the aforementioned phenomenon.
% However, OOD modifications involve real-time generated data that the models have not been trained on and, in many cases, have not even encountered in terms of data format. 
% As a result, the performance of all models declined on the datasets altered by OOD modifications, which also demonstrates the validity of the OOD modification methods we employ to construct the dataset.

We also observe that models with fewer parameters generally perform worse and are less robust than those with more parameters, especially within the Llama 3.1 model family. As the parameter count increases, both performance and robustness improve significantly, consistent with scaling laws. Thus, the number of parameters is crucial for ensuring robustness.
% We also observe that, in most cases, models with fewer parameters demonstrate significantly poorer performance and robustness compared to those with a larger number of parameters. This phenomenon is particularly evident within the Llama 3.1 model family. Specifically, as the parameter count of Llama 3.1 model increases, we see a notable enhancement in both performance and robustness. This trend aligns with the scaling laws governing performance and robustness, which suggest that the model with a larger number of parameters tends to exhibit significantly better and more robust performance. Therefore, the number of model parameters plays a critical role in ensuring robustness.


% 对比模型在原始的数据集与ADV扰动后的数据集，我们可以发现所有模型的性能都下降了，ADV数据集我们通过形式上对原始数据进行了简单改动，这样简单的形式上改动在实际应用场景中是经常出现的，（打错字，多打了一个词，词不达意等）。而大部分模型在形式上改动后的结果性能都下降了，说明模型还是存在着一点死记硬背题目的情况，泛化性上仍然欠缺，原始与ADV改动后的差距同样说明了我们构造的ADV扰动数据集是合理的，我们仅通过简单的符合实际应用场景的扰动，就可以展示出原始与改动后的性能差距。值得注意的是，与chatgpt（引用论文）相比，以o1为代表的test-time compute models体现了明显更强的鲁棒性，甚至与较新的gpt-4o相比，也有着较大提升，这进一步说明了test-time compute models相较于train-time compute models不仅在性能上有较大优势，同样在对抗扰动方面也有着较大优势，泛化性更强，也更利于进一步用在各种实际场景中。

% When comparing the performance of the models on the original dataset versus the ADV dataset, we observe a decline in performance across all models in Table~\ref{tab:main table2}. The ADV dataset is created through simple but effective modifications in format to the original data, which indeed reflects common errors encountered in practical applications, such as typographical mistakes and unintended word repetitions. The performance degradation observed in most models following these straightforward format modifications indicates that they still exhibit some degree of rote memorization, revealing a lack of generalization. The performance gap between the original and ADV datasets also supports the rationale behind our construction of ADV modified dataset. By employing simple, realistic perturbations, we are able to effectively demonstrate the performance gap between the original and the modified dataset. 
% Notably, compared to ChatGPT~\citep{achiam2023gpt}, the test-time compute models represented by o1-preview exhibit markedly superior robustness. Even in comparison to the more recent GPT-4o, the o1-preview shows considerable improvement. This demonstrates that test-time compute models not only have a significant performance advantage over train-time compute models but also excel in handling adversarial perturbations, showcasing stronger generalization capabilities and greater applicability in various real-world scenarios.

% o1-mini 和 o1-preview 只有数学 其他的



%

% 我们在表X中还给出了OOD与几种ADV攻击的均值，可以发现AIME与GPQA这样得出的模型性能与原始性能存在差距，但差距较小，这一定程度上（或许）可以说明o1和其他模型确实如o1的report中所言，AIME与GPQA确实没有对模型而言有原题泄露的问题。（加图的分析）

% ID和OOD performance分别代表了LLMs在AIME 500的original test和Out-of-distribution test上解决问题的正确率。
% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\columnwidth]{Graph/exp1.pdf}
%     \caption{
%     The performance gap between ID and OOD test on AIME. ``ID performance'' and ``OOD performance'' represent the accuracy of LLMs in solving problems on the AIME 500's original test and out-of-distribution test, respectively.
%     }
%     \label{fig:aime_bar}
% \end{figure}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{Graph/aime2024_line.pdf}
%     \caption{
%     Math Reasoning Gap on AIME 2024. 
%     }
%     \label{fig:aime2024_bar}
% \end{figure*}


% \begin{figure*}[ht]
% \centering
% \subfloat[Language Understanding Gap.]
% %After the input prompt ends, we should start to generate an entity from entities prefix tree $T$, e.g., both ``Florida'' and ``China'' are valid beginning tokens in $T$.]
% { \label{fig:mmlu_line} 
% \includegraphics[width=0.45\columnwidth]{Graph/mmlu_1.pdf} 
% }
% \qquad
% \subfloat[Scientific Questions Gap.] 
% { \label{fig:gpqa_line} 
% \includegraphics[width=0.45\columnwidth]{Graph/gpqa_1.pdf} 
% } 
% \caption{Out-of-distribution Gap.}
% \label{fig:line}
% \end{figure*}

% \begin{figure*}[ht]
% \centering
% \subfloat[OOD performance compared to ID performance for Language Understanding.]
% { \label{fig:mmlu_line} 
% \includegraphics[width=0.45\columnwidth]{Graph/mmlu_1.pdf} 
% }
% \qquad
% \subfloat[OOD performance compared to ID performance for Scientific Questions.] 
% { \label{fig:gpqa_line} 
% \includegraphics[width=0.45\columnwidth]{Graph/gpqa_1.pdf} 
% } 

% \subfloat[Language Understanding Gap.] 
% { \label{fig:mmlu_bar} 
% \includegraphics[width=0.45\columnwidth]{Graph/mmlu_2.pdf} 
% }
% \qquad
% \subfloat[Scientific Questions Gap.] 
% { \label{fig:gpqa_bar} 
% \includegraphics[width=0.45\columnwidth]{Graph/gpqa_2.pdf} 
% } 
% \caption{The performance gap between ID and OOD test on MMLU and GPQA Diamond.
% %Out-of-distribution Gap. We report the performance gap between ID and OOD test on MMLU and GPQA Diamond, respectively.
% }
% \label{fig:bar}
% \end{figure*}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{Graph/gpqa_2.pdf}
%     \caption{
%     The performance gap between OOD and ID test on GPQA Diamond. 
%     }
%     \label{fig:gpqa_bar} 
% \end{figure}

% \begin{figure*}[t]
% \centering
% \subfloat[Language Understanding Gap.] 
% { \label{fig:mmlu_bar} 
% \includegraphics[width=0.45\textwidth]{Graph/mmlu_2.pdf} 
% }
% \qquad
% \subfloat[Scientific Questions Gap.] 
% { \label{fig:gpqa_bar} 
% \includegraphics[width=0.45\textwidth]{Graph/gpqa_2.pdf} 
% } 
% \caption{The performance gap between ID and OOD test on MMLU and GPQA Diamond.
% %Out-of-distribution Gap. We report the performance gap between ID and OOD test on MMLU and GPQA Diamond, respectively.
% }
% \label{fig:bar}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{Graph/mmlu_1.pdf}
%     \caption{
%     Language Understanding Gap on MMLU 570. 
%     }
%     \label{fig:mmlu_line}
% \end{figure*}

% \begin{figure*}[t]
% \centering
% \subfloat[Language Understanding Gap.] 
% { \label{fig:mmlu_bar} 
% \includegraphics[width=0.45\textwidth]{Graph/mmlu_2.pdf} 
% }
% \qquad
% \subfloat[Scientific Questions Gap.] 
% { \label{fig:gpqa_bar} 
% \includegraphics[width=0.45\textwidth]{Graph/gpqa_2.pdf} 
% } 
% \caption{The performance gap between ID and OOD test on MMLU and GPQA Diamond.
% %Out-of-distribution Gap. We report the performance gap between ID and OOD test on MMLU and GPQA Diamond, respectively.
% }
% \label{fig:bar}
% \end{figure*}


\subsubsection{Math Reasoning}

% 比较模型们在AIME-500与AIME 2024上的表现，我们可以发现，o1系列模型在最新的2024年AIME（高级国际数学考试）上的表现显著下降，尽管这些模型是在今年考试数据之前的数据上训练的。且包括o1在内的几乎所有模型在AIME-500上效果OOD与ID之间的差距都显著大于AIME 2024上OOD与ID之间的差距。o1-preview在AIME 2024和AIME-500上OOD与ID之间的性能差距分别为-3.3\%和-11.2%，o1-mini则是+5.8\%和-12.8\%,这说明2024之前的题目确实存在一定程度的数据泄露。Ideally, a robust model that has genuinely learned to apply knowledge for reasoning should not exhibit significant performance degradation when the expression and scenario of a question change, provided that the core knowledge being assessed remains the same。同时也说明了我们构造的OOD数据是高质量的，可以区分出OOD与ID之间的性能差距。同时可以判断出模型在考察同样核心知识只是换了场景与表述的题目上，是否可以保持几乎持平的能力，而不是靠记忆题目体现出的在ID题目上较高的能力水平。我们的构建的OOD数据可以一定程度上解耦出对能力的评测，而非与对知识的评测杂糅在一起。

Figure~\ref{fig:comparison} presents a comparative analysis of the performance of various LLMs on the AIME-500 and AIME 2024 dataset, specifically contrasting their performance on the original ID data with that on OOD data. 
% The x-axis represents the accuracy of each model on the original data, while the y-axis indicates the accuracy of the models on the OOD data. 
The results on the original set are mainly consistent with previous work~\cite{glazer2024frontiermath,li2024gsm}.
Notably, the dashed line at a 45-degree angle from the origin signifies, where a robust model exhibits nearly equivalent performance on both OOD and ID data. These two datasets primarily evaluate mathematical reasoning and logical capabilities, featuring relatively complex problem types.

\textbf{AIME-500 vs. AIME 2024.} It is evident that o1 series models, o3, Deepseek-R1, and s1 exhibit a small decline in performance on the latest 2024 AIME. Moreover, the performance gap between OOD and ID is markedly larger on AIME-500 for nearly all models, including the o1 series, compared to the OOD and ID gap observed on AIME 2024. Specifically, the o1-preview shows a performance gap of -3.3\% on AIME 2024 and -11.2\% on AIME-500, while the o1-mini model exhibits a gap of +5.8\% and -12.8\%, respectively. The most advanced reasoning models -- o3-mini, Deepseek-R1 and s1.1-32B -- show the same phenomenon. This indicates that there was indeed some degree of data leakage in questions before 2024. 

Ideally, a robust model that has genuinely learned to apply knowledge for reasoning should not exhibit significant performance degradation when the expression and scenario of a question change, provided that the core knowledge being assessed remains the same. 
Additionally, s1.1-32B shows a significant gap in AIME-500 ID-OOD performance. One possible explanation for its low robustness is the limited amount of data used for training. Furthermore, most models fall below the dashed line, indicating a decline in performance when transitioning to OOD data. This phenomenon reveals their limitations in generalizing to previously unseen complex mathematical problems. 
% In contrast, certain models, such as o1-preview and o1-mini, distinguish themselves by achieving relatively high accuracy on both the original and OOD datasets, placing them closer to the ideal performance line. 
% These models demonstrate a stronger capacity to handle distributional shifts, underscoring their robustness.

% Although o1-mini is positioned further from the o1-preview along the x-axis, it still demonstrates superior performance on the OOD data. This advantage may be due to the inclusion of certain mathematical problems in the pre-training, which do not negatively impact its generalization capabilities.


% % Although o1-mini is positioned further from o1-preview along the x-axis, it still exhibits superior performance on the OOD data, which we hypothesize may be attributed to the inclusion of certain mathematical problems in the training that do not adversely affect its generalization capabilities.
% Moreover, models like LLAMA3.1-8B-IT and Gemma-2-2.7B-IT cluster near the lower-left corner, exhibiting limited accuracy on both OOD and ID datasets, indicating poor performance on the complex mathematical task. 
% % This visualization highlights significant disparities in model performance and robustness across different data distributions, with only a select few models achieving high accuracy on both the original and OOD data.


\textbf{ID vs. OOD.} Figure~\ref{fig:aime2024_bar} and Figure~\ref{fig:aime500_bar} illustrates the percentage difference in performance between OOD and ID for various models on the AIME 2024 and AIME-500. 
% The x-axis lists the different models, while the y-axis quantifies the performance degradation, expressed as negative percentages, indicating the drop in accuracy when models are applied to OOD data. 
Certain models, such as o3-mini, Deepseek-R1, and o1-series models distinguish themselves by achieving relatively high accuracy on both the original and OOD datasets. They also exhibit minimal OOD performance degradation, signifying strong generalization capabilities and robustness to distributional changes. 
% In contrast, models such as Gemma-2-9B-IT exhibit more pronounced declines, exceeding -30\% in AIME-500. 
% This greater degradation suggests that these models are more sensitive to OOD data, indicating a reliance on dataset-specific patterns and cannot generalize to other patterns.



% % O1-mini and Qwen-Math include mathematical problems similar to those in AIME during their pre-training. However, the patterns in the OOD data have changed significantly, resulting in a significant percentage decline.
% Conversely, models such as GPT-4o, o1-preview, and Qwen2.5-72B-IT exhibit minimal OOD performance degradation, with bar heights approaching zero, signifying strong generalization capabilities and robustness to distributional changes.
% Another possible reason for better generalization is that it stems from a stronger language comprehension ability, which goes beyond the generalization provided by mathematical problems. This is why general models like o1-preview show a low percentage decline.
% % Another potential explanation for enhanced generalization could be superior language comprehension abilities that extend beyond the generalization afforded by mathematical problems; thus, universal models like o1-preview demonstrate low percentage declines. 
% In contrast, models such as LLAMA3.1-8B-IT, LLAMA3.2-3B-IT, and Yi-Lightning exhibit more pronounced declines, exceeding -0.5\%. 
% This greater degradation suggests that these models are more sensitive to OOD data, indicating a reliance on dataset-specific patterns and cannot generalize to other patterns.

% % 图X显示了各LLMs在AIME 500的原始数据与OOD数据的性能比较分析。x轴表示每个模型在原始AIME数据集中的准确性，而y轴表示模型在AIME OOD数据集中的准确度。值得注意的是，与原点成45度角的虚线表明了一种较为理想的情况，即鲁棒性足够好的模型可以在OOD数据上表现出和ID数据几乎一致的性能。这个数据集主要考察数学推理和逻辑能力，题型较为复杂。大多数模型，如Qwen2.5-72B-IT和Deepseek-v2.5，往往都低于这条线，这表明在转换到OOD数据时性能会下降，这种情况通常会暴露出其泛化到别的没见过的题目上的的局限性。一些模型，如“o1-preview”和“o1-mini”，在原始数据集和OOD数据集上都以相对较高的准确性脱颖而出，使它们更接近或高于理想的性能线。这些模型表现出更强的处理分布变化的能力，表明了它们的稳健性。o1-mini虽然比o1-preview离线更远，但OOD上的的性能更高，我们猜测是因为一些数学题的加入不会影响数学题的泛化性。此外，像LLAMA3.1-8B-IT和Gemma-2-2.7B-IT这样的模型聚集在左下角附近，在OOD和ID两种数据上的准确性都有限，这表明它们对这些任务的效果较差。这种可视化突出了不同数据分布中模型性能和鲁棒性的显著差异，只有少数模型在原始数据集和OOD数据集上实现了高精度。
% % 此外，条形图X显示了AIME 500数据集上各种模型的分布外（OOD）性能与ID性能差距百分比，x轴列出了不同的模型，而y轴量化了性能衰减，表示为负百分比，表明当模型应用于OOD数据时精度下降百分比。可以看出，而o1-mini和qwen math一样，可能预训练中包含了与AIME类似的数学题，所以百分比下降较高。“GPT-4o”、“o1-preview”和“Qwen2.5-72B-IT”等模型表现出最小的OOD性能衰减，条形图接近零，表明具有很强的泛化能力和对分布变化的鲁棒性。泛化性更好的另一种可能在于语言理解能力更强，不仅仅来自于数学题带来的泛化性，所以通用模型o1-preview等百分比下降很低。像“LLAMA3.1-8B-IT”、“LLAMA3.2-3B-IT”和“Yi-Lightning”这样的模型显示出更大的下降，差距超过-0.5%。这种更大的衰减表明这些模型对OOD数据更敏感，表明它们可能依赖于数据集特定的pattern，这些pattern不能很好地泛化。

% % AIME（数学题）：

% %4.2
% % 特点：主要考察数学推理和逻辑能力，题型较为复杂。
% % ID与OOD差异：模型可能在训练集上对公式和解题步骤进行了良好的学习，但由于题目风格和复杂度的变化，导致在OOD测试集上的表现下滑。这说明模型可能过于依赖于训练集中的特定模式，未能有效泛化。
% % 数据集的有效性

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{Graph/gpqa_1.pdf}
%     \caption{
%    Scientific Questions Gap on GPQA Diamond Robustness.
%     }
%     \label{fig:gpqa_line}
% \end{figure}


% \subsubsection{Language Understanding Results on MMLU 570}
% Figures ~\ref{fig:mmlu_line} and ~\ref{fig:mmlu_bar} present a comparative analysis of ID accuracy against OOD accuracy for various models, revealing a generally positive correlation between the two metrics.
% Most models cluster along the diagonal line, indicating that higher ID accuracy tends to correspond with better OOD performance. 

% In terms of performance on MMLU 570 dataset, both o1-preview and o1-mini exhibit a reduced lead over other models compared to their pronounced superiority on the AIME 500 dataset. This shift can be attributed to the nature of the MMLU 570, which emphasizes language understanding and the application of comprehensive knowledge through a diverse array of question formats. The overall difficulty of MMLU 570 is considerably lower than that of AIME 500, which places a greater emphasis on complex mathematical reasoning and logical capabilities.

% A significant cluster of models demonstrates comparable performance, resulting in minimal differences among them. Notably, models such as Mixtral-8x7B-Instruct-v0.1 and Qwen2.5-Math-7B exhibit lower performance across both dimensions. Specifically, Mixtral-8x7B-Instruct-v0.1 consistently underperforms across all three datasets. The marked disparity in performance of Qwen2.5-Math-7B between the AIME 500 and MMLU 570 datasets suggests that merely augmenting the pre-training phase with mathematical problems does not necessarily enhance its capabilities in language understanding and the application of comprehensive knowledge. This indicates a limitation in its generalization ability.
% % 对于o1-preview和o1-mini在MMLU 570的性能，不再像在AIME 500数据集上断档式领先别的模型，差距小了一些，这是因为MMLU 570侧重于语言理解和综合知识的运用，题目形式多样，但比起AIME 500对复杂的数学推理和逻辑能力考察，难度还是小了很多。中间存在一大批模型性能聚集在一起，差距并不明显，Mixtral-8x7B-Instruct-v0.1和Qwen2.5-Math-7B等模型在两个维度上的性能都较低，Mixtral-8x7B-Instruct-v0.1在三种数据集上都表现较差。而Qwen2.5-Math-7B在AIME 500和MMLU 570数据集上的显著差异，则说明在预训练阶段增加数学题并没法让其在语言理解和综合知识的运用上表现较好的性能，其泛化能力存在局限性。

% % 不像AIME的断档 差距较小 中间一批模型大家差不多 最后的性能不行的模型

% % The scatter plot presents a comparative analysis of in-distribution (ID) accuracy against out-of-distribution (OOD) accuracy for various models, revealing a generally positive correlation between the two metrics. Most models cluster along the diagonal line, indicating that higher ID accuracy tends to correspond with better OOD performance. Notably, models such as o1-preview and GPT-4o exhibit high ID and OOD accuracies, suggesting their robustness across different data distributions. In contrast, models like Mixtral-8x7B-Instruct-v0.1 and Qwen2.5-Math-7B display lower performance in both dimensions, highlighting potential limitations in their generalization capabilities. The overall trend underscores the importance of achieving high ID accuracy as a predictor of OOD performance, emphasizing the need for continued research into enhancing model robustness to diverse and unseen data.

% % The bar chart illustrates the performance decay ratio between in-distribution (ID) and out-of-distribution (OOD) accuracy for various models, measured as the MMLU OOD gap. Most models exhibit a negative gap, indicating a decline in performance when evaluated on OOD data compared to ID data. Notably, models such as Qwen2.5-Math-7B and Phi-3-mini-4k-IT show the least performance degradation, suggesting that they maintain a relatively robust capability when faced with OOD challenges. In contrast, models like Mixtral-8.7B-Instruct-v0.1 and Gemma-2.2B-IT demonstrate a more pronounced performance drop, reflecting potential vulnerabilities in their generalization to unseen data. This analysis underscores the variability in OOD robustness across different architectures, highlighting the importance of model selection based on performance stability in diverse conditions.

% % MMLU（语言理解）：

% % 特点：侧重于语言理解和综合知识的运用，题目形式多样。
% % ID与OOD差异：在语言理解上，模型通常能够捕捉到更为丰富的上下文信息，因此在OOD任务上表现较优。语言模型的训练通常更加注重上下文的理解，使其在面对新问题时能够更好地推理。

% % 正相关性：从图中可以观察到，ID性能和OOD性能之间存在一定的正相关性。即，ID性能较高的模型通常在OOD性能上也表现较好。这表明模型在训练集上的表现与其在未见数据上的泛化能力有一定关系。

% % 各模型表现
% % 高ID和高OOD性能：

% % 一些模型如Yi Lightning和Deepseek-v2.5，在ID和OOD性能上均表现优异。这类模型可能在数据集上经过了良好的训练和优化，具有较强的泛化能力。
% % 低ID但高OOD性能：

% % 某些模型可能在ID性能上表现一般，但在OOD性能上却相对较好。这可能表明这些模型在特定的任务或领域上进行了有效的调整或微调，使其在未见数据上的表现优于在训练集上的表现。
% % 高ID但低OOD性能：

% % 也有模型在ID性能上表现良好，但在OOD性能上却相对较弱。这可能表明这些模型在训练集上过拟合，未能有效地学习到泛化的特征。
% % 数据集特性
% % AIME（数学题）：模型在数学题上的表现普遍较低，可能导致其整体ID和OOD性能不佳。需要更多的专门化训练来提升数学推理能力。
% % GPQA（科学问题）：科学问题的准确性较高，模型在OOD性能上表现较好，显示出科学知识的有效学习。
% % MMLU（语言理解）：语言理解问题的模型表现普遍较强，说明良好的语言模型能够更好地处理未见数据。


\subsubsection{Science Questions}
% 和别的数据集 和4o比 preview的增长位于middle
% 更分散 没有AIME难


Figures~\ref{fig:gpqa_line} show ID-OOD performance on GPQA Diamond dataset. 
% % Compared to the previous two datasets, it is observed that the improvement of o1-preview relative to GPT-4o is situated in the middle range on this dataset; it exhibits less growth than AIME 500 but greater growth than MMLU 570.
% Compared to the other two datasets, we observe that the improvement of o1-preview relative to GPT-4o is in the middle range on this dataset. Specifically, it shows less growth than AIME 500 but greater growth than MMLU 570.
The distribution of model performance points indicates that most models are more dispersed on this graph, suggesting that the difficulty of the GPQA Diamond dataset is less than that of AIME-500 and AIME 2024.
% , yet more complex than the straightforward knowledge assessment required by MMLU 570.

Models such as Mixtral-8x7B-Instruct-v0.1 and LLAMA3.2-3B-IT demonstrate relatively low accuracy levels. Notably, LLAMA3.2-3B-IT shows a 4.9\% improvement in OOD performance compared to its ID performance on the bar chart. 
This phenomenon can be attributed to the model's inherently poor performance. For the difficult questions in the GPQA Diamond dataset, the model seems to resort to random guessing, resulting in an accuracy that fails to reach the 25\% level expected from random guessing.
% This phenomenon can be attributed to the model's inherently poor ID performance; for the moderately challenging questions in the GPQA Diamond Robustness dataset, the model appears to resort to random guessing, failing to achieve even the 25\% accuracy expected from random guessing. 
Consequently, the OOD performance reflecting a higher accuracy than ID is a result of the same guessing strategy employed for both ID and OOD questions.

% 图X和图X在GPQA Diamond Robustness数据集上the relationship between ID accuracy and OOD accuracy for various models。与前面的两个数据集相比，可以发现对于o1-preview而言相对于GPT-4o的增长，在这个数据集上是位于中间的，不如AIME 500增长得多，比MMLU 570增长得多。此外，从所有模型的点所处的位置来看，大部分模型在图上是更分散的，因为GPQA Diamond Robustness的难度不如AIME 500，但又不像MMLU 570只对知识掌握有简单要求。像Mixtral-8x7B-Instruct-v0.1和LLAMA3.2-3B-IT这样的型号显示出较低的精度水平。LLAMA3.2-3B-IT在柱状图上展示了OOD比IT的表现搞了30%，这是因为本身ID表现很差，对于GPQA Diamond Robustness这种稍难的题目基本是根据猜测做题，甚至达不到正常猜测做题的25%的正确率，OOD的题目一样是根据猜测做题的才会有OOD表现高于ID的现象。


% The bar chart illustrates the relationship between ID accuracy and OOD accuracy for various models, highlighting a generally positive correlation between the two metrics. As ID accuracy increases along the x-axis, there is a corresponding rise in OOD accuracy on the y-axis, suggesting that models with higher performance on in-distribution tasks tend to maintain better performance on out-of-distribution tasks. Notably, models such as Qwen2.5-72B-IT and GPT-4o exhibit high levels of both ID and OOD accuracy, positioning them favorably in the upper right quadrant of the chart. Conversely, models like Mixtral-8x7B-Instruct-v0.1 and LLAMA3.2-3B-IT demonstrate lower accuracy levels, indicating potential areas for improvement. The dotted line, serving as a reference, further emphasizes the trend, as most models cluster around it, reaffirming the notion that robust ID accuracy is a strong indicator of OOD performance. This analysis underscores the importance of comprehensive evaluation in model development, where achieving high ID accuracy can significantly enhance out-of-distribution robustness.

%The bar chart presents the performance decay ratio between ID and OOD accuracy for various AI models, measured as the GPQA OOD Gap. A notable observation is that models such as LLAMA3.2-3B-IT and Yi-Lightning exhibit significant performance decay, with positive gap values indicating a pronounced decline in OOD accuracy compared to their ID counterparts. In contrast, models like Deepseek-v2.5 and GPT-4o demonstrate negative gap values, suggesting that their OOD performance is not only comparable to but may even exceed their ID accuracy, highlighting their robustness in handling out-of-distribution scenarios. Additionally, the variability in performance decay among the models underscores the importance of model selection and training methodologies, as some models, such as Qwen2.5-Math-72B-IT, show relatively minor decay, indicating better generalization capabilities. This analysis emphasizes the necessity for further investigation into the factors contributing to these performance disparities, as enhancing OOD robustness remains a critical challenge in the development of AI systems.

% GPQA（科学问题）：

% 特点：涉及科学知识的理解与应用，题目类型多样。
% ID与OOD差异：模型在科学知识的掌握上表现较好，可能因为科学问题的知识结构相对稳定，模型能够较好地迁移学习。OOD性能较好，说明模型在未见数据上也能有效应用已学知识。

% \paragraph{Adversarial Results.}

% \paragraph{OOD Results.}
%Adv结果

%OOD结果


% \subsubsection{Benchmarking the Performance of PRM}
\begin{figure*}[t]
\centering
\subfloat[Performance on AIME 2024 OOD data.]
{ \label{fig:prm_1} 
\includegraphics[width=0.45\textwidth]{Graph/prm_1.pdf} 
}
\qquad
\subfloat[Performance on AIME-500 OOD data.] 
{ \label{fig:prm_1} 
\includegraphics[width=0.45\textwidth]{Graph/prm_2.pdf} 
} 
\caption{ \textbf{Test-time Scaling Law.} We show that the model's performance increases on the OOD dataset with the test-time computation budget increases using Qwen2.5-Math-7B-IT as the policy model, along with several PRMs. 
}
    \label{fig:analysis}
\end{figure*}

\section{Analysis and Discussion}

In this section, we present a detailed analysis of the OOD robustness of reasoning and non-reasoning models. We aim to address several open research questions that are fundamental to building efficient reasoning models.

\noindent\textbf{RQ1: Do reasoning models deliver the significant performance and robustness improvements claimed in their reports?}

Overall, the o1-series models, along with o3, Deepseek-R1, and s1, demonstrate strong performance in complex logical reasoning tasks, such as AIME and GPQA Diamond, on the original datasets. Despite a general performance drop from ID to OOD across various datasets, these models maintain notable robustness. Their improved performance, consistent with the claims in their reports~\cite{o1, o3, guo2025deepseek, muennighoff2025s1}, suggests that these advancements are reliable.

Notably, on AIME-500 and AIME 2024, while o3-mini and Deepseek-R1 exhibit a larger ID-OOD performance gap than o1-preview, they still achieve the highest absolute performance in the OOD setting. This superior mathematical reasoning capability is likely due to their exposure to a larger corpus of mathematical problems during training, enhancing their problem-solving abilities. On GPQA Diamond, o3-mini and Deepseek-R1 also achieve the highest performance, underscoring their general reasoning advantages.


\noindent\textbf{RQ2: Is there a possibility of data leakage in the original datasets for AIME-500?}

Analyzing the ID-OOD performance gap, as depicted in Figures \ref{fig:comparison}, reveals a notable difference between AIME 2024 and AIME-500. While the gap between ID and OOD performance is relatively small for AIME 2024, AIME-500 consistently shows a larger gap. This observation suggests that a significant portion of the models may have encountered similar AIME-500 data during training. Given that AIME data prior to 2024 was publicly available, we posit that data leakage is likely for most models.


\begin{table}
\centering
\small
\caption{Performance comparison of different process reward models using Qwen2.5-Math-7B-IT under the Best-of-256 test. All methods utilize Qwen2.5-Math-7B-IT as the base model. Qwen2.5-Math-7B-IT's performance represents majority vote (Maj@256) results, while other methods are based on PRMs.
% \ys{In the table, can directly use "Majority Vote", or use "Maj@256"? Also should we also include PRM aggregation strategies? Can we also include a "Gap" column to show the OOD more clearly?}
}
\label{tab:main table3}
 \begin{adjustbox}{width=.49\textwidth}{
\begin{tabular}{lcccc} 
\toprule
\multirow{2}{*}{}                 & \multicolumn{2}{c}{\textbf{AIME 2024}} & \multicolumn{2}{c}{\textbf{AIME-500}}   \\ 
\cmidrule(l){2-5}
                                  & Original & OOD                & Original & OOD                \\ 
\midrule
Qwen2.5-Math-7B-IT (Maj@256)  & 0.167    & 0.133              & 0.524    & 0.464              \\
% \midrule 
% PRM-based Methods & & & &\\
\midrule
+Math-Shepherd & 0.233    & 0.233              & 0.528    & 0.458              \\
+OpenR         & 0.233    & 0.200              & 0.526    & 0.472              \\
+Skywork       & 0.200    & 0.233              & 0.582    & 0.500              \\
+Qwen          & 0.300    & 0.300              & 0.538    & 0.476              \\ 
% \midrule
% Qwen2.5-Math-7B (Majority Vote)   & 0.167    & 0.133              & 0.524    & 0.464              \\
\bottomrule
\end{tabular}
}
    \end{adjustbox}
\end{table}

\noindent\textbf{RQ3: Can our OOD dataset serve as a test-time computation benchmark?}

Based on our analysis, the AIME-500, AIME 2024, and GPQA Diamond datasets present significant challenges and possess strong discriminative power. As shown in Figure~\ref{fig:analysis}, model performance improves with increased test-time computation, underscoring the quality of our data. The upward trend and variation among the five lines indicate the dataset's ability to differentiate model performance during test-time computation.

Furthermore, Table~\ref{tab:main table3} presents results from 256 inferences using Qwen2.5-Math-7B-IT to evaluate different PRMs. For AIME 2024, OOD performance shows slight variations compared to ID across PRMs. However, for AIME-500, all PRMs experience a notable decline in OOD performance, suggesting possible exposure to similar problems before 2024, and thus indicating data leakage. In contrast, AIME 2024 data appears largely unseen by the models in terms of the low-level performance decay. Our methodology provides valuable insights into assessing robust LLM reasoning. The dynamic generation capability allows model validation, preventing exploitation or overfitting, which can serve as an effective benchmark for facilitating future research on test-time computation.

% Based on the analysis presented above, it is evident that AIME-500, AIME 2024, and GPQA Diamond,  we construct, which include OOD tests, are sufficiently challenging and possess a certain degree of discriminative power. As illustrated in Figure~\ref{fig:analysis}, the model's performance improves with an increase in the test-time computation budget, which further demonstrates the high quality of our data. The upward trend of the five lines, along with their differences, also indicates that our dataset has a certain level of discriminative capability, effectively testing the model's performance in test-time computation.

% % 在Table~\ref{tab:main table3}中，我们使用Qwen2.5-Math-7B-IT做256次Inference，评测不同的PRMs，可以看到AIME 2024上不同的PRMs OOD相较于ID小范围上升或下降，而AIME-500上所有PRMs较ID，OOD都有了相对大幅度的下降。这一定程度上说明不同的PRMs见过AIME 2024年之前的题或与之相似的题，存在一定的数泄露，而AIME 2024的数据基本没有被见过。
% In Table~\ref{tab:main table3}, we conduct 256 inferences using Qwen2.5-Math-7B-IT to evaluate different PRMs. The results indicate that for AIME 2024, there is a slight increase or decrease in OOD performance compared to ID across various PRMs. Conversely, for AIME-500, all PRMs exhibit a relatively significant decline in OOD performance compared to ID. This suggests that different PRMs may have been exposed to problems from AIME prior to 2024 or similar ones, indicating a degree of data leakage. In contrast, the data from AIME 2024 appears to have been largely unseen.

% % Moreover, the data in our benchmark can be dynamically generated in real time, which helps avoid the impact of data contamination. This ensures a fair assessment of the actual performance and robustness of different models, as it eliminates the risk of models memorizing answers from previously seen data during training, which would lead to inaccurate testing.



% The dynamic generation capability enables repeated validation of models, making it difficult for them to exploit or cheat. Therefore, our proposed OOD dataset can be utilized as an effective benchmark for test-time computation.



\section{Conclusion}
% Existing benchmarks primarily focus on train-time compute models, while paying limited attention to test-time models.
We presented ThinkBench, a first robust dynamic evaluation benchmark for testing reasoning capability in LLMs, unifying the reasoning models and non-reasoning models evaluation. ThinkBench offers dynamic data generation to evaluate the out-of-distribution of models through various math reasoning, and science questions samples. Experiments over AIME-500, AIME 2024, and GPQA Diamond indicate that while reasoning models represented by o1 and o3 consistently demonstrate relatively strong robustness, most LLMs fall short of robust performance. 
% We also explore the impact of PRMs on test-time inference and offer a comprehensive, fine-grained, dynamic analysis of PRMs.
Experiments on ThinkBench highlight the importance of eliminating data contamination, especially in reasoning tasks where golden answers are prone to leakage. ThinkBench offers an effective solution to mitigate data leakage issues during reasoning evaluation.
In the future, we will study more factors of robust reasoning models. 

% 根据上面的分析，可以看出来我们构建的包含OOD和ADV test的AIME 500、GPQA Diamond Robustness和MMLU 570足够难，且具备一定的区分度。而通过图~\ref{fig:analysis}可以看出随着test-time computation budget increases，model’s performance increases，这也足以说明我们构建的数据质量高。同时，五条线上涨的趋势以及彼此间的不同也说明我们的数据集具备一定的区分度，可以有效测试模型的test-time computation性能。

% 此外，因为我们的benchmark中的数据是可以实时动态生成的，这避免了数据污染带来的影响，可以公平地测出不同模型的实际性能以及鲁棒性，而不会因其在训练过程中见过数据而记住了答案，从而导致测试不准。动态生成可以是我们具备重复校验模型的能力，这对模型来说是hard to cheat。因此，我们提出的OOD datasetcan be leveraged as the test-time computation benchmark.

% 足够难 有区分度 数据质量较高
% 这组数据为什么可以用来测test-
% 重复校验 hard to cheat 因为动态生成


% 图可以体现出我们构建的数据集可以用来评测test-compute models 且prm是通向test compute models的路中关键的一步 就像o1报告中所称

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Limitations}

% While ThinkBench presents dynamic OOD data construction to evaluate LLMs, 
While ThinkBench presents a systematic dynamic OOD data generation framework that uniquely decouples reasoning robustness from memorization biases,
it still has some limitations. First, while it supports MMLU, the evaluation predominantly focuses on mathematical and scientific reasoning tasks (e.g., AIME, GPQA), lacking diversity in reasoning types such as social reasoning. Second, due to cost constraints, Scenario-level semi-fact generation adopts a single-path rephrasing strategy instead of hierarchical multi-scenario branching, limiting OOD diversity. Future work needs to consider expanding task coverage and integrating tree-structured generation for richer scenario variations.
%\section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}

\newpage
\section{Appendix}



\begin{table*}
 \centering
 \small
\caption{Performance on Scenario-level semi-fact data and Attack-level semi-fact data. Each performance value represents the accuracy in solving problems from the dataset. The accuracy of performance on attack-level semi-factual data is determined by computing the minimum accuracy across three different attacks.}
 % \begin{adjustbox}{width=.7\textwidth}{
\begin{tabular}{lcccccc} 
\toprule
\multirow{2}{*}{}    & \multicolumn{3}{c}{AIME 2024} & \multicolumn{3}{c}{AIME-500}  \\ 
\cmidrule(l){2-7}
                     & Original & OOD (Scenario) & OOD (Attack)  & Original & OOD (Scenario) & OOD (Attack)  \\ 
\midrule
o1-preview           & 0.500    & 0.500    & 0.467   & 0.742    & 0.638    & 0.680   \\
o1-mini              & 0.567    & 0.600    & 0.600   & 0.864    & 0.756    & 0.750   \\
o3-mini              & 0.767    & 0.667    & 0.767   & 0.922    & 0.848    & 0.820   \\
Deepseek-R1          & 0.800    & 0.733    & 0.767   & 0.920    & 0.816    & 0.840   \\
GPT-4o               & 0.133    & 0.100    & 0.100   & 0.278    & 0.204    & 0.190   \\
Deepseek-V3          & 0.367    & 0.333    & 0.333   & 0.528    & 0.438    & 0.420   \\ 
\midrule
Mixtral-8x7B-IT-v0.1 & 0.000    & 0.000    & 0.000   & 0.012    & 0.000    & 0.012   \\
Qwen2.5-72B-IT       & 0.200    & 0.167    & 0.200   & 0.432    & 0.290    & 0.392   \\
Qwen2.5-Math-72B-IT  & 0.267    & 0.233    & 0.200   & 0.536    & 0.360    & 0.500   \\
LLAMA3.1-70B-IT      & 0.200    & 0.167    & 0.167   & 0.424    & 0.244    & 0.392   \\ 
\midrule
s1.1-32B             & 0.533    & 0.500    & 0.478   & 0.752    & 0.654    & 0.494   \\
Gemma-2-27B-IT       & 0.033    & 0.033    & 0.000   & 0.062    & 0.028    & 0.062   \\
Gemma-2-9B-IT        & 0.000    & 0.000    & 0.000   & 0.032    & 0.016    & 0.026   \\
LLAMA3.1-8B-IT       & 0.000    & 0.033    & 0.000   & 0.132    & 0.074    & 0.100   \\
% Qwen2.5-Math-7B      & 0.067    & 0.100    & 0.100   & 0.426    & 0.334    & 0.382   \\
Phi-3-mini-4k-IT     & 0.000    & 0.000    & 0.000   & 0.046    & 0.024    & 0.028   \\
LLAMA3.2-3B-IT       & 0.033    & 0.033    & 0.033   & 0.122    & 0.066    & 0.108   \\
\bottomrule
\end{tabular}
% }
%     \end{adjustbox}
        \label{tab:main table2}
\end{table*}

\subsection{Process Reward Models}
In the past, language model training primarily used Outcome-based Reinforcement Models (ORM)~\citep{wang2024openr}. A foundational example is the ORM-based model by~\citet{cobbe2021gsm8k}, which focuses on training evaluators to assess the correctness of answers, providing crucial feedback. Meanwhile, the Process Reward Model (PRM) aims to provide stepwise rewards, offering fine-grained supervision.
DeepMind~\citep{uesato2022solving} supervises both reasoning steps and final results, while OpenAI~\citep{lightman2023let} introduces PRM800K, a human-annotated dataset, emphasizing step verification. \citet{li2022making} enhance result reliability with evaluator models and majority voting. \citet{yu2024ovm} improve reasoning through reinforcement learning with outcome and process supervision. The Generative Reward Model (GenRM)\citep{zhang2024generative} allows rich interaction between evaluators and generators, reflecting a demand for sophisticated process supervision. Recent work~\citep{zheng2024processbench} offers a benchmark for evaluating error identification in mathematical reasoning, fostering scalable oversight research. As a supplement, we propose a dynamic benchmark for testing reasoning capability in LLMs with the help of PRMs.

\subsection{Process Reward Models on Test-time Computation Budget}
\subsubsection{Settings}
% To comprehensively compare the impact of different PRM (Performance Reward Model) methods on model performance
To comprehensively conduct the test-time scaling evaluation for the various PRMs on AIME 2024 and AIME-500, we employ these PRMs: 
% , we fine-tune the qwen2.5-Math-7B model~\citep{yang2024qwen2} to predict the probability of positive, negative, and neutral labels. The fine-tuning is performed on a dataset containing solution prefixes that end with one of our labeled steps. These PRMs are subsequently used as both verifiers and generators to test the results on the newly created AIME-OOD dataset.

% Next, we evaluate a reward model based on its ability to perform a best-of-N search over uniformly sampled solutions from the generator. For each test problem, we select the solution ranked highest by the reward model and automatically grade it based on the final answer. The fraction of correct solutions is then reported. A more reliable reward model will select the correct solution more frequently.

% To facilitate the parsing of individual steps, we train the generator to produce solutions in a newline-delimited, step-by-step format. Specifically, we use few-shot learning to generate solutions to MATH training problems. From this set, we filter the solutions to those that reach the correct final answer and fine-tune the base model on this dataset for a single epoch. This step is not intended to impart new skills to the generator but to ensure that the solutions are formatted in the desired step-by-step manner.


% For process supervision in training our PRMs, we utilize a combination of the PRM800k (\cite{lightman2023let}) and math-shepherd (\cite{wang2024math}) datasets. Each step in these datasets is labeled with a corresponding positive, negative, or neutral classification. A positive label indicates that the step is both correct and reasonable, while a negative label indicates that the step is incorrect. As conceptualized by \cite{lightman2023let}, the PRM training task is framed as a three-class classification problem, where each step is classified as either ``good'', ``neutral'', or ``bad''. However, we observe that there is little difference in performance between binary and three-class classifications, and thus, we treat PRM training as a binary classification task (\cite{wang2024math}).

% Each instance in our dataset consists of three fields: ``input'', ``label'', and ``task'', with each step labeled as either positive or negative. This binary classification approach simplifies the task without sacrificing the effectiveness of the PRM model training.


(1) \textbf{Math-Shepherd-PRM}: 
The scoring mechanism employed in Math-Shepherd is essential for understanding the performance of reasoning steps. In this framework, the token ``kn'' is used to indicate the position where the step score is predicted. A ``+'' token represents a good step, one that contributes positively towards reaching the correct answer, while a ``-'' token signals a bad step. Notably, during the training of PRMs, the loss is computed only at positions marked with ``kn''. The PRM denoted as \( (P \times S \to \mathbb{R}^+) \), assigns a score to each reasoning step \( s \). The model is typically trained using the following binary cross-entropy loss function:
\begin{equation}
\mathcal{L}_{PRM} = \sum_{i=1}^{K} y_{s_i} \log r_{s_i} + (1 - y_{s_i}) \log (1 - r_{s_i}),
\end{equation}


where \( y_{s_i} \) represents the ground-truth label of the \( i \)-th reasoning step \( s_i \), \( r_{s_i} \) is the sigmoid output score predicted by the PRM for step \( s_i \), and \( K \) is the total number of reasoning steps in a given solution \( s \). To estimate the quality of each reasoning step, two methods are employed: hard estimation (HE) and soft estimation (SE). In HE, a step is considered good if it contributes to reaching the correct answer \( a^* \), as defined by:

\begin{equation}
y^{HE}_{s_i} =
\begin{cases} 
1, & \text{if } \exists a_j \in A, a_j = a^* \\
0, & \text{otherwise}.
\end{cases}
\end{equation}

In contrast, SE estimates the quality of a step based on the frequency with which it leads to the correct answer across multiple attempts:

\begin{equation}
y^{SE}_{s_i} = \frac{\sum_{j=1}^{N} \mathbb{I}(a_j = a^*)}{N},
\end{equation}

where \( \mathbb{I}(a_j = a^*) \) is an indicator function that returns 1 if the \( j \)-th attempt \( a_j \) equals the correct answer \( a^* \), and \( N \) is the total number of attempts. Once the labels for each reasoning step are obtained, the PRM is trained using the cross-entropy loss function to optimize the model’s ability to correctly classify each step as either good or bad (\cite{wang2024math}).



(2) \textbf{OpenR-PRM}:
The Problem Resolution Model (PRM) computes a score, denoted as \( p_t \), based on the current problem \( q \) and the sequence of solution steps up to time \( t \), represented as \( [x_1, \dots, x_t] \). This approach allows for a precise and detailed analysis of the solution process, helping identify errors as they occur, as noted by \cite{lightman2023let}. The main goal of PRMs is to assess whether the solution process is on track, with a score \( y_t \) calculated to represent the correctness of the solution at step \( t \), ranging from 0 to 1. This score is given by \( y_t = \text{PRM}(q, x_1, x_2, \dots, x_t) \). During training, the model is framed as a next-token prediction task, where labels are assigned as correct or incorrect, represented by positive (+) or negative (-) tokens. After training, OpenR uses the PRM to evaluate the correctness of each solution step during inference, assigning a score \( r_{PRM_t} \) to each step. Two strategies are employed to compute a final score: the PRM-Min strategy, which selects the minimum score among all steps, \( v = \min\{r_{PRM_t}\}_{t=0}^T \), and the PRM-Last strategy, which takes the score of the final step, \( v = r_{PRM_T} \). PRMs function as dense verifiers, providing strong feedback that can significantly enhance the overall solution process (\cite{wang2024openr}).

(3) \textbf{Skywork-PRM}: We evaluate Skywork o1 Open-PRM-Qwen-2.5-7B~\cite{skyworkopeno12024}, which Extends the capabilities of the 1.5B model by scaling up to handle more demanding reasoning tasks, pushing the boundaries of AI reasoning.

(4) \textbf{Qwen-PRM}: We evaluate 7B version of Qwen-PRM~\cite{zhang2025lessons}. This is a process reward model designed to offer feedback on the quality of reasoning and intermediate steps in mathematical problems. It is part of the Qwen2.5-Math series and has shown impressive performance in identifying errors in reasoning processes. 


% This model is particularly effective in the Best-of-N evaluation and error identification tasks on ProcessBench, outperforming other open-source alternatives



% (3) \textbf{PRM(o1-journey)}: We utilize the dataset released by \textbf{o1-journey}(\cite{o1journey}) and fine-tune the qwen2.5-7b model using the Supervised Fine-Tuning (SFT) method. We then employ the Best-of-N strategy as the search algorithm to benchmark the performance of the four series of PRMs and majority voting.
% The \textbf{Best-of-N} sampling approach generates $N$ outputs in parallel using a base model and selects the answer with the highest score according to a learned process using Process Reward Models (PRMs). This method is similar to previous work that leverages verifiers or reward models \cite{cobbe2021gsm8k, lightman2023let}. While simple, it serves as an effective baseline that improves the performance of large language models (LLMs) by leveraging test-time computation and the result can be found in Figure \ref{fig:analysis}.

% %We preprocess the dataset to create training pairs of input-output sequences that the models can learn from(e.g., a dialogue system, text generation task). 




% (4) \textbf{GenRM}: GenRM(\cite{wu2024autogen}) is a generative validation model that leverages the advantages of large language models (LLMs), such as instruction tuning and reasoning capabilities, to enhance solution verification. It also utilizes additional computational resources at test time, including majority voting, to improve validation accuracy. The model verifies solutions by generating a 'Yes' or 'No' token, maximizing the likelihood of producing the correct token for each case. Specifically, for correct solutions, it maximizes \( \log p_\theta(\text{Yes} \mid (x, y^+)) \), and for incorrect solutions, it maximizes \( \log p_\theta(\text{No} \mid (x, y^-)) \). During training, the model receives problem-solution pairs labeled with 'Yes' for correct solutions and 'No' for incorrect solutions, which are represented as \( D_{\text{Direct}} = \{(x, y^+, I), \text{Yes}\} \cup \{(x, y^-, I), \text{No}\} \). At inference time, the model uses the likelihood of generating the 'Yes' token to rank solutions, with the score reflecting its confidence in the correctness of the solution: \( r_{\text{Direct}}(x, y) = p_\theta(\text{Yes} \mid x, y, I) \). To train the model, GenRM integrates both solution generation and verification into a unified loss function, which promotes positive transfer between the two tasks. This combined loss function is given by \( L_{\text{GenRM}}(\theta) = L_{\text{SFT}}(\theta, D_{\text{verify}}) + \lambda L_{\text{SFT}}(\theta, D_{\text{correct}}) \), where \( \lambda > 0 \) controls the balance between the verification and generation tasks. Additionally, to address the challenge of acquiring human-generated rationales for solution verification, GenRM uses synthetic verification rationales generated by an LLM. These rationales are filtered based on their correctness using reference-guided grading, which improves the quality of synthetic rationales and the model's verification accuracy during training. However, reference-guided grading is only applied during training, as reference solutions are not available for test problems.


As shown in Figure \ref{fig:analysis}, the model's performance improves on the AIME 2024 OOD and AIME-500 OOD dataset as the test-time computation budget increases and the best-of-N performance of each reward model varies as a function of N. Since majority voting is known to be a strong baseline(\cite{lewkowycz2022solving, wang2022self}), we also include this method as a point of comparison. We compared a series of PRM methods and then observed the performance changes of various PRM methods as N increased. 
% All four series of PRM methods are more effective than majority voting.

% Let's focus more closely on the figure \ref{fig:analysis}. All methods demonstrate an improvement in problem-solving performance as the test-time computation budget $N$ increases, corroborating the Test-time Scaling Law. 
% While Skywork-PRM achieves the best performance across all computation budgets, showing a steepeN values. PRM (o1journey) closely follows Math-Shepherd and consistently outperforms the remaining methods and PRM (OpenR) shows moderate performance, improving steadily with increasing N, but its improvement rate is slower compared to Math-Shepherd and PRM(o1journey).GenRM achieves lower performance, with improvements being less pronounced compared to top-performing methods. Besides, Majority Voting consistently underperforms all other methods, even with increased computation budgets, showing limited scalability.

Skywork-PRM and Qwen-PRM achieve relatively high performance across all computation budgets. OpenR shows moderate performance, improving steadily with increasing N, but its improvement rate is slower compared to Skywork-PRM and Qwen-PRM. Majority Voting mainly underperforms other methods, even with increased computation budgets, showing limited scalability.
The possible underlying reasons are these: (1)Model-specific Capabilities: The superior performance of Skywork-PRM and Qwen-PRM may result from their advanced mechanisms for aggregating or utilizing the increased number of solutions, likely benefiting from better exploration of the solution space or more robust voting schemes. (2)Algorithmic Limitations: The suboptimal performance of Majority Voting suggests its inability to effectively utilize additional solutions. 
% For Majority Voting, this may stem from the lack of sophisticated reasoning mechanisms, while GenRM might suffer from insufficient diversity or quality in the generated solutions. 
(3)Dataset Complexity: The AIME 2024 OOD dataset and AIME-500 OOD dataset likely require nuanced reasoning and adaptability, favoring methods like Skywork-PRM and Qwen-PRM that can better handle out-of-distribution (OOD) generalization tasks. Thus, the results highlight the importance of adopting advanced techniques that can efficiently utilize increased test-time computation budgets. Methods like Skywork-PRM and Qwen-PRM demonstrate superior scaling behavior, suggesting their robustness and adaptability in solving complex problems. 
% In contrast, traditional approaches such as Majority Voting exhibit limited scalability, likely due to its inability to fully exploit the increased computation resources.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\columnwidth]{Graph/gpqa_2.pdf}
%     \caption{
%     The performance gap between OOD and ID test on GPQA Diamond. 
%     }
%     \label{fig:gpqa_bar} 
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{Graph/gpqa_1.pdf}
    \caption{
   Scientific Questions Gap on GPQA Diamond.
    }
    \label{fig:gpqa_line}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{Graph/mmlu_1.pdf}
    \caption{
    Language Understanding Gap on MMLU 570. 
    }
    \label{fig:mmlu_line}
\end{figure}




% than simply memorize.



\begin{table*}
\small
\centering
% \scalebox{0.95}{
\begin{tabular}{l|l} 
\toprule
\multicolumn{2}{l}{\begin{tabular}[c]{@{}p{0.96\linewidth}@{}}
\textbf{Original Question}: Carruthers approaches questions about the moral standing of nonhuman animals from the perspective of which of the following moral theories?\\
\textbf{Original Choices}: A: consequentialism B: Kantianism C: virtue ethics D: contractualism\\
\textbf{Original Answer}: D\\
\textbf{OOD Question (Scenario-level Semi-fact Question)}: In his exploration of ethical frameworks, Carruthers considers various moral theories before honing in on a particular approach to evaluate the moral status of nonhuman animals. From which moral theory does Carruthers analyze the ethical significance of these animals?\\
\textbf{OOD Choices}: A: ethics of character B: contractualism C: outcomes-based ethics D: Kantianism\\
\textbf{OOD Answer}: B
\end{tabular}} \\ 
\midrule
\begin{tabular}[c]{@{}p{0.46\linewidth}@{}}
\sethlcolor{yellow}\hl{\textbf{GPT-4o on Original:}}\\
Peter Carruthers is known for approaching questions about the moral standing of nonhuman animals from the perspective of contractualism. Contractualism is a moral theory that determines moral principles based on what rational agents would agree upon as part of a social contract. Carruthers argues that nonhuman animals do not have direct moral standing within contractualism because they are not capable of participating in the contract-making process. Therefore, the correct answer is \textbf{D}.

\end{tabular} & \begin{tabular}[c]{@{}p{0.46\linewidth}@{}}
\sethlcolor{yellow}\hl{\textbf{Llama3.1-70B-IT on Original:}}\\
Carruthers approaches questions about the moral standing of nonhuman animals from the perspective of contractualism. Contractualism is a moral theory that emphasizes the importance of mutual agreements and contracts in determining moral obligations. In the context of nonhuman animals, contractualism would suggest that animals have moral standing because they are capable of participating in a ``contract'' or social agreement with humans, even if it is not a formal or explicit one. The correct answer is \textbf{D}.
\end{tabular} \\ 
\midrule
\begin{tabular}[c]{@{}p{0.5\linewidth}@{}}
\sethlcolor{yellow}\hl{\textbf{GPT-4o on OOD:}}\\ Carruthers is known for utilizing Kantianism as a framework for evaluating the moral status of nonhuman animals. This approach emphasizes the importance of rationality and moral agency, aligning with Kantian ethical principles. Therefore, the correct answer is \textbf{D}.
\end{tabular} & \begin{tabular}[c]{@{}p{0.46\linewidth}@{}}
\sethlcolor{yellow}\hl{\textbf{Llama3.1-70B-IT on OOD:}}\\
Based on the information provided, the correct answer is \textbf{C}: outcomes-based ethics.
%\textbf{Host:} \textbf{\{Puzzle\}~\{Rules Introduction\}} Please begin your questions.\\\textbf{Player:} Question 1: What happened?\\\textbf{Host:} Response 1: Irrelevant\\\textbf{Player:} Question 2: How many people were there in the boat?\\\textbf{Host:} Response 2: Irrelevant\\\textbf{Player:} Question 3: Were any life jackets worn?\\\textbf{Host:} Response 3: Irrelevant\\\textbf{Player:} Question 4: Was anyone drunk while operating the ship? \\\textbf{Host:} Response 4: No\\\textbf{Player:} Question 5: Did someone accidentally hit something beneath the water without realizing it? \\\textbf{Host:} Response 5: No\\\textbf{Player:} \ul{Answer: The reason why the passenger steamboat overturned was because some object fell into the hull below the surface and cracked the bottom of the vessel, causing rapid flooding and capsizing.}
\end{tabular} \\
% \midrule

\bottomrule
\end{tabular}
\caption{Case Study on MMLU 570.}
\label{table:samples_mmlu}
\end{table*}
% \midrule
% % }
% \begin{tabular}{l|l} 
% \toprule

\begin{table*}
\small
\centering
\scalebox{0.92}{
\begin{tabular}{l|l} 
\toprule
\multicolumn{2}{l}{\begin{tabular}[c]{@{}p{0.96\linewidth}@{}}
\textbf{Original Question}: There exist real numbers $x$ and $y$, both greater than 1, such that $\log_x(y^x) = \log_y(x^{4y}) = 10$. Find $xy$.\\
% \textbf{Original Choices}: A: consequentialism B: Kantianism C: virtue ethics D: contractualism\\
\textbf{Original Answer}: 25\\
\textbf{OOD Question (Scenario-level Semi-fact Question)}: Consider two species of plants with growth rates represented by real numbers $x$ and $y$, both exceeding 1, such that the growth rate of one species raised to the power of the other results in the equation $\log_x(y^x) = \log_y(x^{4y}) = 10$. Determine the product of their growth rates, $xy$.\\
% \textbf{OOD Choices}: A: ethics of character B: contractualism C: outcomes-based ethics D: Kantianism\\
\textbf{OOD Answer}: 25
\end{tabular}} \\ 
\midrule
\begin{tabular}[c]{@{}p{0.46\linewidth}@{}}
\sethlcolor{yellow}\hl{\textbf{GPT-4o on Original:}}\\
\[
\log_x(y^x) = 10
\]
\[
\log_y(x^{4y}) = 10
\]
We'll solve for \(xy\).
Starting with the first equation:
\[
\log_x(y^x) = 10
\]
This can be rewritten using the logarithm power rule:
\[
x \log_x y = 10
\]
Thus,
\[
\log_x y = \frac{10}{x}
\]
The base change formula \(\log_x y = \frac{\log y}{\log x}\) gives us:
\[
\frac{\log y}{\log x} = \frac{10}{x}
\]
So, 
\[
\log y = \frac{10}{x} \log x
\]
Now, for the second equation:
\[
\log_y(x^{4y}) = 10
\]
This expands to:
\[
4y \log_y x = 10
\]
Thus,
\[
\log_y x = \frac{10}{4y} = \frac{5}{2y}
\]
The base change formula \(\log_y x = \frac{\log x}{\log y}\) gives us:
\[
\frac{\log x}{\log y} = \frac{5}{2y}
\]
So,
\[
\log x = \frac{5}{2y} \log y
\]
Now substitute \(\log y\) from the first equation into this:
\[
\log x = \frac{5}{2y} \times \frac{10}{x} \log x
\]
Simplifying,
\[
\log x = \frac{50}{2xy} \log x
\]
Cancel \(\log x\) from both sides (assuming \(\log x \neq 0\)):
\[
1 = \frac{50}{2xy}
\]
Thus,
\[
2xy = 50
\]
So,
\[
xy = 25
\]
The value of \(xy\) is 
\(
\textbf{25}
\).
\end{tabular} & \begin{tabular}[c]{@{}p{0.46\linewidth}@{}}
\sethlcolor{yellow}\hl{\textbf{GPT-4o on OOD:}}\\
% To solve the given problem, we need to analyze the equations provided:

% 1. \(\log_x(y^x) = 10\)
% 2. \(\log_y(x^{4y}) = 10\)

% Let's solve these step by step:

Step 1: Solve the first equation

The first equation is:
\[
\log_x(y^x) = 10
\]

This implies:
\[
y^x = x^{10}
\]

Step 2: Solve the second equation

The second equation is:
\[
\log_y(x^{4y}) = 10
\]

This implies:
\[
x^{4y} = y^{10}
\]

Step 3: Express \(y\) in terms of \(x\) from the first equation

From \(y^x = x^{10}\), we can express \(y\) as:
\[
y = x^{10/x}
\]

Step 4: Substitute \(y\) in the second equation

Substitute \(y = x^{10/x}\) into the second equation \(x^{4y} = y^{10}\):
\[
x^{4(x^{10/x})} = (x^{10/x})^{10}
\]

Simplifying the right side:
\[
x^{4(x^{10/x})} = x^{100/x}
\]

Step 5: Equate the exponents

Since the bases are the same, equate the exponents:
\[
4 \cdot x^{10/x} = \frac{100}{x}
\]

Step 6: Solve for \(x\)

Rearrange and simplify:
\[
4x^{10/x} \cdot x = 100
\]
\[
4x^{1 + 10/x} = 100
\]
\[
x^{1 + 10/x} = 25
\]

Step 7: Solve for \(x\) using trial and error or numerical methods

Let \(z = 1 + \frac{10}{x}\), so \(x^z = 25\). We need to find \(x\) such that:
\[
z = 1 + \frac{10}{x} \quad \text{and} \quad x^z = 25
\]

By trial and error or numerical methods, you can find that \(x = 5\) satisfies the equation.

Step 8: Find \(y\)

Now, substitute \(x = 5\) back into \(y = x^{10/x}\):
\[
y = 5^{10/5} = 5^2 = 25
\]

Step 9: Calculate the product \(xy\)

Finally, calculate the product:
\[
xy = 5 \cdot 25 = 125
\]

Thus, the product of their growth rates is \textbf{400}.

\end{tabular} \\ 

\bottomrule
\end{tabular}}
\caption{Case Study on AIME 2024.}
\label{table:samples_AIME}
\end{table*}

\begin{table*}
\small
\centering
\scalebox{0.94}{
\begin{tabular}{l} 
\toprule
\multicolumn{1}{l}{\begin{tabular}[c]{@{}p{0.96\linewidth}@{}}
\textbf{Original Question}: cyclohexanone is treated with LDA at low temperature, followed by benzaldehyde. the reaction mixture is then acidified, forming product 1. 1 is then treated with an excess of diethylaminosulfur trifluoride, forming product 2. what is the structure of prodcut 2?\\
\textbf{Original Choices}: A: ((R)-((R)-2,2-difluorocyclohexyl)fluoromethyl)benzene B: (S)-2-((R)-fluoro(phenyl)methyl)cyclohexan-1-one C: (2R)-1-fluoro-2-((S)-fluoro(phenyl)methyl)cyclohexan-1-ol D: ((S)-((R)-2,2-difluorocyclohexyl)fluoromethyl)benzene\\
\textbf{Original Answer}: A\\
\textbf{OOD Question (Scenario-level Semi-fact Question)}: In the realm of organic synthesis, understanding the behavior of reagents under various conditions is crucial for designing effective synthetic routes. Let's consider a scenario where cyclohexanone, a common ketone, is reacted with lithium diisopropylamide (LDA), a strong base, at reduced temperature to form an enolate, which is then treated with benzaldehyde. This reaction typically forms an aldol product, which, upon acidification, yields a compound known as product 1. Further exploring the reactivity of such aldol products, if this product 1 is treated with an excess of diethylaminosulfur trifluoride, a reagent known for its ability to affect sulfur-based modifications in organic molecules, what would be the structure of the resulting product 2?\\
\textbf{OOD Choices}: A: ((S)-((R)-2,2-difluorocyclohexyl)fluoromethyl)benzene B: ((R)-((R)-2,2-difluorocyclohexyl)fluoromethyl)benzene C: (2R)-1-fluoro-2-((S)-fluoro(phenyl)methyl)cyclohexan-1-ol D: (S)-2-((R)-fluoro(phenyl)methyl)cyclohexan-1-one\\
\textbf{OOD Answer}: B
\end{tabular}} \\ 
\midrule
\begin{tabular}[c]{@{}p{\linewidth}@{}}
\sethlcolor{yellow}\hl{\textbf{o1-preview on Original:}}\\
To solve this problem, let's break down the reactions step by step. **First Reaction:** Cyclohexanone is treated with LDA (Lithium Diisopropylamide) at low temperature. LDA is a strong, non-nucleophilic base that deprotonates the most accessible $\alpha$-hydrogen to form the kinetic enolate. In cyclohexanone, both $\alpha$-carbons are equivalent due to symmetry, so the enolate forms at one of these positions. The enolate then attacks benzaldehyde in an aldol addition reaction. The nucleophilic carbon of the enolate adds to the electrophilic carbonyl carbon of benzaldehyde, forming a new carbon-carbon bond. After protonation, this gives a $\beta$-hydroxy ketone (product 1). **Second Reaction:** Product 1, the $\beta$-hydroxy ketone, is treated with an excess of diethylaminosulfur trifluoride (DAST). DAST is a fluorinating agent that can convert alcohols to alkyl fluorides and ketones to geminal difluorides. In this case, DAST converts: - The hydroxyl group at the $\beta$-position into a fluorine atom. - The ketone group into a geminal difluoride. The overall result is a cyclohexane ring with: - Two fluorine atoms attached to C1 (from the ketone conversion). - A fluorine atom and a benzyl group attached to C2 (from the hydroxyl conversion). This gives us a product where: - C1 is now a difluoromethylene group (\(\text{CF}_2\)). - C2 has a fluorine and a benzyl group, with specific stereochemistry retained from the initial aldol addition. Considering the stereochemistry: - The initial aldol addition creates a new chiral center at C2. Since the starting materials are achiral and no chiral catalysts are used, a racemic mixture is formed. - The DAST reaction retains the configuration at C2 when converting the hydroxyl group to fluorine. - The former carbonyl carbon (C1) becomes a new chiral center after forming the geminal difluoride. From the options provided, option **A** corresponds to the product formed: - **Option A:** ((R)-((R)-2,2-difluorocyclohexyl)fluoromethyl)benzene This option indicates that: - The cyclohexane ring has two fluorines at C2 (now numbered as C1 in the name), making it a 2,2-difluorocyclohexyl group. - There is a fluoromethyl group attached to benzene, corresponding to the fluorine and benzyl group at C2. - Both chiral centers have the **R** configuration, matching the stereochemistry of the reactions. Thus, the correct answer is: \textbf{A}
\end{tabular} \\ 

\midrule
\begin{tabular}[c]{@{}p{\linewidth}@{}}
\sethlcolor{yellow}\hl{\textbf{o1-preview on OOD:}}\\
To solve this problem, let's systematically break down each step of the reaction and understand the transformations that occur. **Step 1: Formation of the Aldol Product (Product 1)** 1. **Formation of the Enolate Ion:** Cyclohexanone is treated with lithium diisopropylamide (LDA), a strong, non-nucleophilic base. LDA deprotonates the $\alpha$-carbon of cyclohexanone, forming the enolate ion. 2. **Aldol Addition Reaction:** The enolate ion is nucleophilic at the $\alpha$-carbon and attacks the electrophilic carbonyl carbon of benzaldehyde. This forms a new carbon-carbon bond between the $\alpha$-carbon of cyclohexanone and the carbonyl carbon of benzaldehyde. 3. **Acidification:** The alkoxide formed after the addition is protonated upon acidification, yielding a $\beta$-hydroxy ketone. The **structure of Product 1** is therefore a cyclohexanone ring with a hydroxyl group (OH) and a phenyl group (\(\text{C}_6\text{H}_5\)) attached to the same carbon (the $\alpha$-carbon). This gives us **2-hydroxy-2-phenylcyclohexanone**. **Step 2: Reaction with Diethylaminosulfur Trifluoride (DAST)** DAST is a fluorinating agent known to convert alcohols to alkyl fluorides (with inversion of configuration) and carbonyl groups to gem-difluorides. However, under standard conditions, ketones are less reactive than alcohols towards DAST, and selective fluorination of the alcohol can occur. In this case, when Product 1 is treated with excess DAST: 1. **Fluorination of the Alcohol:** The hydroxyl group is converted to a fluorine atom with inversion of configuration. This transforms the chiral center at the $\alpha$-carbon (which was R or S in the aldol product) into the opposite configuration due to the inversion during the SN2-like process. 2. **Ketone Retention:** The ketone group remains largely unreacted under these conditions because ketones are less reactive towards DAST compared to alcohols, especially in the presence of steric hindrance and electronic effects from adjacent groups (like the phenyl group). Thus, **Product 2** is **2-((R)-fluoro(phenyl)methyl)cyclohexan-1-one**, where the hydroxyl group has been replaced by a fluorine atom, and the configuration at the chiral center has inverted compared to Product 1. **Matching with the Given Options:** Option D corresponds exactly to this structure: - **(S)-2-((R)-fluoro(phenyl)methyl)cyclohexan-1-one** This indicates that the final product has: - A cyclohexanone ring (ketone at position 1) - A fluorine atom and a phenyl group attached to the same carbon at position 2 - Specific stereochemistry indicating the inversion at the $\alpha$-carbon Therefore, **Option D is the correct answer**.\textbf{D}.
\end{tabular} \\ 


%\begin{tabular}[c]{@{}p{\linewidth}@{}}
%\sethlcolor{yellow}\hl{\textbf{Llama3.1-70B-IT on Original:}}\\ 
%To solve this problem, let's break it down step by step. 1. **Step 1: Reaction of cyclohexanone with LDA and benzaldehyde**   Cyclohexanone is treated with LDA (lithium diisopropylamide), a strong base, at low temperature. This will form an enolate ion from the cyclohexanone. The enolate then reacts with benzaldehyde in a nucleophilic addition reaction to form a new carbon-carbon bond, resulting in the formation of a secondary alcohol after acidification. This product is **1**.   The reaction sequence is as follows:   Cyclohexanone → Enolate (with LDA) → Addition to benzaldehyde → Acidification → **1**   The structure of **1** would be a cyclohexanone derivative with a phenyl group attached to the alpha carbon (the carbon next to the carbonyl group) and a hydroxyl group on the same carbon, resulting from the addition of the enolate to the aldehyde and subsequent acidification.   So, **1** is likely (S)-2-((R)-hydroxy(phenyl)methyl)cyclohexan-1-one.2. **Step 2: Reaction of 1 with diethylaminosulfur trifluoride (DAST)**   DAST is a reagent used for the conversion of alcohols to fluorides. When **1** reacts with an excess of DAST, the hydroxyl group in **1** will be replaced by a fluorine atom, resulting in the formation of **2**.   Given the structure of **1**, the reaction with DAST would replace the hydroxyl group with a fluorine atom, leading to the structure of **2**.   Therefore, the structure of **2** would be (S)-2-((R)-fluoro(phenyl)methyl)cyclohexan-1-one.The correct answer is \textbf{B}.
%\end{tabular} \\ 

%\midrule
%\begin{tabular}[c]{@{}p{\linewidth}@{}}
%\sethlcolor{yellow}\hl{\textbf{Llama3.1-70B-IT on OOD:}}\\
%Based on the information provided, the correct answer is \textbf{C}: outcomes-based ethics.
%\textbf{Host:} \textbf{\{Puzzle\}~\{Rules Introduction\}} Please begin your questions.\\\textbf{Player:} Question 1: What happened?\\\textbf{Host:} Response 1: Irrelevant\\\textbf{Player:} Question 2: How many people were there in the boat?\\\textbf{Host:} Response 2: Irrelevant\\\textbf{Player:} Question 3: Were any life jackets worn?\\\textbf{Host:} Response 3: Irrelevant\\\textbf{Player:} Question 4: Was anyone drunk while operating the ship? \\\textbf{Host:} Response 4: No\\\textbf{Player:} Question 5: Did someone accidentally hit something beneath the water without realizing it? \\\textbf{Host:} Response 5: No\\\textbf{Player:} \ul{Answer: The reason why the passenger steamboat overturned was because some object fell into the hull below the surface and cracked the bottom of the vessel, causing rapid flooding and capsizing.}
%\end{tabular} \\
\bottomrule
\end{tabular}}
\caption{Case Study on GPQA.}
\label{table:samples_gpqa}
\end{table*}

\section{ThinkBench on Language Understanding}
%如图X，我们同样展示了使用ThinkBench对570条MMLU进行动态改写，OOD与ID之间在不同模型上存在着较大差距，o1-preview在这种更偏知识性的数据上性能更好，且o1-preview和o1-mini这种test-time computing models相较于GPT-4o这种train-time computing models鲁棒性更好。
As illustrated in Figure \ref{fig:mmlu_line}, we also present the results of dynamically constructing 570 MMLU OOD data using ThinkBench. There is a significant performance gap between Out-Of-Distribution (OOD) and In-Distribution (ID) data across different models. The o1-preview model demonstrates superior performance on this knowledge-intensive dataset. Furthermore, reasoning models like o1-preview and o1-mini exhibit greater robustness compared to non-reasoning models such as GPT-4o.
% We also provide the result of 
\section{Case Study}
Table~\ref{table:samples_mmlu}, Table~\ref{table:samples_AIME} and Table~\ref{table:samples_gpqa} show the case of different models tackling ThinkBench, including question, choices, answer in Original set and OOD set, and responses from models.

Through the provided example in Table~\ref{table:samples_mmlu}, we observe that our dynamic construction transforms the original question into a new question, with the options also being rephrased and reordered. Our benchmark reduces the likelihood of models achieving high scores through rote memorization. In the responses from GPT-4o and Llama3.1-70B-IT, it is evident that while both models correctly answer the original question, they fail to provide correct responses to the corresponding OOD data. Both models provide detailed analyses and correct answers to the original test data. However, for the OOD data, Llama3.1-70B-IT does not offer a detailed analysis and instead directly gives an incorrect answer. Although GPT-4o conducts some analysis, it confuses the concepts of ``Kantianism'' and ``contractualism'', even mixing them up during the reasoning process, ultimately leading to an incorrect answer.

\subsection{Instruction for Human Annotation}
% Instruction for Annotating Math Problem Consistency
This task involves checking if the modified question and the original answer are consistent. Follow the steps below: (1) Read the questions and the original answer: Carefully read the modified question, the original question, and the original answer. (2) Identify Key Changes: Note any changes in numbers, operations, or conditions between the original and modified questions. (3) Verify Consistency: Check if the original answer is consistent with the modified question. Mark as ``Consistent'' if it does, otherwise mark it as ``Inconsistent''.
\end{document}
