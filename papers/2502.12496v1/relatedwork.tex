\section{Related work}
There has been an explosion of works on the theory of DNNs in the last few
years. In this paper we focus on the scenario where the target functions
are smooth, but have very many variables and are costly to evaluate. We
assume that there is a sequence $(b_j)_{j\ge 1}$ which describes the
regularity features of the target function and there is a summability
exponent $p^* \in (0,1)$ for which $\sum_{j\ge 1} b_j^{p^*} < \infty$, as
mentioned above. This is very similar to the setting of the recent review
\cite{ABDM24} which assumes that the target functions are
\emph{holomorphic} (see e.g., \cite{SZ19}): there is a sequence
$(b_j)_{j\ge 1}$ which controls the \emph{anisotropy} of the target
functions, and the results were stated in terms of the summability
exponent $p^*$.

As explained in \cite{ABDM24}, much of the recent research on the
approximation theory of DNNs aim to establish \emph{existence theorems}:
they prove the \emph{expressivity} or \emph{universality} of DNNs, i.e.,
the existence of DNNs of a certain depth and width that approximate a
class of functions to a prescribed accuracy. Such proofs are typically
achieved by \emph{emulation}, i.e., by showing that the network parameters
can be handcrafted so that the DNNs become arbitrarily close to a known
approximation method such as the \emph{best $N$-term approximation}. For
example, for the class of holomorphic functions mentioned above, there
exist DNNs of width $\calO(N^2)$ and depth $\calO(\log(N))$ that achieve
the near-optimal generalization errors of $\calO(N^{-(1/p^*-1/2)})$, see
e.g., \cite[Theorem~7.2]{ABDM24}.

There is however still a significant gap between the approximation theory
of DNNs and their practical performance (see e.g., \cite{AD21,GV24}). Some
papers tried to narrow this gap by developing \emph{practical existence
theory} (see e.g., \cite{ABDM25}) which takes into account the practical
training strategies in minimizing a loss function, including the existence
of a good regularization parameter, see e.g., \cite[Theorem~8.1]{ABDM24}.
For further related literature we refer the readers to the review
\cite{ABDM24} and the references therein.

The review \cite{ABDM24} is devoted to the situation where the learning
methods are independent of the sequence $(b_j)_{j\ge1}$; it is referred to
as the ``unknown anisotropy setting''. In contrast, in this paper we
propose to make explicit use of our knowledge of the sequence
$(b_j)_{j\ge1}$ in our construction of lattice training points as well as
in the design of our practical regularization term. This is a realistic
setting in the examples of parametric PDEs in uncertainty quantification,
since we know explicitly how the random fields are modeled.

``Exploding and vanishing gradients'' in neural network training have been recognized early
on, leading to various approaches to address the training stability issue. Among these, the
most prominent ones are the introduction of gating to enable a constant error flow
during training \cite{LSTM97}, identity mappings \cite{StableNN,ResNet1,ResNet2}
as a simplistic and most natural way of ensuring constant error flow, and numerous normalization
techniques, for example, the currently most popular layer normalization \cite{LayerNorm} to normalize the
inputs to a neuron.

In this paper, we are interested in understanding and developing the basic theory of multi-layer perceptrons (DNNs). Hence, we leave the investigation
of architecture modification for future work and instead turn to the design of the loss function.
Equipped with the regularity bounds in Section~\ref{sec:reg} and error analysis in Section~\ref{sec:err},
we can tailor a loss function that improves multi-layer perceptrons without architectural changes.