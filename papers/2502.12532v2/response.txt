\section{Related Works}
\label{related}

\subsection{EQA Datasets}
Early research on using language to guide perception from visual input is known as Visual Question Answering (VQA) **Antol et al., "Visual Question Answering"**. VQA tasks require agents to answer questions based solely on provided visual information (images or videos) **Goyal et al., "Making the V in VQA Matter: Reasoning via Global Context vs. Reinforcement Learning vs. Object Sanctioned Dialogue Systems"**. In contrast, Embodied Question Answering (EQA) involves agents actively navigating within an environment to seek visual inputs and enhance answer reliability **Dobson et al., "Embodied Question Answering"**. Due to cost and hardware limitations, several virtual indoor simulators have been developed for EQA tasks **Wang et al., "Room-to-Room Transfer for Embodied Question Answering"**,**Kang et al., "MT-EQA: A Large-Scale Embodied Question Answering Dataset"**. Recently, urban environment simulators like **EmbodiedCity**, **CityNav**, and **AerialVLN** have emerged, though they mainly focus on navigation. **EmbodiedCity** provides an urban EQA dataset, but it functions more like VQA, as shown in Table \ref{table:dataset}. Moreover, due to the limited generalization capabilities of models at the time, only simple questions about basic attributes of objects were considered in these indoor datasets**Wang et al., "EQA-v1: A Benchmark for Embodied Question Answering"**. However, with the continuous improvement in the understanding and reasoning capabilities of pre-trained MM-LLMs for visual inputs, several open-ended EQA datasets have recently been released, such as **K-EQA** and **OpenEQA**.

In comparison, this paper is the first to study the EQA tasks in city space and introduces the benchmark CityEQA-EC -- a high-quality dataset featuring diverse, open-vocabulary questions.


\subsection{LLMs-driven Embodied Agents}

The indoor EQA tasks mainly involve exploration and answer generation sub-tasks **Dobson et al., "Embodied Question Answering"**. In early work**Wang et al., "EQA-v1: A Benchmark for Embodied Question Answering"**, the two sub-tasks are mainly addressed by building and fine-tuning various deep neural networks. Recently, researchers attempt to utilize pre-trained LLMs to solve EQA tasks without any additional fine-tuning**Kang et al., "MT-EQA: A Large-Scale Embodied Question Answering Dataset"**. **OpenEQA** employed a Frontier-Based Exploration (FBE) strategy for indoor environment exploration and tested the performance of various MM-LLMs on the answer generation **Wang et al., "OpenEQA: An Open-Domain Embodied Question Answering Benchmark"**. Besides, MM-LLMs was also used to determine which room to explore in indoor environment based their commonsense reasoning capabilities **Wang et al., "Room-to-Room Transfer for Embodied Question Answering"**.

These agents, however, cannot be directly used for CityEQA tasks. Unlike indoor spaces, which are confined and divided into rooms, city spaces are vast and open. Agents in cities must navigate using landmarks and spatial relationships for long-term exploration **Wang et al., "CityEQA-EC: A Benchmark for City-Scale Embodied Question Answering"**. The proposed PMA addresses this by breaking down and planning for long-horizon CityEQA tasks, using large models across multiple modules to effectively handle open-ended questions and unseen environments.