@inproceedings{chandrasegaranhourvideo,
  title={HourVideo: 1-Hour Video-Language Understanding},
  author={Chandrasegaran, Keshigeyan and Gupta, Agrim and Hadzic, Lea M and Kota, Taran and He, Jimming and Eyzaguirre, Cristobal and Durante, Zane and Li, Manling and Wu, Jiajun and Fei-Fei, Li},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@inproceedings{das2018embodied,
  title={Embodied question answering},
  author={Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--10},
  year={2018}
}

@article{duan2022survey,
  title={A survey of embodied ai: From simulators to research tasks},
  author={Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={6},
  number={2},
  pages={230--244},
  year={2022},
  publisher={IEEE}
}

@article{gao2024embodiedcity,
  title={EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment},
  author={Gao, Chen and Zhao, Baining and Zhang, Weichen and Mao, Jinzhu and Zhang, Jun and Zheng, Zhiheng and Man, Fanhang and Fang, Jianjie and Zhou, Zile and Cui, Jinqiang and others},
  journal={arXiv preprint arXiv:2410.09604       }    ,
  year={2024}
}

@inproceedings{guo2023images,
  title={From images to textual prompts: Zero-shot visual question answering with frozen large language models},
  author={Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10867--10877},
  year={2023}
}

@article{huang2024manipvqa,
  title={Manipvqa: Injecting robotic affordance and physically grounded information into multi-modal large language models},
  author={Huang, Siyuan and Ponomarenko, Iaroslav and Jiang, Zhengkai and Li, Xiaoqi and Hu, Xiaobin and Gao, Peng and Li, Hongsheng and Dong, Hao},
  journal={arXiv preprint arXiv:2403.11289}  ,
  year={2024}
}

@article{ishmam2024image,
  title={From image to language: A critical analysis of visual question answering (vqa) approaches, challenges, and opportunities},
  author={Ishmam, Md Farhan and Shovon, Md Sakib Hossain and Mridha, Muhammad Firoz and Dey, Nilanjan},
  journal={Information Fusion},
  pages={102270},
  year={2024},
  publisher={Elsevier}
}

@article{lee2024citynav,
  title={CityNav: Language-Goal Aerial Navigation Dataset with Geographic Information},
  author={Lee, Jungdae and Miyanishi, Taiki and Kurita, Shuhei and Sakamoto, Koya and Azuma, Daichi and Matsuo, Yutaka and Inoue, Nakamasa},
  journal={arXiv preprint arXiv:2406.14240}      ,
  year={2024}
}

@inproceedings{liu2023aerialvln,
  title={Aerialvln: Vision-and-language navigation for uavs},
  author={Liu, Shubo and Zhang, Hongsheng and Qi, Yuankai and Wang, Peng and Zhang, Yanning and Wu, Qi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15384--15394},
  year={2023}
}

@article{liu2024aligning,
  title={Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI},
  author={Liu, Yang and Chen, Weixing and Bai, Yongjie and Li, Guanbin and Gao, Wen and Lin, Liang},
  journal={CoRR},
  year={2024}
}

@article{liu2024navagent,
  title={NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation},
  author={Liu, Youzhi and Yao, Fanglong and Yue, Yuanchang and Xu, Guangluan and Sun, Xian and Fu, Kun},
  journal={arXiv preprint arXiv:2411.08579}  ,
  year={2024}
}

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{majumdar2024openeqa,
  title={Openeqa: Embodied question answering in the era of foundation models},
  author={Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16488--16498},
  year={2024}
}

@article{mu2024embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{ren2024explore,
  title={Explore until Confident: Efficient Exploration for Embodied Question Answering},
  author={Ren, Allen Z and Clark, Jaden and Dixit, Anushri and Itkina, Masha and Majumdar, Anirudha and Sadigh, Dorsa},
  booktitle={First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024},
  year={2024}
}

@article{tan2023knowledge,
  title={Knowledge-based embodied question answering},
  author={Tan, Sinan and Ge, Mengmeng and Guo, Di and Liu, Huaping and Sun, Fuchun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={10},
  pages={11948--11960},
  year={2023},
  publisher={IEEE}
}

@article{xiang2024language,
  title={Language models meet world models: Embodied experiences enhance language models},
  author={Xiang, Jiannan and Tao, Tianhua and Gu, Yi and Shu, Tianmin and Wang, Zirui and Yang, Zichao and Hu, Zhiting},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{yinsgsg-nav,
  title={SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation},
  author={Yin, Hang and Xu, Xiuwei and Wu, Zhenyu and Zhou, Jie and Lu, Jiwen},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2025}
}

@inproceedings{yu2019multi,
  title={Multi-target embodied question answering},
  author={Yu, Licheng and Chen, Xinlei and Gkioxari, Georgia and Bansal, Mohit and Berg, Tamara L and Batra, Dhruv},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6309--6318},
  year={2019}
}

@article{zeng2024perceive,
  title={Perceive, reflect, and plan: Designing llm agent for goal-directed city navigation without instructions},
  author={Zeng, Qingbin and Yang, Qinglong and Dong, Shunan and Du, Heming and Zheng, Liang and Xu, Fengli and Li, Yong},
  journal={arXiv preprint arXiv:2408.04168}  ,
  year={2024}
}

