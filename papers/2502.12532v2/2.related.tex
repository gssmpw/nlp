\section{Related Works}
\label{related}

\subsection{EQA Datasets}
Early research on using language to guide perception from visual input is known as Visual Question Answering (VQA) \cite{ishmam2024image, guo2023images}. VQA tasks require agents to answer questions based solely on provided visual information (images or videos) \cite{chandrasegaranhourvideo}. In contrast, Embodied Question Answering (EQA) involves agents actively navigating within an environment to seek visual inputs and enhance answer reliability \cite{das2018embodied}. Due to cost and hardware limitations, several virtual indoor simulators have been developed for EQA tasks \cite{liu2024aligning}, resulting in indoor-focused datasets such as EQA-v1 \cite{das2018embodied} and MT-EQA \cite{yu2019multi}. Recently, urban environment simulators like EmbodiedCity \cite{gao2024embodiedcity}, CityNav \cite{lee2024citynav}, and AerialVLN \cite{liu2023aerialvln} have emerged, though they mainly focus on navigation. EmbodiedCity provides an urban EQA dataset, but it functions more like VQA, as shown in Table \ref{table:dataset}. Moreover, due to the limited generalization capabilities of models at the time, only simple questions about basic attributes of objects were considered in these indoor datasets\cite{ren2024explore}. However, with the continuous improvement in the understanding and reasoning capabilities of pre-trained MM-LLMs for visual inputs, several open-ended EQA datasets have recently been released, such as K-EQA \cite{tan2023knowledge} and OpenEQA \cite{majumdar2024openeqa}.

In comparison, this paper is the first to study the EQA tasks in city space and introduces the benchmark CityEQA-EC ---- a high-quality dataset featuring diverse, open-vocabulary questions.


\subsection{LLMs-driven Embodied Agents}

The indoor EQA tasks mainly involve exploration and answer generation sub-tasks \cite{ren2024explore}. In early work\cite{duan2022survey, das2018embodied, lu2019vilbert}, the two sub-tasks are mainly addressed by building and fine-tuning various deep neural networks. Recently, researchers attempt to utilize pre-trained LLMs to solve EQA tasks without any additional fine-tuning\cite{mu2024embodiedgpt, xiang2024language, huang2024manipvqa}. OpenEQA employed a Frontier-Based Exploration (FBE) strategy for indoor environment exploration and tested the performance of various MM-LLMs on the answer generation \cite{majumdar2024openeqa}. Besides, MM-LLMs was also used to determine which room to explore in indoor environment based their commonsense reasoning capabilities \cite{yinsgsg-nav}.

These agents, however, cannot be directly used for CityEQA tasks. Unlike indoor spaces, which are confined and divided into rooms, city spaces are vast and open. Agents in cities must navigate using landmarks and spatial relationships for long-term exploration \cite{zeng2024perceive, liu2024navagent}. The proposed PMA addresses this by breaking down and planning for long-horizon CityEQA tasks, using large models across multiple modules to effectively handle open-ended questions and unseen environments.