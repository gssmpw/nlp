\section{Experiment}
\label{expriment}

% 所有实验部分按照五部分来写
% 1. 为什么要实验、要做什么实验
% 2. 如何setup
% 3. 结果在哪里
% 4. 实验的直接观察是什么
% 5. 结果的分析结论是什么
% 所有subsection里面都用itemize来写结论，重要的放前面，次要的放后面，每个结论要有加粗的话

% In this section, we conduct extensive experiments to investigate the characteristic of the CityEQA task and the performance of the PMA, aiming to answer the following research questions (RQ).

% \textbf{RQ1}: What is the performance of the PMA on the CityEQA tasks?

% \textbf{RQ2}: What are factors that pose the bottlenecks to CityEQA task performance?

\subsection{Experiment setup}

\paragraph{Evaluation Metrics}
In CityEQA, we adopted three widely used metrics for evaluating EQA tasks \cite{das2018embodied}: Question Answering Accuracy (QAA), Navigation Accuracy (NA), and Mean Time Step (MTS). QAA assesses the correctness of the answers by comparing them to the ground truth. The open-vocabulary nature of the CityEQA task poses challenges for evaluation. Inspired by OpenEQA \cite{majumdar2024openeqa}, we employed an LLM as the judge to assign scores $\theta  \in \left\{ {1,2,...,5} \right\}$ to the answers. For detailed information, please refer to the Appendix \ref{metric}. NA is measured by the distance between the agent's final position and the target object ${p^{tar}}$ upon task completion, reflecting whether the agent successfully located and approached the target. MTS is calculated as the average number of time steps required to complete all tasks, indicating the efficiency of the embodied agent's action strategy.

\paragraph{Implementation Details}

We employed GPT-4o as the MM-LLM for visual analysis, which was utilized in both the Explorer and Collector modules. Meanwhile, GPT-4 was adopted as the text analysis model, responsible for question parsing, plan generation, answer generation, and automated scoring. For each task, the object-centric cognitive map is constructed centered around the agent's initial pose, with a side length of 400 meters and a resolution of 1 meter. We considered buildings as landmarks and accounted for four spatial relationships: north, south, east, and west. Additionally, we limited the total number of time steps for navigation and exploration to 50 steps and restricted the number of steps for collection to 10 steps. Due to API limitations, 200 tasks are randomly selected from CityEQA-EC for the experiments.

\paragraph{Baselines}

Our guiding principle is to investigate how to use foundation models to complete CityEQA tasks without any additional fine-tuning. Therefore, we employed four baselines that are widely employed in the studies of EQA tasks. More details of baselines can be found in Appendix \ref{a_baseline}.
\vspace{-6pt}
\begin{itemize}[leftmargin=*]
    \item \textbf{\colorbox{LightBlue}{Blind Agents}} generate answers based solely on the text of questions without obtaining any visual inputs. It serves as a reference for assessing the extent to which one can rely purely on prior world knowledge and/or random guessing \cite{majumdar2024openeqa}.
    \vspace{-6pt}
    \item \textbf{\colorbox{pink}{LLM-VQA}} bypasse the active exploration process and is directly provided with the RGB image obtained from the ${p^{obs}}$ to answer the questions. This approach aims to assess the visual perception and reasoning capabilities of MM-LLMs in urban environments, while eliminating the interference of embodied actions.
    \vspace{-6pt}
    \item  
    {\spaceskip=0.1em \xspaceskip=0.1em \textbf{\colorbox{LightYellow}{Frontier-Based Exploration (FBE) Agent}}, commonly used indoor baseline \cite{ren2024explore}, does not utilize landmarks or spatial relationships.}
    
    \vspace{-6pt}
    \item \textbf{\colorbox{LightPurple}{Human Agents}} are employed to establish human-level performance metrics on our benchmark. We categorized human agents into two types, H-VQA and H-EQA.  H-VQA is directly provided with an RGB image to perform Visual VQA tasks, similar to the setup of LLM-VQA. H-EQA launches from the initial pose and actively explores the environment based on the question description to find the answer.

\end{itemize}


\subsection{Comparison with Baselines}


\begin{table}
\centering
\caption{Performance of baselines and the proposed PMA on the CityEQA tasks. (PMA-7 means the PMA uses 7 steps to perform collection sub-tasks)}
\label{table:comparison}
\renewcommand\arraystretch{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\multicolumn{1}{c}{} & QAA (1-5)      & NA (m)          & MST         \\ \hline
\colorbox{LightBlue}{Blind Agents}          &           &             &             \\
1. GPT-4             & 1.90±1.64 & -           & -           \\
2. Qwen-2.5          & 2.34±1.88 & -           & -           \\ \hline
\colorbox{pink}{LLM-VQA}           &           &             &             \\
1. GPT-4o            & 4.37±1.35 & -           & -           \\
2. Qwen-2.5          & 4.00±1.67 & -           & -           \\ \hline
\colorbox{LightYellow}{FBE Agent}            & 2.31±2.54 & 86.92±53.71 & 39.31±32.17 \\ \hline
\colorbox{LightPurple}{Human Agents}               &           &             &             \\
1. H-VQA             & 4.87±0.72 & -           & -           \\
2. H-EQA             & 4.94±0.21 & 38.72±40.87 & 9.31±6.32   \\ \hline
\colorbox{shadecolor}{PMA-7}                & 3.00±1.96 & 46.56±36.39 & 24.44±14.39 \\ \hline
\end{tabular}
}
\end{table}


\begin{figure}[!htb]
\centering
    \includegraphics[width=\linewidth]{figures/category.pdf}
% \vspace{-0.2cm}
\caption{Categroy-level performance of the proposed PMA.}
% \vspace{-0.2cm}
\label{fig:category}
\end{figure}

The results are shown in Table \ref{table:comparison} and the category-level performance of PMA is shown in Figure \ref{fig:category}. Some observations can be obtained:

\begin{itemize}[leftmargin=*]
    \item The proposed PMA outperforms the Blind Agent and FBE Agent, as it leverages visual inputs and conducts more efficient perception activities guided by landmarks and spatial relationships. 
    % However, PMA still has significant room for improvement, particularly in terms of perception efficiency (MST) and answer correctness (QAA). Its QAA is only 60.73\% of that of the H-EQA.
    Compared to human agents, PMA shows a significant gap in QAA, achieving only 60.73\% of H-EQA. However, despite the considerable difference in MST, the NA gap is relatively small. This reveals that PMA's navigation and exploration strategies are effective, allowing it to approach target objects even with more time steps.

    \vspace{-6pt}
    \item PMA's performance varies across task types. It achieves the highest QAA on World Knowledge tasks, likely because these tasks rely partially on the LLM's inherent knowledge and require minimal visual inputs. However, it performs the worst on Object Recognition tasks due to their open-ended answers and greater reliance on visual inputs. 
    % For example, in the Object Recognition question “What is the type of the car parked in front of the shop?”, where the correct answer is “police car.”,  the MM-LLM struggles to answer correctly when the car is not fully observed.
    \vspace{-6pt}
    \item Humans excel in both H-VQA and H-EQA tasks. Notably, the QAA of H-EQA is slightly higher than that of H-VQA, indicating actively adjusting the observation view helps address challenges like occlusion and reflection. An illustrative case is provided in Appendix \ref{a_baseline}.
    \vspace{-6pt}
    \item The FBE Agent performs poorly, with a QAA even lower than that of the blind Qwen2.5. This highlights the importance of utilizing landmarks and spatial relationships in exploring urban environments. It also indicates that embodied models designed for indoor environments cannot be directly applied to open-ended city space.
    \vspace{-6pt}
    \item LLM-VQA correctly answers most questions, although its QAA is lower than humans. This confirms the validity of our dataset. Moreover, the performance gap between Qwen-2.5 and GPT-4o indicates that the inherent differences in visual understanding and reasoning capabilities of MM-LLMs are also important factors influencing agent performance.
    \vspace{-6pt}
    \item The Blind Agent achieves a certain level of accuracy, although it is significantly lower than that of humans and GPT-4o. This reveals the regularities of the real world that can be leveraged for answering questions.
    
\end{itemize}
\vspace{-6pt}

Overall, the comparison with baselines reveals that \textit{accurate visual inputs and reasoning are crucial for improving performance in CityEQA tasks}. Additionally, obtaining accurate visual inputs \textit{relies on the efficient exploration using landmarks and spatial relationships} in urban environments. 
 
\subsection{Study on Collector Module}

Previous experimental results have confirmed the effectiveness of navigation and exploration strategies in PMA. Therefore, in this section, we aim to investigate the impact of fine-grained adjustments in observations on performance. To achieve this, we recorded the Collector's pose at each step (up to 10 steps) along with the generated responses and calculated relevant metrics. The results are shown in Figure \ref{fig:pma}. 

\begin{figure}[!htb]
\centering
    \includegraphics[width=\linewidth]{figures/pma.pdf}
% \vspace{-0.2cm}
\caption{The performance of the Collector module at different steps.}
% \vspace{-0.2cm}
\label{fig:pma}
\end{figure}

\textit{It is clear that the Collector significantly impacts outcomes.} As Collector steps increase, NA decreases and QAA increases, suggesting that the Collector aids the agent in getting closer to targets and achieving accurate answers. However, there is a noticeable limit to QAA improvement; at Step 10, QAA is slightly lower than at Step 9. This may be due to the Collector's poor judgment regarding action magnitude, resulting in "over-adjustment" of the observation and degrading visual input quality.

We further analyzed the Collector's taken actions, as detailed in Appendix \ref{collector_action}. The most frequent action was \textit{KeepStill}, reflecting effective Navigation and Exploration sub-tasks that help the agent successfully approach the target object. Additionally, the proportions of \textit{MoveForward}, \textit{TurnLeft}, and \textit{TurnRight} were also relatively high. 
Case analysis revealed that when a target object enters the agent’s view, it tends to stop, possibly cause the object too far away or only partially visible. In such instances, the agent must either \textit{MoveForward} to reduce distance or use \textit{TurnLeft} and \textit{TurnRight} to adjust its orientation for better observation and information gathering about the target object. However, these adjustments remain limited, as illustrated in two cases presented in Appendix \ref{a_ablation}.



