\appendix
\section{Appendix}

\subsection{Dataset Collection and Validation}
\label{a_data_collection}
The collection and validation process of the CityEQA-EC dataset is shown in Figure \ref{fig:dataset_collection}, including Initialization (Step 1), Raw Q\&A Generation (Step 2 to 4), Task Supplementation (Step 5 to 6), and Dataset Validation (Step 7).

In the initialization phase, human annotators were provided with comprehensive briefings and training, during which they were introduced to six distinct types of tasks. Subsequently, in the raw question-and-answer generation stage, annotators were randomly placed within the environment, allowing them to move freely and explore in order to generate questions and answers. Additionally, both the target pose ${p^{tar}}$ and observed pose ${p^{obs}}$ were recorded manually. Then, each question-answer pair was then reviewed by two additional annotators to identify specific issues: (1) Task Duplication, indicating that a similar instance had already been collected; (2) Task Invalidity, meaning that there was no match between the question and answer based on the image. Any tasks identified as problematic were discarded. Furthermore, to ensure the accuracy of pose annotations, we randomly selected 20\% of raw task examples for two rounds of verification regarding their pose annotations.

In the task supplementation phase human annotators were asked to add the initial pose for the task and expand the question.  Buildings are primarily used as landmark objects to expand the question. Then, in the validation stage, each task was independently evaluated by two human reviewers. The details of the review policy are as follows:

\begin{itemize}
    \item Spelling and grammar check is conducted.
    
    \item The target object must be uniquely identifiable based on descriptions of landmarks and spatial positions.
    
    \item The distance between the initial pose and the target pose must be less than 200 meters.
    
    \item The initial pose is located at a movable position rather than within an obstacle.

    
\end{itemize}

Any tasks identified as problematic were removed.


\subsection{PMA Agent Details}

\paragraph{Details of Planner}
\label{a_planner}
We present the detailed CoT used by the Planner here.

\begin{figure}[!htb]
\centering
    \includegraphics[width=\linewidth]{figures/collection_process.png}
% \vspace{-0.2cm}
\caption{The collection and validation process of the CityEQA dataset.}
% \vspace{-0.2cm}
\label{fig:dataset_collection}
\end{figure}

\textbf{Step 1}. All the objects mentioned in the question are extracted, along with the spatial relationships between them. Each object is assigned a unique identifier to ensure distinction. Additionally, the state of each object is marked as \texttt{Unknown} as their locations remain uncertain. The agent itself is treated as a special object, with its state marked as \texttt{Known}, allowing it to serve as a unique initial landmark.

\textbf{Step 2}. The information necessary to answer the question is extracted as corresponding information requirements. This step forms the purpose for the following plan generation, as the entire perception process is driven by the need to gather this critical information.

\textbf{Step 3}. An executable plan is formulated by combining three types predefined sub-tasks based on information requirements. To guide LLMs reasoning and constructing an executable plan, we establish a set of simple rules. First, collecting information requires the Collection sub-task. However, before executing this sub-task, the states of the relevant objects must be \texttt{Known}, meaning the objects must already have been located in the environment. Second, the Exploration sub-task can transition an object's state from \texttt{Unknown} to \texttt{Known}. Third, before performing Exploration, the Navigation sub-task can be employed to leverage a \texttt{Known} object as the landmark, enabling the agent to efficiently reach specific locations. This sub-task can reduce the exploration scope and enhances overall efficiency.

\begin{figure*}[!htb]
\centering
    \includegraphics[width=\linewidth]{figures/construct.png}
% \vspace{-0.2cm}
\caption{The workflow of the construction of the added map.}
% \vspace{-0.2cm}
\label{fig:cosntruction}
\end{figure*}

\paragraph{Details of Object-Centric Cognitive Map}
\label{occm}

The processing procedure of the function $ Construct()$ is illustrated in Figure \ref{fig:cosntruction}. Firstly, the GroundSAM model is utilized to process the RGB image to obtain object segmentation masks and captions. Meanwhile, the pose and depth image are combined with the camera intrinsic parameters to obtain 3D point cloud data. Then, these two data are merged to obtain the object-centric 3D point cloud. Further, this data is projected onto a 2D grid, and the point cloud data outside the map range is filtered out to obtain the object-centric 2D grids. Finally, objects with repetitive grids are fused to obtain the object-centric added map.

The purpose of the function $ Merge()$ is to fuse the added objects in added map into the global map. This is to ensure that the same object observed from different views is uniquely recorded and retrieved on the map. Therefore, for each added object, we first determine whether the distribution of the object overlaps or is adjacent to any object in the global map. If so, the two objects are merged; if not, the object is directly added to the global map. This paper adopts a simple and effective strategy to determine whether objects are adjacent: when at least one pair of grids in which the two objects are distributed are adjacent, they are considered to have an adjacent relationship. Additionally, it should be noted that multiple object merges may occur in the same round, so the merged object needs to be judged against all other objects in the global map in another round.

\paragraph{Details of Collector}
\label{a_collector}
The prompt provided for MM-LLM in Collector is presented in Figure \ref{fig:prompt_collector}. The Collector needs to complete two tasks in sequence. The first is the VQA task, which involves answering the corresponding questions based on the provided RGB image. The second is action selection, which requires choosing an appropriate action from a discrete set of actions to adjust the observation. The action set used in this study includes \{\textit{MoveForward}, \textit{MoveBack}, \textit{MoveLeft}, \textit{MoveRight}, \textit{MoveUp}, \textit{MoveDown}, \textit{TurnLeft}, \textit{TurnRight}, \textit{KeepStill}\}.

\begin{figure}[!htb]
\centering
    \includegraphics[width=\linewidth]{figures/collector.pdf}
% \vspace{-0.2cm}
\caption{The prompt used for Collector.}
% \vspace{-0.2cm}
\label{fig:prompt_collector}
\end{figure}


\subsection{Experiments Details}


\paragraph{LLM Scoring}
\label{metric}

For QAA, we designed an LLM-based automated scoring method by referring to the LLM-Match mechanism in OpenEQA \cite{majumdar2024openeqa}. We show the designed prompt for LLM in Figure \ref{fig:llmscore}.

To investigate the validation of using the LLM as judge, a double blind study is conducted. We randomly sampled 100 answers from the results including the answer generated by the 4 baselines and PMA. Then 2 human evaluators are required to provide their score of the answers while using the prompt in Figure \ref{fig:llmscore} as the task instruction. Since the distribution of scores did not conform to a normal distribution, Spearman's correlation analysis was adopted. The results indicated a significant positive correlation between the scores given by human evaluators and those by LLM judges ($R_s$ = 0.85, $p$ = 0.002). This suggests that using LLMs as judges can effectively evaluate open-ended question-answering results and align well with human judgments.


\begin{figure}[!htb]
\centering
    \includegraphics[width=\linewidth]{figures/llmscore.pdf}
% \vspace{-0.2cm}
\caption{The prompt used for LLM scoring.}
% \vspace{-0.2cm}
\label{fig:llmscore}
\end{figure}

\paragraph{Baselines Details}

This section provides additional details for the baselines.

\begin{itemize}[leftmargin=*]

    \item \textbf{Blind Agents.} We choose GPT-4 \cite{achiam2023gpt} and Qwen2.5 \cite{yang2024qwen2} to answer questions as blind agents.
    
    \item \textbf{LLM-VQA.} We choose GPT-4o and Qwen-2.5 to perform VQA tasks as LLM-VQA agents.

    \item \textbf{FBE Agent.} Instead of utilizing landmarks and spatial relationships, it identifies the frontiers between explored and unexplored regions, samples one as the navigation point, and employs the A* algorithm to find a path.  We also limit the path length to 10 meters at each step, consistent with the setting of the Navigator in the PMA.

    \item \textbf{Human Agents.} At each step, H-EQA can only access the RGB image of the current pose and must choose one action from {\textit{MoveForward}, \textit{TurnLeft}, \textit{TurnRight}, \textit{Stop}}. The angles for \textit{TurnLeft} and \textit{TurnRight} are set at 30Â°. When selecting \textit{MoveForward}, the agent must also provide an integer distance within 10 meters. When choosing \textit{Stop}, the H-EQA is required to provide the answer.
    
\end{itemize}


\paragraph{A Case of Human Agent}
\label{a_baseline}

In Figure \ref{fig:case1}, we provide a case to illustrate why the performance of H\_EQA is superior to that of H\_VQA. The given question is "What is the color of the car next to the red car?" The ground truth answer is "Black". H\_VQA was provided with the RGB image on the left for question answering. However, in this image, due to the influence of outdoor lighting, the originally black car appears gray, thus H\_VQA provided an incorrect answer. In contrast, H\_EQA can actively adjust the observation pose, observing the side of the car to reduce the impact of the lighting, and thereby providing the correct answer.


\begin{figure*}[!htb]
\centering
    \includegraphics[width=0.6\linewidth]{figures/case1.pdf}
% \vspace{-0.2cm}
\caption{The images obtained by H-VQA and H-EQA.}
% \vspace{-0.2cm}
\label{fig:case1}
\end{figure*}

\paragraph{Actions of Collector}
\label{collector_action}
The statistics of various actions taken by Collector are shown in Figure \ref{fig:action_count}.

\begin{figure*}[!htb]
\centering
    \includegraphics[width=0.6\linewidth]{figures/actioncount.pdf}
% \vspace{-0.2cm}
\caption{The proportion of different actions taken by Collector.}
% \vspace{-0.2cm}
\label{fig:action_count}
\end{figure*}


\paragraph{Cases of Collector}
\label{a_ablation}

We present two cases to illustrate the effect of the collector. In the first case, as shown in Figure \ref{fig:case2}, since the shop with black signboard was discovered too early in the Exploration stage, the starting pose of the collector was far from the target pose. Even after moving 10 steps promptly, it still failed to recognize the text on the black signboard. In the second case, as shown in Figure \ref{fig:case3}, in Step 1, the yellow signboard that the collector needed to recognize was on the left side of the picture and seemed not to be fully displayed. At this time, the collector took the \textit{TurnLeft} action, thus observing the entire yellow signboard in Step 2 and easily providing the correct answer.


\begin{figure*}[!htb]
\centering
    \includegraphics[width=0.6\linewidth]{figures/case2.pdf}
% \vspace{-0.2cm}
\caption{The failure case for collection.}
% \vspace{-0.2cm}
\label{fig:case2}
\end{figure*}


\begin{figure*}[!htb]
\centering
    \includegraphics[width=0.6\linewidth]{figures/case3.pdf}
% \vspace{-0.2cm}
\caption{The successful case for collection.}
% \vspace{-0.2cm}
\label{fig:case3}
\end{figure*}

% \todo{\subsection{Ablation Study}
% future add an ablation study here.}