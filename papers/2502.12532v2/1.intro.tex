\section{Introduction}
\label{sec::intro}

Embodied Question Answering (EQA) \cite{das2018embodied} represents a challenging task at the intersection of natural language processing, computer vision, and robotics, where an embodied agent (e.g., a UAV) must actively explore its environment to answer questions posed in natural language. While most existing research has concentrated on indoor EQA tasks \cite{gao2023room, pena2023visual}, such as exploring and answering questions within confined spaces like homes or offices \cite{liu2024aligning}, relatively little attention has been dedicated to EQA tasks in  open-ended city space. Nevertheless, extending EQA to city space is crucial for numerous real-world applications, including autonomous systems \cite{kalinowska2023embodied}, urban region profiling \cite{yan2024urbanclip}, and city planning \cite{gao2024embodiedcity}. 
% 1. 环境复杂性   
%    - 地标重复性问题（如区分相似建筑）  
%    - 动态干扰因素（交通流、行人）  
% 2. 行动复杂性  
%    - 长程导航路径规划  
%    - 移动控制、角度等  
% 3. 感知复杂性  
%    - 复合空间关系推理（"A楼东侧商铺西边的车辆"）  
%    - 时序依赖的观察结果整合

EQA tasks in city space (referred to as CityEQA) introduce a unique set of challenges that fundamentally differ from those encountered in indoor environments. Compared to indoor EQA, CityEQA faces three main challenges: 

1) \textbf{Environmental complexity with ambiguous objects}: 
Urban environments are inherently more complex,  featuring a diverse range of objects and structures, many of which are visually similar and difficult to distinguish without detailed semantic information (e.g., buildings, roads, and vehicles). This complexity makes it challenging to construct task instructions and specify the desired information accurately, as shown in Figure \ref{fig:example}. 

2) \textbf{Action complexity in cross-scale space}: 
The vast geographical scale of city space compels agents to adopt larger movement amplitudes to enhance exploration efficiency. However, it might risk overlooking detailed information within the scene. Therefore, agents require cross-scale action adjustment capabilities to effectively balance long-distance path planning with fine-grained movement and angular control.

3) \textbf{Perception complexity with observation dynamics}: 
% Rich semantic information in urban settings leads to varying observations depending on distance and orientation, which can impact the accuracy of answer generation. 
Observations can vary greatly depending on distance, orientation, and perspective. For example, an object may look completely different up close than it does from afar or from different angles. These differences pose challenges for consistency and can affect the accuracy of answer generation, as embodied agents must adapt to the dynamic and complex nature of urban environments.


\begin{table}
\centering
\caption{CityEQA-EC vs existing benchmarks.}
\label{table:dataset}
\renewcommand\arraystretch{1.2}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
             & Place  & Open Vocab & Active & Platform  & Reference \\ \hline
EQA-v1      & Indoor & \textcolor{red}{\ding{55}}          & \textcolor{green}{\ding{51}}      & House3D      & \cite{das2018embodied}  \\
IQUAD        & Indoor & \textcolor{red}{\ding{55}}          & \textcolor{green}{\ding{51}}      & AI2-THOR     & \cite{gordon2018iqa} \\
MP3D-EQA     & Indoor & \textcolor{red}{\ding{55}}          & \textcolor{green}{\ding{51}}      & Matterport3D & \cite{wijmans2019embodied} \\
MT-EQA       & Indoor & \textcolor{red}{\ding{55}}          & \textcolor{green}{\ding{51}}      & House3D      & \cite{yu2019multi} \\
ScanQA       & Indoor & \textcolor{red}{\ding{55}}          & \textcolor{red}{\ding{55}}      & -            & \cite{azuma2022scanqa} \\
SQA3D        & Indoor & \textcolor{red}{\ding{55}}          & \textcolor{red}{\ding{55}}      & -            & \cite{masqa3d} \\
K-EQA        & Indoor & \textcolor{green}{\ding{51}}          & \textcolor{green}{\ding{51}}      & AI2-THOR     & \cite{tan2023knowledge} \\
OpenEQA      & Indoor & \textcolor{green}{\ding{51}}          & \textcolor{green}{\ding{51}}      & ScanNet/HM3D & \cite{majumdar2024openeqa} \\
 \hline
CityEQA-EC   & City (Outdoor)  & \textcolor{green}{\ding{51}}          & \textcolor{green}{\ding{51}}      & EmbodiedCity & - \\ \hline
\end{tabular}}
\end{table}

\begin{figure*}[!htb]
\centering
    \includegraphics[width=0.78\linewidth]{figures/example.pdf}
% \vspace{-0.2cm}
\caption{The typical workflow of the PMA to address City EQA tasks. There are two cars in this area, thus a valid question must contain landmarks and spatial relationships to specify a car. Given the task, PMA will sequentially complete multiple sub-tasks to find the answer.}
% \vspace{-0.2cm}
\label{fig:example}
\end{figure*}

As an initial step toward CityEQA, we developed \textbf{CityEQA-EC}, a benchmark dataset to evaluate embodied agents' performance on CityEQA tasks. The distinctions between this dataset and other EQA benchmarks are summarized in Table \ref{table:dataset}. CityEQA-EC comprises six task types characterized by open-vocabulary questions. These tasks utilize urban landmarks and spatial relationships to delineate the expected answer, adhering to human conventions while addressing object ambiguity. This design introduces significant complexity, turning CityEQA into long-horizon tasks that require embodied agents to identify and use landmarks, explore urban environments effectively, and refine observation to generate high-quality answers.

To address CityEQA tasks, we introduce the \textbf{Planner-Manager-Actor (PMA)}, a novel baseline agent powered by large models, designed to emulate human-like rationale for solving long-horizon tasks in urban environments, as illustrated in Figure \ref{fig:example}. PMA employs a hierarchical framework to generate actions and derive answers. The Planner module parses tasks and creates plans consisting of three sub-task types: navigation, exploration, and collection. The Manager oversees the execution of these plans while maintaining a global object-centric cognitive map \cite{deng2024opengraph}. This 2D grid-based representation enables precise object identification (retrieval) and efficient management of long-term landmark information. The Actor generates specific actions based on the Manager's instructions through its components: Navigator, Explorer, and Collector. Notably, the Collector integrates a Multi-Modal Large Language Model (MM-LLM) as its Vision-Language-Action (VLA) module to refine observations and generate high-quality answers.
PMA's performance is assessed against four baselines, including humans. 
Results show that humans perform best in CityEQA, while PMA achieves 60.73\% of human accuracy in answering questions, highlighting both the challenge and validity of the proposed benchmarks. 

% The Frontier-Based Exploration (FBE) Agent, widely used in indoor EQA tasks, performs worse than even a blind LLM. This underscores the importance of PMA's hierarchical framework and its use of landmarks and spatial relationships for tackling CityEQA tasks.

In summary, this paper makes the following significant contributions:
\vspace{-8pt}
\begin{itemize}[leftmargin=*]
    \item To the best of our knowledge, we present the first open-ended embodied question answering benchmark for city space, namely CityEQA-EC.
    \vspace{-7pt}
    \item We propose a novel baseline model, PMA, which is capable of solving long-horizon tasks for CityEQA tasks with a human-like rationale.
     \vspace{-7pt}
    \item Experimental results demonstrate that our approach outperforms existing baselines in tackling the CityEQA task. However, the gap with human performance highlights opportunities for future research to improve visual thinking and reasoning in embodied intelligence for city spaces.
\end{itemize}



