\begin{figure*}[!htb]
\centering
    \includegraphics[width=\linewidth]{figures/dataset.pdf}
% \vspace{-0.2cm}
\caption{Example questions and dataset statistics of CityEQA-EC.}
% \vspace{-0.2cm}
\label{fig:dataset}
\end{figure*}

\section{CityEQA-EC Dataset}\label{sec::dataset}
\vspace{-0.2cm}
In this section, we outline the formulation of the EQA task and describe the dataset collection process for CityEQA-EC. To address real-world demands, such as urban governance and public services, we draw upon previous research \cite{majumdar2024openeqa, das2018embodied} to define six distinct task types. Examples and statistics of the dataset are presented in Figure \ref{fig:dataset}. 

\vspace{-0.2cm}

\subsection{Task Formulation}

An instance of the EQA task is defined by the 4-tuple: $\xi =(e,q,y,{{p}_{0}})$, where $e$ is the simulated or real 3D scene that agent can interact with, $q$ is the question, and $y$ is the ground truth answer. The ${{p}_{0}}$ denotes the agent’s initial pose, including 3D position and orientation. Given the instance $\xi $, the goal is for the embodied agent (e.g., drones) to complete the task by gathering the required information from $e$ and generating the answer $\hat{y}$ in response to $q$. Specifically, the agent starts at the initial pose ${{p}_{0}}$ and interacts with the scene $e$ step by step. At each time step$t$, the agent can move to a specific pose ${{p}_{t}}$, and obtain an observation ${{o}_{t}}=(I_{t}^{rgb},I_{t}^{d})$ from the scene, where $I_{t}^{rgb}\in {{\mathbb{R}}^{H\times W\times 3}}$ is the RGB image and $I_{t}^{d}\in {{\mathbb{R}}^{H\times W}}$ is the depth image. Based on these observations, the agent generates the answer $\hat{y}$. The key challenge is to produce a high-quality answer while minimizing the time steps required. 

\subsection{Dataset Collection and Validation}

To obtain a high-quality dataset, we employed the EmbodiedCity \cite{gao2024embodiedcity}, which is a highly realistic 3D simulation platform based on the buildings, roads, and other elements in a real city. It is implemented using Unreal Engine 4 \cite{sanders2016introduction} and Microsoft AirSim plugins \cite{shah2018airsim}. The collection process is to determine the 4-tuple elements $\xi =(e,{{p}_{0}},q,y)$ of each instance. Unlike indoor simulators with many different scenes, EmbodiedCity is a coherent and extensive scene. As a result, for all instances, their scene $e$ corresponds to EmbodiedCity.

The dataset collection process involves two steps, completed by five human annotators. The first step is raw Q\&A generation, where raw questions and answers are created. The second step is task supplementation, which includes determining the agent's initial pose and and refining the question descriptions accordingly. Once these steps are completed, the dataset undergoes validation and filtering. More details can be found in Appendix \ref{a_data_collection}.  

\paragraph{Raw Q\&A Generation} 

We instructed human annotators to explore the EmbodiedCity environment freely and generate question-answer pairs based on their observations of RGB images. The raw questions ${q^r}$ and answers $y$ are presented as open-vocabulary text. In addition to documenting the question-answer pairs, annotators were also required to record the pose ${p^{obs}}$ from which the RGB images were captured, along with the pose ${p^{tar}}$ of the target object referenced in each question. These information can be leveraged for a comprehensive evaluation of the agent's performance. After basic revision process, we have finally collected a total of 443 such instances, with each raw task instance denoted as ${\xi ^r} = ({q^r}, y, {p^{obs}}, {p^{tar}})$.

\paragraph{Task Supplementation} 

Building upon the raw task instances, we further established the agent's initial pose and refined the questions accordingly. For each raw task, the initial pose \( p_{0} \) of the agent was set within a 200-meter range of the target object's pose \( p^{tar} \). Given the complexity of urban environments, and to ensure that each expected answer is unique, we enriched the questions with descriptions based on landmarks. An example of this process is illustrated in Figure \ref{fig:example}. For each raw task, we generated at least four distinct initial poses and transformed each raw question into at least four different inquiries. Ultimately, this process yielded a total of 2,212 task instances.


\paragraph{Dataset Validation} 

Each task instance created by human annotators was rigorously evaluated by two independent human reviewers. These reviewers were responsible for determining whether the questions posed were answerable and clear, as well as verifying the uniqueness and accuracy of the target objects and their corresponding answers. Any task instance identified with issues was excluded. The final dataset comprises 1,412 task instances, with detailed statistics presented in Figure \ref{fig:dataset}.

% \todo{The description of the data validation process is vague; it is recommended to include quality evaluation metrics (can be placed in the appendix). 

% The analysis of the dataset itself and the description of the dataset split in AerialVLN are relatively clear and thorough. Do we have something similar here?
% }

% | 评估维度       | 标准                  | 通过率 |  
% |----------------|-----------------------|-------|  
% | 问题可回答性   | 需≤3次环境交互可解    | 92%   |  
% | 答案唯一性     | 双盲验证一致率≥95%    | 97%   |  
% | 环境可达性     | 初始位置到目标≤200m   | 100%  |