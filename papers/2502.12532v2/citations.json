[
  {
    "index": 0,
    "papers": [
      {
        "key": "ishmam2024image",
        "author": "Ishmam, Md Farhan and Shovon, Md Sakib Hossain and Mridha, Muhammad Firoz and Dey, Nilanjan",
        "title": "From image to language: A critical analysis of visual question answering (vqa) approaches, challenges, and opportunities"
      },
      {
        "key": "guo2023images",
        "author": "Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven",
        "title": "From images to textual prompts: Zero-shot visual question answering with frozen large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chandrasegaranhourvideo",
        "author": "Chandrasegaran, Keshigeyan and Gupta, Agrim and Hadzic, Lea M and Kota, Taran and He, Jimming and Eyzaguirre, Cristobal and Durante, Zane and Li, Manling and Wu, Jiajun and Fei-Fei, Li",
        "title": "HourVideo: 1-Hour Video-Language Understanding"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "das2018embodied",
        "author": "Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv",
        "title": "Embodied question answering"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu2024aligning",
        "author": "Liu, Yang and Chen, Weixing and Bai, Yongjie and Li, Guanbin and Gao, Wen and Lin, Liang",
        "title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "das2018embodied",
        "author": "Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv",
        "title": "Embodied question answering"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yu2019multi",
        "author": "Yu, Licheng and Chen, Xinlei and Gkioxari, Georgia and Bansal, Mohit and Berg, Tamara L and Batra, Dhruv",
        "title": "Multi-target embodied question answering"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "gao2024embodiedcity",
        "author": "Gao, Chen and Zhao, Baining and Zhang, Weichen and Mao, Jinzhu and Zhang, Jun and Zheng, Zhiheng and Man, Fanhang and Fang, Jianjie and Zhou, Zile and Cui, Jinqiang and others",
        "title": "EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lee2024citynav",
        "author": "Lee, Jungdae and Miyanishi, Taiki and Kurita, Shuhei and Sakamoto, Koya and Azuma, Daichi and Matsuo, Yutaka and Inoue, Nakamasa",
        "title": "CityNav: Language-Goal Aerial Navigation Dataset with Geographic Information"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2023aerialvln",
        "author": "Liu, Shubo and Zhang, Hongsheng and Qi, Yuankai and Wang, Peng and Zhang, Yanning and Wu, Qi",
        "title": "Aerialvln: Vision-and-language navigation for uavs"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ren2024explore",
        "author": "Ren, Allen Z and Clark, Jaden and Dixit, Anushri and Itkina, Masha and Majumdar, Anirudha and Sadigh, Dorsa",
        "title": "Explore until Confident: Efficient Exploration for Embodied Question Answering"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tan2023knowledge",
        "author": "Tan, Sinan and Ge, Mengmeng and Guo, Di and Liu, Huaping and Sun, Fuchun",
        "title": "Knowledge-based embodied question answering"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "majumdar2024openeqa",
        "author": "Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others",
        "title": "Openeqa: Embodied question answering in the era of foundation models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ren2024explore",
        "author": "Ren, Allen Z and Clark, Jaden and Dixit, Anushri and Itkina, Masha and Majumdar, Anirudha and Sadigh, Dorsa",
        "title": "Explore until Confident: Efficient Exploration for Embodied Question Answering"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "duan2022survey",
        "author": "Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston",
        "title": "A survey of embodied ai: From simulators to research tasks"
      },
      {
        "key": "das2018embodied",
        "author": "Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv",
        "title": "Embodied question answering"
      },
      {
        "key": "lu2019vilbert",
        "author": "Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan",
        "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "mu2024embodiedgpt",
        "author": "Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping",
        "title": "Embodiedgpt: Vision-language pre-training via embodied chain of thought"
      },
      {
        "key": "xiang2024language",
        "author": "Xiang, Jiannan and Tao, Tianhua and Gu, Yi and Shu, Tianmin and Wang, Zirui and Yang, Zichao and Hu, Zhiting",
        "title": "Language models meet world models: Embodied experiences enhance language models"
      },
      {
        "key": "huang2024manipvqa",
        "author": "Huang, Siyuan and Ponomarenko, Iaroslav and Jiang, Zhengkai and Li, Xiaoqi and Hu, Xiaobin and Gao, Peng and Li, Hongsheng and Dong, Hao",
        "title": "Manipvqa: Injecting robotic affordance and physically grounded information into multi-modal large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "majumdar2024openeqa",
        "author": "Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others",
        "title": "Openeqa: Embodied question answering in the era of foundation models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "yinsgsg-nav",
        "author": "Yin, Hang and Xu, Xiuwei and Wu, Zhenyu and Zhou, Jie and Lu, Jiwen",
        "title": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zeng2024perceive",
        "author": "Zeng, Qingbin and Yang, Qinglong and Dong, Shunan and Du, Heming and Zheng, Liang and Xu, Fengli and Li, Yong",
        "title": "Perceive, reflect, and plan: Designing llm agent for goal-directed city navigation without instructions"
      },
      {
        "key": "liu2024navagent",
        "author": "Liu, Youzhi and Yao, Fanglong and Yue, Yuanchang and Xu, Guangluan and Sun, Xian and Fu, Kun",
        "title": "NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation"
      }
    ]
  }
]