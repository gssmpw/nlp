

\section{PMA: A Hierarchical LLM Agent for CityEQA Task}\label{method}

 \begin{figure*}[!htb]
\centering
    \includegraphics[width=0.8\linewidth]{figures/framework.pdf}
% \vspace{-0.2cm}
\caption{The overview of our proposed PMA agent.}
% \vspace{-0.2cm}
\label{fig:framework}
\end{figure*}
% （1）标题不能太长，要突出核心贡献和特色
% （2）在引出具体设计的时候，要加to xxx作为动机
% （3）在每个section的开头句，要道明现在准备做什么（建立context）；在结束的时候，要让人清晰看到这个section或者subsection做到了什么，拿到了什么新的output
% （4）要有合适的文献引用
% （5）不能有大段无公式的文字，显得不够技术

\subsection{Overview}

An overview of the proposed PMA agent for CityEQA tasks is shown in Figure \ref{fig:framework}. The PMA agent comprises three major modules: Planner, Manager, and Actor, all powered by pre-trained large models. The Planner is responsible for parsing the question $q$ and formulating an executable plan before any actions are taken. The Manager serves as the core module, receiving structured information from the Planner and processing observations at each time step to maintain an object-centric cognitive map using an MM-LLM. Additionally, through a process control module, the Manager issues task instructions to the Actor, which then utilizes various action generators to execute the required responses. Once the plan is completed, the Manager generates an answer based on  its accumulated memory.

\subsection{Planner Module}
The question descriptions in CityEQA tasks contain extensive information, including several objects, spatial relationships, and the information that needs to be collected. To address the open-ended question descriptions, we leveraged pre-trained LLMs and designed a few-shot prompt that employs a three-step Chain of Thought (CoT) reasoning \cite{wei2022chain} to parse the question and formulate a plan.

As illustrated in Figure \ref{fig:framework}, all objects and spatial relationships mentioned in the question are first extracted. Simultaneously, the information necessary to answer the question is identified as corresponding requirements. Based on these requirements, a plan is created consisting of three distinct types of sub-tasks: (1) \textbf{Collection sub-tasks} gather the requisite information, (2) \textbf{Exploration sub-tasks}  identify landmarks or target objects, and (3) \textbf{Navigation sub-tasks} enable efficient access to specific areas, thereby narrowing the exploration scope. To ensure the plan is executable, we have developed several strategies to guide the LLMs, with details provided in Appendix \ref{a_planner}.


\subsection{Manager Module}

The Manager possesses the capability to oversee and manage the gradual implementation of long-term plans. This is made possible by its Memory module and Map module, which facilitate the organized storage of observations and track execution progress as the plan unfolds.

\paragraph{Object-Centric Cognitive Map} 
The object-centric cognitive map takes the initial pose of the agent as the origin, uses 2D grids to discretize the surrounding environment, and  records the distribution of landmark objects based on grid indices. The map at time step $t$-1 is represented as ${M_{t{\rm{ - }}1}}{\rm{ = }}\{ obj\_1,{\rm{ }}obj\_2,{\rm{ }}...\} $, where the $obj\_1$ and $obj\_2$ are the object IDs corresponding to specific objects in the environment. At each time step $t$,  the agent leverages egocentric observations represented as ${{o}_{t}}=(I_{t}^{rgb},I_{t}^{d})$ to construct the added map ${{m}_{t}}$ to record the landmark objects appeared at current observation, denoting as ${{m}_{t}}=Construct({{o}_{t}, p_t})$. To implement the functionality of $Construct()$, we utilized the GroundSAM model \cite{bousselham2024grounding} for grounding and segmenting landmark objects from $I_{t}^{rgb}$. By integrating pose information with depth data from $I_{t}^{d}$, we can obtain a 3D point cloud representation of these objects, subsequently projected onto 2D grids. After denoising and filtering, we obtained the finalized added map, denoted by ${{m}_{t}}$.

The added map ${{m}_{t}}$ will be fused with the ${{M}_{t\text{-}1}}$ by merging the same object observed at different time steps, so objects are guaranteed to be unique in the map, denoting as ${{M}_{t}}=Merge({{m}_{t}},{{M}_{t-1}})$. More details can be found in Appendix \ref{occm}.

\paragraph{Other Modules}
Memory module records important information in the perceptual process, which mainly includes three aspects. \textit{Req\_info} records the collected information, and \textit{Object\_info} records object information, such as the object's ID in the map. \textit{History} records the completion progress of sub-tasks and the execution results of actions.

Process Control is designed to determine the next sub-task to be executed based on the current progress of the plan. It also serves as the interface for interaction with the Actor. Once all sub-tasks in the plan have been completed, Process Control invokes the Answer Generation module to produce the final response. The Answer Generation process is also driven by LLMs, employing a zero-shot prompt specifically crafted to generate answers based on the \textit{Req\_info} stored in memory.


\subsection{Actor Module}
To address the distinct objectives of the three types of sub-tasks, we introduce three specialized low-level action generators: Navigator, Explorer, and Collector. The Navigator and Explorer rely on distinct deterministic policies to generate actions based on the cognitive map. In contrast, the Collector uses a VLA policy, which directly derives actions from RGB images. These action models serve as fundamental baselines and provide a foundation for future research enhancements.

\paragraph{Navigator}
The navigation sub-task instructions specify a landmark and a directional relationship. For instance, \textit{Navigation(building\_1, west)} indicates that building\_1 serves as the landmark, with navigation directed to the west of it, where the target object is likely located. The Navigator identifies the nearest navigation point on the map by analyzing the landmark's distribution in conjunction with its spatial relationship. It then employs the A* algorithm to plan a path from the agent's current position to this navigation point. Given the potential incompleteness of recorded landmarks on the map, a multi-step approach is adopted, restricting each step's path length ${L^{nav}}$ to 10 meters. The navigation point is updated following each cognitive map update.

\paragraph{Explorer}
The typical exploration sub-task is described as \textit{Exploration(building\_1, west, red\_car)}, which means the goal is to explore the west side of building\_1 to find a red car. The explorer uses the Move and Look Around (MLA) strategy due to the complexity of outdoor environments, where re-observing previously explored areas from different angles can yield different results. The exploration area is defined on the map based on landmark distribution and spatial relationships. A set of exploration points is generated within this area, maintaining a fixed distance of ${L^{exp}=10}$ meters between them. At each point, the agent thoroughly observes its surroundings by looking in four directions: front, back, left, and right. After completing observations at one point, the agent moves to the next closest point and continues until either the target object is found or all points are covered. A MM-LLM is employed to determine whether the target appears in any given observation.

\paragraph{Collector}
% 基于多模态大模型快推理的视觉信息收集自主调整机制
The collection sub-task instructions only include an information requirement. We provide an MM-LLM-driven Collector to gather the required information from observations. Additionally, the Collector can select an action from a predefined action set to fine-tune its observation view, enabling the collection of higher-quality information. More details of the design of Collector is presented in Appendix \ref{a_collector}.