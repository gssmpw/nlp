\PassOptionsToPackage{svgnames,x11names}{xcolor}
\documentclass{article} % For LaTeX2e
\usepackage{iclr25/iclr2025_conference,times}

\iclrfinalcopy 
\input{iclr25/config}

\title{Learning to Plan\\with Personalized Preferences\vspace{-6pt}}

\author{%
    Manjie Xu$^{\star,1,2}$, Xinyi Yang$^{\star,1}$, Wei Liang$^{2,3,\textrm{\Letter}}$, Chi Zhang$^{4,\,\textrm{\Letter}}$, Yixin Zhu$^{1,\,\textrm{\Letter}}$
    \\
    \small $\star$ equal contributors\quad{}\quad{}$\textrm{\Letter}$ corresponding authors\\
    \small $^1$ Institute for Artificial Intelligence, Peking University\\
    \small $^2$ School of Computer Science \& Technology, Beijing Institute of Technology\\
    \small $^3$ Yangtze Delta Region Academy of Beijing Institute of Technology, Jiaxing, China\\
    \small $^4$ National Key Laboratory of General Artificial Intelligence, BIGAI\\
    \href{https://sites.google.com/view/personalized-planning}{https://sites.google.com/view/personalized-planning}
    \vspace{-6pt}
}

\begin{document}

\maketitle

\begin{abstract}
Effective integration of \ac{ai} agents into daily life requires them to understand and adapt to individual human preferences, particularly in collaborative roles. Although recent studies on embodied intelligence have advanced significantly, they typically adopt generalized approaches that overlook personal \emph{preferences in planning}. We address this limitation by developing agents that not only learn preferences from few demonstrations but also learn to adapt their planning strategies based on these preferences. Our research leverages the observation that preferences, though implicitly expressed through minimal demonstrations, can generalize across diverse planning scenarios. To systematically evaluate this hypothesis, we introduce \ac{pbp} benchmark, an embodied benchmark featuring hundreds of diverse preferences spanning from atomic actions to complex sequences. Our evaluation of \ac{sota} methods reveals that while symbol-based approaches show promise in scalability, significant challenges remain in learning to generate and execute plans that satisfy personalized preferences. We further demonstrate that incorporating learned preferences as intermediate representations in planning significantly improves the agent's ability to construct personalized plans. These findings establish preferences as a valuable abstraction layer for adaptive planning, opening new directions for research in preference-guided plan generation and execution.
\end{abstract}

\section{Introduction}

\begin{figure}[t!]
    \centering
    \small
    \includegraphics[width=\linewidth]{PbP-intro}
    \caption{\textbf{An example of preference-based planning in a food preparation scenario.} When the assistant receives a natural language instruction for a food preparation task, it can follow one of two approaches: (Left, traditional methods) The assistant verifies details with the user at each step through exhaustive communication; or (Right, our personalized approach) it first learns from previous user action sequences to infer explicit preference labels and then generates a personalized plan based on the learned preferences. The planning tree (middle) illustrates how preferences guide the whole decision-making process across multiple dimensions. By learning preferences as a key intermediate representation from minimal human demonstrations, our approach enables \ac{ai} agents to deliver personalized and adaptable assistance without explicit step-by-step instructions.}
    \label{fig:env}
\end{figure}

The field of embodied \acf{ai} is rapidly advancing, driven by significant progress in foundation models for vision and language \citep{bommasani2021opportunities,peng2023kosmos,achiam2023gpt,bai2023qwen}. These advances enable \ac{ai} systems to autonomously collaborate with or assist humans in daily tasks, particularly in domestic settings \citep{driess2023palm,leal2023sara,zitkovich2023rt,ahn2024autort}. However, recent approaches utilizing natural language instructions \citep{mu2023embodiedgpt,zitkovich2023rt,singh2023progprompt} face fundamental limitations in capturing human preferences \citep{zhu2016inferring}. While natural language is our primary means of communication, its inherent ambiguity creates a gap between instructions and intended executions \citep{yuan2022in,jiang2022what,jiang2021individual,yuan2020joint}. For instance, when a user requests help in preparing an apple, the agent needs to understand specific preferences about apple selection, washing requirements, cutting style, and container choice---details that vary significantly across individuals; see also \cref{fig:env} for a graphical illustration.

Preference, central to personalization \citep{slovic1995construction}, remains inadequately addressed in embodied \acf{ai}. Integrating personalized preferences is crucial for tailoring agent actions to individual users, thereby enhancing the effectiveness and satisfaction of embodied assistants \citep{lee2012personalization,leyzberg2014personalizing}. Moreover, preferences guide human-like decision-making and intelligent behavior. Psychological research emphasizes that understanding preferences is vital for interpreting human behaviors \citep{fawcett2010children} and facilitating social interactions \citep{gerson2017you,liberman2021origins}, suggesting that preference understanding could enable more grounded planning in embodied assistants.

Learning human preferences in real-world settings presents unique challenges \citep{peng2024tong}. Humans typically communicate their needs succinctly \citep{levinson1983pragmatics}, without exhaustive preference details \citep{lichtenstein2006construction}, and many preferences include unconscious or instinctive elements difficult to articulate \citep{epstein1994integration,simonson2008will}. A more practical approach is to infer preferences from observed human choices and decision-making patterns, as illustrated in \cref{fig:env}, where a robot assistant can learn users' preferences and behavior habits from previous observations.

In this paper, we focus on developing agents capable of learning preferences from human behavior and subsequently planning actions guided by these learned preferences. While previous studies like NeatNet \citep{kapelyukh2022my} and SAND \citep{yuan2023learning} have explored preference-based learning, they are limited to specific tasks (\eg, rearrangement) and fail to generalize across different situations. To address this limitation, we introduce \acf{pbp}, a comprehensive embodied benchmark built upon NVIDIA Omniverse and OmniGibson \citep{li2023behavior}. \ac{pbp} provides realistic simulation and real-time rendering for thousands of daily activities across 50 scenes, featuring a parameterized vocabulary of 290 diverse preferences. These preferences span multiple levels, from specific action-level preferences (\eg, preferred glass type, water temperature) to task sequence-level preferences (\eg, task ordering, subtask prioritization).

Given the expensive nature of data collection \citep{akgun2012keyframe} and the few-shot nature of preference acquisition, we frame preference learning as a few-shot learning from demonstration task. In this framework, agents must respond to ambiguous instructions by formulating plans aligned with preferences demonstrated in limited example sequences. Specifically, an agent needs to analyze behavioral data, identify consistent patterns, and extrapolate these patterns to higher-level preference abstractions that can generalize across various tasks \citep{chao2011towards}. Furthermore, when confronted with new tasks, the agent should leverage these learned preferences to generate adaptive action sequences that align with user preferences while maintaining task efficiency. 

With the \ac{pbp} benchmark developed, we challenge existing learning agents on their ability to learn human preference and subsequently conduct preference-based planning. Our systematic evaluation of \ac{sota} algorithms on \ac{pbp} reveals that preferences serve as valuable abstractions of human behaviors, and their incorporation as intermediate planning steps significantly enhances agent adaptability. Through extensive experimentation, we demonstrate that symbol-based approaches show promise in scalability, yet significant challenges remain in both preference learning and planning. These challenges stem from the complexity of planning intricate activities and the nuanced nature of learning preferences through perception. Our analysis particularly highlights the difficulties in few-shot preference learning and preference-guided planning, establishing preferences as a crucial abstraction layer between high-level goals and low-level actions. We present this work as a foundation for addressing these challenges in preference-based embodied \ac{ai}.

\section{Related Work}

\subsection{Theoretical Foundations of Human Preferences}

Preference theory originates from psychological research, where it describes predictable patterns in human behavior that can be modeled mathematically \citep{kahneman1982psychology}. These preferences reflect individual attitudes towards available choices in decision-making \citep{lichtenstein2006construction} and operate both consciously and unconsciously to shape behavior \citep{coppin2010m}. A fundamental principle is that underlying preferences can be inferred from consistent behavioral patterns \citep{sen1973behaviour}, enabling systematic analysis of decision-making processes. This framework has extended beyond psychology into economics, where Rational Choice Theory \citep{scott2000rational} models decision-making based on rational self-interest \citep{zey1998rational}. Building on this, Utility Theory provides a mathematical foundation for modeling how preferences relate to attitudes toward rewards and risks \citep{mongin1998expected, aleskerov2007utility}. These theoretical foundations establish preferences as fundamental elements in shaping both individual behavior and broader societal dynamics. In recent years, these preference models have found new applications in artificial intelligence and robotics, particularly in developing human-centric AI assistants capable of understanding and adapting to individual user preferences.

\subsection{Preference in Embodied Task Planning}

The application of preferences in embodied task planning encompasses two distinct approaches. The first focuses on \textbf{general} preference-based planning, where robots leverage commonsense knowledge to execute universally accepted behavioral norms. Object rearrangement exemplifies this approach, with systems organizing items based on common occurrence patterns and spatial relationships \citep{taniguchi2021autonomous,sarch2022tidee}. The second approach emphasizes \textbf{personalized preferences}, where embodied agents align their actions with individual user habits. This includes personalized object placement strategies \citep{abdo2015robot,kapelyukh2022my,wu2023tidybot}, preference-aware table setting \citep{puig2020watch}, and multi-agent coordination where agents maintain individual preferences while achieving optimal coordination \citep{shu2018m}.

Our work extends these approaches by considering preferences across diverse situations and scenes. Beyond spatial arrangements, we address temporal action sequences, state transitions during interactions, and few-shot preference learning. This comprehensive framework enables robust preference modeling and adaptation in real-world scenarios.

\begin{figure}[b!]
    \centering
    \small
    \begin{subfigure}{0.333\linewidth}
        \centering
        \includegraphics[trim={1.2cm 1.2cm 1.2cm 1.2cm}, clip, width=\linewidth]{fusion_action}
        \caption{Action level}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.333\linewidth}
        \centering
        \includegraphics[trim={1.2cm 1.2cm 1.2cm 1.2cm}, clip, width=\linewidth]{fusion_option}
        \caption{Option level}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.333\linewidth}
        \centering
        \includegraphics[trim={1.2cm 1.2cm 1.2cm 1.2cm}, clip, width=\linewidth]{fusion_sequence}
        \caption{Sequence level}
    \end{subfigure}%
    \caption{\textbf{Hierarchical organization of user preferences.} Our framework organizes preferences in a three-tiered structure, visualized through sunburst diagrams: (a) Action level captures fine-grained execution details within specific tasks, from quantity preferences in ``Contain'' (\eg, ``half a cup'' \vs ``full cup'') to environmental controls (\eg, lighting and window operations). (b) Option level represents spatial preferences for object categories, encoding both storage decisions (\eg, table \vs fridge for fruits) and organizational choices (\eg, shelf levels and boxes for tools/toys). (c) Sequence level defines temporal relationships between tasks, encompassing both basic preparation sequences (\eg, ``Prepare Food first'') and conditional orderings (\eg, ``Clean after Cook,'' ``[A]->[B]''). Each diagram's hierarchical structure branches from general categories to specific instances, revealing detailed preference patterns upon closer inspection. (Vector graphics; zoom in for details.)}
    \label{fig:statistics}
\end{figure}

\subsection{Embodied Assistants}

The development of intelligent embodied assistants has evolved from basic \ac{vln} tasks \citep{anderson2018vision,chen2019touchdown,thomason2020vision} to complex interactive scenarios. ALFRED \citep{shridhar2020alfred} introduced object manipulation, state tracking, and temporal dependencies between instructions, while platforms like Habitat \citep{savva2019habitat,puig2023habitat} and AI2-THOR \citep{kolve2017ai2} emphasize active perception, long-term planning, and interactive learning in realistic environments. Recent research has shifted toward implicit-instruction scenarios, particularly in housekeeping tasks \citep{kapelyukh2022my,kant2022housekeep,sarch2022tidee,wu2023tidybot}, where robots must reason about object arrangements without explicit directives. Works on proactive assistance \citep{patel2023proactive,patel2023predicting,puig2023nopa} further explore anticipating temporal patterns in humans' daily routines.

Methodologically, recent advances utilize \acp{llm} as few-shot planners to generate language-based action sequences from limited demonstrations \citep{song2023llm,driess2023palm,ding2023integrating,zhang2023building}. Foundation \acp{vlm} have enhanced robotic systems' perception and reasoning capabilities \citep{ahn2024autort,leal2023sara,gu2023rt,brohan2022rt,zitkovich2023rt,xu2024active}, enabling understanding of complex visual and linguistic inputs in everyday tasks. However, while these foundation models excel at reasoning from text or image information, their ability to learn individual preferences from limited demonstrations and plan adaptively remains an open challenge, particularly in multi-step tasks requiring personalized execution strategies.

\section{Formulating Preference-based Planning}\label{sec:formulation}

Tasks in \ac{pbp} mirror real-world watch-and-help scenarios \citep{pui2021gwatch}, where an agent observes a few demonstrations of a user performing tasks that reveal preferences. The agent must then complete similar tasks in different setups while adhering to the demonstrated preferences.

Preference-based planning comprises two key components: few-shot \textbf{preference learning} of user preferences and subsequent \textbf{planning} guided by these learned preferences. Since humans, even infants, can naturally detect others' preferences from limited decisions \citep{choi2023understanding}, and collecting extensive personal demonstrations is impractical in daily life, we formulate this as few-shot learning from demonstration.

Given a user with preference $\mathbf{p}$, the agent observes the user performing tasks from a first-person perspective, denoted as $\mathbf{O}$. These observations span multiple demonstrations. Formally, $\mathbf{O}$ contains both state and action observations: $\mathbf{O}=\{(\mathcal{S}_i, \mathcal{A}_i, \mathcal{M})_N\}$, where $\mathcal{S}_i$ denotes the egocentric observation sequence in the $i$-th demonstration, $\mathcal{A}_i$ represents the action sequence, and $\mathcal{M}$ optionally provides a bird's-eye view of the entire scene map.

In the first stage, the objective is to learn the preference representation demonstrated through user actions:
\begin{equation}
    \mathbf{p} = f(\mathbf{O}; \theta_f),
\end{equation}
where $\mathbf{p}$ denotes the learned preference representation here. It can either be a hidden representation or an explicit textual label, depending on the task settings.

The learned preference $\mathbf{p}$ should then guide planning when the agent faces different setups with varying objects, room layouts, or entire scenes. Specifically, the agent optimizes:
\begin{equation}
    \mathcal{L} = \sum_{i=1} \ell(g(s_i, f(\mathbf{O}; \theta_f); \theta_g), a_i),
\end{equation}
where $g(\cdot)$ represents a potentially parameterized planning function that maps the current state and preference representation to the next action, and $a_i$ denotes the ground-truth action demonstrating the user's preference at the current stage.

\section{The \texorpdfstring{\acf{pbp}}{} Benchmark}

Built on NVIDIA's Omniverse and OmniGibson simulation environment \citep{li2023behavior}, our \ac{pbp} benchmark enables realistic simulation of thousands of daily activities. It spans 50 distinct scenes and encodes 290 unique preferences, with a comprehensive test set of 5000 instances. Below, we detail the preference structure and test set construction.

\subsection{Definition of Preferences}\label{sec:preference}

We organize preferences in a three-tiered hierarchical structure that captures varying degrees of specificity across tasks. \Cref{fig:statistics} provides an overview of all preferences and their distribution, while \cref{fig:dataset} illustrates concrete examples of preferences and corresponding agent actions. The 290 preferences are distributed across three levels: 80 sequence-level, 135 option-level, and 75 action-level preferences.

\paragraph{Action Level}

These bottom-level preferences govern fine-grained execution details within specific sub-tasks, such as water quantity preferences when filling cups or shelf placement choices for books.

\paragraph{Option Level}

Middle-level preferences encode alternative approaches to sub-tasks. For instance, in ``storing-nonperishable-food,'' users may prefer cabinet storage versus table placement. These preferences can bind to different objects and may compose multiple action-level preferences.

\paragraph{Sequence Level}

Top-level preferences define task ordering and prioritization. They capture temporal dependencies between sub-tasks, such as cleaning furniture before rearranging kitchen utensils, followed by dinner preparation upon returning home.

\begin{figure}[t!]
    \centering
    \small
    \includegraphics[width=\linewidth]{dataset}
    \caption{\textbf{Example of preferences and their corresponding actions in \ac{pbp}.} At the primitive action level, we demonstrate preferences through basic tasks: (a) cooking using microwave, (b) washing in the sink, and (c) cutting into halves. At the option level, we showcase different approaches to object rearrangement, where users can prefer either (d) grouping objects by their categories (v1) or (e) placing them on the same layer of the fridge (v2). At the sequence level, we illustrate how preferences guide task ordering: (f) shows a user's preference to have fruits first, followed by specific cleaning tasks.}
    \label{fig:dataset}
\end{figure}

\subsection{Constructing \texorpdfstring{\ac{pbp}}{} Test Set}\label{sec:preference_tasks}

Our \ac{pbp} benchmark includes a default test set for systematic model evaluation. Following the formulation in \cref{sec:formulation}, we structure \ac{pbp} tasks as few-shot learning-from-demonstration problems. Each test point comprises several (typically three) unique demonstrations with egocentric observations of action sequences and their corresponding preference labels. As illustrated in \cref{fig:datapoint}, a demonstration includes an egocentric video of agent activity, a bird's-eye-view map tracking agent position, and frame-level action annotations. We also provide third-person view recordings for enhanced visualization. We prioritize the egocentric perspective for two reasons: 1) it offers a clear view with minimal occlusions, and 2) it aligns with human perception, facilitating transfer to real-world data from head-mounted devices.

The test set construction follows a two-stage process. First, we build a reusable and extensible demonstration pool. To generate each demonstration, we randomly assign a preference from our defined primitives to one of 50 OmniGibson scenes, then sample relevant objects within the chosen scene. We generate multi-perspective observations using rule-based planners for high-level planning and predefined scripts for low-level execution (\eg, \ac{ik} for grasping, A$^\star$ for movement).

Second, we construct test points by sampling preferences and retrieving relevant demonstrations from the pool. To reflect real-world few-shot scenarios, each preference pairs with demonstrations that share the same high-level preference but vary in scene settings or object selections. We also include unrelated demonstrations to prevent sampling bias.

The default test set contains 5,000 test points, drawing from a pool of 15,000 unique recordings. Unless specified otherwise, all experiments use this default set. The benchmark also supports custom test point generation through flexible demonstration sampling, preference definition, and third-person view video creation.

\begin{figure}[t!]
    \centering
    \small
    \includegraphics[width=\linewidth]{datapoint}
    \caption{\textbf{Example of a demonstration in \ac{pbp}.} The robot in the demonstration is executing the task ``Pick Apple from Fridge and place on Table''. \textbf{Top:} A third-person view video provides an overhead perspective of the entire scene. \textbf{Middle:} The bird's-eye-view map displays the robot's relative position within the scene. \textbf{Bottom:} The egocentric video captures the robot's first-person observations during task execution. \textbf{Text:} The per-frame action annotations contain Omniverse object IDs, which ensure each object reference is unique and enable the model to identify specific objects precisely.}
    \label{fig:datapoint}
\end{figure}

\subsection{Models}

Our evaluation focuses primarily on multimodal models that incorporate \acp{llm} and demonstrate strong few-shot learning capabilities. The \ac{llm} component serves as a knowledge base that can enhance preference learning through commonsense reasoning. We also include symbol-based \ac{llm} models for ablation studies to analyze how different modalities impact \ac{pbp} performance. Most models evaluated can function in both end-to-end and two-stage pipeline configurations.

\paragraph{\acs{vivit}}

As a baseline, we employ the pure-Transformer-based \ac{vivit} \citep{arnab2021vivit}, an end-to-end trainable model with proven capabilities in extracting spatial and temporal information from video inputs. Since it lacks a \ac{llm} component, \ac{vivit} likely serves as a lower bound for commonsense understanding in \ac{pbp} tasks.

\paragraph{\acs{llava}}

Building on more sophisticated architectures, \ac{llava} \citep{liu2024improved} represents an end-to-end trainable large multimodal model that integrates vision and text for comprehensive visual-language understanding. We specifically evaluate LLaVA-NeXT, which has been finetuned to excel at zero-shot video understanding tasks.

\paragraph{\acs{EILEV}}

For specialized egocentric video processing, we incorporate \ac{EILEV} \citep{yu2023efficient}, which achieves in-context learning through architectural modifications to a pretrained \ac{vlm}. Our implementation uses OPT-2.7B \citep{zhang2022opt} as the language backbone. The model's pretraining on Ego4D \citep{grauman2022ego4d} aligns well with \ac{pbp}'s egocentric perspective.

\paragraph{GPT-4V}

To benchmark against state-of-the-art visual-language models, we evaluate GPT-4V using the Azure OpenAI API (version ``gpt-4-turbo-2024-04-09''). Due to image token limitations, we implement video input subsampling while maintaining temporal coherence.

Beyond multimodal approaches, we also evaluate single-modal models that process only action sequences:

\paragraph{DAG-Opt}

We approach symbolic reasoning by framing the problem as a DAG-Optimization task that uncovers dependency relations between actions and preferences \citep{zheng2018dags}. Our implementation uses a score-based NOTEARS model to learn a generalized \ac{sem}, following previous few-shot reasoning frameworks \citep{zhang2021acre, xu2024interactive} based on causal dependency structures.

\paragraph{\acp{llm}}

To assess pure language understanding, we evaluate advanced \acp{llm} including Llama3 \citep{touvron2023llama} and GPT-4 \citep{achiam2023gpt} using only action sequence inputs. This approach treats actions as high-level abstractions of egocentric videos, reducing visual complexity while maintaining task semantics. We benchmark Llama3-8B as our baseline against GPT-4-Turbo as the current state-of-the-art, employing prompt designs informed by the OpenAI Cookbook for optimal few-shot performance. Detailed model architectures and prompt engineering strategies are discussed in \cref{sec:supp:baseline}.

\section{Experiments}\label{sec:experiments}

\subsection{Experimental Setup}

We evaluate preference learning capabilities across two distinct settings: end-to-end and two-stage approaches. In the end-to-end setting, models directly map raw state inputs to action outputs. Leveraging models' in-context learning abilities, we provide demonstrations alongside current state information as input and evaluate the generated action sequences against ground truth.

The two-stage setting introduces an intermediate step where models first learn to predict explicit preference labels during training. These predicted labels then serve as preference representations for subsequent planning stages. For black-box models, we employ carefully designed prompts rather than fine-tuning approaches.

All demonstration videos maintain consistent technical specifications across models and agents: egocentric perspective, 512 $\times$ 512 resolution, and 8 fps frame rate. Video duration matches the corresponding action sequence length. For \ac{llm} inference, we use conservative decoding parameters: temperature of 0.05, top-k of 1, and top-p of 0.05. All experiments run on a single machine with 8 NVIDIA A100 GPUs.

\subsection{End-to-end Action Preference Learning}

We first evaluate model performance in the end-to-end setting, where models generate actions directly from previous demonstrations and current state information. To quantify performance, we use Levenshtein distance to measure discrepancies between generated and ground truth action sequences, treating each individual action as a token.

As shown in \cref{tab:results_levenshtein_distance} (the \colorbox{LightCyan1}{End-to-end} row), video-based models produce Levenshtein distances approaching the average ground truth sequence lengths (15.80 at option level, 35.87 at sequence level). These high distances indicate that the models generate predominantly inconsistent action sequences, suggesting a failure to grasp preferences embedded in demonstration videos. While symbol-based models show modest improvements, their performance gains remain limited.

\begin{table}[t!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \caption{\textbf{Levenshtein distance between generated and ground truth action sequences.} \textbf{End-to-end} represents models directly generating action sequences from demonstration-preference pair examples. \textbf{Second-stage} indicates generation using both demonstrations and previously inferred preference labels based on demonstrations. \textbf{Second-stage (gt)} uses demonstrations alongside ground truth preference labels for sequence generation.}
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{cccccccccc}
            \toprule
            && \multicolumn{4}{c}{\textbf{VIDEO-BASED INPUT}} & & \multicolumn{2}{c}{\textbf{SYMBOL-BASED INPUT}}\\
            \midrule
            && \acs{vivit} & LLaVA-Next & EILEV & GPT-4V & & Llama3-8B & GPT-4\\
            \multirow{3}{*}{\textbf{Option Level}} & \cellcolor{LightCyan1} \textbf{End-to-end} & \cellcolor{LightCyan1} 15.49$\pm$1.29 & \cellcolor{LightCyan1} 15.94$\pm$3.41 & \cellcolor{LightCyan1} 12.88$\pm$2.20 & \cellcolor{LightCyan1} 15.63$\pm$2.31 & \cellcolor{LightCyan1} & \cellcolor{LightCyan1} 14.74$\pm$3.21 & \cellcolor{LightCyan1} 12.23$\pm$2.96\\
            & \cellcolor{MistyRose1} \textbf{Second-stage} & \cellcolor{MistyRose1} - & \cellcolor{MistyRose1} 12.46$\pm$3.23 & \cellcolor{MistyRose1} 12.89$\pm$3.74 & \cellcolor{MistyRose1} 8.37$\pm$2.19 & \cellcolor{MistyRose1} & \cellcolor{MistyRose1} 9.67$\pm$5.16 & \cellcolor{MistyRose1} 2.23$\pm$2.79\\
            & \cellcolor{MistyRose1} \textbf{Second-stage (gt)} & \cellcolor{MistyRose1} - & \cellcolor{MistyRose1} 3.28$\pm$5.29 & \cellcolor{MistyRose1} 11.18$\pm$4.20 & \cellcolor{MistyRose1} 1.26$\pm$2.55 & \cellcolor{MistyRose1} & \cellcolor{MistyRose1} 8.22$\pm$5.58 & \cellcolor{MistyRose1} 0.12$\pm$3.12\\
            \midrule
            \multirow{3}{*}{\textbf{Sequence Level}} & \cellcolor{LightCyan1} \textbf{End-to-end} & \cellcolor{LightCyan1} 34.04$\pm$11.84 & \cellcolor{LightCyan1} 34.76$\pm$11.25 & \cellcolor{LightCyan1} 33.10$\pm$12.21 & \cellcolor{LightCyan1} 33.75$\pm$11.15 & \cellcolor{LightCyan1} & \cellcolor{LightCyan1} 31.79$\pm$7.32 & \cellcolor{LightCyan1} 27.85$\pm$6.57\\
            & \cellcolor{MistyRose1} \textbf{Second-stage} & \cellcolor{MistyRose1} - & \cellcolor{MistyRose1} 30.02$\pm$13.54 & \cellcolor{MistyRose1} 33.03$\pm$13.61 & \cellcolor{MistyRose1} 27.52$\pm$9.48 & \cellcolor{MistyRose1} & \cellcolor{MistyRose1} 25.46$\pm$5.93 & \cellcolor{MistyRose1} 16.45$\pm$4.00\\
            & \cellcolor{MistyRose1} \textbf{Second-stage (gt)} & \cellcolor{MistyRose1} - & \cellcolor{MistyRose1} 18.92$\pm$14.18 & \cellcolor{MistyRose1} 26.57$\pm$12.21 & \cellcolor{MistyRose1} 11.36$\pm$8.05 & \cellcolor{MistyRose1} & \cellcolor{MistyRose1} 19.02$\pm$7.10 & \cellcolor{MistyRose1} 12.29$\pm$3.12\\
            \midrule
            \multirow{3}{*}{\textbf{Overall}} & \cellcolor{LightCyan1} \textbf{End-to-end} & \cellcolor{LightCyan1} 24.76 & \cellcolor{LightCyan1} 25.35 & \cellcolor{LightCyan1} \textbf{22.99} & \cellcolor{LightCyan1} 24.69 & \cellcolor{LightCyan1} & \cellcolor{LightCyan1} 23.26 & \cellcolor{LightCyan1} \textbf{20.04}\\
            & \cellcolor{MistyRose1} \textbf{Second-stage} & \cellcolor{MistyRose1} - & \cellcolor{MistyRose1} 21.24 & \cellcolor{MistyRose1} 22.96 & \cellcolor{MistyRose1} \textbf{17.94} & \cellcolor{MistyRose1} & \cellcolor{MistyRose1} 17.56 & \cellcolor{MistyRose1} \textbf{9.34}\\
            & \cellcolor{MistyRose1} \textbf{Second-stage (gt)} & \cellcolor{MistyRose1} - & \cellcolor{MistyRose1} 11.10 & \cellcolor{MistyRose1} 18.88 & \cellcolor{MistyRose1} \textbf{6.31} & \cellcolor{MistyRose1} & \cellcolor{MistyRose1} 13.62 & \cellcolor{MistyRose1} \textbf{6.21}\\
            \bottomrule  
        \end{tabular}%
    }%
    \label{tab:results_levenshtein_distance}
\end{table}

These findings expose a fundamental limitation in current models: they struggle to extract underlying relationships from perceptual inputs without explicit intermediate guidance. The models appear to learn individual, isolated actions rather than cohesive action patterns that reflect implicit preferences. This significant gap underscores the inherent challenge of performing end-to-end preference learning solely from demonstrations.

\subsection{Two-Stage Learning-Planning}

Given the limitations of end-to-end learning, we implement a two-stage approach to decompose the preference learning problem. The first stage focuses on preference prediction, where we provide models with auxiliary preference token labels and train them to predict hidden preferences explicitly. These preference tokens, as discussed in \cref{sec:preference}, maintain sufficient semantic content for translation into primitive actions.

Results from the first stage (\cref{tab:results_few_shot}) reveal significant performance variations across models. At the option level, GPT-4V achieves superior performance with 48.48\% accuracy, demonstrating strong capability in interpreting demonstrated preferences. Among symbol-based models, the stark contrast between DAG-Opt's limited performance and the improved results from Llama3-8B and GPT-4 highlights the advantage of next-token prediction over dependency learning for preference inference. Models with language components consistently show improved preference understanding compared to end-to-end learning.

\begin{table}[b!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \caption{\textbf{Preference prediction accuracy in few-shot setting.}}
    \rowcolors{2}{LightCyan1}{}
    \begin{tabular}{ccccccccc}
        \toprule
        & \multicolumn{4}{c}{\textbf{VIDEO-BASED INPUT}} & & \multicolumn{3}{c}{\textbf{SYMBOL-BASED INPUT}}\\
        \midrule
        & \acs{vivit} & LLaVA-Next & EILEV & GPT-4V & & DAG-Opt &Llama3-8B & GPT-4\\
        \textbf{Option Level} & 9.38 & 36.87 & 38.33 & 48.48 & & 10.15 & 72.98 & 86.27\\
        \textbf{Sequence Level} & 4.24 & 24.85 & 32.69 & 37.50 && 13.49 & 67.18 & 68.42\\
        \textbf{Overall} & 6.81 & 30.86 & 35.51 & \textbf{42.99} && 11.82 & 70.08 & \textbf{77.34}\\
        \bottomrule  
    \end{tabular}%
    \label{tab:results_few_shot}
\end{table}

The second stage involves generating action sequences based on both demonstrations and predicted preference labels from the first stage, introducing potential error propagation. Results in \cref{tab:results_levenshtein_distance} (\colorbox{MistyRose1}{Second-stage} row) and \cref{fig:distance} show significant improvements when models receive explicit preferences. For comprehensive evaluation, we include planning results using ground truth preference labels (\colorbox{MistyRose1}{Second-stage (gt)} row). GPT-4V and GPT-4 achieve near-zero Levenshtein distances, indicating almost perfect alignment with ground truth action sequences.

Analysis of both stages reveals distinct challenges across model types. Vision-based models like LLaVA-Next and GPT-4V struggle with preference inference but excel in action planning given preference labels, suggesting difficulty in abstracting preferences from visual input. Symbol-based models perform well in both preference inference and preference-guided planning, yet underperform in end-to-end settings. This indicates that models may lack innate preference-based reasoning capabilities but can effectively plan when preferences are explicitly provided.

To isolate the impact of prior knowledge versus in-context learning, we conduct ablation studies by removing demonstrations and testing preference prediction on isolated test sequences. Results in \cref{table:results_zero_shot} show significant performance degradation compared to few-shot learning (\cref{tab:results_few_shot}), particularly at the sequence level. This suggests that while models may encode basic task-specific preferences, they rely heavily on demonstrations to recognize complex preference patterns in varied sequences.

\begin{figure}[t!]
    \centering
    \small
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{distance_option}
        \caption{Option level}
    \end{subfigure}%
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{distance_sequence}
        \caption{Sequence level}
    \end{subfigure}%
    \caption{\textbf{Levenshtein distance between generated and ground truth action sequences.} Results shown for both (a) option level and (b) sequence level under two conditions: \colorbox{LightCyan1}{End-to-end} bars represent direct sequence generation from previous observations, while \colorbox{MistyRose1}{Second-stage} bars show performance when models receive predicted preference labels. The \textcolor{BurlyWood}{---} line indicates average ground truth sequence length (option level: 15.80, sequence level: 35.87). Lower distances indicate better performance. Results demonstrate significantly improved performance under the two-stage approach compared to end-to-end generation.}
    \label{fig:distance}
\end{figure}

\begin{table}[b!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \caption{\textbf{Preference prediction accuracy in ablative setting.} Evaluation of models' performance when demonstrations are removed, testing their ability to predict preferences from test sequences alone.}
    \rowcolors{2}{LightCyan1}{}
    \begin{tabular}{ccccccccc}
        \toprule
        & \multicolumn{4}{c}{\textbf{VIDEO-BASED INPUT}} & & \multicolumn{3}{c}{\textbf{SYMBOL-BASED INPUT}}\\
        \midrule
        & \acs{vivit} & LLaVA-Next & EILEV & GPT-4V & & DAG-Opt &Llama3-8B & GPT-4\\
        \textbf{Option Level} & 9.16 & 15.47 & 4.77 & 29.42 & & 3.84 & 39.50 & 73.87\\
        \textbf{Sequence Level} & 4.38 & 8.13 & 0.00 & 0.00 && 1.28 & 6.25 & 9.42\\
        \textbf{Overall} & 6.77 & 11.8 & 2.38 & \textbf{14.71} && 2.56 & 22.88 & \textbf{41.64}\\
        \bottomrule  
    \end{tabular}%
    \label{table:results_zero_shot}
\end{table}

\subsection{Generalization}\label{sec:generalization}

While human actions may vary across different objects and scenes, underlying preferences often remain consistent. We evaluate the models' ability to generalize preference learning across varying visual contexts. The original test set inherently tests generalization by randomly sampling scenes and objects when rendering video demonstrations for each preference. To gain additional insights, we conduct complementary experiments with controlled conditions where demonstration and test videos are rendered in identical rooms with the same objects. This controlled setting enables direct performance comparisons under consistent conditions. We evaluate \ac{EILEV}, \ac{llava}, and GPT-4 series models on this variant of \ac{pbp}, as these models previously demonstrated strong few-shot reasoning capabilities. Results are summarized in \cref{table:results_generalization}.

\begin{table}[ht!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \caption{\textbf{Models' generalization ability.} \textit{direct} denotes experiments conducted \textit{without} generalization. \textit{orig} denotes the original experiments conducted \textit{with} generalization cases. Also the accuracy of preference prediction.}
    \begin{tabular}{cxyxy}
        \toprule
        & LLaVA-Next & EILEV & GPT-4V & GPT-4\\
        \midrule
        \textbf{Option Level \textit{direct}} & 33.25 & 46.93 & 53.24 & 86.32\\
        \textbf{Option Level \textit{orig}} & 36.87 & 38.33 & 48.48 & 86.27\\
        \textbf{Sequence Level \textit{direct}} & 33.12 & 37.53 & 39.42 & 70.27\\
        \textbf{Sequence Level \textit{orig}} & 24.85 & 32.69 & 37.50 & 68.42\\
        \bottomrule 
    \end{tabular}
    \label{table:results_generalization}
\end{table}

\begin{figure}[b!]
    \small
    \centering
    \includegraphics[width=\linewidth]{heatmap}
    \caption{\textbf{Analysis of test samples in \textit{direct} and \textit{generalization} settings.} Lines represent distinct scenes, with grid colors indicating different sample statuses.}
    \label{fig:test_points}
\vspace{-10pt}
\end{figure}

Symbol-based reasoning (GPT-4) demonstrates consistent performance regardless of scene or object variations, while vision-based models show greater sensitivity to scene changes. This distinction stems from the nature of our predefined preferences, which are sufficiently abstract and general to apply across diverse scenes and objects. Vision-based models, however, tend to anchor their few-shot learned preferences to specific visual features of scenes or objects. When these visual elements change, preference recognition accuracy may deteriorate. This contextual dependence remains a persistent challenge for vision-based models, which often overfit to scene-specific features from training videos.

Analysis of test points across \textit{direct} and \textit{gen} conditions (\cref{fig:test_points}) reveals two key findings: (i) Preference learning performance correlates with scene characteristics, with certain scenes proving consistently challenging across both conditions. (ii) While \textit{direct} cases show better performance overall, failure patterns differ between conditions, particularly for vision-based models. This suggests models rely heavily on visual context consistency--including object arrangement and scene layout--for accurate predictions, indicating potential superficial learning rather than true preference understanding. Symbol-based reasoning maintains robust performance across varied scenes due to the general nature of predefined preferences, whereas vision-based models' strong dependence on specific visual contexts limits their generalization capability.

\subsection{Ablations on Demonstration Numbers}

We examine the effect of demonstration quantity on model performance through an ablation study (\cref{fig:test_num}). Results show that increasing demonstration numbers generally improves preference learning and planning effectiveness. This improvement is most evident in second-stage planning, where models achieve lower sequence distances by more accurately replicating human actions. Models like GPT-4, Llama3, and EILEV show consistent performance gains with additional demonstrations. However, we observe that excessive demonstrations (\eg, 5-demo cases for GPT-4 and EILVE) can sometimes impair first-stage prediction accuracy. Despite these occasional exceptions, the overall trend confirms our intuition: more demonstrations enhance learning and planning performance. These findings highlight the importance of demonstration quantity in developing effective personalized planning systems that align with user preferences.

\begin{figure}[t!]
    \centering
    \small
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{model_demo_num}
        \caption{First-stage preference learning}
    \end{subfigure}%
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{model_demo_num_2}
        \caption{Second-stage action planning}
    \end{subfigure}%
    \caption{\textbf{Ablation study on the number of demonstrations.} Models are evaluated across both of the two stages within \ac{pbp} task: (a) first-stage preference learning and (b) second-stage action planning. We evaluate both \colorbox{LightCyan1}{Option Level} and \colorbox{MistyRose1}{Sequence Level} tasks. The number of few-shot demonstrations varies from [1, 2, 3, 5], presented left to right. For (a), higher accuracy indicates better performance. For (b), lower distance indicates better performance. Results demonstrate that increased demonstration quantity generally improves both preference learning capability and planning effectiveness.}
    \label{fig:test_num}
\end{figure}

\section{Conclusion}\label{sec:conclusion}

We investigate methods for embodied agents to learn and implement human preferences through behavioral observation and user interaction. We present \acf{pbp}, a comprehensive embodied benchmark designed to capture the complexity of real-world human preferences. We also develop an evaluation framework to assess models' preference learning and implementation capabilities. Our findings demonstrate that preferences effectively abstract human behaviors and guide planning processes. While current models still face challenges in preference inference and adaptive planning from limited observations, incorporating preference-based reasoning improves both effectiveness and generalization, particularly in symbol-based systems that represent idealized scenarios. We aim to stimulate further research in this crucial yet understudied domain of developing preference-aware embodied agents.

\paragraph{Limitations and Societal Impacts}

Our work's primary limitation stems from its reliance on synthetic data. Despite Omniverse's high-quality scene rendering, the simulator cannot fully replicate real-world complexity and variability. Furthermore, human-defined preference labels may not completely capture preference subtleties and diversity. We are addressing these limitations by collecting real-world preference demonstrations using head-worn devices, despite associated challenges. Given our focus on private scenarios, we anticipate minimal negative societal impact from this research.

\paragraph*{Acknowledgment}

The authors would like to thank Yuyang Li and Bo Dai for helpful discussions, and NVIDIA for their generous support of GPUs and hardware. This work is supported in part by the National Science and Technology Major Project (2022ZD0114900), the National Natural Science Foundation of China (62376031), the Beijing Nova Program, the State Key Lab of General AI at Peking University, the PKU-BingJi Joint Laboratory for Artificial Intelligence, and the National Comprehensive Experimental Base for Governance of Intelligent Society, Wuhan East Lake High-Tech Development Zone.

{
\small
\bibliographystyle{apalike}
\bibliography{reference_header,references}
}

\clearpage
\appendix
\renewcommand\thefigure{A\arabic{figure}}
\setcounter{figure}{0}
\renewcommand\thetable{A\arabic{table}}
\setcounter{table}{0}
\renewcommand\theequation{A\arabic{equation}}
\setcounter{equation}{0}
\pagenumbering{arabic}% resets `page` counter to 1
\renewcommand*{\thepage}{A\arabic{page}}
\setcounter{footnote}{0}

\section{Dataset Card}\label{sec:supp:dataset}

We follow the datasheet proposed in \citet{gebru2021datasheets} for documenting our proposed \ac{pbp}:
\begin{enumerate}
    \item \textbf{Motivation}
    \begin{enumerate}
    \item \textbf{For what purpose was the dataset created?}\\
    The benchmark was created to evaluate existing learning agents on their ability to understand and adapt to various human preferences. Specifically, it aims to test the agents' proficiency in few-shot learning from demonstrations, where they must respond to ambiguous task instructions and formulate adaptive task plans based on limited examples of user preferences. The benchmark is designed to highlight the challenges and gaps in current AI systems' capabilities in planning activities and abstracting human preferences, ultimately driving advancements towards developing more intelligent and personalized embodied agents.
    \item \textbf{Who created the dataset and on behalf of which entity?}\\
    N/A.
    \item \textbf{Who funded the creation of the dataset?}\\
    N/A.
    \item \textbf{Any other Comments?}\\
    None.
    \end{enumerate}
    \item \textbf{Composition}
    \begin{enumerate}
        \item \textbf{What do the instances that comprise the dataset represent?}\\
        Each instance contains an egocentric video of an agent's activity, its bird's-eye-view map of the position of the agent, and a frame-level textual annotation of the current action, as shown in \cref{fig:datapoint}. Additionally, we provide a rendered third-person view of the entire process.
        \item \textbf{How many instances are there in total?}\\
        15000. 
        \item \textbf{Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?}\\
        No. The dataset contains a set of demonstrations rendered within the simulator, The users can render more diverse instances if they want. We have provided the rendering instructions.
        \item \textbf{What data does each instance consist of?}\\
        The instances that comprise the benchmark represent various types of human preferences applied to different tasks within a realistic embodied scene. Each instance is designed to challenge the learning agents to understand and adapt to these preferences based on a few demonstration examples, reflecting the diverse and hierarchical nature of user preferences in real-world scenarios. See above for data details.
        \item \textbf{Is there a label or target associated with each instance?}\\
        Yes.
        \item \textbf{Is any information missing from individual instances?}\\
        No.
        \item \textbf{Are relationships between individual instances made explicit?}\\
        Yes.
        \item \textbf{Are there recommended data splits?}\\
        No.
        \item \textbf{Are there any errors, sources of noise, or redundancies in the dataset?}\\
        No.
        \item \textbf{Is the dataset self-contained, or does it link to or otherwise rely on external resources (\eg, websites, tweets, other datasets)?}\\
        Self-contained.
        \item \textbf{Does the dataset contain data that might be considered confidential (\eg, data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?}\\
        No.
        \item \textbf{Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?}\\
        No.
        \item \textbf{Does the dataset relate to people?}\\
        No.
        \item \textbf{Does the dataset identify any subpopulations (\eg, by age, gender)?}\\
        No.
        \item \textbf{Is it possible to identify individuals (\ie, one or more natural persons), either directly or indirectly (\ie, in combination with other data) from the dataset?}\\
        No.
        \item \textbf{Does the dataset contain data that might be considered sensitive in any way (\eg, data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?}\\
        No.
        \item \textbf{Any other comments?}\\
        None.
    \end{enumerate}
    \item \textbf{Collection Process}
    \begin{enumerate}
        \item \textbf{How was the data associated with each instance acquired?}\\
        We render \ac{pbp} using NVIDIA's Omniverse and OmniGibson simulation environment \citep{li2023behavior}. 
        \item \textbf{What mechanisms or procedures were used to collect the data (\eg, hardware apparatus or sensor, manual human curation, software program, software API)?}\\
        The data for each instance in the benchmark was acquired by sampling preferences from a predefined set and constructing tasks paired with a few demonstrations that shared high-level preferences but differed in specific objects and scenes. Each sampled preference was randomly assigned to one of the 50 scenes provided by OmniGibson, with relevant objects sampled within the scene. Egocentric observation and action sequences of an embodied agent were generated as the agent performed tasks guided by a rule-based planner using planning primitives like inverse kinematics for grasping and the A* algorithm for movement.  
        \item \textbf{If the dataset is a sample from a larger set, what was the sampling strategy (\eg, deterministic, probabilistic with specific sampling probabilities)?}\\
        N/A.
        \item \textbf{Who was involved in the data collection process (\eg, students, crowdworkers, contractors) and how were they compensated (\eg, how much were crowdworkers paid)?}\\
        N/A. 
        \item \textbf{Over what timeframe was the data collected?}\\
        N/A.
        \item \textbf{Were any ethical review processes conducted (\eg, by an institutional review board)?}\\
        The dataset raises no ethical concerns.
        \item \textbf{Does the dataset relate to people?}\\
        No.
        \item \textbf{Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (\eg, websites)?}\\
        N/A.
        \item \textbf{Were the individuals in question notified about the data collection?}\\
        N/A.
        \item \textbf{Did the individuals in question consent to the collection and use of their data?}\\
        N/A.
        \item \textbf{If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?}\\
        N/A.
        \item \textbf{Has an analysis of the potential impact of the dataset and its use on data subjects (\eg, a data protection impact analysis) been conducted?}\\
        Yes.
        \item \textbf{Any other comments?}\\
        None.
    \end{enumerate}
    \item \textbf{Preprocessing, Cleaning and Labeling}
    \begin{enumerate}
        \item \textbf{Was any preprocessing/cleaning/labeling of the data done (\eg, discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?}\\
        N/A.
        \item \textbf{Was the ``raw'' data saved in addition to the preprocessed/cleaned/labeled data (\eg, to support unanticipated future uses)?}\\
        N/A.
        \item \textbf{Is the software used to preprocess/clean/label the instances available?}\\
        N/A.
        \item \textbf{Any other comments?}\\
        None.
    \end{enumerate}
    \item \textbf{Uses}
    \begin{enumerate}
        \item \textbf{Has the dataset been used for any tasks already?}\\
        No, the dataset is newly proposed by us.
        \item \textbf{Is there a repository that links to any or all papers or systems that use the dataset?}\\
        No, the dataset is new.
        \item \textbf{What (other) tasks could the dataset be used for?}\\
        This dataset could be used for research topics like embodied AI and human-computer interaction.
        \item \textbf{Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?}\\
        N/A.
        \item \textbf{Are there tasks for which the dataset should not be used?}\\
        N/A.
        \item \textbf{Any other comments?}\\
        None.
    \end{enumerate}
    \item \textbf{Distribution}
    \begin{enumerate}
        \item \textbf{Will the dataset be distributed to third parties outside of the entity (\eg, company, institution, organization) on behalf of which the dataset was created?}\\
        No before it is made public.
        \item \textbf{How will the dataset be distributed (\eg, tarball on website, API, GitHub)?}\\
        On our project website upon acceptance.
        \item \textbf{When will the dataset be distributed?}\\
        Upon acceptance.
        \item \textbf{Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?}\\
        Under CC BY-NC \footnote{\url{https://creativecommons.org/licenses/by-nc/4.0/}} license.
        \item \textbf{Have any third parties imposed IP-based or other restrictions on the data associated with the instances?}\\
        No.
        \item \textbf{Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?}\\
        No.
        \item \textbf{Any other comments?}\\
        None.
    \end{enumerate}
    \item \textbf{Maintenance}
    \begin{enumerate}
        \item \textbf{Who is supporting/hosting/maintaining the dataset?}\\
        The authors.
        \item \textbf{How can the owner/curator/manager of the dataset be contacted (\eg, email address)?}\\
        N/A.
        \item \textbf{Is there an erratum?}\\
        Future erratum will be released through the website.
        \item \textbf{Will the dataset be updated (\eg, to correct labeling errors, add new instances, delete instances')?}\\
        Yes.
        \item \textbf{If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (\eg, were individuals in question told that their data would be retained for a fixed period of time and then deleted)?}\\
        N/A. The dataset does not relate to people.
        \item \textbf{Will older versions of the dataset continue to be supported/hosted/maintained?}\\
        Yes.
        \item \textbf{If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?}\\
        Yes. We will release the source code as well as a licence on our project website after acceptance. 
        \item \textbf{Any other comments?}\\
        None.
    \end{enumerate}
\end{enumerate}

\section{Dataset Statistics}

The length of the simulations in the dataset ranges from 1 to 5 minutes, depending on the tasks recorded. And the videos are recorded at 30 fps.

\subsection{Preferenes}

See \cref{tab:dataset_statistics} for the preference statistics in \ac{pbp}.
\begin{table}[!h]
    \setlength{\tabcolsep}{3pt}
    \caption{\textbf{Dataset Statistics in \ac{pbp}.}}
    \centering
    \small
    \rowcolors{2}{LightCyan1}{}
    \begin{tabular}{cccc} \toprule
          & \textbf{Action\_Level} & \textbf{Option\_Level} & \textbf{Sequence\_Level}\\ \midrule
         \textbf{Preference Num} & 75 & 135 & 80\\ 
         \textbf{Video Num} & 5000 & 5000 & 5000\\
         \textbf{Sub-task Num} & 1 & 2-3 & 2-3 \\ \bottomrule
    \end{tabular}
    \label{tab:dataset_statistics}
\end{table}

\subsection{Actions}\label{sec:supp:statistics}

See \cref{tab:action_statistics} for the action statistics in \ac{pbp}. We implement 17 action primitives in \ac{pbp} to assist with model planning and dataset rendering. These action primitives have parameters that simplify tasks and are considered the lowest-level actions. Each sub-task contains 8 to 20 such lowest-level actions. Generally, most of these actions consist of two parts: the robot movement part and the arm (gripper) execution part. For robot movement, we use the A* algorithm to find paths and avoid collisions. We build a connection map during scene initialization for navigation, taking the robot's width into consideration. For the arm (gripper) execution, we primarily use the \ac{ik} algorithm to compute arm movements. However, since \ac{ik} cannot handle complex tasks, such as picking objects from the fridge, we also leverage the \ac{ompl} planner \citep{sucan2012open} with forward planning to assist in planning the arm positions.

\subsection{More Dataset Details and Discussion}
\paragraph{Dataset production} The process of producing data is mainly explained in \cref{sec:preference_tasks}. In summary, we follow the order of ``sample preference - sample scene - sample objects to be manipulated - generate actions guided by a rule-based planner''.
\paragraph{Length and FPS of the simulations} The length of the simulations ranges from 1 to 5 minutes, depending on the tasks recorded. The videos are recorded at 30 fps. 
\paragraph{Actions contained in each simulation} The number of actions in simulations varies among different preference levels. There is 1 subtask for action-level, 2-3 subtasks for option-level, and 2-3 subtasks for sequence-level preferences. Each subtask contains 8-20 actions. 
\paragraph{Scenes and rooms} Each scene contains various types of rooms. The main differences between scenes are the type, number, and layout of both rooms and furniture. Additionally, each room may contain different objects and have unique layouts. Details of the scenes and rooms can be found in Omnigibson's official documentation (https://behavior.stanford.edu/omnigibson/), as we directly adopt these scenes from the open-sourced project. 
\paragraph{290 preference types} Considering that preferences in household activities are not only multi-dimensional but also hierarchical, we first define a hierarchy of preferences from the perspective of how things happen in a life scenario, that is, from each specific action to a sub-task consisting of several actions, and then to the sequence combining these sub-tasks. The next step is to expand each level with typical tasks and actions. The detailed definition of the 290 preferences can be found in \cref{sec:preference}.
\paragraph{The egocentric view} Collecting both egocentric observations and third-person views is feasible in PbP or similar environments built on simulators like iGibson. However, in real-world scenarios, it is generally easier to gather egocentric observations of human daily activities, as these can be efficiently captured through wearable devices. Additionally, there are numerous egocentric-view datasets available, such as Ego4D\citep{grauman2022ego4d}, which further facilitate this approach. While third-person views can provide a different perspective, they often encounter issues such as occlusion. Although research based on third-person views is essential for applications involving real robots, focusing on egocentric views in the current work allows for a more straightforward exploration of preference learning and planning. Nevertheless, third-person view data can be obtained by integrating additional cameras, as outlined in our provided code.
\paragraph{Action ground truth} In experiments involving vision input, we do not explicitly provide the action sequence of the user. In the symbolic-based experiment, we provide the action sequence to reduce the perception cost to concentrate more effectively on the inference and planning aspects of the study. 

\begin{table}[ht!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \caption{Action Primitives in \ac{pbp}.}
    \rowcolors{2}{LightCyan1}{}
    \begin{tabular}{cc} \toprule
        \textbf{Action List} & \textbf{Explanation}\\ \midrule
        \textbf{Move\_to\_[]} & Move to a specified location, or a specified room, or a specified object\\ 
        \textbf{Rotate\_to\_[]} & Rotate to a specified orientation or a specified object\\ 
        \textbf{Pick\_[]} & Pick up an object using the gripper, \eg, ``Pick\_apple''\\ 
        \textbf{Place\_[]} & Place an object at a location, \eg, ``Place\_apple\_on\_table''\\ 

        \textbf{Fill\_[]\_with\_[]} & Fill a container with a substance, \eg, ``Fill\_glass\_with\_water''\\ 
        \textbf{Pour\_[]} & Pour a substance from a container, \eg, ``Pour\_milk''\\ 
        \textbf{Open\_[]} & Open an object, \eg, ``Open\_door''\\ 
        \textbf{Close\_[]} & Close an object, \eg, ``Close\_fridge''\\ 
        \textbf{Cut\_[]} & Cut an object, \eg, ``Cut\_carrot''\\ 
        \textbf{Cook\_[]} & Cook an item, \eg, ``Cook\_pasta''\\ 
        \textbf{Wash\_[]} & Wash an object, \eg, ``Wash\_dishes''\\ 
        \textbf{Clean\_[]} & Clean a surface or object, \eg, ``Clean\_counter''\\ 
        \textbf{Cover\_[]} & Cover an object, \eg, ``Cover\_bowl''\\ 
        \textbf{Uncover\_[]} & Uncover an object, \eg, ``Uncover\_bowl''\\ 
        \textbf{Toggle\_on\_[]} & Turn on a device, \eg, ``Toggle\_on\_light''\\ 
        \textbf{Toggle\_off\_[]} & Turn off a device, \eg, ``Toggle\_off\_stove''\\ 
        \textbf{Wait\_[]} & Wait some time\\
        \bottomrule
    \end{tabular}
    \label{tab:action_statistics}
\end{table}

\section{Experiment Details}\label{sec:supp:experiment}

\subsection{Case Study}

We also provide a case with preference \textit{Put fruit on the bed} in the following table. We present a simplified version of the demonstrations, where all video outputs have been translated into symbol-based action sequences for ease of understanding. Video-based models such as LLaVA-Next and GPT-4V struggle with comprehending preferences and tend to replicate certain action patterns from the video demonstration, such as ``move to'' and ``pick up''. Llama3 demonstrates a partial understanding and execution of the preference. It correctly moves to each fruit (grape, banana), picks them up, and places them on the bed. However, it also interacts with the pencil and places it on the bed, which is not required by the preference. Ideally, the pencil should be placed on the table, similar to the pen. On the other hand, GPT-4(Symbol) accurately interacts with the grape and banana by moving to each fruit, picking it up, and placing it on the bed. This demonstrates a better understanding and execution of the preference compared to the other models.

\begin{table}[ht!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{cp{3cm}p{2cm}p{7cm}}
        \toprule
        \textbf{Case} & \textbf{ID} & \textbf{Scene} & \textbf{Actions}\\
        \midrule
        \textbf{Context} & Demo 1 & Beechwood & move to orange, pick up orange, move to bed with orange, place orange on bed, move to apple, pick up apple, move to bed with apple, place apple on bed, move to dvd, pick up dvd, move to table with dvd, place dvd on table. \\
         & Demo 2 & Rs & move to banana, pick up banana, move to bed with banana, place banana on bed, move to club sandwich, pick up club sandwich, move to countertop with club sandwich, place club sandwich on countertop, move to orange, pick up orange, move to countertop with orange, move to bed with orange, place orange on bed. \\
        & Demo 3 & Merom & move to pen, pick up pen, move to table with pen, place pen, move to strawberry, pick up strawberry, move with to bed with strawberry, place strawberry on bed, move to textbook, pick up textbook, move with textbook, move to bookshelf with textbook, place textbook in bookshelf. \\\midrule
        \textbf{Test Scene} & & Beechwood & grape, pencil, banana \\\midrule
        \textbf{Answer} & LLaVA-Next & Beechwood & move to grape, move to bed with grape,       move to pencil, pick up pencil, move to banana, pick up banana \\
         & GPT-4V & Beechwood & move to grape, pick up grape, move to bed with grape, place grape on bed \\
         & Llama3 & Beechwood & move to grape, pick up grape, move to bed with grape, place grape on bed, move to pencil, pick up pencil, move to bed with pencil, place pencil on bed, move to banana, pick up banana, move to bed with banana, place banana on bed \\
         & GPT-4(Symbol) & Beechwood & move to grape, pick up grape, move to bed with grape, place grape on bed, move to banana, pick up banana, move to bed with banana, place banana on bed \\
        \bottomrule 
    \end{tabular}
    \caption{Case Study with preference \textit{Put fruit on the bed}.}
\end{table}

\section{Baseline Details}\label{sec:supp:baseline}

\subsection{ViViT}

Inspired by Vision Transformer, \ac{vivit} extracts spatio-temporal tokens from the input video and outputs video classification labels for classification. We adopt the \ac{vivit} implementation from the official GitHub repo \url{https://github.com/google-research/scenic}. 

Specifically, we utilize a \ac{vivit} with an image size of 224 and a patch size of 16. We extract 2 frames per second from the input video and pad them with the last frame. The Transformer architecture with 3 attention heads operates on features of hidden size of 192 and depth of 4. Each attention head operates on a dimension of 64. We train our model for 30 epochs with a learning rate 3e-5. For the few-shot setting, we concatenate the demo videos temporally.

\subsection{\acs{llava}}

Following the official implementation of \acs{llava} from \url{https://github.com/LLaVA-VL/LLaVA-NeXT}, we test the LLaVA-NeXT-Video-7B-DPO model which is designed for video understanding. Specifically, we run the model following the default inference settings, with vicuna\_v1 as the prompt mode, a sample frame number of 32, and a spatial pooling stride of 2. The textual prompts are as follows\footnote{For the textual prompts, we aim to maintain consistency across all \acp{llm}, although some baselines may have additional requirements for the input format. The prompt design is mainly motivated by OpenAI Cookbook \url{git@github.com:openai/openai-cookbook.git}. We omitted the prompt tuning process, as we found that minor changes in the prompt were unlikely to significantly impact the results. Conversely, selecting the proper demonstrations in the few-shot examples has a much greater influence on the results.}:

\begin{Verbatim}

Stage One / Preference Prediction
You are a robot assistant that can help summarize the host's preference.
All possible preferences are: {ALL POSSIBLE PREFERENCES}
Now there are some prevous video demos:
[VIDEO_DEMO_1] The preference is [PREFERENCE_1]
[VIDEO_DEMO_2] The preference is [PREFERENCE_2]
[VIDEO_DEMO_3] The preference is [PREFERENCE_3]
Now, please summarize the preference from the last video: [TEST_CASE]
Quesiton: What's the user's preference? Choose from the preference listed before:

Stage Two / Planning
You are a robot assistant. Please view the demos and help generate action sequence.
All possible preferences are: {ALL POSSIBLE ACTIONS}
Now there are some prevous video demos:
[VIDEO_DEMO_1]
[VIDEO_DEMO_2]
[VIDEO_DEMO_3]
Now you are in the scene with [SCENE DESCRIPTIONS]. Your action sequence is:
\end{Verbatim}

\subsection{\acs{EILEV}}

Following the official implementation from \url{https://github.com/yukw777/EILEV.git}, we test the \acs{EILEV} model in \ac{pbp}. There are two reasons we chose \acs{EILEV} among other \acp{vlm} as one of our baselines: 1) \acs{EILEV} elicits in-context learning through a series of architectural modifications and a unique training process, 2) \acs{EILEV} is trained using ego-centric data, which is compatible with \ac{pbp}'s input. The textual prompts are as follows. Since \acs{EILEV} requires the input of the videos and texts to follow a certain pattern for better in-context learning, there are some small modifications to the prompt:

\begin{Verbatim}
Stage One / Preference Prediction
You are a robot assistant that can help summarize the host's preference.
All possible preferences are: {ALL POSSIBLE PREFERENCES}
Quesiton: What's the user's preference? Choose from the preference listed before:
Now there are some prevous video demos:
[VIDEO_DEMO_1] The preference is [PREFERENCE_1]
[VIDEO_DEMO_2] The preference is [PREFERENCE_2]
[VIDEO_DEMO_3] The preference is [PREFERENCE_3]
[TEST_CASE]

Stage Two / Planning
You are a robot assistant. Please view the demos and help generate action sequence.
All possible preferences are: {ALL POSSIBLE ACTIONS}
Now there are some prevous video demos:
[VIDEO_DEMO_1]
[VIDEO_DEMO_2]
[VIDEO_DEMO_3]
Now you are in the scene with [SCENE DESCRIPTIONS]. Your action sequence is:

\end{Verbatim}

\subsection{GPT-4V}

We run our GPT-4 model through the AzureOpenAI API using the GPT version ``gpt-4-turbo-2024-04-09''. The API has a limit of 10 images per request. Consequently, for the zero-shot setting, we resample each input video to 8 frames of size 224. For the few-shot setting, where we need to input 3 extra video demonstrations, we concatenate 4 images into a frame, thereby obtaining 4 videos in 8 frames, maintaining the same frame number as the previous setting. We test the model with a temperature of 0.05. The textual prompts are as follows:

\begin{Verbatim}
Stage One / Preference Prediction
You are a robot assistant that can help summarize the host's preference.
All possible preferences are: {ALL POSSIBLE PREFERENCES}
Now there are some prevous video demos:
[VIDEO_DEMO_1] The preference is [PREFERENCE_1]
[VIDEO_DEMO_2] The preference is [PREFERENCE_2]
[VIDEO_DEMO_3] The preference is [PREFERENCE_3]
Now, please summarize the preference from the last video: [TEST_CASE]
Quesiton: What's the user's preference? Choose from the preference listed before:

Stage Two / Planning
You are a robot assistant. Please view the demos and help generate action sequence.
All possible preferences are: {ALL POSSIBLE ACTIONS}
Now there are some prevous video demos:
[VIDEO_DEMO_1]
[VIDEO_DEMO_2]
[VIDEO_DEMO_3]
Now you are in the scene with [SCENE DESCRIPTIONS]. Your action sequence is:
\end{Verbatim}

\subsection{DAG-Opt}

We implement the DAG-Opt baseline following \url{https://github.com/xunzheng/notears.git}. Specifically, we implement a nonlinear NOTEARS using MLP in evaluation.

\subsection{Llama3-8B}

We test the Llama3 series model with the official scripts from \url{https://github.com/meta-llama/llama3}. Specifically, we test the 8B instruction-tuned variant ``Meta-Llama-3-8B-Instruct'' on \ac{pbp}. We test the model with a temperature of 0.05. The textual prompts are as follows:

\begin{Verbatim}
Stage One / Preference Prediction
You are a robot assistant that can help summarize the host's preference.
Please read the following text file and summarize the user's preference.
All possible preferences are: {ALL POSSIBLE PREFERENCES}
[TEXT_ANNOTATION_1] The preference is [PREFERENCE_1]
[TEXT_ANNOTATION_2] The preference is [PREFERENCE_2]
[TEXT_ANNOTATION_3] The preference is [PREFERENCE_3]
Now, please summarize the preference from the last tet file: [TEST_CASE]
Quesiton: What's the user's preference? Choose from the preference listed before:

Stage Two / Planning
You are a robot assistant. Please read the following text files and help generate action sequence.
All possible preferences are: {ALL POSSIBLE ACTIONS}
Now there are some prevous video demos:
[TEXT_ANNOTATION_1] (action sequence)
[TEXT_ANNOTATION_2] (action sequence)
[TEXT_ANNOTATION_3] (action sequence) 
Now you are in the scene with [SCENE DESCRIPTIONS]. Your action sequence is:
\end{Verbatim}

\subsection{GPT-4}

We use ``gpt-4-turbo-2024-04-09'' with a temperature of 0.05. The textual prompts are as follows:

\begin{Verbatim}
Stage One / Preference Prediction
You are a robot assistant that can help summarize the host's preference.
Please read the following text file and summarize the user's preference.
All possible preferences are: {ALL POSSIBLE PREFERENCES}
[TEXT_ANNOTATION_1] The preference is [PREFERENCE_1]
[TEXT_ANNOTATION_2] The preference is [PREFERENCE_2]
[TEXT_ANNOTATION_3] The preference is [PREFERENCE_3]
Now, please summarize the preference from the last tet file: [TEST_CASE]
Quesiton: What's the user's preference? Choose from the preference listed before:

Stage Two / Planning
You are a robot assistant. Please read the following text files and help generate action sequence.
All possible preferences are: {ALL POSSIBLE ACTIONS}
Now there are some prevous video demos:
[TEXT_ANNOTATION_1] (action sequence)
[TEXT_ANNOTATION_2] (action sequence)
[TEXT_ANNOTATION_3] (action sequence) 
Now you are in the scene with [SCENE DESCRIPTIONS]. Your action sequence is:
\end{Verbatim}



\clearpage
\end{document}