\section{Related Work}
\subsection{Traditional ACS Prediction Methods}

Acute Coronary Syndrome (ACS) risk prediction methods are essential tools for estimating an individual's likelihood of experiencing cardiovascular events. Traditional methods for predicting ACS risk such as Framingham Risk Score (FRS)\cite{Hemann2007Framingham}, QRISK\cite{Hippisley2017Development}, Reynolds Risk Score \cite{Ridker2007Development} and European SCORE System \cite{Nashef1999European} utilise a combination of demographic, clinical, lifestyle and medical history data to provide an overall risk assessment, but frequently cannot be utilised once a diagnosis of underlying cardiovascular disease has been made. The data used can be categorised as follows: 1) Demographic Information: Age, gender, and family history of cardiovascular diseases\cite{Steen2022Event}. 2) Clinical History: Previous incidences of myocardial infarction, angina, hypertension, diabetes, and hypercholesterolemia. 3) Laboratory and and Clinical Tests: Blood pressure readings, cholesterol levels, and creatinine levels. 4) Electrocardiogram (ECG) Readings: Used to diagnose the type and severity of myocardial infarctions (e.g., Discrimination between STEMI and NSTEMI) \cite{Bhatt2022Diagnosis}.

These traditional methods systematically assess cardiovascular event risk. The FRS, Reynolds Risk Score, National Early Warning Score (NEWS) \cite{Smith2013ability}  and Global Registry of Acute Coronary Events (GRACE) (‘Rationale and Design of the GRACE (Global Registry of Acute Coronary Events) Project: A Multinational Registry of Patients Hospitalized with Acute Coronary Syndromes’ 2001) primarily relied on point-based systems incorporating traditional clinical and lifestyle factors. Each risk factor was assigned a specific number of points based on predefined scales. The QRISK algorithm employed a comprehensive multivariate approach using a wide array of data points to reflect a diverse population. The European SCORE system utilised regional risk charts to account for geographic variability in cardiovascular disease prevalence. 

\subsection{Machine Learning in ACS Prediction}

While these traditional models have been instrumental in guiding clinical practice, they have notable limitations in adaptability, data integration, and predictive accuracy.  Machine learning/deep learning methods offer significant advantages in these areas, providing more dynamic, accurate, and comprehensive risk assessments by leveraging advanced data analytics and continuous learning capabilities \cite{Ke2022Machine}.  Wu et al. \cite{Wu2021Machine} utilised machine learning to predict in-hospital cardiac arrest in ACS patients, finding that the XGBoost model outperformed traditional risk scores such as GRACE and NEWS , achieving high accuracy and AUC. A total of 45 risk features were selected in this work from the electronic health record including age, gender, history of smoking and laboratory features, Killip classification, vital signs, mental status, etc.  Similarly, Hadanny et al. \cite{Hadanny2022Machine} used Random Survival Forest (RSF) and deep neural network (DeepSurv) models to predict 1-year mortality in ACS patients, highlighting the improved performance of RSF over traditional methods. Acute Coronary Syndrome Israeli Survey (ACSIS) and the Myocardial Ischemia National Audit Project (MINAP) data were used in this work. 69 risk factors including demographics, prior medical history, prior medication, clinical presentation, basic laboratory data with admission were selected and evaluated.

D’Ascenzo et al. \cite{D2021Machine} developed the PRAISE risk scores, a machine learning tool validated with external cohorts, which showed high accuracy in predicting post-discharge outcomes for ACS patients. The 25 risk factors included 16 clinical variables, 5 therapeutic variables, and 2 angiographic variables. Research by Emakhu et al. \cite{Emakhu2022Acute} compared various machine learning techniques for predicting ACS outcomes based on 58 variables, including demographic and clinical factors ( e.g. brain natriuretic peptide (BNP), creatinine, glucose, heart rate, red cell distribution width, systolic blood pressure, and troponin), with models like XGBoost and RSF significantly improving prediction accuracy over traditional methods. Ke et al. \cite{Ke2022Machine} focused on early ACS onset prediction using ensemble methods such as gradient boosting, demonstrating the benefits of integrating machine learning into clinical practice for enhanced early detection and management of ACS using demographic characteristics, comorbidities, thrombolytic therapy, laboratory test data, and physical examination data. The study in \cite{Lee2021Machine} concluded that the ML-based approach improved the prediction of mortality, particularly in patients with non-ST-segment elevation myocardial infarction (NSTEMI) based on demographic characteristics, medical history, symptom, initial presentation, laboratory findings, clinical manifestation, echocardiographic finding, coronary angiographic finding, and medication at discharge. 

In recent years, deep learning techniques have shown significant advancements in various healthcare applications, including cardiovascular disease prediction. In \cite{duan2019utilizing}, a deep learning-based approach is introduced to analyse a massive volume of heterogeneous electronic health records in order to predict MACEs following ACS. In \cite{liu2023deep}and \cite{liu2021deep}, the authors introduce deep learning model for classifying ACS abnormalities using electrocardiogram (ECG) data. However, due to the structured and tabular nature of clinical data, the application of deep learning models in ACS prediction remains relatively limited compared to traditional machine learning approaches.




\subsection{Environmental Factors and ACS}


The effects of environmental factors on ACS have also been increasingly studied. Ku\'{z}ma et al. \cite{Kuzma2021Impact} investigated the short-term impact of air pollution on ACS incidence in industrial versus non-industrial areas, finding significant associations between air pollution and ACS admissions. Chen et al. \cite{Chen2022Hourly} examined hourly air pollutant concentrations and their association with ACS onset, employing a case-crossover design that revealed a strong link between short-term exposure to pollutants and increased ACS risk. Gestro et al. \cite{Gestro2020Short} developed models to analyse the delayed effects of air pollutants on emergency department admissions for ACS, underscoring the need for continuous air quality monitoring.

However, environmental time series data have more complex temporal properties than clinic data.  Current approaches often rely on manually selected features from time series datasets, including basic statistical measures such as mean, variance, and autocorrelation. These methods may not capture all relevant information, particularly complex, non-linear patterns. Recently, deep learning has emerged as a powerful tool for time series analysis, especially in examining the impact of air pollution on health outcomes. Various deep learning algorithms have been employed to analyse time series environmental data, such as Recurrent Neural Networks (RNNs) \cite{Husken2003Recurrent,Hewamalage2021Recurrent}, Multi-Layer Perceptrons (MLPs)\cite{Mayer1999Evolutionary}, Convolutional Neural Networks (CNNs)\cite{Zhao2017Convolutional}, and Transformers \cite{Zeng2023Are}.


\textbf{RNNs} are specifically designed for sequence data, making them a natural fit for time series analysis. Their internal state allows them to retain information from previous inputs, which is crucial for understanding temporal dependencies. For instance, Villegas et al. \cite{Villegas2023Predicting} proposed a predictive model for COVID-19 mortality risk using RNNs with attention mechanisms to enhance interpretability. Results indicate that the RNN model outperforms traditional baselines like Support Vector Classifier and Random Forest in sensitivity and overall stability.  

\textbf{MLPs}, though simpler than other models, can effectively model temporal dependencies when applied along the temporal dimension. MLP methods encode these temporal dependencies into the fixed parameters of MLP layers, adopting the MLP framework along the temporal axis. This approach allows MLPs to model sequential patterns in time-series data, providing a straightforward yet powerful means to analyse complex temporal relationships. Suttaket and Kok \cite{Suttaket2024Interpretable} introduced Rational Multi-Layer Perceptrons (RMLP) as a novel interpretable predictive model for healthcare, addressing the limitations of deep learning’s black-box nature. The model combined the strengths of weighted finite state automata and multi-layer perceptrons to process sequential data from electronic health records (EHRs). The study demonstrated that RMLP achieved strong predictive accuracy and enhanced interpretability across six clinical tasks. 

\textbf{CNNs} are traditionally used for image processing but have been adapted for time series analysis\cite{Muller2024IMU}. The key advantage of CNNs is their ability to capture local patterns and hierarchical features through convolutional operations.

\textbf{Transformers}, introduced by Vaswani et al. \cite{Vaswani2017Attention}, have revolutionised sequence modelling by relying on self-attention mechanisms rather than recurrent structures. This allows for parallel processing of data and better handling of long-range dependencies. Ni et al. \cite{Ni2024Time} compared traditional time series models such as ARIMA and Prophet with advanced transformers based models for predicting heart rate dynamics. The study demonstrated that deep learning approaches, particularly transformer-based models like PatchTST \cite{Nie2023Time}, significantly outperformed traditional methods in capturing complex temporal dependencies and non-linear relationships. 

Despite their success, each of these models has specific strengths and limitations that make them suitable for different aspects of time series analysis. RNNs and LSTMs are excellent for handling temporal dependencies but are computationally intensive and complex. MLPs are simpler but less effective for complex time series compared to RNNs or LSTMs and less flexible for variable-length sequences due to requiring fixed input sizes. CNNs excel at capturing local patterns but often miss long-term dependencies. Transformers are powerful but have quadratic complexity relative to sequence length, making them computationally expensive for very long sequences.

Moreover, to the best of our knowledge, there are a few studies \cite{Sayed2024Novel} combining time series air pollution analysis and clinical data using deep learning algorithms for ACS prediction. Integrating heterogeneous data types remains challenging.  Multimodal deep learning frameworks are required to effectively combine clinical and environmental data, addressing issues related to alignment, synchronization, and fusion of multimodal information.  

Our proposed TabulaTime model represents a significant advancement in this field by leveraging deep learning to integrate time-series air pollution data with traditional clinical data. It efficiently processes long sequences while maintaining linear computational complexity, making it more efficient than traditional RNNs and transformers, which have quadratic complexity. This approach addresses the limitations of previous models, providing a more comprehensive and accurate assessment of ACS risk.