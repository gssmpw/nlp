\section{Related Work}
\label{sec:rw}

\paragraph{Inertial Posers.}
Motion capture solutions using inertial measurement units (IMUs) have gained significant traction in recent years. Commercial products like Noitom \cite{noitom} and Movella \cite{Movella} leverage dense IMUs to offer high-quality, portable, and real-time applications. However, the usage of these IMU systems can be cumbersome because they require numerous sensors to be attached to the body, which can be inconvenient and intrusive for users. Since the exploration of SIP \cite{von2017SIP}, learning-based methods under the sparse sensor configuration \cite{huang2018DIP,TransPoseSIGGRAPH2021,TIP22,PIPCVPR2022,van2024diffusionposer,yi2024pnp} (called ``inertial poser'') and head-mounted device \cite{du2023avatars,yang2024divatrack,dai2024hmd,starke2024categorical} (called ``three-point tracker'') markedly improved the cost and convenience. These advancements, facilitated by real and synthetic datasets \cite{trumble2017total,huang2018DIP,AMASS:ICCV:2019}, have led to consumer-level products like Mocopi \cite{mocopi}. Despite these developments, inertial methods inherently suffer from issues such as sensor drift and lack of globally positional reference. To mitigate such defects, hybrid approaches \cite{liang2023hybridcap,ren2023lip,EgoLocate2023,pan2023fusing} fuse sparse IMU sensors with monocular vision signals. Furthermore, the recognition and analysis of human motion based on IMUs remain limited to simple classification tasks within some fixed action categories \cite{stromback2020mm,chen2015utd,yan2024language}. Such constraints highlight the need for a more robust ``inertial poser'' and an open-vocabulary system for human motion analysis from IMU signals.

\vspace{-4mm}
\paragraph{Human Motion Understanding with Natural Language.}
Significant strides have been made in the field of human motion understanding with natural language, driven by advanced transformer \cite{vaswani2017attention} and diffusion models \cite{ho2020denoising,song2020denoising}. Specifically, techniques such as text-to-motion generation \cite{tevet2023human,tevet2022motionclip,zhang2024motiondiffuse,Guo_2022_CVPR,zhang2024motiongpt}, motion controlling \cite{delmas2023posefix,athanasiou2024motionfix,xie2024omnicontrol,huang2024controllable}, and motion recognition \cite{locate:2024} learn a conditional probability distribution given textual descriptions and motion sequences respectively. More recent work \cite{jiang2023motiongpt,li2024unimotion,wu2024motionllm,li2024lamp,zhou2024avatargpt,wang2024motiongpt} utilized powerful LLMs to build unified models, enabling versatile motion-language tasks within a single framework. In addition, the skeleton-based action recognition problem also achieved promising progress under the paradigm of fine-tuning LLMs \cite{qu2024llms,mo2024mochat,do2024tdsm}. Despite the impressive performance of these approaches, there still remain two notable issues. Firstly, the motion-language alignment is sub-optimal for understanding and analysis, because the SMPL parametric representation \cite{SMPL2015,MANO:SIGGRAPHASIA:2017,SMPLX2019} is essentially an approximation of real human movements. It simplifies the complex and nuanced nature of human motion, which can fail to express the subtleties and variances of actual inertia. Secondly, to interact with agents using human behavior in the real world, these methods inevitably rely on pose estimators which can introduce uncontrollable noises and errors. These limitations underscore the importance of the alignment between observable and raw sensor signals with natural language.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/pipeline.png}
  \vspace{-20px}
  \captionof{figure}{\textbf{Overview of our training pipeline.} We quantize continuous and jittery IMU signals to a sequence of jitter-reduced and motion-aware inertial tokens by learning a IMU tokenizer through distribution matching strategy and adopt semantic aligned and LoRA fine-tuned LLM to generate precise, professional and stylistic text feedback for human motion analysis.}
\label{fig:train}
\vspace{-10px}
\end{figure*}

\vspace{-4mm}
\paragraph{Human Motion Analysis by Multimodal LLMs.}
In recent years, vision-language models (VLMs) have gained significant traction, demonstrating impressive results in video captioning, reasoning, and understanding tasks \cite{feng2024chatpose,chen2024motionllm,wang2024internvideo2,li2024human,internlmxcomposer,internlmxcomposer2,internlmxcomposer2_4khd,internlmxcomposer2_5}. These models have shown their potential to bridge visual and textual modalities for complex reasoning and semantic understanding \cite{jiang2024motionchain,bao2024exploiting,lin2024chathuman,li2024techcoach,han2023imagebind}. However, when applied to human activity analysis \cite{chen2024internvl2,li2024mvbench,zhang2024narrative}, video data often proves to be a heavy and redundant modality. It introduces substantial amounts of static scenes and irrelevant background information while being susceptible to occlusion, making vision-based approaches inefficient and resource-intensive for human activity analysis. Moreover, state-of-the-art VLMs, including proprietary large language models like GPT-4 and Gemini, fall short of providing real-time analysis and dynamic feedback, which significantly limits their applicability in real-world interactive scenarios. In contrast, we leverage inertial measurement unit (IMU) signals for human motion analysis\cite{li2024sensorllm,moon2022imu2clip,moon2024anymal,girdhar2023imagebind}. IMU signal is a sparse and lightweight modality, captured through body-worn devices, and provides an accurate representation of human actions in the 3D world. Its efficiency and low computational cost make it well-suited for enabling real-time interactive applications, addressing the limitations of vision-based approaches, and expanding the scope of multimodal human activity analysis.