\section{Experiments}
\label{sec:experiment}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.87\linewidth]{figures/gallery.png}
    \caption{\textbf{Results Gallery.} We present input IMU signals, MoCap results, system analysis, and RGB references.}
\label{fig:gallery}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/mocap_comparison.png}
    % \caption{\textbf{Qualitative Comparisons of Motion Reconstruction.} We test on TotalCapture~\cite{trumble2017total} and PICO-FreeDancing~\cite{dai2024hmd} dataset.
    % The IMU sensors attached to highlighted body parts in red are disturbed by noises. The first row shows the single-sensor noised condition, where noise is applied to the left wrist sensor. The second row shows the two-sensor noised condition, with noise introduced to the pelvis and head sensors. The third row illustrates the three-sensor noised condition, where noise affects the pelvis and both knee sensors, while the head and wrist sensors remained unaffected. This configuration aligns with the three-point tracking setup in VR.}
    \caption{\textbf{Qualitative Comparisons of Motion Reconstruction.}
    We evaluate our method on the TotalCapture~\cite{trumble2017total} and PICO-FreeDancing~\cite{dai2024hmd} dataset. IMU sensors attached to the highlighted body parts (shown in red) are disturbed by noises. The first row demonstrates the single-sensor noised condition, where noise is applied to the left wrist sensor. The second row presents the two-sensor noised condition, with noise introduced to the pelvis and head sensors. The third row depicts the three-sensor noised condition, where noise affects the pelvis and both knee sensors, while the head and wrist sensors remain unaffected. This configuration corresponds to the three-point tracking setup commonly used in VR systems.
    }
\label{fig:mocap_comparison}
\end{figure*}

\begin{table*}[t!]
    \centering
    \tabcolsep=0.15cm
    \tiny
    \begin{tabular}{lcccccccccc}
        \toprule
        & \multicolumn{3}{c}{Noised=1 (MPJPE/Mesh Err/Jitter) $\downarrow$} & \multicolumn{3}{c}{Noised=2 (MPJPE/Mesh Err/Jitter) $\downarrow$} & \multicolumn{3}{c}{Noised=3 (MPJPE/Mesh Err/Jitter) $\downarrow$}\\
        \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
        Methods & BABEL & TotalCapture & Pico-FreeDancing & BABEL & TotalCapture & Pico-FreeDancing & BABEL & TotalCapture & Pico-FreeDancing\\
        \midrule
        TransPose 
        & 15.20/18.07/1387.71 & 16.95/20.08/1285.59 & 18.89/23.93/1401.72 
        & 22.76/27.63/2070.31 & 23.67/28.68/1913.04 & 24.87/30.67/2092.98
        & 36.27/42.19/3960.68 & 37.02/43.31/4930.36 & 37.66/44.34/3979.20\\
        PIP 
        & 16.13/19.89/129.77 & 26.25/32.32/200.70 & 11.96/17.53/27.57$^*$
        & 15.34/19.69/133.95 & 32.54/45.31/374.82 & 13.87/22.12/51.38$^*$
        & 33.81/40.43/367.92 & 32.62/45.23/510.24$^*$ & 33.43/46.55/480.17$^*$\\
        Ours & 
        \textbf{10.44}/\textbf{13.63}/\textbf{1.40} & 
        \textbf{12.27}/\textbf{15.37}/\textbf{1.46} & 
        \textbf{11.69}/\textbf{16.59}/\textbf{1.04} &
        \textbf{12.81}/\textbf{16.97}/\textbf{1.47} &
        \textbf{14.30}/\textbf{18.54}/\textbf{1.52} &
        \textbf{13.54}/\textbf{19.14}/\textbf{1.11} &
        \textbf{16.05}/\textbf{21.41}/\textbf{1.86} &
        \textbf{17.19}/\textbf{22.54}/\textbf{2.18} &
        \textbf{16.73}/\textbf{22.45}/\textbf{1.38} &
        \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Accuracy On Different Noised Levels.} For the single-sensor noised condition, noise was independently applied to each of the six IMU signals, and the results were averaged. For the two- and three-sensor noised conditions, random sensor combinations were selected, and the results were averaged accordingly. 
    % Note that ``*'' indicates that PIP failed to solve all the noised sequences due to the numerical instability of physical solver. Therefore, the failed sequences were excluded from evaluation.
    Note that the symbol ``*'' indicates that PIP failed to solve all noisy sequences due to the numerical instability of the physical solver. As a result, these failed sequences were excluded from evaluation.
    }
\label{tab:noised_mocap_comparison}
\end{table*}

In this section, we first introduce motion datasets containing various modalities that we utilize for training and evaluation in Sec.~\ref{sec:datasets}. Subsequently, in Sec.~\ref{sec:mocap_comparison}, we conduct extensive qualitative and quantitative comparison experiments with other state-of-the-art inertial posers to demonstrate the robustness of our method under various noisy environments, which is benefited from jitter-reduced inertial tokens. 
% We then compare the precision, reasonability, and professionalism of textual feedback of our method with a well-constructed baseline method based on TransPose~\cite{TransPoseSIGGRAPH2021} and MotionGPT~\cite{jiang2023motiongpt}, and other well-known vision-language models. Additional results related to motion reconstruction along with text descriptions and instructions are provided in Fig.~\ref{fig:gallery}.
We further compare the precision, brevity, naturalism and professionalism of the textual feedback generated by our method against a well-constructed baseline method based on TransPose~\cite{TransPoseSIGGRAPH2021} and MotionGPT~\cite{jiang2023motiongpt}, as well as other prominent vision-language models. Additional results related to motion reconstruction, accompanied by text descriptions and instructions, are provided in Fig.~\ref{fig:gallery}.

\subsection{Datasets and Evaluation Metrics}
\label{sec:datasets}

\paragraph{Datasets.} We utilize a diverse collection of datasets encompassing motion-only data, real IMU recordings, and textual annotations. Specifically, our motion-aware and jitter-reduced IMU tokenizer is trained on 3DPW~\cite{vonMarcard2018}, Human3.6M \cite{h36m_pami}, TotalCapture \cite{trumble2017total}, PICO-FreeDancing \cite{dai2024hmd}, BABEL \cite{AMASS:ICCV:2019,BABEL:CVPR:2021}, Motion-X++ \cite{lin2024motion}, Fit3D \cite{Fieraru_2021_CVPR}, FLAG3D \cite{tang2023flag3d}, MOYO \cite{tripathi2023ipman}, and EC3D \cite{zhao2022exercise}. For training our language model, we re-formulate textual annotations of BABEL \cite{AMASS:ICCV:2019,BABEL:CVPR:2021} and Motion-X++ \cite{lin2024motion} on daily motions, and generate interactive dialogues based on Fit3D \cite{Fieraru_2021_CVPR}, FLAG3D \cite{tang2023flag3d}, MOYO \cite{tripathi2023ipman}, and EC3D \cite{zhao2022exercise} through the preparation approach mentioned in Sec.~\ref{sec:data_preparation}.

\vspace{-4mm}
\paragraph{Evaluation Metrics.} 
% In following experiments, we evaluate our method's capability in robust motion capture, evaluating by comprehensive metrics:
% \begin{itemize}[leftmargin=*]
%     \item \textbf{MPJPE (cm)} measures the average Euclidean distance between reconstructed and ground-truth joint positions.
%     \item \textbf{Mesh Error (cm)} evaluates the mean Euclidean error of the reconstructed SMPL mesh~\cite{SMPL2015} across all vertices.
%     \item \textbf{Jitter ($\bm{10^2}\textrm{m/s}\bm{^3}$)} determines the third derivative of joint positions over time to evaluate motion smoothness and reasonability.
% \end{itemize}
In the following experiments, we evaluate our method's capability in robust motion capture using comprehensive metrics, including:
\begin{itemize}[leftmargin=*]
    \item \textbf{MPJPE (cm)} measures the average Euclidean distance between reconstructed and ground-truth joint positions.
    \item \textbf{Mesh Error (cm)} evaluates the mean Euclidean error of the reconstructed SMPL mesh~\cite{SMPL2015} across all vertices.
    \item \textbf{Jitter ($\bm{10^2}\textrm{m/s}\bm{^3}$)} computes the third derivative of joint positions over time to assess motion smoothness and reasonability.
\end{itemize}

% To evaluate the precision and professionalism of textual feedback of our method, we calculate the following two commonly used metrics:
% \begin{itemize}[leftmargin=*]
%     \item \textbf{BERTScore} computes semantic similarity by averaging the cosine similarity of contextual embeddings from models like BERT.
To evaluate the precision and professionalism of the textual feedback generated by our method, we calculate the following two widely used metrics:
\begin{itemize}[leftmargin=*]
    \item \textbf{BERTScore} measures semantic similarity by computing the average consine similarity of contextual embeddings derived from models such as BERT.
    \item \textbf{METEOR} assesses similarity through stem matching, synonym matching, and positional penalties, while also evaluating fluency.
\end{itemize}


\subsection{Evaluating Robustness of Motion Capture}
\label{sec:mocap_comparison}
Inertial poser methods based on recurrent neural networks (RNN) and physics solvers have consistently faced significant challenges related to noise sensitivity. IMU signals are highly susceptible to noises introduced by various uncontrolled factors, such as magnetic fields interference, prolonged usage, and sub-optimal sensor placement. As a result, robustness remains a crucial yet unresolved issue for real-world applications.
The primary limitation of previous works~\cite{huang2018DIP,TransPoseSIGGRAPH2021,PIPCVPR2022,TIP22,yi2024pnp} lies in their reliance on data-driven neural networks trained to map IMU signals directly to human motion through continuous functions. Consequently, when IMU signals contain outliers, these methods fail to effectively eliminate noise, leading to the propagation of signal artifacts into the reconstructed motion results.
In contrast, our method addresses this issue by tokenizing continuous IMU signals into discrete tokens through a motion-aware and jitter-reduced IMU tokenizer. The quantization operation within this process effectively mitigates various noisy conditions, enabling the system to filter out irregularities and even tolerate severely corrupted IMU signals.
To comprehensively evaluate the robustness of our motion capture, we simulate multiple levels of noisy input conditions by adding random noises to the orientation, acceleration, and gyroscope data of different combinations of IMU configurations. Based on these simulated noisy inputs, we qualitatively and quantitatively demonstrate the advantages of our method over existing approaches.


\begin{table}[t!]
    \centering
    \tabcolsep=0.18cm
    \footnotesize
    \begin{tabular}{lccccc}
        \toprule
        & \multicolumn{2}{c}{BERT $\uparrow$} & \multicolumn{2}{c}{METEOR $\uparrow$}\\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}
        Methods & Descriptive & Instructive & Descriptive & Instructive\\
        \midrule
        Baseline & 0.8603 & 0.8483 & 0.0706 & 0.0746\\
        InternVideo2 & 0.8467 & 0.8454 & 0.0551 & 0.0583\\
        MotionLLM & \cellcolor{pink}{\textbf{0.9085}} & \cellcolor{cyan!30}{0.8622} & \cellcolor{pink}{\textbf{0.3912}} & \cellcolor{cyan!30}{0.1218}\\
        Ours & \cellcolor{cyan!30}{0.8781} & \cellcolor{pink}{\textbf{0.8667}} & \cellcolor{cyan!30}{0.1205} & \cellcolor{pink}{\textbf{0.1510}}\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Quantitative Comparison on textual feedback accuracy.} 
    % Quantative comparison across four methods (Baseline, InternVideo2, MotionLLM, and ours) on two key metrics (BERT, METEOR). The results are presented under two categories,``Descriptive" and ``Instructive", revealing that our approach achieves better performance than baseline and InternVideo2, and comparable to VLM-based MotionLLM in contextually rich text feedback.}
    Quantative comparison across four methods (Baseline, InternVideo2, MotionLLM, and ours) on two key metrics (BERT, METEOR). The results are presented under two categories, "Descriptive" and "Instructive", revealing that our approach outperforms the baseline and InternVideo2 while achieving performance comparable to VLM-based MotionLLM in generating contextually rich textual feedback.}
\label{tab:analysis}
\end{table}

\vspace{-4mm}
\paragraph{Qualitative Results} As illustrated in Fig.~\ref{fig:mocap_comparison}, our method consistently outperforms prior works \cite{TransPoseSIGGRAPH2021,PIPCVPR2022} across various noise levels and configurations. In cases where noised signals affect specific body part (e.g., the left arm highlighted in red), RNN-based and physics solver-based methods frequently generate inaccurate motions. This is attributed to the continuous function mapping learned by their networks, which are inherently sensitive to input outliers. In contrast, the quantization step in our jitter-reduced IMU tokenizer effectively filters out corrupted parts in inputs, enabling accurate motion reconstruction even under imperfect IMU signals. Notably, our method demonstrates robust performance even when the IMU sensor attached to the pelvis is absent. While existing methods, which rely on root-relative input data representations, tend to generate messy motions when global orientation is invalid, our method effectively leverages valid signals from other sensors and quantizes them into reasonable discrete latent features. This capability significantly enhances the robustness of inertial posers in such challenging scenarios.

\vspace{-4mm}
\paragraph{Quantitative Results}
In addition to qualitative comparisons, we also quantitatively evaluate the motion capture accuracy and robustness under various noisy input configurations. As reported in Tab.~\ref{tab:noised_mocap_comparison}, we present performance of our method and two state-of-the-art approaches~\cite{TransPoseSIGGRAPH2021,PIPCVPR2022} on both synthesized IMU data from BABEL~\cite{braams:babel,AMASS:ICCV:2019} and real IMU recordings from TotalCapture~\cite{trumble2017total} and PICO-FreeDancing~\cite{dai2024hmd}. Our method outperforms other approaches by a large margin in both motion capture accuracy and smoothness. In particular, under severe noise conditions, our method maintains stable performance, achieving 2 times lower MPJPE and hundreds times lower jitters.

\subsection{Evaluating Quality of Textual Feedback}
We further evaluate the precision and professionalism of the motion analysis generated by our method. To establish a meaningful baseline, we integrate TransPose~\cite{TransPoseSIGGRAPH2021} with MotionGPT~\cite{jiang2023motiongpt} as a toy system for both motion capture and analysis via sparse inertial signals, taking SMPL motion representation~\cite{SMPL2015} as the intermediate. To demonstrate the capability of our method in precisely analyzing detailed and diverse human motion via sparse signals compared to vision-based approaches, we also compare our method with two well-known open-source Vision-Language Models (VLMs)~\cite{wang2024internvideo2,chen2024motionllm} on motion description and instruction tasks. As shown in Tab.~\ref{tab:analysis}, our method consistently outperforms the baseline method and InternVideo2~\cite{wang2024internvideo2} in both tasks and achieves performance comparable to MotionLLM~\cite{chen2024motionllm}. This demonstrates the advantage of our method in motion understanding and analysis via IMU signals, which are far sparser than vision modalities, significantly facilitating downstream real-time applications. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/real_time.png}
  \captionof{figure}{\textbf{Web-based live demo.} 
  % Our system receives streaming IMU signals and processes it in real time for simultaneous motion capture and analysis. We show the live recording of human performer in real world and the replicated virtual motions in web frontend, accompanied with an online chat window.}
  Our system processes streaming IMU signals in real time, enabling simultaneous motion capture and analysis. The live recording of a human performer in real world is displayed alongside the replicated virtual motions in the web interface, which also includes an integrated online chat window.}
\label{fig:real_time}
\end{figure}

\subsection{Web-based Live Demo}
% In addition, we developed a web-based live motion capture and analysis demonstration driven by six IMU sensors accompanied by a LLM. As shown in Fig.~\ref{fig:real_time}, we showcase two primary application scenarios of our method in daily motion understanding and exercising motion instruction, with visual and textual feedback. We implement the web demo following the inference pipeline shown in the Fig. ~\ref{fig:inference_pipeline}. Specifically, we attach six Movella DOT IMU sensors~\cite{Movella} to the corresponding body parts and establish the sensors' connection through Bluetooth transmission. The user begins by performing a T-pose for around 10 seconds to calibrate the sensors. Once the calibration is completed, the streaming signals are processed and fed into our IMU tokenizer, which generates jitter-reduced inertial tokens. Our method then uses the learned motion decoder to reconstruct the user's motion, with a post-preprocessing module utilizing SmoothNet~\cite{zeng2022smoothnetplugandplaynetworkrefining}. Meanwhile, the IMU siganals are continuously analyzed in the backend. For practical usage, we monitor the microphone input to capture the user's verbal questions and employ Whisper-base-en~\cite{radford2023robust} to transcribe the audio into text. Finally, the textual feedback generated by our method is displayed in the chat window, and converted to speech audio by SpeechT5-TTS~\cite{ao2021speecht5}.
In addition, we developed a web-based live demonstration for motion capture and analysis, powered by six IMU sensors and integrated with an LLM. As illustrated in Fig.~\ref{fig:real_time}, we present two primary application scenarios of our method: daily motion understanding and exercising motion instruction, providing both visual and textual feedback. The web demonstration follows the inference pipeline depicted in Fig.~\ref{fig:inference_pipeline}. Specifically, six Movella DOT IMU sensors~\cite{Movella} are attached to the corresponding body parts, and their connection is established via Bluetooth transmission. The user begins by performing a T-pose for approximately 10 seconds to calibrate the sensors. Once calibration is complete, the streaming signals are processed and fed into our IMU tokenizer, which generates jitter-reduced inertial tokens. Our method then employs the learned motion decoder to reconstruct the user's motion, with a post-processing module utilizing SmoothNet~\cite{zeng2022smoothnetplugandplaynetworkrefining}. Simultaneously, the IMU signals are continuously analyzed in the backend. For practical interaction, we monitor microphone input to capture the user's verbal questions and utilize Whisper-base-en~\cite{radford2023robust} to transcribe the audio into text. Finally, the textual feedback generated by our method is displayed in the chat window and converted into speech audio using SpeechT5-TTS~\cite{ao2021speecht5}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/user_study.png}
  \captionof{figure}{\textbf{User study.} 
  % for comparing subjective preferences of textual feedback across four dimensions: Professionalism, Precision, Naturalism, and Brevity. Participants are invited to evaluate textural feedback of four different methods (Baseline, InternVideo2, MotionLLM, and Ours). Our approach secures the highest vote ratings in all categories, indicating superior performance in all evaluation aspects.}
  A comparison of subjective preferences for textual feedback across four dimensions: Professionalism, Precision, Naturalism, and Brevity. Participants were asked to evaluate the textual feedback generated by four different methods (Baseline, InternVideo2, MotionLLM and Ours). Our approach achieved the highest ratings in all categories, demonstrating superior performance across all evaluated aspects.}
\label{fig:user_study}
\end{figure}

\subsection{User Study}
% To comprehensively evaluate the performance and user experience of our system in user perspective, we conduct a user study focusing on the professionalism, precision, naturalism and brevity of system responses, containing the baseline, our method, MotionLLM~\cite{chen2024motionllm}, and InternVideo2~\cite{wang2024internvideo2}. We distribute our study form to more than 20 volunteers, and invite participants to select their preferred options based on four different system responses, as well as the interaction processes demonstrated in pre-recorded videos. As depicted in Fig.~\ref{fig:user_study}, our method secures around 50\% votes in accurate and reasonable feedback. While, in aspect of naturalism, our method dominates other methods in user interactions.

To comprehensively evaluate the performance and user experience of our system from a user perspective, we conducted a user study focusing on the professionalism, precision, naturalism, and brevity of system responses. The study compared four methods: the baseline, our method, MotionLLM~\cite{chen2024motionllm}, and InternVideo2~\cite{chen2024internvl2}. We distributed the study form to over 20 volunteers and invited participants to select their preferred options based on four different system responses, as well as the interaction processes demonstrated in pre-recorded videos. As illustrated in Fig.~\ref{fig:user_study}, our method secured approximately 50\% of the votes for providing accurate and reasonable feedback. Furthermore, in terms of naturalism, our method outperformed the other methods in user interactions.