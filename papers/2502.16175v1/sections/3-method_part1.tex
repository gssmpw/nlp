\section{Jitter-reduced IMU Tokenizer}
\label{sec:method1}
In practical application of long-sequence motion capture and online motion analysis using IMU sensors, device connection, signal transmission and wearing fashion can significantly influence the quality of motion capture and the convenience of user experience. However, it is challenging due to the inherent defects of IMUs such as data drifting and jittery signals. Therefore, we start by proposing a jitter-reduced and motion-aware IMU tokenizer to represent sparse inertial signals by discrete tokens, which can compress continuous IMU signals into a fixed collection of latent codes shared with motion latent space. Consequently, the discrete inertial tokens can be integrated into the vocabulary of LLMs, while also support high-quality motion reconstruction. The IMU tokenizer is built upon the standard VQ-VAE framework \cite{van2017neural}, with a novel distribution matching strategy to learn an approximate latent space of corresponding human motion. Additionally, linguistic properties are assigned to the learned inertial tokens through regularization under Zipf's law \cite{zipf2013psycho}, which facilitates following semantic alignment with natural language.

\subsection{Motion VQ-VAE}
\label{sec:motion_vq}
We first follow MotionGPT \cite{jiang2023motiongpt} to learn a VQ-VAE for human motion. Differently, we represent human motion with a complete state of root joint and foot-ground contacts to suit IMU sensor characteristics. In addition, we involve regularization terms on foot-ground contacts to eliminate jittery results and sliding artifacts in decoded motion.

\vspace{-4mm}
\paragraph{Motion Representation.} While HumanML3D \cite{Guo_2022_CVPR} establishes an effective motion representation for text-to-motion generation tasks, it is limited to incomplete global dynamics and missing foot-ground contacts. Inspired by HuMoR \cite{rempe2021humor}, we incorporate root translation and angular velocity along all three axes into our representation to improve expressiveness. Specifically, we represent a motion sequence as
\begin{equation}
    \mathbf M^{1:T} = \left[
        \mathbf r
        \quad \dot{\mathbf{r}}
        \quad \mathbf \Phi 
        \quad \dot{\mathbf{\Phi}}
        \quad \mathbf{j}^r
        \quad \mathbf{j}^p
        \quad \mathbf{j}^v
        \quad \mathbf{p}
    \right] \in \mathbb{R}^{T\times d_m}\text{,}
\end{equation}
where $T$ is the sequence length. Within the representation, we first include the root translation $\mathbf r\in\mathbb R^{T\times3}$, linear velocity $\dot{\mathbf{r}} \in\mathbb R^{T\times3}$, orientation $\mathbf \Phi\in\mathbb R^{T\times6}$, and angular velocity $\dot \Phi\in\mathbb R^{T\times 3}$. Then, we use $\mathbf {j}^r\in \mathbb R^{T\times 6J}$, $\mathbf {j}^p\in\mathbb R^{T\times 3J}$, $\mathbf {j}^v\in\mathbb R^{T\times3J}$ to represent local joint rotations, positions, and velocities, respectively. Finally, $\mathbf p\in \mathbb R^{T\times 4}$ records the binary contact labels of toes and heels. Here, $d_m=271$ is the dimension of our motion representation, and $J=21$ is the number of local joints. All the rotational parts are in 6D rotation convention \cite{zhou2019continuity}.

\vspace{-4mm}
\paragraph{Training of Motion VQ-VAE} Given a motion sequence $\mathbf{M}^{1:T}$, we first encode it into discrete latent codes $\mathbf{Z}^{\text{motion}} \in \mathbb{R}^{S\times d_z}$ using 1D convolution layers, where $d_z=512$ is the dimension of latent code, and $S$ is the number of the resulting latent codes. We define the hyperparameter $l=\lfloor T/S\rfloor$ as the compression rate for discretization. Following the encoding process, each latent code $\mathbf{z}_s^{\text{motion}}$ is quantized to a learned codebook $\mathbf{C}^{\text{motion}}\in \mathbb{R}^{K\times d_z}$, where $K$ is the codebook size. The quantization process runs as follows
\begin{equation}
    \mathbf{b}_{s}^{\text{motion}} = \underset{\mathbf{c}_k^{\text{motion}}}{\argmin}\left\Vert \mathbf{z}_{s}^{\text{motion}}-\mathbf{c}_k^{\text{motion}} \right\Vert_2^2\text{,}
\end{equation}
which selects the nearest codebook entry according to Euclidean metric, and results in the motion token sequence $\mathbf B^{\text{motion}}\in \mathbb{R}^{S\times d_z}$. Subsequently, $\mathbf{B}^{\text{motion}}$ is fed into the decoder to reconstruct original motion sequence $\hat{\mathbf M}^{1:T'}$ with possible truncation $T'=lS$. To train the motion VQ-VAE, we utilize the discrete representation learning objective \cite{van2017neural} to supervise our network
\begin{equation}
    \mathcal{L}_{\text{vq}} = \lambda_{\text{recon}}\mathcal{L}_{\text{recon}} + \lambda_{\text{commit}}\mathcal{L}_{\text{commit}}\text{.}
\end{equation}
Specifically, the reconstruction loss is defined as
\begin{equation}
    \mathcal L_{\textrm{recon}} = \frac{1}{T'}\left\Vert \hat{\mathbf M}^{1:T'} - \mathbf M^{1:T'} \right\Vert_2^2\text{,}
    \label{eq:recon_loss}
\end{equation}
and the commit loss with gradient pass-through is
\begin{equation}
    \mathcal L_{\textrm{commit}} = \frac{1}{S} \left\Vert\mathbf{Z}^{\textrm{motion}} - \mathbf{B}^{\textrm{motion}}\right\Vert_2^2\text{.}
    \label{eq:commit_loss}
\end{equation}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/imu_tokenization.png}
  \vspace{-15px}
  \captionof{figure}{\textbf{IMU Tokenizing Process.} The rotation, acceleration, and angular velocity components of the IMU signal are first flattened and concatenated. The resulting sequence is then processed by an encoder comprising multiple 1D convolutional layers and subsequently passed through a quantizer to generate the jitter reduced inertial tokens.}
\label{fig:imu_tokenizer}
\vspace{-10px}
\end{figure}
Additionally, to improve the fidelity of reconstructed motion and eliminate the foot-ground sliding artifacts, we follow HuMoR \cite{rempe2021humor} to constrain the foot-ground interactions using
\begin{equation}
    \mathcal{L}_{\text{foot}} = \lambda_{\textrm{contact}}\mathcal L_{\textrm{contact}} + \lambda_{\textrm{slide}}\mathcal L_{\textrm{slide}}\text{,}
\end{equation}
where the contact discrimination loss is
\begin{footnotesize}
\begin{equation}
    \mathcal L_{\textrm{contact}} = \frac{1}{T'}\sum_{i\in\left\{1,2,3,4\right\}} \left[-\mathbf{p}_i\log\hat{\mathbf{p}}_i-\left(1-\mathbf{p}_i\right)\log\left(1-\hat{\mathbf{p}}_i\right)\right] \text{,}
    \label{eq:contat_loss}
\end{equation}
\end{footnotesize}
and the sliding penalty is
\begin{equation}
    \mathcal L_{\textrm{slide}} = \frac{1}{T'}\sum_{i\in\{1,2,3,4\}}\hat{\mathbf{p}}_i\left\Vert \mathbf j^v_{\operatorname{foot}\left(i\right)}\right\Vert_2^2\text{.}
    \label{eq:velocity_loss}
\end{equation}
Overall, the total training loss of our motion VQ-VAE is
\begin{equation}
    \mathcal{L}_{\text{motion}} = \mathcal{L}_\text{vq} + \mathcal{L}_\text{foot}\text{.}
\end{equation}
For following distribution matching, we maintain the frequency distribution of the motion codebook within each training batch
\begin{footnotesize}
\begin{equation}
  \mathbf{F}^{\textrm{motion}} = \frac{1}{S}\pi\left(\sum_{s=1}^S \mathcal{G}\left(\left\{-\left\Vert\mathbf{z}_s^\textrm{motion}-\mathbf{c}_k^\textrm{motion}\right\Vert_2^2\right\}_{k=1}^K\right)\right)\text{,}
\label{eq:gumbel_softmax}
\end{equation}
\end{footnotesize}
where $\mathcal{G}(\cdot):\mathbb{R}^{K}\mapsto\mathbb{R}^{K}$ is the differentiable sampling procedure using Gumbel-Softmax trick \cite{jang2016categorical}, and $\pi(\cdot)$ operates sorting on the token frequencies in descending order.

\subsection{IMU Tokenizer}
\label{sec:imu_vq}
In this subsection, we introduce the jitter-reduced and motion-aware IMU tokenizer. To facilitate the integration of continuous inertial signals with natural language in a manner compatible with large language models (LLMs), we propose a novel approach that encodes inertial signals into discrete tokens, as shown in Fig.~\ref{fig:imu_tokenizer}. These tokens are designed to align seamlessly with the LLM vocabulary, enabling direct incorporation into the language modeling framework. Meanwhile, to empower inertial tokens with the capability of reproducing high-quality 3D motion, we devise a novel distribution matching strategy to approximate the corresponding motion latent space. Therefore, the learned IMU codebook can be utilized for motion reconstruction and analysis.

\vspace{-4mm}
\paragraph{Inertia Representation} In prior works \cite{huang2018DIP,TransPoseSIGGRAPH2021,TIP22,PIPCVPR2022,yi2024pnp}, inertial signals are considered as the composition of orientation and linear acceleration. However, to fully utilize the sensor measurements of the accelerometer, gyroscope, and magnetometer, we represent an inertia sequence as follows
\begin{equation}
    \mathbf I^{1:T} = [\mathbf{q} \quad \mathbf{a} \quad \bm{\omega}] \in\mathbb R^{T\times d_u}\text{,}
\label{eq:inertia_representation}
\end{equation}
which includes orientation $\mathbf q\in\mathbb R^{T\times 6N}$, free acceleration $\mathbf a\in\mathbb R^{T\times 3N}$ and angular velocity $\bm{\omega} \in \mathbb{R}^{T\times 3N}$. In this work, we utilize a configuration of $N=6$ IMU sensors. The collected inertial data is represented in the feature space with a dimensionality of $d_u=72$, capturing comprehensive motion characteristics.

\vspace{-4mm}
\paragraph{Data Pre-processing} Due to the scarcity of MoCap data paired with real IMU readings \cite{huang2018DIP,trumble2017total,dai2024hmd}, we simulate synthetic IMU signals on extensive motion data \cite{huang2018DIP,TransPoseSIGGRAPH2021}. To model the characteristics of IMU sensors, such as data drift, we follow PNP \cite{yi2024pnp} to use random walk variables to mimic cumulative error. Since acceleration data can fluctuate violently within a wide range, we normalize it to a standard normal distribution using the mean and variance determined on the training dataset. This preprocessing procedure mitigates the impact of high-frequency noise spikes and irregular waves while preserving the drifting feature, improving the learning stability of the tokenizer.

\vspace{-4mm}
\paragraph{Training of IMU Tokenizer} Given an inertia sequence $\mathbf{I}^{1:T}$, we learn to construct a codebook $\mathbf{C}^{\text{imu}} \in \mathbb{R}^{K\times d_z}$. To be specific, each codebook entry $\mathbf{c}_k^{\text{imu}}$ is updated through exponential moving average (EMA) according to \cite{razavi2019generating}
\begin{align}
    \mathbf{c}_k^{\textrm{imu}} &\leftarrow \frac{\bm{\sigma}_k}{\delta_k} \notag\\
    \bm{\sigma}_k &\leftarrow \gamma\bm{\sigma}_k + \left(1-\gamma\right)\sum_{s=1}^S
    \mathbbm{1}\left(\mathbf{b}_s^{\textrm{imu}}=\mathbf{c}_k^{\textrm{imu}}\right)\mathbf{z}_s^{\textrm{imu}} \notag\\
    \delta_k &\leftarrow \gamma\delta_k + \left(1-\gamma\right)\sum_{s=1}^S\mathbbm{1}\left(\mathbf{b}_s^{\textrm{imu}}=\mathbf{c}_k^{\textrm{imu}}\right)\text{,}
\label{eq:codebook_update}
\end{align}
where the summation $\sum_{s=1}^S\mathbbm{1}\left(\mathbf{b}_s^{\textrm{imu}}=\mathbf{c}_k^{\textrm{imu}}\right)$ records the count that $\mathbf{c}_k^{\textrm{imu}}$ is selected. Similar to Eq.\ref{eq:gumbel_softmax}, we also maintain the frequency distribution of IMU codebook $\mathbf{F}^{\text{imu}}$ within each training batch. To inject motion dynamics and inductive bias of natural language into inertial tokens, we propose to learn by unsupervised distribution matching, inspired by CM \cite{starke2024categorical}. Specifically, the training objective of our motion-aware IMU tokenizer is
\begin{equation}
    \mathcal{L}_{\text{imu}} = \lambda_{\text{code}}\mathcal{L}_{\text{code}} + \lambda_{\text{dist}}\mathcal{L}_{\text{dist}}\text{,}
\label{eq:loss_imu_tokenizer}
\end{equation}
where the code matching loss enforces the quantized token from the IMU tokenizer close to that from the motion tokenizer
\begin{equation}
    \mathcal L_{\text{code}} = \frac{1}{S}\left\Vert \mathbf{B}^{\text{imu}}-\mathbf{B}^{\text{motion}}\right\Vert_2^2\text{.}
\label{eq:token_match_mse}
\end{equation}
We also incorporate Zipf's law \cite{zipf2013psycho,piantadosi2014zipf}, a principle about the word frequency distribution in natural language, as a regularization term to enhance the linguistic properties of the inertial tokens \cite{qu2024llms,papadimitriou2023pretrain}. Formally, the Zipfian distribution $\mathbf{F}^{\text{zipf}}$ is defined as
\begin{equation}
    \mathbf{F}^{\text{zipf}} \propto \left\{\frac{1}{(k+\beta)^\alpha} \;\middle|\; k\in\left\{1\dots K\right\}\right\}\text{,}
\label{eq:zipf_law}
\end{equation}
where $\alpha\approx 1$, $\beta\approx2.7$, and the distribution matching loss tries to minimize the Jensen-Shannon (JS) divergence between the categorical frequency distribution of IMU and motion codebook
\begin{small}
\begin{equation}
    \mathcal L_{\text{dist}} = \operatorname{JS}\left({\mathbf{F}^\text{imu}} \;\middle|\middle|\; {\mathbf{F}^{\text{motion}}}\right) + \lambda_{\text{zipf}}\operatorname{JS}\left({\mathbf{F}^\text{motion}} \;\middle|\middle|\; {\mathbf{F}^{\text{zipf}}}\right)\text{.}
\label{eq:distribution_matching}
\end{equation}
\end{small}

\subsection{Implementation Details}
\label{sec:implementation_detail_part1}
To accommodate the high frame rates typical of IMU sensors, we standardize motion data across various datasets to 50 and 60 frames per second (fps). Consequently, we adopt a codebook size of $K=1024$, which exceeds that used in MotionGPT \cite{jiang2023motiongpt}, and trained on 20 fps data to better capture the increased temporal resolution. We observed that higher compression rates $l$ can introduce square-wave-like artifacts in the encoded IMU signals in our experiment. To address this, we set $l=4$, achieving a balanced trade-off between the compactness and the expressiveness of the discrete token representation. During training, the EMA coefficient is $\gamma=0.99$, and the loss weights are configured as: $\lambda_{\textrm{recon}}=1.0$, $\lambda_{\textrm{commit}}=0.02$, $\lambda_{\textrm{contact}}=0.01$, $\lambda_{\textrm{slide}}=0.01$, $\lambda_{\textrm{dist}}=1.0$, $\lambda_{\textrm{code}}=1.0$, and $\lambda_{\textrm{zipf}}=0.2$.
We utilize the AdamW optimizer \cite{loshchilov2017decoupled} with learning rate $\textrm{lr}=2\times10^{-4}$ and cosine annealing scheduler \cite{loshchilov2016sgdr}. The training batch size is set to $512$ for both motion and IMU tokenizer.
