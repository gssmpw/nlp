\section{Introduction}
\label{sec:intro}

Human motion encapsulates rich information about action intentions and thought processes of us humans, serving as a crucial foundation for understanding human behavior patterns. Inertia, a contactless and continuously measurable physical quantity from IMUs, can directly reflect the dynamic forces and kinematic states underlying human movement. This measurement enables the reconstruction of physically consistent motion in virtual environments, transforming physical actions into analyzable digital representations. However, merely replicating motion in virtual spaces remains insufficient. Intelligent systems should also provide real-time feedback during human-computer interactions to enhance behavioral understanding and facilitate self-improvement. 
Therefore, a motion agent capable of real-time motion reconstruction and online behavior analysis becomes vital for various application scenarios such as exercising, rehabilitation, and skill development. Ideally, the capturing and analysis process should be user-friendly, highly accessible, and intuitive for interactions, just like modern conversational AI agents.

Recent advances in large language models (LLMs), have driven significant progress in multimodal intelligent systems, enabling natural language interaction across text, vision, audio, and even human motion expressed in SMPL~\cite{SMPL2015} parameters. However, they struggle to capture the dynamic forces and torques governing three-dimensional movement. Existing parametric motion representations, though useful for approximating body poses, omit critical temporal derivatives like velocity and acceleration, limiting their capacity to model the physics underlying motion. In contrast, IMUs overcome these limitations by providing wearable, high-frequency measurements of acceleration, angular velocity, and rotation, thereby offering spatio-temporally precise motion characterization. Therefore, for building an intelligent multimodal system that is capable of understanding and analyzing human motions, it is crucial to integrate and align the IMU modality with foundational perception modalities such as natural language. Nonetheless, naively taking SMPL~\cite{SMPL2015} as an intermediary to link raw IMU signals with natural language is suboptimal, since it inevitably discards certain raw signal patterns during parameterization. Moreover, achieving true multimodal alignment requires deeper integration of IMU data with foundational perception modalities, such as natural language investigated in this work.

In the field of computer graphics, IMU has become an essential tool for real-time human motion capture~\cite{noitom,Movella,mocopi} due to its practicality. Unlike camera-based systems for human mesh recovery, IMUs offer sparse, lightweight, occlusion-resistant sensing while preserving user privacy. Early IMU-based methods primarily relied on traditional optimization strategies to estimate human kinematic poses~\cite{von2017SIP}. More recent approaches, termed ``inertial posers"~\cite{huang2018DIP,TransPoseSIGGRAPH2021,PIPCVPR2022,van2024diffusionposer,yi2024pnp}, utilized data-driven neural networks to directly translate raw IMU signals into parametric body models~\cite{SMPL2015}, enabling wearable and efficient motion capture. However, existing methods remain limited to motion reconstruction, lacking higher-level analysis and contextual understanding of human movements. Advancing beyond basic capture capabilities, an intelligent motion agent capable of real-time feedback and multi-turn interaction with users could unlock transformative applications in healthcare, education, and digital fitness. For instance, text-based conversational interfaces could democratize access to skill development—novices in specialized exercises or rehabilitation routines might receive instant, tailored guidance through intuitive language interactions. Such a system would reduce learning barriers, lower costs, and enhance accessibility for diverse user groups.

In this work, we present Mojito, a novel IMU-based motion intelligence agent for real-time, continuous human motion capture and online analysis. Due to the inherent limitations of IMU sensors such as drift, cumulative errors, and external noise from connectivity or transmission issues, the feasibility of IMU-based systems in reliable and long-term motion analysis is hindered. To address these limitations, we introduce a noise-robust approach that diverges from prior methods. Specifically, instead of continuous representation, we encode IMU signals into a discrete latent space, within which the quantization strategy reduces jitter by mapping continuous IMU streams to fixed token sequences. Additionally, we learn a shared latent space between human motion and IMU data, incorporating with Zipf’s law regularization to align the frequency distribution of tokens with the inductive bias of natural language.

In order to integrate learned inertial tokens with language vocabulary, it is required to map tokens of multiple modalities into a shared semantic space. However, modern LLMs typically rely on high-dimensional token embeddings (e.g., $3,584$-dimensional text embeddings for Qwen2-7B-Instruct~\cite{yang2024qwen2} model). Therefore, directly learning the discrete latent space of sparse IMU data on such a high dimension is inefficient. To address this challenge, we pretrain a projection layer composed of $8$ Transformer blocks to project low-dimensional inertial tokens onto the LLM's text embedding space. The projected inertial tokens are then concatenated with text tokens and fed into causal language model~\cite{yang2024qwen2} with masks, enabling whole-sequence understanding of inertial tokens. Finally, we fine-tune the language model using LoRA adapters~\cite{hu2022lora} to enhance its flexibility and customization such as acting as a professional fitness coach or nutritionist, delivering tailored feedback on user actions.

To summarize, our main contributions include:
\begin{itemize}
    \item We propose the first multimodal system with real-time motion capture and online motion analysis through sparse IMU signals.
    \item We introduce a novel distribution matching learning strategy and jitter-reduced tokenizer for representing continuous and jittery IMU signals as a sequence of tokens, achieving robust motion capture results under various noisy input conditions.
    \item We integrate jitter-reduced inertial tokens into LLM and enable interaction-friendly applications for real-time motion understanding, including description and instruction with optionally customized styles.
\end{itemize}