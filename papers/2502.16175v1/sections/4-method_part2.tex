\section{Language Model with Inertial Tokens}
\label{sec:method2}

% With the aforementioned IMU tokenizer, our method discretizes continuous and jittery IMU signals to sequential jitter-reduced tokens. However, compared to high-dimensional embedding space of LLMs, our learned inertial tokens are situated in a compact latent space with low dimension. Therefore, it's necessary to pre-align inertial tokens with language embeddings for following multimodal understanding. In this section, we first introduce our method for preparing curated textual annotations paired with inertia and motion sequences (Sec. \ref{sec:data_preparation}). Subsequently, we present our method for projecting the inertial tokens onto the vocabulary space of Qwen2-7B-Instruct language model \cite{yang2024qwen2} in Sec.~\ref{sec:pretrain}, and our LoRA model adapters to further improve the professionalism, reasonability and stylization of our system in Sec.~\ref{sec:finetuning_lora}.

Using the aforementioned IMU tokenizer, our method discretizes continuous and jittery IMU signals into sequential jitter-reduced tokens. However, unlike the high-dimensional embedding space of LLMs, the learned inertial tokens reside in a compact, low-dimensional latent space. Consequently, it is essential to pre-align these inertial tokens with language embeddings to facilitate subsequent multimodal understanding. In this section, we first introduce our method for generating curated textual annotations paired with inertial and motion sequences. (Sec.~\ref{sec:data_preparation}). Following this, we detail our method for projecting the inertial tokens into the vocabulary space of Qwen2-7B-Instruct language model \cite{yang2024qwen2} in Sec.~\ref{sec:pretrain}, and introduce our LoRA model adapters, which enhance the system's professionalism, rationality, and stylization in Sec.~\ref{sec:finetuning_lora}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/data_preparation.png}
  \vspace{-20px}
  \captionof{figure}{\textbf{Data Generation Pipeline.} The corresponding motion label is first extracted and expanded into a descriptive sentence using the LLM. Subsequently, a prompt is employed to generate a more refined and professional description or instructional output.}
\label{fig:data}
\vspace{-10px}
\end{figure}

\subsection{Data Preparation}
\label{sec:data_preparation}
% In order to prepare extensive training data, we instruct GPT-4o-mini using specifically devised prompts to automatically rephrase the raw textual annotations in original datasets or generate interactive dialogues based on concise action labels. Given the rich diversity of human motion, as depicted in Fig.~\ref{fig:data}, we split our collected datasets into two broad categories: ``daily motion” and ``professional exercise”, and annotate with captions and instructions respectively.

To prepare extensive training data, we instruct GPT-4o-mini with carefully designed prompts to automatically rephrase raw textual annotations from original datasets or generate interactive dialogues based on concise action labels. Given the rich diversity of human motion, as illustrated in Fig.~\ref{fig:data}, we categorize the collected datasets into two broad groups: ``daily motion'' and ``professional exercise'', and annotate them with descriptions and instructions respectively.

\vspace{-4mm}
\paragraph{Descriptive and Instructive Text Data Generation} 
% For daily motion, we induce GPT-4o-mini to output responses composed of two structural parts: objective description of the given action, and subjective reasoning of the motivation or intention of the acting human. For instructive split, we ask for detailed motion analysis, accompanied with assessment and instruction, including encouragements or critiques with clear attitudes. To ensure the human-like feedback, we enforce the generated texts to be in second-person narrative style.

For the daily motion category, we prompt GPT-4o-mini to generate responses structured into two parts: an objective description of the given action and a subjective analysis of the motivation or intention behind the human actor's behavior. For the instructive category, we request detailed motion analysis, supplemented with assessments and instructions, including encouragements or critiques expressed with clear attitudes. To ensure the feedback resembles human-like communication, we constrain the generated texts to adopt a second-person narrative style.

\vspace{-4mm}
\paragraph{Stylized Feedback Generation} 
% To further diversify and enrich the dataset, we introduce stylized roles that incorporate distinct personalities and tones. We first characterize each role by specifying its personality, typical wording, and overarching stylistic features. Next, we either select an existing role utterance, or virtually construct one as an example. This yields vivid, character-driven dialogues that encompass various styles for our system interactions.

To further diversify and enrich the dataset, we introduce stylized roles that incorporate distinct personalities and linguistic tones. First, we define each role by specifying its personality traits, typical phrasing and overarching stylistic characteristics. Subsequently, we either select an existing role-specific utterance or virtually construct one as an exemplar. This approach generates vivid, character-driven dialogues that encompass a wide range of stylistic variations, enhancing the diversity of interactions within our system.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/inference.png}
  \vspace{-20px}
  % \captionof{figure}{\textbf{Inference Pipeline.} Jittery IMU signals are initially tokenized into jitter-reduced inertial tokens, which are simultaneously decoded by the learned motion decoder to reconstruct human motion, and mapped to the language semantic space via the pretrained projection module for motion analysis.}
  \captionof{figure}{\textbf{Inference Pipeline.}
    Jittery IMU signals are first tokenized into jitter-reduced inertial tokens. These tokens are concurrently processed in two ways: (1) they are decoded by the learned motion decoder to reconstruct human motion, and (2) they are projected into the language semantic space via the pretrained projection module for motion analysis.  
  }
\label{fig:inference_pipeline}
\vspace{-10px}
\end{figure}


\subsection{Projecting Inertial Token to Text Embedding}
\label{sec:pretrain}
% Seamless translating and understanding between IMU and textual modalities require a shared and well-aligned embedding space. However, it is challenging to achieve this since the text embedding dimension of well-known LLMs is usually high, for example the Qwen2-7B-Instruct~\cite{yang2024qwen2} defines its embedding dimension as $d_h=3584$, which is far higher than the dimension $d_z=512$ of our inertial tokens. To resolve this challenge, we draw inspiration from OneLLM~\cite{han2024onellm} by introducing a projection module $\mathcal{P}_{\theta}$, which projects inertial tokens $\mathbf{b}_s^{\textrm{imu}}$ onto the text embedding space. 

Seamless translation and understanding between IMU and textual modalities necessitate a shared and well-aligned embedding space. However, achieving this is challenging due to the significant disparity in embedding dimensions between well-known LLMs and inertial tokens. For instance, the Qwen2-7B-Instruct~\cite{yang2024qwen2} employs a text embedding dimension $d_h=3584$, which is substantially larger than $d_z=512$ dimension of our inertial tokens. To address this issue, we adopt a strategy inspired by OneLLM~\cite{han2024onellm}, introducing a projection module $\mathcal{P}_{\theta}$ that maps inertial tokens $\mathbf{b}_s^{\textrm{imu}}$ into the text embedding space.


\vspace{-4mm}
\paragraph{Training of Projection Module} 
% As illustrated in Fig.~\ref{fig:train}, our projection module consists of 8 transformer blocks followed by a linear layer. In each transformer block, we follow Llama3~\cite{dubey2024llama} to incorporate self-attention layer, feed-forward network, and skip connections to ensure gradient flow. For each inertial token $\mathbf{b}_s^{\textrm{imu}}$, our projection module maps it onto the text embedding space of Qwen2-7B-Instruct~\cite{yang2024qwen2}
% \[
% \mathbf{e}_s = \mathcal{P}_{\theta}(\mathbf{b}_s^{\textrm{imu}}) \in \mathbb{R}^{d_h}\text{.}
% \]
% Simultaneously, the optional user textual prompts are tokenized and embedded into the identical embedding space, which are subsequently concatenated with projected IMU tokens, formulating the input of our language model. In current stage, we freeze the Qwen2-7B-Instruct~\cite{yang2024qwen2} model and only train our projection layers $\mathcal{P}_{\theta}$ using the cross-entropy loss. In order to build the semantic association between inertial tokens and texts, rather than the inner causality of inertial token sequence, we apply the training strategy of chatting language models, which augments mask to all the input tokens and excludes them from the training loss computation.

As illustrated in Fig.~\ref{fig:train}, our projection module comprises eight transformer blocks followed by a linear layer. Each transformer block incorporates a self-attention layer, a feed forward network, and skip connections, following the architecture of Llama3~\cite{dubey2024llama}, to ensure effective gradient flow. For each inertial token $\mathbf{b}_s^{\textrm{imu}}$, the projection module maps it to the text embedding space of Qwen2-7B-Instruct~\cite{yang2024qwen2}:
\[
\mathbf{e}_s = \mathcal{P}_{\theta}(\mathbf{b}_s^{\textrm{imu}}) \in \mathbb{R}^{d_h}\text{.}
\]
Concurrently, optional user-provided textual prompts are tokenized and embedded into the same space, after which they are concatenated with projected inertial tokens to form the input for the language model. At this stage, we keep the Qwen2-7B-Instruct~\cite{yang2024qwen2} model frozen and train only the projection layer $\mathcal{P}_{\theta}$ using cross-entropy loss. To establish semantic associations between inertial tokens and text, rather than focusing on the intrinsic causality within the inertial token sequence, we employ a training strategy inspired by chatting language models. This involves augmenting mask to all input tokens and excluding them from the loss computation during training.

\subsection{Fine-tuning Language Model Adapters}
\label{sec:finetuning_lora}

% To further empower our language model with more flexibilities and customization capabilities, we finetune 4 Low-Rank Adaptation (LoRA)~\cite{hu2022lora} adapters, which enable stylized feedback with character-specific tones in customized responsive roles. During fine-tuning, we freeze both the projection module $\mathcal{P}_{\theta}$ and the pretrained weights of Qwen2-7B-Instruct~\cite{yang2024qwen2} language model, solely adjusting the LoRA adapters. Our finetuning process provides our model diverse linguistic understanding on same inertial sequences, enabling personalized and flexible usage.

To further empower our language model with greater flexibility and customization capabilities, we finetune 4 Low-Rank Adaptation (LoRA)~\cite{hu2022lora} adapters. These adapters enable the generation of stylized feedback with character-specific tones, allowing for tailored responses in customized roles. During fine-tuning, both the projection module $\mathcal{P}_{\theta}$ and the pretrained weights of the Qwen2-7B-Instruct~\cite{yang2024qwen2} language model remain frozen, with updates applied exclusively to the LoRA adapters. This fine-tuning process enriches the model's linguistic understanding of the same inertial sequences, facilitating personalized and adaptable usage scenarios.