The Principal Component Analysis (PCA) was the first dimensionality reduction technique that focused on extracting uncorrelated features. 
This transformation is achieved while preserving the maximal variance in the original dataset.

\paragraph{Autoencoders~(AEs).} The PCA reduction is closely related to autoencoders \citep{kramer1991autoencoder_nlpca, rumelhart1986ae_structure}. 
An AE consists of two neural networks: an encoder that compresses the input into a lower-dimensional code, and a decoder that reconstructs the input from this representation. 
Both networks are trained jointly with a reconstruction error. 
Interestingly, the optimal solution of a linear AE corresponds to performing PCA. 
However, unlike PCA, an autoencoder can learn nonlinear dimensionality reductions, but its latent space is not guaranteed to have uncorrelated dimensions. 
The variational autoencoder (VAE, \cite{kingma2013VAE}) is an extension of AEs in which the encoder maps the input data into a probabilistic representation and the decoder reconstructs data by sampling from this distribution. %
This extension of AEs enabled the generation of new samples similar to the training data. 
Multiple works~\citep{higgins2022beta_vae, burgess2018understand_beta_vae, kim2018factor_vae, chen2018tc_vae} proposed variations of the VAE objective to encourage disentangled representations. Intuitively, disentanglement implies learning representations where changes in one dimension correspond to changes in a single latent variable, but disentangled concepts may be dependent. 
From this perspective, the problem is ill-defined~\citep{locatello2019challenging_disentanglement} and is not the focus of this study. 

\paragraph{Independent Component Analysis~(ICA).} 
ICA~\citep{jutten1991ica_bss} is a method to separate mixed signals into independent ones. Unlike PCA, ICA does not reduce dimensionality but instead focuses on identifying statistically independent components. 
When combined with an AE, our dependence minimization algorithm solves a combination of PCA and ICA. 
However, the successive application of PCA and ICA~\citep{back1997IPCA_first,yao2012ipca_later} is not equivalent to our approach since the PCA decomposition may extract redundant features on which ICA is then ill-posed. 

\paragraph{Self-supervised learning~(SSL).} 
SSL is an active research area in the field of representation learning. 
Popularized by SimCLR~\citep{chen2020SimCLR_ssl}, a leading paradigm in SSL is to train a network to be invariant under carefully designed data augmentations (such as illumination changes, cropping, ...). 
While effective, this idea comes with the risk of collapsed representations~\citep{hua2021feature_decorr}: dimensional collapse and collapse to a single value. 
The former happens when embedding vectors span a lower-dimensional subspace instead of the entire available space~\citep{jing2021understand_ssl_dim_collapse}, while the latter is generally due to a lack of counterweight to the invariance term. 
Many SSL approaches relied on linear decorrelation of output dimensions~\citep{huang2018decorrelated_bn,zbontar2021barlow_ssl,ermolov2021whitening_WMSE_ssl,bardes2021vicreg_ssl} as a means to avoid collapse. 
However, as illustrated in Example \ref{example:uncorrelated_but_dependent}, linearly uncorrelated dimensions can still co-vary. %
Our algorithm extends the idea to nonlinear mutual dependence minimization, and finds a broader scope of applications. 

\paragraph{Input-output mutual information maximization.} 
Another line of work on dimensionality reduction is based on the infomax principle~\citep{linsker1988infomax,bell1995infomax}, which suggests maximizing the mutual information (MI) between the input data and the output of a neural network to learn informative representations. 
Mutual Information Neural Estimation~(MINE, \cite{belghazi2018MINE}) provided a first estimate of the MI between high-dimensional continuous random variables using neural networks. Based on MINE, DeepInfoMax~\citep{hjelm2019deep_infomax} learns representations by: maximizing the input-output MI, maximizing the MI between global and local representations, and matching the output to a uniform prior with adversarial learning. 
Similarly to our work's objective, MI is an information-theoretic measure of the information shared by two random variables. However, our algorithm does not explicitly maximize input-output MI but instead minimizes the MI between output dimensions.

\paragraph{Adversarial learning.} 
We now discuss the core training paradigm behind our algorithm. %
Generative Adversarial Networks (GANs, \cite{goodfellow2014generative}) are the first approach to training neural networks adversarially: a generator is trained to create realistic synthetic data, while a discriminator is trained to predict whether a sample came from the training dataset or the generator. 
InfoGANs~\cite{chen2016infogan} extends GANs by adding a criterion that maximizes a lower bound of the MI between the representations and the generated data. This helps learn disentangled representations. 
Our approach differs from InfoGANs as it does not maximize a proxy for MI and it is not bound to generative networks. 
Most similar to our work,~\cite{brakel2017indep_feats_adversarial} used adversarial networks to decrease dependence by training an encoder to produce samples from a joint distribution that are indistinguishable from samples of the product of its marginals. However, this training objective is unstable and requires careful tuning, whereas ours systematically converges to the desired equilibrium. 





