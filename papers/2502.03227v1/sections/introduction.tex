In representation learning~\citep{rumelhart1986learning_repres,hinton2006deep_belief,bengio2013representation_learn}, algorithms learn to extract lower-dimensional encodings from input data, aiming for compact yet informative representations. 
A key strategy is to learn representations with minimally redundant dimensions, where each feature encodes a distinct concept. 
Such minimally redundant dimensions may offer several advantages: efficient data compression, improved generalization to unseen data, enhanced interpretability of the learned representations, ...




The strategy of minimizing feature redundancy has been applied in various methods, from classical techniques like the Principal Component Analysis~\citep{pearson1901pca_lines_and_planes, hotelling1933pca_2} to more recent self-supervised learning (SSL) approaches~\citep{huang2018decorrelated_bn, zbontar2021barlow_ssl, bardes2021vicreg_ssl}. Both PCA and these SSL methods aim to extract representations with uncorrelated dimensions. 
However, it is important to note that while these methods minimize pairwise linear correlations ($\rho$) between embedding dimensions, non-linear dependencies may still be present. 
We illustrate this with Example~\ref{example:uncorrelated_but_dependent} (Figure~\ref{subfig:limitation_of_linear}). 
\begin{example} \label{example:uncorrelated_but_dependent}
    Let a random variable $\rx_1$ be drawn from a uniform distribution over the interval $\left[-a,a\right]$ and $\rx_2 = \rx_1^2$. Since $\rx_2$ is a deterministic function of $\rx_1$, it is clear that the variables co-vary. 
    However, we find that: $\Cov(\rx_1, \rx_2) = 0$ and $\rho(\rx_1, \rx_2) = 0$. 
    The random variables are thus dependent despite zero correlation. 
\end{example} 
\begin{figure*}
    \vspace{-8px}
    \centering
    \begin{subfigure}[t]{0.31\textwidth}  %
        \centering
        \def\svgwidth{120px}
        {\scriptsize \input{dependence_example_1.pdf_tex}}
        %\includesvg[pretex=\scriptsize,width=120px,height=120px]{dependence_example_1.svg}
        \vspace{0px}
        \caption{Lin. $\rx_2=-\rx_1$: $\rho = -1$ and $\mathcal{R} = 1$.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.35\textwidth}
        \centering
        \def\svgwidth{120px}
        {\scriptsize \input{dependence_example_2.pdf_tex}}
        %\includesvg[pretex=\scriptsize,width=120px,height=120px]{dependence_example_2.svg}
        \vspace{-0.5px}
        \caption{Nonlin. $\rx_2=\rx_1^2$: $\rho = 0$ and \smash{$\mathcal{R} = \sqrt{1/2}$}.}
        \label{subfig:limitation_of_linear}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \def\svgwidth{120px}
        {\scriptsize \input{dependence_example_3.pdf_tex}}
        %\includesvg[pretex=\scriptsize,width=120px,height=120px]{dependence_example_3.svg}
        \vspace{0px}
        \caption{Indep. $\rx_2 \in \mathcal{U}(-1,1)$: $\rho = \mathcal{R} = 0$.}
    \end{subfigure}
    \caption{Illustration of the joint and marginal distributions for different types of dependencies between random variables $\rx_1 \in \mathcal{U}(-1,1)$ and $X_2$. The variables are linearly uncorrelated ($\rho = 0$) in both (b) and (c) but are independent only in (c).} %
    \label{fig:correlation_and_dependence_ex}
\end{figure*}
This example highlights a limitation of algorithms that rely on Pearson correlation: 
they may not eliminate all forms of dependencies/redundancies, as linearly uncorrelated variables can still exhibit nonlinear relationships~\citep{hyvarinen2000ICA_book}.  
Still, developing a stable method for mutual and nonlinear dependence reduction remains an unresolved problem. The challenge lies in designing a training objective that is simultaneously differentiable, scalable, and distribution-agnostic.

This paper presents a training algorithm to reduce the dependence among learned embedding dimensions using neural networks. 
The algorithm involves an adversarial game between two types of players: 
(1) a series of small neural networks are trained to predict one dimension of a representation given the other dimensions, and (2) an encoder is trained to counter reconstruction by updating the representations. 

Our adversarial objective can be viewed as a soft independence constraint in a task-specific optimization problem, allowing its incorporation into various learning algorithms. In this work, we explore three potential applications:
\begin{enumerate}[itemsep=2pt,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item extending the PCA algorithm to non-linear decorrelation (unsupervised learning)
    \item learning features that generalize beyond label supervision in supervised learning
    \item preventing dimensional collapse in SSL by learning minimally redundant representations
\end{enumerate}

This paper's main contribution is the introduction of an algorithm for nonlinear mutual dependence minimization and showing that, when combined with a standardization of the representations, it systematically converges to an equilibrium where the dependence among the embedding dimensions is minimal. 
In addition, we 
(1) illustrate the extension of the PCA algorithm with an example, 
(2) provide an alternative formulation that allows for a small degree of redundancy, and run experiments that suggest that this formulation helps a classification model learn concepts beyond label supervision, 
and finally (3) show how our SSL method generalizes current SSL decorrelation techniques, and analyze its benefits and shortcomings. 
