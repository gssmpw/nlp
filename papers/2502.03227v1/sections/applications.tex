\label{sec:applications}
Having developed an algorithm for interdependence minimization, we explore its applicability in diverse tasks: consider a task-specific minimization problem with objective function $\mathcal{L}_{\mathrm{task}}$ that we would like to restrict to solutions for which the output dimensions are independent:
\begin{equation} \label{eq:constrained_problem_strict}
\begin{aligned} %
    \underset{\vtheta}{\min} & \quad \mathcal{L}_{\mathrm{task}}(\mX, \cdot ; \vtheta) \\ \textrm{subject to} & \quad \rz_1 \indep \rz_2 \indep \cdots \indep \rz_d
\end{aligned}
\end{equation}
where $\indep$ denotes that each random variable is mutually independent of all others. 

We observe that the encoder's objective from Section~\ref{sec:method_description} can be seen as a Lagrangian relaxation of the independence constraint. Indeed, both the standardized and margin formulations can be rewritten as constraints of the form $g(x) = 0$, where the constraint is fulfilled when dimensions are minimally dependent\footnote{Assuming that dependency predictors have enough capacity and that the dependence is minimal when dependency predictors are fooled.}. 
Therefore, the problem from Equation~\ref{eq:constrained_problem_strict} can be relaxed to:
\begin{equation} \label{eq:approx_problem}
    \underset{\vtheta}{\min} \quad \mathcal{L}_{\mathrm{task}}(\mX, \cdot ; \vtheta) + \lambda \mathcal{L}_{\mathrm{adv}}(\mZ, \hat{\mZ}; \vtheta, \vphi)
\end{equation} %
where $\lambda \in \mathbb{R}$ is a Lagrange multiplier and where the adversarial objective from the encoder is denoted by $\mathcal{L}_{\mathrm{adv}}(\vz, \hat{\vz}) = 1 - \lVert \vz - \hat{\vz} \rVert_2^2$ for the standardized formulation or by $\mathcal{L}_{\mathrm{adv}}(\vz, \hat{\vz}) = \max \left(0, \alpha  - \lVert \vz - \hat{\vz} \rVert_2^2\right)$ for the margin loss formulation. 

In this approximated objective, the soft constraint pushes towards minimally dependent embedding dimensions, and thus minimally redundant solutions. 
In the following, we investigate three applications for this objective: an extension of PCA to nonlinear decorrelation (Section~\ref{subsec:pca_application}), the minimization of redundancy in a supervised classification setting (Section~\ref{subsec:classif_application}) and the use of the algorithm to prevent dimensional collapse in SSL (Section~\ref{subsec:ssl_application}). 

\subsection{Principal and Independent Component Analysis} \label{subsec:pca_application}

PCA is a linear dimensionality reduction technique that transforms a set of variables $\rmX = \{\vx^{(i)}\}_{i=1}^N$, $\vx^{(i)} \in \mathbb{R}^l$ into a set of uncorrelated representations $\vz^{(i)} = \mW^T\vx^{(i)}$, $\vz^{(i)} \in \mathbb{R}^d$, where the columns of the matrix $\mW \in \mathbb{R}^{l \times d}$ are called principal components. 
Intuitively, PCA identifies directions $\mW_{:,i}$ in the input space along which the variance of the projected data is maximized. Assuming centered data, the PCA reduction solves the following problem:
\begin{equation} \label{eq:pca_max_var_def}
    \max_{\mW} \quad \frac{1}{N-1} \sum_{i=1}^N \lVert \mW^T\vx^{(i)} \rVert_2^2 \quad
    \textrm{s.t.} \quad \mW^T\mW = \mI_d 
\end{equation}
We proceed to discuss two key properties of PCA. 
Firstly, the orthogonality constraint on the projection matrix $\mW$ ensures that the dimensions of the projections are uncorrelated: $\Cov(\rz_i, \rz_j) = 0 \text{ for all } i \neq j$. 
Secondly, PCA can equivalently be defined as finding the projection matrix minimizing the MSE for reconstructions $\hat{\vx}^{(i)} = \mW\vz^{(i)} = \mW\mW^T\vx^{(i)}$. This dual form led to the use of autoencoders to solve PCA~\citep{baldi1989pca_and_sgd,kramer1991autoencoder_nlpca,hinton2006pca_ae_sgd}. 
It is worth observing that the constraint on the unit norm of the principal components is not needed in the dual formulation, as the reconstruction error inherently accounts for the scale of the projections. 

Given these observations, we extend the PCA algorithm to non-linear decorrelation by replacing the covariance constraint with our more general (standardized) dependence minimization objective: 
\begin{multline} \label{eq:extended_pca_loss}
    \min_{\mW} \quad \frac{1}{N} \sum_{i=1}^N \lVert \vx^{(i)} - \mW \vz^{(i)} \rVert_2^2 
    + \lambda \mathcal{L}_{\mathrm{adv}}(\vz^{(i)}, \hat{\vz}^{(i)}) %
\end{multline} %
In the following, we illustrate the relevance of this PCA extension with a simple example and refer to it as \textit{Principal and Independent Component Analysis} (PICA). 
\begin{example} \label{ex:pca_vs_pica}
    let two independent latent factors $\rv_1$ and $\rv_2$ be uniformly distributed over the interval $[-\sqrt{3},\sqrt{3}]$ (so that $\mathbb{V}[\rv_1] = \mathbb{V}[\rv_2] = 1$) and observations of a random vector defined by: 
    \smash{$
        \rvx = \big[5\rv_1, 3 \cos(2\pi\rv_1/\sqrt{3}), \rv_2 \big]^T
    $}. 
    The observations are linearly uncorrelated by construction: $\Cov[\rx_i,\rx_j] = 0$ for all $i \neq j$, and with descending variances: $\mathbb{V}[\rx_1] = 25$, $\mathbb{V}[\rx_2] = 9/2$, $\mathbb{V}[\rx_3] = 1$. 
    Since PCA extracts uncorrelated variables with maximal variance, its solution for $d=2$ is: $\rvz_{\mathrm{PCA}} = [\rx_1,\rx_2]^T$. Yet, this solution is highly redundant since $\rx_1$ and $\rx_2$ are both functions of $\rv_1$. 
    In contrast, the global minimum to the PICA objective is the maximum variance solution with independent dimensions: $\rvz_{\mathrm{PICA}} = [\rx_1,\rx_3]^T = [5\rv_1,\rv_2]^T$. 
    Unlike the PCA reduction, these learned features capture both true latent factors. 
    We further discuss this example and empirically study its solutions in Appendix \ref{subapp:pca_ipca_example_eval}.
\end{example}

The PCA extension also holds for non-linear AEs, leading to a NLPCA reduction~\citep{kramer1991autoencoder_nlpca} with minimal redundancy. We refer interested readers to Appendix~\ref{subapp:nlpca_extension}.


\subsection{Classifiers and Generalization} \label{subsec:classif_application}

Example~\ref{example:classif_subset} illustrated that a classifier may rely only on a subset of the relevant characteristics to discriminate training classes. %
We encourage a classifier to minimize redundancy among the embedding dimensions to counter this limitation. This could push the model to encode additional features (e.g., \textit{shape} and \textit{color} in this example) and better generalize. %
Furthermore, since perfectly independent embedding dimensions may not be optimal for classification, we implement the adversarial objective with the margin loss formulation, which only maximizes the reconstruction error until dependency predictors fail to reconstruct samples with an error lower than a margin $\alpha$. 
Formally, the loss function is a weighted combination of the softmax cross-entropy loss and the adversarial objective: 
\begin{multline} \label{eq:cls_min_redund_loss}
    \min_{\vtheta, W, \vb} \quad \frac{1}{N} \sum_{i=1}^N \mathcal{L}_{\mathrm{CE}}(\softmax(W\vz^{(i)} + \vb), y^{(i)}) \\ 
    + \lambda \mathcal{L}_{\mathrm{adv}}(\vz^{(i)}, \hat{\vz}^{(i)})
\end{multline}
where $\vz^{(i)} \in \mathbb{R}^d$ is the penultimate representation of the classifier, $y^{(i)}$ is the target class index, $W \in \mathbb{R}^{n_c \times d}$ and $\vb \in \mathbb{R}^{n_c}$ are respectively the weights and biases from the classification head with $n_c$ classes.

\subsection{Self-Supervised Learning} \label{subsec:ssl_application}

Following SimCLR~\citep{chen2020SimCLR_ssl} and the SSL techniques built on it \citep{zbontar2021barlow_ssl,caron2021DINO_ssl,bardes2021vicreg_ssl}, we learn representations by training a network to be invariant to data augmentations. 
Intuitively, it enforces consistency between the input and the output by pushing two augmentations of the same image to lie close in the embedding space, i.e. a small change in the input should not lead to a completely different output. 
Furthermore, in a similar spirit to decorrelation SSL techniques~\citep{huang2018decorrelated_bn, zbontar2021barlow_ssl}, our ADMin algorithm prevents dimensional collapse~\citep{jing2021understand_ssl_dim_collapse} by pushing representations to be minimally redundant. However, unlike those approaches, ours is not bound to pairwise decorrelation and can thus eliminate all forms of redundancies. 

In practice, we sample a mini-batch of $n$ images and duplicate every image. We then apply different data augmentations to the two views of each image and enforce invariance by minimizing the MSE between the representations $\vz$ and $\vz^\prime$ from corresponding augmented views: 
\begin{multline} \label{eq:ssl_min_redund_loss}
    \min_\vtheta \quad \frac{1}{N} \sum_{i=1}^N \lVert \vz^{(i)} - (\vz^\prime)^{(i)} \rVert_2^2 \\
    + \frac{\lambda}{2} \left[ \mathcal{L}_{\mathrm{adv}}(\vz^{(i)}, \hat{\vz}^{(i)}) + \mathcal{L}_{\mathrm{adv}}((\vz^\prime)^{(i)}, (\hat{\vz}^\prime)^{(i)}) \right]
\end{multline}  %
where invariance is applied to standardized representations following BYOL~\citep{grill2020BYOL_ssl}.



