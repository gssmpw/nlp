This work introduced an algorithm to minimize the dependence among feature dimensions of a neural network. 
Its standardized formulation leads to stable training and empirically converges to minimally dependent dimensions. 
We additionally introduced a margin loss formulation allowing for controlled redundancy levels and validated experimentally that it could benefit certain applications. 
The algorithm's applications include: extending PCA to nonlinear decorrelation (PICA), classifier regularization for improved generalization, and learning minimally redundant representations in SSL. 

\paragraph{Limitations and future work. } 
Some applications require careful tuning to balance interdependence minimization and task-specific performance, highlighting the need to further study the training dynamics and trade-offs. 
Other promising areas for future work include exploring how our approach's generalization capabilities can be leveraged for out-of-distribution detection, as well as independence as a property for representations' interpretability. 




