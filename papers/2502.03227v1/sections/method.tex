\begin{figure*} %
    \centering
    \def\svgwidth{0.68\textwidth}
    {\scriptsize \input{ADMin_illustr_edited.pdf_tex}}
    % \includesvg[pretex=\scriptsize,width=0.75\textwidth]{ADMin_illustr_edited.svg}
    \caption{Illustration of the \textit{adversarial dependence minimization} architecture. The \textit{dependency predictors} minimize the reconstruction error by learning how dimensions relate, while the encoder maximizes the error by reducing dependencies.}
    \label{fig:adversarial_game_illustr}
\end{figure*} %
\label{sec:method_description}

We present the \textit{Adversarial Dependence Minimization} (ADMin) algorithm to reduce the nonlinear mutual dependence among learned embedding dimensions. 

Consider the representations $\vz^{(i)} = f_{\vtheta}(\vx^{(i)})$ from input samples $\vx^{(i)}$ of a dataset $\rmX=\{\vx^{(i)}\}_{i=1}^N$ with i.i.d. samples.
Our algorithm involves two types of networks: an encoder $f_\vtheta: \mathcal{X} \to \mathbb{R}^d$ that learns lower-dimensional representations of the training data and a small \textit{dependency predictor} for every embedding dimension $h_{\vphi_i}: \mathbb{R}^{d-1} \to \mathbb{R}$.  %
The \textit{dependency predictors} are trained to learn how dimensions relate by imputing a missing value $z_i$ based on the other values $\vz_{-i}$: 
\begin{equation} \label{eq:min_mse_reco}
    \hat{z}_i = h_{\vphi_i}(\vz_{-i}) = h_{\vphi_i}(z_1, \dots, z_{i-1}, z_{i+1}, \dots, z_d)
\end{equation}
Importantly, each network takes all $d-1$ remaining dimensions as input to effectively capture mutual dependencies. 
We implement \textit{dependency predictors} with Multi-Layer Perceptrons (MLP) since they are universal approximators~\citep{hornik_mlp_universal,cybenko1989universal_approx_orig, leshno1993universal_approx_nonpolyn} and can, in theory, approximate arbitrarily well the relation among the variables if given enough capacity. 

Intuitively, we now aim to design an algorithm that would exploit the knowledge extracted by the \textit{dependency predictors} to guide the encoder to reduce the dependence among the embedding dimensions. 
Taking inspiration from Generative Adversarial Networks~\citep{goodfellow2014generative}, we train both networks simultaneously and model the objective as a two-player game where the encoder and \textit{dependency predictors} are respectively trained to maximize and minimize the expected reconstruction error:
\begin{equation} \label{eq:minmax_game}
    \min_{\vphi} \max_{\vtheta} \quad \mathbb{E}_{\vz\sim P(\mathcal{X};\vtheta)} \lVert \vz - \hat{\vz} \rVert_2^2
\end{equation}
where $P(\mathcal{X};\vtheta)$ is the distribution of representations learned by the encoder, parameterized by $\vtheta$. 
In this adversarial game (depicted in Figure~\ref{fig:adversarial_game_illustr}), the \textit{dependency predictors} learn how dimensions relate by minimizing the reconstruction error ($\min_{\vphi}$), while the encoder maximizes the error ($\max_{\vtheta}$) by exploiting this information to reduce dependencies. 

\paragraph{Linear example.} Take for instance linear dependency networks: each network will learn affine relations among the representation's dimensions to reconstruct the remaining one accurately. 
However, a linear network cannot succeed when the output dimensions are affinely independent. 
Therefore, training an encoder to counter the reconstruction can be interpreted as a proxy objective for the decorrelation of the dimensions. 

\paragraph{Efficient implementation.} Since running many small \textit{dependency predictors} sequentially is inefficient on GPUs, we concatenate the inputs from the $d$ networks and implement the dependency networks as one large convolutional network with one-dimensional grouped convolutions~\citep{krizhevsky2012_alexnet} with $d$ groups. This equivalent implementation allows all models to be run at once.

\subsection{Standardized Formulation} \label{subsec:stdized_formulation}

This section introduces a bounded formulation of the problem that empirically converges to minimally dependent dimensions. %

The encoder is trained to maximize the reconstruction error. It may therefore indefinitely enlarge the norm of the representations to increase the error. 
We overcome this trivial solution by standardizing the distribution dimension-wise before reconstruction: 
$z_i \leftarrow \left(z_i - \mathbb{E}[z_i]\right)/\sqrt{\mathbb{V}[z_i]}
$ where $\mathbb{E}[z_i]$ and $\mathbb{V}[z_i]$ are respectively the mean and variance of dimension $i$. These quantities are estimated from the current mini-batch, similarly to batch-normalization layers~\citep{ioffe2015batchnorm}. 

\begin{proposition} \label{prop:convergence_std_game}
    The adversarial game of Equation~\ref{eq:minmax_game} converges, under standardized representations, to a solution where the dependence among the dimensions is minimal and where the dependency predictors predict the mean (zero) vector. The average reconstruction error is then equal to the (unit) variance.
\end{proposition}
Here follows an intuitive justification of the proposition, which we empirically verify in Section~\ref{sec:results_convergence}:  
if the \textit{dependency predictors} are trained with an MSE reconstruction loss and predict the mean vector, then the expected value of the loss is, by definition, equal to the variance~\citep{baldi1989pca_and_sgd}. 
This variance is consistently one for representations standardized to have a zero mean and unit variance. 
Assuming that the dependency networks can approximate the (fixed) mean, this forms an upper bound on the expected error since it can be achieved regardless of the distribution of representations. 
The mean is also the prediction with the lowest-cost expected error when embedding dimensions are statistically independent since the input of the \textit{dependency predictors} is then irrelevant to the quantity to estimate. Furthermore, a dependence among the feature dimensions learned by the encoder could lead to a lower cost loss as the dependency networks may exploit the dependence to improve the reconstruction. 

Let us further rewrite the adversarial objective from the encoder as the following equivalent minimization problem: $\arg\max_{\vtheta} \mathbb{E}_{\vz\sim P(\mathcal{X};\vtheta)} \lVert \vz - \hat{\vz} \rVert_2^2 = \arg\min_{\vtheta} 1 - \mathbb{E}_{\vz\sim P(\mathcal{X};\vtheta)} \lVert \vz - \hat{\vz} \rVert_2^2$. This loss is equal to zero when the \textit{dependency predictors} can do no better than predicting the mean since the average expected error is then equal to one. 

\subsection{Margin Loss Formulation} \label{subsec:margin_formulation}

What if certain applications required representations with a small degree of redundancy? 
Take, for instance, the following classification problem: %
\begin{example} \label{example:classif_subset}
    Consider a dataset of colored shapes with three training classes: \textit{"red squares"}, \textit{"green triangles"}, and \textit{"blue triangles"}. 
    A classifier could achieve a minimum loss value of zero and perfect accuracy by solely relying on the color.
    However, when presented with unseen \textit{"red triangles"} during inference, the model would then confidently misclassify these as \textit{"red squares"}, despite the different shape. 
\end{example}
Fixing the limitation in Example~\ref{example:classif_subset} requires the classifier to extract features for both \textit{shape} and \textit{color}, which could be encouraged by minimizing redundancy in the output representation. 
Unfortunately, the two latent variables are not statistically independent in the training set. Indeed, assuming a balanced number of samples in every class, we find $P(\textit{"green"} \cap \textit{"triangle"}) = \frac{1}{3}$ and $ P(\textit{"green"})P(\textit{"triangle"}) = \frac{1}{3}\frac{2}{3} = \frac{2}{9}$, so $P(\textit{"green"} \cap \textit{"triangle"}) \neq P(\textit{"green"})P(\textit{"triangle"})$.


Hence, we introduce an alternative formulation that only minimizes dependence up to a certain threshold. 
We formulate the encoder's objective as the maximization of a pairwise margin~\citep{cortes1995svn_margin, tsochantaridis2005large_margin} with hyper-parameter $\alpha$: 
$\min_{\vtheta} \max \left(0, \alpha  - \lVert \vz - \hat{\vz} \rVert_2^2\right)$. 
With this formulation, the encoder does not push the reconstruction error beyond $\alpha$, while \textit{dependency predictors} are still trained to minimize the reconstruction error with no margin: $\lVert \vz - \hat{\vz} \rVert_2^2$.

