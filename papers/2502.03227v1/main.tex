\documentclass{article} 

\usepackage{hyperref}
\usepackage{url}

\usepackage[preprint]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{wrapfig}

\input{math_commands.tex}

\usepackage[capitalize,noabbrev]{cleveref}


\newcommand{\cmark}{\ding{51}}  %
\newcommand{\xmark}{\ding{55}}  %

\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{0em}{-0.5em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother

\newcommand{\indep}{\perp \!\!\! \perp}

\definecolor{lightgray}{HTML}{EFEFEF}

\newcommand{\updated}[1]{{\color{blue}#1}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\icmltitlerunning{Adversarial Dependence Minimization}

\begin{document}

\twocolumn[
\icmltitle{Adversarial Dependence Minimization}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pierre-François De~Plaen}{kuleuven}
\icmlauthor{Tinne Tuytelaars}{kuleuven}
\icmlauthor{Marc Proesmans}{kuleuven}
\icmlauthor{Luc Van~Gool}{kuleuven,ethz,sofia}
\end{icmlauthorlist}

\icmlaffiliation{kuleuven}{ESAT-PSI, KU Leuven, Belgium}
\icmlaffiliation{ethz}{CVL, ETH Zürich, Switzerland}
\icmlaffiliation{sofia}{INSAIT, Sofia University, Bulgaria}

\icmlcorrespondingauthor{Pierre-François De~Plaen}{pdeplaen@esat.kuleuven.be}

\icmlkeywords{representation learning, adversarial networks, independence, PCA, ICA, SSL, generalization}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
Many machine learning techniques rely on minimizing the covariance between output feature dimensions to extract minimally redundant representations from data. 
However, these methods do not eliminate all dependencies/redundancies, as linearly uncorrelated variables can still exhibit nonlinear relationships. 
This work provides a differentiable and scalable algorithm for dependence minimization that goes beyond linear pairwise decorrelation. 
Our method employs an adversarial game where small networks identify dependencies among feature dimensions, while the encoder exploits this information to reduce dependencies. 
We provide empirical evidence of the algorithm's convergence and demonstrate its utility in three applications: 
extending PCA to nonlinear decorrelation, improving the generalization of image classification methods, and preventing dimensional collapse in self-supervised representation learning. 
\end{abstract}

\section{Introduction}
\input{sections/introduction}

\section{Related Work}
\input{sections/related_work}

\section{Background and Motivation}
\input{sections/background}

\section{Adversarial Dependence Minimization}
\input{sections/method}

\section{Applications}
\input{sections/applications}

\section{Experiments}
\input{sections/experiments}

\section{Conclusion}
\input{sections/conclusion}

\section*{Impact Statement}
This paper presents work that aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 




\bibliography{bib}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\input{appendix/pca_extension}
\input{appendix/analysis}
\input{appendix/independence}
\input{appendix/algo}
\input{appendix/additional_results}
\input{appendix/architectures}
\input{appendix/training_details}
\input{appendix/clevr4_ssl_baselines}

\end{document}
