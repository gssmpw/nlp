\section{Principal and Independent Component Analysis (PICA)}

\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\linewidth]{latent_space_AE-linear_CovReg.jpg}
        \caption{PCA reduction implemented with a linear autoencoder and a covariance regularization term.}
        \label{fig:latent_space_pca_covreg}
    \end{subfigure}
    \vspace{20px}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\linewidth]{latent_space_AE-linear_dependence-linear.jpg}
        \caption{PCA reduction implemented with a linear autoencoder and linear \textit{dependency predictors}.}
        \label{fig:latent_space_pca_lin_dependency}
    \end{subfigure}
    \vspace{20px}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.6\linewidth]{latent_space_AE-linear_dependence-nonlinear.jpg}
        \caption{PICA reduction implemented with a linear autoencoder and nonlinear \textit{dependency predictors}.}
        \label{fig:latent_space_pica_nonlin_dependency}
    \end{subfigure}%
    \caption{Learned representations $\rvz$. The colors indicate the value of the original latent factors $\rv_1$ (left) and $\rv_2$ (right). }
    \label{fig:latent_spaces_pca_pica_example}
\end{figure}


\subsection{Empirical Study of Example~\ref{ex:pca_vs_pica}} \label{subapp:pca_ipca_example_eval}

In Example~\ref{ex:pca_vs_pica}, the solution that maximizes the explained variance under the zero correlation constraint is $\rvz_{\mathrm{PCA}} = [\rx_1,\rx_2]^T$, with a total variance of $\mathbb{V}[\rvz_{\mathrm{PCA}}] = 25 + 4.5 = 29.5$. 
However, this is not a solution to PICA since $\rx_1 = 5 \rv_1$ and $\rx_2 = 3 \cos{2\pi\rv_1/\sqrt{3}}$ are both functions of the same latent factor $\rv_1$. 
The solution to PICA is thus $\rvz_{\mathrm{PICA}} = [\rx_1,\rx_3]^T$, with a total explained variance of $\mathbb{V}[\rvz_{\mathrm{PICA}}] = 25 + 1 = 26$. 
Attention should be drawn to the fact that the explained variance of PICA is always smaller or equal to the PCA decomposition. The equality occurs only when the highest variance uncorrelated combination of the inputs is mutually (and non-linearly) independent. 

We empirically study this example by comparing the solutions to four different implementations:
\begin{enumerate}
    \item the PCA decomposition solved with a Singular Value Decomposition. For this example, we rely on the \textit{scikit-learn}~\citep{scikit-learn} implementation: \url{https://scikit-learn.org/1.6/modules/generated/sklearn.decomposition.PCA.html}.
    \item the PCA decomposition implemented with a linear autoencoder and a covariance minimization objective. This approach is similar to~\cite{mialon2022vcreg} but without their variance regularization term. 
    \item the PCA decomposition implemented with a linear autoencoder and our standardized adversarial objective with linear dependency predictors. 
    \item the PICA decomposition implemented with a linear autoencoder and our standardized adversarial objective with nonlinear dependency predictors. 
\end{enumerate}

\paragraph{Implementation details.} 
We train methods (2) to (4) for 5000 steps. We generate 512 observations $\rvx$ by sampling from the uniform latent factors $\rvv$ at every iteration. 
The encoder is implemented with a projection $\mW^T \in \mathbb{R}^{3 \times 2}$ and the decoder uses the same matrix $\mW$. The autoencoders and dependency predictors are trained with the Adam optimizer, with learning rates of respectively $5 \cdot 10^{-3}$ and $2 \cdot 10^{-2}$. The training steps ratios are set to $k=16$. 
The covariance/dependence minimization loss coefficients are set to $\lambda=1$, while the reconstruction loss coefficient is set to 0.02. This weighting strategy aims to push the method not to compromise the covariance/dependence for a decreased reconstruction error.  

The PCA implementations from (1), (2) and (3) find respectively:
\begin{align}
    W_{\mathrm{PCA,1}} \approx 
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0
    \end{bmatrix} 
    \quad W_{\mathrm{PCA,2}} \approx 
    \begin{bmatrix}
        0.00 & 1.00 & -0.01 \\
        1.00 & -0.01 & 0.00
    \end{bmatrix} 
    \quad W_{\mathrm{PCA,3}} \approx 
    \begin{bmatrix}
        0.00 & 1.00 & 0.01 \\
        -1.00 & 0.00 & 0.01
    \end{bmatrix} 
\end{align} 
In line with theory, the three PCA implementations thus extracted the first two observed variables $\rx_1$ and $\rx_2$. 
For implementations (1) to (3), the total explained variances for the representations $\rz$ are all approximately equal to $29.5$ and covariance matrices are close to diagonal. 
However, the learned latent dimensions are not independent, with a distance correlation of $\mathcal{R}(\rz_1, \rz_2) \approx 0.25$.
The methods accurately reconstruct the first two observed variables $\rx_1$ and $\rx_2$, and achieve an average reconstruction error of 1 for $\rx_3$. It should be emphasized that this average error corresponds to the variance of $\rx_3$. 

The representations $\rvz$ for experiments (2) and (3) are shown in Figure~\ref{fig:latent_space_pca_covreg} and Figure~\ref{fig:latent_space_pca_lin_dependency}. The abscissa and ordinate depict the learned latent variables $\rz_1$ and $\rz_2$, while the color of the predictions indicates the true value of the true latent factors $\rv_1$ (left figure) and $\rv_2$ (right figure). The distributions align with our previous finding and illustrate that none of the PCA implementations encoded the latent factor $\rv_2$. 

On the contrary, the PICA implementation (4) learns:
\begin{align}
    W_{\mathrm{PICA}} \approx 
    \begin{bmatrix}
        1.00 & 0.00 & 0.01 \\
        0.00 & -0.01 & 1.00
    \end{bmatrix} 
\end{align}
The average reconstruction error is 4.46, which approximately equals the second dimension's variance. 
Furthermore, the distance correlation between the two learned representations is $\mathcal{R}(\rz_1, \rz_2) \approx 0.008$. 
These empirical results validate that the PICA reduction captured the two independent latent factors $\rv_1$ and $\rv_2$. Furthermore, the method could not reconstruct $\rx_2$ since it is implemented with a linear autoencoder. 
The distribution of the learned representations $\rvz$ is shown in Figure~\ref{fig:latent_space_pica_nonlin_dependency}. 

\subsection{Nonlinear Principal and Independent Component Analysis} \label{subapp:nlpca_extension}

The Non-Linear Principal Component Analysis (NLPCA) extended PCA to non-linear transformations. 
It was introduced by~\cite{kramer1991autoencoder_nlpca} as an auto-encoder learning the identity mapping. The architecture included an intermediate "bottleneck" representation, forcing the network to learn low-dimensional data representations. 
The original architecture was a four-layer feed-forward neural network with sigmoid activation functions. 
Following this definition, one can learn a NLPCA reduction by training an encoder $f_{\vtheta}: \mathbb{R}^l \to \mathbb{R}^d$ and a decoder $g_{\vpsi}: \mathbb{R}^d \to \mathbb{R}^l$ to minimize the reconstruction error: 
\begin{equation} \label{eq:original_nlpca}
    \min_{\vtheta,\vpsi} \quad \quad \frac{1}{N} \sum_{i=1}^N \lVert \vx^{(i)} - g_{\vpsi}(f_{\vtheta}(\vx^{(i)})) \rVert_2^2
\end{equation}
One may assume that a solution with minimal reconstruction error should exhibit low redundancy in the bottleneck representation. 
Yet, equation~\ref{eq:original_nlpca} does not explicitly push the bottleneck representation to have uncorrelated dimensions. 
Therefore, starting from the encoder-decoder architecture from the equation~\ref{eq:original_nlpca}, we add the ADMin objective by following the development from Section~\ref{subsec:pca_application}:
\begin{equation}
    \min_{\vtheta,\vpsi} \quad \quad \frac{1}{N} \sum_{i=1}^N \lVert \vx^{(i)} - g_{\vpsi}(f_{\vtheta}(\vx^{(i)})) \rVert_2^2 
    + \lambda \mathcal{L}_{\mathrm{adv}}(\vz^{(i)}, \hat{\vz}^{(i)})
\end{equation}
where $\vz^{(i)} = f_{\vtheta}(\vx^{(i)})$ is the bottleneck representation and $\hat{\vz}^{(i)}_j = h_{\vphi_j}(z^{(i)}_1, \dots, z^{(i)}_{j-1}, z^{(i)}_{j+1}, \dots, z^{(i)}_d)$ is the prediction from the $j$-th \textit{dependency predictor}. 
We denote this extension as \textit{Non-Linear Principal and Independent Component Analysis} (NLPICA) since it is an extension of the NLPCA reduction with an additional objective pushing the representations to have minimally interdependent dimensions. 

Note that the solution to the NLPICA reduction is not unique. 
In fact, research in nonlinear independent component analysis (NLICA) demonstrated that there are countless ways to transform independent variables while maintaining statistical independence~\citep{darmois1951analyse_liaisons,jutten2004_BSS_NLICA}. These transformations can involve complex mixing functions that result in representations that may be challenging to interpret or exhibit undesirable properties for certain applications. 
For examples of mixing transformations, we refer to~\citep{taleb1999example_mixing}. 
Despite this limitation, the problem can be made identifiable again with additional assumptions such as temporal non-stationarity~\citep{hyvarinen2016time_contrast_ica}, a conditionally factorized prior distribution over the latent variables of a VAE~\citep{khemakhem2020ivae_ica} or constraining the function class with constraints on their partial derivatives~\citep{buchholz2022ident_nlica_fn_class}.
