\section{Independence and Correlation} 

\subsection{Distance Correlation} \label{subapp:dcorr_details}

Distance correlation~\citep{szekely2007_dcorr} is a non-negative coefficient that characterizes both linear and nonlinear correlations between random vectors. %  in arbitrary dimensions. 
Let $\rx_1$ and $\rx_2$ be two random vectors with finite first moments, their respective characteristic functions be denoted $\psi_{\rx_1}$ and $\psi_{\rx_2}$, and their joint characteristic function be denoted $\psi_{\rx_1,\rx_2}$. Distance covariance measures the distance between their joint characteristic function and the product of the marginal characteristic functions:
\begin{equation}
    \mathcal{V}^2(\rx_1, \rx_2)=\int_{\mathbb{R}^{p+q}}\left|\psi_{\rx_1, \rx_2}(t, s)-\psi_{\rx_1}(t) \psi_{\rx_2}(s)\right|^2 w(t, s) d t d s
\end{equation}
where $w(t, s)$ is a positive weight function and characteristic functions are $\psi_{\rx}(t)=\mathbb{E}\left[e^{i t \rx}\right]$. Analogous to Pearson correlation, the squared distance correlation $\mathcal{R}^2$ is defined by: $\mathcal{R}^2(\rx_1,\rx_2) = \mathcal{V}^2(\rx_1,\rx_2) / \sqrt{\mathcal{V}^2(\rx_1, \rx_1) \mathcal{V}^2(\rx_2, \rx_2)}$ if $\mathcal{V}^2(\rx_1, \rx_1) \mathcal{V}^2(\rx_2, \rx_2)>0$ and $0$ otherwise. 

We refer to the original paper~\citep{szekely2007_dcorr} for the empirical estimation of distance correlation and to the library~\citep{carreno2023_dcor_python} for its implementation. 

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.75\linewidth]{pairwise_not_mutual_example_3d.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{pairwise_not_mutual_example_top_view.png}
    \end{subfigure}
    \caption{Illustration of Example \ref{example:pairwise_vs_mutual}: the random variables $\rx_1$, $\rx_2$ and $\rx_3$ are all mutually dependent despite being all pairwise independent.}
    \label{fig:pairwise_not_mutual_illustrs}
\end{figure}

\subsection{Pairwise and Mutual Independence} \label{subapp:pairwise_vs_mutual_ex}

\begin{example} \label{example:pairwise_vs_mutual}
    Let two random variables $\rx_1$ and $\rx_2$ be drawn from uniform distributions in the interval $\left[0,1\right]$. Then, define the random variable $\rx_3 = \rx_1 + \rx_2 - \floor{\rx_1 + \rx_2}$ where $\floor{\cdot}$ denotes the integral part of the value (e.g., $\floor{3.14} = 3$).
    \newline The random variable $\rx_3$ is uniformly distributed on $\left[0, 1\right]$ (see Figure~\ref{fig:pairwise_not_mutual_illustrs}) and the pairs $\{\rx_1, \rx_2\}$, $\{\rx_1, \rx_3\}$ and $\{\rx_2, \rx_3\}$ are pairwise independent. Still, the variables are not mutually independent since the uncertainty of $\rx_3$ is zero when the random vector $\rx_{-3} = [\rx_1, \rx_2]^T$ is known.
\end{example}
