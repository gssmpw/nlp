\section{Detailed Experimental Setups} \label{app:detailed_setup}

We provide here a detailed description of the training settings and hyper-parameters to facilitate the reproducibility of our experimental results.

The encoder and dependence networks are trained alternately, following the algorithm presented in Appendix~\ref{app:algo}. Epochs are counted relative to the encoder, which means that the dependence networks loop through the dataset $k$ times per encoder epoch. 

\paragraph{Dependence networks.} Dependence networks are always trained with the same optimizer and schedulers as their respective encoder. The default dependence network is a two-layer fully-connected network with a hidden dimension of size 32 and intermediate GELU~\citep{hendrycks2016gelu} activation function. There is no activation function at the output of the network.

\subsection{Experimental Setup: Convergence Analysis} \label{subapp:setup_convergence}

\paragraph{TinyImageNet experiments.} We trained two different dependence networks: a linear and a two-layer fully-connected network. Both are trained on standardized representations. The encoder is a ResNet-18 backbone with no projection head. We used the SGD optimizer with a momentum of 0.9, a learning rate of 0.8, a batch size of 256, and no weight decay. No learning rate schedule is used in this setting. The dependence networks are trained with a ratio of $k=2$ steps with learning rates of respectively 0.04 and 3.2. 
The models are trained on the TinyImageNet dataset for 100 epochs without data augmentations, but images are normalized with ImageNet mean and standard deviation per-channel values.

\paragraph{ImageNet experiment.} We trained two-layer dependence networks on standardized representations. The encoder is a ResNet-18 backbone with a three-layer fully-connected projection head with a hidden dimension of 4096, an output dimension of 512, ReLU activation functions, and intermediate BatchNorm layers. 
We used the SGD optimizer with a momentum of 0.9, a learning rate of 3.2, a batch size of 1024, and no weight decay. 
The learning rate follows a cosine decay schedule~\citep{loshchilov2016sgdr} with 10 epochs of linear warmup. The dependence networks are trained with a ratio of $k=4$ steps with a learning rate of 16. 
The model is trained on the ImageNet dataset for 50 epochs and follows the same data augmentations as the first views in~\cite{grill2020BYOL_ssl}.


\subsection{Experimental Setup: Clevr-4} \label{subapp:setup_clevr4}

\paragraph{Classification.} The baseline and adversarial approaches are trained with the same set of hyper-parameters, we therefore describe only the adversarial setting. 
We trained two-layer dependence networks. The encoder is a ResNet-18 backbone with no projection head. 
We used the SGD optimizer with a momentum of 0.9, a learning rate of 0.1, a batch size of 256, and a weight decay of $2 \cdot 10^{-5}$. 
The learning rate follows a cosine decay schedule~\citep{loshchilov2016sgdr} with 10 epochs of linear warmup. The dependence networks are trained with a ratio of $k=1$ steps with a learning rate of 0.3. The adversarial objective is a l1 margin loss on unstandardized representations with margin $\alpha=0.4$. The task weight is $\lambda = 5$. 
The models are trained for 200 epochs and data augmentations are described in Section~\ref{sec:results_infomax}. 

\paragraph{SSL.} We trained two-layer dependence networks on standardized representations. The encoder is a ResNet-18 backbone with no projection head. 
We used the SGD optimizer with a momentum of 0.9, a learning rate of 0.8, a batch size of 256, and a weight decay of $2 \cdot 10^{-5}$. 
The learning rate follows a cosine decay schedule~\citep{loshchilov2016sgdr} with 10 epochs of linear warmup. The dependence networks are trained with a ratio of $k=1$ steps with a learning rate of 0.3. The adversarial objective is a l1 margin loss with margin $\alpha=0.4$. The task weight is $\lambda = 5$. 
The models are trained for 200 epochs and data augmentations are described in Section~\ref{sec:results_infomax}. 


\subsection{Experimental Setup: ImageNet SSL} \label{subapp:setup_imagenet_ssl}

We trained two-layer dependence networks on standardized representations. The encoder is a ResNet-50 backbone with a three-layer fully-connected projection head with a hidden dimension of 4096, an output dimension of 512, ReLU activation functions, and intermediate BatchNorm layers. 
We used the LARS optimizer~\citep{you2017LARS} with a momentum of 0.9, a base learning rate of 1.5 with linear scaling rule~\citep{goyal2017lr_scaling_rule}, a batch size of 1024, and a weight decay of $10^{-4}$. 
The learning rate follows a cosine decay schedule~\citep{loshchilov2016sgdr} with 10 epochs of linear warmup. The dependence networks are trained with a ratio of $k=4$ steps with a base learning rate of 6. 
The model is trained on the ImageNet dataset for 100 epochs and follows the same data augmentations as in~\cite{grill2020BYOL_ssl}. 

\paragraph{Linear evaluation.} We followed standard procedure and trained a linear classifier on top of the frozen representations from the backbone. We used the SGD optimizer with a learning rate of 1.5, a weight decay of $10^{-6}$, a batch size of 256, and trained for 100 epochs. The learning rate follows a cosine decay schedule~\citep{loshchilov2016sgdr}. 
We applied two data augmentations during training: random horizontal flipping with $p=0.5$ and random cropping by keeping at least 8\% of the image area, followed by resizing to 224 $\times$ 224 pixels. 
During the evaluation, the images were resized so that the smaller side was 256 pixels wide and then center cropped to 224 $\times$ 224 pixels.


\subsection{Experimental Setup: Redundancy Study} \label{subapp:setup_loss_study}

The experimental setup is the same as for the classification model from Section~\ref{sec:results_infomax} already described in Appendix~\ref{subapp:setup_clevr4}. The only difference is that the model trained with standardized representations for reconstruction is trained with a mean squared reconstruction loss. Its dependence networks are trained with a ratio of $k=2$ steps instead of $k=1$ since this model did not converge with $k=1$. 


