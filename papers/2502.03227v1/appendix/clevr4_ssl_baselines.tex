\begin{table}[]
    \centering
    \caption{Results for the grid-search on SimCLR's hyper-parameters on the Clevr-4 dataset. \textit{LR} stands for base learning rate and \textit{BS} stands for batch size. The best-performing model is \textbf{highlighted}.}
    \label{tab:tuning_simclr}
    \vskip 0.15in
    \begin{tabular}{llll rrrr}
        \toprule
        \multicolumn{4}{c}{hyper-parameters} & \multicolumn{4}{c}{kNN top-1 accuracy} \\ 
        head & output dim. & \textit{LR} & \textit{BS} & shape & texture & color & count \\
        \midrule 
        Identity & (512) & 0.025 & 256 & 42.6 & 40.7 & 83.1 & 24.3 \\
        Identity & (512) & 0.025 & 512 & 40.6 & 40.4 & 83.1 & 24.2 \\
        Identity & (512) & 0.05 & 256 & 40.7 & 39.1 & 83.9 & 24.1 \\
        Identity & (512) & 0.05 & 512 & 47.7 & 39.8 & 80.8 & 24.3 \\
        Identity & (512) & 0.1 & 256 & 47.9 & 40.0 & 80.4 & 24.2 \\
        Identity & (512) & 0.1 & 512 & 10.1 & 9.9 & 9.8 & 9.4 \\
        Identity & (512) & 0.2 & 256 & 39.7 & 34.8 & 76.8 & 24.0 \\
        Identity & (512) & 0.2 & 512 & 40.3 & 32.2 & 75.4 & 23.5 \\
        Identity & (512) & 0.4 & 256 & 40.9 & 31.6 & 72.0 & 23.6 \\
        Identity & (512) & 0.4 & 512 & 42.6 & 29.9 & 73.3 & 22.6 \\
        Identity & (512) & 0.6 & 256 & 36.2 & 28.0 & 70.2 & 23.6 \\
        Identity & (512) & 0.6 & 512 & 40.6 & 31.9 & 73.4 & 22.8 \\
        \textbf{MLP} & \textbf{128} & \textbf{0.025} & \textbf{256} & \textbf{57.1} & \textbf{53.5} & \textbf{92.7} & \textbf{29.9} \\
        MLP & 128 & 0.025 & 512 & 55.6 & 51.0 & 90.9 & 28.2 \\
        MLP & 128 & 0.05 & 256 & 57.7 & 51.7 & 92.0 & 28.9 \\
        MLP & 128 & 0.05 & 512 & 55.4 & 47.3 & 87.8 & 27.6 \\
        MLP & 128 & 0.1 & 256 & 57.6 & 48.4 & 89.1 & 28.0 \\
        MLP & 128 & 0.1 & 512 & 47.7 & 41.2 & 86.4 & 27.3 \\
        MLP & 128 & 0.2 & 256 & 48.2 & 39.2 & 82.8 & 27.2 \\
        MLP & 128 & 0.2 & 512 & 45.9 & 39.3 & 84.2 & 26.5 \\
        MLP & 128 & 0.4 & 256 & 44.7 & 34.9 & 74.7 & 26.3 \\
        MLP & 128 & 0.4 & 512 & 46.0 & 35.6 & 75.1 & 26.7 \\
        MLP & 128 & 0.6 & 256 & 44.0 & 34.4 & 73.8 & 26.6 \\
        MLP & 128 & 0.6 & 512 & 40.8 & 37.3 & 70.4 & 26.4 \\
        MLP & 512 & 0.025 & 256 & 47.3 & 47.7 & 89.1 & 29.4 \\
        MLP & 512 & 0.025 & 512 & 46.7 & 46.2 & 89.4 & 28.4 \\
        MLP & 512 & 0.05 & 256 & 46.6 & 45.7 & 88.0 & 28.3 \\
        MLP & 512 & 0.05 & 512 & 46.2 & 43.7 & 87.1 & 28.2 \\
        MLP & 512 & 0.1 & 256 & 47.0 & 42.6 & 84.9 & 28.2 \\
        MLP & 512 & 0.1 & 512 & 54.4 & 44.2 & 85.7 & 27.3 \\
        MLP & 512 & 0.2 & 256 & 44.8 & 37.9 & 81.1 & 27.1 \\
        MLP & 512 & 0.2 & 512 & 49.4 & 42.6 & 79.6 & 26.3 \\
        MLP & 512 & 0.4 & 256 & 44.9 & 38.7 & 75.7 & 26.4 \\
        MLP & 512 & 0.4 & 512 & 46.6 & 37.0 & 75.9 & 26.3 \\
        MLP & 512 & 0.6 & 256 & 44.4 & 35.0 & 75.1 & 27.4 \\
        MLP & 512 & 0.6 & 512 & 45.2 & 34.1 & 73.3 & 27.0 \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{table}[]
    \centering
    \caption{Results for the grid-search on VICReg's hyper-parameters on the Clevr-4 dataset. \textit{LR} stands for base learning rate and \textit{BS} stands for batch size. The best-performing model is \textbf{highlighted}.}
    \label{tab:tuning_vicreg}
    \vskip 0.15in
    \begin{tabular}{llll rrrr}
        \toprule
        \multicolumn{4}{c}{hyper-parameters} & \multicolumn{4}{c}{kNN top-1 accuracy} \\ 
        head & output dim. & \textit{LR} & \textit{BS} & shape & texture & color & count \\
        \midrule 
        Identity & (512) & 0.005 & 256 & 82.1 & 86.7 & 100.0 & 28.2 \\
        Identity & (512) & 0.005 & 512 & 88.3 & 87.4 & 100.0 & 26.5 \\
        Identity & (512) & 0.01 & 256 & 86.9 & 85.3 & 100.0 & 34.1 \\
        \textbf{Identity} &  \textbf{(512)} & \textbf{0.01} & \textbf{512} & \textbf{91.4} & \textbf{88.6} & \textbf{100.0} & \textbf{27.2} \\
        Identity & (512) & 0.025 & 256 & 84.1 & 79.4 & 99.5 & 29.8 \\
        Identity & (512) & 0.025 & 512 & 86.2 & 84.3 & 99.4 & 31.9 \\
        Identity & (512) & 0.05 & 256 & 73.9 & 71.0 & 98.9 & 26.3 \\
        Identity & (512) & 0.05 & 512 & 81.0 & 77.5 & 98.9 & 28.8 \\
        Identity & (512) & 0.1 & 256 & 72.5 & 66.9 & 98.8 & 24.2 \\
        Identity & (512) & 0.1 & 512 & 60.6 & 65.0 & 98.4 & 23.7 \\
        Identity & (512) & 0.2 & 256 & 63.1 & 57.0 & 98.2 & 22.9 \\
        Identity & (512) & 0.2 & 512 & 51.1 & 54.8 & 97.0 & 20.0 \\
        Identity & (512) & 0.4 & 256 & 44.4 & 48.3 & 97.0 & 22.9 \\
        Identity & (512) & 0.4 & 512 & 50.3 & 51.2 & 97.4 & 23.5 \\
        MLP & 128 & 0.005 & 256 & 51.0 & 57.6 & 99.5 & 30.4 \\
        MLP & 128 & 0.005 & 512 & 51.4 & 61.3 & 99.0 & 29.7 \\
        MLP & 128 & 0.01 & 256 & 44.4 & 54.9 & 98.2 & 29.0 \\
        MLP & 128 & 0.01 & 512 & 47.7 & 57.5 & 98.3 & 27.7 \\
        MLP & 128 & 0.025 & 256 & 34.8 & 51.1 & 97.0 & 24.2 \\
        MLP & 128 & 0.025 & 512 & 42.8 & 50.6 & 97.0 & 25.8 \\
        MLP & 128 & 0.05 & 256 & 36.1 & 44.6 & 95.8 & 25.0 \\
        MLP & 128 & 0.05 & 512 & 36.8 & 49.9 & 97.3 & 24.7 \\
        MLP & 128 & 0.1 & 256 & 31.1 & 42.3 & 95.3 & 24.5 \\
        MLP & 128 & 0.1 & 512 & 34.3 & 44.3 & 96.2 & 25.2 \\
        MLP & 128 & 0.2 & 256 & 30.3 & 40.9 & 95.8 & 23.3 \\
        MLP & 128 & 0.2 & 512 & 31.7 & 42.7 & 96.3 & 23.5 \\
        MLP & 128 & 0.4 & 256 & 28.1 & 25.8 & 94.5 & 23.2 \\
        MLP & 128 & 0.4 & 512 & 26.8 & 14.1 & 33.3 & 21.5 \\
        MLP & 512 & 0.005 & 256 & 63.7 & 68.3 & 99.5 & 30.5 \\
        MLP & 512 & 0.005 & 512 & 63.8 & 66.1 & 99.4 & 28.4 \\
        MLP & 512 & 0.01 & 256 & 59.6 & 63.5 & 98.7 & 28.3 \\
        MLP & 512 & 0.01 & 512 & 61.5 & 64.3 & 98.6 & 26.2 \\
        MLP & 512 & 0.025 & 256 & 61.2 & 60.2 & 97.9 & 26.6 \\
        MLP & 512 & 0.025 & 512 & 62.2 & 62.8 & 98.0 & 24.7 \\
        MLP & 512 & 0.05 & 256 & 59.8 & 58.5 & 97.4 & 25.7 \\
        MLP & 512 & 0.05 & 512 & 58.6 & 59.1 & 97.2 & 23.9 \\
        MLP & 512 & 0.1 & 256 & 57.2 & 57.2 & 97.1 & 24.7 \\
        MLP & 512 & 0.1 & 512 & 57.2 & 56.8 & 97.0 & 23.5 \\
        MLP & 512 & 0.2 & 256 & 56.9 & 52.9 & 96.6 & 24.5 \\
        MLP & 512 & 0.2 & 512 & 55.7 & 54.2 & 96.8 & 23.5 \\
        MLP & 512 & 0.4 & 256 & 46.2 & 49.0 & 96.3 & 23.7 \\
        MLP & 512 & 0.4 & 512 & 44.1 & 51.2 & 96.5 & 23.0 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Clevr-4 Baselines} \label{app:clevr4_baselines}

This section details the hyper-parameter tuning of the SimCLR and VICReg baselines. 

We implemented the models following the original papers from SimCLR \cite{chen2020SimCLR_ssl} and VICReg \cite{bardes2021vicreg_ssl}. 
We trained ResNet-18 backbones and trained each model with and without a projection head to find which setup works best for each technique when applied to the Clevr-4 dataset. 
For a fair comparison, we followed the same experimental setup as for our SSL method: 
we used the SGD optimizer with a momentum of 0.9, and a weight decay of $2 \cdot 10^{-5}$. The learning rate follows a cosine decay schedule~\citep{loshchilov2016sgdr} with 10 epochs of linear warmup and is scaled with a linear scaling rule~\citep{goyal2017lr_scaling_rule}.

We ran a grid search on the projection head choice, the learning rate, and the batch size. The models were trained for 80 epochs and the best-performing model was then re-trained for 200 epochs. Its results are reported in Table~\ref{tab:results_clevr4_ssl} from Section~\ref{sec:results_infomax}.

Results for the grid search on the hyper-parameters from SimCLR and VICReg are reported respectively in Table~\ref{tab:tuning_simclr} and in Table~\ref{tab:tuning_vicreg}. We observe that the best-performing model for SimCLR has a projection head, while the VICReg technique works better with no projection head. 
This observation for VICReg is consistent with findings from our method applied to Clevr-4. This may be because the taxonomies are statistically independent and the augmentations are minimal, reducing the need for a projection head to prevent true invariance to data augmentations~\citep{bordes2022guillotine}.
