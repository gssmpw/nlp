
\documentclass[final,12pt]{arxiv} % Anonymized submission
%\documentclass[final,12pt]{colt2025} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Decision Making in Hybrid Environments]{Decision Making in Hybrid Environments: \\A Model Aggregation Approach}
\usepackage{times}
\input{commands_colt}
\newcommand{\HL}[1]{{\color{orange}[\text{HL:} #1]}}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\coltauthor{%
 \Name{Haolin Liu} \Email{srs8rh@virginia.edu}\\
 \addr University of Virginia
 \AND
 \Name{Chen-Yu Wei} \Email{chenyu.wei@virginia.edu}\\
 \addr University of Virginia
 \AND
 \Name{Julian Zimmert} \Email{zimmert@google.com}\\
 \addr Google Research
}

\begin{document}

\maketitle

\begin{abstract}%
Recent work by \cite{foster2021statistical, foster2022complexity, foster2023tight} and \cite{xu2023bayesian} developed the framework of \emph{decision estimation coefficient} (DEC) that characterizes the complexity of general online decision making problems and provides a general algorithm design principle. These works, however, either focus on the pure stochastic regime where the world remains fixed over time, or the pure adversarial regime where the world arbitrarily changes over time. For the hybrid regime where the dynamics of the world is fixed while the reward arbitrarily changes, they only give pessimistic bounds on the decision complexity. In this work, we propose a general extension of DEC that more precisely characterizes this case. Besides applications in special cases, our framework leads to a flexible algorithm design where the learner learns over subsets of the hypothesis set, trading estimation complexity with decision complexity, which could be of independent interest. Our work covers model-based learning and model-free learning in the hybrid regime, with a newly proposed extension of the bilinear classes \citep{du2021bilinear} to the adversarial-reward case. We also recover some existing model-free learning results in the pure stochastic regime. 



%The decision-estimation coefficient (DEC) characterizes the worst-case complexity for many online decision making problems such as multi-armed bandits or Markov decision processes. 
%
%However, the upper bound can become superfluous in mixed environments, where the transition is stochastic, but the reward is adversarial. We propose a general extension of DEC that covers such cases, matching known bounds in several regimes. It also allows us to establish novel bounds on the complexity for Model-based bilinear MDPs with adversarial rewards for both the full-information as well as the bandit setting. %
\end{abstract}

%\begin{keywords}%
%  Decision Estimation Coefficient (DEC), Algorithmic Information Ratio (AIR), Adversarial Markov Decision Process, Model-Based RL, Model-Free RL
%\end{keywords}

\section{Introduction}
The study of decision making problems based on the decision-estimation coefficient (DEC) complexity measure \citep{foster2021statistical} has been shown to characterize the sample complexity of decision making problems. Unlike previous studies on general function approximation that focus more on upper bounds \citep{jiang2017contextual, sun2019model, du2021bilinear, xie2022role}, DEC not only provides a general algorithm design principle that interleaves model estimation and decision making, but also characterizes the regret lower bound for decision making problems.


Since being proposed by \cite{foster2021statistical}, the DEC framework has been refined or extended in different directions: \cite{chen2022unified} extended the framework to various learning targets beyond no-regret learning, \cite{foster2024model} proposed a related complexity measure for model-free learning, \cite{foster2023tight} refined the gap between the regret upper and lower bounds, \cite{foster2023complexity} applied the approach to multi-agent learning, and \cite{wagenmaker2023instance} established instance-optimal guarantees. Another highly related line of research by \cite{foster2022complexity} and \cite{xu2023bayesian} focus on characterizing the complexity of \emph{adversarial} decision making, where the environment may change its underlying model arbitrarily across time. 

Yet, there are still important open questions remaining in this area. One of them is how to close the gap between the upper and lower bounds in \cite{foster2023tight}. The lower bound only captures the complexity of decision making (i.e., the cost of exploration), while the upper bound includes both the complexity of decision making and model estimation. The nature of the algorithm in \cite{foster2023tight} is a model-based algorithm, so the model estimation error naturally appears in the regret bound. To mitigate such gap, \cite{chen2024assouad} introduced a novel model-estimation complexity measure and provide a complete characterization for bandit learnability. However, their approach still exhibits a significant gap in reinforcement learning.  For general decision making, a common route to reduce estimation errors is to avoid fine-grained model estimation, but only perform \emph{value} estimation, as is the case of model-free value-based learning. However, such a coarser estimation may lead to loss of information that affect the complexity of decision making. It remains open how to characterize such trade-off. 

Another open direction is the study of intermediate environments that sit between pure stochastic environments \citep{foster2021statistical, chen2022unified, foster2023tight, foster2024model} and pure adversarial environments \citep{foster2022complexity, xu2023bayesian}. For learning in Markov decision processes (MDPs), it is well-known that when the transition changes arbitrarily, the worst-case sample complexity could be exponential in the horizon length. Such bound gives a very pessimistic indication for learning in adversarial MDPs. However, it has been shown in some special cases, when only the reward function is adversarial and transition remains fixed, polynomial regret or sample complexity is possible \citep{neu2013online, rosenberg2019online, jin2020learning}. This setting has practical significance, as it models the scenario where the the learner continues to learn new skills while the underlying world remains static \citep{abel2024definition, kumar2023continual}. %
%the non-stationarity of real-world decision-making and serves as a key instance of continual learning \citep{abel2024definition, kumar2023continual} where the the learner continues to learn new skills while the underlying world remains static.
Can we characterize the complexity of such hybrid setting  general cases? Being raised since \cite{foster2022complexity}, this question still remains open.



%However, existing works on adversarial MDPs with fixed transition only design tailored algorithms for specific settings like linear MDPs \citep{luo2021policy, sherman2023improved, dai2023refined, kong2023improved,liu2023towards} and low-rank MDPs \citep{liu2024beating}. 
In this paper, we propose a general framework that contributes to the above two directions. Compared to pure model-based learning approach in \cite{foster2021statistical, foster2023tight} that leads to large estimation complexity, or pure policy-based learning approach in \cite{foster2022complexity} and \cite{xu2023bayesian} that leads to large decision complexity, our framework allows learning over flexible partitions in the joint model and policy space, and trade-off estimation complexity with decision complexity. Under this framework, each partition may aggregate a subset of models or policies within which the learner does not perform finer-grained estimation.  It naturally captures settings such as model-free value learning (aggregating models that correspond to the same value functions), or fixed-transition adversarial-reward settings (aggregating models that have the same transition function), or a combination of them.  We expect that such a general framework may find its more uses beyond the applications in this paper. 

We apply this general framework to the following concrete settings, obtaining new results or recovering existing results from a different viewpoint. Below, hybrid setting refers to MDPs with fixed transition and adversarial reward. 

%In this paper, we take a step towards answering this question. We propose a general framework based on aggregation over model-policy space that can capture different level of granularity in the learning procedure and the target we learn. This conceptually captures different RL learning paradigm: model-based, value-based, or policy-based approaches, and characterize the trade-off. Building on this unified perspective, we not only establish the general learnability of the hybrid setting but also provide improved guarantees for specific instances in stochastic environment. Concretely, our new results are listed below, where the hybrid setting refers to adversarial MDPs with fixed transition.
\begin{enumerate}
    
    \item \textbf{Statistical complexity for the hybrid setting. } 
    % Whenever the adversary controls are over a convex subset of environments (think of a convex reward set under fixed transition in MDPs), 
    We show that the hybrid setting has essentially the same complexity as the fully stochastic setting, as long as the set of reward functions is convex (which is usually the case).  The only overhead for adversarial reward is a $\log(|\Pi||\calP|)$ estimation error as opposed to the $\log(|\calM|) = \log(|\calR||\calP|)$ in the stochastic setting, where $\Pi, \calM, \calP, \calR$ are the policy space, model space, transition space, and reward space, respectively. See \pref{sec: model-based}. 
   
    In scenarios where the adversarial reward function is revealed at the end of each episode (full-information), the same complexity measure applies. However, $\log(|\Pi||\calP|)$ can be improved in MDPs to $\log(|\calA||\calP|)$, where $\calA$ is the action space. See \pref{sec: model-based}. 
    
    \item \textbf{Model-free guarantee for the hybrid setting.} We propose a natural extension of the bilinear class \citep{du2021bilinear} to the hybrid setting. We derive regret bounds that only scale with $\log(|\calF|)$ for when there is full-information loss feedback,  where $\calF$ is the set of value functions, and $|\calF|$ could be much smaller than $|\calP|$. See \pref{sec: model-free adversarial}. 
    
    \item \textbf{Recovering results for the stochastic setting.} For model-free learning in stochastic environments, our framework reaches $\sqrt{T}$ regret for the linear $Q^\star/V^\star$ setting \citep{du2021bilinear}. While this result does not surpass the result of \cite{du2021bilinear}, it advances the broader goal of adapting the DEC framework for model-free learning, an objective shared by \cite{foster2024model}. See \pref{sec: stochastic model-free}.   % \cw{why's this? isn't the bilinear paper already getting this? }\HL{I only saw PAC guarantee in the bilinear paper, and did not find any paper that achieve this.} \cw{Just use doubling trick in their algorithm will get the $\sqrt{T}$ regret? } \HL{I actually cannot figure out it... Do not know how to get rid of the epoch in their algorithm..}
\end{enumerate}
%These findings open up a range of new directions and we hope they can inspire more unified understanding of the complexity for general decision-making.


\paragraph{Related work in the hybrid setting}
There is a rich literature in learning fixed-transition adversarial-reward MDPs. Most results are for the tabular case \citep{neu2013online, rosenberg2019online, jin2020learning, shani2020optimistic, chen2021finding}, or the case of linear function approximation \citep{luo2021policy, dai2023refined, sherman2023improved, liu2023towards, kong2023improved, zhao2023learning, li2024improved}. Extensions beyond linear function approximation are scarce, as only seen in \cite{zhao2023learning} and \cite{liu2024beating} for low-rank MDPs. We note that the work by \cite{foster2022complexity} and \cite{xu2023bayesian} for general adversarial decision making can also be applied here, but only achieves tight guarantees when the transition function is known. 

\paragraph{Concurrent work} Independently and concurrently, \cite{chen2025decision} proposed a closely related extension to the DEC framework. Their work and ours, however, focus on rather different sets of applications.

%Algorithms developed in this framework are crucial building block for multi-agent reinforcement learning \citep{jin2020learning, wang2023breaking, daskalakis2023complexity} as their ability to handle changing rewards facilitates efficient adaptation to other agents' behaviors. 

%Recently, this approach has been applied to reinforcement learning with human feedback (RLHF) under general preferences \citep{wang2023rlhf, swamyminimaximalist, rosset2024direct}. Notably, \cite{wang2023rlhf} establishes that RLHF with general preference can be reduce to learning adversarial MDPs with fixed transitions. This reduction implies that advances in adversarial MDPs with fixed transitions can directly enhance our theoretical understanding of RLHF with general preferences.
~\\
%\textbf{Outline}

%- Briefly describe the DEC framework 

%- Existing DEC work focus on pure stochastic or pure adversarial settings. Existing hybrid setting are limited to special cases (our own previous work). 

%- In this work we take a first step in this general setting

%- We further propose a general framework based on aggregation over model-policy space that can capture different level of granularity in the learning procedure and the target we learn. This conceptually captures different RL learning paradigm: model-based, value-based, or policy-based approaches, and characeterize the tradeoff. 

%- Future direction: combined with more different kinds of divergence measure




% \textbf{Other general function approximation papers:} maybe briefly mention a series other general function approximation papers? For example \citep{jiang2017contextual, sun2019model, du2021bilinear, xie2022role}. 

\section{Preliminaries}
We consider the general decision making problem.  Let $\calM$ be a model class, $\Pi$ be a policy class, and $\calO$ be an observation space. Every model $M: \Pi\to \Delta(\calO)$ specifies a distribution over observations when executing policy $\pi$ in model $M$.  At each round $t=1,2,\ldots, T$, the adversary first chooses a model $M_t\in\calM$ without revealing it to the learner. Then the learner chooses a policy $\pi_t\in\Pi$, and observes an observation $o_t\sim M_t(\cdot |  \pi_t)$. Every model $M$ is associated with a value function $V_M: \Pi\to [0,1]$ that specifies the expected reward of policy $\pi$ in model $M$. We also define $\pi_M$ as the policy that maximize $V_M$.

The regret with respect to the policy sequence $\pi_{1:T}^\star=(\pi_1^\star, \pi_2^\star, \ldots, \pi_T^\star)$ is defined as 
\begin{align*}
    \Reg(\pi_{1:T}^\star) = \sum_{t=1}^T \left(V_{M_t}(\pi_t^\star) - V_{M_t}(\pi_t)\right). 
\end{align*}
%
For simplicity, in this work, we assume $|\calM|$ and $|\Pi|$ are finite. 

%For a distribution $\nu\in \Delta(\calM\times \Pi)$ and a subset $\phi\subset \calM\times\Pi$, we write $\nu(\phi) = \sum_{(M,\pi)\in \phi} \nu(M,\pi)$ and $\nu(M,\pi|\phi) = \frac{\nu(M,\pi)\mathbf{1}\{(M,\pi)\in\phi\}}{\nu(\phi)}$. Let $\Phi\subset 2^{\calM\times \Pi}$ be a set of disjoint subsets of $\calM\times \Pi$. The posterior distribution of $\nu$ after executing $\pi$ and observing $o$ is defined as $\nu(\phi|\pi, o)\propto \nu(\phi)\E_{(M,\pi)\sim \nu(\cdot|\phi)}$


\subsection{Complexity Measures in Prior Work}
\paragraph{DEC} Let $\textsc{co}(\calM)$ be the convex hull of $\calM$. The offset DEC with KL-divergence \citep{foster2021statistical,xu2023bayesian} is defined as 
\begin{align}
\dec_{\eta}^{\textsc{KL}}\left(\calM\right) = \max_{\Bar{M} \in \textsc{co}(\calM)} \min_{p \in \Delta(\Pi)}\max_{M \in \calM} \E_{\pi \sim p}\left[V_M(\pi_M) - V_M(\pi) - \frac{1}{\eta}D_{\textsc{KL}}\left(M(\cdot|\pi), \Bar{M}(\cdot|\pi)\right)\right]. 
\label{eq:DEC}
\end{align}
The DEC aims to achieve the optimal balance between exploitation (minimizing suboptimality $V_M(\pi_M) - V_M(\pi)$) and exploration (maximizing information gain $D_{\textsc{KL}}\left(M(\cdot|\pi), \Bar{M}(\cdot|\pi)\right)$) under the worst possible environment.

% A general parameterized information ratio, which is related to the general AIR.
% \begin{align*}
% \info_{\gamma}^{\KL} = \sup_{\nu \in \Delta\left(\calM \times \Pi\right)}  \inf_{p \in \Delta(\Pi)} \E_{\pi \sim p}\E_{(M, \pi^\star) \sim \nu}\E_{o \sim M(\pi)}\left[V_M(\pi^\star) - V_M(\pi) - \gamma D_{\textsc{KL}}\left(v_{\bo{\phi}}(\cdot|\pi, o), v_{\bo{\phi}}\right)\right]
% \end{align*}
% We have
% \begin{align*}
%     \info_{\gamma}^{\KL}  &=  \inf_{p \in \Delta(\Pi)} \sup_{\nu \in \Delta\left(\calM \times \Pi\right)} 
%  \E_{\pi \sim p}\E_{(M, \pi^\star) \sim \nu}\E_{o \sim M(\pi)}\left[V_M(\pi^\star) - V_M(\pi) - \gamma D_{\textsc{KL}}\left(v_{\bo{\phi}}(\cdot|\pi, o), v_{\bo{\phi}}\right)\right]
%  \\&= \inf_{p \in \Delta(\Pi)} \sup_{\nu \in \Delta\left(\calM \times \Pi\right)} 
%  \E_{\pi \sim p}\E_{(M, \pi^\star) \sim \nu}\E_{\phi \sim v_{\Phi}}\left[V_M(\pi^\star) - V_M(\pi) - \gamma D_{\textsc{KL}}\left(v_{\bo{o}}(\cdot|\pi, \phi), v_{\bo{o}}(\cdot|\pi)\right)\right]
% \end{align*}
% % The general parameterized information ratio is equivalent to
% % \begin{align*}
% %  \inf_{p \in \Delta(\Pi)}\sup_{(M, \pi^\star) \in \calM \times \Pi} \E_{\pi \sim p}\left[V_M(\pi_M) - V_M(\pi) - \gamma  D_{\textsc{KL}}\left(M(\pi), \Bar{M}(\pi)\right)\right]
% % \end{align*}
% where $\nu_{\bo{o}}(o|\pi, \phi) = \sum_{M} \nu(M|\phi)P(o|\pi, M)$
% This is bounded by the following complexity
% \begin{align*}
% &\dec_{\gamma}^{\textsc{KL}}\left(\calM, \Phi\right) 
% \\&= \sup_{\Bar{M} \in \textsc{co}(\calM)} \inf_{p \in \Delta(\Pi)}\sup_{\nu \in \Delta\left(\calM \times \Pi\right)} \E_{\pi \sim p}\E_{(M, \pi^\star) \sim \nu}\E_{\phi \sim v_{\Phi}}\left[V_M(\pi^\star) - V_M(\pi) - \gamma  D_{\textsc{KL}}\left(v_{\bo{o}}(\cdot|\pi, \phi), \Bar{M}(\pi)\right)\right]
% \end{align*}
% For the model-based stochastic setting, $\dec_{\gamma}^{\textsc{KL}}\left(\calM, \Phi\right) $ has the same order as $\dec_{\gamma}^{\textsc{KL}}\left(\calM\right)$,  the proof is similar to page 24--26 in \cite{foster2022complexity}.


% \begin{align*}
%     &\sup_{\Bar{M} \in \textsc{co}(\calM)} \inf_{p \in \Delta(\Pi)}\sup_{\nu \in \Delta\left(\calM \times \Pi\right)} \E_{\pi \sim p}\E_{(M, \pi^\star) \sim \nu}\E_{M' \sim v_{M}}\left[V_M(\pi^\star) - V_M(\pi) - \gamma  D_{\textsc{KL}}\left(v_{\bo{o}}(\cdot|\pi, M'), \Bar{M}(\pi)\right)\right]
%     \\&\le \sup_{\Bar{M} \in \textsc{co}(\calM)} \inf_{p \in \Delta(\Pi)}\sup_{\nu \in \Delta\left(\calM \times \Pi\right)} \E_{\pi \sim p}\E_{M \sim \nu_M}\E_{M' \sim v_{M}}\left[V_M(\pi_M) - V_M(\pi) - \gamma  D_{\textsc{KL}}\left(v_{\bo{o}}(\cdot|\pi, M'), \Bar{M}(\pi)\right)\right]
%     \\&= \sup_{\Bar{M} \in \textsc{co}(\calM)} \inf_{p \in \Delta(\Pi)}\sup_{\nu \in \Delta\left(\calM \right)} \E_{\pi \sim p}\E_{M \sim \nu}\left[V_M(\pi_M) - V_M(\pi) - \gamma  D_{\textsc{KL}}\left(M(\pi), \Bar{M}(\pi)\right)\right]
%     \\&= \dec_{\gamma}^{\textsc{KL}}\left(\calM\right) 
% \end{align*}



%\subsection{Algorithmic Information Ratio (AIR) and Model-Index AIR (MAIR)}
\paragraph{AIR} \citet{xu2023bayesian} introduced AIR as a framework to analyse the regret of arbitrary algorithms and derive upper bounds on the complexity of online learning problems. Given $\rho\in\Delta(\Pi)$ and $\eta>0$, $\air:\Delta(\Pi)\times\Delta(\Pi \times \calM)\rightarrow \bbR$ is defined as
\begin{align*}
    \air_{\rho,\eta}(p,\nu) = \E_{\pi\sim p}  \E_{(M, \pi^\star) \sim \nu} \E_{o\sim M(\cdot|\pi)}\left[ 
 V_M(\pi^\star) - V_M(\pi) - \frac{1}{\eta}\KL\left(  \nu_{\bo{\pi^\star}}(\cdot | \pi, o), \rho \right) \right].
\end{align*}
Here, $\nu$ is a distribution over $\calM\times \Pi$, and $\nu_{\bo{\pi^\star}}(\cdot|\pi,o)$ is a posterior distribution over $\pi^\star$ given $(\pi,o)$, which can be calculated as
\begin{align}
    \nu_{\bo{\pi^\star}}(\pi^\star|\pi,o) = \frac{\sum_{M}\nu(M,\pi^\star)M(o|\pi)}{\sum_{M,\pi'}\nu(M,\pi')M(o|\pi)}.  \label{eq: for example}
\end{align}
The subscript $\bo{\pi^\star}$ in the notation $\nu_{\bo{\pi^\star}}(\cdot|\pi,o)$ specifies what variable the `~$\cdot$~' represents. We place it in the subscript and make it bold to distinguish it from a realized value of that random variable. When the variable is clear, we omit this subscript. For example, the left-hand side of \pref{eq: for example} can be simply written as $\nu(\pi^\star|\pi, o)$. 
Similar to DEC in \pref{eq:DEC}, AIR also seeks the optimal balance between minimizing sub-optimality gap and maximizing information gain. The key difference is that AIR considers an arbitrary comparator policy $\pi^\star$ instead of $\pi_M$ as in DEC, with exploration measured by the information gain associated with $\pi^\star$. The flexibility of the comparator policy enables AIR to handle the adversarial setting.  % but the overhead is to maintain a joint distribution over $\calM \times \Pi$ instead of $\calM$.



\paragraph{MAIR} For model based stochastic environments, given $\rho\in\Delta(\calM)$ and $\eta$, \citet{xu2023bayesian} further define $\mair:\Delta(\Pi)\times\Delta(\calM)\rightarrow \bbR$ as 
\begin{align*} 
    \mair_{\rho,\eta}(p,\nu) = \E_{\pi\sim p}  \E_{M \sim \nu}\E_{o\sim M(\cdot|\pi)}\left[ 
 V_M(\pi_M) - V_M(\pi) - \frac{1}{\eta}\KL\left(  \nu_{\bo{M}}(\cdot | \pi, o), \rho \right) \right].
\end{align*}
Here $\nu$ is only a distribution over $\calM$. This is because in stochastic setting, it suffices to compare against $V_M(\pi_M)$, making it unnecessary to consider a joint distribution over $\calM \times \Pi$. \cite{xu2023bayesian} also shows that MAIR is tightly related to DEC.

\ \\


\noindent\textbf{EXO} by \cite{lattimore2020exploration} and \cite{foster2022complexity} is closely related to AIR, and also serves as a useful complexity measure and algorithm design principle for adversarial decision making, with the additional benefit of getting high-probability bounds. In this work, we base our presentation in AIR due to its conciseness, but everything could be translated to the EXO framework. 


\subsection{Markov Decision Processes} 
A Markov decision process (MDP) is defined by a tuple $(\calS, \calA, P, R, H, s_1)$, where $\calS$ is the state space, $\calA$ is the action space, $P:\calS\times\calA\rightarrow\Delta(\calS)$ the transition kernel, $R:\calS\times\calA\rightarrow[0,1]$ the reward function,  $H$ the horizon and $s_1$ the starting state.
A learner interacts with an MDP for $H$ rounds $h=1,\dots, H$.
In every round, the learner observes the current state $s_h$ and selects an action $a_h\in\calA$. The state is transitioning into next state via the transition kernel $s_{h+1}\sim P(\cdot|s_h,a_h)$ and receives the reward $R(a_h,s_h)$.
We assume that the reward functions are constraint such that $\sum_{h=1}^HR(s_h,a_h)\leq 1$ for any policy almost surely. Given a policy $\pi:\calS\to\calA$, the $Q$-function and value function are defined by $Q^\pi_h(s, a)=\E^\pi[\sum_{h'=h}^{H-1}R(s_h,a_h)\,|\,s_h=s,a_h=a]$, $V^\pi_h(s)=Q^\pi_h(s,\pi(s))$ respectively. The value and $Q$-function of an optimal policy $\pi^\star$ are abbreviated with $V^\star/Q^\star$. We use $Q_h^\pi(s,a;M)$ and $Q_h^\star(s,a;M)$ where $M=(P,R)$ to denote the value functions under model $M$. 


\section{A General Framework}\label{sec: general frame}
In order to formalize mixed-adversarial and stochastic regimes, we propose the following framework.
Let $\Phi$ be a collection of disjoint subsets of $\calM\times\Pi$. That is, 1) each element $\phi\in\Phi$ is a subset of $\calM\times \Pi$, and 2) for any $\phi_1, \phi_2\in \Phi$, if $\phi_1\neq \phi_2$, then $\phi_1\cap \phi_2=\emptyset$.\footnote{The requirement that subsets in $\Phi$ be disjoint is not really necessary, as models can be duplicated if needed. However, we adopt this assumption as it holds in all our applications and simplifies the notation.}
\begin{definition}\label{def: restricted env} A $\Phi$-restricted environment is an (adversarial) decision making problem in which the environment commits to $\phi^\star\in\Phi$ at the beginning of the game and henceforth selects $(M_t,\pi^\star_t)\in \phi^\star$ in every round $t$ arbitrarily based on the history.
\end{definition}


\paragraph{Example}  
Let $\calM=\calP\times\calR$ decomposes into the set of transitions and rewards of a class of MDPs respectivly. Set $\Phi=\{\phi_{P,\pi^\star}|(P,\pi^\star)\in\calP\times\Pi\}$, where $\phi_{P,\pi}=\{((P,R),\pi )|R\in\calR\}$. The $\Phi$-restricted environment is exactly the setting of an MDP with fixed transition, fixed comparator and adversarial rewards.


To characterize the complexity of $\Phi$-restricted environments, we define for $\rho\in\Delta(\Phi)$, $\eta>0$, 
\begin{align*}
    \air^{\Phi}_{\rho,\eta}(p,\nu) = \E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^\star) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), \rho)\right],  
\end{align*}
where $\nu_{\bo{\phi}}(\cdot|\pi,o)$ is the posterior over $\Phi$ given $(\pi, o)$, which can be calculated as 
\begin{align*}
    \nu(\phi|\pi,o) = \frac{\sum_{(M,\pi^\star)\in \phi} \nu(M,\pi^\star) M(o|\pi) }{\sum_{\phi'\in\Phi}\sum_{(M,\pi^\star)\in \phi'} \nu(M,\pi^\star) M(o|\pi)}. 
\end{align*}
Again, this follows our convention that the $\bo{\phi}$ in the subscript of $\nu_{\bo{\phi}}(\cdot|\pi,o)$ is only an indicator of the variable represented by `~$\cdot$~', rather than a realized value, and it will be omitted when it is clear.  

$\air^\Phi$ recovers $\air$ and $\mair$. Specifically, $\air^\Phi$ recovers $\air$ when every $\phi$ corresponds to a single $\pi^\star\in\Pi$ and all $M\in\calM$, i.e., $\Phi=\{\phi_{\pi^\star}: \pi^\star\in\Pi\}$ where $\phi_{\pi^\star}=\{(M,\pi^\star): M\in \calM\}$. $\air^\Phi$ recovers $\mair$ when every $\phi$ corresponds to a single model $M$ and the associated best policy $\pi_M$, i.e., $\Phi=\{\phi_M:~ M\in\calM\}$ where $\phi_M = \{(M,\pi_M)\} = \{(M,\argmax_{\pi\in\Pi} V_M(\pi) )\}$. We define $\joint = \left(\bigcup_{\phi\in\Phi} \phi\right) \subset \calM\times \Pi$. 


With this definition, we propose the general algorithm in \pref{alg:general AIR}. Similar to \cite{xu2023bayesian}, we can decompose the objective in \pref{alg:general AIR} as the following terms: 
\begin{align*}
   %\air^\Phi_{\rho_t, \eta}(p,\nu) = 
   \underbrace{\E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu} \left[V_M(\pi^\star) - V_M(\pi)\right]}_{\text{expected regret}} - \frac{1}{\eta} \underbrace{\E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu} \E_{o\sim M(\cdot|\pi)} \left[ \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), \nu_{\bo{\phi}})\right]}_{\text{information gain}} - \frac{1}{\eta}\underbrace{\KL(\nu_{\bo{\phi}}, \rho_t)}_{\text{regularization}}
\end{align*}
where $\nu_{\bo{\phi}}(\phi) = \sum_{(M,\pi^\star)\in\phi} \nu(M,\pi^\star)$. The algorithm proceeds as follows. 
Upon receiving a reference distribution $\rho_t\in\Delta(\Phi)$ (an estimation from the previous round about~$\phi^\star$), the algorithm finds a policy distribution $p_t$ that simultaneously tries to minimize the expected regret and maximize the information gain about $\phi^\star$ against the worst case choice of the world distribution~$\nu_t\in\Delta(\calM\times \Pi)$ that is not too far from the previous estimation $\rho_t$. At the end of round $t$, the algorithm calculates $\rho_{t+1}$ as a posterior of $\nu_t$ with the new observation $(\pi_t, o_t)$, only stores this information, and forgets everything else. 

The algorithm $\air$ by \cite{xu2023bayesian} only maintains an estimation of the optimal policy $\pi^\star$, while their $\mair$ maintains an estimation of the underlying model $M^\star$. Our framework provides a natural generalization of them by allowing the target of estimation be subsets of $\calM\times \Pi$. For example, in MDPs with fixed transition and adversarial reward, we jointly estimate $(P^\star,\pi^\star)$, while for model-free value learning in stochastic MDPs, we estimate $(f^\star, \pi_{f^\star})$ where $f^\star$ is the true optimal value function and $\pi_{f^\star}$ is its corresponding optimal policy. These are achievable by properly choosing $\Phi$, as will be elaborated in the following sections. 

\begin{algorithm}[t]
\caption{General Algorithm} \label{alg:general AIR}
    Let $\rho_1(\phi) = \frac{1}{|\Phi|}$ for all $\phi \in \Phi$. \\
    \For{$t=1,2,\ldots, T$}{
       Find a distribution $p_t$ of $\pi$ and a distribution $\nu_t$ of $\Psi$ that solve the saddle-point of 
       \begin{align*}
           \min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)}  \air^{\Phi}_{\rho_t,\eta}(p,\nu). 
       \end{align*}
       Sample decision $\pi_t\sim p_t$ and observe $o_t\sim M_t(\cdot|\pi_t)$. \\
       Update $\rho_{t+1}(\phi)=\nu_{t}(\phi|\pi_t,o_t)$ for all $\phi\in\Phi$.  
       
    }
\end{algorithm}

The regret bound achieved by \pref{alg:general AIR} is stated in the following theorem. 

\allowdisplaybreaks
 \begin{theorem}\label{thm: general}
 For a $\Phi$-restricted environment, there is an algorithm such that 
\begin{align*}
    \E\left[\Reg(\pi^\star_{1:T})\right] \leq \frac{\log|\Phi|}{\eta} + T \max_{\rho \in\Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)} \air^\Phi_{\rho,\eta}(p,\nu). 
\end{align*}
\end{theorem}
The bound in \pref{thm: general} consists of two parts. The first part $\frac{\log|\Phi|}{\eta}$ captures the \emph{estimation complexity} over $\Phi$, while the other part $T\cdot \air^\Phi$ captures the \emph{decision complexity} that quantifies the cost of exploitation-exploration tradeoff. A finer partition $\Phi$ leads to larger estimation complexity but smaller decision complexity, and a coarser partition leads to the opposite. Since a coarser partition makes the environment strictly more powerful, the learner has the freedom to find the best trade-off over coarser partitions, while still ensuring a bound on the environment at hand.


\begin{remark}
All concrete applications studied in this paper actually consider the standard regret $\Reg(\pi^\star)$ where the regret comparator $\pi^\star$ is fixed over time. That requires each $\phi\in\Phi$ to have a unique $\pi^\star$.  The generality of our framework in allowing changing comparators $\pi^\star_{1:T}$ may be of independent interest and is left for future investigation. We note that this theorem does not violate the impossibility result for general time-varying comparator $\pi^\star_{1:T}$, as in that case the $\air^\Phi$ term in \pref{thm: general} could be of order $\Omega(1)$. This is discussed more deeply in \pref{lem: air by dec}.   %due to the constraint that $(M_t,\pi^\star_t)\in\phi^\star$ and that $\Phi$ consists of disjoint sets. For example, if we want the adversary to be able to select any combination of model and comparator, then $|\Phi|=1$, there is no information gain from the KL divergence term, the maxmin of $\air$ is 1 for any $\eta$ and the bound is indeed vacuous.
\end{remark}
%\jz{I have quite a problem coming up with any interesting regime where the generality would be useful. On the other hand, it will induce quite a bit of confusion among reviewers.} 
%\cw{What if we make the remark as above? On the other hand, it might be useful when there are multiple policies that are actually ``equivalent'' to each other and aggregating them might improve the bound. Alternatively, we may have to set additional restriction on $\Phi$. }


%\section{Proof of \pref{thm: general}}


\subsection{Relation with DEC}
As noted in \cite{foster2022complexity} and \cite{xu2023bayesian}, for pure adversarial decision making, the decision complexity is governed by the DEC of the convexified model class $\co(\calM)$. In the $\Phi$-restricted environment (\pref{def: restricted env}) that we consider, we show that the complexity is upper bounded by the DEC of a refined convexified model class taking the partition into account. 
\begin{definition}
The $\Phi$-aware convexification of the model class $\calM$ is defined as 
$$\bar\calM(\Phi):=\bigcup_{\phi\in\Phi}\co(\{M\in\calM~|~(M,\cdot)\in\phi\}).$$
\end{definition}
In other words, models are convexified within each subset $\phi$. This describes the complexity of the $\Phi$-restricted environment as follows.
%
\begin{lemma}\label{lem: air by dec}
    It holds that
    $\max_{\rho \in \Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)}  
\air^\Phi_{\rho,\eta}(p,\nu)\leq\dec^{\textsc{KL}}_\eta(\bar\calM(\Phi))$, \textbf{unless} $\Phi$ is an environment with adaptive comparator where sublinear regret is impossible. In this case
    $\max_{\rho \in \Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)}  
\air^\Phi_{\rho,\eta}(p,\nu)=\Omega(1)$ independently of $\eta$ 
.
\end{lemma}
While this is only providing an upper bound, it is actually tight in many environments of interest. Let a \emph{fixed comparator game} be such that all $\phi\in\Phi$ contain a single policy $\pi^\phi$. 
\begin{lemma}
\label{lem: dec equiv}
In all fixed comparator games if the choice of model is independent of the choice of comparator then $\max_{\rho \in \Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)}  
\air^\Phi_{\rho,\eta}(p,\nu)=\dec^{\textsc{KL}}_\eta(\bar\calM(\Phi))$ .
\end{lemma}
The choice of model being independent of the choice of comparator means that $\Phi=\Theta\times\Pi$ for an arbitrary disjoint partition $\calM=\bigcup_{\theta\in\Theta}\theta$ of models. This is in fact the case for most of our applications except for the application in \pref{sec: model-free adversarial}. 

Note that in stochastic regimes, we often trim the set to pairs of model and its optimal policy (see \pref{sec: stochastic model-free}). This trimming neither impacts the maxmin value of $\air^\Phi$ nor the convexification (see \pref{lem: trimming} in the appendix), so equality still holds.


\subsection{Informed Comparator Case}\label{sec: full-info}
In this section, we consider the fixed-comparator regime where the choice of model is independent of the comparator, that means there is a partition $\Theta$ of $\calM$, such that $\Phi=\Theta\times\Pi$. 

The complexity $|\Phi|$ is $|\Theta|\cdot|\Pi|$, which can be highly sub-optimal when $|\Theta|\ll|\Pi|$.
To overcome this limitation, we propose to split the online learning problem into two modified subgames defined as follows. 

\emph{Simultaneous learning game:} The environment mainly follows the same protocol as before, but the learner selects a meta-policy $(\pi_t^\theta)_{\theta\in\Theta}\in\Pi^{|\Theta|}$ in every round instead. Essentially, the learner is allowed to choose a dedicated policy for \emph{each} model subset $\theta\in\Theta$. However, the learner loses control over the observation policy $\pi_t$. The regret is only evaluated on the choice of $\pi_t^{\theta^\star}$, where $\theta^\star$ is the unknown ground truth decided by the environment at the beginning (see \pref{eq: decomposition}). 

\emph{Informed comparator game:} At the beginning of the game, the environment still secretly decides $\theta^\star\in\Theta$. In each round, additionally to deciding $M_t\in \theta^\star$, the environment also chooses $(\pi_t^\theta)_{\theta\in\Theta}\in\Pi^{|\Theta|}$, which is revealed to the learner before they make their decision $\pi_t$. The comparator in round $t$ is $\pi_t^{\theta^\star}$. 

These games are motivated by the following regret decomposition.
\begin{align}
    \E\left[\Reg(\pi^\star)\right] = \underbrace{\E\left[\sum_{t=1}^TV_{M_t}(\pi^\star)-V_{M_t}(\pi_t^{\theta^\star})\right]}_{ \text{Regret in \emph{Simultaneous learning game}}}+\underbrace{\E\left[\Reg(\pi^{\theta^\star}_{1:T})\right]}_{ \text{Regret in \emph{Informed comparator game}}}\label{eq: decomposition}\,.
\end{align}
In general, the loss of control over the observation policy makes the \emph{Simultaneous learning game} hard to solve. However, when the observation is independent of the observation policy, this game is strictly easier than the vanilla setting. We present a specific example at the end of the section.

Let us first show that the \emph{Informed comparator game} can be solved independently of the size of the policy space.
Denote $\pi^\Theta=(\pi^\theta)_{\theta\in\Theta}$ and define for a distribution $\nu\in\Delta(\calM)$
%\begin{align*}
%    \infoair^\Phi_{\rho,\eta}(p,\nu,\pi^\Phi) 
%    &= \E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu(\cdot| \pi^\Phi)} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^\star) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), \rho)\right] %\tag{\cw{would this be a more correct definition? }}
%\end{align*}
\begin{align*}
    \infoair^\Theta_{\rho,\eta}(p,\nu,\pi^\Theta) = \E_{\pi\sim p}\E_{\theta^\star \sim \nu} \E_{M \sim \nu(\cdot|\theta^\star)} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^{\theta^\star}) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\theta}}(\cdot|\pi, o), \rho)\right]. 
\end{align*}
The following theorem refers to the use of \pref{alg:general AIR full info}.
\begin{theorem}\label{thm: informed}
   In the informed comparator setting, there exists an algorithm with regret
   \begin{align*}
    \E\left[\Reg(\pi^{\theta^\star}_{1:T})\right] &\leq \frac{\log|\Theta|}{\eta} +  T\max_{\pi^\Theta\in\Pi^\Theta}\max_{\rho \in\Delta(\Theta)}\max_{\nu\in\Delta(\calM)}\min_{p\in\Delta(\Pi)} \infoair^\Phi_{\rho,\eta}(p,\nu, \pi^\Theta) \\
    &= \frac{\log|\Theta|}{\eta} + T\cdot \dec^{\textsc{KL}}_\eta(\bar\calM(\Theta)). 
\end{align*}
\label{thm:general-full}
\end{theorem}
Since $\bar{\calM}(\Theta)=\bar{\calM}(\Phi)$, this is the same bound as \pref{thm: general}, except that we only suffer the penalty $\log(|\Theta|)$ instead of $\log(|\Theta||\Pi|)$.

Finally, let us present a case in which the \emph{Simultaneous learning game} is efficiently solvable. We define full-information feedback such that the learner receives a candidate model $M_t^\theta$ for every $\theta\in\Theta$ after round $t$, with the guarantee that $M_t=M_t^{\theta^\star}$.
\begin{theorem}\label{thm: simultaneous full-info}
    Let $\Theta$ describe an environment over $H$-staged MDPs with action set $\calA$ and full-information reward feedback. For any $\gamma>0$ there exists an algorithm in the \emph{Simultaneous learning game} with regret bounded by $\left(\frac{\log|\calA|}{\gamma} + \gamma T\right) H$. 
\end{theorem}
In $H$-staged MDPs with state set $\calS$, the policy space we consider can be as large as $|\Pi|=|\calA|^{|\calS|}$
In cases where $\calS$ is very large, i.e. $|\calS|\gg\max\{|\calA|,|\Theta|\}$, combining \pref{thm: informed} and \pref{thm: simultaneous full-info} yield significantly better bounds than applying \pref{thm: general} directly.

\begin{algorithm}[t]
\caption{General Algorithm with Informed Comparator} \label{alg:general AIR full info}
    Let $\rho_1(\theta) = \frac{1}{|\Theta|}$ for all $\theta \in \Theta$. \\
    \For{$t=1,2,\ldots, T$}{
       Receive $\pi_t^\Theta=(\pi_t^\theta)_{\theta\in\Theta}$ from the environment. \\
       Find a distribution $p_t$ of $\pi$ and a distribution $\nu_t$ of $\calM$ that solve the saddle-point of 
       \begin{align*}
           \min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\calM)} \infoair^\Theta_{\rho_t,\eta}(p,\nu, \pi_t^\Theta)  
       \end{align*}
       Sample decision $\pi_t\sim p_t$ and observe $o_t\sim M_t(\cdot|\pi_t)$. \\
       Update $\rho_{t+1}(\theta)=\nu_{t}(\theta|\pi_t,o_t)$ for all $\theta\in\Theta$.  
       
    }
\end{algorithm}
\iffalse
\subsubsection{old draft to be removed is new version is better}
We overcome this limitation in the full-information regime. Full-information means that the learner receives a candidate model $M_t^\theta$ for every $\theta\in\Theta$ after round $t$, with the guarantee that $M_t=M_t^{\theta^\star}$. We propose a decomposition approach, where we separate the problem of identifying $\theta^\star$ from finding $\pi^\star$. At every round for every $\theta\in\Theta$, let the leaner produce a policy $\pi_t^{\theta}$ trying to obtain low regret to $\pi^\star$ if $\theta$ is the true model.

The regret decomposes into
\begin{align}
    \E\left[\Reg(\pi^\star)\right] = \E\left[\sum_{t=1}^TV_{M_t}(\pi^\star)-V_{M_t}(\pi_t^{\theta^\star})\right]+\E\left[\Reg(\pi^{\theta^\star}_{1:T})\right]\label{eq: decomposition}
\end{align}
To bound the first term, define the partition based policy as
\begin{align*}
    \pi_t^\theta(a|s) \propto \exp\left(\gamma\sum_{i=1}^{t-1}Q^{\pi_i^\theta}(s,a;M^\theta_i)\right)\,,
\end{align*}
where $Q^{\pi}(\cdot,\cdot;M)$ is the $Q$ function of policy $\pi$ under model $M$. That this policy is computable relies crucially on the availability of $M_t^{\theta}$ for all $\theta\in\Theta$.
\cw{I created in Section 2 a subsection to introduce MDP. } We have

\begin{align*}
&\E\left[\sum_{t=1}^T \left(V_{M_t}(\pi^\star) - V_{M_t}(\pi_t^{\theta^\star})\right)\right]
\\&\le \E\left[\sum_{h=1}^H\E_{s_h}^{M_t^{\theta^\star},\pi^\star} \left[\sum_{t=1}^T \sum_{a_h}\left(\pi^\star(a_h|s_h) - \pi_t^{\theta^\star}(a_h|s_h)\right)Q^{\pi_t^{\theta^\star}}(s_h, a_h; M_t^{\theta^\star})\right]\right]  \tag{\pref{lem:PDL}}
\\&\le \frac{\log|\calA|}{\gamma} + \gamma T H^3 \tag{$Q^\pi(s,a;M) \le 1$ and \pref{lem:EXP}}
\\&\le \sqrt{TH^3\log|\calA|} \,.   
\end{align*}
\cw{Should change the assumption to $Q^\pi \leq 1$ for consistency. }
It remains to bound the second term. Notice that this can be seen as a special game, in which the learner receives information about the comparator of round $t$ ahead of time.
To formalize the modified game, suppose that the environment secretly decides $\theta^\star\in\Theta$ at the beginning of the game. In each round, the environment decides $M_t\in \theta^\star$, as well as $(\pi_t^\theta)_{\theta\in\Theta}\in\Pi^{|\Theta|}$. The comparator for the regret in round $t$ is $\pi_t^{\theta^\star}$. However, the environment reveals $(\pi_t^\theta)_{\theta\in\Theta}$ to the learner before they take the decision at round $t$. 

Denote $\pi^\Theta=(\pi^\theta)_{\theta\in\Theta}$ and define
%\begin{align*}
%    \infoair^\Phi_{\rho,\eta}(p,\nu,\pi^\Phi) 
%    &= \E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu(\cdot| \pi^\Phi)} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^\star) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), \rho)\right] %\tag{\cw{would this be a more correct definition? }}
%\end{align*}
\begin{align*}
    \infoair^\Theta_{\rho,\eta}(p,\nu,\pi^\Theta) = \E_{\pi\sim p}\E_{\theta^\star \sim \nu} \E_{M \sim \nu(\cdot|\theta^\star)} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^{\theta^\star}) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\theta}}(\cdot|\pi, o), \rho)\right]. 
\end{align*}






\begin{theorem}\label{thm: informed}
   In the informed comparator setting, there exists an algorithm such that 
   \begin{align*}
    \E\left[\Reg(\pi^{\theta^\star}_{1:T})\right] &\leq \frac{\log|\Theta|}{\eta} +  T\max_{\pi^\Theta\in\Pi^\Theta}\max_{\rho \in\Delta(\Theta)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)} \infoair^\Phi_{\rho,\eta}(p,\nu, \pi^\Theta) \\
    &\leq \frac{\log|\Theta|}{\eta} + T\cdot \dec^{\textsc{KL}}_\eta(\bar\calM(\Theta)). 
\end{align*}
\label{thm:general-full}
\end{theorem}
%
Combining this with the decomposition of \pref{eq: decomposition} yields $$\E[\Reg(\pi^\star)]\leq \sqrt{TH^3\log|\calA|}+\frac{\log|\Theta|}{\eta} + T\cdot \dec^{\textsc{KL}}_\eta(\bar\calM(\Theta))\,.$$
\jz{Maybe make this a Theorem?}
\fi
\section{Model-Based RL in Adversarial MDPs with Fixed Transition}\label{sec: model-based}
In this section, we consider the setting where the reward is adversarially chosen but the transition is fixed.
We decompose the model space $\calM = \calR \times \calP$ where $\calR$ is the reward space and $\calP$ is the transition space. 

This corresponds to the partition  $\Phi_{\textsc{A}} = \left\{\phi_{{\pi^\star, P^\star}}: \pi^\star \in \Pi, P^\star \in \calP\right\}$ where \sloppy$\phi_{{\pi^\star, P^\star}} = \left\{( (P^\star, R), \pi^\star): R \in \calR\right\}$. Moreover, now $\joint_{\sA} = \left(\bigcup_{\phi\in\Phi_\textsc{A}} \phi\right) = \calM\times \Pi$. 
By definition, we have $|\Phi_A|=|\Pi|\cdot|\calP|$ and $\bar\calM(\Phi_A)=\calP\times\co(\calR)$. Note that for typical applications, $\calR$ is convex and hence $\bar\calM(\Phi_A)=\calM$.


\begin{corollary}
If the reward space $\calR$ is convex, there exists an algorithm for the hybrid problem with regret
\begin{align*}
    \E[\Reg(\pi^\star)] \leq \frac{\log\left(|\Pi||\calP|\right)}{\eta} +T\cdot\dec^{\textsc{KL}}_\eta(\calM)\,.
\end{align*} 
\end{corollary}
The proof follows directly from combining \pref{thm: general} and \pref{lem: air by dec}. Furthermore, when there is full-information reward feedback, 
\begin{corollary}
If the reward space $\calR$ is convex and the learner observe $R_t$ after round $t$, then there exists an algorithm with regret
$$
    \E[\Reg(\pi^\star)]\leq \left(\frac{\log(|\calA|)}{\gamma}+\gamma T\right)H+\frac{\log|\calP|}{\eta} + T\cdot \dec^{\textsc{KL}}_\eta(\calM)\,.$$
\end{corollary}
This is a direct corollary of \pref{thm: informed}, \pref{thm: simultaneous full-info} and \pref{lem: air by dec}.

\noindent \textbf{Example\ \  } For the model-based low-rank MDP setting with fixed transition and adversarial linear rewards studied in \cite{liu2024beating}, these corollaries directly improve the best-known regret guarantees from $T^\frac{2}{3}$ to $\sqrt{T}$ for both full-information and bandit feedback.


\iffalse

In this instance, we have
\begin{align*}
    \air^{\Phi_{\textsc{A}}}_{q,\eta}(p,\nu) = \E_{\pi\sim p}  \E_{(M, \pi^\star) \sim \nu} \E_{o\sim M(\cdot|\pi)}\left[ 
 V_M(\pi^\star) - V_M(\pi) - \frac{1}{\eta}\KL\left(  \nu_{\bo{\pi^\star, P^\star}}(\cdot | \pi, o), q \right) \right]
\end{align*}
where $q \in \Delta\left(\Pi \times \calP\right)$. From \pref{thm: general} and \pref{lem: air by dec},  if \pref{alg:general AIR} is realized by $\Phi_{\textsc{A}}$, it guarantees
\begin{align*}
    &\E\left[\Reg(\pi^\star)\right] 
    \\&\leq \frac{\log |\Phi_{\textsc{A}}|}{\eta} + T \max_{\rho \in\Delta(\Phi_{\textsc{A}})}\max_{\nu\in\Delta(\joint_{\sA})}\min_{p\in\Delta(\Pi)} \air^\Phi_{\rho,\eta}(p,\nu)
    \\&= \frac{\log|\calP||\Pi|}{\eta} + T \max_{\rho \in\Delta(\calP \times \Pi)}\max_{\nu\in\Delta(\calM \times \Pi)}\min_{p\in\Delta(\Pi)} \air^\Phi_{\rho,\eta}(p,\nu)
    \\&=  \frac{\log|\calP||\Pi|}{\eta} + T \cdot \dec_{\eta}^{\textsc{KL}}\left(\calM, \Phi_{\sA}\right)
    \\&= \frac{\log|\calP||\Pi|}{\eta} +
    \\&\,\, T \cdot \max_{\Bar{M} \in \textsc{co}(\calM)}\min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\calM \times \Pi)}   \E_{\pi\sim p} \E_{(P, \pi^\star)\sim \nu}  \E_{R \sim \nu(\cdot|P, \pi^\star)} \left[V_{M}(\pi^\star) - V_{M}(\pi) - \frac{1}{\eta}  \KL\left( M^{\nu(\cdot|P, \pi^\star)}(\cdot|\pi), \Bar{M}(\cdot|\pi) \right)\right]
    \\&\le \frac{\log|\calP||\Pi|}{\eta} + T \cdot \dec_{\eta}^{\textsc{KL}}\left(\co(\calR) \times \calP\right)
\end{align*}
\HL{Actually, showing the last inequality require reversing some steps in the proof of Theorem 2. Specifically, the complexity is equal to
\begin{align*}   
\end{align*}
}
\jz{Can we prove a general statement about the $\dec$? Something along the lines that if $\calM = \calP\times \calR$ and $\Phi=\{\{\pi^\star,P\}\times\calR|(\pi^\star,P)\in(\Pi\times\calP)\}$, then $\dec^{KL}_\eta(\calM,\Phi)\leq \dec^{KL}_\eta(\calP\times \co(\calR))$? Then we don't need to redo the same analysis.} \HL{Yeah, I am doing this now.}
\jz{I think I have a good general formulation. Working on this in a bit.} \HL{Thanks, I can write other things now.}
\fi




%\begin{proof}
%For any $\pi^{}$
%    \begin{align*}
%&\max_{\pi^\Phi\in\Pi^\Phi}\max_{\rho \in\Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)} \infoair^\Phi_{q,\eta}(p,\nu, \pi^\Phi) 
%\\&=\max_{\pi^\Phi\in\Pi^\Phi}\max_{\rho \in\Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)}\E_{\pi\sim p} \E_{\phi\sim \nu} \E_{M\sim \nu(\cdot|\phi, \pi^\phi)} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^\phi) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), q)\right]
%\\&= \max_{\pi^\Phi\in\Pi^\Phi}\max_{\rho \in\Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)}\E_{\pi\sim p} \E_{\phi\sim \nu} \E_{M\sim \nu(\cdot|\phi, \pi^\phi)} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^\phi) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), q)\right]
%\end{align*}
%\end{proof}

\iffalse
\begin{proof}
    \begin{align*}
&\max_{\pi^\Phi\in\Pi^\Phi}\max_{\rho \in\Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)} \infoair^\Phi_{q,\eta}(p,\nu, \pi^\Phi) 
\\&=\max_{\pi^\Phi\in\Pi^\Phi}\max_{\rho \in\Delta(\Phi)}\max_{\nu\in\Delta(\joint)}\min_{p\in\Delta(\Pi)}\E_{\pi\sim p} \E_{\phi\sim \nu} \E_{M\sim \nu(\cdot|\phi, \pi^\phi)} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^\phi) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), q)\right]
\\&= \max_{\pi^\Phi\in\Pi^\Phi}\min_{p\in\Delta(\Pi)}\max_{\nu\in\Delta(\joint)}\max_{\rho \in\Delta(\Phi)}\E_{\pi\sim p} \E_{\phi\sim \nu} \E_{M\sim \nu(\cdot|\phi, \pi^\phi)} \E_{o\sim M(\cdot|\pi)}\left[V_M(\pi^\phi) - V_M(\pi) - \frac{1}{\eta} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), q)\right]    \tag{\pref{lem:minmax} and joint concave over $(\nu, q)$} \\
       % &\max_{\rho \in \Delta(\Phi)}\max_{\nu\in \Delta(\Psi)} \min_{p\in\Delta(\Pi)} \E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu} \E_{o\sim M(\cdot|\pi)}\left[ V_{M}(\pi^\star) -  V_{M}(\pi) - \frac{1}{\eta} \KL\left( \nu_{\bo{\phi}}(\cdot|\pi, o), q\right) \right] 
       % \\&= \max_{\rho \in \Delta(\Phi)}\max_{\nu\in \Delta(\Psi)} \min_{p\in\Delta(\Pi)} \E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu} \E_{o\sim M(\cdot|\pi)}\left[ V_{M}(\pi^\star) -  V_{M}(\pi) - \frac{1}{\eta} \KL\left( \nu_{\bo{\phi}}(\cdot|\pi, o), \nu_{\bo{\phi}}\right) - \frac{1}{\eta}\KL\left( \nu_{\bo{\phi}}, q\right)\right]
       % \\&= \min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)}   \max_{\rho \in \Delta(\Phi)}\E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu} \E_{o\sim M(\cdot|\pi)}\left[ V_{M}(\pi^\star) -  V_{M}(\pi) - \frac{1}{\eta} \KL\left( \nu_{\bo{\phi}}(\cdot|\pi, o), \nu_{\bo{\phi}}\right) - \frac{1}{\eta}\KL\left( \nu_{\bo{\phi}}, q\right)\right]
        %\tag{\pref{lem:minmax} and joint concave over $(\nu, q)$}\\
        &= \max_{\pi^\Phi\in \Pi^\Phi}\min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)}   \E_{\pi\sim p} \E_{\phi\sim \nu}\E_{M\sim \nu(\cdot|\phi, \pi^\phi)} \E_{o\sim M(\cdot|\pi)}\left[ V_{M}(\pi^\phi) -  V_{M}(\pi) - \frac{1}{\eta} \KL\left( \nu_{\bo{\phi}}(\cdot|\pi, o), \nu_{\bo{\phi}}\right) \right] 
    \end{align*}
    Note that for fixed $\pi$, 
    \begin{align*}
        &\E_{\phi\sim \nu}\E_{M\sim \nu(\cdot|\phi, \pi^\phi)}\E_{o\sim M(\cdot|\pi)} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), \nu_{\bo{\phi}})  \\
        &= \E_{M\sim \nu(\cdot|\phi, \pi^\phi)}\E_{o\sim M(\cdot|\pi)} \KL(\nu_{\bo{\phi}}(\cdot|\pi, o), \nu_{\bo{\phi}}) \\
        &= \E_{M\sim \nu(\cdot|\phi, \pi^\phi)}\E_{o\sim M(\cdot|\pi)} \sum_{\phi'} \nu(\phi'|\pi,o) \log \frac{\nu(\phi'|\pi, o)}{\nu(\phi')}  \\
        &= \sum_{M, o, \phi'}\nu(M|\phi,\pi^\phi) M(o|\pi) \nu(\phi'|\pi, o) \log \frac{\nu(\phi'|\pi, o)}{\nu(\phi')} \\
        &= \sum_{M, o, \phi'} \nu(M,o,\phi'|\pi, \phi, \pi^\phi) \log \frac{\nu(\phi',o|\pi)}{\nu(\phi') \nu(o|\pi)} \\
        &= \sum_{M, o, \phi}\nu(\phi) \nu(M,\pi^\star,o| \pi,\phi) \log\frac{\nu(o|\pi,\phi)}{\nu(o|\pi)} \\
        &= \E_{\phi\sim \nu}\sum_{M,\pi^\star, o}  \nu(M,\pi^\star| \phi) M(o|\pi) \log\frac{\sum_{M',\pi',o} \nu(M',\pi'|\phi)  M'(o|\pi) }{\sum_{M',\pi',o} \nu(M',\pi')  M'(o|\pi) } \\
        &= \E_{\phi\sim \nu}\sum_{o}  M^{\nu(\cdot|\phi)}(o|\pi) \log\frac{M^{\nu(\cdot|\phi)}(o|\pi)}{M^\nu(o|\pi)} \\
        &= \E_{\phi\sim \nu} \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi),  M^{\nu}(\cdot|\pi) \right).  
    \end{align*}

    Thus, we can continue with 
    \begin{align*}
        &\min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)}   \E_{\pi\sim p} \E_{(M,\pi^\star)\sim \nu} \left[ V_{M}(\pi^\star) -  V_{M}(\pi) - \frac{1}{\eta} \E_{\phi\sim \nu}  \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi),  M^{\nu}(\cdot|\pi) \right)\right] \\
        %&= \min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)}   \E_{\pi\sim p} \E_{\phi\sim \nu} \E_{(M,\pi^\star)\sim \nu}  \left[ V_{M}(\pi^\star) -  V_{M}(\pi) - \frac{1}{\eta}  \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi),  M^{\nu}(\cdot|\pi) \right)\right]  \\
        &= \min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)}   \E_{\pi\sim p} \E_{\phi\sim \nu}  \left[\E_{(M,\pi^\star)\sim \nu(\cdot|\phi)}  \left[V_{M}(\pi^\star)\right] - \E_{(M,\pi^\star)\sim \nu(\cdot|\phi)}  \left[V_{M}(\pi)\right] - \frac{1}{\eta}  \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi),  M^{\nu}(\cdot|\pi) \right)\right] 
        \\&= \min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)}   \E_{\pi\sim p} \E_{\phi\sim \nu}  \E_{(M,\pi^\star)\sim \nu(\cdot|\phi)} \left[V_{M}(\pi^\star) - V_{M}(\pi) - \frac{1}{\eta}  \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi), M^{\nu}(\cdot|\pi) \right)\right]
        \\&= \min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)}  \max_{z \in \Delta(\calM)}\E_{\pi\sim p} \E_{\phi\sim \nu}  \E_{(M,\pi^\star)\sim \nu(\cdot|\phi)} \left[V_{M}(\pi^\star) - V_{M}(\pi) - \frac{1}{\eta}  \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi), M^{\nu}(\cdot|\pi) \right)\right. 
        \\& \hskip31em \left.-\frac{1}{\eta}  \KL\left( M^{\nu}(\cdot|\pi), M^{z}(\cdot|\pi) \right)\right]
        \\&=  \max_{z \in \Delta(\calM)} \max_{\nu\in \Delta(\Psi)} \min_{p\in\Delta(\Pi)}\E_{\pi\sim p} \E_{\phi\sim \nu}  \E_{(M,\pi^\star)\sim \nu(\cdot|\phi)} \left[V_{M}(\pi^\star) - V_{M}(\pi) - \frac{1}{\eta}  \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi), M^{\nu}(\cdot|\pi) \right)\right. 
        \\& \hskip31em \left.-\frac{1}{\eta}  \KL\left( M^{\nu}(\cdot|\pi), M^{z}(\cdot|\pi) \right)\right]   \tag{\pref{lem:minmax} and joint concave over $(\nu, z)$}
        \\&= \max_{z \in \Delta(\calM)} \max_{\nu\in \Delta(\Psi)} \min_{p\in\Delta(\Pi)}\E_{\pi\sim p} \E_{\phi\sim \nu}  \E_{(M,\pi^\star)\sim \nu(\cdot|\phi)} \left[V_{M}(\pi^\star) - V_{M}(\pi) - \frac{1}{\eta}  \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi), M^{z}(\cdot|\pi) \right)\right]
 \\&=  \max_{\Bar{M} \in \textsc{co}(\calM)}    
 \min_{p\in\Delta(\Pi)} \max_{\nu\in \Delta(\Psi)} \E_{\pi\sim p} \E_{\phi\sim \nu}  \E_{(M,\pi^\star)\sim \nu(\cdot|\phi)} \left[V_{M}(\pi^\star) - V_{M}(\pi) - \frac{1}{\eta}  \KL\left( M^{\nu(\cdot|\phi)}(\cdot|\pi), \Bar{M}(\cdot|\pi) \right)\right]   \tag{\pref{lem:minmax}}
 \\&= \dec_{\eta}^{\textsc{KL}}\left(\calM, \Phi\right)
\end{align*}

\end{proof}
\fi
\iffalse
\cw{comment on why the improvement is important --- the lower bound in our low-rank MDP paper}
% In this setting, the learner interact with a set of MDPs with adversarially chosen reward and fixed transition, and the reward function $R_t$ for every episode $t$ is revealed at the end of $t$. We define the partition $\Phi_{\textsc{I}} = \left\{\phi_{P^\star}: P^\star \in \calP\right\}$ where $\phi_{P^\star} = \left\{((P^\star, R), \pi^{P^\star}): R \in \mathcal{R})\right\}$. Fix partition, conditional distribution at every round? Or change partition instance in every round for each group, but do not use conditinal distribution. Looks the same.

In this setting, the learner interacts with a set of MDPs with adversarially chosen reward and fixed transition, and the reward function $R_t$ for every episode $t$ is revealed at the end of $t$. We define the partition $\Theta_{\textsc{I}} = \left\{\theta_{P^\star}: P^\star \in \calP\right\}$ where $\theta_{P^\star} = \left\{(P^\star, R): R \in \mathcal{R})\right\}$. We run \pref{alg:general AIR full info} with the revealed policy $\{\pi^P_t\}_{t=1}^T$ for any partition $\theta_P$ defined as 
\begin{align*}
    \pi_t^P(a|s) \propto \exp\left(\gamma\sum_{i=1}^{t-1}Q_{P}^{\pi_i^P}(s,a; R_i)\right)
\end{align*}
where $Q_P^\pi(\cdot,\cdot\, ;R)$ is the $Q$ function of policy $\pi$ under transition $P$ and reward $R$. We have

\begin{align*}
&\E\left[\sum_{t=1}^T \left(V_{(R_t, P^\star)}(\pi^\star) - V_{(R_t, P^\star)}(\pi_t^{P^\star})\right)\right]
\\&\le \E\left[\sum_{h=1}^H\E_{s_h \sim d_h^{\pi^\star}}\left[\sum_{t=1}^T \sum_{a_h}\left(\pi^\star(a_h|s_h) - \pi_t^{P^\star}(a_h|s_h)\right)Q_{P^\star}^{\pi_t^{P^\star}}(s_h, a_h; R_t)\right]\right]  \tag{\pref{lem:PDL}}
\\&\le \frac{\log|\calA|}{\gamma} + \gamma T H^3 \tag{$Q^\pi_P(s,a;R) \le H$ and \pref{lem:EXP}}
\\&\le \sqrt{TH^3\log|\calA|} \,.
\end{align*}
Combining this with the decomposition of \pref{eq: decomposition} and \pref{thm: informed} yields $\E[\Reg(\pi^\star)]\leq \sqrt{TH^3\log|\calA|}+\frac{\log|\Theta|}{\eta} + T\cdot \dec^{\textsc{KL}}_\eta(\bar\calM(\Theta))$.
\iffalse
\begin{align*}
&\E[\Reg(\pi^{\star})] 
\\&= \E\left[\sum_{t=1}^T \left(V_{M_t}(\pi^\star) - V_{M_t}(\pi_t)\right)\right]
\\&= \E\left[\sum_{t=1}^T \left(V_{(R_t, P^\star)}(\pi^\star) - V_{(R_t, P^\star)}(\pi_t^{P^\star})\right)\right] +   \E\left[\sum_{t=1}^T \left(V_{M_t}(\pi_t^{P^\star}) - V_{M_t}(\pi_t)\right)\right] 
\\&\le \E\left[\sum_{h=1}^H\E_{s_h \sim d_h^{\pi^\star}}\left[\sum_{t=1}^T \sum_{a_h}\left(\pi^\star(a_h|s_h) - \pi_t^{P^\star}(a_h|s_h)\right)Q_{P^\star}^{\pi_t^{P^\star}}(s_h, a_h; R_t)\right]\right] +  T\cdot\dec^{\textsc{KL}}_\eta(\bar\calM(\Phi_{\textsc{I}})) + \frac{\log|\calP|}{\eta} \tag{\pref{lem:PDL} and \pref{thm:general-full}}
\\&\le \frac{\log|\calA|}{\gamma} + \gamma T H^3 +  T\cdot\dec^{\textsc{KL}}_\eta(\co(\calR) \times \calP) + \frac{\log|\calP|}{\eta} \tag{$Q^\pi_P(s,a;R) \le H$ and \pref{lem:EXP}}
\\&\le \sqrt{TH^3\log|\calA|} + T\cdot\dec^{\textsc{KL}}_\eta(\co(\calR) \times \calP) + \frac{\log|\calP|}{\eta}
\end{align*}
\fi

\fi
\section{Model-Free RL in Stochastic MDPs}\label{sec: stochastic model-free}
In this section, we apply the general framework to model-free RL in stochastic MDPs. In model-free RL with value function approximation, the learner is provided with a function set $\calF$ that contains possible value functions of the world. We define $\Phi$ as $\Phi=\{\phi_f: f\in\calF\}$ where $\phi_f=\{(M, \pi_M):~ M \text{\ induces\ } f\}$.  With \pref{alg:general AIR}, we are able to obtain the regret bound 
\begin{align*}
    \E\left[\Reg(\pi^\star)\right] \leq \frac{\log|\calF|}{\eta} + T\cdot \dec^{\textsc{KL}}_\eta (\bar\calM(\Phi)). 
\end{align*}
In fact, model-free learning in stochastic MDPs is easier than the $\Phi$-restricted environment defined in \pref{def: restricted env}, as the adversary cannot modify the underlying model in every round. However, the general framework offers a means to reduce estimation complexity from $\log|\calM|$ to $\log|\calF|$. If we can show that aggregation does not significantly increase decision complexity, this approach could provide a viable path to improving the regret bound.

Existing literature has identified general classes where model-free RL allows for $\log|\calF|$ estimation complexity and bounded decision complexity. One of the most general classes is the \emph{bilinear class} \citep{du2021bilinear}, where the \emph{bilinear rank} serves as the decision complexity. 
While we are currently unable to relate  $\dec^{\textsc{KL}}_\eta (\bar\calM(\Phi))$ to bilinear rank in full generality, we identify special cases where such a connection holds.  Specifically, in the linear $Q^\star/V^\star$ setting---a subclass of bilinear class---we show that $\dec^{\textsc{KL}}_\eta (\bar\calM(\Phi))$ can be upper bounded by the bilinear rank. We formally define the linear $Q^\star/V^\star$ setting as the following: 
\begin{definition}
    A class of MDPs satisfies linear $Q^\star/V^\star$ if there are known feature vectors $\varphi: \calS\times \calA\to \mathbb{R}^d$ and $\psi: \calS\to \mathbb{R}^d$ and known function class $\calF=\{(\theta,w)\}\subset \mathbb{R}^{d+d}$ such that $Q^\star(s,a) = \varphi(s,a)^\top \theta^\star$ and $V^\star(s) = \psi(s)^\top w^\star$ for some $(\theta^\star, w^\star)\in\calF$. 
\end{definition}
We assume $|\calF|$ is finite for simplicity. It is straightforward to extend the result to the case where $\calF$ lies in a bounded region in $\mathbb{R}^{d+d}$. With this, we let $\Phi = \{\phi_{\theta,w}~:~(\theta,w) \in \calF\}$, where 
\begin{align*}
     \phi_{\theta,w} = \{(M, \pi_M)~:~ Q^\star(s,a; M) = \varphi(s,a)^\top \theta, \quad V^\star(s; M) = \psi(s)^\top w\}. 
\end{align*}
Then we have the following bound for $\dec^{\textsc{KL}}_\eta(\bar\calM(\Phi))$. 
\begin{theorem}\label{thm: linear QV}
    In the linear $Q^\star/V^\star$ setting, $\dec^{\textsc{KL}}_\eta(\bar\calM(\Phi))\leq 4\eta dH^2$. 
\end{theorem}
Establishing similar results for general bilinear classes or other existing complexity measures for model-free RL is left as future work. We provide more discussions in \pref{sec: discussion}. 



\section{Model-Free RL in Adversarial MDPs with Fixed Transition and Full Information}\label{sec: model-free adversarial} 

% Use $f_{P}^\pi$ to define $Q$ function of transition $P$ and policy $\pi$.


% We assume getting access to a value space $\mathcal{Q}$ contains a series of function  $f: \calS \times \calA \times \calR \rightarrow \mathbb{R}$ such that $f_{P^\star}^\pi \in \mathcal{Q}$ for any $\pi$.  We also define separate $\mathcal{Q}^\pi$ such that $f^\pi_{P^\star} \in \mathcal{Q}^\pi$.  $\calQ$ can be regarded as $\cup_{\pi \in \Pi} \mathcal{Q}^\pi$. The following algorithm consider group $\Phi_{\textsc{MF}} = \left\{\phi_{f, \pi}: f \in \mathcal{Q}, \pi \in \Pi\right\}$, which is large but serve as a good start for any extension. Working on $f \in \calQ^\pi$ and $\pi$ is also feasible, but seems that the order is the same ($\log(|\Pi|)$ vs $\log(|\Pi|^2)$). Will write more.

% Define optimistic DEC based on general grouping $\Phi$ and general divergence $D$:
% \begin{align*}
%     \odec^{D}(\calM, \rho, \Phi) = \min_{p \in \Delta(\Pi)}\max_{(P,R) \in \calM}\E_{\pi' \sim p}\E_{\phi \sim \rho}\left[f^{\phi}(\pi^\phi, R) - V_{P,R}(\pi)  - \gamma D^{\pi'}(\phi||P, R)\right]
% \end{align*}
% In our case $\phi(R) = f(\pi;R)$ for $\phi_{(f,\pi)}$. In stochastic setting, $\phi(R) = f(\pi^f, R)$ for $\phi_{f}$. \HL{This is not a unified framework. In adversarial setting, $f$ is a  ternary function with additional $R$, provided as full information. In stochastic setting, $f$ is binary function. $f(\pi^f, R)$ is ill-defined. }

% \subsection{A general definition for bilinear and optimistic DEC}


\iffalse
\cw{ \\
\textbf{Stochastic setting}:\\ 
Assume there is a function class $\calF$. Each function $f\in\calF$ defines $f(s,a)$ and $\pi_f(s) = \argmax_a f(s,a)$. We say a model $M$ induces (or is consistent with) $f$ if $Q_M^{\pi_M}(s,a) = f(s,a)$. With abuse of notation, we write $f(\pi_f)  = f(s_0, \pi_f(s_0))= \max_a f(s_0, a)$ to indicate that it is the value of policy $\pi_f$ predicted by function $f$. \\
\ \\
Bilinear assumption: 
\begin{align*}
    &|f(\pi_f) - V_M(\pi_f)| \leq |\inner{X(\pi_f, M), W(f, M)}| \\
    &|\inner{X(\pi_f, M), W(f', M)}| \leq \E^{\pi_f, M}[\ellest(f', o)]. \tag{$o=(s,a,r,s')$}
\end{align*}\\
\ \\
Partition: \\
$\Phi=\left\{ \phi_f: f\in \calF\right\}$ where $\phi_f=\{(M, \pi_M):  M \text{\ induces\ } f\}$. Since every partition corresponds to a single $f$ and policy $\pi_f$, for any partition $\phi\in\Phi$, we denote its corresponding $f$ as $f^\phi$, and $\pi_f$ as $\pi^\phi$. \\
\ \\
\ \\
\textbf{Adversarial setting}: \\
Assume for each policy $\pi$ there is a function class $\calF^\pi$. Each function $f\in\calF^\pi$ defines $f(s,a,R)$. We say a transition $P$ induces (or is consistent with) $f\in\calF^\pi$ if $Q_P^{\pi}(s,a;R) = f(s,a; R)$. With abuse of notation, for any $\pi$ and $f\in\calF^\pi$, we write $f(\pi; R) = f(s_0, \pi(s_0), R)$ to indicate that it is the value of policy $\pi$ predicted by function $f$ under reward $R$.  \\
\ \\
Bilinear assumption: for any $\pi$, any  $f\in\calF^\pi$, and any $f'\in \calF\triangleq \cup_{\pi'\in\Pi} \calF^{\pi'}$, 
\begin{align*}
    &|f(\pi, R) - V_{P,R}(\pi)| \leq |\inner{X(\pi; P), W(f, R; P)}| \\
    &|\inner{X(\pi, P), W(f', R, P)}| \leq \E^{\pi, P}[\ellest(f', R, o)]. \tag{$o=(s,a,s')$}
\end{align*}\\
\ \\
Partition: \\
$\Phi=\left\{ \phi_{\pi, f}: \pi\in\Pi, f\in \calF^\pi\right\}$, where $\phi_{\pi,f}=\{(M, \pi):  M \text{\ induces\ } 
 f\}$ is defined for any 
$f\in\calF^\pi$. Since every partition corresponds to a single $f$ and policy $\pi$, for any partition $\phi\in\Phi$, we denote its corresponding $f$ as $f^\phi$, and $\pi$ as $\pi^\phi$. \\
}
\fi

% \begin{assumption}[Realization for general paratition]
%     Suppose for any partition $\phi \in \Phi$, there exists a function $f^\phi: \calS \times \calA \times \calM \rightarrow \mathbb{R}$ and a policy $\pi^{\phi}: \calS \rightarrow \Delta(\calA)$ such that for every $(M, \pi^\star) \in \phi^\star$, we have $V_{M}(\pi^\star) = \E_{a \sim \pi^{\phi^\star}(\cdot|s_1)}\left[f^{\phi^\star}(s_1, a,M)\right] = f^{\phi^\star}(\pi^{\phi^\star},M)$.
% \end{assumption}

In this section, we study model-free RL in the hybrid setting with full-information reward feedback. The algorithm used in this section deviates from the general algorithm in \pref{alg:general AIR}, but it is still based on the idea of learning over partitions of $\calM\times \Pi$.  To the best of our knowledge, there is no general value function approximation scheme defined for the hybrid setting in the literature. Therefore, we propose the following framework. 

\begin{definition}[Value function approximation for the hybrid setting]\label{def: func approx}
    Assume for each policy $\pi\in\Pi$, there is a function class $\calF^\pi$. Each function $f\in\calF^\pi$ is a ternary function mapping $\calS \times \calA \times \calR\to \mathbb{R}$. We say a transition $P$ induces $f\in\calF^\pi$ if $Q^{\pi}(s,a;(P,R)) = f(s,a, R)$\footnote{Similar to \cite{foster2024model}, for the sake of simplicity, we adopt a less general definition where ``functions'' only represent $Q$-functions. The results can be readily extended to more general settings where functions may additionally represent e.g. $V$-functions. } for any $s, a, R$. %, where $Q_P^{\pi}(\cdot, \cdot; P,R)$ is the $Q$-function for model $(P,R)$ and policy $\pi$. 
    With abuse of notation, for any $\pi\in\Pi$ and $f\in\calF^\pi$, we write $f(\pi; R) = f(s_0, \pi(s_0), R)$, indicating that it is the value of policy $\pi$ predicted by function $f$ under reward $R$. Define $\calF =  \cup_{\pi\in\Pi} \calF^{\pi}$. We assume $f(\pi, R) \in [0,1]$ for any $f, \pi, R$, which matches the assumption on value function $V_M$.
\end{definition}

Compared to standard value function approximation for the stochastic setting (e.g., \cite{du2021bilinear}), the framework in \pref{def: func approx} is different in two ways: 
1) In the usual stochastic setting, it is a full model $M=(P,R)$ that underlies (or induces) a function. In other words, given a model $(P,R)$, there is a unique function $f$ corresponding to it. However, in the hybrid setting, it is a transition $P$ that induces a function. The reward $R$, on the other hand, is an input to the function. This is natural because reward functions change over time and no single function can capture it. 
2) In the usual stochastic setting, the functions only represent the optimal $Q$-functions, while in the hybrid setting, the functions represent $Q$-functions of all policies.  This is also natural because even under a fixed transition, every policy has the possibility to become the optimal policy when the reward can vary. 

%Although model-free RL algorithms are typically designed for static environments, in this section we demonstrate how they can be adapted to the hybrid setting, thus removing the dependence on the entire model space appearing in \pref{sec: model-based}. 

%To capture possible value functions, we assume for each policy $\pi$, there is a function class $\calF^\pi$. Each function $f\in\calF^\pi$ is a ternary function mapping $\calS \times \calA \times \calR$ to $\mathbb{R}$. We say a transition $P$ induces (or is consistent with) $f\in\calF^\pi$ if $Q_P^{\pi}(s,a;R) = f(s,a, R)$ for any $s,a,R$, where $Q_P^{\pi}(\cdot, \cdot, R)$ is the $Q$-function for model $(P,R)$ and policy $\pi$. With abuse of notation, for any $\pi$ and $f\in\calF^\pi$, we write $f(\pi; R) = f(s_0, \pi(s_0), R)$ to indicate that it is the value of policy $\pi$ predicted by function $f$ under reward $R$. Define $\calF =  \cup_{\pi\in\Pi} \calF^{\pi}$. We assume $|f(\pi, R)| \le 1$ for any $f, \pi, R$, which matches the assumption on value function $V_M$.


In the model-free hybrid setting where the learner is equipped with a function class $\calF$ described in \pref{def: func approx}, we make the partition be  
$\Phi=\left\{ \phi_{\pi, f}: \pi\in\Pi, f\in \calF^\pi\right\}$, where $\phi_{\pi,f}=\{(P, R, \pi): P \text{\ induces\ } f, \,  R \in \mathcal{R}\}$ for any $\pi\in\Pi$ and 
$f\in\calF^\pi$. 
Since every partition corresponds to a single $f$ and policy $\pi$, for any partition $\phi\in\Phi$, we denote its corresponding $f$ as $f^\phi$, and $\pi$ as $\pi^\phi$. 

Note that, unlike the applications in previous sections, here we have a \emph{policy-dependent} partition over models. This is why, in \pref{sec: general frame}, we construct $\Phi$ based on the joint model and policy class rather than solely on the model class.

Next, we further make structural assumptions on the model and function classes. Specifically, we provide a natural extension of the bilinear class \citep{du2021bilinear} to the hybrid setting described in \pref{def: func approx}. 

% \begin{definition}[Bilinear Class for Adversarial Settings]
% \label{def:adv bilinear}
%     Given the definition of $\Phi, f^\phi, \pi^\phi$ above, there exists features $W(\phi, R; P), X(\phi; P) \in \mathbb{R}^d$ such that for any $\phi \in \Phi$, and $(P,R) \in \calP \times \calR$,
%     \begin{align*}
%         \left|f^\phi(\pi^\phi; R) - V_{P, R}(\pi^\phi)\right| \leq \left|\langle X(\phi; P), W(\phi, R; P)\rangle\right|
%     \end{align*}
%     and there exists a discrepancy function $\ellest(\phi, P, R; o)$ (here $o = \{(s_h, a_h)\}_{h=1}^H$) such that 
%     \begin{align*}
%         \left|\langle X(\phi; P), W(\phi' , R; P) \rangle\right| = \left|\E^{\pi^\phi, P} \left[\ellest(\phi', P,  R; o)\right]\right|
%     \end{align*}
% \end{definition}
\begin{definition}[Bilinear class for the hybrid setting]
\label{def:adv bilinear}
A hybrid function class $\calF$ (defined in \pref{def: func approx}) induced by a model class $\calM=\calP\times\calR$ has bilinear rank $d$ if there exists functions $X_h: \Pi \times \mathcal{P} \rightarrow \mathbb{R}^d$ and  $W_h: \mathcal{F} \times \mathcal{R} \times \mathcal{P} \rightarrow \mathbb{R}^d$ for all $h\in[H]$ such that for any $\pi\in\Pi$, $f \in \calF^\pi$, $R \in \mathcal{R}$, and $ P \in \mathcal{P}$, we have
    \begin{align*}
        \left|f(\pi; R) - V_{P, R}(\pi)\right| \leq \sum_{h=1}^H \left|\langle X_h(\pi; P), W_h(f, R; P)\rangle\right|.
    \end{align*}
    Moreover, there exists an estimation policy mapping function $\esttt: \Pi \rightarrow \Pi$, and for every $h \in [H]$, there exists a discrepancy function $\ellest_h: \mathcal{F} \times \calO \times \calR  \rightarrow \mathbb{R}$ such that for any $\pi\in\Pi, f \in \calF^\pi$, and $f'\in \calF$, 
    \begin{align*}
        \left|\langle X_h(\pi; P), W_h(f', R; P) \rangle\right| = \left|\E^{\pi \,\circ_h\, \esttt(\pi),\, P} \left[\ellest_h(f', o_h, R)\right]\right|
    \end{align*}
    where $\mathcal{O}$ is the space of per-step observation $(s,a,s')$, and $o_h = (s_h,a_h,s_{h+1})$. $\E^{\pi, P}[\cdot]$ denotes the expectation over the occupancy measure generated by policy $\pi$ and transition $P$. Let $\pi \circ_h \esttt(\pi)$ denote playing $\pi$ for the first $h-1$ steps and play policy $\esttt(\pi)$\, at the $h$-th step.
\end{definition}
\pref{def:adv bilinear} encompasses numerous interesting instances, such as low-rank MDPs and low-occupancy MDPs \citep{du2021bilinear}. Their learnability in the hybrid setting remains largely unexplored. Further discussion on these specific instances can be found in \pref{app:model-free full proof}.



%\begin{remark}
%Standard bilinear class \citep{du2021bilinear, foster2021statistical, foster2024model} defines step-level functions $X_h$ and $W_h$ for every $h \in [H]$, whereas our definition only requires a single $X$ and $W$ for simplicity. These can be regraded as the concatenation of the $X_h$ and $W_h$ from previous works across all $h \in [H]$. The only difference is that our summation over $H$ is placed inside the absolute value, while in other definitions it occurs outside. We see no disadvantage in our definition, and it can still recover all cases highlighted in \cite{du2021bilinear}.
%\end{remark}


% This definition recovers model-based, model-free and the adversarial bilinear class we have defined

% \textbf{Discussion on this assumption:} Analog to Bellman error decomposition



% \HL{Maybe a more general definition for bilinear:

% Suppose we have group based on $\phi$, and $f^\phi, \pi^{\phi}$ are well-defined for each partition $\phi$. Then a generalization of bilinear could be:

% There exists $W(\phi, R; P)$ and $X(\pi; P)$ such that for any $\phi$, and any $P, R$, we have 
%     \begin{align*}
%         \left|\E^{{\pi^\phi}, P} \left[f^{\phi}(s_h,a_h,R) - R(s_h,a_h) - \E_{a_{h+1} \sim \pi^{\phi}(\cdot|s_{h+1})}\left[f^{\phi}(s_{h+1}, a_{h+1}, R)\right]\right]\right| \leq \left|\langle X_h(\pi^{\phi}; P), W_h(\phi, R; P)\rangle\right|
%     \end{align*}
%     and there exists a discrepancy function $\ellest_h(\phi, R; o_h)$ such that 
%     \begin{align*}
%         \left|\langle X_h(\pi^\phi; P), W_h(\phi' , R; P) \rangle\right| = \left|\E^{\pi^\phi, P} \left[\ellest_h(\phi', R; o_h)\right]\right|
%     \end{align*}
% }

% \cw{Actually, is the first Bellman error needed? Is it sufficient to simply assume
% \begin{align*}
%     |f^\phi(\pi^\phi; R) - V_{P,R}(\pi^\phi)|  \tag{I'm not 100\% sure about this}
% \end{align*}
% is bounded by right-hand side? (I write $f$ instead of $V$ to make it look different from the general $V$ function) 

% \HL{I feel they are the same?}

% Also, the $s,a,s'$ in $\ellest$ may potentially be written as just $o$. 

% Also, do we need the $h$ thing in $X_h, W_h$? Can we just concatenate $X_h$ as a single big vector?  

% }


We will show that MDPs that satisfy \pref{def:adv bilinear} is learnable in the hybrid setting with model-free guarantees. To emphasize more on the idea of partition, we use the partition notation to describe our \pref{alg:MAF}. For partition $\phi = \phi_{\pi, f}$ where $\pi \in \Pi$ and $f \in \calF^{\pi}$, we write $X_h(\phi; P) = X_h(\pi; P)$, $W_h(\phi; P) = W_h(f, R; P)$, $\ellest_h(\phi, o_h, R) 
 = \ellest_h(f, o_h, R)$, and $\pi_{est}^{\phi} = \esttt(\pi)$. %We define $\ellest(\phi, o, R) = \sum_{h=1}^H\ellest_h(\phi, o_h, R)$ where $o = (o_h)_{h=1}^H$ is the whole trajectory. 
We define the bilinear divergence
\begin{align*}
    D_{bi}^{\pi}(\phi||P, R) = \sum_{h=1}^H \left(\E^{\pi, P}\left[\ellest_h(\phi, o_h, R)\right]\right)^2.
\end{align*}
% When $P$ is unknown but is fixed, we can only get acces to $\tau$ i.i.d samples $\{o^i\}_{i=1}^{\tau}$ where $o^i = \{ (s_h^i, a_h^i)\}_{h=1}^H$ such that $s_h^i \sim d_{P, h}^{\pi'}$, $a_h^i \sim \pi'(\cdot|s_h)$, and $s_{h+1}^i \sim P(\cdot|s_h^i, a_h^i)$ for any $h$ and $i$. We define the estimated divergence as
% \begin{align*}
%      \widehat{D}_{bi}^{\pi'}(\phi||P, R) = \sum_{h=1}^H\left(\frac{1}{\tau}\sum_{i \in [\tau]}\ellest_h(\phi, o^i, R)\right)^2
% \end{align*}


\begin{algorithm}[t]
    \caption{Model-free learning for the hybrid setting with full information}
    \label{alg:MAF}
    \textbf{Input:} Partition set $\Phi =  \left\{\phi_{\pi, f}: \pi \in \Pi, f \in \calF^\pi\right\}$,  $p_1(\phi) = \frac{1}{|\Phi|},\, \forall \phi \in \Phi$, 
   epoch length $\tau$, and learning rate $\gamma$, $\eta$. \\
    \For{\textup{\textbf{epoch}} $k=1, 2, \ldots, \frac{T}{\tau}$}{
         Sample $\pi_k  \sim p_k$. \\
         \For{$i=1,\ldots, \tau$}{
         Execute $\pi_k$, obtain trajectory $(o_{k,1}^i, \ldots, o_{k,H}^i)$ where $o_{k,h}^i = (s_{k,h}^i, a_{k,h}^i, s_{k,h+1}^i)$, and observe the full reward function $R_k^i$. 
         }
         
         Define $R_k = \frac{1}{\tau}\sum_{i \in [\tau]} R_k^i$ as the average reward in the $k$-th epoch.
        
         Compute $\rho_{k+1} \in \Delta(\Phi)$ as 
         \begin{align}
             \rho_{k+1}(\phi) \propto \rho_{k}(\phi) \exp\left(\gamma \eta f^{\phi}(\pi^{\phi}; R_k) -  \gamma \sum_{h=1}^H \left(\frac{1}{\tau} \sum_{i \in [\tau]}\ellest_h(\phi, o_{k,h}^i, R_k)\right)^2\right).
             \label{eq:opt posterior}
         \end{align}
        
        % Define $p_{k+1}(\pi) = \sum_{f \in \mathcal{Q}} \rho_{k+1}(f, \pi)$. Or
        
        Solve the following minimax optimization for $p_{k+1}$
        \begin{align}
            p_{k+1} = \min_{p \in \Delta(\Pi)}\max_{(P,R) \in \calM} \E_{\pi' \sim p}\E_{\phi \sim \rho_{k+1}}\left[f^{\phi}(\pi^{\phi}; R) -V_{(P,R)}(\pi') - \frac{1}{8\eta } D_{bi}^{\pi'}\left(\phi||P, R\right)\right].
            \label{eq:optimis minmax}
        \end{align}
        }
\end{algorithm}


For this setting, we devise \pref{alg:MAF}, which is inspired by \cite{foster2024model} who studied the problem in the stochastic setting. The episodes are divided into epochs. At the beginning of each epoch, a reference distribution $\rho \in \Delta(\Phi)$ is selected based on optimistic posterior sampling \citep{zhang2022feel} with estimated bilinear divergence (\pref{eq:opt posterior}). Subsequently, the algorithm computes a behavior policy distribution $p_t$ through a minimax optimization \pref{eq:optimis minmax} analogous to the DEC objective defined in \pref{eq:DEC}. However, unlike the standard DEC that uses the optimal value $V_M(\pi_M)$ as the comparator where $M$ is the worst-case model, \pref{eq:optimis minmax} adopts $f^\phi(\pi^\phi, R)$ as the comparator, where $\phi$ is drawn from the optimistic posterior determined in \pref{eq:opt posterior}. As in \cite{foster2024model}, such change of comparator enables the effective use of optimism to derive improved model-free guarantee. %This approach is introduced by \cite{foster2024model}, and they also define the corresponding optimistic version of DEC for stochastic settings. In \pref{app:model-free full proof}, we further extend this idea by presenting a generalized version of the optimistic DEC in the hybrid setting. 
With the policy distribution obtained from \pref{eq:optimis minmax}, a policy is sampled from it and executed throughout the entire epoch to gather data for the next iteration. The guarantee of \pref{alg:MAF} is presented in \pref{thm:mf-f}. In \pref{app: odec}, besides proving \pref{thm: bilinear result}, we introduce the optimistic DEC notion, which is an extension from \cite{foster2024model} to the hybrid setting. The result in \pref{thm:mf-f} can thus also be generalized to learnable classes under optimistic DEC. 


% Define $f^\pi_{\star} = Q^\pi_{P^\star} \in \mathcal{Q}$. For simplicity, let $f^\star = Q^{\pi^\star}_{P^\star} $.

\begin{theorem}\label{thm: bilinear result}
For the hybrid bilinear class defined in \pref{def:adv bilinear}, \pref{alg:MAF} ensures with probability at least $1-\delta$:
\begin{itemize}
    \setlength{\itemsep}{0pt} % Adjust item spacing
    \setlength{\parskip}{0pt} 
    \setlength{\topsep}{0pt}
    \item If for all $\pi\in\Pi$, $\esttt(\pi) = \pi$, then $\Reg \le O\big(\sqrt{d\log\left(|\calF||\Pi|/\delta\right)}T^{\frac{3}{4}}\big)$.
    \item If there exists $\pi\in\Pi$ such that $\esttt(\pi) \neq \pi$, then $\Reg \le \order\big(\left(d\log\left(|\calF||\Pi|/\delta\right)\right)^{\frac{1}{3}}T^{\frac{5}{6}}\big)$.
\end{itemize}
\label{thm:mf-f}
\end{theorem}


\section{Discussions and Future Work}\label{sec: discussion}
In this work, we showed that the general framework developed in \pref{sec: general frame} is able to characterize the statistical complexity of hybrid MDPs with fixed transitions and adversarial rewards, and leads to near-optimal regret under model-based learning (\pref{sec: model-based}). 

The framework also extends to model-free value learning, as shown in \pref{sec: stochastic model-free} and \pref{sec: model-free adversarial}. However, additional challenges arise as it does not fully recover the result for the general bilinear class in the stochastic regime, and requires modifying the objective to an alternative DEC formulation in the hybrid regime. The latter poses a barrier to extending the framework to the model-free bandit feedback case. One future direction would be to more deeply investigate the role of the divergence measure in $\air^\Phi$ and see if there are better alternatives for model-free value learning. 

We believe that the framework has the potential to characterize the complexity of more settings that lie between model learning and policy learning.
Another avenue for future work is leveraging the framework we provide to derive efficient algorithms that do not have to maintain memory expensive priors or solve computationally complex saddle-point problems.


%\acks{We thank a bunch of people and funding agency.}

\bibliography{ref}

\appendix

% \crefalias{section}{appendix} % uncomment if you are using cleveref

\clearpage

\input{appendix-overview}
\input{appendix-stochastic}
\input{appendix-adversarial}
\input{appendix-support-lemma}




%\input{attempt}
\end{document}
