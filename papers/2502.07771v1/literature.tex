\section{Literature Review}
\begin{comment}
\subsection{Bias Literature (Including Mitigation Strategies)}

Recent research has consistently demonstrated the presence of various biases in the outputs of LLMs. In particular, multiple studies have shown that prompting LLMs with names associated with different demographic groups yields systematically disparate outcomes. For instance, queries about identical products or financial opportunities often favor names associated with White individuals over those linked to Black individuals \citet{haim2024whatsnameauditinglarge}, \citet{seshadri2025doesgiantnumberpile}, \citet{salinas2023unequal}. These biases are not limited to race nor quantitative outcomes, as research has found female-associated names tend to receive more simplified outputs than male-associated names \citet{eloundou2024firstpersonfairnesschatbots}.


\begin{enumerate}


    \item \citet{eloundou2024firstpersonfairnesschatbots} One should be interested in biases in LLMs produced by names because of how these models are used in real-world applications (resume screeing, credit-scoring). Female-associtated names receive simpler language responses more often than male-associated names. Name-based biases in name-sensititve chatbots.

    \item \citet{ganguli2023capacity} language models trained with RLHF have the capability to "morally self-correct" to avoid producing harmful outputs if instructed to do so. One hypothesis is that models learn complex normative concepts of harm like stereotyping, bias, and discrimination.

    \item \citet{borah2024towards} investigates the presence of implicit gender biases in multi-agent llm interactions and proposes two strategies to mitigate these biases. Create dataset of scenarios where biases arise, then develop metric to assess. Then do self-reflection with in-context examples, and supervised fine-tuning.


    \item \citet{haim2024whatsnameauditinglarge} Prompting the models for advice when using names as proxies for race leads to showing that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Systemic bias across models.
    

    \item \citet{manvi2024largelanguagemodelsgeographically} LLMs biased against locations with lower socioeconomic conditions (most of Africa) on a variety of sensitive topics such as attractiveness, morality, and intelligence. Introduce a bias score to quantify it using Mean Absolute Deviation of Ratings. All LLMs are biased to some degree.

    \item \citet{bai2024measuringimplicitbiasexplicitly} introduce a new measure (prompt-based method) for revealing implicit bias (IATs), and a strategy to detect subtle discrimination in decision-making tasks. Pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes.
    
\end{enumerate}


\subsection{Model Pruning and Localization Literature}

\begin{enumerate}

    \item \citet{sun2024simpleeffectivepruningapproach} Introduce WandA (Pruning by weights and activations) designed to induce sparsity in pretrained LLMs with the goal of droping a subset of network weights while preserving performance. Prune weights with smallest magnitudes multiplied by their corresponding input activations, on a per-output basis.No re-training or weight update, use pruned model as is. 

    \item \citet{wei2024assessingbrittlenesssafetyalignment} explore brittlness of safety alignment by using WandA pruning. Develop methods to identify critical regions vital for safety guardrails and disentangle from utility-relevant regions at neuron level. Removing regions compromises safety while only mildly impacting utility.
    
    
    \item \citet{chang2024localizationmethodsactuallylocalize} Propose two complementary benchmarks that evaluate how successful localization methods are and to pinpoint LLM components responsible for memorized data. Pruning-based methods perform the bests, but leaves the open question of whether perfect localization is even possible.

    
    \item \citet{wu2024retrievalheadmechanisticallyexplains} 

    \item \citet{paulo2024automaticallyinterpretingmillionsfeatures} Build a pipeline to generate and evaluate natural language interpretations for SAE latents using LLMs. A scalable way of interpreting the latents of SAEs and assessing the quality of those interpretations by using LLMs.
    
    \item \citet{chavhan2024memorizedimagesdiffusionmodels} Contrasting neuron activations of a set of memorized and non-memorized prompts. Experiments reveal that many different sets of memorized prompts significantly activate a common subspace in the model. Thus, memorization in diffusion models lies in a special subspace. They edit pre-trained models, by mitigating memorization with straightforward pruning of weights in specialized subspaces.
    
    \item \citet{templeton2024scaling} SAEs produce interpretable features for large models. Resulting features are highly abstract: multilingual, multimodal, generalizing between concrete and abstract references. Features can be used to steer large models (influence on behavior). They observe features related to broad range of safety concerns including bias.

\end{enumerate}

\subsubsection{Model Pruning as a Bias Mitigation Strategy}

\begin{enumerate}
    \item  \citet{zayed2023FairnessAwarePruning} investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. Prune attention heads that negatively impact fairness (large discrepancy in toxicity levels across groups means bias), while retaining heads critical for performance (that is, language capabilities). Checked how other social biases are affected when gender bias is reduced (religion, race, sexual orientation, nationality).

    \item \citet{durmus2024steering} investigate how feature steering (using SAEs) creates safer model outputs. within a certain range, one can steer the model without damaging other model capabilities, but past a certain point, it comes at the cost of decreasing model capabilities. Off-target effects (features related to multiple biases). There is a neutrality feature that decreases social biases on nine social dimensions without impacting capabilities too much. Performance are MMLU and PubMedQA, for social biases they used BBQ dataset plus a subset of model-written evals dataset (subjective multiple choice questions on abortion and immigration). Underlying correlations between concepts represented by the features (this is a similar conclusion to our paper's as generalizing is complex due to intertwined are the concepts in the network).

   \item  \citet{yang2024mitigatingbiasesinstructionfollowinglanguage} they do bias neuron pruning without losing the model's task performance and existing knowledge. They argue they achieved generalizability since it shows robsutness under various instructions and datasets (they only eliminated a few neurons, three). BBQ for bias (social bias QA), NLU for performance. They found increases in the task performance of language models by mitigating biases under instruction-following settings. Only  afew bias neurons affect language models to infer biased outputs.

   \item \citet{adiga2024attention} localize bias to specific layers of the LLM by analyzing attention scores and reduce bias by scaling attention in these biased layers.  Bias with BBQ, crows-pairs, winogender, creating bias ratio (how strongly the model prefers one entity over the other). Conclusion is that bias is concentrated in the later layers, typically around the last third (similar to our findings). No compromise of downstream performance (perplexity).

\end{enumerate}

\subsection{AI Regulation Literature}

\begin{enumerate}
    \item \citet{ferrara2023should} delves into distinctive challenges and risks associated with biases. Ethical implications arising from unintended consequences of biased model outputs. Tackles whether biases are inevitable, continuous monitoring and evaluation, making adjustments over time. Developers must commit to ongoing process of evaluating, refining and improving AI models to address biases.
\end{enumerate}

    % Closest to our work are \citet{zayed2023FairnessAwarePruning}, \citet{durmus2024steering}, \citet{yang2024mitigatingbiasesinstructionfollowinglanguage}, and \citet{adiga2024attention}, which examine structured pruning or interpretability methods to steer models to be unbiased. [TODO: a key question then is how is the work different aside from different data?]

    % \item Fairness-Aware Structured Pruning in Transformers
\begin{enumerate}
    \item \textbf{Summary:} In this paper, \cite{zayed2023FairnessAwarePruning} investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. They also propose a novel method to prune the attention heads that impact fairness while retaining performance heads (language modeling capabilities). They reduced gender bias in LLMs of different sizes. They also examined how their method applied to gender bias impacts other social biases such as religion, race, sexual orientation, and nationality. They found a positive correlation between biases.

    \item \textbf{Models tested:} DistilGPT-2, GPT-2, GPT-Neo, GPT-J, Llama-2

    \item \textbf{Methodology:}
    
    \textbf{Bias:} They compute how biased a model is by comparing the toxicity associated with each subgroup to the average toxicity across all subgroups. An output is toxic if it refers to content that leads individuals to disengage from a discussion. Their claim is that large discrepancies in toxicity levels across different groups is a good proxy for measuring the model's bias.

    \textbf{Performance:} They define it in the context of language modeling. Thus, they use perplexity of a model.

    \textbf{Scoring:} They quantify each attention head's contribution to bias or performance as the difference between the model's bias or performance before and after pruning such head.

    \textbf{Fairness-aware structured pruning (FASP):} focuses on removing heads with a negative impact on fairness while ensuring the model's language modeling abilities are minimally affected. They first take p\% of top crucial performance heads. Then they rank 1-p/100 heads with respect to bias impact and prune q\%. 

\end{enumerate}

\subsection{Attention Head Related}
\begin{enumerate}
    \item Cascade head pruning to remove unessential
heads. Used maximum to score the heads~\cite{Wang_2021}
    \item Trained a mask on heads. Task is also around bias~\cite{yang2024biasaheadanalyzingbias}
    \item prune unimportant heads~\cite{voita2019analyzing}
    \item prune unimportant heads~\cite{zhang2021know}
    \item \cite{adiga2024attention} look at bias via attention heads, though a different context and approach
\end{enumerate}
\end{comment}

Recent studies have revealed pervasive biases in LLMs, showing that prompts using names associated with different demographic groups yield systematically disparate outcomes. Names linked to white individuals often receive more favorable responses in areas like product recommendations and financial opportunities compared to those associated with Black individuals, while names associated with women tend to elicit simpler language compared to those associated with men. Overall, this literature documents pervasive biases along both racial and gender lines~\cite{haim2024whatsnameauditinglarge, seshadri2025doesgiantnumberpile, salinas2023unequal, eloundou2024firstpersonfairnesschatbots, borah2024towards}. Moreover, LLMs exhibit biases that disadvantage individuals based on geographic and social factors, disproportionately rating regions with lower socioeconomic conditions (such as parts of Africa) unfavorably across sensitive topics like attractiveness, morality, and intelligence, while also reinforcing societal stereotypes across race, gender, religion, and health~\cite{manvi2024largelanguagemodelsgeographically, bai2024measuringimplicitbiasexplicitly}. Some studies suggest that LLMs trained with reinforcement learning from human feedback (RLHF) can “morally self-correct” when prompted, offering a potential pathway to mitigate harmful biases by incorporating normative concepts such as fairness and discrimination~\cite{ganguli2023capacity}. Despite these advancements, biases remain a significant concern, necessitating continued research to develop more robust mitigation strategies.

Model localization and pruning has emerged as a crucial technique for optimizing large language models (LLMs) by selectively removing network components to improve efficiency while maintaining performance. Various pruning methods have been proposed, each targeting different objectives such as computational efficiency~\cite{Wang_2021,voita2019analyzing,zhang2021know,sun2024simpleeffectivepruningapproach}, interpretability~\cite{templeton2024scaling, paulo2024automaticallyinterpretingmillionsfeatures}, and safety~\cite{chavhan2024memorizedimagesdiffusionmodels, chang2024localizationmethodsactuallylocalize}. One notable approach, WandA (Pruning by Weights and Activations), achieves sparsity by pruning weights with the smallest magnitudes multiplied by their corresponding input activations, all without requiring retraining or weight updates~\cite{sun2024simpleeffectivepruningapproach}. Beyond efficiency, pruning has been leveraged to identify critical model regions, such as those essential for maintaining safety guardrails~\cite{wei2024assessingbrittlenesssafetyalignment}, or subspaces responsible for memorization of sensitive information~\cite{chavhan2024memorizedimagesdiffusionmodels, chang2024localizationmethodsactuallylocalize}. Moreover, sparse autoencoders (SAEs) have been used to generate interpretable features that generalize across multilingual and multimodal contexts, offering insights into various aspects of model behavior, including bias and safety concerns~\cite{templeton2024scaling, paulo2024automaticallyinterpretingmillionsfeatures}.

Among the various applications, localization and pruning has been increasingly explored as a strategy for mitigating biases embedded in LLMs by targeting specific components responsible for biased outputs. One promising approach involves pruning attention heads that exhibit high discrepancies in toxicity levels across demographic groups, effectively reducing gender bias while preserving language capabilities and monitoring unintended effects on other social biases such as race and nationality~\cite{zayed2023FairnessAwarePruning}. Similarly, feature steering through sparse autoencoders has proven effective in reducing biases across multiple social dimensions without significantly compromising overall model performance, allowing for fine-grained control over biased representations~\cite{durmus2024steering}. In addition, bias neuron pruning has shown that--in some contexts--a minimal number of neurons contribute to biased outputs, and their removal may enhance fairness while ensuring robustness~\cite{yang2024mitigatingbiasesinstructionfollowinglanguage}. Other approaches focus on identifying layers within the model that concentrate bias, revealing that biases often emerge in later layers and can be mitigated by scaling attention in these layers without affecting downstream task performance~\cite{adiga2024attention}. These advancements in bias mitigation through pruning highlight the potential of targeted interventions to foster fairness in LLMs while preserving their functional integrity.

Regulatory efforts surrounding Artificial Intelligence (AI), particularly LLMs, have intensified in parallel with their widespread adoption across critical domains such as healthcare~\cite{mesko2023imperative} and hiring~\cite{jain2024ethical}. Proposals emphasize the need for comprehensive policy frameworks that integrate ethical principles, stakeholder responsibilities, and industry-specific guidelines to safeguard citizens' rights while encouraging innovation \cite{jain2024ethical}. Comparative analysis, such as those in \textcite{poncibo2025comparative}, shed light into how regulatory models in the European Union, China, and the United States differ in both scope and enforcement. An interesting development in self-regulation is the introduction of acceptable use policies by foundation model developers, although opaque governance mechanisms may still undermine accountability \cite{klyman2024acceptable}. Furthermore, the increasingly interconnected global landscape calls for an overarching AI governance framework without marginalizing civil society \cite{erman2024democratization}. Consequently, recent work underscores the need for systematic risk categorization and the creation of safety benchmarks to unify evaluations across jurisdictions and industry sectors \cite{zeng2024airiskcategorizationdecoded, zeng2024air}. Altogether, these developments show the urgency of addressing the complexity of regulating LLMs in a manner that balances innovation, fairness, and societal well-being.