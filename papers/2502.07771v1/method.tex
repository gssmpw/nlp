\section{Methods}
We consider the following generic framework:
We assume two groups, a majority group $\mathcal{G}_{\mathrm{maj}}$ and a minority group $\mathcal{G}_{\mathrm{min}}$. A language model is prompted to elicit a response, where group membership is communicated through the prompt (e.g. implicit or explicit mentions of race). We generically define bias as the difference in model response, conditional on group membership status.

Our pruning approach then consists of three key steps: (1) developing a scoring mechanism to quantitatively assess the influence of attention heads and neurons on model outputs, (2) localizing bias by leveraging the computed scores to identify components that disproportionately contribute to observed disparities, and (3) implementing targeted pruning, where the identified components are selectively zeroed out to mitigate bias while preserving the functionality and performance of the model in the specific original task.



% To address bias disparities, we conduct a systematic investigation into the presence of bias in LLM behavior and implement methodologies aimed at mitigating such disparities.

\begin{comment}
\subsection{Prompt Design}
To assess bias, we designed a dataset of prompts with three key objectives:
\begin{enumerate}
    \item \textbf{Advice-Seeking Scenarios.} All prompts were crafted to involve advice-seeking scenarios directed towards a third person. This setup follows ~\cite{haim2024whatsnameauditinglarge} to mimic a realistic context where the AI system provides advice based on the name of a person, ensuring the outcome is influenced by the name. The focus on third-person bias is also a response to limitations of our experimental setup, as we cannot replicate the model's "memory" feature, where a name provided in an unrelated conversation influences independent tasks.
    \item \textbf{Quantifiable Outcomes.} To ensure measurable and objective assessments of bias, all prompts were designed to yield numeric outcomes. This approach allows for straightforward quantification of disparities, eliminating reliance on subjective interpretations.
    \item \textbf{Consistency Across Prompts.} Within each scenario, we maintain uniformity in the prompt structure, varying only the name and the central element (e.g., the product in the \textit{Purchase} scenario). Across different scenarios, we adhere to this approach to the greatest extent possible, ensuring a consistent evaluation of neuron behavior.
\end{enumerate}

Table \ref{} details the process of crafting these prompts. Below, we elaborate on the most critical aspects of their design:
% generic definition of bias, majority and minority groups
\begin{enumerate}
    \item \textbf{Scenario and Variation.} Following \cite{haim2024whatsnameauditinglarge}, we adapted the \textit{Purchase} scenario as the baseline for our study. In it, the user enters a prospective negotiation over an item with a seller and asks about the amount that should be offered. The prompt varies the seller's name and implicit race association.  The scenario was selected due to its straightforward prompt design and interpretability, as well as its observed disparities. Variations within this and other scenarios were created by modifying the item involved with the goal of ensuring a diverse set of prompts, reducing the risk of overfitting to specific cases.

    To identify these variations, we prompt GPT-4o to generate a list of the top 50 products that potentially exhibit the highest racial disparities. Among these 50 products, we filtered out 20 products with for which a model would yield invalid responses frequently. We then assessed baseline disparities in the remaining 30 products through prompting over 100 iterations. Finally, we selected the 10 products that showed the most significant disparities. Details of this process can be found in Appendix \ref{}.

    To investigate the generalizability of bias within the model, we extended this method to identify three additional scenarios: \textit{Services}, \textit{Activities}, and \textit{Finance}. In each, we again identify three variations with high baseline disparities, resulting in a dataset of nine unique variations. Further details on these scenarios are provided in Appendix \ref{}.

    \item \textbf{Name.} Following \cite{haim2024whatsnameauditinglarge}, we curated a list of first names with strong high rates of congruent racial perception across race groups (Gaddis [CITE]). We pair each first name with the surnames that represent, according to the U.S. Census Bureau (2012), are most strongly associated with Black and white individuals: "Washington" and "Becker", respectively. These first names were chosen based on their high rates of congruent racial perception across race-gender groups. Expanding the name list to 64 names allowed us to collect more samples, which was necessary for aggregating neurons and heads scores by race. Including additional names beyond this threshold risked introducing ambiguity, as some names lacked clear racial associations within the participants of the study. 
    
\end{enumerate}
\end{comment}

\subsection{Localization}
To identify pruning targets, we employed two alternative localization methodologies frequently referenced in the literature. Each method focuses on a specific component of the LLM: attention head scoring inspired by~\textcite{Wang_2021} and~\textcite{adiga2024attention}, and neuron localization inspired by the approaches outlined by~\textcite{sun2024simpleeffectivepruningapproach} and~\textcite{wei2024assessingbrittlenesssafetyalignment}.

% Unsure if it should be here but this is something common to both localization methods. EXPLANATION AND JUSTIFICATION OF THREE VERSIONS: SPECIFIC, COMMON, SCENARIO.

\subsubsection{Scoring for Neurons}

We begin by measuring the importance of individual neurons in contributing to model outputs for specific tasks, with the eventual goal of mitigating biases. Our objective is to identify and score neurons that disproportionately influence the model's responses under these prompts.

We adopt the WandA (Weights and Activations) method proposed by~\textcite{sun2024simpleeffectivepruningapproach}, which evaluates a neuron's importance by quantifying how its activations affect the model's final predictions. 
% Although this procedure highlights which neurons are most impactful, one should be aware that it does not yet distinguish \textit{biased} neurons from those that serve a different function (e.g. \textit{utility} neurons). Instead, it provides an importance scoring that will help the pruning process.

Formally, for a given prompt of length $l$, let $\mathbf{A} \in \mathbb{R}^{l \times d}$ represent the activation matrix for $d$ neurons in a particular layer, and let $\mathbf{W} \in \mathbb{R}^{d \times o}$ be the corresponding weight matrix that projects these activations to $o$ output dimensions. The contribution of neuron $n$ to the output for each token $t$ is calculated as the product of its activation and weight:

\begin{equation}
    S^{\mathrm{neuron}}_{t,n} = Act_{t,n} W_{n}
\end{equation}

where $Act_{t,n}$ is the activation of neuron $n$ for token $t$, and $W_n$ is the corresponding associated column in $\mathbf{W}$. The total score of neuron $n$ across all tokens in a given prompt is then aggregated as:

\begin{equation}
    S^{\mathrm{neuron}}_n = \sum_{t=1}^{l} S^{\mathrm{neuron}}_{t,n} = \sum_{t=1}^{l} Act_{t,n} W_n
\end{equation}

This procedure yields a neuron-level score, $S^\mathrm{head}_n$, reflecting the aggregate contribution of each neuron to the model's final outputs under a particular prompt. 
%By repeating this process over the calibration set $D_{r_{i}}$, for each of the two races assessed, we obtain race-importance scores.



\subsubsection{Scoring for Attention Heads}

 We hypothesize that reducing the model's focus on the tokens with the group ($\mathcal{G}_{\mathrm{maj}}$ or $\mathcal{G}_{\mathrm{maj}}$) membership information, denoted as group tokens $\mathcal{T}_{\mathcal{G}\in\{\mathcal{G}_{\mathrm{maj}}, \mathcal{G}_{\mathrm{min}}\}}$, should reduce the bias observed in the outputs. Consistent with this hypothesis, we examine the attention that is allocated to such tokens.

In the Transformer architecture, each attention head computes a set of attention weights that determine how much focus each token in the input sequence places on earlier tokens. This mechanism enables the model to selectively incorporate relevant contextual information when generating each token. For a given attention head, the attention mechanism is defined as:


\begin{equation}
    \mathrm{Attention}_h(Q_h, K_h, V_h) = \underbrace{\mathrm{softmax}\left(\frac{Q_hK_h^\top}{\sqrt{d_k}}\right)}_{A_h} V_h
    \label{eq:attn}
\end{equation}


where $Q_h$, $K_h$, and $V_h$ are the query, key, and value matrices derived from the input embeddings, and $d_k$ is the dimension of the key vectors. The matrix $A_h \in \mathbb{R}^{n \times n}$ represents the attention weights after applying the softmax function, where $n$ is the total number of tokens in the input sequence. The softmax function ensures that each row of $A_h$ forms a probability distribution over the input tokens, determining how much attention each token pays to others.

In decoder-only models, due to their autoregressive nature, tokens can only attend to previous tokens in the sequence. Let $\mathcal{T}$ denote the set of tokens that follow $\mathcal{T_G}$. The submatrix $A_h^{\mathcal{T} \leftarrow \mathcal{T_G}} \in \mathbb{R}^{|\mathcal{T}| \times |\mathcal{T_G}|}$ is extracted from $A_h$, where each entry $A_h(i, j)$ represents the attention weight from the $i^\text{th}$ token in $\mathcal{T}$ to the $j^\text{th}$ token in $\mathcal{T_G}$.

The intuition behind this focus is grounded in how attention mechanisms update token representations. Specifically, multiplying $A_h$ by $V$ results in a weighted sum of value vectors, producing new token embeddings. For the tokens after the group tokens, a large attention weight on the group tokens would imply that the updated representations are heavily influenced by the membership information. To mitigate potential bias, we prefer these attention weights to be small, reducing their influence on subsequent tokens.

To quantify this influence, we define the attention score $S^\mathrm{head}_h$ for head $h$ as the maximum attention weight\footnote{We explored alternative scoring methods, such as averaging across different dimensions; however, maximizing over both dimensions yielded the best results.} from any token in $\mathcal{T}$ to any token in $\mathcal{T_G}$:

\begin{equation}
    S_h^{\mathrm{head}} = \max_{i \in \mathcal{T}, j \in \mathcal{T_G}} A_h^{\mathcal{T} \leftarrow \mathcal{T_G}}(i, j)
\end{equation}

A higher $S_h^\mathrm{head}$ indicates that some token after the group-related tokens heavily rely on the group-related tokens, which could lead to biased or contextually skewed outputs. By identifying and analyzing heads with high $S_h^\mathrm{head}$, we can better understand and mitigate the model's reliance on them, promoting more contextually balanced generation.

% Our intuition is supported by the structure of attention mechanisms: if the model assigns significant weight to the name token, this focus can propagate through the decoding process, leading to biased or less contextually appropriate outputs. By identifying and potentially down-weighting heads with high $S_{head}$, we can encourage the model to generate responses that are less dependent on specific names and more aligned with the overall context.


\subsection{Bias Localization}
\label{sec:bias-localization}

We define a model component $c$ as either an attention head $h$ or a neuron $n$, where the score for each component is represented as $S_c \in \{S_h^{\mathrm{head}}, S_n^{\mathrm{neuron}}\}$. To analyze systematic differences in model behavior, we extend this scoring metric across a broader set of prompts. Specifically, we evaluate $S_c$ using a list of majority group members $\mathcal{G}_{\mathrm{maj}}$ and minority group members $\mathcal{G}_{\mathrm{min}}$. For each model component $c$, this yields $|\mathcal{G}_{\mathrm{maj}}|$ scores associated with the majority group, denoted as $\{S_c^{\mathrm{maj}, i}\}_{i=1}^{|\mathcal{G}_{\mathrm{maj}}|}$, and $|\mathcal{G}_{\mathrm{min}}|$ scores associated with the minority group, denoted as $\{S_c^{\mathrm{min}, i}\}_{i=1}^{|\mathcal{G}_{\mathrm{min}}|}$.


To summarize the influence of each component across these groups, we compute the average score for the majority and minority group separately:

\begin{equation}
    \bar{S}_c^{\mathrm{maj}} = \frac{1}{|\mathcal{G}_{\mathrm{maj}}|} \sum_{i=1}^{|\mathcal{G}_{\mathrm{maj}}|} S_c^{\mathrm{maj}, i}, \quad \bar{S}_c^{\mathrm{min}} = \frac{1}{|\mathcal{G}_{\mathrm{min}}|} \sum_{i=1}^{|\mathcal{G}_{\mathrm{min}}|} S_c^{\mathrm{min}, i}
\end{equation}



These averages reflect the overall influence each component assigns to the prompts associated with majority and minority group membership, respectively.


Next, we sort the components based on these average scores in descending order, generating two separate rankings:

\begin{equation}
    \mathcal{C}_{\mathrm{maj}} = \mathrm{argsort}\left(\{\bar{S}_c^{\mathrm{maj}}\}_{c=1}^{C}\right), \quad \mathcal{C}_{\mathrm{min}} = \mathrm{argsort}\left(\{\bar{S}_c^{\mathrm{min}}\}_{c=1}^{C}\right)
\end{equation}


where $C$ is the total number of components. $\mathcal{C}_{\mathrm{maj}}$ and $\mathcal{C}_{\mathrm{min}}$ represent the ordered lists of components ranked by their average influence in processing majority and minority group membership, respectively.

We treat the majority group as the reference group, seeking to identify biases to the disadvantage of the minority group. To capture and isolate the components that disproportionately influence outputs for minority-associated prompts, we focus on the set-difference with respect to the minority group. That is, we isolate those components that are particularly influential for the minority group, but not for the majority group. From each list, we select the top $\tau_{\mathrm{min}}$ components from $\mathcal{C}_{\mathrm{min}}$ and the top $\tau_{\mathrm{maj}}$ components from $\mathcal{C}_{\mathrm{maj}}$. We then compute the set difference:



\begin{equation}
    \mathcal{D} = \left\{\mathcal{C}_{\mathrm{min}}^{(1)}, \dots, \mathcal{C}_{\mathrm{min}}^{(\tau_{\mathrm{min}})}\right\} \setminus \left\{\mathcal{C}_{\mathrm{maj}}^{(1)}, \dots, \mathcal{C}_{\mathrm{maj}}^{(\tau_{\mathrm{maj}})}\right\}
    \label{eq:set_diff}
\end{equation}

The set $\mathcal{D}$ contains the components that rank among the top $\tau_{\mathrm{min}}$ for the minority group but do not appear in the top $\tau_{\mathrm{maj}}$ for the majority group. This selection highlights components that are disproportionately influential when processing minority group membership, aligning with our goal of identifying model elements that may contribute to biased behavior.


\subsection{Model Intervention}
\label{sec:model-editing}
% PH: Model editing is a specific area of ML research nowadays, changed it to model intervention.
Having identified the sets of components that consistently exhibit disproportionate focus on minority group membership, we proceed to mitigate this bias by selectively pruning these components. The goal is to reduce the model's reliance on these influential components without disrupting the overall structure and utility of the model.

The core idea behind our pruning method is to effectively \textit{zero out} the contributions of the identified components, thereby preventing them from influencing downstream processing. For neuron pruning, this means directly setting the activations of the pruned neurons to $0$, while for attention heads, we zero out the corresponding neurons in the value matrix $V_h$.


In the Transformer architecture, the output of each attention head is computed as the product of the attention weight matrix $A_h$ and the value matrix $V$, as shown in~\Cref{eq:attn}.To prune an attention head $h$, we set the corresponding neurons in $V_h$ to $0$:

% The outputs of all heads within a multi-head attention layer are concatenated and passed through a linear transformation before being forwarded to subsequent layers.

% To prune the attention heads identified in the influential sets (e.g., $\mathcal{D}^{\text{final}}$), we effectively erase their contributions by zeroing out their value matrices. For any attention head $h$ in the pruning set $\mathcal{P}$, we set $V_h = \mathbf{0}$. Substituting this into the attention computation yields:

\begin{equation}
    \mathrm{Attention}_h = A_h V_h = A_h \cdot \mathbf{0} = \mathbf{0}
\end{equation}

Thus, the pruned attention head no longer contributes to the output, preventing its influence from the following contextual encoding process.


% This operation nullifies the output of the pruned heads, ensuring that they contribute nothing to the model's contextual representations. Importantly, this pruning is applied before the concatenation of all head outputs, preserving the model's structural integrity while selectively removing biased components.


% \subsection{Model Editing}
% We take the set difference of black neurons/heads with top $b$ scores and white neurons/heads with top $w$ scores. The intuition is that we want to look into neurons that appeared influential for black only
% \begin{equation}
%     f(b,w, v)=\{h | \text{top $b$ black in variation $v$}\}\setminus\{h | \text{top $w$ white in variation $v$}\}
% \end{equation} 

% Then we change the weight of $f(b,w)$ to 0


% Specific, Common, and Scenarios

% The set $\mathcal{D}$ contains the attention heads that rank among the top $b$ for Black names but do not appear in the top $w$ for white names. This selection highlights attention heads that are disproportionately influential when processing Black names, aligning with our goal of identifying model components that may contribute to biased behavior.