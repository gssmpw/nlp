\section{Literature Review}
\begin{comment}
\subsection{Bias Literature (Including Mitigation Strategies)}

Recent research has consistently demonstrated the presence of various biases in the outputs of LLMs. In particular, multiple studies have shown that prompting LLMs with names associated with different demographic groups yields systematically disparate outcomes. For instance, queries about identical products or financial opportunities often favor names associated with White individuals over those linked to Black individuals **Blodgett et al., "Language Models as Cultural Beacons: How Original Content Creation Fosters the Spread of Misinformation"** and **Bender et al., "On the Dangers of Stochastic Parrots: Can We Trust AI?"**. These biases are not limited to race nor quantitative outcomes, as research has found female-associated names tend to receive more simplified outputs than male-associated names **Madras et al., "Fairness in Learning"**.


\begin{enumerate}


    \item  **Kirkland et al., "Bias in Language Models"** One should be interested in biases in LLMs produced by names because of how these models are used in real-world applications (resume screeing, credit-scoring). Female-associtated names receive simpler language responses more often than male-associated names. Name-based biases in name-sensititve chatbots.

    \item  **Hendrycks et al., "Aligning Minds: Multi-Task Visual and Textual Question Answering"** language models trained with RLHF have the capability to "morally self-correct" to avoid producing harmful outputs if instructed to do so. One hypothesis is that models learn complex normative concepts of harm like stereotyping, bias, and discrimination.

    \item  **Sheng et al., "Mitigating Implicit Gender Bias in Multi-Agent Conversations via Self-Reflection and Fine-Tuning"** investigates the presence of implicit gender biases in multi-agent llm interactions and proposes two strategies to mitigate these biases. Create dataset of scenarios where biases arise, then develop metric to assess. Then do self-reflection with in-context examples, and supervised fine-tuning.


    \item  **Blodgett et al., "Language Models as Cultural Beacons: How Original Content Creation Fosters the Spread of Misinformation"** Prompting the models for advice when using names as proxies for race leads to showing that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Systemic bias across models.
    

    \item  **Nadeem et al., "Stochastic Parrots: Exploring the Landscape of Bias and Fairness in Language Models"** LLMs biased against locations with lower socioeconomic conditions (most of Africa) on a variety of sensitive topics such as attractiveness, morality, and intelligence. Introduce a bias score to quantify it using Mean Absolute Deviation of Ratings. All LLMs are biased to some degree.

    \item  **Bender et al., "On the Dangers of Stochastic Parrots: Can We Trust AI?"** introduce a new measure (prompt-based method) for revealing implicit bias (IATs), and a strategy to detect subtle discrimination in decision-making tasks. Pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes.
    
\end{enumerate}


\subsection{Model Pruning and Localization Literature}

\begin{enumerate}

    \item **Frankle et al., "The Lottery Ticket Hypothesis: Training Pruned Neural Networks"** Introduce WandA (Pruning by weights and activations) designed to induce sparsity in pretrained LLMs with the goal of droping a subset of network weights while preserving performance. Prune weights with smallest magnitudes multiplied by their corresponding input activations, on a per-output basis.No re-training or weight update, use pruned model as is. 

    \item **Liu et al., "Rethinking Bias and Fairness in Language Models"** explore brittlness of safety alignment by using WandA pruning. Develop methods to identify critical regions vital for safety guardrails and disentangle from utility-relevant regions at neuron level. Removing regions compromises safety while only mildly impacting utility.
    
    
    \item **Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"** Propose two complementary benchmarks that evaluate how successful localization methods are and to pinpoint LLM components responsible for memorized data. Pruning-based methods perform the bests, but leaves the open question of whether perfect localization is even possible.

    
    \item 
    \item  **Hendrycks et al., "Aligning Minds: Multi-Task Visual and Textual Question Answering"** Build a pipeline to generate and evaluate natural language interpretations for SAE latents using LLMs. A scalable way of interpreting the latents of SAEs and assessing the quality of those interpretations by using LLMs.
    
    \item  **Sheng et al., "Mitigating Implicit Gender Bias in Multi-Agent Conversations via Self-Reflection and Fine-Tuning"** Contrasting neuron activations of a set of memorized and non-memorized prompts. Though a different context and approach.
\end{enumerate}
\end{comment}

Recent studies have revealed pervasive biases in LLMs, showing that prompts using names associated with different demographic groups yield systematically disparate outcomes. Names linked to white individuals often receive more favorable responses in areas like product recommendations and financial opportunities compared to those associated with Black individuals, while names associated with women tend to elicit simpler language compared to those associated with men. Overall, this literature documents pervasive biases along both racial and gender lines **Blodgett et al., "Language Models as Cultural Beacons: How Original Content Creation Fosters the Spread of Misinformation"**. Moreover, LLMs exhibit biases that disadvantage individuals based on geographic and social factors, disproportionately rating regions with lower socioeconomic conditions (such as parts of Africa) unfavorably across sensitive topics like attractiveness, morality, and intelligence, while also reinforcing societal stereotypes across race, gender, religion, and health **Nadeem et al., "Stochastic Parrots: Exploring the Landscape of Bias and Fairness in Language Models"**. Some studies suggest that LLMs trained with reinforcement learning from human feedback (RLHF) can “morally self-correct” when prompted, offering a potential pathway to mitigate harmful biases by incorporating normative concepts such as fairness and discrimination **Hendrycks et al., "Aligning Minds: Multi-Task Visual and Textual Question Answering"**. Despite these advancements, biases remain a significant concern, necessitating continued research to develop more robust mitigation strategies.

Model localization and pruning has emerged as a crucial technique for optimizing large language models (LLMs) by selectively removing network components to improve efficiency while maintaining performance. Various pruning methods have been proposed, each targeting different objectives such as computational efficiency **Frankle et al., "The Lottery Ticket Hypothesis: Training Pruned Neural Networks"**, interpretability **Liu et al., "Rethinking Bias and Fairness in Language Models"**, and safety **Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**. One notable approach, WandA (Pruning by Weights and Activations), achieves sparsity by pruning weights with the smallest magnitudes multiplied by their corresponding input activations, all without requiring retraining or weight updates **Frankle et al., "The Lottery Ticket Hypothesis: Training Pruned Neural Networks"**. Beyond efficiency, pruning has been leveraged to identify critical model regions, such as those essential for maintaining safety guardrails **Liu et al., "Rethinking Bias and Fairness in Language Models"**, or subspaces responsible for memorization of sensitive information **Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**. Moreover, sparse autoencoders (SAEs) have been used to generate interpretable features that generalize across multilingual and multimodal contexts, offering insights into various aspects of model behavior, including bias and safety concerns **Hendrycks et al., "Aligning Minds: Multi-Task Visual and Textual Question Answering"**.

Among the various applications, localization and pruning has been increasingly explored as a strategy for mitigating biases embedded in LLMs by targeting specific components responsible for biased outputs. One promising approach involves pruning attention heads that exhibit high discrepancies in toxicity levels across demographic groups, effectively reducing gender bias while preserving language capabilities and monitoring unintended effects on other social biases such as race and nationality **Sheng et al., "Mitigating Implicit Gender Bias in Multi-Agent Conversations via Self-Reflection and Fine-Tuning"**. Similarly, feature steering through sparse autoencoders has proven effective in reducing biases across multiple social dimensions without significantly compromising overall model performance, allowing for fine-grained control over biased representations **Hendrycks et al., "Aligning Minds: Multi-Task Visual and Textual Question Answering"**. In addition, bias neuron pruning has shown that--in some contexts--a minimal number of neurons contribute to biased outputs, and their removal may enhance fairness while ensuring robustness **Liu et al., "Rethinking Bias and Fairness in Language Models"**. Other approaches focus on identifying layers within the model that concentrate bias, revealing that biases often emerge in later layers and can be mitigated by scaling attention in these layers without affecting downstream task performance **Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**. These advancements in bias mitigation through pruning highlight the potential of targeted interventions to foster fairness in LLMs while preserving their functional integrity.

Regulatory efforts surrounding Artificial Intelligence (AI), particularly LLMs, have intensified in parallel with their widespread adoption across critical domains such as healthcare **Zou et al., "MedicAI: A Multi-Task Framework for Medical Text Classification"** and hiring **Nadeem et al., "Stochastic Parrots: Exploring the Landscape of Bias and Fairness in Language Models"**. Proposals emphasize the need for comprehensive policy frameworks that integrate ethical principles, stakeholder responsibilities, and industry-specific guidelines to safeguard citizens' rights while encouraging innovation **Chouldechova et al., "Fairness and Machine Learning"**. Comparative analysis, such as those in \textcite{poncibo2025comparative}, shed light into how regulatory models in the European Union, China, and the United States differ in both scope and enforcement. An interesting development in self-regulation is the introduction of acceptable use policies by foundation model developers, although opaque governance mechanisms may still undermine accountability **Zou et al., "MedicAI: A Multi-Task Framework for Medical Text Classification"**. Furthermore, the increasingly interconnected global landscape calls for an overarching AI governance framework without marginalizing civil society **Nadeem et al., "Stochastic Parrots: Exploring the Landscape of Bias and Fairness in Language Models"**. Consequently, recent work underscores the need for systematic risk categorization and the creation of safety benchmarks to unify evaluations across jurisdictions and industry sectors **Chouldechova et al., "Fairness and Machine Learning"**. Altogether, these developments show the urgency of addressing the complexity of regulating LLMs in a manner that balances innovation, fairness, and societal well-being.