\section{Introduction}
Generative models like large language models (LLMs) are becoming a progressively central technology across many areas of life, including healthcare~\cite{singhal2025toward, mansoor2024evaluating}, finance~\cite{kim2024financial, zhao2024revolutionizing}, and education~\cite{bewersdorff2025taking, cain2024prompting}. Users also increasingly turn to these models for advice as they navigate day-to-day challenges, such as solving homework tasks~\cite{pew2025chatgpt} or improving mental health~\cite{lemonde2024ai}. But as the utilization of this technology proliferates, so do reservations rooted in the risks and dangers the models may pose. Among the diverse set of risks, concerns that generative models could solidify or exacerbate bias are of particular sensitivity~\cite{ferrara2023fairness, gautam2024melting, hacker2024generative, Xiang_2024}. To mitigate the potential for generative models to harm society, several regulatory and liability frameworks have been proposed~\cite{eu2021aiact,whitehouse2023ai}. For instance, the European Union Artificial Intelligence Act provides a multi-pronged risk-based approach. One central question in developing these frameworks is how to effectively allocate responsibility for harmful model behavior. The core tension is the direct result of generative models being developed and deployed as a ``general purpose technology,'' i.e. a technology with a wide variety of potential uses, which is adapted to a specific domain with specialization~\cite{bresnahan2010general}.
In this context, a \textit{developer} trains the general purpose model (hence also called a ``foundation model''~\cite{bommasani2021opportunities}) in a way that is designed to be suitable for a variety of downstream tasks. A \textit{deployer} then takes the general purpose model and deploys it for a specific use case in a particular domain, often after making changes to the model through adaptation, system prompts, fine-tuning etc.~\cite{laufer2024fine, xu2024economics, chen2024overview}.
In designing an effective regulatory regime, the question thus arises whether the right addressee of the enforcement action should be the \textit{developer} or the \textit{deployer}. This question, in turn, depends on how the harmful tendencies are encoded in the model. 

Consider the example of racial bias in the U.S. Recent empirical evidence points to racial bias being a pernicious problem across a variety of contexts such as policing \cite{pierson2020large}, health care \cite{pregnancy_bias}, and education \cite{pnasSchool}. At the same time, the dynamics that lead to the manifestation of disparities can vary markedly from one domain to the next. For instance, racial bias in policing may in part be the consequence of selection into the police force \cite{racist_cops}, perceptions of violent tendencies \cite{perception_violent, tv_perception}, and disparities in media coverage \cite{facebook_nyarko, suspects_black}. In contrast, racial bias in hiring is often linked to perceived differences in productivity \cite{kirshenman2019we} as well as racial homogeneity of social ties, which may make it more difficult for job advertisements to be communicated to participants outside the group \cite{job_network}. 
The complex nature in which racial and other forms of bias manifest in society raises similar questions about their manifestation in generative models. On one extreme, it is possible that bias is encoded as a general concept in a small number of ``biased neurons'' that remain consistent and affect model generations irrespective of the specific context or domain. If that is the case, it appears plausible that an effective mitigation strategy is generalizable. \textit{Developers} are best positioned to make general adjustments to model behavior, who can implement these strategies upstream to the benefit of any specific task in which the model is used. On the other extreme, it is possible that bias is encoded as a highly contextual concept, with a diffuse representation inside the model. Under this assumption, it is not possible to implement general mitigation strategies. Instead, effective mitigation strategies would need to be tailored to the specific scenario in which the models are utilized. It is generally difficult for the \textit{developer} to anticipate all possible downstream tasks that their model will be used for. In addition, evaluating and mitigating bias can require access to private, contextual information that is not readily available to the \textit{developer}. Under this scenario, it thus may be more effective to impose legal liability on the model \textit{deployer}, who can directly anticipate and has control over the downstream task for which the model is used.
In this article, we explore how one popular generative text model (Llama-3-8B-Instruct) encodes biases. Although bias can manifest in different forms, such as through implicit associations \cite{kotek2023gender}, here we follow \textcite{haim2024whatsnameauditinglarge, eloundou2024firstpersonfairnesschatbots} and focus on bias as the quantifiable difference in model generation when the prompt pertains to members of the majority vs the minority group. Although the framework we present is of a general nature, in our evaluations, we focus on racial bias, where the majority group consists of white individuals and the minority group consists of black individuals. In particular, we prompt the model for advice about an individual, implicitly manipulating the race-association of said individual through the name in the prompt. \textcite{haim2024whatsnameauditinglarge} and \textcite{bai2024measuringimplicitbiasexplicitly} have shown--and we confirm--that many LLMs yield responses that are disadvantageous to the individual if their name implies that they are Black. We then explore different model pruning strategies in an attempt to reduce bias in model responses.

Our investigation yields several findings: First, while the pruning of both attention heads and individual neurons can successfully reduce bias in model outputs, neuron-level is more effective at reducing model bias. Second, we find that the effectiveness of pruning depends heavily on the similarity between the training set, which is used to identify the neurons to be pruned, and the test set on which the pruning strategy is evaluated. If train and test set are drawn from an identical context, pruning is able to achieve an average bias that is close to 0, demonstrating the theoretical strength of the approach. However, as the difference in context between training and test set increases, the performance of pruning decreases rapidly. For instance, when we train the pruning approach on biases in financial decision-making and apply it to remove biases in commercial transactions, we only achieve about a 40\% reduction in bias. 
Our results support the hypothesis that racial bias is, at least in part, represented as a domain-specific construct. This suggests that generalized mitigation strategies may have limited potential to remove biases in downstream tasks. Instead, effective mitigation may require context- and domain-specific adjustments, a task that can most productively be performed by the \textit{deployer}. Overall, our findings suggest that legal liability of the \textit{deployer} may be a necessary component in a holistic approach the bias in general purpose, generative models.