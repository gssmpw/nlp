\section{Appendix}

\subsection{Prompt Selection for Pruning Evaluation}
\label{app-prompt-selection}

To systematically evaluate the impact of pruning on model disparities and utility, we first select prompt variations that exhibit significant bias. We focus on the \textit{Purchase} scenario, which demonstrates the most pronounced disparities, making it an ideal test case for bias analysis. The prompt selection process consists of the following steps:

\begin{enumerate}
    \item We generate 30 prompt variations (e.g., different products to purchase) using a combination of LLM-generated suggestions and manual inspection to ensure diversity and relevance.
    \item Each variation is processed through the LLaMA 3-Instruct 8B model to measure the initial disparity in outcomes.
    \item Based on these evaluations, we select the $N=10$ variations with the highest disparities for further analysis.
\end{enumerate}

These selected variations serve as the foundation for subsequent pruning experiments, allowing us to focus on cases where bias is most evident.

\begin{comment}
\subsection{Specific Model (Robustness Test for B and W)}
\label{app-bw}
\begin{enumerate}
    \item We choose purchase as the main scenario to evaluate because it shows the largest disparity (maybe some other reasons or just no reason)
    \item We come up with 30 variations (i.e. products to purchase) with the help of LLMs and manual inspection
    \item We run the 30 variations through Llama3-instruct to get the model's initial disparities and pick the 10 $V_p$ with the largest disparities

\end{enumerate}

\begin{enumerate}

    \item For each variation $v$, we first determine the grid search range by varying $b$ and $w$ and tracking the size of $f(b,w,v)$. We pick the $b$ and $w$ at the elbow points [DEFINE] where the size stops decreasing rapidly. We found that it's roughly when $b=w$. Thus we pick $b\in\{i|0 \leq i \leq 50, i\equiv0\pmod5\}$, and for each picked $b$, we pick $w\in\{i|0 \leq i \leq b, i\equiv0\pmod5\}$

    \item Then we prune the model with respective $b$ and $w$ combinations and got the 66 results. We analyze the disparity and utility of each result.
    \begin{enumerate}
        \item Utility is defined as the fraction of prices outside the range of the initial model's output
    \end{enumerate}
\end{enumerate}
\end{comment}

\subsection{Threshold Optimization for Pruning}
\label{app-bw}

We conduct an extensive grid search to determine the optimal values of pruning thresholds $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ for neurons and attention heads. The optimization process follows these steps:

\begin{enumerate}
    \item For each prompt variation, we explore different values of $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ while monitoring the size of the bias-influential set $\mathcal{D}$ defined in~\autoref{eq:set_diff}. 
    \item The thresholds are selected based on the \textit{elbow point} principle, where the set size stabilizes, indicating diminishing returns. Empirical observations suggest $\tau_{\text{min}} \approx \tau_{\text{maj}}$, leading to the choice of the following ranges.
    
    For neuron pruning, the selected ranges were:
    
    We define the parameter selection ranges for pruning as follows. Let $\mathcal{R}_{\tau_{\text{min}}}$ and $\mathcal{R}_{\tau_{\text{maj}}}$ denote the ranges for $\tau_{\text{min}}$ and $\tau_{\text{maj}}$, respectively.
    
    For neuron pruning, the ranges are defined as:
    \begin{equation}
        \mathcal{R}_{\tau_{\text{min}}}^{\text{neuron}} = \left\{ 0.05 \cdot k \mid k \in \mathbb{Z}, 1 \leq k \leq 10 \right\}
    \end{equation}
    \begin{equation}
        \mathcal{R}_{\tau_{\text{maj}}}^{\text{neuron}} = \left\{ 0.05 \cdot k \mid k \in \mathbb{Z}, 1 \leq k \leq \frac{\tau_{\text{min}}}{0.05} \right\}
    \end{equation}
    
    For attention head pruning, the ranges are defined as:
    \begin{equation}
        \mathcal{R}_{\tau_{\text{min}}}^{\text{head}} = \left\{ 5 \cdot k \mid k \in \mathbb{Z}, 1 \leq k \leq 10 \right\}
    \end{equation}
    \begin{equation}
        \mathcal{R}_{\tau_{\text{maj}}}^{\text{head}} = \left\{ 5 \cdot k \mid k \in \mathbb{Z}, 1 \leq k \leq \frac{\tau_{\text{min}}}{5} \right\}
    \end{equation}
    
    \item The model is pruned using the selected $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ values across the 66 combinations, and the disparities and utility of each setting are analyzed.
    
\end{enumerate}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\linewidth]{neuron_figures/smd_combinations_hm.png}
    \includegraphics[width=0.49\linewidth]{neuron_figures/inlier_combination_hm.png}
    \caption{Grid search results for neuron pruning. Plot on the left is SMD and the plot on the right is inlier ratio}
    \label{fig:nueron-grid-search}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\linewidth]{head_figures/smd_combinations_hm_heads_LAST.png}
    \includegraphics[width=0.49\linewidth]{head_figures/utility_combinations_hm_heads_LAST.png}
    \caption{Grid search results for neuron pruning. Plot on the left is SMD and the plot on the right is inlier ratio}
    \label{fig:head-grid-search}
\end{figure}

The resulting SMD for different parameter combinations is shown in~\autoref{fig:nueron-grid-search} (neuron pruning) and~\autoref{fig:head-grid-search} (attention head pruning). 


\subsection{Prompt Design}
\label{app-promptDesign}
\input{tables/table1}
\autoref{tab:prompts} presents the comprehensive list of names used in our study, categorized into majority and minority groups based on their racial associations, as well as the structured prompt templates employed to evaluate model biases. The prompts were designed to simulate real-world scenarios where an AI system provides recommendations or estimates based on the provided names. 

%% This appendix should include the iterative process we did in order to obtain the variations/scenarios with the highest disparities. But this appendix should also include how we selected the names and last names which is essentially just a reference to the original auditing paper.

%%Most of the aforementioned is commented in the main paper

\subsection{Utility definition}
\label{app-utility}
%% Winsorize process, how we encompass here non-quantitative answers, in my case, it was everything that was not able to be converted to float after extracting the quant parts with regex rules. And to that, add everything that lies out of the min-max range established by the unpruned model.

In our evaluation framework, utility is defined as the model's ability to generate responses within a reasonable and expected range, ensuring that pruning does not significantly alter the model's functional capabilities. Specifically, we consider a response to be utility-preserving if it falls within the established bounds derived from the unpruned model's outputs.

To achieve this, we follow a two-step process:

\begin{enumerate}
    \item \textbf{Quantitative Filtering:} 
    Since model responses may contain both numerical and non-numerical elements, we apply regular expression (regex) rules to extract quantitative parts of the output. If a response cannot be successfully parsed into a numerical value (e.g., free-text responses, incomplete numbers, or ambiguous answers), it is classified as a non-quantitative response and is considered out of the utility-preserving range.

    \item \textbf{Range-Based Filtering:} 
    We establish a reference range for acceptable numerical values based on the minimum and maximum outputs generated by the unpruned model across all prompt variations. Any response that falls outside this range is marked as a utility violation. This range-based filtering ensures that extreme deviations resulting from pruning do not adversely impact the modelâ€™s expected behavior.
\end{enumerate}

To mitigate the impact of outliers and ensure robustness, we apply a winsorization process to the unpruned model outputs, capping extreme values at predefined percentiles. This prevents the influence of exceptionally high or low values from skewing the utility range and provides a more stable evaluation metric. Results without winsorization are included in Appendix \ref{app-robustness} and are consistent with the findings in the main paper.


\subsection{Shared Pruned Neurons}
\label{app-shared-neurons}
To better understand the consistency of pruning across different prompt variations, we analyze the overlap of pruned components within the \textit{Purchase} scenario. \Cref{fig-hm-vars-neuron} and \Cref{fig-hm-vars-head} provide heatmaps that quantify the extent to which biased neurons and attention heads are shared across prompt variations. This analysis helps to assess whether certain components consistently contribute to biased behavior, thereby informing the reliability of our pruning strategy.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.75\linewidth]{neuron_figures/variation_overlap_hm.png}
%   \caption{Overlap in biased neurons across \textit{Purchase} variations. Heat is defined as a fraction, with the numerator being X and the denominator being Y. Higher values indicate stronger overlap. }
%     \Description{ }
%   \label{fig-hm-vars-neuron}
% \end{figure}
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.75\linewidth]{head_figures/common_head_heatmap (3).png}
%   \caption{Overlap in biased neurons across \textit{Purchase} variations. Heat is defined as a fraction, with the numerator being X and the denominator being Y. Higher values indicate stronger overlap. }
%     \Description{ }
%   \label{fig-hm-vars-head}
% \end{figure}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{neuron_figures/variation_overlap_hm.png}
      \caption{Neuron pruning overlap}
      \label{fig-hm-vars-neuron}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{head_figures/common_head_heatmap_LAST.png}
      \caption{Attention head pruning overlap}
      \label{fig-hm-vars-head}
  \end{subfigure}
  \caption{Overlap of pruned neurons and attention heads across \textit{Purchase} prompt variations. 
  The heatmaps display the fraction of shared pruned components, with warmer colors indicating a higher fraction of shared components between variations.
  The numerator in each cell represents the intersection of pruned components between two variations, while the denominator represents the total number of pruned components for the variation corresponding to its row. 
  The neuron pruning overlap (left) highlights a moderate level of consistency, whereas the attention head pruning overlap (right) reveals greater variation across different prompt settings, suggesting differing levels of bias localization effectiveness.}
  \label{fig-hm-vars-combined}
\end{figure}

\subsection{Robustness Tests}
\label{app-robustness}

To evaluate the robustness of our findings, we conduct two additional analyses using a variant of our main metric (SMD) without winsorizing outliers and an alternative metric, Earth Mover's Distance (EMD).

\Cref{fig-robust-smd-combined} presents the SMD across the ten variations of the \textit{Purchase} scenario without removing extreme values. As noted in \Cref{fig-neuron-scatter} and \Cref{app-utility}, our main analyses winsorize outliers to avoid double-counting them in both the bias metric (SMD) and the utility metric (inlier ratio). In this robustness test, however, we evaluate the raw distributions to test how sensitive our results are to outliers handling. The results show a similar pattern, albeit less pronounced. 

In addition, \Cref{fig-robust-emd-combined} reports the Earth Mover's Distance for these same variations. Unlike SMD, which focuses on capturing mean disparities and depends on a pooled standard deviation, EMD measures the "cost" to transform one distribution into another. Thus, it provides insight into broader distributional attributes, such as differences in shape and skewness, beyond just mean-level effects. This additional analyses confirm the key takeaways from our main results: while pruning strategies can reduce racial bias, their ability to do so in a generalizable manner remains limited. 

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{neuron_figures/smd_no_winsorize.png}
      \caption{Neuron pruning SMD no Winsorizing}
      \label{fig-robust-smd-neuron}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{head_figures/sibo_smd_no_winsorize.png}
      \caption{Attention head pruning SMD no Winsorizing}
      \label{fig-robust-smd-head}
  \end{subfigure}
  \caption{Impact of Neuron and Attention Head Pruning on Bias as Measured by SMD without Winsorizing. The plots present the Standardized Mean Difference (SMD) scores without winsorizing the data across ten variations of the \textit{Purchase} scenario, comparing the unpruned baseline (green) with three pruning approaches: Prompt-Specific (orange), Within-Context (blue), and Cross-Context (brown). Vertical dashed lines indicate the mean SMD without winsorizing for each approach.}
  \label{fig-robust-smd-combined}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{neuron_figures/emd_winsorize.png}
      \caption{Neuron pruning EMD}
      \label{fig-robust-emd-neuron}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth]{head_figures/sibo_emd_winsorize.png}
      \caption{Attention head pruning EMD}
      \label{fig-robust-emd-head}
  \end{subfigure}
  \caption{Impact of Neuron and Attention Head Pruning on Bias as Measured by EMD. The plots present the Earth Mover's Distance (EMD) scores across ten variations of the \textit{Purchase} scenario, comparing the unpruned baseline (green) with three pruning approaches: Prompt-Specific (orange), Within-Context (blue), and Cross-Context (brown). Vertical dashed lines indicate the mean EMD for each approach.}
  \label{fig-robust-emd-combined}
\end{figure}
