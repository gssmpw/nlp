% Prompt design (3.1)
% Generalizability Stuff (3.4)
% Explain how prompts relate to generalizability stuff
\section{Evaluation Setup}

Next, we assess and mitigate bias in language models, with a specific focus on disparities between Black and white racial groups. Our goal is twofold: first, to identify the model components that contribute to biased outputs; and second, to implement targeted interventions that effectively reduce these disparities while preserving the specific functionality under investigation. Furthermore, we hypothesize that the presence of \emph{biased} components may offer a systematic pathway for localizing and potentially generalizing bias mitigation strategies across different contexts. To achieve these objectives, we introduce a structured evaluation setup that examines bias from multiple perspectives. 

\subsection{Prompt Design}
To systematically assess potential biases, we create a diverse set of prompts with three key objectives:

\begin{itemize}

    \item \textbf{Advice-Seeking Scenarios.}  
    We follow the approach outlined in \textcite{haim2024whatsnameauditinglarge}, crafting single-turn prompts to simulate realistic requests for advice regarding a third person. That third person's name is perceived to be a strong predictor of race \cite{gaddis2017black}. The design isolates the effect of bias at the response-generation stage, offering an interpretable framework for bias detection.\footnote{A key advantage of focusing on third-person contexts, such as that of our prompts, is that it allows us to observe model biases in a controlled environment without interference from prior conversational history, a distinguishing factor from multi-turn interactions where models can accumulate contextual dependencies over time.}

    \item \textbf{Quantifiable Outcomes.} Each prompt yields a numeric outcome, enabling straightforward bias quantification and reducing ambiguity or subjective interpretation.

    \item \textbf{Consistency Across Prompts.} Ensuring structural consistency across prompts is crucial for isolating the impact of implicit race-associations. We maintain a uniform structure within each scenario, altering only the name and key contextual elements (e.g., the product in a \textit{Purchase} scenario). This consistency minimizes confounding variables and promotes reproducibility.

\end{itemize}

% Table \ref{} details the process of crafting these prompts. Below, we elaborate on the critical aspects of their design. 
% Following \cite{haim2024whatsnameauditinglarge}, we curated a list of first names with strong high rates of congruent racial perception across race groups (Gaddis [CITE]). We pair each first name with the surnames that represent, according to the U.S. Census Bureau (2012), are most strongly associated with Black and white individuals: "Washington" and "Becker", respectively. These first names were chosen based on their high rates of congruent racial perception across race-gender groups. Expanding the name list to 64 names allowed us to collect more samples, which was necessary for aggregating neurons and heads scores by race. Including additional names beyond this threshold risked introducing ambiguity, as some names lacked clear racial associations within the participants of the study.

% Table \ref{tab:prompts} provides an overview of the prompt creation process.

To select names for inclusion, we rely on first and last names as shown in~\textcite{haim2024whatsnameauditinglarge}, which in turn rely on \textcite{gaddis2017black}. The names reflect those with the highest rates of congruent racial perceptions across racial groups. We expanded the list from the original 40 to 64 names for robust group-level evaluations while avoiding less strongly associated names that could introduce ambiguity. \Cref{app-promptDesign} further details our prompt creation process.

% generic definition of bias, majority and minority groups
% \begin{enumerate}
%     \item \textbf{Scenario and Variation.} 

%     To investigate the generalizability of bias within the model, we extended this method to identify three additional scenarios: \textit{Services}, \textit{Activities}, and \textit{Finance}. In each, we again identify three variations with high baseline disparities, resulting in a dataset of nine unique variations. Further details on these scenarios are provided in Appendix \ref{}.

%     \item \textbf{Name.} 
 
    
% \end{enumerate}

\subsection{Bias and Utility}
\label{sec:smd-utility}
To compare model performance across different settings, we quantify disparities using the Standardized Mean Difference (SMD). The SMD measures the difference in means between two groups (Black- and white-associated names) relative to the pooled standard deviation \textcite{andrade2020SMD}. It offers a standardized scale that enables meaningful comparisons across different prompt variations and experimental conditions. More than just measuring raw mean differences, SMD accounts for variability within each group, making it more robust to differences in scale and distribution. Additionally, SMD allows us to combine results from various variations and scenarios in a consistent manner, as its standardized nature ensures that differing units or scales do not distort the overall bias measurement.\footnote{For consistent results using the Wasserstein distance as an alternative metric, see Appendix \ref{app-robustness}.} It is formally defined as\footnote{For response generation, we set the model temperature to 0.6 to introduce a moderate level of randomness while maintaining coherence in the generated text. Each prompt is run 100 times to account for variability in model outputs and to ensure statistical robustness in our SMD calculations. Thus we have $100\cdot|\mathcal{G}_{\text{black}}|$ outputs for Black-associated group and $100\cdot|\mathcal{G}_{\text{white}}|$ for white-associated group. This repeated sampling strategy helps mitigate the effects of stochasticity inherent in LLMs and provides a more reliable estimate of disparities across demographic groups.}:


    \begin{equation}
        \text{SMD} = \frac{\bar{X}_{\text{black}} - \bar{X}_{\text{white}}}{s_p}
    \end{equation}

    where $\bar{X}_{\text{black}}$ and $\bar{X}_{\text{white}}$ represent the mean outputs for prompts containing Black- and white-associated names, respectively, and $s_p$ is the pooled standard deviation given by:

    \begin{equation}
        s_p = \sqrt{\frac{(100\cdot|\mathcal{G}_{\text{black}}| - 1)s_{\text{black}}^2 + (100\cdot|\mathcal{G}_{\text{white}}| - 1)s_{\text{white}}^2}{100\cdot|\mathcal{G}_{\text{black}}| + 100\cdot|\mathcal{G}_{\text{white}}| - 2}}
    \end{equation}

    % When SMD is standardized using the pooled standard deviation, it is also referred to as Cohen's $d$.

As shown by \textcite{sun2024simpleeffectivepruningapproach} and \textcite{wei2024assessingbrittlenesssafetyalignment}, pruning may decrease model performance due to the removal of knowledge. To assess a potential reduction in model capabilities, we define a context-specific utility metric. In our application, a model without utility is one that either yields no quantitative response or yields responses of an implausible quantity (e.g. several million dollars for a car). Consequently, we define our utility metric as the Inlier Ratio, which is the fraction of predicted prices that fall within the range of the unpruned model's outputs, excluding all generated non-numeric answers. This metric captures the extent to which the pruning methods preserve the original model's output distribution. Further details of our method are provided in~\Cref{app-utility}.


\subsection{Localization \& Generalizability Analysis}

% Following~\autoref{sec:bias-localization}, While the set $\mathcal{D}$ identifies components that disproportionately focus on Black names for a single prompt variation, this analysis alone does not guarantee that these heads consistently exhibit this behavior across different contexts. To evaluate the generalizability and robustness of these influential heads, we contrast three procedures designed to progressively assess their stability and relevance.

Following~\Cref{sec:bias-localization}, the set $\mathcal{D}$ identifies components that disproportionately focus on Black names within a single prompt variation. However, this analysis alone does not confirm whether these components consistently exhibit the same behavior across different contexts. To rigorously evaluate the generalizability and robustness of these influential components, we employ a three-step approach designed to progressively assess their stability and relevance.

Each step in our evaluation progressively increases the complexity, diversity, and abstractness of the tested scenarios to determine whether the identified components reflect systematic, generalizable patterns of racial bias or context-specific artifacts. 


\subsubsection{Prompt Specific Performance}
To evaluate the method's ability to identify influential components under ideal conditions, we create distinct variations of the prompt, each introducing slight contextual changes while preserving the core structure. This step allows us to assess the best possible performance of our method in detecting biased components. 
% Following \cite{haim2024whatsnameauditinglarge}, we adapted the \textit{Purchase} scenario as the baseline for our study. In it, the user enters a prospective negotiation over an item with a seller and asks about the amount that should be offered. The prompt varies the seller's name and implicit race association.  The scenario was selected due to its straightforward prompt design and interpretability, as well as its observed disparities. Variations within this and other scenarios were created by modifying the item involved with the goal of ensuring a diverse set of prompts, reducing the risk of overfitting to specific cases.

% To identify these variations, we prompt GPT-4o to generate a list of the top 50 products that potentially exhibit the highest racial disparities. Among these 50 products, we filtered out 20 products with for which a model would yield invalid responses frequently. We then assessed baseline disparities in the remaining 30 products through prompting over 100 iterations. Finally, we selected the 10 products that showed the most significant disparities. Details of this process can be found in Appendix \ref{}.

Following~\textcite{haim2024whatsnameauditinglarge}, we adopt the \textit{Purchase} scenario as our baseline, in which a user negotiates with a seller and seeks advice on an appropriate offer. The prompt varies the seller's name, introducing implicit racial associations. We select $N=10$ products/variations with large baseline disparities (for details on the selection process, see~\Cref{app-prompt-selection}).

For each of the selected $N=10$ variations, we repeat the procedure outlined in~\autoref{eq:set_diff}, generating the set of influential components $\mathcal{D}_k$ for each prompt variation $k$:

\begin{equation}
    \{\mathcal{D}_1, \mathcal{D}_2, \dots, \mathcal{D}_{N}\}
\end{equation}

Each set $\mathcal{D}_k$ aims to capture the components that are uniquely influential for Black names in the $k^\text{th}$ prompt variation. To evaluate their impact, we prune the identified components in $\mathcal{D}_k$ from the model and assess the pruned model's performance on the corresponding $k^\text{th}$ prompt variation. This pruning is applied individually for each variation to observe how the removal of these components affects the model’s responses within the specific context of that prompt.

\subsubsection{Within-Context Generalization}

To assess the stability and consistency of these influential components across similar prompt variations, we perform a leave-one-out analysis. This step evaluates whether the identified components are sensitive to minor contextual changes or if they represent a more general, albeit still within-context, pattern. For each prompt variation $k$, we take the intersection of the remaining $N-1=9$ sets:


\begin{equation}
    \mathcal{D}_k^{\text{LOO}} = \bigcap_{\substack{i=1 \\ i \neq k}}^{N} \mathcal{D}_i
\end{equation}

This results in $N$ refined sets, denoted as $\{\mathcal{D}_1^{\text{LOO}}, \mathcal{D}_2^{\text{LOO}}, \dots, \mathcal{D}_{N}^{\text{LOO}}\}$, where each $\mathcal{D}_k^{\text{LOO}}$ captures the components consistently identified across all variations except the $k^\text{th}$ one. This approach helps determine whether the selected components exhibit robustness within the same context, ensuring they are not solely driven by isolated prompt structures.

This process results in $N$ refined sets, denoted as $\{\mathcal{D}_1^{\text{LOO}}, \mathcal{D}_2^{\text{LOO}}, \dots, \mathcal{D}_{N}^{\text{LOO}}\}$, where each $\mathcal{D}_k^{\text{LOO}}$ captures the components consistently identified across all variations except the $k^\text{th}$ one. To evaluate the practical impact of these refined component sets, we prune the components in $\mathcal{D}_k^{\text{LOO}}$ and measure the model’s performance on the corresponding $k^\text{th}$ prompt, observing changes in the resulting disparities.

\subsubsection{Cross-Context Generalization}

To further challenge the generalizability of the identified components across distinct and diverse prompt contexts, we identify $3$ additional scenarios: \textit{Services}, \textit{Activities}, and \textit{Finance}. In each, we again identify $3$ variations with high baseline disparities, resulting in a dataset of $M=9$ unique variations. Further details on these scenarios are provided in~\Cref{app-promptDesign}. This step examines whether the components consistently exhibit biased behavior across a broader range of inputs. Applying the same method, we compute the intersection of the newly derived sets:

\begin{equation}
    \mathcal{D}^{\text{ctx}} = \bigcap_{j=1}^{M} \mathcal{D}_j^{\text{new}}
\end{equation}

The resulting set $\mathcal{D}^{\text{ctx}}$ contains components that consistently influence responses across varying contexts. To evaluate their generalizability, we prune the components in $\mathcal{D}^{\text{ctx}}$ and analyze the model’s performance on the original $10$ purchase prompt variations, assessing whether the removal of these cross-context components affects disparities within the \textit{Purchase} scenario.

\subsection{Parameter Selection}
% An important choice for our approach are the parameters $b$ and $w$. These represent the thresholds to retrieve the highest scored top b\% for Black heads or neurons and top w\% for white heads or neurons. For the attention-head analysis, we select $b^*=40$ $w^*=5$, whereas for neurons the optimized parameters were $b^*=0.40$ $w^*=0.35$. This is based on an evaluation across [choices] (for details, see appendix).

An important consideration in our approach is the choice of the parameters $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ defined in~\Cref{sec:bias-localization}. These parameters determine the thresholds for selecting the most influential attention heads or neurons. In our analysis, we consider the minority and majority groups in the context of racial bias, specifically focusing on Black- and white-associated names as representatives of these groups. 
For neurons, we define $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ as the percentages of the highest-scored neurons for Black-associated and white-associated names, respectively. This approach ensures that we capture the most impactful neurons in relation to each demographic group. In the case of attention heads, $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ are defined as fixed raw counts of the top-ranking heads for Black-associated and white-associated names, reflecting a more discrete selection criterion. 
These parameters directly influence the construction of the set $\mathcal{D}$, which is formally defined in~\Cref{sec:bias-localization}, and play a crucial role in our bias localization and mitigation strategy.


% An important consideration in our approach is the choice of the parameters $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ defined in~\Cref{sec:bias-localization}, which determine the thresholds for selecting the most influential attention heads or neurons. Specifically, for neurons, we let $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ represent the percentage of the highest-scored neurons for Black and white names, respectively. For attention heads, we let $\tau_{\text{min}}$ and $\tau_{\text{maj}}$ represent fixed raw counts of the top-ranking heads for Black and white names. These parameters directly influence the construction of the set $\mathcal{D}$, which is also defined in~\Cref{eq:bias-localization}. 


% The set $\mathcal{D}$ consists of attention heads that are ranked among the top $b$ for Black names but do not appear in the top $w$ for white names. This selection process aims to identify heads that disproportionately influence the processing of Black names, thus aligning with our objective of mitigating biased model behavior.

For attention-head pruning, the optimal values determined through empirical evaluation are $\tau_{\text{min}}^* = 40$ and $\tau_{\text{maj}}^* = 5$, while for neuron-level pruning, the optimized parameters are $\tau_{\text{min}}^* = 0.40$ and $\tau_{\text{maj}}^* = 0.35$. These values were selected based on an evaluation across multiple threshold choices, ensuring a balance between bias mitigation and high inlier ratio. Additional details on the evaluation process and selection criteria are provided in~\Cref{app-bw}.