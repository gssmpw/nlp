\documentclass[11pt]{article}
\usepackage{authblk}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{comment}
\usepackage[margin=1.25in]{geometry}
\usepackage{appendix}
%\usepackage[natbib=true]{biblatex}
\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{references.bib}
\usepackage{todonotes}
\usepackage{soul}
\usepackage{tablefootnote}
\usepackage{pdflscape}
\usepackage{placeins}
\usepackage{longtable}
\usepackage{nameref}
\usepackage{adjustbox}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{makecell, caption, booktabs, multirow, siunitx}
\renewcommand\theadfont{\bfseries}
\renewcommand\cellalign{lc}
\newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{amsfonts}
%\usepackage[
%backend=biber,
%style=numeric, % Change the citation style to numeric
%]{biblatex}
%\addbibresource{references.bib} %Import the bibliography file

% \usepackage{caption}


\title{Breaking Down Bias: On The Limits of Generalizable Pruning Strategies}

\author[1]{Sibo Ma\thanks{Joint first authors with equal contribution.}}
\author[1]{Alejandro Salinas\thanks{Joint first authors with equal contribution.}}
\author[2]{Peter Henderson}
\author[1]{Julian Nyarko}
\affil[1]{Stanford University}
\affil[2]{Princeton University}
% \footnotetext[1]{Joint first authors with equal contribution.}

% Joint first authors with equal contribution.

\date{\today}


\begin{document}
\maketitle

\begin{abstract}
We employ model pruning to examine how LLMs conceptualize racial biases, and whether a generalizable mitigation strategy for such biases appears feasible. Our analysis yields several novel insights. We find that pruning can be an effective method to reduce bias without significantly increasing anomalous model behavior. Neuron-based pruning strategies generally yield better results than approaches pruning entire attention heads. However, our results also show that the effectiveness of either approach quickly deteriorates as pruning strategies become more generalized. For instance, a model that is trained on removing racial biases in the context of financial decision-making poorly generalizes to biases in commercial transactions. Overall, our analysis suggests that racial biases are only partially represented as a general concept within language models. The other part of these biases is highly context-specific, suggesting that generalizable mitigation strategies may be of limited effectiveness. Our findings have important implications for legal frameworks surrounding AI. In particular, they suggest that an effective mitigation strategy should include the allocation of legal responsibility on those that deploy models in a specific use case.
\end{abstract}



\input{introduction}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{pipeline.png}
    \caption{Illustration of our pruning-based bias mitigation method. Initially, the unpruned model (red) exhibits disparities in responses to prompts associated with different racial groups. For example, in the \textit{Purchase} scenario, the model suggests significantly different price estimates when prompted with a white-associated name (\textit{Hunter Becker}) versus a Black-associated name (\textit{Jamal Washington}). To address this, we localize the model components that are most influential for the majority (green) and minority group (blue) prompts. Components uniquely influential to the minority group are identified and pruned (i.e. zeroed out), with the goal of reducing bias. The pruned model (red) demonstrates similar responses across groups, as shown in the final price distributions.}
    \label{fig:enter-label}
\end{figure}


\input{literature}
 
\input{method}

\input{evaluation_setup}

\input{results}




    
\section{Discussion}    

Our results lend support to an emerging consensus (along with concurrent work ~\cite{durmus2024steering}) that \emph{domain-specific} adaptations ---- not merely broad, ``one-size-fits-all'' mitigations---play an important role in effectively reducing biased outputs from generative models.
As illustrated by our evaluations, these models often encode biases in ways that are heavily intertwined with particular domains (e.g., bias in financial decision-making vs. bias in commercial transactions). 
When pruned using training data from one domain, the improvements did not fully transfer to another domain.
As far as current pruning methods are able to identify, there is no ``bias neuron'' that will affects outputs equally across scenarios.
These findings speak directly to an ongoing legal and policy debate about which party should be incentivized to address potential mechanisms for discrimination in the model. 
The open question, in particular, is whether legal liability should be assigned primarily to the \emph{developers} of a general-purpose model or to the \emph{deployers} who adapt the model for specialized use. Two examples reflect this debate.

First, our observations resonate with key provisions in the proposed EU AI Act, which adopts a {risk-based} approach for regulating AI systems \cite{eu2021aiact}. 
The Act mandates that deployers of models for ``high-risk'' applications (e.g., medical devices, hiring tools) face additional duties for monitoring, auditing, and mitigating discriminatory impacts. By contrast, general purpose AI developers are subject to a different set of requirements without having to address every downstream use.
Our findings suggest that the EU AI Act's emphasis on use-specific deployments is the more effective target, since those providers will be able to better identify sources of bias for the specific application setting.

Second, the domain-specificity of bias also has particular relevance for emerging employment discrimination cases in the United States. In \textit{Mobley v. Workday}, plaintiffs argued that the resume-screening service providers (like Workday) should be liable for discrimination like downstream employers. The court agreed, refusing to ``draw an artificial distinction between software decision-makers and human decision-makers,'' as this would ``gut anti-discrimination laws in the modern era.'' In this setting, too, a key question is then whether this liability would extend further to general purpose model providers selling services to Workday. 
Our findings suggest that the general purpose model provider may not be as effective as a target---unless the provider can obtain domain-specific data from the deployer's uses.


\section{Limitations}
% UTILITY, MODEL SPECIFIC, narrow bias definition, method dependent conclusion, smd bad distribution,  

While our study provides insights into mitigating racial bias in language models, it has several limitations.
The conclusions drawn from our work are closely tied to the specific pruning strategies and evaluation metrics employed. Although we observe a consistent trend of reduced effectiveness as generalization increases, the extent of this trade-off may vary depending on the underlying model architecture and dataset. 
On the measurement side, our reliance on Standardized Mean Difference (SMD) to quantify disparities introduces certain limitations. While SMD effectively summarizes mean differences between demographic groups, it may not always capture critical distributional aspects such as variance, skewness, and the presence of outliers. These limitations can obscure important nuances in the modelâ€™s outputs and make the final SMD values sensitive to extreme values, potentially impacting robustness. 
To address this concern, in \Cref{app-robustness}, we explore the Wasserstein distance as an alternative measure, and find that they produce similar trends and conclusions, reinforcing the reliability of our findings.
Lastly, it is important to note that our evaluation primarily focuses on a specific form of racial bias associated with names in an advice-seeking context. While the methodology we introduce is quite general, the results from the specific evaluation are thus limited in scope. Among others, we do not assess other forms of bias a language model may display, such as implicit associations~\cite{kotek2023gender}. Our method is also tailored to making binary comparisons between two groups. More work is needed to extend the methodology in order to allow it to capture the full and diverse breadth of identities a users may hold.


\printbibliography

\newpage
\appendix
\input{appendix}

\end{document}