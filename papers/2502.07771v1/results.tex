\section{Results}

Overall, we find that specific context information allows for the effective reduction of bias in many scenarios, without creating a significant increase in outlier model behaviors (\cref{sec:behavior}). More generalized approaches, while still mitigating bias, do so to a lesser extent when compared to context-specific methods (\cref{sec:gen}). At least when leveraging the methods we examine, our findings suggest that pruning might be a useful mechanism for reducing bias in narrow contexts, but there is no universal ``bias neuron'' that drives disparate outcomes universally.



\subsection{Pruned Models Behavior}
\label{sec:behavior}

In~\Cref{fig-neuron-scatter}, the top panel depicts the SMDs across the ten variations of the \textit{Purchase} scenario, comparing the unpruned baseline (green) against the three pruning approaches. Without any pruning strategy, the model suggests drastically higher prices for white-associated names than for Black-associated ones, making it evident that the unpruned model exhibits the largest negative shift (mean $\approx-0.59$). After applying the ``Prompt-Specific'' Neuron Pruning approach (orange), we observe that SMDs are close to zero (mean $\approx+0.07$) suggesting that it effectively reduces the targeted bias. This approach represents the best-case scenario and the method may overfit to the specific context, leading to optimal but potentially not generalizable performance. Similarly, the same approach for Attention Head Pruning mitigates bias, albeit to a smaller extent (mean $\approx-0.17$).

The ``Within-Context'' (blue) and ``Cross-Context'' (brown) pruning approaches for both Neuron and Attention Head pruning also reduce bias, although to a lesser extent (means of $\approx-0.20$ and $\approx-0.37$, respectively for Neuron; and means of $\approx-0.31$ and $\approx-0.57$, correspondingly for Attention Head). These distributions show that a pruning approach tailored to a particular bias context can substantially mitigate bias, whereas more generalized pruning offers only partial improvement. Additionally, our findings also shows that Neuron pruning outperforms Attention Head pruning in every approach we tested.

In the bottom panel, the inlier ratio remains high ($\geq 0.98$) for all variations and pruning strategies for both Neurons and Attention Heads, indicating that pruning does not drastically diminish the model's functionality that we are testing in this study. Notably, the "Cross-Context" approach has the highest mean inlier ratio ($\approx1.0$ for both methods), consistent with it being the strategy with the fewest alterations to the model components.

Together, the findings illustrate the trade-off between bias reduction and preserving the studied model functionality. They also highlight the trade-off between specificity and generalizabilityâ€”while all pruning methods appear to reduce biases relative to the unpruned model, the largest gains occur when the pruning is closely tailored to the particular context of the bias in question. Considering the decreases in broader, more diverse contexts, these findings also underscore the need for more adaptable bias mitigation strategies. Finally, we show that Neuron Pruning appears to be a more effective strategy for mitigating biases than Attention Head Pruning.



\begin{figure}[t]
  \centering
  % first image with sublabel
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{neuron_figures/smd_inlier_combined_updated.png}
    \caption{Neuron pruning}
    \label{fig:sub-neuron}
  \end{subfigure}
  \hspace{0.02\textwidth}
  % second subfigure
  \begin{subfigure}[b]{0.47\textwidth}
      \centering
      \includegraphics[width=\textwidth]{head_figures/utility_new_wanda_head_LAST.png}
      \caption{Attention Head Pruning on Bias and Utility}
      \label{fig:sub-head}
  \end{subfigure}
  
  \caption{Impact of Neuron and Attention Head Pruning on Bias and Utility. The top panels present the Standardized Mean Difference (SMD) scores across ten variations of the \textit{Purchase} scenario, comparing the unpruned baseline (green) with three pruning approaches: Prompt-Specific (orange), Within-Context (blue), and Cross-Context (brown). Vertical dashed lines indicate the mean SMD for each approach. The bottom panels illustrate the inlier ratio across all variations and pruning methods, measuring the model's ability to generate reasonable outputs post-pruning.}
  \label{fig-neuron-scatter}
\end{figure}


\subsection{Generalizability through the Lens of Shared Pruned Neurons}
\label{sec:gen}

\Cref{fig-hm-sce_var} visualizes the overlap between the biased neurons in our \textit{Purchase} scenario and from other scenarios. Specifically, the heat is defined as a fraction, with the numerator being the intersection of pruned neurons between every scenario's variation and each \textit{Purchase} variations. The denominator is the total size of pruned neurons for the corresponding scenario's variation. The overlap ranges from around 0.12 to 0.16. Two main patterns emerge. First, there is heterogeneity in overlap, with the biased neurons identified in \textit{medical}, \textit{tax preparation} and \textit{personal cheffing} consistently showing the highest similarities to the \textit{Purchase} variations. These 3 variations in particular are part of the \textit{Services} scenario. Second, other scenario variations (e.g., ``bird watching,'' ``skiing,'' and ``pottery'') which are all part of the \textit{Activities} scenario share fewer pruned neurons (around 0.12-0.13). These small percentages, specially when compared to the amount of neurons shared across variations within-context (see~\Cref{app-shared-neurons}) support our findings from~\Cref{fig-neuron-scatter}, where we see a decreasing effectiveness of pruning as the contextual difference increases.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\linewidth]{neuron_figures/hm_sce_var.png}
  \caption{Overlap in biased neurons between \textit{Purchase} variations and variations from other scenarios. Heat is defined as a fraction, with the numerator being the intersection of pruned neurons between every scenario's variation and each \textit{Purchase} variations. The denominator is the total size of pruned neurons for the corresponding scenario's variation. Higher values indicate stronger overlap. }
  \label{fig-hm-sce_var}
\end{figure}





\subsection{Location of Pruned Neurons}

\Cref{fig-hm-layers} presents a layer-by-layer heatmap illustrating how pruned neurons are distributed across both the network's subcomponents (q, k, v, gate, up, down) and layer indices (0-31). The heat represents the percentage of neurons pruned at each location, normalized by the total neuron count in that location. Warmer regions indicate that a higher fraction of neurons were pruned. The visualization demonstrates that neuron pruning is not uniform; there is a tendency for certain layers and sub-components to have consistently higher percentages of pruned neurons than others.

In particular, the attention sub-components (q,k,v) near the top of the \Cref{fig-hm-layers} exhibit relatively lighter coloration, suggesting fewer neurons pruned at those locations. This behavior is consistent throughout all the network's layer indices. In contrast, the MLP sub-component (gate, up, down) show more intense reds, indicating that a substantial fraction of pruned neurons originate from these sub-components. 

A line plot (blue) depicts the the neurons pruned at a given layer index, divided by the total number of neurons pruned. It shows a mild upwards trend, suggesting that the distribution of pruned neurons is a bit more concentrated towards the mid-to-late layers. These findings are broadly consistent with those by \textcite{adiga2024attention}, who found that bias tends to be more concentrated in the later layers. At the same time, the skew in our analysis is less pronounced than in their findings.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\linewidth]{neuron_figures/layers_heatmap.png}
  % \caption{Neuron Pruning Distribution across Layers}
  \caption{Neuron Pruning Distribution across Layers and Subcomponents. This heatmap illustrates the distribution of pruned neurons across different layers (0-31) and network subcomponents (q, k, v, gate, up, down). The color intensity represents the proportion of pruned neurons relative to the total count in each location, with warmer colors indicating more pruning. The blue line shows the contribution of each layer to the total number of pruned neurons.}

  % \Description{Heatmap of layer level localization of pruned neurons}
  \label{fig-hm-layers}
\end{figure}