\section{Related Work}
\subsection{Generative Speech Enhancement}
Generative speech enhancement learns the distribution of clean speech as a prior for enhancement, rather than directly mapping noisy speech to clean speech. Most of generative models have been applied in SE, including generative adversarial networks (GANs)~\citep{fu2019metricgan,liu2021voicefixer}, variational autoencoders (VAEs)~\citep{vae_ref1,vae_ref2}, flow-based models~\citep{flow_ref}, and diffusion probabilistic models~\citep{lemercier2023storm,tai2024dose,yang2024is_diffse,scheibler2024is_diffse}. 
A series of studies~\citep{le2024voicebox,yang2023uniaudio,wang2024speechx} have leveraged large-scale datasets to develop audio generation models capable of handling multiple tasks, such as speech synthesis~\citep{zhang2024speaking}, voice conversion~\citep{yao2024promptvc,yao2024stablevc}, and speech recognition~\citep{liu2024aligning}. While these models demonstrate versatility, their performance may be constrained compared to systems specifically optimized and designed for a particular task.
Meanwhile, with the growing popularity of discrete representations extracted from neural audio codecs, some works have begun to use discrete tokens for speech enhancement. 
Genhancer~\citep{yanggenhancer} argues that discrete codec tokens offer an efficient latent domain, which can replace the conventional time or time-frequency domain of signals, allowing for more complex modeling of speech.
Another work, called SELM~\citep{wang2024selm}, is a pioneer in exploring using LMs for speech enhancement. It also employs discrete semantic tokens as the speech representation and incorporates LMs to generate clean semantic tokens conditioned on noisy input. However, these approaches still fall short in terms of speech quality and similarity compared to clean speech. 
% They also suffer from degraded generalization performance when dealing with unseen types of noise.



\subsection{Speech Tokenization}

There are two widely used speech tokens in speech language models: semantic tokens and acoustic tokens. Semantic tokens are typically derived from representations produced by self-supervised speech models like HuBERT~\citep{hsu2021hubert} or WavLM~\citep{chen2022wavlm}, and they primarily capture the phonetic and semantic content of speech signals. While semantic tokens are initially designed as training targets for self-supervised models, recent efforts have used them to directly represent high-level semantic information~\citep{borsos2023audiolm}. Acoustic tokens, on the other hand, are extracted from neural audio codec models, which tokenize high-rate audio signals into a finite set of tokens~\citep{defossez2022encodec}. AudioLM~\citep{borsos2023audiolm} pioneered the use of LMs for audio generation by employing both semantic and acoustic tokens and building several LMs across different stages. VALL-E~\citep{wang2023valle} further extended the AudioLM framework and applied it to text-to-speech (TTS), achieving impressive zero-shot voice cloning. Inspired by VALL-E, several speech-to-speech generation tasks, such as speech translation~\citep{dong2023polyvoice} and voice conversion~\citep{wang2023lmvc}, have replaced phonemes with semantic tokens and introduced powerful LMs to achieve remarkable results.

\subsection{Neural Audio Codec}
% 介绍codec模型，码本数量，rvq，codebook size
Previous neural audio codec models have been widely used in audio communication to compress audio into discrete representations. Recently, these codec models have gained interest in speech generation tasks, bridging continuous speech and token-based language models. A typical codec model consists of an encoder, a quantization module, and a decoder, with the quantization module playing a crucial role in tokenizing continuous speech features into discrete tokens.
Several works explore different architectures to pursue better reconstruction quality~\citep{wu2023audiodec,kumar2024dac,ai2024apcodec}, others enhance the compression rate~\citep{yang2023hifi,ji2024languagecodec,ji2024wavtokenizer} or optimize codec space~\citep{ren2024ticodec,ju2024naturalspeech3,zhang2023speechtokenizer,du2024funcodec,liu2024semanticodec}. 
Most approaches employ residual vector quantization (RVQ), which typically uses eight quantizers, each with a 1024-codebook size, to provide sufficient discrete space for quantization. However, the numerous acoustic tokens generated by these models create significant complexity and challenges in the LM prediction.
Although some works~\citep{li2024singlecodec,ji2024wavtokenizer} explore the use of a single quantizer, their reconstruction quality often falls short or relies on an additional reference encoder to disentangle global representations, which are unsuitable for speech enhancement.