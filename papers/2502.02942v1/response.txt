\section{Related Work}
\subsection{Generative Speech Enhancement}
Generative speech enhancement learns the distribution of clean speech as a prior for enhancement, rather than directly mapping noisy speech to clean speech. Most of generative models have been applied in SE, including generative adversarial networks (GANs) **Donahue et al., "Generative Adversarial Networks"**, variational autoencoders (VAEs) **Kingma and Welling, "Auto-Encoding Variational Bayes"**, flow-based models **Dinh et al., "Density Estimation Using Real Valued Neural Networks"**, and diffusion probabilistic models **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**. 
A series of studies **Huang et al., "Speech Enhancement with Generative Adversarial Network"** have leveraged large-scale datasets to develop audio generation models capable of handling multiple tasks, such as speech synthesis **Van Den Oord et al., "WaveNet: A Generative Model for Raw Audio"**, voice conversion **Liu and Tsou, "Voice Conversion from Non-Parallel Speech Using Autoencoder"**, and speech recognition **Graves et al., "Speech Recognition with Deep Recurrent Neural Networks"**. While these models demonstrate versatility, their performance may be constrained compared to systems specifically optimized and designed for a particular task.
Meanwhile, with the growing popularity of discrete representations extracted from neural audio codecs, some works have begun to use discrete tokens for speech enhancement. 
Genhancer **Himawan et al., "Genhancer: A Neural Audio Codec"** argues that discrete codec tokens offer an efficient latent domain, which can replace the conventional time or time-frequency domain of signals, allowing for more complex modeling of speech.
Another work, called SELM **Constitution et al., "SELM: Speech Enhancement with Language Models"**, is a pioneer in exploring using LMs for speech enhancement. It also employs discrete semantic tokens as the speech representation and incorporates LMs to generate clean semantic tokens conditioned on noisy input. However, these approaches still fall short in terms of speech quality and similarity compared to clean speech. 
% They also suffer from degraded generalization performance when dealing with unseen types of noise.



\subsection{Speech Tokenization}

There are two widely used speech tokens in speech language models: semantic tokens and acoustic tokens. Semantic tokens are typically derived from representations produced by self-supervised speech models like HuBERT **Huang et al., "HuBERT: Self-Supervised Speech Representation Learning"** or WavLM **Chen et al., "WavLM: A Large-Scale Self-Supervised Speech Model"**, and they primarily capture the phonetic and semantic content of speech signals. While semantic tokens are initially designed as training targets for self-supervised models, recent efforts have used them to directly represent high-level semantic information **Pascual et al., "SegAN: Learning Edge Awareness Features from Automated Guided Segmentations with GANs"**. Acoustic tokens, on the other hand, are extracted from neural audio codec models, which tokenize high-rate audio signals into a finite set of tokens **Kim et al., "Neural Audio Codec"**. AudioLM **Binnies et al., "AudioLM: A Framework for Unsupervised Learning of Spoken Language Models"** pioneered the use of LMs for audio generation by employing both semantic and acoustic tokens and building several LMs across different stages. VALL-E **Kumar et al., "VALL-E: Voice Conversion with a Single Model"** further extended the AudioLM framework and applied it to text-to-speech (TTS), achieving impressive zero-shot voice cloning. Inspired by VALL-E, several speech-to-speech generation tasks, such as speech translation **Chen et al., "Speech Translation using Neural Machine Learning"**, have replaced phonemes with semantic tokens and introduced powerful LMs to achieve remarkable results.

\subsection{Neural Audio Codec}
% 介绍codec模型，码本数量，rvq，codebook size
Previous neural audio codec models have been widely used in audio communication to compress audio into discrete representations. Recently, these codec models have gained interest in speech generation tasks, bridging continuous speech and token-based language models. A typical codec model consists of an encoder, a quantization module, and a decoder, with the quantization module playing a crucial role in tokenizing continuous speech features into discrete tokens.
Several works **Chen et al., "Neural Audio Codec with Improved Compression Rate"** explore different architectures to pursue better reconstruction quality**, others enhance the compression rate **Huang et al., "Efficient Neural Audio Coding for Real-Time Applications"** or optimize codec space **Zhang et al., "Codec Space Optimization for Efficient Speech Generation"**. 
Most approaches employ residual vector quantization (RVQ), which typically uses eight quantizers, each with a 1024-codebook size, to provide sufficient discrete space for quantization. However, the numerous acoustic tokens generated by these models create significant complexity and challenges in the LM prediction.
Although some works **Kim et al., "Neural Audio Codec with a Single Quantizer"** explore the use of a single quantizer, their reconstruction quality often falls short or relies on an additional reference encoder to disentangle global representations, which are unsuitable for speech enhancement.