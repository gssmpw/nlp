\section{Experiments}
\label{sec:experiments}

\begin{table}[t]
  \caption{Main results on the ScanNet-Reflection Test and Validation sets.}
  \label{tab:reflection}
  \begin{center}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}c|lllccccccc@{}}
  % SCANNET REFLECTION TEST
  \toprule
    \multicolumn{2}{c}{~Backbone} & Training Scheme & Method & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    % \cmidrule{2-11}
    \midrule
    \multirow{15}{*}{\rotatebox[origin=c]{90}{\textbf{ScanNet-Reflection Test}}} & \multirow{5}{*}{Monodepth2} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.181 & 0.160 & 0.521 & 0.221 & 0.758 & 0.932 & 0.976 \\
    % \cmidrule{3-10}
    & & & \textit{Ours} & \textbf{0.157} & \textbf{0.096} & \textbf{0.468} & \textbf{0.201} & \textbf{0.762} & \textbf{0.949} & \textbf{0.988} \\
    \cmidrule{3-11}
    & & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.179 & 0.146 & 0.502 & 0.218 & 0.750 & 0.938 & 0.980 \\
    & & & 3D Distillation & 0.156 & 0.096 & 0.459 & 0.195 & 0.766 & 0.945 & 0.988 \\
    % \cmidrule{3-10}
    & & & \textit{Ours}$^{\dagger}$ & \textbf{0.150} & \textbf{0.087} & \textbf{0.446} & \textbf{0.192} & \textbf{0.777} & \textbf{0.955} & \textbf{0.990} \\
    \cmidrule{2-11}
    & \multirow{5}{*}{HRDepth} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.182 & 0.168 & 0.530 & 0.225 & 0.749 & 0.937 & 0.979 \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & \textbf{0.162} & \textbf{0.105} & \textbf{0.464} & \textbf{0.203} & \textbf{0.758} & \textbf{0.942} & \textbf{0.985} \\
    & & & \textit{Ours} & \textbf{0.157} & \textbf{0.098} & \textbf{0.470} & \textbf{0.201} & \textbf{0.763} & \textbf{0.952} & \textbf{0.989} \\
    \cmidrule{3-11}
    & & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.175 & 0.145 & 0.492 & 0.215 & 0.757 & 0.936 & 0.982 \\
    & & & 3D Distillation & 0.152 & \textbf{0.089} & 0.451 & \textbf{0.190} & 0.771 & \textbf{0.956} & \textbf{0.990} \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & 0.153 & 0.099 & \textbf{0.449} & 0.195 & \textbf{0.776} & 0.949 & 0.986 \\
    & & & \textit{Ours}$^{\dagger}$ & \textbf{0.150} & 0.092 & \textbf{0.434} & 0.192 & \textbf{0.780} & 0.950 & 0.988 \\
    \cmidrule{2-11}
    & \multirow{5}{*}{MonoViT} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.154 & 0.129 & 0.458 & 0.197 & 0.822 & 0.955 & 0.979 \\
    % \cmidrule{3-10}
    & & & \textit{Ours} & \textbf{0.136} & \textbf{0.087} & \textbf{0.414} & \textbf{0.178} & \textbf{0.831} & \textbf{0.967} & \textbf{0.988} \\
    % 0.136	0.087	0.414	0.178	0.831	0.967	0.988
    % & & \textit{Ours} & \textbf{0.133} & \textbf{0.079} & \textbf{0.410} & \textbf{0.172} & \textbf{0.839} & \textbf{0.965} & \textbf{0.991} \\
    \cmidrule{3-11}
    & & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.151 & 0.130 & 0.439 & 0.191 & 0.837 & 0.950 & 0.978 \\
    & & & 3D Distillation & 0.127 & \textbf{0.069} & \textbf{0.379} & \textbf{0.162} & 0.846 & 0.961 & \textbf{0.992} \\
    % \cmidrule{3-10}
    & & & \textit{Ours}$^{\dagger}$ & \textbf{0.126} & 0.074 & 0.395 & 0.167 & \textbf{0.854} & \textbf{0.969} & 0.990 \\
  % \bottomrule
  % SCANNET REFLECTION VALIDATION
  \toprule
    % \multirow{2}{*}{Backbone} & \multirow{2}{*}{Training Scheme} & \multirow{2}{*}{Method} & \multicolumn{7}{c}{ScanNet-Reflection Val. Set} \\
    % & & & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    % \midrule
    % MONODEPTH2
    \multirow{15}{*}{\rotatebox[origin=c]{90}{\textbf{ScanNet-Reflection Validation}}} & \multirow{5}{*}{Monodepth2} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.206 & 0.227 & 0.584 & 0.246 & 0.750 & 0.912 & 0.961 \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & \textbf{0.169} & \textbf{0.119} & \textbf{0.497} & \textbf{0.214} & \textbf{0.750} & \textbf{0.931} & \textbf{0.981} \\
    & & & \textit{Ours} & \textbf{0.166} & \textbf{0.125} & \textbf{0.492} & \textbf{0.209} & \textbf{0.763} & \textbf{0.934} & \textbf{0.981} \\
    \cmidrule{3-11}
    & & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.192 & 0.188 & 0.548 & 0.233 & 0.764 & 0.920 & 0.967 \\
    & & & 3D Distillation & 0.156 & \textbf{0.093} & \textbf{0.442} & \textbf{0.191} & 0.786 & 0.943 & \textbf{0.987} \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & \textbf{0.155} & 0.110 & 0.463 & 0.197 & \textbf{0.787} & \textbf{0.944} & 0.985 \\
    & & & \textit{Ours}$^{\dagger}$ & \textbf{0.151} & 0.105 & 0.454 & 0.193 & \textbf{0.796} & \textbf{0.944} & 0.985 \\
    \cmidrule{2-11}
    % HRDEPTH
    & \multirow{5}{*}{HRDepth} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.213 & 0.244 & 0.605 & 0.255 & 0.741 & 0.906 & 0.961 \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & \textbf{0.170} & \textbf{0.126} & \textbf{0.496} & \textbf{0.212} & \textbf{0.762} & \textbf{0.932} & \textbf{0.981} \\
    & & & \textit{Ours} & \textbf{0.167} & \textbf{0.127} & \textbf{0.496} & \textbf{0.210} & \textbf{0.770} & \textbf{0.937} & \textbf{0.982} \\
    \cmidrule{3-11}
    & & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.202 & 0.208 & 0.565 & 0.243 & 0.756 & 0.914 & 0.964 \\
    & & & 3D Distillation & 0.153 & \textbf{0.090} & \textbf{0.430} & \textbf{0.188} & 0.789 & 0.948 & \textbf{0.989} \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & 0.155 & 0.115 & 0.463 & 0.196 & \textbf{0.794} & \textbf{0.948} & 0.986 \\
    & & & \textit{Ours}$^{\dagger}$ & \textbf{0.151} & 0.104 & 0.450 & 0.192 & \textbf{0.800} & \textbf{0.949} & 0.987 \\
    \cmidrule{2-11}
    % MONOVIT
    & \multirow{5}{*}{MonoViT} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.179 & 0.206 & 0.557 & 0.227 & 0.819 & 0.930 & 0.963 \\
    % \cmidrule{3-10}
    & & & \textit{Ours} & \textbf{0.139} & \textbf{0.107} & \textbf{0.452} & \textbf{0.183} & \textbf{0.836} & \textbf{0.954} & \textbf{0.984} \\
    % 0.139	0.107	0.452	0.183	0.836	0.954	0.984
    % & & \textit{Ours} & \textbf{0.142} & \textbf{0.113} & \textbf{0.453} & \textbf{0.183} & \textbf{0.840} & \textbf{0.953} & \textbf{0.984} \\
    \cmidrule{3-11}
    & & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.176 & 0.195 & 0.537 & 0.224 & 0.823 & 0.930 & 0.963 \\
    & & & 3D Distillation & \textbf{0.126} & \textbf{0.068} & \textbf{0.367} & \textbf{0.159} & \textbf{0.851} & \textbf{0.965} & \textbf{0.991} \\
    % \cmidrule{3-10}
    & & & \textit{Ours}$^{\dagger}$ & 0.130 & 0.091 & 0.420 & 0.173 & \textbf{0.851} & 0.960 & 0.987 \\
  \bottomrule
  \end{tabular}}
  \end{center}
  % \vspace{-7pt}
\end{table}

\paragraph{Datasets.}

\textbf{\textit{ScanNet (v2)}}~\citep{dai2017scannet} is a comprehensive indoor RGB-D video dataset comprising 2.7 million images across 1,216 interior scene sequences.
Traditionally, this dataset has been pivotal for evaluating multi-view stereo~\citep{im2019dpsnet, bae2022multi} and scene reconstruction applications~\citep{murez2020atlas, zhou2024neural}.
\textbf{\textit{KITTI}}~\citep{geiger2013vision} captures autonomous driving information from outdoor scenes via cameras and lidar sensors and is the most representative and long-standing benchmark for depth estimation research.
\textbf{\textit{NYU-v2}}~\citep{silberman2012indoor} serves as one of the most established and widely used benchmarks for depth estimation, as it provides a diverse set of indoor scenes with varying lighting conditions and a wide variety of objects, captured using a Kinect sensor.
\textbf{\textit{7-Scenes}}~\citep{shotton2013scene} is a challenging RGB-D dataset captured in indoor scenes with a similar distribution to \textit{ScanNet} but dominated by non-reflective surfaces.
We follow the evaluation protocol of~\citet{long2021multi, bae2022multi} to demonstrate the cross-dataset generalization performances.
\textbf{\textit{Booster}}~\citep{ramirez2023booster} includes a variety of non-Lambertian objects within indoor settings, such as transparent basins, mirrors, and specular surfaces.
Following the~\citet{costanzino2023learning}, we use the training split as our test set, which showcases our methodâ€™s adaptability to more complex scenes.

\paragraph{Training scenario.}
In the work of 3D distillation~\citep{shi20233d}, the ScanNet dataset has been further segmented into ScanNet-Reflection and ScanNet-NoReflection subsets based on the presence of reflective objects within the scenes.
This subdivision results in a ScanNet-Reflection dataset consisting of 45,539 training, 439 validation, and 121 testing samples.
Additionally, a ScanNet-NoReflection validation set comprising 1,012 samples evaluates the model's generalization when trained in reflective environments.
Aligning with these methodologies, the training process leverages the ScanNet-Reflection train set to simulate real-world scenarios involving reflective surfaces.
For the KITTI and NYU-v2 experimental setups, we follow the training protocol of \citet{godard2019digging}, incorporating our reflection-aware triplet loss and distillation training procedure.
All experiments conducted on KITTI and NYU-v2 utilized a ResNet-18 pose network instead of a GT pose.

\paragraph{Evaluation.}
For quantitative evaluations, we employ standard metrics from the depth estimation literature~\citep{eigen2014depth, geiger2012we}.
We differentiate our training approaches into end-to-end and multi-stage (distillation) strategies to effectively assess the models.
The model trained solely using reflection-aware triplet mining loss $\mathcal{L}_{tri}$, referred to as ``\textit{Ours}'', and another utilizing the proposed distillation method $\mathcal{L}_{rkd}$, referred to as ``\textit{Ours}$^{\dagger}$'', are evaluated under their respective conditions.
We compare these against both end-to-end and multi-stage baselines across three sets: ScanNet-Reflection \{Test, Validation\} sets, and ScanNet-NoReflection Validation set.
For the KITTI~\citep{geiger2013vision} and NYU-v2~\citep{silberman2012indoor} datasets, we follow the standard evaluation protocol of \citet{godard2019digging}.
To underline the cross-dataset generalizability of our methods, we also perform zero-shot evaluations on the 7-Scenes and Booster.

\paragraph{Implementation details.}
Our experiments incorporate three leading architectures in SSMDE: \textbf{\textit{Monodepth2}}~\citep{godard2019digging}, \textbf{\textit{HRDepth}}~\citep{lyu2021hr}, and \textbf{\textit{MonoViT}}~\citep{zhao2022monovit}, which have demonstrated exceptional performance in previous studies.
Each backbone is trained by different training schemes, including Self-Supervised~\citep{godard2019digging}, Self-Teaching~\citep{poggi2020uncertainty}, and 3D Distillation~\citep{shi20233d}, to compare with our method.
To align closely with 3D Distillation, all training particulars follow their documented conditions, with adaptations only in our proposed training strategy.
Specifically, the models are trained using the reflection triplet split introduced in 3D Distillation.
To finely tune the margin $\delta$ across positive and negative pairs, it is adaptively selected based on the difference between the first quartile (Q1) of $\mathbf{E}^{+}$ and the third quartile (Q3) of $\mathbf{E}^{-}$.

\subsection{Evaluation on reflection datasets}
\paragraph{ScanNet-Reflection dataset.}
To demonstrate the effectiveness of the proposed method on reflective surfaces, we conduct a quantitative evaluation using the ScanNet-Reflection dataset. The evaluations are divided into end-to-end and multi-stage methodologies.
As depicted in \tabref{tab:reflection}, \textit{Ours}, categorized under end-to-end training schemes, significantly outperforms self-supervised methods across all backbones, achieving an \textit{Abs Rel} average increase of 12.90\% in the test split and 21.12\% in the validation split.
Moreover, it is noteworthy that \textit{Ours} shows a significant performance boost, with an average improvement of 10.75\% over Self-Teaching across all metrics in both the test and validation splits, with only two exceptions in 42 metrics ($\delta < 1.25$ of Monodepth2 and MonoViT).
This demonstrates that our reflection-aware triplet mining loss is effective in detecting reflective surfaces and encourages the model to obtain accurate depth on these surfaces as shown in \Figref{fig:qualitative}.
Additionally, our multi-stage approach, which employs reflection-aware knowledge distillation (denoted as \textit{Ours$^\dagger$}), delivers comparable results across all backbone models of 3D Distillation.
Note that the proposed method does not require complex scene reconstruction procedures such as mesh rendering~\citep{pyrender, newcombe2011kinectfusion} or ensembles of multiple neural network models~\citep{lakshminarayanan2017simple}. 

\begin{table}[t]
  \caption{Main results on the ScanNet-NoReflection Validation set.}
  \label{tab:noreflection}
  \begin{center}
  \resizebox{\textwidth}{!}{
\begin{tabular}{@{}lllccccccc@{}}
  % SCANNET REFLECTION TEST
  \toprule
    Backbone & Training Scheme & Method & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    \midrule
    \multirow{5}{*}{Monodepth2} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.169 & 0.100 & \textbf{0.395} & \textbf{0.206} & \textbf{0.759} & \textbf{0.932} & 0.979\\
    % \cmidrule{3-10}
    % & & \textit{Ours} & \textbf{0.168} & \textbf{0.097} & 0.398 & 0.207 & 0.753 & \textbf{0.932} & \textbf{0.980} \\
    & & \textit{Ours} & \textbf{0.168} & \textbf{0.095} & \textbf{0.395} & 0.208 & 0.751 & 0.931 & \textbf{0.980} \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.161 & 0.090 & 0.375 & 0.196 & 0.777 & 0.939 & 0.981\\
    & & 3D Distillation & 0.159 & 0.087 & \textbf{0.373} & \textbf{0.195} & \textbf{0.779} & 0.941 & \textbf{0.983} \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & \textbf{0.158} & \textbf{0.085} & \textbf{0.373} & 0.196 & 0.773 & \textbf{0.941} & \textbf{0.983} \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.157} & \textbf{0.085} & \textbf{0.373} & \textbf{0.195} & 0.776 & \textbf{0.942} & \textbf{0.983} \\
    \midrule
    % HRDEPTH
    \multirow{5}{*}{HRDepth} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.169 & 0.102 & \textbf{0.388} & \textbf{0.202} & \textbf{0.766} & \textbf{0.933} & \textbf{0.980} \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & \textbf{0.163} & \textbf{0.095} & 0.389 & \textbf{0.202} & \textbf{0.769} & \textbf{0.936} & \textbf{0.980} \\
    & & \textit{Ours} & \textbf{0.167} & \textbf{0.096} & 0.389 & 0.204 & 0.764 & \textbf{0.933} & 0.979 \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.160 & 0.089 & 0.367 & 0.192 & 0.784 & 0.941 & 0.982 \\
    & & 3D Distillation & 0.158 & \textbf{0.086} & \textbf{0.365} & \textbf{0.190} & \textbf{0.786} & \textbf{0.942} & \textbf{0.983} \\
    % \cmidrule{3-10}
    % & & \textit{Ours} & 0.159 & 0.089 & 0.370 & 0.194 & 0.784 & 0.939 & 0.981 \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.157} & \textbf{0.086} & 0.366 & 0.192 & 0.784 & \textbf{0.942} & \textbf{0.983} \\
    \midrule
    % MONOVIT
    \multirow{5}{*}{MonoViT} & \multirow{2}{*}{End-to-End} & Self-Supervised & \textbf{0.140} & 0.074 & \textbf{0.333} & \textbf{0.171} & \textbf{0.829} & \textbf{0.952} & 0.984 \\
    % \cmidrule{3-10}
    & & \textit{Ours} & 0.141 & \textbf{0.072} & 0.338 & 0.174 & 0.823 & \textbf{0.952} & \textbf{0.987} \\
    % 0.141	0.072	0.338	0.174	0.823	0.952	0.987
    % & & \textit{Ours} & 0.146 & 0.076 & 0.338 & 0.176 & 0.820 & 0.950 & \textbf{0.985} \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.134 & 0.068 & 0.317 & 0.164 & \textbf{0.840} & 0.956 & \textbf{0.987} \\
    & & 3D Distillation & \textbf{0.133} & \textbf{0.065} & \textbf{0.311} & \textbf{0.162} & 0.838 & 0.956 & \textbf{0.987} \\
    % \cmidrule{3-10}
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.133} & 0.066 & 0.320 & 0.166 & 0.837 & \textbf{0.957} & \textbf{0.987} \\
  \bottomrule
  \end{tabular}}
  \end{center}
\end{table}

\begin{figure}[t]
\begin{center}
  \hspace{-15pt}
  \includegraphics[width=\linewidth]{figure/qualitative.png}
\end{center}
\caption{Qualitative results of the proposed methods on the ScanNet. We visualize the predicted depth of the Monodepth2~\citep{godard2019digging} trained by three different methods including the proposed method: Self-supervised, \textit{Ours} and \textit{Ours}$^{\dagger}$. Note that the error map represents the absolute difference between prediction and ground truth depth, normalized to between 0 and 255.}
\label{fig:qualitative}
\end{figure}

% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

\paragraph{ScanNet-NoReflection dataset.}
\tabref{tab:noreflection} summarizes the results of a quantitative evaluation performed on the ScanNet-NoReflection dataset.
This evaluation aims to measure the generalization performance of models trained on datasets that include reflective surfaces.
In an end-to-end training scheme, \textit{Ours} achieves performance comparable to or within an acceptable margin of self-supervised methods.
This confirms that our proposed reflection-aware triplet mining loss effectively prevents the incorrect back-propagation of the photometric loss gradient on reflective surfaces, as illustrated in \Figref{fig:triplet}.
Furthermore, the model trained by our reflection-aware knowledge distillation (\textit{i.e.}, \textit{Ours}$^{\dagger}$) shows a noticeable performance improvement, which is comparable performance to the 3D distillation method.
These results suggest that extending our reflection-aware triplet mining loss to distillation techniques offers a straightforward yet effective strategy for managing reflective surfaces.


\begin{table}[!t]
  \caption{Main results on the KITTI and NYU-v2 datasets.}
  \label{tab:kitti}
  \begin{center}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}l|lcccccccc@{}}
  \toprule
    \multicolumn{2}{c}{~Backbone} & Method & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    % \cmidrule{2-10}
    \midrule
    % MONODEPTH2
    \multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{KITTI}}} & \multirow{3}{*}{Monodepth2} & Self-Supervised & 0.118 & 0.908 & 4.919 & 0.196 & \textbf{0.871} & 0.957 & 0.980 \\
    & & \textit{Ours} & 0.118 & 0.912 & 4.943 & 0.198 & 0.867 & 0.956 & 0.980 \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.116} & \textbf{0.856} & \textbf{4.796} & \textbf{0.194} & 0.870 & \textbf{0.958} & \textbf{0.981} \\
  \toprule
    \multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{NYUv2}}} & \multirow{3}{*}{Monodepth2} & Self-Supervised & 0.171 & 0.144 & 0.622 & 0.213 & 0.746 & 0.941 & 0.985 \\
    % & & \textit{Ours} & \textbf{0.168} & \textbf{0.097} & 0.398 & 0.207 & 0.753 & \textbf{0.932} & \textbf{0.980} \\
    & & \textit{Ours} & 0.166 & 0.139 & 0.616 & 0.209 & 0.759 & 0.943 & 0.985 \\
    % & \textit{Ours} & 0.421 & 0.291 & 0.495 & 0.384 & \textbf{0.370} & \textbf{0.685} & \textbf{0.898} \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.155} & \textbf{0.121} & \textbf{0.573} & \textbf{0.196} & \textbf{0.782} & \textbf{0.951} & \textbf{0.988} \\
  \bottomrule
  \end{tabular}}
  \end{center}
  % \vspace{-7pt}
\end{table}


\paragraph{KITTI and NYU-v2 datasets.}
\tabref{tab:kitti} presents the quantitative results of the Monodepth2 architecture on the KITTI and NYU-v2 datasets, both widely used for depth estimation.
In the KITTI dataset, most scenes share similar attributes and predominantly consist of non-reflective objects, limiting performance improvements in reflective regions.
However, as highlighted in our study, our selective triplet mining loss using $M_r$ and the distillation process effectively preserves performance in non-reflective areas.
Consequently, our method achieves performance comparable to existing approaches with negligible margins.
These results are consistent with the trends observed in the ScanNet-NoReflection validation split experiments presented in \tabref{tab:noreflection}.
On the other hand, in the NYU-v2 dataset, where reflections are more prevalent, our triplet mining loss and distillation method yield significant performance gains.
These results demonstrate that the broad applicability of our approach to indoor environments with complex light-object interactions, ensuring consistent enhancements even when incorporating a pose network.
% This evaluation aims to measure the generalization performance of models trained on datasets that include reflective surfaces.
% In an end-to-end training scheme, \textit{Ours} achieves performance comparable to or within an acceptable margin of self-supervised methods.
% This confirms that our proposed reflection-aware triplet mining loss effectively prevents the incorrect back-propagation of the photometric loss gradient on reflective surfaces, as illustrated in \Figref{fig:triplet}.
% Furthermore, the model trained by our reflection-aware knowledge distillation (\textit{i.e.}, \textit{Ours}$^{\dagger}$) shows a noticeable performance improvement, which is comparable performance to the 3D distillation method.
% These results suggest that extending our reflection-aware triplet mining loss to distillation techniques offers a straightforward yet effective strategy for managing reflective surfaces.

\begin{table}[t]
  \caption{Cross-dataset evaluation result on the 7-Scenes and Booster datasets.}
  \label{tab:7scenes}
  \begin{center}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}l|llccccccc@{}}
  \toprule
    \multicolumn{2}{c}{~Backbone} & Method & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    % \cmidrule{2-10}
    \midrule
    % MONODEPTH2
    \multirow{9}{*}{\rotatebox[origin=c]{90}{\textbf{7-Scenes}}} & \multirow{3}{*}{Monodepth2} & Self-Supervised & 0.210 & 0.130 & 0.445 & 0.248 & 0.656 & 0.906 & 0.974 \\
    & & \textit{Ours} & 0.207 & 0.125 & 0.441 & 0.248 & 0.656 & 0.904 & 0.975 \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.198} & \textbf{0.110} & \textbf{0.415} & \textbf{0.238} & \textbf{0.667} & \textbf{0.911} & \textbf{0.980} \\
    \cmidrule{2-10}
    % HRDEPTH
    & \multirow{3}{*}{HRDepth} & Self-Supervised & 0.193 & 0.115 & 0.421 & 0.231 & 0.682 & 0.921 & 0.982 \\
    & & \textit{Ours} & 0.195 & 0.109 & 0.419 & 0.232 & 0.674 & 0.921 & 0.984 \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.183} & \textbf{0.096} & \textbf{0.389} & \textbf{0.219} & \textbf{0.706} & \textbf{0.931} & \textbf{0.986} \\
    \cmidrule{2-10}
    % MONOVIT
    & \multirow{3}{*}{MonoViT} & Self-Supervised & 0.173 & 0.093 & 0.365 & 0.201 & 0.752 & 0.945 & 0.988 \\
    & & \textit{Ours} & 0.175 & 0.090 & 0.361 & 0.204 & 0.746 & 0.944 & 0.987 \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.162} & \textbf{0.077} & \textbf{0.335} & \textbf{0.191} & \textbf{0.776} & \textbf{0.951} & \textbf{0.989} \\
  \toprule
    \multirow{9}{*}{\rotatebox[origin=c]{90}{\textbf{Booster}}} & \multirow{3}{*}{Monodepth2} & Self-Supervised & 0.520 & 0.429 & 0.601 & 0.444 & 0.305 & 0.591 & 0.827 \\
    % & & \textit{Ours} & \textbf{0.168} & \textbf{0.097} & 0.398 & 0.207 & 0.753 & \textbf{0.932} & \textbf{0.980} \\
    & & \textit{Ours} & 0.430 & 0.301 & 0.501 & 0.389 & 0.362 & 0.675 & 0.893 \\
    % & \textit{Ours} & 0.421 & 0.291 & 0.495 & 0.384 & \textbf{0.370} & \textbf{0.685} & \textbf{0.898} \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.419} & \textbf{0.288} & \textbf{0.487} & \textbf{0.381} & \textbf{0.370} & \textbf{0.678} & \textbf{0.897} \\
    \cmidrule{2-10}
    % HRDEPTH
    & \multirow{3}{*}{HRDepth} & Self-Supervised & 0.495 & 0.391 & 0.559 & 0.426 & 0.307 & 0.611 & 0.852 \\
    & & \textit{Ours} & \textbf{0.414} & \textbf{0.276} & \textbf{0.482} & \textbf{0.379} & 0.364 & \textbf{0.680} & \textbf{0.907} \\
    % 0.414	0.276	0.482	0.379	0.364	0.68	0.907
    % & \textit{Ours} & 0.446 & 0.319 & 0.519 & 0.404 & 0.341 & \textbf{0.659} & 0.872 \\
    & & \textit{Ours}$^{\dagger}$ & 0.429 & 0.292 & 0.487 & 0.385 & \textbf{0.366} & 0.659 & 0.878 \\
    \cmidrule{2-10}
    % MONOVIT
    & \multirow{3}{*}{MonoViT} & Self-Supervised & 0.418 & 0.327 & 0.504 & 0.374 & \textbf{0.425} & 0.679 & 0.888 \\
    & & \textit{Ours} & 0.408 & 0.302 & 0.482 & 0.362 & 0.414 & 0.677 & 0.916 \\
    % & \textit{Ours} & 0.397 & 0.289 & 0.473 & 0.357 & 0.420 & 0.684 & 0.924 \\
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.375} & \textbf{0.249} & \textbf{0.440} & \textbf{0.337} & 0.422 & \textbf{0.734} & \textbf{0.944} \\
  \bottomrule
  \end{tabular}}
  \end{center}
  % \vspace{-7pt}
\end{table}

\subsection{Cross-dataset generalizability}
To demonstrate the generalization ability across different datasets, we conduct a zero-shot evaluation using 7-Scenes and Booster datasets.
As shown in \tabref{tab:7scenes}, our proposed methods (denoted as \textit{Ours} and \textit{Ours}$^{\dagger}$) consistently enhances performance.
Specifically, across all backbone architectures and all metrics, \textit{Ours}$^{\dagger}$ improved by an average of 5.47\% and 13.89\% for the 7-Scenes and Booster datasets, respectively.
Exceptionally, there is no significant difference between \textit{Ours} and the self-supervised method on the 7-Scenes dataset. This may be attributed to the predominance of non-reflective surfaces in the 7-Scenes dataset, where our model, trained with the reflection-aware triplet mining loss, slightly loses high-frequency details on non-reflective surfaces.
Conversely, the consistent performance improvement of \textit{Ours}$^{\dagger}$ across both reflective and non-reflective surfaces demonstrates the robust generalization capabilities of our method based on reflective region selection.