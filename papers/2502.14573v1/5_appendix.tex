\appendix
\section{More Detailed Experimental Setups}
As aforementioned in the main manuscripts, we follow all training details and experimental setups mentioned in 3D Distillation~\citep{shi20233d}.
We train all models with the reflection triplet split proposed by 3D Distillation for 41 epochs through the Adam optimizer~\citep{kingma2014adam} with an image resolution of 384$\times$288, implemented in PyTorch.
The training batch sizes of the Monodepth2~\citep{godard2019digging}, HRDepth~\citep{lyu2021hr}, and MonoViT~\citep{zhao2022monovit} are \{$12, 12, 8$\}, respectively.
The initial learning rate is $10^{-4}$, and we adopt the multi-step learning rate scheduler that decays the learning rate by $\gamma=0.1$ once the number of epochs reaches one of the milestones $[26, 36]$.
% All the training details followed the conditions mentioned in 3D Distillation. 
Moreover, with 3D Distillation, the pose between cameras is ground truth during training, and the minimum and maximum depths used for training and evaluation are 0.1m and 10m.
In our evaluation, we do not apply post-processing techniques such as averaging the estimates of both the flipped and original images or using median scaling.


\begin{table}[ht]
  \caption{Main results on the ScanNet-Original Test and Validation sets.}
  \label{tab:original}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}l|c|c|ccccccc@{}}
  % SCANNET ORIGINAL TEST
  \toprule
    \multirow{2}{*}{Backbone} & \multirow{2}{*}{Training Scheme} & \multirow{2}{*}{Method} & \multicolumn{7}{c}{ScanNet-Original Test Set} \\
    & & & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    \midrule
    \multirow{5}{*}{Monodepth2} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.189 & 0.116 & 0.407 & \textbf{0.217} & \textbf{0.731} & 0.921 & 0.974 \\
    % \cmidrule{3-10}
    & & \textit{Ours} & \textbf{0.185} & \textbf{0.109} & \textbf{0.405} & \textbf{0.217} & 0.730 & \textbf{0.923} & \textbf{0.975} \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.184 & 0.109 & 0.392 & 0.210 & 0.742 & 0.925 & 0.976 \\
    & & 3D Distillation & 0.181 & 0.105 & 0.388 & 0.208 & \textbf{0.746} & 0.927 & 0.976 \\
    % \cmidrule{3-10}
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.175} & \textbf{0.098} & \textbf{0.385} & \textbf{0.206} & \textbf{0.746} & \textbf{0.930} & \textbf{0.979} \\
    \midrule
    \multirow{5}{*}{HRDepth} & \multirow{2}{*}{End-to-End} & Self-Supervised & \textbf{0.184} & 0.111 & 0.399 & \textbf{0.212} & \textbf{0.739} & 0.925 & 0.976 \\
    % \cmidrule{3-10}
    & & \textit{Ours} & 0.186 & \textbf{0.106} & \textbf{0.397} & 0.213 & 0.735 & \textbf{0.927} & \textbf{0.977} \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.178 & 0.102 & 0.381 & 0.204 & 0.752 & 0.931 & 0.979 \\
    & & 3D Distillation & 0.176 & 0.098 & 0.378 & \textbf{0.202} & 0.754 & 0.932 & 0.979 \\
    % \cmidrule{3-10}
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.173} & \textbf{0.096} & \textbf{0.375} & \textbf{0.202} & \textbf{0.755} & \textbf{0.934} & \textbf{0.980} \\
    \midrule
    \multirow{5}{*}{MonoViT} & \multirow{2}{*}{End-to-End} & Self-Supervised & \textbf{0.154} & 0.082 & \textbf{0.343} & \textbf{0.182} & \textbf{0.801} & \textbf{0.948} & \textbf{0.984} \\
    % \cmidrule{3-10}
    & & \textit{Ours} & 0.155 & \textbf{0.081} & 0.345 & 0.185 & 0.795 & 0.945 & \textbf{0.984} \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.152 & 0.081 & 0.329 & 0.177 & 0.811 & 0.948 & 0.983 \\
    & & 3D Distillation & \textbf{0.149} & \textbf{0.075} & \textbf{0.324} & \textbf{0.174} & \textbf{0.812} & \textbf{0.949} & \textbf{0.985} \\
    % \cmidrule{3-10}
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.149} & \textbf{0.075} & 0.335 & 0.179 & 0.805 & \textbf{0.949} & 0.980 \\
  \bottomrule
  % SCANNET ORIGINAL VALIDATION
  \toprule
    \multirow{2}{*}{Backbone} & \multirow{2}{*}{Training Scheme} & \multirow{2}{*}{Method} & \multicolumn{7}{c}{ScanNet-Original Val. Set} \\
    & & & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    \midrule
    % MONODEPTH2
    \multirow{5}{*}{Monodepth2} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.167 & 0.100 & 0.385 & 0.203 & 0.764 & 0.935 & 0.981 \\
    % \cmidrule{3-10}
    & & \textit{Ours} & \textbf{0.162} & \textbf{0.090} & \textbf{0.378} & \textbf{0.201} & \textbf{0.765} & \textbf{0.937} & \textbf{0.983} \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage}
    & Self-Teaching & 0.160 & 0.090 & 0.365 & 0.193 & 0.780 & 0.941 & 0.983 \\
    & & 3D Distillation & 0.157 & 0.083 & \textbf{0.357} & \textbf{0.190} & 0.782 & 0.943 & \textbf{0.985} \\
    % \cmidrule{3-10}
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.153} & \textbf{0.080} & 0.358 & \textbf{0.190} & \textbf{0.783} & \textbf{0.944} & \textbf{0.985} \\
    \midrule
    % HRDEPTH
    \multirow{5}{*}{HRDepth} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.166 & 0.100 & 0.381 & 0.200 & 0.771 & 0.937 & 0.982 \\
    % \cmidrule{3-10}
    & & \textit{Ours} & \textbf{0.160} & \textbf{0.089} & \textbf{0.373} & \textbf{0.197} & \textbf{0.772} & \textbf{0.941} & \textbf{0.984} \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.159 & 0.090 & 0.360 & 0.190 & 0.785 & 0.943 & 0.984 \\
    & & 3D Distillation & 0.154 & 0.080 & \textbf{0.349} & \textbf{0.186} & 0.788 & 0.945 & 0.986 \\
    % \cmidrule{3-10}
    & & \textit{Ours}$^{\dagger}$ & \textbf{0.151} & \textbf{0.078} & 0.350 & \textbf{0.186} & \textbf{0.790} & \textbf{0.948} & \textbf{0.987} \\
    \midrule
    % MONOVIT
    \multirow{5}{*}{MonoViT} & \multirow{2}{*}{End-to-End} & Self-Supervised & 0.138 & 0.077 & 0.331 & \textbf{0.171} & \textbf{0.831} & 0.955 & 0.986 \\
    % \cmidrule{3-10}
    & & \textit{Ours} & \textbf{0.137} & \textbf{0.069} & \textbf{0.328} & 0.172 & 0.826 & \textbf{0.958} & \textbf{0.989} \\
    \cmidrule{2-10}
    & \multirow{3}{*}{Multi-Stage} & Self-Teaching & 0.133 & 0.071 & 0.314 & 0.163 & 0.844 & 0.959 & 0.988 \\
    & & 3D Distillation & \textbf{0.128} & \textbf{0.060} & \textbf{0.296} & \textbf{0.157} & \textbf{0.846} & \textbf{0.962} & \textbf{0.990} \\
    % \cmidrule{3-10}
    & & \textit{Ours}$^{\dagger}$ & 0.129 & 0.062 & 0.310 & 0.163 & 0.840 & 0.961 & \textbf{0.990} \\
  \bottomrule
  \end{tabular}}
\end{table}

\section{Evaluations on ScanNet Dataset}
To demonstrate the generalizability of our proposed method, we conduct the experiment on several ScanNet~\citep{dai2017scannet} splits denoted as ScanNet-Original \{Test, Val.\} sets and ScanNet-Robust Test set following~\citet{shi20233d} and~\citet{fu2018deep, bae2022multi}, respectively.
ScanNet-Original sets include both reflective and non-reflective surfaces, it is well-suited to evaluate the impact of reflective surfaces on training comprehensively.
In addition, the ScanNet-Robust test set was used to measure the generalization performance of Monocular Depth Estimation in the Robust Vision Challenge 2018~\citep{geiger2018robust}, as it is small-scale but suitable for evaluating generalization performance.

\subsection{Evaluation on ScanNet-Original Sets}
\tabref{tab:original} summarizes the quantitative evaluation results of the ScanNet-Original sets. 
We achieve steady performance improvement across most metrics for all backbone models in the end-to-end training scheme, suggesting that the proposed method minimizes the influence of reflective surfaces, which contributes to the general depth estimation performance improvement.
% Furthermore, in the case of the multi-stage training scheme, for Monodepth2~\cite{godard2019digging}, \textit{Ours}$^{\dagger}$ achieves a significant performance improvement of 5.28\% and 6.52\% on average for all metrics on the test and validation sets, respectively.
% For HRDepth~\cite{lyu2021hr}, \textit{Ours}$^{\dagger}$ achieves a remarkable performance improvement of 4.83\% and 7.19\% on average for all metrics on the test and validation sets, respectively. 
% For MonoViT~\cite{zhao2022monovit}, \textit{Ours}$^{\dagger}$ achieves a consistent performance improvement of 2.28\% and 5.59\% on average for all metrics on the test and validation sets, respectively. 
% In addition, \textit{Ours}$^{\dagger}$ increased by 1.76\% and 0.87\% on average across all metrics in Monodepth2 for the test split and validation split, respectively, compared to 3D Distillation~\cite{shi20233d}. 
% Also, HRDepth increased by an average of 0.71\% and 0.69\% across all metrics in the test split and validation split, respectively. 
% On the other hand, MonoViT decreased by an average of 1.09\% and 1.92\% compared to 3D Distillation across all metrics for the test and validation splits, respectively.
Furthermore, Our multi-stage training scheme (\textit{i.e.}, \textit{Ours}$^{\dagger}$) dramatically elevates performance across various depth estimation models. For Monodepth2, \textit{Ours}$^{\dagger}$ achieves a remarkable average increase of 5.28\% on the test set and 6.52\% on the validation set across all metrics. HRDepth reaps substantial benefits, with improvements of 4.83\% on the test set and 7.19\% on the validation set. Likewise, MonoViT consistently gains, with enhancements of 2.28\% on the test set and 5.59\% on the validation set.
When benchmarked against 3D Distillation~\citep{shi20233d}, \textit{Ours}$^{\dagger}$ provides an enhanced performance for Monodepth2, showing an average increase of 1.76\% on the test set and 0.87\% on the validation set. HRDepth also gains an average of 0.71\% on the test set and 0.69\% on the validation set. However, for MonoViT, \textit{Ours}$^{\dagger}$ shows a slight decline, with decreases of 1.09\% on the test set and 1.92\% on the validation set compared to 3D Distillation.

\begin{table}[ht]
  \caption{Main results on the ScanNet-Robust Test set.}
  \label{tab:robust}
  \centering
  \resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|c|ccccccc@{}}
  \toprule
    \multirow{2}{*}{Backbone} & \multirow{2}{*}{Method} & \multicolumn{7}{c}{ScanNet-Robust Test Set} \\
    & & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    \midrule
    \multirow{3}{*}{Monodepth2} & Self-Supervised & 0.193 & 0.118 & 0.395 & 0.219 & 0.729 & 0.921 & 0.973 \\
    & \textit{Ours} & 0.186 & 0.107 & 0.388 & 0.216 & 0.729 & 0.926 & 0.976 \\
    & \textit{Ours}$^{\dagger}$ & \textbf{0.179} & \textbf{0.099} & \textbf{0.371} & \textbf{0.207} & \textbf{0.744} & \textbf{0.930} & \textbf{0.978} \\
    % 0.193	0.118	0.395	0.219	0.729	0.921	0.973
    % 0.186	0.107	0.388	0.216	0.729	0.926	0.976
    % 0.179	0.099	0.371	0.207	0.744	0.93	0.978
    \midrule
    % HRDEPTH
    \multirow{3}{*}{HRDepth} & Self-Supervised & 0.190 & 0.112 & 0.387 & 0.216 & 0.729 & 0.924 & 0.976 \\
    & \textit{Ours} & 0.188 & 0.107 & 0.384 & 0.215 & 0.731 & 0.926 & 0.976 \\
    & \textit{Ours}$^{\dagger}$ & \textbf{0.177} & \textbf{0.095} & \textbf{0.362} & \textbf{0.203} & \textbf{0.750} & \textbf{0.935} & \textbf{0.979} \\
    % 0.19	0.112	0.387	0.216	0.729	0.924	0.976
    % 0.188	0.107	0.384	0.215	0.731	0.926	0.976
    % 0.177	0.095	0.362	0.203	0.75	0.935	0.979
    \midrule
    % MONOVIT
    \multirow{3}{*}{MonoViT} & Self-Supervised & 0.158 & 0.082 & 0.328 & 0.181 & 0.799 & 0.948 & 0.984 \\
    & \textit{Ours} & 0.155 & 0.078 & 0.327 & 0.183 & 0.798 & 0.949 & 0.985 \\
    & \textit{Ours}$^{\dagger}$ & \textbf{0.150} & \textbf{0.073} & \textbf{0.319} & \textbf{0.178} & \textbf{0.806} & \textbf{0.952} & \textbf{0.986} \\
    % 0.158	0.082	0.328	0.181	0.799	0.948	0.984
    % 0.155	0.078	0.327	0.183	0.798	0.949	0.985
    % 0.15	0.073	0.319	0.178	0.806	0.952	0.986
  \bottomrule
  \end{tabular}}
\end{table}

\subsection{Evaluation on ScanNet-Robust Test Set}
\tabref{tab:robust} summarizes the quantitative evaluation results of the ScanNet-Robust test set.
Due to the 3D-distillation baselines~\citep{shi20233d} did not release the source code, and the reported performance on this split not existing, we compare the models trained by our methods to the self-supervised methods across three backbones, similar to previous experiments. As depicted in \tabref{tab:robust}, our proposed methods (\textit{i.e.}, \textit{Ours}, \textit{Ours}$^{\dagger}$) achieve significant performance gains for all evaluation metrics and all backbones, consistently. 
Specifically, for Monodepth2, \textit{Ours} and \textit{Ours}$^{\dagger}$ demonstrate an average performance improvement of 2.42\% and 5.49\%, respectively, across all metrics. Similarly, for HRDepth, \textit{Ours} showed an average improvement of 1.03\%, and \textit{Ours}$^{\dagger}$ achieved a 5.55\% increase in performance across all metrics.
In the case of MonoViT, \textit{Ours} resulted in an average performance improvement of 0.87\%, and \textit{Ours}$^{\dagger}$ achieved a 3.13\% improvement across all metrics. 
The consistent improvements across all metrics for Monodepth2, HRDepth, and MonoViT indicate that our methods effectively mitigate the risk of erroneous learning induced by reflective surfaces.

% mdp2 2.42, 5.49
% hrdepth 1.03, 5.55
% monovit 0.87, 3.13


\begin{table}[!t]
  \caption{Evaluation results on the ScanNet-Reflection Test set for the combination of state-of-the-art self-supervised monocular depth framework and our approach.}
  \label{tab:sota-ssmde}
  \begin{center}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}llccccccc@{}}
  \toprule
    Backbone & Method & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log $\downarrow$ & $\delta < 1.25 \uparrow$ & $\delta < 1.25^2 \uparrow$ & $\delta < 1.25^3 \uparrow$ \\
    % \cmidrule{2-10}
    \midrule
    % MONODEPTH2
    \multirow{3}{*}{RA-Depth} & Self-Supervised & 0.161 & 0.124 & 0.477 & 0.203 & 0.779 & 0.947 & 0.981 \\
    & RA-Depth $+ \mathcal{L}_{tri}$ & 0.138 & 0.091 & 0.444 & 0.186 & 0.804 & 0.963 & 0.987 \\
    & RA-Depth $+ \mathcal{L}_{tri} + \mathcal{L}_{rkd}$ & \textbf{0.130} & \textbf{0.076} & \textbf{0.402} & \textbf{0.171} & \textbf{0.834} & \textbf{0.966} & \textbf{0.990} \\
    \midrule
    \multirow{3}{*}{GasMono} & Self-Supervised & 0.156 & 0.123 & 0.462 & 0.198 & 0.810 & 0.949 & 0.980 \\
    & GasMono $+ \mathcal{L}_{tri}$ & 0.139 & 0.089 & 0.425 & 0.178 & 0.827 & 0.964 & 0.989 \\
    & GasMono $+ \mathcal{L}_{tri} + \mathcal{L}_{rkd}$ & \textbf{0.127} & \textbf{0.072} & \textbf{0.386} & \textbf{0.164} & \textbf{0.843} & \textbf{0.968} & \textbf{0.993} \\
    \midrule
    \multirow{3}{*}{Lite-mono} & Self-Supervised & 0.179 & 0.172 & 0.517 & 0.221 & 0.775 & 0.935 & 0.973 \\
    & Lite-mono $+ \mathcal{L}_{tri}$ & 0.159 & 0.113 & 0.462 & 0.201 & 0.775 & 0.947 & 0.983 \\
    & Lite-mono $+ \mathcal{L}_{tri} + \mathcal{L}_{rkd}$ & \textbf{0.148} & \textbf{0.101} & \textbf{0.433} & \textbf{0.190} & \textbf{0.788} & \textbf{0.960} & \textbf{0.984} \\
  \bottomrule
  \end{tabular}}
  \end{center}
  % \vspace{-7pt}
\end{table}

\section{Extensibility Analysis of The Proposed Method}
To demonstrate performance improvements not only across different architectures but also by applying our method while preserving state-of-the-art methodologies, we evaluate the three outperforming SSMDE models, including RA-Depth~\citep{he2022ra}, GasMono~\citep{zhao2023gasmono}, Lite-mono~\citep{zhang2023lite}, which have own unique methods (\textit{e.g.}, iterative self-distillation in GasMono).
We apply our reflection-aware triplet mining loss (denoted as [Baseline] + $\mathcal{L}_{tri}$, and reflection-aware knowledge distillation method (denoted as [Baseline] + $\mathcal{L}_{tri} + \mathcal{L}_{rkd}$) to each baseline.
\tabref{tab:sota-ssmde} summarizes the results of the ScanNet-Reflection test split, comparing the performance of each method when incorporating the triplet loss and our distillation method.
The results demonstrate that our method provides substantial performance gains even for recent SoTA methods, underscoring its robustness to reflective surfaces.


\begin{table}[ht]
  \caption{Evaluation results on the ScanNet-Reflection Validation and ScanNet-NoReflection Validation sets. w.r.t. reflective region mask $M_r$.}
  \label{tab:mask}
  \centering
  \resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|c|cccc|cccc@{}}
  \toprule
    \multirow{2}{*}{Backbone} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{ScanNet-Reflection Validation Set} & \multicolumn{4}{c}{ScanNet-NoReflection Validation Set} \\
    & & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & $\delta < 1.25^3 \uparrow$ & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & $\delta < 1.25^3 \uparrow$ \\
    \midrule
    \multirow{3}{*}{Monodepth2} & $M_r = 0$ & 0.206 & 0.227 & 0.584 & 0.961 & 0.169 & 0.100 & \textbf{0.395} & 0.979 \\
    & $M_r = 1$ & 0.170 & 0.132 & 0.505 & 0.979 & 0.171 & 0.099 & 0.402 & 0.978 \\
    & \textit{Ours} & \textbf{0.166} & \textbf{0.125} & \textbf{0.492} & \textbf{0.981} & \textbf{0.168} & \textbf{0.095} & \textbf{0.395} & \textbf{0.980} \\
    % 0.193	0.118	0.395	0.219	0.729	0.921	0.973
    % 0.186	0.107	0.388	0.216	0.729	0.926	0.976
    % 0.179	0.099	0.371	0.207	0.744	0.93	0.978
    \midrule
    % HRDEPTH
    \multirow{3}{*}{HRDepth} & $M_r = 0$ & 0.213 & 0.244 & 0.605 & 0.961 & 0.169 & 0.102 & \textbf{0.388} & \textbf{0.980} \\
    & $M_r = 1$ & 0.184 & 0.167 & 0.564 & 0.965 & 0.179 & 0.113 & 0.433 & 0.968 \\
    & \textit{Ours} & \textbf{0.167} & \textbf{0.127} & \textbf{0.496} & \textbf{0.982} & \textbf{0.167} & \textbf{0.096} & 0.389 & 0.979 \\
    % 0.19	0.112	0.387	0.216	0.729	0.924	0.976
    % 0.188	0.107	0.384	0.215	0.731	0.926	0.976
    % 0.177	0.095	0.362	0.203	0.75	0.935	0.979
    \midrule
    % MONOVIT
    \multirow{3}{*}{MonoViT} & $M_r = 0$ & 0.179 & 0.206 & 0.557 & 0.963 & \textbf{0.140} & 0.074 & \textbf{0.333} & 0.984 \\
    & $M_r = 1$ & 0.155 & 0.151 & 0.527 & 0.971 & 0.168 & 0.112 & 0.420 & 0.954 \\
    & \textit{Ours} & \textbf{0.139} & \textbf{0.107} & \textbf{0.452} & \textbf{0.984} & 0.141 & \textbf{0.072} & 0.338 & \textbf{0.987} \\
    % 0.158	0.082	0.328	0.181	0.799	0.948	0.984
    % 0.155	0.078	0.327	0.183	0.798	0.949	0.985
    % 0.15	0.073	0.319	0.178	0.806	0.952	0.986
  \bottomrule
  \end{tabular}}
\end{table}

% \begin{figure}{\textwidth}
\begin{figure}[ht]
% \includegraphics[trim=70 30 90 60,clip,width=\textwidth,height=8cm]
\includegraphics[width=\textwidth,height=8cm]{figure/qualitative_mask.png}
\caption{Qualitative results of the proposed methods w.r.t. reflective region mask $M_r$.}
\label{fig:qualitative_mask}
\end{figure}

\section{Impact of the reflection-aware triplet mining loss w.r.t. reflective region localization}
\label{sec:impact_of_triplet_minig_loss}
% \textcolor{red}{TODO: subfigure package, table, observation}
% case 1: $M_r = 0 \rightarrow$ same with self-supervised method.
% case 2: $M_r = 1 \rightarrow$ triplet loss for all regions.
% case 3: $M_r$ from equation 8 -> Ours.

As aforementioned in the main manuscript, the proposed reflection-aware triplet mining loss is applied to reflective regions, thus preserving performance in non-reflective regions.
To validate this claim, we conduct an experiment to evaluate the impact of varying the reflection mask $M_r$ with three configurations as follows:

\begin{itemize}
    \item[1.] $M_r = 0$: This configuration exactly corresponds to the traditional self-supervised method without the triplet mining loss.
    \item[2.] $M_r = 1$: In this configuration, the triplet loss is applied to both reflective and non-reflective regions of the image.
    \item[3.] \textit{Ours}: This configuration leverages $M_r$, which is calculated through \Eqref{eq:lambertian_assumption} in the main manuscript, to selectively regulate the reflective regions.
\end{itemize}

As shown in \tabref{tab:mask}, the results demonstrate that \textit{Ours} significantly improves performance on reflective datasets while maintaining comparable performance on non-reflective regions when compared to the first configuration (denoted as $M_r = 0$).
On the other hand, applying the triplet mining loss across all regions ($M_r = 1$) led to some performance improvement in reflective regions but resulted in a notable drop in performance in non-reflective regions compared to other configurations.
These findings verify that the proposed reflection-aware triplet mining loss effectively identifies reflective regions and applies the triplet loss selectively, thereby preserving the performance in non-reflective regions.


\begin{table}[ht]
  \caption{Computational overhead comparison.}
  \label{tab:computational}
  \centering
  % \resizebox{\textwidth}{!}{
\begin{tabular}{@{}lll@{}}
  \toprule
  Method & Task & Training cost (hours) \\
  \midrule
  Self-supervised \hfill \textbf{(1)} & End-to-end training & 11.5 \\
  \midrule
  Ours (triplet loss) \hfill \textbf{(2)} & End-to-end training & 14.1 \\
  \midrule
  Ours (distillation) \hfill \textbf{(3)} & Multi-stage training & 27.3 \\
  \midrule
  3D Distillation \hfill \textbf{(4)} & Total (1+2+3+4) & 95.5 \\
  & 1. Ensemble model pre-training & 65.8 \\
  & 2. Ensemble model inference time & 4.8 \\
  & 3. Mesh reconstruction and rendering & 11.7 \\
  & 4. Student model training & 13.2 \\
  \bottomrule
  \end{tabular}%}
\end{table}

\section{Computational overhead comparison with 3D Distillation}
The proposed triplet mining loss has a negligible impact on the training cost of the model.
Although the proposed simple distillation method involves multi-stage training and incurs additional training costs, it remains significantly more efficient than the existing 3D distillation approach~\citep{shi20233d}.
To provide clarity, we summarized the training costs of Monodepth2~\citep{godard2019digging} under four scenarios: \textbf{(1)} training with traditional self-supervision, \textbf{(2)} applying our triplet loss, \textbf{(3)} applying our proposed distillation method, and \textbf{(4)} employing the 3D distillation approach.
All training times were measured using a single \textit{RTX A6000} GPU, as detailed in the \tabref{tab:computational}.

\begin{figure}[ht]

\begin{subfigure}{\textwidth}
% \includegraphics[trim=70 30 90 60,clip,width=\textwidth,height=8cm]
\includegraphics[width=\textwidth,height=8cm]{figure/qualitative_seven.png}
\caption{Qualitative results of the proposed methods on the 7-Scenes dataset.}
\label{fig:qualitative_seven}
\end{subfigure}

\bigskip

\begin{subfigure}{\textwidth}
% \includegraphics[trim=70 30 90 60,clip,width=\textwidth,height=8cm]
\includegraphics[width=\textwidth,height=8cm]{figure/qualitative_booster.png}
\caption{Qualitative results of the proposed methods on the Booster dataset.}
\label{fig:qualitative_booster}
\end{subfigure}
\caption{Qualitative results of the proposed methods on the 7-scenes and Booster datasets. Note that the error map represents the absolute difference between prediction and ground truth depth, normalized to between 0 and 255.}
\label{fig:qualitative_supp}
\end{figure}

\section{Qualitative Results on 7-Scenes and Booster Datasets}
We provide additional qualitative results of the proposed methods denoted as \textit{Ours} and \textit{Ours}$^{\dagger}$ as discussed in \tabref{tab:7scenes} of the main manuscript, utilizing the 7-Scenes~\citep{shotton2013scene} and Booster~\citep{ramirez2023booster} datasets.
% Note that we visualized the error map in the same manner as in Figure 3 of the manuscript, employing the monodepth2 model for this visualization.
In \Figref{fig:qualitative_supp}, we showcase the predicted depth and error map of the Monodepth2 trained by self-supervised, $\mathcal{L}_{tri}$, and $\mathcal{L}_{rkd}$.
Our methods alleviate the black-hole effect on specular highlight regions while preserving high-frequency details on non-reflective areas. As demonstrated by the qualitative evaluation of the 7-Scenes dataset, our proposed methods exhibit robustness to reflective surfaces and impressive performance in preserving details on non-reflective surfaces in other indoor scenes that are similar to the training environment.

\section{Limitations}
Despite the promising results, our study has several limitations.
One major limitation is that the proposed method cannot handle transparent or mirror (ToM) objects.
Secondly, a few cases do not satisfy the assumption of \Eqref{eq:lambertian_assumption} of the manuscript (\textit{e.g.}, surfaces including multiple reflection lobes).
Lastly, similar to 3D distillation~\citep{shi20233d}, the conducted experiments assume that the ground truth camera pose is known during training.
Future research should aim to address this limitation by exploring more effective solutions.
We encourage future researchers to further investigate this issue and develop improved methodologies to overcome these challenges.