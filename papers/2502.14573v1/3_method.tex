\section{Method}
\label{sec:method}
Our method aims to enhance depth prediction accuracy on reflective surfaces by strategically penalizing the inappropriate photometric error minimization between the view-synthesized image and the reference image.
In \Secref{sec:preliminary}, we discuss the photometric constancy principle, which posits that correctly minimizing photometric error is crucial for accurately determining depth (\Secref{sec:photometric_constancy_principle}).
We also provide an overview of the standard training strategies employed in SSMDE frameworks (\Secref{sec:problem_definition}).
In \Secref{sec:methodology}, we detail the three components of our training strategy: reflective region localization (\Secref{sec:reflective_region_localization}), reflection-aware triplet mining loss (\Secref{sec:mining}), and reflection-aware knowledge distillation (\Secref{sec:distillation}).

\subsection{Preliminary}
\label{sec:preliminary}

\subsubsection{Photometric constancy principle}
\label{sec:photometric_constancy_principle}
The photometric constancy principle is foundational in SSMDE frameworks, positing that surfaces exhibit uniform reflectance (\textit{i.e.}, Lambertian reflectance) from all viewing angles.
A surface adheres to this principle if its color and luminance observed through a camera remain constant, regardless of the camera's viewing angle.
By leveraging this property, depth and pose can be accurately estimated by minimizing the photometric error between the view-synthesized image and the reference image, as described in \Eqref{eq:warping}.
However, real-world scenes rarely adhere strictly to this principle.
Non-Lambertian surfaces, such as specular reflections from light sources or mirrored objects, are prevalent, leading to violations of photometric constancy.
These deviations result in significant errors when attempting to minimize photometric error, thus compromising the effectiveness of depth estimation methods based on these assumptions.

\subsubsection{Training strategy of general SSMDE framework}
\label{sec:problem_definition}
The objective of SSMDE is to predict a per-pixel basis depth map $\mathbf{D}_{ref}$ of a reference image $\mathbf{I}_{ref}$ given the reference image itself, a source image $\mathbf{I}_{src}$ (or source images) and their camera intrinsics $\displaystyle \mK$.
The framework consists of a depth network $\mathcal{F}_{\theta}(\cdot)$, and a pose network $\mathcal{G}_{\phi}(\cdot, \cdot)$ to respectively estimate the depth of the reference image $\mathbf{D}_{ref}$, and the relative pose ${[{\displaystyle \mR}|{\displaystyle \vt}]}_{r2s}$ as follows:
\begin{alignat}{3}
\label{eq:forward}
    \mathbf{D}_{ref} &= \mathcal{F}_{\theta}(\mathbf{I}_{ref}),\quad &&~\mathcal{F}_{\theta}:\mathbb{R}^{3 \times h \times w} &&\rightarrow \mathbb{R}^{1 \times h \times w},\\
    {[{\displaystyle \mR}|{\displaystyle \vt}]}_{r2s} &= \mathcal{G}_{\phi}(\mathbf{I}_{src}, \mathbf{I}_{ref}),\quad &&~\mathcal{G}_{\phi}:\mathbb{R}^{2 \times 3 \times h \times w} &&\rightarrow \mathbb{R}^{3 \times 4},
\end{alignat}
where $(h, w)$ represent the spatial resolution of $\mathbf{I}_{ref}$.
Using the obtained relative pose ${[{\displaystyle \mR}|{\displaystyle \vt}]}_{r2s}$, and depth map $\mathbf{D}_{ref}$, the source image $\mathbf{I}_{src}$ is warped into the reference coordinates, generating the view-synthesized image $\mathbf{I}_{s2r}$ as follows:
\begin{align}
\begin{split}
    \label{eq:warping}
    (\mathbf{I}_{s2r})_{:, u, v} = \mathbf{I}_{src}(\langle {\displaystyle \mK}{[{\displaystyle \mR}|{\displaystyle \vt}]}_{r2s} (\mathbf{D}_{ref})_{:,u,v} {\displaystyle \mK}^{-1}[u,v,1]^{T}\rangle),
    \end{split}
\end{align}
where $(u,v)$ represent the image coordinates and $\langle \cdot \rangle$ is the projection function that maps homogeneous coordinates to image coordinates.
By the photometric constancy principle detailed in \Secref{sec:photometric_constancy_principle}, the synthesized image $\mathbf{I}_{s2r}$ should exhibit the same colors and luminances as the reference image on a pixel-by-pixel basis.
Consequently, the model can determine the accurate depth and pose by minimizing the photometric errors, $\mathcal{P}(\cdot, \cdot)$, between $\mathbf{I}_{s2r}$ and $\mathbf{I}_{ref}$ as follows:
\begin{align}
\begin{split}
    \label{eq:photometric_error}
    \mathcal{P}(\mathbf{I}_{s2r}, \mathbf{I}_{ref}) = \mathbf{M} \odot \bigg(\alpha_{1}\frac{1 - \mathcal{S}(\mathbf{I}_{ref}, \mathbf{I}_{s2r})}{2} + \alpha_{2}||\mathbf{I}_{ref} - \mathbf{I}_{s2r}||_{1}\bigg),\\
    \mathcal{P}:\mathbb{R}^{2 \times 3 \times h \times w} \rightarrow \mathbb{R}^{1 \times h \times w},~~~~~\mathcal{S}:\mathbb{R}^{2 \times 3 \times h \times w} \rightarrow \mathbb{R}^{1 \times h \times w},
\end{split}
\end{align}
where $\mathcal{S}(\cdot, \cdot)$ is the mixture of structural similarity index~\citep{wang2004image}, and $\mathbf{M}$ is the principled mask~\citep{godard2019digging} to prevent the backpropagation of corrupted gradients, caused by anomalies like moving objects in the scene.
The weights $\alpha_1$ and $\alpha_2$ serve as balance terms between two losses, and $\odot$ represents the element-wise multiplication operator.
However, if the surface does not conform to the principle of photometric constancy, minimizing photometric errors on such reflective surfaces can lead to significant inaccuracies in the estimated depth.

\subsection{Methodology}
\label{sec:methodology}
% In this paper, our primary focus is on training the depth network $\mathcal{F}_{\theta}$ to enhance depth accuracy on reflective surfaces, while preserving the high-frequency details on the non-reflective surface.
% To achieve this, we propose a reflection-aware triplet mining scheme that encourages the network to estimate the accurate depth on reflective surfaces.

\begin{figure}[t]
\begin{center}
  \includegraphics[width=\linewidth]{figure/triplet_final_ver.png}
\end{center}
\caption{The effect of the proposed method on reflective/non-reflective surfaces. (\includegraphics[height=8pt]{figure/icons/blue.png}/\includegraphics[height=8pt]{figure/icons/red.png},\includegraphics[height=8pt]{figure/icons/green.png}) imply the projected non-reflective/reflective surface points, respectively, and (\includegraphics[height=8pt]{figure/icons/dot_red.png}, \includegraphics[height=8pt]{figure/icons/dot_green.png}) denotes the location of reflection lobe in view-synthesized image coordinate. Our proposed method cancels out the wrong photometric error minimization in reflection areas by contrasting the negative pair samples.}
\label{fig:triplet}
\end{figure}

\subsubsection{Reflective region localization}
\label{sec:reflective_region_localization}
The photometric error, as calculated through \Eqref{eq:photometric_error} between $I_{s2r}$ and $I_{ref}$ tends to be smaller in non-reflective regions. This is because these areas reflect consistent colors and luminances irrespective of the viewing direction, adhering to the photometric constancy principle.
On the other hand, reflective regions, which violate the photometric constancy principle, exhibit abnormally low disparities due to the additional distance from the reflected light source.
Consequently, as illustrated in \Figref{fig:triplet}, reflection lobes from different images appear relatively closer in image coordinates, resulting in reduced photometric errors in the RGB images of two different viewpoints.

This characteristic is crucial for isolating reflective regions within the spatial dimension of the image. To capitalize on this, we first generate cross-view synthesized images $I_{s2r}, I_{r2s}$ in a manner similar to the process outlined in \Eqref{eq:warping}:
\begin{align}
    % I_{s2r}([u,v])=I_{src}(\langle K{[R|t]}_{r2s} {D}_{ref} K^{-1} [u,v,1]\rangle),\\
    % I_{r2s}([u,v])=I_{ref}(\langle K{[R|t]}_{s2r} {D}_{src} K^{-1} [u,v,1]\rangle),
    (\mathbf{I}_{s2r})_{:, u, v} = \mathbf{I}_{src}(\langle {\displaystyle \mK}{[{\displaystyle \mR}|{\displaystyle \vt}]}_{r2s} (\mathbf{D}_{ref})_{:,u,v} {\displaystyle \mK}^{-1}[u,v,1]^{T}\rangle),\\
    (\mathbf{I}_{r2s})_{:, u, v} = \mathbf{I}_{ref}(\langle {\displaystyle \mK}{[{\displaystyle \mR}|{\displaystyle \vt}]}_{s2r} (\mathbf{D}_{src})_{:,u,v} {\displaystyle \mK}^{-1}[u,v,1]^{T}\rangle),
\end{align}
where the relative pose ${[{\displaystyle \mR}|{\displaystyle \vt}]}_{s2r}$ can be obtained by computing the inverse of the predicted pose ${[{\displaystyle \mR}|{\displaystyle \vt}]}_{r2s}$, and $\mathbf{D}_{src}$ is predicted depth from $\mathbf{I}_{src}$, following a procedure similar to \Eqref{eq:forward}.
Utilizing these synthesized cross-view images, we compute two photometric errors to measure discrepancies between image pairs as follows:
\begin{align}
\begin{split}
    \label{eq:photoloss}
    \mathbf{E}^{+} = \mathcal{P}(\mathbf{I}_{s2r}, \mathbf{I}_{ref}),~~~\mathbf{E}^{-} = \mathcal{P}(\mathbf{I}_{s2r}, \mathbf{I}_{r2s}).
\end{split}
\end{align}
The first error, $\mathbf{E}^{+}$, quantifies discrepancies between images taken from the same viewpoint ($\mathbf{I}_{s2r}, \mathbf{I}_{ref}$).
This error is minimized when depth and pose are accurately estimated on non-reflective surfaces.
Conversely, the second error, $\mathbf{E}^{-}$, measures discrepancies between images from different viewpoints ($\mathbf{I}_{s2r}, \mathbf{I}_{r2s}$). 
% Due to the variations in light reflection, $\mathbf{E}^{-}$ generally shows lower values on reflective surfaces, indicating more significant errors caused by reflections.
In general, the expected photometric error for $\mathbf{E}^{-}$ should be substantial due to the different camera coordinate systems. However, on reflective surfaces, the variations in light reflection may result in a reduced photometric error.
Based on these observations, we identify pixel-level reflective regions $\mathbf{M}_{r}\in\mathbb{R}^{1 \times h \times w}$ as follows: 
\begin{align}
\begin{split}
    \label{eq:lambertian_assumption}
    % pe(I_{s2r}, I_{ref}) < pe(I_{s2r}, I_{r2s}),
    % pe^{+} &< pe^{-},
    % M^{\leftrightarrow}([u, v]) =
    % \begin{cases}
    %     0, & \mbox{if }pe^{+} < pe^{-} \\
    %     1, & \mbox{if }pe^{+} \geq pe^{-}
    % \end{cases}
    (\mathbf{M}_r)_{:, u, v} =
    \begin{cases}
        1, & \mbox{if}~(\mathbf{E}^{-})_{:, u, v} - (\mathbf{E}^{+})_{:, u, v} \leq \delta , \\
        0, & \mbox{else}, 
    \end{cases} \\
    % \text{where}~\mathcal{L}_{ph}^{+} = \mathcal{L}_{ph}(I_{s2r}, I_{ref}),~~~\mathcal{L}_{ph}^{-} = \mathcal{L}_{ph}(I_{s2r}, I_{r2s}),
\end{split}
\end{align}
% where $\delta$ is a certain margin that represents the minimum significant photometric difference necessary to distinguish between the two types of surfaces (1 and 0 indicate reflective and non-reflective pixel, respectivley).
where $\delta$ is a certain margin that represents the minimum significant photometric difference required to distinguish between the two surface types, where a value of 1 corresponds to a reflective pixel and 0 to a non-reflective pixel, respectively.
This method effectively utilizes discrepancies in photometric errors to distinguish between reflective and non-reflective surfaces, providing a precise mapping of surface properties within the image. (refer to \Secref{sec:impact_of_triplet_minig_loss} in the supplementary materials.)

\subsubsection{Reflection-aware triplet mining loss}
\label{sec:mining}
We introduce the reflection-aware triplet mining loss, $\mathcal{L}_{tri}$, which addresses the limitations of using $\mathbf{E}^{+}$ alone in environments where reflections disrupt depth accuracy.
In reflective regions, simply minimizing $\mathbf{E}^{+}$ does not effectively discern between real and reflected disparities.
To counteract this, we assert that $\mathbf{E}^{-}$ should be significantly greater than $\mathbf{E}^{+}$. This approach is inspired by triplet mining techniques that aim to minimize the distance within positive pairs and maximize it within negative pairs, enhancing the model's ability to distinguish between reflective and non-reflective surfaces.
To implement this, we formulate the reflection-aware triplet mining loss as follows:
\begin{align}
\begin{split}
    \label{eq:cond_triplet}
    \mathcal{L}_{tri}(\mathbf{I}_{ref}, \mathbf{I}_{s2r}, \mathbf{I}_{r2s}) = \mathbf{M}_r \odot (\mathbf{E}^{+} - \mathbf{E}^{-} + \delta)_{hinge} + \big(1 - \mathbf{M}_r\big) \odot \mathbf{E}^{+},
\end{split}
\end{align}
where $(\cdot)_{hinge}$ is the hinge loss function described in~\citet{hearst1998support}.
In this configuration, the reflection-aware triplet mining loss is applied specifically to regions identified as reflective. For non-reflective regions, where reflections do not disrupt photometric assessments, we apply the photometric loss $\mathbf{E}^{+}$ as it reliably reflects photometric consistency.
This differentiation allows the model to address the unique challenges presented by each type of region effectively.

As illustrated in \Figref{fig:triplet}, this strategy involves not only penalizing the minimization of $\mathbf{E}^+$ but also actively maximizing $\mathbf{E}^-$. This method effectively counteracts the contaminated gradients typically found in reflective regions.
By adjusting the balance between $\mathbf{E}^+$ and $\mathbf{E}^-$ based on the presence of reflective surfaces, our method not only improves depth estimation in complex scenarios but also ensures robust performance against the challenges posed by reflective surfaces. This comprehensive approach results in a model that accurately reflects the true topography of both reflective and non-reflective environments. 

\subsubsection{Reflection-aware knowledge distillation}
\label{sec:distillation}
The proposed end-to-end training scheme described in \Secref{sec:mining} effectively handles the depth estimation on both reflective and non-reflective surfaces.
To further refine depth estimation quality, we introduce a reflection-aware knowledge distillation strategy inspired by the fusion techniques discussed in \citet{shi20233d}, aimed at retaining high-frequency details in depth prediction.

Our approach begins by training two separate SSMDE networks. The first is trained using our reflection-aware triplet mining loss, $\mathcal{L}_{tri}$, as defined in \Eqref{eq:cond_triplet}, and the second employs the conventional photometric loss, $\mathbf{E}^{+}$. 
From these models, we generate two types of depth maps: $\mathbf{D}_{tri}$, derived from the reflection-aware model, and $\mathbf{D}_{ori}$, obtained from the model trained with conventional photometric loss. We then merge these depth maps into a single pseudo depth map $\mathbf{D}_{pse}$ utilizing a reflective region mask $\mathbf{M}_r$. This mask facilitates the adaptive fusing of depth information from both teacher models based on the presence of reflective properties in the image. The pseudo depth map generation and distillation process is detailed in the following equation:
\begin{align}
\begin{split}
\mathcal{L}_{rkd}(\hat{\mathbf{D}}, \mathbf{D}_{pse}) = |\log \hat{\mathbf{D}} - \log \mathbf{D}_{pse}|,~
\text{where}~\mathbf{D}_{pse} = \mathbf{M}_r \odot \mathbf{D}_{tri} + (1 - \mathbf{M}_r) \odot \mathbf{D}_{ori},
\end{split}
\end{align}
where $\hat{\mathbf{D}}$ represents the depth predicted by the student model.
It is important to note that the student model and the two teacher models share the same network architecture as the general SSMDEs.
This structured training approach not only addresses the specific challenges posed by reflective surfaces but also ensures that the high-frequency detail is not lost, thus achieving a balanced and accurate depth prediction across different surface types.