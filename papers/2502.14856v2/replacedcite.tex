\section{Related Work}
\label{sec:relate}

This section mainly introduces model acceleration methods related to large vocabulary and speculative sampling. More details on how LLMs work can refer to surveys____. Other acceleration methods such as quantization and distillation can refer to suverys____.

\subsection{Acceration on Large Vocabulary}
Recent advancements in large language models (LLMs) have prompted a growing interest in addressing the challenges associated with large vocabularies. While several optimization efforts have been proposed to tackle these issues, the majority focus primarily on the training phase.
Computing the LM Head and the loss function over large vocabularies requires storing a huge intermediate state before gradient computation. Therefore, MST____ and CCE____ tried to mitigate the memory overhead caused by computing loss functions over large vocabularies. These approaches address the issue by using input partitioning or weight partitioning, and conduct activation recomputation____ during the backward propagation.
In addition to the aforementioned works that require no modifications to the model architecture, ____ proposes a hierarchical vocabulary structure to eliminate the computation of irrelevant vocabulary adaptively.

Constrained Decoding____ restricts the vocabulary space to generate highly structured outputs, particularly in the context of LLM agents, where the generated content must adhere to specific formats, such as producing parsable code or invocable functions.

\subsection{Speculative Sampling}

Traditional autoregressive generation in LLMs suffers from low generation speed due to the sequential nature of token prediction. To address this limitation, speculative sampling has emerged as a promising approach, leveraging draft-then-verification paradigms to accelerate decoding____. Existing speculative sampling methods can be categorized into two branches:
(1) \textit{retrieval-based drafting} approaches like PLD____, LLMA____, and REST____ retrieve relevant context from the prompt, gaining significant speedups in context-dependent tasks (e.g., summarization) by reusing retrieved text spans from the prompt.
(2) \textit{model-based drafting} methods exemplified by SpecInfer____, DistillSpec____, Medusa____ and EAGLE____, which employ a draft model for general-purpose acceleration. Our work focuses on the latter category due to its broader applicability.
The draft models' structures also differ. For example, Medusa generates draft tokens based solely on the model's last hidden state, using a ``MLP+LM Head'' structure, while EAGLE incorporates both the last hidden state and preceding tokens, using a transformer structure. Among these model-based drafting methods, EAGLE-2____ achieves the current state-of-the-art speed.

To further accelerate existing speculative sampling methods, recent advancements have been made at both the algorithmic and implementation levels.
At the algorithm level, HASS____ has enhanced the training tasks for draft models, AdaEAGLE____ and OPT-Tree____ introduced adaptive draft tree structures at inference time. Additionally, TriForce____ employs KV-Cache compression on draft models to accelerate the drafting process in long-context scenarios, Ouroboros____ utilize Lookahead Decoding____ to accelerates the draft models when the draft model is not lightweight enough.
From an implementation perspective, efficient LLM frameworks like vLLM____ and SGLang____ have integrated speculative sampling. DeFT____ leverages FlashAttention____ to enhance the efficiency of speculative sampling.