% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{specbench,
    title = "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding",
    author = "Xia, Heming  and
      Yang, Zhe  and
      Dong, Qingxiu  and
      Wang, Peiyi  and
      Li, Yongqi  and
      Ge, Tao  and
      Liu, Tianyu  and
      Li, Wenjie  and
      Sui, Zhifang",
    booktitle = "Findings of the ACL",
    year = "2024",
    pages = "7655--7671"
}

% HumanEval
@article{humaneval,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021},
}

% MBPP
@article{mbpp,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021},
}

% GSM8K
@article{gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021},
}

@inproceedings{eagle, 
	author = {Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang}, 
	title = {EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty}, 
	booktitle = {Proceedings of ICML},
	year = {2024}
}

@inproceedings{microsoftspec,
  title={Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation},
  author={Xia, Heming and Ge, Tao and Wang, Peiyi and Chen, Si-Qing and Wei, Furu and Sui, Zhifang},
  booktitle={Proceedings of EMNLP},
  pages={3909--3925},
  year={2023},
}

@inproceedings{googlespec,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={Proceedings of ICML},
  pages={19274--19286},
  year={2023},
}

@article{deepmindspec,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023},
}

@inproceedings{specinfer,
  title={Specinfer: Accelerating large language model serving with tree-based speculative inference and verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others},
  booktitle={Proceedings of ASPLOS},
  pages={932--949},
  year={2024}
}

@inproceedings{distillspec,
  title={DistillSpec: Improving Speculative Decoding via Knowledge Distillation},
  author={Zhou, Yongchao and Lyu, Kaifeng and Rawat, Ankit Singh and Menon, Aditya Krishna and Rostamizadeh, Afshin and Kumar, Sanjiv and Kagy, Jean-Fran{\c{c}}ois and Agarwal, Rishabh},
  booktitle={Proceedings of ICLR}
}

@inproceedings{medusa,
title={Medusa: Simple {LLM} Inference Acceleration Framework with Multiple Decoding Heads},
author={Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
booktitle={Proceedings of ICML},
year={2024}
}

@inproceedings{eagle2,
  title={EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  booktitle={Proceedings of EMNLP},
  pages={7421--7432},
  year={2024}
}

@misc{pld,
    title = {Prompt Lookup Decoding},
    author = {Apoorv Saxena},
    year = {2023},
    month = {November},
    url = {https://github.com/apoorvumang/prompt-lookup-decoding/}
}

@article{llma,
  title={Inference with reference: Lossless acceleration of large language models},
  author={Yang, Nan and Ge, Tao and Wang, Liang and Jiao, Binxing and Jiang, Daxin and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2304.04487},
  year={2023}
}

@inproceedings{rest,
    title = "{REST}: Retrieval-Based Speculative Decoding",
    author = "He, Zhenyu  and
      Zhong, Zexuan  and
      Cai, Tianle  and
      Lee, Jason  and
      He, Di",
    booktitle = "Proceedings of NAACL",
    year = "2024",
    pages = "1582--1595",
}

@inproceedings{triforce,
    title={TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding},
    author={Hanshi Sun and Zhuoming Chen and Xinyu Yang and Yuandong Tian and Beidi Chen},
    booktitle={Proceedings of COLM},
    year={2024},
}

@inproceedings{yao2025deft,
    title={De{FT}: Decoding with Flash Tree-attention for Efficient Tree-structured {LLM} Inference},
    author={Jinwei Yao and Kaiqi Chen and Kexun Zhang and Jiaxuan You and Binhang Yuan and Zeke Wang and Tao Lin},
    booktitle={Proceedings of ICLR},
    year={2025},
}

@article{zhang2024adaeagle,
  title={AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of Adaptive Draft Structures},
  author={Zhang, Situo and Wang, Hankun and Ma, Da and Zhu, Zichen and Chen, Lu and Lan, Kunyao and Yu, Kai},
  journal={arXiv preprint arXiv:2412.18910},
  year={2024}
}

@article{wang2024opt,
  title={OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure},
  author={Wang, Jikai and Su, Yi and Li, Juntao and Xia, Qingrong and Ye, Zi and Duan, Xinyu and Wang, Zhefeng and Zhang, Min},
  journal={arXiv preprint arXiv:2406.17276},
  year={2024}
}

@inproceedings{zhao-etal-2024-ouroboros,
    title = "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding",
    author = "Zhao, Weilin  and
      Huang, Yuxiang  and
      Han, Xu  and
      Xu, Wang  and
      Xiao, Chaojun  and
      Zhang, Xinrong  and
      Fang, Yewei  and
      Zhang, Kaihuo  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "Proceedings of EMNLP",
    year = "2024",
    pages = "13378--13393",
}

@inproceedings{fu2024break,
title={Break the Sequential Dependency of {LLM} Inference Using Lookahead Decoding},
author={Yichao Fu and Peter Bailis and Ion Stoica and Hao Zhang},
booktitle={Proceedings of ICML},
year={2024},
}

@inproceedings{zhang2025learning,
    title={Learning Harmonized Representations for Speculative Sampling},
    author={Lefan Zhang and Xiaodan Wang and Yanhua Huang and Ruiwen Xu},
    booktitle={Proceedings of ICLR},
    year={2025}
}

@inproceedings{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Proceedings of NeurIPS},
  pages={1877--1901},
  year={2020}
}

@article{chatgpt,
  title={Chatgpt: Optimizing language models for dialogue},
  author={OpenAI, TB},
  journal={OpenAI},
  year={2022}
}

@article{deepseekr1,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{deepseekv3,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{qwen25,
  title={Qwen2.5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@misc{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and others},
      year={2024},
      journal={arXiv preprint arXiv:2407.10671},
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{cerebras2023slimpajama,
  author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
  month = June,
  year = 2023
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Proceedings of NeurIPS},
  volume={32},
  year={2019}
}

@article{flashattention-2,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{luo2024mini,
  title={MINI-SEQUENCE TRANSFORMER: Optimizing Intermediate Memory for Long Sequences Training},
  author={Luo, Cheng and Zhao, Jiawei and Chen, Zhuoming and Chen, Beidi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2407.15892},
  year={2024}
}

@article{wijmans2024cut,
  title={Cut Your Losses in Large-Vocabulary Language Models},
  author={Wijmans, Erik and Huval, Brody and Hertzberg, Alexander and Koltun, Vladlen and Kr{\"a}henb{\"u}hl, Philipp},
  journal={arXiv preprint arXiv:2411.09009},
  year={2024}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@inproceedings{joulin2017efficient,
  title={Efficient softmax approximation for GPUs},
  author={Joulin, Armand and Ciss{\'e}, Moustapha and Grangier, David and J{\'e}gou, Herv{\'e} and others},
  booktitle={Proceedings of ICML},
  pages={1302--1310},
  year={2017},
  organization={PMLR}
}

@inproceedings{hokamp2017lexically,
  title={Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search},
  author={Hokamp, Chris and Liu, Qun},
  booktitle={Proceedings of ACL},
  pages={1535--1546},
  year={2017}
}

@article{dong2024xgrammar,
  title={Xgrammar: Flexible and efficient structured generation engine for large language models},
  author={Dong, Yixin and Ruan, Charlie F and Cai, Yaxing and Lai, Ruihang and Xu, Ziyi and Zhao, Yilong and Chen, Tianqi},
  journal={arXiv preprint arXiv:2411.15100},
  year={2024}
}

@article{zheng2024sglang,
  title={Sglang: Efficient execution of structured language model programs},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2312.07104},
  year={2024}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of SOSP},
  pages={611--626},
  year={2023}
}

@dataset{sharegpt2023,
    title = {ShareGPT},
    author = {ShareGPT},
    year = {2023},
    url = {https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered}
}

@inproceedings{mtbench,
 author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion},
 booktitle = {Proceedings of CoNLL},
 pages = {46595--46623},
 title = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
 volume = {36},
 year = {2023}
}

@inproceedings{cnn/daily_mail,
    title = "Abstractive Text Summarization using Sequence-to-sequence {RNN}s and Beyond",
    author = {Nallapati, Ramesh  and
      Zhou, Bowen  and
      dos Santos, Cicero  and
      Gu{\ensuremath{\dot{}}}l{\c{c}}ehre, {\c{C}}a{\u{g}}lar  and
      Xiang, Bing},
    booktitle = {Proceedings of CoNLL},
    year = {2016},
    pages = {280--290}
}

@article{natural_questions,
    author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
    title = {Natural Questions: A Benchmark for Question Answering Research},
    journal = {TACL},
    volume = {7},
    pages = {453-466},
    year = {2019}
}

@inproceedings{wmt14,
    title = {Findings of the 2014 Workshop on Statistical Machine Translation},
    author = {Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Federmann, Christian  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Leveling, Johannes  and
      Monz, Christof  and
      Pecina, Pavel  and
      Post, Matt  and
      Saint-Amand, Herve  and
      Soricut, Radu  and
      Specia, Lucia  and
      Tamchyna, Ale{\v{s}}},
    booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
    year = {2014},
    pages = {12--58}
}

@article{takase2024large,
  title={Large Vocabulary Size Improves Large Language Models},
  author={Takase, Sho and Ri, Ryokan and Kiyono, Shun and Kato, Takuya},
  journal={arXiv preprint arXiv:2406.16508},
  year={2024}
}

@inproceedings{tao2024scaling,
 author = {Tao, Chaofan and Liu, Qian and Dou, Longxu and Muennighoff, Niklas and Wan, Zhongwei and Luo, Ping and Lin, Min and Wong, Ngai},
 booktitle = {Proceedings of NeurIPS},
 pages = {114147--114179},
 title = {Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies},
 volume = {37},
 year = {2024}
}


@article{zipf-law,
  title={Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology},
  author={George Kingsley Zipf},
  journal={Language},
  year={1950},
  volume={26},
  pages={394},
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  volume={63},
  number={10},
  pages={1872--1897},
  year={2020},
  publisher={Springer}
}
@article{han2021pre,
  title={Pre-trained models: Past, present and future},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021},
  publisher={Elsevier}
}
@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@inproceedings{xu2023survey,
  title={A survey on model compression and acceleration for pretrained language models},
  author={Xu, Canwen and McAuley, Julian},
  booktitle={Proceedings of AAAI},
  volume={37},
  number={9},
  pages={10566--10575},
  year={2023}
}

@article{li2024large,
  title={Large language model inference acceleration: A comprehensive hardware perspective},
  author={Li, Jinhao and Xu, Jiaming and Huang, Shan and Chen, Yonghua and Li, Wen and Liu, Jun and Lian, Yaoxiu and Pan, Jiayi and Ding, Li and Zhou, Hao and others},
  journal={arXiv preprint arXiv:2410.04466},
  year={2024}
}