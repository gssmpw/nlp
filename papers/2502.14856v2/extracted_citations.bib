@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{deepmindspec,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023},
}

@inproceedings{distillspec,
  title={DistillSpec: Improving Speculative Decoding via Knowledge Distillation},
  author={Zhou, Yongchao and Lyu, Kaifeng and Rawat, Ankit Singh and Menon, Aditya Krishna and Rostamizadeh, Afshin and Kumar, Sanjiv and Kagy, Jean-Fran{\c{c}}ois and Agarwal, Rishabh},
  booktitle={Proceedings of ICLR}
}

@article{dong2024xgrammar,
  title={Xgrammar: Flexible and efficient structured generation engine for large language models},
  author={Dong, Yixin and Ruan, Charlie F and Cai, Yaxing and Lai, Ruihang and Xu, Ziyi and Zhao, Yilong and Chen, Tianqi},
  journal={arXiv preprint arXiv:2411.15100},
  year={2024}
}

@inproceedings{eagle2,
  title={EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  booktitle={Proceedings of EMNLP},
  pages={7421--7432},
  year={2024}
}

@article{flashattention-2,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@inproceedings{fu2024break,
title={Break the Sequential Dependency of {LLM} Inference Using Lookahead Decoding},
author={Yichao Fu and Peter Bailis and Ion Stoica and Hao Zhang},
booktitle={Proceedings of ICML},
year={2024},
}

@inproceedings{googlespec,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={Proceedings of ICML},
  pages={19274--19286},
  year={2023},
}

@article{han2021pre,
  title={Pre-trained models: Past, present and future},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{hokamp2017lexically,
  title={Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search},
  author={Hokamp, Chris and Liu, Qun},
  booktitle={Proceedings of ACL},
  pages={1535--1546},
  year={2017}
}

@inproceedings{joulin2017efficient,
  title={Efficient softmax approximation for GPUs},
  author={Joulin, Armand and Ciss{\'e}, Moustapha and Grangier, David and J{\'e}gou, Herv{\'e} and others},
  booktitle={Proceedings of ICML},
  pages={1302--1310},
  year={2017},
  organization={PMLR}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of SOSP},
  pages={611--626},
  year={2023}
}

@article{li2024large,
  title={Large language model inference acceleration: A comprehensive hardware perspective},
  author={Li, Jinhao and Xu, Jiaming and Huang, Shan and Chen, Yonghua and Li, Wen and Liu, Jun and Lian, Yaoxiu and Pan, Jiayi and Ding, Li and Zhou, Hao and others},
  journal={arXiv preprint arXiv:2410.04466},
  year={2024}
}

@article{llma,
  title={Inference with reference: Lossless acceleration of large language models},
  author={Yang, Nan and Ge, Tao and Wang, Liang and Jiao, Binxing and Jiang, Daxin and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2304.04487},
  year={2023}
}

@article{luo2024mini,
  title={MINI-SEQUENCE TRANSFORMER: Optimizing Intermediate Memory for Long Sequences Training},
  author={Luo, Cheng and Zhao, Jiawei and Chen, Zhuoming and Chen, Beidi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2407.15892},
  year={2024}
}

@inproceedings{medusa,
title={Medusa: Simple {LLM} Inference Acceleration Framework with Multiple Decoding Heads},
author={Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
booktitle={Proceedings of ICML},
year={2024}
}

@inproceedings{microsoftspec,
  title={Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation},
  author={Xia, Heming and Ge, Tao and Wang, Peiyi and Chen, Si-Qing and Wei, Furu and Sui, Zhifang},
  booktitle={Proceedings of EMNLP},
  pages={3909--3925},
  year={2023},
}

@misc{pld,
    title = {Prompt Lookup Decoding},
    author = {Apoorv Saxena},
    year = {2023},
    month = {November},
    url = {https://github.com/apoorvumang/prompt-lookup-decoding/}
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  volume={63},
  number={10},
  pages={1872--1897},
  year={2020},
  publisher={Springer}
}

@inproceedings{rest,
    title = "{REST}: Retrieval-Based Speculative Decoding",
    author = "He, Zhenyu  and
      Zhong, Zexuan  and
      Cai, Tianle  and
      Lee, Jason  and
      He, Di",
    booktitle = "Proceedings of NAACL",
    year = "2024",
    pages = "1582--1595",
}

@inproceedings{specinfer,
  title={Specinfer: Accelerating large language model serving with tree-based speculative inference and verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others},
  booktitle={Proceedings of ASPLOS},
  pages={932--949},
  year={2024}
}

@inproceedings{triforce,
    title={TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding},
    author={Hanshi Sun and Zhuoming Chen and Xinyu Yang and Yuandong Tian and Beidi Chen},
    booktitle={Proceedings of COLM},
    year={2024},
}

@article{wang2024opt,
  title={OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure},
  author={Wang, Jikai and Su, Yi and Li, Juntao and Xia, Qingrong and Ye, Zi and Duan, Xinyu and Wang, Zhefeng and Zhang, Min},
  journal={arXiv preprint arXiv:2406.17276},
  year={2024}
}

@article{wijmans2024cut,
  title={Cut Your Losses in Large-Vocabulary Language Models},
  author={Wijmans, Erik and Huval, Brody and Hertzberg, Alexander and Koltun, Vladlen and Kr{\"a}henb{\"u}hl, Philipp},
  journal={arXiv preprint arXiv:2411.09009},
  year={2024}
}

@inproceedings{xu2023survey,
  title={A survey on model compression and acceleration for pretrained language models},
  author={Xu, Canwen and McAuley, Julian},
  booktitle={Proceedings of AAAI},
  volume={37},
  number={9},
  pages={10566--10575},
  year={2023}
}

@inproceedings{yao2025deft,
    title={De{FT}: Decoding with Flash Tree-attention for Efficient Tree-structured {LLM} Inference},
    author={Jinwei Yao and Kaiqi Chen and Kexun Zhang and Jiaxuan You and Binhang Yuan and Zeke Wang and Tao Lin},
    booktitle={Proceedings of ICLR},
    year={2025},
}

@article{zhang2024adaeagle,
  title={AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of Adaptive Draft Structures},
  author={Zhang, Situo and Wang, Hankun and Ma, Da and Zhu, Zichen and Chen, Lu and Lan, Kunyao and Yu, Kai},
  journal={arXiv preprint arXiv:2412.18910},
  year={2024}
}

@inproceedings{zhang2025learning,
    title={Learning Harmonized Representations for Speculative Sampling},
    author={Lefan Zhang and Xiaodan Wang and Yanhua Huang and Ruiwen Xu},
    booktitle={Proceedings of ICLR},
    year={2025}
}

@inproceedings{zhao-etal-2024-ouroboros,
    title = "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding",
    author = "Zhao, Weilin  and
      Huang, Yuxiang  and
      Han, Xu  and
      Xu, Wang  and
      Xiao, Chaojun  and
      Zhang, Xinrong  and
      Fang, Yewei  and
      Zhang, Kaihuo  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "Proceedings of EMNLP",
    year = "2024",
    pages = "13378--13393",
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{zheng2024sglang,
  title={Sglang: Efficient execution of structured language model programs},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2312.07104},
  year={2024}
}

