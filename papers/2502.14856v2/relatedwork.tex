\section{Related Work}
\label{sec:relate}

This section mainly introduces model acceleration methods related to large vocabulary and speculative sampling. More details on how LLMs work can refer to surveys~\citep{qiu2020pre,han2021pre,bommasani2021opportunities,zhao2023survey}. Other acceleration methods such as quantization and distillation can refer to suverys~\citep{xu2023survey,li2024large}.

\subsection{Acceration on Large Vocabulary}
Recent advancements in large language models (LLMs) have prompted a growing interest in addressing the challenges associated with large vocabularies. While several optimization efforts have been proposed to tackle these issues, the majority focus primarily on the training phase.
Computing the LM Head and the loss function over large vocabularies requires storing a huge intermediate state before gradient computation. Therefore, MST~\cite{luo2024mini} and CCE~\cite{wijmans2024cut} tried to mitigate the memory overhead caused by computing loss functions over large vocabularies. These approaches address the issue by using input partitioning or weight partitioning, and conduct activation recomputation~\cite{chen2016training} during the backward propagation.
In addition to the aforementioned works that require no modifications to the model architecture, \citet{joulin2017efficient} proposes a hierarchical vocabulary structure to eliminate the computation of irrelevant vocabulary adaptively.

Constrained Decoding~\cite{hokamp2017lexically, dong2024xgrammar} restricts the vocabulary space to generate highly structured outputs, particularly in the context of LLM agents, where the generated content must adhere to specific formats, such as producing parsable code or invocable functions.

\subsection{Speculative Sampling}

Traditional autoregressive generation in LLMs suffers from low generation speed due to the sequential nature of token prediction. To address this limitation, speculative sampling has emerged as a promising approach, leveraging draft-then-verification paradigms to accelerate decoding~\cite{microsoftspec, googlespec, deepmindspec}. Existing speculative sampling methods can be categorized into two branches:
(1) \textit{retrieval-based drafting} approaches like PLD~\cite{pld}, LLMA~\cite{llma}, and REST~\cite{rest} retrieve relevant context from the prompt, gaining significant speedups in context-dependent tasks (e.g., summarization) by reusing retrieved text spans from the prompt.
(2) \textit{model-based drafting} methods exemplified by SpecInfer~\cite{specinfer}, DistillSpec~\cite{distillspec}, Medusa~\cite{medusa} and EAGLE~\cite{eagle2}, which employ a draft model for general-purpose acceleration. Our work focuses on the latter category due to its broader applicability.
The draft models' structures also differ. For example, Medusa generates draft tokens based solely on the model's last hidden state, using a ``MLP+LM Head'' structure, while EAGLE incorporates both the last hidden state and preceding tokens, using a transformer structure. Among these model-based drafting methods, EAGLE-2~\cite{eagle2} achieves the current state-of-the-art speed.

To further accelerate existing speculative sampling methods, recent advancements have been made at both the algorithmic and implementation levels.
At the algorithm level, HASS~\cite{zhang2025learning} has enhanced the training tasks for draft models, AdaEAGLE~\cite{zhang2024adaeagle} and OPT-Tree~\cite{wang2024opt} introduced adaptive draft tree structures at inference time. Additionally, TriForce~\cite{triforce} employs KV-Cache compression on draft models to accelerate the drafting process in long-context scenarios, Ouroboros~\cite{zhao-etal-2024-ouroboros} utilize Lookahead Decoding~\cite{fu2024break} to accelerates the draft models when the draft model is not lightweight enough.
From an implementation perspective, efficient LLM frameworks like vLLM~\cite{kwon2023efficient} and SGLang~\cite{zheng2024sglang} have integrated speculative sampling. DeFT~\cite{yao2025deft} leverages FlashAttention~\cite{flashattention-2} to enhance the efficiency of speculative sampling.