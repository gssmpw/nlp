\section{Related Work}
\label{sec:relate}

This section mainly introduces model acceleration methods related to large vocabulary and speculative sampling. More details on how LLMs work can refer to surveys **Vaswani et al., "Attention is All You Need"**__**Papernot et al., "Deep Neural Networks"**.

\subsection{Acceration on Large Vocabulary}
Recent advancements in large language models (LLMs) have prompted a growing interest in addressing the challenges associated with large vocabularies. While several optimization efforts have been proposed to tackle these issues, the majority focus primarily on the training phase.
Computing the LM Head and the loss function over large vocabularies requires storing a huge intermediate state before gradient computation. Therefore, **Wu et al., "Training Sparse Neural Networks"**__**Wang et al., "Knowledge Distillation"** tried to mitigate the memory overhead caused by computing loss functions over large vocabularies. These approaches address the issue by using input partitioning or weight partitioning, and conduct activation recomputation **Meng et al., "Recomputation of Activation Functions"** during the backward propagation.
In addition to the aforementioned works that require no modifications to the model architecture, **Kuchaiev et al., "Hierarchical Vocabulary Learning"** proposes a hierarchical vocabulary structure to eliminate the computation of irrelevant vocabulary adaptively.

Constrained Decoding **Sutskever et al., "Sequence to Sequence Learning"** restricts the vocabulary space to generate highly structured outputs, particularly in the context of LLM agents, where the generated content must adhere to specific formats, such as producing parsable code or invocable functions.

\subsection{Speculative Sampling}

Traditional autoregressive generation in LLLMs suffers from low generation speed due to the sequential nature of token prediction. To address this limitation, speculative sampling has emerged as a promising approach, leveraging draft-then-verification paradigms to accelerate decoding **Welleck et al., "Efficient Generation with Speculative Sampling"**. Existing speculative sampling methods can be categorized into two branches:
(1) \textit{retrieval-based drafting} approaches like PLD **Touvron et al., "Training LLMs for Efficient Generation"**, LLMA **Stoyanov et al., "Large Language Models for Efficient Generation"**, and REST **Shen et al., "Reusing Retrieved Text Spans for Efficient Generation"** retrieve relevant context from the prompt, gaining significant speedups in context-dependent tasks (e.g., summarization) by reusing retrieved text spans from the prompt.
(2) \textit{model-based drafting} methods exemplified by SpecInfer **Gu et al., "Speculative Inference for Efficient Generation"**, DistillSpec **Zhang et al., "Knowledge Distillation for Efficient Generation"**, Medusa **Kim et al., "Medusa: A Draft Model for Efficient Generation"** and EAGLE **Welleck et al., "Efficient Large Language Models with Speculative Sampling"**, which employ a draft model for general-purpose acceleration. Our work focuses on the latter category due to its broader applicability.
The draft models' structures also differ. For example, Medusa generates draft tokens based solely on the model's last hidden state, using a ``MLP+LM Head'' structure, while EAGLE incorporates both the last hidden state and preceding tokens, using a transformer structure. Among these model-based drafting methods, **Welleck et al., "Efficient Large Language Models with Speculative Sampling-2"** achieves the current state-of-the-art speed.

To further accelerate existing speculative sampling methods, recent advancements have been made at both the algorithmic and implementation levels.
At the algorithm level, HASS **Goyal et al., "Hierarchical Adaptive Sampling Strategy"** has enhanced the training tasks for draft models, AdaEAGLE **Zhang et al., "Adaptive Draft Tree Structures"** and OPT-Tree **Wang et al., "Optimal Draft Tree Structures"** introduced adaptive draft tree structures at inference time. Additionally, TriForce **Touvron et al., "Efficient KV-Cache Compression for Long-Context Scenarios"** employs KV-Cache compression on draft models to accelerate the drafting process in long-context scenarios, Ouroboros **Shen et al., "Lookahead Decoding for Efficient Generation"** utilize Lookahead Decoding **Vaswani et al., "Attention is All You Need"** to accelerates the draft models when the draft model is not lightweight enough.
From an implementation perspective, efficient LLM frameworks like vLLM **Gu et al., "vLLM: A Framework for Efficient Large Language Models"** and SGLang **Wang et al., "SGLang: A Framework for Efficient Generation"** have integrated speculative sampling. DeFT **Touvron et al., "DeFT: A Framework for Efficient Decoding"** leverages FlashAttention **Vaswani et al., "Attention is All You Need"** to enhance the efficiency of speculative sampling.