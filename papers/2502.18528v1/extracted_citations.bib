@inproceedings{huang_penheal_2023,
	address = {Salt Lake City UT USA},
	title = {{PenHeal}: {A} {Two}-{Stage} {LLM} {Framework} for {Automated} {Pentesting} and {Optimal} {Remediation}},
	isbn = {9798400712296},
	shorttitle = {{PenHeal}},
	url = {https://dl.acm.org/doi/10.1145/3689933.3690831},
	doi = {10.1145/3689933.3690831},
	language = {en},
	urldate = {2025-02-14},
	booktitle = {Proceedings of the {Workshop} on {Autonomous} {Cybersecurity}},
	publisher = {ACM},
	author = {Huang, Junjie and Zhu, Quanyan},
	month = nov,
	year = {2023},
	pages = {11--22},
}

@misc{muzsai_hacksynth_2024,
	title = {{HackSynth}: {LLM} {Agent} and {Evaluation} {Framework} for {Autonomous} {Penetration} {Testing}},
	shorttitle = {{HackSynth}},
	url = {http://arxiv.org/abs/2412.01778},
	doi = {10.48550/arXiv.2412.01778},
	abstract = {We introduce HackSynth, a novel Large Language Model (LLM)-based agent capable of autonomous penetration testing. HackSynth's dual-module architecture includes a Planner and a Summarizer, which enable it to generate commands and process feedback iteratively. To benchmark HackSynth, we propose two new Capture The Flag (CTF)-based benchmark sets utilizing the popular platforms PicoCTF and OverTheWire. These benchmarks include two hundred challenges across diverse domains and difficulties, providing a standardized framework for evaluating LLM-based penetration testing agents. Based on these benchmarks, extensive experiments are presented, analyzing the core parameters of HackSynth, including creativity (temperature and top-p) and token utilization. Multiple open source and proprietary LLMs were used to measure the agent's capabilities. The experiments show that the agent performed best with the GPT-4o model, better than what the GPT-4o's system card suggests. We also discuss the safety and predictability of HackSynth's actions. Our findings indicate the potential of LLM-based agents in advancing autonomous penetration testing and the importance of robust safeguards. HackSynth and the benchmarks are publicly available to foster research on autonomous cybersecurity solutions.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Muzsai, Lajos and Imolai, David and Lukács, András},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01778 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{pasquini_hacking_2024,
	title = {Hacking {Back} the {AI}-{Hacker}: {Prompt} {Injection} as a {Defense} {Against} {LLM}-driven {Cyberattacks}},
	shorttitle = {Hacking {Back} the {AI}-{Hacker}},
	url = {http://arxiv.org/abs/2410.20911},
	doi = {10.48550/arXiv.2410.20911},
	abstract = {Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker's LLM to disrupt their own operations (passive defense) or even compromise the attacker's machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker's LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95\% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project\_mantis},
	urldate = {2025-02-15},
	publisher = {arXiv},
	author = {Pasquini, Dario and Kornaropoulos, Evgenios M. and Ateniese, Giuseppe},
	month = nov,
	year = {2024},
	note = {arXiv:2410.20911 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{xu_autoattacker_2024,
	title = {{AutoAttacker}: {A} {Large} {Language} {Model} {Guided} {System} to {Implement} {Automatic} {Cyber}-attacks},
	shorttitle = {{AutoAttacker}},
	url = {http://arxiv.org/abs/2403.01038},
	doi = {10.48550/arXiv.2403.01038},
	abstract = {Large language models (LLMs) have demonstrated impressive results on natural language tasks, and security researchers are beginning to employ them in both offensive and defensive systems. In cyber-security, there have been multiple research efforts that utilize LLMs focusing on the pre-breach stage of attacks like phishing and malware generation. However, so far there lacks a comprehensive study regarding whether LLM-based systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or "hands-on-keyboard" attacks, under various attack techniques and environments. As LLMs inevitably advance, they may be able to automate both the pre- and post-breach attack stages. This shift may transform organizational attacks from rare, expert-led events to frequent, automated operations requiring no expertise and executed at automation speed and scale. This risks fundamentally changing global computer security and correspondingly causing substantial economic impacts, and a goal of this work is to better understand these risks now so we can better prepare for these inevitable ever-more-capable LLMs on the horizon. On the immediate impact side, this research serves three purposes. First, an automated LLM-based, post-breach exploitation framework can help analysts quickly test and continually improve their organization's network security posture against previously unseen attacks. Second, an LLM-based penetration test system can extend the effectiveness of red teams with a limited number of human analysts. Finally, this research can help defensive systems and teams learn to detect novel attack behaviors preemptively before their use in the wild....},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Xu, Jiacen and Stokes, Jack W. and McDonald, Geoff and Bai, Xuesong and Marshall, David and Wang, Siyue and Swaminathan, Adith and Li, Zhou},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01038 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{zhang_when_2025,
	title = {When {LLMs} meet cybersecurity: a systematic literature review},
	volume = {8},
	issn = {2523-3246},
	shorttitle = {When {LLMs} meet cybersecurity},
	url = {https://doi.org/10.1186/s42400-025-00361-w},
	doi = {10.1186/s42400-025-00361-w},
	abstract = {The rapid development of large language models (LLMs) has opened new avenues across various fields, including cybersecurity, which faces an evolving threat landscape and demand for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper addresses this gap by providing a systematic literature review, covering the analysis of over 300 works, encompassing 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three key research questions: the construction of cybersecurity-oriented LLMs, the application of LLMs to various cybersecurity tasks, the challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices and serve as a valuable resource for applying LLMs in this field. We also maintain and regularly update a list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.},
	number = {1},
	urldate = {2025-02-14},
	journal = {Cybersecurity},
	author = {Zhang, Jie and Bu, Haoyu and Wen, Hui and Liu, Yongji and Fei, Haiqiang and Xi, Rongrong and Li, Lun and Yang, Yun and Zhu, Hongsong and Meng, Dan},
	month = feb,
	year = {2025},
	keywords = {Agent, Cyber attack, Cyber defense, Cybersecurity, Large language model},
	pages = {55},
}

