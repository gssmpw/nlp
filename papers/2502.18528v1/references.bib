
@misc{dong_safeguarding_2024,
	title = {Safeguarding {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Safeguarding {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.02622},
	doi = {10.48550/arXiv.2406.02622},
	abstract = {In the burgeoning field of Large Language Models (LLMs), developing a robust safety mechanism, colloquially known as "safeguards" or "guardrails", has become imperative to ensure the ethical use of LLMs within prescribed boundaries. This article provides a systematic literature review on the current status of this critical mechanism. It discusses its major challenges and how it can be enhanced into a comprehensive mechanism dealing with ethical issues in various contexts. First, the paper elucidates the current landscape of safeguarding mechanisms that major LLM service providers and the open-source community employ. This is followed by the techniques to evaluate, analyze, and enhance some (un)desirable properties that a guardrail might want to enforce, such as hallucinations, fairness, privacy, and so on. Based on them, we review techniques to circumvent these controls (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the techniques mentioned above represent the current status and the active research trends, we also discuss several challenges that cannot be easily dealt with by the methods and present our vision on how to implement a comprehensive guardrail through the full consideration of multi-disciplinary approach, neural-symbolic method, and systems development lifecycle.},
	urldate = {2025-02-21},
	publisher = {arXiv},
	author = {Dong, Yi and Mu, Ronghui and Zhang, Yanghao and Sun, Siqi and Zhang, Tianle and Wu, Changshun and Jin, Gaojie and Qi, Yi and Hu, Jinwei and Meng, Jie and Bensalem, Saddek and Huang, Xiaowei},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02622 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{ahmed_spade_2025,
	title = {{SPADE}: {Enhancing} {Adaptive} {Cyber} {Deception} {Strategies} with {Generative} {AI} and {Structured} {Prompt} {Engineering}},
	shorttitle = {{SPADE}},
	url = {http://arxiv.org/abs/2501.00940},
	doi = {10.48550/arXiv.2501.00940},
	abstract = {The rapid evolution of modern malware presents significant challenges to the development of effective defense mechanisms. Traditional cyber deception techniques often rely on static or manually configured parameters, limiting their adaptability to dynamic and sophisticated threats. This study leverages Generative AI (GenAI) models to automate the creation of adaptive cyber deception ploys, focusing on structured prompt engineering (PE) to enhance relevance, actionability, and deployability. We introduce a systematic framework (SPADE) to address inherent challenges large language models (LLMs) pose to adaptive deceptions, including generalized outputs, ambiguity, under-utilization of contextual information, and scalability constraints. Evaluations across diverse malware scenarios using metrics such as Recall, Exact Match (EM), BLEU Score, and expert quality assessments identified ChatGPT-4o as the top performer. Additionally, it achieved high engagement (93\%) and accuracy (96\%) with minimal refinements. Gemini and ChatGPT-4o Mini demonstrated competitive performance, with Llama3.2 showing promise despite requiring further optimization. These findings highlight the transformative potential of GenAI in automating scalable, adaptive deception strategies and underscore the critical role of structured PE in advancing real-world cybersecurity applications.},
	urldate = {2025-02-21},
	publisher = {arXiv},
	author = {Ahmed, Shihab and Rahman, A. B. M. Mohaimenur and Alam, Md Morshed and Sajid, Md Sajidul Islam},
	month = jan,
	year = {2025},
	note = {arXiv:2501.00940 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{sladic_llm_2024,
	title = {{LLM} in the {Shell}: {Generative} {Honeypots}},
	shorttitle = {{LLM} in the {Shell}},
	url = {https://ieeexplore.ieee.org/document/10628775},
	doi = {10.1109/EuroSPW61312.2024.00054},
	abstract = {Honeypots are essential tools in cybersecurity for early detection, threat intelligence gathering, and analysis of attacker's behavior. However, most of them lack the required realism to engage and fool human attackers long-term. Being easy to distinguish honeypots strongly hinders their effectiveness. This can happen because they are too deterministic, lack adaptability, or lack deepness. This work introduces shelLM, a dynamic and realistic software honeypot based on Large Language Models that generates Linux-like shell output. We designed and implemented shelLM using cloud-based LLMs. We evaluated if shelLM can generate output as expected from a real Linux shell. The evaluation was done by asking cybersecurity researchers to use the honeypot and give feedback if each answer from the honeypot was the expected one from a Linux shell. Results indicate that shelLM can create credible and dynamic answers capable of addressing the limitations of current honeypots. ShelLM reached a TNR of 0.90, convincing humans it was consistent with a real Linux shell. The source code and prompts for replicating the experiments have been publicly available.},
	urldate = {2025-02-20},
	booktitle = {2024 {IEEE} {European} {Symposium} on {Security} and {Privacy} {Workshops} ({EuroS}\&{PW})},
	author = {Sladić, Muris and Valeros, Veronica and Catania, Carlos and Garcia, Sebastian},
	month = jul,
	year = {2024},
	note = {ISSN: 2768-0657},
	keywords = {Computer security, Large Language Models, Large language models, Linux, Software, Source coding, honeypots, shelLM},
	pages = {430--435},
}

@misc{pasquini_hacking_2024,
	title = {Hacking {Back} the {AI}-{Hacker}: {Prompt} {Injection} as a {Defense} {Against} {LLM}-driven {Cyberattacks}},
	shorttitle = {Hacking {Back} the {AI}-{Hacker}},
	url = {http://arxiv.org/abs/2410.20911},
	doi = {10.48550/arXiv.2410.20911},
	abstract = {Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker's LLM to disrupt their own operations (passive defense) or even compromise the attacker's machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker's LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95\% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project\_mantis},
	urldate = {2025-02-15},
	publisher = {arXiv},
	author = {Pasquini, Dario and Kornaropoulos, Evgenios M. and Ateniese, Giuseppe},
	month = nov,
	year = {2024},
	note = {arXiv:2410.20911 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{pasquini_hacking_2024-1,
	title = {Hacking {Back} the {AI}-{Hacker}: {Prompt} {Injection} as a {Defense} {Against} {LLM}-driven {Cyberattacks}},
	shorttitle = {Hacking {Back} the {AI}-{Hacker}},
	url = {http://arxiv.org/abs/2410.20911},
	doi = {10.48550/arXiv.2410.20911},
	abstract = {Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker's LLM to disrupt their own operations (passive defense) or even compromise the attacker's machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker's LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95\% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project\_mantis},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Pasquini, Dario and Kornaropoulos, Evgenios M. and Ateniese, Giuseppe},
	month = nov,
	year = {2024},
	note = {arXiv:2410.20911 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{xu_autoattacker_2024,
	title = {{AutoAttacker}: {A} {Large} {Language} {Model} {Guided} {System} to {Implement} {Automatic} {Cyber}-attacks},
	shorttitle = {{AutoAttacker}},
	url = {http://arxiv.org/abs/2403.01038},
	doi = {10.48550/arXiv.2403.01038},
	abstract = {Large language models (LLMs) have demonstrated impressive results on natural language tasks, and security researchers are beginning to employ them in both offensive and defensive systems. In cyber-security, there have been multiple research efforts that utilize LLMs focusing on the pre-breach stage of attacks like phishing and malware generation. However, so far there lacks a comprehensive study regarding whether LLM-based systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or "hands-on-keyboard" attacks, under various attack techniques and environments. As LLMs inevitably advance, they may be able to automate both the pre- and post-breach attack stages. This shift may transform organizational attacks from rare, expert-led events to frequent, automated operations requiring no expertise and executed at automation speed and scale. This risks fundamentally changing global computer security and correspondingly causing substantial economic impacts, and a goal of this work is to better understand these risks now so we can better prepare for these inevitable ever-more-capable LLMs on the horizon. On the immediate impact side, this research serves three purposes. First, an automated LLM-based, post-breach exploitation framework can help analysts quickly test and continually improve their organization's network security posture against previously unseen attacks. Second, an LLM-based penetration test system can extend the effectiveness of red teams with a limited number of human analysts. Finally, this research can help defensive systems and teams learn to detect novel attack behaviors preemptively before their use in the wild....},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Xu, Jiacen and Stokes, Jack W. and McDonald, Geoff and Bai, Xuesong and Marshall, David and Wang, Siyue and Swaminathan, Adith and Li, Zhou},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01038 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{muzsai_hacksynth_2024,
	title = {{HackSynth}: {LLM} {Agent} and {Evaluation} {Framework} for {Autonomous} {Penetration} {Testing}},
	shorttitle = {{HackSynth}},
	url = {http://arxiv.org/abs/2412.01778},
	doi = {10.48550/arXiv.2412.01778},
	abstract = {We introduce HackSynth, a novel Large Language Model (LLM)-based agent capable of autonomous penetration testing. HackSynth's dual-module architecture includes a Planner and a Summarizer, which enable it to generate commands and process feedback iteratively. To benchmark HackSynth, we propose two new Capture The Flag (CTF)-based benchmark sets utilizing the popular platforms PicoCTF and OverTheWire. These benchmarks include two hundred challenges across diverse domains and difficulties, providing a standardized framework for evaluating LLM-based penetration testing agents. Based on these benchmarks, extensive experiments are presented, analyzing the core parameters of HackSynth, including creativity (temperature and top-p) and token utilization. Multiple open source and proprietary LLMs were used to measure the agent's capabilities. The experiments show that the agent performed best with the GPT-4o model, better than what the GPT-4o's system card suggests. We also discuss the safety and predictability of HackSynth's actions. Our findings indicate the potential of LLM-based agents in advancing autonomous penetration testing and the importance of robust safeguards. HackSynth and the benchmarks are publicly available to foster research on autonomous cybersecurity solutions.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Muzsai, Lajos and Imolai, David and Lukács, András},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01778 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{kong_vulnbot_2025,
	title = {{VulnBot}: {Autonomous} {Penetration} {Testing} for {A} {Multi}-{Agent} {Collaborative} {Framework}},
	shorttitle = {{VulnBot}},
	url = {http://arxiv.org/abs/2501.13411},
	doi = {10.48550/arXiv.2501.13411},
	abstract = {Penetration testing is a vital practice for identifying and mitigating vulnerabilities in cybersecurity systems, but its manual execution is labor-intensive and time-consuming. Existing large language model (LLM)-assisted or automated penetration testing approaches often suffer from inefficiencies, such as a lack of contextual understanding and excessive, unstructured data generation. This paper presents VulnBot, an automated penetration testing framework that leverages LLMs to simulate the collaborative workflow of human penetration testing teams through a multi-agent system. To address the inefficiencies and reliance on manual intervention in traditional penetration testing methods, VulnBot decomposes complex tasks into three specialized phases: reconnaissance, scanning, and exploitation. These phases are guided by a penetration task graph (PTG) to ensure logical task execution. Key design features include role specialization, penetration path planning, inter-agent communication, and generative penetration behavior. Experimental results demonstrate that VulnBot outperforms baseline models such as GPT-4 and Llama3 in automated penetration testing tasks, particularly showcasing its potential in fully autonomous testing on real-world machines.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Kong, He and Hu, Die and Ge, Jingguo and Li, Liangxiong and Li, Tong and Wu, Bingzhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.13411 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{huang_penheal_2023,
	address = {Salt Lake City UT USA},
	title = {{PenHeal}: {A} {Two}-{Stage} {LLM} {Framework} for {Automated} {Pentesting} and {Optimal} {Remediation}},
	isbn = {9798400712296},
	shorttitle = {{PenHeal}},
	url = {https://dl.acm.org/doi/10.1145/3689933.3690831},
	doi = {10.1145/3689933.3690831},
	language = {en},
	urldate = {2025-02-14},
	booktitle = {Proceedings of the {Workshop} on {Autonomous} {Cybersecurity}},
	publisher = {ACM},
	author = {Huang, Junjie and Zhu, Quanyan},
	month = nov,
	year = {2023},
	pages = {11--22},
}

@misc{sheng_lprotector_2024,
	title = {{LProtector}: {An} {LLM}-driven {Vulnerability} {Detection} {System}},
	shorttitle = {{LProtector}},
	url = {http://arxiv.org/abs/2411.06493},
	doi = {10.48550/arXiv.2411.06493},
	abstract = {This paper presents LProtector, an automated vulnerability detection system for C/C++ codebases driven by the large language model (LLM) GPT-4o and Retrieval-Augmented Generation (RAG). As software complexity grows, traditional methods face challenges in detecting vulnerabilities effectively. LProtector leverages GPT-4o's powerful code comprehension and generation capabilities to perform binary classification and identify vulnerabilities within target codebases. We conducted experiments on the Big-Vul dataset, showing that LProtector outperforms two state-of-the-art baselines in terms of F1 score, demonstrating the potential of integrating LLMs with vulnerability detection.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Sheng, Ze and Wu, Fenghua and Zuo, Xiangwu and Li, Chao and Qiao, Yuxin and Hang, Lei},
	month = nov,
	year = {2024},
	note = {arXiv:2411.06493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{song_audit-llm_2024,
	title = {Audit-{LLM}: {Multi}-{Agent} {Collaboration} for {Log}-based {Insider} {Threat} {Detection}},
	shorttitle = {Audit-{LLM}},
	url = {http://arxiv.org/abs/2408.08902},
	doi = {10.48550/arXiv.2408.08902},
	abstract = {Log-based insider threat detection (ITD) detects malicious user activities by auditing log entries. Recently, large language models (LLMs) with strong common sense knowledge have emerged in the domain of ITD. Nevertheless, diverse activity types and overlong log files pose a significant challenge for LLMs in directly discerning malicious ones within myriads of normal activities. Furthermore, the faithfulness hallucination issue from LLMs aggravates its application difficulty in ITD, as the generated conclusion may not align with user commands and activity context. In response to these challenges, we introduce Audit-LLM, a multi-agent log-based insider threat detection framework comprising three collaborative agents: (i) the Decomposer agent, breaking down the complex ITD task into manageable sub-tasks using Chain-of-Thought (COT) reasoning;(ii) the Tool Builder agent, creating reusable tools for sub-tasks to overcome context length limitations in LLMs; and (iii) the Executor agent, generating the final detection conclusion by invoking constructed tools. To enhance conclusion accuracy, we propose a pair-wise Evidence-based Multi-agent Debate (EMAD) mechanism, where two independent Executors iteratively refine their conclusions through reasoning exchange to reach a consensus. Comprehensive experiments conducted on three publicly available ITD datasets-CERT r4.2, CERT r5.2, and PicoDomain-demonstrate the superiority of our method over existing baselines and show that the proposed EMAD significantly improves the faithfulness of explanations generated by LLMs.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Song, Chengyu and Ma, Linru and Zheng, Jianming and Liao, Jinzhi and Kuang, Hongyu and Yang, Lin},
	month = aug,
	year = {2024},
	note = {arXiv:2408.08902 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@article{pratama_cipher_2024,
	title = {{CIPHER}: {Cybersecurity} {Intelligent} {Penetration}-{Testing} {Helper} for {Ethical} {Researcher}},
	volume = {24},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {{CIPHER}},
	url = {https://www.mdpi.com/1424-8220/24/21/6878},
	doi = {10.3390/s24216878},
	abstract = {Penetration testing, a critical component of cybersecurity, typically requires extensive time and effort to find vulnerabilities. Beginners in this field often benefit from collaborative approaches with the community or experts. To address this, we develop Cybersecurity Intelligent Penetration-testing Helper for Ethical Researchers (CIPHER), a large language model specifically trained to assist in penetration testing tasks as a chatbot. Unlike software development, penetration testing involves domain-specific knowledge that is not widely documented or easily accessible, necessitating a specialized training approach for AI language models. CIPHER was trained using over 300 high-quality write-ups of vulnerable machines, hacking techniques, and documentation of open-source penetration testing tools augmented in an expert response structure. Additionally, we introduced the Findings, Action, Reasoning, and Results (FARR) Flow augmentation, a novel method to augment penetration testing write-ups to establish a fully automated pentesting simulation benchmark tailored for large language models. This approach fills a significant gap in traditional cybersecurity Q\&A benchmarks and provides a realistic and rigorous standard for evaluating LLM’s technical knowledge, reasoning capabilities, and practical utility in dynamic penetration testing scenarios. In our assessments, CIPHER achieved the best overall performance in providing accurate suggestion responses compared to other open-source penetration testing models of similar size and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B Chat, particularly on insane difficulty machine setups. This demonstrates that the current capabilities of general large language models (LLMs) are insufficient for effectively guiding users through the penetration testing process. We also discuss the potential for improvement through scaling and the development of better benchmarks using FARR Flow augmentation results.},
	language = {en},
	number = {21},
	urldate = {2025-02-14},
	journal = {Sensors},
	author = {Pratama, Derry and Suryanto, Naufal and Adiputra, Andro Aprila and Le, Thi-Thu-Huong and Kadiptya, Ahmada Yusril and Iqbal, Muhammad and Kim, Howon},
	month = jan,
	year = {2024},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI penetration testing assistant, LLM evaluation, domain specific LLM, large language model, penetration testing, pentesting LLM, vulnerabillity detection},
	pages = {6878},
}

@article{zhang_when_2025,
	title = {When {LLMs} meet cybersecurity: a systematic literature review},
	volume = {8},
	issn = {2523-3246},
	shorttitle = {When {LLMs} meet cybersecurity},
	url = {https://doi.org/10.1186/s42400-025-00361-w},
	doi = {10.1186/s42400-025-00361-w},
	abstract = {The rapid development of large language models (LLMs) has opened new avenues across various fields, including cybersecurity, which faces an evolving threat landscape and demand for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper addresses this gap by providing a systematic literature review, covering the analysis of over 300 works, encompassing 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three key research questions: the construction of cybersecurity-oriented LLMs, the application of LLMs to various cybersecurity tasks, the challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices and serve as a valuable resource for applying LLMs in this field. We also maintain and regularly update a list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.},
	number = {1},
	urldate = {2025-02-14},
	journal = {Cybersecurity},
	author = {Zhang, Jie and Bu, Haoyu and Wen, Hui and Liu, Yongji and Fei, Haiqiang and Xi, Rongrong and Li, Lun and Yang, Yun and Zhu, Hongsong and Meng, Dan},
	month = feb,
	year = {2025},
	keywords = {Agent, Cyber attack, Cyber defense, Cybersecurity, Large language model},
	pages = {55},
}

@misc{zelikman_star_2022,
	title = {{STaR}: {Bootstrapping} {Reasoning} {With} {Reasoning}},
	shorttitle = {{STaR}},
	url = {http://arxiv.org/abs/2203.14465},
	doi = {10.48550/arXiv.2203.14465},
	abstract = {Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\${\textbackslash}times\$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D.},
	month = may,
	year = {2022},
	note = {arXiv:2203.14465 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{Garcia_NetSecGame_a_RL,
	title = {{NetSecGame}, a {Reinforcement} {Learning} envirnoment for training and evaluating {AI} agents in network security tasks.},
	url = {https://github.com/stratosphereips/NetSecGame},
	author = {Garcia, Sebastian and Lukas, Ondrej and Rigaki, Maria and Catania, Carlos},
}

@article{fikes_strips_1971,
	title = {Strips: {A} new approach to the application of theorem proving to problem solving},
	volume = {2},
	issn = {00043702},
	shorttitle = {Strips},
	doi = {10.1016/0004-3702(71)90010-5},
	language = {en},
	number = {3-4},
	urldate = {2024-07-24},
	journal = {Artificial Intelligence},
	author = {Fikes, Richard E. and Nilsson, Nils J.},
	month = dec,
	year = {1971},
	pages = {189--208},
}

@inproceedings{yao_react_2022,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	language = {en},
	urldate = {2023-07-26},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R. and Cao, Yuan},
	month = sep,
	year = {2022},
}

@inproceedings{shinn_reflexion_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '23},
	title = {Reflexion: language agents with verbal reinforcement learning},
	shorttitle = {Reflexion},
	urldate = {2024-07-18},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = may,
	year = {2024},
	pages = {8634--8652},
}

@inproceedings{wang_describe_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '23},
	title = {Describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents},
	shorttitle = {Describe, explain, plan and select},
	urldate = {2024-07-18},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wang, Zihao and Cai, Shaofei and Chen, Guanzhou and Liu, Anji and Ma, Xiaojian and Liang, Yitao and CraftJarvis, Team},
	month = may,
	year = {2024},
	pages = {34153--34189},
}

@misc{wang_voyager_2023,
	title = {Voyager: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models}},
	shorttitle = {Voyager},
	url = {http://arxiv.org/abs/2305.16291},
	doi = {10.48550/arXiv.2305.16291},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	month = may,
	year = {2023},
	note = {arXiv:2305.16291 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{wang_voyager_2024,
	title = {Voyager: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models}},
	issn = {2835-8856},
	shorttitle = {Voyager},
	language = {en},
	urldate = {2024-07-18},
	journal = {Transactions on Machine Learning Research},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	year = {2024},
}

@inproceedings{park_generative_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	isbn = {9798400701320},
	shorttitle = {Generative {Agents}},
	doi = {10.1145/3586183.3606763},
	urldate = {2024-07-18},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = oct,
	year = {2023},
	pages = {1--22},
}

@misc{park_generative_2023-1,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	doi = {10.48550/arXiv.2304.03442},
	urldate = {2023-05-14},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03442 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{janisch_nasimemu_2024,
	address = {Cham},
	title = {{NASimEmu}: {Network} {Attack} {Simulator} \& {Emulator} for {Training} {Agents} {Generalizing} to {Novel} {Scenarios}},
	isbn = {978-3-031-54129-2},
	shorttitle = {{NASimEmu}},
	doi = {10.1007/978-3-031-54129-2_35},
	language = {en},
	booktitle = {Computer {Security}. {ESORICS} 2023 {International} {Workshops}},
	publisher = {Springer Nature Switzerland},
	author = {Janisch, Jaromír and Pevný, Tomáš and Lisý, Viliam},
	year = {2024},
	pages = {589--608},
}

@inproceedings{misra_lms_2022,
	title = {{LMs} go {Phishing}: {Adapting} {Pre}-trained {Language} {Models} to {Detect} {Phishing} {Emails}},
	isbn = {978-1-66549-402-1},
	shorttitle = {{LMs} go {Phishing}},
	doi = {10.1109/WI-IAT55865.2022.00028},
	booktitle = {2022 {IEEE}/{WIC}/{ACM} {International} {Joint} {Conference} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology} ({WI}-{IAT})},
	publisher = {IEEE},
	author = {Misra, Kanishka and Rayz, Julia Taylor},
	month = nov,
	year = {2022},
	keywords = {Adaptation models, Analytical models, BERT, Data models, GPT2, Natural language processing, Phishing, Training, Training data, deep learning, language models, phishing detection},
	pages = {135--142},
}

@misc{andrew_developing_nodate,
	title = {Developing {Optimal} {Causal} {Cyber}-{Defence} {Agents} via {Cyber} {Security} {Simulation}},
	doi = {10.48550/arXiv.2207.12355},
	publisher = {arXiv},
	author = {Andrew, Alex and Spillard, Sam and Collyer, Joshua and Dhir, Neil},
	note = {arXiv:2207.12355},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{rigaki_out_2024,
	address = {Roma, Italy},
	title = {Out of the {Cage}: {How} {Stochastic} {Parrots} {Win} in {Cyber} {Security} {Environments}},
	copyright = {All rights reserved},
	isbn = {978-989-758-680-4},
	doi = {10.5220/0012391800003636},
	abstract = {Digital Library},
	urldate = {2024-07-10},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Agents} and {Artificial} {Intelligence} - {Volume} 3: {ICAART}},
	publisher = {SciTePress},
	author = {Rigaki, Maria and Lukáš, Ondřej and Catania, Carlos and Garcia, Sebastian},
	month = jul,
	year = {2024},
	pages = {774--781},
}

@inproceedings{elderman_adversarial_2017,
	address = {Porto, Portugal},
	title = {Adversarial {Reinforcement} {Learning} in a {Cyber} {Security} {Simulation}:},
	isbn = {978-989-758-219-6 978-989-758-220-2},
	shorttitle = {Adversarial {Reinforcement} {Learning} in a {Cyber} {Security} {Simulation}},
	doi = {10.5220/0006197105590566},
	language = {en},
	urldate = {2023-08-21},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Agents} and {Artificial} {Intelligence}},
	publisher = {SCITEPRESS},
	author = {Elderman, Richard and J. J. Pater, Leon and S. Thie, Albert and M. Drugan, Madalina and M. Wiering, Marco},
	year = {2017},
	pages = {559--566},
}

@inproceedings{NEURIPS2023_46c2a9a6,
	title = {{SPRING}: {Studying} papers and reasoning to play games},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/46c2a9a6f2b2be68682013eb1173c801-Paper-Conference.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Yue and Min, So Yeon and Prabhumoye, Shrimai and Bisk, Yonatan and Salakhutdinov, Russ R and Azaria, Amos and Mitchell, Tom M and Li, Yuanzhi},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	pages = {22383--22687},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	urldate = {2024-06-07},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{agarwal_many-shot_2024,
	title = {Many-{Shot} {In}-{Context} {Learning}},
	url = {http://arxiv.org/abs/2404.11018},
	doi = {10.48550/arXiv.2404.11018},
	abstract = {Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Agarwal, Rishabh and Singh, Avi and Zhang, Lei M. and Bohnet, Bernd and Chan, Stephanie and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and Co-Reyes, John D. and Chu, Eric and Behbahani, Feryal and Faust, Aleksandra and Larochelle, Hugo},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11018 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{motlagh_large_2024,
	title = {Large {Language} {Models} in {Cybersecurity}: {State}-of-the-{Art}},
	shorttitle = {Large {Language} {Models} in {Cybersecurity}},
	url = {http://arxiv.org/abs/2402.00891},
	abstract = {The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Motlagh, Farzad Nourmohammadzadeh and Hajizadeh, Mehrdad and Majd, Mehryar and Najafi, Pejman and Cheng, Feng and Meinel, Christoph},
	month = jan,
	year = {2024},
	note = {arXiv:2402.00891 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{khattab_dspy_2023,
	title = {{DSPy}: {Compiling} {Declarative} {Language} {Model} {Calls} into {Self}-{Improving} {Pipelines}},
	shorttitle = {{DSPy}},
	url = {http://arxiv.org/abs/2310.03714},
	doi = {10.48550/arXiv.2310.03714},
	abstract = {The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25\% and 65\%, respectively) and pipelines with expert-created demonstrations (by up to 5-46\% and 16-40\%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03714 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@inproceedings{lin_et-bert_2022,
	title = {{ET}-{BERT}: {A} {Contextualized} {Datagram} {Representation} with {Pre}-training {Transformers} for {Encrypted} {Traffic} {Classification}},
	shorttitle = {{ET}-{BERT}},
	url = {http://arxiv.org/abs/2202.06335},
	doi = {10.1145/3485447.3512217},
	abstract = {Encrypted traffic classification requires discriminative and robust traffic representation captured from content-invisible and imbalanced traffic data for accurate classification, which is challenging but indispensable to achieve network security and network management. The major limitation of existing solutions is that they highly rely on the deep features, which are overly dependent on data size and hard to generalize on unseen data. How to leverage the open-domain unlabeled traffic data to learn representation with strong generalization ability remains a key challenge. In this paper,we propose a new traffic representation model called Encrypted Traffic Bidirectional Encoder Representations from Transformer (ET-BERT), which pre-trains deep contextualized datagram-level representation from large-scale unlabeled data. The pre-trained model can be fine-tuned on a small number of task-specific labeled data and achieves state-of-the-art performance across five encrypted traffic classification tasks, remarkably pushing the F1 of ISCX-Tor to 99.2\% (4.4\% absolute improvement), ISCX-VPN-Service to 98.9\% (5.2\% absolute improvement), Cross-Platform (Android) to 92.5\% (5.4\% absolute improvement), CSTNET-TLS 1.3 to 97.4\% (10.0\% absolute improvement). Notably, we provide explanation of the empirically powerful pre-training model by analyzing the randomness of ciphers. It gives us insights in understanding the boundary of classification ability over encrypted traffic. The code is available at: https://github.com/linwhitehat/ET-BERT.},
	urldate = {2024-03-27},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	author = {Lin, Xinjie and Xiong, Gang and Gou, Gaopeng and Li, Zhen and Shi, Junzheng and Yu, Jing},
	month = apr,
	year = {2022},
	note = {arXiv:2202.06335 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Networking and Internet Architecture},
	pages = {633--642},
}

@misc{shao_empirical_2024,
	title = {An {Empirical} {Evaluation} of {LLMs} for {Solving} {Offensive} {Security} {Challenges}},
	url = {http://arxiv.org/abs/2402.11814},
	abstract = {Capture The Flag (CTF) challenges are puzzles related to computer security scenarios. With the advent of large language models (LLMs), more and more CTF participants are using LLMs to understand and solve the challenges. However, so far no work has evaluated the effectiveness of LLMs in solving CTF challenges with a fully automated workflow. We develop two CTF-solving workflows, human-in-the-loop (HITL) and fully-automated, to examine the LLMs' ability to solve a selected set of CTF challenges, prompted with information about the question. We collect human contestants' results on the same set of questions, and find that LLMs achieve higher success rate than an average human participant. This work provides a comprehensive evaluation of the capability of LLMs in solving real world CTF challenges, from real competition to fully automated workflow. Our results provide references for applying LLMs in cybersecurity education and pave the way for systematic evaluation of offensive cybersecurity capabilities in LLMs.},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Shao, Minghao and Chen, Boyuan and Jancheska, Sofija and Dolan-Gavitt, Brendan and Garg, Siddharth and Karri, Ramesh and Shafique, Muhammad},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11814 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{fang_llm_2024,
	title = {{LLM} {Agents} can {Autonomously} {Hack} {Websites}},
	url = {http://arxiv.org/abs/2402.06664},
	abstract = {In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Fang, Richard and Bindu, Rohan and Gupta, Akul and Zhan, Qiusi and Kang, Daniel},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06664 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{noauthor_reasoning_nodate,
	title = {Reasoning in {Large} {Language} {Models}},
	url = {https://github.com/atfortes/LLM-Reasoning-Papers},
	abstract = {Collection of papers and resources on how to unlock the reasoning ability of Large Language Models.},
}

@misc{fortes_reasoning_2023,
	title = {Reasoning in {Large} {Language} {Models}},
	copyright = {MIT},
	url = {https://github.com/atfortes/LLM-Reasoning-Papers},
	abstract = {Collection of papers and resources on Reasoning in Large Language Models (LLMs), including Chain-of-Thought  (CoT), Instruction-Tuning, and others.},
	urldate = {2023-08-16},
	author = {Fortes, Armando},
	month = aug,
	year = {2023},
	note = {original-date: 2022-11-05T08:03:53Z},
	keywords = {arithmetic-reasoning, awesome, chain-of-thought, chatgpt, commonsense-reasoning, cot, datasets, gpt3, in-context-learning, instruction-tuning, language-models, lm, logical-reasoning, prompt, prompt-engineering, prompt-learning, question-answering, reasoning, symbolic-reasoning},
}

@misc{gao_retrieval-augmented_2023,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large language models (LLMs) demonstrate powerful capabilities, but they still face challenges in practical applications, such as hallucinations, slow knowledge updates, and lack of transparency in answers. Retrieval-Augmented Generation (RAG) refers to the retrieval of relevant information from external knowledge bases before answering questions with LLMs. RAG has been demonstrated to significantly enhance answer accuracy, reduce model hallucination, particularly for knowledge-intensive tasks. By citing sources, users can verify the accuracy of answers and increase trust in model outputs. It also facilitates knowledge updates and the introduction of domain-specific knowledge. RAG effectively combines the parameterized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models. This paper outlines the development paradigms of RAG in the era of LLMs, summarizing three paradigms: Naive RAG, Advanced RAG, and Modular RAG. It then provides a summary and organization of the three main components of RAG: retriever, generator, and augmentation methods, along with key technologies in each component. Furthermore, it discusses how to evaluate the effectiveness of RAG models, introducing two evaluation methods for RAG, emphasizing key metrics and abilities for evaluation, and presenting the latest automatic evaluation framework. Finally, potential future research directions are introduced from three aspects: vertical optimization, horizontal scalability, and the technical stack and ecosystem of RAG.},
	urldate = {2023-12-24},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
	month = dec,
	year = {2023},
	note = {arXiv:2312.10997 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{wang_openchat_2023,
	title = {{OpenChat}: {Advancing} {Open}-source {Language} {Models} with {Mixed}-{Quality} {Data}},
	shorttitle = {{OpenChat}},
	url = {http://arxiv.org/abs/2309.11235},
	doi = {10.48550/arXiv.2309.11235},
	abstract = {Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat.},
	urldate = {2023-12-22},
	publisher = {arXiv},
	author = {Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11235 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{fedus_switch_2022,
	title = {Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}},
	shorttitle = {Switch {Transformers}},
	url = {http://arxiv.org/abs/2101.03961},
	doi = {10.48550/arXiv.2101.03961},
	abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
	urldate = {2023-12-22},
	publisher = {arXiv},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	month = jun,
	year = {2022},
	note = {arXiv:2101.03961 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{du_guiding_2023,
	address = {Honolulu, USA},
	title = {Guiding {Pretraining} in {Reinforcement} {Learning} with {Large} {Language} {Models}},
	language = {en},
	urldate = {2023-07-26},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	author = {Du, Yuqing and Watkins, Olivia and Wang, Zihan and Colas, Cédric and Darrell, Trevor and Abbeel, Pieter and Gupta, Abhishek and Andreas, Jacob},
	month = jun,
	year = {2023},
}

@misc{standen_cyborg_2021,
	title = {{CybORG}: {A} {Gym} for the {Development} of {Autonomous} {Cyber} {Agents}},
	shorttitle = {{CybORG}},
	url = {http://arxiv.org/abs/2108.09118},
	doi = {10.48550/arXiv.2108.09118},
	abstract = {Autonomous Cyber Operations (ACO) involves the development of blue team (defender) and red team (attacker) decision-making agents in adversarial scenarios. To support the application of machine learning algorithms to solve this problem, and to encourage researchers in this field to attend to problems in the ACO setting, we introduce CybORG, a work-in-progress gym for ACO research. CybORG features a simulation and emulation environment with a common interface to facilitate the rapid training of autonomous agents that can then be tested on real-world systems. Initial testing demonstrates the feasibility of this approach.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Standen, Maxwell and Lucas, Martin and Bowman, David and Richer, Toby J. and Kim, Junae and Marriott, Damian},
	month = aug,
	year = {2021},
	note = {arXiv:2108.09118},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{microsoft_cyberbattlesim_2021,
	title = {{CyberBattleSim}},
	copyright = {MIT},
	url = {https://github.com/microsoft/CyberBattleSim},
	urldate = {2023-07-27},
	publisher = {Microsoft},
	author = {Microsoft},
	year = {2021},
	note = {Microsoft Defender Reasearch Team},
}

@misc{wu_spring_2023,
	title = {{SPRING}: {GPT}-4 {Out}-performs {RL} {Algorithms} by {Studying} {Papers} and {Reasoning}},
	shorttitle = {{SPRING}},
	url = {http://arxiv.org/abs/2305.15486},
	doi = {10.48550/arXiv.2305.15486},
	abstract = {Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context "reasoning" induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Wu, Yue and Min, So Yeon and Prabhumoye, Shrimai and Bisk, Yonatan and Salakhutdinov, Ruslan and Azaria, Amos and Mitchell, Tom and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.15486},
}

@misc{janisch_nasimemu_2023,
	title = {{NASimEmu}: {Network} {Attack} {Simulator} \& {Emulator} for {Training} {Agents} {Generalizing} to {Novel} {Scenarios}},
	shorttitle = {{NASimEmu}},
	url = {http://arxiv.org/abs/2305.17246},
	doi = {10.48550/arXiv.2305.17246},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Janisch, Jaromír and Pevný, Tomáš and Lisý, Viliam},
	month = may,
	year = {2023},
	note = {arXiv:2305.17246},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{rafailov_direct_2023,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = may,
	year = {2023},
	note = {arXiv:2305.18290 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_dpo_nodate,
	title = {{DPO} {Debate}: {Is} {RL} needed for {RLHF}?},
	url = {https://www.youtube.com/watch?v=YJMCSVLRUNs},
}

@misc{tunstall_zephyr_2023,
	title = {Zephyr: {Direct} {Distillation} of {LM} {Alignment}},
	shorttitle = {Zephyr},
	url = {http://arxiv.org/abs/2310.16944},
	abstract = {We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Clémentine and Habib, Nathan and Sarrazin, Nathan and Sanseviero, Omar and Rush, Alexander M. and Wolf, Thomas},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16944 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_huggingface_nodate,
	type = {github},
	title = {huggingface alignment-handbook},
	url = {https://github.com/huggingface/alignment-handbook},
	journal = {Huggingface alignment-handbook},
}

@misc{zhang_instruction_2023,
	title = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.10792},
	doi = {10.48550/arXiv.2308.10792},
	abstract = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
	month = oct,
	year = {2023},
	note = {arXiv:2308.10792 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{tunstall_zephyr_2023-1,
	title = {Zephyr: {Direct} {Distillation} of {LM} {Alignment}},
	shorttitle = {Zephyr},
	url = {http://arxiv.org/abs/2310.16944},
	doi = {10.48550/arXiv.2310.16944},
	abstract = {We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Clémentine and Habib, Nathan and Sarrazin, Nathan and Sanseviero, Omar and Rush, Alexander M. and Wolf, Thomas},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16944 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chen_extending_2023,
	title = {Extending {Context} {Window} of {Large} {Language} {Models} via {Positional} {Interpolation}},
	url = {http://arxiv.org/abs/2306.15595},
	doi = {10.48550/arXiv.2306.15595},
	abstract = {We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least \${\textbackslash}sim 600 {\textbackslash}times\$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15595 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_lost_2023,
	title = {Lost in the {Middle}: {How} {Language} {Models} {Use} {Long} {Contexts}},
	shorttitle = {Lost in the {Middle}},
	url = {http://arxiv.org/abs/2307.03172},
	doi = {10.48550/arXiv.2307.03172},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03172 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2023-08-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{wang_self-instruct_2023,
	title = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	doi = {10.48550/arXiv.2212.10560},
	abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = may,
	year = {2023},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yu_metamath_2023,
	title = {{MetaMath}: {Bootstrap} {Your} {Own} {Mathematical} {Questions} for {Large} {Language} {Models}},
	shorttitle = {{MetaMath}},
	url = {http://arxiv.org/abs/2309.12284},
	doi = {10.48550/arXiv.2309.12284},
	abstract = {Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs ({\textbackslash}eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose {\textbackslash}emph\{MetaMath\}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called \{MetaMathQA\}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks ({\textbackslash}ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves \$66.4{\textbackslash}\%\$ on GSM8K and \$19.4{\textbackslash}\%\$ on MATH, exceeding the state-of-the-art models of the same size by \$11.5{\textbackslash}\%\$ and \$8.7{\textbackslash}\%\$. Particularly, \{MetaMath-70B\} achieves an accuracy of \$82.3{\textbackslash}\%\$ on \{GSM8K\}, slightly better than \{GPT-3.5-Turbo\}. We release the \{MetaMathQA\} dataset, the \{MetaMath\} models with different model sizes and the training code for public use.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T. and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
	month = sep,
	year = {2023},
	note = {arXiv:2309.12284 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{xi_rise_2023,
	title = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}: {A} {Survey}},
	shorttitle = {The {Rise} and {Potential} of {Large} {Language} {Model} {Based} {Agents}},
	url = {http://arxiv.org/abs/2309.07864},
	doi = {10.48550/arXiv.2309.07864},
	abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for AI agents. Building upon this, we present a conceptual framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored to suit different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge when they form societies, and the insights they offer for human society. Finally, we discuss a range of key topics and open problems within the field.},
	urldate = {2023-09-17},
	publisher = {arXiv},
	author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Liu, Qin and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huan, Xuanjing and Gui, Tao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{seita_koala_nodate,
	title = {Koala: {A} {Dialogue} {Model} for {Academic} {Research}},
	shorttitle = {Koala},
	url = {http://bair.berkeley.edu/blog/2023/04/03/koala/},
	abstract = {The BAIR Blog},
	urldate = {2023-09-15},
	journal = {The Berkeley Artificial Intelligence Research Blog},
	author = {Seita, Daniel and Song, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, Dawn, Xinyang Geng},
}

@misc{phd_llm_2023,
	title = {{LLM} {Training}: {RLHF} and {Its} {Alternatives}},
	shorttitle = {{LLM} {Training}},
	url = {https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives},
	abstract = {I frequently reference a process called Reinforcement Learning with Human Feedback (RLHF) when discussing LLMs, whether in the research news or tutorials. RLHF is an integral part of the modern LLM training pipeline due to its ability to incorporate human preferences into the optimization landscape, which can improve the model's helpfulness and safety.},
	language = {en},
	urldate = {2023-09-13},
	author = {PhD, Sebastian Raschka},
	month = apr,
	year = {2023},
}

@misc{wang_survey_2023,
	title = {A {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents}},
	url = {http://arxiv.org/abs/2308.11432},
	doi = {10.48550/arXiv.2308.11432},
	abstract = {Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository for the related references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
	urldate = {2023-08-26},
	publisher = {arXiv},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.11432 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{chen_how_2023,
	title = {How is {ChatGPT}'s behavior changing over time?},
	url = {http://arxiv.org/abs/2307.09009},
	doi = {10.48550/arXiv.2307.09009},
	abstract = {GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84\% accuracy) but GPT-4 (June 2023) was poor on these same questions (51\% accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings show that the behavior of the "same" LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
	month = aug,
	year = {2023},
	note = {arXiv:2307.09009 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{happe_getting_2023,
	title = {Getting pwn'd by {AI}: {Penetration} {Testing} with {Large} {Language} {Models}},
	shorttitle = {Getting pwn'd by {AI}},
	url = {http://arxiv.org/abs/2308.00121},
	doi = {10.1145/3611643.3613083},
	abstract = {The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providing AI-based sparring partners.},
	urldate = {2023-08-21},
	author = {Happe, Andreas and Cito, Jürgen},
	month = aug,
	year = {2023},
	note = {arXiv:2308.00121 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{happe_getting_2023-1,
	title = {Getting pwn'd by {AI}: {Penetration} {Testing} with {Large} {Language} {Models}},
	shorttitle = {Getting pwn'd by {AI}},
	url = {http://arxiv.org/abs/2308.00121},
	doi = {10.1145/3611643.3613083},
	abstract = {The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providing AI-based sparring partners.},
	urldate = {2023-08-21},
	author = {Happe, Andreas and Cito, Jürgen},
	month = aug,
	year = {2023},
	note = {arXiv:2308.00121 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{drasar_session-level_2020,
	title = {Session-level {Adversary} {Intent}-{Driven} {Cyberattack} {Simulator}},
	doi = {10.1109/DS-RT50469.2020.9213690},
	abstract = {Recognizing the need for proactive analysis of cyber adversary behavior, this paper presents a new event-driven simulation model and implementation to reveal the efforts needed by attackers who have various entry points into a network. Unlike previous models which focus on the impact of attackers' actions on the defender's infrastructure, this work focuses on the attackers' strategies and actions. By operating on a request-response session level, our model provides an abstraction of how the network infrastructure reacts to access credentials the adversary might have obtained through a variety of strategies. We present the current capabilities of the simulator by showing three variants of Bronze Butler APT on a network with different user access levels.},
	booktitle = {2020 {IEEE}/{ACM} 24th {International} {Symposium} on {Distributed} {Simulation} and {Real} {Time} {Applications} ({DS}-{RT})},
	author = {Drašar, Martin and Moskal, Stephen and Yang, Shanchieh and Zat'ko, Pavol},
	month = sep,
	year = {2020},
	note = {ISSN: 1550-6525},
	keywords = {APT, Analytical models, Cyberattack, DEVS, Data models, Malware, Tools, adversary behavior, cybersecurity},
	pages = {1--9},
}

@inproceedings{kant_housekeep_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Housekeep: {Tidying} {Virtual} {Households} {Using} {Commonsense} {Reasoning}},
	isbn = {978-3-031-19842-7},
	shorttitle = {Housekeep},
	doi = {10.1007/978-3-031-19842-7_21},
	abstract = {We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. Instead, the agent must learn from and is evaluated against human preferences of which objects belong where in a tidy house. Specifically, we collect a dataset of where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms. Next, we propose a modular baseline approach for Housekeep that integrates planning, exploration, and navigation. It leverages a fine-tuned large language model (LLM) trained on an internet text corpus for effective planning. We find that our baseline planner generalizes to some extent when rearranging objects in unknown environments. See our webpage for code, data and more details: https://yashkant.github.io/housekeep/.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Kant, Yash and Ramachandran, Arun and Yenamandra, Sriram and Gilitschenski, Igor and Batra, Dhruv and Szot, Andrew and Agrawal, Harsh},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {355--373},
}

@inproceedings{hammar_learning_2021,
	title = {Learning {Intrusion} {Prevention} {Policies} through {Optimal} {Stopping}},
	url = {http://arxiv.org/abs/2106.07160},
	doi = {10.23919/CNSM52442.2021.9615542},
	abstract = {We study automated intrusion prevention using reinforcement learning. In a novel approach, we formulate the problem of intrusion prevention as an optimal stopping problem. This formulation allows us insight into the structure of the optimal policies, which turn out to be threshold based. Since the computation of the optimal defender policy using dynamic programming is not feasible for practical cases, we approximate the optimal policy through reinforcement learning in a simulation environment. To define the dynamics of the simulation, we emulate the target infrastructure and collect measurements. Our evaluations show that the learned policies are close to optimal and that they indeed can be expressed using thresholds.},
	urldate = {2023-08-21},
	booktitle = {2021 17th {International} {Conference} on {Network} and {Service} {Management} ({CNSM})},
	author = {Hammar, Kim and Stadler, Rolf},
	month = oct,
	year = {2021},
	note = {arXiv:2106.07160 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture},
	pages = {509--517},
}

@inproceedings{formby_lowering_2018,
	title = {Lowering the {Barriers} to {Industrial} {Control} {System} {Security} with \{{GRFICS}\}},
	url = {https://www.usenix.org/conference/ase18/presentation/formby},
	language = {en},
	urldate = {2023-08-21},
	author = {Formby, David and Rad, Milad and Beyah, Raheem},
	year = {2018},
}

@inproceedings{chowdhary_autonomous_2020,
	title = {Autonomous {Security} {Analysis} and {Penetration} {Testing}},
	doi = {10.1109/MSN50589.2020.00086},
	abstract = {Security Assessment of large networks is a challenging task. Penetration testing (pentesting) is a method of analyzing the attack surface of a network to find security vulnerabilities. Current network pentesting techniques involve a combination of automated scanning tools and manual exploitation of security issues to identify possible threats in a network. The solution scales poorly on a large network. We propose an autonomous security analysis and penetration testing framework (ASAP) that creates a map of security threats and possible attack paths in the network using attack graphs. Our framework utilizes: (i) state of the art reinforcement learning algorithm based on Deep-Q Network (DQN) to identify optimal policy for performing pentesting testing, and (ii) incorporates domain-specific transition matrix and reward modeling to capture the importance of security vulnerabilities and difficulty inherent in exploiting them. ASAP framework generates autonomous attack plans and validates them against real-world networks. The attack plans are generalizable to complex enterprise network, and the framework scales well on a large network. Our empirical evaluation shows that ASAP identifies non-intuitive attack plans on an enterprise network. The DQN planning algorithm employed scales well on a large network 60 -70(s) for generating an attack plan for network with 300 hosts.},
	booktitle = {2020 16th {International} {Conference} on {Mobility}, {Sensing} and {Networking} ({MSN})},
	author = {Chowdhary, Ankur and Huang, Dijiang and Mahendran, Jayasurya Sevalur and Romo, Daniel and Deng, Yuli and Sabur, Abdulhakim},
	month = dec,
	year = {2020},
	keywords = {Attack Graphs, Cloud Network, Deep-Q Network (DQN), Internet of Things (IoT), Manuals, Penetration Testing, Penetration testing, Reinforcement Learning, Reinforcement learning, Security, Task analysis, Testing, Tools},
	pages = {508--515},
}

@inproceedings{drasar_session-level_2020-1,
	title = {Session-level {Adversary} {Intent}-{Driven} {Cyberattack} {Simulator}},
	doi = {10.1109/DS-RT50469.2020.9213690},
	abstract = {Recognizing the need for proactive analysis of cyber adversary behavior, this paper presents a new event-driven simulation model and implementation to reveal the efforts needed by attackers who have various entry points into a network. Unlike previous models which focus on the impact of attackers' actions on the defender's infrastructure, this work focuses on the attackers' strategies and actions. By operating on a request-response session level, our model provides an abstraction of how the network infrastructure reacts to access credentials the adversary might have obtained through a variety of strategies. We present the current capabilities of the simulator by showing three variants of Bronze Butler APT on a network with different user access levels.},
	booktitle = {2020 {IEEE}/{ACM} 24th {International} {Symposium} on {Distributed} {Simulation} and {Real} {Time} {Applications} ({DS}-{RT})},
	author = {Drašar, Martin and Moskal, Stephen and Yang, Shanchieh and Zat'ko, Pavol},
	month = sep,
	year = {2020},
	note = {ISSN: 1550-6525},
	keywords = {APT, Analytical models, Cyberattack, DEVS, Data models, Malware, Tools, adversary behavior, cybersecurity},
	pages = {1--9},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{janisch_nasimemu_2023-1,
	title = {{NASimEmu}: {Network} {Attack} {Simulator} \& {Emulator} for {Training} {Agents} {Generalizing} to {Novel} {Scenarios}},
	shorttitle = {{NASimEmu}},
	url = {http://arxiv.org/abs/2305.17246},
	doi = {10.48550/arXiv.2305.17246},
	abstract = {Current frameworks for training offensive penetration testing agents with deep reinforcement learning struggle to produce agents that perform well in real-world scenarios, due to the reality gap in simulation-based frameworks and the lack of scalability in emulation-based frameworks. Additionally, existing frameworks often use an unrealistic metric that measures the agents' performance on the training data. NASimEmu, a new framework introduced in this paper, addresses these issues by providing both a simulator and an emulator with a shared interface. This approach allows agents to be trained in simulation and deployed in the emulator, thus verifying the realism of the used abstraction. Our framework promotes the development of general agents that can transfer to novel scenarios unseen during their training. For the simulation part, we adopt an existing simulator NASim and enhance its realism. The emulator is implemented with industry-level tools, such as Vagrant, VirtualBox, and Metasploit. Experiments demonstrate that a simulation-trained agent can be deployed in emulation, and we show how to use the framework to train a general agent that transfers into novel, structurally different scenarios. NASimEmu is available as open-source.},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Janisch, Jaromír and Pevný, Tomáš and Lisý, Viliam},
	month = may,
	year = {2023},
	note = {arXiv:2305.17246 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{shin_autoprompt_2020,
	title = {{AutoPrompt}: {Eliciting} {Knowledge} from {Language} {Models} with {Automatically} {Generated} {Prompts}},
	shorttitle = {{AutoPrompt}},
	url = {http://arxiv.org/abs/2010.15980},
	doi = {10.48550/arXiv.2010.15980},
	abstract = {The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.},
	urldate = {2023-04-16},
	publisher = {arXiv},
	author = {Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L. and Wallace, Eric and Singh, Sameer},
	month = nov,
	year = {2020},
	note = {arXiv:2010.15980 [cs]},
}

@misc{hafner_benchmarking_2022,
	title = {Benchmarking the {Spectrum} of {Agent} {Capabilities}},
	url = {http://arxiv.org/abs/2109.06780},
	doi = {10.48550/arXiv.2109.06780},
	abstract = {Evaluating the general abilities of intelligent agents requires complex simulation environments. Existing benchmarks typically evaluate only one narrow task per environment, requiring researchers to perform expensive training runs on many different environments. We introduce Crafter, an open world survival game with visual inputs that evaluates a wide range of general abilities within a single environment. Agents either learn from the provided reward signal or through intrinsic objectives and are evaluated by semantically meaningful achievements that can be unlocked during each episode, such as discovering resources and crafting tools. Consistently unlocking all achievements requires strong generalization, deep exploration, and long-term reasoning. We experimentally verify that Crafter is of appropriate difficulty to drive future research and provide baselines scores of reward agents and unsupervised agents. Furthermore, we observe sophisticated behaviors emerging from maximizing the reward signal, such as building tunnel systems, bridges, houses, and plantations. We hope that Crafter will accelerate research progress by quickly evaluating a wide spectrum of abilities.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Hafner, Danijar},
	month = feb,
	year = {2022},
	note = {arXiv:2109.06780 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{raffel_exploring_2020,
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	volume = {21},
	issn = {1532-4435},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jan,
	year = {2020},
	keywords = {attention based models, deep learning, multi-task learning, natural language processing, transfer learning},
	pages = {140:5485--140:5551},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	doi = {10.48550/arXiv.1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13461 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lu_learn_2022,
	title = {Learn to {Explain}: {Multimodal} {Reasoning} via {Thought} {Chains} for {Science} {Question} {Answering}},
	shorttitle = {Learn to {Explain}},
	url = {http://arxiv.org/abs/2209.09513},
	doi = {10.48550/arXiv.2209.09513},
	abstract = {When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of {\textasciitilde}21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20\% in few-shot GPT-3 and 3.99\% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96\%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40\% of the data. The data and code are available at https://scienceqa.github.io.},
	urldate = {2023-08-18},
	publisher = {arXiv},
	author = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
	month = oct,
	year = {2022},
	note = {arXiv:2209.09513 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@misc{zelikman_star_2022-1,
	title = {{STaR}: {Bootstrapping} {Reasoning} {With} {Reasoning}},
	shorttitle = {{STaR}},
	url = {http://arxiv.org/abs/2203.14465},
	doi = {10.48550/arXiv.2203.14465},
	abstract = {Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\${\textbackslash}times\$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
	urldate = {2023-08-18},
	publisher = {arXiv},
	author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D.},
	month = may,
	year = {2022},
	note = {arXiv:2203.14465 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{li_starcoder_2023,
	title = {{StarCoder}: may the source be with you!},
	shorttitle = {{StarCoder}},
	url = {http://arxiv.org/abs/2305.06161},
	doi = {10.48550/arXiv.2305.06161},
	abstract = {The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40{\textbackslash}\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and Liu, Qian and Zheltonozhskii, Evgenii and Zhuo, Terry Yue and Wang, Thomas and Dehaene, Olivier and Davaadorj, Mishig and Lamy-Poirier, Joel and Monteiro, João and Shliazhko, Oleh and Gontier, Nicolas and Meade, Nicholas and Zebaze, Armel and Yee, Ming-Ho and Umapathi, Logesh Kumar and Zhu, Jian and Lipkin, Benjamin and Oblokulov, Muhtasham and Wang, Zhiruo and Murthy, Rudra and Stillerman, Jason and Patel, Siva Sankalp and Abulkhanov, Dmitry and Zocca, Marco and Dey, Manan and Zhang, Zhihan and Fahmy, Nour and Bhattacharyya, Urvashi and Yu, Wenhao and Singh, Swayam and Luccioni, Sasha and Villegas, Paulo and Kunakov, Maxim and Zhdanov, Fedor and Romero, Manuel and Lee, Tony and Timor, Nadav and Ding, Jennifer and Schlesinger, Claire and Schoelkopf, Hailey and Ebert, Jan and Dao, Tri and Mishra, Mayank and Gu, Alex and Robinson, Jennifer and Anderson, Carolyn Jane and Dolan-Gavitt, Brendan and Contractor, Danish and Reddy, Siva and Fried, Daniel and Bahdanau, Dzmitry and Jernite, Yacine and Ferrandis, Carlos Muñoz and Hughes, Sean and Wolf, Thomas and Guha, Arjun and von Werra, Leandro and de Vries, Harm},
	month = may,
	year = {2023},
	note = {arXiv:2305.06161 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@misc{magister_teaching_2023,
	title = {Teaching {Small} {Language} {Models} to {Reason}},
	url = {http://arxiv.org/abs/2212.08410},
	doi = {10.48550/arXiv.2212.08410},
	abstract = {Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11\% to 21.99\% when finetuned on PaLM-540B generated chains of thought.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei},
	month = jun,
	year = {2023},
	note = {arXiv:2212.08410 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-08-13},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
	language = {en},
	urldate = {2023-08-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
	month = dec,
	year = {2022},
	pages = {24824--24837},
}

@misc{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	doi = {10.48550/arXiv.1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2023-08-11},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv:1312.5602 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	doi = {10.48550/arXiv.1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv:1606.01540 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{noauthor_openaigym_2023,
	title = {openai/gym},
	url = {https://github.com/openai/gym},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms.},
	urldate = {2023-08-08},
	publisher = {OpenAI},
	month = aug,
	year = {2023},
	note = {original-date: 2016-04-27T14:59:16Z},
}

@misc{noauthor_gym_nodate,
	title = {Gym {Documentation}},
	url = {https://www.gymlibrary.dev/},
	urldate = {2023-08-08},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	doi = {10.1007/BF00992698},
	language = {en},
	number = {3},
	urldate = {2023-08-08},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {Q-learning, asynchronous dynamic programming, reinforcement learning, temporal differences},
	pages = {279--292},
}

@article{seyyar_attack_2022,
	title = {An {Attack} {Detection} {Framework} {Based} on {BERT} and {Deep} {Learning}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3185748},
	journal = {IEEE Access},
	author = {Seyyar, Yunus Emre and Yavuz, Ali Gökhan and Ünver, Halil Murat},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Anomalous request, BERT, Bit error rate, Deep learning, Firewalls (computing), Natural language processing, Protocols, Structured Query Language, Uniform resource locators, deep learning, multilayer perceptron, natural language processing, web attack},
	pages = {68633--68644},
}

@article{chen_bert-log_2022,
	title = {{BERT}-{Log}: {Anomaly} {Detection} for {System} {Logs} {Based} on {Pre}-trained {Language} {Model}},
	volume = {36},
	issn = {0883-9514},
	shorttitle = {{BERT}-{Log}},
	url = {https://doi.org/10.1080/08839514.2022.2145642},
	doi = {10.1080/08839514.2022.2145642},
	number = {1},
	urldate = {2023-07-13},
	journal = {Applied Artificial Intelligence},
	author = {Chen, Song and Liao, Hai},
	month = dec,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08839514.2022.2145642},
	pages = {2145642},
}

@misc{cyborg_team_cyber_2022,
	title = {Cyber {Operations} {Research} {Gym}},
	copyright = {MIT},
	url = {https://github.com/cage-challenge/CybORG},
	author = {CybORG Team},
	year = {2022},
	note = {Created by Maxwell Standen, David Bowman, Son Hoang, Toby Richer, Martin Lucas, Richard Van Tassel, Phillip Vu, Mitchell Kiely, KC C., Natalie Konschnik, Joshua Collyer},
}

@inproceedings{rahali_malbert_2021,
	title = {{MalBERT}: {Malware} {Detection} using {Bidirectional} {Encoder} {Representations} from {Transformers}},
	shorttitle = {{MalBERT}},
	doi = {10.1109/SMC52423.2021.9659287},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Rahali, Abir and Akhloufi, Moulay A.},
	month = oct,
	year = {2021},
	note = {ISSN: 2577-1655},
	keywords = {Codes, Computer architecture, Conferences, Deep learning, Malware, Static analysis, Transformers},
	pages = {3226--3231},
}

@misc{lee_catbert_2020,
	title = {{CATBERT}: {Context}-{Aware} {Tiny} {BERT} for {Detecting} {Social} {Engineering} {Emails}},
	shorttitle = {{CATBERT}},
	url = {http://arxiv.org/abs/2010.03484},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Lee, Younghoo and Saxe, Joshua and Harang, Richard},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03484 [cs]},
	keywords = {\#SocialEngineeringAttackPrevention, \#ThreatDetectionandPrevention, Computer Science - Cryptography and Security},
}

@inproceedings{pearce_examining_2023,
	title = {Examining {Zero}-{Shot} {Vulnerability} {Repair} with {Large} {Language} {Models}},
	doi = {10.1109/SP46215.2023.10179324},
	abstract = {Human developers can produce code with cybersecurity bugs. Can emerging ‘smart’ code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI’s Codex and AI21’s Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information— both semantically and syntactically—with natural languages. We perform a large scale study of five commercially available, black-box, "off-the-shelf" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100\% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model’s performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan},
	month = may,
	year = {2023},
	note = {ISSN: 2375-1207},
	keywords = {AI, CWE, Closed box, Codes, Computer bugs, Computer crime, Cybersecurity, Maintenance engineering, Natural languages, Privacy, code generation},
	pages = {2339--2356},
}

@misc{pearce_examining_2022,
	title = {Examining {Zero}-{Shot} {Vulnerability} {Repair} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2112.02125},
	doi = {10.48550/arXiv.2112.02125},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan},
	month = aug,
	year = {2022},
	note = {arXiv:2112.02125 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@inproceedings{aghaei_securebert_2023,
	address = {Cham},
	series = {Lecture {Notes} of the {Institute} for {Computer} {Sciences}, {Social} {Informatics} and {Telecommunications} {Engineering}},
	title = {{SecureBERT}: {A} {Domain}-{Specific} {Language} {Model} for {Cybersecurity}},
	isbn = {978-3-031-25538-0},
	shorttitle = {{SecureBERT}},
	doi = {10.1007/978-3-031-25538-0_3},
	language = {en},
	booktitle = {Security and {Privacy} in {Communication} {Networks}},
	publisher = {Springer Nature Switzerland},
	author = {Aghaei, Ehsan and Niu, Xi and Shadid, Waseem and Al-Shaer, Ehab},
	editor = {Li, Fengjun and Liang, Kaitai and Lin, Zhiqiang and Katsikas, Sokratis K.},
	year = {2023},
	keywords = {Cyber automation, Cyber threat intelligence, Language model},
	pages = {39--56},
}

@misc{elhafsi_semantic_2023,
	title = {Semantic {Anomaly} {Detection} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.11307},
	doi = {10.48550/arXiv.2305.11307},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Elhafsi, Amine and Sinha, Rohan and Agia, Christopher and Schmerling, Edward and Nesnas, Issa and Pavone, Marco},
	month = may,
	year = {2023},
	note = {arXiv:2305.11307 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{thapa_transformer-based_2022,
	address = {New York, NY, USA},
	series = {{ACSAC} '22},
	title = {Transformer-{Based} {Language} {Models} for {Software} {Vulnerability} {Detection}},
	isbn = {978-1-4503-9759-9},
	url = {https://dl.acm.org/doi/10.1145/3564625.3567985},
	doi = {10.1145/3564625.3567985},
	urldate = {2023-07-28},
	booktitle = {Proceedings of the 38th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Thapa, Chandra and Jang, Seung Ick and Ahmed, Muhammad Ejaz and Camtepe, Seyit and Pieprzyk, Josef and Nepal, Surya},
	month = dec,
	year = {2022},
	keywords = {BERT, GPT-2, Software vulnerability detection, transformer-based models},
	pages = {481--496},
}

@misc{ding_is_2023,
	title = {Is {GPT}-3 a {Good} {Data} {Annotator}?},
	url = {http://arxiv.org/abs/2212.10450},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Ding, Bosheng and Qin, Chengwei and Liu, Linlin and Chia, Yew Ken and Joty, Shafiq and Li, Boyang and Bing, Lidong},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10450 [cs]},
	keywords = {\#DatasetAnnotators},
}

@misc{kim_language_2023,
	title = {Language {Models} can {Solve} {Computer} {Tasks}},
	url = {http://arxiv.org/abs/2303.17491},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen},
	month = jun,
	year = {2023},
	note = {arXiv:2303.17491 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@misc{rahali_malbert_2021,
	title = {{MalBERT}: {Using} {Transformers} for {Cybersecurity} and {Malicious} {Software} {Detection}},
	shorttitle = {{MalBERT}},
	url = {http://arxiv.org/abs/2103.03806},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Rahali, Abir and Akhloufi, Moulay A.},
	month = mar,
	year = {2021},
	note = {arXiv:2103.03806 [cs]},
	keywords = {\#ThreatDetectionandPrevention, \#VulnerabilityDetectionandPatchPrioritization:, Computer Science - Cryptography and Security},
}

@misc{ferrag_revolutionizing_2023,
	title = {Revolutionizing {Cyber} {Threat} {Detection} with {Large} {Language} {Models}},
	publisher = {arXiv},
	author = {Ferrag, Mohamed Amine and Ndhlovu,, Mthandazo and Tihanyi, Norbert and Cordeiro, Lucas C. and Debbah, Merouane and Lestable, Thierry},
	month = jun,
	year = {2023},
}

@phdthesis{schwartz_autonomous_2019,
	title = {Autonomous {Penetration} {Testing} using {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1905.05965},
	urldate = {2023-07-27},
	school = {arXiv},
	author = {Schwartz, Jonathon and Kurniawati, Hanna},
	month = may,
	year = {2019},
	note = {arXiv:1905.05965 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to {Network} {Attack} {Simulator}’s documentation! — {NASim} 0.12.0 documentation},
	url = {https://networkattacksimulator.readthedocs.io/en/latest/},
	urldate = {2023-07-27},
}

@inproceedings{hammar_finding_2020,
	title = {Finding {Effective} {Security} {Strategies} through {Reinforcement} {Learning} and {Self}-{Play}},
	doi = {10.23919/CNSM50824.2020.9269092},
	abstract = {We present a method to automatically find security strategies for the use case of intrusion prevention. Following this method, we model the interaction between an attacker and a defender as a Markov game and let attack and defense strategies evolve through reinforcement learning and self-play without human intervention. Using a simple infrastructure configuration, we demonstrate that effective security strategies can emerge from self-play. This shows that self-play, which has been applied in other domains with great success, can be effective in the context of network security. Inspection of the converged policies show that the emerged policies reflect common-sense knowledge and are similar to strategies of humans. Moreover, we address known challenges of reinforcement learning in this domain and present an approach that uses function approximation, an opponent pool, and an autoregressive policy representation. Through evaluations we show that our method is superior to two baseline methods but that policy convergence in self-play remains a challenge.},
	booktitle = {2020 16th {International} {Conference} on {Network} and {Service} {Management} ({CNSM})},
	author = {Hammar, Kim and Stadler, Rolf},
	month = nov,
	year = {2020},
	note = {ISSN: 2165-963X},
	keywords = {Approximation algorithms, Games, Markov Security Games, Markov processes, Monitoring, Network Security, Reconnaissance, Reinforcement Learning, Reinforcement learning, Security},
	pages = {1--9},
}

@misc{campbell_chatbot_nodate,
	title = {Chatbot {Honeypot}: {How} {AI} {Companions} {Could} {Weaken} {National} {Security}},
	shorttitle = {Chatbot {Honeypot}},
	url = {https://www.scientificamerican.com/article/chatbot-honeypot-how-ai-companions-could-weaken-national-security/},
	abstract = {AI chatbots blur the line between intimacy and secrecy, posing risks for users with national security interests and access to sensitive information},
	language = {en},
	urldate = {2023-07-21},
	journal = {Scientific American},
	author = {Campbell, Remaya M.},
}

@inproceedings{udhani_human_2019,
	title = {Human vs {Bots}: {Detecting} {Human} {Attacks} in a {Honeypot} {Environment}},
	shorttitle = {Human vs {Bots}},
	doi = {10.1109/ISDFS.2019.8757534},
	abstract = {The increase in the automated attacks has motivated security researchers to focus on identifying patterns of attacker to safeguard the system. Humans have some basic behavioral characteristics and limitations, which can be identified and used to distinguish them from automated attackers. The network log data collected from a Honeypot uncovers such traits which are otherwise not noticeable. The paper analyses a SSH-based Honeypot deployed over a period of 423 days to identify human behavior traits which can essentially distinguish an automated attacker and a human attacker.},
	booktitle = {2019 7th {International} {Symposium} on {Digital} {Forensics} and {Security} ({ISDFS})},
	author = {Udhani, Shreya and Withers, Alexander and Bashir, Masooda},
	month = jun,
	year = {2019},
	keywords = {Authentication, Botnet, Force, Password, Protocols, SSH, Servers, Tools, behavioral analysis, bot, brute force attacks, honeypot},
	pages = {1--6},
}

@misc{chen_how_2023,
	title = {How is {ChatGPT}'s behavior changing over time?},
	url = {http://arxiv.org/abs/2307.09009},
	doi = {10.48550/arXiv.2307.09009},
	abstract = {GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6\%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4\%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLM quality.},
	urldate = {2023-07-19},
	publisher = {arXiv},
	author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09009 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-judge with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80{\textbackslash}\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena.},
	urldate = {2023-07-16},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = jul,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{ram_-context_2023,
	title = {In-{Context} {Retrieval}-{Augmented} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.00083},
	abstract = {Retrieval-Augmented Language Modeling (RALM) methods, that condition a language model (LM) on relevant documents from a grounding corpus during generation, have been shown to significantly improve language modeling while also providing a natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper proposes an under-explored alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input. We show that in-context RALM which uses off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that in-context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access. To that end, we make our code publicly available.},
	urldate = {2023-07-14},
	publisher = {arXiv},
	author = {Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
	month = jan,
	year = {2023},
	note = {arXiv:2302.00083 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{thapa_transformer-based_2022,
	title = {Transformer-{Based} {Language} {Models} for {Software} {Vulnerability} {Detection}},
	url = {http://arxiv.org/abs/2204.03214},
	doi = {10.48550/arXiv.2204.03214},
	abstract = {The large transformer-based language models demonstrate excellent performance in natural language processing. By considering the transferability of the knowledge gained by these models in one domain to other related domains, and the closeness of natural languages to high-level programming languages, such as C/C++, this work studies how to leverage (large) transformer-based language models in detecting software vulnerabilities and how good are these models for vulnerability detection tasks. In this regard, firstly, a systematic (cohesive) framework that details source code translation, model preparation, and inference is presented. Then, an empirical analysis is performed with software vulnerability datasets with C/C++ source codes having multiple vulnerabilities corresponding to the library function call, pointer usage, array usage, and arithmetic expression. Our empirical results demonstrate the good performance of the language models in vulnerability detection. Moreover, these language models have better performance metrics, such as F1-score, than the contemporary models, namely bidirectional long short-term memory and bidirectional gated recurrent unit. Experimenting with the language models is always challenging due to the requirement of computing resources, platforms, libraries, and dependencies. Thus, this paper also analyses the popular platforms to efficiently fine-tune these models and present recommendations while choosing the platforms.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Thapa, Chandra and Jang, Seung Ick and Ahmed, Muhammad Ejaz and Camtepe, Seyit and Pieprzyk, Josef and Nepal, Surya},
	month = sep,
	year = {2022},
	note = {arXiv:2204.03214 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{liu_no_2023,
	title = {No {Privacy} {Among} {Spies}: {Assessing} the {Functionality} and {Insecurity} of {Consumer} {Android} {Spyware} {Apps}},
	volume = {2023},
	issn = {2299-0984},
	shorttitle = {No {Privacy} {Among} {Spies}},
	url = {https://petsymposium.org/popets/2023/popets-2023-0013.php},
	doi = {10.56553/popets-2023-0013},
	abstract = {Consumer mobile spyware apps covertly monitor a user's activities (i.e., text messages, phone calls, e-mail, location, etc.) and transmit that information over the Internet to support remote surveillance. Unlike conceptually similar apps used for state espionage, so-called "stalkerware" apps are mass-marketed to consumers on a retail basis and expose a far broader range of victims to invasive monitoring. Today the market for such apps is large enough to support dozens of competitors, with individual vendors reportedly monitoring hundreds of thousands of phones. However, while the research community is well aware of the existence of such apps, our understanding of the mechanisms they use to operate remains ad hoc. In this work, we perform an in-depth technical analysis of 14 distinct leading mobile spyware apps targeting Android phones. We document the range of mechanisms used to monitor user activity of various kinds (e.g., photos, text messages, live microphone access) — primarily through the creative abuse of Android APIs. We also discover previously undocumented methods these apps use to hide from detection and to achieve persistence. Additionally, we document the measures taken by each app to protect the privacy of the sensitive data they collect, identifying a range of failings on the part of spyware vendors (including privacy-sensitive data sent in the clear or stored in the cloud with little or no protection).},
	language = {en},
	number = {1},
	urldate = {2023-07-10},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Liu, Enze and Rao, Sumanth and Havron, Sam and Ho, Grant and Savage, Stefan and Voelker, Geoffrey M. and McCoy, Damon},
	month = jan,
	year = {2023},
	pages = {207--224},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wu_read_2023,
	title = {Read and {Reap} the {Rewards}: {Learning} to {Play} {Atari} with the {Help} of {Instruction} {Manuals}},
	shorttitle = {Read and {Reap} the {Rewards}},
	url = {http://arxiv.org/abs/2302.04449},
	doi = {10.48550/arXiv.2302.04449},
	abstract = {High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. Auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. When assisted by our design, A2C improves on 4 games in the Atari environment with sparse rewards, and requires 1000x less training frames compared to the previous SOTA Agent 57 on Skiing, the hardest game in Atari.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Wu, Yue and Fan, Yewen and Liang, Paul Pu and Azaria, Amos and Li, Yuanzhi and Mitchell, Tom M.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04449 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{karpas_mrkl_2022,
	title = {{MRKL} {Systems}: {A} modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
	shorttitle = {{MRKL} {Systems}},
	url = {http://arxiv.org/abs/2205.00445},
	doi = {10.48550/arXiv.2205.00445},
	abstract = {Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and Leyton-Brown, Kevin and Muhlgay, Dor and Rozen, Noam and Schwartz, Erez and Shachaf, Gal and Shalev-Shwartz, Shai and Shashua, Amnon and Tenenholtz, Moshe},
	month = may,
	year = {2022},
	note = {arXiv:2205.00445 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{phung_generative_2023,
	title = {Generative {AI} for {Programming} {Education}: {Benchmarking} {ChatGPT}, {GPT}-4, and {Human} {Tutors}},
	shorttitle = {Generative {AI} for {Programming} {Education}},
	url = {http://arxiv.org/abs/2306.17156},
	abstract = {Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors' performance for several scenarios. These results also highlight settings where GPT-4 still struggles, providing exciting future directions on developing techniques to improve the performance of these models.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Phung, Tung and Pădurean, Victor-Alexandru and Cambronero, José and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17156 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@article{kasneci_chatgpt_2023,
	title = {{ChatGPT} for good? {On} opportunities and challenges of large language models for education},
	volume = {103},
	issn = {10416080},
	shorttitle = {{ChatGPT} for good?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1041608023000195},
	doi = {10.1016/j.lindif.2023.102274},
	language = {en},
	urldate = {2023-07-02},
	journal = {Learning and Individual Differences},
	author = {Kasneci, Enkelejda and Sessler, Kathrin and Küchemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and Günnemann, Stephan and Hüllermeier, Eyke and Krusche, Stepha and Kutyniok, Gitta and Michaeli, Tilman and Nerdel, Claudia and Pfeffer, Jürgen and Poquet, Oleksandra and Sailer, Michael and Schmidt, Albrecht and Seidel, Tina and Stadler, Matthias and Weller, Jochen and Kuhn, Jochen and Kasneci, Gjergji},
	month = apr,
	year = {2023},
	pages = {102274},
}

@misc{wu_how_2023,
	title = {How {Effective} {Are} {Neural} {Networks} for {Fixing} {Security} {Vulnerabilities}},
	url = {http://arxiv.org/abs/2305.18607},
	doi = {10.1145/3597926.3598135},
	abstract = {Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4\%), the most number of vulnerabilities. (2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.3 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.},
	urldate = {2023-06-28},
	author = {Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena},
	month = may,
	year = {2023},
	note = {arXiv:2305.18607 [cs]},
	keywords = {\#},
}

@misc{fu_chain--thought_2023,
	title = {Chain-of-{Thought} {Hub}: {A} {Continuous} {Effort} to {Measure} {Large} {Language} {Models}' {Reasoning} {Performance}},
	shorttitle = {Chain-of-{Thought} {Hub}},
	url = {http://arxiv.org/abs/2305.17306},
	abstract = {As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Fu, Yao and Ou, Litu and Chen, Mingyu and Wan, Yuhao and Peng, Hao and Khot, Tushar},
	month = may,
	year = {2023},
	note = {arXiv:2305.17306 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{rahali_malbertv2_2023,
	title = {{MalBERTv2}: {Code} {Aware} {BERT}-{Based} {Model} for {Malware} {Identification}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-2289},
	shorttitle = {{MalBERTv2}},
	url = {https://www.mdpi.com/2504-2289/7/2/60},
	doi = {10.3390/bdcc7020060},
	abstract = {To proactively mitigate malware threats, cybersecurity tools, such as anti-virus and anti-malware software, as well as firewalls, require frequent updates and proactive implementation. However, processing the vast amounts of dataset examples can be overwhelming when relying solely on traditional methods. In cybersecurity workflows, recent advances in natural language processing (NLP) models can aid in proactively detecting various threats. In this paper, we present a novel approach for representing the relevance and significance of the Malware/Goodware (MG) datasets, through the use of a pre-trained language model called MalBERTv2. Our model is trained on publicly available datasets, with a focus on the source code of the apps by extracting the top-ranked files that present the most relevant information. These files are then passed through a pre-tokenization feature generator, and the resulting keywords are used to train the tokenizer from scratch. Finally, we apply a classifier using bidirectional encoder representations from transformers (BERT) as a layer within the model pipeline. The performance of our model is evaluated on different datasets, achieving a weighted f1 score ranging from 82\% to 99\%. Our results demonstrate the effectiveness of our approach for proactively detecting malware threats using NLP techniques.},
	language = {en},
	number = {2},
	urldate = {2023-06-28},
	journal = {Big Data and Cognitive Computing},
	author = {Rahali, Abir and Akhloufi, Moulay A.},
	month = jun,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {malware detection, natural language processing, transformer-based model},
	pages = {60},
}

@inproceedings{rahali_malbert_2021-1,
	title = {{MalBERT}: {Malware} {Detection} using {Bidirectional} {Encoder} {Representations} from {Transformers}},
	shorttitle = {{MalBERT}},
	doi = {10.1109/SMC52423.2021.9659287},
	abstract = {In recent years we have witnessed an increase in cyber threats and malicious software attacks on different platforms with important consequences to persons and businesses. It has become critical to find automated machine learning techniques to proactively defend against malware. Transformers, a category of attention-based deep learning techniques, have recently shown impressive results in solving different tasks mainly related to the field of Natural Language Processing (NLP). In this paper, we propose the use of a Transformers architecture to automatically detect malicious software. We propose MalBERT, a model based on BERT (Bidirectional Encoder Representations from Transformers) which performs a static analysis on the source code of Android applications using preprocessed features to characterize existing malware and classify it into different representative malware categories. The obtained results are promising and show the high performance obtained by Transformer-based models for malicious software detection.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Rahali, Abir and Akhloufi, Moulay A.},
	month = oct,
	year = {2021},
	note = {ISSN: 2577-1655},
	keywords = {Codes, Computer architecture, Conferences, Deep learning, Malware, Static analysis, Transformers},
	pages = {3226--3231},
}

@misc{cambiaso_scamming_2023,
	title = {Scamming the {Scammers}: {Using} {ChatGPT} to {Reply} {Mails} for {Wasting} {Time} and {Resources}},
	shorttitle = {Scamming the {Scammers}},
	url = {http://arxiv.org/abs/2303.13521},
	doi = {10.48550/arXiv.2303.13521},
	abstract = {The use of Artificial Intelligence (AI) to support cybersecurity operations is now a consolidated practice, e.g., to detect malicious code or configure traffic filtering policies. The recent surge of AI, generative techniques and frameworks with efficient natural language processing capabilities dramatically magnifies the number of possible applications aimed at increasing the security of the Internet. Specifically, the ability of ChatGPT to produce textual contents while mimicking realistic human interactions can be used to mitigate the plague of emails containing scams. Therefore, this paper investigates the use of AI to engage scammers in automatized and pointless communications, with the goal of wasting both their time and resources. Preliminary results showcase that ChatGPT is able to decoy scammers, thus confirming that AI is an effective tool to counteract threats delivered via mail. In addition, we highlight the multitude of implications and open research questions to be addressed in the perspective of the ubiquitous adoption of AI.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Cambiaso, Enrico and Caviglione, Luca},
	month = feb,
	year = {2023},
	note = {arXiv:2303.13521 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Human-Computer Interaction},
}

@misc{noauthor_introducing_nodate,
	title = {Introducing {VirusTotal} {Code} {Insight}: {Empowering} threat analysis with generative {AI}},
	shorttitle = {Introducing {VirusTotal} {Code} {Insight}},
	url = {https://blog.virustotal.com/2023/04/introducing-virustotal-code-insight.html},
	abstract = {At the RSA Conference 2023 today, we are excited to unveil VirusTotal Code Insight, a cutting-edge feature that leverages artificial intelli...},
	urldate = {2023-06-27},
	keywords = {\#VulnerabilityDetectionandPatchPrioritization:},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{google_inc_how_2023,
	title = {How {Google} {Cloud} plans to supercharge security with generative {AI}},
	shorttitle = {Sec-{Palm2}},
	url = {https://cloud.google.com/blog/products/identity-security/rsa-google-cloud-security-ai-workbench-generative-ai},
	abstract = {At the RSA Conference, we are excited to announce Google Cloud Security AI Workbench, an industry-first extensible platform powered by the specialized LLM Sec-PaLM.},
	language = {en},
	urldate = {2023-06-26},
	journal = {Google Cloud Blog},
	author = {{Google Inc.}},
	year = {2023},
	keywords = {\#AnalysisandIntelligence, \#CybersecurityTrainingandEducation},
}

@misc{anil_palm_2023,
	title = {{PaLM} 2 {Technical} {Report}},
	url = {http://arxiv.org/abs/2305.10403},
	doi = {10.48550/arXiv.2305.10403},
	abstract = {We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Anil, Rohan and Dai, Andrew M. and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chu, Eric and Clark, Jonathan H. and Shafey, Laurent El and Huang, Yanping and Meier-Hellstern, Kathy and Mishra, Gaurav and Moreira, Erica and Omernick, Mark and Robinson, Kevin and Ruder, Sebastian and Tay, Yi and Xiao, Kefan and Xu, Yuanzhong and Zhang, Yujing and Abrego, Gustavo Hernandez and Ahn, Junwhan and Austin, Jacob and Barham, Paul and Botha, Jan and Bradbury, James and Brahma, Siddhartha and Brooks, Kevin and Catasta, Michele and Cheng, Yong and Cherry, Colin and Choquette-Choo, Christopher A. and Chowdhery, Aakanksha and Crepy, Clément and Dave, Shachi and Dehghani, Mostafa and Dev, Sunipa and Devlin, Jacob and Díaz, Mark and Du, Nan and Dyer, Ethan and Feinberg, Vlad and Feng, Fangxiaoyu and Fienber, Vlad and Freitag, Markus and Garcia, Xavier and Gehrmann, Sebastian and Gonzalez, Lucas and Gur-Ari, Guy and Hand, Steven and Hashemi, Hadi and Hou, Le and Howland, Joshua and Hu, Andrea and Hui, Jeffrey and Hurwitz, Jeremy and Isard, Michael and Ittycheriah, Abe and Jagielski, Matthew and Jia, Wenhao and Kenealy, Kathleen and Krikun, Maxim and Kudugunta, Sneha and Lan, Chang and Lee, Katherine and Lee, Benjamin and Li, Eric and Li, Music and Li, Wei and Li, YaGuang and Li, Jian and Lim, Hyeontaek and Lin, Hanzhao and Liu, Zhongtao and Liu, Frederick and Maggioni, Marcello and Mahendru, Aroma and Maynez, Joshua and Misra, Vedant and Moussalem, Maysam and Nado, Zachary and Nham, John and Ni, Eric and Nystrom, Andrew and Parrish, Alicia and Pellat, Marie and Polacek, Martin and Polozov, Alex and Pope, Reiner and Qiao, Siyuan and Reif, Emily and Richter, Bryan and Riley, Parker and Ros, Alex Castro and Roy, Aurko and Saeta, Brennan and Samuel, Rajkumar and Shelby, Renee and Slone, Ambrose and Smilkov, Daniel and So, David R. and Sohn, Daniel and Tokumine, Simon and Valter, Dasha and Vasudevan, Vijay and Vodrahalli, Kiran and Wang, Xuezhi and Wang, Pidong and Wang, Zirui and Wang, Tao and Wieting, John and Wu, Yuhuai and Xu, Kelvin and Xu, Yunhan and Xue, Linting and Yin, Pengcheng and Yu, Jiahui and Zhang, Qiao and Zheng, Steven and Zheng, Ce and Zhou, Weikang and Zhou, Denny and Petrov, Slav and Wu, Yonghui},
	month = may,
	year = {2023},
	note = {arXiv:2305.10403 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{microsoft_microsoft_2023,
	title = {Microsoft {Security} {Copilot} {\textbar} {Microsoft} {Security}},
	shorttitle = {Microsoft {Security} {Copilot}},
	url = {https://www.microsoft.com/en-us/security/business/ai-machine-learning/microsoft-security-copilot},
	abstract = {Get tailored insights that empower your team to defend at machine speed with generative AI.},
	language = {en-US},
	urldate = {2023-06-26},
	author = {{Microsoft}},
	year = {2023},
}

@misc{wei_finetuned_2022,
	title = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	urldate = {2023-06-26},
	publisher = {arXiv},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.01652 [cs]},
	keywords = {\#DatasetAnnotators},
}

@misc{greshake_not_2023,
	title = {Not what you've signed up for: {Compromising} {Real}-{World} {LLM}-{Integrated} {Applications} with {Indirect} {Prompt} {Injection}},
	shorttitle = {Not what you've signed up for},
	url = {http://arxiv.org/abs/2302.12173},
	doi = {10.48550/arXiv.2302.12173},
	abstract = {Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
	month = may,
	year = {2023},
	note = {arXiv:2302.12173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@article{nam_intrusion_2021,
	title = {Intrusion {Detection} {Method} {Using} {Bi}-{Directional} {GPT} for in-{Vehicle} {Controller} {Area} {Networks}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9530394/},
	doi = {10.1109/ACCESS.2021.3110524},
	urldate = {2023-06-21},
	journal = {IEEE Access},
	author = {Nam, Minki and Park, Seungyoung and Kim, Duk Soo},
	year = {2021},
	keywords = {\#MalwareDetectors, \#ThreatDetectionandPrevention},
	pages = {124931--124944},
}

@misc{mckee_chatbots_2023,
	title = {Chatbots in a {Honeypot} {World}},
	url = {http://arxiv.org/abs/2301.03771},
	abstract = {Question-and-answer agents like ChatGPT offer a novel tool for use as a potential honeypot interface in cyber security. By imitating Linux, Mac, and Windows terminal commands and providing an interface for TeamViewer, nmap, and ping, it is possible to create a dynamic environment that can adapt to the actions of attackers and provide insight into their tactics, techniques, and procedures (TTPs). The paper illustrates ten diverse tasks that a conversational agent or large language model might answer appropriately to the effects of command-line attacker. The original result features feasibility studies for ten model tasks meant for defensive teams to mimic expected honeypot interfaces with minimal risks. Ultimately, the usefulness outside of forensic activities stems from whether the dynamic honeypot can extend the time-to-conquer or otherwise delay attacker timelines short of reaching key network assets like databases or confidential information. While ongoing maintenance and monitoring may be required, ChatGPT's ability to detect and deflect malicious activity makes it a valuable option for organizations seeking to enhance their cyber security posture. Future work will focus on cybersecurity layers, including perimeter security, host virus detection, and data security.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {McKee, Forrest and Noever, David},
	month = jan,
	year = {2023},
	note = {arXiv:2301.03771 [cs]},
	keywords = {\#CybersecurityTrainingandEducation, \#SimulationandTraining, Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@misc{jin_darkbert_2023,
	title = {{DarkBERT}: {A} {Language} {Model} for the {Dark} {Side} of the {Internet}},
	shorttitle = {{DarkBERT}},
	url = {http://arxiv.org/abs/2305.08596},
	abstract = {Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Jin, Youngjin and Jang, Eugene and Cui, Jian and Chung, Jin-Woo and Lee, Yongjae and Shin, Seungwon},
	month = may,
	year = {2023},
	note = {arXiv:2305.08596 [cs]},
	keywords = {\#AnalysisandIntelligence, \#DarkWebMonitoring},
}

@misc{aghaei_securebert_2022,
	title = {{SecureBERT}: {A} {Domain}-{Specific} {Language} {Model} for {Cybersecurity}},
	shorttitle = {{SecureBERT}},
	url = {http://arxiv.org/abs/2204.02685},
	abstract = {Natural Language Processing (NLP) has recently gained wide attention in cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber automation. Increased connection and automation have revolutionized the world's economic and cultural infrastructures, while they have introduced risks in terms of cyber attacks. CTI is information that helps cybersecurity analysts make intelligent security decisions, that is often delivered in the form of natural language text, which must be transformed to machine readable format through an automated procedure before it can be used for automated security measures. This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text (e.g., CTI) and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual efforts. SecureBERT has been trained using a large corpus of cybersecurity text.To make SecureBERT effective not just in retaining general English understanding, but also when applied to text with cybersecurity implications, we developed a customized tokenizer as well as a method to alter pre-trained weights. The SecureBERT is evaluated using the standard Masked Language Model (MLM) test as well as two additional standard NLP tasks. Our evaluation studies show that SecureBERT{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/ehsanaghaei/SecureBERT\}\} outperforms existing similar models, confirming its capability for solving crucial NLP tasks in cybersecurity.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Aghaei, Ehsan and Niu, Xi and Shadid, Waseem and Al-Shaer, Ehab},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02685 [cs]},
	keywords = {\#AnalysisandIntelligence, \#CyberThrea IntelligenceAnalysis, Computer Science - Cryptography and Security},
}

@misc{greshake_not_2023-1,
	title = {Not what you've signed up for: {Compromising} {Real}-{World} {LLM}-{Integrated} {Applications} with {Indirect} {Prompt} {Injection}},
	shorttitle = {Not what you've signed up for},
	url = {http://arxiv.org/abs/2302.12173},
	doi = {10.48550/arXiv.2302.12173},
	abstract = {Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.},
	urldate = {2023-06-21},
	publisher = {arXiv},
	author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
	month = may,
	year = {2023},
	note = {arXiv:2302.12173 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@inproceedings{noauthor_detecting_nodate,
	title = {Detecting {Phishing} {Sites} {Using} {ChatGPT}},
	url = {https://arxiv.org/pdf/2306.05816.pdf},
	urldate = {2023-06-16},
}

@article{chilton_new_2023,
	title = {The {New} {Risks} {ChatGPT} {Poses} to {Cybersecurity}},
	issn = {0017-8012},
	url = {https://hbr.org/2023/04/the-new-risks-chatgpt-poses-to-cybersecurity},
	abstract = {The FBI’s 2021 Internet Crime Report found that phishing is the most common IT threat in America. From a hacker’s perspective, ChatGPT is a game changer, affording hackers from all over the globe a near fluency in English to bolster their phishing campaigns. Bad actors may also be able to trick the AI into generating hacking code. And, of course, there’s the potential for ChatGPT itself to be hacked, disseminating dangerous misinformation and political propaganda. This article examines these new risks, explores the needed training and tools for cybersecurity professionals to respond, and calls for government oversight to ensure that AI usage doesn’t become detrimental to cybersecurity efforts.},
	urldate = {2023-06-20},
	journal = {Harvard Business Review},
	author = {Chilton, Jim},
	month = apr,
	year = {2023},
	note = {Section: Cybersecurity and digital privacy},
	keywords = {AI and machine learning, ChatGPT, Cybersecurity and digital privacy},
}

@article{pall_semi-supervised_2019,
	title = {Semi-supervised deep learning in sequence labeling},
	url = {https://dspace.cuni.cz/handle/20.500.11956/110219},
	abstract = {Označování sekvencí ve strojovém učení je typ problému, který zahrnuje při- řazování označení jednotlivým členům sekvence. Pro tento typ problému dosáhlo hluboké učení dobrého výkonu. Jedna z nevýhod tohoto přístupu je jeho zá- vislost na velkém množství označených dat. Semi-supervizované učení zmírňuje tento problém používáním levnějších neoznačených dat spolu s daty označenými. V současnosti je použití semi-supervizovaného hlubokého učení v označování sekvencí limitované. Z tohoto důvodu se tato práce zaměřuje na aplikaci semi- supervizovaného hlubokého učení v označování sekvencí. Práce prozkoumává exis- tující přístupy semi-supervizovaného hlubokého učení a navrhuje vlastní přístupy. Navržené přístupy jsou experimentálně vyhodnocené na úlohách rozpoznávání po- jmenovaných entit a tvaroslovného značkování.},
	language = {en\_US},
	urldate = {2023-06-20},
	author = {Páll, Juraj Eduard},
	month = sep,
	year = {2019},
	note = {Accepted: 2021-03-26T11:18:45Z
Publisher: Univerzita Karlova, Matematicko-fyzikální fakulta},
}

@misc{kim_language_2023-1,
	title = {Language {Models} can {Solve} {Computer} {Tasks}},
	url = {http://arxiv.org/abs/2303.17491},
	abstract = {Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen},
	month = jun,
	year = {2023},
	note = {arXiv:2303.17491 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@misc{koide_detecting_2023,
	title = {Detecting {Phishing} {Sites} {Using} {ChatGPT}},
	url = {http://arxiv.org/abs/2306.05816},
	doi = {10.48550/arXiv.2306.05816},
	abstract = {The rise of large language models (LLMs) has had a significant impact on various domains, including natural language processing and artificial intelligence. While LLMs such as ChatGPT have been extensively researched for tasks such as code generation and text synthesis, their application in detecting malicious web content, particularly phishing sites, has been largely unexplored. To combat the rising tide of automated cyber attacks facilitated by LLMs, it is imperative to automate the detection of malicious web content, which requires approaches that leverage the power of LLMs to analyze and classify phishing sites. In this paper, we propose a novel method that utilizes ChatGPT to detect phishing sites. Our approach involves leveraging a web crawler to gather information from websites and generate prompts based on this collected data. This approach enables us to detect various phishing sites without the need for fine-tuning machine learning models and identify social engineering techniques from the context of entire websites and URLs. To evaluate the performance of our proposed method, we conducted experiments using a dataset. The experimental results using GPT-4 demonstrated promising performance, with a precision of 98.3\% and a recall of 98.4\%. Comparative analysis between GPT-3.5 and GPT-4 revealed an enhancement in the latter's capability to reduce false negatives. These findings not only highlight the potential of LLMs in efficiently identifying phishing sites but also have significant implications for enhancing cybersecurity measures and protecting users from the dangers of online fraudulent activities.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Koide, Takashi and Fukushi, Naoki and Nakano, Hiroki and Chiba, Daiki},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05816 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{nair_dera_2023,
	title = {{DERA}: {Enhancing} {Large} {Language} {Model} {Completions} with {Dialog}-{Enabled} {Resolving} {Agents}},
	shorttitle = {{DERA}},
	url = {http://arxiv.org/abs/2303.17071},
	doi = {10.48550/arXiv.2303.17071},
	abstract = {Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output. We test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance in both human expert preference evaluations and quantitative metrics. In a new finding, we also show that GPT-4's performance (70\%) on an open-ended version of the MedQA question-answering (QA) dataset (Jin et al. 2021, USMLE) is well above the passing level (60\%), with DERA showing similar performance. We release the open-ended MEDQA dataset at https://github.com/curai/curai-research/tree/main/DERA.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Nair, Varun and Schumacher, Elliot and Tso, Geoffrey and Kannan, Anitha},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17071 [cs]},
}

@misc{wang_describe_2023,
	title = {Describe, {Explain}, {Plan} and {Select}: {Interactive} {Planning} with {Large} {Language} {Models} {Enables} {Open}-{World} {Multi}-{Task} {Agents}},
	shorttitle = {Describe, {Explain}, {Plan} and {Select}},
	url = {http://arxiv.org/abs/2302.01560},
	doi = {10.48550/arXiv.2302.01560},
	abstract = {In this paper, we study the problem of planning in Minecraft, a popular, democratized yet challenging open-ended environment for developing multi-task embodied agents. We've found two primary challenges of empowering such agents with planning: 1) planning in an open-ended world like Minecraft requires precise and multi-step reasoning due to the long-term nature of the tasks, and 2) as vanilla planners do not consider the proximity to the current agent when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient. To this end, we propose "Describe, Explain, Plan and Select" (DEPS), an interactive planning approach based on Large Language Models (LLMs). Our approach helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goal Selector, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly. Our experiments mark the milestone of the first multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly doubles the overall performances. Finally, the ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the \${\textbackslash}texttt\{ObtainDiamond\}\$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01560 [cs]},
}

@misc{shinn_reflexion_2023,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	doi = {10.48550/arXiv.2303.11366},
	abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Shinn, Noah and Cassano, Federico and Labash, Beck and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = may,
	year = {2023},
	note = {arXiv:2303.11366 [cs]},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {http://arxiv.org/abs/2305.10601},
	doi = {10.48550/arXiv.2305.10601},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = may,
	year = {2023},
	note = {arXiv:2305.10601 [cs]},
}

@article{radford_language_2019,
	title = {Language models are unsupervised multitask learners},
	volume = {1},
	number = {8},
	journal = {OpenAI blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and {others}},
	year = {2019},
	pages = {9},
}

@misc{lialin_scaling_2023,
	title = {Scaling {Down} to {Scale} {Up}: {A} {Guide} to {Parameter}-{Efficient} {Fine}-{Tuning}},
	shorttitle = {Scaling {Down} to {Scale} {Up}},
	url = {http://arxiv.org/abs/2303.15647},
	abstract = {This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.},
	urldate = {2023-05-21},
	publisher = {arXiv},
	author = {Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
	month = mar,
	year = {2023},
	note = {arXiv:2303.15647 [cs]},
}

@misc{raschka_finetuning_nodate,
	title = {Finetuning {LLMs} {Efficiently} with {Adapters}},
	url = {https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters},
	abstract = {Why Finetuning LLMs? Large language models (LLMs) like BERT, GPT-3, GPT-4, LLaMA, and others are trained on a large corpus of data and have general knowledge. However, they may not perform as well on specific tasks without finetuning. For example, if you want to use a pretrained LLM for analyzing legal or medical documents, finetuning it on a corpus of legal documents can significantly improve the model's performance. (Interested readers can find an overview of different LLM finetuning methods in my previous article,},
	language = {en},
	urldate = {2023-05-21},
	author = {Raschka, Sebastian},
}

@misc{raschka_finetuning_nodate-1,
	title = {Finetuning {Large} {Language} {Models}},
	url = {https://magazine.sebastianraschka.com/p/finetuning-large-language-models},
	abstract = {An introduction to the core ideas and approaches},
	language = {en},
	urldate = {2023-05-21},
	author = {Raschka, Sebastian},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ramón Risco and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and {Shyamolima} and {Debnath} and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Statistics - Machine Learning},
}

@misc{miessler_ai_2023,
	title = {The {AI} {Attack} {Surface} {Map} v1.0},
	url = {https://danielmiessler.com/blog/the-ai-attack-surface-map-v1-0/},
	abstract = {Introduction Purpose Components Attacks Discussion Summary Introduction This resource is a first thrust at a framework for thinking about how to attack AI},
	language = {en-US},
	urldate = {2023-05-17},
	journal = {Daniel Miessler},
	author = {Miessler, Daniel},
	month = may,
	year = {2023},
}

@misc{noauthor_brexs_2023,
	title = {Brex's {Prompt} {Engineering} {Guide}},
	copyright = {MIT},
	url = {https://github.com/brexhq/prompt-engineering},
	abstract = {Tips and tricks for working with Large Language Models like OpenAI's GPT-4.},
	urldate = {2023-05-17},
	publisher = {Brex},
	month = may,
	year = {2023},
	note = {original-date: 2023-04-21T00:18:54Z},
}

@misc{noauthor_security_nodate,
	title = {Security with generative {AI}},
	url = {https://cloud.google.com/security/ai},
	abstract = {At Google Cloud, we're building a new security world with generative AI},
	language = {en},
	urldate = {2023-05-17},
	journal = {Google Cloud},
}

@misc{noauthor_creating_nodate,
	title = {Creating a {Coding} {Assistant} with {StarCoder}},
	url = {https://huggingface.co/blog/starchat-alpha},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	language = {en},
	urldate = {2023-05-15},
}

@misc{glaese_improving_2022,
	title = {Improving alignment of dialogue agents via targeted human judgements},
	url = {http://arxiv.org/abs/2209.14375},
	doi = {10.48550/arXiv.2209.14375},
	abstract = {We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78\% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8\% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Glaese, Amelia and McAleese, Nat and Trębacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and Campbell-Gillingham, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez and Green, Richard and Mokrá, Soňa and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14375 [cs]},
}

@misc{askell_general_2021,
	title = {A {General} {Language} {Assistant} as a {Laboratory} for {Alignment}},
	url = {http://arxiv.org/abs/2112.00861},
	doi = {10.48550/arXiv.2112.00861},
	abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = dec,
	year = {2021},
	note = {arXiv:2112.00861 [cs]},
}

@misc{rochefort-beaudoin_training_2023,
	title = {Training a {Language} {Model} {To} {Give} ({Non}) {Legal} {Advice}},
	url = {https://pub.towardsai.net/training-a-large-language-model-to-give-non-legal-advice-b9f6d7d11016},
	abstract = {In this article, I go through the basics for finetuning large language models like BLOOM on a legal text dataset. You can try it here !},
	language = {en},
	urldate = {2023-05-02},
	journal = {Medium},
	author = {Rochefort-Beaudoin, Thomas},
	month = apr,
	year = {2023},
}

@misc{yang_harnessing_2023,
	title = {Harnessing the {Power} of {LLMs} in {Practice}: {A} {Survey} on {ChatGPT} and {Beyond}},
	shorttitle = {Harnessing the {Power} of {LLMs} in {Practice}},
	url = {http://arxiv.org/abs/2304.13712},
	abstract = {This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at {\textbackslash}url\{https://github.com/Mooler0410/LLMsPracticalGuide\}.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing and Hu, Xia},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13712 [cs]},
}

@misc{noauthor_chatgpt_nodate,
	title = {{ChatGPT} {Prompt} {Engineering} for {Developers}},
	url = {https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/},
	abstract = {What you’ll learn in this course In ChatGPT Prompt Engineering for Developers, you will learn how to use a large language model (LLM) to quickly build new and powerful applications.  Using the OpenAI API, you’ll...},
	language = {en},
	urldate = {2023-04-28},
}

@misc{wolfram_what_2023,
	title = {What {Is} {ChatGPT} {Doing} … and {Why} {Does} {It} {Work}?},
	url = {https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/},
	abstract = {Stephen Wolfram explores the broader picture of what's going on inside ChatGPT and why it produces meaningful text. Discusses models, training neural nets, embeddings, tokens, transformers, language syntax.},
	language = {en},
	urldate = {2023-04-06},
	author = {{Wolfram}},
	month = feb,
	year = {2023},
}

@misc{noauthor_huggingchat_nodate,
	title = {{HuggingChat}},
	url = {https://hf.co/chat},
	abstract = {The first open source alternative to ChatGPT. 💪},
	language = {en},
	urldate = {2023-04-26},
}

@misc{raschka_understanding_nodate,
	title = {Understanding {Large} {Language} {Models}},
	url = {https://magazine.sebastianraschka.com/p/understanding-large-language-models},
	abstract = {A Cross-Section of the Most Relevant Literature To Get Up to Speed},
	language = {en},
	urldate = {2023-04-23},
	author = {Raschka, Sebastian},
}

@misc{vries_go_2023,
	title = {Go smol or go home},
	url = {https://www.harmdevries.com/post/model-size-vs-compute-overhead/},
	abstract = {Analysis of the Chinchilla scaling laws showing that we can significantly reduce the compute-optimal model size with minimal compute overhead.},
	language = {en-us},
	urldate = {2023-04-18},
	journal = {Harm de Vries},
	author = {Vries, Harm de},
	month = mar,
	year = {2023},
}

@misc{prystawski_why_2023,
	title = {Why think step-by-step? {Reasoning} emerges from the locality of experience},
	shorttitle = {Why think step-by-step?},
	url = {http://arxiv.org/abs/2304.03843},
	doi = {10.48550/arXiv.2304.03843},
	abstract = {Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare language models' ability to match conditional probabilities both with and without intermediate reasoning steps, finding that intermediate steps help only when the training data is locally structured with respect to dependencies between variables. Furthermore, intermediate variables need to be relevant to the relationship between observed information and target inferences. Our results illustrate how the statistical structure of training data drives the effectiveness of reasoning step by step.},
	urldate = {2023-04-16},
	publisher = {arXiv},
	author = {Prystawski, Ben and Goodman, Noah D.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03843 [cs]},
}

@inproceedings{zhang_automatic_2023,
	title = {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=5NTt8GFjUHkr&utm_source=substack&utm_medium=email},
	abstract = {Large Language Models (LLMs) can carry out complex reasoning tasks by generating intermediate reasoning steps. These steps are triggered by what is called chain-of-thought (CoT) prompting, which comes in two flavors: one leverages a simple prompt like "Let’s think step by step" to facilitate step-by-step reasoning before answering a question (Zero-Shot-CoT). The other uses manual demonstrations, each composed of a question and a reasoning chain that leads to an answer (Manual-CoT). Unfortunately, the superior performance of the latter strategy crucially hinges on manually generating task-specific demonstrations. This makes it far less scalable and more dependent on the talent of the CoT engineer. We show that such manual efforts may be eliminated by leveraging LLMs to generate the reasoning chains on its own. Since these generated chains often come with mistakes we propose a number of mitigation strategies. Our proposed Auto-CoT method automaticaly samples diverse questions and we perform post-processing quality control to generate usable reasoning chains from Zero-Shot-CoT. On ten public benchmark reasoning tasks, Auto-CoT performs on par with Manual-CoT without the need for human intervention. Code is available at https://github.com/amazon-research/auto-cot.},
	language = {en},
	urldate = {2023-04-16},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
	month = feb,
	year = {2023},
}

@misc{karpas_mrkl_2022-1,
	title = {{MRKL} {Systems}: {A} modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
	shorttitle = {{MRKL} {Systems}},
	url = {http://arxiv.org/abs/2205.00445},
	doi = {10.48550/arXiv.2205.00445},
	abstract = {Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and Leyton-Brown, Kevin and Muhlgay, Dor and Rozen, Noam and Schwartz, Erez and Shachaf, Gal and Shalev-Shwartz, Shai and Shashua, Amnon and Tenenholtz, Moshe},
	month = may,
	year = {2022},
	note = {arXiv:2205.00445 [cs]},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
}

@article{rahwan_machine_2019,
	title = {Machine behaviour},
	volume = {568},
	copyright = {2019 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1138-y},
	doi = {10.1038/s41586-019-1138-y},
	abstract = {Machines powered by artificial intelligence increasingly mediate our social, cultural, economic and political interactions. Understanding the behaviour of artificial intelligence systems is essential to our ability to control their actions, reap their benefits and minimize their harms. Here we argue that this necessitates a broad scientific research agenda to study machine behaviour that incorporates and expands upon the discipline of computer science and includes insights from across the sciences. We first outline a set of questions that are fundamental to this emerging field and then explore the technical, legal and institutional constraints on the study of machine behaviour.},
	language = {en},
	number = {7753},
	urldate = {2023-04-04},
	journal = {Nature},
	author = {Rahwan, Iyad and Cebrian, Manuel and Obradovich, Nick and Bongard, Josh and Bonnefon, Jean-François and Breazeal, Cynthia and Crandall, Jacob W. and Christakis, Nicholas A. and Couzin, Iain D. and Jackson, Matthew O. and Jennings, Nicholas R. and Kamar, Ece and Kloumann, Isabel M. and Larochelle, Hugo and Lazer, David and McElreath, Richard and Mislove, Alan and Parkes, David C. and Pentland, Alex ‘Sandy’ and Roberts, Margaret E. and Shariff, Azim and Tenenbaum, Joshua B. and Wellman, Michael},
	month = apr,
	year = {2019},
	keywords = {Human behaviour, Research management},
	pages = {477--486},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
}

@misc{eloundou_gpts_2023,
	title = {{GPTs} are {GPTs}: {An} {Early} {Look} at the {Labor} {Market} {Impact} {Potential} of {Large} {Language} {Models}},
	shorttitle = {{GPTs} are {GPTs}},
	url = {http://arxiv.org/abs/2303.10130},
	doi = {10.48550/arXiv.2303.10130},
	abstract = {We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of LLMs, while approximately 19\% of workers may see at least 50\% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15\% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56\% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10130 [cs, econ, q-fin]},
	keywords = {Computer Science - Computers and Society, Economics - General Economics},
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: {Solving} {AI} {Tasks} with {ChatGPT} and its {Friends} in {HuggingFace}},
	shorttitle = {{HuggingGPT}},
	url = {http://arxiv.org/abs/2303.17580},
	doi = {10.48550/arXiv.2303.17580},
	abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward advanced artificial intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards advanced artificial intelligence.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	month = apr,
	year = {2023},
	note = {arXiv:2303.17580 [cs]},
	keywords = {autoGPT},
}

@misc{noauthor_chinchilla_2023,
	title = {Chinchilla data-optimal scaling laws: {In} plain {English}},
	shorttitle = {Chinchilla data-optimal scaling laws},
	url = {https://lifearchitect.ai/chinchilla/},
	abstract = {Alan D. Thompson February 2023 Summary: Chinchilla showed that we need to be using 11× more data during training than that used for GPT-3 and similar models. This means that we need to source, clean, and filter to around 33TB of text data for a 1T-parameter model. How much text data should we use when [...]},
	language = {en-AU},
	urldate = {2023-04-09},
	journal = {Dr Alan D. Thompson – Life Architect},
	month = feb,
	year = {2023},
}

@misc{noauthor_stackllama_nodate,
	title = {{StackLLaMA}: {A} hands-on guide to train {LLaMA} with {RLHF}},
	shorttitle = {{StackLLaMA}},
	url = {https://huggingface.co/blog/stackllama},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	language = {en},
	urldate = {2023-04-09},
	keywords = {\#llamaccp},
}

@misc{noauthor_vicuna_nodate,
	title = {Vicuna: {An} {Open}-{Source} {Chatbot} {Impressing} {GPT}-4 with 90\%* {ChatGPT} {Quality}},
	shorttitle = {Vicuna},
	url = {https://vicuna.lmsys.org/},
	abstract = {by the Team with members from UC Berkeley, CMU, Stanford, and UC San Diego},
	language = {en-US},
	urldate = {2023-04-09},
	journal = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
}

@misc{noauthor_edge_nodate,
	title = {Edge {AI} {Just} {Got} {Faster}},
	url = {https://justine.lol/mmap/},
	abstract = {Using mmap() to load LLaMA faster in parallel with less memory.},
	language = {en},
	urldate = {2023-04-09},
	keywords = {\#llamaccp},
}

@misc{noauthor_check_2023,
	title = {Check {Your} {Facts} and {Try} {Again}: {Improving} {Large} {Language} {Models} with {External} {Knowledge} and {Automated} {Feedback}},
	shorttitle = {Check {Your} {Facts} and {Try} {Again}},
	url = {https://www.microsoft.com/en-us/research/group/deep-learning-group/articles/check-your-facts-and-try-again-improving-large-language-models-with-external-knowledge-and-automated-feedback/},
	abstract = {Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This blog introduces our work on LLM-Augmenter, […]},
	language = {en-US},
	urldate = {2023-04-06},
	journal = {Microsoft Research},
	month = mar,
	year = {2023},
}

@misc{weil_you_2023,
	title = {You {Are} {Not} a {Parrot}},
	url = {https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html},
	abstract = {And a chatbot is not a human. And a linguist named Emily M. Bender is very worried what will happen when we forget this.},
	language = {en-us},
	urldate = {2023-04-08},
	journal = {Intelligencer},
	author = {Weil, Elizabeth},
	month = mar,
	year = {2023},
}

@inproceedings{bender_dangers_2021,
	address = {Virtual Event Canada},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? 🦜},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	language = {en},
	urldate = {2023-04-08},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}
