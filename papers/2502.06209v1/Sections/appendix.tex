
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\renewcommand*{\theHsection}{\thesection}

\begin{center}
    {\Large \textbf{Enhancing Cost Efficiency in Active Learning with Candidate Set Query\\
    \vspace{0.5cm}
    {\it  ---Appendix--- }}}\\
    \vspace{0.5cm}
\end{center}

\section{Details of user study}
\label{app:user-study}

\begin{figure*}[!h]
\centering
\includegraphics[width=1\linewidth]{figure/app_1.pdf}
\vspace{-0.5cm}
\caption{Questionnaire and examples used in the user study.
(a) Each question contains an instruction, an image, and a set of candidates. In this case, the candidate set size is 4.
(b) We utilize 20 images in CIFAR-100, each with a resolution of 128 x 128 pixels.}
\label{app:query-examples}
\end{figure*}

We conduct a user study to examine how the size of a candidate set, $k$ in Sec.~\ref{sec:candidate_set_query}, affects the annotation time in practice.
Figure~\ref{app:query-examples} presents examples of the questionnaire and all images used in our user study.
To facilitate easy comparison with the theoretical costs~\citep{hu2018squeeze}, we set the candidate set sizes to 4, 8, 16, and 32.
To be specific about Figure~\ref{app:query-examples}, 
we use CIFAR-100 images resized to $128 \times 128$ using super resolution\footnote{https://www.kaggle.com/datasets/joaopauloschuler/cifar100-128x128-resized-via-cai-super-resolution} to enhance visibility for annotators.
We first randomly select 20 classes in CIFAR-100 and choose one image per class to organize the questionnaires.
For small-sized candidate sets, we ensure the inclusion of the ground truth by randomly trimming around it when generating the candidate sets.

We divide 44 annotators into four groups of 11 for each candidate set size to perform labeling tasks.
To account for potential outliers, we exclude the results of the annotators whose time taken deviates the most from the average time in each group.
Table~\ref{tab:user-study-app} shows that as the candidate set size increases, the time per query increases and the accuracy decreases. In addition, on the right side of Table~\ref{tab:user-study-app}, the comparison between the experimental costs and theoretical costs reveals a significant correlation of 0.97.


\begin{table}[h!]
    \caption{User study for different sizes of candidate set query.}
    \label{tab:user-study-app}
    \begin{center}
        \begin{small}
            \setlength\tabcolsep{6pt}
            \centering
            \begin{tabular}{c|ccc|cc}
                \toprule
                $k$ & Total time (s) & Time per query (s) & Accuracy (\%) & Experimental & Theoretical \\ \midrule
                4 & \textbf{69.4$_{\pm 13.8}$} & \textbf{3.47$_{\pm 0.69}$} & \textbf{100.0$_{\pm 0.0}$} & 2.0 & 2 \\
                8 & 91.5$_{\pm 27.3}$ & 5.20$_{\pm 1.36}$ & 98.5$_{\pm 3.2}$ & 2.6 & 3 \\
                16 & 116.9$_{\pm 29.6}$ & 6.94$_{\pm 1.48}$ & 99.5$_{\pm 1.5}$ & 3.4 & 4 \\
                32 & 166.9$_{\pm 30.8}$ & 8.35$_{\pm 1.54}$ & 95.5$_{\pm 5.2}$ & 4.8 & 5 \\
                \bottomrule
            \end{tabular}
        \end{small}
    \end{center}
\end{table}

\section{Implementation details and configuration}
\label{app:impelment_details}

Table~\ref{tab:budget_cost} presents the configuration of our main experiments for each dataset. In all experiments, we fixed the per-round budget, which limits the number of annotated instances per active learning (AL) round. Given this budget constraint, we compute the labeling cost for each AL round to assess labeling efficiency.
The batch size for CIFAR-10 and CIFAR-100 was determined to be 128, while that for ImageNet64x64 is set to 128. We normalized the input image to ensure the stability of the training.
We trained our classification model on CIFAR-10 and CIFAR-100 using NVIDIA RTX 3090 and on ImageNet64x64 using 4 NVIDIA A100 GPUs in parallel. 
The training requires about 5 GPU hours for CIFAR-10 and CIFAR-100, and about 1.5 GPU days for ImageNet64x64.


\begin{table}[h!]
    \caption{Detailed dataset and budget configuration for the proposed scenario.}
    \label{tab:budget_cost}
    \begin{center}
        \begin{small}
            \setlength\tabcolsep{6pt}
            \centering
            \begin{tabular}{cccccccc}
                \toprule
                Dataset & $L$ & $\text{log}_2{L}$ & Size & Cost of full label & \# of rounds & Per-round budget \\ \midrule
                CIFAR-10      & 10   & 3.322 & 50K     & 166.1K   & 10 & 6K  \\
                CIFAR-100     & 100  & 6.644 & 50K     & 332.2K   & 9 & 6K  \\
                ImageNet64x64 & 1000 & 9.966 & 1.2M & 12.7M & 16 & 60K \\
                \bottomrule
            \end{tabular}
        \end{small}
    \end{center}
\end{table}


\noindent\textbf{Code.}
This part demonstrates the reproducibility of our work by providing comprehensive details on the source code release. We have made available the entire framework, which includes the data sampling methods, evaluation procedures, and the overall training pipeline. Our aim is to ensure that other researchers can easily replicate and build upon our results. To get started with running the code, please refer to the \verb|script.sh| and \verb|readme.md| files. 
\verb|readme.md| contains the instructions to comprehend and execute our experiments seamlessly, and \verb|script.sh| includes some example commands. 
To understand our proposed method better, you can examine the Python script \verb|al/strategy_dtopk.py|. This file includes the implementation details of our active learning strategies, particularly \emph{candidate set query} design. Furthermore, our code can run on CIFAR-10, CIFAR-100~\footnote{\url{https://www.cs.toronto.edu/~kriz/cifar.html}}, and ImageNet64x64~\footnote{\url{https://patrykchrabaszcz.github.io/Imagenet32/}}, which are available online. Note that you can modify the running configuration such as dataset, sampling method, and budget through command-line arguments.

\section{Additional clarification on candidate set construction}
\label{sec:clarification}

\noindent\textbf{The detailed procedure of computing $\hat{Q}(\alpha)$ in~\Eq{quantile}}.
We begin with computing the collection of conformal scores $\mathbf s$ for the calibration dataset \(\mathcal{D}_\text{cal}\). For each data point \((\bx_i, y_i) \in \mathcal{D}_\text{cal}\), the conformal score is defined as:
\begin{equation}
    s_i := 1 - P_{\boldsymbol{\theta}}(y_i \mid \bx_i), \quad \text{for } i = 1, 2, \cdots, n_\text{cal} \;,
\end{equation}
where $n_\text{cal} = |\mathcal{D}_\text{cal}|$.
Using these scores, we define the empirical distribution function \(F_n(s)\), which measures the proportion of scores less than or equal to a given value \(s\). Formally, \(F_n(s)\) is expressed as:
\begin{equation}
    F_n(s) = \frac{1}{n_\text{cal}} \sum_{i=1}^{n_\text{cal}} \mathds{1}[s_i \leq s] \;,
\end{equation}
where $\mathds{1}[\cdot]$ is an indicator function.
The $(1-\alpha)$ empirical quantile is then defined as the smallest score $s_i$ such that the proportion of scores satisfying $s_i \leq s$ is at least $(1 - \alpha)$.
Mathematically, this is given as $\min_{i \in [n_\text{cal}]} \left\{ F_n(s_i) \geq 1 - \alpha \right\}$, where $[n_\text{cal}] = \{1, 2, \cdots, n_\text{cal}\}$.
\begin{equation}
    \hat{Q}(\alpha) := \min_{i \in [n_\text{cal}]} \left\{ F_n(s_i) \geq (1 - \alpha) \right\} \;.
    \label{eq:quantile2}
\end{equation}
Note that Eq.~(\ref{eq:quantile2}) is equivalent to Eq.~(\ref{eq:quantile}).

\section{Impact of proposed cost-efficient sampling across different sampling strategies}




\Fig{main} illustrates that combining CSQ with our cost-efficient sampling method results in a significant performance improvement. Additionally, \Fig{comparison-(a)} examines how the cost-efficient sampling improves performance compared to using CSQ alone. In this section, we compare different sampling strategies when combined with their cost-efficient variants. Notably, the performance of cost-efficient sampling improves substantially when paired with \sftype{Ent}, \sftype{ProbCover}, and \sftype{SAAL}. This demonstrates that our cost-efficient acquisition method (~\Eq{acquisition}) can be integrated with any sample-wise acquisition strategy, including but not limited to entropy-based sampling, ProbCover~\citep{yehuda2022active}, and SAAL~\citep{kim2023saal}. However, BADGE~\citep{ash2019deep} does not involve sample-wise acquisition and instead performs random sampling based on k-means++ initialization. This explains why \sftype{CSQ+Cost(BADGE)} does not outperform \sftype{CSQ+BADGE}. 
Nevertheless, this is not a major drawback, \sftype{CSQ+BADGE} itself without cost-efficient sampling still achieves significantly better performance compared to \sftype{CQ+BADGE}.



\input{graph/appendix/sampling2x2}

\section{Change in candidate set size across rounds}
In~\Fig{set-size-accuracy}, we show that the CSQ effectively reduces the candidate set size $k$ throughout AL rounds on CIFAR-10, CIFAR-100, and ImageNet64x64 datasets.
After the first round, CSQ achieves a sufficiently small candidate set size and continues to reduce it as accuracy improves, thereby enhancing labeling efficiency.

\input{graph/avg_candidate_set_size}

\section{Examples of constructed candidate sets}
In~\Fig{qual}, we present example results showing input images and their corresponding candidate sets on ImageNet64x64.
Thanks to the conformal prediction, the proposed method allows for flexible adjustment of the candidate set for each sample.
For certain samples (\Fig{qual}(\textit{left})), the candidate set is reduced to minimize labeling cost, while for uncertain samples (\Fig{qual}(\textit{right})), the candidate set is expanded to include the true label.
\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{figure/qualitative_result_v5.pdf}
    \caption{Examples of input images and their corresponding candidate sets constructed from our method in fifth round on ImageNet64x64.
    The ground-truth class is highlighted in red (best viewed in color).}
    \label{fig:qual}
\end{figure*}

\section{Impact of cost-optimized error rate selection}
\label{app:ablation_cost_opt}
In~\Fig{optim_app}, we present the impact of cost-optimized error rate selection as in~\Eq{optimize}, evaluated on CIFAR-100 using entropy sampling, in terms of relative labeling cost (\%).
As shown in~\Fig{optim_app-(a)}, the proposed optimization consistently reduces labeling cost across all rounds by selecting the optimal $\alpha = \alpha^*$.
In~\Fig{optim_app-(b)}, the pink triangle indicate how the most cost-effective $\alpha$ changes with each active learning round, showing that labeling costs vary depending on the chosen $\alpha$.
Our method enhances cost efficiency by selecting the $\alpha^*$ (blue square) in each round through cost optimization.
    
\input{graph/alpha_ablation}

\section{Impact of hyperparameter $d$}
\label{app:ablation}

\noindent\textbf{Impact of informativeness-cost balancing hyperparameter $d$.}
The hyperparameter $d$ in our acquisition function (\Eq{acquisition}) balances the trade-off between labeling cost and the informativeness of a selected sample, requiring both factors to be considered.
We provide a comprehensive analysis showing the trend of performance in accuracy with varying $d$ values over AL rounds for CIFAR-10, CIFAR-100, and ImageNet64x64 in \Fig{tuning}. For CIFAR-10 (\Fig{tuning}a), both accuracy and labeling cost remain robust to the change of $d$, varying only 0.5\%p in accuracy. For CIFAR-100 (\Fig{tuning}b), the overall performance is still insensitive yet slightly increasing as $d$ decreases. For ImageNet64x64 (\Fig{tuning}c), on the other hand, the performance decreases as $d$ increases.
Regarding that a larger $d$ prioritizes more uncertain samples, this result aligns with recent observations in AL that uncertainty-based selection performs better in scenarios with larger labeling budgets~\citep{hacohen2022active}.

\noindent\textbf{Guidelines for selecting proper hyperparameter $d$.}
We provide the following guidelines for setting $d$. For datasets with fewer than 100 classes, $d$ values between 0.3 and 1.0 may be effective, as they ensure robustness on simple datasets like CIFAR-10 and reduce labeling costs on more complex datasets like CIFAR-100. For larger datasets closer in scale to ImageNet, exploring $d \geq 1.0$ can help further improve the model performance.
\input{graph/appendix/d_tuning}

\edited{\section{Discussion on handling outliers and anomalous datapoints}
\label{sec:ood}
Dealing with out-of-distribution (OOD) data points showing high uncertainty scores has been a chronic issue in active learning and may affect the efficiency of candidate set query (CSQ). Recent open-set active learning approaches~\citep{du2021contrastive, kothawade2021similar, ning2022active, park2022meta, yang2024not} tackle this by filtering out OOD samples during active sampling using an OOD classifier. Our CSQ framework integrates seamlessly with these methods, focusing on labeling in-distribution (ID) samples to prevent cost inefficiencies.

However, as OOD classifiers are not flawless, some OOD samples may still be selected. One advantage of our method is its ability to leverage the calibration set to capture information about such mixed OOD samples. This enables adjustments such as increasing the OOD classifier threshold to exclude more OOD-like data or incorporating the OOD ratio into the alpha optimization process in~\Eq{optimize}.
Optimizing the combination of OOD and ID classifier scores within the calibration set or designing better OOD-aware queries presents promising future research directions.}


\edited{\section{{Compatibility between candidate set construction and uncertain samples}}
Figure~\ref{fig:superiority} compares CSQ and conventional query (CQ) on CIFAR-100 with entropy-based sampling (Ent) and our acquisition function with entropy measure (\sftype{Cost(Ent)},~\Eq{acquisition}) across AL rounds, with a fixed number of samples per round.

\noindent\textbf{Our acquisition function provides superior accuracy per cost.} The comparison between \sftype{CSQ+Cost(Ent)} and \sftype{CSQ+Ent} demonstrates that the proposed acquisition function reduces labeling costs with only a marginal accuracy trade-off. 

\noindent\textbf{Candidate set query (CSQ) can reduce labeling costs even for uncertain samples.} The comparison between \sftype{CQ+Ent} and \sftype{CSQ+Ent} demonstrates that CSQ effectively reduces labeling costs, even with uncertainty-based sampling methods like entropy sampling. This shows that CSQ can narrow down annotation options even for uncertain samples. Note that \sftype{CSQ+Ent} shows the same accuracy as \sftype{CQ+Ent}, since they used the same sampling method.}
\input{graph/appendix/superiority}

\edited{\section{Experiments in language domain}
\label{sec:language}
\noindent\textbf{Dataset.}
The R52 dataset~\citep{lewis1997reuters} is a subset of the Reuters-21578~\citep{lewis1997reuters} news collection, specifically curated for text classification tasks.
It comprises documents categorized into 52 distinct classes, with a total of 9,130 documents. The dataset is divided into 6,560 training documents and 2,570 testing documents.
Each document is labeled with a single category, and the categories are selected to ensure that each has at least one document in both the training and testing sets.
This structure makes the R52 dataset particularly suitable for evaluating text classification models.

\noindent\textbf{Implementation details.}
We adopt an SVM model~\citep{cortes1995support} with sigmoid kernel for classification.
We conduct 11 AL rounds of consecutive data sampling and model updates, where the per-round budget is 600.
The hyperparameter $d$ for our acquisition function is set as 1.2.
In the initial round, we randomly sample 300 samples.
In each round, the model is evaluated based on three factors: its accuracy (\%) and Micro-F1 (\%).

Figure~\ref{fig:nlp} presents a comparison of candidate set query (CSQ) and conventional query (CQ) on the text classification dataset (R52) with random sampling (\sftype{Rand}), entropy sampling (\sftype{Ent}), and our acquisition function with entropy measure (\sftype{Cost(Ent)},~\Eq{acquisition}) across AL rounds. CSQ approaches consistently outperform the CQ baselines by a significant margin across various budgets and acquisition functions. Especially at round 10, \sftype{CSQ+Rand} reduces labeling cost by 65.6\%p compared to its conventional query baseline. The result demonstrates that the proposed CSQ framework generalizes to the text classification domain.}
\input{graph/appendix/nlp}


\edited{\section{Experiments on real-world datasets}
\noindent\textbf{Experiment on datasets containing label noise.}
\label{sec:noise}
We evaluate the candidate set query (CSQ) framework on CIFAR-100 with noisy labels, simulating a scenario where human annotators misclassify images into random classes with a noise rate $\epsilon$. This is modeled using a uniform label noise~\citep{frenay2013classification} with $\epsilon$ set to 0.05 and 0.1. Note that this scenario is unfavorable for CSQ, as a misclassifying annotator would reject the actual true label even if the candidate set includes it.
Figure~\ref{fig:noise} compares CSQ and conventional query (CQ) on CIFAR-100 with noisy labels using entropy sampling (\sftype{Ent}) and our acquisition function with entropy measure (\sftype{Cost(Ent)}) across 2, 6, and 9 rounds.

Despite the disadvantageous scenario, our method (\sftype{CSQ+Cost(Ent)}) reduces labeling cost compared to the baseline (\sftype{CQ+Ent}) across varying AL rounds and noise rates. At round 9, \sftype{CSQ+Cost(Ent)} achieves cost reductions of 33.4\%p and 27.4\%p at noise rates of 0.05 and 0.1, respectively. It also consistently outperforms the baseline in terms of accuracy per labeling cost, demonstrating the robustness of CSQ.
Additionally, CSQ has the potential to reduce label noise, as narrowing the candidate set can lead to more precise annotations. Our user study (\Tbl{user-study}) shows that reducing candidate set size improves annotation accuracy, suggesting that CSQ can further enhance performance by reducing label noises.

\noindent\textbf{Experiment on datasets containing class imbalances.}
\label{sec:imbalance}
Figure~\ref{fig:imbalance} compares candidate set query (CSQ) and conventional query (CQ) on CIFAR-100-LT~\citep{cui2019class}, a class-imbalanced version of CIFAR-100, using entropy sampling (\sftype{Ent}), and our acquisition function with entropy measure (\sftype{Cost(Ent)}) across AL rounds. The experiments use imbalance ratios (\ie, ratios between the largest and smallest class sizes) of 3, 6, and 10. Note that the maximum AL rounds vary with the imbalance ratio due to dataset size, with a maximum of 4 rounds for ratios of 3 and 6, and 6 rounds for a ratio of 10.

The result shows that our method (\sftype{CSQ+Cost(Ent)}) reduces labeling cost compared to the baselines (\sftype{CQ+Ent}) by significant margins across varying AL rounds and imbalance ratios. Specifically, at round 4, \sftype{CSQ+Cost(Ent)} achieves cost reductions of 31.1\%p and 29.2\%p at imbalance ratios of 6 and 10, respectively. In terms of accuracy per labeling cost, \sftype{CSQ+Cost(Ent)} consistently outperforms the baseline, demonstrating the robustness of the CSQ framework in class-imbalanced scenarios.}
\input{graph/appendix/noisy_labels}
\input{graph/appendix/class_imbalances}


\pagebreak

\input{graph/appendix/larger_main}
