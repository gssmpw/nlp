\section{Related Work}

\textbf{Acquisition functions in AL.}
The key to AL is to select and annotate the most informative samples~\cite{settles2009active, dasgupta2011two, hanneke2014theory}.
To assess informativeness,
various acquisition functions have been proposed, considering either the uncertainty of model predictions~\citep{asghar2016deep, he2019towards, ostapuk2019activelink,fuchsgruber2024uncertainty,kim2024active,cho2024querying,kim2023saal}, diversity in feature space~\citep{sener2017active, sinha2019variational,yehuda2022active}, or both~\citep{ash2019deep, hwang2022combating, wang2015querying, wang2019incorporating,hacohen2022active,NEURIPS2023_2b09bb02, hacohen2023select}.
\edited{Disagreement-based AL and its variants are supported by rigorous theoretical learning guarantees~\citep{hanneke2014theory, krishnamurthy2019active}.}
However, these methods assume uniform sample costs and select based solely on the amount of information. We emphasize that the labeling cost required for each sample varies and prioritize samples offering the best information-to-cost ratio.

\textbf{Efficient query design.}
Designing efficient annotation queries reduces the annotation costs of crafting datasets.
In AL, diverse types of queries have been investigated, including conventional classification queries, one-bit queries~\citep{hu2020one, joshi2010breaking} asking for yes or no answers, multi-class queries~\citep{hwang2023active} identifying all classes within a set of multiple instances, relative quires~\citep{qian2013active} asking for similarity of triplets, and correction queries~\citep{kim2024active} utilizing pseudo labels from the model.
While these query methods require tailored loss functions, our candidate set query (CSQ) is cost-efficient and provides complete supervision, integrating seamlessly with existing loss functions.
The approach closely related to CSQ is the $n$-ary query~\citep{bhattacharya2019active}, which reduces the search space by asking for the correct class among top-$n$ predictions of the model.
However, the $n$-ary query uses a fixed number of top-$n$ predictions for all data without considering individual sample difficulty.
CSQ, on the other hand, adjusts the candidate set size based on sample difficulty and model performance using conformal prediction.
Through rigorous comparisons, we demonstrate that CSQ achieves a superior model performance at the same cost compared to the previous query designs.

\textbf{Conformal prediction (CP).}
CP enables us to quantify uncertainty in predictions with an associated confidence level~\citep{shafer2008tutorial}. 
Recent advances in CP empower classifiers to generate predictive sets that include the true label with a probability chosen by the user~\citep{angelopoulos2020uncertainty, angelopoulos-gentle}.
In the field of AL, nonconformity measurements from CP are employed in the acquisition function to select informative samples~\citep{matiz2020conformal}.
In contrast, we utilize CP not only to develop a cost-efficient acquisition function but also to design an efficient candidate set query reducing the labeling cost.
