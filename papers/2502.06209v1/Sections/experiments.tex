\section{Experiments}
\label{sec:exp}

\subsection{Experimental setup}
\label{sec:setup}

\textbf{Datasets.}
We use three image classification datasets: CIFAR-10 \citep{krizhevsky2009learning}, CIFAR-100 \citep{krizhevsky2009learning}, and ImageNet64x64~\citep{chrabaszcz2017downsampled}.
CIFAR-10 comprises 50K training and 10K validation images across 10 classes.
CIFAR-100 contains the same number of images as CIFAR-10, but is associated with 100 classes.
ImageNet64x64 is a downsampled version of ImageNet~\citep{5206848} with a resolution of $64 \times 64$, which consists of 1.2M training and 50K validation images with 1000 classes.
Following previous studies, we evaluate a model using the validation split of each dataset.
\input{graph/main_exp}





\noindent\textbf{Implementation details.}
For CIFAR-10 and CIFAR-100, we adopt ResNet-18~\citep{he2016deep} as a classification model.
We train it for 200 epochs using AdamW~\citep{loshchilov2017decoupled} optimizer with an initial learning rate of $\expnum{1}{3}$, decreasing by a factor of 0.2 at epochs 60, 120, and 160.
We apply a weight decay of $\expnum{5}{4}$ and a data augmentation consists of random crop, random horizontal flip, and random rotation.
For ImageNet64x64, we adopt WRN-36-5~\citep{zagoruyko2016wide}, and train it for 30 epochs using AdamW optimizer with an initial learning rate of $\expnum{8}{3}$.
We apply a learning rate warm-up for 10 epochs from $\expnum{2}{3}$.
After the warm-up, we decay the learning rate by a factor of 0.2 every 10 epochs.
We adopt random horizontal flip and random translation as data augmentation.
For all the datasets, we use Mix-up~\citep{zhang2018mixup}, where a mixing ratio is sampled from $\text{Beta}(1,1)$.
We set the size of the calibration dataset $n_\text{cal}$ to 500 for CIFAR-10 and CIFAR-100, and 5K for ImageNet64x64.
For CIFAR-10 and CIFAR-100, $d$ in~\Eq{acquisition} is set to 0.3 for all samplings. For ImageNet64x64, $d$ is set to 1.2.
The analysis of the impact of $d$ and the dataset-wise guidelines for determining $d$ are provided in Appendix~\ref{app:ablation}.

\noindent\textbf{Active learning protocol.}
For CIFAR-10, we conduct 10 AL rounds of consecutive data sampling and model updates, while for CIFAR-100, we perform 9 AL rounds.
In both cases, the per-round budget is 6K images.
For ImageNet64x64, we conduct 16 AL rounds with a per-round budget of 60K images.
The detailed budget configuration for the three datasets is shown in~\Tbl{budget_cost}.
In the initial round, we randomly sample 1K images for CIFAR-10, 5K images for CIFAR-100, and 60K images for ImageNet64x64.
In each round, the model is evaluated based on two factors: its accuracy (\%) on the validation set, and the accumulated annotation cost required to train it.
The annotation cost is defined as a relative labeling cost (\%) compared to the cost of labeling the entire training set using the conventional query, given by $N \log_2{L}$, where $N$ is the size of the entire training set, and $L$ is the number of classes.
We conduct all experiments with three independent trials with different random seeds and report the mean and standard deviation to ensure reproducibility.

\noindent\textbf{Baseline methods.} 
We compare our candidate set query (\sftype{CSQ}) with the conventional query (\sftype{CQ}) in combination with various sampling strategies.
To be specific, we employ random (\sftype{Rand}), entropy (\sftype{Ent}), \sftype{BADGE}~\citep{ash2019deep}, \sftype{ProbCover}~\citep{yehuda2022active}, and \sftype{SAAL}~\citep{kim2023saal} as the sampling strategies.
\sftype{Cost($\cdot$)} indicates the proposed cost-efficient sampling (\Eq{acquisition}) using conventional acquisition scores; \eg, \sftype{Cost(SAAL)} is the one combined with SAAL.
We denote the combination of the query and sampling method with `\sftype{+}', \eg, \sftype{CSQ+Rand} is a candidate set query with random sampling. 
\input{graph/avg_candidate_set_size_imagenet}
\begin{table}[t!]
    \caption{The results of the user study showing the annotation time (second) and accuracy (\%) for the same images with varying size of class options (candidate set).
    This result demonstrates that a small candidate set improves both labeling efficiency and accuracy.
    The results also align closely with theoretical costs, as shown in~\Fig{teaser}(\emph{right}).}
    \label{tab:user-study}
    \centering
    \scalebox{0.86}{
    \begin{tabular}{l|cccc}
        \toprule
        Set size & 4 & 8 & 16 & 32 \\ \midrule
        Time (s) & \textbf{69.4$_{\pm 13.8}$} & 91.5$_{\pm 27.3}$ & 116.9$_{\pm 29.6}$ & 166.9$_{\pm 30.8}$\\
        Acc. (\%) & \textbf{100.0$_{\pm 0.0}$} & 98.5$_{\pm 3.2}$ & 99.5$_{\pm 1.5}$ & 95.5$_{\pm 5.2}$ \\
        \bottomrule
    \end{tabular}
    }
\end{table}
\input{graph/component_analysis}
\subsection{Experimental results}
\noindent\textbf{Candidate set query vs. Conventional query.}
In~\Fig{main}, we compare the performance of the candidate set query (\sftype{CSQ}) with the conventional query (\sftype{CQ}) on CIFAR-10, CIFAR-100, and ImageNet64x64 with different acquisition functions.
\sftype{CSQ} approaches consistently outperform the \sftype{CQ} approaches across various acquisition functions and datasets, demonstrating the general effectiveness of our method.
Notably, \sftype{CSQ} reduces the labeling cost of \sftype{CQ} by 43\%, 54\%, and 42\% on CIFAR-10, CIFAR-100, and ImageNet64x64, respectively.
This is promising as it shows that the same volume of labeled data can be obtained at roughly half the cost, without introducing any label noise or sample bias.
Notably, the performance gain of \sftype{CSQ} increases as the model improves, as it is tailored to the improved model.
In the appendix, we also present experiments on a text classification task (\Fig{nlp}) showing the generalization ability of the proposed method to the natural language domain.
Additionally, we provide the zoomed version of~\Fig{main} in~\Fig{app:cifar10} and~\Fig{app:cifar100}.

{\noindent\textbf{Progressive reduction in candidate set size.}
The effectiveness of \sftype{CSQ} stems from its ability to reduce labeling costs through smaller candidate sets.
To verify this, \Fig{set-size-accuracy-imagenet} shows the average size of the candidate sets and accuracy (\%) of our method with varying AL rounds on ImageNet64x64.
After the first round, CSQ achieves a sufficiently small candidate set size and continues to reduce it as accuracy improves.
More results on CIFAR-10 and CIFAR-100 are shown in~\Fig{set-size-accuracy}.}


\suha{\noindent\textbf{Empirical validation for our cost model.}
\label{sec:user-study}
We conduct a user study with 40 annotators who label samples using candidate sets of various sizes;
see Appendix~\ref{app:user-study} for more details.
The results in Table~\ref{tab:user-study} suggest that reducing candidate sets improves both labeling efficiency and accuracy.
They also align closely with the theoretical cost~\citep{hu2020one}, as shown in~\Fig{teaser}(\emph{right}).}

\subsection{Ablation studies}
\input{graph/query_design}

\noindent\textbf{Contribution of each component.}
Figure~\ref{fig:comparison-(a)} demonstrates the contribution of each component in our method across varying AL rounds: candidate set query (\Eq{candidate}), cost optimization of $\alpha$ (\Eq{optimize}), and the proposed acquisition function (\Eq{acquisition}).
The results show consistent performance improvements from each component in every round.
The performance gap between \sftype{CQ+Ent} and \sftype{CSQ}($\alpha=0.1$)+\sftype{Ent} verifies the efficacy of proposed CSQ framework, which provides the largest improvement.
The gap between \sftype{CSQ}($\alpha=0.1$)+\sftype{Ent} and \sftype{CSQ+Ent} shows the impact of $\alpha$ optimization, offering modest but steady gains across rounds.
Finally, the gap between \sftype{CSQ+Ent} and \sftype{CSQ+Cost(Ent)} shows the effectiveness of our acquisition function, particularly from 4 to 6 rounds.

\noindent\textbf{Impact of calibration set size.}
In~\Fig{comparison-(b)}, we evaluate the relative labeling cost (\%) at the fifth round with varying calibration set sizes $n_\text{cal}$ in~\Eq{quantile} to assess its impact on the performance on CIFAR-100.
As shown in~\Fig{comparison-(b)}, our method shows consistent performance, varying by less than 2\%p as the calibration set size changes from 0.1K to 2K, and significantly outperforms the baseline.

\noindent\textbf{Ablation study on candidate set design.}
Figure~\ref{fig:candidate} illustrates the effectiveness of using conformal prediction (\sftype{Conformal}~($\alpha=0.1$)) for candidate set construction on CIFAR-100, compared to baselines: \sftype{Conventional} (using all classes), \sftype{Top1} (top-1 prediction), \sftype{Top10} (top-10 predictions), and \sftype{Oracle} (smallest top-$k$ set always containing the ground truth).
Note that \sftype{Oracle} represents an unattainable upper bound requiring knowledge of the ground truth.
\hsh{\sftype{Top1} and \sftype{Top10} are variants of the $n$-ary query~\citep{bhattacharya2019active} baseline.}
For consistency, we fixed $\alpha=0.1$ in~\Eq{candidate}.
Figures~\ref{fig:candidate-(a)} and \ref{fig:candidate-(b)} show that conformal prediction consistently reduces labeling cost compared to the baselines.
While \sftype{Top10} is effective in the early rounds and \sftype{Top1} becomes more efficient as the model improves, our method adapts throughout and outperforms all baselines in every round.
Figure~\ref{fig:candidate-(c)} demonstrates that with $\alpha=0.1$, our method includes the ground-truth class in over 90\% of cases, aligning with~\Eq{conf}, while the top-$k$ baselines show lower inclusion rates, especially in early and middle rounds.
This demonstrates that conformal prediction effectively adjusts candidate set sizes based on sample uncertainty, ensuring ground-truth inclusion and improving labeling efficiency.
Examples of the candidate sets on ImageNet64x64 are presented in~\Fig{qual}.


\noindent\textbf{Impact of cost-efficient acquisition function.}
In~\Tbl{ablation-cost}, we investigate the impact of the proposed cost-efficient sampling (\Sec{acquisition}) on CIFAR-100, in terms of accuracy per relative labeling cost.
Our cost-efficient sampling strategy consistently improves the cost-effectiveness across various conventional acquisition functions.

\subsection{Additional experiments}
In the appendix, we provide additional results verifying the impact of cost-optimized error rate selection in~\Eq{optimize} (\Fig{optim_app}), generalization ability to language domain (\Fig{nlp}), and robustness to label noise (\Fig{noise}) and class imbalance (\Fig{imbalance}).
\begin{table}[t!]
    \caption{Effectiveness of the proposed cost-efficient sampling (\sftype{Cost($\cdot$)}) with CSQ evaluated on CIFAR-100, measured by accuracy per cost.}
    \label{tab:ablation-cost}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{l|ccc}
        \toprule
        Sampling & 3 \textsuperscript{rd} round & 6 \textsuperscript{th} round & 9 \textsuperscript{th} round \\ \midrule
        \sftype{Ent} & 1.74 & 1.36 & 1.24 \\
        \rowcolor[HTML]{EFEFEF}
        \sftype{Cost(Ent)} & \textbf{2.09} & \textbf{1.56} & \textbf{1.30}  \\
        \sftype{ProbCover} & 1.72 & 1.47 & 1.30  \\
        \rowcolor[HTML]{EFEFEF}
        \sftype{Cost(ProbCover)} & \textbf{2.10} & \textbf{1.66} & \textbf{1.32}  \\
        \sftype{SAAL} & 1.83 & 1.37 & 1.25  \\
        \rowcolor[HTML]{EFEFEF}
        \sftype{Cost(SAAL)} & \textbf{2.12} & \textbf{1.64} & \textbf{1.31}  \\
        \bottomrule
    \end{tabular}
    }
\end{table}
