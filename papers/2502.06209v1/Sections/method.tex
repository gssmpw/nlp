\section{Proposed Method}
\label{sec:method}
We consider general classification tasks such that for input $\bx$ and a categorical variable $y \in \mathcal{Y} = \{1,2, \ldots, L\}$, a model parameterized by $\boldsymbol{\theta}$ predicts the class of the input as $\argmax_{y \in \mathcal{Y}} P_{\boldsymbol{\theta}}(y|\bx)$.
We study an active learning (AL) scenario conducted over $R$ rounds.
In each round $r$, a budget of $B$ samples is actively selected from the unlabeled data pool $\mathcal{X}$ using an acquisition function.
This actively selected set $\mathcal{A}_r$ is then labeled by an annotator to form the labeled dataset $\mathcal{D}_{r}$ with labeling cost $C_r$, and is used to update the model.
Let $\boldsymbol{\theta}_r$ denote the model trained on the accumulated labeled data up to round $r$, $\bigcup_{i=0}^{r}\mathcal{D}_{i}$.  
Our goal is to maximize the performance of $\boldsymbol{\theta}_r$, while minimizing the accumulated cost $\bigcup_{i=0}^{r} C_{i}$.
The key aspect of the proposed method is the candidate set query (CSQ), which reduces $C_r$ by narrowing the set of candidate classes presented to annotators.
For simplicity, we omit the round index $r$ from $\boldsymbol{\theta}_r$ in the remainder of this section.

In the following, we first introduce CSQ and discuss its efficiency in labeling cost (\Sec{candidate_set_query}).
Then, we present a method to construct a candidate set based on the prediction uncertainty of a trained model for a given sample (\Sec{candidate_set_construction}).
Lastly, we introduce an acquisition function designed to consider cost efficiency as well as information gain (\Sec{acquisition}).
The overall pipeline of the CSQ framework is summarized in~\Alg{overall}.
\input{algs/alg_1_overall}


\subsection{Candidate set query}
\label{sec:candidate_set_query}
CSQ for an instance $\bx$ is associated with a (non-empty) candidate set $\hat{Y}(\bx) \subseteq \mathcal{Y}$ such that $1 \leq |\hat{Y}(\bx)| \leq L$.
CSQ first asks the annotator to choose the ground-truth class in $\hat{Y}(\bx)$ (if exists) or to verify the absence of the ground-truth label in $\hat{Y}(\bx)$,
\ie, the annotator is first asked to pick an option out of $(k+1)$ choices, where $k=|\hat{Y}(\bx)|$.
Only if the absence of the ground-truth class in the candidate set is verified, the annotator is further asked to select the ground-truth class from the remaining ones $\mathcal{Y} \setminus \hat{Y}(\bx)$.
To analyze the cost of CSQ, following the information-theoretic cost model~\citep{hu2020one} and our empirical study in Table.~\ref{tab:user-study}, we assume that the cost of choosing an option out of $k$ many candidates is $\log_2 k$.
Then, the labeling cost $\Gamma(\hat{Y}(\bx), y)$ of CSQ for input $\bx$, ground-truth label $y$, and candidate set $\hat{Y}(\bx)$ can be obtained as:
\begin{equation}
        \Gamma(\hat{Y}(\bx), y) \!=\!
        \begin{cases}
            \log_2 (k + 1) & \text{if } y \in \hat{Y}(\bx) \\
            \log_2 (k + 1) + \log_2 (L-k) & \text{otherwise}
        \end{cases} \!.
        \label{eq:cost}
\end{equation}
The conventional query in AL is a special case of CSQ where $\hat{Y}(\bx) = \mathcal{Y}$, and it is inefficient since the annotator must search through the entire set of size $L$ with a cost of $\log_2 L$.
The following theorem reveals the condition under which the expected cost of CSQ offers an improvement over that of the conventional query.
\begin{theorem}
    \label{theorem_1}
    Assume the information-theoretic cost model~\citep{hu2020one} of selecting one out of $L$ possible options to be $\log_2 L$. 
    Let $L \ge 2$ be the number of classes, $k = |\hat{Y}(\bx)|$, and $\alpha$ be the probability that the candidate set $\hat Y(\bx)$ does not include the ground-truth class of instance $\bx$.
    For the expected cost of conventional query $C_{\textnormal{con}}$ and that of candidate set query $C_{\textnormal{csq}}$,
    if
    \begin{align}
        \frac{\log_2 (k+1)}{\log_2 L} < 1 - \alpha \;,        
        \label{eq:theorem}
    \end{align}
    then $C_{\textnormal{csq}} (L, \bx, \alpha) < C_{\textnormal{con}}(L,\bx)$.
\end{theorem}
\begin{proof}\renewcommand{\qedsymbol}{}
    Recalling the definition of $\alpha$, we have $C_{\textnormal{csq}} (L, \bx, \alpha)= (1 - \alpha) \log_2 (k+1) + \alpha \{ \log_2 (k+1) + \log_2 (L-k) \}$ from~\Eq{cost}.
    As $ L-k < L $, the cost ratio of $C_{\textnormal{csq}} (L, \bx, \alpha)$ to $C_{\textnormal{con}} (L, \bx)$ for instance $\bx$ is induced as:
    \begin{align}
        \nonumber \frac{C_{\textnormal{csq}} (L, \bx, \alpha)}{C_{\textnormal{con}}(L, \bx)} &= \frac{\log_2 (k+1) + \alpha \log_2 (L-k)}{\log_2 L} \\
        & < \frac{\log_2 (k+1)}{\log_2 L} + \alpha \;.
        \label{eq:ratio}
    \end{align}
\end{proof}
Although we adopt the cost model from~\citet{hu2020one}, Theorem~\ref{theorem_1} holds for any cost model that increases monotonically with the number of options.
\begin{remark}
\label{theorem_2}
If we constrain all candidate set sizes $k$ to be fixed, then $1 - \alpha$ corresponds to the top-$k$ accuracy $p_k$ of the model.
Therefore, when $p_k \geq \log_L (k+1)$, CSQ consistently offers an improvement over the conventional query.
For example, in datasets such as CIFAR-10 ($L=10$), CIFAR-100 ($L=100$), and ImageNet ($L=1000$), if the model has a top-1 accuracy (\ie, $k=1$) of at least 30.1\%, 15.1\%, and 10.0\% respectively, then CSQ always provides an improvement.
\end{remark}
The above proof and remark demonstrate that under moderate conditions, CSQ is more efficient than the conventional query.
As described in~\Eq{ratio}, the cost of CSQ decreases as both $\alpha$ and $k$ become smaller.
However, since $k$ and $\alpha$ are inversely related,
balancing the trade-off between $\alpha$ and $k$ is essential to fully leverage CSQ.
Also, fixing candidate set sizes as in Remark~\ref{theorem_2} is suboptimal because it does not consider the uncertainty of individual samples.
In the following section, we introduce our candidate set construction method, which both reflects the uncertainty of each sample and automatically balances the trade-off between $\alpha$ and $k$.

\subsection{Construction of cost-efficient candidate set}
\label{sec:candidate_set_construction}
As shown in~\Eq{cost} and Theorem~\ref{theorem_1}, a candidate set needs to be both small and accurate in covering the ground-truth class.
To do so, we propose using conformal prediction~\citep{romano2020classification} to get a reliable and cost-optimized prediction set using the trained model $\boldsymbol{\theta}$ of the previous round.

\noindent\textbf{Calibration set collection.} \label{sec:calib}
\edited{Conformal prediction requires a labeled set for calibration that has not been used during the model training phase; this set must follow the same distribution as the target data for prediction~\citep{vovk1999machine, angelopoulos-gentle}.}
To achieve this, we randomly select $n_\text{cal}$ samples from the actively selected data $\mathcal{A}_r$ and annotate them within the given budget to form $\mathcal{D}_{\text{cal}} = \{(\bx_i, y_i)\}_{i=1}^{n_\text{cal}}$.
The calibration set $\mathcal{D}_\text{cal}$ is used for conformal prediction and candidate set optimization, which will be explained in the following sections.
\edited{Note that $\mathcal{D}_{\text{cal}}$ also contributes to model training after candidate set construction.}

\noindent\textbf{\hsh{Conformal prediction.}}
Using $\boldsymbol{\theta}$ from the previous round and calibration set $\mathcal{D}_\text{cal}$ randomly sampled from $\mathcal{A}_r$,
we obtain a collection of conformal scores $\mathbf s := \left\{s_i \right\}_{i\in[n_\text{cal}]}$, where $s_i : = 1 - P_{\boldsymbol{\theta}}(y_i \mid \bx_i)$ for $(\bx_i,y_i) \in \mathcal{D}_\text{cal}$.
\hsh{Then, we obtain the $(1-\alpha)$ empirical quantile $\hat{Q}(\alpha)$ of $\mathbf s$, indicating that at least $100 \times (1-\alpha)\%$ of the scores in $\mathbf s$ are smaller than $\hat{Q}(\alpha)$.
This quantile $\hat{Q}(\alpha)$ is given as,}
\begin{equation}
        \hat{Q}(\alpha) := \min_{s \in \mathbf s}\left\{ s: \frac{1}{n_\text{cal}} \sum_{s^\prime \in \mathbf s } \big (\mathds{1}[s^\prime \leq s] \big ) \geq 1-\alpha \right\} \;,
        \label{eq:quantile}
\end{equation}
where $\alpha \in (0,1)$ is an error rate hyperparameter, and $\mathds{1}[\cdot]$ is an indicator function.
Then, we define the candidate set for unlabeled data $\bx$ as follows:
\begin{equation}
        \hat{Y}_{\boldsymbol{\theta}}(\bx, \alpha) := \big\{y: P_{\boldsymbol{\theta}}(y|\bx) \geq 1 - \hat{Q}(\alpha),\; y \in \mathcal{Y}\big\}\;.
        \label{eq:candidate}
\end{equation}
Previous study~\citep{vovk1999machine, angelopoulos-gentle} proved that the candidate set includes the correct label with the probability not less than $1-\alpha$, which is,
\begin{equation}
    \label{eq:conf}
    P\big(y \in \hat{Y}_{\boldsymbol{\theta}}(\bx, \alpha)\big) \geq 1-\alpha \;.
\end{equation}
This ensures the inclusion of the ground-truth classes even under model overconfidence, while adaptively reflecting uncertainties throughout the AL process.
\edited{More detailed procedure of conformal prediction is in Appendix~\ref{sec:clarification}.}

\noindent\textbf{Cost-optimized \hsh{error rate selection}.} \label{sec:optimize}
Although conformal prediction aims at adjusting candidate set $\hat{Y}_{\boldsymbol{\theta}}(\bx, \alpha)$ to fit the condition of $\alpha$ as in~\Eq{conf}, it does not take into account the size $k$ of the candidate set.
The efficiency of CSQ improves as both $\alpha$ and the candidate set size $k$ decrease, as shown in~\Eq{ratio}. \edited{Since $\alpha$ and $k$ are inversely related, finding an optimal hyperparameter $\alpha$ to reduce the labeling cost is not straightforward.}
Hence, we optimize $\alpha$ to minimize labeling cost for the calibration set $\mathcal{D}_{\text{cal}}$
for further improvement of CSQ efficiency. 
To be specific, $\alpha$ is optimized by
\begin{equation}
    \alpha^* := \argmin_{\alpha \in (0,1)}{\sum_{(\bx,y)\in \mathcal D_\text{cal}}{\Gamma(\hat{Y}_{\boldsymbol{\theta}}(\bx, \alpha),y)}}\;,
    \label{eq:optimize}
\end{equation}
where $\Gamma(\bx, y, \hat{Y}_{\boldsymbol{\theta}}(\bx, \alpha))$ is the labeling cost in~\Eq{cost}.
By optimizing $\alpha$ in this way, we utilize conformal prediction to construct candidate sets in a more cost-efficient manner, as the error rate is tailored to minimize the expected labeling cost for each round.
Notably, if we define the corner case $\hat{Y}_{\boldsymbol{\theta}}(\bx, 0) = \mathcal{Y}$, CSQ includes the conventional query at $\alpha = 0$ within the search space for $\alpha^*$.
This makes CSQ is at least as efficient as, and in general more efficient than, the conventional query.

Note that to construct the candidate set query, the calibration set $\mathcal{D}_\text{cal}$ is required to \edited{calculate} $(1-\alpha^*)$ quantile in~\Eq{quantile}.
Thus, when getting annotations of $\mathcal{D}_\text{cal}$ in the calibration set collection step, candidate set query of the current round cannot be applied.
To avoid this circular dependency, the quantile from the previous round is used when labeling $\mathcal{D}_\text{cal}$.

\subsection{Cost-efficient acquisition function}
\label{sec:acquisition}
Since the labeling cost of each sample varies in CSQ, we propose to consider the cost for active sampling.
\edited{We implement an acquisition function that evaluates samples based on the ratio of the estimated information gain to the estimated labeling cost.}
The information gain is quantified using established acquisition scores from prior research like entropy and SAAL~\citep{kim2023saal}, though our approach can integrate any acquisition scoring function.
Given a conventional acquisition score $g_\text{score}(\bx)$, the proposed cost-efficient acquisition function $g_\text{cost}$ is given as,
\begin{equation}
        g_\text{cost}(\bx) := \frac{\left(1 + g_\text{score}(\bx)\right)^d}
        {\log_2 (k + 1) + \alpha^* \log_2 (L-k)} \;,
        \label{eq:acquisition}
\end{equation}
where $d$ is a hyperparameter adjusting the influence of $g_\text{score}(\bx)$ and $\alpha^*$ is the optimized error rate hyperparameter obtained by~\Eq{optimize}.
\edited{The denominator is an expected cost derived from our cost model (\Eq{cost}), considering two cases: the correct label is included or excluded from the candidate set, which is $(1 - \alpha^*) \log_2 (k+1) + \alpha^* \left\{ \log_2 (k+1) + \log_2 (L-k)\right\}$.
This expected cost assumes the candidate set to include the ground-truth class with a probability of $1-\alpha^*$, which is supported by the coverage guarantee in~\Eq{conf}.}


            



