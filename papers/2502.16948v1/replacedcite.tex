\section{Related Work for Inner Minimization}
\label{subsec:related_work_min}

To learn a model for $\pi \ne \pi^{\train}$ from given training data, importance weighting is widely used ____. For a loss function $l(y,f(x))$ where $f(x)$ is a model output, a different per-class weight is assigned to each class to compensate for the disparity between $\pi$ and $\pi^{\train}$. Specifically, to minimize the loss at $\pi \ne \pi^{\train}$, one can assign weights $\pi_{y} / \pi^{\train}_{y}$ for each class and then train a network on the training data. Then,
\begin{align}
&\E_{(x,y)\sim \pi^{\train}_{y} p(x|y)} \left[ \frac{\pi_{y}}{\pi^{\train}_{y}} l(y,f(x))  \right] \\
&= \E_{(x,y) \sim \pi_{y} p(x|y)} \left[ l(y,f(x))  \right].
\end{align}
Thus, minimizing a new loss $\frac{\pi_{y}}{\pi^{\train}_{y}} l(y,f(x))$ on training data is the same as minimizing $l(y,f(x))$ on a new dataset with prior $\pi$.

However, this weight adjustment method turns out to damage the quality of extracted features and have a limited effect on moving decision boundaries for overparametrized networks____. To address them, we propose a targeted logit adjustment (TLA) loss function. The model trained with our proposed loss successfully increases the accuracy of the worst classes by finding good decision boundaries under a given adversarial prior distribution without damaging feature quality. 
 Moreover, based on our newly derived prior-dependent generalization bound, we theoretically prove that the model trained with our TLA loss has a better generalization ability than the model with the previous weighting methods under the adversarial prior distribution. Detailed theoretical analysis and experimental validations are given in Sections~\ref{subsec:TLA method} and~\ref{sec:minmax_result}.