\documentclass[11pt,letterpaper]{berkeley}

\usepackage[all]{hypcap}

\usepackage[authoryear, round]{natbib}
% \bibliographystyle{plainnat}

\usepackage{hyperref}[citecolor=lightblue]

\hypersetup{
    colorlinks = true,
    citecolor = {blue},
}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{float}
\usepackage{bxcoloremoji}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{nicefrac}
\usepackage{dsfont}
\usepackage{enumitem}
%\usepackage{minted}
\usepackage{float}

\setlength\parindent{0pt}
%\setminted[python]{frame=lines, breaklines, framesep=2mm, fontsize=\footnotesize, numbersep=5pt}


\usepackage{xspace}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{relsize}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}

\usepackage{algpseudocode}
\usepackage{setspace}

\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\newcommand{\sref}[1]{\S\ref{#1}}
\usepackage{tabularx}    % For flexible column widths
\usepackage{xcolor}      % For row colors like gray
\usepackage{array}       % For better column formatting
\usepackage{colortbl}    % For coloring table cells or rows
\usepackage{multirow}



\newcommand\pythonstyle{\lstset{
basicstyle=\ttfamily\footnotesize,
language=Python,
morekeywords={self, clip, exp, mse_loss, uniform_sample, concatenate, logsumexp},              %
keywordstyle=\color{deepblue},
emph={MyClass,__init__},          %
emphstyle=\color{deepred},    %
stringstyle=\color{deepgreen},
frame=single,                         %
showstringspaces=false
}}

\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
\newcommand{\github}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{figures/github-logo.pdf}}}


\input{macro}

\makeatletter
\def\mathcolor#1#{\@mathcolor{#1}}
\def\@mathcolor#1#2#3{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}
\makeatother

\usepackage[textsize=tiny]{todonotes}
\usepackage{algorithm}
\usepackage{amssymb}

\usepackage{listings}
\usepackage{adjustbox}
\usepackage[skins,theorems]{tcolorbox}

\tcbset{
  aibox/.style={
    width=474.18663pt,
    top=7pt,
    bottom=5pt,
    colback=blue!6!white,
    colframe=black,
    colbacktitle=black,
    enhanced,
    center,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt,colframe=white,},
  }
}
\newtcolorbox{AIbox}[2][]{aibox,title=#2,#1}

\Crefformat{equation}{#2Eq.\;(#1)#3}

\Crefformat{figure}{#2Figure #1#3}
\Crefformat{assumption}{#2Assumption #1#3}
\Crefname{assumption}{Assumption}{Assumptions}

\usepackage{crossreftools}
\pdfstringdefDisableCommands{%
    \let\Cref\crtCref
    \let\cref\crtcref
}
\newcommand{\creftitle}[1]{\crtcref{#1}}


\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage{dsfont}
\usepackage{nicefrac}
\usepackage{inconsolata}
\usepackage{tcolorbox}
\tcbuselibrary{breakable}

\newcommand{\rr}[1]{\textbf{\color{green}[RR: #1]}}

\title{Knowing When to Stop: Dynamic Context Cutoff for Large Language Models}


\reportnumber{} %

\author[1]{Roy Xie}
\author[1]{Junlin Wang}
\author[1]{Paul Rosu}
\author[2]{Chunyuan Deng}
\author[3]{Bolun Sun}
\author[4]{Zihao Lin}
\author[1]{Bhuwan Dhingra}


\affil[1]{Duke University} 
\affil[2]{Rice University}
\affil[3]{Johns Hopkins University}
\affil[4]{University of California, Davis}

\correspondingauthor{Roy Xie \href{mailto:ruoyu.xie@duke.edu}{ruoyu.xie@duke.edu}; Bhuwan Dhingra \href{mailto:bd149@cs.duke.edu}{bd149@cs.duke.edu}}

\hyphenation{pre-print}

\newcommand{\se}[1]{\textcolor{red}{[SE: #1]}}

\begin{abstract}
\vspace{-0.3cm}
Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient in cases where the
information required to answer a query is localized within the context.
We present dynamic context cutoff, a human-inspired method enabling LLMs to self-terminate processing upon acquiring sufficient task-relevant information. Through analysis of model internals, we discover that specific attention heads inherently encode ``sufficiency signals'' – detectable through lightweight classifiers – that predict when critical information has been processed. This reveals a new efficiency paradigm: models' internal understanding naturally dictates processing needs rather than external compression heuristics. Comprehensive experiments across six QA datasets (up to 40K tokens) with three model families (LLaMA/Qwen/Mistral, 1B-70B) demonstrate $1.33\times$ average token reduction while improving accuracy by $1.3$\%. Furthermore, our method demonstrates better performance with the same rate of token reduction compared to other context efficiency methods.
Additionally, we observe an emergent scaling phenomenon: while smaller models require require probing for sufficiency detection, larger models exhibit intrinsic self-assessment capabilities through prompting. 
\vspace{4mm}

\coloremojicode{1F310} \textbf{Website}: \href{https://royxie.com/when-to-stop-project/}{\textcolor{orange}{\textbf{royxie.com/when-to-stop-project}}}

\github{} \textbf{Code Repository}: \href{https://github.com/ruoyuxie/when-to-stop}{\textcolor{orange}{\textbf{github.com/ruoyuxie/when-to-stop}}}

\end{abstract}

\begin{document}

\maketitle


\input{sections/intro}
\input{sections/related_work}
\input{sections/methods}
\input{sections/experiments}
\input{sections/discussion}
\newpage
\bibliographystyle{iclr2025_conference}  % or whatever style you're using
\bibliography{iclr2025_conference}       % without the .bib extension

\appendix
\onecolumn
\input{appendix}

\end{document}
