
\begin{table*}[t]
%\label{tab:results_long}
\begin{center}
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\textwidth}{l *{15}{>{\centering\arraybackslash}X}@{}}
\toprule
{\textbf{Method}} & \multicolumn{3}{c}{\textbf{LLaMA3.2-1B}} & \multicolumn{3}{c}{\textbf{Ministral-8B}} & \multicolumn{3}{c}{\textbf{Qwen2.5-14B}} & \multicolumn{3}{c}{\textbf{LLaMA3.3-70B}} & \multicolumn{3}{c}{\textbf{Avg.}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}\cmidrule(lr){14-16}
\rowcolor{gray!15} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Avg} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Avg} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Avg} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Avg} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Total} \\
\midrule
Full Context 
& 5.0 & 10.4 & 7.7
& 18.3 & \textbf{38.8} & 28.5
& 29.9 & 40.0 & 35.0
& 29.3 & 70.8 & 50.0
& 20.6 & \textbf{40.0} & 30.3 \\
BM25 
& \textbf{5.7} & 12.2 & 8.9
& \textbf{20.9} & 38.7 & \textbf{29.8}
& 30.2 & 39.2 & 34.7
& 28.7 & 68.7 & 48.7
& \textbf{21.3} & 40.0 & 30.5\\
SBERT
& 5.6 & \textbf{12.5} & \textbf{9.1}
& 20.2 & 37.7 & 29.4
& 30.0 & 38.9 & 34.4
& 27.7 & 68.8 & 48.3
& 21.1 & 39.5 & 30.3 \\
LLMlingua 
& 3.8 & 12.1 & 7.9
& 17.1 & 35.8 & 26.5
& 27.1 & 41.8 & 34.5
& 23.3 & 65.4 & 44.4
& 17.8 & 38.8 & 28.3 \\
LongLLMlingua
& 3.3 & 12.1 & 7.7
& 15.0 & 37.1 & 26.0
& 28.0 & 40.1 & 34.1
& 27.5 & 67.9 & 47.7
& 18.5 & 39.3 & 28.9 \\
LLMlingua2
& 2.7 & 9.6 & 6.2
& 17.1 & 38.3 & 27.7
& 28.8 & 42.9 & 35.8
& 28.2 & 69.2 & 48.7
& 19.2 & 40.0 & 29.6 \\
FT 
& 2.6 & 8.4 & 5.5
& 14.9 & 31.5 & 23.3
& 21.4 & 33.2 & 27.3
& 21.4 & 47.1 & 34.3
& 15.1 & 30.0 & 22.6  \\
Self-Prompt 
& 4.2 & 7.3 & 5.7
& 17.4 & 32.6 & 25.0
& \textbf{30.5} & \textbf{45.8} & \textbf{37.6}
& 29.0 & 65.4 & 47.2
&20.3& 37.8 & 29.0  \\
\midrule
\rowcolor{gray!15}\textit{Ours}
& 5.0 & 9.9 & 7.5
& 19.1 & 37.7 & 28.4
& 29.8 & 43.2 & 36.5
& \textbf{30.8} & \textbf{70.9} & \textbf{50.9}
& 21.2 & 39.9 & \textbf{30.8} \\
\bottomrule
\end{tabularx}
\vspace{-1mm}
\caption{Performance comparison across different models on Single-hop and Multi-hop tasks on the \textit{long form} dataset. Our method achieves a token reduction of $1.27\times$, while outperforming static methods with a targeted compression rate at $1.25\times$}
\vspace{-2.5em}
\label{tab:results_long}
\end{center}
\end{table*}

