
\vspace{-2mm}
\begin{table*}[ht!]
\begin{center}
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\textwidth}{l *{15}{>{\centering\arraybackslash}X}@{}}
\toprule
{\textbf{Method}} & \multicolumn{3}{c}{\textbf{LLaMA3.2-1B}} & \multicolumn{3}{c}{\textbf{Ministral-8B}} & \multicolumn{3}{c}{\textbf{Qwen2.5-14B}} & \multicolumn{3}{c}{\textbf{LLaMA3.3-70B}} & \multicolumn{3}{c}{\textbf{Avg.}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}\cmidrule(lr){14-16}
\rowcolor{gray!15} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Avg} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Avg} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Avg} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Avg} 
 & \textbf{Multi} & \textbf{Single} & \textbf{Total} \\
\midrule
Full Context 
& 10.4 & 17.9 & 14.2
& \textbf{29.6} & 44.8 & 37.2
& 30.4 & 57.6 & 44.0 
& 37.1 & 75.0 & 56.1
& 26.6 & 48.7 & 37.9 \\
BM25 
& \textbf{11.2} & 16.2 & 13.7
& 20.8 & 27.5 & 35.6
& 25.8 & 40.8 & 36.5
& 21.7 & 37.1 & 41.7
& 19.9 & 30.4 & 31.9 \\
SBERT
& 10.2 & 17.8 & 14.0
& 19.6 & 37.5 & 35.2
& 26.3 & 51.3 & 42.3
& 22.1 & 41.7 & 40.8
& 19.6 & 37.1 & 33.1 \\
LLMlingua 
& 6.3 & 18.3 & 12.3
& 22.1 & 41.7 & 31.9
& 24.2 & 52.5 & 38.3
& 35.8 & 74.1 & 55.0
& 22.1 & 46.7 & 34.4 \\
LongLLMlingua
& 6.7 & 20.0 & 13.3
& 22.1 & 41.7 & 31.9
& 26.3 & 55.8 & 41.1
& 35.4 & 71.7 & 53.6
& 22.6 & 47.3 & 35.0 \\
LLMlingua2
& 7.9 & \textbf{20.8} & \textbf{14.4}
& 28.3 & 43.3 & 35.8
& 32.1 & 57.9 & 45.0
& 35.8 & 75.0 & 55.4
& 26.1 & 49.3 & 37.7 \\
FT 
& 6.2 & 13.8 & 10.0
& 21.5 & 34.7 & 28.1
& 22.3 & 35.1 & 28.7
& 35.6 & 52.4 & 44.0
& 21.4 & 34.0 & 27.7\\
Self-Prompt
& 6.4 & 11.4 & 8.9 
& 23.8 & 36.2 & 30.0 
& \textbf{38.2} & 52.0 & 45.1 
& \textbf{48.3} & 69.9 & 59.1 
& 28.9 & 42.6 & 35.8\\

\midrule
\rowcolor{gray!15}\textit{Ours}
& 10.3 & 17.5 & 13.9
& 28.8 & \textbf{45.8} & \textbf{37.3}
& 33.3 & \textbf{59.2} & \textbf{46.3}
& 43.8 & \textbf{75.3} & \textbf{59.5}
& \textbf{29.0} & \textbf{49.4} & \textbf{39.2} \\
\bottomrule
\end{tabularx}
\vspace{-2mm}

\caption{Performance comparison across different models on Single-hop and Multi-hop tasks on the \textit{short-form} dataset. Our method achieves a token reduction of $1.33\times$, while outperforming static methods with a targeted compression rate at $1.25\times$.
}
\vspace{-1.5em}
\label{tab:tasks_result}
\end{center}
\end{table*}

