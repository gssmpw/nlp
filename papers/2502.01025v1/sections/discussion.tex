\section{Analysis}
In this section, we conduct a series of investigations to better understand our method. We focus on the short-form dataset and mainly use LLaMA3.2-1B model for our analysis. 


\noindent\begin{minipage}[t]{0.48\textwidth}
\centering
\vspace{-2mm}
\includegraphics[width=\linewidth]{figures/p90_metrics.png}
\vspace{-2mm}
\captionof{figure}{F1 score and Recall at 90\% precision for sufficiency detection. Our approach reliably identifies when enough context is present while minimizing false positives. More results can be find in Appendix \ref{app:recall_precision}.
}
\vspace{-5em}
\label{fig:f1_p90}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering

\centering
\vspace{-2mm}
\includegraphics[width=\linewidth]{figures/confidence_distribution.png}
\vspace{-2mm}
\captionof{figure}{Confidence progression across context chunks. Models prediction confidence increases monotonically with more context.
}
% \vspace{mm}
\label{fig:confidence_distribution}
\end{minipage}


\subsection{Classification Threshold Analysis}
\label{sec:threshold}
The balance between the model’s prediction confidence and the classification threshold $\tau$ is a key factor in our proposed method. In Figure~\ref{fig:confidence_distribution}, we plot the model’s prediction confidence averaged over different numbers of chunks. We observe that the confidence in sufficiency predictions grows steadily as more context is processed, which indicates that useful signals are accumulating over the chunks. Consequently, once the model’s confidence exceeds $\tau$, it has likely integrated enough information. Note that stopping too early can cause information loss when critical elements of the context are excluded. 
Although F1 score is a useful measure for detecting context sufficiency, we also report Recall at high Precision to show how well our method identifies truly sufficient contexts while minimizing false positives. In Figure~\ref{fig:f1_p90}, we show results at 90\% precision and provide further findings at 95\% and 98\% precision in the appendix. This metric measures the fraction of actually sufficient contexts that are correctly identified when precision is at least 90\%. Such a metric is critical for our task as a mistaken early cutoff (false positive) can exclude relevant content and degrade the final performance. 


\noindent\begin{minipage}[t]{0.48\textwidth}
\vspace{-3em}

\subsection{Chunking and Inference Time Analysis}
\label{sec:chunking}
Chunking determines how efficiently the model processes and evaluates context sufficiency. Table~\ref{tab:chunking_strategies} compares different chunking strategies for Qwen2.5-14B. Percentage-based chunking performs consistently well, with 10\% chunking offering the best trade-off between accuracy and efficiency. While sentence-level chunking achieves the highest classification performance, it is impractical due to the increased overhead of frequent sufficiency checks.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
    % \centering
    
    % \small
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Metric} & \textbf{Sent.} & \textbf{1\%} &  \textbf{5\%} & \textbf{10\%} & \textbf{20\%} \\
        \midrule
F1-Score & 96.8 & 87.2 & 87.0 & 88.3 & 88.3 \\
R@90P & 95.4 & 90.9 & 78.4 & 85.9 & 85.8 \\
Acc. & 14.5 & 13.7 & 12.8 & 13.9 & 13.7 \\
    \bottomrule
    \end{tabular}
\captionof{table}{Sentence-level chunking achieves the highest performance but is computationally expensive. 10\% chunking offers the best balance between accuracy and efficiency.}
\label{tab:chunking_strategies}

    % \vspace{-3em}
\end{minipage}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/inference_time_comparison_subplots.png}
    \caption{For short contexts (1K tokens), full-context processing is faster. However, beyond 2K tokens, our method becomes more efficient, achieving faster inference when fewer than six chunks (60\% of the full context) are processed.
    }
\label{fig:inference_time_comparison_subplots}
    % \vspace{-1em}
\end{figure*}

Since these checks require processing chunks sequentially, smaller chunks lead to higher latency, as each additional step incurs computational overhead before reaching a decision even with caching. 10\% chunking is chosen to best balance the granularity and efficiency.
~\Cref{fig:inference_time_comparison_subplots} shows inference time between our method (10\% chunking) and full-context processing. For short contexts (1K tokens), directly processing the full-context is faster; however, beyond 2K tokens, our method provides great inference time savings-when fewer than six chunks (60\% of the full context) are processed. This demonstrates that our approach scales efficiently, offering increasing benefits for longer inputs.



\subsection{Universal Cutoff vs. Model-Specific Cutoff} \label{sec:future_work}
From a human perspective, each task has a ``gold'' location in the context where the final relevant information resides—once an answer is directly yielded, any further context is redundant. In such cases, a universal stopping point may be plausible. However, from a model perspective, defining a single optimal cutoff is challenging and ambiguous. For example, in in-context learning (ICL), models observe demonstration examples without a clear threshold for sufficiency. Smaller models may require more examples to generalize, while larger models may reach high confidence with fewer. This suggests a model-specific cutoff, where each model determines its own stopping threshold rather than adhering to a universal standard. This is particularly relevant in real-world applications, where different LLMs and tasks have varying context requirements. Future work could explore adaptive algorithms that learn individualized thresholds based on task structure and internal model signals. Appendix~\ref{app:model_cutoff} provides further discussion on this issue, espcially in ICL.



\section{Conclusion}
We introduce dynamic context cutoff, a method that enables LLMs to process only the minimal necessary context by detecting context sufficiency signals using model's internal representation. This approach reduces token processing by 1.33$\times$ on average while improving accuracy by 1.3\%, outperforming static methods like RAG and compression-based heuristics. We find that larger models develop emergent self-assessment capabilities, allowing them to detect sufficiency through self-prompting. By enabling models to terminate processing dynamically, our method enhances efficiency and scalability for LLM inference, paving the way for more adaptive and intelligent context processing for LLMs.


% \section*{Impact Statement}
% Our work enhance the efficiency of Large Language Models (LLMs) by enabling them to dynamically determine when sufficient information has been processed. By leveraging inherent ``sufficiency signals'' within model attention heads, our approach reduces computational costs while maintaining or even improving accuracy. This contributes to the broader goal of making LLMs more scalable and practical for real-world deployment. Beyond technical efficiency, our work has societal implications, particularly in reducing the environmental impact of large-scale inference.
