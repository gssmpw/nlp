\section{Implementation Details}
\label{app:details}
In implementation, models were served using either vLLM or Hugging Face Transformers, depending on compatibility and efficiency considerations. The GPU configurations for each model are summarized in Table~\ref{tab:gpu_config}. We used our anonymous institution’s compute cluster, which was graciously provided for our experiments.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{l c}
        \toprule
        \textbf{Model} & \textbf{GPUs Used} \\
        \midrule
        LLaMA 3.2-1B   & 2 × Nvidia A5000 \\
        Mistral 8B     & 4 × Nvidia A5000 \\
        Qwen 2.5-14B   & 4 × Nvidia A5000 \\
        LLaMA 3.3-70B  & 4 × Nvidia A6000 \\
        \bottomrule
    \end{tabular}
    \caption{GPU configurations used for different models in our experiments.}
    \label{tab:gpu_config}
\end{table}

\subsection{Dynamic Context Cutoff}
\label{app:details:dynamic_context_cutoff}
\paragraph{Ensemble Classifier}
\label{app:details:classifier} 
For the ensemble classifier, the folds are constructed from the training split during cross-validation. The validation split is held out for the evaluation after the classifier is built. Tables \ref{tab:heads_cla_number_compare} shows the performance comparsion in different number of attention heads and different classifiers used in ensmble. for attention heads, we found that using only the top 5 selected heads yeild best performance, and use the top 4 out of 7 classifiers is the best configuration. 

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcccccc}
        \toprule
        & \multicolumn{3}{c}{\textbf{Head Numbers}} & \multicolumn{3}{c}{\textbf{Classifier Numbers}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        \textbf{Metrics} & \textbf{5} & \textbf{10} & \textbf{20} & \textbf{2} & \textbf{4} & \textbf{6} \\
        \midrule
        F1-Score & 88.3 & 87.3 & 87.9 & 87.4 & 88.3 & 87.3 \\
        R@90P & 85.9 & 78.0 & 78.0 & 77.6 & 85.9 & 78.0 \\
        Acc. & 13.9  & 13.0& 12.8 & 12.7 & 13.9 & 12.9 \\
        \bottomrule
    \end{tabular}
    \caption{Performance comparison across head selections and number of classifiers for our method.}
    \label{tab:heads_cla_number_compare}
\end{table}




\subsection{Fine-Tuned Classifier (FT)}
\label{app:details:small_llm}
  We fine-tune meta-llama/Llama-3.2-1B to predict the context cutoff point in long-context inputs, formulating this as a binary classification task. The model is trained on the Short-form dataset specified in \Cref{app:data}. We optimize using the AdamW optimizer with a learning rate of 8.0e-05 and a batch size of 32, employing a cosine learning rate schedule with linear warmup. The fine-tuned model achieves a development set accuracy of 0.8346, demonstrating strong predictive capability. We chose meta-llama/Llama-3.2-1B due to its efficiency in capturing long-range dependencies while maintaining manageable computational costs. Additionally, framing the task as binary classification simplifies optimization and enables robust generalization across diverse long-context scenarios. We include meta-llama/Llama-3.2-3B results and the performances of them training on Long dataset in  for reference. All models are fine-tuned for one epoch.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Base Model} & \textbf{Trained \& Evaluated on} & \textbf{Test Accuracy} \\
\midrule
Llama3.2-1b  & Short Dataset  & 0.8346 \\
Llama3.2-1b     & Long Dataset  & 0.7515 \\
Llama3.2-3b  & Short Dataset   & 0.8413 \\
Llama3.2-3b     & Long Dataset & 0.7456 \\
\bottomrule
\end{tabular}
\caption{Performance of fine-tuned classifiers tuned on different datasets.}
\label{tab:ft_classifier}
\end{table}


% \subsection{Llmlingua}
% \label{app:details:llmlingua}
% % compression rate, side LLM, differnt llmlingua versions.
% \subsection{RAG}
% \label{app:details:rag}
% top-k sampling etc.
 