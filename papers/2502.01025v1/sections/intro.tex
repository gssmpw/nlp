\vspace{-1em}
\section{Introduction}  
Large language models (LLMs) demonstrate remarkable capabilities across diverse tasks, yet their indiscriminate processing of entire input contexts creates inefficiencies. Standard Transformer-based architectures process every token with equal computational priority, regardless of its actual relevance to the task \citep{vaswani2017attention}. This brute-force approach creates fundamental inefficiencies: models waste computation on irrelevant context while simultaneously struggling with the ``lost-in-the-middle'' phenomenon where critical information becomes diluted in lengthy inputs \citep{lostinmiddle, RULER}. For instance, when answering a simple factual question, models process an entire document even after encountering the relevant information.


\noindent\begin{minipage}[t]{0.48\textwidth}
The human cognitive system offers an instructive contrast. When solving problems, people dynamically assess information sufficiency – we stop processing once we gather enough evidence, ignoring redundant details \citep{fiske1991social}. On the other hand, LLMs process entire contexts even after acquiring sufficient information. This raises a question: ``Can we equip LLMs with the ability to self-assess context sufficiency and terminate early without compromising accuracy?''
\\
In this work ,we present dynamic context cutoff, which enables LLMs to identify when they have acquired sufficient information for a task. 
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\vspace{-5mm}
\includegraphics[width=\linewidth]{figures/f1.pdf}
\vspace{-6mm}
\captionof{figure}{Our method enables language models to perform early termination by capturing sufficiency signals in key attention heads, reducing the amount of processed content while preserving performance.}
\label{fig:f1}
\end{minipage}


Our key insight emerges from the analysis of the model internals: specific attention heads in transformer layers exhibit strong sensitivity to information sufficiency (\sref{sec:probing_sufficiency}). By monitoring these ``indicator heads'' with lightweight classifiers, we enable models to make early stopping decisions while maintaining performance. 


Our approach challenges existing methods for context efficiency. Previous work show that retrieval-augmented generation (RAG) struggles with multi-hop reasoning and cross-context dependencies \citep{ragvslc}. Meanwhile, compression methods risk information loss through static processing. For instance, the LLMLingua family \citep{llmlingua,longllmlingua,llmlingua2} uses a small language model to filter out unimportant tokens, reducing context based on a fixed compression target. These methods impose predefined compression rates, enforcing a \textit{one-size-fits-all} reduction regardless of content complexity. Similarly, RAG methods predefine a fixed number of top-$k$ retrieved documents. We refer to both RAG and compression-based approaches as \textit{static} methods, in contrast to \textit{dynamic} methods, which do not require predefined compression targets. Our dynamic method preserves the model's native reasoning capabilities – models process the minimal context needed, expanding it only when necessary. This creates a new paradigm where \textit{efficiency emerges naturally from the model's own understanding} rather than external compression heuristics, as demonstrated in Figure \ref{fig:f1}.

 
We validate our approach through comprehensive experiments across six QA datasets (context lengths 0.5K-40K tokens) and three model families (LLaMA, Qwen, Mistral; 1B-70B parameters). We found that LLMs inherently encode context sufficiency signals in specific attention heads (90\% F1 detection). Notably, our method reveals behaviors that aligns well with the scaling behavior of modern LLMs: while smaller models (1B-8B parameters) require explicit sufficiency detection to achieve competitive efficiency, larger models (14B+) exhibit emergent self-assessment capabilities through simple prompting. Our method achieves $1.33\times$ average token reduction with 1.3\% accuracy improvement, while outperforming existing both RAG and the state of the art compression methods. 


