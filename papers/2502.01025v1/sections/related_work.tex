\section*{Related Work}
\label{sec:related_work}
\paragraph{Context Efficiency in LLMs.} Transformer-based LLMs initially faced fundamental limitations from fixed context windows \citep{vaswani2017attention}, but recent architectural advances enable processing hundreds of thousands of tokens \citep{jin2024llm,reid2024gemini,llama3.1modelcard,deepseekr1}. However, longer contexts exacerbate computational inefficiencies and performance degradation, where critical information becomes obscured by irrelevant detailsâ€”a phenomenon termed the ``lost-in-the-middle'' problem \citep{lostinmiddle,RULER}. 
Many context compression methods have been proposed to address this problem \citep{llmlingua,longllmlingua,llmlingua2,icformer,li2024500xcompressor} by predefining a compression rate target and leverage external LLM to filter tokens  or reordering documents. However, these approaches risk discarding task-critical information by prioritizing size reduction over contextual relevance. In contrast, our method dynamically determines sufficiency during processing through the model's internal signals, eliminating rigid compression targets.

\paragraph{Latent Knowledge in Model Activations.} LLMs encode task-relevant knowledge within intermediate activations, often more accurately than their final surface outputs \citep{saunders2022self,iti}. This latent knowledge enables high-quality knowledge graph construction without supervision \citep{wang2020language}, demonstrating its potential for informed decision-making. We extend this insight by probing activation subspaces to detect context sufficiency dynamically. Our approach uniquely identifies when the model has internally synthesized adequate information, mirroring human-like selective processing while reducing computational overhead. This differs fundamentally from static compression methods by leveraging the model's inherent information integration capabilities rather than predefined external heuristics.
