\section{Experiments}
\label{sec:experiments}
In this section we conduct comprehensive experiments to demonstrate the effectiveness of our method. 
\subsection{Experimental Setup}
\paragraph{Models}
We evaluate 4 open-source LLMs from different model families, ranging from 1B to 70B parameters: LLaMA3.2-1B \citep{dubey2024llama}, Ministral-8B \citep{mistral}, Qwen2.5-14B \citep{qwen2}, and LLaMA3.3-70B \citep{dubey2024llama}. Note that our proposed method is model-agnostic and can be applied to any Transformer-based LLM.


\paragraph{Datasets}
\label{sec:data}
In this work, we focus on two categories of datasets based on reasoning complexity: single-hop and multi-hop. These datasets are chosen to assess the ability of models to locate the key information across varying context structures and task.

\begin{itemize}[leftmargin=*,noitemsep,nolistsep]
    \setlength{\itemsep}{2pt}
    \setlength{\parskip}{2pt}
    \setlength{\parsep}{2pt}
        \item \textbf{Single-hop Reasoning:} The answers are typically found within a single passage, requiring minimal context dependency.
\begin{itemize}[leftmargin=*,noitemsep,nolistsep]
\setlength{\itemsep}{2pt}
\setlength{\parskip}{2pt}
\setlength{\parsep}{2pt}

        \item \textbf{SQuAD} \citep{squad}: A widely used dataset with questions based on Wikipedia passages.
        \item \textbf{Natural Questions} \citep{nq}: Contains questions derived from real-world search queries, with answers located in a single but longer passage.
        \item \textbf{Code Understanding}: We use GPT-4o to synthetically generate multiple single-function code snippets as distracor, and use the orignal PCSD \citep{pcsd} data to create a QA task dataset, requiring to first locate and then understand the relevant code. 
    \end{itemize}
    
    \item \textbf{Multi-hop Reasoning:} It requires the model to combine information from multiple parts of the context to arrive at the correct answer. Missing a single piece of information could lead to incorrect answers.
\begin{itemize}[leftmargin=*,noitemsep,nolistsep]
\setlength{\itemsep}{2pt}
\setlength{\parskip}{2pt}
\setlength{\parsep}{2pt}
        \item \textbf{HotpotQA} \citep{yang-etal-2018-hotpotqa}: A popular dataset with multi-hop questions that require reasoning across multiple paragraphs from Wikipedia.
        \item \textbf{MUSIQUE} \citep{musique}: A dataset with compositional and nested questions that require multi-step reasoning across multiple sources.
        \item \textbf{Multi-hop Key-Value Retrieval}: A synthetic dataset designed to simulate multi-hop reasoning by requiring the exact retrieval of dependent ``key-value'' pairs. The key-value pairs are randomly generated.
    \end{itemize}
\end{itemize}

\paragraph{}
While most of these datasets are naturally short, we extend them to approximately 40K tokens to evaluate LLMs' long-context capabilities. Following approaches from \citet{lostinmiddle} and \citet{infinitybench}, we combine multiple unique documents within each dataset to create a long-form version from the original short-form datasets. We conduct experiments on both short-form and long-form (\sref{sec:context_length}). Additionally, we analyze the trade-off between inference time and context length in our method in \sref{sec:chunking}. For more details about the dataset, refer to~\Cref{app:data}. The ground-truth sufficiency cutoff for each input in the dataset is defined as the position of the \textit{last} token in the gold information span (i.e., the minimal required context for answering correctly), normalized by the total number of tokens in the input. However, this definition assumes that sufficiency is a property of the dataset rather than the model, meaning a single cutoff point applies universally across models. In reality, different models may require varying amounts of context to generate correct answers, suggesting that sufficiency could be inherently model-dependent. Investigating this distinction further is an important direction for future work and is described in more detail in \sref{sec:future_work}. 


\paragraph{Baselines}
We comprehensively evaluate our method against several existing approaches for context processing:

\begin{itemize}[leftmargin=*,noitemsep,nolistsep]
\setlength{\itemsep}{2pt}
\setlength{\parskip}{2pt}
\setlength{\parsep}{2pt}

\item \textbf{BM25} (RAG) \citep{INR-019}: A traditional retrieval algorithm ranks context chunks based on their term-frequency-based relevance to the query. The LLM processes only the top-ranked chunks.

\item \textbf{SBERT} (RAG) \citep{sbert}: Uses a transformer-based sentence encoder to compute dense vector embeddings of both queries and context chunks. Chunks are ranked by cosine similarity, improving semantic relevance over BM25.

\item \textbf{LLMLingua} (Compression) \citep{llmlingua}: 
A token-level compression approach that removes low-entropy tokens from the context based on perplexity scores from an external small language model. This reduces input length but may discard relevant information.


\item \textbf{LongLLMLingua} (Compression) \citep{longllmlingua}: An extension of LLMLingua designed for long-context scenarios. It applies a hierarchical, question-aware filtering strategy to retain critical content in multi-doc settings.

\item \textbf{LLMLingua2} (Compression) \citep{llmlingua2}: Improves over LLMLingua by explicitly learning a task-agnostic prompt compression objective. Instead of relying solely on token perplexity, it distills knowledge from a large LLM into a smaller transformer encoder, trained as a token classifier to retain essential information while compressing input.

\item \textbf{Fine-Tuned Classifier}: A small, fine-tuned model learns to predict when sufficient information has been processed. The LLM halts further input processing once the fine-tuned model classifies the current context as sufficient. More details can be found in Appendix~\ref{app:details:small_llm}.

\item \textbf{Self-Prompting}: The LLM predicts sufficiency dynamically by responding to a self-assessment prompt. See \sref{sec:self_prompt} for more details. 

\end{itemize}


\paragraph{Evaluation Metrics}
We evaluate the proposed dynamic context cutoff methods using metrics for both sufficiency classification and task performance. For sufficiency classification, we use \textbf{F1 Score}, which balances precision and recall, capturing the trade-off between false positives (overestimating sufficiency) and false negatives (underestimating sufficiency). Additionally, we report \textbf{Recall at 90\% Precision (R@90P)}, which measures the percentage of sufficient contexts correctly identified while maintaining a precision of at least 90\%. This ensures that the method reliably avoids excessive false positives while achieving high recall. For task performance, we evaluate \textbf{Accuracy}, which measures the percentage of correct task outputs (e.g., accurate answers in question answering) after context cutoff, as well as \textbf{Token Reduction}, quantifying the proportion of tokens processed relative to the full context. Following previous work \citep{helmet}, we perform model-based evaluation by using GPT-4o Mini.\footnote{gpt-4o-mini-2024-07-18} as judge to evaluate the task performance. The evaluation prompt can be found in \Cref{app:eval_prompt}.

\paragraph{Implementation Details}
The proposed dynamic context cutoff method involves three hyperparameters: the classification threshold \(\tau\), the number of attention heads used for training, and the number of classifiers in the ensemble. Among these, \(\tau\) is the key hyperparameter as it directly impacts the trade-off between efficiency and performance, as discussed in \sref{sec:threshold}. The remaining two hyperparameters are determined empirically via a standard hyperparameter sweep on the validation set, the details can be found in Appendix \Cref{app:details:classifier}.
Specifically, we set $k$ = 5 for attention heads with the highest F1 scores and train 8 lightweight classifiers for each head, selecting the top 4 with the highest AUC scores to form the ensemble. More details can be found in \Cref{app:details:dynamic_context_cutoff}. For all methods, including the proposed dynamic context cutoff method, we evaluate using percentage-based chunking with a 10\% incremental threshold, meaning each chunk contains 10\% more of the full context than the previous one. We explore different chunking strategies and inference time trade-off in \Cref{sec:chunking}. 
We use all of the datasets from both categories for probe training, refer to \Cref{app:data} for more details.


\subsection{Results}  
\label{sec:results}


\begin{figure*}[htpb!]
    \centering
    \includegraphics[width=\textwidth]{figures/accuracy_vs_reduction.png}
    \caption{Our method achieves superior efficiency-accuracy trade-offs compared to baselines. RAG degrades with scale, while Lingua2 remains competitive but lags on multihop tasks. Larger models (14B+) exhibit emergent self-awareness on context sufficiency through prompting.}
    \label{fig:accuracy_vs_reduction}
\end{figure*}



\noindent\begin{minipage}[t]{0.48\textwidth}
\paragraph{Sufficiency Classification}
Table \ref{tab:sufficiency_f1} shows that probing internal attention heads achieves superior sufficiency detection (F1 = 91.1) compared to supervised fine-tuning (79.5) and self-prompting (83.1) in 70B models, demonstrating that latent sufficiency signals are more reliable than surface-level outputs. 
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
    \centering
    % \small
    \vspace{-3em}
    \begin{tabular}{l c c c}
        \toprule
        \textbf{Model} & \textbf{FT} & \textbf{Prompt} & \textbf{Ours} \\
        \midrule
        LLaMA3.2-1B  & \multirow{4}{*}{79.5} & 52.6 & 88.3 \\
        Mistral-8B   &                        & 69.7 & 89.8 \\
        Qwen2.5-14B  &                        & 78.3 & 87.2 \\
        LLaMA3.3-70B &                        & 83.1 & 91.1 \\
        \bottomrule
    \end{tabular}
    \captionof{table}{Probing attention heads achieves superior F1 scores compared to supervised fine-tuning (FT) and self-prompting across all models.}
    \label{tab:sufficiency_f1}
    % \vspace{-3em}
\end{minipage}

We also observe an interesting phenomenon - while 1B models struggle with self-prompting (F1 = 52.6), 70B versions achieve much higher performance, suggesting larger models intrinsically develop self-assessment capabilities. However, our probing approach maintains consistent high performance across all model sizes, which confirms that internal activation provides the most reliable sufficiency detection regardless of model scale. 

\paragraph{Efficiency vs. Performance}
\label{sec:short_results}
Figure \ref{fig:accuracy_vs_reduction} shows the efficiency and performance trade-off between different methods. Unlike static methods (RAG and lingua family) that require predefined compression rates or top-k document selection, dynamic methods (FT, self-prompting, and our approach) adaptively determine cutoff points based on content understanding. For our proposed method, we sweep through 4 different $\tau$ values. For 1B models, our method matches RAG and LLMLingua2 in token reduction and achieves comparable accuracy. At 8B, it processes about $1.5\times$ fewer tokens with minimal accuracy drop, outperforming all baselines. For 14B+ models, the method not only reduces tokens up to $1.22\times$ but also improves accuracy.
In contrast, RAG degrades sharply with scale. FT underperforms universally, likely due to misaligned sufficiency signals across models. Interestingly, larger models (14B+) exhibit emergent self-awareness via prompting, whereas smaller models (1B-8B) perform poorly in prompting, as instruction following ability is critical for self-prompting to work effectively. The results suggest that context truncation may also mitigate the ``lost-in-the-middle'' problem, as models focus more on the end of the context, which are likely to contain key information after removal. 

\input{tables/main_results_short}
\input{tables/long_results}

\paragraph{Individual Task Performance}
\label{sec:tasks_result}
Table~\ref{tab:tasks_result} shows that our method maintains consistent performance on both single-hop tasks (49.4\% average accuracy) and multi-hop tasks (29\%), outperforming the top static baseline, LLMLingua2, by +1.5\% absolute accuracy score. In contrast, RAG methods experience a considerable drop in both settings. For a fair comparison, we report RAG results only for $k=8$, which corresponds to a compression rate of $0.8$ for lingua family or a token reduction factor of $1.25\times$. It is important to note that dynamic methods stop naturally and do not target a specific token reduction rate. Our method achieves a $1.33\times$ reduction in tokens, while the FT and Prompt methods achieve reductions of $1.54\times$ and $1.42\times$ on average, respectively. 


\paragraph{Longer Context Scenario}
\label{sec:context_length}
We evaluate our method with longer context to assess its scalability. For fair comparison, static methods are evaluated at a fixed 1.25$\times$ token reduction.  
Table \ref{tab:results_long} shows that our method consistently outperforms baselines, especially in multi-hop setting. Dynamic methods adaptively determine cutoff points, with FT achieving 1.61$\times$, Self-Prompt 1.41$\times$, and our method 1.27$\times$, ensuring minimal performance loss. Notably, RAG performs better in long-context settings, particularly for multi-hop reasoning. However, FT remains the weakest method, struggling with generalization. Self-Prompting improves with model size, as larger models better follow instructions for self-assessment. The result confirms that dynamic cutoff outperforms static heuristics. For longer context, our method provides an alternative and scalable solution for efficient inference.

