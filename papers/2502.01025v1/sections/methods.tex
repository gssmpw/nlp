\section{Methodology}  
\label{sec:method}

\noindent\begin{minipage}[t]{0.48\textwidth}
We propose dynamic context cutoff, a method that enables LLMs to identify and process only the minimal sufficient context required for a given task. Our approach leverages internal model activations to detect when enough information has been gathered, reducing token processing while maintaining performance. As illustrated in Figure \ref{fig:method_overview}.

\subsection{Problem Formulation}
Let \(\mathcal{M}\) denote a pre-trained language model. Given an input textual context \(\mathbf{C}\), we process \(\mathbf{C}\) sequentially from left to right by partitioning it into an ordered sequence of chunks \(\{\mathfrak{s}_j\}_{j=1}^m\), where each chunk \(\mathfrak{s}_j\) comprises a contiguous subset of \(\mathbf{C}\) (e.g., a sentence or a fixed percentage of the total tokens). These chunks form a non-overlapping covering of \(\mathbf{C}\), meaning:
\[
\mathlarger{\mathlarger{\mathlarger{\mathbin\Vert}}}_{j=1}^{m} \mathfrak{s}_j = \mathbf{C}, \quad \mathfrak{s}_i \cap \mathfrak{s}_j = \emptyset \text{ for } i \neq j
\]
where \(\mathbin\Vert\) denotes concatenation. We define a sequence of cumulative contexts \(\{\mathbf{C}_i\}_{i=1}^m\), where each cumulative context \(\mathbf{C}_i\) consists of all chunks up to and including the \(i\)-th chunk: 
\[
\mathbf{C}_i = \mathfrak{s}_1 \mathbin\Vert \mathfrak{s}_2 \mathbin\Vert \dots \mathbin\Vert \mathfrak{s}_i, \quad 1 \leq i \leq m
\]
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\vspace{-5mm}
\includegraphics[width=\linewidth]{figures/f2.pdf}
\vspace{-6mm}
\captionof{figure}{Our dynamic context cutoff method leverages the model's internal representations to identify when sufficient information has been processed. A lightweight classifier is trained on selected attention heads to detect context sufficiency, leading to token savings while maintaining task performance.
}
\label{fig:method_overview}
\end{minipage}

By construction, these cumulative contexts satisfy the nested proper subset relationship: $\mathbf{C}_1 \subset \mathbf{C}_2 \subset \dots \subset \mathbf{C}_m = \mathbf{C}.$ Given a query \(q\), the goal is to identify the smallest prefix \(\mathbf{C}_k\) (where \(k \leq m\)) such that:
\[
\mathcal{M}(q, \mathbf{C}_k) \approx \mathcal{M}(q, \mathbf{C}).
\]


Let \(\mathcal{M}\) denote a pre-trained language model. Given an input textual context \(\mathbf{C}\), we process \(\mathbf{C}\) sequentially from left to right by partitioning it into an ordered sequence of chunks \(\{\mathfrak{s}_j\}_{j=1}^m\), where each chunk \(\mathfrak{s}_j\) comprises a contiguous subset of \(\mathbf{C}\) (e.g., a sentence or a fixed percentage of the total tokens). These chunks form a non-overlapping covering of \(\mathbf{C}\), meaning:
\[
\mathlarger{\mathlarger{\mathlarger{\mathbin\Vert}}}_{j=1}^{m} \mathfrak{s}_j = \mathbf{C}, \quad \mathfrak{s}_i \cap \mathfrak{s}_j = \emptyset \text{ for } i \neq j
\]
where \(\mathbin\Vert\) denotes concatenation. We define a sequence of cumulative contexts \(\{\mathbf{C}_i\}_{i=1}^m\), where each cumulative context \(\mathbf{C}_i\) consists of all chunks up to and including the \(i\)-th chunk: 
\[
\mathbf{C}_i = \mathfrak{s}_1 \mathbin\Vert \mathfrak{s}_2 \mathbin\Vert \dots \mathbin\Vert \mathfrak{s}_i, \quad 1 \leq i \leq m
\]
By construction, these cumulative contexts satisfy the nested proper subset relationship: $\mathbf{C}_1 \subset \mathbf{C}_2 \subset \dots \subset \mathbf{C}_m = \mathbf{C}.$ Given a query \(q\), the goal is to identify the smallest prefix \(\mathbf{C}_k\) (where \(k \leq m\)) such that:
\[
\mathcal{M}(q, \mathbf{C}_k) \approx \mathcal{M}(q, \mathbf{C}).
\]
Here, \(\mathbf{C}_k\) represents the minimal sufficient context required for the model to answer \(q\) with comparable performance to using the full context \(\mathbf{C}\).

At each step \(i\), a sufficiency classifier \(\mathcal{S}\) iteratively checks whether the current cumulative context \(\mathbf{C}_i\) contains enough information, by comparing its confidence \(\mathcal{S}_c(C_i)\) with a threshold \(\tau\). Formally,
\[
\mathcal{S}(\mathbf{C}_i) = 
\begin{cases} 
1 & \text{if } \mathcal{S}_c(\mathbf{C}_i) \geq \tau  \\
0 & \text{otherwise}
\end{cases},
\]
where \(\mathcal{S}_c: \mathbb{R}^d \rightarrow [0,1]\) is the sufficiency confidence score function, and \(\tau\) is a predefined threshold. If \(\mathcal{S}(\mathbf{C}_i) = 1\), processing terminates, and \(\mathbf{C}_i\) is selected as the minimal sufficient context \(\mathbf{C}_k\). The remaining chunks \(\{\mathfrak{s}_{i+1}, \mathfrak{s}_{i+2}, \ldots, \mathfrak{s}_m\} = \mathbf{C} \setminus \mathbf{C}_k\) are ignored. 

We formulate the problem as early cutoff rather than \textit{searching for an optimal chunk or selecting an arbitrary subset of context}, as is common in RAG methods. This is because that LLMs process text from left to right anyways, so we can reuse the precomputed cache from previous chunks. As opposed to RAG, this allows a contextual understanding of the chunk to determine information sufficiency.


\subsection{Probing for ``Context Sufficiency''}
\label{sec:probing_sufficiency}

We are interested in understanding how context sufficiency is represented within the model and how it can be leveraged to improve efficiency. We probe its intermediate activations. Following prior work on neural network interpretability \citep{iti,deng2024language}, we assess whether certain attention heads encode information predictive of sufficiency. 
The data for probing consists of input cumulative contexts \(\{\mathbf{C}_i\}_{i=1}^n\), each labeled as either sufficient (\(y = 1\)) or insufficient (\(y = 0\)). 

\noindent\begin{minipage}[t]{0.48\textwidth}

For each \(\mathbf{C}_i\), the model produces attention head activations \(\{x_l^h\}\), where \(x_l^h \in \mathbb{R}^D\) is the activation of the \(h\)-th head in the \(l\)-th layer. We train a lightweight binary classifier \(p_\theta(x_l^h)\) on these activations to predict sufficiency:
$p_\theta(x_l^h) = \sigma(\langle \theta, x_l^h \rangle),$ where \(\theta \in \mathbb{R}^D\) are the parameters of the probe, and \(\sigma\) denotes the sigmoid function. The dataset is split into training and validation sets (4:1 ratio) per task. Each classifier's validation F1 score determines the predictive ability of the corresponding head. This selection process is performed only once for all tasks. We discuss more details about the data used probe in \sref{sec:data}.

Figure~\ref{fig:head_acc_sufficiency} shows the F1 scores of probes for all attention heads in LLaMA3.2-1B. A subset of heads, primarily in middle layers, exhibit significantly higher predictive performance. However, depending on different model's architecture, the performance of the probes may vary.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\vspace{-2mm}
\includegraphics[width=\linewidth]{figures/head_heatmap_1b.png}
\vspace{-3mm}
\captionof{figure}{Validation F1 scores for linear probes across all attention heads in LLaMA3.2-1B, sorted row-wise by F1. Darker blue represents higher F1 scores. Some heads show significantly higher performance. More visualizations can be found in \Cref{app:probe}.}
\label{fig:head_acc_sufficiency}
\end{minipage}

These results suggest that the model's internal representations encode latent information about context sufficiency. We leverage this insight to identify the most informative attention heads for further processing.


\subsection{Dynamic Context Cutoff}
\paragraph{Sufficiency Classification}
After identifying the top heads from the probing step, we train multiple lightweight base classifiers \(\{\mathcal{S}_1, \mathcal{S}_2, \ldots, \mathcal{S}_e\}\) on these heads to form an ensemble. The ensemble is constructed using StratifiedKFold with $n=5$ folds, with the best performing models selected based on their mean cross-validation AUC scores to form the final weighted ensemble:

\[
\mathcal{S}_{\text{ensemble}}(\mathbf{C}_i) = \frac{1}{e} \sum_{j=1}^e \mathcal{S}_j(\mathbf{C}_i).
\]
More details in classifier can be found in \Cref{app:details:classifier}. 

\paragraph{Inference with Iterative Forward Passes}
During inference, the full context is processed incrementally as a sequence of nested \(\{\mathbf{C}_i\}_{i=1}^n\), where each \(\{\mathbf{C}_i\}\) contains all preceding tokens. These progressively expanding subsets are passed through the model to extract activations at each step. Next, the ensemble classifier \(\mathcal{S}_{\text{ensemble}}\) predicts whether the current context \(\{\mathbf{C}_i\}_{i=1}^n\) is sufficient.

The activations of all processed chunks are cached to avoid redundant computation. Let \(\mathbf{A}_{\text{cache}}^{i}\) denote cached activations for \(\mathbf{C}_i\), containing the activations of all previous chunks by construction. Activations for the current \(\mathbf{C}_i\) are computed as:
\[\mathbf{A}(\mathbf{C}_i) = f_{\text{model}}(\mathbf{C}_i \setminus \mathbf{C}_{i-1}, \mathbf{A}_{\text{cache}}^{i-1}),\]
where \(f_{\text{model}}\) is the model's forward pass function conditioned on the cached activations \(\mathbf{A}_{\text{cache}}\). 
The iterative process continues until a sufficient context $\mathbf{C}_k$ is identified, as determined by \(\mathcal{S}_{\text{ensemble}}\). If no $\mathbf{C}_i$ is deemed sufficient, the entire input context is processed. In either cases, the cached activations will be reused for generation. The final output is computed as:
\[
\mathcal{M}(\mathbf{C}_k) = \mathcal{M}(\mathbf{C}_k \setminus C_{k-1}, \mathbf{A}_{\text{cache}}^{k-1}).
\]
% The algorithm pseudocode can be found in \Cref{app:algo}.

\paragraph{Alternative Sufficiency Detection}
\label{sec:self_prompt}
As an alternative to classifier-based approach, larger LLMs can leverage their own reasoning capabilities through self-prompting. For each cumulative context $\mathbf{C}_i$, we append a meta-prompt asking the model to evaluate whether it has sufficient information to answer query $q$. The prompt can be found in \Cref{app:prompt}. The model's binary response determines sufficiency, enabling dynamic cutoff without classifiers. In \sref{sec:short_results}, we show that self-prompting becomes increasingly reliable with larger model sizes (14B+ parameters), suggesting sufficiency detection emerges as a capability with scale.



