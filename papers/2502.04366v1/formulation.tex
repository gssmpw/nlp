In this section, we discuss the problem formulation of explaining GNN-based rumour detection models. First, we elucidate the task outline for the rumour detection task using event propagation structures as well as variants of this task. Then we outline our problem formulation for explaining the predictions of the rumour detection model. The notations used in this section and subsequent sections are summarized in Table \ref{tab:notation}.

\subsection{Rumour Detection with Event Propagation Structure}

Let $G = (V, E)$ represent a graph where $V$ is the set of nodes and $E$ is the set of edges. In the context of rumour detection, the graph $G$ corresponds to an event propagation graph, with the node set $V$ representing posts made during the event and the edge set $E$ capturing the interactions between these posts. Each node $v \in V$ is associated with a feature vector $\textbf{x}_v \in \mathbb{R}^{|D|}$, where $D$ denotes the set of features. Typically, in rumour detection, the feature vector represents a text embedding of the post's content. Consequently, the feature matrix $\textbf{X} \in \mathbb{R}^{|V| \times |D|}$ contains the text embeddings of all posts within the event. The objective of the rumour detection task is to learn a function $f$ that, given an event propagation graph, predicts whether the source post $v_0$ is a rumour, i.e., $f(G) = \hat{y}$. Depending on the dataset, this task can either be a binary classification problem, as seen in the Weibo dataset \cite{ma2016detecting}, or a four-way classification problem, as in the Twitter15/16 dataset \cite{ma2016detecting}. In this paper, we focus on GNN-based models as the function $f$. Since the feature vector dimensions correspond to latent features in the text embedding space rather than explicit discrete features, our aim is to enhance the explainability of these models, thereby improving the resolution of the generated explanations.


\subsection{Graph-Augmented Rumour Detection Variants} 

As an extension of the basic task, some rumour detection approaches incorporate additional features, such as user attributes or other handcrafted features like node centrality. These features are often represented within a graph structure. For instance, consider user features represented by the graph $G_{user} = (V_{user}, E_{user})$, where $V_{user}$ represents users involved in the event, and $E_{user}$ captures the interactions between them. Each node $v_{user}$ has an associated feature vector $\textbf{x}_{v_{user}} \in \mathbb{R}^{|D_{user}|}$,  where $D_{user}$ includes user-specific features such as follower counts, account age, and other relevant attributes. In our study, we opt not to include these variants of rumour detection, as the feature dimensions in the node representation vector correspond directly to these handcrafted user features, and existing explainability methods adequately capture the significance of such features.

\subsection{Problem Formulation}
Let $G = (V, E, \textbf{X})$ represent the attributed event propagation graph, where $V = {0, 1, \ldots, n-1}$ is the set of nodes, $E$ is the set of edges, and $\textbf{X} = [\textbf{x}_0^\text{T}, \textbf{x}_1^\text{T}, \ldots, \textbf{x}_{n-1}^\text{T}]^\text{T} \in \mathbb{R}^{|V| \times |D|}$ is the node feature matrix. Let $P \in \mathbb{Z^+}^{|V| \times |T|}$ denote the tokenized text content associated with the nodes in $G$, where $T$ is the set of tokens for each node. The text embedding function, $f_{text}(P|\Theta_{text}) = \textbf{X}$, maps the tokenized text $P$ to the feature matrix $\textbf{X}$. Additionally, let $f(G|\Theta) = \hat{y}$ represent a trained GNN rumour detection model that takes the attributed event propagation graph $G$ as input and outputs a prediction $\hat{y} \in C$, where $C$ is the set of possible classes. The objective is to produce an explanation $\textbf{Z}^{(\hat{y})} = [\textbf{z}_0^\text{T}, \ldots, \textbf{z}_v^\text{T}, \ldots, \textbf{z}_{n-1}^\text{T}]^\text{T}$, where each $\textbf{z}_v \in \mathbb{R}^{|T_v|}$ contains the attribution for each token in node $v$ with respect to the prediction $\hat{y}$.