We validate CT-LRP through quantitative experiments on three representative GNN-based models trained on publicly available rumour detection datasets. These experiments assess the framework’s effectiveness and propose a novel paradigm for evaluating explainability in GNN-based models. This section outlines the models, datasets, and preprocessing steps, followed by the baselines and evaluation metrics. We conclude with a presentation and discussion of the experimental results.

%We conduct quantitative studies by applying CT-LRP to three representative GNN-based models, each trained on a different publicly available rumour detection dataset. These experiments serve to validate the effectiveness of our framework and to introduce a new paradigm for evaluating explainability in GNN-based rumour detection models. 

%This section is organized as follows: First, we describe the models under study, the datasets on which they are trained, and the corresponding data preprocessing and model training procedures. Next, we detail the baselines used for comparison and provide a brief explanation of the evaluation metrics employed. Finally, we present and discuss the results of our experiments.

%We conduct quantitative studies by applying CT-LRP on three representative GNN-based models trained on three publicly available rumour detection datasets. Through our experiments, we justify the use of our framework and the new paradigm with which we evaluate explainability for GNN-based rumour detection models. 

%In this section, we first cover the models studied, the datasets they are trained along with the data preprocessing and model training procedure. Following that we detail our baselines for comparison and briefly explain the evaluation metrics used. Finally, we show and discuss the results of our experiments.

%\subsection{Models and Datasets}
%\subsubsection{Models}
%We select three representative GNN models for our quantitative studies and briefly describe their architectures:

%\begin{itemize} 
%\item \textbf{Bi-Directional Graph Convolution Network (BiGCN)} \cite{Bian2020RumorDO}: This model utilizes bipartite top-down and bottom-up directed graphs to represent the event propagation structure. It employs two Graph Convolution Networks (GCNs), one for each directed graph. Each GCN consists of two convolution layers. The final graph representation for each GCN is obtained through mean pooling of the node representations, which are then concatenated to produce a joint representation for the final classification layer.

%\item \textbf{Edge-enhanced Bayesian Graph Convolutional Network (EBGCN)} \cite{wei-etal-2021-towards}: Similar to BiGCN, this model also utilizes bipartite top-down and bottom-up directed graphs. It features an edge consistency module that learns dynamic edge weights for the second convolution layer, aiming to synchronize the edge weights between the top-down and bottom-up GCNs. The model uses the same graph aggregation strategy as BiGCN. 

%\item \textbf{Claim-guided Hierarchical Graph Attention Network (ClaHi-GAT)} \cite{lin-etal-2021-rumor}: This model uses an undirected graph for the event propagation structure and incorporates a Graph Attention Network (GAT) with two convolution layers. Additionally, it includes source post-attention and event-level attention mechanisms to refine the final event representation. 
%\end{itemize}

%We select three representative models to conduct our quantitative studies on and briefly describe their model architectures:

%\begin{itemize}
%    \item \textbf{Bi-Directional Graph Convolution Network (BiGCN)} is a GNN model which utilises bipartite top-down and bottom-up directed graphs for the event propagation structure. It contains two Graph Convolution Networks (GCN), one for each directed graph. Each GCN has two convolution layers and the final graph representation for each GCN is obtained via mean pooling of node representations and subsequently concatenated to give the joint representation for the final classification layer. \cite{Bian2020RumorDO}
%    \item \textbf{Edge-enhanced Bayesian Graph Convolutional Network (EBGCN)} is another GNN model which utilises bipartite top-down and bottom-up directed graphs. It includes an edge consistency module to learn dynamic edge weights for the second convolution layer which aims to make the edge weights for the second convolution layer of both the top-down and bottom-up GCN as similar to each other as possible. It uses the same graph aggregation strategy as BiGCN. \cite{wei-etal-2021-towards}
%    \item \textbf{Claim-guided Hierarchical Graph Attention Network (ClaHi-GAT)} is a GNN model which utilises an undirected graph of the event propagation structure with a Graph Attention Network (GAT) that has two convolution layers. It also has additional source post attention and event-level attention to refine the final event representation. \cite{lin-etal-2021-rumor}
%\end{itemize}

%\subsubsection{Datasets} 
%We combine the Twitter 15 \cite{ma2016detecting} and Twitter 16 \cite{ma2016detecting} datasets into a single dataset, referred to as Twitter. Additionally, we utilize the Weibo dataset from the same paper \cite{ma2016detecting} and the PHEME dataset \cite{Zubiaga2015AnalysingHP}. Each dataset contains instances consisting of a source post, all posts within the corresponding conversation thread, and the source post's label. The Twitter and PHEME datasets have four class labels, while the Weibo dataset has two class labels. For each post, the datasets provide the original text, the post's identification number, and the identification number of the post it responded to (null for the source post).

%We combine Twitter 15 \cite{ma2016detecting} and Twitter 16 \cite{ma2016detecting} datasets into a single dataset which we call Twitter. We also use the Weibo dataset introduced in that same paper \cite{ma2016detecting}. Our last dataset is the PHEME dataset \cite{Zubiaga2015AnalysingHP}. In all three datasets, each data instance consists of a source post, all posts within that conversation thread, and the source post's label. In the case of the Twitter and PHEME datasets, there are four class labels and for the Weibo dataset, there are two class labels. Information for each post contains the original text, its post identification number, and the post identification number of the post it responded to (null for the source post).

\subsection{Models and Datasets}
\subsubsection{Models}
We evaluate CT-LRP using three representative GNN-based models, chosen for their distinct architectures and approaches to event propagation:

\begin{itemize} 
\item \textbf{Bi-Directional Graph Convolution Network (BiGCN)} \cite{Bian2020RumorDO}: This model represents event propagation as bipartite top-down and bottom-up directed graphs, processed by two separate Graph Convolution Networks (GCNs). Each GCN comprises two convolution layers, with mean pooling applied to node representations to generate graph-level embeddings. The embeddings are concatenated to form the final input for classification.

\item \textbf{Edge-enhanced Bayesian Graph Convolutional Network (EBGCN)} \cite{wei-etal-2021-towards}: Similar to BiGCN, this model uses bipartite directed graphs but introduces an edge consistency module to dynamically learn and synchronize edge weights between top-down and bottom-up GCNs. The graph aggregation strategy mirrors that of BiGCN.

\item \textbf{Claim-guided Hierarchical Graph Attention Network (ClaHi-GAT)} \cite{lin-etal-2021-rumor}: Using an undirected graph structure, this model incorporates a Graph Attention Network (GAT) with two convolution layers. It also employs source post-attention and event-level attention mechanisms to refine event representations for classification.
\end{itemize}

\subsubsection{Datasets}
We use three publicly available rumour datasets:

\begin{itemize}
\item \textbf{Twitter (combined Twitter15 and Twitter16)} \cite{ma2016detecting}: This dataset contains four-class instances, each consisting of a source post, its thread, and the source post’s label. The class labels are Non-Rumour, True-Rumour, False-Rumour and Unverified.

\item \textbf{Weibo} \cite{ma2016detecting}: This two-class dataset provides the same structure as Twitter but is based on posts from the Chinese microblogging platform Weibo.

\item \textbf{PHEME} \cite{Zubiaga2015AnalysingHP}: Similar to Twitter, this dataset also contains four-class instances. Each instance includes a source post, its thread, and a label.

\end{itemize}

For all datasets, each instance includes the original text, the source post's label, and the conversation structure, represented by the post IDs and their corresponding reply relationships. The source post has no reply relationship (null parent ID).


\subsection{Data Preprocessing and Model Training}
\subsubsection{Data Preprocessing} 
For each dataset, we use the post metadata to construct the top-down graph representing the event propagation structure. We then reverse the direction of the edges to create the bottom-up graph and combine both graphs to form the undirected version of the graph. To standardize text preprocessing across all three datasets, we use a pre-trained multilingual BERT model \cite{Devlin2019BERTPO} and its associated tokenizer to tokenize the post text and generate token embeddings. We opted for a multilingual BERT model because the PHEME dataset includes events in multiple languages. Additionally, using the same multilingual BERT model across datasets ensures that the generated explanations are consistent and not influenced by differences in text embedding models that could arise if different monolingual BERT models were used. We used mean pooling for the main experiments as they demonstrated the best overall model performance on the rumour detection task. We include the results in Appendix \ref{results}.

%For each dataset, we use the post metadata to construct the top-down graph for the event, we then flip the direction of the edges to obtain the bottom-up graph and combine both graphs to obtain the undirected version of the graph. We standardise our text preprocessing for all three datasets by using a pre-trained multilingual BERT model \cite{Devlin2019BERTPO} and its paired tokeniser to tokenise the post text and obtain the token embeddings for each token. We choose a multilingual BERT as the PHEME dataset contains events in more than one language. Also, we chose a multilingual BERT so that the same embedding parameters are used for the text embedding to ensure that the explanations generated for each dataset are not affected by differences between text embedding models had we opted to use different monolingual BERT models. For simplicity, we choose to use mean pooling to obtain the vector representation of each post.

\subsubsection{Model Training} 
Following the task setup in the original papers for our three selected models \cite{Bian2020RumorDO, wei-etal-2021-towards, lin-etal-2021-rumor}, we train each model using a five-fold cross-validation split for the Twitter and Weibo datasets, and a nine-fold event-wise cross-validation split for the PHEME dataset. We adopt the hyperparameter settings specified in the original papers for each model and train them for 200 epochs, with early termination triggered if the loss plateaued for 10 consecutive epochs.

%Following the task setup for the three datasets in the original papers for our three models \cite{Bian2020RumorDO, wei-etal-2021-towards, lin-etal-2021-rumor}, we train each model using a five-fold cross-validation split for Twitter and Weibo datasets and a nine-fold event wise cross-validation split for the PHEME dataset. We adopt the hyperparameter settings used in the original papers for each model and train each model for 200 epochs with early termination after a loss plateau of 10 epochs. 

\subsection{Baselines and Evaluation Metrics}
\subsubsection{Baselines}
\begin{itemize} 
    \item \textbf{LRP} \cite{bach2015pixel}: A decomposition-based method that utilizes deep Taylor approximation and relevance conservation principles to explain the individual feature dimensions of nodes for the class of interest. 
    \item \textbf{Grad-CAM} \cite{Selvaraju2017ICCV}: A gradient-based method that weights the class activation map by using the mean gradient of each feature dimension of the final convolution layer for the class of interest. 
    \item \textbf{Contrastive EB (c-EB)} \cite{zhang2018top}: Another decomposition-based method that applies a probabilistic Winner-Takes-All (WTA) process to generate the attribution map for each feature dimension of the node features. 
\end{itemize}

%\begin{itemize}
%    \item \textbf{LRP} is a decomposition-based method which uses deep Taylor approximation with a relevance conservation principle to explain the individual feature dimensions of nodes with respect to the class of interest.
%    \item \textbf{Grad-CAM} is a gradient-based method that uses the mean gradient of each feature dimension of the final convolution layer with respect to the class of interest to weight the class activation map.
%    \item \textbf{Contrastive EB (c-EB)} is another decomposition-based method which uses a probabilistic Winner-Takes-All process to obtain the attribution map for each feature dimension of the node features.
%\end{itemize}

\subsubsection{Evaluation Metrics}
We use the \textit{Fidelity} and \textit{Sparsity} metrics introduced in \cite{Pope2019ExplainabilityMF}, as well as a combined Fidelity-Sparsity metric that balances these two measures.

\begin{itemize}
    \item \textit{Fidelity} is defined as the proportion of data instances where the model's prediction changes when elements with an attribution score greater than 0.01 are removed \cite{Pope2019ExplainabilityMF}. The intuition is that removing elements deemed most salient by the model should result in a change in its prediction, indicating the importance of those elements.
    \item \textit{Sparsity} is calculated as one minus the ratio of identified elements in the explanation to the total number of nodes in the graph, i.e., $1 - \frac{m^{(c)}}{|V|}$, where $m^{(c)}$ represents the number of identified elements in the explanation for class $c$ \cite{Pope2019ExplainabilityMF}. For CT-LRP, we adapt this metric by considering the number of identified tokens in the explanation relative to the total tokens in the data instance, rather than nodes. Sparsity measures the concentration of the explanation; sparser explanations are preferred as they focus on the most salient elements, which is particularly useful in large graphs where manual inspection of every element is impractical.
    \item \textit{Fidelity-Sparsity} is the product of the \textit{Fidelity} and \textit{Sparsity} metrics. This metric captures the balance between explanation saliency and sparsity, with higher values indicating a more balanced and effective explanation.
\end{itemize}

%We use the Fidelity and Sparsity metrics introduced in \cite{Pope2019ExplainabilityMF} and another Fidelity-Sparsity Metric which considers the balance between the two metrics. 

%Fidelity is defined as the proportion of data instances in which the model's prediction changes when elements with attribution of more than 0.01 are removed. \cite{Pope2019ExplainabilityMF} The intuition behind this is that elements considered most salient to the model's prediction should result in a change in model prediction when they are removed.

%Sparsity is defined as one minus the number of identified elements in the explanation over the total number of nodes in the graph, i.e. $1 - \frac{m^{(c)}}{|V|}$ where $m^{(c)}$ is the number of identified elements in the explanation for class $c$. \cite{Pope2019ExplainabilityMF} For CT-LRP, we consider the number of identified tokens in the explanation and total tokens in the data instance instead of identified nodes and total nodes. Sparsity measures the localisation of the explanation. Sparser explanations are more desirable as they retain only the most salient elements which is useful when dealing with large graphs wherein manual inspection of every element is too time-consuming. 

%Fidelity-Sparsity is the product of the Fidelity and the Sparsity of the explanation. Intuitively, taking the product of the two measures the balance between explanation saliency and sparsity, with higher Fidelity-Sparsity values indicating a more balanced explanation.

\input{tables/fidelity-sparsity}

\begin{figure*}[!ht]
    \centering
    % \includegraphics[width=\linewidth]{figs/CT-LRP Twitter graphs (Web4Good).pdf}
    % \includegraphics[width=\linewidth]{figs/CT-LRP Weibo graphs (Web4Good).pdf}
    % \includegraphics[width=\linewidth]{figs/CT-LRP PHEME graphs (Web4Good).pdf}
    \includegraphics[width=\linewidth]{figs/Twitter_graphs_IEEE.pdf}
    \includegraphics[width=\linewidth]{figs/Weibo_graphs_IEEE.pdf}
    \includegraphics[width=\linewidth]{figs/PHEME_graphs_IEEE.pdf}
    \caption{Quantitative performance comparisons on three models trained on three datasets with the curves obtained by varying sparsity levels.}
    % \Description{Graphs showing the change of Fidelity with Sparisty levels for each explanation method applied to each model.}
    \label{fig:graphs}
\end{figure*}

\subsection{Quantitative Study}
We evaluate fidelity at fixed sparsity levels for each model using k-fold cross-validation. At a given sparsity level, elements highlighted by the explanation are removed in decreasing order of importance until the sparsity limit is reached. For node-level baselines, at a sparsity of 0.8, the most salient nodes are removed until 20\% of the total nodes in the graph remain. Similarly, for CT-LRP, at the same sparsity level, the most salient tokens are removed until 20\% of the total tokens in the graph remain. To prove the effectiveness of the token class disambiguation step, we also test LRP at the token level to provide a comparison. The results are summarized in Tables \ref{tab:fidelity} and \ref{tab:sparsity}, with the top results in bold and the second-best marked with an asterisk (*).

% We evaluate fidelity at fixed sparsity levels for each model using k-fold cross-validation. At a given sparsity level, elements highlighted by the explanation are removed in decreasing order of importance until the sparsity limit is reached. For node-level baselines, at a sparsity of 0.8, the most salient nodes are removed until 20\% of the total nodes in the graph remain. Similarly, for CT-LRP, at the same sparsity level, the most salient tokens are removed until 20\% of the total tokens in the graph remain. We also test a variant of CT-LRP with a node sparsity constraint (\textit{CT-LRP w/ NSC}), where only salient tokens from the top salient nodes are removed, again capped at 20\% of the total nodes. The results are summarized in Tables \ref{tab:fidelity} and \ref{tab:sparsity}, with the top results in bold and the second-best marked with an asterisk (*).


%For each model, we conduct fidelity tests at fixed sparsity levels across each fold in the k-fold cross-validation split. At a given sparsity level, elements highlighted by the explanation are removed in decreasing order of importance until the sparsity limit is reached. For instance, in the node-level baselines, with a fixed sparsity of 0.8, all salient nodes are removed up to a maximum of 20\% of the total number of nodes in the graph. Similarly, for CT-LRP, at a fixed sparsity of 0.8, we remove all salient tokens up to a maximum of 20\% of the total number of tokens in the graph. Additionally, we test a variant of CT-LRP that includes a node sparsity constraint. In this variant, for a fixed sparsity of 0.8, only salient tokens from the top salient nodes are removed, up to a maximum of 20\% of the total nodes in the graph. We refer to this variant as \textit{CT-LRP with Node Sparsity Constraint} (CT-LRP w/ NSC). The results are summarized in Tables \ref{tab:fidelity} and  \ref{tab:sparsity}, with the top result in bold and the second-best result marked with an asterisk (*).

%For each model, we conduct fidelity tests at fixed sparsity levels for each fold in the k-fold split. At a given sparsity level, we remove elements highlighted by the explanation in decreasing order of importance until we hit the sparsity limit. For example, for all node-level baselines, for a fixed sparsity of 0.8, we remove all salient nodes up to a maximum of 20\% of the total number of nodes in the graph. For CT-LRP, for a fixed sparsity of 0.8, we remove all salient tokens up to a maximum of 20\% of the total number of tokens in the graph. Furthermore, we test a variant of CT-LRP with node sparsity constraint as well, meaning that for a fixed sparsity of 0.8, we only remove salient tokens from the top salient nodes up to a maximum of 20\% of the total nodes in the graph. We call this variant \textbf{CT-LRP with Node Sparsity Constraint} (CT-LRP w/ NSC). Our results are summarised in Tables \ref{tab:fidelity} and  \ref{tab:sparsity} with the top result in bold and the second best result marked with an asterisk (*).

The results show that our framework provides superior explanations of model behaviour, as evidenced by the average fidelity scores. Both CT-LRP and token-level LRP achieve the highest fidelity scores across all models and datasets. Among the two, CT-LRP performs better, showing the importance of the class membership disambiguation step, with an average of 25.77\% increase in performance over token-level LRP.

% The results show that our framework provides superior explanations of model behaviour, as evidenced by the average fidelity scores. Both CT-LRP and \textit{CT-LRP w/ NSC} achieve the highest fidelity scores across all models and datasets. Among the two, the unconstrained version consistently performs better, supporting the hypothesis that node-level saliency lacks the granularity needed to fully explain model behaviour.


%The results indicate that our framework provides a superior explanation of model behaviour, as evidenced by the average fidelity scores. Both CT-LRP and its constrained variant (CT-LRP w/ NSC) achieve the highest fidelity scores across all models and datasets. When comparing the two CT-LRP variants, the unconstrained version consistently performs better. This finding supports the hypothesis that determining saliency at the node level does not offer sufficient granularity to explain model behaviour fully.

%As can be seen from the results, our framework is better able to explain model behaviour based on the average fidelity scores with both CT-LRP and its constrained variant, CT-LRP w/ NSC, having the highest results for all models across all datasets. When we compare the results of our two CT-LRP variants, we notice that the unconstrained variant performs better. This further confirms the hypothesis that determining saliency at the node level does not provide enough granularity to fully explain model behaviour.  

On sparsity, c-EB achieves the sparsest explanations due to its WTA process. However, its low fidelity severely limits its utility in understanding model behaviour. In contrast, the CT-LRP provide the second-highest sparsity scores while maintaining high fidelity. CT-LRP demonstrates superior performance over the baselines in achieving an optimal balance between fidelity and sparsity, as evidenced by the Fidelity-Sparsity scores in Table \ref{tab:fidelity-sparsity}, with an average 66.98\% increase in score over the next best baseline.



%Regarding sparsity, c-EB produces the sparsest explanations due to its WTA process, however, it has the lowest fidelity which severely limits how it can help users accurately understand model behaviour. Conversely, CT-LRP variants achieved the second-highest sparsity scores whilst maintaining high fidelity. When considering the balance between fidelity and sparsity, as reflected in the Fidelity-Sparsity scores in Table \ref{tab:fidelity-sparsity}, our methods outperform the baselines.

% Regarding sparsity, the c-EB baseline consistently produces the sparsest explanations. However, both CT-LRP variants also perform well, achieving the second-highest sparsity scores. When considering the balance between fidelity and sparsity, as reflected in the Fidelity-Sparsity scores in Table \ref{tab:fidelity-sparsity}, our methods outperform the baselines.

%For sparsity, c-EB consistently produces the sparsest explanations, with both of our CT-LRP variants performing well with the second-highest results. However, when we consider the balance between fidelity and sparsity, our methods perform the best as evidenced by the Fidelity-Sparsity scores shown in Table \ref{tab:fidelity-sparsity}.

Fig.~\ref{fig:graphs} shows a detailed analysis of CT-LRP and other baselines at different sparsity levels for each model. Generally, as sparsity increases, CT-LRP and token-level LRP experience a notable drop in fidelity although CT-LRP exhibits a great drop in fidelity only at higher sparsities. This suggests that the disambiguation of token class membership successfully removed elements that have a greater impact on other class outputs, thereby ensuring the remaining tokens in the explanation are those which are most salient to the predicted class. On the other hand, the baselines show little change in fidelity at higher sparsity levels, likely due to the smoothing effect of node-level saliency. In such cases, removing all tokens from a node impacts the output more uniformly across classes.

% Fig.~\ref{fig:graphs} shows a detailed analysis of CT-LRP and other baselines at different sparsity levels for each model. Generally, as sparsity decreases, CT-LRP, especially the constrained variant, experiences a notable drop in fidelity. The opposite trend is observed for EBGCN. This suggests that, in BiGCN and ClaHi-GAT, removing elements with lower attribution has less impact on the predicted class's output compared to other classes. Conversely, the baselines show increased fidelity at lower sparsity levels, likely due to the smoothing effect of node-level saliency. In such cases, removing all tokens from a node impacts the output more uniformly across classes.

% Fig.~\ref{fig:graphs} shows a detailed analysis of CT-LRP and its variants at different sparsity levels for each model. Generally, as sparsity decreases, CT-LRP, especially the constrained variant, experiences a notable drop in fidelity. The opposite trend is observed for EBGCN. This suggests that, in BiGCN and ClaHi-GAT, removing elements with lower attribution has less impact on the predicted class's output compared to other classes. Conversely, the baselines show increased fidelity at lower sparsity levels, likely due to the smoothing effect of node-level saliency. In such cases, removing all tokens from a node impacts the output more uniformly across classes.


%A more detailed analysis of CT-LRP and its variants at different sparsity levels for each model is shown in Fig. \ref{fig:graphs}. Generally, as the fixed sparsity decreases, CT-LRP—especially the constrained variant—experiences a notable drop in fidelity. Conversely, in the case of EBGCN, the opposite trend is observed. This suggests that in BiGCN and ClaHi-GAT, removing elements with lower attribution to the original prediction has a lesser impact on the predicted class's output than on other classes. In contrast, reduced sparsity leads to increased fidelity for the baseline methods, likely due to the smoothing effect of considering only node-level saliency. In such cases, removing all tokens from a node affects the output for all classes more uniformly.

%Taking a deeper look into the performance of CT-LRP and its variants at different sparsity levels for each model, we refer to the plots in Fig. \ref{fig:graphs}. We notice in general, that as the fixed sparsity is decreased, CT-LRP, in particular the constrained variant, witnesses a significant drop in fidelity. On the other hand, for EBGCN, the reverse trend is observed. This is interesting as it indicates that when the elements with lower attribution to the original prediction are selected for removal in BiGCN and ClaHi-GAT, it affects the output of the predicted class less than it does for the other classes. By contrast, decreased sparsity results in increased fidelity for the baseline methods. This is likely due to the smoothing effect of considering only node-level saliency whereby the removal of all tokens in the node results in the output for all classes being affected roughly the same.