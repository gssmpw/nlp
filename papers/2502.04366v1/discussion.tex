CT-LRP represents a significant advancement in explainability for GNN-based rumour detection models by providing token-level explanations that are both class-specific and task-relevant. Our experiments reveal that existing GNN explainability methods, which primarily focus on node-level explanations, fall short of capturing the intricate decision-making processes underlying these models. In contrast, CT-LRP's token-level granularity offers a more nuanced understanding of model behaviour, as demonstrated by the substantial improvements in fidelity across all models and datasets. This granularity is essential for rumour detection tasks, where unseen data and dynamic content make accurate interpretation of predictions particularly challenging. 

Key findings from our application of CT-LRP to the Twitter, Weibo, and PHEME datasets underscore its ability to improve fidelity and uncover dataset-specific biases. By distinguishing between class-specific and dataset-specific tokens, our framework enables the identification of patterns and biases that might otherwise remain hidden. This insight is invaluable for researchers seeking to understand how rumour detection models generalize across datasets, as it highlights potential weaknesses and areas for improvement. Additionally, identifying biases makes these models more transparent and trustworthy, benefiting fact-checkers who rely on evidence-based explanations to substantiate their decisions.

Beyond rumour detection, CT-LRP lays the groundwork for advancing explainability in related tasks, such as fake news detection and other misinformation challenges. By framing GNN explainability as a token-level problem, it enables more trustworthy and robust AI systems. Additionally, CT-LRP is computationally efficient, requiring only a single backward pass per class, making it scalable for large graphs and real-world applications.

Despite its strengths, CT-LRP has limitations. It implicitly considers graph inductive biases during token attribution but does not explicitly explain them. Future work could address this by incorporating attribution scores that account for specific edges or subgraphs. Additionally, while our framework is tailored to a specific rumour detection task, extending it to multimodal tasks or heterogeneous graphs will require modifications to accommodate diverse data types and structures.

In summary, CT-LRP not only enhances the explainability of GNN-based rumour detection models but also sets the stage for broader applications in misinformation detection. Addressing both class-specific and dataset-specific attribution contributes to building more interpretable, trustworthy, and effective AI systems.


%In summary, CT-LRP enhances the explainability of GNN-based models, providing deeper insights into their behaviour and improving trust in automated systems. By addressing class-specific and dataset-specific attribution, it contributes to building more interpretable and effective AI systems for misinformation detection and beyond.



%Our proposed framework, CT-LRP, marks a significant advancement in explainability for GNN-based rumour detection models by introducing token-level explanations that are both class-specific and task-relevant. Through our experiments, we have demonstrated that current GNN explainability techniques, which rely on node-level explanations, are insufficient to fully capture the intricate decision-making processes of these models. In contrast, CT-LRP’s token-level granularity provides a more nuanced understanding of model behaviour, as evidenced by the substantial increase in fidelity across all models and datasets when applying our method. This granular level of explanation is crucial for rumour detection, where unseen data and dynamic content make it challenging to interpret model predictions accurately. \red{This is particularly important for engendering greater trust in automated detection methods which are crucial to preventing the spread of rumours and misinformation which disproportionately affect minorities and vulnerable members of society.}

% \red{CT-LRP's granularity is better suited to explain the noisy data in rumour detection, especially in cases where entire sentences may not be informative.}

% Our proposed framework, CT-LRP, marks a significant advancement in explainability for GNN-based rumour detection models by introducing token-level explanations that are both class-specific and task-relevant. Through our experiments, we have demonstrated that current GNN explainability techniques, which rely on node-level explanations, are insufficient to fully capture the intricate decision-making processes of these models. In contrast, CT-LRP’s token-level granularity provides a more nuanced understanding of model behaviour, as evidenced by the substantial increase in fidelity across all models and datasets when applying our method. This granular level of explanation is crucial for rumour detection, where unseen data and dynamic content make it challenging to interpret model predictions accurately.

%Key findings from applying CT-LRP on the Twitter, Weibo, and PHEME datasets reveal that token-level explanations not only enhance fidelity but also enable the identification of biases specific to each dataset. Our framework successfully distinguishes between class-specific and dataset-specific tokens, which helps uncover patterns that might otherwise remain hidden. This capability allows for a deeper examination of how rumour detection models generalize across different datasets, shedding light on potential weaknesses and areas for improvement which is invaluable for researchers. Also, the identification of biases can help make rumour detection models more trustworthy for fact-checkers who need to substantiate their decisions with evidence.

% For instance, the ability to pinpoint task-relevant tokens offers valuable insights into how a model differentiates between classes, providing an effective tool for addressing overfitting or model bias in future iterations. \red{This identification of biases is also important for fact-checkers and researchers to accurately explain model behaviour compared to node-level explanations which failed to do so.}

%The impact of this work extends beyond rumour detection. By framing GNN-based rumour detection explainability as a token-level explainability problem, CT-LRP lays the foundation for more refined explainability techniques in related tasks, such as fake news detection and other misinformation-related challenges. In these domains, understanding the specific textual elements driving model decisions can lead to more trustworthy and robust models. Furthermore, our framework is highly scalable, requiring only a single backward pass to compute the attribution for each class, making it feasible for large graphs.

% Furthermore, as the field of misinformation detection continues to evolve, our framework can inspire future research into explainability techniques that go beyond node-level and graph-level features, enabling the development of models that are both interpretable and effective across a wider range of tasks. 

% The impact of this work extends beyond rumour detection. By framing GNN explainability as a token-level explainability problem, CT-LRP lays the foundation for more refined explainability techniques in related tasks, such as fake news detection and other misinformation-related challenges. In these domains, understanding the specific textual elements driving model decisions can lead to more trustworthy and robust models. Furthermore, as the field of misinformation detection continues to evolve, our framework can inspire future research into explainability techniques that go beyond node-level and graph-level features, enabling the development of models that are both interpretable and effective across a wider range of tasks.

%However, there are limitations to our approach. While CT-LRP effectively identifies dataset-specific biases, it does not explicitly explain graph inductive biases learned by GNNs, instead implicitly accounting for them when computing token attribution. Future work could explore making this more explicit, for example, by providing different attribution scores for each token when considering certain edges or subgraphs. Additionally, our current framework is designed for a specific type of rumour detection task. To apply it to more complex multimodal tasks or heterogeneous graphs, modifications will be necessary to accommodate the diverse feature types.

% However, there are limitations to our approach. While CT-LRP effectively identifies dataset-specific biases, it does not fully account for the graph inductive biases learned by GNNs, which play a critical role in how models process relational information. Addressing this limitation requires further research into how graph structure can be more explicitly incorporated into token-level explanations. Additionally, our current framework is designed for a specific type of rumour detection task. To apply it to more complex multimodal tasks or heterogeneous graphs, modifications will be necessary to accommodate the diverse feature types and structures involved.

%\red{Through our experiments, we have shown that the use of existing GNN explainability techniques for GNN-based rumour detection is unable to fully account for model behaviour which is evidenced by the significant increase in fidelity simply from considering token-level instead of just node-level explanations. Furthermore, our explanation method identifies the exact tokens that contributed to model prediction for both class-specific and task-relevant tokens. This enables users to identify class-specific and dataset-specific biases in models where other methods have failed. This is particularly crucial for rumour detection which has to deal with unseen data and allows users to examine the commonalities and differences between datasets. Our framework although applied to rumour detection can also be applied to other misinformation tasks like GNN-based fake news detection and aid in the discovery of new model insights.}

%\red{However, our work still has some limitations. Although our method can identify dataset-specific biases, it is unable to fully account for the graph inductive bias learned by the model, which is something to be addressed in future research. In addition, our work considers a specific type of rumour detection task and modifications to our framework are required for multimodal task settings or when using heterogeneous graphs.}