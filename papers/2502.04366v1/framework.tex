%In this section, we first motivate our framework by narrating the limitations of applying existing graph explanation techniques to GNN-based rumour detection models. Then, we introduce CT-LRP which addresses these limitations.

% In this section, we first discuss the limitations of existing graph explanation techniques when applied to GNN-based rumour detection models. We then introduce our framework, CT-LRP, which addresses these limitations.

\begin{figure*}[hbt!]
    \centering
    % \includegraphics[width=\linewidth]{figs/CT-LRP Vis v2.pdf}
    \includegraphics[width=\linewidth]{figs/CT-LRP_Vis_v3.pdf}
    \caption{Token-level Explanation produced by CT-LRP. Node-level attribution is shown with darker shades of blue indicating greater attribution from that node. Token-level explanation with blue and green highlights for class-specific and common task-relevant attribution, and red highlights for negative attribution. Darker shades indicate a greater magnitude of importance. The input is the source claim and the responses are modelled as a graph.}
    % \Description{A graphical visualisation showing how CT-LRP provides node and token-level explanations.}
    \label{fig:ct-lrp-vis}
\end{figure*}

% \subsection{Motivation}
% We begin by examining the shortcomings of current graph explainability methods in the context of GNN-based rumour detection. Perturbation-based and surrogate-based techniques are commonly used to explain GNNs by highlighting subsets of nodes, edges, or subgraphs that contribute most significantly to the modelâ€™s predictions \cite{ying2019gnnexplainer, luo2020parameterized, funke2022zorro, huang2022graphlime}. While methods like GNNExplainer can also assess the importance of node features relative to the model's output \cite{ying2019gnnexplainer}, these explanations often fail to generalize to unseen data instances. This limitation arises because these techniques typically do not consider the model's internal learning mechanisms, which are crucial for understanding model behavior on new data. In the context of rumour detection, this lack of generalizability hinders our ability to accurately identify the causes of the model's decisions on unseen examples, ultimately reducing the robustness of the model.

% Next, we consider gradient-based and decomposition-based methods. While these methods generally produce more generalizable explanations, they too have significant drawbacks. Techniques such as Grad-CAM and its variants typically provide explanations at the node level by analyzing model activations and features in higher-level node representations. However, these methods lack the granularity needed to fully capture the complex behavior of rumour detection models. Other approaches that trace gradients or relevance back to the initial node features can produce feature attribution maps at the node level, which may suffice for non-latent features. However, when these features correspond to latent dimensions in the text embedding space, the explanations often lack interpretability. Consequently, the generated explanations tend to either highlight entire posts (represented by the corresponding nodes) or specific dimensions of node input features, which may not directly correspond to individual tokens in the original text.
% \red{(Roy: I thought this section will be better if it is integrated with Related Works. Like highlighting the limitations of these existing works and briefly mentionhow we plan to address them with CT-LRP.)}

%\subsubsection{Existing techniques.} In this subsection, we detail the limitations of employing existing graph explainability techniques on GNN-based rumour detection models. Firstly, we consider perturbation and surrogate-based explainability methods. Typically such techniques designed for GNNs highlight either a subset of nodes, edges or a subgraph which contributes to the model prediction the most. \cite{ying2019gnnexplainer, luo2020parameterized, funke2022zorro, huang2022graphlime} Some methods like GNNExplainer can also explain the importance of node features with respect to the model's prediction \cite{ying2019gnnexplainer}. However, explanations produced by such techniques do not generalise well to unseen data instances and fail to faithfully explain model behaviour on these new data instances because they ignore the model's internal learning mechanisms. In the context of rumour detection, this makes it harder for us to understand the actual causes of model behaviour on unseen data and hurts rather than enhances our ability to increase model robustness.

%Next, we consider gradient and decomposition-based methods and their naive application on GNN-based rumour detection models. Although these two classes of methods can produce more generalisable explanations, they still have their drawbacks. Methods such as Grad-CAM and its variants can only provide meaningful explanations at the node level as they consider only model activations and features in the higher-level node representation space. As such they lack the granularity required to truly explain the behaviour of rumour detection models. Other methods which trace gradients or relevance to the initial node features yield feature attribution maps at the node level which is sufficient to explain model behaviour concerning individual node feature dimensions in the case of non-latent features. However, because these feature dimensions correspond to the latent features of the text embedding space, the meaning of individual feature dimensions cannot be fully explained. Hence, when explanations for such models are generated, they either highlight entire posts (corresponding to the highlighted nodes) or specific dimensions in the node input features that do not correspond to particular tokens in the original post.

% \subsubsection{Intrinsicly Interpretable GNN-based Rumour Detection Models.} Since GNN models have been applied to rumour detection, there have been several attempts to elucidate the workings of these models by including intrinsic interpretability in their design. These models primarily rely on the use of self-attention or co-attention mechanisms to provide model interpretability in the form of attention coefficients. For example, Diverse Counterfactual Evidence framework for Rumour Detection (DCE-RD) 

%\subsection{Contrastive Token Layerwise Relevance Propagation}
%Our strategy to overcome the low resolution of current explanations is to treat the problem of explaining GNN-based rumour detection models similarly to explaining text classification models. Therefore, the explanations generated should be able to highlight individual tokens which are responsible for the model's prediction of a particular class and not just the entire sentence or post which corresponds to the node in the graph. In this subsection, we first present the problem formulation for explaining GNN-based models for rumour detection with event propagation structure task. Then we present our framework for dealing with attribution for the GNN-encoder portion of the model and subsequently the token-level attribution.

% \red{(Roy: Just start this section with an overview of CT-LRP.)}
%Our approach to addressing the low resolution of existing explanations is to approach the explanation of GNN-based rumour detection models in a manner similar to explaining text classification models. Specifically, we aim to generate explanations that highlight individual tokens responsible for the model's prediction of a particular class, rather than merely identifying entire sentences or posts corresponding to nodes in the graph. In this subsection, we first define the problem formulation to explain GNN-based rumour detection models within an event propagation structure. We then introduce our framework, which provides detailed attribution at both the GNN-encoder level and the token level, ensuring that the generated explanations are both granular and interpretable.

Our approach addresses the low resolution of existing GNN explainability methods by reframing the explanation process to align with techniques used in text classification models. Instead of attributing relevance to entire sentences or posts represented as nodes in the graph, we aim to pinpoint the specific tokens that drive the model's predictions for a given class. This fine-grained approach enhances both the granularity and interpretability of the explanations. In this subsection, we first formalize the task of explaining GNN-based rumour detection models within the context of event propagation graphs. We then introduce our framework, which delivers detailed attributions at both the GNN-encoder and token levels, bridging the gap between high-dimensional embeddings and actionable insights.

% \subsection{Problem Formulation}
\subsection{Task Formulation}
Building on the graph representation outlined in Section \ref{sec:preliminaries}, we expand the attributed event propagation graph \( G = (V, E, \mathbf{X}) \) to incorporate tokenized text content \( P \) associated with the nodes. Here, \( V = \{0, 1, \ldots, n-1\} \) is the set of nodes, \( E \) is the set of edges, and \( \mathbf{X} \in \mathbb{R}^{|V| \times |D|} \) is the node feature matrix. The tokenized text \( P \in \mathbb{Z^+}^{|V| \times |T|} \) is mapped to the feature matrix \( \mathbf{X} \) via a text embedding function \( f_{\text{text}}(P | \Theta_{\text{text}}) = \mathbf{X} \), where \( T \) represents the set of tokens for each node. The rumour detection model is implemented as a GNN \( f(G | \Theta) = \mathbf{y} \), parameterized by \( \Theta \). Given \( G \), the model outputs logits \( \mathbf{y} \in \mathbb{R}^{|C|} \), where the predicted class \( \hat{y} \) is obtained as \( \hat{y} = \text{argmax}(\mathbf{y}) \). The goal of explainability is to generate a token-level attribution map \( \mathbf{Z}^{(\hat{y})} = [\mathbf{z}_0^\text{T}, \ldots, \mathbf{z}_v^\text{T}, \ldots, \mathbf{z}_{n-1}^\text{T}]^\text{T} \), where \( \mathbf{z}_v \in \mathbb{R}^{|T_v|} \) quantifies the contribution of each token \( t \in T_v \) in node \( v \) to the prediction \( \hat{y} \).

\textbf{Framework Overview.} Fig.~\ref{fig:ct-lrp} illustrates the flow of information through our framework. The process involves a \textit{forward pass} and a \textit{backward pass}. In the \textit{Forward Pass}, tokenized text \( P \) is transformed into the feature matrix \( \mathbf{X} \) via \( f_{\text{text}} \). The GNN processes \( \mathbf{X} \) along with the adjacency matrix \( \mathbf{A} \in \{0, 1\}^{|V| \times |V|} \), producing logits \( \mathbf{y} \). In the \textit{Backward Pass}, relevance scores are calculated for the predicted class \( \hat{y} \) by masking all other logits. These scores are backpropagated through the GNN to generate node-level attributions \( \mathbf{R}_{\text{node}} \). Finally, \( \mathbf{R}_{\text{node}} \) is backpropagated through \( f_{\text{text}} \) to produce token-level attributions \( \mathbf{Z}^{(\hat{y})} \).

\textbf{Motivation for Token-Level Attribution.} 
As discussed in Section \ref{sec:preliminaries}, the feature matrix \( \mathbf{X} \) comprises latent text embeddings, which are challenging to interpret directly. Traditional gradient and decomposition-based methods provide node-level attributions, but they fail to attribute relevance to individual tokens in \( P \). By leveraging these methods to compute intermediate node-level attributions and extending them through relevance backpropagation, our framework produces fine-grained, interpretable token-level explanations. This approach bridges the gap between high-dimensional latent features and actionable insights. The complete steps for generating token-level attributions are detailed in Algorithm~\ref{alg:ct-lrp}, and the effectiveness of our method is demonstrated through experiments in subsequent sections.



%Let $G = (V, E, \textbf{X})$ be the attributed event propagation graph with node set $V = \{0, 1, ... , n-1\}$, edge set $E$ and node feature set $\textbf{X} = [\textbf{x}_0^\text{T}, \textbf{x}_1^\text{T}...,\textbf{x}_{n-1}^\text{T}]^\text{T} \in \mathbb{R}^{|V|\times |D|}$. Next, let $P \in \mathbb{Z^+}^{|V| \times |T|}$ be the tokenised text content of the nodes in $G$, where $T$ is the set of tokens associated with each node and let $f_{text}(P|\Theta_{text}) = \textbf{X}$ be the text embedding function that transforms the tokenised text content $P$ to the feature matrix $\textbf{X}$. Furthermore, let $f(G|\Theta) = \hat{y}$ be a trained GNN rumour detection model which takes as input the attributed event propagation graph $G$ and gives an output prediction $\hat{y} \in C$ where $C$ is the set of classes. Given the model $f$ and the input graph $G$, our goal is to produce an explanation $\textbf{Z}^{(\hat{y})} = [\textbf{z}_0^\text{T}, ...,\textbf{z}_v^\text{T},...,\textbf{z}_{n-1}^\text{T}]^\text{T}$ such that $\textbf{z}_v \in \mathbb{R}^{|T_v|}$ contains the attribution for each token in node $v$ with respect to the prediction $\hat{y}$.

%Let $G = (V, E, \textbf{X})$ represent the attributed event propagation graph, where $V = {0, 1, \ldots, n-1}$ is the set of nodes, $E$ is the set of edges, and $\textbf{X} = [\textbf{x}_0^\text{T}, \textbf{x}_1^\text{T}, \ldots, \textbf{x}_{n-1}^\text{T}]^\text{T} \in \mathbb{R}^{|V| \times |D|}$ is the node feature matrix. Let $P \in \mathbb{Z^+}^{|V| \times |T|}$ denote the tokenized text content associated with the nodes in $G$, where $T$ is the set of tokens for each node. The text embedding function, $f_{text}(P|\Theta_{text}) = \textbf{X}$, maps the tokenized text $P$ to the feature matrix $\textbf{X}$. Additionally, let \red{$(G|\Theta) = \textbf{y}$} represent a trained GNN rumour detection model that takes the attributed event propagation graph $G$ as input and outputs a \red{vector $\textbf{y}\in\mathbb{R}^{|C|}$ where the} prediction \red{is} $\red{argmax\ \textbf{y} = }\ \hat{y} \in C$, where $C$ is the set of possible classes. The objective is to produce an explanation $\textbf{Z}^{(\hat{y})} = [\textbf{z}_0^\text{T}, \ldots, \textbf{z}_v^\text{T}, \ldots, \textbf{z}_{n-1}^\text{T}]^\text{T}$, where each $\textbf{z}_v \in \mathbb{R}^{|T_v|}$ contains the attribution for each token in node $v$ with respect to the prediction $\hat{y}$.

% \subsection{Overview}
% Applying gradient or decomposition-based methods can explain the importance of feature dimensions at the node level, this works well for features which map to concrete variables such as follower counts or reposts. However, these feature dimensions of the text embedding do not directly map to the individual tokens. To further explain the importance of individual tokens in the input text, we can use the node feature attribution maps produced by these methods as intermediate inputs for further token-level attribution.

%Naively applying gradient or decomposition-based methods can explain the importance of latent features in the text embedding space but not the actual tokens in the text. However, these methods are advantageous because the node attribution maps they produce can be used as intermediate inputs for further token-level attribution.

%The naive application of any gradient or decomposition-based methods is only suitable to explain the importance of latent features in the text embedding space but not the actual tokens in the text themselves. However, the advantage of such methods is that we can use the node attributions maps produced by such methods as intermediate inputs to perform the token-attribution level step. 

%We refer to Fig. \ref{fig:ct-lrp} to demonstrate the flow of information through our framework. In the forward pass, the tokenised text $P$ is passed through the embedding function to obtain the feature matrix $\textbf{X}$ for the GNN model to encode. The graph's feature matrix and adjacency matrix, $\textbf{A}\in \{0,1\}^{|V|\times |V|}$, are then passed to the GNN model to obtain the output logits. In the backward pass, the relevance of the logits is masked except for the selected class and backpropagated through the network to obtain the node feature attribution map for that class. Then the node feature attribution map values are backpropagated as relevances through the text embedding function to obtain the token attribution map for that class. The steps are summarised in the pseudocode provided in Algorithm \ref{alg:ct-lrp}.

%Refer to Fig. \ref{fig:ct-lrp} for a visual representation of the flow of information through our framework. In the forward pass, the tokenized text $P$ is processed by the embedding function to generate the feature matrix $\textbf{X}$, which is then encoded by the GNN model. The feature matrix $\textbf{X}$, along with the adjacency matrix $\textbf{A} \in {0,1}^{|V| \times |V|}$, is passed through the GNN model to produce the output logits. In the backward pass, the relevances of the logits are masked except for the selected class and then backpropagated through the network to generate the node feature attribution map for that class. Subsequently, the values in the node feature attribution map are backpropagated through the text embedding function to derive the token attribution map for that class. The complete steps are detailed in the pseudocode provided in Algorithm \ref{alg:ct-lrp}.

\begin{algorithm}[t]
\caption{Contrastive Token Layerwise Relevance Propagation (CT-LRP)}
\label{alg:ct-lrp}
\DontPrintSemicolon
\SetKwInOut{}{}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$G=(V,E)$, $P\in \mathbb{Z^+}^{|V|\times |T|}$, $f(G|\Theta)$, $f_{text}(P|\Theta_{text})$, $C$ which is set of class labels}
\Output{$\mathbf{Z}^{(\hat{y})}\in \mathbb{R}^{|V|\times |T|}$, $\textbf{Z}_{mask} = \{True, False\}^{|V|\times |T|}$}
\Begin{
encode: $f_{text}(P) = \textbf{X}$, where $\mathbf{X} \in \mathbb{R}^{|V| \times |D|}$\;
assign: $G \longleftarrow (V,E,\textbf{X})$\;
predict: $f(G) = \textbf{y}, argmax\ \textbf{y} = \hat{y}$\;
\For{$c\in C$}{
LRP for $f$: $LRP_{GNN}(f,c,G)= \textbf{R}^{(c)}$, where $\textbf{R}^{(c)} \in \mathbb{R}^{|V|\times |D|}$\;
LRP for $f_{text}$: $LRP_{text}(f_{text},\textbf{R}^{(c)},P) = \textbf{Z}^{(c)}$, where $\textbf{Z}^{(c)} \in \mathbb{R}^{|V| \times |T|}$\;
}
create: $\textbf{Z}_{mask} = \{False\}^{|V|\times |T|}$\;
\For{$c \in C$ and $c \neq \hat{y}$}{
\For{$\textbf{z}^{(c)}_{v} \in \textbf{Z}^{(c)}$}{
\For{$\textbf{z}^{(c)}_{v,t} \in \textbf{z}^{(c)}_{v}$}{
\If{$z^{(\hat{y})}_{v,t} \leq 0$}{assign: $\textbf{Z}_{mask,v,t} \longleftarrow False$}
\If{$z^{(c)}_{v,t} > 0$ and $z^{(\hat{y})}_{v,t} > 0$}{
assign: $G' = G - \textbf{x}_{t_v}$\;
do: $f(G')=\textbf{y'}$\;
\If{$\textbf{y}_{c} - \textbf{y'}_{c} > \textbf{y}_{\hat{y}} - \textbf{y'}_{\hat{y}}$}{assign: $\textbf{Z}_{mask,v,t} \longleftarrow False$}
\Else{assign: $\textbf{Z}_{mask,v,t} \longleftarrow True$}
}
% \If{$z^{(\hat{y})}_{v,t} > 0$ and $z^{(c)}_{v,t} > z^{(\hat{y})}_{v,t}$}{assign: $\textbf{Z}_{mask,v,t} \longleftarrow False$}
\Else{assign: $\textbf{Z}_{mask,v,t} \longleftarrow True$}
% \If{$z^{(c)}_{v,t} \leq 0$ and $z^{(\hat{y})}_{v,t} > 0$}{assign: $\textbf{Z}_{mask,v,t} \longleftarrow True$}
% \If{$z^{(c)}_{v,t} > 0$ and $z^{(\hat{y})}_{v,t} > 0$}{assign: $\textbf{Z}_{mask,v,t} \longleftarrow False$}
% \Else{assign: $\textbf{Z}_{mask,v,t} \longleftarrow False$}
}   
}
}
}
\Return $\textbf{Z}^{(\hat{y})}$, $\textbf{Z}_{mask}$
\end{algorithm}

\subsection{GNN explanation}
We select LRP~\cite{bach2015pixel} as our base method due to its strong attribution conservation properties, making it well-suited for generating interpretable explanations. Given our focus on token-level explanations, we prioritize node feature attributions while treating the structure of the input graph \( G \) as static. As a result, the importance of edges is implicitly accounted for through LRPâ€™s attribution propagation.

For the graph convolutional and classifier layers, we employ the epsilon-stabilized LRP rule, which generates sparser explanations by focusing on the most salient features. This property is particularly valuable in large graphs, where sparsity enhances both interpretability and usability. The epsilon-stabilized LRP rule is defined as:

\begin{equation} 
    r_j = \sum_k \frac{a_j w_{jk}}{\epsilon + \sum_{j'} a_{j'} w_{j'k}} r_k 
    \label{eq:lrp-eps} 
\end{equation}

Here, \( r_j \) and \( r_k \) denote the relevance scores of neurons \( j \) and \( k \), respectively, \( a_j \) represents the activation of neuron \( j \), \( w_{jk} \) is the weight of the connection between neurons \( j \) and \( k \), and \( \epsilon \) is a small stabilizing constant. By applying this rule during the backward pass, we obtain the node feature attribution map \( \mathbf{R} \in \mathbb{R}^{|V| \times |D|} \) for the target class, with dimensions corresponding to the original node feature matrix \( \mathbf{X} \). In this attribution map, positive values indicate components that contribute to an increase in the model's output for the target class, while negative values indicate components that decrease the output for the target class. This node-level attribution serves as an intermediate representation, bridging the gap between GNN explanations and token-level attributions.


%We select LRP~\cite{bach2015pixel} as our base method due to its strong attribution conservation properties. Given our focus on token-level explanations, we prioritize node feature explanations and treat the structure of the input graph $G$ as static. Consequently, \red{the importance of edges are accounted for implicitly by LRP.} 

%For the graph convolutional layers and the classifier layers, we adopt the epsilon-stabilized LRP rule. This rule is advantageous because it generates sparser explanations, which is particularly useful in the context of large graphs, where highlighting only the most salient features is desirable. The formulation of the epsilon-stabilized LRP rule is as follows:

%\begin{equation} 
%    r_j = \sum_k \frac{a_j w_{jk}}{\epsilon + \sum_{0,j} a_j w_{jk}} r_k 
%    \label{eq:lrp-eps} 
%\end{equation}

%In this equation, $r_j$ and $r_k$ represent the relevance scores of neurons $j$ and $k$, respectively. The term $a_j$ denotes the activation of neuron $j$, $w_{jk}$ is the weight of the connection between neurons $j$ and $k$, and $\epsilon$ is a small stabilizing constant. By applying this rule during the backward pass, we obtain the intermediate node feature attribution map $\textbf{R} \in \mathbb{R}^{|V| \times |D|}$ \red{for the target class}, where the dimensions correspond to those of the original node feature input matrix $\textbf{X}$. \red{Within this attribution map, positive values represent components which induce an increase in the function output for the target class while negative values represent components which induce a decrease in the function output of the target class.}


%We select LRP as our base method for its strong attribution conservation properties. Furthermore, since we are interested in the token-level explanations, we only need the node feature explanations and can treat the structure of the input graph $G$ as static. Therefore, we choose not to directly explain the importance of edges in the graph and instead focus on the nodes. For the graph convolution layers and the classifier layers, we elect to use the epsilon-stabilised LRP rule. The advantage of using this rule is that it produces sparser explanations which in the case of large graphs can be helpful to only highlight the most salient features. The formulation of the rule is shown below:
%\begin{equation}
%    r_j = \sum_k \frac{a_j w_{jk}}{\epsilon + \sum_{0,j} a_j w_{jk}} r_k
%    \label{eq:lrp-eps}
%\end{equation}
%$r_j$ and $r_k$ is the relevance of neurons $j$ and $k$ respectively, $a_j$ is the activation of neuron $j$, $w_{jk}$ is the weight on the connection between neurons $j$ and $k$, and $\epsilon$ is some small stablising constant. By applying this rule in the backward pass, we obtain the intermediate node feature attribution map $\textbf{R} \in \mathbb{R}^{|V|\times |D|}$ with dimensions corresponding to that of the original node feature input matrix $\textbf{X}$.

\input{tables/fidelity}
\input{tables/sparsity}

\subsection{Token-level explanation}
%Using the node feature attribution map $\textbf{R}$ as input to our embedding function, we select the appropriate rules to backpropagate the relevance through the embedding function. First, let us consider the embedding process to be a two-stage process. In stage one, each token $t$ in node $v$ is embedded in the text embedding space with the function $f_{embed}$ which yields the vector $\textbf{x}_{v,t}$. In the second stage, the token vectors are combined to form the node vector with the function $f_{pool}$. The function $f_{pool}$ can be any function, from a mean pooling or max pooling layer to a Multi-Layer Perceptron (MLP) or even a Transformer network.

%Using the node feature attribution map $\textbf{R}$ as input to our embedding function, we select appropriate rules to backpropagate the relevance through the embedding function. We conceptualize the embedding process as a two-stage operation. In the first stage, each token $t$ in node $v$ is embedded into the text embedding space via the function $f_{embed}$, resulting in the token vector $\textbf{x}_{v,t}$. In the second stage, these token vectors are combined to form the node vector using a pooling function $f_{pool}$. The function $f_{pool}$ can range from simple operations like mean pooling or max pooling to more complex models such as a Multi-Layer Perceptron (MLP) or a Transformer network.

To generate token-level attributions, we backpropagate the node feature attribution map \( \mathbf{R} \) through the embedding function. The embedding process is conceptualized as a two-stage operation. In the first stage, each token \( t \) in node \( v \) is embedded into the text embedding space via the function \( f_{\text{embed}} \), producing the token vector \( \mathbf{x}_{v,t} \). In the second stage, these token vectors are aggregated to form the node vector using a pooling function \( f_{\text{pool}} \). The pooling function \( f_{\text{pool}} \) may involve simple operations like mean or max pooling, or more complex architectures such as a Multi-Layer Perceptron (MLP) or a Transformer network.


%For simplicity, consider the case where mean pooling is used. The mean pooling function is formulated as follows: \begin{equation}
%    \textbf{x}_{v,d} = \frac{1}{|T_v|} \sum_{t_v \in T_v} \textbf{x}_{t_v,d}
%    \label{eq:meanpool}
%\end{equation}

%where $T_v$ is the set of tokens in node $v$, and $t_v \in T_v$. Here, $\textbf{x}_{v,d}$ and $\textbf{x}_{t_v,d}$ represent the values of the $d^{th}$ dimension of the node vector for node $v$ and the token vector for token $t_v$, respectively.

For simplicity, we first consider the case of mean pooling. The node vector for mean pooling is defined as:
\begin{equation}
    \mathbf{x}_{v,d} = \frac{1}{|T_v|} \sum_{t_v \in T_v} \mathbf{x}_{t_v,d}
    \label{eq:meanpool}
\end{equation}
where \( T_v \) is the set of tokens in node \( v \), and \( \mathbf{x}_{v,d} \) and \( \mathbf{x}_{t_v,d} \) represent the \( d^{th} \) dimension of the node vector and token vector, respectively.


%For simplicity, let us consider the case when mean pooling is used. The formulation of the mean pooling function is as follows:
%\begin{equation}
%    \textbf{x}_{v,d} = \frac{1}{|T_v|} \sum_{t_v \in T_v} \textbf{x}_{t_v,d}
%    \label{eq:lrp-meanpool}
%\end{equation}
%$T_v$ is the set of tokens in $v$ and $t_v \in T_v$, while $\textbf{x}_{v,d}$ and $\textbf{x}_{t_v,d}$ are the values of the $d^{th}$ dimension of the node vector of node $v$ and the token vector of token $t_v$ respectively.

%Applying the epsilon-stabilized LRP rule from equation \ref{eq:lrp-eps}, the relevance propagation rule for the mean pooling layer is given by: 
%\begin{equation}
%    \textbf{r}_{t_v,d} = \frac{\textbf{x}_{t_v,d}}{\epsilon + \textbf{x}_{v,d}} \textbf{r}_{v,d}
%    \label{eq:lrp-meanpool}
%\end{equation}

Applying the epsilon-stabilized LRP rule (Eq.~\ref{eq:lrp-eps}), the relevance propagation for mean pooling is given by:
\begin{equation}
    \mathbf{r}_{t_v,d} = \frac{\mathbf{x}_{t_v,d}}{\epsilon + \mathbf{x}_{v,d}} \mathbf{r}_{v,d}
    \label{eq:lrp-meanpool}
\end{equation}

%Using the epsilon-stabilised LRP rule in equation \ref{eq:lrp-eps}, the relevance propagation rule for the mean pooling layer would be:
%\begin{equation}
%    \textbf{r}_{t_v,d} = \frac{\textbf{x}_{t_v,d}}{\epsilon + \textbf{x}_{v,d}} \textbf{r}_{v,d}
%\end{equation}

%For max pooling, the corresponding function and its relevance propagation rule are: 

%\begin{equation}
%    \textbf{x}_{v,d} = max_{t_v \in T_v}(\textbf{x}_{t_v,d})
%    \label{eq:maxpool}
%\end{equation}
%\begin{equation}
%    \textbf{r}_{t_v,d} = \left\{
%    \begin{array}{c l}
%        \textbf{x}_{t_v,d} \times \textbf{r}_{v,d} & \quad\text{if } \textbf{x}_{t_v,d} = max_{t_v \in T_v}(\textbf{x}_{t_v,d}), \\
%        0 & \quad \text{otherwise}
%    \end{array}
%    \right.
%    \label{eq:lrp-maxpool}
%\end{equation}

For max pooling, the node vector and its relevance propagation rule are defined as:
\begin{equation}
    \mathbf{x}_{v,d} = \max_{t_v \in T_v}(\mathbf{x}_{t_v,d})
    \label{eq:maxpool}
\end{equation}
\begin{equation}
    \mathbf{r}_{t_v,d} = 
    \begin{cases} 
        \mathbf{x}_{t_v,d} \cdot \mathbf{r}_{v,d}, & \text{if } \mathbf{x}_{t_v,d} = \max_{t_v \in T_v}(\mathbf{x}_{t_v,d}) \\
        0, & \text{otherwise}
    \end{cases}
    \label{eq:lrp-maxpool}
\end{equation}

When \( f_{\text{pool}} \) is implemented using an MLP or a more complex network, the epsilon-stabilized LRP rule (Eq.~\ref{eq:lrp-eps}) is applied to the individual layers within the network as needed.

%In cases where $f_{pool}$ is an MLP or a more complex network, the epsilon-stabilized LRP rule from equation \ref{eq:lrp-eps} is applied to the component layers as needed.

%In the case of max pooling, the function and its relevance propagation equivalent would be:
%\begin{equation}
%    \textbf{x}_{v,d} = max_{t_v \in T_v}(\textbf{x}_{t_v,d})
%    \label{eq:maxpool}
%\end{equation}
%\begin{equation}
%    \textbf{r}_{t_v,d} = \left\{
%    \begin{array}{c l}
%        \textbf{x}_{t_v,d} \times \textbf{r}_{v,d} & \quad\text{if } \textbf{x}_{t_v,d} == max_{t_v \in T_v}(\textbf{x}_{t_v,d}), \\
%        0 & \quad \text{otherwise}
%    \end{array}
%    \right.
%    \label{eq:lrp-maxpool}
%\end{equation}
%In the case of the function $f_{pool}$ being an MLP or some other type of more complex network, we apply the equation \ref{eq:lrp-eps} to its component layers as necessary.

%Once the attribution maps for the token vectors are obtained, we sum the values across all dimensions of the vector to produce the final token attribution map: 
%\begin{equation}
%    \textbf{z}_{t_v} = \sum_{d \in D} \textbf{r}_{t_v,d}
%\end{equation}
%where $\textbf{z}_{t_v}$ represents the attribution of token $t_v$ associated with node $v$ \red{for the target class $c$}.

Finally, the token-level attribution map is obtained by summing the relevance values across all dimensions of the token vectors:
\begin{equation}
    \mathbf{z}_{t_v} = \sum_{d \in D} \mathbf{r}_{t_v,d}
\end{equation}
where \( \mathbf{z}_{t_v} \) represents the total attribution of token \( t_v \) associated with node \( v \) for the target class \( c \).


%With the attribution maps for the token vectors, we simply sum the values in all dimensions of the vector to obtain the final token attribution map:
%\begin{equation}
%    \textbf{z}_{t_v} = \sum_{d \in D} \textbf{r}_{t_v,d}
%\end{equation}
%Where $\textbf{z}_{t_v}$ is the attribution of token $t_v$ belonging to node $v$.

%\subsection{Contrastive Token-level Explanation}
%Using the framework described above, we obtain token attribution maps for each class, denoted as $\textbf{Z}^{(c)}$ for $c \in C$. We then iterate through the attribution maps of all other classes, comparing them against the attribution map of the predicted class. \red{During this comparison, we identify tokens with positive attribution for the predicted class and any other class. For each shared token, we construct a perturbed input, $G'= G - \textbf{x}_{t_{v}}$ by removing the token from the original input and obtaining the logits of this perturbed input, $f(G) = \textbf{y'}$. We find the difference between the original logits and the perturbed logits to determine the influence of the removed token. If the difference for the predicted class is larger than the other classes, i.e. $\textbf{y}_{c} - \textbf{y'}_{c} > \textbf{y}_{\hat{y}} - \textbf{y'}_{\hat{y}}$, then the token has a greater impact on the output of the predicted class and belongs in the explanation of the predicted class, otherwise, it does not. To identify tokens which belong to the predicted class, we generate a mask matrix which eliminates shared tokens that have a greater influence on other classes and tokens with negative attribution values. The final output gives us the original attribution map and the token mask which when multiplied with each other yields an exclusive set of tokens that are responsible for the model's prediction thus disambiguating membership for tokens which contribute positively to the function outputs of multiple classes.} 

%An example explanation generated by CT-LRP is provided in Fig. \ref{fig:ct-lrp-vis}. In this figure, tokens highlighted in blue and green represent class prediction-specific and general task-relevant tokens, respectively. Both types of tokens contribute positively to the model's output for the predicted class. Conversely, tokens highlighted in red are those \red{with negative relevance scores for the predicted class.} 

\subsection{Contrastive Token-Level Explanation}
Using the framework described above, we compute token attribution maps for each class, denoted as \( \mathbf{Z}^{(c)} \) for \( c \in C \). To refine the attribution map for the predicted class \( \hat{y} \), we compare it against the attribution maps of all other classes. Specifically, we identify tokens with positive attribution values in both the predicted class and any other class. 

For each shared token, we construct a perturbed graph input \( G' = G - \mathbf{x}_{t_v} \), where the token vector \( \mathbf{x}_{t_v} \) is removed before the node feature aggregation step. The logits for the perturbed input are then calculated as \( f(G') = \mathbf{y}' \). The token's influence is determined by the difference between the original logits and the perturbed logits. If the influence on the predicted class satisfies $\mathbf{y}_{\hat{y}} - \mathbf{y}'_{\hat{y}} > \mathbf{y}_c - \mathbf{y}'_c, \quad \forall c \neq \hat{y}$, then the token contributes more strongly to the predicted class and is retained in the explanation. Otherwise, it is excluded.

To finalize the explanation, we generate a mask matrix to eliminate shared tokens that have a greater influence on other classes and tokens with negative attribution values. Multiplying the original attribution map by this mask yields an exclusive set of tokens that are specific to the predicted class, effectively disambiguating tokens that contribute positively to multiple class outputs.

An example explanation generated by CT-LRP is shown in Fig.~\ref{fig:ct-lrp-vis}. In this figure, tokens highlighted in blue and green represent class prediction-specific and general task-relevant tokens, respectively, the former exclusively contributing positively to the model's output for the predicted class and the latter contributing to multiple classes but with the most contribution to the predicted class. Tokens highlighted in red have negative attribution values for the predicted class, indicating a suppressive effect.

% An example explanation generated by CT-LRP is shown in Fig.~\ref{fig:ct-lrp-vis}. In this figure, tokens highlighted in blue and green represent class prediction-specific and general task-relevant tokens, respectively, both of which contribute positively to the model's output for the predicted class. Tokens highlighted in red have negative attribution values for the predicted class, indicating a suppressive effect.


%Using the framework above, we obtain token attribution maps for each class, $\textbf{Z}^{(c)} \text{ for } c \in C$. Next, we iterate through the attribution maps for all other classes, compare them against that of the predicted class and identify any tokens which are common to both attribution maps. We produce a token mask matrix which masks all common tokens and tokens with negative attribution values. We multiply that mask with the predicted class attribution map and the resultant token attribution map will contain tokens relevant only to the predicted class. The intuition behind this step is that it removes tokens which are considered to be task-relevant to the model (and hence may appear in more than one class attribution map) but not class-specific. We provide an example explanation by CT-LRP in Fig. \ref{fig:ct-lrp-vis}. The tokens highlighted in blue and green are class prediction-specific and general task-relevant tokens respectively, both these types of tokens induce the model to produce positive outputs for the predicted class. Tokens highlighted in red are tokens which induce the model to produce negative outputs for the predicted class.

