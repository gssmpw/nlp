In this study, we introduced CT-LRP, a novel framework for enhancing the explainability of GNN-based rumour detection models by providing fine-grained, class-specific, and task-relevant token-level explanations. By reframing explainability from the node-level to the token-level, CT-LRP addresses critical challenges in interpreting high-dimensional latent features, achieving a new standard of granularity and fidelity in model explanations.

Our experimental results on three real-world datasets demonstrate the framework's effectiveness, highlighting its superiority in generating interpretable and high-fidelity explanations compared to existing methods. Furthermore, CT-LRP's ability to identify both class-specific and dataset-specific patterns sheds light on potential biases and generalization gaps, offering valuable insights for improving model reliability and fairness.

Beyond rumour detection, the principles underlying CT-LRP extend to broader misinformation detection and graph-based applications, making it a versatile tool for advancing trust in AI systems. As misinformation continues to pose significant societal risks, CT-LRP represents a meaningful step toward building transparent, trustworthy, and ethical AI solutions. Future work will focus on extending this framework to multimodal and heterogeneous graphs, exploring its potential in increasingly complex real-world scenarios.


%In this study, we introduced CT-LRP, a framework that provides class-specific token-level explanations for GNN-based rumour detection models. By reframing the task of explaining GNN-based rumour detection as a text explainability problem, we achieved more granular and high-fidelity explanations. Experimental results across three real-world models on three publicly available rumour detection datasets demonstrate the effectiveness of our approach. This work represents a significant step toward enhancing the robustness and trustworthiness of rumour detection models by offering more informative and interpretable explanations.

%In this study, we introduce CT-LRP, which provides class-specific token-level explanations for GNN-based rumour detection models. By reframing the task setup for explaining GNN-based rumour detection as a type of text explainability, we can obtain more granular, high-fidelity explanations. Experimental results on three real-world models on three publicly available rumour detection datasets demonstrate the effectiveness of our approach. Our work is a step towards increasing the robustness and trustworthiness of existing rumour detection work by providing more informative explanations.

