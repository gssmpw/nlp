In this section, we discuss the relation of our work to existing research in GNN explainability and text explainability.

\subsection{GNN Explainability}
Explainability techniques for GNNs can be broadly categorized into two groups: (i) methods adapted from Convolutional Neural Networks (CNNs) and (ii) methods specifically designed for GNNs. The first category includes gradient-based and decomposition-based methods such as LRP \cite{bach2015pixel}, Grad-CAM \cite{Selvaraju2017ICCV}, Excitation Backpropagation (EB) \cite{zhang2018top}, and Sensitivity Analysis (SA) \cite{gevrey2003review}, as well as perturbation-based approaches like LIME \cite{ribeiro2016should} and SHAP \cite{lundberg2017unified}. These techniques, originally developed for CNNs, generalize well to GNNs by treating graphs as lattice-shaped structures, where nodes represent pixels or features, and convolution filters act as subgraph kernels. However, these methods typically treat nodes like pixels with node features analogous to colour channels which fail to capture the nuance within the text represented by the node.
% However, their explanations are typically limited to node or edge-level attributions, which fail to capture the nuanced behaviour of GNNs in tasks like rumour detection.

The second category includes techniques specifically designed for GNNs, such as GNNExplainer \cite{ying2019gnnexplainer}, PGExplainer \cite{luo2020parameterized}, ZORRO \cite{funke2022zorro}, and GraphLIME \cite{huang2022graphlime}. These methods provide insights into node, edge, or subgraph-level attributions, offering explainability for graph-specific tasks like node classification or link prediction. However, their reliance on perturbation or surrogate models often limits their generalizability to unseen data, particularly in dynamic applications like rumour detection. Gradient-based methods, while more scalable, lack the granularity needed to interpret latent textual features represented in high-dimensional node embeddings.

Our framework, CT-LRP, addresses these limitations by extending GNN explainability to token-level resolution, overcoming the challenges of interpreting latent text features in rumor detection. By leveraging the strengths of gradient-based and decomposition-based methods, CT-LRP provides fine-grained insights that generalize effectively to unseen data while maintaining high fidelity.


%Explainability and interpretability techniques for GNNs can be broadly categorized into two groups: (i) methods adapted from Convolutional Neural Networks (CNNs) and (ii) methods specifically designed for GNNs. The first category includes gradient-based and decomposition-based techniques, such as LRP~\cite{bach2015pixel}, Grad-CAM~\cite{Selvaraju2017ICCV}, Excitation Backpropagation (EB) \cite{zhang2018top}, and Sensitivity Analysis (SA) \cite{gevrey2003review}. These techniques typically involve backpropagating modified gradients or relevance scores through the model to generate attribution maps that highlight the contribution of individual features—originally pixels in CNNs but nodes or edges in GNNs. Perturbation-based methods, including Local Interpretable Model-agnostic Explanations (LIME) \cite{ribeiro2016should} and SHapley Additive exPlanations (SHAP) \cite{lundberg2017unified}, have also been successfully adapted from CNNs to GNNs. In this context, CNNs can be conceptualized as a special case of GNNs, where an image is represented as a lattice-shaped graph, with each pixel and its three colour channels corresponding to a node with three feature dimensions. The convolution filters in CNNs are analogous to subgraph kernels in GNNs, where the height and width of the convolution kernel correspond to the number of hops in the subgraph. Thus, the explainability techniques developed for CNNs can be generalized to GNNs, particularly concerning node features. 

%The second category comprises methods specifically developed for GNNs, focusing on graph-specific tasks such as node classification and link prediction. These include perturbation and surrogate-based techniques like GNNExplainer \cite{ying2019gnnexplainer}, PGExplainer \cite{luo2020parameterized}, ZORRO \cite{funke2022zorro}, and GraphLIME \cite{huang2022graphlime}. These approaches are tailored to the unique challenges of explaining GNNs, providing insights into model behaviour at the node and edge levels.

%However, these methods are unable to adequately explain model behaviour in GNN-based rumour detection. While perturbation-based and surrogate-based methods which are commonly used to explain GNNs are powerful model-agnostic tools \cite{ying2019gnnexplainer, luo2020parameterized, funke2022zorro, huang2022graphlime} that explain GNNs at the node, edge or subgraph level, they often fail to generalize to unseen data instances. This limitation arises because these types of techniques ignore the model's internal learning mechanisms which are crucial for understanding model behaviour on new data. In the context of rumour detection, this lack of generalizability hinders our ability to accurately identify the causes of the model’s decisions on unseen examples, ultimately reducing the robustness of the model.

%Next, we consider gradient-based and decomposition-based methods. While these methods generally produce more generalizable explanations, they too have significant drawbacks. Techniques such as Grad-CAM and its variants typically provide explanations at the node level by analyzing model activations and features in higher-level node representations. However, these methods lack the granularity needed to fully capture the complex behaviour of rumour detection models. Other approaches that trace gradients or relevance back to the initial node features can produce feature attribution maps at the node level, which may suffice for non-latent features. However, when these features correspond to latent dimensions in the text embedding space, the explanations often lack interpretability. Consequently, the generated explanations tend to either highlight entire posts (represented by the corresponding nodes) or specific dimensions of node input features, which may not directly correspond to individual tokens in the original text.
%{Our framework which is focused on GNN-based rumour detection, extends existing GNN explainability beyond traditional node and edge level explanations and adapts them to the unique challenges of explaining rumour detection to overcome the limitations of existing work. We leverage the strengths of gradient-based and decomposition-based techniques, specifically their use of model internal mechanisms, and use them in the first step of our framework, which requires accurate node feature level attribution.

% \red{Roy: Add in comments on how our work related to the above-mentioned.}

%Explainability and interpretability techniques for GNNs can be divided into two categories: methods adapted from work on Convolutional Neural Networks (CNN) and original methods unique to GNNs. The first category of work includes gradient and decomposition-based techniques such as LRP \cite{bach2015pixel}, Grad-CAM \cite{Selvaraju2017ICCV}, Excitation Backpropagation (EB) \cite{zhang2018top}, and Sensitivity Analysis (SA) \cite{gevrey2003review}. These methods typically involved backpropagating some form of modified gradient or relevance quantity through the model to obtain an attribution map for the individual pixels. Perturbation-based methods such as Local Interpretable Model-agnostic Explanations (LIME) \cite{ribeiro2016should} and SHapley Additive exPlanations (SHAP) \cite{lundberg2017unified} have also been applied to CNNs successfully. CNNs can be seen as a special case of GNNs wherein the image is a lattice-shaped graph with each pixel and its three colour channels corresponding to a node with three feature dimensions. Convolution filters of CNNs are thus analogous to subgraph kernel learning wherein the number of hops in the subgraph corresponds to the height and width of the convolution kernel. Under this premise, each pixel, which is analogous to a node, within the receptive field of the convolution kernel can be seen as connected to the centre pixel (node) by an edge with a weight of one. Thus, the techniques used to explain CNNs can be generalised to explain GNNs in so far as node features are concerned.

%Original methods designed to explain GNNs mainly consist of perturbation and surrogate-based techniques which include GNNExplainer \cite{ying2019gnnexplainer}, PGExplainer \cite{luo2020parameterized}, ZORRO \cite{funke2022zorro} and GraphLIME \cite{huang2022graphlime}. These works are designed mainly to explain GNNs in the context of either node classification or link prediction which are graph-specific problems. These techniques are thus better at explaining model behaviour on the aforementioned tasks at the node and edge level.

\subsection{Text Explainability}
Explainability methods originally developed for text classification tasks can be categorized into gradient-based, decomposition-based, and perturbation-based approaches. Gradient-based and decomposition-based methods, including LRP \cite{bach2015pixel}, Saliency Maps \cite{simonyan2013deep}, and Guided Backpropagation \cite{springenberg2014striving}, trace backpropagated gradients or relevance scores to determine the contribution of individual tokens to the model's decision. Perturbation-based techniques such as LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified}, and Occlusion \cite{zeiler2014visualizing} identify influential tokens by substituting input elements and observing changes in model output. These methods often struggle with latent feature interpretability, particularly when applied to high-dimensional embeddings.

Our approach recasts the explainability problem for GNN-based rumor detection as a text explanation task. By combining GNN explainability methods to accurately attribute node features with text explainability techniques to generate token-level explanations, CT-LRP bridges the gap between node feature attribution and interpretable token-level insights. This hybrid approach ensures that latent text features are effectively explained, offering a novel solution to the challenges faced by existing methods.

%Many explainability techniques originally developed for CNNs have also been successfully applied to text classification tasks. These techniques can be broadly categorized into three main types: gradient-based, decomposition-based, and perturbation-based methods.

%Gradient-based and decomposition-based methods include LRP \cite{bach2015pixel}, Saliency Maps \cite{simonyan2013deep}, and Guided Backpropagation \cite{springenberg2014striving}. These approaches operate by modifying the backpropagated gradients or relevance scores within the network, effectively tracing the contribution of each token to the model's decision for the class of interest, similar to how they are applied in image processing.

%Perturbation-based methods, such as LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified}, and Occlusion \cite{zeiler2014visualizing}, have also been adapted for text classification. These methods typically involve perturbing the input by substituting selected tokens with a baseline value to observe changes in the model's output, thereby identifying the most influential tokens. Like their applications in CNNs and GNNs, these perturbation techniques often involve training locally linear models to approximate the complex decision boundaries of the classifier.

%A recent study evaluating the performance of various explainability techniques on text classification tasks across three benchmark datasets found that gradient-based methods consistently produced the most effective explanations across different architecture types \cite{atanasova2024diagnostic}.

%Our work which generates token-level explanations recasts the problem of explaining GNN-based rumour detection as a text explanation task. We leverage the techniques used in GNN explainability to obtain accurate node feature-level attribution that is used with existing techniques in text explainability to generate the token-level explanations, thus addressing the issue of node feature-level explanations lacking interpretability for latent features.

% \red{Roy: Add in comments on how our work related to the above-mentioned.}

%Many explainability techniques for CNNs were also successfully applied to text classification. These techniques are categorised into three main types: gradient-based, decomposition-based and perturbation-based. Gradient and decomposition-based methods include LRP \cite{bach2015pixel}, Saliency \cite{simonyan2013deep}, Guided Backpropagation \cite{springenberg2014striving}. Like in the case of images, these methods work by modifying the backpropagated gradient or relevance quantity through the network to each token representation for the class of interest. Perturbation-based methods such as LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified} and Occlusion \cite{zeiler2014visualizing} were also used on text classification models. Similarly to other applications on CNNs and GNNs, these methods train locally linear models or in the case of Occlusion, substitute selected input tokens with a baseline to measure the change in the model's output. A recent study in diagnosing the performance of explainability techniques on text classification on three benchmark datasets concluded that gradient-based methods produced the best explanations over three different architecture types. \cite{atanasova2024diagnostic}

\input{tables/notation}

\begin{figure*}[hbt!]
    \centering
    % \includegraphics[width=\linewidth]{figs/CT-LRP Framework v2.pdf}
    \includegraphics[width=\linewidth]{figs/CT-LRP_Framework_v3.1.pdf}
    \caption{Overview of the proposed CT-LRP framework showing the flow of information through the GNN and text embedding function. Inputs to the forward and backward pass are colour-coded in green, intermediate outputs in purple and final outputs in blue.}
    % \Description{A flowchart illustrating the flow of information through the GNN rumour detection model and text embedding function under the CT-LRP framework.}
    \label{fig:ct-lrp}
\end{figure*}