

    
\section{Algorithms}
\label{alg:generalizeformer}
In this section, we provide the algorithms for source training and test-time generalization in Algorithm~\ref{alg:GF1} and \ref{alg:GF2}. 

\begin{algorithm}[ht!]
\small
\caption{Training for GeneralizeFormer \\
{\textbf{Input:}} $\mathcal{S}=\left \{ D_{s} \right \}^{S}_{s=1}$: source domains with corresponding ${(\x_s, \y_s})$; $\btheta$: model parameters of backbone; $\bphi$: model parameters of Transformer;
$\mathcal{B}_{tr}$: batch size during training; 
$N_{iter}$: the number of iterations. \\
{\textbf{Output:}}
Learned $\btheta, \bphi$
}
\label{alg:GF1}
\begin{algorithmic}[1]
\FOR{\textit{iter} in $N_{iter}$}
\STATE \textcolor{gray}{{Mimicking the domain shifts}} \\ $\mathcal{T'}$ $\leftarrow$ Randomly Sample ($\left \{ D_{s} \right \}^{S}_{s=1}$, $t'$); 
\\
$\mathcal{S'}$ $\leftarrow$ $\left \{ D_{s} \right \}^{S}_{s=1}$ $\backslash$ $\mathcal{T'}$; 

\STATE Sample datapoints $\{(\mathbf{x}_{s'}^{(k)}, \mathbf{y}_{s'}^{(k)})\}_{k=1}^{\mathcal{B}_{tr}} \sim \mathcal{S'}$, $\{(\mathbf{x}_{t'}^{(k)}, \mathbf{y}_{t'}^{(k)})\}_{k=1}^{\mathcal{B}_{tr}} \sim \mathcal{T'}$.
\STATE \textbf{Meta-source stage:} 
\\
\STATE 
Obtain meta-source model by training with the cross-entropy loss ($\mathcal{L}_{\mathrm{CE}}$) on meta-source labels and predictions 
$\btheta_{s'} = \mathop{\min}\limits_{\btheta} \mathbb{E}_{(\x_{s'}, \y_{s'}) \in \mathcal{S}'} [\mathcal{L}_{\mathrm{CE}}(\x_{s'}, \y_{s'}; \btheta)]$
\STATE \textbf{Meta-generalization stage:} 
\STATE Calculate meta-target features by $\z_{t'} {=} f_{\btheta_{s'}}(\x_{t'})$
\STATE Calculate layer-wise gradients with unsupervised loss function by $\g_{t'}^l {=} {\partial \mathcal{L} (\x_{t'})} / {\partial \btheta^l_{s'}}$


\STATE Generate the meta-target batch norm and classifier parameters of each layer by \\
$\btheta^l_{t'} {=} \bphi(\btheta^l_{s'}, \z_{s'}, \g_{s'}^l), \forall l=1,2,\cdots, L$, \\



\STATE Optimize transformer $\bphi$ by 

$\bphi = \mathop{\min}\limits_{\bphi}  \mathbb{E}_{(\x_{t'}, \y_{t'})}[\mathcal{L}_{\mathrm{CE}}(\x_{t'}, \y_{t'};\btheta_{t'})], \btheta_{t'} = \{ \btheta^l_{t'} \}_{l=1}^L$

\ENDFOR
\end{algorithmic}
\end{algorithm}



\begin{algorithm}[ht!]
\small
\caption{Test-time Generalization by GeneralizeFormer \\
{\textbf{Input:}} $\mathcal{T}$: target domain with $N_t$  unlabeled samples ${\x_t}$; $\btheta_s, \bphi_s$: source trained model parameters;
$\mathcal{B}_{te}$: batch size for each online step at test time.
}
\label{alg:GF2}
\begin{algorithmic}[1]
\FOR{\textit{iter} in $(N_t / \mathcal{B}_{te})$}
\STATE Sample one batch of target samples from the target domain $\{(\mathbf{x}_{t}^{(k)}\}_{k=1}^{\mathcal{B}_{te}} \sim \mathcal{T}$.

\STATE Calculate meta-target features by $\z_{t} {=} f_{\btheta_{s}}(\x_{t})$
\STATE Calculate layer-wise gradients with unsupervised loss function by $\g_{t}^l {=} {\partial \mathcal{L} (\x_{t})} / {\partial \btheta^l_{s}}$

\STATE Generate the meta-target batch norm and classifier parameters of each layer by \\
$\btheta^l_{t} {=} \bphi(\btheta^l_{s}, \z_{t}, \g_{t}^l), \forall l=1,2,\cdots, L$, \\



\STATE Make predictions by $p(\y|\x,\btheta_{t}), \btheta_{t} = \{ \btheta^l_{t} \}_{l=1}^L$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Additional Implementation details}
We follow the training setup as \cite{iwasawa2021test} that includes dataset splits and hyperparameter selection for our method. We utilize Imagenet pretrained ResNet-18 and ResNet-50 models for all domain generalization datasets, which is the same as previous methods. In the main paper, ERM baseline refers to evaluating the source-trained model directly on the given target set without any model adjustment at test time \cite{gulrajani2020search}. 

We describe the training and test-time procedures in the algorithm section. We implement the lightweight $\bphi$ model with the PyTorch transformer encoder module and utilize only one GPU to run the experiments on ResNet-18. {Following the common convention in the literature, e.g. [26, 32, 61], we utilize the given annotations of different domains that are predefined in the common domain generalization datasets. }The only hyperparameter involved here is the number of layers that we have experimented with in Section~\ref{section:additional_results}. We utilized identical settings and hyperparameters in the main paper for all domain generalization benchmarks. We utilize the train domain validation selection method to obtain the model for test-time domain generalization same as \cite{iwasawa2021test}. 

For test-time generalization, we utilize a small batch of 20 samples per batch. We generate the target model parameters using the $\bphi$ model and do not perform any backpropagation on the source model, which helps in reducing our computational time, as shown in Section 4 of the main paper. We do not have any additional hyperparameters in our method and utilize PyTorch to implement the method. For ResNet-18 models, we require only one GPU and utilize NVIDIA 1080Ti. We conducted all the experiments using five different random seeds. We will release the code in the final version as a link to the public repository. \\

\noindent {\textbf{Architecture of Generalizeformer. }GeneralizeFormer utilizes the transformer-encoder module from Pytorch with 8 layers, which consists of multi-head attention modules and feedforward modules. 
The source parameters, target features, and gradients are all formatted to the same dimension as the source parameters (e.g., 512 for the last block for ResNet-18) and then concatenated and utilized as input tokens to the transformer. 
The attention is calculated between these inputs to enhance each other according to their relationships.
We use the output features of the source parameters as the generated target parameters, which therefore match the dimensions of the source ones. At inference, we directly replace the source parameters with the generated parameters (Section 3 of the main paper)}. \\













\noindent \textbf{Runtime Comparison. } To show the efficiency of our method, we provide comparisons of the runtime cost at both training and test time, as well as the memory usage during training.
Our method requires more training time and parameters while having the lowest time cost at test time compared with other test-time generalization methods. 
Due to the meta-generalization stage, the proposed method takes 9 hours for 10,000 iterations utilizing a ResNet-18 and an NVIDIA Tesla 1080Ti GPU, which is longer than the ERM baseline of 6.5 hours.
In addition, the ERM method based on ResNet-18 requires 11.18M memory. In contrast, our method requires 39M when utilizing the 8-layer transformer for parameter generation. 
Our method can also be implemented with 4-layer and 2-layer transformers, which consume 32\% and 48\% fewer memory with similar performance of 85.2\% and 84.9\% on PACS, respectively.


Moreover, we also provide the computational time comparisons during test-time generalization on different datasets (Table~\ref{table:runtime_ttdg}), which is more important for test-time methods. 
We consume less time than all alternative methods in Table \ref{table:runtime_ttdg} on four domain generalization datasets. This ability is ideal for real-world deployment scenarios. The proposed method even consumes less time on than the classifier adjustment method \cite{iwasawa2021test} that only updates the classifier.

\noindent\textbf{Datasets details. } As mentioned in the main paper, we perform the experiments on image classification problems and demonstrate its effectiveness on six datasets namely: \textit{PACS} \cite{li2017deeper}, \textit{VLCS} \cite{fang2013video}, \textit{Office-Home}\cite{venkateswara2017deep}, \textit{TerraIncognita} \cite{beery2018recognition}, \textit{Living-17} \cite{santurkar2020breeds}, \textit{Rotated MNIST} and \textit{Fashion MNIST} \cite{piratla2020efficient}. 
\textit{PACS} \cite{li2017deeper} consists of 9,991 samples, 7 classes, and 4 domains: Photo, Art-painting, Cartoon, and Sketch. 
\textit{VLCS} \cite{fang2013video} consists of 10,729 samples, 5 classes and 4 domains: Pascal, LabelMe, Caltech, and SUN. 
\textit{Office-Home} \cite{venkateswara2017deep} consists of 15,5000 images, 65 classes and 4 domains: Art, Clipart, Product, and Real-World. 
\textit{TerraIncognita} \cite{beery2018recognition} consists of 34,778 samples, 65 classes, and 4 domains:  Location 100, Location 38, Location 43, and Location 46. We followed \cite{li2017deeper} for training and validation split. 
We follow the ‘leave-one-out” protocol \cite{li2017deeper,carlucci2019domain} by evaluating the model on each target domain with the parameters trained on the other source domains. 
We utilize \textit{Living-17} \cite{santurkar2020breeds}, which contains 17 classes with subclasses and 39780 images in source while 1700 images in target. Our performance is reported on the target domain.
For MNIST and Fashion-MNIST, we utilize the \textit{rotated MNIST} and \textit{rotated Fashion-MNIST} and follow \cite{piratla2020efficient} where the images are rotated by different angles for different domains. 
We use the subsets with rotation angles from $15^\circ$ to $75^\circ$ in intervals of $15^\circ$ as five source domains, and images rotated by $0^\circ$ and $90^\circ$ as the target domains.






\begin{table}[ht!]
\centering

\resizebox{0.9\columnwidth}{!}{%
		\setlength\tabcolsep{4pt} 
\begin{tabular}{lllll}
\toprule

 & VLCS & PACS & Terra & OfficeHome  \\ \midrule
 Tent \cite{wang2021tent}   & 7m 28s & 3m 16s & 10m 34s & 7m 25s \\
 Tent \cite{wang2021tent} (BN)   & 2m 8s & 33s  & 2m 58s & 1m 57s\\
 SHOT \cite{liang2020we} & 8m 09s & 4m 22s  & 12m 40s &8m 38s \\
 TAST \cite{jang2022test} & 10m 34s & 9m 30s  & 26m 14s & 22m 24s \\
 T3A \cite{iwasawa2021test}    & 2m 09s & 33s & 2m 59s & 2m 15s \\
\textit{\textbf{This paper}}  & 47s & 20s & 52s & 44s  \\


\bottomrule

\end{tabular}
}
\caption{{\textbf{Computational time comparison on different datasets with ResNet-18 as a backbone network during test-time generalization.}} 
The proposed method has better overall time consumption than existing test-time adaptation and test-time domain generalization methods.
}
\label{table:runtime_ttdg}
\end{table}



\section{Additional results and discussion}
\label{section:additional_results}

\noindent {\textbf{Why GeneralizeFormer works. }}
{To achieve good performance in a target domain, obtaining target-specific model parameters is crucial. Existing fine-tuning methods approximate target parameters by MAP estimation with an unsupervised loss (Section 3 of the main paper). Since their approximation depends on the original parameter quality and the number of target samples, errors accumulate. Our method avoids this by directly inferring batch-specific parameter distributions for each target batch in a feedforward pass. By doing so, our method is more practical for scenarios where the number of test samples is small, the test tasks are unknown, and a specific model cannot be selected, as evident in Figure 4 of the main paper. } \\


\noindent \textcolor{black}{{\textbf{How Generalizeformer retains source data.}} \textcolor{black}{The motivation behind the method is online adaptation can lead to error accumulation and forgetting due to iterative backpropagations. To address this issue, we learn a transformer to directly generate the parameters for each target batch individually. Therefore, the generated parameters are specific to each target sample, without affecting other batches. The source-specific parameters can also be recalled by inferring model parameters using each source batch, therefore avoiding source forgetting. We have added this discussion to the appendix.}}  \\ \\
\textbf{Further clarification of adaptively generating layer-wise model parameters per sample.  }
Technically, we introduce a transformer for parameter generation, whose attention mechanism effectively aggregates useful knowledge in source parameters and target features to avoid information loss.
We further consider layer-wise gradients per target batch as input of the transformer, which indicates the relationships between each layer of the source parameters and each target batch. By doing so, the gradients guide model generation for each layer and for different target domains, batches, and even samples.
This reduces error transmission among target samples and layers while enhancing the generalization ability across samples and domain shifts. Meta-learning is utilized just to mimic domain shifts to learn the ability of model generation; we do not claim it as a contribution.\\

\noindent \textbf{Visualization of generated weights.}
In Fig.~\ref{fig:generated_weights}, we provide a visualization of the generated weights and real weights through filters for the photo domain PACS dataset. The filters obtained through the use of (a) generated weights are identical to the filters obtained through the use of (b) real weights. \\


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{fig/graph/activations_grid__generated.jpg}
        \caption{}
        \label{fig:weightsgen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{fig/graph/activations_grid_original.jpg}
        \caption{}
        \label{fig:realweights}
    \end{subfigure}
    \caption{{\textbf{Visualization of generated weights} on PACS. Each row visualizes a 28x28 filter from the batch norm layer for a sample image from the photo domain. We show the (a) Generated weights by GeneralizeFormer (b) Real weights.}}
    \vspace{-6mm}
    \label{fig:generated_weights}
\end{figure}


\begin{figure}[t]
    \vspace{5mm}
    \centering
    \includegraphics[width=0.90\linewidth]{fig/Adapt_Source.png}
    \caption{\textcolor{black}{\textbf{Avoiding source forgetting across adaptation steps of our method} with ResNet-18. Each line graph represents the accuracy on the source domains by utilizing the model which was adapted on the sketch domain of the PACS dataset.  }}
    \vspace{-2mm}
    \label{fig:source_forget}
    
\end{figure}




\noindent\textcolor{black}{\noindent\textbf{Avoiding source forgetting across steps. }
In Fig.~\ref{fig:source_forget}, we also provide the visualization of retaining the source information across adaptation steps. For this experiment, initially, at test-time, the model is adapted to the sketch domain of the PACS dataset. Next, the adapted model is re-evaluated on the source domains: photo, art-painting, and cartoon to evaluate the performance on the source domains. The conclusion is similar to the ablation study of avoiding source forgetting from the main paper, where our method retains the source data.} 


\noindent\textbf{Ablation of different inputs for $\bphi$ network.} 
As aforementioned in the methodology section, the $\bphi$ model utilizes the target features, source-trained parameters and layer gradients to generate the target parameters for test-time generalization. In Table~\ref{tab:ablate_inputs}, to show the benefits of utilizing these three inputs, we perform an ablation study by utilizing a subset of the inputs in each experiment. Notably, all inputs help in achieving the best performance. 
The source parameters provide the basic ability of feature extraction and classification learned during training. Without it, it is difficult for fast model generation in one feedforward pass (82.0\% on PACS). The target features are essential for tailoring the generated model to specific target data, otherwise, it will cause unfitness (82.7\%). Without the gradients, it is difficult to adaptively control the generation of parameters, leading to performance degradation (81.9\%). The integration of all these inputs results in a comprehensive approach, leading to an improvement of 85.5\% to effectively and adaptively generate the target-specific parameters.\\



\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccll}
\toprule
& \multicolumn{3}{c}{\textbf{Inputs}} &  \multicolumn{1}{c}{\textbf{}} \\
\cmidrule(lr){2-4} 
 & Target features & Layer gradients & Source parameters & {Mean}\\
 \midrule
ERM Baseline & &  & & 79.6 \\
 \midrule
\multirow{3}*{\textit{\textbf{This paper}}} & \ymark & \ymark  &  &  82.0\scriptsize{$\pm$0.3}\\


 &  & \ymark & \ymark  & \underline{82.7}\scriptsize{$\pm$0.3} \\
 & \ymark &  & \ymark  &  {81.9}\scriptsize{$\pm$0.3}\\

 & \ymark & \ymark & \ymark  & \textbf{85.5}\scriptsize{$\pm$0.2} \\
\bottomrule
\end{tabular}}
\vspace{1mm}
\caption{\textbf{Ablation of different inputs for $\bphi$ network} for ResNet-18 on \textit{PACS}. Utilizing all three inputs achieves the best results, followed by using the layer gradients and source parameters.
}
\label{tab:ablate_inputs}
\vspace{-3mm}
\end{table}

\noindent\textbf{Analyses for only generating parameters of Batch Normalization layers and classifiers.}
We generate only the BN and classifier parameters for computational efficiency since they are low dimensional with much fewer parameters.
Moreover, BN and classifiers have significant influences on domain shifts.
BN parameters affect the statistics of the features, which contain style or domain information \cite{huang2017arbitrary}. Previous methods like Tent also update BN parameters to handle domain shifts. Additionally, classifier parameters further handle the domain shifts at the semantic-level, as also evident in T3A~\cite{iwasawa2021test} and Xiao \etal \cite{xiao2022learning}. Overall, by generating BN and classifier parameters, we handle domain shifts across different feature levels in an efficient way.\\




\noindent {\textbf{Generating Batch norms at different levels. }
From Table 3 in the main paper, generating only the classifier achieves 84.0\% on PACS.
We also conduct experiments to generate the BN layer in different blocks, where we get 84.5\%, 84.8\%, and 84.9\% for generating blocks 5, 6, and 7, respectively. 
All these settings underperform GeneralizeFormer (85.5\%), showing the effectiveness of adaptive generation of different layers.}\\

\noindent\textbf{Performance without meta-learning.} 
We also investigate the effectiveness of the meta-learning strategy in the proposed method on PACS.
Without meta-learning, the performance with ResNet-18 degrades from 85.5\% to 84.7\%, while still performing better than ERM (79.6\%) and other baselines.\\



\noindent\textbf{Models without Batch Normalization layers.}
We generate parameters of both normalization and linear layers, where the former seems to be more important in ResNet-based models.
However, next to affine parameters of Batch Normalization layers, the generation of the linear layer also performs well, achieving 84.0\% and 65.7\% on PACS and Office-Home from Table 4 in the main paper. 
This indicates that the proposed method can also be extended to handle domain shifts within other model architectures without Batch Normalization layers, e.g., MLP-based models or Transformers akin to T3A \cite{iwasawa2021test}.\\





\noindent\textbf{Detailed results of limited batch sizes.} As shown in the main paper, we conducted experiments using limited batch sizes. We also performed the challenging single-sample generalization setting that widens its scope for deployment in real applications. In Table~\ref{table:ablate_batch_size}, we provide detailed results of small batch sizes ablation from the main paper. The conclusion is similar to the main paper, where our method performs better than Tent \cite{wang2021tent}, and the difference increases with batch sizes. For single sample, we are competitive to \cite{xiao2022learning} while achieving better performance than it for larger batch sizes. By generating sample-specific models, the proposed method can achieve generalization with limited information.\\


\begin{table}[t]

\centering
	\resizebox{\columnwidth}{!}{%
		\setlength\tabcolsep{8pt}
\begin{tabular}{llllll}
\toprule
~ & \textbf{Photo}    & \textbf{Art}         & \textbf{Cartoon}       & \textbf{Sketch}        & \textit{Mean}       \\ \midrule





Baseline & 94.1 & 78.0 & 73.1 & 73.3 & 79.6\scriptsize{$\pm$0.4} \\
\rowcolor{mColor2}
\hline
\multicolumn{6}{l}{\textit{Test batch size = 1}} \\ 

\quad Tent \cite{wang2021tent} & 84.6  & 65.1& 69.5 & 49.7& 67.2\scriptsize{$\pm$0.4}\\
\quad Xiao \etal \cite{xiao2022learning} &95.8  & 82.0& 79.7 & 78.9 & 84.1\scriptsize{$\pm$0.2}\\
\quad \textbf{\textit{This paper}} & 95.5 &83.4  &80.4 &74.9 & 83.6\scriptsize{$\pm$0.2} \\
\hline
\rowcolor{mColor2}
\multicolumn{6}{l}{\textit{Test batch size = 16}} \\ 
\quad Tent \cite{wang2021tent} & 93.6   & 80.2& 76.9 & 68.4 & 79.8\scriptsize{$\pm$0.3}\\
\quad Xiao \etal \cite{xiao2022learning} &96.1  & 82.3 & 80.8 & 78.6 & 84.5\scriptsize{$\pm$0.2}\\
\quad \textbf{\textit{This paper}} & 96.4&82.0  &82.7 &74.0 & 83.8\scriptsize{$\pm$0.2} \\
\rowcolor{mColor2}
\hline
\multicolumn{6}{l}{\textit{Test batch size = 64}} \\ 
\quad Tent \cite{wang2021tent} & 96.0   & 81.9 & 80.3 & 75.9  & 83.5\scriptsize{$\pm$0.4} \\
\quad Xiao \etal \cite{xiao2022learning} &96.0  & 82.5 & 81.3 & 78.8 & 84.7\scriptsize{$\pm$0.2}\\

\quad \textbf{\textit{This paper}} &96.8 & 84.5& 83.6& 76.3 &85.3\scriptsize{$\pm$0.2}\\
\hline
\rowcolor{mColor2}
\multicolumn{6}{l}{\textit{Test batch size = 128}} \\ 
\quad Tent \cite{wang2021tent} & 97.2  & 84.9& 81.1& 76.8 & 85.0\scriptsize{$\pm$0.5}\\


\quad Xiao \etal \cite{xiao2022learning} &96.2  & 83.2 & 82.3 & 79.0 & 85.2\scriptsize{$\pm$0.2}\\

\quad \textbf{\textit{This paper}} &97.1 &85.7 & 85.2 &  76.9 & 86.2\scriptsize{$\pm$0.2} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Detailed results of limited batch sizes.} GeneralizeFormer performs better than Tent \cite{wang2021tent} with different batch sizes. The proposed method achieves competitive results with \cite{xiao2022learning} for small batch sizes and outperforms it on larger batch sizes. 
} 
\label{table:ablate_batch_size}

\end{table}


\noindent\textbf{Different losses for gradient information.}
As mentioned, the method can utilize different unsupervised losses for gradient information. In Table~\ref{table:ab_loss}, we utilize different unsupervised based losses such as \cite{zhang2021memo} and loss through pseudo labeling. Notably, unsupervised entropy minimization, which is the default loss function, performs well. This study shows the versatility of the proposed method, such that it can integrate different losses. Consequently, the efficacy and applicability of the method may be further improved by utilizing different unsupervised loss functions in the future. 





\begin{table}[h]
\centering
\vspace{-1mm}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccl}
\toprule
Strategies & Photo & Art & Cartoon & Sketch & Mean\\
 \midrule
 Memo \cite{zhang2021memo} &  96.2 & 82.1 & 81.5 & 70.0 & 82.5\scriptsize{$\pm$0.4} \\

 Pseudo labels   & 96.6 &  80.4 & 82.7 & 75.2  & 83.7\scriptsize{$\pm$0.3} \\
 
 

 Entropy Minimization &  96.9 & 85.0 & 83.3 & 76.7 & 85.5\scriptsize{$\pm$0.2} \\

 
\bottomrule
\end{tabular}}
\vspace{1mm}
\caption{\textbf{Different losses for gradient information} for ResNet-18 on PACS dataset. The proposed method can make use of different losses for the gradient information to achieve good performance. We utilize entropy minimization as the default for our experiments.}
\label{table:ab_loss}
\vspace{-4mm}
\end{table}

