 \section{Related Work}


\noindent
\textbf{Domain adaptation and domain generalization.}
Distribution shifts have been widely investigated in domain adaptation \cite{long2015learning, wilson2020survey, you2019universal} and domain generalization methods \cite{arjovsky2019invariant, zhou2022domain, wang2022generalizing}.
Domain adaptation deals with the distribution shifts between training and test data by assuming the accessible labeled or unlabeled target samples during training \cite{ben2006analysis, farahani2021brief, csurka2017domain, guo2017calibration}. 
However, in real applications, the target data is usually unknown during training \cite{zhao2020domain}.
To solve this problem, domain generalization is proposed to train the model only on source domains and evaluate it on unseen target domains \cite{muandet2013domain, li2018domain, dou2019domain, gulrajani2020search, xiao2021bit, park2023test}. 
The problem of domain generalization is the lack of target information of the source-trained model, which may lead to the adaptivity gap \cite{dubey2021adaptive}.
Recently, source-free domain adaptation has emerged \cite{liang2020we, kundu2022balancing, wang2022exploring, roy2022uncertainty, fang2022source, jing2022variational, yu2023comprehensive, thopalli2023target} where source model is adapted to target domains at test time. 
Our test-time domain generalization setting is related to the above settings, where we train the model on several source domains as domain generalization. 
Rather than adapting the model to the entire target domain, we achieve \blue{adaptation} at each test step.

\noindent
\textbf{Test-time adaptation and generalization.}
Test-time adaptation is also an emerging method for handling distribution shifts at test time \cite{sun2020test, wang2021tent, liu2021ttt++, gao2022back, niu2023towards, sun2022dynamic, liu2024cross, yi2023source, zhang2023domainadaptor}. These methods usually train the model on a single source distribution and achieve adaptation at test time in an online manner, focusing on covariate shifts such as corruptions  \cite{wang2021tent, goyal2022test, zhang2021memo, niu2022efficient}.
The idea has also been applied to the domain generalization setting, where multiple source domains are available during training  \cite{du2020metanorm, li2018learning, xiao2023energy, jang2022test, zhang2023adanpc, ambekar2024variational, ambekar2023variational_iclr}. 
We refer to this setting as test-time domain generalization.
One commonly used test-time method is fine-tuning the source-trained model with an unsupervised loss like entropy minimization \cite{wang2021tent, zhang2021memo, zhou2021bayesian, goyal2022test, zhang2023domainadaptor,niu2022efficient}. As an alternative, changing the statistics of the Batch Normalization layers is also widely utilized \cite{schneider2020improving, du2020metanorm, you2021test, hu2021mixnorm, gong2022note,lim2023ttn}.
Another method for test-time adaptation and generalization is classifier adjustment \cite{iwasawa2021test, xiao2022learning, zhang2023adanpc}, where the prototypes or the last fully-connected layer of the model is adjusted at test time, without fine-tuning.
Our method focuses on the test-time domain generalization setting.
Lee \etal \cite{lee2022surgical} showed that fine-tuning different layers benefits different distribution shifts.
In contrast to the previous methods, we propose to adaptively generate different layers parameters according to each batch of target samples, which avoids fine-tuning and adaptively addresses different distribution shifts.

\noindent
\textbf{Model generation networks.} 
The idea of using neural networks to generate model weights was introduced by Hypernetworks \cite{ha2016hypernetworks}. 
This approach is usually used for compressing large models into smaller ones. Recently, the concept of hypernetworks to generate compressed networks has been explored by \cite{zhmoginov2022hypertransformer} to generate weights for models using task descriptions. 
The techniques are also utilized in natural language processing tasks \cite{pilault2020conditionally, mahabadi2021parameter} and \blue{neural representations~\cite{ashkenazi2022nern}.}
Different from these methods, we utilize transformers to generate the layer-wise target model parameters according to the source model, target features, and gradient information. The model generation is meta-learned to generalize to unseen target during source training.
























            







