\section{Introduction}

To counter the performance degradation of deep learning algorithms when test data strays from the training data distribution, the framework of test-time adaptation emerged \cite{wang2021tent, iwasawa2021test, zhang2023adanpc}. In test-time adaptation \cite{xiao2024beyond}, one trains a model only on source domains and adapts the source-trained model during inference on (unseen) target data. Various methods are proposed for adaptation to a single source distribution, as typical for image corruption \cite{sun2020test, wang2021tent, lim2023ttn}, and for multiple source distributions, as common in domain generalization \cite{iwasawa2021test, xiao2022learning, zhang2023adanpc}. In this paper, we focus on the latter scenario, referring to it as test-time domain generalization.


\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{fig/Fig1_mod2.pdf}
\caption{\textbf{Illustration of test-time generalization methods.} (a) Fine-tuning methods update the model online with large batches of samples. 
(b) Classifier adjustment methods feedforwardly update the classifier according to the test data. 
(c) Our method adaptively generates target-specific parameters in different layers according to the target distribution, enabling it to handle various distribution shifts without fine-tuning. 
}
\label{fig1:all}
\vspace{-4mm}
\end{figure}

Current test-time generalization methods focus on fine-tuning the model with unsupervised loss functions on the target data \cite{wang2021tent,goyal2022test} or adjusting the linear classifiers during inference \cite{iwasawa2021test, zhang2023adanpc}. 
However, \blue{these methods utilize batches of target data at the same time \cite{liang2020we, kundu2022balancing} or in an online manner \cite{wang2021tent}.}
The alternative of classifier adjustment only considers the distribution shifts during classification, but ignores shifts resulting from feature extraction \cite{zhen_learning, lee2022surgical}. 
Moreover, these methods rely on online updates of model parameters on each specific target distribution and tend to forget the discriminative ability of previous source distributions, which makes them struggle in complex or dynamic scenarios \cite{yuan2023robust, zhang2023adanpc}.
Recently, Lee \etal \cite{lee2022surgical} propose to handle different distribution shifts by \emph{manually} selecting and fine-tuning layers, which, however, still requires different selection and fine-tuning operations for different specific target distributions.
Building on these problems and observations, we propose to adaptively generate target-specific parameters in different model layers to handle various distribution shifts without re-training the parameters (see also Fig. \ref{fig1:all}).

We propose \textit{GeneralizeFormer}, a transformer that generates target-specific layer-wise model parameters for each target batch at test-time. 
The target parameters are generated from the source-trained parameters, the target features, and the layer-wise gradient information.
With the layer-wise generation of the target parameters, the method can adaptively handle distribution shifts in different feature levels without manually selecting and fine-tuning parameters.
To enable model generation ability across distribution shifts, we train the transformer under a meta-learning strategy. At test time, GeneralizeFormer achieves \blue{adaptation} without online optimization of the model, making it especially effective in dynamic scenarios that suffer from multiple target distributions. This also helps the model to preserve the source model information and avoid forgetting after model \blue{adaptation}. To reduce the computational and time costs for \blue{adaptation}, we fix the parameters of the convolutional layers while generating the parameters of all Batch Normalization layers and that of the classifiers in the network.
Experiments on six domain generalization benchmarks and various distribution shifts show the ability of our method to handle source forgetting, dynamic scenarios, and multiple distribution shifts.
