\section{Experiments}


\subsection{Datasets and implementation details}

\vspace{1mm}
\noindent
\textbf{Six datasets.} 
We perform the experiments on image classification problems and demonstrate its effectiveness on six datasets namely: \textit{PACS} \cite{li2017deeper}, \textit{VLCS} \cite{fang2013video}, \textit{Office-Home}\cite{venkateswara2017deep}, \textit{TerraIncognita} \cite{beery2018recognition}, \textit{Living-17} \cite{santurkar2020breeds}, \textit{Rotated MNIST} and \textit{Fashion MNIST} \cite{piratla2020efficient} and provide details in supplementary. 




\vspace{2mm}
\noindent
\textbf{Implementation details.}
We evaluate on the online test-time domain generalization setting defined by Iwasawa and Matsuo \cite{iwasawa2021test}, where the data at test-time is iteratively incremented and we update the model per target batch externally through our $\bphi$ without backpropagation of the source trained model. 
The ERM baseline refers to directly evaluating the source-trained model on the target set \cite{gulrajani2020search}. 
\textcolor{black}{We implement the lightweight $\bphi$ model by the PyTorch transformer encoder module with 8 layers.
We also tried 4-layer and 2-layer transformers, which consume fewer memory with similar performance as shown in the supplemental material.}
\pink{For the transformer, we train it for 10,000 iterations alongside the ResNet. The source parameters, target features, and gradients are all formatted to the same dimension as the source parameters (e.g., 512 for the last block for ResNet-18) and then concatenated and utilized as input tokens to the transformer. 
The attention is calculated between these inputs to enhance each other according to their relationships.
We use the output features of the source parameters as the generated target parameters, which therefore match the dimensions of the source ones.}
For source training, we utilize a learning rate of $1e-4$ with Adam optimizer for the ResNet18 and ResNet-50 models (pre-trained on Imagenet) and the $\bphi$ model with a batch size of 64 during training. The hyperparameters for the best model during source training have been selected by the validation set following \cite{iwasawa2021test,gulrajani2020search}. \textcolor{black}{For test-time generalization, we utilize a small batch size of 20 samples. We will show in the ablations that our method also performs well with smaller batches. We generate the target model parameters by passing the inputs to the lightweight transformer encoder $\bphi$ without any backpropagation on the backbone model.} These new weights are directly assigned to the source-trained model. There are no additional hyperparameters involved. We run all experiments with five different random seeds. We have provided additional implementation details and our algorithm in the supplementary.


\input{tex_files/sections/table1}

\subsection{Results on different distribution shifts}


Due to adaptively generating the parameters of different layers in the model, GeneralizeFormer is able to handle various distribution shifts at different levels. We demonstrate this ability on the input level, output level, and feature level.

\noindent
\textbf{Input-level shift.} 
First, we conduct experiments on the common domain generalization setting, where the domain shifts between the source and target domains mainly exist in the input images such as different image styles. Thus, we treat these domain shifts as the input-level distribution shifts. 

We report the averaged results per dataset based on both ResNet-18 and ResNet-50 in Table~\ref{table:all_datasets}.
Our method outperforms the ERM baseline and the standard domain generalization methods on all datasets and backbones.
Moreover, compared with the test-time adaptation Tent and generalization methods \cite{iwasawa2021test, chen2023improved, xiao2022learning, jang2022test}, our method also achieves the best performance on all datasets based on ResNet-18, and top-2 performance based on ResNet-50, demonstrating the effectiveness of the method on the input-level shifts. 
\textcolor{black}{We also compare with generalization methods that only update the BN statistics at test time, without changing the parameters, e.g., alpha-BN \cite{you2021test} and MetaNorm \cite{du2020metanorm}. Based on ResNet-18, we perform comparably on PACS and outperform the two methods on Office-Home by 2.8\% and 2.5\%, respectively.}



\input{tables/table2}

\noindent
\textbf{Output-level shift.}
In addition to the input-level shifts in domain generalization datasets, we also conduct experiments with output-level distribution shifts. 
The output-level shifts are designed by introducing category shifts \cite{shen2022association, park2023test} in the domain generalization setting.
Specifically, we assign samples of different classes for different source domains during training.
The experiments are conducted on \textit{PACS}, where data from three source domains and seven classes are available during training. 
We select 3 of the 7 classes from the first source domain, the other 2 classes from the second source domain, and the remaining 2 classes from the last source domain as the training data.
\textcolor{black}{Therefore, the source domains have different label spaces (3, 2, 2 classes) from the target domain (entire label space with 7 classes), leading to spurious correlations between domains and classes during training, which does not exist at test time.
After training on the spurious correlated source domains, the model is then evaluated on the target domain with all 7 classes.}
Thus, there are both input-level shifts across domains and output-level shifts across categories at test time.

We compare to existing out-of-distribution generalization methods \cite{zhou2021mixstyle, zhang2022exact} and test-time adaptation methods like Tent under this setting. The results are in Table~\ref{table:dist_shift}.
Our method again achieves the best overall performance and outperforms the other alternatives on two of the four target domains.
Despite not observing all categories in each source domain, our method adaptively generates the specific layer parameters to jointly address the input and output shifts at test time. 


\noindent
\textbf{Feature level shifts.}
To demonstrate the ability of the method on different levels of distribution shifts, apart from the input-level and output-level shifts, we further conduct experiments on the feature-level shifts.
Following \cite{garg2023rlsbench}, we conduct the experiments on Living-17 \cite{santurkar2020breeds}, where the source and target distributions consist of different subpopulations of the same superclasses. 
Since the input and label space are the same while the shifts are the features for prediction, we treat the distribution shift as the feature-level shift.
We report the results based on ResNet-18 in Table~\ref{tab:feautre_level} and compare our method with alternative methods. As shown in the table, our method again outperforms the other methods on feature-level shifts.

Overall, %
the results demonstrate that by adaptively generating the layer-wise model parameters for different test distributions, the proposed method is able to handle various distribution shifts well.

\begin{figure*}[t]
\centering 
\centerline{   
    \includegraphics[width=2\columnwidth]{fig/GF_Sameer-Vis_Fig.drawio.png}} 
    \vspace{-1em}
\caption{\textbf{Visualizations of adaptive model generation} \blue{using ResNet-18 on PACS. }(a) For input level shifts, based on the samples, our method focuses on generating the low-level layers. (b) Similarly, for feature-level shifts that consist of subpopulations, our method mainly changes the middle layers. (c) For the output level shifts, due to category shifts our method changes more on the high-level layers while also generating the initial layers since there are also input-level shifts in this setting. }
\vspace{-4mm}
\label{fig:visualization}  
\end{figure*}

\noindent
\textbf{Visualizations of adaptive model generation.} 
To further demonstrate the adaptive generation of the method for different distribution shifts, we also provide some visualizations of the differences between the source-trained model parameters and the generated ones. We calculate the layer-wise distance between the generated target-specific model parameters and the original source-trained one and compare the values with the other layers.
As shown in the first column in Fig.~\ref{fig:visualization}, the model generated across input-level shifts mainly changes the bottom layers. 
Other layers are also changed but not obviously.
For the feature-level distribution shifts (Fig.~\ref{fig:visualization} second column), the parameters in the middle layers change more obviously. 
In the last column, we find that the top layers generated by our method change more compared with the source-trained model. 
The low-level layers also show differences from the source model, which is because we also include input-shifts in this setting. 
The visualizations further demonstrate that our method adaptively generates the model parameters according to different distribution shifts.


\subsection{Ablation studies}
\label{sec: ablation}


\begin{table}[t]
\centering

\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{lccll}
\toprule
& \multicolumn{2}{c}{\textbf{Generated layers}} &  \multicolumn{2}{c}{\textbf{Mean accuracy}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Methods} & BN & Classifier & \textbf{PACS} & \textbf{Office-Home}\\
 \midrule
ERM Baseline & &  & 79.6 &  61.0 \\
 \midrule
\multirow{3}*{\textit{\textbf{This paper}}} & \ymark &  & \underline{85.3} \scriptsize{$\pm$0.2} &  64.8 \scriptsize{$\pm$0.3}\\
 & & \ymark & 84.0 \scriptsize{$\pm$0.2}& \underline{65.7} \scriptsize{$\pm$0.3} \\
 & \ymark & \ymark &  \textbf{85.5} \scriptsize{$\pm$0.2} &  \textbf{66.0} \scriptsize{$\pm$0.3}\\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{\textbf{Benefits of generating BN layers and classifiers} for ResNet-18 on \textit{PACS} and \textit{Office-Home}. Generating the parameters of both BN layers and classifiers improves the performance,  their combination achieves the best results. 
}
\label{tab:ab_fc}
\vspace{-6mm}
\end{table}

\noindent
\textbf{Benefits of generating different layers.}
We use GeneralizeFormer to generate the parameters of both Batch Normalization layers and classifiers for test-time domain generalization.
To show the benefits of both settings, we conduct experiments of generating the parameters of different layers separately.
We report results for a ResNet-18 on \textit{PACS} and \textit{Office-Home} in Table~\ref{tab:ab_fc}.
Generating BN and classifier parameters both improve the performance of the ERM baseline on the two datasets. 
Notably, generating BN parameters performs better than the classifiers on \textit{PACS} but worse on \textit{Office-Home}. 
The reason can be that \textit{Office-Home} contains more classes (65) than \textit{PACS} (7), which requires more adjustment on the classifier for generalization.
The results also demonstrate the benefits of generalizing different layers for different datasets or distribution shifts. 
Moreover, performance is further improved by adaptively generating both BN and classifier parameters, demonstrating the effectiveness of the proposed method. 
We report the results of adaptively generating both for the remaining ablations.







\begin{figure*}[t]
\centering 
\centerline{   
    \includegraphics[width=0.6\columnwidth]{fig/smallbatch2.png} ~
    \includegraphics[width=0.66\columnwidth]{fig/forgetting_mod.png} ~
    \includegraphics[width=0.66\columnwidth]{fig/multi-distribution_mod.png} ~
} 
\vspace{-4mm}
\caption{\textbf{Generalization in different scenarios: (a) small batch sizes, (b) source forgetting, and (c) multiple target distributions.} Our method performs well in small batch sizes and complex scenarios with multiple distributions. The method also avoids source forgetting. 
}
\vspace{-4mm}
\label{fig:investigate-limdata_gf}  
\end{figure*} 

\vspace{0.5mm}
\noindent
\textbf{Generalization with small batch sizes.}
GeneralizeFormer learns the ability to adaptively generate the specific model for each target batch at test time. 
The requirement for batch sizes is more relaxed than the common fine-tuning and target statistics re-calculating methods, broadening the deployment in real applications. 
To demonstrate the effectiveness on small batch sizes, we compare with alternatives with different batch sizes. 
As shown in Fig.~\ref{fig:investigate-limdata_gf} (a), our method performs better than the common fine-tuning method Tent and the difference increases with small batch sizes. 
The proposed method can also handle the more challenging single-sample generalization problem with acceptable performance degradation while the performance of Tent collapses. Our performance on single samples is competitive with \cite{xiao2022learning}, which meta-learns the single-sample generalization ability.
Moreover, our method performs better with larger batch sizes while \cite{xiao2022learning} tends to saturate.
With the ability to generate sample-specific models, the proposed method can achieve generalization with very limited information and perform well in various complex scenarios.
We provide detailed results in the supplemental material. 








\noindent
\textbf{GeneralizeFormer avoids source forgetting.}
Common fine-tuning methods update their model with successive steps to each batch of data, which can lead to catastrophic forgetting of source data. 
This problem is particularly detrimental during real-world deployment where the model is expected to maintain good performance on the source domains during generalization.
By contrast, GeneralizeFormer learns to directly generate target-specific model parameters for each test batch, so the information of the source distributions is not forgotten after \blue{adaptation}.
To demonstrate this, we conduct source-forgetting experiments on \textit{PACS}, \blue{where the model is re-evaluated on the source domains after adapting to the target domain.}
As shown in Fig.~\ref{fig:investigate-limdata_gf} (b), the common fine-tuning methods Tent and classifier adjustment method T3A \cite{iwasawa2021test} forget some knowledge learned from the source domains during the \blue{adaptation} at test-time, leading to performance decline on the source data. 
By contrast, our method directly generates the target-specific model parameters, without changing the knowledge in the source-trained model. \textcolor{black}{Hence, our method obtains similar performance (\{99.2\%, 99.6\%, 99.5\%, 99.6\%\}) as the source-trained models (\{99.7\%, 99.7\%, 99.8\%, 99.7\%\}) on four domains of PACS. 
}
This enhances the model's overall generalization performance in real-world deployment scenarios. 

\noindent
\textbf{Generalization for multiple target distributions.}
GeneralizeFormer adaptively generates model parameters for each batch of target data at test time. 
This enables the model to handle various target domains at test time, as is common in dynamic real-world scenarios. To show the benefits of the method in dynamic scenarios, we further conduct experiments with multiple target domains at test time on Rotated MNIST, where we use source domains with rotated angles $0^\circ$, $15^\circ$, $75^\circ$, and $90^\circ$, $30^\circ$, $45^\circ$, and $60^\circ$ as \blue{unseen target domains and unknown domain ids.} 
We report the results in both single-target and multiple-target settings in Fig.~\ref{fig:investigate-limdata_gf} (c).
In the single-target setting, the source-trained model is evaluated on each single target domain and the final accuracy is the average of the single-target results. 
In the multiple-target setting, the model is evaluated on all the target domains simultaneously, where the test samples would come from any target domain during test-time generalization.
As shown in Fig.~\ref{fig:investigate-limdata_gf} (c), the common fine-tuning method Tent performs well under the single-target setting. However, its performance under the multiple-target setting is worse, even comparable with the ERM baseline.
The reason is that the target samples of different domains contain conflicting information, which harms the online adjustment of the model.
Xiao \etal \cite{xiao2022learning} performs more robustly on the multiple-target setting since their model is trained to generalize on each target sample.
Nevertheless, our method achieves the best performance on both settings since we learn to adaptively generate model parameters according to the test-time target samples.









 
 










 









 


















 













\input{tables/table4}



\noindent
\textbf{Inference time for generalization.} 
We provide the time cost of generalization on the target set at test time and compare it with other test-time methods.
The experiments are conducted on PACS based on ResNet-18.
As shown in Table~\ref{tab:inf_time_new}, GeneralizeFormer consumes less time than the fine-tuning-based methods Tent and \cite{liang2020we}. %
The proposed method achieves even slightly less time cost than the classifier adjustment method~\cite{iwasawa2021test}, which adjusts the classifier online at test time. 
\textcolor{black}{We also conduct experiments to generate all parameters of the model on PACS with a ResNet-18.  The method consumes 1m 10s, which is more computationally expensive than generating BN and classifier parameters (20s as shown in Table \ref{tab:inf_time_new}).}
We provide additional computational time comparisons in the supplemental materials.
