\section{Related Work}
\paragraph{Multi-Task Learning.}
To effectively boost information sharing and alleviate task conflict, significant efforts have been invested in multi-task architecture designs. Existing techniques can be categorized into different parameter sharing methods ____ and optimization strategies ____.
Multi-task learning has also
been widely applied to recommender systems and achieved substantial improvement ____.
However, in practice, it is difficult to ensure that the performance of each task improves after multi-task learning, especially when the number of tasks continues to increase ____. 
Thus, it's often necessary to carefully design the model structure based on the data proportions and the task relationships.
This has also led to the isolated states among different recommendation problems, such as multi-scenario modeling ____, multi-objective modeling ____, long-tail recommendation ____, etc. 
In addition, search tasks can be viewed as a specialized type of recommendation with explicit query constraints. 
Due to significant distribution discrepancies and limited model capacity, traditional recommendation models cannot easily simultaneously handle scenarios both including and excluding explicit inputs ____.
In this paper, we propose to treat all the above problems as multi-task learning problems and address them simultaneously by LLMs, in an end-to-end and fully parameter-sharing manner.

\vspace{-10pt}
\paragraph{LLMs for Recommendation.}
Large Language Models (LLMs)  have demonstrated significant capabilities in natural language processing ____, encouraging researchers to explore their application in recommender systems.
Recent approaches  treat  recommendation tasks as natural language tasks, generating recommendations directly through prompting and in-context learning ____. 
However, in real systems, users typically have hundreds or even millions of behaviors, leading to at least tens of thousands of text tokens, which increases inference costs and decreases LLM performance.
Thus some methods introduce a hierarchical structure that encodes each item's text or image information into item representations and feeds them into LLMs to generate a high-level user representation ____. Yet these representations still suffer from poor discriminability and lead to low performance in industrial applications. 
Another approach employs traditional ID embeddings to represent items ____, yet LLMs often struggle to interpret the intrinsic meaning of these embeddings. 
This limitation hinders the ability of LLMs to effectively adapt recommendations based on input prompts, thus degenerating their versatility. 
In this paper,  we integrate the ID embeddings and text embedding 
within a single LLM to balance performance and versatility.