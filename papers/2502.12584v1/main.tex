\documentclass{article}

\usepackage{graphicx}%
% \usepackage[dvipdfmx]{graphicx} % fixing bounding box error for arxiv latex
\usepackage{multirow}%
\usepackage{makecell}
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
% \usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%
\usepackage{array}
\usepackage{caption}
\usepackage{pdflscape}
% \usepackage{natbib}
\usepackage[numbers]{natbib} % fixing error for submitting in arxiv.
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage[draft]{fixme}

\usepackage{mathtools}
\usepackage[margin=1in]{geometry}

\usepackage{bbm}
\usepackage[T1]{fontenc}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% \usepackage{bbold}
\fxsetup{inline,nomargin,theme=color}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\definecolor{todo-color}{HTML}{ff3232}
\newcommand{\todo}[1]{{\color{todo-color}[#1]}}
\newcommand\todocite{{\color{red}{CITE}}\xspace}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\tba}{\textcolor{red}{To Be Added }}

\newcommand{\ours}{ZeroMatch } % our work name?
\newcommand{\ourmethod}{ZMT } % our work name?

% used for conditioning - icml vs arxiv
\def \isarxiv {Arxiv}


\title{Enhancing Semi-supervised Learning with Noisy Zero-shot Pseudolabels}
\author{Jichan Chung$^{1}$ and Irene Y. Chen$^{1,2}$}
\date{$^1$University of California, Berkeley, $^2$University of California, San Francisco}

\begin{document}

\maketitle

\begin{abstract}
% Semi-supervised learning (SSL) aims to leverage a small amount of labeled data with a large pool of unlabeled data, addressing the challenges of labeling costs in various domains. 
% Recent advancements in foundation models have popularized zero-shot methods, with some works proposing the integration of these methods into SSL to generate pseudo-labels for the unlabeled data. 
% However, we observe that the unreliability of zero-shot responses often diminishes the effectiveness of these approaches, leading to suboptimal performance compared to traditional SSL methods that do not use such pseudo-labels. 
% In this paper, we introduce ZMT (Zero-shot Multi-Task Learning), a novel multi-task learning-based framework that combines both zero-shot pseudo-labels and unsupervised features from modern SSL techniques. 
% We evaluate ZMT across a range of datasets spanning vision, language, and audio modalities. Our empirical results show that ZMT consistently improves performance in scenarios with varying pseudo-label quality, outperforming baselines when pseudo-labels are reliable, while maintaining the effectiveness of standard SSL methods when pseudo-labels are less reliable.

Semi-supervised learning (SSL) leverages limited labeled data alongside abundant unlabeled data to address labeling costs in machine learning. While recent foundation models enable zero-shot inference, attempts to integrate these capabilities into SSL through pseudo-labeling have shown mixed results due to unreliable zero-shot predictions.
We present ZMT (Zero-Shot Multi-Task Learning), a framework that jointly optimizes zero-shot pseudo-labels and unsupervised representation learning objectives from contemporary SSL approaches. Our method introduces a multi-task learning-based mechanism that incorporates pseudo-labels while ensuring robustness to varying pseudo-label quality.
Experiments across 8 datasets in vision, language, and audio domains demonstrate that ZMT reduces error by up to 56\% compared to traditional SSL methods, with particularly compelling results when pseudo-labels are noisy and unreliable. ZMT represents a significant step toward making semi-supervised learning more effective and accessible in resource-constrained environments.
% Our analysis reveals that the effectiveness stems from the complementary nature of coarse-grained zero-shot predictions and fine-grained SSL objectives.
\end{abstract}



\section{Introduction}
\label{intro}
% Foundation model's inference capabilities are known to be useful in many cases.

% Its in-context learning capabilities are known to be useful for some tasks, but it is not perfect. Often it can fail.

% In semi-supervised learning, labels are costly.

% We consider semi-supervised learning problem with pseudo-labels from in-context learning with foundation models.

% To mitigate foundation model's inaccurate pseudo-label, we select unlabeled samples that are likely to have correct pseudo-label, based on pseudo-label's accuracy statistics on labeled samples.
% \tba

The growing scale of machine learning applications has made data labeling costs a critical bottleneck in deploying ML systems~\cite{northcutt1pervasive,sun2017revisiting,shen2024data}. Semi-supervised learning (SSL) addresses this challenge by leveraging unlabeled data alongside limited labeled examples~\cite{tarvainen2017mean}. Traditional SSL approaches like pseudo-labeling and consistency regularization have demonstrated strong performance across domains, particularly in computer vision and natural language processing~\cite{sohn2020fixmatch,laine2022temporal,tarvainen2017mean}.

Recent advances in foundation models have enabled zero-shot inference on novel tasks without task-specific training~\cite{brown2020language,liangholistic}. These models can generate predictions for unseen tasks by leveraging their pre-trained knowledge, offering a promising direction for reducing labeling requirements. Several works have proposed integrating these zero-shot capabilities into SSL frameworks~\cite{shi2023rethinking,zhu2024doubly}. Current approaches primarily use foundation models as teacher networks for generating pseudo-labels through inference, which requires complex model distillation and introduces additional training overhead. In contrast, we leverage zero-shot predictions of foundation models directly as pseudo-labels---a simpler yet underexplored direction.

However, zero-shot predictions often exhibit high uncertainty and domain-specific biases~\cite{hendrycks2020measuring}. This unreliability can introduce noise into the training process, potentially degrading SSL performance rather than improving it. Building on prior work on doubly-robust self-training~\cite{zhu2024doubly}, we propose ZMT (Zero-Shot Multi-Task Learning), which bridges this gap through four key technical contributions:
\begin{enumerate}
    \itemsep0em
    % \item \todo{ideas: mechanism, architecture, experiments, and then analysis of why it works}
    \item A novel framework to directly leverage zero-shot predictions from foundation models as pseudo-labels, eliminating the need for teacher model inference or distillation
    \item A multi-task learning-based mechanism that incorporates zero-shot pseudo-labels and unsupervised SSL objectives
    \item Extensive experiments on 8 datasets across vision, language, and audio domains which demonstrate that ZMT outperforms both standard SSL methods and zero-shot augmented approaches
    \item Additional analysis on why ZMT outperforms baselines, including the impact of coarse-grained pseudo-labels, unlabeled data accuracy during training, and hyperparameter stability 
\end{enumerate}

By incorporating foundation models into SSL via joint modeling of pseudo-labels, our work demonstrates that zero-shot predictions can reduce error by up to 56\%  compared to traditional SSL methods while remaining robust to low-quality pseudo-labels. Our results hold particular promise for low-resource scenarios where labeled data is extremely limited.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{icml2025/figs/fig1.pdf}
    \caption{Diagram of Zero-shot Multi-Task Learning (ZMT). Both labeled (top image) and unlabeled (bottom image) receive pseudo-labels, potentially from foundation models. Our model encoder (red box) has two heads, a prediction head (blue box) and a pseudo-label head (green box). Unlike prior SSL approaches, our ZMT loss incorporates the pseudo-label prediction loss for both labeled and unlabeled (gray box).}
\label{fig:diagram}
\end{figure*}


\section{Related Work}
\label{sec:related}
% \tba

\subsection{Semi-Supervised Learning}

Semi-supervised learning (SSL) has evolved from foundational consistency regularization methods to sophisticated deep learning approaches. The $\Pi$-Model~\cite{laine2022temporal} and Mean Teacher~\cite{tarvainen2017mean} established core principles by enforcing consistent predictions across different model states. FixMatch~\cite{sohn2020fixmatch} later unified these ideas by combining weak and strong augmentations with pseudo-labeling. Subsequent work focused on reliability: UPS~\cite{rizvedefense} introduced confidence-based filtering while FlexMatch~\cite{zhang2021flexmatch} developed adaptive thresholding for pseudo-label selection. Recent approaches like SimMatch~\cite{zheng2022simmatch} have further advanced the field by incorporating contrastive learning principles.


% Semi-supervised learning (SSL) has emerged as a crucial paradigm for leveraging unlabeled data to improve model performance when labeled data is scarce. Especially popular in computer vision work, traditional SSL approaches build on consistency regularization principles~\cite{tarvainen2017mean,sohn2020}. These methods typically enforce consistent predictions across different augmentations of the same input. The field has since evolved toward more sophisticated techniques, with "Meta Pseudo Labels" (Pham et al., 2021) introducing meta-learning for pseudo-label refinement and "SimMatch: Semi-supervised Learning with Similarity Matching" (Zheng et al., 2022) leveraging similarity-based matching. Recent advances focus on reliability issues, as seen in "Robust Pseudo-Labels through Confidence Calibration" (Chen et al., 2023) and "Ensemble-based Self-training for Foundation Models" (Wang et al., 2023). Some researchers have explored alternative learning objectives, such as "ContrastMatch: Self-supervised Learning for Semi-supervised Classification" (Wei et al., 2023), which combines contrastive and pseudo-labeling approaches. Despite these advances, a key challenge remains in effectively balancing supervised and unsupervised learning signals while maintaining robustness to varying data quality.

\subsection{SSL with Side Information}

The integration of additional information sources has significantly enhanced SSL performance. S4L~\cite{zhai2019s4l} pioneered this direction by incorporating self-supervised pretext tasks into the SSL framework. Building on this foundation, CoMatch~\cite{li2021comatch} introduced graph-based relationships to capture sample similarities, while MPL~\cite{pham2021meta} employed meta-learning within teacher-student frameworks to refine pseudo-labels. More recent approaches have explored diverse information sources: WiSE-FT~\cite{wortsman2022robust} leverages weak supervision signals to guide model training, and SSDG~\cite{wang2023better} taps into external knowledge bases to generate more reliable pseudo-labels. 

% The integration of external knowledge sources has significantly enhanced SSL performance. Doubly Robust self-training (Wei et al., 2023) improves pseudo-label reliability through probability calibration. Recent works leverage LLMs for pseudo-labeling, as seen in TabLLM (Wang et al., 2023) and code-switching ASR (Li et al., 2024). Weak supervision approaches combine multiple noisy sources with limited labeled data, as demonstrated in "Learning from Rules Generalizing Labeled Exemplars" (Zhou et al., 2024) and "Self-Training with Weak Supervision" (Karamanolakis et al., 2021). These methods show particular promise in scenarios where obtaining clean labels is costly.

\subsection{Foundation Models in SSL}
The integration of foundation models into SSL frameworks is an emerging direction. As foundation models show impressive zero-shot performance~\cite{liangholistic, lehman2023we}, the question becomes: how can we incorporate foundation models into SSL frameworks? The three main emerging approaches are: 1) pre-training on the unlabeled data before fine-tuning on the labeled data, or 2) using a teacher model to assign pseudo-labels~\cite{shi2023rethinking}. The first approach provides a strong and robust learner but requires significant compute for the fine-tuning step~\cite{ganerasing,you2024diffusion,hegselmann2023tabllm}. The second approach leverages the teacher model for inference~\cite{chen2020big,yang2024knowledge}. In contrast, our model leverage foundation models as a pseudo-label generator. Motivated by the concept of unreliable pseudo-labels, researchers have also proposed theoretical guarantees to accommodate potentially noisy pseudo-labels~\cite{zhu2024doubly}. Our work builds on this methodological foundation to develop a practical method for incorporating noisy pseudo-labels.

% \todo{Ask Jichan, are we the first to use foundation model pseodulabels direclty?}


\section{Problem Setup}
\label{sec:problem-setup}

\subsection{Motivation}
When leveraging foundation models for SSL, practitioners face two approaches: direct fine-tuning, or using inference-only access. While fine-tuning can yield strong performance, it faces practical limitations:
\begin{itemize}
\itemsep0em
    \item \textbf{Resource Requirements:} Fine-tuning large models (e.g., Llama-70B) requires significant computational resources---even with LoRA and 16-bit quantization, it demands 160GB+ GPU memory.
    \item \textbf{API Constraints:} Commercial APIs often restrict fine-tuning capabilities. For instance, OpenAI's fine-tuning service only supports supervised learning with fixed input-output pairs, preventing usage of unlabeled data.
    \item \textbf{Cost and Reproducibility:} The financial costs of API calls scale with model size and data volume. Version changes in hosted models can affect reproducibility.
    \item \textbf{Efficiency:} The computational and memory requirements of fine-tuning large foundation models often exceed practical resource constraints for many applications~\cite{wang2024slm}.
\end{itemize}

% In-context learning with few labeled examples typically underperforms compared to fine-tuning-based SSL approaches[], as it cannot leverage unlabeled data effectively. 
Given these constraints, we focus on a practical setting where foundation model access is restricted to inference only. Although we refer to zero-shot inference as our motivating source of pseudo-labels, in-context learning with few labeled examples could also be used to generate pseudo-labels.

\subsection{Problem Setup}

Consider a semi-supervised setting with read-only access to a foundation model $\hat{f}$. The training data consists of a labeled set $\mathcal{D}_L = {(x_i, y_i) : i \in [N_L] }$ and an unlabeled set $\mathcal{D}_U = {(u_i) : i \in [N_U] }$, where $\mathcal{D} = \mathcal{D}_L \cup \mathcal{D}_U$ represents the full dataset and $[N]$ denotes integers ${1, 2,..., N}$.
The foundation model $\hat{f}$ generates pseudo-label distributions $\hat{y}^L_i = \hat{f}(x_i)$ for labeled data and $\hat{y}^U_i = \hat{f}(u_i)$ for unlabeled data. 
% We define quality metrics $q^L_i, q^U_i \in [0,1]$ for these pseudo-labels based on: 
% Model confidence scores
% Agreement with labeled data (where available)
% Consistency across augmentations[]
The goal is to train a classifier $f$ that outputs class distributions $\mathbf{p}(y|x)$ using $\mathcal{D}_L$, $\mathcal{D}_U$, and pseudo-labels $\hat{y}^L_i, \hat{y}^U_i$.
%alongside their quality metrics.
\footnote{Our usage of "pseudo-labels" differs from traditional SSL literature, where they typically refer to predictions on $\mathcal{D}_U$ from a model partially trained on $\mathcal{D}_L$. Here, they represent foundation model predictions without any task-specific training.}

We assume $\hat{f}$ was not trained on $\mathcal{D}_L$ or $\mathcal{D}_U$. The pseudo-label quality depends on the model's capabilities and prompt design, with potential failure modes including hallucinations and out-of-domain responses. 
% We define robustness formally as:
% \begin{definition}[Robustness]
% A method is $\epsilon$-robust if its performance degradation is bounded by $\epsilon$ when pseudo-label quality metrics $q^L_i, q^U_i$ approach 0.
% \end{definition}
Our objective is to develop a SSL method that maximally leverages high-quality pseudo-labels when available and gracefully degrades to standard SSL performance when pseudo-labels are unreliable.

% We consider a semi-supervised setting where a foundation model is used to generate zero-shot responses, which can potentially improve performance.
% In this setting, the training data consists of a labeled set $\mathcal{D}_L = \{(x_i, y_i) : i \in [N_L] \}$ and an unlabeled set $\mathcal{D}_U = \{(u_i) : i \in [N_U] \}$, where $\mathcal{D} = \mathcal{D_L} \cup \mathcal{D}_U$ represents the full dataset and $[N]$ denotes the set of integers ${1, 2,..., N}$. A foundation model $\hat{f}$ is used to generate pseudo-labels $\hat{y}^{L}_{i} = \hat{f}(x_i)$ for each labeled data point $x_i \in \mathcal{D}_L $  and $\hat{y}^{U}_{i} = \hat{f}(u_i)$ for each unlabeled data point $u_i \in \mathcal{D}_U$, using zero-shot methods. The goal is to train a classifier $f$ using the semi-supervised dataset $\mathcal{D}_L$, $\mathcal{D}_U$, and its generated pseudo-labels $\hat{y}^{L}_{i}, \hat{y}^{U}_{i}$.\footnote{We note that the term pseudo-label in this work differs from its usage in the semi-supervised learning (SSL) literature. In many SSL studies, pseudo-label refers to the predictions made on unlabeled data $\mathcal{D}_U$ using a model that has undergone a few optimization steps with labeled set $\mathcal{D}_L$.} We denote the model $f$'s predicted class distribution of input data $x$ by $\mathbf{p}(y|x)$.

% We further assume that the foundation model $\hat{f}$ is not trained on the datasets $\mathcal{D}_L$ or $\mathcal{D}_U$. As a result, the quality of the pseudo-labels $\hat{y}_i$ is unknown, and it depends on the foundation model's performance and the design of the prompt. It is important to note that the foundation model’s zero-shot responses can sometimes be noisy, leading to hallucinations[] or responses that are out of domain[].
% Our objective is to develop a robust method that can improve semi-supervised learning regardless of the quality of the pseudo-labels. When the pseudo-labels are accurate, the method should effectively leverage them to maximize performance. However, when the pseudo-labels are inaccurate, the method should focus on the available task data $\mathcal{D_L}$ and $\mathcal{D_U}$, potentially disregarding the pseudo-labels entirely.

% \todo{We need to discuss that pseudo-label in our method is not limited to zero-shot ex. ICL, other pretrained model}

\subsection{Baseline methods}
We discuss previous approaches for our semi-supervised learning setup given with pseudo-labels and compare against our method. See feature comparison matrix Table~\ref{tab:feature-comparison} in appendix.

\paragraph{Pseudo-supervision}
When given a semi-supervised dataset that includes pseudo-labels, a straightforward approach is to fill in the prediction targets for the unlabeled samples using the pseudo-labels, resulting in fully pseudo-labeled unlabeled set $\hat{\mathcal{D}_U} = \{(u_1, \hat{y}^U_1) , (u_2, \hat{y}^U_2),  \ldots, (u_{N_U}, \hat{y}^U_{N_U})\}$. The model can then be trained using a supervised learning algorithm on this fully-labeled dataset $\hat{\mathcal{D}} = \mathcal{D}_L \cup \hat{\mathcal{D}_U}$ . We refer to this method as \textit{pseudo-supervision}. % When given labeled batch with size $B_L$ and unlabeled batch with size $B_U$, 
The method optimizes following loss:
\begin{align*}
\mathcal{L}_{PS} = \frac{1}{N} \left( \sum^{N_L}_{i=1} \mathcal{H}(y_i, \mathbf{p}(y|x_i)) + \sum^{N_U}_{i=1} \mathcal{H}(\hat{y}^U_i, \mathbf{p}(y|u_i)) \right). 
\end{align*}
where $N = N_L + N_U$ represents number of samples in full dataset.

Pseudo-supervision has been shown to improve performance in semi-supervised learning scenarios, particularly when a zero-shot method provides accurate pseudo-labels for the task at hand~\cite{hegselmann2023tabllm,nam2023semi}. However, when the zero-shot predictions are inaccurate, it remains unclear whether pseudo-supervision will still be beneficial. 

% In the motivating example described in Figure~\ref{fig:motiv}, we demonstrate that the performance may actually degrade when the zero-shot predictions are unreliable, compared to training solely on the labeled dataset $D_L$. 

This highlights a critical challenge in practical SSL scenarios: when the quality of the pseudo-labels on a large unlabeled dataset $D_U$ is difficult to assess, users may struggle to determine whether incorporating pseudo-labels into the training process will be helpful or counterproductive.

\paragraph{Doubly Robust Self-Training}
Doubly Robust Self-Training~\cite{zhu2024doubly} addresses a problem setting similar to ours, proposing a method that robustly improves performance despite varying quality of pseudo-labels. The method employs the following loss function:

\ifx\isarxiv\undefined
	% for icml format
    \begin{align*}
     & \mathcal{L}_{DR}  = \frac{1}{N_L} \sum^{N_L}_{i=1} \mathcal{H}(y_i, \mathbf{p}(y|x_i)) - \frac{1}{N_L} \sum^{N_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{p}(y|x_i)) \\
    & + \frac{1}{N} \left( \sum^{N_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{p}(y|x_i)) + \sum^{N_U}_{i=1} \mathcal{H}(\hat{y}^U_i, \mathbf{p}(y|u_i)) \right). 
    \end{align*}
\else
	% for arxiv format
    \begin{align*}
     \mathcal{L}_{DR}  = \frac{1}{N_L} \sum^{N_L}_{i=1} \mathcal{H}(y_i, \mathbf{p}(y|x_i)) - \frac{1}{N_L} \sum^{N_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{p}(y|x_i)) + \frac{1}{N} \left( \sum^{N_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{p}(y|x_i)) + \sum^{N_U}_{i=1} \mathcal{H}(\hat{y}^U_i, \mathbf{p}(y|u_i)) \right). 
    \end{align*}
\fi

When the pseudo-labels are accurate, the loss function approximates training with the full dataset $\mathcal{D}_L$ and $\mathcal{D}_U$, both with ground truth labels. Conversely, when the pseudo-labels are inaccurate, the loss function asymptotically approaches training with only the labeled samples $\mathcal{D}_L$. The work provides convergence improvement guarantees over training with labeled data alone, with varying pseudo-label quality, under certain regularity conditions.

While the study presents experimental results using a single pseudo-label set inferred from a pre-trained model, its practical effectiveness with inaccurate pseudo-labels in low-label settings is unexplored.

\paragraph{Semi-supervised learning methods}

Recent advancements in SSL have led to the development of methods that jointly train a classifier from both supervision with labeled set, and unsupervised feature learning on unlabeled data. The training objective is typically expressed as $\mathcal{L} = \mathcal{L}_s + \mathcal{L}_u$ where $\mathcal{L}_s$ is the supervised loss, often the cross-entropy loss computed on the labeled data: 
\begin{align*}
\mathcal{L}_{s} = \frac{1}{B_L} \sum^{B_L}_{i=1} \mathcal{H}(y_i, \mathbf{p}(y|x_i)). 
\end{align*}
where $B_L$ is the size of data batch sampled from labeled set $\mathcal{D}_L$.

On the other hand, $\mathcal{L}_u$ represents the unsupervised loss, which leverages different strategies to incorporate unlabeled data into the training process. Some notable strategies include the following:
\begin{itemize}
    \item \textbf{Self-training}: This strategy involves using the model trained on labeled data to generate temporary predictions for the unlabeled samples, known as pseudo-labeling with supervision (PLS)\footnote{The term ``pseudo-label with supervision'' in this context was used to indicate model's temporary prediction in SSL method, not zero-shot responses.}. These predictions are then incorporated into the model's supervision objective~\cite{lee2013pseudolabel}.
    \item \textbf{Confidence-based Sample Selection}: Unlabeled samples with not confident or potentially incorrect pseudo-labels are filtered out based on predefined thresholds. This ensures that only samples with reliable predictions contribute to the training process~\cite{qizhe2020}.
    \item \textbf{Strong Augmentation}: To further enhance feature learning, the model is optimized so that its predictions on weakly augmented and strongly augmented versions of the same sample agree ~\cite{qizhe2020}. This encourages the model to learn robust features.
    \item \textbf{Distribution Alignment}: This technique adjusts the output probability distribution of each class based on the input data distribution~\cite{berthelot2020}. 
\end{itemize}
With strong augmentation $\mathcal{A}_s(u_i)$ and weak augmentation $\mathcal{A}_w(u_i)$ of unlabeled samples $u_i$, we denote predictions from classifier $f$ of these augmented samples by 
$p^s_i=\mathbf{p}(y|\mathcal{A}_s(u_i))$ and $p^w_i=\mathbf{p}(y|\mathcal{A}_w(u_i))$, respectively.

When a data batch of size $B_U$ is sampled from $\mathcal{D}_U$, the unsupervised training objective in these methods typically takes the following form:
\begin{align*}
\mathcal{L}_{u} = \frac{1}{B_U} \sum^{B_U}_{i=1} \mathbbm{1}(\text{max}(\hat{p}^w_i) > \tau) \mathcal{H}(y_i, \mathbf{p}(y|x_i)) 
\end{align*}
where $\hat{p}^w_i = DA(p^w_i)$ is label prediction for input $\mathcal{A}_w(u_i)$ and $DA$ represents distribution alignment process.

% \tba add more example works (that are most relevant to ours)

% \paragraph{Preliminary Benchmark}
% Each of the aforementioned methods employs different strategies and features to improve performance with limited labeled data. To compare them, we present the initial benchmark results of these methods on the XX dataset in Figure XX. 

\section{Zero-Shot Multi-Task Learning}
\label{sec:zmt}

One explanation for the failure of pseudo-supervision and doubly robust method with inaccurate pseudo-labels is that these approaches directly optimize the classifier to predict pseudo-labels with cross-entropy loss. This can lead to corrupting the overall training process when the pseudo-labels are noisy~\cite{arpit2017a}. To mitigate this issue, we propose a method that indirectly incorporates pseudo-labels into the objective function. Additionally, we aim to develop a method that fully leverages SSL's unsupervised feature learning capabilities while benefiting from the pseudo-labels provided. 
% This is motivated by our observation in Section XX, which highlights the effectiveness of SSL in the low-label regime.

Multi-task learning~\cite{crawshaw2020multi} offers a framework for learning multiple tasks simultaneously. When the tasks are similar and related, they can mutually benefit from each other. However, when the tasks are dissimilar, the model learn them independently, provided the model's capacity is large enough~\cite{banayeeanzade2024}.

To improve robustness against noisy pseudo-labels, we propose a multi-task learning-based approach that combines the SSL task as the primary task with pseudo-label prediction (extraction) as an auxiliary task. When the pseudo-labels are accurate, the model benefits from the pseudo-label prediction task. Conversely, when the pseudo-labels are noisy, the model can still benefit from SSL's objective, with less the impact from the inaccurate pseudo-label prediction task. We denote our method by \ourmethod (\textbf{Z}ero-shot pseudo-label \textbf{M}ulti-\textbf{T}ask learning), and highlight the key differences between our method and existing approaches in Table~\ref{tab:feature-comparison}.


% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.8\linewidth]{icml2025/figs/tmp_table1.png}
%     \caption{\tba feature comparison matrix.}
% \label{fig:feature_matrix}
% \end{figure}
% \begin{footnotesize}

% \end{footnotesize}

% first two rows: "Unsupervised feature learning, e.g., confidence-based sample selection or strong augmentation"



We illustrate our method in Figure~\ref{fig:diagram}. The classifier $f$ consists of a non-linear projector head $h(\cdot)$ and a feature encoder $g(\cdot)$, such that $f = h \circ g $. 
We introduce an additional linear projector head $h_p(\cdot)$, which learns a pseudo-label prediction task, while sharing the encoder $g$ with the main classification task. 

Given a batch of labeled and unlabeled data of size $B_L$ and $B_U$, the pseudo-label prediction task optimizes $h_p \circ g$ with the following loss:
\begin{align*}
\mathcal{L}_{p} = \frac{1}{B} \left( \sum^{B_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{q}(y|x_i)) + \sum^{B_U}_{i=1} \mathcal{H}(\hat{y}^U_i, \mathbf{q}(y|u_i)) \right). 
\end{align*}
where $ B = B_L + B_U$ and $\mathbf{q}(y|x)$ indicates predicted class distribution of $h_p(g(x))$.
Together with the SSL objective, the overall loss function is:
\begin{align*}
\mathcal{L} = \mathcal{L}_s + \mathcal{L}_u + \alpha_t \cdot \lambda_p \mathcal{L}_p
\end{align*}
where $\alpha_t$ is annealing parameter that linearly increases value from $0$ to $1$ during training based on training step $t$, and $\lambda_p$ is a fixed scalar hyperparameter indicating relative weight of pseudo-label prediction task. We denote the inclusion of annealing with binary variable $\alpha_p$.

Note that during model selection using the validation set, the best model is chosen without considering the pseudo-label prediction score from \( h_p \), as the primary goal is to achieve optimal performance for the original classifier $f = h \circ g $.

\paragraph{ZMT+: a resource-efficient version ZMT}
We note that ZMT may use higher memory than SSL due to backpropagation on weakly augmented unlabeled samples. In many SSL algorithms, weak augmentations are used only to obtain PLS and do not require storing activations for backpropagation.
In contrast, ZMT's pseudo-label prediction loss involves labeled samples and weakly augmented unlabeled samples, requiring activation storage until backpropagation happens. When combined with an SSL algorithm, ZMT needs activations for added strongly augmented unlabeled batches for SSL algorithm, increasing VRAM usage by up to 50\% compared to running SSL.

To mitigate this, we propose a variation called ZMT+, which reduces memory usage by incorporating strongly augmented samples in the pseudo-label prediction loss:

\begin{align*}
    \mathcal{L}_{p} = \frac{1}{B} \left( \sum^{B_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{q}(y|x_i)) + \sum^{B_U}_{i=1} \mathcal{H}(\hat{y}^U_i, \mathbf{q}(y|\mathcal{A}_s(u_i))) \right). 
\end{align*}


ZMT+ maintains memory usage similar to the integrated SSL method, with only a minor overhead from activations in the classification head $h_p$.

% \paragraph{Pseudo-labels from weakly-related task}
% Since our method is essentially an application of multi-task learning, we note that it can also benefit from pseudo-labels derived from weakly related tasks. This flexibility allows our approach to handle a wider range of zero-shot responses, including coarse-grained labels. In fact, coarse-grained zero-shot responses often outperform fine-grained ones in certain scenarios~\todo{cite}. To accommodate such responses, one can modify the non-linear head, $h_p$, to adjust to the desired number of output classes.


\begin{table*}[t!]
    \centering
    \small
    \caption{Summary of data used in the experiments. For each dataset, we run experiments with at most 3 pseudo-label (PL) sets A, B, C of decreasing quality. The foundation model $\hat{f}$ used to generate each pseudo-label set are shown below.}
    \begin{tabular}{llccccc}
        \toprule
        Domain & Dataset & \# Train & \# Val. & \# Test & \# Class & Foundation model $\hat{f}$ \\
        \midrule
        \multirow{1}{*}{CV} & CIFAR100 & 50,000 & - & 10,000 & 100 & CLIP: large(A), base(B) \\
        \midrule
        \multirow{4}{*}{NLP} & Amazon Review & 250,000 & 25,000 & 65,000 & 5 & FLAN-T5: XL(A), base(B), small(C) \\
        & Yelp Review & 250,000 & 25,000 & 50,000 & 5 & FLAN-T5: XL(A), base(B), small(C) \\
        & AG News & 100,000 & 10,000 & 7,600 & 4 & FLAN-T5-small(A), Roberta-large(B) \\
        & Yahoo! Answer & 500,000 & 50,000 & 60,000 & 10 & FLAN-T5: XXL(A), base(B), small(C) \\
        \midrule
        \multirow{3}{*}{Audio} & UrbanSound 8k & 7,079 & 816 & 837 & 10 & Whisper: medium(A), base(B), tiny(C) \\
        & ESC-50 & 1,200 & 400 & 400 & 50 & Whisper: large-v2(A), base(B), tiny(C) \\
        & GTZAN & 7,000 & 1,500 & 1,500 & 10 & Whisper: large-v2(A), base(B), tiny(C) \\
        \bottomrule
    \end{tabular}
    \label{tab:data_summary}
\end{table*}

\section{Experiments}
\label{experiments}

\subsection{Experiment Setup}
% We evaluate our method on SSL datasets across various domains to compare its performance with baseline methods and show improvements achieved using pseudo-labels with our method. We consider  combinations of datasets and pseudo-label sets with following criteria.

\paragraph{Datasets}
We use 8 publicly-available datasets across vision, natural language processing, and audio classification tasks (Table~\ref{tab:data_summary}). Dataset sizes range from 100s to 100,000s of data points. For dataset-specific training setup details, see Appendix~\ref{sec:training-setup} including the choice of foundation models, hyperparameters used.
We evaluate on tasks where additional label information meaningfully improves performance over SSL baselines. We exclude datasets like CIFAR-10~\cite{Krizhevsky09learningmultiple}, SVHN~\cite{netzer2011reading}, and STL-10~\cite{coates2011analysis} where recent SSL methods already achieve near-optimal performance with limited labels, making pseudo-label information redundant. 
To evaluate the effect of labeled dataset sizes, for each dataset, we create up to three tasks with different sizes of labeled data while the number of total data points stays the same. For example, in the CIFAR100 dataset, we create sets with 100, 200, and 400 labeled data points.
% \todo{We need to explain the different number of labeled data (columns in the big tables) and the different models that we train to get A, B, C}

% \todo{say what datasets table 1, refer to appendix for more details}
\paragraph{Pseudo-label}
For each dataset, we consider multiple sets of pseudo-labels with varying quality. In practice, high-quality foundation models are often used to generate the best pseudo-labels for a given task. 
However, to evaluate the robustness of our method against less accurate pseudo-labels, we also test our approach using pseudo-labels generated from smaller-weight versions of the same foundation model, described as A, B, and C (in declining pseudo-label quality) in Table~\ref{tab:data_summary} and in experiment results. Depending on the scores, at most 3 different pseudo-label sets from distinct foundation models are tested. These pseudo-labels are generated using a zero-shot method, with basic prompting on the input data. 

The details of the datasets, the architecture of training model $f$, and the foundation models $\hat{f}$ used to generate the pseudo-label sets are provided in Table \ref{tab:data_summary}. Further information on the prompting and post-processing procedures for pseudo-label generation can be found in Appendix \ref{sec:plabel}.

\paragraph{Integrating ZMT with a SSL baseline}
We implement our proposed method using the Unified SSL Benchmark (USB) framework~\cite{wang2022generalizing}. 
To minimize the complexity of integrating our method with the existing SSL baseline, we simply add an additional multi-layer perceptron (MLP) classification head with the same architecture as the baseline model's MLP head. 
% This results in two MLP heads, where the additional head is optimized to predict pseudo-labels in a multi-task learning manner.
We primarily benchmark our method against AdaMatch~\cite{berthelotadamatch}, which is a highly performant SSL baseline across various datasets and domains, as demonstrated by benchmark results in the USB framework. We refer to our methods ZMT, ZMT+ integrated with AdaMatch as ZeroMatch and ZeroMatch+ respectively.

\paragraph{Hyperparameters used in ZMT}
To ensure a fair comparison with SSL methods, we use the exact same hyperparameters as the SSL method we combine with. 
Additionally, our method introduces two extra hyperparameters, $\alpha_p$ and $\lambda_p$. To determine the optimal values for these hyperparameters, we conduct experiments with four different sets: $(\alpha_p, \lambda_p) = (0, 0.5), (0, 1.0), (1, 0.5), (1, 1.0)$, and report the best-performing set. 

\paragraph{Pseudo-Supervision and Doubly-Robust Self-Training}
In addition to comparing our method with SSL baselines, we also implement Pseudo-supervision (PS) and Doubly-robust Self-training (DR) to evaluate their performance when pseudo-labels are available. 
To ensure a fair comparison with SSL baselines, we use the same number of labeled and unlabeled batches participating during the optimization process.
In each optimization step, when a labeled batch of size $B_L$ and an unlabeled batch of size $B_U$ are given, PS optimizes the following loss:
\begin{align*}
\mathcal{L}_{PS} = \frac{1}{B} \left( \sum^{B_L}_{i=1} \mathcal{H}(y_i, \mathbf{p}(y|x_i)) + \sum^{B_U}_{i=1} \mathcal{H}(\hat{y}^U_i, \mathbf{p}(y|u_i)) \right). 
\end{align*}
where $B = B_L + B_U$. Similarly, DR optimizes the following:
\ifx\isarxiv\undefined
	% for icml format
    \begin{align*}
     & \mathcal{L}_{DR}  = \frac{\alpha_t}{B_L} \sum^{B_L}_{i=1} \mathcal{H}(y_i, \mathbf{p}(y|x_i)) - \frac{\alpha_t}{B_L} \sum^{B_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{p}(y|x_i)) \\
    & + \frac{1}{B} \left( \sum^{B_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{p}(y|x_i)) + \sum^{B_U}_{i=1} \mathcal{H}(\hat{y}^U_i, \mathbf{p}(y|u_i)) \right). 
    \end{align*}    
\else
	% for arxiv format
    \begin{align*}
     \mathcal{L}_{DR}  = \frac{\alpha_t}{B_L} \sum^{B_L}_{i=1} \mathcal{H}(y_i, \mathbf{p}(y|x_i)) - \frac{\alpha_t}{B_L} \sum^{B_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{p}(y|x_i)) + \frac{1}{B} \left( \sum^{B_L}_{i=1} \mathcal{H}(\hat{y}^L_i, \mathbf{p}(y|x_i)) + \sum^{B_U}_{i=1} \mathcal{H}(\hat{y}^U_i, \mathbf{p}(y|u_i)) \right). 
    \end{align*}
\fi

where $\alpha_t$ represents the annealing parameter, following the original implementation in~\cite{zhu2024doubly}.

We report the median and standard deviation of each method, based on three random seeds for model initialization and a randomly selected set of labeled samples. For each domain, we also report the scores of the two highest-performing SSL baseline methods, based on benchmark results introduced in USB~\cite{wang2022usb}.

\begin{table*}[t!]
\centering
\small
\caption{Top-1 error rate (\%) in CIFAR-100 of 3 different random seeds. * indicates numbers published in USB~\cite{wang2022usb}. Best score among experiments with same pseudo-label set are in bold. `PL set' refers to pseudo-label set used, i.e., foundation models of decreasing quality labels.}
\begin{tabular}{c|c|ccc}
\hline
                      & Dataset           & \multicolumn{3}{c}{CIFAR100}                                    \\ \hline
PL Set                & Label size        & 100                 & 200                 & 400                 \\ \hline
\multirow{4}{*}{None} & Supervised        & 49.05±1.69          & 35.83±0.17          & 26.22±0.53          \\
                      & AdaMatch          & 28.57±2.48          & 21.68±0.46          & 15.98±0.74          \\
                      & ReMixMatch*       & -                   & 20.85±1.42          & 16.80±0.59          \\
                      & SimMatch*         & -                   & 23.26±1.25          & 16.82±0.40          \\ \hline
\multirow{5}{*}{A}    & Zero-shot         & 39.35 & 39.35 & 39.35                                       \\
                      & Pseudo-supervise  & 29.03±0.41          & 28.14±0.63          & 25.95±0.42          \\
                      & Doubly-robust     & 29.09±0.31          & 30.41±0.78          & 29.84±0.75          \\
                      & ZeroMatch (ours)  & 24.84±1.61          & 18.70±1.60          & \textbf{14.67±0.31} \\
                      & ZeroMatch+ (ours) & \textbf{23.93±1.62} & \textbf{18.69±0.82} & 14.77±0.48          \\ \hline
\multirow{5}{*}{B}    & Zero-shot         & 51.93 & 51.93 & 51.93                                       \\
                      & Pseudo-supervised & 37.81±0.26          & 34.04±0.37          & 28.97±0.23          \\
                      & Doubly-robust     & 39.07±0.28          & 38.81±0.46          & 39.47±0.41          \\
                      & ZeroMatch (ours)  & \textbf{26.03±1.97} & 21.75±0.51          & \textbf{16.32±0.22} \\
                      & ZeroMatch+ (ours) & 26.21±1.89          & \textbf{19.65±0.64} & 16.46±0.54          \\ \hline
\end{tabular}
\label{tab:cifar_results}
\end{table*}

% \paragraph{Training Setup}
% \label{sec:training-setup}

\begin{table*}[ht]
\centering
\small
\caption{Top-1 error rate (\%) in Yahoo Answers, AG News, Amazon Review, Yelp Review of 3 different random seeds. * indicates numbers published in USB~\cite{wang2022usb}. Best score among experiments with same pseudo-label set are in bold. For SSL baselines without pseudo-label set, numbers performed better than our method (outside standard deviation) with low quality pseudo-label set are in bold. `PL set' refers to pseudo-label set used. }
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|ccc|ccc|ccc|ccc}
\hline
                      & Dataset           & \multicolumn{3}{c|}{Yahoo Answers}                              & \multicolumn{3}{c|}{AG News}                                    & \multicolumn{3}{c|}{Amazon Review}                              & \multicolumn{3}{c}{Yelp Review}                                 \\ \hline
PL set                & Label size        & 250                 & 500                 & 2000                & 20                  & 40                  & 100                 & 125                 & 250                 & 1000                & 125                 & 250                 & 1000                \\ \hline
\multirow{2}{*}{None} & Supervised        & 39.17±1.47          & 37.75±1.01          & 33.57±0.05          & 15.78±0.34          & 14.08±1.14          & 12.61±1.19          & 56.63±1.35          & 51.84±0.58          & 47.67±0.46          & 52.88±2.31          & 50.24±2.00          & 46.94±0.19          \\
                      & AdaMatch          & 35.19±1.29          & 32.70±0.53          & 30.58±0.42          & 11.87±2.55          & 14.79±1.44          & 11.67±1.94          & 54.97±4.05          & 47.61±1.65          & 43.82±0.45          & 54.31±1.14          & 45.87±0.96          & 41.60±0.85          \\
                      & FlexMatch*        & -                   & 35.81±1.09          & 31.42±0.41          & -                   & 16.90±6.76          & 11.43±0.91          & -                   & 45.75±1.21          & 43.14±0.82          & -                   & 46.37±0.74          & 40.86±0.74          \\
                      & SimMatch*         & -                   & 34.15±0.91          & 30.64±0.42          & -                   & 14.80±0.57          & \textbf{11.12±0.15}          & -                   & 47.27±1.73          & 43.09±0.50          & -                   & 46.40±1.71          & 41.24±0.17          \\ \hline
\multirow{5}{*}{A}    & Zero-shot         & 33.37               & 33.37               & 33.37               & 12.47               & 12.47               & 12.47               & 47.63               & 47.63               & 47.63               & 40.47               & 40.47               & 40.47               \\
                      & Pseudo-supervise  & 34.21±0.18          & 33.69±0.46          & 34.08±0.33          & 11.30±0.16          & 11.49±0.20          & 11.59±0.18          & 48.89±0.27          & 49.14±0.27          & 49.59±0.41          & 41.78±0.26          & 42.45±0.14          & 41.51±0.47          \\
                      & Doubly-robust     & 48.61±12.13         & 55.68±10.91         & 52.10±5.68          & 12.03±5.97          & 12.17±2.29          & 11.32±0.53          & 53.06±0.72          & 52.75±0.63          & 52.24±0.87          & 44.51±0.86          & 43.47±0.95          & 46.35±0.70          \\
                      & ZeroMatch (ours)  & 29.99±0.63          & \textbf{30.24±0.22} & \textbf{28.78±0.27} & \textbf{9.37±0.05}  & \textbf{9.43±0.20}  & \textbf{9.53±0.68}  & \textbf{44.84±0.30} & \textbf{44.25±0.65} & \textbf{41.47±0.36} & \textbf{41.95±1.19} & \textbf{42.61±2.65} & \textbf{36.35±1.36} \\
                      & ZeroMatch+ (ours) & \textbf{29.73±1.59} & 30.78±0.26          & 29.78±0.16          & 10.22±0.70          & 9.58±0.20           & 10.09±0.06          & 46.47±0.55          & 45.38±0.87          & 42.27±0.55          & 48.63±2.96          & 43.80±0.95          & 39.59±0.60          \\ \hline
\multirow{5}{*}{B}    & Zero-shot         & 41.93               & 41.93               & 41.93               & 35.8                & 35.8                & 35.8                & 57.77               & 57.77               & 57.77               & 54.68               & 54.68               & 54.68               \\
                      & Pseudo-supervise  & 41.14±0.34          & 41.09±0.04          & 41.15±0.08          & 33.76±0.23          & 34.07±0.60          & 33.20±0.31          & 57.87±0.40          & 57.87±0.34          & 57.75±0.13          & 54.80±0.23          & 54.65±0.16          & 54.42±0.23          \\
                      & Doubly-robust     & 80.32±1.97          & 80.43±8.50          & 81.05±13.25         & 35.00±0.91          & 35.34±0.90          & 35.78±1.27          & 61.09±0.62          & 60.00±0.54          & 60.20±0.64          & 57.16±0.29          & 57.29±1.55          & 57.19±0.60          \\
                      & ZeroMatch (ours)  & \textbf{31.68±1.43} & 32.31±0.40          & \textbf{29.90±0.38} & 19.47±15.63         & \textbf{12.66±0.32} & \textbf{11.61±0.31} & 48.60±1.07          & \textbf{45.15±1.07} & \textbf{42.18±0.56} & 53.98±3.67          & \textbf{49.14±1.40} & \textbf{39.37±0.91} \\
                      & ZeroMatch+ (ours) & 34.25±1.28          & \textbf{32.15±0.40} & 30.17±0.16          & \textbf{13.26±1.88} & 14.63±1.08          & 11.70±0.33          & \textbf{47.44±0.41} & 45.74±0.88          & 43.25±0.78          & \textbf{50.12±2.52} & 46.29±1.16          & 40.92±0.63          \\ \hline
\multirow{5}{*}{C}    & Zero-shot         & 64.71               & 64.71               & 64.71               & -                   & -                   & -                   & 64.3                & 64.3                & 64.3                & 63.2                & 63.2                & 63.2                \\
                      & Pseudo-supervised & 63.62±0.51          & 63.71±0.38          & 63.63±0.51          & -                   & -                   & -                   & 63.98±0.04          & 64.03±0.05          & 63.96±0.03          & 63.05±0.05          & 63.07±0.06          & 63.09±0.04          \\
                      & Doubly-robust     & 75.47±4.84          & 77.48±5.54          & 66.12±2.07          & -                   & -                   & -                   & 64.20±0.13          & 64.10±0.07          & 64.04±0.03          & 63.49±0.18          & 63.35±0.18          & 63.25±0.06          \\
                      & ZeroMatch (ours)  & 33.40±0.13          & \textbf{32.95±0.44} & 30.78±0.53          & -                   & -                   & -                   & 54.47±4.91          & 46.81±1.68          & \textbf{43.10±0.63} & 56.12±4.36          & \textbf{46.17±2.19} & 41.69±0.45          \\
                      & ZeroMatch+ (ours) & \textbf{33.33±1.13} & 33.37±0.16          & \textbf{30.57±0.56} & -                   & -                   & -                   & \textbf{47.17±2.55} & \textbf{46.80±0.83} & 43.43±0.69          & \textbf{53.36±1.81} & 46.58±0.51          & \textbf{41.03±0.47} \\ \hline
\end{tabular}
}
\label{tab:nlp_results}
\end{table*}


\begin{table*}[ht]
\centering
\small
\caption{Top-1 error rate (\%) in Urbansound 8K, ESC50, GTZAN of 3 different random seeds. * indicates numbers published in USB~\cite{wang2022usb}. ``PL set'' refers to pseudo-label set used. Best score among experiments with same pseudo-label set are in bold. For SSL baselines without pseudo-label set, numbers performed better than our method (outside standard deviation) with low quality pseudo-label set is indicated by bold.}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|ccc|cc|ccc}
\hline
                      & Dataset           & \multicolumn{3}{c|}{Urbansound 8k}                              & \multicolumn{2}{c|}{ESC50}                & \multicolumn{3}{c}{GTZAN}                                       \\ \hline
PL Set                & Label size        & 50                  & 100                 & 400                 & 250                 & 500                 & 50                  & 100                 & 400                 \\ \hline
\multirow{2}{*}{None} & Supervised        & 56.87±5.92          & 40.98±2.80          & 27.72±1.11          & 49.75±2.07          & 35.75±2.29          & 53.13±1.30          & 48.13±1.43          & 32.47±0.55          \\
                      & AdaMatch          & 49.10±7.36          & 33.69±1.91          & 21.51±2.95          & 38.25±1.39          & 32.50±1.06          & 40.47±1.22          & 31.80±1.87          & 21.87±1.12          \\
                      & CoMatch*          & -                   & 30.59±2.45          & 21.35±1.49          & 40.17±2.08          & 29.83±1.31          & -                   & 36.93±1.23          & 22.20±1.39          \\
                      & SimMatch*         & -                   & 31.70±6.05          & \textbf{19.55±1.89} & 39.92±2.35          & 32.83±1.43          & -                   & 32.42±2.18          & \textbf{20.80±0.77} \\ \hline
\multirow{5}{*}{A}    & Zero-shot         & 39.9                & 39.9                & 39.9                & 35.75               & 35.75               & 50.07               & 50.07               & 50.07               \\
                      & Pseudo-supervise  & 43.73±1.65          & 41.46±1.00          & 37.51±3.11          & 40.50±1.74          & 34.75±0.82          & 45.53±0.68          & 45.60±0.25          & 45.00±0.27          \\
                      & Doubly-robust     & 61.77±4.33          & 63.92±3.90          & 55.91±3.44          & 52.00±1.01          & 42.25±0.82          & 67.47±7.46          & 78.20±7.19          & 76.47±7.47          \\
                      & ZeroMatch (ours)  & 32.11±6.13          & \textbf{24.88±7.03} & \textbf{18.26±1.52} & \textbf{34.50±2.06} & \textbf{28.00±1.12} & \textbf{34.67±1.44} & 30.33±1.58          & 20.40±1.46          \\
                      & ZeroMatch+ (ours) & \textbf{31.42±7.17} & 24.97±2.55          & 21.98±1.57          & 38.00±1.84          & 31.00±1.33          & 35.00±1.58          & \textbf{30.07±0.58} & \textbf{20.07±0.89} \\ \hline
\multirow{5}{*}{B}    & Zero-shot         & 66.79               & 66.79               & 66.79               & 61.5                & 61.5                & 59.93               & 59.93               & 59.93               \\
                      & Pseudo-supervised & 65.11±1.04          & 64.40±1.32          & 63.08±2.25          & 56.25±1.78          & 47.25±0.62          & 57.93±0.68          & 57.20±1.17          & 55.20±0.52          \\
                      & Doubly-robust     & 89.49±3.10          & 88.41±6.45          & 70.85±1.52          & 75.00±2.13          & 55.75±1.54          & 82.20±3.92          & 80.60±5.59          & 68.73±7.85          \\
                      & ZeroMatch (ours)  & \textbf{37.62±3.91} & \textbf{26.23±7.09} & \textbf{21.69±0.85} & \textbf{37.50±1.81} & \textbf{32.00±1.16} & 37.93±3.76          & 30.73±0.74          & \textbf{21.27±1.44} \\
                      & ZeroMatch+ (ours) & 42.29±5.21          & 33.21±3.62          & 21.74±2.64          & 37.50±2.08          & 34.00±2.13          & \textbf{34.40±3.54} & \textbf{30.40±2.14} & 22.20±0.66          \\ \hline
\multirow{5}{*}{C}    & Zero-shot         & 70.01               & 70.01               & 70.01               & 85.75               & 85.75               & 78.47               & 78.47               & 78.47               \\
                      & Pseudo-supervised & 70.13±1.83          & 68.94±1.52          & 67.74±0.72          & 76.75±1.20          & 63.75±1.77          & 76.40±0.14          & 76.67±0.05          & 72.20±0.46          \\
                      & Doubly-robust     & 81.24±5.35          & 88.77±2.60          & 76.82±3.81          & 89.00±2.27          & 70.25±0.82          & 90.67±3.13          & 88.73±2.43          & 81.87±3.82          \\
                      & ZeroMatch (ours)  & 44.24±2.72          & 32.60±6.67          & \textbf{20.22±0.93} & 39.25±1.81          & 32.75±0.66          & 39.80±3.43          & 33.27±1.33          & 23.40±0.54          \\
                      & ZeroMatch+ (ours) & \textbf{42.17±2.62} & \textbf{30.59±4.80} & 22.34±1.46          & \textbf{39.00±1.50} & \textbf{31.50±1.65} & \textbf{36.27±1.16} & \textbf{32.87±1.53} & \textbf{22.00±0.86} \\ \hline
\end{tabular}   
}
\label{tab:audio_results}
\end{table*}

\subsection{ZMT Improves Performance on SSL Baselines Despite Pseudo-Label Quality}
Our ZMT approach consistently achieves the highest scores in all settings with accurate pseudo-labels. Benchmark results for the CIFAR-100, NLP, and audio datasets are shown in Tables~\ref{tab:cifar_results}, \ref{tab:nlp_results}, and \ref{tab:audio_results}, respectively. Our results find that ZMT largely improves upon SSL baselines (e.g., AdaMatch, SimMatch), Pseudo-supervision, and Doubly-robust methods. For example, in the AG News dataset with 40 labels and pseudo-set label set A, ZeroMatch achieves an error of $9.43$ compared to the supervised model error of $14.08$, yielding a reduction of $49.3\%$. The highest improvement comes from ZeroMatch+ producing a 56.2\% error reduction on pseudo-label set A in the 50-label setting of the Urbansound 8k dataset, compared with AdaMatch.

In cases where a low- or medium-quality pseudo-label set is given, our method generally improves or at least maintains performance close to that of SSL baselines, demonstrating its robustness against noisy labels. For example, for CIFAR100 set B---which has low quality pseudo-labels---ZeroMatch+ achieves an error of 19.65, outperforming AdaMatch's error of 21.68.
In most of the settings with inaccurate pseudo-label sets, our method is better or maintains the performance of baseline within one standard deviation, with 3 exceptions (of 23 total experiments) where SimMatch outperforms ZeroMatch or ZeroMatch+. 

% which slightly outperforms our method with largest label setting in AG News (100-label), Urbansound 8K (400-label), and GTZAN (400-label).

% \todo{Exact numbers for: 1) audio performance (), 2) NLP, ( 14.08 -> 9.43 : 49.3\%) 3) vision (28.57 -> 23.93, 19.3\%), 4) compared to Adamatch, 5) compared to CoMatch/SimMatch, 6) doubly robust commentary, 7) pseudo-supervise, 8) zero-shot, 9) good labels and bad labels}

% can often outperform the SSL baseline (e.g., AdaMatch, SimMatch, etc) with accurate pseudo-labels, but it underperforms with bad pseudo-label

Our ZMT approaches are particularly effective compared to methods that leverages pseudo-labels, such as Pseudo-supervision (PS) or Doubly-robust (DR) Self-training. For example, in 250 label setting of Yahoo Answers, PS reaches $34.21\%$ error with pseudo-label set A, which is better than $35.19\%$ error of AdaMatch, but ZeroMatch achieves an error of 29.73. The Doubly Robust Self-Training method remains particularly ineffective, never outperforming ZeroMatch or ZeroMatch+. We hypothesize this is due to asymptotic guarantees of Doubly-robust Self-training method failing to hold in the low-label regime.

% \tba ``We hypothesize when in the low-label regime, asymptomic guarantees do not hold.'' (add to experiment analysis)
% \tba can we point out exactly what assumption breaks in low-label / bad plabel setting? like asymptotics fail?

% \tba put back section explaining why baseline methods are so terrible into appendix

\begin{table}[t!]
\centering
\caption{Top-1 error (\%) comparison of AdaMatch and ZeroMatch in Amazon Review dataset with 3-class zero-shot pseudo-label. A, B, and C refers to pseudo-label set used with declining pseudo-label quality.}
\ifx\isarxiv\undefined
	% for icml format
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{c|cccc}
    \hline
    PL set & 50         & 100        & 250        & 500        \\ \hline
    AdaMatch   & 64.55±4.60 & 57.74±4.46 & 47.28±1.99 & 44.97±1.56 \\ \hline
    ZeroMatch-A & 56.79±2.61 & 50.87±1.67 & 45.33±0.82 & 43.83±0.58 \\
    ZeroMatch-B & 58.75±2.98 & 57.05±3.06 & 48.50±1.55 & 42.75±0.50 \\
    ZeroMatch-C & 61.14±2.57 & 57.87±3.45 & 46.68±0.87 & 43.25±1.18 \\ \hline
    \end{tabular}
    }
\else
	% for arxiv format
    \begin{tabular}{c|cccc}
    \hline
    PL set & 50         & 100        & 250        & 500        \\ \hline
    AdaMatch   & 64.55±4.60 & 57.74±4.46 & 47.28±1.99 & 44.97±1.56 \\ \hline
    ZeroMatch-A & 56.79±2.61 & 50.87±1.67 & 45.33±0.82 & 43.83±0.58 \\
    ZeroMatch-B & 58.75±2.98 & 57.05±3.06 & 48.50±1.55 & 42.75±0.50 \\
    ZeroMatch-C & 61.14±2.57 & 57.87±3.45 & 46.68±0.87 & 43.25±1.18 \\ \hline
    \end{tabular}
\fi
\label{tbl:coarse}
\end{table}


\subsection{ZMT Leverages Coarse-Grained Zero-Shot Pseudo-Labels to Improve Learning}
Our ZMT architecture enables the use of coarse-grained pseudo-labels, unlike Pseudo-supervision and Double-robust Self-training which cannot due to the use of the pseudo-labels as a substitute for prediction target. To accommodate such responses, one can modify the non-linear head, $h_p$, to adjust to the desired number of output classes.
% Zero-shot methods often yield less accurate responses for fine-grained classification than for coarse-grained tasks~\cite{ref} . 
% However, baseline methods like pseudo-supervision and doubly-robust learning cannot leverage coarse-grained pseudo-labels effectively, as they substitute them directly for target labels, even though coarse-grained labels may still contain valuable information for learning the task.
% In contrast, our approach naturally accommodates coarse-grained pseudo-labels since it is inherently a multi-task learning method. 
We demonstrate improved performance as a result of coarse-grained pseudo-labels through additional experiments on the Amazon Reviews dataset, where the original target classes are given as 5-star ratings of a product. We generate pseudo-labels from zero-shot classifications with three candidate labels: \texttt{Negative, Neutral, Positive} and include it in ZeroMatch. Detailed experimental setup is described in Appendix~\ref{sec:training-setup}.

When incorporating these coarse-grained pseudo-labels, we find that ZeroMatch consistently outperforms AdaMatch (Table~\ref{tbl:coarse}), improving robustness even when pseudo-labels differ from the SSL task. ZeroMatch gives maximum 15.2\% error reduction from getting $56.79\%$ error compared with AdaMatch achieving $64.55\%$ error. 

% Note that for this experiment, we used different set of weight decay and batch size. 



\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.48\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{icml2025/figs/pls_amazon_review_util_1.png}
         \caption{Amazon Review}
         % \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{icml2025/figs/pls_urbansound8k_util_1.png}
         \caption{Urbansound 8k}
         
     \end{subfigure}
     \caption{Pseudo-label with supervision (PLS) accuracy on utilized samples that participate in the unsupervised loss. We observe that ZeroMatch produces more accurate predictions on the utilized samples with high-quality pseudo-labels (PL set A) during training compared to AdaMatch. PLS accuracy on low-quality pseudo-labels (PL set C) in ZeroMatch is comparable to AdaMatch.}
     \label{fig:pls_utilized}
\end{figure}


\subsection{ZMT Improves Accuracy in Unlabeled Data}
To gain a deeper understanding of how ZeroMatch improves training with pseudo-labels, we compare the PLS accuracy---meaning the prediction accuracy of the unlabeled data during training---of ZeroMatch and AdaMatch on unlabeled samples utilized during training. The term ``utilized'' refers to unlabeled samples whose PLS predictions exceed AdaMatch’s pre-defined confidence threshold $\tau$, which actually participates in optimization.


% (with both high- and low-quality pseudo-labels)
In Figure~\ref{fig:pls_utilized}, we observe that the PLS accuracy is higher with high-quality pseudo-labels than AdaMatch. Notably, PLS accuracy with low-quality pseudo-labels is consistent with AdaMatch, indicating that ZMT preserves performance close to the AdaMatch by leveraging AdaMatch's intrinsic ability to predict based on labeled samples and unlabeled feature representations.  We observe similar results in the PLS accuracy of all unlabeled samples (Appendix Figure~\ref{fig:pls_unlabeled}). 
% The term ``utilized'' refers to unlabeled samples whose PLS predictions exceed AdaMatch’s pre-defined confidence threshold $\tau$, which actually participates in optimization.

% When ZeroMatch is trained with high-quality pseudo-labels, we observe improved PLS predictions during the initial training stage. 
% This results in better AdaMatch performance by incorporating more correct samples into the unsupervised loss. 
% Conversely, when inaccurate pseudo-labels are provided, ZMT maintains a PLS accuracy trend similar to AdaMatch among confident samples. 
% This suggests that ZMT preserves performance close to the SSL AdaMatch baseline by leveraging AdaMatch's intrinsic ability to predict based on labeled samples and unlabeled feature representations. 

\subsection{ZMT May Be Sensitive to Hyperparameter Selection}

We run a sensitivity analysis of ZeroMatch on the two newly introduced hyperparameters, $\alpha_p$ and $\lambda_p$, which control the scale of the pseudo-label prediction loss relative to the SSL objective function. The results of this analysis are provided in Appendix~\ref{sec:sens}.
We find that performance may be sensitive to these hyperparameters in some cases. For example, the 125-label setting for the Yelp Review dataset shows a large difference in performance ($55.32\%$ error with worst hyperparameters compared to $41.95\%$ error with best hyperparameters) as does GTZAN dataset ($66.40\%$ error with worst hyperaparameters compared to $39.80\%$ error with best hyperparameters) when run with different $\alpha_p$ and $\lambda_p$ values. In contrast, Yahoo Answers yields robustness over searched hyperparameters with less than 2\% difference overall.
The choice of best hyperparameter remained consistent across most dataset. We find that \textbf{in all NLP datasets and CIFAR100, the combination of $\alpha_p = 0$ and $\lambda_p = 1.0$ yields the best results.} For the audio datasets the best parameter found was $(\alpha_p, \lambda_p) = (1, 1.0)$ for ESC50 and $(1, 0.5)$ for GTZAN.
% file for conclusion and impact statement sections.


\section{Conclusion}

This work introduces ZMT, a framework for robustly integrating foundation model predictions into semi-supervised learning. Through extensive experiments across multiple domains, we demonstrate that ZMT achieves state-of-the-art performance while maintaining robustness to varying pseudo-label quality. Our learning-based mechanism effectively balances between limited labels and plentiful pseudo-labels, enabling practitioners to leverage foundation models without the computational overhead of direct fine-tuning. Future work could explore extending ZMT to more complex tasks and investigating theoretical guarantees for the weighting mechanism.

\ifx\isarxiv\undefined
	% for icml format
    \section*{Impact Statement}

    Our work aims to make semi-supervised learning more accessible by reducing the computational resources needed to leverage foundation models. This has positive implications for democratizing AI development, particularly benefiting researchers and organizations with limited computing infrastructure.
    
    However, improved semi-supervised learning techniques could also lower the barrier for developing AI systems with potential misuse cases. While our method focuses on improving efficiency rather than expanding capabilities, we acknowledge the need for careful consideration of deployment contexts.
    
    The adaptive weighting mechanism we introduce could potentially mitigate some biases in foundation models by automatically reducing reliance on unreliable predictions. However, this requires further investigation, particularly regarding the method's behavior across different demographic groups and task domains.

\else
	% for arxiv format
\fi



\clearpage
\bibliographystyle{unsrt}
\bibliography{ref}
\appendix
\onecolumn
\section{Problem Overview}
In Table~\ref{tab:feature-comparison}, we demonstrate how our method improves upon comparable baselines in features addressed.

\begin{table}[ht]
    \centering
    % \footnotesize
    \begin{tabular}{p{0.25\linewidth}p{0.2\linewidth}p{0.12\linewidth}p{0.15\linewidth}p{0.1\linewidth}}
        \toprule
        % \footnotesize
        & AdaMatch (or other SSL methods) & Pseudo-supervision & Doubly-robust methods & ZMT (Ours) \\
        \midrule
        \makecell{Unsupervised\\ feature learning} & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ \\
        \midrule
        \makecell{Uses foundation model\\ pseudo-labels} & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
        \midrule
        \makecell{Robust against\\ low-quality pseudo-label} & $-$ & $\times$ & $\checkmark$ & $\checkmark$ \\
        \bottomrule
    \end{tabular}    
    \caption{Feature comparison matrix. Unlike prior work, our model enables unsupervised feature learning, e.g., confidence-based sample selection or strong augmentation, leverages pseudo-labels from foundation models, and is robust against low-quality (meaning noisy or incorrect) pseudo-labels.
    }
    \label{tab:feature-comparison}
\end{table}

\section{Training Setup}
\label{sec:training-setup}

\paragraph{Setup for CIFAR-100}
We use ViT~\cite{dosovitskiy2020image} as the backbone of our model and employ pre-trained ViT-Small models with a patch size of 2 and an image size of 32, provided by the USB framework~\cite{wang2022generalizing}. The optimization is performed using the AdamW optimizer with a cosine learning rate schedule given by $\eta_t = \eta_0 \cos \left( \frac{7\pi t}{16T}\right)$ where the initial learning rate is set to $\eta_0 = 5\times10^{-4}$. We train the model for a total of $T = 204,800$ steps, with a warm-up phase of 5,120 steps. 
The batch sizes for both labeled and unlabeled data are set to 16. 
For AdaMatch-specific parameters, the threshold for the utilization cutoff mask is set to 0.95. 
For data augmentation, we apply random cropping and random horizontal flipping for weak augmentation, while RandAugment~\cite{cubuk2020randaugment} is utilized for strong augmentation. Regarding the hyperparameters specific to our method, we report results with $\lambda_p = 1$ and without annealing ($\alpha_p=0$). Detailed hyperparameter settings are provided in Table~\ref{tab:hyperparams_cifar}. For pseudo-labeling, we leverage the zero-shot classification capabilities of the CLIP~\cite{radford2021learning} foundation model. The detailed descriptions for generating pseudo-labels is provided in Appendix~\ref{sec:plabel_cifar}.

\paragraph{Setup for Natural Language Classification}
For NLP tasks, we fine-tune the pre-trained BERT-Base~\cite{devlin2018bert} using the AdamW optimizer with a cosine learning rate schedule. 
The total number of training steps is set to 102,400, with a warm-up phase of 5,120 steps. 
Both the labeled and unlabeled batch sizes are set to 4. 
All input text is truncated to ensure its length remains within the context length of BERT-Base. 
For data augmentation, we employ back-translation~\cite{zhuincorporating} using German-English and Russian-English translation as strong augmentations. No weak augmentation is applied, and the original text input is used instead. 
We run AdaMatch with a cutoff threshold of $\tau=0.95$. 
Dataset-specific hyperparameters are detailed in Table~\ref{tab:hyperparams_nlp}. Pseudo-labels are generated using Flan-T5~\cite{chung2024scaling} models with basic prompt instructions, asking the model to classify the input text’s sentiment or topic, as explained in detail in Appendix~\ref{sec:plabel_nlp}. We report the performance of ZeroMatch with $\lambda_p = 1$ and without annealing ($\alpha_p=0$). Note that for experiments with coarse-grained pseudo-labels, we use batch size $8$ for both labeled and unlabeled dataset, and weight decay $0.0005$.

\paragraph{Setup for Audio Classification}
For audio classification tasks, we utilize Wav2Vec 2.0~\cite{baevski2020wav2vec20frameworkselfsupervised} and HuBert~\cite{hsu2021hubertselfsupervisedspeechrepresentation} as the pre-trained models. 
The labeled and unlabeled batch sizes are set to 8, and the input audio signals are sampled at a rate of 16,000Hz. 
For data augmentation, we apply random sub-sampling as weak augmentation, while strong augmentation includes random modifications to the gain, pitch, and speed of the audio input, along with random sub-sampling. 
We adopt the same optimization configuration as in NLP tasks, training the model using the AdamW optimizer with a cosine learning rate schedule for a total of 102,400 steps, with the first 5,120 steps reserved for warm-up. 
For pseudo-labeling, we leverage the zero-shot classification capability of the Whisper~\cite{radford2022robustspeechrecognitionlargescale} model. 
Additionally, we refine the zero-shot predictions using a prior-matching process based on the class distribution of the dataset, following~\cite{ma2024}. 
The detailed procedure is explained in Appendix~\ref{sec:plabel_audio}.
When running ZMT, we observed that some audio classification tasks performed significantly better with annealing applied ($\alpha_p=1$), therefore we report the results with this setting. 
Detailed hyperparameter settings for each dataset are provided in Table~\ref{tab:hyperparams_audio}. 

\begin{table}[h!]
\centering
\caption{Hyperparameters of CIFAR100.}
\begin{tabular}{@{}c|c@{}}
    \toprule
    Image Size              & 32                                                     \\ 
    Model                   & ViT-S-P4-32                                            \\
    Weight Decay            & 5e-4                                                   \\
    Labeled Batch size      & 16                                                     \\
    Unlabeled Batch size    & 16                                                     \\
    Learning Rate           & 5e-4                                                   \\
    Layer Decay Rate        & 0.5                                                    \\
    Scheduler               & $\eta = \eta_0 \cos \left( \frac{7\pi k}{16K} \right)$ \\
    Model EMA Momentum      & 0.0                                                    \\
    Prediction EMA Momentum & 0.999                                                  \\
    Weak Augmentation       & Random Crop, Random Horizontal Flip                    \\
    Strong Augmentation     & RandAugment~\cite{cubuk2020randaugment}                  \\
    $\alpha_p$       & 0                    \\
    $\lambda_p$     & 1.0                \\ \bottomrule
\end{tabular}
\label{tab:hyperparams_cifar}
\end{table}

\begin{table}[h!]
\centering
\caption{Hyperparameters of NLP tasks.}
\begin{tabular}{@{}c|cccc@{}}
\toprule
Dataset                 & AG News      & Yahoo! Answer      & Amazon Review     & Yelp Review     \\ \midrule
Max Length              & \multicolumn{4}{c}{512}                                                 \\ \midrule
Model                   & \multicolumn{4}{c}{Bert-Base}                                           \\ \midrule
Weight Decay            & \multicolumn{4}{c}{1e-4}                                                \\ \midrule
Labeled Batch size      & \multicolumn{4}{c}{4}                                                   \\ \midrule
Unlabeled Batch size    & \multicolumn{4}{c}{4}                                                   \\ \midrule
Learning Rate           & 5e-5         & 1e-4               & 5e-5              & 5e-5            \\ \midrule
Layer Decay Rate        & 0.65         & 0.65               & 0.75              & 0.75            \\ \midrule
Scheduler               & \multicolumn{4}{c}{$\eta = \eta_0 \cos\left(\frac{7\pi k}{16K}\right)$} \\ \midrule
Model EMA Momentum      & \multicolumn{4}{c}{0.0}                                                 \\ \midrule
Prediction EMA Momentum & \multicolumn{4}{c}{0.999}                                               \\ \midrule
Weak Augmentation       & \multicolumn{4}{c}{None}                                                \\ \midrule
Strong Augmentation     & \multicolumn{4}{c}{Back-Translation~\cite{qizhe2020}}  \\ \midrule
$\alpha_p$      & \multicolumn{4}{c}{0}  \\ \midrule
$\lambda_p$     & \multicolumn{4}{c}{1.0}  \\ \bottomrule
\end{tabular}
\label{tab:hyperparams_nlp}
\end{table}

\begin{table}[h]
\centering
\caption{Hyperparameters of audio classification tasks.}
\begin{tabular}{@{}c|ccc@{}}
\toprule
Dataset                 & GTZAN                     & UrbanSound8k                & ESC-50               \\ \midrule
Sampling Rate           & \multicolumn{3}{c}{16,000}                                                     \\ \midrule
Max Length (Sec.)       & 3.0                       & 4.0                         & 5.0                  \\ \midrule
Model                   & Wav2Vecv2-Base            & HuBERT-Base          & HuBERT-Base          \\ \midrule
Weight Decay            & \multicolumn{3}{c}{5e-4}                                                       \\ \midrule
Labeled Batch size      & \multicolumn{3}{c}{8}                                                          \\ \midrule
Unlabeled Batch size    & \multicolumn{3}{c}{8}                                                          \\ \midrule
Learning Rate           & 2e-5                      & 5e-5                        & 1e-4                 \\ \midrule
Layer Decay Rate        & 1.0                       & 0.75                        & 0.85                 \\ \midrule
Scheduler               & \multicolumn{3}{c}{\(\eta = \eta_0 \cos\left(\frac{7\pi k}{16K}\right)\)}      \\ \midrule
Model EMA Momentum      & \multicolumn{3}{c}{0.0}                                                        \\ \midrule
Prediction EMA Momentum & \multicolumn{3}{c}{0.999}                                                      \\ \midrule
Weak Augmentation       & \multicolumn{3}{c}{Random Sub-sample}                                          \\ \midrule
Strong Augmentation     & \multicolumn{3}{c}{Random Sub-sample, Random Gain, Random Pitch, Random Speed} \\ \midrule
$\alpha_p$              & 1                         & 1                           & 0                    \\ \midrule
$\lambda_p$             & 0.5                       & 1.0                         & 0.5                  \\ \bottomrule
\end{tabular}
\label{tab:hyperparams_audio}
\end{table}


\section{Details of Pseudo-label Generation Process}
\label{sec:plabel}

\subsection{CIFAR100}
\label{sec:plabel_cifar}
To generate the pseudo-labels for CIFAR100 dataset using CLIP model, we follow the procedures of zero-shot classification in ~\cite{radford2021learning}. 
We compute the feature embedding of a given image, and for each label in candidate labels, we prompt the model with following prompt: \texttt{This is a photo of \{label\}} and get feature embedding. 
We then obtain the pseudo-label by finding the label that gives closest embedding to the image embedding, based on cosine similarity.

\subsection{NLP classification tasks}
\label{sec:plabel_nlp}
\subsubsection{Topic classification: AG News, Yahoo Answers}
To obtain pseudo-labels for topic classification tasks, we use following prompting template with FLAN-T5 models for given input text:

\begin{table}[h]
\centering
\begin{tabular}{|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Select the topic that the given article is about. The topics are: \textit{\{candidate\_labels\}}.\\ \\ Article: \textit{\{input\_text\}},\\ Answer:\end{tabular} \\ \hline
\end{tabular}
\end{table}

For AG News, the candidate labels are \texttt{world, sports, business, technology}, and for Yahoo Answers, the candidate labels are \texttt{society, science, health, education, computer, sports, business, entertainment, relationship, politics}.
In case the output does not contain labels from candidate labels or is not parsable, we set the pseudo-label to be
\texttt{world} for AG News, and \texttt{society} for the Yahoo Answers as the default label.

When obtaining pseudo-labels with Roberta-large, We simply compare the cosine similarities of encoder representeations of input text and labels, similar to procedure described in Appendix~\ref{sec:plabel_nlp}.

\subsubsection{Amazon Review, Yelp Review}
We use following prompt template with FLAN-T5 models for given input text to obtain the review text's sentiment:

\begin{table}[h]
\centering
\begin{tabular}{|l|}
\hline
\begin{tabular}[c]{@{}l@{}}\textit{\{input\_text\}}\\ What is the sentiment of this review?\\ \\ OPTIONS:\\ - very negative\\ - negative\\ - neutral\\ - positive\\ - very positive\end{tabular} \\ \hline
\end{tabular}
\end{table}

In case of model output not being recognized as one of candidate labels, we select \texttt{neutral} as the default pseudo-label.

\subsection{Audio classification tasks}
\label{sec:plabel_audio}

For obtaining pseudo-labels with Whisper~\cite{radford2022robustspeechrecognitionlargescale} model, we follow zero-shot approach introduced in ~\cite{ma2024}.
Similar to procedure described in Appendix~\ref{sec:plabel_nlp}, we first compute the feature representations of given audio and possible labels and compute the cosine similarity score for each pair of input audio and labels. Below table lists the prompt used for each dataset.
\begin{table}[h]
\centering
\caption{Prompt used in generating pseudo-labels for audio tasks.}
\begin{tabular}{l|l}
\hline
Task                 & Prompt                                      \\ \hline
GTZAN                & This is an audio of \textit{\{class\_label\}} music. \\
Urbansound 8K, ESC50 & This is a sound of \textit{\{class\_label\}}.        \\ \hline
\end{tabular}
\end{table}

We also apply prior-matching process introduced in ~\cite{liusie2023}. After obtaining the representations of audio and labels, normalized probability score of each class label is computed with softmax. Prior-matching procedure then re-weights class probability so that the output prior matches the true prior, which normally assumes uniform distribution. 
For detailed desciption of prior-matching with audio data, we refer to~\cite{ma2024}.

\section{Additional Experiments}
\subsection{Plotting PLS accuracy of unlabeled samples}
We plot the pseudo-label with supervsion (PLS) of all unlabeled samples during training in Figure ~\ref{fig:pls_unlabeled}.

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{icml2025/figs/pls_amazon_review_all_1.png}
         \caption{Amazon Review.}
         % \label{fig:}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{icml2025/figs/pls_urbansound8k_all_1.png}
         \caption{Urbansound 8k.}
         % \label{fig:pls-unlabeled}
     \end{subfigure}
     \caption{Pseudo-label with supervision accuracy on unlabeled samples. We observe that ZMT produces more accurate predictions on the unlabeled samples during training.}
     \label{fig:pls_unlabeled}
\end{figure}


\section{Sensitivity Analysis}
\label{sec:sens}

Sensitivity analysis results over hyperparmeters $(\alpha_p, \lambda_p)$ on  CIFAR100  are given in Table ~\ref{tab:sens_cifar}, and results on Yahoo Answers, Yelp Review are shown in Table~\ref{tab:sens_nlp}. Results on ESC50 and GTZAN are shown in Table~\ref{tab:sens_audio}.

\begin{table}[h]
\centering
\caption{Performance(error) of ZeroMatch with different configurations of $(\alpha_p, \lambda_p)$ on CIFAR100.}
\begin{tabular}{|c|c|ccc|}
\hline
PL Set             & Label size & 100                 & 200                 & 400                 \\ \hline
\multirow{4}{*}{A} & (0, 0.5)   & 26.03±1.89          & 18.15±0.59          & 14.68±0.15          \\
                   & (0, 1.0)   & \textbf{24.84±1.61} & \textbf{18.70±1.60} & \textbf{14.67±0.31} \\
                   & (1, 0.5)   & 28.43±0.66          & 20.67±1.09          & 15.97±0.37          \\
                   & (1, 1.0)   & 27.99±1.30          & 21.00±1.50          & 15.47±0.13          \\ \hline
\multirow{4}{*}{B} & (0, 0.5)   & 28.23±1.02          & 22.19±1.43          & 16.20±0.56          \\
                   & (0, 1.0)   & \textbf{26.03±1.97} & 21.75±0.51          & 16.32±0.22          \\
                   & (1, 0.5)   & 27.79±0.82          & \textbf{19.95±0.89} & \textbf{15.80±0.12} \\
                   & (1, 1.0)   & 30.92±1.70          & 21.58±1.41          & 16.13±0.42          \\ \hline
\end{tabular}
\label{tab:sens_cifar}
\end{table}

\begin{table}[h!]
\centering
\caption{Performance(error) of ZeroMatch with different configurations of $(\alpha_p, \lambda_p)$ on NLP tasks.}
\begin{tabular}{|c|c|ccc|ccc|}
\hline
                   & Dataset    & \multicolumn{3}{c|}{Yahoo Answers}                              & \multicolumn{3}{c|}{Yelp Review}                                \\ \hline
PL Set             & Label size & 250                 & 500                 & 2000                & 125                 & 250                 & 1000                \\ \hline
\multirow{4}{*}{A} & (0, 0.5)   & 31.49±0.20          & 30.12±0.18          & 29.15±0.42          & 49.74±2.18          & 45.04±1.44          & 37.52±0.67          \\
                   & (0, 1.0)   & \textbf{29.99±0.63} & 30.24±0.22          & \textbf{28.78±0.27} & \textbf{41.95±1.19} & \textbf{42.61±2.65} & \textbf{36.35±1.36} \\
                   & (1, 0.5)   & 30.99±0.47          & 30.36±0.56          & 29.11±0.08          & 55.32±4.23          & 48.59±2.91          & 38.98±0.78          \\
                   & (1, 1.0)   & 31.63±2.12          & \textbf{29.93±0.33} & 29.30±0.22          & 47.19±1.98          & 44.71±1.58          & 37.61±0.25          \\ \hline
\multirow{4}{*}{C} & (0, 0.5)   & 33.71±1.48          & \textbf{32.47±0.51} & \textbf{30.56±0.11} & 57.61±0.57          & 49.45±2.18          & \textbf{40.41±0.86} \\
                   & (0, 1.0)   & \textbf{33.40±0.13} & 32.95±0.44          & 30.78±0.53          & \textbf{56.12±4.36} & \textbf{46.17±2.19} & 41.69±0.45          \\
                   & (1, 0.5)   & 33.96±0.45          & 32.97±1.13          & 30.61±0.41          & 58.78±2.93          & 49.92±2.07          & 41.48±0.59          \\
                   & (1, 1.0)   & 34.05±0.36          & 32.64±0.18          & 31.24±0.31          & 58.84±2.40          & 47.54±3.15          & 40.77±0.59          \\ \hline
\end{tabular}
\label{tab:sens_nlp}
\end{table}


\begin{table}[h!]
\centering
\caption{Performance(error) of ZeroMatch with different configurations of $(\alpha_p, \lambda_p)$ on audio tasks.}
\begin{tabular}{|c|c|cc|ccc|}
\hline
                   & Dataset    & \multicolumn{2}{c|}{ESC50}                & \multicolumn{3}{c|}{GTZAN}                                      \\ \hline
PL Set             & label size & 250                 & 500                 & 50                  & 100                 & 400                 \\ \hline
\multirow{4}{*}{A} & (0, 0.5)   & \textbf{33.50±0.72} & \textbf{30.00±1.96} & 35.60±2.14          & 33.40±0.85          & 22.40±0.49          \\
                   & (0, 1.0)   & 34.50±1.41          & 31.50±1.76          & 45.47±1.52          & 39.87±2.54          & 25.40±1.38          \\
                   & (1, 0.5)   & 34.75±1.20          & 30.00±1.81          & 34.67±1.44          & 30.33±1.58          & \textbf{20.40±1.46} \\
                   & (1, 1.0)   & 34.50±2.06          & 28.00±1.12          & \textbf{34.60±2.27} & \textbf{28.33±2.68} & 21.87±0.35          \\ \hline
\multirow{4}{*}{C} & (0, 0.5)   & 43.75±0.77          & 32.75±1.01          & 49.93±1.46          & 38.20±0.76          & 25.20±0.57          \\
                   & (0, 1.0)   & 47.00±2.07          & 36.25±1.16          & 66.40±4.75          & 44.20±0.37          & 28.27±0.80          \\
                   & (1, 0.5)   & 40.50±1.59          & 33.75±2.84          & \textbf{39.80±3.43} & \textbf{33.27±1.33} & \textbf{23.40±0.54} \\
                   & (1, 1.0)   & \textbf{39.25±1.81} & \textbf{32.75±0.66} & 45.27±0.24          & 37.27±0.65          & 24.67±0.98          \\ \hline
\end{tabular}
\label{tab:sens_audio}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}