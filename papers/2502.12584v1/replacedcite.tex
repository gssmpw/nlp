\section{Related Work}
\label{sec:related}
% \tba

\subsection{Semi-Supervised Learning}

Semi-supervised learning (SSL) has evolved from foundational consistency regularization methods to sophisticated deep learning approaches. The $\Pi$-Model____ and Mean Teacher____ established core principles by enforcing consistent predictions across different model states. FixMatch____ later unified these ideas by combining weak and strong augmentations with pseudo-labeling. Subsequent work focused on reliability: UPS____ introduced confidence-based filtering while FlexMatch____ developed adaptive thresholding for pseudo-label selection. Recent approaches like SimMatch____ have further advanced the field by incorporating contrastive learning principles.


% Semi-supervised learning (SSL) has emerged as a crucial paradigm for leveraging unlabeled data to improve model performance when labeled data is scarce. Especially popular in computer vision work, traditional SSL approaches build on consistency regularization principles____. These methods typically enforce consistent predictions across different augmentations of the same input. The field has since evolved toward more sophisticated techniques, with "Meta Pseudo Labels" (Pham et al., 2021) introducing meta-learning for pseudo-label refinement and "SimMatch: Semi-supervised Learning with Similarity Matching" (Zheng et al., 2022) leveraging similarity-based matching. Recent advances focus on reliability issues, as seen in "Robust Pseudo-Labels through Confidence Calibration" (Chen et al., 2023) and "Ensemble-based Self-training for Foundation Models" (Wang et al., 2023). Some researchers have explored alternative learning objectives, such as "ContrastMatch: Self-supervised Learning for Semi-supervised Classification" (Wei et al., 2023), which combines contrastive and pseudo-labeling approaches. Despite these advances, a key challenge remains in effectively balancing supervised and unsupervised learning signals while maintaining robustness to varying data quality.

\subsection{SSL with Side Information}

The integration of additional information sources has significantly enhanced SSL performance. S4L____ pioneered this direction by incorporating self-supervised pretext tasks into the SSL framework. Building on this foundation, CoMatch____ introduced graph-based relationships to capture sample similarities, while MPL____ employed meta-learning within teacher-student frameworks to refine pseudo-labels. More recent approaches have explored diverse information sources: WiSE-FT____ leverages weak supervision signals to guide model training, and SSDG____ taps into external knowledge bases to generate more reliable pseudo-labels. 

% The integration of external knowledge sources has significantly enhanced SSL performance. Doubly Robust self-training (Wei et al., 2023) improves pseudo-label reliability through probability calibration. Recent works leverage LLMs for pseudo-labeling, as seen in TabLLM (Wang et al., 2023) and code-switching ASR (Li et al., 2024). Weak supervision approaches combine multiple noisy sources with limited labeled data, as demonstrated in "Learning from Rules Generalizing Labeled Exemplars" (Zhou et al., 2024) and "Self-Training with Weak Supervision" (Karamanolakis et al., 2021). These methods show particular promise in scenarios where obtaining clean labels is costly.

\subsection{Foundation Models in SSL}
The integration of foundation models into SSL frameworks is an emerging direction. As foundation models show impressive zero-shot performance____, the question becomes: how can we incorporate foundation models into SSL frameworks? The three main emerging approaches are: 1) pre-training on the unlabeled data before fine-tuning on the labeled data, or 2) using a teacher model to assign pseudo-labels____. The first approach provides a strong and robust learner but requires significant compute for the fine-tuning step____. The second approach leverages the teacher model for inference____. In contrast, our model leverage foundation models as a pseudo-label generator. Motivated by the concept of unreliable pseudo-labels, researchers have also proposed theoretical guarantees to accommodate potentially noisy pseudo-labels____. Our work builds on this methodological foundation to develop a practical method for incorporating noisy pseudo-labels.

% \todo{Ask Jichan, are we the first to use foundation model pseodulabels direclty?}