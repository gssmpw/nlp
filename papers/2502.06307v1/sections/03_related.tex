\section{Background and Related Work}
\label{sec:related}

\subsection{Transformers for Vision}
\label{sec:related:transformers}

\paragraph{Vision Transformer (ViT) \cite{dosovitskiy2020vit}} ViT is a pioneering work using transformers for vision tasks, concretely for image classification. The model tokenizes input images by partitioning them into non-overlapping patches. The patches are linearly projected and ordered in a sequence. A two-dimensional positional encoding is used to overcome the permutation invariant nature of self-attention mechanism. Additionally, a [CLASS] token is appended at the beginning of the sequence as global token for image classification after multiple transformer layers. The transformer layer architecture comprises a multi-headed self-attention module (MHA), a multi-layer perceptron (MLP) as well as two normalization layers (LN). The output of layer $(l)$ is obtained as:

\begin{align}
    \label{eq:vit}
    \mathbf{\hat{z}}^{(l)} = \text{MHA}( \text{LN}(\mathbf{z}^{(l-1)}) ) + \mathbf{z}^{(l-1)} \\
    \mathbf{z}^{(l)} = \text{MLP}( \text{LN}(\mathbf{\hat{z}}^{(l-1)}) ) + \mathbf{\hat{z}}^{(l)}
\end{align}

\paragraph{Swin Transformer (Swin) \cite{liu2021swin}} Swin is an adaptation of the transformer architecture specifically designed for images, incorporating inductive biases for tasks requiring higher resolutions such as segmentation and detection. The model introduces a hierarchical structure obtained via window attention and shifted windows (S-WMHA). Swin also partitions the image into patches, usually smaller than ViT, and performs self-attention within local windows of patches. Windows are shifted between layers to capture cross-window interactions. The model is composed of four sequential stages, each working at different resolutions and involving multiple transformer layers. At the beginning of every stage, neighboring patches are combined to create the hierarchical structure. Similarly to ViT, the output of layer $(l)$ is computed as:

\begin{align}
\label{eq:swin}
\mathbf{\hat{z}}^{(l)} = \text{S-WMHA} \left [ \text{LN} \left ( \mathbf{z^{(l-1)}} \right ) \right ] + \mathbf{z^{(l-1)}}\\
\mathbf{z}^{(l)} =  \text{MLP} \left [ \text{LN} \left ( \mathbf{\hat{z}^{(l)}} \right ) \right ] + \mathbf{\hat{z}}^{(l)}
\end{align}


\paragraph{Detection Transformer (DETR) \cite{carion2020end}} DETR introduces a novel approach to end-to-end object detection by formulating it as a direct set prediction problem. The model consists of a backbone network that extracts image features, which are then flattened into a sequence of tokens and processed through a transformer encoder. A set of learnable object queries is then fed into the transformer decoder, where they interact with the encoded features to predict bounding boxes and class labels for potential objects in the image. Each query produces a confidence score that indicates the presence of an object, allowing the model to determine which queries correspond to actual objects.

DETR is trained end-to-end using Hungarian matching \cite{kuhn1955hungarian}, a method that matches predicted objects to ground truth objects in a one-to-one manner. To ensure that all potential objects are detected, the number of object queries must exceed the maximum number of objects in an image. A key innovation of DETR is its elimination of traditional detection components such as anchors and non-maximum suppression, thereby simplifying the detection pipeline. Instead, DETR relies on the confidence scores output by each query to filter out non-object queries during inference and evaluation by applying a threshold to these scores.

\paragraph{Deformable Detection Transformer (Deformable-DETR) \cite{zhu2020deformable}} Deformable-DETR enhances DETR by replacing the global self-attention mechanism with deformable attention. Deformable attention is designed to limit the attention scope of each token to a set of learnable key sampling points around a reference point, rather than considering all possible locations. This targeted attention mechanism significantly improves the model's ability to handle small objects and densely packed scenes. Moreover, recognizing the benefits of multi-scale representations in object detection, the authors extend deformable attention to operate across multiple scales of hierarchical feature maps produced by the backbone. This extension allows Deformable-DETR to better manage objects of varying sizes and improves its overall detection performance.


\subsection{Cell Segmentation, Detection and Classification}
\label{sec:related:cell}

Given the importance of cell information for digital pathology image analysis, tasks such as cell segmentation, detection, and classification are highly relevant due to their contribution to understanding tissue architecture and disease pathology. Cell segmentation \cite{graham2019hover,baumann2024hover,anglada2024enhancing} has traditionally been more popular than detection due to the small size and frequent overlap of cell nuclei, which pose challenges for detection methods.

\paragraph{HoVer-Net \cite{graham2019hover}} HoVer-Net is a convolutional architecture designed to simultaneously solve cell segmentation and classification in histopathological images. The architecture consists of a U-Net \cite{ronneberger2015u} model with three parallel decoder branches, each addressing a different pixel-wise task. The nuclear pixel (NP) branch predicts the probability of each individual pixel belonging to a cell instance. The Horizontal-Vertical (HV) branch predicts the horizontal and vertical distances of each pixel to its corresponding cell instance center of mass, if present. The classification branch predicts the class label for each pixel. HoVer-Net effectively overcomes the overlap between cells by utilizing a post-processing step that combines the outputs of the NP and HV branches to identify distinct cell instances accurately.

While HoVer-Net demonstrates great potential, it faces several challenges in practical applications. Specifically, the model's inference time on WSIs can be considerable, which may limit its utility. Additionally, its performance on certain cell classes leaves room for improvement, suggesting that further optimizations are needed to enhance its effectiveness across a broader range of conditions.

\paragraph{CellViT \cite{h√∂rst2023cellvit}} With the rise of transformers in vision tasks, CellViT extends HoVer-Net by incorporating a Vision Transformer (ViT) as its encoder, enhancing the model's ability to capture longer distance dependencies and improve feature representations by leveraging self-attention. Given the single-scale nature of ViTs, the model incorporates upsampling modules in the skip connections to handle multi-scale information. These skip connections are taken from different layers of the ViT. The architecture retains the three parallel decoder branches from HoVer-Net. The ViT encoder enables the model to handle different-sized images without relying on a sliding window approach for inference on larger tiles. The authors observed that, while being much more efficient, directly inferring on tiles of $1024 \times 1024$ pixels achieves similar performance to using an overlapped sliding window approach with smaller windows matching the training set image size.

The motivation of developing faster pipelines for inference on WSIs has also inspired concurrent work in the field, with different approaches being explored to address similar challenges.

\paragraph{HoVer-NeXt \cite{baumann2024hover}} HoVer-NeXt enhances the segmentation and classification performance of HoVer-Net by replacing the traditional convolutional encoder with a ConvNeXt-V2 backbone. Additionally, HoVer-NeXt incorporates test-time augmentations to further refine predictions and enhance robustness. In terms of efficiency, HoVer-NeXt significantly speeds up the inference process on WSIs. Indeed, their pipeline is $\times17$ and $\times5$ faster than HoVer-Net and CellViT, respectively. The model simplifies the three-branch decoder architecture of HoVer-Net by consolidating it into two branches: one for instance segmentation and another for classification. The instance segmentation branch is designed as a three-class pixel segmentation task, distinguishing between background, foreground, and border regions. For WSI inference, HoVer-NeXt processes larger patches compared to HoVer-Net and utilizes a "stitcher" module to combine predictions from multiple patches.

Despite the advancements introduced by HoVer-NeXt, its detection and classification performance remains lower than that of CellViT. Additionally, as segmentation-based methods, they both still require significant computational resources for post-processing, which can be particularly demanding in large WSIs. This is notable because the primary outputs of interest in digital pathology are the cell centroids and their corresponding labels, rather than the full segmentation masks.

In our work, we demonstrate that by directly performing detection rather than segmentation to extract nuclei information, it is possible to achieve superior detection and classification performance while also significantly improving computational efficiency. Detection methods tend to be less computationally intensive than segmentation, and the absence of post-processing further streamlines the pipeline. This increased efficiency and accuracy make our approach particularly well-suited for implementation in clinical practice, where quick and reliable analysis of WSIs is crucial for diagnostic and prognostic decision-making.