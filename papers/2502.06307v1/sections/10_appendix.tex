\appendix

\section{Implementation Details}

This section outlines the specifics of the training and inference processes, including the computational resources utilized, hyperparameters, and data augmentation techniques. All models were implemented in PyTorch, based on the official repositories of Deformable-DETR \cite{zhu2020deformable} and DETR \cite{carion2020end}.

\subsection{Training and Evaluation Details}

The training was conducted on a cluster equipped with four NVIDIA Quadro 16GB GPUs, 16GB of RAM, and 8 CPUs. The training hyperparameters, such as learning rate, batch size, and other optimization parameters, are summarized in Table \ref{tab:hyperparams}. The batch size indicated in the table corresponds to the batch size per GPU. The learning rate was linearly scaled according to the batch size and the number of devices, based on the original batch size of 16 from the Deformable-DETR paper, to ensure consistent optimization across different hardware configurations.

Data augmentation techniques employed during training—such as rotation, flipping, and stain augmentations—are detailed in Table \ref{tab:data-augmentation}. These augmentations help enhance model generalization by simulating various staining protocols and slide preparation techniques.

The models for PanNuke dataset results presented in Tables \ref{tab:pannuke} and \ref{tab:pannuke05} were trained using the standard three-fold split provided with the dataset. Although we did not perform an exhaustive hyperparameter search, the default hyperparameters were used, with validation sets from each fold employed to avoid overfitting and to determine the optimal confidence threshold for filtering out no-object queries. The threshold was selected by maximizing the harmonic mean between the detection F1-score and the macro-average classification F1-score, ensuring balanced performance across both metrics.

For cross-dataset evaluations in Table \ref{tab:consep}, models were trained on 80\% of the PanNuke dataset, with 80\% taken from each fold to create an expanded training set. The remaining 20\% served as a validation set to prevent overfitting and to fine-tune the confidence threshold. For the MoNuSeg dataset, which includes only detection labels, the threshold was chosen to maximize the detection F1-score on the PanNuke validation set (20\%). Although using CoNSeP and MoNuSeg training sets could potentially yield better thresholds, we opted to base this decision solely on the PanNuke dataset to maintain a strict cross-dataset evaluation protocol.

\subsection{Inference Details}

Inference on WSIs for CellNuc-DETR, CellViT, and HoVerNeXt was conducted using an NVIDIA GeForce 24GB GPU, along with 128GB of RAM and 8 CPUs. Additional hyperparameters related to the inference process, including tile sizes, overlaps, and batch processing strategies, are provided in Table Z. These parameters were carefully selected to strike a balance between computational efficiency and model accuracy.

\begin{table}[h]
\centering
\caption{Training hyperparameters}
\label{tab:hyperparams}
\begin{tabular}{l|cc}
\toprule
\textbf{Group} & \textbf{Hyperparameter} & \textbf{Value} \\
\midrule
\multirow{4}{*}{\textbf{Solver}} & Epochs & 100 \\
 & Base LR & 2e-4 \\
 & Batch Size & 2 \\
 & LR Drop & 0.10 \\
 & LR Steps & 70, 90 \\
\midrule
\multirow{3}{*}{\textbf{Matcher}} & $\lambda_{\text{giou}}$ & 2 \\
 & $\lambda_{\text{bbox}}$ & 2 \\
 & $\lambda_{\text{focal}}$ & 5 \\
\midrule
\multirow{4}{*}{\textbf{Loss}} & $\lambda_{\text{giou}}$ & 2 \\
 & $\lambda_{\text{bbox}}$ & 1 \\
 & $\lambda_{\text{focal}}$ & 5 \\
 & $\alpha_{\text{focal}}$ & 0.25 \\
\bottomrule

\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{Data augmentation hyperparameters}
\label{tab:data-augmentation}
\begin{tabular}{lc|cc}
\toprule
\textbf{Augmentation} & \textbf{Probability} & \textbf{Hyperparameter} & \textbf{Value} \\ 
\midrule
\multirow{2}{*}{Elastic} & \multirow{2}{*}{0.2} & $\alpha$ & 0.5 \\ 
 &  & $\sigma$ & 0.25 \\ 
\midrule
Horizontal Flip & 0.5 & - & - \\ 
\midrule
Vertical Flip & 0.5 & - & - \\ 
\midrule
\multirow{1}{*}{Rotate} & \multirow{1}{*}{1.0} & angles & [0,90,180,270] \\ 
\midrule
\multirow{2}{*}{Blur} & \multirow{2}{*}{0.2} & kernel size & 9 \\ 
 &  & $\sigma$ & [0.2, 1.0] \\ 
\midrule
\multirow{2}{*}{HED Transform} & \multirow{2}{*}{0.2} & $\alpha$ & 0.04 \\ 
 &  & $\beta$ & 0.04 \\ 
\midrule
\multirow{2}{*}{Resized Crop} & \multirow{2}{*}{0.2} & size & 256 \\ 
 &  & scale & [0.8, 1.0] \\ 
\bottomrule
\end{tabular}
\end{table}
