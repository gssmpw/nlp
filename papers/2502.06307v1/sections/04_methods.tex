\section{Materials and Methods}
\label{sec:methods}

\subsection{Datasets}
\label{sec:methods:datasets}

In this section, we describe the datasets used in our experiments. Each dataset serves a specific purpose in evaluating and enhancing models for cell nuclei detection, classification, and inference efficiency.

\paragraph{TCGA} The Cancer Genome Atlas (TCGA) is a public repository that contains a large collection of Whole Slide Images (WSIs) from a wide variety of cancer types. The dataset includes histopathological images from over 11,000 patients, representing diverse tumor types such as lung, breast, colon, prostate, and kidney cancers. To evaluate the efficiency of the CellNuc-DETR pipeline and compare it with existing cell segmentation methods in real-world scenarios, we utilize 20 WSIs from TCGA and report the inference times. 

\paragraph{PanNuke} 
The PanNuke dataset \cite{gamper2020pannuke} consists of 7,904 image patches, each with dimensions of $256 \times 256$ pixels, derived from WSIs within TCGA. These patches cover 19 different tissue types, all captured at a $40\times$ magnification. The dataset includes detailed annotations for 189,744 nuclei, classified into five clinically relevant categories: neoplastic, inflammatory, connective, necrotic, and epithelial. Figure \ref{fig:pannuke-stats} illustrates the distribution of cell counts across tissues and nucleus types, highlighting the significant class imbalance both within nucleus types and across different tissues. This variability in cell count and distribution makes PanNuke a challenging dataset, but also a valuable resource for addressing generalization in digital pathology. Such variability is crucial for developing algorithms that can perform well across different tissue types and staining protocols. The dataset is divided into three predefined folds for fair model comparison, with fold partitioning designed to ensure each fold contains an equal portion of the smallest class \cite{gamper2020pannuke}.

%The PanNuke dataset \cite{gamper2020pannuke} consists of 7,904 image patches, each with dimensions of $256 \times 256$ pixels, derived from WSIs within The Cancer Genome Atlas (TCGA) dataset. These patches cover 19 different tissue types, all captured at a $40 \times$ magnification. The dataset includes detailed annotations for 189,744 nuclei, classified into five clinically relevant categories: neoplastic, inflammatory, connective, necrotic, and epithelial. The class labels are imbalanced, reflecting the natural frequency distribution of the various cell types. The statistics of class imbalance across tissues and nuclei types can be found in Figure \ref{fig:pannuke-stats}. The dataset is divided into three predefined folds for a fair model comparison. The fold partition is designed so that each fold contains an equal posrtion of the smallest class \cite{gamper2020pannuke}.

\paragraph{CoNSeP}
The CoNSeP dataset \cite{graham2019hover} comprises 41 tiles, each measuring $1000 \times 1000$ pixels, sourced from H\&E-stained colorectal adenocarcinoma WSIs at a $40 \times$ magnification. This dataset is notably diverse, covering various tissue regions including stromal, glandular, muscular, collagenous, adipose, and tumorous areas. It features a wide array of nuclei from different cell types, initially labeled as normal epithelial, malignant/dysplastic epithelial, fibroblast, muscle, inflammatory, endothelial, or miscellaneous. The miscellaneous category includes necrotic, mitotic, and unclassifiable cells. Following previous work \cite{graham2019hover}, we consolidated the normal and malignant/dysplastic epithelial nuclei into a single class (epithelial), and combined the fibroblast, muscle, and endothelial nuclei into a class named spindle-shaped nuclei. The dataset is divided into predefined training and validation sets to facilitate model development and evaluation.

\paragraph{MoNuSeg}
The MoNuSeg dataset \cite{kumar2019multi} includes 44 images, each with dimensions of 1000 × 1000 pixels at a $40 \times$ magnification. Sourced from H\&E-stained tissue samples from various organs in the TCGA archive, this dataset provides detailed annotations of nuclear boundaries. It is divided into a training set with 30 images, containing around 22,000 nuclear boundary annotations, and a test set with 14 images, including an additional 7,000 annotations. Notably, this dataset does not provide class labels for the nuclei, focusing solely on detection and segmentation.

%\paragraph{Camelyon16}
%The Camelyon16 dataset \cite{bejnordi2017diagnostic} comprises 400 H\&E-stained WSIs of lymph node sections, scanned at $40 \times$ magnification. Each WSI includes annotations identifying tumor and normal regions. The WSIs have average dimensions of 189,832 × 95,590 pixels, with approximately 29\% of each slide representing tissue area.

The datasets used in our study provide image patches of varying sizes, each tailored to specific research objectives. The PanNuke dataset consists of patches sized $256 \times 256$ pixels at 0.25 $\mu$m/px, translating to an area of 4096 $\mu$m$^2$. These patches are employed for both training and evaluating cell nuclei detection and classification models. The CoNSeP and MoNuSeg datasets offer larger patches, each measuring 1000$\times$1000 pixels, covering an area of 62,500 $\mu$m$^2$. These datasets are utilized for cross-domain evaluation to assess the generalizability of our models. The slides extracted from the TCGA have an average area of 300 mm$^2$ per slide, used specifically to evaluate the inference times of the models. This approach ensures robust training, evaluation, and performance assessment across diverse tissue types and scales.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/pannuke_plots.pdf}
    \caption{Cell nuclei class distribution across tissues on the PanNuke dataset.}
    \label{fig:pannuke-stats}
\end{figure}

\subsection{Cell Nuclei detection and Classification with Transformers}
\label{sec:methods:CellNuc-DETR}

\subsubsection{Architecture}
\label{sec:methods:CellNuc-DETR:architecture}

The architecture employed in our study adopts a hierarchical backbone that generates a multi-level feature pyramid from input images. This is followed by a multi-scale deformable transformer \cite{zhu2020deformable}, including the encoder and decoder components. Figure \ref{fig:architecture} shows the model architecture. The encoder enhances input features through multi-scale deformable self-attention mechanisms, while the decoder outputs predictions for bounding boxes and labels based on a set of object queries. These queries are initialized to represent potential object locations within the input images following the two-stage approach  \cite{zhu2020deformable}. Both the backbone and transformer components are pretrained on large-scale datasets to capture diverse feature representations. Our experiments explore various backbone architectures and transformer configurations to assess their impact on detection and classification performance.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/architecture.png}
    \caption{CellNuc-DETR model architecture.}
    \label{fig:architecture}
    \footnotesize{The model consists of a hierarchical backbone, such as a Swin Transformer, which outputs feature maps at multiple resolutions. These maps are flattened and embedded with positional and level information to form input sequences for the deformable transformer encoder. The encoder processes these sequences using multi-scale deformable attention (MSDefAttn). It then generates initial bounding boxes, which are used as input queries for the decoder and as memory for the deformable cross-attention modules. The model outputs bounding box and label predictions for each input image.}
\end{figure}

\subsubsection{Training}
\label{sec:methods:CellNuc-DETR:training}

\paragraph{Model initialization}
The backbone and the deformable transformer utilized in our experiments are pretrained on ImageNet \cite{deng2009imagenet} and Common Objects in Context (COCO) \cite{lin2014microsoft} datasets, respectively. However, the neck connecting the backbone and the transformer, as well as the classification layers of the transformer, are initialized using a random strategy.

\paragraph{Data augmentation}
Digital pathology images exhibit substantial diversity due to various factors, including differences in staining protocols, elapsed time since slide staining before digitization, and the diverse tissue types. This variability poses challenges for model generalization and performance in tasks such as cell nuclei detection and classification. Data augmentation serves as strategy to mitigate these challenges by enriching the dataset with diverse representations. Drawing inspiration from insights in \cite{10.1117/12.2293048}, our augmentation pipeline combines traditional techniques such as rotation, flipping, color jittering, and blurring with advanced stain augmentations. Specifically, we transform RGB images into Hematoxylin-Eosin-DAB (HED) space, independently manipulate channels to simulate staining variations, and then revert to RGB format. This approach ensures our models are trained on a robust dataset that captures the complexities and variability inherent in digital pathology images, thereby enhancing their ability to generalize and perform effectively across different conditions.

\paragraph{Loss function}
In our study, we utilize the standard loss function recommended for Detection Transformers in natural images, which includes three components: bounding box L1 regression loss, generalized intersection over union (GIoU) loss, and focal loss for cell nuclei classification.

The bounding box L1 regression loss measures the difference between predicted and ground truth bounding box coordinates, ensuring precise localization of cell nuclei. To address scale dependency, GIoU loss is incorporated. Finally, given the imbalanced nature of cell nuclei classification, focal loss is used to handle this challenge by down-weighting well-classified examples and focusing on hard-to-classify instances. This enhances the model’s ability to accurately classify diverse cell types.

\paragraph{Optimization}
Training is distributed across four NVIDIA Quadro RTX 16GB GPUs to expedite computation. We employ the Adam optimizer with a base learning rate of $2 \times 10^{-4}$ defined for a batch size of 16 in the original paper, which we linearly scale based on our setting. The learning rate for the parameters in the backbone and the multi-scale deformable attention modules are initialized at $2 \times 10^{-5}$. A weight decay of $1 \times 10^{-4}$ is applied to prevent overfitting. Hyperparameter adjustment accommodates our specific batch size and GPU setup, aligning with practices established in related works. Learning rate scheduling follows a multi-step approach, reducing the base learning rate by a factor of 0.1 at 70\% and 90\% of the total training duration to stabilize convergence and enhance model performance. All models are trained for 100 epochs to ensure comprehensive convergence and evaluation across datasets and experimental conditions.

\paragraph{Hyperparameters}
We adopt configurations based on the original Deformable DETR framework \cite{zhu2020deformable}, leveraging established hyperparameters to avoid exhaustive search. These configurations include optimization hyperparameters such as learning rate and weight decay, as well as loss weights for the composite loss function. Minor adjustments were made to align with our specific batch size and GPU setup. Additionally, we modified the learning rate scheduler based on related work to better suit our digital pathology tasks.

\subsubsection{Inference pipeline}
\label{sec:methods:CellNuc-DETR:inference}

\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pipeline-step1.png} % Replace with your image path
        %\caption{Caption for the first figure}
        %\label{fig:pipeline-s1}
    \end{subfigure}
    
    %\vspace{0.5cm} % Adjust vertical space between figures

    % Second subfigure
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pipeline-step2.png} % Replace with your image path
        %\caption{Caption for the second figure}
        %\label{fig:pipeline-s2}
    \end{subfigure}
    
    %\vspace{0.5cm} % Adjust vertical space between figures

    % Third subfigure
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pipeline-step3.png} % Replace with your image path
        %\caption{Caption for the third figure}
        %\label{fig:pipeline-s3}
    \end{subfigure}
    
    \caption{CellNuc-DETR inference pipeline on WSIs.}
    \label{fig:pipeline}
    \footnotesize{
    \textit{(1) Pre-processing:}  Tissue segmentation removes background regions, and full-resolution tiles are extracted. \textit{(2) Inference:} Each tile is partitioned into overlapping windows and processed in parallel by the model, then predictions are combined. \textit{(3) Post-processing:} Predictions from all tiles are combined, with edge cells outside central borders removed to avoid duplication.
    }
\end{figure}

The inference pipeline for CellNuc-DETR on Whole Slide Images (WSIs) involves three main steps: pre-processing, inference, and post-processing, as illustrated in Figure \ref{fig:pipeline}. In this section, we detail each of these steps, highlighting how they contribute to efficient and accurate cell nuclei detection.

\paragraph{Pre-processing} 
The giga-size of WSIs limits the feasibility of end-to-end processing pipelines, necessitating the subdivision of these images into smaller tiles for independent analysis. Additionally, many areas within WSIs are non-informative background regions, making it essential to detect and ignore these sections to optimize computational resources.

To address these challenges, we first detect the tissue regions within the WSIs. We convert the WSI thumbnail into the HED color space \cite{ruifrok2001quantification}, enhancing the contrast between tissue and background. By applying specific thresholds to each channel in the HED space, we create a binary mask that isolates the tissue regions from the non-tissue areas. After identifying the tissue regions, we subdivide the WSIs into overlapping tiles. The overlap ensures comprehensive coverage and captures sufficient contextual information, which is crucial for areas with high cell density. This approach also facilitates seamless merging of predictions in later stages.

The pre-processing step returns the top-left corner coordinates of the tiles to be processed, guiding the subsequent stages and ensuring that all relevant regions of the WSI are analyzed efficiently and accurately.

\paragraph{Large tile inference} A significant constraint of DETR-like models is the necessity for the number of queries in the decoder to surpass the potential objects present in an image. In regions characterized by a high cell density, a $256 \times 256$px image patch may contain up to 300 cell nuclei. Consequently, increasing the input image size to larger tiles, such as $1024 \times 1024$px, becomes non-trivial. The number of cell nuclei, and therefore the required input DETR queries, can substantially increase, potentially resulting in prohibitive computational demands.

To address this challenge during inference on larger image tiles, we adopt a simultaneous processing of overlapped sliding windows approach. The model, trained on smaller patches (e.g., $256 \times 256$px), processes larger images by dividing them into overlapping windows. These windows are processed in-device, minimizing GPU-CPU communication and enhancing inference speed. Specifically, the model splits the original image into overlapped windows, processes them in parallel, and then combines the outputs to derive the final results. To merge predictions from overlapping windows, we only keep the centroids within the central crop of each window, leaving a border of half the overlap size on each side. This strategy ensures that detections near the window borders are excluded to avoid redundancy, as they are covered by the central regions of adjacent windows. For windows at the edges of the tiles, this exclusion applies solely to the sides overlapped by another window, not to those corresponding to the image borders. The process is illustrated in Figure \ref{fig:edge-cells}. This approach is faster than pre-processing image patches before sending them to the device, and its efficiency is particularly beneficial when processing WSIs. By employing this strategy, CellNuc-DETR effectively manages the computational demands of high-density cell regions and large images, ensuring robust performance in practical applications.

\paragraph{Slide inference} The slides, which consist of multiple overlapping tiles, are processed using this approach. Tiles are dynamically retrieved from the slide using OpenSlide \cite{goode2013openslide} based on the coordinates obtained in the pre-processing step. By retrieving large tiles from the WSI and partitioning them into overlapping windows directly on the device, we achieve a more efficient workflow. This method is faster than retrieving small patches from the slide or partitioning large tiles into overlapping windows before sending them to the device. The efficiency gains come from minimizing interactions with the slide (on disk) and reducing CPU-GPU communication, resulting in a optimized inference process.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/edge-cells-2win.png}
    %\includesvg[width=0.3\textwidth]{figures/edgecells2.svg}
    \caption{Resolution of edge cells between overlapped windows.}
    \footnotesize{
        The plotted cells are detected during the processing of both the green and the yellow windows. However, only those detections inside the central crop of the windows are considered. The filled green (yellow) centroids will be assigned to the left (right) window, whereas the unfilled ones will only be considered if there are no windows above or below.
    }
    \label{fig:edge-cells}
\end{figure}

\paragraph{Post-processing}
Given that the tiles are also defined with overlap, we follow a strategy similar to the sliding window approach to combine predictions between tiles. After processing the tiles, we merge the predictions by only keeping the centroids within the central crop of each tile, excluding detections near the borders to avoid redundancy. This method leverages the overlap to ensure consistent and accurate merging of detections across tile boundaries. As the post-processing for cell nuclei detection is relatively simple, involving just the merging of centroid coordinates, it is much faster compared to the complex post-processing required for segmentation methods. This efficiency in post-processing significantly enhances the overall performance, making it well-suited for practical applications in digital pathology.

By default, we define a tile size of $1024 \times 1024$px with an overlap of 64px. Each tile is divided into windows of size $256 \times 256$px with an overlap of 64px. As the overlap between tiles is the same as the overlap between windows, the result after merging the predictions is agnostic to the tile partitioning.

%\paragraph{Large tiles} A significant constraint of DETR-like models is the necessity for the number of queries in the decoder to surpass the potential objects present in an image. In regions characterized by a high cell density, a $256 \times 256$px image patch may contain up to 300 cell nuclei. Consequently, increasing the input image size to larger tiles, such as $1024 \times 1024$px or 2048$\times$2048px, becomes non-trivial. The number of cell nuclei, and therefore the required input DETR queries, can substantially increase, potentially resulting in prohibitive computational demands.

%To address this challenge during inference on larger image tiles, we adopt an overlapped sliding window approach. The model, trained on smaller patches (e.g., $256 \times 256$px), processes larger images by dividing them into overlapping windows. These windows are processed in-device, minimizing GPU-CPU communication and enhancing inference speed. Specifically, the model splits the original image into overlapped windows, processes them in parallel, and then combines the outputs to derive the final results. To merge predictions from overlapping windows, only detections whose centroids fall within the central region of the window are considered. This strategy excludes detections near the borders of the window, as they are likely to be covered by the central region of adjacent windows. For windows at the edges of the tiles, this exclusion applies solely to the sides overlapped by another window, not to those corresponding to the image borders. This approach is faster than pre-processing image patches before sending them to the device, and its efficiency is particularly beneficial when processing Whole Slide Images (WSIs). By employing this strategy, CellNuc-DETR effectively manages the computational demands of high-density cell regions and large images, ensuring robust performance in practical applications.


%\paragraph{Whole Slide Images}
%Scalability of cell nuclei detection and classification on WSIs is central to our approach, driven by the enormous size of these images. We have developed a robust pipeline tailored for efficient inference on WSIs, leveraging an overlapped sliding window approach. In this pipeline, the tissue regions of the slide are subdivided into large tiles, which are then processed using the sliding window procedure. This method ensures that each region of the WSI is examined thoroughly while maintaining computational efficiency. All predictions from the overlapping windows are subsequently aggregated to obtain the final results. This approach is more suitable than directly partitioning the image into smaller patches that could be individually fed into the model, as it reduces GPU-CPU communication and streamlines the inference process. By executing the sliding window approach in-device, our pipeline effectively addresses the unique challenges posed by the substantial scale of WSIs, ensuring accurate and efficient cell nuclei detection and classification.
