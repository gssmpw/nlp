% definitaion

% response planning: 
%   1. prompt representations encode information predictive of future responses.
%   2. prompt representations store high-level information influencing attributes in the whole future response.



\begin{abstract}
In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: \textbf{their hidden representations encode future outputs beyond the next token}.
Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including \textit{structural attributes} (response length, reasoning steps),  \textit{content attributes} (character choices in storywriting, multiple-choice answers at the end of response), and \textit{behavioral attributes} (answer confidence, factual consistency). 
In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggests potential applications for improving transparency and generation control.

% Next, we explore how the phenomenon of response planning scale with model sizes across different tasks.
% We also study how these representations evolve during generation by examining attributes predictions at different sequence positions.
% While our focus is on demonstrating the existence of these planning-related representations rather than their direct use during generation, our findings reveal that LLMs systematically encode future outputs in their hidden states. This suggests potential applications in output anticipation and optimization.

% dzc: Large Language Models (LLMs) are typically seen as next-token predictors, but we find they exhibit spontaneous planning behaviors beyond next tokens - their hidden representations encode non-trivial predictive attributes of future responses beyond the next token.
% dzc: Using MLP probes on hidden layer representations, we identify three types of encoded features in greedy-decoded outputs: structural (remaining token length, number of reasoning steps), content (character choices, multiple-choice answers), and behavioral (answer confidence, response truthfulness).
% dzc: We then perform a scaling analysis to investigate how these planning-related attributes vary with model sizes and capabilities across different tasks, measuring feature significance through probe complexity. 
% Then, in a scaling study, we investigate how these planning-related features vary with model capabilities across different tasks through probe complexity measurements.
% Finally, we track how these features evolve during generation by examining feature predictions across sequence positions.
% While we focus on demonstrating the existence of planning-related features rather than whether LLMs use these features during generation, our findings reveal systematic encoding of future outputs in LLMs' hidden states, suggesting applications in output anticipation and optimization.
% dzc: Large Language Models (LLMs) are typically seen as next-token predictors, but we find their hidden states encode information about their future responses. Using MLP probes on hidden layer representations, we demonstrate that LLMs encode structural features (response length, reasoning steps), content features (character choices, multiple-choice answers), and behavioral features (answer confidence, response truthfulness) of their upcoming greedy-decoded outputs. We then explore how these planning-related attributes vary with static characteristics like model size, capability, and task type, using probe complexity as an indicator of feature extraction difficulty. Finally, we track how these attributes evolve during generation by examining feature predictions across sequence positions. Our findings reveal systematic encoding of future response attributes in LLMs' hidden states, suggesting potential applications in anticipating and optimizing model outputs.

% dzc: Large Language Models (LLMs) are traditionally viewed as next-token predictors, but we discover they spontaneously encode attributes of their entire future response in their hidden states. Through MLP probes trained on hidden layer representations, we discover distinct patterns of planning-related feature encoding across different temporal scales. Our probes achieve high accuracy in predicting proximate but non-immediate attributes (target translation language), long-range properties (full response length, number of reasoning steps), and meta-cognitive features (model's answer confidence) of the yet-to-be-generated text. These findings suggest the presence of implicit planning mechanisms within LLM architectures, offering new perspectives for understanding and optimizing language model generation processes, as we can anticipate key response characteristics before the first token is generated.
% Large Language Models (LLMs) are typically seen as next-token predictors, but we find they encode attributes of their future responses in their hidden states. Through MLP probes trained on hidden layer representations, we first demonstrate that LLMs encode both structural characteristics (response length, reasoning steps), content characteristics (character choices, multiple-choice answers), and behavioral characteristics (model confidence, response truthfulness) of their upcoming greedy-decoded outputs. We then explore how these planning-related features vary with static factors like model size, capability, and task type, using probe complexity as an indicator of feature extraction difficulty. Finally, we reveal how planning-related features evolve throughout generation by analyzing feature predictions across different sequence positions. While we focus on demonstrating the existence rather than LLMs' own active usage of these encoded attributes, our findings suggest implicit planning mechanisms that could help optimize model outputs during generation.
\end{abstract}
