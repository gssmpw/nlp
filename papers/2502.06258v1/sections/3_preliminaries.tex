    \section{Emergent Response Planning in LLMs}
    If LLMs plan ahead for their entire response in prompt representations, then some global attributes of their upcoming responses can be predicted from the prompt, without generating any tokens.
    In this section, we first describe how existing probing techniques can investigate the global responses encoded in LLM prompt representations (Section~\ref{subsec:probing-for-future-responses}).
    We then outline the setup for training our probes, including the response attributes of interest and the data collection pipeline (Section~\ref{subsec:probing-setup}).
    Finally, we discuss experimental details before presenting our results (Section~\ref{subsec:experimental=details}).
    
    % dzc: In this section, we provide an overall view of our experiments. We first illustrate the probing method for detecting future response attributes from hidden representations, and then detail the common pipeline and settings of the experiments.
    % We first describe how we can use existing techniques of probing to investitage that LLM encodes in their prompt representations their global responses beyond (Section~\ref{subsec:probing-for-future-responses}), then we describe the setup for training our probes including the high-level attributes we care in the output po
    % \subsection{Probing Future Response Attributes from Early Hidden Representations in LLMs}
    % dzc: Generally, for decoder LLMs with $L$ layers, at any timestep $t$ in a sequence of length $T$, we have input embeddings $\mathbf{E}_{1:t}$ and layer-wise hidden representation $\mathbf{H}^l_{1:t}$ (where $l \in {1,...,L}$), from which the model generates next token through $\mathbf{y}_{t+1} = \texttt{softmax}(f_{\text{out}}(\mathbf{H}^L_{1:t}))$. Here, $T$ represents the minimal sequence length needed for a target attribute to be revealed. Our key observation is that these response attributes can be derived \textbf{through two paths}: \textbf{(1)} by applying attribute rules $g$ (e.g., counting tokens for response length) on the complete generated sequence: $g(y_{1:T}) = g(f_\text{out}(\mathbf{H}^L_{1:T}))$, or \textbf{(2)} by directly probing hidden representation at an earlier timestep $t < T$: $h(\mathbf{H}^l_{t})$. We demonstrate that a simple MLP probe $h(\cdot)$ can effectively learn this mapping: $h(\mathbf{H}^l_{t}) = g(f_\text{out}(\mathbf{H}^L_{1:T}))$.
    
    \subsection{Probing for Future Responses}\label{subsec:probing-for-future-responses}
    We study an $L$-layer decoder LLM $\pi(\mathbf{y} \mid \mathbf{x})$ that generates a response $\mathbf{y} = (y_1, \dots, y_n)$ given a prompt $\mathbf{x} = (x_1, \dots, x_m)$ sampled from a prompt distribution $p(\mathbf{x})$. During generation, the model encodes the input $(\mathbf{x} \circ \mathbf{y}_{1:t})$ into layer-wise representations $\{ \mathbf{H}^l_{\mathbf{x} \circ \mathbf{y}_{1:t}} \}^L_{l-1}$, with the next token greedily decoded from the projection of final-layer representations $y_{t+1} = \arg \max(f_{\text{out}}(\mathbf{H}^L_{\mathbf{x} \circ \mathbf{y}_{1:t}}))$.
    
    We investigate whether the prompt representations $\mathbf{H}^l_{\mathbf{x}}$, which produce the first response token $y_1$, also capture some global attributes of their upcoming response $\mathbf{y}$ (e.g., response length).
    
    Formally, we define the \textit{attribute rule} as $g(\mathbf{y})$, which summarizes the attributes from the generated responses (e.g., counting tokens in $\mathbf{y}$). Building on prior work on interpretability, if the prompt representations do capture these attributes, we can ``probe'' the hidden representations to predict the attributes without generating any response token: $h_\theta(\mathbf{H}^l_{\mathbf{x}}) \rightarrow g(\mathbf{y})$. If probing yields non-trivial predictions, we conclude that the LLM exhibits \textit{response planning}.
    
    \subsection{Probing Setup}\label{subsec:probing-setup}
    
    %To investigate emergent response planning in LLMs, we need paired data of intermediate representations and their corresponding response attributes that only become apparent beyond the next token. 
    % To study response planning in LLMs, \textbf{we need to collect paired data of \textit{(intermediate representation as probe input, corresponding future response attribute as probe target)}. }
    % To this end, we design tasks where target attributes cannot be determined from immediate next-token prediction alone, and implement a general pipeline for pair data collection.
    % To this end, we carefully design tasks where target attributes cannot be determined from immediate next-token prediction alone, organizing them into three categories: \textbf{1)} structural attributes that capture response-level metrics like  response length; \textbf{2)} content attributes that track specific words that may appear anywhere in the response; and \textbf{3)} behavioral attributes that require external ground truth labels to evaluate. 
    
    To study response planning in LLMs, we first design tasks $T = (p(\mathbf{x}), g(\mathbf{y}))$, consisting of a prompt distribution $p(\mathbf{x})$ eliciting key response attributes of interests $g(\mathbf{y})$ as probing targets. 
    Next, we introduce the data collection pipeline for training probes.
    
    \textbf{Task design.} 
    % The response attributes worth studying must be global, meaning they cannot be determined from the first response token and should ideally be distributed across the entire response.
    % To this end, we focus on six attributes across three categories: structural, content, and behavioral attributes. 
    The studied response attributes must be global, meaning they cannot be determined from the first response token and should ideally be distributed across the entire response. We focus on six tasks that elicit response attributes across three categories: structural, content, and behavioral.
    \begin{enumerate}
        \item \textbf{Structure attributes} capture response-level features: the \textit{response length prediction} prompts LLMs to follow human instructions, with the number of tokens counted as the probing target; the \textit{reasoning steps prediction} prompts LLMs to solve math problems, with the number of reasoning steps as the probing target. 
        \item \textbf{Content attributes} track specific words appearing anywhere but not at the start of the response: \textit{character choices prediction} prompts LLMs to write a story featuring an animal character, with the character choice as the probing target; \textit{multiple-choice answers prediction} prompts LLMs to answer a question after reasoning (e.g., ``please first explain then give your answer''), with the selected answer as the probing target.
        \item \textbf{Behavior attributes} require external ground truth labels for validation: the \textit{answer confidence prediction} prompts LLMs to answer challenging multiple-choice questions, with the correctness of answers judged by ground-truth labels as the probing target; the \textit{factual consistency prediction} prompts LLMs to discuss and then agree/disagree with given statements, with the match between LLM's stance and statement ground-truth validity as the probing target.
    \end{enumerate}
    
    % Then, for each task, we pair carefully selected datasets with designed prompts to elicit LLM responses with desired attributes. 
    % We describe our prompting strategies for both fine-tuned models and base models.
    % For fine-tuned models: For the response length task, we prompt LLMs to directly respond to questions from Ultrachat~\cite{ding2023enhancing} (5K samples) and AlpacaEval~\cite{alpaca} (1K samples). For the reasoning steps task, we prompt LLMs to solve math problems from GSM8k~\cite{cobbe2021gsm8k} (5K samples) using chain-of-thought reasoning. For the character choices task, we prompt LLMs to write single-sentence stories with one animal character, using story beginnings from TinyStories~\cite{eldan2023tinystoriessmalllanguagemodels} and ROCStories~\cite{mostafazadeh2016corpus} (10K samples each). For the multiple-choice answers task, we prompt LLMs to analyze and then select from 5 choices in problems from CommonsenseQA~\cite{talmor-etal-2019-commonsenseqa} (10K samples). For the answer confidence task, we prompt LLMs to analyze and answer challenging problems from MedMCQA~\cite{pmlr-v174-pal22a} (10K samples) and ARC-Challenge~\cite{allenai:arc} (2.6K samples). For the factual consistency task, we prompt LLMs to analyze and state agree/disagree with statements from CREAK~\cite{onoe2021creakdatasetcommonsensereasoning} (10K samples). 
    % For base models, we use these same prompting strategies with Claude 3.5 Sonnet to generate input-output pairs as few-shot examples, with explicit [END OF RESPONSE] signals to help recognize response boundaries.
\begin{figure*}[tb!] 
    \renewcommand{\thesubfigure}{\alph{mycounter}}
    \newcounter{mycounter}
    \centering
    \setcounter{mycounter}{1}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_barbasicResults_ultrachat_spearman.pdf}
        \vspace{-15pt}
        \caption{Response length prediction.}
        \label{fig:exp_bar_response_length_inDataset}
    \end{subfigure}
    \hfill
    \setcounter{mycounter}{3}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_basicResults_TinyStories_f1_score.pdf}
        \vspace{-15pt}
        \caption{Character choices prediction.}
        \label{fig:exp_character_choices_inDataset}
    \end{subfigure}
    \hfill  % Creates horizontal spacing between subfigures
    \setcounter{mycounter}{5}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_basicResults_medmcqa_f1_score.pdf}
        \vspace{-15pt}
        \caption{Answer confidence prediction.}
        \label{fig:exp_answer_confidence_inDataset}
    \end{subfigure}
    \\
    \setcounter{mycounter}{2}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_barbasicResults_gsm8k_spearman.pdf}
        \vspace{-15pt}
        \caption{Reasoning steps prediction.}
        \label{fig:exp_bar_reasoning_steps_inDataset}
    \end{subfigure}
    \hfill
    \setcounter{mycounter}{4}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_basicResults_commonsense_qa_f1_score.pdf}
        \vspace{-15pt}
        \caption{Multiple-choice answers prediction.}
        \label{fig:exp_multiplechoice_answers_inDataset}
    \end{subfigure}
    \hfill  % Creates horizontal spacing between subfigures
    \setcounter{mycounter}{6}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_basicResults_CREAK_f1_score.pdf}
        \vspace{-15pt}
        \caption{Factual consistency prediction.}
        \label{fig:exp_factual_consistency_inDataset}
    \end{subfigure}
    \\
    \setcounter{mycounter}{7}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \vspace{2pt}
        \includegraphics[width=\textwidth]{figures/exp_basicResults_ultrachat_spearman_part.pdf}
        \caption{Example fitting results for response length prediction.}
        \label{fig:exp_response_length_inDataset}
    \end{subfigure}
    \hfill  % Creates horizontal spacing between subfigures
    \setcounter{mycounter}{8}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \vspace{2pt}
        \includegraphics[width=\textwidth]{figures/exp_basicResults_gsm8k_spearman_part.pdf}
        \caption{Example fitting results for reasoning steps prediction.}
        \label{fig:exp_reasoning_steps_inDataset}
    \end{subfigure}
    \\
    \vspace{-8pt}
    \caption{\label{fig:exp_inDataset} Prediction results within the dataset. Regression tasks (response length, reasoning steps) show high accuracy and strong correlation with targets, as measured by Kendall (K), Spearman (S), and Pearson (P) coefficients. Classification tasks (character choices, multiple-choice answers, confidence, factual consistency) perform significantly above random baseline according to F1 scores. These results suggest that the model demonstrates emergent planning capabilities for future response attributes.}
    \vspace{-8pt}
\end{figure*}

    
    Following the prompting strategies described in each task, we carefully pair datasets with corresponding prompts. We use prompts from Ultrachat~\cite{ding2023enhancing} and AlpacaEval~\cite{alpaca} for response length; GSM8K~\cite{cobbe2021gsm8k} and MATH~\cite{2019arXivMATH} for reasoning steps; TinyStories~\cite{eldan2023tinystoriessmalllanguagemodels} and ROCStories~\cite{mostafazadeh2016corpus} for character choices; CommonsenseQA~\cite{talmor-etal-2019-commonsenseqa} and SocialIQA~\cite{sap2019socialiqacommonsensereasoningsocial} for multiple-choice answers; MedMCQA~\cite{pmlr-v174-pal22a} and ARC-Challenge~\cite{allenai:arc} for answer confidence;  CREAK~\cite{onoe2021creakdatasetcommonsensereasoning} and FEVER~\cite{Thorne19FEVER2} for factual consistency.
    % For fine-tuned models, we directly apply these prompts to the datasets. For base models, we first use these prompt-dataset pairs with Claude 3.5 Sonnet~\cite{claude2024} to generate input-output pairs as few-shot examples, with explicit [END OF RESPONSE] signals to help recognize response boundaries.
    Please see Appendix~\ref{appendix:setup_prompts} for more details about task design.
    % \textbf{(1)} Structural attributes capture response-level metrics through two tasks: the \textit{response length task} on UltraChat for predicting remaining token count, and \textit{the reasoning steps task} on GSM8K for predicting remaining steps needed for the final answer. \textbf{(2)} Content attributes track specific words that may appear in the response through two tasks: the \textit{story character choice task} on TinyStories for predicting animal character selection in story continuation, and the \textit{multiple-choice selection task} on CommonsenseQA for predicting choices after question analysis. \textbf{(3)} Behavioral attributes require external ground truth labels through two tasks: the \textit{answer confidence task} on MedMCQA for predicting response correctness, and the \textit{factual consistency task} for predicting truthfulness when responding to true/false statements.
    
    \textbf{Data collection.} For each task $T = (p(\mathbf{x}), g(\mathbf{y}))$, we collect datasets for probing. We sample prompts $\mathbf{x}_i$ from the prompt distribution $p(\mathbf{x})$, store prompt representations $\mathcal{H}_i = \{ \mathbf{H}^l_{\mathbf{x}_i}\}^L_{l=1}$, generate responses to the prompts $\mathbf{y}_i = \arg \max \pi(\mathbf{y} \mid \mathbf{x_i})$, and store probing targets $\hat{g}_i = g(\mathbf{y}_i)$. This creates a dataset of prompt representations and their future response attributes: $\mathcal{D} = \{\mathcal{H}_i, \hat{g}_i \}^{N}_{i=1}$.
    With this dataset, we then train a probe to predict targets from representations.
    % We then train a probe to predict targets from representations: $h^* = \arg \min_{h_\theta} \sum^{N}_{i=1} \mathcal{L}(h_\theta(\mathcal{H}_i), \hat{g}_i)$. 
    
    See Appendix~\ref{appendix:setup_detailed_data_collection} for details on data collection, including task-specific and model-specific prompt templates, as well as data filtering and augmentation methods.
    
    % \textbf{Data collection.} \DZCtodo{to be commented} Our data collection follows three steps: response collection $\rightarrow$ response labeling and filtering $\rightarrow$ representation collection, ultimately yielding the paired (representation, attribute) data. 
    % For response collection, we query LLMs using predefined templates combined with dataset inputs. For fine-tuned LLMs, we directly use dataset questions (e.g., UltraChat questions), while for base LLMs, we construct few-shot examples using excluded data and Claude 3.5 Sonnet responses, appending \texttt{[END OF RESPONSE]} to delineate response boundaries. We then label the collected responses using predefined rules (e.g., token length using the corresponding tokenizer), filter out cases where models fail to exhibit target attributes (e.g., incomplete generations due to token limits), and balance class distributions for classification tasks to ensure equal sample sizes across classes.
    % We also augment the data by truncating responses at random positions before the label reveals, calculating corresponding labels for truncated responses, and group corresponding original and augmented responses to ensure they appear in the same train / test / validation splits. Finally, we collect layer-wise representations from all model layers at all positions where we compute labels to create our final paired dataset.
    % To collect dataset, for each task,
    
    
    %To illustrate this pipeline, we use response length prediction as an example: We first specify the target attribute as the total remaining token length of the LLM response \textit{[attribute design]}. With this target, we select UltraChat and Alpaca\_eval datasets for their diverse question types that yield varied response lengths, and construct prompt templates using these questions [dataset selection]. For response collection, we query chat models directly, while for base models, we use few-shot prompting with examples constructed from the first 10 entries in UltraChat (excluded from later collection) using Claude 3.5 Sonnet's responses [response collection]. We then label each response with its token length using the corresponding tokenizer, filter out responses that exceed token limits (1000 tokens), and augment the data by truncating responses at random positions before the EOS token, calculating remaining tokens for each truncation point, and grouping original and augmented data to ensure they appear in the same train/test/validation splits [response labeling, filtering and augmentation]. Finally, we collect representations at both initial and truncated positions to create our final paired dataset [representation collection].
    
    

\begin{figure*}[tb!] 
    \centering
    \renewcommand{\thesubfigure}{\alph{mycounter}}  % 重定义编号
    \setcounter{mycounter}{1}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_bargeneralizationResults_ultrachat_To_alpaca_eval_spearman.pdf}
        \vspace{-15pt}
        \caption{Response length prediction.}
        \label{fig:exp_bar_response_length_crossDataset}
    \end{subfigure}
    \hfill
    \setcounter{mycounter}{3}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_generalizationResults_tinystories_To_rocstories_f1_score.pdf}
        \vspace{-15pt}
        \caption{Character choices prediction.}
        \label{fig:exp_character_choices_crossDataset}
    \end{subfigure}
    \hfill  % Creates horizontal spacing between subfigures
    \setcounter{mycounter}{5}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_generalizationResults_CREAK_To_fever_f1_score.pdf}
        \vspace{-15pt}
        \caption{Factual consistency prediction.}
        \label{fig:exp_factual_consistency_crossDataset}
    \end{subfigure}
    \\
    \setcounter{mycounter}{2}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_bargeneralizationResults_gsm8k_To_MATH_spearman.pdf}
        \vspace{-15pt}
        \caption{Reasoning steps prediction.}
        \label{fig:exp_bar_reasoning_steps_crossDataset}
    \end{subfigure}
    \hfill
    \setcounter{mycounter}{4}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_generalizationResults_commonsense_qa_To_social_i_qa_f1_score.pdf}
        \vspace{-15pt}
        \caption{Multiple-choice answers.}
        \label{fig:exp_multiplechoice_answers_crossDataset}
    \end{subfigure}
    \hfill
    \setcounter{mycounter}{6}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_generalizationResults_medmcqa_To_ai2_arc-Challenge_f1_score.pdf}
        \vspace{-15pt}
        \caption{Answer confidence prediction.}
        \label{fig:exp_answer_confidence_crossDataset}
    \end{subfigure}
    \\
    \setcounter{mycounter}{7}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_generalizationResults_ultrachat_To_alpaca_eval_spearman_part.pdf}
        \caption{Example fitting results for response length prediction.}
        \label{fig:exp_response_length_crossDataset}
    \end{subfigure}
    \hfill  % Creates horizontal spacing between subfigures
    \setcounter{mycounter}{8}
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \vspace{2pt}
        \includegraphics[width=\textwidth]{figures/exp_generalizationResults_gsm8k_To_math_spearman_part.pdf}
        \caption{Example fitting results for reasoning steps prediction.}
        \label{fig:exp_reasoning_steps_crossDataset}
    \end{subfigure}
    \\
    \vspace{-8pt}
    \caption{\label{fig:exp_crossDataset} Cross-dataset generalization results. For regression tasks (response length, reasoning steps), correlations with targets remain strong despite reduced accuracy compared to in-dataset testing, as shown by Kendall (K), Spearman (S), and Pearson (P) coefficients. Classification tasks (character choices, multiple-choice, confidence, factual consistency) maintain above-baseline F1 scores. These results suggest the probes detect generalizable patterns rather than dataset-specific features, indicating transferable emergent planning capabilities within the task domain.}
    \vspace{-4pt}
\end{figure*}

    
    \subsection{Experimental Details}\label{subsec:experimental=details}
    \paragraph{Probe training.} We train one-hidden-layer MLPs with ReLU activation, with hidden sizes chosen among $\mathcal{W} = \{1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024\}$. 
    The output size is 1 for regression and the number of classes with a softmax layer for classification. 
    Each probe is trained for $400$ epochs using MSELoss for regression and CrossEntropyLoss for classification. Datasets are split $60:20:20$ for train-validation-test. We perform a grid search over MLP hidden sizes $\mathcal{W}$ and representation layers $\mathcal{H}$ (as inputs to the probes), reporting the test scores for the best hyperparameters. Results are averaged across three random seeds.
    % Using a $6:2:2$ train-validation-test split, we train each probe for $400$ epochs and select the model with the best validation loss. We extract hidden representations from each layer of the model and train separate probes for each layer, testing across different hidden sizes. The test performance for each layer-hidden size combination is recorded to enable various types of analysis.
    
    \paragraph{Probe evaluation.} For regression tasks (response length and reasoning steps), we evaluate using Spearman, Kendall and Pearson correlation coefficients, which measure the strength and direction of monotonic (Spearman, Kendall) and linear (Pearson) relationships between predicted and target values. For classification tasks, we evaluate using F1 scores: 4-class classification for character choices, 5-class classification for multiple-choice answers, and binary classification for answer confidence and factual consistency. 
    In our setup, accuracy aligns with F1 score for classification due to strict class balance across tasks.
    % \paragraph{Probe evaluation.} \DZCtodo{too verbose, see comments} We define regression and classification targets for each task with corresponding evaluation metrics. For regression tasks: response length probes predict the token count, and reasoning steps probes predict the number of steps. For classification tasks: character choice probes predict one of four possible animals, multiple-choice answer probes predict one of five options (A-E), answer confidence probes predict binary correctness (whether the response matches ground truth), and factual consistency probes predict binary agreement accuracy (correct agreement with true statements and disagreement with false ones). We evaluate regression tasks using Spearman, Kendall, and Pearson coefficients. For classification tasks, we report accuracy and F1 scores.
    
    % \paragraph{Metrics.} For regression tasks (numerical attributes like response length), we use Spearman, Kendall, and Pearson coefficients to measure both monotonic and linear relationships. For classification tasks (categorical attributes like multiple-choice selections), we report accuracy and F1 scores for comprehensive evaluation.
    
    
    % \paragraph{Baselines.} For classification, we compare against random selection using accuracy and F1 scores.
    % \DZCtodo{todo} To validate our findings, we implement two ablation baselines in addition to our main probing experiments: (1) training probes with randomly shuffled labels to verify that probes learn meaningful representation-label relationships rather than fitting arbitrary distributions, and (2) evaluating randomly initialized probes without training to assess the impact of probe training. For classification tasks, we compare against random selection. While we do not explicitly include a baseline that learns projections directly from the embedding space, our analysis of layer-wise representation activity suggests that such mappings would be difficult without transformer layer processing.
    
    \paragraph{Language models.} We experiment with both instruction-tuned models (Llama-2-7B-chat, Llama-3-8B-Instruct, Mistral-7B-Instruct, and Qwen2-7B-Instruct) and their corresponding base models (Llama-2-7B, Llama-3-8B, Mistral-7B, and Qwen2-7B). See Appendix~\ref{appendix:model_specification_and_links} for model specifications and links.
    
    % \textbf{Analysis for each task.} For each task, we conduct analysis across four dimensions: 1) within-dataset testing using train-test splits from the same dataset, 2) cross-dataset generalization testing on datasets different from the training set, 3) layer-wise analysis examining how each model layer's representations perform in predicting the final result, and 4) probe complexity analysis investigating the impact of probe hidden size and sufficiency of probe capacity.
    
    
    
    