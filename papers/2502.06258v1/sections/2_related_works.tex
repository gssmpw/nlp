\section{Related Work}

% TODO: We further clarify the scope of our study on emergent planning. First, we define response attributes as global features beyond next-token prediction - focusing on attributes that are not represented in the immediately next token (the first response token), unlike analyses of immediate token-level decisions (e.g., harm detection where ``Sorry'' vs ``Sure'' appears in the first token~\cite{qi2024safety, zhou-etal-2024-emulated}). Second, unlike prior work using hidden states as feature extractors for external tasks, we probe model-generated data to understand how states encode the model's own planning attributes during generation. Finally, our analysis establishes correlational rather than causal relationships - while we demonstrate that LLMs encode predictable features of their upcoming responses, we make no claims about whether models actively utilize these encodings or whether modifying them would influence generation, leaving questions of causality for future research.

\textbf{Understanding LLM hidden representations.}
LLM hidden representations encode more information than they actively use~\cite{saunders2022self, burns2022discovering}. Patterns in these representations can be identified using linear or MLP probes~\cite{nostalgebraist2020logitlens, li2022emergent, belrose2023eliciting, zou2023representation, ji2024llm} and leveraged to influence model behaviors such as truthfulness~\cite{hernandez2023inspecting, li2024inference}, instruction-following~\cite{heo2024llms}, and sentiment~\cite{turner2024steeringlanguagemodelsactivation}. They are also useful for training additional regression or classification heads on transformer layers for tasks like reasoning~\cite{han2024token, damani2024learning}, high-dimensional regression~\cite{tang2024understanding}, and harmful content detection~\cite{rateike2023weakly, macdiarmid2024simple, qian2024hsf}.

Our work also utilizes LLM hidden representations but differs in focus. Rather than using hidden states as feature extractors for external tasks, we probe model-generated data to understand how these states encode the model’s own planning attributes during generation.

\textbf{Prior works exploring response planning in LLM.}
Previous study have examined whether LLMs can anticipate beyond the next token. Future Lens~\cite{pal2023future} models token distributions beyond the immediate next token using linear approximation. ~\cite{geva2023dissecting} studies how LLMs retrieve factual associations during generation, while ~\cite{men2024unlocking} extends this to Blocksworld planning, suggesting LLMs consider multiple planning steps simultaneously. ~\cite{pochinkov2024extracting} finds that tokens at context-shifting positions may encode information about the next paragraph. ~\cite{wu2024language} hypothesizes LLMs’ lookahead capability and tests two mechanisms—pre-caching and breadcrumbs—in a myopic training setting.

% However, there has not been a systematic study on whether and how LLMs can perform response planning across different tasks.

While prior works examine relatively narrow aspects like predictions several tokens ahead or knowledge retrieval in specialized scenarios, our work delves deeper to reveal the broader response planning landscape of LLMs. We provide the first formal definition of response planning in LLM, investigate comprehensive planning attributes, and demonstrate planning capabilities across diverse real-world tasks.

% Previous works mainly focus on relatively simple future attributes like next-token distributions or knowledge retrieval in specialized scenarios, and lack a systematic definition of response planning. Our work investigates more sophisticated planning behavior through structural, content, and behavioral attributes, demonstrates planning across diverse real-world tasks in both structured and open-ended domains, and provides the first formal definition of planning in LLMs by examining how future responses attributes are encoded in hidden representations.

% \DZCtodo{our difference}

% \textbf{What can I say?}