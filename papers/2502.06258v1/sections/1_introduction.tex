\section{Introduction}\label{sec:introduction}
Large Language Models (LLMs) have demonstrated powerful capabilities across various tasks~\cite{brown2020language, achiam2023gpt, touvron2023llama, claude2024}. However, their next-token-prediction training objective leads to the view that they generate text through local, per-token prediction, without considering future outputs beyond the next immediate token~\cite{bachmann2024pitfalls,cornille2024learning}
. This makes controlling the generation process challenging: \textit{we are blind to the model's output tendency until keywords or the full response appear}. While prompt engineering and inference-time interventions~\cite{liu2023pre, li2024inference, zhou2024weak} can guide responses, they lack insight and transparency into the model's internal plan for outputs.

In this work, we argue that LLMs, though trained to predict only the next token, display emergent planning behaviors: their hidden representations encode their future outputs beyond just the next token. %~\cite{pal2023future,wu2024language,men2024unlocking,pochinkov2024extracting}. 
Specifically, we observe that LLM prompt representations encode interesting global attributes of their upcoming responses. We call this phenomenon \textit{response planning} and classify these global attributes into three categories: \textit{structural attributes} (response length, reasoning steps), \textit{content attributes} (character choices in storywriting, multiple-choice answers at the end of response), and \textit{behavioral attributes} (answer confidence, factual consistency).

We empirically identify response planning by training simple probes on LLM prompt representations to predict the global attributes of their upcoming responses. We find that these probes achieve non-trivial prediction accuracy, providing strong evidence that LLMs plan at least part of their entire response in advance as soon as they read the prompt.
Through further ablation experiments, we find that planning abilities positively scale with fine-tuned model size, peak at the beginning and end of responses, share certain planning patterns across models, and exceed models' self-reported awareness.
% Then we conduct a wide range of ablation studies \DZCtodo{ablation, to be done}

The contribution of our work is two-fold: 
(1) To our best knowledge, we introduce the first formal definition and framework of emergent response planning in LLM.
(2) We demonstrate empirically that LLMs perform emergent response planning through systematic probing experiments across various attributes types and tasks, and investigate their properties.
% \DZCtodo{todo}
% (2) Through further ablation studies, we reveal that this planning capability systematically scales with model size, exhibits consistent patterns across architectures, and often exceeds models' explicit predictions about their own behavior, suggesting it represents a fundamental emergent property of LLMs. 
These findings shed light on LLMs' internal mechanisms and suggest novel approaches for predicting and controlling outputs pre-generation, potentially enhancing model controllability.
% These findings provide new insights into LLM internal mechanisms and offer new possibilities for predicting and controlling model outputs before generation begins, potentially enabling more effective steering methods.

% \DZCtodo{The contribution of our work is two-fold: (1); (2).}In summary, our findings about LLM emergent response planning challenge the view of these models as purely short-sighted next-token predictors. By revealing that model representations contain predictive features across structural, content, and behavioral attributes, our work opens new possibilities for understanding and guiding model responses before they are generated. These insights could significantly impact the development of more effective control methods and improve our understanding of LLM generation mechanisms.

% LLM are strong; but steering LLM is difficult: generally we think for auto-regressive models, the future responses remain unknown until the target tokens are generated; information beyond tokens is somewhat unreachable.

% dzc: Large Language Models (LLMs) have shown powerful capabilities across various tasks. However, the conventional view of LLMs as next-token predictors suggests they generate text through local, token-by-token prediction, without consideration for their future responses. This makes controlling the generation process challenging: \textbf{we remain blind to the model's intended output until key words or the complete response is generated.} While techniques like prompt engineering and inference-time intervention can guide responses towards desired attributes, they operate without insight into the model's actual planned outputs.

% dzc: In this work, we argue that LLMs, though trained to predict only the next token, display emergent planning behaviors: their hidden representations encode their future outputs beyond just the next token. We demonstrate this through probing studies based on two key insights: \textbf{1)} LLM hidden states encode substantially more information about future generation than what is revealed in their token-by-token  outputs~\cite{saunders2022selfcritiquingmodelsassistinghuman, li2024inferencetimeinterventionelicitingtruthful}, and \textbf{2)} these representations can be accessed using simple MLP probes. Our methodology involves collecting model responses under greedy decoding to capture inherent generation tendencies, extracting hidden states from positions before key attributes appear in the sequence, and training probes to predict future response attributes from these representations.

% We further clarify the scope of our study on emergent planning. First, we define response attributes as global features beyond next-token prediction - focusing on attributes that are not represented in the immediately next token (the first response token), unlike analyses of immediate token-level decisions (e.g., harm detection where ``Sorry'' vs ``Sure'' appears in the first token). Second, unlike prior work using hidden states as feature extractors for external tasks, we probe model-generated data to understand how states encode the model's own planning attributes during generation. Finally, our analysis establishes correlational rather than causal relationships - while we demonstrate that LLMs encode predictable features of their upcoming responses, we make no claims about whether models actively utilize these encodings or whether modifying them would influence generation, leaving questions of causality for future research.

% dzc: In summary, our findings reveal that LLMs systematically encode global attributes of their future responses in representations, challenging the conventional view of these models as purely short-sighted next-token predictors. By demonstrating that model representations contain predictive features across structural, content, and behavioral attributes, our work opens new possibilities for understanding and potentially steering model outputs before they are generated. These insights could have significant implications for developing more effective control methods and improving our understanding of LLM generation mechanisms.

% In this work, we argue that we can indeed steer LLMs with "emergent response planning": LLM encodes clues of their own future responses in hidden representations.

% Recent advances in Large Language Models (LLMs) have repeatedly challenged our understanding of their capabilities. While these models continue to demonstrate increasingly sophisticated behaviors in tasks ranging from few-shot learning to chain-of-thought reasoning, our understanding of their internal mechanisms remains limited. The conventional view of LLMs as next-token predictors suggests that they generate text primarily through local, step-by-step token prediction, without consideration for the broader context or future content of their responses.

% This traditional perspective, however, may underestimate the sophistication of LLMs' internal representations. Recent studies have begun to reveal complex patterns in the hidden states of these models, suggesting that their generation process might involve more than simple token-by-token prediction. Building on these observations, we investigate whether LLMs develop implicit planning mechanisms during their pre-training process, encoding information about their future responses before generation begins.

% Our investigation focuses on three distinct aspects of planning-related encoding in LLM hidden states. First, we examine immediate-term planning through the lens of translation tasks, where models must choose and maintain consistency in target language selection. Second, we explore long-range planning by analyzing how models encode information about the full length and structure of their future responses. Finally, we investigate meta-cognitive aspects by examining how models encode their confidence and approach to different types of queries.

% Through a series of probing experiments using MLPs trained on hidden layer representations, we demonstrate that LLMs indeed encode rich information about their future outputs across these different temporal scales. These findings not only challenge the simple next-token prediction view of LLMs but also suggest new approaches for understanding and potentially optimizing these models' generation processes.