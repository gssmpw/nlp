\newpage
\appendix
\onecolumn
\section{Further Details on the Experimental Setup}
\subsection{Model Specification}
The following table lists the models and their corresponding links.
\label{appendix:model_specification_and_links}

\begin{longtable}{p{7cm}p{9cm}}
% \begin{longtable}{@{}p{7cm}p{9cm}@{}}
\toprule
\textbf{Models} & \textbf{Links} \\
\midrule
\texttt{Llama-2-7B}~\citep{touvron2023llama2} & \url{https://huggingface.co/meta-llama/Llama-2-7b-hf} \\
\texttt{Llama-2-7B-Chat}~\citep{touvron2023llama2} & \url{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf} \\
\texttt{Llama-2-13B-Chat}~\citep{touvron2023llama2} & \url{https://huggingface.co/meta-llama/Llama-2-13b-chat-hf} \\
\texttt{Llama-2-70B-Chat}~\citep{touvron2023llama2} & \url{https://huggingface.co/meta-llama/Llama-2-70b-chat-hf} \\
\texttt{Llama-3-8B}~\citep{llama3modelcard} & \url{https://huggingface.co/meta-llama/Meta-Llama-3-8B} \\
\texttt{Llama-3-8B-Instruct}~\citep{llama3modelcard} & \url{https://huggingface.co/meta-llama/Llama-2-7b-hf} \\
\texttt{Llama-3-70B-Instruct}~\citep{llama3modelcard} & \url{https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct} \\
\texttt{Mistral-7B}~\citep{jiang2023mistral7b} & \url{https://huggingface.co/mistralai/Mistral-7B-v0.1} \\
\texttt{Mistral-7B-Instruct}~\citep{jiang2023mistral7b} & \url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2} \\
\texttt{Qwen2-7B}~\citep{qwen2} & \url{https://huggingface.co/Qwen/Qwen2-7B} \\
\texttt{Qwen2-7B-Instruct}~\citep{qwen2} & \url{https://huggingface.co/Qwen/Qwen2-7B-Instruct} \\
\texttt{Qwen2-72B-Instruct}~\citep{qwen2} & \url{https://huggingface.co/Qwen/Qwen2-72B-Instruct} \\
\texttt{Qwen2.5-1.5B-Instruct}~\citep{qwen2.5} & \url{https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct} \\
\texttt{Qwen2.5-32B-Instruct}~\citep{qwen2.5} & \url{https://huggingface.co/Qwen/Qwen2.5-32B-Instruct} \\
\texttt{Qwen2.5-72B-Instruct}~\citep{qwen2.5} & \url{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct} \\
\bottomrule
\end{longtable}

\subsection{Dataset Specification}
The following table lists the datasets and their corresponding links.
\begin{longtable}{@{}p{6cm}p{10cm}@{}}
\toprule
\textbf{Datasets} & \textbf{Links} \\
\midrule
\texttt{Ultrachat}~\citep{ding2023enhancing} & \url{https://huggingface.co/datasets/stingning/ultrachat} \\
\texttt{AlpacaEval}~\citep{alpaca} & \url{https://huggingface.co/datasets/tatsu-lab/alpaca} \\
\texttt{GSM8K}~\citep{cobbe2021gsm8k} & \url{https://huggingface.co/datasets/openai/gsm8k} \\
\texttt{MATH}~\citep{2019arXivMATH} & \url{https://huggingface.co/datasets/deepmind/math_dataset} \\
\texttt{TinyStories}~\citep{eldan2023tinystoriessmalllanguagemodels} & \url{https://huggingface.co/datasets/roneneldan/TinyStories} \\
\texttt{ROCStories}~\citep{mostafazadeh2016corpus} & \url{https://huggingface.co/datasets/Ximing/ROCStories} \\
\texttt{CommonsenseQA}~\citep{talmor-etal-2019-commonsenseqa} & \url{https://huggingface.co/datasets/tau/commonsense_qa} \\
\texttt{SocialIQA}~\citep{sap2019socialiqacommonsensereasoningsocial} & \url{https://huggingface.co/datasets/allenai/social_i_qa} \\
\texttt{MedMCQA}~\citep{pmlr-v174-pal22a} & \url{https://huggingface.co/datasets/openlifescienceai/medmcqa} \\
\texttt{ARC-Challenge}~\citep{allenai:arc} & \url{https://huggingface.co/datasets/allenai/ai2_arc} \\
\texttt{CREAK}~\citep{onoe2021creakdatasetcommonsensereasoning} & \url{https://huggingface.co/datasets/amydeng2000/CREAK} \\
\texttt{FEVER}~\citep{Thorne19FEVER2} & \url{https://huggingface.co/datasets/fever/fever} \\
\bottomrule
\end{longtable}


\subsection{Detailed Process of Response Collection and Labeling}
\label{appendix:setup_detailed_data_collection}
In this section, we detail the process of collecting a dataset $\mathcal{D} = \{\mathcal{H}_i, \hat{g}_i \}^{N}_{i=1}$ for each task $T = (p(\mathbf{x}), g(\mathbf{y}))$, pairing prompt representations with their corresponding attribute labels. First, we construct the prompt distribution $p(\mathbf{x})$ to elicit responses with target attributes from the models~(Sec.\ref{appendix:setup_prompts}). Second, we label these responses according to specific criteria $\hat{g}_i = g(\mathbf{y}_i)$ to capture their key attributes (Sec.\ref{appendix:setup_labeling}). Finally, we collect representations $\mathcal{H}_i = \{ \mathbf{H}^l_{\mathbf{x}_i}\}^L_{l=1}$ for each prompt (Sec.~\ref{appendix:setup_representation}).

\subsubsection{Prompt templates}
\label{appendix:setup_prompts}
To elicit responses with target attributes, we construct prompt distributions using carefully designed templates paired with datasets. We present the prompt templates for both chat and base models across all tasks, along with representative input-output examples.


% \texttt{Tokenizer.apply\_chat\_template}(\{\texttt{''data''}\}, \texttt{tokenize=False})\\

\begin{longtable}{p{14.5cm}}
\toprule
\rowcolor{blue!10}
\multicolumn{1}{c}{\texttt{Task 1: Response Length}} \\ 
\midrule

% Fine-tuned models section
\rowcolor{gray!10}
\texttt{Prompt for fine-tuned models} \\
\texttt{''} \\
\{\texttt{data}\} \\
\texttt{''} \\
($\rightarrow$ Gets formatted according to model's template) \\

% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} Why are oceans important to the global ecosystem? \\
\textbf{Output:} The oceans play a crucial role [...] \\ 
\midrule

% Base models section
\rowcolor{gray!10}
\texttt{Prompt for base models} \\
\texttt{''} \\
\texttt{Q: How can cross training benefit athletes?} \\
\texttt{A: Cross training offers various benefits [...] [END OF RESPONSE]} \\
\texttt{Q: What role does collaboration play in creativity?} \\
\texttt{A: Collaboration and originality complement each other [...] [END OF RESPONSE]} \\
\texttt{Q: \{data\}} \\
\texttt{A:} \\ 
\texttt{''} \\

% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} What are positive impacts of Reality TV? \\
\textbf{Output:} Reality TV provides entertainment and [...] [END OF RESPONSE] \\
\midrule



\rowcolor{blue!10}
\multicolumn{1}{c}{\texttt{Task 2: Reasoning Steps}} \\ \midrule

% Fine-tuned models section
\rowcolor{gray!10}
\texttt{Prompt for fine-tuned models} \\
\texttt{''} \\
\texttt{Provide step-by-step solution, starting with 'Step 1:'.} \\
\texttt{Problem:} \\
\texttt{\{data\}} \\
\texttt{''} \\
($\rightarrow$ Gets formatted according to model's template) \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} Randy has 60 mango trees on his farm. He also has 5 less than half as many coconut trees as mango trees. How many trees does Randy have in all? \\
\textbf{Output:} Step 1: Write down the information [...] \\ 
\midrule
% Base models section
\rowcolor{gray!10}
\texttt{Prompt for base models} \\
\texttt{''} \\
\texttt{Solve this problem step-by-step, starting with 'Step 1:'.} \\
\texttt{Few-shot examples:} \\
\texttt{Problem: Let f(x)=\{ax+3 if x$>$2; x-5 if -2$\leq$x$\leq$2; 2x-b if x$<$-2\}. Find a+b if f is continuous.} \\
\texttt{Step 1: At x=2: a(2)+3=2-5 [...] [END OF RESPONSE]} \\
\texttt{Problem: If x=2 and y=5, find (x\^{}4+2y\^{}2)/6.} \\
\texttt{Step 1: Substitute: (2\^{}4+2(5\^{}2))/6 [...] [END OF RESPONSE]} \\
\texttt{Problem: \{data\}} \\\\
\texttt{''} \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} Weng earns \$12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? \\
\textbf{Output:} Step 1: Substitute: 12(50/60) [...] [END OF RESPONSE] \\
\midrule


\rowcolor{blue!10}
\multicolumn{1}{c}{\texttt{Task 3: Character Choices}} \\ \midrule
% Fine-tuned models section
\rowcolor{gray!10}
\texttt{Prompt for fine-tuned models} \\
\texttt{''} \\
\texttt{Here's the first sentence of a story:} \texttt{\{data\}} \\
\texttt{Continue this story with one sentence that introduces a new animal character.} \\
\texttt{''} \\
($\rightarrow$ Gets formatted according to model's template) \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} Once upon a time, there was a big car named Dependable. \\
\textbf{Output:} As Dependable was cruising down the highway, a chatty parrot [...] \\ 
\midrule
% Base models section
\rowcolor{gray!10}
\texttt{Prompt for base models} \\
\texttt{''} \\
\texttt{First sentence: Lily was a little mouse who liked to follow her big brother Leo.} \\
\texttt{Continuation: The garden was peaceful that morning until [...] [Animal: owl] [END OF RESPONSE]} \\
\texttt{First sentence: Lila and Ben were playing in the park with their toys.} \\
\texttt{Continuation: While building their epic sandcastle [...] [Animal: rabbit] [END OF RESPONSE]} \\
\texttt{First sentence: Sara was lonely.} \\
\texttt{Continuation: As she sat on the front steps drawing patterns [...] [Animal: puppy] [END OF RESPONSE]} \\
\texttt{First sentence: Lily and Ben were twins who liked to go on walks with their mom and dad.} \\
\texttt{Continuation: Their morning hike through the woods [...] [Animal: squirrel] [END OF RESPONSE]} \\
\texttt{First sentence: \{data\}} \\
\texttt{Continuation:} \\
\texttt{''} \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} One day, a girl named Mia went for a walk. \\
\textbf{Output:} As she strolled through the park, she noticed a group of birds [...] [END OF RESPONSE] \\
\midrule


% Task 4
\rowcolor{blue!10}
\multicolumn{1}{c}{\texttt{Task 4: Multiple-Choice Answers}} \\ \midrule
% Fine-tuned models section
\rowcolor{gray!10}
\texttt{Prompt for fine-tuned models} \\
\texttt{''} \\
\texttt{Before choosing your answer, *briefly explain why in one short sentence*. Then select from the options:} \\
\texttt{\{data\}} \\
\texttt{''} \\
($\rightarrow$ Gets formatted according to model's template) \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} Sammy wanted to go to where the people were. Where might he go? A) race track B) populated areas C) the desert D) apartment E) roadblock \\
\textbf{Output:} **He wants to be around people, so he would go to a populated area.**B) populated areas \\ 
\midrule
% Base models section
\rowcolor{gray!10}
\texttt{Prompt for base models} \\
\texttt{''} \\
\texttt{Select the correct answer. Choose the single best answer.} \\
\texttt{Q: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?} \\
\texttt{A)ignore B)enforce C)authoritarian D)yell at E)avoid} \\
\texttt{A: The sanctions ignore the school's reform efforts, contradicting their purpose. Therefore A.[END OF RESPONSE]} \\
\texttt{Q: Sammy wanted to go to where the people were. Where might he go?} \\
\texttt{A)race track B)populated areas C)the desert D)apartment E)roadblock} \\
\texttt{A: If Sammy wants to find people, he would logically go to populated areas where many people gather. Therefore B.[END OF RESPONSE]} \\
\texttt{Q: \{data\}} \\
\texttt{A:} \\
\texttt{''} \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} Where do you put your grapes just before checking out? A) mouth B) grocery cart C) super market D) fruit basket E) fruit market \\
\textbf{Output:} The question asks where you put your grapes just before checking out. The answer is the grocery cart. Therefore B.[END OF RESPONSE] \\
\midrule


\rowcolor{blue!10}
\multicolumn{1}{c}{\texttt{Task 5: Answer Confidence}} \\ \midrule
% Fine-tuned models section
\rowcolor{gray!10}
\texttt{Prompt for fine-tuned models} \\
\texttt{''} \\
\texttt{Please first write analysis of the problem, then select the single correct answer to the following question from the options, and state your option explicitly:} \\
\texttt{\{data\}} \\
\texttt{''} \\
($\rightarrow$ Gets formatted according to model's template) \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} Damage to median nerve produces - A) Claw hand B) Winging ofscapule C) Ape thumb D) Wrist drop \\
\textbf{Output:} Damage to the median nerve [...] The correct answer is: B) Winging of scapula \\ 
\midrule
% Base models section
\rowcolor{gray!10}
\texttt{Prompt for base models} \\
\texttt{''} \\
\texttt{Select the correct answer. Choose the single best answer.} \\
\texttt{Q: Which vitamin is supplied from only animal source:} \\
\texttt{A)Vitamin C B)Vitamin B7 C)Vitamin B12 D)Vitamin D} \\
\texttt{A: Vitamin B12 (cobalamin) is exclusively found in animal products as it is synthesized by bacteria in animals, and cannot be obtained from plant sources. Therefore C.[END OF RESPONSE]} \\
\texttt{Q: All of the following are surgical options for morbid obesity except -} \\
\texttt{A)Adjustable gastric banding B)Biliopancreatic diversion C)Duodenal Switch D)Roux en Y Duodenal By pass} \\
\texttt{A: While Roux-en-Y gastric bypass is a common bariatric procedure, there is no such thing as 'Roux en Y Duodenal Bypass'. The correct procedures include gastric banding, biliopancreatic diversion, and duodenal switch. Therefore D.[END OF RESPONSE]} \\
\texttt{Q: \{data\}} \\
\texttt{A:} \\
\texttt{''} \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} Which of the following amino acid is active at neutral pH? A) Histidine B) Glycine C) Leucine D) Arginine \\
\textbf{Output:} Histidine is the only amino acid that is active at neutral pH. Therefore A.[END OF RESPONSE] \\
\midrule

\rowcolor{blue!10}
\multicolumn{1}{c}{\texttt{Task 6: Factual Consistency}} \\ \midrule
% Fine-tuned models section
\rowcolor{gray!10}
\texttt{Prompt for fine-tuned models} \\
\texttt{''} \\
\texttt{First discuss on this statement briefly in one short sentence, then explicitly state that you are in favour of the statement or explicitly state if you feel unsure instead:} \\
\texttt{\{data\}} \\
\texttt{''} \\
($\rightarrow$ Gets formatted according to model's template) \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} American children's adventure film The Wizard of Oz was made by Warner Bros. \\
\textbf{Output:} The Wizard of Oz is a classic American children's adventure film [...] I am unsure about the statement as it appears to be incorrect. \\ 
\midrule
% Base models section
\rowcolor{gray!10}
\texttt{Prompt for base models} \\
\texttt{''} \\
\texttt{Q: Only people named Floyd wearing pink are allowed to attend Pink Floyd concerts.} \\
\texttt{A: This contradicts known concert admission practices and Pink Floyd's actual ticket policies. I cannot be confident about the accuracy of this statement.[END OF RESPONSE]} \\
\texttt{Q: Fax works without any internet connection.} \\
\texttt{A: Fax machines transmit data through telephone lines independently of internet infrastructure. I support this statement as accurate.[END OF RESPONSE]} \\
\texttt{Q: Larry King served tea during his show.} \\
\texttt{A: Larry King was known to have beverages during his long-running CNN show to maintain vocal comfort during interviews. I support this statement as accurate.[END OF RESPONSE]} \\
\texttt{Q: The band Lynyrd Skynyrd formed up in Beijing, China.} \\
\texttt{A: Available historical documentation about Lynyrd Skynyrd's origins contains conflicting or unclear information about their early formation. I cannot be confident about the accuracy of this statement.[END OF RESPONSE]} \\
\texttt{Q: \{data\}} \\
\texttt{A:} \\
\texttt{''} \\
% Example response section
\rowcolor{gray!10}
\texttt{Example Response} \\
\textbf{Data:} The crack in the Liberty Bell sets it apart from other famous bells. \\
\textbf{Output:} The Liberty Bell is a famous bell in the United States. I support this statement as accurate.[END OF RESPONSE] \\
\midrule



\end{longtable}


\subsubsection{Labeling}
\label{appendix:setup_labeling}

After collecting model responses, we first extract base model outputs using the \texttt{[END OF RESPONSE]} signal. Then, for both base and fine-tuned models, we label and filter responses using these criteria:
\begin{enumerate}
\item \textbf{Response length}: Calculate token count using the model's tokenizer, excluding special tokens. Exclude responses exceeding the 1000-token limit or those that are incomplete.
\item \textbf{Reasoning steps}: Count remaining steps by identifying step markers (e.g., ``Step 1:''). Exclude responses with more than 8 steps.
\item \textbf{Character choices}: Identify animal mentions in responses, excluding cases with no animals, multiple animals, or animals in the first two words. Select the top-4 most frequent animals per model and label them 0-3.
\item \textbf{Multiple-choice answers}: Extract answer selections (e.g., ``the answer is D'') using pattern matching. Exclude responses with zero or multiple answers, or answers at sentence start. Label options A-E as 0-4.
\item \textbf{Answer confidence}: Match the model's selected option against ground truth, excluding cases with multiple or no choices. Label correct answers as 1, incorrect as 0.
\item \textbf{Factual consistency}: Identify explicit agree/disagree statements and compare with ground truth, excluding cases without explicit agreement/disagreement. Label as 1 if the model agrees with true statements or disagrees with false ones, 0 otherwise.
\end{enumerate}

Then we perform data augmentation by: (1) removing responses shorter than 8 tokens and balancing class distributions across classification tasks while equalizing dataset sizes across models; (2) generating additional examples by randomly truncating responses several tokens before key information appears (e.g., end-of-response token, animal names in character choices, or option selections in multiple-choice answers), computing corresponding labels, and grouping original and augmented data to ensure they are assigned to the same data split (train/test/validation).



\subsubsection{Representation Collection}
\label{appendix:setup_representation}
For each truncated response, we concatenate the original LLM input with the truncated text and perform a forward pass to obtain representations from all layers at the truncation point. For answer-start representations, we directly use a forward pass on the original input. We then pair these collected representations with their corresponding labels to create the final dataset.





