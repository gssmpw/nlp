\begin{figure*}[tb!] 
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_ScalingLaw_TinyStories_f1_score.pdf}
        \caption{Character choices prediction (TinyStories).}
    \end{subfigure}
    \hfill  % Creates horizontal spacing between subfigures
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_ScalingLaw_ultrachat_spearman.pdf}
        \caption{Response length prediction (Ultrachat).}
    \end{subfigure}
    \vspace{-5pt}
    \caption{Scaling effects on planning capabilities. Evaluated across four model families (LLaMA-2-chat, LLaMA-3-Instruct, Qwen-2-Instruct, Qwen-2.5-Instruct; 1.5Bâ€“72B) using UltraChat and TinyStories, structure and content attributes show family-specific scaling: larger models within each family improve planning.}
    \label{fig:exp_ablation_scaling}
    \vspace{-5pt}
\end{figure*}

\begin{figure*}[tb!] 
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_TinyStories_stepwise_f1_dynamic_uniform.pdf}
        \caption{Character choice prediction (TinyStories).}
    \end{subfigure}
    \hfill  % Creates horizontal spacing between subfigures
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp_medmcqa_stepwise_f1_dynamic_uniform.pdf}
        \caption{Answer confidence prediction (MedMCQA).}
    \end{subfigure}
    \vspace{-10pt}
    \caption{U-shaped planning dynamics during generation. Probing at equidistant positions (character choice, answer confidence) shows three-phase patterns: high accuracy in early segments (global planning intent), mid-segment decline (local token focus), and late-stage recovery (contextualized refinement). This suggests models first outline global attributes, then refine locally, before finalizing coherent plans.}
    \label{fig:exp_ablation_dynamics}
    \vspace{-10pt}
\end{figure*}

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/exp_reflection_analysis_ultrachat_final.pdf}
    \vspace{-30pt}
    \caption{Implicit-explicit planning discrepancy. Models struggle to explicitly predict their own response lengths, with base models showing near-zero Spearman correlation and most fine-tuned models achieving only marginal gains. This gap implies limited introspective awareness despite underlying capability.}
    \label{fig:exp_ablation_selfAware_ultrachat}
    \vspace{-18pt}
\end{figure}

\section{Ablation}

\subsection{Planning Ability Scales with Model Size}
We analyze how emergent response planning scales across different model sizes using four model families: LLama-2-chat (7B, 13B, 70B), Llama-3-Instruct (8B, 70B), Qwen-2-Instruct (7B, 72B), and Qwen-2.5-Instruct (1.5B, 32B, 72B). Using grid search over layers and hidden sizes, we identify optimal configurations and evaluate models on UltraChat and TinyStories datasets, focusing on structure and content attributes.
We exclude base models as the relatively small models have short context which limit few-shot prompts, while the same prompts fail to effectively prompt larger base models to follow instructions. We omit the behavior attribute type as larger models tend to give correct answers consistently, making it difficult to obtain balanced data for analysis.

Results shown in Fig.~\ref{fig:exp_ablation_scaling} exhibit two key insights:  \textbf{(1)} within each model family, larger models demonstrate stronger planning capabilities, and \textbf{(2)} this scaling pattern does not generalize across different model families, suggesting that other factors like architectural differences also influence planning behavior.





\subsection{Evolution of Planning Representations During Response Generation}
We analyze how planning features evolve during generation by probing at different positions in the response sequence. For each response, we collect activations from the first token up to the token before attribute-revealing keywords (e.g., animal words in story character selection tasks) or throughout the entire sequence for tasks requiring external ground-truth labels (e.g., answer confidence tasks). We divide these positions into equal segments and apply  probes previously trained with in-dataset settings at each division point.
We conduct experiments on two datasets: TinyStories for character choice prediction and MedMCQA for answer confidence prediction. Results in Fig.~\ref{fig:exp_ablation_dynamics} reveal a distinctive pattern: \textbf{probing accuracy is high initially, decreases in the middle segments, and rises again toward the end}. This pattern suggests a three-phase planning process:
(1) initial phase with strong planning that provides an overview of the intended response;
(2) middle phase with weaker planning, characterized by more local, token-by-token generation;
(3) final phase with increased planning clarity as accumulated context makes the target attributes more apparent.



\subsection{Gap Between Probing and LLMs' Self-Predicted Results}

We investigate how the models' ability to predict their own response attributes compare to probe-based predictions with the UltraChat dataset on response length task. For fine-tuned models, we prompt: "Estimate your answer length in tokens using \texttt{[TOKENS]number[/TOKENS]}, then provide your answer." For base models, we provide few-shot examples with pre-calculated lengths. To evaluate self-prediction accuracy, we compare the predicted token count collected in a separated run against the length of the previous-collected model's greedy-decoded response.

As shown in Fig.~\ref{fig:exp_ablation_selfAware_ultrachat}, base models achieve near-zero Spearman correlation when predicting token lengths, even with examples. While fine-tuned models perform marginally better, there remains a substantial gap between models' direct predictions and probe-based predictions.
This gap suggests that \textbf{models encode more planning information in their hidden representations than they can explicitly access during token-by-token generation}, indicating a discrepancy between implicit planning capabilities and explicit self-awareness.

% While our results show emergent planning capabilities in LLMs, we also investigate whether models can explicitly predict attributes of their future responses. We test this using the response length prediction task on the UltraChat dataset, comparing direct model predictions against probe-based predictions.
% For fine-tuned models, we prompt: "Given this question, first estimate the number of tokens you would need for a complete answer using format as \texttt{[TOKENS]number[/TOKENS]}, then give your answer." For base models, we provide few-shot examples with pre-calculated token lengths.

% As shown in Fig.~\ref{fig:exp_ablation_selfAware_ultrachat}, we evaluate prediction accuracy using Spearman correlation. Base models struggle to predict their token lengths even with examples, while fine-tuned models perform better. However, there is an obvious gap between models' direct predictions with probe-based predictions, suggesting that probing better captures this capability.





% \subsection{Models across Different Architectures Show Similar Deep-in Planning Patterns}


% We investigate whether representations from one model can predict the behavior of another. Specifically, given a prompt $\mathbf{x}i$, can we use Model A's representations $\mathcal{H}_{i, A} = \{ \mathbf{H}^l{\mathbf{x}_{i, A}}\}^L_{l=1}$ to predict Model B's output label $\hat{g}_{i, B} = g(\mathbf{y}_{i, B})$? Success would suggest these representations capture intrinsic patterns beyond model-specific features.

% Results on the TinyStories dataset (Figure~\ref{fig:exp_cross_TinyStories}) compare models as predictors (horizontal axis) versus targets (vertical axis). We observe that: (1) diagonal terms (same predictor and target) show highest performance, indicating models best predict their own behavior; (2) strong non-diagonal performance suggests shared representations across different architectures, implying common underlying planning mechanisms.


% \begin{figure}[tb!]
%     \centering
%     % \vspace{-18pt}
%     \includegraphics[width=1.0\columnwidth]{figures/ablation_cross_model_correlations_f1_score_TinyStories_story_continuation.png}
%     \vspace{-4pt}
%     \caption{
%     Cross-model test for chat models on TinyStories dataset, with F1 score as metric.
%     }
%     \label{fig:exp_cross_TinyStories}
%     \vspace{-7pt}
% \end{figure} 

% \DZCtodo{First collect model A's activations on model B's responses, then use features of model A to predict label of model B's response;
% Results: 1) non-diagonal terms are much worse than diagonal terms, thus the planning features are model-specific, rather than a simple problem-label mapping; 2) some non-diagonal terms are non-zero, means different models show similar planning features to some degrees}

