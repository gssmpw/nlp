% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% added by Luca for equations:
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{subcaption}




% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora}




\author{
  Tristan Karch\thanks{Equal contribution.} \\
  EPFL / DHLab \\
  \texttt{tristan.karch@gmail.com} 
  \And
  Luca Engel$^*$ \\
  EPFL / DHLab \\
  \\
  \\\AND
  Philippe Schwaller \\
  EPFL / ILIAC \\
  \And 
  Frédéric Kaplan \\
  EPFL / DHLab \\
}


\begin{document}
\maketitle
\begin{abstract}
As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using three strategically selected datasets: EPFL PhD manuscripts (likely containing novel specialized knowledge), Wikipedia articles (presumably part of training data), and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.
\end{abstract}

\section{Introduction}

Recent advances in large language models (LLMs) have revealed a striking phenomenon: as these models scale, they tend to develop remarkably similar internal representations and capabilities regardless of their architecture \cite{huh2024platonicrepresentationhypothesis}. This convergence, coupled with established scaling laws \cite{kaplan2020scalinglawsneurallanguage} and the growing recognition that AI development should shift from model-centric to data-centric approaches \cite{zha2025datacentric}, suggests that the key to advancing LLM capabilities lies in identifying and incorporating novel, high-quality information sources rather than architectural innovations. However, identifying valuable text collections for model enhancement presents significant challenges: digitization and preprocessing are costly, and training or fine-tuning models on new data requires substantial computational resources.\\

This creates a critical need: how can we efficiently evaluate whether a text collection contains information that would meaningfully expand an LLM's knowledge? Current approaches typically require actually training or fine-tuning models on new data to assess its value - an expensive and time-consuming process. While retrieval-augmented generation (RAG) \cite{lewis2020rag} offers a promising approach for helping models access long-tail knowledge without full retraining \cite{pmlr-v202-kandpal23a}, this still requires careful curation of knowledge bases and comes with its own computational costs. Moreover, the challenge of identifying valuable text collections remains: digitization and preprocessing are costly, and organizations need ways to evaluate the information potential of document collections before investing in their integration.\\

We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates and leverages multiple choice questions (MCQs) to systematically probe whether the knowledge contained in a text collection is already accessible to an LLM or represents genuinely novel information.\\

To rigorously validate our approach, we evaluate it across three strategically selected datasets: (1) EPFL PhD manuscripts, containing specialized academic knowledge likely novel to LLMs, (2) Wikipedia articles that were presumably part of the LLMs' training data, and (3) a synthetic baseline dataset composed of model-generated texts. Our key contributions are:
%
\begin{enumerate}[noitemsep, nolistsep]
  \item A novel end-to-end pipeline that uses MCQs to efficiently evaluate the potential information gain from text collections without requiring model training or fine-tuning
  
  \item A self-supervised methodology for ensuring MCQ quality through complementary similarity metrics that address two key challenges: verifying question relevance and grounding in source material, while ensuring distractors are plausible yet unambiguously incorrect
  
  \item A systematic analysis of knowledge gaps across diverse text collections that identifies areas of model uncertainty, quantifies the potential value of different information sources, and guides strategic decisions about data collection and integration
\end{enumerate}

Importantly, our approach is dataset-agnostic and can be applied to any text corpus. This makes it applicable to different domains and research questions. This pipeline could be leveraged to select textual data for model enhancements through fine-tuning or retrieval-augmented generation systems.

\section{Related Work}

\noindent\textbf{Information Gain. }
The challenge of efficiently selecting new information sources for LLMs can be viewed through the lens of optimal experiment design, a field pioneered by \citet{fedorov1972theory}. This framework emphasizes maximizing information gain while being strategic about resource allocation -- a particularly relevant consideration given the costs associated with integrating new data into LLM systems. Information gain itself has been conceptualized across various fields: in information theory, it relates to reductions in algorithmic information content \cite{cover1989kolmogorov}; in machine learning, it quantifies a feature's contribution to model performance \cite{ODHIAMBOOMUYA2021114765}; and in cognitive science, it represents uncertainty reduction in our experience of the world \cite{damiano2021visual}. While these theoretical frameworks provide valuable insights, they have nott been previously applied to the specific challenge of evaluating the potential value of text collections for enhancing LLM knowledge. Our work bridges this gap by proposing a practical, MCQ-based approach that quantifies information gain by measuring an LLM's ability to answer questions about a text collection with and without access to the source material.\\

\noindent\textbf{Knowledge Detection in LLMs. }
Prior research has developed several methods to analyze how LLMs process and retain textual information. Work on memorization \cite{hartmann2023sokmemorizationgeneralpurposelarge,shi2024detectingpretrainingdatalarge} and data contamination \cite{yax2024assessingcontaminationlargelanguage,golchin2024datacontaminationquiztool} focuses on identifying verbatim recall of training data, while hallucination detection \cite{farquhar_detecting_hallucinations_llms_semantic_entropy} aims to identify when models generate false information. Research on novelty detection has primarily focused on linguistic and semantic novelty \cite{language_novelty_1,semantic_linguistic_novelty_focus_paper}, with less attention paid to factual novelty. While these approaches provide valuable insights into model behavior, they are retrospective -- analyzing what models have already learned or memorized. In contrast, our work takes a prospective approach, developing metrics to evaluate the potential value of new information sources before investing in their integration into LLM systems.


\section{Methods}

Multiple Choice Questions (MCQs) are a well-established tool for knowledge assessment, supported by research in cognitive science and educational psychology~\cite{Haladyna01072002}. Their four-option format, consisting of one correct answer and three distractors, offers an optimal balance between assessment reliability and cognitive load~\cite{Vyas2008}. MCQs are particularly valuable for automated evaluation as they provide objective correctness measures while efficiently testing understanding across diverse topics~\cite{Oc21102024}. When working with Large Language Models (LLMs), MCQs offer an additional advantage: they constrain the output space to a finite set of options, eliminating the ambiguity and variability inherent in open-ended responses and enabling precise evaluation of model knowledge.

Generating high-quality MCQs presents unique challenges, particularly due to the absence of ground truth signals typically available in human-curated educational assessments. Without explicit supervision on what constitutes a good question or appropriate distractors, we must derive reliable metrics to filter and validate the generated MCQs. This self-supervised setting raises two critical challenges: ensuring questions are both relevant and non-trivial, and guaranteeing that distractors are plausible yet unambiguously incorrect. The latter is particularly crucial when evaluating LLMs -- if distractors are too simple, models might succeed through elimination rather than true knowledge; if too similar to the correct answer, they might confuse models even when provided with context. To address these challenges, we propose similarity-based filtering metrics that serve as proxy signals for question quality and difficulty, enabling automated quality control without human supervision.

% \textcolor{red}{Add a brief theoretical justification for why MCQs are particularly suitable for knowledge evaluation, Include references to cognitive science or educational assessment literature about MCQ effectiveness}

\subsection{Pipeline}
As illustrated in Figure \ref{fig:pipeline}, our pipeline consists of three key stages: (1) MCQ generation from text using LLMs, (2) quality-focused filtering using similarity metrics, and (3) position-debiased evaluation to assess model knowledge both with and without context. 

\begin{figure}[h]
    \centering
    \includegraphics[width=.35\textwidth]{Images/pipeline_vert.png}
    \caption{\textbf{Overview of our knowledge evaluation pipeline.} 
    The framework consists of three main components: 
    (1) MCQ Generation using LLMs to create questions from input datasets, 
    (2) Quality-focused filtering using similarity metrics, and 
    (3) Position-debiased evaluation to assess model knowledge with and without context. 
    ($c$: context, $q$: question, $g$: ground truth, $d_i$: distractors)}
    \label{fig:pipeline}
\end{figure}


\subsection{MCQ Generation and Filtering}

\noindent\textbf{Question Generation Process.} The input text is first divided into manageable chunks of 2000 words to ensure consistent context length across questions. These chunks serve as the basis for LLM-generated MCQs, where each question is crafted to test understanding of specific information within the chunk. The exact prompts used for generation can be found in table~\ref{tab:prompts} of the Supplementary Methods.\\

\noindent\textbf{Two-Stage Quality Filtering.} To address the dual challenges of ensuring question relevance and appropriate distractor difficulty without human supervision, we employ two complementary filtering mechanisms:

\begin{enumerate}
\item \textit{Context-Answer Alignment Filter:} To ensure questions are both relevant and grounded in the source material, we verify that the correct answer is more strongly aligned with the source context than any distractor:
\begin{equation}
    \min_i \left[\text{sim}(c, g) - \text{sim}(c, d_i)\right] \quad \text{for} \ i = 1, 2, 3
    \label{eq:similarity_comparison_input_correct_minus_input_distractor}
\end{equation}
where similarity is measured using both Jaccard index and ROUGE-L score~\cite{lin-2004-rouge}. This helps eliminate misleading or incorrect MCQs while ensuring questions test information actually present in the context.

\item \textit{Distractor Plausibility Filter:} To prevent trivial questions while maintaining unambiguous correctness, we ensure distractors are semantically similar to the correct answer but not identical:
\begin{equation}
    \max_i \left[ \text{cos-sim}(g, d_i) \right] \quad \text{for} \ i = 1, 2, 3
    \label{eq:similarity_comparison_correct_answer_distractor}
\end{equation}
using NVIDIA's state-of-the-art NV-Embed-v2 model~\cite{nvidia_embed_v2_first,nvidia_embed_v2_second} for text embeddings. This creates challenging questions where distractors are plausible enough to require true understanding while remaining distinctly incorrect.
\end{enumerate}


\subsection{Evaluation}

We evaluate the information potential ($IP$) of a context $c$ for a given LLM $f$ with Eq~\ref{eq:knowledge_gain_formula} as the improvement in correctness when incorporating context in the prompt. Formally, it is defined as:
\begin{equation}
    IP = \frac{C_{\text{context}} - C_{\text{direct}}}{|\mathcal{Q}| - (I_{\text{context}} + I_{\text{direct}})}
    \label{eq:knowledge_gain_formula}
\end{equation}
where:
\begin{small}
\begin{align*}
    C_{\text{context}} &= \sum_{q, c \in \mathcal{Q}} \mathbbm{1}(f(q|c) = g), \quad C_{\text{direct}} = \sum_{q \in \mathcal{Q}} \mathbbm{1}(f(q) = g), \\
     \\
    I_{\text{context}} &= \sum_{q, c \in \mathcal{Q}} \mathbbm{1}(f(q|c) \neq g), \quad I_{\text{direct}} = \sum_{q \in \mathcal{Q}} \mathbbm{1}(f(q) \neq g).
\end{align*}
\end{small}
Here, \( \mathbbm{1}(\cdot) \) is an indicator function equal to 1 if the condition is true, and 0 otherwise. The denominator ensures that we focus on questions t rather than letting widespread model failures distort the results. \\

\noindent\textbf{Position Bias Mitigation.} To address the known issue of positional bias in LLMs~\cite{position_bias_paper}, we evaluate each MCQ four times, rotating the correct answer through all possible positions (A, B, C, D) while randomly arranging the distractors. This effect can be seen in Figures \ref{fig:positional_bias_gpt_mcq_generation}, \ref{fig:positional_bias_mcq_eval_llama70b_combined}, and \ref{fig:positional_bias_mcq_eval_combined}. This ensures that model performance reflects true knowledge rather than position-based preferences.

\subsection{Data}

To evaluate our pipeline's efficacy, we conduct experiments across three strategically selected datasets representing distinct knowledge domains: (1) 177 EPFL PhD manuscripts containing specialized academic research, (2) Wikipedia articles, and (3) synthetic LLM-generated text. This dataset composition enables systematic assessment of knowledge novelty. We hypothesize that PhD manuscripts will yield the highest knowledge gaps, containing recent research contributions likely absent from training data. The synthetic dataset, comprising model-generated content, serves as a lower-bound baseline for knowledge novelty. Wikipedia articles, being a primary source for LLM training \cite{wikipedia_for_gpt_3,wikipedia_pretraining_llama_1}, function as a control group representing knowledge presumably well-embedded in the models' parameters.\\

\noindent\textbf{Data collection. }  The Wikipedia dataset is created by leveraging the Wikipedia API. With the help of the PhD manuscript titles, related articles are fetched and combined into similar-sized texts. Similarly, the manuscript titles are used to generate the baseline dataset. To overcome the LLMs' generation limits, in a first step, subtopics are generated surrounding the manuscript titles. Then, chunks of around 600 words are generated, using the subtopics as overarching themes, and concatenated.\\

All data used in our experiments (including chunks, synthetic and baseline datasets) as well as the complete code of our evaluation pipeline are made available open-source with this submission under creative commons license.


% \section{Experiments}

\section{Results}

Our analysis reveals three key findings: (1) similarity thresholding effectively increases question difficulty while maintaining answerable questions, (2) PhD manuscripts contain significantly more novel information compared to Wikipedia and synthetic datasets, and (3) larger language models show consistently lower information potential across all datasets, suggesting better knowledge retention during pre-training. We present detailed evidence for each of these findings below.

\subsection{Effectiveness of Similarity Thresholding}

\begin{figure}[h]
    \centering
    % Subfigure 1
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/cutoffs/Llama_70B_EPFLonlyCosine_one_column.png}
        \caption{Only Thresholding with Cosine Similarity}
        \label{fig:cutoffs_llama_70b_epfl_only_cosine}
    \end{subfigure}
    
    \vspace{1em} % Adds vertical spacing between figures
    
    % Subfigure 2
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/cutoffs/Llama_70B_EPFL_only_jaccard_and_rouge_one_column.png}
        \caption{Only Thresholding with Jaccard and Rouge-L}
        \label{fig:cutoffs_llama_70b_epfl_only_jacc_rouge_l}
    \end{subfigure}

    \caption{\textbf{Llama 3 70B Performance on EPFL Dataset Across Different Cutoff Percentages.} (with Separated Thresholding and Number of MCQs for the Corresponding Percentiles). \textbf{NC 4x:} Evaluation with no context correct for all 4 bias mitigation evaluations of each question; and 
    \textbf{WC 4x:} Evaluation with context correct for all 4 bias mitigation evaluations of each question.}
    \label{fig:llama_70b_epfl_cutoffs}
\end{figure}


\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/Llama_70B_Baseline_by_Gpt-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{Llama 70B Baseline\\IP: 0.125}
        \label{fig:epfl_overall_baseline}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/Llama_70B_Wikipedia_by_GPT-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{Llama 70B \textcolor{blue}{Wikipedia}\\IP: 0.136}
        \label{fig:epfl_overall_wikipedia_2}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/Llama_70B_EPFL_MCQs_by_GPT-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{Llama 70B \textcolor{teal}{EPFL}\\IP: 0.229}
        \label{fig:epfl_overall_mcqs}
    \end{subfigure}


    \vspace{0.1cm}

    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/GPT_4o_Wikipedia_by_GPT-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{GPT-4o \textcolor{blue}{Wikipedia}\\ IP: 0.110}
        \label{fig:epfl_overall_wikipedia}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/GPT_4o_EPFL_by_GPT-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{GPT-4o \textcolor{teal}{EPFL}\\IP: 0.211}
        \label{fig:epfl_overall_books}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/overall_eval_legend.png}
        \caption{Legend}
        \label{fig:overall_eval_venn_lengend}
    \end{subfigure}
    
    \caption{\textbf{Information Potential Analysis Across Datasets and Models.}
Venn diagrams showing correct response overlap between context-free and context-provided conditions. Higher Information Potential (IP) scores indicate greater novel information content, with PhD manuscripts (EPFL) showing consistently higher IP (0.211-0.229) compared to Wikipedia (0.110-0.136) and synthetic baseline (0.125). Both open and closed-source models exhibit similar patterns despite architectural differences.}
    \label{fig:venn_statistics_overall}
\end{figure*}

\noindent\textbf{Effect of Cosine Similarity Thresholding.} Figure \ref{fig:cutoffs_llama_70b_epfl_only_cosine} displays the effect of cosine thresholding. It shows a strong decrease of around 10\% between the absence of thresholding and the 50th percentile cutoff in the performance of the model with no context (NC 4x), while that of the model with context (WC 4x) remains steady. Therefore, the cosine similarity has the expected effect of increasing the MCQ difficulty when answering without context while maintaining the context-based performance.\\

\noindent\textbf{Effect of Jaccard and ROUGE-L Thresholding.} A different trend is observed when applying the Jaccard and ROUGE-L thresholding, as seen in Figure \ref{fig:cutoffs_llama_70b_epfl_only_jacc_rouge_l}. Here, a slight increase in performance with context (WC 4x) of 2\% can be observed between no thresholding and the 50th percentile cutoff, while performance without context (NC 4x) remains stable.\\

\noindent\textbf{Comparative Analysis of Thresholding Methods.} Our results demonstrate that cosine similarity thresholding is more effective at increasing question difficulty than Jaccard and ROUGE-L methods. This difference likely stems from cosine similarity's ability to capture semantic relationships in the embedded space, while Jaccard and ROUGE-L focus on surface-level textual overlap. The gradual reduction in MCQ count under cosine thresholding (from 1633 to 653) suggests a more controlled filtering process compared to the sharp drop-off observed with Jaccard and ROUGE-L (907 to 479 MCQs between 50th and 60th percentiles).


\subsection{Information Potential (IP) Across Different Datasets}


\noindent\textbf{EPFL PhD Manuscripts (IP: 0.229).} The EPFL dataset yields the highest information potential among all tested collections. We observe a significant performance gap between context-free (73.4\%) and context-provided (98\%) conditions, highlighting the substantial amount of novel information in these documents. The questions generated from this dataset predominantly focus on specialized technical concepts and novel research findings. These performance patterns strongly indicate that PhD manuscripts contain substantial novel information not present in the model's pre-training data.\\

\noindent\textbf{Wikipedia Dataset (IP: 0.136).} The Wikipedia dataset demonstrates moderate information potential, with a notably smaller gap between context-free and context-provided performance (83.6\% vs. 97.8\%). Questions from this dataset primarily cover general knowledge and well-documented facts. The performance patterns suggest partial inclusion of this content in the model's training data, which aligns with our expectations given that these articles were selected from versions predating the model's training cutoff date.\\

\noindent\textbf{Synthetic Baseline Dataset (IP: 0.125).} Among the three datasets, the synthetic baseline yields the lowest information potential. We observe a minimal performance gap between conditions (85.5\% vs. 98.8\%), with high performance in the context-free setting reflecting the model's inherent familiarity with self-generated content. This dataset effectively serves as a lower bound for the information potential metric, providing a useful reference point for evaluating other collections.

\subsection{Comparison between open and closed source model}

Despite their architectural and size differences, both Llama 70B and GPT-4o exhibit similar patterns in information potential across datasets, though with notable variations in magnitude. GPT-4o shows slightly lower information potential scores  compared to Llama 70B, suggesting better baseline knowledge retention in the larger model. Interestingly, the relative gap between datasets remains consistent across both models - EPFL manuscripts consistently show approximately double the information potential compared to Wikipedia. This consistency across different model architectures and training approaches strengthens the validity of our information potential metric. Additionally, both models maintain near-perfect performance (>97\%) when provided with context, regardless of the dataset, indicating robust comprehension capabilities when given access to relevant information.

While a more comprehensive analysis comparing a wider range of LLMs could provide additional insights into the relationship between model architecture, size, and information potential, such an investigation falls outside the scope of this study, which focuses on establishing the validity of our evaluation methodology.

\subsection{Qualitative Analysis of High-Value Information}

\begin{table*}[h]
    \centering
    % \renewcommand{\arraystretch}{1.2} % Adjust row height
    \setlength{\tabcolsep}{5pt} % Adjust column spacing$
    \renewcommand{\arraystretch}{1.5}
    \footnotesize
    \begin{tabular}{|p{7cm}|p{7cm}|}
        \hline
        \normalsize \textbf{MCQ} (correct answer in italics) & \normalsize \textbf{Relevant Context Passages}\\ %& 
        \hline
        \textbf{What is a common feature of 'interference errors' and 'reduction of intentionality errors'?} \newline
          \textit{A) Both involve replacing the correct subject with another similar one.} \newline
          B) Both typically result from incorrect or incomplete mental models. \newline
          C) Both involve action reversals. \newline
          D) Both require complex decision-making at the knowledge-based level.
          % Confirmed: A. \newline 
          % Refuted: B, D (Errors occur at the skill-based level, not due to mental models or complex decision-making). \newline
          % Refuted: C (Action reversals involve undoing an action, not replacing a subject). \\ 
          &
          [...] Interference errors. These errors occur when people multi-task, i.e., when multiple sequences of action are active at the same time, which can result in a person combining them. [...] this error and the ones for reduced intentionality and perceptual confusions can be modeled by replacing the correct subject with another one. [...] Reversals [...] cause a person to undo a previously performed action
          \\
        \hline
        \textbf{Which error describes memorizing an action without proceeding to its next logical step?} \newline
          \textit{A) Repetition.} \newline
          B) Omission. \newline
          C) Timing error. \newline
          D) Sequence error. 
          % Confirmed: A (Repetition leads to misjudging progress and repeating an action instead of moving forward). \newline
          % Refuted: B (Omission skips an action instead of repeating). \newline
          % Refuted: C (Timing errors relate to incorrect timing, not memorization). \newline
          % Refuted: D (Sequence errors relate to out-of-order execution). \\ 
          &
          [...] Repetitions. These errors cause a person to misjudge the progress of a sequence of actions, making them perform an action already carried on. [...] Omission, when a user skips the current action and execution continues with its successor, e.g., jumping from action i to action i + 1 [...] Timing errors, when users interact with a system at the wrong time, e.g., too early or too late [...] Sequence errors, when users execute an action out of order [...]
          \\
        \hline

        \textbf{How are articulatory features (AF) different from phone posterior features in terms of prediction?} \newline
          A) AFs rely on spectral analysis. \newline
          B) AFs use a frame-to-phoneme alignment. \newline
          \textit{C) AFs map phonemes to articulatory features.} \newline
          D) AFs predict phonemes directly. 
          % Confirmed: C (AFs use a mapping from phonemes to features). \newline
          % Refuted: A (AFs do not rely on spectral analysis). \newline
          % Refuted: B (Both AFs and phone posterior features use frame-to-phoneme alignment). \newline
          % Refuted: D (AFs do not predict phonemes directly). \\ 
          &
          [...] There are different ways to represent phonemes as articulatory features, e.g. as binary features (Chomsky and Halle, 1968) or multi-valued features (Ladefoged, 1993). Similar to phone posterior features, they are trained from a frame-to-phoneme alignment. However, instead of predicting phonemes, a mapping from phones to AF is used as targets of the predictor. [...] AFs are modeled by 18 off-the-shelf recurrent neural networks (RNN) based binary classifiers, i.e. D = 18 × 2. The RNNs take as input log energies of 33-dimensional Mel filterbank energies. [...] Similar to phone posterior features, [AFs] are trained from a frame-to-phoneme alignment [...]
          \\
        \hline
    \end{tabular}
    \caption{\textbf{Examples of technical terminology, novel findings, and complex relationships in EPFL PhD manuscripts.} MCQs requiring context for correct model responses}
    \label{tab:mcq_analysis}
\end{table*}


The MCQs presented in Table \ref{tab:mcq_analysis} exemplify three key patterns in identifying valuable information:

\begin{enumerate}[noitemsep]
\item \textbf{Technical Terminology:} The question about articulatory features demonstrates the significant performance gap when dealing with specialized terminology. This question, drawn from speech recognition research, requires specific context to understand how articulatory features differ from phone posterior features in their prediction approach. The model's inability to answer correctly without context highlights how technical domains in PhD manuscripts contain specialized knowledge not captured in pre-training. Such terminology-heavy questions serve as reliable indicators of domain-specific knowledge.

\item \textbf{Novel Findings:} The question regarding "interference errors" exemplifies how PhD manuscripts capture recent research outcomes. The detailed distinction between interference errors and reduction of intentionality errors represents novel academic insights that weren't available during model pre-training. This type of question effectively identifies valuable new knowledge contributions from academic manuscripts. The consistency with which models fail these questions without context, despite their strong general reasoning capabilities, suggests genuine knowledge gaps rather than reasoning limitations.

\item \textbf{Complex Relationships:} The question about repetition errors showcases the importance of precise contextual information in understanding intricate conceptual relationships. The distinction between repetition and other error types (omission, timing, sequence) requires careful understanding of how these concepts interrelate. This category highlights a key challenge in assessing knowledge: the line between pure knowledge recall and reasoning ability becomes blurred when concepts are interconnected in complex ways. The model's performance on such questions suggests that even sophisticated reasoning capabilities cannot compensate for missing foundational knowledge.
\end{enumerate}
%
Additional examples of these patterns can be found in Table \ref{tab:mcq_analysis_annex}.



\section{Conclusion and Limitations}

\subsection{Summary of Contributions}

This work introduces a novel and efficient approach for evaluating the information potential of text collections for large language models. Our key contributions include:
\begin{enumerate}[nolistsep]
\item \textbf{Efficient Evaluation Pipeline:} We present a systematic approach combining automated MCQ generation, sophisticated filtering mechanisms, and comparative evaluation to assess information potential without requiring model training or fine-tuning.
\item \textbf{Empirical Validation:} Our results validate the method's effectiveness by demonstrating alignment with intuitive expectations across both dataset types and model scales. The information potential increases from synthetic baseline (0.125) to Wikipedia (0.136) to EPFL manuscripts (0.229), while larger models (GPT-4o) consistently show lower information potential than smaller ones (Llama 70B), suggesting better knowledge retention.
\item \textbf{Qualitative Framework:} We propose a taxonomy of high-value information types (technical terminology, novel research findings, and complex relationships), providing deeper insights into the nature of valuable information sources for LLMs.

\end{enumerate}

\subsection{Limitations and Future Work}

\noindent\textbf{Practical Considerations. }Our approach, while effective, presents several methodological limitations that suggest directions for future research. The current pipeline's generation-first approach, where questions are created before applying filtering criteria, could be enhanced by incorporating context-answer alignment and distractor plausibility directly into the generation process. Such an integrated approach could improve question quality while reducing computational overhead.

The scope of our evaluation faces an inherent challenge that manifested both as a limitation of our study and, ironically, in the process of writing this paper: identifying datasets that are definitively outside of LLMs' training data. While PhD manuscripts serve as a reasonable proxy for novel information, the rapid integration of online content into training sets makes finding suitable test collections increasingly difficult. This challenge underscores the timeliness of our work, as it becomes crucial to identify and preserve valuable information sources before they are absorbed into general training data.

Perhaps most significantly, our method deliberately focuses on information selection rather than distillation. This choice, while aligned with our goal of evaluating potential value, leaves open questions about how selected information can be optimally incorporated into LLM training. Recent work has begun to address this gap, with approaches such as SKILL \citep{moiseev-etal-2022-skill} exploring efficient knowledge integration methods. However, \citet{liu-etal-2024-untangle} highlight potential conflicts between newly added information and existing knowledge compressed in LLMs' internal representations, suggesting that information integration remains a complex challenge requiring further investigation.\\

\noindent\textbf{Broader Considerations. }While our approach requires digital text for MCQ generation, it offers a significantly more efficient alternative to full model training or fine-tuning. This enables strategic sampling approaches where representative portions of larger collections can be evaluated to inform broader digitization decisions. For instance, assessing a few chapters can inform decisions about entire book collections, making the method particularly valuable for resource-constrained scenarios. This sampling-based approach could revolutionize how institutions prioritize their digitization efforts, allowing for data-driven decisions about resource allocation in preservation projects.

Our effort to categorize high value information sheds light on fundamental challenges in distinguishing between information that is unknown to an LLM versus information it fails to retrieve. This raises important questions about the relationship between knowledge possession and practical competence in AI systems. When an LLM consistently fails to demonstrate knowledge in a specific domain, the distinction between these cases may become less relevant from a practical perspective. This observation has broader implications for how we conceptualize and evaluate knowledge in AI systems, suggesting that performance-based metrics might be more meaningful than theoretical attempts to map internal knowledge representations. Furthermore, this highlights the need for a more nuanced understanding of how LLMs process and utilize information, particularly when developing strategies for knowledge integration and model enhancement.



% \section{Acknowledgements\newpage
\bibliography{main}


\newpage
\appendix
\onecolumn
\section{Appendix}
\label{sec:appendix}


\subsection{Supplementary Methods}
\subsubsection{LLM Prompts}

\begin{table*}[ht]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{5pt} % Adjust column spacing$
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|p{0.2\textwidth}|p{0.7\textwidth}|} % Adjust widths to your liking
    \hline
    \normalsize \textbf{Task} & \normalsize \textbf{LLM Prompt} \\
    \hline
    
    \textbf{Multiple-Choice Question Generation} & 
    From the following piece of a scientific PhD manuscript: \newline
    \texttt{'TEXT\_HERE'} \newline
    Design a multiple-choice question with four answers: 'A', 'B', 'C', 'D'. Please provide the correct answer. The question needs to be difficult, but answers should not be ambiguous. Start the question with ['QUESTION'] and the answers with 'A', 'B', 'C', 'D'. Be concise! \newline
    Please generate a total of 10 MCQs. Avoid references to the manuscript itself (e.g., do not use phrases like 'according to the text,' 'as stated in the manuscript,' or 'based on the passage' etc.). Use the following format: '\texttt{[QUESTION] <question>} \newline
    \texttt{A) <option A>} \newline
    \texttt{B) <option B>} \newline
    \texttt{C) <option C>} \newline
    \texttt{D) <option D>} \newline
    \texttt{Correct answer: <correct answer letter>) <correct answer>}' \\
    \hline
    
    \textbf{Multiple Choice Question Answer Generation} & 
    For the following multiple choice question: \newline
    \texttt{'QUESTION\_TEXT\_HERE'} \newline
    Please write which answer option (A, B, C, or D) is the correct one. Answer in the following format: '\texttt{Correct answer: <answer letter>.}' \\
    \hline
    
    \textbf{Context-Based Multiple Choice Question Answer Generation} & 
    Using the information of the following passage: \newline
    \texttt{'PASSAGE\_TEXT\_HERE'} \newline
    Answer the following multiple-choice question: \newline
    \texttt{'QUESTION\_TEXT\_HERE'} \newline
    Please write which answer option (A, B, C, or D) is the correct one. Answer in the following format: '\texttt{Correct answer: <answer letter>.}' \\
    \hline
    
    \textbf{Baseline Subtopic List Generation} & 
    For the following topic: \newline
    \texttt{'TOPIC\_HERE'} \newline
    Please generate a list of 5 subtopics that could be used to create a comprehensive PhD manuscript about this topic. List them in order and number them in the following format: '\texttt{1) <write subtopic 1 here>} \newline
    \texttt{2) <write subtopic 2 here>} \newline
    \texttt{3) <write subtopic 3 here>} \newline
    \texttt{4) <write subtopic 4 here>} \newline
    \texttt{5) <write subtopic 5 here>}\newline
    \texttt{<end>}' \\
    \hline
    
    \textbf{Baseline Chapter Generation} & 
    For a scientific manuscript with the following title: \newline
    \texttt{'MANUSCRIPT\_TITLE\_HERE'} \newline
    Please generate a comprehensive chapter that covers the following subtopic: \texttt{'SUBTOPIC\_HERE'}. Aim for around 600 words, include facts and numbers, and focus solely on substantial content. Omit any introductory or closing remarks and just output the content that this chapter would have. \\
    \hline
    \end{tabular}
    \caption{LLM Task Prompt Templates}
    \label{tab:prompts}
\end{table*}




\newpage
\subsection{Supplementary Results}


\subsubsection{Positional Bias}
For all three generated MCQ datasets, GPT-4o shows a strong tendency to place the correct answer in the answer options B and C over A and D in around 80\% of the time. This tendency may come from training biases where datasets exhibited a similar distribution in MCQ formats.

When ignored, this positional bias may skew a model's performance during evaluation. To counteract this effect, this project employed the rotation of the correct answer position and evaluated each question four times independently. This ensures a balanced distribution of the correct answer among the four positions and reduces the risk of skewing the evaluation statistics with the positional MCQ generation bias. After the positional bias mitagation strategy is applied, the correct answer is distrubuted evenly, appearing in each option 25\% of the time. During evaluation, the models also show some levels of positional bias, however on a lower scale than during the MCQ generation.

\begin{figure}[h] % 'H' makes it appear exactly at the point in the document where you place it.
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/Baseline_positional_bias_gpt_mcq_generation.png}
        \caption{Baseline}
        \label{subfig:positional_bias_baseline_mcq_gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/Wikipedia_positional_bias_gpt_mcq_generation.png}
        \caption{Wikipedia}
        \label{subfig:positional_bias_wikipedia_mcq_gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/Venice_positional_bias_gpt_mcq_generation.png}
        \caption{Venice}
        \label{subfig:positional_bias_venice_mcq_gen}
    \end{subfigure}
    \caption{Distribution of the Correct Answer Among the Answer Options for the MCQ Dataset Generated with GPT-4o Before Positional Bias Mitigation.}
    \label{fig:positional_bias_gpt_mcq_generation}
\end{figure}

\begin{figure*}[h] % 'H' makes it appear exactly at the point in the document where you place it.
    \centering
    % First row: EPFL dataset
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/GPT_4o_EPFL_by_GPT-4o_No_Context_positional_bias_gpt_mcq_eval.png}
        \caption{EPFL - No Context}
        \label{subfig:positional_bias_mcq_eval_no_context_epfl_gpt}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/GPT_4o_EPFL_by_GPT-4o_With_Context_positional_bias_gpt_mcq_eval.png}
        \caption{EPFL - With Context}
        \label{subfig:positional_bias_mcq_eval_with_context_epfl_gpt}
    \end{subfigure}

    % Second row: Wikipedia dataset
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/GPT_4o_Wikipedia_by_GPT-4o_No_Context_positional_bias_gpt_mcq_eval.png}
        \caption{Wikipedia - No Context}
        \label{subfig:positional_bias_mcq_eval_no_context_wikipedia_gpt}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/GPT_4o_Wikipedia_by_GPT-4o_With_Context_positional_bias_gpt_mcq_eval.png}
        \caption{Wikipedia - With Context}
        \label{subfig:positional_bias_mcq_eval_with_context_wikipedia_gpt}
    \end{subfigure}

    \caption{Distribution of Correct Answer Letter Prediction for EPFL and Wikipedia MCQ Datasets Evaluated on GPT-4o After Positional Bias Mitigation.}
    \label{fig:positional_bias_mcq_eval_combined}
\end{figure*}




\begin{figure*}[h] % 'H' makes it appear exactly at the point in the document where you place it.
    \centering
    % First row: EPFL dataset
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/Llama_70B_EPFL_MCQs_by_GPT-4o_No_Context_positional_bias_gpt_mcq_eval.png}
        \caption{EPFL - No Context}
        \label{subfig:positional_bias_mcq_eval_no_context_llama70b_epfl_books_mcqs}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/Llama_70B_EPFL_MCQs_by_GPT-4o_With_Context_positional_bias_gpt_mcq_eval.png}
        \caption{EPFL - With Context}
        \label{subfig:positional_bias_mcq_eval_with_context_llama70b_epfl_books_mcqs}
    \end{subfigure}

    % Second row: Wikipedia dataset
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/Llama_70B_Wikipedia_by_GPT-4o_No_Context_positional_bias_gpt_mcq_eval.png}
        \caption{Wikipedia - No Context}
        \label{subfig:positional_bias_mcq_eval_no_context_llama70b_wikipedia}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/Llama_70B_Wikipedia_by_GPT-4o_With_Context_positional_bias_gpt_mcq_eval.png}
        \caption{Wikipedia - With Context}
        \label{subfig:positional_bias_mcq_eval_with_context_llama70b_wikipedia}
    \end{subfigure}

    % Third row: Baseline dataset
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/Llama_70B_Baseline_by_Gpt-4o_No_Context_positional_bias_gpt_mcq_eval.png}
        \caption{Baseline - No Context}
        \label{subfig:positional_bias_mcq_eval_no_context_llama70b_baseline}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/pos_bias_eval/Llama_70B_Baseline_by_Gpt-4o_With_Context_positional_bias_gpt_mcq_eval.png}
        \caption{Baseline - With Context}
        \label{subfig:positional_bias_mcq_eval_with_context_llama70b_baseline_gpt}
    \end{subfigure}

    \caption{Distribution of Correct Answer Letter Prediction for EPFL, Wikipedia, and Baseline MCQ Datasets Evaluated on Llama 70B After Positional Bias Mitigation.}
    \label{fig:positional_bias_mcq_eval_llama70b_combined}
\end{figure*}



\newpage
\ \newpage
\subsubsection{Information Potential}
Figure \ref{fig:venn_statistics_4x_eval} shows the models' performance with an emphasis on the positional bias mitigation strategy. As every MCQ is evaluated four times, this allows the analysis of a model's consistency. It is clearly visible that the models become less consistent without context in datasets with higher information potential while this trend is  less pronounced with context.
Figure \ref{fig:llama_70b_cutoffs} shows the performance of Llama 70B along with the information potential across the datasets and cutoff percentiles. 
% Here too, the information potential of the EPFL dataset is significantly higher than that of the other two across all relevant cutoffs.

\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/Llama_70B_Baseline_by_Gpt-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{Llama 70B Baseline\\IP: 0.125}
        \label{fig:epfl_eval_baseline}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/Llama_70B_Wikipedia_by_GPT-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{Llama 70B \textcolor{blue}{Wikipedia}\\IP: 0.136}
        \label{fig:epfl_eval_wikipedia_2}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/Llama_70B_EPFL_MCQs_by_GPT-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{Llama 70B \textcolor{teal}{EPFL}\\IP: 0.229}
        \label{fig:epfl_eval_mcqs}
    \end{subfigure}

    \vspace{0.1cm}

    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/GPT_4o_Wikipedia_by_GPT-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{GPT-4o \textcolor{blue}{Wikipedia}\\IP: 0.110}
        \label{fig:epfl_eval_wikipedia}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/GPT_4o_EPFL_by_GPT-4o_overall_stats_venn_diagram_no_legend.png}
        \caption{GPT-4o \textcolor{teal}{EPFL}\\IP: 0.211}
        \label{fig:epfl_eval_books}
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/results/venn_diagrams/overall_eval_legend.png}
        \caption{Legend}
        \label{fig:4x_eval_legend}
    \end{subfigure}
    
    \caption{Statistics of Model Performances Including whether Model was Correct on all Four Evaluations of Each MCQ
    \\Venn diagrams of the number of correctly answered questions when answered with or without context.
    \\ \textbf{IP}: Information Potential computed with Equation \ref{eq:knowledge_gain_formula}}
    \label{fig:venn_statistics_4x_eval}
\end{figure*}


\begin{figure*}[h]
    % \hspace*{-0.15\textwidth} % Shift the entire figure to the left
    % \resizebox{1.3\textwidth}{!}{ % Scale the figure
        % Subfigure 1
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Images/results/cutoffs/Llama_70B_Baseline.png}
            \caption{Baseline}
            \label{fig:cutoffs_llama_70b_baseline}
        \end{subfigure}
        % \hspace{0.01\textwidth}
        % Subfigure 3
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Images/results/cutoffs/Llama_70B_Wikipedia.png}
            \caption{Wikipedia}
            \label{fig:cutoffs_llama_70b_wikipedia}
        \end{subfigure}
        % \hspace{0.01\textwidth}
        % Subfigure 2
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Images/results/cutoffs/Llama_70B_EPFL.png}
            \caption{EPFL}
            \label{fig:cutoffs_llama_70b_epfl}
        \end{subfigure}
        \hspace{0.147\textwidth}
        \begin{subfigure}{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Images/results/cutoffs/cutoffs_llama_70b_legend.png}
            \caption{Legend}
            \label{fig:cutoffs_llama_70b_legend}
        \end{subfigure}
    % }
    \caption{Llama 3 70B Performances Across Different Cutoff Percentiles with Joint Cosine and Jaccard-ROUGE-l Metric Thresholding and Number of MCQs for the Corresponding Percentiles.\\
    \textbf{NC 4x:} Evaluation with no context correct for all 4 bias mitigation evaluations of each question.\\
    \textbf{WC 4x:} Evaluation with context correct for all 4 bias mitigation evaluations of each question.\\
    \textbf{Information Potential}: The value computed with Equation \ref{eq:knowledge_gain_formula}
    }
    \label{fig:llama_70b_cutoffs}
\end{figure*}


\newpage

\subsubsection{Qualitative Analysis of EPFL MCQ Dataset}


% \textcolor{red}{add small introduction referencing the possible cases of MCQs once that section is written.}

\begin{table*}[h]
    \centering
    \setlength{\tabcolsep}{3pt}  % Reduced padding
    \renewcommand{\arraystretch}{1.3}  % Slightly reduced line spacing
    \footnotesize
    \begin{tabular}{|p{0.45\textwidth}|p{0.4\textwidth}|p{0.1\textwidth}|}
        \hline
        \textbf{MCQ} (correct answer in italics) & \textbf{Relevant Context Passages} & \textbf{Category}\\ 
        \hline
        
        \textbf{What is the only possible scheme for Bernstein wave excitation in the TCV tokamak due to its plasma equilibria?} \newline
          \textit{A) O-SX-B scheme.} \newline
          B) FX-B scheme. \newline
          C) EB-B scheme. \newline
          D) SX-O-B scheme.
          &
          \scriptsize [...] As a consequence, the extremely steep density gradients necessary for the FX-B mode conversion cannot be obtained in TCV plasma equilibria and the only possible Bernstein waves excitation scheme in TCV is the O-SX-B double mode conversion. [...]
          & Technical Terminology \\
        \hline

        \textbf{What technological challenge is associated with reducing power consumption in CMOS circuits?} \newline
          \textit{A) Maintaining acceptable dynamic range in the face of digital noise} \newline
          B) Reducing the intrinsic capacitance per unit area. \newline
          C) Ensuring constant voltage swing at all frequencies. \newline
          D) Achieving higher gain at lower supply voltages.
          &
          \scriptsize [...] In downscaled processes with lower supply voltages, the coupling and noise through the substrate is higher, partially because of the limitations of the substrate and well bias [40,41]. Therefore, sometimes noise that is produced by the chip due to the digital blocks may be orders of magnitude above the thermal noise, so to achieve the required dynamic range we require a proportional increase in power. [...]
          & Technical Terminology \\
        \hline
        
        \textbf{In learning molecule representations directly in the sparse code domain, what is the main constraint imposed on sparse codes?} \newline
          \textit{A) They must be linear combinations of deformed molecules.} \newline
          B) They must be nonlinear mixtures of entire signal sets. \newline
          C) They must consist of deactivated elements. \newline
          D) They must strictly adhere to original signal morphology.
          &
          \scriptsize [...] We constrain sparse codes to be linear combinations of a few, possibly deformed, molecules and we design an algorithm that can learn the structure from the codes without transforming them back into the signal domain. [...]
          & Technical Terminology \\
        \hline

        \textbf{Which component significantly contributes to total variance in back-to-back scan-rescan scenarios?} \newline
          A) 2-week-gap variance. \newline
          \textit{B) Scan-rescan variability} \newline
          C) Session-dependent offsets. \newline
          D) Repositioning effects.
          &
          \scriptsize [...] The scan-rescan differences in back-to-back scanning scenario significantly contributed to the total variance and represented a significant proportion of between-subject variance for all of the investigated structures. [...] Both repositioning (R2) and 2-week-gap between a rescan (R3) did not significantly contribute to the total variability compared to back-to-back scans and between-subject variability. [...]
          & Complex Relationship \\
        \hline
        
        \textbf{Which factor most critically affects the measurement noise in an ex-situ detection setup?} \newline
          \textit{A) The frequency at which measurements are taken.} \newline
          B) The remanence of the magnetic core. \newline
          C) The sensitivity of the lock-in amplifier. \newline
          D) The microbead placement precision.
          &
          \scriptsize [...] However, this increases the measurement noise, as the measurement is carried out in the 1/f noise frequency range. [...]
          & Complex Relationship \\
        \hline
        
        \textbf{What condition allows the bond in the RMIB model to be unbreakable under compressive deformation?} \newline
          \textit{A) High hydrostatic compressive stress.} \newline
          B) High thermal conductivity. \newline
          C) Low volumetric strain. \newline
          D) Low thermal resistance.
          &
          \scriptsize [...] It is known that the hydrostatic compressive strength is infinite for most materials, which means the bond in RMIB model for these cases cannot be broken under compressive deformation. [...]
          & Complex Relationship \\
        \hline

        \textbf{What is the approach used by RouLette to manage materialization overhead in symmetric joins?} \newline
          \textit{A) Symmetric join pruning of tuples forming outputs.} \newline
          B) Incremental materialization of queried tuples. \newline
          C) Partial materialization of all relations. \newline
          D) Deferred materialization until query execution.
          &
          \scriptsize [...] Symmetric joins require that all relations be materialized and hence incur materialization overhead. To reduce the overhead, RouLette materializes only tuples that can form output tuples for their query-set. We call this symmetric join pruning [...]
          & Novel Findings \\
        \hline
        
        \textbf{What primary limitation affects the clinical success of MPCs in bone healing?} \newline
          \textit{A) Limited number of available endogenous MPCs.} \newline
          B) Extensive proliferation in vitro. \newline
          C) Over-differentiation into non-mesenchymal lineages. \newline
          D) High heterogeneity in cell populations.
          &
          \scriptsize [...] its clinical outcome was rather disappointing 4 . One of reasons for this seems to be the limiting number of available endogenous mesenchymal progenitor cells (MPCs) that can give rise to bone cells. [...]. Hence, there is a clear clinical need for implants that augment the homing/recruitment of endogenous MPCs to fracture sites [...]
          & Novel Findings \\
          % \small [...] Although promising results were obtained in preclinical studies, with quantities needed that exceed the physiological concentrations by magnitudes, its clinical outcome was rather disappointing 4 . One of reasons for this seems to be the limiting number of available endogenous mesenchymal progenitor cells (MPCs) that can give rise to bone cells. The availability of endogenous MPCs is especially critical for elderly patients, which show with aging a decrease in MPC numbers that results in reduced bone formation and osteogenesis [...]. Hence, there is a clear clinical need for implants that augment the homing/recruitment of endogenous MPCs to fracture sites [...]
          % & Novel Findings \\
        \hline
        
        \textbf{What leads to the gradual increase in average THC concentration over time during oscillations?} \newline
          \textit{A) Accumulation of carbonates on ceria sites.} \newline
          B) Thermal degradation of the catalyst. \newline
          C) Continuous ceria site activation. \newline
          D) Increasing gas hour space velocity (GHSV).
          &
          \scriptsize [...] The higher average THC concentrations levels with time was caused by the gradual accumulation of carbonates on ceria sites during the periodic oscillations. [...]
          % \small [...] The higher average THC concentrations levels with time was caused by the gradual accumulation of carbonates on ceria sites during the periodic oscillations. During rich phases, carbonates slowly formed and deactivated ceria sites. During lean phases, the formed carbonates were partially regenerated by the excess oxygen. Due to the short duration of the lean phases, not all deactivated ceria sites were regenerated. A part of the formed carbonates accumulated over the cycles. [...]
          & Novel Findings \\
        \hline

    \end{tabular}
    % }
    \caption{Selection of EPFL Dataset MCQs and their Relevant Context Passages Categorized by Question Type where GPT-4o Required Context to Correctly Answer Consistently}
    \label{tab:mcq_analysis_annex}
\end{table*}






\end{document}