@article{ODHIAMBOOMUYA2021114765,
    title = {Feature Selection for Classification using Principal Component Analysis and Information Gain},
    journal = {Expert Systems with Applications},
    volume = {174},
    pages = {114765},
    year = {2021},
    issn = {0957-4174},
    doi = {https://doi.org/10.1016/j.eswa.2021.114765},
    url = {https://www.sciencedirect.com/science/article/pii/S0957417421002062},
    author = {Erick {Odhiambo Omuya} and George {Onyango Okeyo} and Michael {Waema Kimwele}},
    keywords = {Feature selection, Classification, Dimensionality reduction, Filter model, Information gain, Principal component analysis},
    abstract = {Feature Selection and classification have previously been widely applied in various areas like business, medical and media fields. High dimensionality in datasets is one of the main challenges that has been experienced in classifying data, data mining and sentiment analysis. Irrelevant and redundant attributes have also had a negative impact on the complexity and operation of algorithms for classifying data. Consequently, the algorithms record poor results or performance. Some existing work use all attributes for classification, some of which are insignificant for the task, thereby leading to poor performance. This paper therefore develops a hybrid filter model for feature selection based on principal component analysis and information gain. The hybrid model is then applied to support classification using machine learning techniques e.g. the Naïve Bayes technique. Experimental results demonstrate that the hybrid filter model reduces data dimensions, selects appropriate feature sets, and reduces training time, hence providing better classification performance as measured by accuracy, precision and recall..}
}

@article{cover1989kolmogorov,
  author    = {Thomas M. Cover and Peter Gács and Robert M. Gray},
  title     = {Kolmogorov's Contributions to Information Theory and Algorithmic Complexity},
  journal   = {Annals of Probability},
  year      = {1989},
  volume    = {17},
  number    = {3},
  pages     = {840--865},
  month     = {July},
  doi       = {10.1214/aop/1176991250}
}

@inproceedings{damiano2021visual,
  author    = {Claudia Damiano and Sander Van de Cruys and Yannick Boddez and Magdalena Król and Lore Goetschalckx and Johan Wagemans},
  title     = {Visual Affects: Linking Curiosity, Aha-Erlebnis, and Memory through Information Gain},
  booktitle = {Vision Sciences Society Annual Meeting Abstracts},
  year      = {2021},
  month     = {September}
}

@article{farquhar_detecting_hallucinations_llms_semantic_entropy,
	title = {Detecting hallucinations in large language models using semantic entropy},
	volume = {630},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/s41586-024-07421-0},
	doi = {10.1038/s41586-024-07421-0},
	abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
	number = {8017},
	journal = {Nature},
	author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
	month = jun,
	year = {2024},
	pages = {625--630},
}

@book{fedorov1972theory,
  author    = {V. V. Fedorov and W. J. Studden and E. M. Klimko},
  title     = {Theory of Optimal Experiments},
  year      = {1972},
  publisher = {Academic Press},
  address   = {New York, NY}
}

@misc{golchin2024datacontaminationquiztool,
      title={Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models}, 
      author={Shahriar Golchin and Mihai Surdeanu},
      year={2024},
      eprint={2311.06233},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.06233}, 
}

@misc{hartmann2023sokmemorizationgeneralpurposelarge,
      title={SoK: Memorization in General-Purpose Large Language Models}, 
      author={Valentin Hartmann and Anshuman Suri and Vincent Bindschaedler and David Evans and Shruti Tople and Robert West},
      year={2023},
      eprint={2310.18362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.18362}, 
}

@article{language_novelty_1,
author = {McCoy, R. and Smolensky, Paul and Linzen, Tal and Gao, Jianfeng and Asli, Celikyilmaz},
year = {2023},
month = {06},
pages = {652-670},
title = {How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN},
volume = {11},
journal = {Transactions of the Association for Computational Linguistics},
doi = {10.1162/tacl_a_00567}
}

@misc{semantic_linguistic_novelty_focus_paper,
      title={Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications}, 
      author={Ethan Lin and Zhiyuan Peng and Yi Fang},
      year={2024},
      eprint={2409.16605},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.16605}, 
}

@misc{shi2024detectingpretrainingdatalarge,
      title={Detecting Pretraining Data from Large Language Models}, 
      author={Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
      year={2024},
      eprint={2310.16789},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.16789}, 
}

@misc{yax2024assessingcontaminationlargelanguage,
      title={Assessing Contamination in Large Language Models: Introducing the LogProber method}, 
      author={Nicolas Yax and Pierre-Yves Oudeyer and Stefano Palminteri},
      year={2024},
      eprint={2408.14352},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.14352}, 
}

