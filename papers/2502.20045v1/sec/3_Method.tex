\section{Methodology}
\label{sec:Method}
Our framework can generate VDM brushes compatible with mainstream modeling software. As shown in \Cref{Fig: Pipeline}, we begin by constructing a dense mesh $\mathcal{M}_0$ from an initial VDM $T_0$.
% and enable control over the mesh volume and the deformable region with a shape map \( S \) and a region mask \( R \) respectively.
% and enable control over the mesh volume with an optional user specified initial VDM shape map \( S \).
% % 对spike pattern的描述
% This effectively adjust the Laplacian term in ~\Cref{ls}, pivoting/steering the gradient direction of the mesh deformation for large 3D components.
% , while ensuring the generated mesh can be directly baked as a brush. 
% We reparameterize the mesh vertices through an implicit formulation with a Laplace-Beltrami operator~\cite{Largesteps:SIGGRAPH:2021} to ensure high-quality mesh deformation.
We then apply score distillation with a pretrained T2I diffusion model to guide mesh deformation through a Laplace-Beltrami operator $L$~\cite{Largesteps:SIGGRAPH:2021} toward the input text prompt $y$.
The deformable region of the mesh can be controlled by an optional user-specified region mask $R$.
% However, directly using the original score-distillation sampling (SDS) can result in semantic coupling when generating 3D components, leading to unintended extra parts connected to the text-described object. 
To produce a high-quality sub-object level structure that only represents the intended part described by the text, we apply CFG-weighted blending to the tokens in the prompt, effectively handling the issue of semantic coupling in SDS.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Brush Initialization}
% % surface brush 和 compoent brush 这两个词要明确一下叫法，并且全文统一
% We start the generation process by initializing a $512\times512$ resolution 3-channel VDM image with all values setting to zeros. We convert this image to a dense planar mesh by creating two triangles from every $2\times2$ pixels, i.e., without loss of generality, one triangle on the top-left and the other on the bottom-right.
% By selecting from either surface brush or component brush, the user may opt to provide guidance for the warm-up of the coming SDS-based mesh deformation.

% \noindent
% \textbf{Surface brush.}
% % warp-up stage, full-optimize stage 还是 stage1, stage2 ? 这个叫法要定一下
% For the surface brush, the user may draw a region mask corresponding to the VDM image. In the warp-up stage of the SDS-based process, mesh deformation will take place only in the user-drawn region, so that the general shape of the surface brush will match the user guidance.

% \noindent
% \textbf{Component brush.}
% For the component brush, the user's drawing is applied to the mesh as a displacement map. This deformation creates an initial volume, shaping the mesh towards the target 3D component.

% To enable users to achieve their desired brush effects, we provide two control methods that allow for additional guidance in initializing the mesh.

% For better generation of surface style, the user may draw a region mask corresponding to the VDM image. 
% Mesh deformation will take place only in the user-drawn region, so that the general shape of the surface brush will match the user guidance, as shown in~\Cref{Fig: Effect of region mask}.
% Note that this displacement map also serves as a region mask in the following text-guided mesh deformation process, limiting the VDM brush to affect only the 3D component.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Brush Initialization}
% 初始化部分只说 shape map；在mesh deformation部分再提region mask
% 预设了一些模板,也提供了一个简单的交互方式控制给用户控制mesh的生长方向，背后的原理：更好的引导梯度下降的方向

We provide three methods to initialize a base mesh for brush generation via a zero-valued VDM, a spike-pattern VDM, or a user-specified VDM.
A VDM is represented as a $512\times512$ three-channel image, in which each channel stores the displacement in the X, Y, or Z direction respectively.
We first construct a planar grid mesh by creating two triangles for every $2\times2$ pixels and then apply the displacement stored in the VDM to mesh vertices.
The values in these three initial VDMs range from 0 to 1, in which 0 represents no displacement, and 1 corresponds to half of the mesh's edge length in the positive axis direction.
Since users can apply sculpting brushes symmetrically, our initial VDM does not need to store any negative values.

Our three methods for brush initialization facilitate the generation of diverse sculpting brush styles.
The zero-valued VDM results in a planar mesh, which is our default setup when no control is provided.
The spike-pattern VDM is suitable for generating protruding geometric structures, as it can effectively adjust the Laplacian term in~\Cref{ls} to steer the gradient direction for mesh deformation.
For better control of the brush's volume and direction, we also provide an interface for users to create custom VDMs, so the user-specified brush initialization can effectively guide mesh deformation toward the target structure.

% We provide three methods to initialize a base mesh for brush generation via a zero-valued VDM, a spike-pattern VDM, or a user-specified VDM.
% A VDM is represented as a $512\times512$ three-channel image, in which the values in each channel store the displacement in XYZ directions.
% We convert the VDM to a dense mesh by creating two triangles from every $2\times2$ pixels, i.e., without loss of generality, one triangle on the top-left and the other on the bottom-right.

% The first approach uses a zero-valued VDM to generate a dense planar mesh. The second approach offers a spike-patterned VDM, which effectively adjusts the Laplacian term in ~\Cref{ls}, steering the gradient direction of the mesh deformation for generating larger 3D components. For better control of the brush's volume and direction, we offer a simple interactive interface for creating custom VDMs, allowing users to create an initial volume and shaping the mesh toward the target sub-object level structure.
% To enable control over the mesh volume with an optional user specified initial VDM shape map \( S \).
% 对spike pattern的描述
% This effectively adjust the Laplacian term in ~\Cref{ls}, pivoting/steering the gradient direction of the mesh deformation for large 3D components.
% For better control of the brush's volume and direction,
% we allow users to provide control through a customize shape map that serve as a template VDM. This deformation creates an initial volume, shaping the mesh toward the target sub-object level structure. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Brush Generation via Mesh Deformation}
\label{sec: mesh_deform}
%直接用SDS优化顶点位置这种方法们虽然能够获得较大的顶点移动，但是需要很强的几何平滑正则项来维持mesh的质量，这导致了优化过程极其不稳定，难以收敛，最后产生过噪或是过平滑的结果
Given the initialized based mesh, our method aims to learn a mesh deformation to the target brush shape. The vertex positions $\hat{v}$ after mesh deformation can be expressed by
\begin{equation}
\label{eq1:vertex}
    \hat{v} = \argmin_{v} \mathcal{L}_{\text{WS}}(\mathcal{D}_c(v), y),
\end{equation}
where $c$ represents the camera setup in a differentiable renderer $\mathcal{D}$~\cite{Laine2020diffrast}. The loss function $\mathcal{L}_{\text{WS}}$ receives the rendered normal image $\mathcal{D}_c(v)$ and text input $y$ to evaluate the semantic guidance, which is detailed in~\Cref{sec: tesds}.
In mesh deformation strategies, directly applying displacement to each vertex often results in unintended self-intersections of mesh faces caused by noisy gradients from pixel-level losses~\cite{single12-text2mesh}. To address it, several works~\cite{SIGGRAPH:TextDeformer:2023,HeadEvolver:Arxiv:2024} adopt the strategy by Aigerman et al.~\cite{SIGGRAPH:NJF:2022}, parameterizing deformation by Jacobian fields that capture the scaling and rotation of each mesh face. 
Although this method effectively smooths vertex displacements, the local deformation represented in Jacobians accumulates, leading to global drifting for open-boundary meshes, making it challenging to bake the mesh as a brush.
% However, as Wang et el.~\cite{} pointed out, such smoothness may result in a loss of fine-grain
% details for local shape features. Although they increased the deformation expressiveness by introducing a learnable per-face factor, the strong shape prior constraints and manifold requirements inherent in Jacobian fields still limit their flexibility, making them less suitable to modify large structures of the mesh.

%mention why lstep is good for this step without strong shape prior

Based on these observations, we reparameterize the vertex optimization in~\Cref{eq1:vertex} through an implicit formulation with a
Laplace-Beltrami operator $L$, similar to the approaches presented by Nicolet et al.~\cite{Largesteps:SIGGRAPH:2021}:
% and Haetinger et al.~\cite{MeshTransfer:SIGGRAPH:2024}:
\begin{equation}
\label{ls}
    v^{*} = (I+\lambda L)v,
\end{equation}
where $I$ is the identity matrix, $ \lambda$ is a hyperparameter to control the extent of gradient diffusion from a given vertex to its neighboring vertices. When $\lambda=0$, this representation degrades to direct vertex displacements. As $\lambda$ increases, the mesh deforms toward more global structural changes. In our experiments, we set $\lambda=15$ to balance the global structure and fine details during mesh deformation (\Cref{Fig: lambda}). Additionally, we provide a region mask to restrict mesh deformation to the user-defined region during optimization. By adjusting the activation ratio of the region mask, the final brush effect can effectively match the user's guidance. For instance, our experiment activated the region mask for the first half of total iterations as a warm-up stage to effectively control the shape of the surface detail (\Cref{Fig: Effect of region mask}). 
% and allowing the overall effect of the brush to align with the user's guidance (\Cref{Fig: Effect of region mask}). 
This differentiable parameterization effectively alters the gradient propagation at each optimization step as:
\begin{equation}
v^{*} \leftarrow v^{*} -\eta (I -\lambda L)^{-1}\frac{\partial{\mathcal{L}_{\text{WS}}}}{\partial{v}^{*}}
\end{equation} 
%\begin{equation}
%    v^{*} \leftarrow v-\eta (\frac{\partial{\mathcal{L}_{\text{WS}}}}{\partial{v}}+\lambda Lv),
%\end{equation}
where $\eta$ is the learning rate.
% $\tau$ represents the current iteration step, and $\mu$ is the ratio of activated region mask iterations within the total iterations $\kappa$. In our experiments, $\mu$ is set to 1 for generating 3D components and to $\frac{1}{2}$ for generating surface styles. 
The advantage of this Laplacian energy-aware mesh deformation is that it enhances the robustness of optimization for non-convex objective functions. The resulting mesh preserves the original structure while incorporating rich local deformations, making it well-suited for baking as a brush. 
\begin{figure}[tbh]
\centering
\includegraphics[width=0.49\textwidth]{sec/Figures/ablation_lambda2.pdf}
\caption{
    \textbf{Effects of $\lambda$.} As $\lambda$ decreases, the mesh deformation shifts from global structural to locality, resulting in finer details, until it becomes a noisy surface at $\lambda$ = 0. 
}
\label{Fig: lambda}
\end{figure}
% Furthermore, this parameterization is not constrained by shape priors or even mesh representations, making it highly versatile. It has been successfully validated to support texture reconstruction~\cite{Largesteps:SIGGRAPH:2021}, which aligns well with our need for VDM generation. Based on the experimental conclusions of Haetinger et al.~\cite{MeshTransfer:SIGGRAPH:2024}, $\lambda$ was set to 20, enabling mesh deformation for both large structures and local details. 


\subsection{CFG-Weighted Score Distillation Sampling}
\label{sec: tesds}
% 先介绍原始的sds，公式（类似preliminary）（参考ThemeStation）
% sds在生成3Dcomponent出现的问题（语义耦合），指出现象，分析一下原因
Current text-to-3D generation methods like DreamFusion~\cite{DreamFusion:ICLR:2022} often optimize a 3D representation parameterized by \( \theta \) so that rendered images \( \mathbf{x} = g(\theta) \) resemble 2D samples produced by a pre-trained T2I diffusion model for a given text prompt \( y \). \( g \) functions as a differentiable renderer. The T2I diffusion model \( \phi \) predicts the sampled noise \( \epsilon_{\phi}(\mathbf{x}_t; y, t) \) of a rendered image \( \mathbf{x}_t \) at a noise level \( t \) for the text input \( y \). To move all rendered images with higher density regions under the text-conditioned diffusion prior, SDS loss estimates the gradient for updating \( \theta \) as:
\begin{equation}
    \nabla_{\theta} \mathcal{L}_{\text{SDS}}(\phi, \mathbf{x}) = \mathbb{E}_{t, \epsilon,c} \left[ \omega(t) \left( \epsilon_{\phi}(\mathbf{x}_t; y, t) - \epsilon \right) \frac{\partial \mathbf{x}}{\partial \theta} \right],
\end{equation}
where $\omega(t)$ is a time-dependent weighting function.

However, directly supervising sub-object structures with SDS, can lead to semantic coupling, which results in the generation of extra semantically related parts. For example, when generating a tortoiseshell, this semantic coupling will cause the generation of the tortoise's tail and head (\Cref{Fig: Effect of CFG-weighted SDS}). We believe that the issue of semantic coupling in SDS stems from the training data of the stable diffusion model, where images often depict partial components along with the complete object. This causes the target distribution conditioned by the text description of sub-object structures containing semantic information related to the full object.
% SDS gradients containing guidance information associated with semantics beyond those described by the input text. 

A straightforward approach is using negative prompts proposed by classifier score distillation (CSD)~\cite{CSD:Arxiv:2023} to mitigate coupled semantics. CSD demonstrates that compared to variational score distillation (VSD)~\cite{VSD}, which adaptively learns negative classifier scores, CSD employs predefined negative prompts, resulting in a more precise optimization process:
%\begin{equation}
%\footnotesize
%    \delta^{cls}_x = \omega_{1} \cdot \epsilon_{\phi}(\mathbf{x}_t; y, t) + (\omega_{2} - \omega_{1}) \cdot \epsilon_{\phi}(\mathbf{x}_t; t) - \omega_{2} \cdot \epsilon_{\phi}(\mathbf{x}_t; y_{neg}, t),
%\end{equation}
\begin{align} % the 
\label{csd}
\nabla_{\theta} \mathcal{L}_{\text{CSD}}(\phi, \mathbf{x}) &= \mathbb{E}_{t, \epsilon,c} [(\omega_{1} \cdot \epsilon_{\phi}(\mathbf{x}_t; y, t) \nonumber \\
&- \omega_{2} \cdot \epsilon_{\phi}(\mathbf{x}_t; y_{neg}, t))\frac{\partial \mathbf{x}}{\partial \theta}],
\end{align}
where $\omega_{1}$ and $\omega_{2}$ denote different weights for positive and negative prompts. We found that the negative prompt's semantic distribution does not align with the associated semantics in the target distribution conditioned by the positive prompt of sub-object structures.
% We found that the negative semantic distribution conditioned by the negative prompt does not align with the associated semantic distribution of full object in the target distribution conditioned by the positive prompt describing sub-object structure.
% During our experiments, we observed that the negative prompt added a term that lies outside the distribution domain of the positive prompt. 
This resulted in noisier gradients, making CSD less effective at decoupling semantics. Furthermore, as the weight of the negative prompt increased, the optimization became more unstable and challenging to converge.

Unlike CSD, we apply CFG-weighted blending to the tokens in the original prompt, which does not require additional inference to construct a negative distribution. This results in semantically focused text embedding, directing toward a more precise target distribution.
% It enhances the semantics of local components within the original prompt to achieve a more precise distribution. 
Specifically, our loss function is defined as:
\begin{equation}
\small
    \nabla_{\theta} \mathcal{L}_{\text{WS}}(\phi, \mathbf{x}) = \mathbb{E}_{t, \epsilon,c} \left[ \omega(t) \left( \epsilon_{\phi}(\mathbf{x}_t; y^{*}, t) - \epsilon \right) \frac{\partial \mathbf{x}}{\partial \theta} \right],
\end{equation}
%\begin{equation}
%\small
%    \mathcal{L}_{\text{WS}}(\phi, \mathbf{x}) = \mathbb{E}_{t, \epsilon} \left[ \omega(t) \left( \epsilon_{\phi}(\mathbf{x}_t; y_{\text{CFG}}, t) - \epsilon \right)  \right],
%\end{equation}
where $y^{*}$ is a text embedding computed by Compel~\cite{Compel:github:2023}. 
Specifically, we assign each word in the prompt a CFG weight $s$ and compute the weighted embedding $e_w$ for each word by blending the original text embedding $e$ with the empty text embedding $e_{\phi}$ as follows: $e_w = e_{\phi} + s \cdot (e - e_{\phi})$. By concatenating the weighted embeddings of each word in sequence, we obtain the final semantically focused text embedding $y^{*}$.
% Specifically, we assign each word in the prompt a weight $s$ and compute the weighted embedding $e_w$ for each word by blending it with the empty text embedding $e_{\phi}$ as follows: $e_w = e_{\phi} + s\cdot 
% (e - e_{\phi}) $. By concatenating the weighted embeddings of each word in sequence, we obtain the final semantically focused text embedding $y^{*}$. 
In our experiments, we found that assigning a weight of 1.21 to words that require enhanced semantics can achieve stable optimizations and effectively alleviate the issue of semantic coupling.
Notably, the CFG weights used for text embedding computation are separate from the CFG guidance scale applied during the computation of the SDS loss. Our experiment uses a CFG guidance scale of 100.

% 具体而言，我们使用compel来进行CFG-weighted blending计算。我们赋予prompt中每个单词一个权重$w$，我们每个单词的加权embedding$e_w$通过与空文本的embedding$e_{\phi}$加权得到：$e_w = e_{\phi} + (e - e_{\phi}) \cdot w$. 我们把各个单词的加权embedding按顺序concat起来得到最后的semantically focused text embedding $y^{*}$。在我们的实验中，我们对需要加强语义的单词统一赋予1.21的权重值。

% 直接能想得到的（最直接的办法）或许能够解决这个问题的是CSD，NFSD这类通过negative prompt改进SDS的方法，能不能用他来解决语义耦合，给出CSD的公式,说明一下VSD和CSD的关系（引用CSD论文里的话），指出CSD相比VSD应该是一种更精确的解决语义耦合的方法
% 实验表明CSD解决不了，还会导致优化过程更不稳定，简单分析一下原因

% 引出我们的方法，与CSD的做法不同，我们采取了另外一种。。。。。。






%-------------------------------------------------------------------------
% \subsection{Preliminary: Score-Distillation Sampling}

%We show an overview of our method in Fig. 3. The inputs to our system are a mesh M and a text description y of the desired local edit. Our system produces a local texture on the mesh M that adheres to the text prompt y. To supervise our optimization, we use score distillation with a pretrained text-to-image diffusion model. However, local editing requires higher detail than standard generation due to the small size and granularity of the desired edits. In order to further improve the detail of our localization and texture, we introduce Cascaded Score Distillation (CSD), a technique that distills scores at multiple resolutions of the 2D cascaded model. This approach enables leveraging all stages of a cascaded model and provides control over both the detail and global understanding of the supervision.


% Our framework is designed to follow the real-world workflow of 3D modeling by introducing a concept art design step before the 3D modeling process. As illustrated in Fig. 2, we first customize a pre-trained text-to-image (T2I) diffusion model to produce a series of concept images that share a consistent theme as the input exemplars, mimicking the concept art designing process in practice (Sec. 3.1). We then utilize an optimization-based method to lift each concept image to a final 3D model, following the practical modeling workflow of pushing a base primitive into a well-crafted 3D model (Sec. 3.2). To this end, we present novel dual score distillation (DSD) that leverages the priors of both the concept images and the exemplars in the optimization process (Sec. 3.3).