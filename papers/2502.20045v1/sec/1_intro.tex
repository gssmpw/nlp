\section{Introduction}
\label{sec:intro}

Sculpting brushes are essential tools in 3D asset creation, as artists often require a variety of brushes to create surface details and geometric structures. In modeling software, 3D sculpting brushes are typically defined as vector displacement maps (VDMs). A VDM is a 2D image where each pixel stores a 3D displacement vector. Through these vectors, VDM brushes can create complex surface details, such as cracks and wood grain, or generate geometric structures like ears and horns. This allows artists to apply the same geometric pattern iteratively while sculpting.

Despite significant advances in text-to-image (T2I)~\cite{sd,dalle} and text-to-3D generation~\cite{Make3D:2023:ICCV,single1-instant3d,single2-wonder3d,DreamGaussian:Arxiv:2023,DreamFusion:ICLR:2022}, existing methods are unsuitable for creating VDM brushes. We summarize the challenges as follows: 1)~Since VDMs are not natural images (\Cref{Fig: vector or height}), it is difficult for existing T2I models to generate them directly. 2)~From a 3D perspective, a VDM represents mesh deformation through per-vertex displacement vectors from a dense planar mesh. Mapping any generated mesh to a dense planar mesh to create a VDM is non-trivial. 3)~Sculpting brushes often involve sub-object structures, whereas most 3D generation methods can only generate full objects. Enabling users to accurately control the generation of sculpting brush through text descriptions in a semantically focused manner remains challenging.

To address the challenges of brush generation, we propose Text2VDM, a novel optimization-based framework that generates diverse and controllable VDM brushes from text input. Our approach does not generate VDMs directly from a T2I model. Instead, we address VDM brush generation from a 3D perspective by applying score distillation with a pre-trained T2I model to guide mesh deformation.
% we formulate VDM brush generation as the geometry deformation from a dense planar mesh.
% to establish a precise mapping between the dense planar mesh and the generated mesh.  
% We design a mesh deformation framework to establish a precise mapping between the dense planar mesh and the generated mesh. 
Our framework supports three ways to initialize a base mesh through a zero-valued, spike-pattern, or user-specified VDM for custom shape control.
% a $512\times512$-resolution 3-channel VDM representing a dense planar mesh
% , where the three values of each pixel represent the three-axis displacement of the corresponding vertex.
% After constructing a dense planar mesh from the VDM, we provide a shape control method that allows users to use either the default spike-pattern VDM or a user-specified VDM to control the initial volume of the mesh.
We reparameterize the mesh vertices through an implicit formulation based on the Laplace-Beltrami operator~\cite{Largesteps:SIGGRAPH:2021} to achieve high-quality optimization of mesh deformations.
We also provide optional region control using a mask of activated mesh deformation, helping users obtain the intended brush effects. We then rasterize normal maps of the mesh using a differentiable renderer for brush optimization.

We observed that the standard score distillation sampling (SDS)~\cite{DreamFusion:ICLR:2022} can lead to semantic coupling 
% and fails to provide precise guidance 
when supervising the generation of sub-object level structures because of associated semantics noisy gradients of full object. For example, a generated tortoiseshell should not be a full tortoise with a head and a tail. A straightforward solution is to use negative prompts~\cite{CSD:Arxiv:2023, NFSD:Arxiv:2023} to exclude undesired semantics, but our experiments show that this approach is ineffective in decoupling semantics and leads to an unstable and more time-consuming optimization process. Instead, we propose to enhance the semantics of part-related words by applying classifier-free guidance (CFG) weighted blending to the tokens in the prompt.  This results in semantically focused text embedding, directing toward a more precise target distribution while reducing noisy gradients during optimization. 
% As a result, the generation process of our approach is stable and semantically focused. 

% To ensure the final brush effect aligns with user expectations, we provide two control methods: shape control and region control. Shape control uses a user-specified VDM to set the desired brush volume and direction, while region control employs a mask to define specific areas of mesh deformation.
% To better align the brush with user application requirements, we provide two control methods for the brush generation process: a shape map and a region mask. The shape map specifies the desired volume and direction of the brush, while the region mask enables users to define specific areas to be generated.

\begin{figure}[tbh]
\centering
\includegraphics[width=0.49\textwidth]{sec/Figures/vector_height.pdf}
\caption{
\textbf{Difference between vector and standard displacement.} The VDM enables full 3D vector displacement, while the height map only allows unidirectional standard displacement.}
\label{Fig: vector or height}
\end{figure}

Our experiments demonstrate
that Text2VDM produces high-quality and diverse VDM brushes that can be directly integrated into mainstream modeling software, such as Blender~\cite{Blender:Community:1999} and ZBrush~\cite{ZBrsuh:Maxon:1999}. 
Compared to existing methods that directly generate full 3D models, our approach 
% of generating VDM brushes 
addresses a different use case where brush-based user sculpting is desirable. We enable artists to interactively use a variety of brushes to sculpt diverse and expressive models from a plain shape.

This paper makes the following contributions:
\begin{itemize}
    \item We first introduce the task of text-to-VDM brush generation, which is challenging to tackle directly using current text-to-image and text-to-3D methods.

    \item We propose Text2VDM, a novel framework for text-to-VDM brush generation that is readily compatible with artists' workflow of 3D asset creation.
    
    \item  We introduce CFG-weighted blending to SDS for modeling a more precise target distribution, mitigating semantic coupling in sub-object structure generation.

% We introduce CFG-weighted blending in SDS, to model a more precise target distribution, in resolving the issue of semantic coupling in sub-object structure generation.
% We introduce CFG-weighted blending of tokens in the text prompt to SDS, which can effectively model a more precise target distribution, which resolve the issue of semantic coupling in sub-object structure generation.
% We propose to model a more precise target distribution in SDS via CFG-weighted blending to effectively resolve the issue of sematic coupling in sub-object structure generation.
% We leverage CFG-weighted blending for the tokens in the prompt, which can effectively resolve the issue of semantic coupling in SDS in sub-object structure generation.
\end{itemize}

%-------------------------------------------------------------------------

\begin{figure*}[!htb]
\centering
\includegraphics[width=1\textwidth]{sec/Figures/pipeline-11.13.pdf}
\caption{
\textbf{Overview of Text2VDM.}
Starting with a dense planar mesh constructed from a zero-valued VDM, users can initialize the mesh volume through a default spike-pattern VDM or a user-specified VDM. Given the text prompt and region mask, we apply CFG-weighted SDS loss $\mathcal{L}_{\text{WS}}$ to guide mesh deformations through a Laplace-Beltrami operator $L$ iteratively, achieving semantically focused generation of surface details or geometric structures.
% Laplace-Beltrami operator 
After optimization, vertex displacements are baked into the final VDM.
}
\label{Fig: Pipeline}
\end{figure*}


% Generative models have enabled remarkable capabilities to create 3D content from text~\cite{Fantasia3D:ICCV:2023,SIGGRAPH:TextDeformer:2023,single1-instant3d,single7-richdreamer,single8-luciddreamer,CLIP-Forge:CVPR:2022,single12-text2mesh} and images~\cite{single2-wonder3d,single3-CRM,single4-meshlrm,single6-mvd++,single5-im3d,single9-lrm,single10-One2345FS,single11-mvdream}. However, these methods often directly generate a final shape that is 
% % not amenable to user editing, and thus are 
% not compatible with artists' interactive workflow of 3D asset creation. 
% % The generation ability of sculpting brushes, which are commonly used in the creation workflows of 3D assets, is less explored.
% Text-guided generation of sculpting brushes, which allows artists to efficiently sculpt complex 3D details onto a model, remains an unexplored problem.

% % \begin{figure}[tbh]
% % \centering
% % \includegraphics[width=0.3\textwidth]{sec/Figures/moon.jpeg}
% % \caption{The classic workflow for creating VDM brushes by artists.}
% % \label{Fig: classic workflow}
% % \end{figure}

% % In this paper, unlike previous works that generate entire 3D models, we focus on generating VDM brushes used by artists during the creation of 3D models. VDM brushes are crucial and commonly used in an artist's modeling workflow. 
% Unlike previous methods that generate full 3D models, this paper focuses on generating sculpting brushes represented as vector displacement maps (VDMs).
% % VDM brushes used by artists during the creation of 3D models. VDM brushes are crucial and commonly used in an artist's modeling workflow. 
% In practice, a VDM brush either represents a certain type of surface detail or a 3D component, 
% % It stores predefined three-axis vertex displacements using a vector displacement map (VDM), 
% allowing artists to iteratively apply the same geometry pattern during sculpting.
% % Notably, the VDM differs significantly from the traditional height map which is merely a degenerate version of the VDM, limited to a single axis of displacement.
% Notably, the VDM differs significantly from the traditional height map, as shown in \Cref{Fig: vector or height}, making it difficult to generate using existing stable diffusion models~\cite{StableDiffusion:Arxiv:2021}.
% % We categorize VDM brushes into two types: 
% We discuss the generation of two types of VDM brushes in this paper:
% \emph{surface brushes} and \emph{component brushes}. 
% The former adds rich surface details to the model, while the latter allows artists to quickly sculpt 3D components onto the model, such as horns and ears. By using these two types of brushes, 3D artists and even beginners can create complex and detailed 3D models from a basic shape.

% Generating a VDM brush differs from directly generating an entire 3D model and has three significant challenges: 1) The generation requires the model to be generated from a fixed topology, i.e., high-subdivision-level planar mesh, ensuring that the final mesh can be baked into a brush. 
% 2) For 3D component brush generation, the VDM brush is expected to represent only the text-described part instead of including any extra semantically related parts. For example, a generated tortoiseshell should not be a full tortoise with a head, a tail, and legs.
% 3) The generation of the two types of brushes has different control requirements. surface brushes require control over the generated region, while component brushes need control over volume and direction to ensure the final brush behaves as expected when applied to the model.

% To address the challenges of brush generation, we propose \textbf{Text2VDM}, a novel optimization-based framework that generates diverse and controllable VDM brushes from text input. We initialize a $512\times512$-resolution 3-channel VDM image representing a dense planar mesh, where the three values of each pixel represent the three-axis displacement of the corresponding vertex. 
% % every $2\times2$ pixels correspond to two triangles. In this VDM image,
% % vertices in the planar mesh at the corresponding UV coordinates.
% After constructing a dense mesh from the VDM, users are provided with two methods for controlling the brush generation process: the shape map and the region mask. The shape map defines the desired volume and direction of the brush, while the region mask allows users to control the specific generated region. We observed that directly using Score Distillation Sampling (SDS)~\cite{DreamFusion:ICLR:2022} to generate component brushes can lead to local semantic coupling and an unstable optimization process.
% % This issue arises when generating individual parts of an object,  which may be influenced by the associated semantics of the whole object. For example, when generating a tortoiseshell, the optimization process may also produce a tortoise head or tail. 
% % We believe that this issue arises in the training dataset of stable diffusion, local components often appear together with the whole object, leading to semantic coupling. 

% % To address it, we expect SDS to provide more precise guidance when supervising the generation of 3D components to avoid noisy gradients caused by associated semantics. 
% We find that the standard SDS loss fails to provide precise guidance when supervising the generation of 3D components in terms of avoiding noisy gradients caused by associated semantics. 
% A straightforward solution is to utilize negative prompts~\cite{CSD:Arxiv:2023, NFSD:Arxiv:2023} to exclude undesired semantics, but our experiments
% % in~\Cref{Fig: Effect of CFG-weighted SDS} 
% show that this approach is ineffective in decoupling semantics and leads to an unstable and more time-consuming optimization process. 
% Instead, we propose to enhance the semantics of part-related words by applying classifier-free guidance (CFG) weighted blending to the tokens in the prompt. 
% % guiding the rendered image distribution toward more precision within the diffusion model. 
% This results in sharper and more focused text embedding, strengthening local semantics while reducing noisy gradients during optimization.

% % conditioned on a re-weighted text prompt. 
% Furthermore, to ensure high-quality mesh deformation, we reparametrize the mesh vertices through an implicit formulation based on the Laplace-Beltrami operator~\cite{Largesteps:SIGGRAPH:2021}. 
% To generate brushes with rich geometric details, we rasterize normal maps of the mesh by a differentiable renderer as the input for calculating the CFG-weighted SDS loss. 
% % As a result, the optimization becomes more stable, leading to higher-quality brush generation.
% % The VDM brushes generated by our approach are semantically focused and with high-quality geometric details.
% As a result, the generation process of our approach is stable and semantically focused.