\begin{figure}[tbh]
\centering
\includegraphics[width=0.49\textwidth]{sec/Figures/quali_2d.pdf}
\caption{
    \textbf{Qualitative comparisons of generated brushes for surface details.} Our method captures geometry details guided by texts, effectively preserving surface structure and avoiding mesh distortions.
}
\label{Fig: Qualitative 2D}
\end{figure}
\vspace{-0.6 cm}
\section{Experiments}
\label{sec:Experiment}
In this section, we conduct experiments to evaluate the various capabilities of Text2VDM both quantitatively and qualitatively for text-to-VDM brush generation.
% in ~\Cref{Qualitative} and ~\Cref{Quantitative}.
We then present an ablation study that validates the significance of our key insight into CFG-weighted SDS, as well as the effect of the region control and shape control.
% in ~\Cref{Ablation}.

\begin{figure*}[tbh]
\centering
\includegraphics[width=1\textwidth]{sec/Figures/quali_3d.pdf}
\caption{
    \textbf{Qualitative comparisons of generated brushes for geometric structures.}  Our method accurately presents key geometric features described by text, facilitating downstream applications in modeling software.
}
\label{Fig: Qualitative 3D}
\end{figure*}

\subsection{Qualitative Evaluation}
\label{Qualitative}
To the best of our knowledge, Text2VDM is the first framework to generate VDM brushes from text.
We adapted three existing methods for comparison and classified them into two categories. The first category includes Text2Mesh~\cite{single12-text2mesh} and TextDeformer~\cite{SIGGRAPH:TextDeformer:2023}, which generate a brush mesh through text-guided mesh deformation on a planar mesh, following a process similar to ours. For the second category, we opt to directly generate VDM via Paint-it~\cite{paintit}. Notably, this method originally uses SDS to optimize a UNet for generating PBR textures. We reframed it to suit our VDM brush generation task, modifying it to generate VDM through SDS optimization of the UNet. We compared the visual results in \Cref{Fig: Qualitative 2D} and \Cref{Fig: Qualitative 3D}.

Compared to other methods, Text2VDM can generate more vivid and better-quality VDM brushes. Text2Mesh applies displacement to each vertex along normal directions, resulting in limited mesh deformation. TextDeformer indicates the accumulation of local deformations in the Jacobians, which results in global mesh drift, making it challenging to bake these meshes into VDM.
Reframed Paint-it VDM generation is equivalent to optimizing the three-axis displacement of each vertex on the mesh with SDS. Although the UNet reduces noise from the SDS~\cite{paintit}, geometric regularization is still required to ensure mesh quality. The generated mesh must compromise between solving the problem and maintaining smoothness, which makes achieving high-quality mesh generation quite challenging.
% Using a UNet to generate VDM is equivalent to optimizing the three-axis displacement of each vertex on the mesh with SDS. Although the UNet reduces noise from the SDS~\cite{paintit}, geometric regularization is still required to ensure mesh quality. The generated mesh must compromise between solving the problem and being smooth, which results in low-quality mesh generation.

\subsection{Quantitative Evaluation}
\label{Sec: Quantitative}

We quantitatively evaluated our framework regarding generation consistency with text input and mesh quality. We used 40 distinctive text prompts for VDM generation.

\noindent\textbf{Generation Consistency with Text.} We initially assessed the relevance of the generated results to the text descriptions~\cite{CLIP:CORR:2021}. 12 different views were rendered for average scores respectively, as presented in Table~\ref{tab:quantitative comp}. Our approach achieves the highest scores compared to baseline methods.


\begin{table}[h!]
\caption{Quantitative evaluation of state-of-the-art methods. The geometry CLIP score is calculated on shaded images with uniform albedo colors~\cite{Richdreamer:CVPR:2024}, and self-intersection is quantified as the ratio of self-intersected mesh faces to the total number of faces.}
\centering
\footnotesize   % incase not overflow
% TADA & TextDeformer & Fantasia3D
\begin{tabular}{*{10}{c}}
         \hline
           & Geometry CLIP Score $\uparrow$ & Mesh Self-Intersection $\downarrow$\\
         \hline 
         Paintit & $0.2375$ & $19.42\%$  \\
         Text2Mesh & \underline{0.2497}  & $7.18\%$\\
         TextDeformer & $0.2477$  & \textbf{0.04\%} \\
         Ours & \textbf{0.2556} &  \underline{0.77\%}\\
         \hline
\end{tabular}
\label{tab:quantitative comp}
\end{table}

\begin{figure*}[tbh!]
\centering
\includegraphics[width=1\textwidth]{sec/Figures/ablation_sds.pdf}
\caption{
    \textbf{Effect of CFG-weighted SDS.} CFG-weighted SDS effectively mitigates semantic coupling issues in SDS, such as generating the tortoise’s tail and head or the snail’s head, by providing more focused semantic guidance. In contrast, CSD adds extra negative terms that fail to decouple semantics, resulting in a less stable and more time-consuming optimization process.
} 
\label{Fig: Effect of CFG-weighted SDS}
\end{figure*}

\noindent\textbf{Mesh Quality.} We evaluated mesh quality by examining self-intersection. Paint-it and Text2Mesh, which utilize direct vertex displacement, often converge to a local minimum and disregard the mesh triangulation. While TextDeformer exhibits the lowest self-intersection, its tendency to produce over-smoothed results frequently results in losing object features described in text prompts. 

\begin{table}[h!]
\caption{User evaluation of generated VDMs.}
\centering
\footnotesize
    \begin{tabular}[width=1.0\textwidth]{*{10}{c}}
         \hline 
         User Preference $\uparrow$  & Geometry Quality & Consistency  with Text\\
         \hline 
         Paintit & $3.1\%$  & $1.7\%$ \\
         Text2Mesh & \underline{$18.3\%$} & \underline{$27.3\%$} \\
         TextDeformer & $3.3\%$ & $3.4\%$ \\
         Ours & \textbf{75.3\%} & \textbf{67.6\%} \\
         \hline
    \end{tabular}
\label{tab:user comp}
\end{table}


\noindent\textbf{User Study.} We further conducted a user study to evaluate the effectiveness and expressiveness of our method. A Google Form was utilized to assess 1) geometry quality and 2) consistency with text. We recruited 32 participants, of whom 14 are graduate students majoring in media arts, and 18 are company employees specializing in AI content generation. In this form, the participants were instructed to choose the preferred renderings of VDM from different methods in randomized order, as shown in Table~\ref{tab:user comp}. The results show participants preferred our method by a significant margin. 
% For practical evaluation, we invited 5 participants to use VDMs generated by our methods in Blender to sculpt 3D models that aligned with their expectations (Figures~\ref{Fig: Local to Global Mesh Stylization} and~\ref{Fig: Coarse to Fine Interactive Modeling}).

\subsection{Ablation Study}
\label{Ablation}
\textbf{Effects of CFG-Weighted SDS.}  %为了验证CFG-weighted SDS的有效性，我们设置的实验对比了直接使用SDS，使用CFG-Weighted SDS 以及使用三种不同negative prompt权重的CSD。As mentioned in ~\Cref{sec: tesds}, SDS在没有全局语义作为reference的情况下进行local component生成时会有语义耦合的情况，生成出来的mesh会有明显的瑕疵。另外我们发现使用negative prompt这种直观的做法并不能有效的解耦语义，并且增大负文本的权重时会使得优化过程变得不稳定，更难收敛，导致了低质量的mesh。相比之下，我们的方法不需要进行额外的Unet推理，并且能够有效的对语义进行解耦生成符合要求的mesh。
We conducted experiments to compare the generated results of directly using SDS~\cite{DreamFusion:ICLR:2022}, CFG-weighted SDS, and CSD~\cite{CSD:Arxiv:2023} with three different annealed weights of negative prompt (\Cref{Fig: Effect of CFG-weighted SDS}). As discussed in \Cref{sec: tesds}, SDS can result in semantic coupling when generating sub-object structure, leading to artifacts like the tortoise's tail and head or the snail's head. We also found that using negative prompts was ineffective at decoupling semantics. Increasing the initial weight of negative prompts further makes the optimization unstable, resulting in low-quality results. In contrast, our method effectively mitigates semantic coupling to produce high-quality meshes without requiring additional UNet inference.
\begin{figure}[tbh!]
\centering
\includegraphics[width=0.48\textwidth]{sec/Figures/ablation_masks2.pdf}
\caption{
    \textbf{Effect of region control.} Region masks can effectively control the shape of surface details based on different text inputs.
}
\label{Fig: Effect of region mask}
\end{figure}

\noindent\textbf{Effects of Region Control.} %我们在图中展示了两组region mask在不同text prompt下对surface detailed brush生成的控制能力。我们生成的结果可以很好得match text生成cloth，metal，stone等不同质感，同时符合region mask所限制的形状
\Cref{Fig: Effect of region mask} demonstrates two sets of region masks and their control over surface details generation under different text prompts. Without using a region mask, the results lack a specific shape, which may not satisfy the desired stylized effect. By using a region mask, our generated results effectively conform to the user's desired shapes while also aligning with the styles specified by the text, such as metal and stone.


\noindent\textbf{Effects of Shape Control.} % 我们的方法在图中展示了生成结果与通过shape map进行初始化体积与方向保持一致的能力，在不同的local component生成中，比如beard，pauldron，elf ear,我们的方法能够在保持体积和方向大致稳定的情况下生成多样的符合文本描述的结果
Our method demonstrates that user-specified VDMs can effectively control the volume and direction of generated geometric structures. As shown in \Cref{Fig: Effect of shape map}, various generated geometric structures, such as elf ears and pauldrons, are high-quality and align with the text descriptions. We also found that without volume initialization, it is challenging to generate desired results. It indicates that this initialization is crucial for steering the gradient flow of geometric structure generation via adjusting the Laplacian term.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.49\textwidth]{sec/Figures/ablation_shape.pdf}
\caption{
    \textbf{Effect of shape control.} User-specified VDMs can help achieve the intended final effect of geometric structures by initializing the brush's volume and direction.
}
\label{Fig: Effect of shape map}
\end{figure}
\vspace{-0.3 cm}
\subsection{Applications}
\label{Application}
Once various VDM brushes are generated, users can directly use these brushes to meet diverse creative needs in mainstream modeling software. For example, they can apply VDM brushes for mesh stylization and engage in a real-time iterative modeling process.

\noindent\textbf{Local-to-Global Mesh Stylization.} Although mesh stylization is a complex task even for professional artists, combining different surface details allows users to achieve stylization quickly. For instance, users can apply a variety of wall-damage brushes to specific areas of a stone pillar, creating a style of damage (~\Cref{Fig: Local to Global Mesh Stylization}).
% Similarly, they can use different rust-effect brushes on a helmet to give it an aged style, 

\begin{figure}[!htb]
\centering
\includegraphics[width=0.48\textwidth]{sec/Figures/application.pdf}
\caption{\textbf{Local to global mesh stylization.} Applying various surface details brushes can create a damaged-style stone pillar model.} 
\label{Fig: Local to Global Mesh Stylization}
\end{figure}

\noindent\textbf{Coarse-to-Fine Interactive Modeling.} Unlike previous methods~\cite{magiclay,tipeditor} that require a lengthy optimization process for each edit and result in non-reusable outcomes, our generated VDM brushes can be directly used in modeling software. This enables users to apply the generated brushes easily and interactively. For example, \Cref{Fig: Coarse to Fine Interactive Modeling} shows that users can combine various brushes, such as skeleton hand, rose pattern, and pauldron to refine a coarse cloth model into a highly detailed one.

