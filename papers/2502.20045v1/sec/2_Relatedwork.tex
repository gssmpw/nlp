\section{Related Work}
\label{sec:Relatedwork}
This section reviews previous work related to 3D sculpting brush generation and summarizes the current research gap.

% Zeyu comment 1107: RW中每一个subsection，要先介绍子领域的发展概况，然后介绍几项关键工作，最后最重要的是，引出当前解决本论文中问题的Research gap（即现有方法不能解决我们的问题），突出我们问题和方法的创新性。
\subsection{Text to Local 3D Generation and Editing}
With recent advances in diffusion models~\cite{StableDiffusion:Arxiv:2021} and differentiable 3D representations~\cite{DMTet:NIPS:2021,TriplaneDiffusion:CVPR:2023,SIGGRAPH:NJF:2022,NeRF:ECCV:2020,Largesteps:SIGGRAPH:2021}, many methods for text-guided full 3D model generation have emerged~\cite{Fantasia3D:ICCV:2023,SIGGRAPH:TextDeformer:2023,single1-instant3d,single7-richdreamer,single8-luciddreamer,CLIP-Forge:CVPR:2022,single12-text2mesh}. Since 3D content creation is an iterative process that often requires user interaction, more attention has been directed toward localized 3D generation and editing. For example, 3D Highlighter~\cite{3Dhighlighter} and 3D Paintbrush~\cite{paintbrush} use text as input, leveraging pre-trained CLIP models~\cite{CLIP:ICML:2021} or diffusion models~\cite{DreamFusion:ICLR:2022} to supervise the optimization of neural networks for segmenting the regions of a 3D model that match the text description. Based on the information from these segmented regions, further editing of texture and geometry can be applied to the 3D model. Furthermore, SKED~\cite{mikaeili2023sked} and SketchDream~\cite{SketchDream2024} introduce sketches as an additional modality to assist in localized editing. To enable more precise control, FocalDreamer~\cite{focaldreamer}, MagicClay~\cite{magiclay}, and Tip-Editor~\cite{tipeditor} allow users to specify the editing location directly within the 3D space. These works rely on optimization-based methods to edit specific objects, often resulting in non-reusable editing outcomes. Additionally, each edit requires a lengthy optimization process, making interactivity difficult to achieve. 
% To address it, we focus on generating reusable local geometric details and 3D component predefined sculpting brushes. These can be directly integrated into existing modeling workflows, allowing users to quickly perform localized edits to create exquisite 3D models.

\subsection{Diffusion Priors for 3D Generation}
% focus on the SDS improvements
\label{sec:related_sds}
Score distillation sampling (SDS)~\cite{SJC:CVPR:2023, DreamFusion:ICLR:2022} provides pixel-level guidance by seeking specific modes in a diffusion model, inspiring further research to improve optimization-based 3D generation~\cite{VSD, Magic3D:CVPR:2023, Perp-Neg:Arxiv:2023, ESD:Arxiv:2023, LODS:Arxiv:2023}. Some studies focus on mitigating the ``Janus'' problem~\cite{LMC-SDS:Arxiv:2024, Debias:NIPS:2023}, while others fine-tune diffusion models with multi-view datasets to enhance 3D consistency~\cite{MVDream:Arxiv:2023, Zero123:ICCV:2023}. Recent research focuses on refining the design of SDS loss to achieve more precise guidance. For instance, Make-it-3D~\cite{Make3D:2023:ICCV} introduces two-stage optimizations to improve textured appearance, while Fantasia3D~\cite{Fantasia3D:ICCV:2023} dynamically modifies the time-dependent weighting function within SDS computations. Additionally, several methods~\cite{CSD:Arxiv:2023,NFSD:Arxiv:2023} incorporate negative prompts as the conditional term to further refine the optimizations. Although diffusion priors have achieved promising results, their application in generating sub-object structures without global context as a reference is still challenging.


\subsection{Appearance and Geometric Brush Synthesis}
The concept of brushes is very common in the creative process of digital artists, serving as a reusable local decorative unit. Appearance brushes focus on color representation and drawing styles in 2D space. With the development of generative models~\cite{GAN:NIPS:2014,StableDiffusion:Arxiv:2021}, many works have explored the synthesis appearance brushes for interactive painting~\cite{DiffusionTexturePainting,NeuralBrushstrokeEngine}, realistic artworks generation~\cite{stylizedneuralpainting,painttransformer,GeneralVS}, and applying stylization~\cite{RethinkingST,snps}. Unlike appearance brushes, geometric brushes focus on modifying geometry by moving the vertices of a mesh in 3D space. VDM brushes, as an extension of standard geometric brushes, provide more complex geometric effects by utilizing VDMs. To the best of our knowledge, only a few techniques adopted the concepts of VDM for generation~\cite{64x64pixels,headcraft} and geometric texture transfer~\cite{deepgeometrysys}. Generating geometric brushes that can be used within existing workflows is still under-explored.

