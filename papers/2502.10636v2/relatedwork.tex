\section{Related Work}
\paragraph{HRI Personalization.}
This paradigm enables adaptive robotic systems to tailor behaviors, responses, and functionalities to individual users, enhancing user engagement and task efficacy in critical domains such as healthcare~\cite{agrigoroaie2016developing}, education~\cite{irfan2021lifelong}, and assistive robotics~\cite{jevtic2018personalized}. Prior work, including \cite{tanevska2020socially}, has investigated personalization and localization frameworks in social robotics, highlighting both capabilities and constraints of current approaches. A persistent limitation lies in the lack of modality-specific representation learning, which impedes cross-modal reasoning, generalization across heterogeneous perceptual inputs, and contextual adaptation in dynamic environments~\cite{wang2024vlm}.

\paragraph{Personalized VLMs.} Recent advancements in personalized LLMs have demonstrated empirical success in aligning outputs with individual user preferences and contextual histories~\cite{zhuang2024hydra, ning2024user}. However, the adaptation of VLMs for HRI remains an under-explored frontier. While foundational frameworks such as MyVLM~\cite{alaluf2025myvlm}, Meta-Personalizing VLM~\cite{yeh2023meta} and MC-LLaVA~\cite{an2024mc} establish preliminary methodologies for VLM personalization, these approaches fail to address persistent challenges unique to HRI. Critically, current methods overlook (1) the intrinsic complexity of multimodal alignment (2) sociotechnical risks such as privacy erosion and bias amplification stemming from personalized model behaviors in socially embedded robotic systems. % For instance, meta-personalization techniques proposed for video instance retrieval~\cite{yeh2023meta} focus narrowly on retrieval tasks without addressing real-time interaction dynamics or safeguarding against biases. 



\paragraph{VLMs for HRI.} Parallel research efforts have explored VLM-based approaches to HRI, tackling challenges in task planning, interpretability, and multimodal perception. Notable contributions include the VLM See, Robot Do framework~\cite{wang2024vlm}, which effectively translates human demonstration videos into executable robot action plans, demonstrating superior performance in long-horizon tasks. Additionally, HuBo-VLM~\cite{dong2023hubo} has made strides by unifying visual grounding and object detection, showcasing robust performance on benchmarks such as Talk2Car~\cite{deruyttere2019talk2car}. However, these frameworks, often built on top of visual foundation models, are predominantly Retrieval-Augmented Generation (RAG)-based~\cite{lewis2020retrieval} and not inherently personalized. They incur high processing costs, latency, and require intensive prompt engineering and computational resources. Furthermore, while task-specific fine-tuning approaches like AlignBot~\cite{chen2024alignbot} exist, they lack a holistic consideration of user bias, privacy, and ethical concerns.