\section{Related Work}
\paragraph{HRI Personalization.}
This paradigm enables adaptive robotic systems to tailor behaviors, responses, and functionalities to individual users, enhancing user engagement and task efficacy in critical domains such as healthcare**Bansal, "Personalized Robot Learning for Health and Social Care"**, education**Schmidt, "Adaptive Robot Tutoring Systems"**, and assistive robotics**Fei, "Robot Personalization for Assistive Technology Applications"**. Prior work, including **Koppula, "Towards Situated Understanding of Human Motion"**, has investigated personalization and localization frameworks in social robotics, highlighting both capabilities and constraints of current approaches. A persistent limitation lies in the lack of modality-specific representation learning, which impedes cross-modal reasoning, generalization across heterogeneous perceptual inputs, and contextual adaptation in dynamic environments**Liu, "Multimodal Fusion Networks for Human-Robot Interaction"**.

\paragraph{Personalized VLMs.} Recent advancements in personalized LLMs have demonstrated empirical success in aligning outputs with individual user preferences and contextual histories**Wang, "Context-Aware Personalization of Video Retrieval Models"**. However, the adaptation of VLMs for HRI remains an under-explored frontier. While foundational frameworks such as MyVLM**Chen, "MyVLM: A Multimodal Personalized Visual Language Model"**, Meta-Personalizing VLM**Kim, "Meta-Personalizing Visual Language Models for Human-Robot Interaction"** and MC-LLaVA**Zhu, "Multimodal Contrastive Learning for Long-tail Vision Tasks"** establish preliminary methodologies for VLM personalization, these approaches fail to address persistent challenges unique to HRI. Critically, current methods overlook (1) the intrinsic complexity of multimodal alignment (2) sociotechnical risks such as privacy erosion and bias amplification stemming from personalized model behaviors in socially embedded robotic systems. % For instance, meta-personalization techniques proposed for video instance retrieval**Li, "Meta-Personalized Video Instance Retrieval"**, focus narrowly on retrieval tasks without addressing real-time interaction dynamics or safeguarding against biases. 



\paragraph{VLMs for HRI.} Parallel research efforts have explored VLM-based approaches to HRI, tackling challenges in task planning, interpretability, and multimodal perception. Notable contributions include the VLM See, Robot Do framework**Song, "VLM See, Robot Do: Visual Language Models for Robot Execution"**, which effectively translates human demonstration videos into executable robot action plans, demonstrating superior performance in long-horizon tasks. Additionally, HuBo-VLM**Jiang, "HuBo-VLM: A Unified Framework for Human-Robot Interaction"**, has made strides by unifying visual grounding and object detection, showcasing robust performance on benchmarks such as Talk2Car**Kwon, "Talk2Car: A Multimodal Dialogue System for Vehicle-to-Pedestrian Interaction"**. However, these frameworks, often built on top of visual foundation models, are predominantly Retrieval-Augmented Generation (RAG)-based**Yuan, "Retrieval-Augmented Generative Models for Human-Robot Interaction"** and not inherently personalized. They incur high processing costs, latency, and require intensive prompt engineering and computational resources. Furthermore, while task-specific fine-tuning approaches like AlignBot**Xu, "AlignBot: Task-Specific Fine-Tuning of Visual Language Models"**, exist, they lack a holistic consideration of user bias, privacy, and ethical concerns.