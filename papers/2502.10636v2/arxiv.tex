%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
%\usepackage{bera}% optional: just to have a nice mono-spaced font
\usepackage{listings}
\usepackage[table]{xcolor} % Required for coloring

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{microtype}
%\usepackage{listingsjson}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{balance}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{caption}
\usepackage{fontawesome5}
\usepackage{colortbl}  
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% Packages for math and symbols
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{listings}
% Optional: For better layout control
%\usepackage[margin=1in]{geometry}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2024}
\usepackage{etoolbox}
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\lstdefinestyle{json}{
    language=JSON,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    showstringspaces=false,
    breaklines=true,
    numbers=none,
    frame=single,
    captionpos=b,
}


\definecolor{lightgray}{rgb}{0.95,0.95,0.95} % Light gray color
\newtcolorbox{graybox}{
    colback=lightgray, % Background color
    colframe=black,    % Border color
    boxrule=0.5pt,     % Border thickness
    arc=4pt,           % Rounded corners
    boxsep=5pt,        % Padding inside the box
    left=6pt,          % Left margin
    right=6pt,         % Right margin
    top=6pt,           % Top margin
    bottom=6pt         % Bottom margin
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{USER-VLM 360°: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions}

\begin{document}

\setlength{\tabcolsep}{3pt} 
\twocolumn[
\icmltitle{\textit{USER-VLM 360°}: 
Personalized Vision Language Models\\ with User-aware Tuning for Social Human-Robot Interactions
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}


\begin{icmlauthorlist}
\icmlauthor{Hamed Rahimi}{yyy}
\icmlauthor{Adil Bahaj}{comp}
\icmlauthor{Mouad Abrini}{yyy}
\icmlauthor{Mahdi Khoramshahi}{yyy}
\icmlauthor{Mounir Ghogho}{comp}
\icmlauthor{Mohamed Chetouani}{yyy}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{ISIR, Sorbonne University, Paris, France}
\icmlaffiliation{comp}{International University of Rabat, Morocco}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Hamed Rahimi}{hamed.rahimi@sorbonne-universite.fr}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Personalized Vision Language Models, User-aware Tuning,  Human-Robot Interactions}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\printAffiliationsAndNotice{}


\begin{abstract}
%Enabling socially intelligent human-robot collaboration necessitates scalable models capable of dynamically personalizing interactions. 
The integration of vision-language models into robotic systems constitutes a significant advancement in enabling machines to interact with their surroundings in a more intuitive manner. While VLMs offer rich multimodal reasoning, existing approaches lack user-specific adaptability, often relying on generic interaction paradigms that fail to account for individual behavioral, contextual, or socio-emotional nuances. When customization is attempted, ethical concerns arise from unmitigated biases in user data, risking exclusion or unfair treatment. To address these dual challenges, we propose User-VLM 360°, a holistic framework integrating multimodal user modeling with bias-aware optimization. Our approach features: (1) user-aware tuning that adapts interactions in real time using visual-linguistic signals; (2) bias mitigation via preference optimization; and (3) curated 360° socio-emotive interaction datasets annotated with demographic, emotion, and relational metadata. Evaluations across eight benchmarks demonstrate state-of-the-art results: +35.3\% F1 in personalized VQA, +47.5\% F1 in facial features understanding, 15\% bias reduction, and 30× speedup over baselines. Ablation studies confirm component efficacy, and deployment on the Pepper robot validates real-time adaptability across diverse users. We open-source parameter-efficient 3B/10B models and an ethical verification framework for responsible adaptation. %This work seamlessly integrates personalization, ethical fairness, and socially intelligent collaboration in human-robot interaction, establishing a new state-of-the-art for equitable human-robot interactions. 

\end{abstract}
 
\begin{center}
    \faGithub  \hspace{0.21cm} \href{https://hamedr96.github.io/User-VLM/}{ https://hamedr96.github.io/User-VLM/}
\end{center}


\section{Introduction}
Ensuring a safe and intuitive interaction between humans and robots requires AI systems that dynamically perceive and adapt to individual needs, behaviors, and preferences~\cite{mataric2023robot}. This adaptability is crucial, as it enables robots to navigate complex social dynamics and establish meaningful connections that respect human cognitive and emotional boundaries~\cite{romeo2022exploring, frith2005theory}. Such capabilities are particularly important in sensitive domains like healthcare and education, where tailored interactions enhance both user safety and engagement~\cite{oertel2020engagement,cavallini2021can, kristen2014theory}. While various approaches have been explored to enable dynamic adaptability in Human-Robot Interactions (HRI)\cite{tanevska2020socially, andriella2020short}, recent advances include integrating robots with vision-language models (VLMs)~\cite{zhang2024vision}, building on prior work in adaptable interaction paradigms~\cite{dong2023hubo,liu2024vision}. These models process and correlate visual data from cameras with linguistic inputs from speech or text, allowing robots to interpret contextual cues and execute tasks aligned with human intentions~\cite{robinson2023robotic,song2024vlm}. 

%Research on VLMs has validated their effectiveness across a broad spectrum of applications, ranging from natural language-guided object manipulation tasks~\cite{liu2024vision} to context-aware navigation systems capable of operating in dynamic human-populated environments.environments~\cite{song2024vlm}. 


However, despite these advancements, deploying current VLMs in HRI scenarios introduces two critical limitations. First, VLMs often exhibit degraded performance when visual context and linguistic queries are semantically misaligned~\cite{gordon2025mismatch}— as shown in \Cref{fig:pepper}, a common occurrence in real-world HRI~\cite{nocentini2019survey}. %For example, a user might inquire about subjects or reference abstract concepts not directly observable in the robot’s immediate field of view. 
This challenge stems from training datasets that lack domain-specific examples of human-robot collaboration, where visual inputs are inherently partial, perspectival, and temporally dynamic~\cite{laurenccon2024matters}. Second, while VLMs excel at general-purpose reasoning, they struggle to generate personalized responses without explicit prior knowledge of user preferences and interaction history. Such information is rarely available during initial interactions; besides, data collection raises ethical concerns around data privacy, particularly in domains where sensitive information must be safeguarded~\cite{ning2024user, sahu2024pop}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/pepper2.pdf}
    \caption{\textbf{Deployment of User-VLM 360° on Pepper Social Robotic Framework.} 
    User-aware Tuning mitigates the semantic gap arising from the misalignment between user queries and the observed scene as captured from the robot's camera perspective. While instruction-tuning could address this for large VLMs, it adds latency and reduces performance. User-VLM 360° overcomes this by inherently aligning cross-modal user representations, enabling robust real-time adaptation in dynamic robotic environments.
    }
    \label{fig:pepper}
\end{figure}

Recent attempts to mitigate these challenges by augmenting prompts \cite{zhou2022learning, eapen2023personalization} with explicit instructions or contextual metadata inadvertently introduce new bottlenecks that undermine real-world deployment. First, appending verbose instructions to queries increases inference latency~\cite{li2024inference}, hindering real-time responsiveness critical for fluid human-robot collaboration. Second, processing extended prompts demands higher computational resources~\cite{zhang2024cls}, escalating operational costs and energy consumption—a critical barrier for resource-constrained edge devices. Third, smaller language models struggle to parse complex, instruction-heavy prompts~\cite{ma2023crepe}. Even large language models exhibit degraded performance in such scenarios\cite{zhou2022learning}, as their ability to maintain coherent reasoning diminishes when reconciling task-specific guidance with broader contextual awareness. %Finally, this approach fails to resolve core challenges in personalization: dynamically adapting outputs to individual users’ unique communication styles or cultural norms requires implicit, on-the-fly learning rather than static prompt engineering. 

However, training VLMs with task-specific user data introduces ethical concerns~\cite{rahimi2025user}, as unmitigated biases may result in exclusion or unfair treatment. % The inherent inefficiencies in current instruction-driven AI systems underscore a critical tension between lightweight autonomy and robust cross-context generalization, often sacrificing speed or privacy to navigate diverse interaction environments. 
As shown in \Cref{fig:user-aware}, this work pioneers the evolution of VLM architectures by moving beyond brittle prompt dependency, embedding intrinsic adaptability through human-centric multimodal training, and introducing zero-shot personalization frameworks that, for the first time, preserve user autonomy while enabling context-sensitive reasoning.
\begin{figure*}[tbh]
    \centering
    \includegraphics[width=\linewidth]{figures/result-final.pdf}
    \caption{\textbf{User-aware Tuning} consists of three key steps: In the first step,\textit{Vision Alignment}, the model is trained to recognize and interpret human emotions, age, gender, and ethnicity based on facial features and visual signals. %This foundational training equips the model with the ability to perceive nuanced human characteristics. 
    In the second step, \textit{Instruction Tuning}, the model undergoes supervised instruction tuning, enabling it to respond effectively to general-purpose questions by incorporating visual cues. Finally, to mitigate over-personalization and prevent biased or unethical responses, the third step, \textit{Bias Mitigation}, focuses on training the model to generate ethical and contextually appropriate responses. %Together, these steps ensure that the VLM is highly capable and responsibly aligned with diverse user needs.
    }
    \label{fig:user-aware}
\end{figure*}

\paragraph{Contributions} %To address the dual challenges of balancing ethical alignment with adaptive efficiency in human-robot interaction, 
This paper features:
\textbf{(1)} User-aware Tuning, a framework integrating visual-linguistic human-robot interaction capabilities into state-of-the-art VLMs with bias-aware optimization, prioritizing lightweight autonomy and contextual reasoning;
\textbf{(2)} a multimodal dataset suite capturing diverse, privacy-conscious interaction scenarios to mitigate exclusionary biases and support zero-shot personalization;
\textbf{(3)} the open-source User-VLM 360° model family, optimized for scalability, facial feature comprehension, and bias-aware responsiveness;
\textbf{(4)} standardized benchmarks for evaluating trust-building adaptability and fairness in real-world deployment; and
\textbf{(5)} a comprehensive analysis of user-aware reasoning, demonstrating superior performance over prompt-dependent baselines in speed, privacy preservation, and nuanced social understanding. 
\textbf{(6)} real-world validation via deployment on the \textit{Pepper} robotic framework, demonstrating real-time adaptability while maintaining computational efficiency. %and ethical constraints. %These contributions collectively bridge the gap between ethical robustness and context-sensitive generalization for trustworthy, long-term human-robot collaboration.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=\linewidth]{figures/tuning-arch.png}
%    \caption{User-aware Tuning Process}
%    \label{fig:user} 
%\end{figure}



\section{Related Work}

\paragraph{HRI Personalization.}
This paradigm enables adaptive robotic systems to tailor behaviors, responses, and functionalities to individual users, enhancing user engagement and task efficacy in critical domains such as healthcare~\cite{agrigoroaie2016developing}, education~\cite{irfan2021lifelong}, and assistive robotics~\cite{jevtic2018personalized}. Prior work, including \cite{tanevska2020socially}, has investigated personalization and localization frameworks in social robotics, highlighting both capabilities and constraints of current approaches. A persistent limitation lies in the lack of modality-specific representation learning, which impedes cross-modal reasoning, generalization across heterogeneous perceptual inputs, and contextual adaptation in dynamic environments~\cite{wang2024vlm}.

\paragraph{Personalized VLMs.} Recent advancements in personalized LLMs have demonstrated empirical success in aligning outputs with individual user preferences and contextual histories~\cite{zhuang2024hydra, ning2024user}. However, the adaptation of VLMs for HRI remains an under-explored frontier. While foundational frameworks such as MyVLM~\cite{alaluf2025myvlm}, Meta-Personalizing VLM~\cite{yeh2023meta} and MC-LLaVA~\cite{an2024mc} establish preliminary methodologies for VLM personalization, these approaches fail to address persistent challenges unique to HRI. Critically, current methods overlook (1) the intrinsic complexity of multimodal alignment (2) sociotechnical risks such as privacy erosion and bias amplification stemming from personalized model behaviors in socially embedded robotic systems. % For instance, meta-personalization techniques proposed for video instance retrieval~\cite{yeh2023meta} focus narrowly on retrieval tasks without addressing real-time interaction dynamics or safeguarding against biases. 



\paragraph{VLMs for HRI.} Parallel research efforts have explored VLM-based approaches to HRI, tackling challenges in task planning, interpretability, and multimodal perception. Notable contributions include the VLM See, Robot Do framework~\cite{wang2024vlm}, which effectively translates human demonstration videos into executable robot action plans, demonstrating superior performance in long-horizon tasks. Additionally, HuBo-VLM~\cite{dong2023hubo} has made strides by unifying visual grounding and object detection, showcasing robust performance on benchmarks such as Talk2Car~\cite{deruyttere2019talk2car}. However, these frameworks, often built on top of visual foundation models, are predominantly Retrieval-Augmented Generation (RAG)-based~\cite{lewis2020retrieval} and not inherently personalized. They incur high processing costs, latency, and require intensive prompt engineering and computational resources. Furthermore, while task-specific fine-tuning approaches like AlignBot~\cite{chen2024alignbot} exist, they lack a holistic consideration of user bias, privacy, and ethical concerns.

\section{Methods}

\subsection{Architecture}
 
 The proposed user-aware tuning operates on the LLaVA model~\cite{liu2024visual}, consisting of a vision encoder~\cite{zhai2023sigmoid} and an LLM~\cite{team2024gemma}. % for general-purpose visual-linguistic understanding. 
 The vision encoder $\mathcal{E}$ transforms user images $X_I$ into a vision user representation $\mathbf{H}_I \in \mathbb{R}^{d_I}$. The LLM is a decoder transformer that generates text tokens $\mathbf{y} = \{y_1, y_2, \ldots, y_L\}$ based on the tokenized question $\mathbf{H}_Q \in \mathbb{R}^{d_Q}$ and the image vector $\mathbf{H}_I$ produced by the vision encoder, where $L$ is the length of the generated sequence.  %The overall architecture operates on the dataset $\mathcal{D}=\{\{(I_k, T_k, Q_i, A_i)\}_{i=1}^N\}_{k=1}^M$ as detailed in \Cref{fig:arch}.

\paragraph{Pre-trained Vision Encoder}
 Given an image user entry $I$, the vision encoder employs $\mathcal{E}: \mathbb{R}^{d_I \times N} \rightarrow \mathbb{R}^{d_z \times N}$, where $d_z$ and $d_I$ denote the hidden dimensions, and $N$ is the batch size. The pre-trained encoder processes the image and produces sequences of feature vectors $\mathcal{E}(I) = \{f_1, f_2, \ldots, f_M\}$, where $M$ is the number of image patches. These vectors are processed through a projection head $P: \mathbb{R}^{d_{z}} \rightarrow \mathbb{R}^{d_h}$, implemented as a multilayer perceptron, which maps $f^I$ into the language embedding space. Specifically, a trainable projection matrix $W$ is applied to transform $f^I$ into the user embedding vector $H_{I}$, with the same dimensionality as the word embedding space in the language model: $H_{I} = W \cdot f^I$.



\paragraph{Large Language Model}
Given an LLM $\xi_\phi(\cdot)$ parameterized by $\phi$, we concatenate the image features $H_I$ projected in the word embedding space with the textual features $H_Q$, forming the input for the LLM to carry out subsequent predictions. More specifically, given the input question $Q$ and answer $A$, a word embedding matrix is used to map them to contextual embeddings $H_Q$ and $H_A$ through the tokenizer, and the distribution over $H_A^{(i+1)}$ can be obtained following the auto-regressive model as:

\begin{dmath}
\label{equation:contt}
  p_\phi\left(H_A^{(i+1)} \mid H_I, H_Q, H_A^{(1:i)}\right)
    \nonumber \\
    = \sigma\left(\xi_\phi(H_I, H_Q, H_A^{(1:i)})\right),
\end{dmath}
%where $\theta$ represents all the trainable parameters in the LLM, $\sigma(\cdot)$ is a \texttt{softmax} function, and $f(\cdot)$ outputs the last token embedding of the whole sequence. We denote $p_\theta$ as the prediction probability for the anticipated answer token $H_a^{(i+1)}$ at the position $i+1$, conditioning on the input user token embedding $H_u$, the question token embedding $H_q$, and the previous answer token embeddings $H_a^{(1:i)}$. 
where $_\phi$ represents all the trainable parameters in the LLM, $\sigma(\cdot)$ is a \texttt{softmax} function, and $\xi_\phi(\cdot)$ outputs the logits (before applying \texttt{softmax}) over the vocabulary for the last position of the sequence. We denote $p_\phi$ as the prediction probability for the anticipated answer token $H_A^{(i+1)}$ at the position $i+1$, conditioned on the input user token embeddings $H_I$, the question token embeddings $H_Q$, and the previous answer token embeddings $H_A^{(1:i)}$. The logits are passed through $\sigma(\cdot)$ to compute the probability distribution over all tokens in the vocabulary, and the most probable token is typically selected using $\texttt{argmax}$ with a greedy search.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/training-data-cropped.pdf}
    \caption{\textbf{Distribution of Training Datasets.} The datasets are constructed by combining high-quality general-purpose datasets with a facial image datasets, further refined to align with both visual and linguistic contexts. %This approach ensures a balanced representation for training across multiple subjects.
}
    \label{fig:dataset}
\end{figure*}


\subsection{User-aware Tuning}
User-aware Tuning is a novel post-training procedure designed to enhance the interaction capabilities of general-purpose models by integrating contextual human-centric understanding. As shown in \Cref{fig:user-aware}, unlike traditional task-specific fine-tuning, user-aware tuning focuses on equipping models with the ability to adapt their responses based on the user's visual context, such as facial expressions, age, gender, and ethnicity. This approach emphasizes the development of personalized, patient, and empathetic interactions by aligning the behavior of the model with the user's emotional state and demographic profile. 


\paragraph{Vision Alignment}
In the initial phase of the tuning process, the parameters of the LLM and the Vision Encoder are kept frozen, focusing the optimization exclusively on continuing pre-training of the Multi-Layer Perceptron layer. 
The training pipeline integrates user profiles and images while intentionally leaving the LLM's text input empty, ensuring the model learns user profiles based on visual cues rather than linguistic context. The data in this step represent the robot's perspective and its interpretation of the environment. Specifically, we provide it with user images (with detailed demographic descriptions), allowing it to dynamically learn and understand what it is observing from its point of view. Formally, the MLP parameters denoted $W$, are trained to transform the visual feature vector $f^I$ into a user-integrating vector $H_I$, represented as $H_I = W \cdot f^I$. The objective is to minimize the cross-entropy loss function $\mathcal{L}_p$, which measures the discrepancy between the predicted user profile and the ground-truth profile. By minimizing $\mathcal{L}_p$, the MLP is optimized to produce latent representations that effectively map visual inputs to user-specific embeddings, thus facilitating the generation of customized outputs by the LLM.

\paragraph{Instruction Tuning}
In the second phase of the training process, we freeze the MLP and Vision Encoder and instruction-tune the LLM's layers on user-aware questions and answers using two methods: (1) Low-Rank Adaptation (LoRA)~\cite{hu2021lora} and (2) Sparse Mixture of LoRA Experts (MoLE)~\cite{chen2024llava}. User-aware questions and answers consist of pairs that combine a user image with personalized Q\&A, generated from the robot's perspective. More formally, in the first method, for a token input $\mathbf{h} \in \mathbb{R}^{d_i}$ to a linear layer $y$, LoRA learns a low-rank update $\Delta \phi$ to the pre-trained weight matrix $\phi \in \mathbb{R}^{d_o \times d_i}$, such that:
\begin{equation}
    \mathbf{y} = \phi (\mathbf{h}) + \Delta \phi (\mathbf{h)}, \quad \Delta \phi = \frac{\alpha}{r} B A,
\end{equation}
where $A \in \mathbb{R}^{r \times d_i}$ and $B \in \mathbb{R}^{d_o \times r}$ are trainable low-rank matrices, $r$ is the rank of the decomposition, and $\alpha$ is a scaling factor controlling the magnitude of the adaptation. During fine-tuning, only $A$ and $B$ are updated, while $W$ remains frozen, enabling parameter-efficient adaptation.

In the second method, MoLE, we extend the LoRA framework by training the self-attention layer with LoRA and introducing $K$ experts, each with independent low-rank matrices $\{A_k, B_k\}_{k=1}^K$, to each Feed Forward Network (FFN) layer of the LLM. A routing function $\mathcal{G}$ dynamically selects the most suitable expert for each token $\mathbf{h}$:
\begin{equation}
    k^* = \arg\max_{k \in \{1, \dots, K\}} \phi^g_k (\mathbf{h}),
\end{equation}
where $\phi^g_k$ are the routing weights for the $k$-th expert. Then, the chosen expert is activated to execute the actual computation, while the rest are simply ignored for the current tokens. The output of the FNN is 
\begin{equation}
    f_{\text{FFN}}'(h)=f_{\text{FFN}}(h) + E_k(h),
\end{equation}
where $f_{\text{FFN}}(.)$ is the original FFN module and $E_k(.)$ is the chosen $k$-th LoRA expert.%, i.e.,
%\begin{equation}
%    E_k(h)=\frac{\alpha}{r}B_kA_kh.
%\end{equation}
%This sparse activation mechanism ensures computational efficiency while enabling domain-specific adaptations for diverse inputs. %In addition, a load balance loss is employed to ensure a uniform token distribution between experts, promoting robust training and effective utilization of all experts.



\paragraph{Bias Mitigation}
The bias mitigation component of our tuning process is specifically designed to ensure that the model generates ethical and responsible responses when addressing questions that may be sensitive, offensive, or unethical. Model alignment with ethical standards - whether universal or community-specific - presents significant challenges in data collection, which is why we developed bias-aware preference optimization. For this step, we continue to keep the vision encoder and MLP layer frozen and instruction-tune the LLM layers to mitigate biases such as racist, sexist, and inappropriate questions and answers using Direct Preference Optimization (DPO)~\cite{rafailov2024direct}. DPO is a computationally efficient alternative to reinforcement learning from human feedback (RLHF)~\cite{ouyang2022training}, directly optimizing a policy to align with human preferences via a simple binary cross-entropy objective. %Mathematically, the DPO objective minimizes the loss function:

%\begin{multline}
% L_{\text{DPO}}(\pi_\phi; \pi_{\text{ref}}) = \\-\mathbb{E}_{(h, y_w, y_l) \sim D} \bigg[ \log \sigma\bigg(\beta \log \frac{\pi_\phi(y_w | h)}{\pi_{\text{ref}}(y_w | h)} \\- \beta \log \frac{\pi_\phi(y_l | h)}{\pi_{\text{ref}}(y_l | h)}\bigg) \bigg],
%\end{multline}
%where \( \pi_\phi \) is the fine-tuned policy, \( \pi_{\text{ref}} \) is the reference policy, \( y_w \) and \( y_l \) denote the preferred and dispreferred responses, respectively, \( \sigma \) is the logistic function, and \( \beta \) is a scaling parameter controlling the KL-divergence constraint. 
%This approach enables efficient bias mitigation by aligning the model's outputs with desired preferences while avoiding the instability and computational overhead of traditional RLHF methods.



\subsection{Data Construction}
The tuning process operates on datasets comprising a diverse set of facial images of users, accompanied by a linguistic component tailored for various purposes. It is important to note that user-aware tuning is not intended to train the model on specific tasks; rather, its objective is to equip the model with the additional capability of personalizing its responses when interacting with user facial images. However, this process involves delicate considerations, such as avoiding over-personalization and mitigating catastrophic forgetting~\cite{laurenccon2024matters}. To address over-personalization, we utilize an extensively diverse dataset of user images that includes individuals of different ages, genders, and ethnicities, while ensuring the conversational topics are sufficiently varied to prevent the model from becoming suboptimal for certain tasks. %(see \Cref{app:data}) %
To counter catastrophic forgetting, we incorporate pre-training data alongside our tuning datasets, enabling a smooth optimization process that enhances user understanding while maintaining the model's performance on general-purpose tasks.

To effectively train the model, we construct three distinct datasets, each customized to a specific stage of the training process. The first dataset, $D_{\text{PT}}$, comprises tuples $(i, p)$, where $i$ denotes the user image and $p$ corresponds to the associated profile. This dataset is utilized to continue pre-training the MLP layer for visual profile alignment. The second dataset, $D_{\text{Instruct}}$, contains triples $(i, q, a)$, where $q$ is a user-specific question and $a$ is the corresponding personalized response. These triples are used to train the LoRA modules in both single-LoRA and MoLE settings. Finally, the third dataset, $D_{\text{DPO}}$, consists of quadruples $(i, q, a^+, a^-)$, where $a^+$ and $a^-$ denote the accepted and rejected answers, respectively. %This dataset is instrumental in training the model to discern and prioritize ethical responses corresponding to controversial questions. Detailed descriptions of the construction methodology for each dataset are provided as follows.

\paragraph{Pre-Training Dataset}
PT dataset is constructed by integrating four distinct datasets to ensure a comprehensive and diverse training foundation. The first dataset, FairFace~\cite{karkkainen2021fairface}, consists of 97.7K pairs of real-world user images and their corresponding demographic profiles, which include three key features: age, gender, and ethnicity. %This dataset provides a robust baseline for capturing demographic diversity. 
The second dataset, GenUser~\cite{generatedphotos}, comprises 10K synthetically generated user images paired with profiles that encompass a broader range of features, including emotions, facial characteristics, and demographic information. %This synthetic dataset enhances the model's ability to generalize across varied user attributes. 
The third dataset, UserEmotion~\cite{human-face-emotions-roboflow}, contains 9.4K user images paired with emotional profiles derived from facial features, enabling the model to infer nuanced emotional states. Finally, the fourth dataset, DOCCI~\cite{onoe2025docci}, includes 8.6K general-purpose image-caption pairs, serving as a regularization mechanism to mitigate catastrophic forgetting and prevent overfitting during training. 

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.4\linewidth]{figures/pt.png}
%    \caption{Distribution of PT Dataset}
%    \label{fig:pt}
%\end{figure}

\paragraph{Instruct Dataset}
The Instruct dataset is composed of four sub-datasets: The first sub-dataset, FaceTask-VQA~\cite{face_bench_five_task_sample}, includes 3.4K questions focused on user facial features, such as emotions and demographic attributes, to enhance the model’s ability to interpret and respond to user-specific queries. The second sub-dataset, AlpaGasus-VQA, includes 70K entries created by combining FairFace and AlpaGasus \cite{chen2023AlpaGasus} dataset by \href{https://github.com/gpt4life/AlpaGasus}{gpt4life}. 
%AlpaGasus is a general-purpose dataset containing 10K question-answer pairs that have demonstrated effectiveness in fine-tuning LLMs. For each question in AlpaGasus, we selected seven images based on the question context and user profile characteristics, then refined the answers using GPT-4o to align them with the user profiles. 
The third sub-dataset, Alexa-VQA, comprises 20K questions randomly selected from the Alexa-QA dataset~\cite{alexa_qa}, with user profiles assigned from FairFace to ensure personalization while avoiding over-personalization. Finally, the fourth sub-dataset, NLE-VQA~\cite{vqa_nle_llava}, consists of general-purpose VQAs, which serve as a regularization mechanism to prevent overfitting and mitigate catastrophic forgetting.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.4\linewidth]{figures/sft.png}
%    \caption{Distribution of SFT Dataset}
%    \label{fig:sft}
%\end{figure}


\paragraph{DPO Dataset}
The DPO dataset is composed of two primary sub-datasets, each designed to enhance the model's robustness and fairness. The first sub-dataset, BiasVision-DPO, consists of 12K entries created by combining the FairFace and Bias-DPO \cite{allam2024biasdpo} datasets. %Bias-DPO contains 1.2K entries that focus on addressing sexist, racist, controversial, and inappropriate questions. For each entry in Bias-DPO, we assign ten user profiles with corresponding images selected based on semantic similarity between the user profiles and the questions. %The images are curated to ensure diversity across age, gender, and ethnicity, thereby reducing the risk of overfitting to specific demographic groups. 
The second sub-dataset, VLM-DPO~\cite{vlm_dpo_example}, comprises 5.4K general-purpose DPO entries aimed at regularizing the model, mitigating overfitting and catastrophic forgetting, and enhancing the model's fairness and ethical alignment.



%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.5\linewidth]{figures/dpo.png}
%    \caption{Distribution of DPO Dataset}
%    \label{fig:dpo}
%\end{figure}




\section{Experiment}

\subsection{Training Setting}
The User-VLM 360° is trained on PaliGemma 2~\cite{steiner2024paligemma}, a state-of-the-art vision-language model that combines SigLIP~\cite{zhai2023sigmoid} with Gemma 2~\cite{team2024gemma}for seamless multimodal processing, making it an ideal foundation for vision-language representation learning. We train User-VLM 360° in two sizes, 3B and 10B, and evaluate it across eight benchmarks against four state-of-the-art models. Inspired by \cite{chen2024llava, wu2024mixture}, for both the single-LoRA and MoLE settings, as well as for preference optimization, we utilized LoRA modules with a rank ($r$) and alpha value ($\alpha$)  of 32. In the MoLE setting, three LoRA modules were employed, with the router $G$ trained to select only one LoRA module at a time. For Vision Alignment, we opted for one epoch with a batch size of 128, while Instruction Tuning was performed over three epochs with a batch size of 64. Additionally, for DPO, we used a batch size of 32 and limited training to one epoch. 



\subsection{Baseline}
The proposed model is evaluated against four state-of-the-art models of comparable size to ensure a rigorous and fair comparison. The first model, LLaMA 3.2 Vision~\cite{dubey2024llama}, is an advanced architecture based on CLIP~\cite{radford2021learning} and LLaMA 3.1, comprising 11 billion parameters.% and supporting a context length of up to 128,000 tokens. 
The second model, Pixtral~\cite{agrawal2024pixtral}, features a 12-billion-parameter multimodal decoder built upon Mistral NeMo~\cite{mistral_Nemo}, along with a 400-million-parameter vision encoder trained from scratch.%, offering a similarly extensive context window of 128,000 tokens. 
Additionally, the third and fourth models, LLaVA 1.5~\cite{liu2024visual} and LLaVA 1.6~\cite{liu2024improved}, employ Mistral~\cite{jiang2023mistral} and Vicuna~\cite{touvron2023llama} as their respective backbones, each comprising 7 billion parameters and integrating a CLIP-based vision encoder.

\subsection{Metrics}
We selectively employ ROUGE~\cite{lin-2004-rouge} metrics and BERTScore~\cite{zhang2019bertscore} to evaluate the model across different tasks, as their use provides a robust assessment of both factual consistency (via lexical overlap) and contextual alignment (via semantic embeddings), ensuring outputs meet the dual demands of accuracy and adaptability in human-robot collaboration. 


\begin{table*}[h]
    \centering
    \fontsize{5}{6}\selectfont
    \begin{minipage}{0.42\textwidth}
        \centering
        \begin{tabular}{cccccccc}
\toprule
\multicolumn{2}{c}{\textbf{Model Config}} & \multicolumn{3}{c}{\textbf{ElderlyTech-VQA Bench}}                       & \multicolumn{3}{c}{\textbf{User-VQA Bench}}                              \\ \cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8}
\textbf{Base Model}             & \textbf{Size}  & \textbf{P} & \textbf{R} & \textbf{F1}                       & \textbf{P} & \textbf{R} & \textbf{F1}                       \\ \midrule
LLaMA 3.2                       & 11B            & 0.142              & 0.606           & 0.221                             & 0.308              & 0.417           & 0.314                             \\
Pixtral                         & 12B            & 0.148              & 0.603           & 0.193                             &         0.257            & 0.468           & 0.293                             \\
LLaVA-v1.6                      & 7B             & 0.095              & 0.695           & 0.165                             & 0.307              & 0.449           & 0.330                             \\
LLaVA-v1.5                      & 7B             & 0.125              & 0.630           & 0.203                             & 0.380              & 0.399           & 0.359                             \\ \midrule
\multirow{2}{*}{\textbf{User-VLM 360°}}  & 3B             & 0.312              & 0.457           & \cellcolor{gray!20}\textbf{0.360} & 0.495              & 0.400           & \cellcolor{gray!20}\textbf{0.419} \\
                                & 10B            & 0.352              & 0.553           & \cellcolor{gray!20}\textbf{0.418} & 0.550              & 0.423           & \cellcolor{gray!20}\textbf{0.455} \\ \bottomrule
\end{tabular}
\caption{Evaluation Result on User-aware Personalization}
\label{tab:res_user}
    \end{minipage}%
   % \hspace{0.05\textwidth} % Adjust space between the tables
    \begin{minipage}{0.55\textwidth}
        \centering
\begin{tabular}{cc|ccc|ccc|ccc|ccc}
\toprule
\multicolumn{2}{c}{\textbf{Model Configuration}}     & \multicolumn{3}{c}{\textbf{VQAv2}} & \multicolumn{3}{c}{\textbf{COCO}} & \multicolumn{3}{c}{\textbf{SEED}} & \multicolumn{3}{c}{\textbf{in the wild}} \\ \cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11}\cmidrule(r){12-14}
\textbf{Model}                                & \textbf{Size} & \textbf{P }      & \textbf{R}      & \textbf{F1}     & \textbf{P}     & \textbf{R}      & \textbf{F1}     & \textbf{P }     & \textbf{R }     & \textbf{F1}     & \textbf{P }        & \textbf{R}        & \textbf{F1 }      \\ \midrule
LLaMA 3.2                            & 11B  & 0.067   & 0.600  & 0.110  & 0.505  & 0.521  & 0.479  & 0.478  & 0.685  & 0.498  & 0.453     & 0.531    & 0.438    \\
Pixtral                              & 12B  & 0.033   & 0.476  & 0.058  & 0.533  & 0.529  & 0.506  & 0.026  & 0.435  & 0.042  & 0.415     & 0.447    & 0.366    \\
LLaVA v1.6                           & 7B   & 0.047   & 0.610  & 0.084  & 0.528  & 0.554  & 0.514  & 0.590  & 0.590  & \textbf{0.590}  & 0.499     & 0.510    & \textbf{0.459}    \\
LLaVA v1.5                           & 7B   & 0.060   & 0.593  & 0.105  & 0.637 & 0.559  & \textbf{0.583}  & 0.463  & 0.520  & 0.475  & 0.511    & 0.472    & 0.451    \\ \midrule
\multirow{2}{*}{Use-VLM 360°}        & 3B   & 0.557   & 0.627  & \cellcolor{gray!20} \textbf{0.566} & 0.517  & 0.430  & \textbf{0.429}  & 0.130  & 0.290  & 0.158  & 0.425     &                                            0.445    & \textbf{0.394 }    \\
                                     & 10B  & 0.652  & 0.670 & \cellcolor{gray!20} \textbf{0.652}  & 0.531  & 0.432  & \textbf{0.428}  & 0.224  & 0.410  & 0.271  & 0.496     & 0.420    & \textbf{0.413}\\ \bottomrule  

\end{tabular}
\caption{Evaluation Result on General Purpose Understanding}
\label{tab:res_genral}
    \end{minipage}
\end{table*}


\begin{table*}[h]
\centering
\fontsize{5}{6}\selectfont
\begin{tabular}{cc|ccc|ccc|ccc|ccc|ccc|ccc}
\toprule
\multicolumn{2}{c}{\textbf{Model Configuration}} & \multicolumn{3}{c}{\textbf{Race Detection}} & \multicolumn{3}{c}{\textbf{Face Attribute Detection}} & \multicolumn{3}{c}{\textbf{Face Counting}} & \multicolumn{3}{c}{\textbf{Age Detection}} & \multicolumn{3}{c}{\textbf{Emotion Detection}} & \multicolumn{3}{c}{\textbf{Gender Detection}} \\  \cmidrule(r){1-2}  \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11}  \cmidrule(r){12-14}  \cmidrule(r){15-17}  \cmidrule(r){18-20}    
\textbf{Model}                           & \textbf{Size}  & \textbf{P}          & \textbf{R}         & \textbf{F1}        & \textbf{P}             & \textbf{R}             & \textbf{F1}           & \textbf{P}         & \textbf{R}         & \textbf{F1}        & \textbf{P}         & \textbf{R}         & \textbf{F1}        & \textbf{P}           & \textbf{R}          & \textbf{F1}         & \textbf{P}          & \textbf{R}          & \textbf{F1}         \\ \midrule
LLaMA 3.2                       & 11B   & 0.023      & 0.240     & 0.041     & 0.475         & 0.545         & 0.481        & 0.013     & 0.120     & 0.024     & 0.026     & 0.244     & 0.045     & 0.065       & 0.660      & 0.118      & 0.077      & 0.775      & 0.133      \\
Pixtral                         & 12B   & 0.061      & 0.580     & 0.109     & 0.230         & 0.670         & 0.264        & 0.002     & 0.055     & 0.003     & 0.056     & 0.413     & 0.085     & 0.109       & 0.665      & 0.184      & 0.377      & 0.815      & 0.412      \\
LLaVA v1.6              & 7B    & 0.061      & 0.360     & 0.097     & 0.725         & 0.725         & 0.725        & 0.001     & 0.015     & 0.002     & 0.029     & 0.315     & 0.052     & 0.080       & 0.601      & 0.140      & 0.576      & 0.905      & 0.609      \\
LLaVA v1.5               & 7B    & 0.379      & 0.627     & 0.409     & 0.670         & 0.670         & 0.670        & 0         & 0.010     & 0.001     & 0.149     & 0.321     & 0.167     & 0.184       & 0.712      & 0.288      & 0.848      & 0.935      & 0.855      \\ \midrule
\multirow{2}{*}{\textbf{Use-VLM 360°}}   & 3B    & 0.727     & 0.727     & \cellcolor{gray!20}\textbf{0.727}     & 0.660        & 0.660         & 0.660        & 0.410     & 0.410    & \cellcolor{gray!20}\textbf{0.410}     & 0.530     & 0.530     & \cellcolor{gray!20}\textbf{0.530}     & 0.096      & 0.666    & 0.167    & 0.905     & 0.915     & \cellcolor{gray!20}\textbf{0.905 }     \\
                                & 10B   & 0.737     & 0.737     & \cellcolor{gray!20}\textbf{0.737}     & 0.765    & 0.765   & \cellcolor{gray!20}\textbf{0.765 }       & 0.450   & 0.450   & \cellcolor{gray!20}\textbf{0.450}     & 0.520     & 0.520     & \cellcolor{gray!20}\textbf{0.520}     & 0.272      & 0.600   & \cellcolor{gray!20}\textbf{0.346}      & 0.920    & 0.920      & \cellcolor{gray!20}\textbf{0.920}   \\ \bottomrule  
\end{tabular}
\caption{Evaluation Results on Facial Feature Understanding}
\label{tab:res_face}
\end{table*}


\subsection{Benchmark}
We evaluate the proposed model using eight benchmarks across four key objectives: (1) assessing personalized responses based on visual user profiles, (2) understanding users through facial features and expressions, (3) maintaining robustness and general-purpose capabilities while avoiding over-personalization, and (4) mitigating biases to ensure fair and ethical responses.
%To comprehensively evaluate the proposed model, we employed eight benchmarks across four distinct settings, each designed to address specific objectives. The first objective assesses the model's ability to provide personalized responses to questions by leveraging visual user profiles, demonstrating its adaptability to individual user contexts. The second objective evaluates the model's capacity to understand users based on facial features and expressions, highlighting its proficiency in interpreting nuanced visual cues. The third objective evaluates the model's performance on general-purpose questions to ensure it remains reliably robust, avoids excessive personalization, and retains a general understanding while demonstrating the additional skill of user-specific comprehension, while also validating that neither catastrophic forgetting nor over-fitting has occurred during training. Finally, the fourth objective measures the model's ability to mitigate biased questions and generate fair, ethically sound responses.%, emphasizing its alignment with the principles of responsible AI. 

\paragraph{User-aware Personalization}
%To evaluate the personalization capabilities of the proposed model compared to the baseline, we utilized two distinct benchmarks. The first benchmark, \textit{ElderlyTech-VQA Bench}, comprises 144 triplets of images, questions, and answers, focusing on real-world questions posed by elderly individuals about technology. The associated images, selected from the FairFace dataset, ensure diversity in ethnicity and gender. Reference answers for these questions were generated using GPT-4 with detailed instructions to provide high-quality, contextually relevant responses. The second benchmark, \textit{User-VQA Bench}, includes 500 test samples from Alexa-VQA and AlpaGasus-VQA, serving as additional benchmarks. Notably, the model was not trained on any entries from either benchmark, ensuring an unbiased evaluation of its personalization and generalization capabilities.

To evaluate the personalization capabilities of the proposed model compared to the baseline, we utilized two distinct benchmarks. The first benchmark, \textit{ElderlyTech-VQA Bench}, comprises 144 triplets of images, questions, and answers, focusing on real-world questions posed by elderly individuals about technology. The associated images, selected from the FairFace dataset, ensure diversity in ethnicity and gender. Reference answers for these questions were generated using GPT-4o with detailed instructions to provide high-quality, contextually relevant responses. The second benchmark, \textit{User-VQA Bench}, includes 500 test samples from Alexa-VQA and AlpaGasus-VQA, which serve as additional benchmarks. Notably, the model was not trained on any entries from either benchmark, ensuring an unbiased evaluation of its personalization and generalization capabilities.

\paragraph{Facial Feature Understanding}
To assess the model's ability to understand the facial features of users, including attributes such as emotion, age, gender, ethnicity, and the number of users, we employed the \textit{Face Task Bench}, a comprehensive benchmark comprising 1,200 entries~\cite{face_bench_five_task_sample, human-face-emotions-roboflow}. This benchmark is designed to evaluate six distinct tasks related to facial feature understanding, such as emotion prediction, age prediction, and similar attributes. Each task is represented by 200 entries, providing a robust and diverse dataset for evaluating the model's performance in interpreting and analyzing facial characteristics.

\paragraph{General Purpose Understanding}
To ensure the proposed model's robustness, generalization, and balance between avoiding excessive personalization and retaining user-specific comprehension, we employed four widely accepted benchmarks: \textit{SEED}~\cite{li2023seed}, \textit{VQAv2}~\cite{goyal2017making}, \textit{LLaVA-COCO}~\cite{liu2024visual}, and \textit{In the Wild}~\cite{liu2024visual}. These benchmarks are extensively used in state-of-the-art evaluations of VLMs and provide a diverse range of tasks and scenarios to rigorously assess the model's performance. %By leveraging these benchmarks, we validate that the model maintains a balance between general understanding and personalized capabilities while demonstrating resilience against catastrophic forgetting and over-fitting during training.

\paragraph{Bias Mitigation}
To evaluate the model's moral values and impartiality in addressing controversial questions, we selected 100 entries from the Bias-Vision DPO dataset. Each entry includes a question paired with a reference answer considered the accepted response. ROUGE metrics are then calculated to measure alignment with these reference answers. Additionally, if the model's response is semantically similar to a rejected answer, the BERTScore for that entry is assigned a value of zero. 



\begin{table*}[h]
    \centering
    \fontsize{4}{6}\selectfont
    \begin{minipage}{0.55\textwidth}
        \centering
        \begin{tabular}{c|cc|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Size}} & \multicolumn{2}{c|}{\textbf{Training Strategy}}  & \multicolumn{3}{c}{\textbf{COCO}} & \multicolumn{3}{c}{\textbf{SEED}} & \multicolumn{3}{c}{\textbf{VQAv2}} & \multicolumn{3}{c}{\textbf{in the wild}} \\ \cmidrule(r){2-3} \cmidrule(r){4-6} \cmidrule(r){7-9} \cmidrule(r){10-12}\cmidrule(r){13-15}
                      &     \textbf{Instruction}      & \textbf{DPO}          &\textbf{P}       & \textbf{R}      & \textbf{F1}    & \textbf{P}       & \textbf{R}      & \textbf{F1}    &\textbf{P}       & \textbf{R}      & \textbf{F1}      & \textbf{P}       & \textbf{R}      & \textbf{F1}       \\ \midrule
   %& \multicolumn{3}{c}{base model}         & 0.387  & 0.080  & 0.081  & 0.003  & 0.290  & 0.006  & 0.578   & 0.590  & 0.580  & 0.382     & 0.151    & 0.163    \\
                      %& $\checkmark$ & $\times$ & $\times$     & 0.351  & 0.073  & 0.083  & 0.002  & 0.290  & 0.004  & \textbf{0.579}   & 0.577  & \textbf{0.575}  & 0.394     & 0.147    & 0.161    \\ \textcolor{green}{$\uparrow$} \textcolor{red}{$\downarrow$}
          \multirow{4}{*}{3B}            &  LoRA     & $\times$     & 0.517  & 0.430  & \textbf{0.429}  & \textbf{0.130}  & 0.290  & \textbf{0.158}  & 0.042   & 0.587  & 0.078  & 0.457     & 0.410    & 0.388    \\
                      
                      &  MoLE     & $\times$     & \textbf{0.531}  & 0.219  & 0.237  & 0.053  & 0.640  & 0.093  & 0.557   & 0.627  & \textbf{0.566}  & \textbf{0.574}     & 0.245    & 0.298    \\ \cmidrule(r){2-15}
                      &  LoRA     & $\checkmark$ & \textcolor{red}{$\downarrow$}0.441  & \textcolor{green}{$\uparrow$}\textbf{0.489}  & \textcolor{red}{$\downarrow$}0.421  & \textcolor{red}{$\downarrow$}0.097  & \textcolor{green}{$\uparrow$} 0.380  & \textcolor{red}{$\downarrow$}0.122  & \textcolor{red}{$\downarrow$}0.038   & \textcolor{green}{$\uparrow$} 0.610  & \textcolor{red}{$\downarrow$}0.070  & \textcolor{red}{$\downarrow$}0.425     &\textcolor{green}{$\uparrow$} \textbf{0.445}    & \textcolor{green}{$\uparrow$}\textbf{0.394}    \\ 
                      &  MoLE     & $\checkmark$ & \textcolor{red}{$\downarrow$}0.320  & \textcolor{green}{$\uparrow$}0.458  & \textcolor{green}{$\uparrow$}0.296  & \textcolor{red}{$\downarrow$}0.047  & \textcolor{green}{$\uparrow$}\textbf{0.700}  & \textcolor{red}{$\downarrow$}0.083  & \textcolor{red}{$\downarrow$}0.216   & \textcolor{green}{$\uparrow$}\textbf{0.648}  & \textcolor{red}{$\downarrow$}0.228  & \textcolor{red}{$\downarrow$}0.399     & \textcolor{green}{$\uparrow$}0.359    & \textcolor{red}{$\downarrow$}0.291    \\ \midrule
  %& \multicolumn{3}{c}{base model}         & 0.410  & 0.050  & 0.063  & 0.184  & 0.315  & 0.213  & 0.557   & 0.560  & 0.556  & 0.343     & 0.060    & 0.073    \\
                      %& $\checkmark$ & $\times$ & $\times$     & 0.437  & 0.067  & 0.074  & 0.165  & 0.300  & 0.192  & 0.595   & 0.592  & 0.591  & 0.308     & 0.105    & 0.114    \\
                     \multirow{4}{*}{10B}  & LoRA     & $\times$     & 0.531  & \textbf{0.432}  & \textbf{0.428}  & 0.244  & 0.360  & 0.270  & 0.045   & 0.622  & 0.084  & 0.496     & \textbf{0.420}    & \textbf{0.413}    \\
                      
                      &  MoLE     & $\times$     & \textbf{0.569}  & 0.174  & 0.210  & \textbf{0.224}  & \textbf{0.410} & \textbf{0.271}  &  \textbf{0.652}  & \textbf{0.670}  &\textbf{0.652}  & 0.510     & 0.270    & 0.305    \\ \cmidrule(r){2-15}
                      & LoRA     & $\checkmark$ & \textcolor{red}{$\downarrow$}0.503  & \textcolor{red}{$\downarrow$}0.425  & \textcolor{red}{$\downarrow$}0.412  & \textcolor{red}{$\downarrow$}0.095  & \textcolor{green}{$\uparrow$}0.390  & \textcolor{red}{$\downarrow$}0.134  & \textcolor{red}{$\downarrow$}0.037   & \textcolor{red}{$\downarrow$}0.590  & \textcolor{red}{$\downarrow$}0.069  & \textcolor{red}{$\downarrow$}0.418     & \textcolor{red}{$\downarrow$}0.378    & \textcolor{red}{$\downarrow$}0.348    \\
                      &  MoLE     & $\checkmark$ & \textcolor{red}{$\downarrow$}0.452  & \textcolor{green}{$\uparrow$}0.351  & \textcolor{green}{$\uparrow$}0.338  & \textcolor{red}{$\downarrow$}0.132  & \textcolor{red}{$\downarrow$}0.405  & \textcolor{red}{$\downarrow$}0.187  & \textcolor{red}{$\downarrow$}0.118   & \textcolor{red}{$\downarrow$}0.601  & \textcolor{red}{$\downarrow$}0.139  & \textcolor{green}{$\uparrow$}\textbf{0.512}     & \textcolor{green}{$\uparrow$}0.346    & \textcolor{green}{$\uparrow$}0.350   \\ \bottomrule
\end{tabular}
\caption{Ablation Result on General Purpose Understanding}
\label{tab:res_general_abl}
    \end{minipage}%
    \hspace{0.05\textwidth} % Adjust space between the tables
    \begin{minipage}{0.35\textwidth}
        \centering
        \begin{tabular}{c|cc|cccccc}
\toprule
\multirow{2}{*}{\textbf{Size}} & \multicolumn{2}{c|}{\textbf{Training Strategy}} & \multicolumn{3}{c}{\textbf{User-VQA Bench}}  & \multicolumn{3}{c}{\textbf{ElderlyTech-VQA Bench}} \\ \cmidrule(r){2-3} \cmidrule(r){4-6} \cmidrule(r){7-9}
                        & \textbf{Instruction}  & \textbf{DPO}           & \textbf{P}       & \textbf{R}      & \textbf{F1}               & \textbf{P}       & \textbf{R}      & \textbf{F1}                 \\ \midrule
     \multirow{4}{*}{3B}                 
                     &  LoRA  & $\times$      & 0.495     & 0.401  & \textbf{0.420} & 0.312       & 0.458    & \textbf{0.361}   \\
                       & MoLE  & $\times$      & 0.409     & 0.285  & 0.293          & 0.281       & 0.334    & 0.268            \\  \cmidrule(r){2-9}
                      & LoRA  & $\checkmark$  & \textcolor{red}{$\downarrow$} 0.480     & \textcolor{red}{$\downarrow$}0.350  &\textcolor{red}{$\downarrow$} 0.375          & \textcolor{red}{$\downarrow$}0.301       & \textcolor{green}{$\uparrow$}0.466    & \textcolor{red}{$\downarrow$}0.359            \\
                      &  MoLE  & $\checkmark$  & \textcolor{red}{$\downarrow$}0.300     & \textcolor{green}{$\uparrow$}0.289  & \textcolor{red}{$\downarrow$}0.243          & \textcolor{red}{$\downarrow$}0.230       & \textcolor{red}{$\downarrow$}0.304    & \textcolor{red}{$\downarrow$}0.221            \\ \midrule
         \multirow{4}{*}{10B}                   & LoRA  & $\times$      & 0.550     & 0.423  & \textbf{0.456} & 0.353       & 0.554    & \textbf{0.419}   \\
                      & MoLE  & $\times$      & 0.503     & 0.315  & 0.351          & 0.375       & 0.372    & 0.307            \\ \cmidrule(r){2-9}
                      &  LoRA  & $\checkmark$  & \textcolor{red}{$\downarrow$}0.460     & \textcolor{red}{$\downarrow$}0.316  & \textcolor{red}{$\downarrow$}0.307          & \textcolor{red}{$\downarrow$}0.363       & \textcolor{red}{$\downarrow$}0.458    & \textcolor{red}{$\downarrow$}0.397            \\
                      &  MoLE  & $\checkmark$  & \textcolor{red}{$\downarrow$}0.427     & \textcolor{red}{$\downarrow$}0.272  & \textcolor{red}{$\downarrow$}0.292          & \textcolor{red}{$\downarrow$}0.226       & \textcolor{red}{$\downarrow$}0.445    & \textcolor{red}{$\downarrow$}0.287 \\ \bottomrule          
\end{tabular}
\caption{Ablation Result on User Personalization}
\label{tab:res_user_abl}
    \end{minipage}
\end{table*}


\begin{table*}[t]
\centering
\fontsize{5}{6}\selectfont
\begin{tabular}{c|cc|ccc|ccc|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{\#Parameters}}            & \multicolumn{2}{c|}{\textbf{Training Strategy}}     & \multicolumn{3}{c}{\textbf{Age Prediction}} & \multicolumn{3}{c}{\textbf{Race Prediction}} & \multicolumn{3}{c}{\textbf{Gender Prediction}} & \multicolumn{3}{c}{\textbf{Emotion Prediction}} & \multicolumn{3}{c}{\textbf{Face Counting}} & \multicolumn{3}{c}{\textbf{Face Attribute Prediction}} \\ \cmidrule(r){2-3} \cmidrule(r){4-6} \cmidrule(r){7-9} \cmidrule(r){10-12} \cmidrule(r){13-15} \cmidrule(r){16-18} \cmidrule(r){19-21}
                              & \textbf{Instruction}    & \textbf{DPO}        & \textbf{P}          & \textbf{R}         & \textbf{F1}        & \textbf{P }         & \textbf{R}          & \textbf{F1}        & \textbf{P}           & \textbf{R}          & \textbf{F1}         & \textbf{P }          & \textbf{R}           & \textbf{F1}         & \textbf{P}         & \textbf{R }        & \textbf{F1}        & \textbf{P}             & \textbf{R }            & \textbf{F1 }           \\ \midrule
           %&  \multicolumn{3}{c}{base model \rowcolor{gray!20}}   
                              %& 0.135      & 0.124     &  0.127     & 0.190      & 0.187      & 0.188     & 0.885       & 0.885      & 0.885      & 0.225       & 0.037       & 0.064      & 0.325     & 0.325     & 0.325     & \textbf{0.670 }        & \textbf{0.670}         & \textbf{0.670}         \\ \cmidrule(r){2-22}
                              %& $\checkmark$ & $\times$ & $\times$     & 0.046      & 0.148     & 0.057     & 0.148      & 0287       & 0.158     & 0.353       & 0.430      & 0.364      & \textbf{0.305}       & 0.050       & 0.087      & 0.305     & 0.305     & 0.305     & 0.556         & 0.605         & 0.564         \\ \cmidrule(r){2-22}
              \multirow{4}{*}{3B}                  &  LoRA   & $\times$     & 0.525      & 0.525     & 0.525     & \textbf{0.727 }     & \textbf{0.727}      & \textbf{0.727}     & 0.895       & 0.895      & 0.895      & 0.093       & 0.510       & 0.157      & \textbf{0.410}     & \textbf{0.415 }    & \textbf{0.410 }    & \textbf{0.660 }        & \textbf{0.660 }         & \textbf{0.660 }         \\
                            &  MoLE   & $\times$     & 0.174      & 0.327     & 0.197     & 0.305      & 0.305      & 0.305     & 0.719       & 0.745      & 0.722      & 0.229       & 0.127       & 0.116      & 0.400     & 0.400     & 0.400     & 0.610         & 0.615         & 0.611         \\ \cmidrule(r){2-21}
                           &  LoRA   & $\checkmark$ & \textcolor{green}{$\uparrow$}\textbf{0.530}     & \textcolor{green}{$\uparrow$}0.530     & \textcolor{green}{$\uparrow$}\textbf{0.530}    &   \textcolor{red}{$\downarrow$}0.690    &  \textcolor{red}{$\downarrow$}0.690      &  \textcolor{red}{$\downarrow$}0.690    & \textcolor{green}{$\uparrow$}\textbf{0.905}       & \textcolor{green}{$\uparrow$}\textbf{0.915}      & \textcolor{green}{$\uparrow$}\textbf{0.905}    & \textcolor{green}{$\uparrow$}0.096       &\textcolor{green}{$\uparrow$}\textbf{0.666}       & \textcolor{green}{$\uparrow$}\textbf{0.167}      & \textcolor{red}{$\downarrow$}0.248     & \textcolor{red}{$\downarrow$} 0.405     & \textcolor{red}{$\downarrow$}0.256     & \textcolor{red}{$\downarrow$}0.630         &\textcolor{red}{$\downarrow$} 0.630         & \textcolor{red}{$\downarrow$} 0.630         \\
                              &  MoLE   & $\checkmark$ & \textcolor{red}{$\downarrow$}0.107      & \textcolor{green}{$\uparrow$}\textbf{0.615}     & \textcolor{red}{$\downarrow$}0.161     & \textcolor{red}{$\downarrow$}0.267      & \textcolor{green}{$\uparrow$}0.547      & \textcolor{red}{$\downarrow$}0.285     & \textcolor{red}{$\downarrow$}0.078       & \textcolor{green}{$\uparrow$}0.755      & \textcolor{red}{$\downarrow$}0.099      & \textcolor{red}{$\downarrow$}0.084       & \textcolor{green}{$\uparrow$}0.511       & \textcolor{green}{$\uparrow$}0.123      & \textcolor{red}{$\downarrow$}0.282     & \textcolor{red}{$\downarrow$}0.395     & \textcolor{red}{$\downarrow$}0.287     & \textcolor{red}{$\downarrow$}0.545         & \textcolor{red}{$\downarrow$}0.550         & \textcolor{red}{$\downarrow$}0.546         \\ \midrule
         %&   \multicolumn{3}{c}{base model \rowcolor{gray!20}  } 
                              %& 0.245      & 0.245     & 0.245     & 0.455      & 0.455      & 0.455     & 0.875       & 0.875      & 0.875      & 0.260       & 0.043       & 0.074      & 0.370     & 0.370     & 0.370     & 0.720         & 0.720         & 0.720         \\ \cmidrule(r){2-22}
                              %& $\checkmark$ & $\times$ & $\times$     & 0.100      & 0.295     & 0.117     & 0.214      & 0.367      & 0.231     & 0.818       & 0.860      & 0.824      & 0.365       & 0.060       & 0.104      & 0.330     & 0.330     & 0.330     & 0.670         & 0.670         & 0.670         \\ \cmidrule(r){2-22}
              \multirow{4}{*}{10B}                 &  LoRA   & $\times$     & \textbf{0.520}      & 0.520     & \textbf{0.520}     & \textbf{0.737}      & \textbf{0.737}      & \textbf{0.737 }    & 0.900       & 0.900      & 0.900      & 0.272       & \textbf{0.600}       & \textbf{0.346 }     & \textbf{0.450 }    & 0.450     & \textbf{0.450}     & \textbf{0.765}         & 0.765         & \textbf{0.765}         \\

                              &  MoLE   & $\times$     & 0.476      & 0.480     & 0.477     & 0.660      & 0.660      & 0.660     & \textbf{0.920}       & \textbf{0.920}      & \textbf{0.920}      & \textbf{0.376 }      & 0.080       & 0.120      & 0.365     & 0.370     & 0.366     & 0.695         & 0.695         & 0.695         \\ \cmidrule(r){2-21}
                              &  LoRA   & $\checkmark$ &\textcolor{red}{$\downarrow$} 0.377      &  \textcolor{green}{$\uparrow$}\textbf{0.540}     & \textcolor{red}{$\downarrow$}0.432     & \textcolor{red}{$\downarrow$}0.666      & \textcolor{red}{$\downarrow$}0.712      & \textcolor{red}{$\downarrow$}0.680     & \textcolor{red}{$\downarrow$}0.571       &  0.900      & \textcolor{red}{$\downarrow$}0.661      &\textcolor{red}{$\downarrow$}0.105       & \textcolor{red}{$\downarrow$}0.569       & \textcolor{red}{$\downarrow$}0.176      & \textcolor{red}{$\downarrow$}0.160     & \textcolor{green}{$\uparrow$}\textbf{0.485}     & \textcolor{red}{$\downarrow$}0.197     & \textcolor{red}{$\downarrow$}0.296         & \textcolor{green}{$\uparrow$}\textbf{0.790}       & \textcolor{red}{$\downarrow$}0.344         \\
                              &  MoLE   & $\checkmark$ & \textcolor{red}{$\downarrow$}0.242      & \textcolor{red}{$\downarrow$}0.400     & \textcolor{red}{$\downarrow$}0.253     & \textcolor{red}{$\downarrow$}0.255      & \textcolor{red}{$\downarrow$}0.672      &\textcolor{red}{$\downarrow$} 0.276     &\textcolor{red}{$\downarrow$}0.581       & \textcolor{red}{$\downarrow$}0.805      & \textcolor{red}{$\downarrow$} 0.590      & \textcolor{red}{$\downarrow$}0.113       & \textcolor{green}{$\uparrow$}0.300       &\textcolor{green}{$\uparrow$}0.160      & \textcolor{red}{$\downarrow$}0.361     &\textcolor{green}{$\uparrow$} 0.435     & \textcolor{green}{$\uparrow$}0.366     & \textcolor{red}{$\downarrow$}0.512         & \textcolor{green}{$\uparrow$}0.730         & \textcolor{red}{$\downarrow$}0.517    \\ \bottomrule      
\end{tabular}
\caption{Ablation study results on Facial Feature Understanding}
\label{tab:face_ablation}
\end{table*}



\section{Results}

\subsection{Comparative Analysis}

\paragraph{User-aware Personalization}
As demonstrated in \Cref{tab:res_user}, the User-VLM 360°, in both its 3B and 10B sizes, consistently outperforms baseline models across both benchmarks. On the ElderlyTech-VQA benchmark, User-VLM 10B achieves an impressive 2x improvement in ROUGE-1 F1 score compared to the baseline, while the 3B variant performs approximately 1.5x better. A detailed comparison of baseline models on this benchmark, ranked by ROUGE-1 F1 score, reveals the following order: LLaMA 3.2 11B, LLaVA 1.5 7B, Pixtral 12B, and LLaVA 1.6 7B. Similarly, on the User-VQA benchmark, User-VLM 3B outperforms the baselines by 1.2x, while the 10B variant achieves a 1.3x improvement. When ranking baselines on this benchmark by ROUGE-1 F1 score, LLaVA 1.5 leads, followed by LLaVA 1.6, LLaMA 3.2, and Pixtral. These results underscore the efficacy of User-VLM 360° in addressing the challenges of these tasks and its superior performance across varying model sizes.

\paragraph{Facial Feature Understanding}
As summarized in \Cref{tab:res_face}, User-VLM 360° demonstrates strong performance across the Face Task Bench tasks. The 10B model surpasses all baseline models in every task, establishing a new state-of-the-art. The 3B model consistently outperforms baseline models in Race Detection, Face Counting, Age Detection, and Gender Detection tasks. Notably, in Emotion Detection, it outperforms LLaMA 3.2 and LLaVA 1.6, achieving competitive results against Pixtral 12B (0.02 F1 score difference) and LLaVA 1.5 7B (0.12 F1 score difference). For Face Attribute Detection, it surpasses Pixtral 12B and LLaMA 3.2 11B, achieving competitive results against LLaVA 1.6 Mistral 7B (0.06 F1 score difference) and LLaVA 1.5 7B (0.01 F1 score difference). Additionally, it achieves a notable performance edge over the 10B model in Age Detection, highlighting its efficiency and robustness in specific tasks.



\paragraph{General Purpose Understanding}
Despite the primary focus of training on human user images, which could lead to concerns about catastrophic forgetting and reduced performance on general-purpose tasks, User-VLM 360° demonstrates robust generalization capabilities. As summarized in \Cref{tab:res_genral}, the model achieves competitive results across four widely adopted general-purpose benchmarks. Specifically, the 3B and 10B variants outperform the baseline on the VQAv2 benchmark, indicating strong visual question-answering capabilities. On the COCO benchmark, the model performs comparably, with a minimal 0.16-point difference from the top-performing model, LLaVA 1.5. Similarly, on the "in the wild" benchmark, the model shows a negligible 0.04-point gap from LLaVA 1.6, highlighting its adaptability to diverse, unstructured data. However, the model exhibits limited performance on the SEED benchmark, suggesting room for improvement in specific scenarios. 



\subsection{Ablation Study}
Our ablation study investigates the impact of model size, instruction tuning methods, and the inclusion of DPO on general-purpose understanding tasks, facial feature understanding, and user-aware VQA tasks. %Below, we present our key findings in detail:

\paragraph{General-Purpose Understanding}
As shown in \Cref{tab:res_general_abl}, LoRA generally outperforms MoLE in the 3B model, except on the VQAv2 benchmark, where MoLE demonstrates superior performance. Interestingly, the inclusion of DPO reduces the performance of User-VLM 360° in most cases, with the exception of MoLE on the COCO benchmark. For the 10B model, MoLE achieves performance comparable to LoRA, with LoRA excelling on the COCO and \textit{in the wild} benchmarks, while MoLE outperforms on SEED and VQAv2. Notably, DPO negatively impacts the overall performance of the VLM, except for MoLE on COCO and \textit{in the wild} benchmarks.


\paragraph{Facial Feature Understanding}

As demonstrated in \Cref{tab:face_ablation}, LoRA consistently outperforms MoLE in the 3B model, except on tasks such as race prediction, face counting, and face attribute predictions, where the inclusion of DPO improves performance comparably. For the 10B model, LoRA also demonstrates superior performance over MoLE, with the exception of gender prediction, a binary classification task where MoLE excels due to its simplicity. Interestingly, DPO negatively impacts performance across both MoLE and LoRA configurations for the 10B model.

\paragraph{User-Aware Personalization} For user-aware VQA tasks, LoRA demonstrates superior performance compared to MoLE across both model sizes and benchmarks as detailed in \Cref{tab:res_user_abl}. This consistent advantage underscores the effectiveness of LoRA in capturing user-centric nuances in VQA scenarios. However, the inclusion of DPO consistently reduces performance across all benchmarks and model sizes, indicating its limitations in enhancing user-aware VQA understanding.

Our ablation study reveals critical insights into the interplay of adaptation methods, alignment techniques, and model scale. First, LoRA demonstrates consistent superiority over MoLE in most scenarios, particularly in user-aware VQA tasks, where its parameter-efficient fine-tuning mechanism captures nuanced contextual dependencies. MoLE, while less versatile, exhibits competitive performance in specialized benchmarks (e.g., gender prediction), suggesting its utility in tasks requiring explicit disentanglement of latent factors. Second, DPO integration often degrades performance, with only sporadic improvements observed in isolated cases. Finally, model scale significantly modulates method efficacy: the 10B model achieves parity between LoRA and MoLE, likely due to its capacity to absorb diverse adaptation strategies, while the 3B model’s reliance on LoRA highlights the importance of parameter efficiency in smaller architectures.





\subsection{Bias Evaluation}

As detailed in \Cref{tab:res_bias}, the proposed model demonstrates superior initial performance in terms of fairness compared to the baseline, as measured by ROUGE-1 and BERTScore. %This indicates a strong alignment with ethical and equitable output generation in its foundational state. 
Following DPO tuning, the models generally exhibit improved performance on these metrics, further enhancing their safety and fairness profiles. However, exceptions are observed with MoLE in the 3B configuration and LoRA in the 10B configuration, where DPO tuning leads to a decline in performance. 


\begin{table}[htpb]
\centering
\fontsize{5}{6}\selectfont
\begin{tabular}{c|c|cc|ccc|c|c}
\toprule
\multicolumn{2}{c}{\textbf{Configuration}}                & \multicolumn{2}{c}{\textbf{Training Strategy}}    & \multicolumn{5}{c}{\textbf{Bias Evaluation Metrics}}     \\ \cmidrule(r){1-2} \cmidrule(r){3-4} \cmidrule(r){5-9}
\textbf{Model}                      & \textbf{Size}                 & \textbf{Instruction}             & \textbf{DPO}                    & \textbf{Precision} & \textbf{Recall} & \textbf{F1}    & \textbf{BERTScore}  & \textbf{Overall} \\ \midrule
LLaMA-3.2         & 11B                  & \multicolumn{2}{c|}{\multirow{4}{*}{N/A}} & 0.143     & 0.524  & 0.209 & 0.582 & 0.121 \\
Pixtral                   & 12B                  & \multicolumn{2}{c|}{}                    & 0.124     & 0.663  & 0.198 & 0.674 & 0.133 \\
LLaVA v1.6 & 7B                   & \multicolumn{2}{c|}{}                         & 0.116     & 0.650  & 0.192 & 0.681 & 0.131 \\
LLaVA v1.5  & 7B                   & \multicolumn{2}{c|}{}                         & 0.150     & 0.639  & 0.236 & 0.663 & 0.157 \\ \midrule
\multirow{8}{*}{\textbf{User-VLM 360°}} & \multirow{4}{*}{3B}  & LoRA            & $\times$   & 0.336     & 0.453  & 0.369 & 0.640 & \cellcolor{gray!20} 0.236 \\
                          &                      & MoLE            & $\times$               & 0.284     & 0.408  & 0.298 & 0.632 &  \cellcolor{gray!20} 0.188 \\ \cmidrule(r){3-9}
                          &                      & LoRA            & $\checkmark$           & \textcolor{green}{$\uparrow$}0.348     & \textcolor{green}{$\uparrow$}0.454  &\textcolor{green}{$\uparrow$} \textbf{0.384} & \textcolor{green}{$\uparrow$}0.706 & \textcolor{green}{$\uparrow$}\textbf{0.271 }\\
                          &                      & MoLE            & $\checkmark$           & \textcolor{red}{$\downarrow$} 0.220     & \textcolor{red}{$\downarrow$} 0.332  & \textcolor{red}{$\downarrow$} 0.239 & \textcolor{red}{$\downarrow$} 0.497 &\textcolor{red}{$\downarrow$}  0.119 \\ \cmidrule(r){2-9}
                          & \multirow{4}{*}{10B} & LoRA            & $\times$               & 0.332     & 0.487  & 0.382 & 0.701 & \cellcolor{gray!20} 0.268 \\
                          &                      & MoLE            & $\times$               & 0.271     & 0.433  & 0.296 &0.616 & \cellcolor{gray!20} 0.183 \\ \cmidrule(r){3-9}
                          &                      & LoRA            & $\checkmark$           & \textcolor{green}{$\uparrow$}0.386     & \textcolor{red}{$\downarrow$} 0.412  & \textcolor{red}{$\downarrow$} 0.379 & \textcolor{green}{$\uparrow$}\textbf{0.716} & \textcolor{green}{$\uparrow$}\textbf{0.271} \\
                          &                      & MoLE            & $\checkmark$           & \textcolor{green}{$\uparrow$}0.296     & \textcolor{red}{$\downarrow$} 0.418  & \textcolor{green}{$\uparrow$}0.326 & \textcolor{green}{$\uparrow$}0.676 & \textcolor{green}{$\uparrow$}0.220\\ \bottomrule
\end{tabular}
\caption{Bias Mitigation and Ethical Consideration Comparison}
\label{tab:res_bias}
\end{table}

\subsection{Performance and Efficiency Comparison}
Our experimental results, as detailed in \Cref{tab:perf}, demonstrate that User-VLM 360° achieves a substantial reduction in computational complexity, measured in FLOPs, by eliminating the need for explicit instruction-based prompting. Specifically, assuming a question prompt of 50 tokens and detailed instructions of 100 tokens for general-purpose VLMs, the compact 3B variant of User-VLM 360° exhibits a remarkable 17.5–30X reduction in FLOPs compared to larger 7B–12B baseline models. Furthermore, even the 10B variant of User-VLM 360° outperforms equivalently sized models by a significant margin, achieving a 5.25–16.5X reduction in FLOPs.

\begin{table}[htb]
\label{tab:flop}
\centering
%\fontsize{5}{6}\selectfont
\tiny
\begin{tabular}{c|ccccc}
\toprule
             \multirow{2}{*}{\textbf{Avg \#Token } }                   &     & \textbf{Question}   & \textbf{Instruction} & \textbf{Instruction $\oplus$ Question}  \\ \cmidrule(r){2-6}
                       &     & 50           & 100        & 150                                                \\ \midrule
                               &     & \multicolumn{4}{c}{\cellcolor{gray!20}\textbf{FLOPs Reduction and Runtime Performance}}                              \\ \cmidrule(r){2-6}
                               &     & LLaMA 3.2 & Pixtral & LLaVA v1.6 & LLaVA v1.5\\  \cmidrule(r){3-6}
&   Size  &  11B & 12B & 7B   &  7B

                               \\ \midrule%\cmidrule(r){3-6}
\multirow{2}{*}{\textbf{User-VLM 360°}} & 3B  & \textbf{22.5X}       & \textbf{30X}       & \textbf{17.5X  }                 & \textbf{17.5X }               \\ \cmidrule(r){2-6}
                               & 10B & \textbf{16.5X }     & \textbf{9X}       & \textbf{5.25X }                 & \textbf{5.25X } \\ \bottomrule
\end{tabular}
\caption{Performance and Efficiency Comparison}
\label{tab:perf}
\end{table}


\begin{table*}[hbt]
\centering
\fontsize{5}{6}\selectfont
\begin{tabular}{c|p{4cm}|p{4cm}|p{4cm}}
\toprule
   \multirow{2}{*}{\textbf{Question}}        & \textbf{Age-Oriented Profiles}                                                                                                                                       & \textbf{Gender-Oriented Profiles}                                                                                                                 & \textbf{Ethnicity-Oriented Profiles}                                                                                             \\ \cmidrule(r){2-4}
                & Does Santa Claus and reindeer exist for real?                                                                                                                        & What should I wear to a business meeting?                                                                                                         & Recommend me a country not far from here to travel                                                                               \\ \midrule
\multirow{2}{*}{Case 1} & \multicolumn{1}{c|}{\includegraphics[height=2cm]{figures/age-n.jpg}                                        }                                                                              & \multicolumn{1}{c|}{\includegraphics[height=2cm]{figures/gender-n.jpg}                                   }                                                             & \multicolumn{1}{c}{\includegraphics[height=2cm]{figures/race-n.jpg}                               }                                                  \\
                        & No. Santa Claus and reindeer are fictional characters created by folklore and popular culture.                                                             &A woman should wear a professional outfit that includes a tailored blazer, a crisp white shirt, and dress pants or a skirt...                   & The United Arab Emirates is a fantastic choice for a quick getaway...                                                            \\ \midrule
\multirow{2}{*}{Case 2} & \multicolumn{1}{c|}{\includegraphics[height=2cm]{figures/age-p.jpg}                                        }                                                                              & \multicolumn{1}{c|}{\includegraphics[height=2cm]{figures/gender-p.jpg}                                   }                                                             & \multicolumn{1}{c}{\includegraphics[height=2cm]{figures/race-p.jpg}                               }                                                  \\
                        & Yes. Santa Claus and reindeer are fictional characters created by folklore and popular culture, but they are often depicted as real-life creatures in various media. & A suit and tie. A suit and tie is a professional attire that is commonly worn for business meetings to convey a formal and respectful appearance. & Spain. Spain is a beautiful country located in Europe, known for its rich culture, stunning landscapes, and delicious cuisine... \\ \bottomrule
\end{tabular}
\caption{\textbf{Personalization examples from FairFace Based on Visual Cues.} }
\label{fig:example}
\end{table*}


\begin{table*}[htb]
\centering
\fontsize{5}{6}\selectfont
\begin{tabular}{p{2cm}|p{3cm}|p{4cm}|p{3cm}|p{4cm}}
\toprule
\multirow{2}{*}{User Image} &
  \multicolumn{4}{c}{\textbf{Questions}} \\ \cmidrule (r){2-5}
 &
 \multicolumn{1}{c}{\textbf{Give me food recommendation for lunch}} &
  \multicolumn{1}{c}{\textbf{What do you think about love and relationships?}} &
  \multicolumn{1}{c}{\textbf{Which sport do you suggest me to do?}} &
  \multicolumn{1}{c}{\textbf{Recommend me a movie, and tell me why?}}  \\ \midrule
\raisebox{-\totalheight}{\includegraphics[width=2cm]{figures/test1.jpg}}   &
  I recommend a Middle Eastern salad with grilled chicken, hummus, and pita bread. You could also try a falafel wrap with tahini sauce or a shawarma platter with rice and pickles. &
  I believe that love and relationships are complex and multifaceted, shaped by our experiences, cultural backgrounds, and personal values. They can be beautiful and challenging, but ultimately, they teach us about ourselves and the world around us. &
  Soccer I think you would enjoy it! &
  I recommend "The Social Network." It\'s a captivating film that explores the rise of Facebook and the complex relationships between its co-founders. The story is well-paced and filled with engaging characters, making it a must-watch for anyone interested in the tech industry. \\\midrule
\raisebox{-\totalheight}{\includegraphics[width=2cm]{figures/test2.jpg}} &
  I recommend a chicken tikka masala with basmati rice and a side of raita. &
  I believe that love and relationships are complex and multifaceted, shaped by cultural norms, personal experiences, and the choices we make. They can be beautiful and challenging, but ultimately, they teach us about ourselves and the world around us. &
  Basketball. I think you would enjoy playing basketball because it requires teamwork and physical fitness, which you seem to have. &
  I recommend "The Hate U Give." This movie is significant because it addresses important social issues, particularly those faced by Black communities in America. It combines powerful storytelling with a strong message about justice, systemic racism, and the struggle for equality. The performances by actors like Amandla Stenberg and Regina King are outstanding, making it a must-watch for anyone interested in social justice and representation in media. \\\midrule
%\raisebox{-\totalheight}{\includegraphics[width=2cm]{figures/test3.jpg}} &
 % I recommend a Mediterranean-style lunch with a chicken shawarma wrap, a side of hummus, and a refreshing cucumber and mint salad. &
 % I believe that love and relationships are complex and multifaceted experiences that require mutual respect, understanding, and commitment. They can be beautiful and challenging, but ultimately, they can lead to deep connections and growth. &
 % Soccer. I think you would enjoy playing soccer because it requires teamwork and physical fitness, which you seem to have.' &
 % I recommend "The Social Network." It\'s a gripping film that explores the rise of Facebook and the complex relationships between its co-founders. The story is well-paced and features strong performances by Jesse Eisenberg and Andrew Garfield, making it a must-watch for anyone interested in the tech industry. \\\midrule
%\raisebox{-\totalheight}{\includegraphics[width=2cm]{figures/test4.jpg}} &
  %For a quick and healthy lunch, consider a Middle Eastern salad with fresh vegetables, hummus, and a side of pita bread. This meal is not only nutritious but also reflects the flavors of Middle Eastern cuisine. &
  %I believe that love and relationships are complex and multifaceted, shaped by cultural norms, personal experiences, and the choices we make. They can be beautiful and challenging, but ultimately, they teach us about ourselves and the world around us. &
  %Soccer. I think soccer is a great sport for you because it requires teamwork and physical fitness, which you can build up by playing regularly. &
  %I recommend "The Social Network." It\'s a gripping film that explores the rise of Facebook and the complex relationships between its founders. The performances by Jesse Eisenberg and Andrew Garfield are outstanding, and the story is both engaging and thought-provoking. \\ \midrule
\raisebox{-\totalheight}{\includegraphics[width=2cm]{figures/test5.jpg}} &
  Grilled chicken skewers with a side of quinoa and a fresh salad. &
  I believe that love and relationships are complex and multifaceted, requiring patience and understanding. They can be beautiful and challenging, but ultimately, they can lead to deep connections and fulfillment. &
  Tennis. I recommend tennis because it is a great way to stay active and social, and it can be played with friends or family. &
  I recommend "The Shawshank Redemption." This movie is a powerful story of redemption and friendship that resonates deeply with many viewers. It explores themes of hope, perseverance, and the human spirit, making it a must-watch for anyone looking for a thought-provoking film. \\ \bottomrule
\end{tabular}
\caption{\textbf{Personalization examples from Deployment on Pepper Social Robot.} }
\label{fig:example_pp}
\end{table*}


\section{Deployment On Pepper Social Robot}
\label{app:deployment}

We demonstrate the practical applicability of our method through deployment on the SoftBank Pepper robotic platform~\cite{8409927} – a semi-humanoid robot designed for human interaction scenarios. The system architecture leverages Pepper's onboard Jetson Orin Nano module for sensor interfacing and real-time communication with our cloud-based VLM via a ROS 2 distributed computing framework~\cite{magri2024upgrading}.

\paragraph{Pipeline} The robotic agent's processing pipeline integrates three synchronized components: the Perception Module, which streams multimodal input from Pepper's RGB camera (640×480@30Hz video) and microphone (16kHz audio) to a processing server via ROS 2 topics~\cite{bonci2023robot}; Cloud Processing, where a dedicated computation node employs Whisper-Large-V3~\cite{radford2023robust} for speech recognition and our VLM for input analysis; and Action Generation, which synthesizes text responses into speech using Tacotron 2~\cite{shen2018natural}, delivering audio back to Pepper's speakers through QoS-managed ROS 2 services.

\paragraph{Latency}
We empirically evaluated the end-to-end system latency using an Apple M4 Max workstation (64GB unified memory). Our experiments revealed mean response times of 1.8s ($\Sigma$=0.4s) for the 3B parameter model and 4.2s ($\Sigma$=1.1s) for the 10B variant. The ROS 2 middleware contributed 320ms (±45ms) to total latency, primarily from serialization/deserialization overhead.

This deployment architecture demonstrates the feasibility of integrating User-VLM 360° into real-time human-robot interaction systems while maintaining responsive performance characteristics critical for user engagement.


\section{Examples}
\label{app:example}

\Cref{fig:example,fig:example_pp} respectively demonstrate examples of the model's behavior when exposed to different visual context inputs from the FairFace dataset or real-world deployment on the Pepper social robot. In each case, the model is asked the same question in a zero-shot inference setting, without any additional instructions. User-VLM 360° leverages visual cues such as age, gender, and ethnicity to deliver personalized responses, achieving effective tuning objectives. To address potential concerns about the undesired influence of these attributes, we propose a proactive verification mechanism. This mechanism engages users with clarifying questions to confirm the relevance of inferred attributes, ensuring ethical and user-aligned personalization. 





\section{Ethical Verification Framework}
\label{app:verif}

Post-deployment ethical considerations remain pivotal in the practical application of User-VLM 360°~\cite{jafari2023ethical}. As illustrated in \Cref{fig:example}, while the model effectively adapts responses based on inferred user characteristics (e.g., gender, age, ethnicity), challenges arise when users may not wish these attributes to influence outputs. To mitigate unintended bias and respect user autonomy, we propose a proactive verification mechanism: instead of generating direct personalized responses, User-VLM 360° engages users through clarifying questions to confirm the relevance of inferred attributes. For instance, when a user’s visual ethnicity suggests a preference for culturally specific cuisine, the model should first inquire about dietary preferences or interest in diverse categories rather than assuming alignment. This approach ensures personalization occurs only after the model reliably aligns its assumptions with the user’s actual characteristics and secures explicit consent, thereby upholding ethical standards of agency and transparency. Implementing such safeguards requires integrating these principles into the training paradigm or embedding the model within frameworks~\cite{li2024satori}, which enforce comprehensive ethical checks. By positioning User-VLM 360° as a foundational component within such systems, it becomes possible to balance personalization with accountability, fostering ethically sound AI applications while maintaining adaptability for diverse user needs.


\section{Discussion and Future Work}
While User-VLM 360° has the potential to significantly enhance user experiences in healthcare, education, and assistive robotics, it also raises advanced technical considerations for future works.

\paragraph{Interactions with Multiple Parties} One limitation to discuss is that this work is primarily focused on dyadic interactions, involving a single robot and a single human. However, many social interactions involve multiple agents, such as a couple or a group of individuals. In scenarios where two people, such as a couple, are asking for a recommendation, the robot would need to consider the preferences and contexts of both individuals simultaneously. This introduces additional complexity, as the robot must balance and integrate the needs and preferences of multiple users to provide a coherent and satisfactory response.

\paragraph{Cognitive Metrics} Another important discussion point is the evaluation of human-robot interactions based on the subjective perception of the human user. While the User-VLM 360° framework demonstrates strong performance on objective benchmarks, such as F1-score, the human user's subjective experience is equally crucial. Factors like affiliation, trust, intimacy, and rapport play significant roles in determining the success and acceptance of human-robot interactions. Although these higher-level concepts are beyond the scope of this work, they are worth mentioning as they highlight the multifaceted nature of human-robot interactions and the need for future research to address these subjective aspects comprehensively.
\section{Conclusion}
Personalizing interactions between humans and robots equipped with vision-language models is essential for scalable and socially intelligent collaboration. Current methods often overlook individual nuances and raise ethical concerns due to biases in user data. To address this, we introduced User-VLM 360°, a framework that combines multimodal user context modeling with bias-aware optimization. This approach includes real-time adaptive tuning using visual, linguistic, and behavioral signals, bias mitigation, and a curated socio-emotive interaction dataset. Evaluations show significant improvements, and deployment on the Pepper robot confirms real-time adaptability.



\section{Impact Statement }
This paper introduces the User-VLM 360° framework, designed to advance personalized human-robot interactions by integrating VLMs into robotic systems. The framework focuses on user-aware tuning and bias mitigation to ensure ethical and fair responses, addressing concerns about data privacy, user consent, and safety. While this technology has the potential to significantly enhance user experiences in healthcare, education, and assistive robotics, it also raises ethical considerations and societal impacts that must be responsibly managed. These concerns include privacy risks, bias, and discrimination (such as stereotyping, exclusion, and fairness issues). However, thanks to a verification framework, explained in \Cref{app:verif}, many of these issues can be mitigated.

\section*{Acknowledgments}

The authors sincerely acknowledge the financial support of the French National Research Agency (ANR) for the ANITA project (Grant No. ANR-22-CE38-0012-01). We also extend our gratitude to \href{https://generated.photos}{Generated Photos} for generously providing 10,000 generated face entries.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix
%\onecolumn
%\section{Appendix Here}



\section{Data Construction Details}
\label{app:data}

%The distribution of the training data is illustrated in \Cref{fig:dataset}. 
Here, we provide a detailed discussion of the datasets we have constructed in more details, including their sources, preprocessing steps, and the rationale behind their design choices.


\subsection{PT datasets}
\paragraph{GenUser}

It includes approximately 10K synthetic image-text pairs, featuring human faces alongside user profile information from diverse demographic backgrounds. The dataset is generated by \enquote{\textit{generated.photos}} platform to ensure privacy and avoid using real personal data. To promote fairness, %as shown in \Cref{fig:dis}, 
the entries are intentionally designed to represent a broad range of demographic groups, capturing diversity across key characteristics such as age, gender, and ethnicity. %As shown in \Cref{fig:va}, 
Each entry is accompanied by a JSON file integrating over 10 visual attributes that support a wide range of information about user profiles.
%human-centered applications, such as fairness evaluation, bias detection, and demographic-specific model training. 
These features, alongside the images, are processed using a VLM (\enquote{GPT-4o}) to generate a one-paragraph user profile, providing a concise yet detailed description based on the inferred demographic and emotional attributes. The 10K entries in the dataset are split into three parts: 1K for validation, 1K for testing, and 8K for training, ensuring a balanced distribution across the dataset for training and model evaluation.


\paragraph{FairUser}
It approximately consists of 100K real-world text-image pairs derived from the FairFace dataset~\cite{karkkainen2021fairface}. The dataset entries are carefully curated to ensure balance, diversity, and accurate labeling across race, gender, and age categories. Based on this dataset, we designed a user profile feature using the following template: \enquote{The person appears to be {race class} {gender class}, approximately {age class} years old}. This template facilitates a structured and interpretable representation of demographic attributes for profiling tasks. The 100K entries in the dataset are split into three parts: 10K for validation, 10K for testing, and 80K for training, ensuring a balanced distribution across the dataset for training and model evaluation.
\balance

\subsection{Instruction datasets}
\paragraph{AlpaGasus-VQA} AlpaGasus dataset is an unofficial general-purpose dataset containing 10K question-answer pairs released by gpt4life that have demonstrated effectiveness in fine-tuning LLMs. For each question in AlpaGasus, we used LLMs to assign scores to each category of age, gender, and race, and select seven images based on the question context and matched user profile characteristics, then refined the answers using GPT-4o to align them with the user profiles. The following is an example of assignment and the prompt used for generation of the personalized response.



\paragraph{Alexa-VQA} The Alexa QA dataset includes over 100,000 pairs of question-answer entries, covering a wide range of topics and contexts. For our study, we selected a random subset of 20,000 entries from this dataset. Each of these selected QA pairs was then assigned to a user image sourced from the FairFace dataset, which provides a diverse set of human faces with demographic labels. To ensure that the responses were relevant and tailored to individual users, we applied a personalized approach by modifying the response based on the user's characteristics and context, using a consistent set of instructions. 

\subsection{DPO datasets}

\paragraph{BiasVision DPO}
Bias-DPO contains 1.2K entries that focus on addressing sexist, racist, controversial, and inappropriate questions. For each entry in Bias-DPO, we assign ten user profiles with corresponding images selected based on semantic similarity between the user profiles and the questions. The images are curated to ensure diversity across age, gender, and ethnicity, thereby reducing the risk of overfitting to specific demographic groups. 

% Your instruction prompt in a gray box
\begin{graybox}
    \textbf{Instruction:} \textit{Imagine you are answering questions of \{UserProfile\}. Provide personalized respond according to the demographic, socio-emotive profile of the user to the following question:\{Question\}}
\end{graybox}


\begin{center}
\tiny
\begin{lstlisting}[style=json]
{
  "question": "Give three tips for staying healthy.",
  "age": [
    {
      "category": "0-2",
      "score": 0,
      "reason": "Too young to understand health tips."
    },    
    {
      "category": "20-29",
      "score": 0.8,
      "reason": "More likely to be interested in health and well-being."
    },    
  ],
  "gender": [
    {
      "category": "Male",
      "score": 0.5,
      "reason": "Men may show varied interest in health."
    },
    {
      "category": "Female",
      "score": 0.6,
      "reason": "Women tend to show higher interest in health and well-being."
    }
  ],
  "race": [
    {
      "category": "east asian",
      "score": 0.6,
      "reason": "Generally health-conscious but varies across groups."
    },
    {
      "category": "indian",
      "score": 0.6,
      "reason": "Generally health-conscious but varies across groups."
    },
  ]
}
\end{lstlisting}
\end{center}







\end{document}

\begin{table*}[t]
\centering
\fontsize{5}{6}\selectfont
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{\#Parameters}}            & \multicolumn{3}{c}{\textbf{Training Strategy}}     & \multicolumn{3}{c}{\textbf{Age Prediction}} & \multicolumn{3}{c}{\textbf{Race Prediction}} & \multicolumn{3}{c}{\textbf{Gender Prediction}} & \multicolumn{3}{c}{\textbf{Emotion Prediction}} & \multicolumn{3}{c}{\textbf{Face Counting}} & \multicolumn{3}{c}{\textbf{Face Attribute Prediction}} \\ \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16} \cmidrule(r){17-19} \cmidrule(r){20-22}
                              & \textbf{PT}         & \textbf{SFT}    & \textbf{DPO}        & \textbf{P}          & \textbf{R}         & \textbf{F1}        & \textbf{P }         & \textbf{R}          & \textbf{F1}        & \textbf{P}           & \textbf{R}          & \textbf{F1}         & \textbf{P }          & \textbf{R}           & \textbf{F1}         & \textbf{P}         & \textbf{R }        & \textbf{F1}        & \textbf{P}             & \textbf{R }            & \textbf{F1 }           \\ \midrule
    \multirow{6}{*}{3B}  &  \multicolumn{3}{c}{base model \rowcolor{gray!20}}   
                              & 0.135      & 0.124     &  0.127     & 0.190      & 0.187      & 0.188     & 0.885       & 0.885      & 0.885      & 0.225       & 0.037       & 0.064      & 0.325     & 0.325     & 0.325     & \textbf{0.670 }        & \textbf{0.670}         & \textbf{0.670}         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & $\times$ & $\times$     & 0.046      & 0.148     & 0.057     & 0.148      & 0287       & 0.158     & 0.353       & 0.430      & 0.364      & \textbf{0.305}       & 0.050       & 0.087      & 0.305     & 0.305     & 0.305     & 0.556         & 0.605         & 0.564         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & LoRA   & $\times$     & 0.525      & 0.525     & 0.525     & \textbf{0.727 }     & \textbf{0.727}      & \textbf{0.727}     & 0.895       & 0.895      & 0.895      & 0.093       & 0.510       & 0.157      & \textbf{0.410}     & \textbf{0.415 }    & \textbf{0.410 }    & 0.660         & 0.660         & 0.660         \\
                            & $\checkmark$ & MoLE   & $\times$     & 0.174      & 0.327     & 0.197     & 0.305      & 0.305      & 0.305     & 0.719       & 0.745      & 0.722      & 0.229       & 0.127       & 0.116      & 0.400     & 0.400     & 0.400     & 0.610         & 0.615         & 0.611         \\ \cmidrule(r){2-22}
                           & $\checkmark$ & LoRA   & $\checkmark$ & \textcolor{green}{$\uparrow$}\textbf{0.530}     & \textcolor{green}{$\uparrow$}0.530     & \textcolor{green}{$\uparrow$}\textbf{0.530}    &   \textcolor{red}{$\downarrow$}0.690    &  \textcolor{red}{$\downarrow$}0.690      &  \textcolor{red}{$\downarrow$}0.690    & \textcolor{green}{$\uparrow$}\textbf{0.905}       & \textcolor{green}{$\uparrow$}\textbf{0.915}      & \textcolor{green}{$\uparrow$}\textbf{0.905}    & \textcolor{green}{$\uparrow$}0.096       &\textcolor{green}{$\uparrow$}\textbf{0.666}       & \textcolor{green}{$\uparrow$}\textbf{0.167}      & \textcolor{red}{$\downarrow$}0.248     & \textcolor{red}{$\downarrow$} 0.405     & \textcolor{red}{$\downarrow$}0.256     & \textcolor{red}{$\downarrow$}0.630         &\textcolor{red}{$\downarrow$} 0.630         & \textcolor{red}{$\downarrow$} 0.630         \\
                              & $\checkmark$ & MoLE   & $\checkmark$ & \textcolor{red}{$\downarrow$}0.107      & \textcolor{green}{$\uparrow$}\textbf{0.615}     & \textcolor{red}{$\downarrow$}0.161     & \textcolor{red}{$\downarrow$}0.267      & \textcolor{green}{$\uparrow$}0.547      & \textcolor{red}{$\downarrow$}0.285     & \textcolor{red}{$\downarrow$}0.078       & \textcolor{green}{$\uparrow$}0.755      & \textcolor{red}{$\downarrow$}0.099      & \textcolor{red}{$\downarrow$}0.084       & \textcolor{green}{$\uparrow$}0.511       & \textcolor{green}{$\uparrow$}0.123      & \textcolor{red}{$\downarrow$}0.282     & \textcolor{red}{$\downarrow$}0.395     & \textcolor{red}{$\downarrow$}0.287     & \textcolor{red}{$\downarrow$}0.545         & \textcolor{red}{$\downarrow$}0.550         & \textcolor{red}{$\downarrow$}0.546         \\ \midrule
    \multirow{6}{*}{10B} &   \multicolumn{3}{c}{base model \rowcolor{gray!20}  } 
                              & 0.245      & 0.245     & 0.245     & 0.455      & 0.455      & 0.455     & 0.875       & 0.875      & 0.875      & 0.260       & 0.043       & 0.074      & 0.370     & 0.370     & 0.370     & 0.720         & 0.720         & 0.720         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & $\times$ & $\times$     & 0.100      & 0.295     & 0.117     & 0.214      & 0.367      & 0.231     & 0.818       & 0.860      & 0.824      & 0.365       & 0.060       & 0.104      & 0.330     & 0.330     & 0.330     & 0.670         & 0.670         & 0.670         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & LoRA   & $\times$     & \textbf{0.520}      & 0.520     & \textbf{0.520}     & \textbf{0.737}      & \textbf{0.737}      & \textbf{0.737 }    & 0.900       & 0.900      & 0.900      & 0.272       & \textbf{0.600}       & \textbf{0.346 }     & \textbf{0.450 }    & 0.450     & \textbf{0.450}     & \textbf{0.765}         & 0.765         & \textbf{0.765}         \\

                              & $\checkmark$ & MoLE   & $\times$     & 0.476      & 0.480     & 0.477     & 0.660      & 0.660      & 0.660     & \textbf{0.920}       & \textbf{0.920}      & \textbf{0.920}      & \textbf{0.376 }      & 0.080       & 0.120      & 0.365     & 0.370     & 0.366     & 0.695         & 0.695         & 0.695         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & LoRA   & $\checkmark$ &\textcolor{red}{$\downarrow$} 0.377      &  \textcolor{green}{$\uparrow$}\textbf{0.540}     & \textcolor{red}{$\downarrow$}0.432     & \textcolor{red}{$\downarrow$}0.666      & \textcolor{red}{$\downarrow$}0.712      & \textcolor{red}{$\downarrow$}0.680     & \textcolor{red}{$\downarrow$}0.571       &  0.900      & \textcolor{red}{$\downarrow$}0.661      &\textcolor{red}{$\downarrow$}0.105       & \textcolor{red}{$\downarrow$}0.569       & \textcolor{red}{$\downarrow$}0.176      & \textcolor{red}{$\downarrow$}0.160     & \textcolor{green}{$\uparrow$}\textbf{0.485}     & \textcolor{red}{$\downarrow$}0.197     & \textcolor{red}{$\downarrow$}0.296         & \textcolor{green}{$\uparrow$}\textbf{0.790}       & \textcolor{red}{$\downarrow$}0.344         \\
                              & $\checkmark$ & MoLE   & $\checkmark$ & \textcolor{red}{$\downarrow$}0.242      & \textcolor{red}{$\downarrow$}0.400     & \textcolor{red}{$\downarrow$}0.253     & \textcolor{red}{$\downarrow$}0.255      & \textcolor{red}{$\downarrow$}0.672      &\textcolor{red}{$\downarrow$} 0.276     &\textcolor{red}{$\downarrow$}0.581       & \textcolor{red}{$\downarrow$}0.805      & \textcolor{red}{$\downarrow$} 0.590      & \textcolor{red}{$\downarrow$}0.113       & \textcolor{green}{$\uparrow$}0.300       &\textcolor{green}{$\uparrow$}0.160      & \textcolor{red}{$\downarrow$}0.361     &\textcolor{green}{$\uparrow$} 0.435     & \textcolor{green}{$\uparrow$}0.366     & \textcolor{red}{$\downarrow$}0.512         & \textcolor{green}{$\uparrow$}0.730         & \textcolor{red}{$\downarrow$}0.517    \\ \bottomrule      
\end{tabular}
\end{table*}




\begin{table*}[t]
\centering
\fontsize{5}{6}\selectfont
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{\#Parameters}}            & \multicolumn{3}{c}{\textbf{Training Strategy}}     & \multicolumn{3}{c}{\textbf{Age Prediction}} & \multicolumn{3}{c}{\textbf{Race Prediction}} & \multicolumn{3}{c}{\textbf{Gender Prediction}} & \multicolumn{3}{c}{\textbf{Emotion Prediction}} & \multicolumn{3}{c}{\textbf{Face Counting}} & \multicolumn{3}{c}{\textbf{Face Attribute Prediction}} \\ \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16} \cmidrule(r){17-19} \cmidrule(r){20-22}
                              & \textbf{PT}         & \textbf{SFT}    & \textbf{DPO}        & \textbf{P}          & \textbf{R}         & \textbf{F1}        & \textbf{P }         & \textbf{R}          & \textbf{F1}        & \textbf{P}           & \textbf{R}          & \textbf{F1}         & \textbf{P }          & \textbf{R}           & \textbf{F1}         & \textbf{P}         & \textbf{R }        & \textbf{F1}        & \textbf{P}             & \textbf{R }            & \textbf{F1 }           \\ \midrule
    \multirow{6}{*}{3B}  &  \multicolumn{3}{c}{base model \rowcolor{gray!20}}   
                              & 0.135      & 0.124     &  0.127     & 0.190      & 0.187      & 0.188     & 0.885       & 0.885      & 0.885      & 0.225       & 0.037       & 0.064      & 0.325     & 0.325     & 0.325     & \textbf{0.670 }        & \textbf{0.670}         & \textbf{0.670}         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & $\times$ & $\times$     & 0.046      & 0.148     & 0.057     & 0.148      & 0287       & 0.158     & 0.353       & 0.430      & 0.364      & \textbf{0.305}       & 0.050       & 0.087      & 0.305     & 0.305     & 0.305     & 0.556         & 0.605         & 0.564         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & LoRA   & $\times$     & 0.525      & 0.525     & 0.525     & \textbf{0.727 }     & \textbf{0.727}      & \textbf{0.727}     & 0.895       & 0.895      & 0.895      & 0.093       & 0.510       & 0.157      & \textbf{0.410}     & \textbf{0.415 }    & \textbf{0.410 }    & 0.660         & 0.660         & 0.660         \\
                            & $\checkmark$ & MoLE   & $\times$     & 0.174      & 0.327     & 0.197     & 0.305      & 0.305      & 0.305     & 0.719       & 0.745      & 0.722      & 0.229       & 0.127       & 0.116      & 0.400     & 0.400     & 0.400     & 0.610         & 0.615         & 0.611         \\ \cmidrule(r){2-22}
                           & $\checkmark$ & LoRA   & $\checkmark$ & \textcolor{green}{$\uparrow$}\textbf{0.530}     & \textcolor{green}{$\uparrow$}0.530     & \textcolor{green}{$\uparrow$}\textbf{0.530}    &   \textcolor{red}{$\downarrow$}0.690    &  \textcolor{red}{$\downarrow$}0.690      &  \textcolor{red}{$\downarrow$}0.690    & \textcolor{green}{$\uparrow$}\textbf{0.905}       & \textcolor{green}{$\uparrow$}\textbf{0.915}      & \textcolor{green}{$\uparrow$}\textbf{0.905}    & \textcolor{green}{$\uparrow$}0.096       &\textcolor{green}{$\uparrow$}\textbf{0.666}       & \textcolor{green}{$\uparrow$}\textbf{0.167}      & \textcolor{red}{$\downarrow$}0.248     & \textcolor{red}{$\downarrow$} 0.405     & \textcolor{red}{$\downarrow$}0.256     & \textcolor{red}{$\downarrow$}0.630         &\textcolor{red}{$\downarrow$} 0.630         & \textcolor{red}{$\downarrow$} 0.630         \\
                              & $\checkmark$ & MoLE   & $\checkmark$ & \textcolor{red}{$\downarrow$}0.107      & \textcolor{green}{$\uparrow$}\textbf{0.615}     & \textcolor{red}{$\downarrow$}0.161     & \textcolor{red}{$\downarrow$}0.267      & \textcolor{green}{$\uparrow$}0.547      & \textcolor{red}{$\downarrow$}0.285     & \textcolor{red}{$\downarrow$}0.078       & \textcolor{green}{$\uparrow$}0.755      & \textcolor{red}{$\downarrow$}0.099      & \textcolor{red}{$\downarrow$}0.084       & \textcolor{green}{$\uparrow$}0.511       & \textcolor{green}{$\uparrow$}0.123      & \textcolor{red}{$\downarrow$}0.282     & \textcolor{red}{$\downarrow$}0.395     & \textcolor{red}{$\downarrow$}0.287     & \textcolor{red}{$\downarrow$}0.545         & \textcolor{red}{$\downarrow$}0.550         & \textcolor{red}{$\downarrow$}0.546         \\ \midrule
    \multirow{6}{*}{10B} &   \multicolumn{3}{c}{base model \rowcolor{gray!20}  } 
                              & 0.245      & 0.245     & 0.245     & 0.455      & 0.455      & 0.455     & 0.875       & 0.875      & 0.875      & 0.260       & 0.043       & 0.074      & 0.370     & 0.370     & 0.370     & 0.720         & 0.720         & 0.720         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & $\times$ & $\times$     & 0.100      & 0.295     & 0.117     & 0.214      & 0.367      & 0.231     & 0.818       & 0.860      & 0.824      & 0.365       & 0.060       & 0.104      & 0.330     & 0.330     & 0.330     & 0.670         & 0.670         & 0.670         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & LoRA   & $\times$     & \textbf{0.520}      & 0.520     & \textbf{0.520}     & \textbf{0.737}      & \textbf{0.737}      & \textbf{0.737 }    & 0.900       & 0.900      & 0.900      & 0.272       & \textbf{0.600}       & \textbf{0.346 }     & \textbf{0.450 }    & 0.450     & \textbf{0.450}     & \textbf{0.765}         & 0.765         & \textbf{0.765}         \\

                              & $\checkmark$ & MoLE   & $\times$     & 0.476      & 0.480     & 0.477     & 0.660      & 0.660      & 0.660     & \textbf{0.920}       & \textbf{0.920}      & \textbf{0.920}      & \textbf{0.376 }      & 0.080       & 0.120      & 0.365     & 0.370     & 0.366     & 0.695         & 0.695         & 0.695         \\ \cmidrule(r){2-22}
                              & $\checkmark$ & LoRA   & $\checkmark$ &\textcolor{red}{$\downarrow$} 0.377      &  \textcolor{green}{$\uparrow$}\textbf{0.540}     & \textcolor{red}{$\downarrow$}0.432     & \textcolor{red}{$\downarrow$}0.666      & \textcolor{red}{$\downarrow$}0.712      & \textcolor{red}{$\downarrow$}0.680     & \textcolor{red}{$\downarrow$}0.571       &  0.900      & \textcolor{red}{$\downarrow$}0.661      &\textcolor{red}{$\downarrow$}0.105       & \textcolor{red}{$\downarrow$}0.569       & \textcolor{red}{$\downarrow$}0.176      & \textcolor{red}{$\downarrow$}0.160     & \textcolor{green}{$\uparrow$}\textbf{0.485}     & \textcolor{red}{$\downarrow$}0.197     & \textcolor{red}{$\downarrow$}0.296         & \textcolor{green}{$\uparrow$}\textbf{0.790}       & \textcolor{red}{$\downarrow$}0.344         \\
                              & $\checkmark$ & MoLE   & $\checkmark$ & \textcolor{red}{$\downarrow$}0.242      & \textcolor{red}{$\downarrow$}0.400     & \textcolor{red}{$\downarrow$}0.253     & \textcolor{red}{$\downarrow$}0.255      & \textcolor{red}{$\downarrow$}0.672      &\textcolor{red}{$\downarrow$} 0.276     &\textcolor{red}{$\downarrow$}0.581       & \textcolor{red}{$\downarrow$}0.805      & \textcolor{red}{$\downarrow$} 0.590      & \textcolor{red}{$\downarrow$}0.113       & \textcolor{green}{$\uparrow$}0.300       &\textcolor{green}{$\uparrow$}0.160      & \textcolor{red}{$\downarrow$}0.361     &\textcolor{green}{$\uparrow$} 0.435     & \textcolor{green}{$\uparrow$}0.366     & \textcolor{red}{$\downarrow$}0.512         & \textcolor{green}{$\uparrow$}0.730         & \textcolor{red}{$\downarrow$}0.517    \\ \bottomrule      
\end{tabular}
\end{table*}


\begin{table*}[]
\centering
\fontsize{5}{6}\selectfont
\begin{tabular}{c|ccc|c|ccc|ccc}
\toprule
\multirow{2}{*}{Size} & \multicolumn{3}{c|}{Training Strategy}   & \multirow{2}{*}{Metric} & \multicolumn{3}{c}{User-VQA Bench}          & \multicolumn{3}{|c}{ElderlyTech-VQA Bench} \\ \cmidrule(r){2-4} \cmidrule(r){6-8} \cmidrule(r){9-11}
                      & PT& SFT& DPO                           &                         & Precision & Recall         & F1             & Precision    & Recall       & F1          \\ \midrule
\multirow{18}{*}{3B}  & \multicolumn{3}{c|}{\multirow{3}{*}{base model}}                                           
                                                & ROUGE-1                 & 0.182     & 0.055          & 0.052          &     0.159 & 0.009 & 0.017 \\
                      & \multicolumn{3}{c|}{}    & ROUGE-2                 & 0.024     & 0.007          & 0.007         & 0.001 & 0.000 & 0.000 \\
                      & \multicolumn{3}{c|}{}    & ROUGE-L                 & 0.166     & 0.044          & 0.042         & 0.159 & 0.009 & 0.017 \\ \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{$\times$} & \multirow{3}{*}{$\times$}     
                      & ROUGE-1  & 0.136     & 0.021          & 0.027          &      0.125& 0.007 & 0.014                                  \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.009     & 0.001          & 0.002          &      0.000        &       0.000       &       0.000      \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.122     & 0.017          & 0.022          &    0.125 &  0.007 &   0.014                                 \\  \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{LoRA}     & \multirow{3}{*}{$\times$}     
                      & ROUGE-1                 & 0.495     & 0.401          & \textbf{0.420}          & 0.312 & 0.458 & \textbf{0.361} \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.210     & 0.171          & \textbf{0.180}         & 0.117 & 0.168 & \textbf{0.134} \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.359     & 0.295          & \textbf{0.306}          & 0.229 & 0.334 & \textbf{0.264} \\ \cmidrule(r){2-11}
                      
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{MoLE}     & \multirow{3}{*}{$\times$}     
                      & ROUGE-1                 & 0.409     & 0.285          & 0.293          & 0.281 & 0.334 & 0.268 \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.132     & 0.089          & 0.091          & 0.082 & 0.101 & 0.078 \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.297     & 0.210          & 0.212         & 0.218 & 0.253 & 0.203 \\ \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{LoRA}     & \multirow{3}{*}{$\checkmark$} 
                      & ROUGE-1                 & 0.480     & 0.350          & 0.375 & 0.301 & 0.466 & 0.359 \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.183     & 0.139          & 0.147          & 0.098 & 0.156 & 0.118 \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.345     & 0.261          & 0.275         & 0.216 & 0.336 & 0.258 \\ \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{MoLE}     & \multirow{3}{*}{$\checkmark$} 
                      & ROUGE-1                 & 0.300     & 0.289 & 0.243          & 0.230 & 0.304 & 0.221 \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.080     & 0.078          & 0.064          & 0.052 & 0.073 & 0.053 \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.204     & 0.207          & 0.166          & 0.174 & 0.214 & 0.155 \\ \midrule
\multirow{18}{*}{10B} & \multicolumn{3}{c|}{\multirow{3}{*}{base model}}                                           
& ROUGE-1                 & 0.150     & 0.041          & 0.044          & 0.181 & 0.010 & 0.019 \\
                      & \multicolumn{3}{c|}{}                                                                      
                      & ROUGE-2                 & 0.028     & 0.015          & 0.016          &       0.000       &      0.000        &    0.000         \\
                      & \multicolumn{3}{c|}{}                                                                      
                      & ROUGE-L                 & 0.142     & 0.037          & 0.040          & 0.181 & 0.010 & 0.019 \\ \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{$\times$} & \multirow{3}{*}{$\times$}     
                      & ROUGE-1                 & 0.074     & 0.018          & 0.023         &     0.050 &  0.004 &  0.007                               \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.006     & 0.002          & 0.003          &       0.000       &      0.000        &    0.000         \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.068     & 0.016          & 0.021          &      0.050 &  0.004 &    0.007                             \\ \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{LoRA}     & \multirow{3}{*}{$\times$}     
                      & ROUGE-1                 & 0.550     & 0.423          & \textbf{0.456}         & 0.353 & 0.554 & \textbf{0.419} \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.250     & 0.193          & \textbf{0.209}         & 0.146 & 0.231 & \textbf{0.174} \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.407     & 0.318          & \textbf{0.340 }         & 0.265 & 0.415 & \textbf{0.315} \\ \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{MoLE}     & \multirow{3}{*}{$\times$}     
                      & ROUGE-1                 & 0.503     & 0.315          & 0.351          & 0.375 & 0.372 & 0.307 \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.188     & 0.119          & 0.133          & 0.123 & 0.150 & 0.122 \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.374     & 0.228          & 0.254          & 0.314 & 0.288 & 0.242 \\ \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{LoRA}     & \multirow{3}{*}{$\checkmark$} 
                      & ROUGE-1                 & 0.460     & 0.316          & 0.307          & 0.363 & 0.458 & 0.397 \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.169     & 0.131          & 0.121          & 0.132 & 0.165 & 0.144 \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.340     & 0.256          & 0.236          & 0.273 & 0.343 & 0.298 \\ \cmidrule(r){2-11}
                      & \multirow{3}{*}{$\checkmark$} & \multirow{3}{*}{MoLE}     & \multirow{3}{*}{$\checkmark$} 
                      & ROUGE-1                 & 0.427     & 0.272          & 0.292          & 0.226 & 0.445 & 0.287 \\
                      &                               &                           &                               
                      & ROUGE-2                 & 0.134     & 0.090          & 0.091          & 0.071 & 0.142 & 0.091 \\
                      &                               &                           &                               
                      & ROUGE-L                 & 0.304     & 0.204          & 0.211          & 0.169 & 0.338 & 0.216 \\ \bottomrule            
\end{tabular}
\end{table*}
