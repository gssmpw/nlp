

The increasing development of LLMs demands robust safeguards against safety risks  such as toxicity, hallucinations, and adversarial attacks during LLM inference.  
While existing solutions often address safety risks in isolation, they fail to 
mitigate safety
risks from a global perspective. \goodname~addresses these challenges through a guardrail pipeline that integrates different functional modules for detection, contextualization, correction, and customization. 
It not only bridges the safety gap in current LLM deployments but also sets a foundation for future research in trustworthy AI. 
Potential directions include extending its modular design to emerging threats, optimizing resource efficiency for low-latency applications, and integrating multimodal safety checks. 

