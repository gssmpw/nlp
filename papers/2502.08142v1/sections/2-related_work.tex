

Moderation-based harmfulness mitigation approaches leverage rule-based methods, ML classifiers, and human interfaces to monitor, evaluate, and manage the outputs produced by LLMs to ensure the outputs generated by LLMs are safe, appropriate, and free from harmful content~\citep{openai-data-paper,nemo,perspective-api,Detoxify,guardrails}.
We compare our approaches with the existing approaches in Table~\ref{tab:moderation_comparison}.



\underline{\textit{Close-sourced solutions. }} 
OpenAI Moderation API~\citep{openai-data-paper} and Perspective API~\citep{perspective-api} utilize ML classifiers to detect undesired contents. These approaches provide scores for pre-defined categories of harmful content, such as toxicity, identity attacks, insults, threats, etc. These tools are widely used in content moderation to filter out harmful content and has been incorporated into various online platforms to protect user interactions~\citep{perspective-api-case-studies}. However, they are less adaptable to emerging safety risks as they are not open-sourced and cannot be finetuned. 


\underline{\textit{Opensourced solutions. }} 
LlamaGuard~\citep{inan2023llamaguard} leverages the zero-shot and few-shot abilities of the Llama2-7B architecture~\citep{touvron2023llama} and can adapt to different taxonomies and sets of guidelines for different applications and users. Despite its adaptability, LlamaGuard's reliability depends on the LLM's understanding of the categories and the model's predictive accuracy. However, deploying LlamaGuard on edge devices is challenging due to its large number of parameters, which typically exceed the computing resources available on edge devices.
Detoxify~\citep{Detoxify} offers open-source models designed to detect toxic comments. These models, based on BERT~\citep{devlin2018bert} and RoBERTac~\citep{liu2019roberta} architectures, are trained on the Jigsaw datasets~\citep{jigsaw-unintended-bias-in-toxicity-classification,jigsaw-toxic-comment-classification,jigsaw-multilingual}. Detoxify provides pre-trained models that can be easily integrated into other systems to identify toxic content. Also, the models are able to recognize subtle nuances in language that might indicate harmful content, making them effective for moderation.



\underline{\textit{Customizable solutions. }}
Guardrails~\citep{guardrails} and Nvidia NeMo~\citep{nemo} employ customizable workflows to enhance safety in LLM inference. 
Guardrails~\citep{guardrails} define flexible components, called ``rails'', to enable users to add wrappers at any stage of inference, which enables  users to add structure, type, and quality guarantees to LLMs outputs. Such rails can be code-based or using ML models. However, it does not have self-developed model and miss a unified solution for general cases. 
Nvidia NeMo Guardrails~\citep{nemo} functions as an intermediary layer that enhances the control and safety of LLMs. 
This framework includes pre-implemented moderation dedicated to fact-checking, hallucination prevention, and content moderation, which offers a robust solution for enhancing LLM safety.

