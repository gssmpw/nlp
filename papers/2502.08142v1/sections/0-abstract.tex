We present \goodname, a guardrail pipeline designed to enhance the safety and reliability of Large Language Model (LLM) inferences by systematically addressing risks across the entire processing workflow. 
\goodname~integrates several core functional modules, including % a moderation model in 
\detection~that identifies unsafe inputs and detects hallucinations in model outputs while generating root-cause explanations,
\grounding~that contextualizes user queries with information retrieved from vector databases, \customization~that adjusts outputs in real time using lightweight, rule-based wrappers, %hallucination detection in \detection~that uniquely enables detecting hallucination and providing explanation simultaneously to address the root causes of hallucinations in the LLM outputs, 
and
\fixing~that corrects erroneous LLM outputs using hallucination explanations provided by \detection.
% \goodname~outperforms existing solutions by offering open-source availability, flexibility, edge-device deployability, etc. 
Results show that our unsafe content detection model in \detection~achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets. 
Meanwhile, the lightweight wrappers can address malicious URLs in model outputs in 1.06s per query with 100\% accuracy without costly model calls. Moreover, the hallucination fixing model demonstrates effectiveness in reducing hallucinations with an accuracy of 80.7\%. 
