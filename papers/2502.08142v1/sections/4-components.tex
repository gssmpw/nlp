\vspace{-1em}

\section{\goodname~\detection}\label{sec:safety_detector}


\detection~addresses unsafe inputs and inappropriate LLM responses to ensure that both the user queries provided to the models and the LLM outputs are safe and free from misinformation. 
\subsection{Unsafe Input Detection}\label{sec: unsafe_input_detection}

We developed a model to detect unsafe contents in user queries before they are processed by LLMs for inference.
While existing approaches categorize unsafe content into various types (e.g.,  toxicity, prompt injection, stereotypes, harassment, threats, identity attacks, and violence)~\citep{openai-data-paper,Wang2023DecodingTrustAC,Detoxify}, our method employs a unified, binary classification model finetuned based on our opensourced LLM~\citep{fox}, classifying content as  safe or unsafe.


This strategy offers several key advantages, as follows: \textit{i}) By fine-tuning our base model, which has been trained on vast amounts of data, the classification model can leverage pre-existing knowledge relevant to safety detection.
\textit{ii}) A binary classification of ``safe'' and ``unsafe'' is both efficient and sufficient for LLM services, as any unsafe query should be rejected, regardless of the specific risk.
\textit{iii}) This approach avoids the complexities and potential inaccuracies of categorizing overlapping or ambiguous types of unsafe content in some publicly available datasets. For example, toxicity toward minority groups could also be classified as bias, but current datasets may inadequately capture such nuances.
\textit{iv}) Using straightforward code logic, we can transform public datasets for safety detection into clear safe/unsafe labels, minimizing ambiguity and ensuring high-quality training data.





The biggest challenge in training such model is the discrepancy between the training data and real-world user query distributions, where using traditional datasets alone can result in poor performance due to their divergence from actual user queries~\citep{openai-data-paper}.
To mitigate these issues, we integrated data of various domains and contexts to better simulate the variety of unsafe queries that users might submit.
We crafted a training dataset
by combining samples randomly selected from 15 public datasets, as will be introduced in Table~\ref{tab:exp_datasets} in \S\ref{sec: exp}. 
Such a dataset captures  diverse contents in user inputs in practice, thus can be more representative on potential real-world inputs. 


\subsection{Hallucination Detection and Reasoning}\label{sec: hallucination_detection_and_reasoning}

\input{tables_and_algos/hallucination_data_processing_algo}






Hallucinations occur when the LLM generates responses that is inaccurate, fabricated, or irrelevant~\citep{filippova2020controlled, maynez2020faithfulness,huang2023survey,rawte2023survey}.
Despite appearing coherent and plausible, hallucinated LLM responses are unreliable, often containing fabricated, misleading information that is  
divergent from the user input, thus fail to meet users' expectations and severely undermine the trustworthiness and utility of the LLM applications.
While grounding can mitigate hallucinations by contextualizing user inputs and enriching the informativeness of user queries, it cannot eliminate hallucinations entirely. 
This is because hallucinations stem from nearly every aspects of LLM training and inference, such as low-quality training data~\citep{lin2021truthfulqa,kang2023impact} and %LLM memorizing training data~\citep{lin-etal-2022-truthfulqa}, 
randomness of sampling strategies~\citep{chuang2023dola}, and moreover, the very nature probabilistic properties of LLMs. 

Effectively handling hallucinations in LLM responses is both crucial and challenging for producing high-quality LLM responses. 
Existing works that detect presence of hallucinations are insufficient~\citep{manakul2023selfcheckgpt,liu2021token}. To provide high-quality responses to users,  we should handle the detected hallucinations properly, i.e., obtaining the explanations for the hallucinations in the LLM responses and further, fixing the hallucinated responses if possible. 


To this end, we propose utilizing our own LLM, Fox-1, as base model~\citep{fox} to finetune a 
hallucination detection model  for detecting hallucinated content and providing explanations, and further, facilitating the subsequent \fixing~in \S\ref{sec:fixing}. The design of the model has the following advantages: \textit{i}) \textit{classification}: it identifies the presence of hallucinations in the LLM output; and \textit{ii}) \textit{reasoning}: it generates explanations for the hallucinated contents, offering insights for the subsequent correction in \fixing; \textit{iii}) \textit{simultaneous classification and reasoning}: it process \textit{i}) and \textit{ii}) at the same time, which saves computation cost and improves efficiency; and \textit{iv}) \textit{vast pre-training data}: it leverages pre-existing knowledge
on hallucination in the base model, which may potentially benefit hallucination detection and reasoning.



\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/training_data_example.pdf}
  \caption{Prompt templates and sample training data for hallucination detection and reasoning.}
  \label{fig: training_data_example}
\end{figure*}

\textbf{Training. }
We feed our base model with hallucination dataset to train a model for both detecting and reasoning for the hallucination. %Thus, the model should have text generation capabilities. 
However, public available datasets for hallucinated LLM responses are mainly classification datasets with texts and labels, e.g., HaluEval~\citep{li2023halueval}. To address this, we utilize the GPT4 API~\citep{openai-data-paper} to generate explanations for hallucinated contents, and
define a prompt template
to create structured prompts based on the classification data to make it suitable for classification and reasoning simultaneously. 
We demonstrate the prompt templates and sample training data in Figure~\ref{fig: training_data_example}, and summarize data processing in Algorithm~\ref{alg:hallucination_data_processing}. 





\input{tables_and_algos/hallucination_inference_algo}

\textbf{Inference. }
We expect the LLM to directly output results whether the LLM response contains hallucinations, \textit{i}.\textit{e}., the first token of outputs to be ``Yes'' or ``No'' as detection results, according to the formatted data sample in Figure~\ref{fig: training_data_example}. However, the first token of the LLM response is probabilistic due to the self-autoregressive nature of decoder-based text generation LLMs. 
To obtain desired outputs, we formulate the text-generation outputs by utilizing the top-$k$ first tokens (and their possibilities) of the outputs to generate classification results. By default, $k$ is 10. 

\begin{definition}[Probability of hallucination]
Let $a$ be an LLM answer, let $\{t_1, ..., t_{k}\}$ be the top-k potential first token, and let $\{p_1, ..., p_{k}\}$  be their top-k probabilities. Let $T$ be a tokenization function, and let $T(\text{"Yes"})$ and $T(\text{"No"})$ be the tokens corresponding to ``Yes'' and ``No'', respectively. The probability of hallucination in $a$ is
$P_\mathit{halu}(a) = \frac{\sum_{i=1}^{k} P(t_i | t_i\in T(\text{"Yes"}))}{\sum_{i=1}^{k} P(t_i | t_i\in T(\text{"Yes"})) + \sum_{i=1}^{k} P(t_i | t_i\in T(\text{"No"}))}
$   
\end{definition}\label{def:halu_prob}

Detection results with $P_{\mathit{halu}}(*)\geq0.5$ indicate the content is classified as ``hallucinated''; otherwise, the content is ``safe''.  The detailed procedure of  inference is described in Algorithm~\ref{alg:inference}.


\section{\goodname~\grounding}\label{sec:grounding}






\goodname~\grounding~enhances the contextual richness and informativeness of user queries by leveraging external knowledge in vector database. Thus, LLMs can utilize such contextual knowledge to generate high-quality outputs, particularly by grounding user queries before they are passed to the LLMs for inference.

To support similarity search over the knowledge data, \goodname~creates vector indexes by vectorizing plaintext knowledge.% This involves vectorizing entire knowledge entries to create vector indexes. 
\goodname~employs two primary methods for indexing: \textit{i}) \textit{{Whole Knowledge Index}} that creates indexes based on each entire data entry in the datasets; and \textit{ii}) \textit{{Key Information Index}} that indexes only the key information in each data entry, i.e., questions in QA datasets. 
Whole Knowledge Index reflects the data distribution and ensurers that the indexed data captures the contextual variety and complexity found in real-world queries, while Key Information Index 
focuses on the core information of each data entry, thus facilitates efficient retrieval of relevant data. 
We evaluate the effectiveness of indexes with \textit{callback}, i.e., the probability of successfully retrieving the original records from a dataset using Top-$k$ queries. 
We experimentally evaluate the indexing methods in \S\ref{sec: exp}.


\begin{definition}[Callback]
    Let $D_v$ be a vector data storage that contains $n$ records, let $Q$ be a plaintext user query set, and let $I(Q)$ be the vector index created based on $Q$. For each query $q\in Q$, let $I_q$ be the vector index created based on $q$, and let $D_v(I_q)$ denote the set of Top-$k$ records returned by querying $D_v$ with $I(q)$, and let $r_q$ denote the most relevant record of $q$ in $D_v$. 
    The callback for Top-$k$ queries on the query set $Q$ is defined as:
$$C_k(Q) = \frac{1}{|Q|} \sum_{q \in Q} [r_q \in D_v(I_q)]$$
where $[\cdot]$ is Iverson Bracket Notation~\citep{iverson1962programming}, equal to 1 if the condition inside is true, and 0 otherwise.
\end{definition}

To ensure effective and informative grounding, 
the distribution of the index should closely align with query patterns, i.e., query distributions. 
By grounding user queries with knowledge retrieved with a proper index, 
the LLMs can generate contextually appropriate responses, and further, reduce hallucinations and improve the quality of the responses. 



\begin{figure*}
  \centering
  \includegraphics[width=0.86\textwidth]{figures/fixing_data_example.pdf}
  \caption{Prompt templates and sample training data for \fixing.}
  \label{fig: training_data_example_fixing}
\end{figure*}

\section{\goodname~\customization}\label{sec:customization}

\goodname~\customization~utilizes lightweight wrappers to flexibly edit or customize LLM outputs to fix some small errors or enhancing the format of the answer. The wrappers integrate code-based rules, APIs, web searches, and small models to efficiently handle editing and customization tasks according to user-defined protocols. \goodname~\customization~ offers several key advantages. It facilitates rapid development and deployment of user-defined protocols, which crucial in production environments where real-time adjustments are necessary. In scenarios where training or fine-tuning LLMs is unfeasible due to time or resource constraints, this method provides an alternative for immediate output customization. Moreover, the wrappers enable flexible incorporation  of various tools and data sources, which enhances the applicability of  \goodname~and reduces resource-intensive LLM calls. 



\begin{example}[Warning URLs]\label{example:waring_urls}
The objective was to detect if LLM outputs contain URLs and prepend a warning message of the unsafe URLs at the beginning of the LLM outputs. \customization~should check the safety of the URLs founded,  i.e., whether they are malicious or unreachable, and includes such information in the warning if they were unsafe.
\customization~utilizes a regular expression pattern 
to identify URLs within the text. Upon URLs founded, \customization~calls APIs for detecting phishing URLs, such as Google SafeBrowsing~\citep{google-safe-browsing}, and assess the accessibility of the benign URL by issuing web requests. Malicious URLs, as well as unreachable URLs that return status codes of 4XX, are added in the warning at the beginning of the LLM outputs.
\end{example}


Note that the task in Example~\ref{example:waring_urls} cannot be achieved through prompt engineering when querying LLMs, as the warning must appear at the beginning, and LLMs generate content token by token, making later content unpredictable.
We use the following example to illustrate this property, and experimentally evaluate the efficiency of \customization~wrappers in \textbf{Exp 4} in~\S\ref{sec: exp}. 


\begin{example}
\label{ex:llm-limitation}
We present a concrete demonstration of tasks that cannot be reliably accomplished through prompt engineering alone, due to the token-by-token generation mechanism inherent in LLMs. This sequential generation process fundamentally precludes anticipatory knowledge of future token occurrences during text production.
Consider the following prompt submitted to GPT-4:



\begingroup
{
% \addtolength\leftmargini{-0.2cm}
\begin{quote}
\ttfamily
\noindent Write an English poem about a rabbit; please include information at the beginning of the poem about how many times the word ``rabbit'' appears in the poem.
\end{quote}
}
\endgroup

The generated response (shown below) claims four occurrences of ``rabbit,'' while actual analysis reveals five occurrences:
\begingroup
{ 
\begin{quote}
\ttfamily
(The word ``rabbit'' appears 4 times in this poem.) 

In a field where soft grasses sway and dip, 
a \textcolor{red}{rabbit} hopped beneath the moon's silver whip. 
With ears like twin sails in the cool night air, 
each twitch and turn done with utmost care. \\

This \textcolor{red}{rabbit}, cloaked in twilight's gentle hug, 
nestled in shadows, a cozy, earthen snug. 
Dreams of dandelion fields in sunny sprawl, 
where no shadows loom, nor fears enthrall. \\

With a nose that twitches at secrets untold, 
and paws that dance over leaves of gold, 
the \textcolor{red}{rabbit} finds joy in the simplest of things—the warmth of the sun, the freedom of springs. \\

But as dawn breaks and stars bid goodbye, 
back to the burrow, this \textcolor{red}{rabbit} must fly. 
For the world is wide and wolves roam free,
yet in his burrow, safe as can be, this \textcolor{red}{rabbit} shall be.
\end{quote}
}
\endgroup

This discrepancy arises from the LLM's inability to maintain an accurate token count during generation while simultaneously composing coherent text. The example underscores the necessity of post-inference wrappers to detect and correct such errors in model outputs.
\end{example}






\section{\goodname~\fixing}\label{sec:fixing}


\goodname~\fixing~addresses errors in the LLM  outputs that are challenging to resolve through editing with wrappers in~\customization, particularly, hallucinated content. \fixing~analyzes and corrects the hallucinated output based on the reason for the hallucinations generated by the hallucination detection model. 


\goodname~\fixing~takes several key inputs, including the user's original query, the context retrieved with \grounding, the hallucinated responses generated by the LLM, as well as the reason for hallucination. 
Given these inputs, \fixing~corrects the flawed output according to the hallucination reason.
To enable \fixing~to handle hallucinations effectively, we  leverage the same hallucination detection dataset as \detection, i.e., HaluEval~\citep{li2023halueval}, that contains user questions, contexts, hallucinated LLM answers, and correct answers. 
We also designed a customized data template that incorporates the information. The data templates for training, inference, as well as an example for the training data, are demonstrated in Figure~\ref{fig: training_data_example_fixing}. 
