\goodname~enhances safety of LLM inputs and outputs while improving their quality. Specifically, it achieves two goals, 1) all user inputs are safe, contextually grounded, and effectively processed, such that the inputs to the LLMs are of high-quality and informative; and 2) the output generated by the LLMs are evaluated and enhanced, such that the outputs passed to users can be both relevant and of high quality. 
The pipeline can be partitioned into two parts, including 
1) processing before LLM inference that enhances user queries, and 2) processing after LLM inference that detects undesired content and handle them properly. We overview our pipeline in Figure~\ref{fig: system_overview}.


\noindent\underline{\textit{Pre-inference processing. }}
Before sending user queries to LLMs, \goodname~detects if there are any safety issues in the queries with \detection~and ground the queries with context knowledge with \grounding. 
\detection~monitors user inputs to identify and reject queries that might be unsafe. The monitoring includes typical safety checks, including toxicity, stereotypes, threats, obscenities, prompt injection attacks, etc. Any form of unsafe content will lead to the queries being rejected. 
Inputs that pass this initial safety check are grounded with context with \grounding, where the user query is contextualized and enhanced with relevant knowledge retrieved from the vector data storage. By equipping the query with some context knowledge, the LLM can do inference with enriched information, thus can reduce hallucinations when generating responses. The details of \detection~ and \grounding~will be introduced in \S\ref{sec:safety_detector} and \S\ref{sec:grounding}, respectively.




\noindent\underline{\textit{Post-inference processing. }}
Upon LLM finishing inference, \detection~detects safety issues in the LLM outputs, specifically, hallucinations. This is because LLM applications typically leverages well-developed LLMs or APIs, such as LLaMA~\citep{touvron2023llama} and ChatGPT API~\citep{openai-data-paper}, which are generally safe and less likely to generate toxic or other unsafe content, while hallucinations occur frequently. \detection~identifies hallucinations and provides reasons for the hallucinations, such that \goodname~can utilize the reasoning for later refinement of the LLM outputs. To achieve goal, \goodname~employs a text generation model to generate explainable results, and adjusts the loss function during training to ensure the model to produce classification results. 
After \detection~finishes detection, \fixing~fixes the problematic content or aligns the outputs with some rule-based wrappers to meet user expectations. 
If the outputs are difficult to fix, e.g., hallucinated responses, 
\fixing~will call a fixing model to fix the answers. Details about \fixing~can be found in \S\ref{sec:fixing}.
