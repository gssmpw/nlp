


\input{tables_and_algos/detector_datasets}

We evaluate the performance of different modules in \goodname. 
We use our self-developed model, Fox-1~\cite{fox}, as our base model for finetuning three models, including an unsafe content detection model for \detection, an explainable hallucination detection model for \detection, and a hallucination fixing model for \fixing. 
Below we first introduce Fox-1 and the finetuned the models for different functional modules, then introduce experiment settings, and finally present our evaluation results. 

\noindent\underline{\textit{Base Model. }}
Fox-1 is self-developed, decoder-only transformer-based language model with only 1.6B parameters~\cite{fox}. 
It was trained with a 3-stage data curriculum on 3 trillion tokens of text and code data in 8K sequence length. The base model uses grouped query attention (GQA) with 4 KV heads and 16 attention heads and has a deeper architecture than other SLMs. Specifically, it has 32 transformer decoder blocks, 78\% deeper than Gemma-2B~\cite{team2024gemma}, 33\% deeper than Qwen1.5-1.8B~\cite{qwen} and StableLM-2-1.6B~\cite{bellagente2024stable}, and 15\% deeper than OpenELM-1.1B~\cite{mehta2022cvnets,mehtaOpenELMEfficientLanguage2024}.



\noindent\underline{\textit{Model Finetuning. }}
\detection~model is trained with a combined dataset that extract from 15 datasets to simulate real world unsafe content.
Hallucination detection and explanation model and the hallucination fixing model are trained with HaluEval dataset~\citep{li2023halueval}. 
The datasets for training and evaluation are summarized in Table~\ref{tab:exp_datasets}. 


\noindent\underline{\textit{Experimental Setting. }}
We utilized datasets that contain important knowledge to evaluate \grounding, where inaccurate retrieval can cause financial losses or harmful medical advice. We selected E-Commerce dataset~\cite{e_commerce_dataset} that contains customer service interactions on an online platform, and two healthcare datasets, PatientDoctor dataset~\cite{patient-doctor-chat-data} and the ChatDoctor dataset~\cite{ChatDoctor-dataset}, which contain QA pairs between doctors and patients. 
We leveraged \textit{callback} to evaluate the effectiveness of the two indexing methods in \grounding. 
\customization~evaluations are conducted with E-Commerce~\cite{e_commerce_dataset} and RedditSYACURL Dataset~\citep{reddit-url-data}.
The information of the datasets is summarized in Table~\ref{tab:exp_datasets}. Evaluations and model training experiments are conducted on a server with 8 NVIDIA
H100 GPUs. 




\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.3\textwidth}
            \centering
    \includegraphics[width=\linewidth]{figures/safety_detection_exp.png}

        \caption{Safety detection}\label{fig:safety_detection_exp}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
            \centering
    \includegraphics[width=\linewidth]{figures/whole_knowledge_index_exp.png}

        \caption{Whole index}\label{fig:whole_knowledge_idx}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
            \centering \includegraphics[width=\linewidth]{figures/key_info_index_exp.png}

        \caption{Key index}\label{fig:key_info_idx}
    \end{minipage}

\end{figure*}

\noindent\textbf{Exp 1. Unsafe user inputs detection in~\detection. }
The datasets and the number of records involved in training, validation, and test phases are summarized in Table \ref{tab:exp_datasets}. We compare our approach with 
Detoxify-Roberta~\citep{Detoxify},
Detoxify-BERT~\citep{Detoxify},
Nvidia NeMo GuardRail~\citep{nemo}, 
OpenAI Moderate~\citep{openai-data-paper}, and 
PerspectiveAPI~\citep{perspective-api} in Figure~\ref{fig:safety_detection_exp}.   % Table~\ref{tab:toxicity detecion}. 
Results show that our model achieves comparable performance with OpenAI API. 
Overall, our model demonstrates robust performance across key metrics, indicates its effectiveness and reliability in real-world applications. 



\begin{table*}[ht]
\centering
\small
\caption{URL Detection Task}
\label{tab:fixing_exp}
% \small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Metrics} & \textbf{Ours} & \textbf{TinyLLama} & \textbf{Mistral-7B} & \textbf{LLama2-7B} & \textbf{LLama3-8B} & \textbf{Falcon-40B} \\ \midrule
Avg. Time (s) & \textbf{1.06} & 13.17 & 10.93 & 9.10 & 20.10 & 34.67 \\
% Detected URL num & 24 & 0 & 22 & 20 & 9 & 0 \\
Detection Acc. & \textbf{100.00\%} & {\redxmark} {(Fail)} & 91.67\% & 83.33\% & 37.50\% & {\redxmark} {(Fail)} \\
Validation Acc. & \textbf{83.33\%} & {\redxmark} {(Fail)}& 45.83\% & 54.17\% & 37.50\% & {\redxmark} {Fail} \\
\bottomrule
\end{tabular}
\end{table*}


\noindent\textbf{Exp 2. Hallucination detection in LLM outputs in~\detection. } 
We fine-tuned our hallucination detection model using the HaluEval dataset \cite{li2023halueval}. We utilize the three subsets in HaluEval, including \textit{i}) the ``qa'' subset that 
contains question, right answer, hallucinated answer, and knowledge,  \textit{ii}) the ``dialogue'' subset that contains dialogue history, right response, hallucinated response, and knowledge, and  \textit{iii})
the ``summarization'' subset that contains document, right summary, and hallucinated summary.
For each subset, we set 8,000 data samples for training, 1,500 for validation, and 500 for testing.  
Our model achieved an accuracy of 0.78 on the testing data. 



% \subsection{Evaluation of \grounding}
\noindent\textbf{Exp 3. Evaluation of different indexing methods in~\grounding. }
To comprehensively evaluate retrieval performance and simulate user queries real-world applications, we used two types of queries, including \textit{i}) \textit{original queries} that match original questions in the datasets (``O'' in Figure~\ref{fig:whole_knowledge_idx} and Figure~\ref{fig:key_info_idx}), and \textit{ii}) \textit{rephrased queries} generated with language models (i.e., TinyLlama~\citep{zhang2024tinyllama} or a summarization  model~\citep{summarization-model}) based on the original questions to simulate variability in user questions (``R'' in Figure~\ref{fig:whole_knowledge_idx} and Figure~\ref{fig:key_info_idx}). 
For each evaluation, we randomly selected 50 questions from the dataset to form a question set $Q$, and processed Top-$k$ queries to compute a callback $C_k(Q)$, where $k$ is set to 1, 3, 5, and 10.
We recorded the callbacks for Whole Knowledge Index and Key Information Index in Figure~\ref{fig:whole_knowledge_idx} and Figure~\ref{fig:key_info_idx}, respectively.
The results indicate that Key Information Indexing outperformed Whole Knowledge Indexing, as key information indexes reflects the user queries better. Also, both original queries and rephrased queries achieved high callback rates, which demonstrates the effectiveness of vector retrieval when handling varied user inputs.





% \subsection{Evaluation of \customization}
\noindent\textbf{Exp 4. Efficiency of wrappers in~\customization. }
We evaluated the efficiency of \customization~in %identifying and addressing unsafe LLM outputs 
with the URL detection and validation task in Example~\ref{example:waring_urls} in \S\ref{sec:customization}. 
We randomly selected 15 records from the each of the E-Commerce dataset~\citep{e_commerce_dataset} and the RedditSYACURL Dataset~\citep{reddit-url-data}, combined each record to construct texts that contained URLs, and set 20\% probability of inserting some malicious URLs into the text. In implementation, we leveraged Regex pattern for detecting URLs, Google SafeBrowsing~\citep{google-safe-browsing} for detecting malicious URLs, and sent HTTP requests to the safe URLs to verify their reachability.
We compared \customization~with several models, including TinyLLama~\citep{zhang2024tinyllama}, Mistral-7B~\citep{jiang2023mistral}, LLama2-7B~\citep{touvron2023llama}, and Falcon-40B~\citep{falcon40b}. 
The results are shown in Table~\ref{tab:fixing_exp}. We record average time to process one query, the success rate of detecting URLs (Detection Acc.), and the accuracy of identifying unsafe URLs (Validation Acc.).
The results show that~\goodname~\customization~takes much less time (1.06s per query) and
significantly outperforms calling the models for editing LLM outputs. Also, TinyLLama and Falcon-40B failed to detect any URLs in the contents. Though Mistral is able to detect URLs with a high accuracy of 91.67\%, the accuracy of identifying unsafe URLs is only 45.83\%.


\noindent\textbf{Exp 5. Effectiveness of fixing hallucinations in~\fixing. } 
We fine-tuned our fixing model using the HaluEval dataset \cite{li2023halueval}. We selected the QA and dialogue subsets. For each subset, we utilized 8,000 data samples for training, 1,000 for validation, and 1000 for testing.  
Also, we augmented the hallucination correction dataset with a \texttt{hallucination\char`_reason} column, derived from the detection results of \detection. Such annotation categorizes root causes of hallucinations identified during the detection phase, enabling mitigation strategies in the fixing stages.
We utilize Vectara hallucination detection model~\citep{vectara_halu} for evaluating the consistency between the LLM outputs and the information provided in the original data, including the user questions, the contexts, and the correct answers. We utilized the 100 records in the test dataset of the HaluEval-QA dataset for evaluation. Results show that our fixing model improves the quality of the LLM outputs by a lot. Moreover, 80.7\% of the hallucinated data were fixed using \fixing.