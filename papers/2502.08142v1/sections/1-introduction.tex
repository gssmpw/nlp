
Large Language Models (LLMs) are increasingly deployed in latency-sensitive, high-stakes systemsâ€”from network automation to real-time decision support in critical fields such like healthcare~\cite{goyal2024healai,qiu2024llm,yang2024talk2care} and finance~\cite{wu2023bloomberggpt,li2023large}. However, their widespread adoption is hindered by significant safety risks. Malicious inputs can exploit prompt injection vulnerabilities to manipulate outputs~\citep{Liu2023PromptIA,Kumar2023CertifyingLS,Zhu2023PromptBenchTE,Chu2024ComprehensiveAO,Tedeschi2024ALERTAC,Zhao2024WeaktoStrongJO}, while unconstrained responses may propagate hallucinations, biases, nonsensical or factually incorrect knowledge, or security threats like phishing URLs~\citep{Zhang2023SirensSI,zhang2024efficient,Wang2023DecodingTrustAC,Fan2023OnTT,Huang2023ASO,Xu2024HallucinationII}. These issues not only compromise user trust but also pose systemic risks, such as resource misuse in cloud networks or erroneous configurations in software-defined infrastructures~\citep{llm_loops,owasp_llm01,owasp_llm03,owasp_llm04,owasp_llm05}.


Safeguarding LLMs is crucial and can never be overstated. Unsafe inputs can manipulate LLM outputs, reveal sensitive information, bypass system instructions, or execute malicious commands~\citep{Russinovich2024GreatNW,xu2024llm,chang2024play,Liu2023PromptIA,Kumar2023CertifyingLS,Zhu2023PromptBenchTE,Chu2024ComprehensiveAO,Tedeschi2024ALERTAC,Zhao2024WeaktoStrongJO}. 
Problematic outputs can confuse users, perpetuate biases, and undermine users' trust in LLM-based systems, particularly in domains like healthcare and finance, where inaccuracies or biases can have legal or societal repercussions.

Addressing safety issues in LLM inference is complex, as risks can arise at any point during processing user queries. 
While existing work addresses isolated aspects of LLM safety~\cite{kumar2024watch,elesedy2024lora,ma2023adapting,jha2024memeguard}, no unified solution holistically mitigates risks across the entire inference pipeline.
Standalone detection models~\citep{Detoxify,perspective-api,openai-data-paper} operate reactively to flag unsafe content, but they require full retraining or finetuning to adapt to new safety requirements and lack mechanisms to correct errors in the LLM outputs. 
Post-hoc correction methods rewrite problematic content in the LLM outputs but fail to address their root causes, such as hallucinations, that often stem from insufficient, inaccurate, or outdated source information~\citep{Xu2024HallucinationII,Zhang2023SirensSI,Huang2023ASO}. While retrieval-augmented generation (RAG)~\cite{rag,chen2024benchmarking,gao2023retrieval} can mitigate hallucinations by enriching user queries with external contextual knowledge, 
the probabilistic retrieval of knowledge cannot enforce deterministic safety policies, e.g., blocking mandated sociopolitical terms. 
Rule-based post-processing ``wrappers''~\citep{guardrails}, on the other hand, offer agility for time-sensitive updates and excel at syntactic filtering (e.g., regex-based phishing URL detection via APIs like Google Safe Browsing~\citep{google-safe-browsing}), but fail to address semantic risks in the contents generated by LLMs. 


\input{tables_and_algos/moderation_comparison_table}

\textit{We argue that, enhancing the overall safety of LLM inference 
demands a comprehensive pipeline that orchestrates heterogeneous functional components, such as ML models, RAG, and light-weighted wrappers.} A well-designed guardrail pipeline not only mitigates safety risks from a global perspective but also enable users to customize their workflows to high flexibility and efficiency. 


This paper introduces \goodname, a guardrail pipeline that systematically integrates detection, contextualization, correction, and customization to ensure robust safety and adaptability during LLM inference. \goodname~integrates four components, including
\textit{i}) \detection~that identifies unsafe content (e.g., toxicity, bias, hallucinations) in user inputs and LLM outputs; \textit{ii}) \grounding~that contextualize user queries with vector databases; \textit{iii})
\customization~that leverages lightweight wrappers to edit LLM output according to user needs in a real-time manner; and \textit{iv})
\fixing~that corrects hallucinated content detected in the LLM outputs.
Our contributions are summarized as follows:

\begin{itemize}[leftmargin=16pt, itemsep=1pt]
    \item \textit{Holistic safety pipeline. }We propose \goodname, a comprehensive guardrail pipeline for safeguarding LLM inferences. \goodname~incorporates
detection, contextual grounding, output correction, and user customization into a unified workflow, addressing safety issues in LLM inference as a systemic challenge rather than isolated subproblems.

\item \textit{Specialized fine-tuned models. } We utilized our pretrained LLM, Fox-1~\cite{fox}, as the base model and fine-tuned three lightweight models, including  \textit{i}) a  moderation model that detects unsafe content in user inputs and LLM outputs (\S\ref{sec: unsafe_input_detection}); 
\textit{ii}) an explainable hallucination detection model that detects hallucinations in the LLM outputs while providing explanations for the hallucinations (\S\ref{sec: hallucination_detection_and_reasoning}); and \textit{iii}) a fixing model that corrects problematic LLM outputs based on the explanations for hallucinations (\S\ref{sec:fixing}).
These models are light-weighted and can be deployed on edge devices.


\item\textit{Explainable Hallucination Mitigation. } We address the hallucination issue through a two-stage approach that first detects hallucination and pinpoints hallucination causes
with \detection (\S\ref{sec: hallucination_detection_and_reasoning}), and then utilize \fixing~to correct the problematic content based on the reason of hallucination (\S\ref{sec:fixing}).



\item\textit{Effective grounding. } We propose two indexing methods, including \textit{Whole Knowledge Index} and \textit{Key Information
Index}, to assist retrieving knowledge from vector data storage  (\S\ref{sec:grounding}). 



\item\textit{Flexible User-defined safety protocols. } \goodname~allows users to define protocols for customizing LLM outputs with
\customization, which is flexible to adapt to evolving user needs while
providing real-time solutions for addressing safety issues in LLM
deployments, without pretraining or fine-tuning models to address
emerging safety challenges (\S\ref{sec:customization}).


\end{itemize}
