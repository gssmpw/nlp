
\section{Introduction}\label{sec:intro}
Multi-armed bandits (MABs) represent a fundamental problem in online learning. In this setting, the goal is to maximize total rewards by balancing the exploitation of known high-reward arms with the exploration of potentially better ones. With the rise of online applications, MAB scenarios often involve human elements, where arms or rounds represent individuals whose needs must be considered. For instance, several lines of work consider content providers as the arms~\cite{hu2023incentivizing,ben2023learning,immorlica2024clickbait}, requiring algorithms to incentivize the creation of high-quality content. Therefore, besides balancing exploration and exploitation, the algorithm must also address fairness~\cite{MatthewKearnsMorgensternRothNIPS2016,liu2017calibrated}, long-term welfare~\cite{ben2023learning}, strategic behavior~\cite{braverman2019multi,feng2020intrinsic}, etc.



A recent line of work, pioneered by the seminal paper of \citet{Kremer2014}, considers MAB as a mechanism design problem \cite{nisan1999algorithmic, nisan2007algorithmic}. Recommender systems suggest actions to agents without forcing their choices. Since agents are selfish and may avoid high-risk, high-reward arms, there is a need to \emph{incentivize exploration} to ensure accurate reward estimates for all arms. Previous works~\cite{mansour2015bayesian,mansour2020bayesian,cohen2019optimal} achieve incentive compatibility by leveraging \emph{information asymmetry}: While each agent possesses some prior information, the mechanism accumulates additional information by observing the actions and rewards of previous agents. Despite addressing agents' incentives, this approach has a major drawback: Since exploring agents could have gotten better rewards had they not used the mechanism, they might see the recommendations of the IC mechanism as betraying their trust.

To address this issue, \citet{Fiduciary} introduce the concept of \emph{individual rationality} (IR) in MAB mechanisms. IR, a celebrated concept in mechanism design~\cite{tadelis2013game}, ensures that each recommendation's expected reward, \emph{based on the mechanism's knowledge}, meets or exceeds the reward from the agent's default arm. Relying on the mechanism's knowledge contrasts with information asymmetry and limits the mechanism's ability to learn through exploration. Consequently, IR and IC mechanisms must adopt a careful exploration policy, maximizing their ability to explore risky arms and thereby maximizing welfare. Such an exploration policy should be randomized, offering \emph{portfolios} that mix low-risk, low-reward and high-risk, high-reward arms. Due to their complex structure, prior work \cite{Fiduciary} has presented solutions with inefficient runtime, particularly exponential in the number of arms. This paper is, to the best of our knowledge, the first to propose a runtime-efficient IR and IC MAB mechanism for non-trivial reward distributions. 


\subsection{Our Contribution}
We consider a MAB setting with a Bayesian prior and static rewards; that is, the reward of each arm is initially unknown and is realized only once.\footnote{Although this assumption is limiting, it allows us to focus on the novel IR viewpoint proposed in this paper. A similar approach is taken by several early works on incentivizing exploration \cite{Bahar2016,cohen2019optimal,Fiduciary}. Despite the limitation, the problem remains technically non-trivial.} Some arms may be risky, having negative expected rewards but also offering high potential rewards. Additionally, we assume that there is a \textit{default arm} that agents would pick if they did not use the algorithm. In each round, the algorithm picks a distribution over the arms, termed a \textit{portfolio}. The algorithm then samples and selects an arm based on the portfolio's weights. We aim to maximize social welfare, defined as the sum of rewards.


Without any further constraints, maximizing welfare is straightforward: Try each arm once, then repeatedly choose the best one. Due to our static rewards assumption, one round of exploration per arm is enough to reveal the highest reward. However, we restrict the algorithm to select only \textit{individually rational} (IR) portfolios. A portfolio is IR if its expected reward is at least as good as that of the default arm \emph{conditioned on the information available to the algorithm} (no information asymmetry). Since rewards are realized only once, careful planning is crucial. Arms with a high expected value extend the set of IR portfolios, enabling the exploration of a priori inferior arms.

Our main technical contribution is an asymptotically optimal IR algorithm under a stochastic order assumption on rewards (see Definition~\ref{def:bayesian safety}).  Stochastic order is common in practical scenarios, e.g., Bernoulli rewards or Gaussian reward distributions with a common variance. We introduce an auxiliary Goal Markov Decision Process (GMDP)~\cite{barto1995learning}. In our GMDP, the action sets are convex polytopes of all IR portfolios that mix a priori superior arms (better than the default arm) with a priori inferior arms. Despite the complex action space, we reveal a simple index-based optimal policy, akin to the seminal result of \citet{weitzman1978optimal} for Pandora's box \cite{gergatsouli2024weitzman, atsidakou2024contextual,boodaghians2020pandora, berger2023pandora}. We then use the optimal GMDP policy to develop $\SEGB$ (see Algorithm~\ref{alg:alg of pi}), an IR and asymptotically optimal algorithm. Informally stated,
\begin{theorem}[Informal version of Corollary~\ref{cor: alg is asym opt}]
Given a problem instance with a stochastic order over rewards, $\SEGB$ has a runtime of $O(K \log K)$, where $K$ is the number of arms, and asymptotically achieves optimal welfare among all IR algorithms.
\end{theorem}
We analyze the algorithm's convergence rate and demonstrate that it is fast in expectation. Additionally, we assume agents are strategic and aim to design an IC and IR mechanism. To achieve this, we apply the hidden exploration technique of \citet{mansour2015bayesian} to modify $\SEGB$, making it both IR and IC.

\subsection{Related Work}\label{subsec:related}
Individual rationality (IR) can be viewed as a safety constraint, connecting to various strands of research on safe exploration. For instance, Bandits with knapsack~\cite{badanidiyuru2013bandits} address a MAB problem with a global budget, aiming to maximize total rewards before exhausting resources. Another approach focuses on stage-wise safety, ensuring that regret performance remains above a threshold set by a baseline strategy in each round~\cite{kazerouni2017conservative,wu2016conservative}. Notably, in these lines of work, constraints apply to cumulative resource consumption or reward throughout the algorithm's run. Conversely, \citet{amani2019linear} apply a reward constraint in each round. Their work leaves the set of safe decisions uncertain due to the inherent uncertainty in the learning process, aiming to minimize regret while learning the safe decision set. Similar to \citet{amani2019linear}, we also consider a stage-wise constraint. It is worth mentioning that safety has been studied beyond MABs~\cite{wachi2020safe,Moldovan:2012}. For example, \citet{Moldovan:2012} introduce  an algorithm enabling safe exploration in Markov Decision Processes to avoid fatal absorbing states. 

Closely related to our work is the research on incentivizing exploration, pioneered by \citet{KremerMP13}, motivated by using a bandit-like setting for recommendations. Since users are selfish, algorithms must incentivize exploration (see \cite{slivkins2019introduction} for an introduction and overview). Subsequent works consider regret minimization \cite{mansour2015bayesian,mansour2020bayesian}, heterogeneous agents \cite{chen18a,immorlica2019bayesian}, social networks \citep{Bahar2016,bahar2019recommendation}, and extensions to Markov Decision Processes \cite{simchowitz2024exploration}. Similarly to several previous works \cite{Fiduciary,BaharST19,cohen2019optimal,KremerMP13}, we assume each arm is associated with a fixed value, which is initially unknown and sampled from a known distribution. Given this, our problem becomes the careful planning of safe exploration.

\citet{Fiduciary} is the work most closely related to ours within the incentivizing exploration line of research. We inherit their model with minor differences and also aim to develop an optimal individually rational mechanism. \citet{Fiduciary} assume rewards are integers in a bounded set of size $H$, construct an auxiliary GMDP model, and devise a dynamic programming-based planning policy with a runtime of $O(2^K K^2 H^2)$. We adopt the same Goal MDP approach (Section~\ref{sec:infinite}) but aim to reduce the exponential dependence on $K$. We achieve a significant time improvement of $O(K \log K)$ for rewards with stochastic order, a special yet common case. Importantly, the techniques we use, such as the Equivalence Lemma (Lemma~\ref{lemma:equivalence body}), are novel and were not examined by \citet{Fiduciary}. 





\section{Problem Statement}\label{sec:problem statement}
This section formally defines the Individually Rational Sequential Recommendation problem ($\ise$ for shorthand). We consider a set $A$ of $K$ arms, $A=\{a_1,\dots a_K\}$. The reward of arm $a_i$ is a random variable $X(a_i)$, and $(X(a_i))_{i=1}^K$ are mutually independent. The rewards are static, i.e., they are realized only once but initially unknown. If the reward $X(a_i)$ is realized before round $t$ and is hence known, we use $x(a_i)$ to denote its value. We denote by $\mu({a_i})$ the expected value of $X(a_i)$, i.e., $\mu(a_i)=\E\left[X(a_i)\right]$. We augment the set of arms with a \textit{default arm}, which we denote by $a_0$. For simplicity, unless stated otherwise, we assume that the reward of the default arm $a_0$ is always 0; hence, $\mu(a_0)=0$.\footnote{The selection of zero as the threshold is arbitrary; all our results hold for any bounded distribution for $X(a_0)$ with minor modifications.} Consequently, we let $A^+ = A\cup\{a_0\}$. As will become apparent later on, it is convenient to distinguish arms with positive expected rewards from those with negative expected rewards. To that end, we let 
$\above(A)=\{a_i\in A:\mu(a_i) > 0 \}$. Analogously, $\below(A)=\{a_i\in A:\mu(a_i)< 0 \}$. We assume for simplicity that there are no arms with an expected reward of 0.\footnote{Our results hold with minor modifications even if we remove this assumption, as clarified later on.} %\omer{I might want to change this! Unless stated otherwise, we assume that the default arm yields 0 w.p. 1. CHANGES: WRITE THE MODEL IN TERMS OF A0, AND THEN SAY THAT UNLESS STATED OTHERWISE, WE ASSUME IT IS 0 EXAMPLE WITH DEFAULT=0, AND }

There are $T$ agents arriving sequentially. We denote by $a^t$ the action of the agent arriving at round $t$. The reward of the agent arriving at round $t$ is denoted by $r^t$, and is a function of the arm she chooses. For instance, by selecting arm $a_l \in A$, the agent arriving at round $t$ obtains $r^t = r^t(a_l)=X(a_l)$. 
Agents are fully aware of the distribution of $(X(a_i))_{i=1}^K$, and each agent cares about her own reward, which she wants to maximize. 

A mechanism is a recommender system that interacts with agents. The input to the mechanism at round $t$ consists of all the information acquired up to round $t$ by the previous $t-1$ agents, which we denote by $\mI_t$. Namely, $\mI_1$ encodes the prior information only, $\mI_2$ encodes both the prior information and the reward of the arm selected at round 1, and so on. The output of the mechanism is a distribution over arms. Formally, a randomized mechanism is a mapping $ \bigcup_{t=1}^T \left(A^+\times \R_+ \right)^{t-1} \rightarrow \Delta(A^+)$, where $\Delta(A^+)$ is the set of probability distributions over the elements of $A^+$. 

Notice that a randomized mechanism selects a distribution of arms in every round, which we term \textit{portfolios}. A portfolio is an element from $\Delta(A^+)$---it is a distribution over the arms and the default arm. Whenever the mechanism picks a portfolio $\bl p \in \Delta(A^+)$, Nature (i.e., a third-party) flips coins according to $\bl p$ to realize one arm from $A^+$. We let $\bl p^t$ denote the portfolio the mechanism selects at round $t$, and $m^t=m^t(\mI_t)$ the recommended arm, i.e., $m^t\sim \bl p^t$. Using this formulation, the expected reward at time $t$ of an agent choosing $m^t$ is determined by Nature's coin flips and the randomness of the rewards. Formally, 
\[
\E[r^t(m^t)\mid \mI_t]=\sum_{a_i \in A^+} \bl p^t(a_i)\E\left[X(a_i)\mid \mI_t\right].
\]
%where $\bl p^t$ is the portfolio the mechanism selects at time $t$, i.e., $M(\mI_t)=\bl p^t$. 
We consider both \textit{non-strategic agents} and \textit{strategic agents} schemes. In the former, agents always follow the recommendation of the mechanism, namely $a^t=m^t$. Note that this is a standard underlying assumption of classical multi-armed Bandit models. In the latter, the mechanism recommends actions to agents but cannot compel them to follow these recommendations, potentially resulting in $a^t \neq m^t$. In this scheme, we say that a mechanism is Bayesian incentive compatible (BIC) if following its recommendations is a dominant strategy in the Bayesian sense. That is, when given a recommendation, an agent's best response is to follow the recommended arm. Formally,
%In this scheme, we say that a mechanism is incentive compatible (IC) when following its recommendations constitutes an equilibrium% \footnote{This is done for ease of presentation. The mechanisms in this paper can be modified to offer dominant strategies to agents.}: that is, when given a recommendation and given that others follow their own recommendations, an agent's best response is to follow her own recommendation. Formally,
\begin{definition}[Bayesian Incentive Compatibility]\label{def:ic}
A mechanism is Bayesian incentive compatible (BIC) if $\forall t \in \{1,\ldots,T\}$, for every information $\mI_t$  and for all action pairs $a_l,a_i \in A$,
\begin{equation}\label{eq: ic const}
\E(X(a_l)-X(a_i)\mid m^t(\mI_t)=a_l)\geq   0.
\end{equation}
%where the expectation is taken over the randomness of the arms and agent $l$'s information on the arrival order.
\end{definition}
We stress that every agent can decide which arm to take \textit{after seeing the realized recommended arm $m^t$} and not the portfolio $\bl p^t$. Unless stated otherwise, we refer to the non-strategic agents scheme. We treat the strategic agents scheme in Section~\ref{sec:ic body}.

\paragraph{Individual Rationality}
In this paper, we focus on \textit{ex-ante individually rational} recommendations, abbreviated as IR. A portfolio $\bl p^t$ is IR at time $t$ if, given the information the mechanism acquired up to round $t$, its expected reward is greater or equal to zero, which is the reward of the default arm. Formally,
\begin{definition}[Ex-ante Individual Rationality]\label{def:bayesian safety}
A portfolio $\bl p$ is individually rational (IR) with respect to $\mI$ if
\begin{equation}\label{ineq:IR def}
\E_{a\sim \bl p}[X(a) \mid \mI]=\sum_{a_i\in A}\bl p (a_i)\E\left[X(a_i)\mid \mI\right]\geq \E\left[X(a_0)\mid \mI\right]= 0.
\end{equation}
\end{definition}
We stress that Definition~\ref{def:bayesian safety} implies only ex-ante IR, as Nature may realize an arm with a negative expectation; nevertheless, we shall continue referring to it as IR. If a mechanism selects IR portfolios for every possible information $\mI$, we say the mechanism is IR. By focusing on IR mechanisms, we ensure that every agent will receive at least the reward of the default arm in expectation, independent of the other agents. The definitions of IR and IC are orthogonal and significantly differ in the events they condition on. The IC Inequality~\eqref{eq: ic const} conditions on the agent's information, which includes the prior information and the recommendation of the mechanism. In contrast, the IR Inequality~\eqref{ineq:IR def} conditions on all the information the mechanism has acquired. Forbidding the mechanism from exploring arms with a negative expected value trivializes the problem as it prevents exploring any such arms. The IR constraint is a compromise: It allows learning about a priori inferior arms, but only when they are mixed with a priori superior arms.

\paragraph{Social Welfare} When agents follows the mechanism, we denote the \textit{social welfare} achieved by a mechanism $\ALG$ by $\mU_T(\ALG)=\E\left[\frac{1}{T}\sum_{t=1}^T r^t\right]$. The global objective of the mechanism is to maximize social welfare, subject to selecting IR portfolios in every round. If agents are strategic, we narrow our attention to BIC mechanisms only, which justifies using the same formulation of social welfare.

To conclude, we represent an instance of the $\ise$ problem by the tuple 
\[
\tupbracket{K, A, (X(a_i))_i, (\mu(a_i))_i}.
\]
Notice that the horizon $T$ is not part of the description since we often discuss a particular instance with varying $T$. When the instance is known from the context, we denote the highest possible social welfare achieved by any algorithm by $\OPT_T$, where the subscript emphasizes the dependence on the number of rounds $T$. Additionally, we let $\OPT_{\infty}$  denote $\lim_{T\rightarrow \infty} \OPT_T$.

\subsection{Example}
Before we proceed, we illustrate our setting and notation with an example.
\begin{example}\label{example with normal}
Let $K=4$, and let{\setmuskip{\thickmuskip}{0mu}
$X(a_1)\sim N(2,1)$, $X(a_2)\sim N(1,1)$, $X(a_3)\sim N(-1,1)$, and $X(a_4)\sim N(-2,1)$, where $N(\mu,\sigma^2)$ denotes the normal distribution with a mean $\mu$ and a variance $\sigma^2$. 
}%
Notice that $\above(A)=\{a_1,a_2\}$ and  $\below(A)=\{a_3,a_4\}$. In the first round, the mechanism can select, for example, a portfolio that comprises $a_1$ w.p. 1. It can also select the portfolio that mixes $a_1$ w.p. $\frac{1}{2}$ with $a_2$ w.p. $\frac{1}{2}$, and infinitely many other IR portfolios.

To demonstrate the technical difficulty of maximizing social welfare, assume that $X(a_1),X(a_2) <0$ while $X(a_3),X(a_4)>0$ (this information is \textit{not} available to the mechanism). Indeed, this occurs with positive non-negligible probability under the distributional assumptions of this example. Consider the case where the mechanism selects the portfolio $a_1$ w.p. 1 in the first round and $a_2$ w.p. 1 in the second round. Notice that these portfolios are IR: Given the information $\mI_1$ in the first round, selecting $a_1$ w.p. 1 is IR, and regardless of $x(a_1)$, selecting $a_2$ w.p. 1 in the second round is also IR. After observing that $a_1$ and $a_2$ have negative rewards (since we momentarily assume $X(a_1)<0$ and $X(a_2) <0$), the only IR portfolio for the subsequent rounds is to select the default arm $a_0$ w.p. 1.


However, there are much better ways to act in the first round. Arms with positive expected rewards are essential for enabling the exploration of arms with negative expected rewards. Consider $\bl p^1$ such that $\bl p^1(a_1)=\frac{1}{3},\bl p^1(a_3)=\frac{2}{3}$ and $\bl p^1(a_2)=\bl p^1(a_4)=0$. Notice that $\bl p^1$ is IR w.r.t. $\mI_1$, since $\bl p^1(a_1)\mu(a_1)+\bl p^1(a_3)\mu(a_3)=0$. If the mechanism selects $\bl p^1$ in the first round, it will already discover the positive reward of arm $a_3$ in the first round w.p. $\frac{2}{3}$. It is thus evident that selecting $\bl p^1$ in the first round results in higher social welfare than selecting $a_1$ w.p. 1.  
\end{example}

This example illustrates that any mechanism seeking to maximize social welfare should leverage $a_1$ and $a_2$ with their a priori positive rewards to explore the a priori inferior arms $a_3$ and~$a_4$. 
%The goal of DM is to maximize $\lim_{T\rightarrow \infty} \sum_{t=1}^T r^t$, where $r^t$ is the reward obtain at time $t$. 
%The two descriptions are equivalent, since in case DM reveals an arm with positive reward, she can use it to explore all the other arms after finitely many rounds. To balance notational overload and rigor analysis, we favor the more simplified presentation.



\subsection{Stochastic Order}
Our main technical contribution, which we present in the next section, assumes that some of the rewards are stochastically ordered. We say that a random variable $X$ stochastically dominates (or, has first-order stochastic dominance over) a random variable $Y$ if for every $x\in (-\infty ,\infty ),$ $\Pr(X\geq x)\geq \Pr(Y\geq x)$. Observe that this dominance immediately implies that $\E\left[X\right] \geq \E\left[Y\right]$.
\begin{assumption}\label{assumption:dominance}
The reward of the arms in $\below(A)$ are stochastically ordered.
\end{assumption}
Notice that this assumption applies only to a priori inferior arms, i.e., the arms in $\below(A)$, and not to all arms. There are many natural cases for this assumption: Unit-variance (or any fixed variance) Gaussian as in Example~\ref{example with normal}, Bernoulli, log-normal, truncated normal, etc. When a formal statement relies on Assumption \ref{assumption:dominance}, we mention it explicitly. 



\section{An Auxiliary Goal Markov Decision Process}\label{sec:infinite}
In this section, we present an auxiliary GMDP problem that helps us focus on the optimal exploration order. We describe its usefulness in Subsection~\ref{subsec:bernoulli trials} and its formal representation in Subsection~\ref{subsec:aux GMDP}. We devote Subsection~\ref{subsec:p valid inefficient} to taking the first step towards efficient computation, as we narrow down the action space. Then, in Subsection~\ref{subsec:optimal GMDP policy}, we present our main technical contribution, which is featured in Policy~\ref{policy:pi star}: An optimal index-based GMDP policy. Subsection~\ref{subsec:stopchastic} further argues for the non-triviality of our results, demonstrating that even stochastically ordered rewards present significant challenges.


\subsection{Motivation for GMPD: Bernoulli Trials}\label{subsec:bernoulli trials}
Assume momentarily that there are infinitely many rounds. In this case, the following interesting property arises.
\begin{observation}\label{obs:eventually will explore}
Assume that during the execution we discover that $X(a_i)>0$ for some arm $a_i\in A$. We can then use $a_i$ to explore all other arms in finite time.
\end{observation}
We call mixing an observed arm with a positive realized reward and an unobserved arm with a negative expected reward a \emph{Bernoulli Trial}. Observation~\ref{obs:eventually will explore} suggests that if we discover an arm with a positive reward, we can use Bernoulli trials to reveal all arms in finite time almost surely. 

To illustrate, recall Example \ref{example with normal}. Assume that we are in the third round, have already explored $a_1$ and $a_2$, and discovered that $X(a_1)=x(a_1)>0$ and that $X(a_2)<0$. Furthermore, we have not yet explored $a_3$ and $a_4$ (one of which may still hide the highest realized reward among all arms). Consider the portfolio 
\begin{align*}
\bl p(a) =
\begin{cases}
\frac{-\mu(a_3)}{x(a_1)-\mu(a_3)} & \textnormal{if } a=a_1\\
\frac{x(a_1)}{x(a_1)-\mu(a_3)} & \textnormal{if } a=a_3\\
0 & \textnormal{otherwise}
\end{cases}.
\end{align*}
Observe that
\[
\sum_{a_i \in A^+} \bl p(a_i)\E\left[X(a_i)\mid \mI_3\right]
%\E_{a\sim \bl p}[X(a)\mid x(a_1)>0]
=x(a_1)\cdot \frac{-\mu(a_3)}{x(a_1)-\mu(a_3)} + \mu(a_3)\cdot  \frac{x(a_1)}{x(a_1)-\mu(a_3)}   =0;
\]
thus, $\bl p$ is IR. If we pick this portfolio, Nature flips coins to choose either $a_1$ and $a_3$, with the latter being selected with positive probability. This Bernoulli trial might result in selecting $a_1$, but we can repeat it until Nature picks $a_3$. The number of rounds required to explore $a_3$ follows the Geometric distribution with a success probability of $\nicefrac{x(a_1)}{x(a_1)-\mu(a_3)}$ in each Bernoulli trial. Indeed, by executing Bernoulli trials until the first success, we can guarantee exploring $a_3$. Zooming out from Example~\ref{example with normal} to the general case, Observation~\ref{obs:eventually will explore} suggests that once a positive reward is realized, we can execute enough Bernoulli trials to explore all the arms in finite time.

Observation \ref{obs:eventually will explore} calls for modeling that abstracts the setting once a positive reward is realized. More specifically, we can focus on the case where all unexplored arms are revealed instantly after a positive reward is discovered. To that end, we introduced the induced constrained Goal Markov Decision Process (GMDP), which we present in the next subsection. Our goal is to find the optimal policy for this GMDP, and then translate it to an asymptotically optimal algorithm for the corresponding $\ise$ instance.\footnote{We keep the terms mechanism and algorithm for solutions to $\ise$ and use the term policy for solutions of the GMDP.} 


\subsection{An Auxiliary Goal Markov Decision Process}\label{subsec:aux GMDP}
We construct the GMDP as follows:
\begin{itemize}%[leftmargin=0cm,itemindent=.5cm,labelwidth=\itemindent,labelsep=0cm,align=left]
    \item Every state is characterized by the set of unobserved arms $s \subseteq A$, and we denote the set of all states by $\mS=2^A$. The initial state is $s_0=A$. 
    \item In every state $s$, the feasible actions are the set of IR portfolio w.r.t. the prior information, i.e., from
    $
    \safe(s)=\left\{\bl p \in \Delta(s) : \sum_{a\in s}\bl p(a)\mu(a) \geq 0 \right\},
    $
    where $\Delta(s)$ is the set of all distributions over the elements of $s$. If $\safe(s)$ is empty, then we say that $s$ is \textit{terminal}. 
    \item Given a non-terminal state $s$ and an action (portfolio) $\bl p$, the transition probability is given by
    \[
    \Pr(s'\mid s, \bl p)=
    \begin{cases}
\bl p(a) & \textnormal{if $s' = s \setminus \{a\}$}\\
%$X(a') >0$ for $a'\in A\setminus s$} \\
0 & \textnormal{otherwise}
\end{cases}.
    \]
    Namely, if the realized arm is $a$, the GMDP transitions to the state $s\setminus \{ a\}$. In particular, we can reach a terminal state if we ran out of arms with a positive expected value, i.e., $\above(s)=\emptyset$, or if we have explored all arms, i.e., $s=\emptyset$.
    \item Rewards are obtained in terminal states solely. The reward of a terminal state $s$ is
\[
R(s) \defeq
\begin{cases}
\max_{a\in A} X(a) & \textnormal{if $ \safe(s)=\emptyset$ and $\max_{a'\in A\setminus s} X(a')>0$}\\
%$X(a') >0$ for $a'\in A\setminus s$} \\
0 & \textnormal{otherwise}
\end{cases}.
%\max\{0, \max_{a\in A\setminus s} X(a)  \}.
\]
In other words, once we reach a terminal state $s$, we sample $X(a)$ for every $a\in A$. If we reached $s$, then we have explored all arms $A\setminus s$. Following our intuition of Bernoulli trials, if at least one of $(X(a))_{a\in A\setminus s}$ is positive, we can use that arm to explore all other arms in finite time. Therefore, the reward we get is the maximum reward among \textit{all} arms $\max_{a\in A} X(a)$, even those we have not explored. 
\end{itemize}


A (stationary) \textit{policy} is a mapping from states to portfolios. Due to standard arguments, which are further elaborated in {\ifnum\Includeappendix=0{the appendix}\else{Section \ref{sec:thm1 outline}}\fi}, there exists an optimal policy that is also stationary. Hence, we limit our attention to stationary policies only.  Let $W(\pi,s)$ be the state-value function: The expected reward of a policy $\pi$ starting at the initial state $s$. Namely, 
\begin{align}
W(\pi,s) = 
\begin{cases}
R(s) & \textnormal{if }\safe(s)=\emptyset\\
\sum_{a\in s}\Pr(s\setminus \{a\}\mid s, \pi(s)) W(\pi,s\setminus \{a\}) & \textnormal{otherwise}
\end{cases}.
\end{align}
Moreover, let $W^{\star}(s)$ denote the highest possible reward of any policy, i.e., $W^{\star}(s)=\sup_{\pi}W(\pi,s)$; hence, $W^{\star}(A)$ symbolizes the optimal expected reward when starting from $s_0=A$. Later on, in Section~\ref{sec:policy to algorithm}, we show the connection between the optimal $\ise$ solution and this GMDP. For now, we focus on finding an optimal GMDP policy.






\subsection{$\mP$-valid Policies and an Inefficient Solution via Dynamic Programming}\label{subsec:p valid inefficient}

%\caption{THIS IS SOME TEXT!}
Next, we take a step toward finding an optimal policy by revealing the neat structure of the problem. To that end, we highlight the following family of portfolios that mix at most two arms. Consider a pair of arms, $a_i\in \above(A)$, and $a_j\in \below(A)$. The portfolio
\begin{align}\label{eq:blp from body}
\bl p_{i,j}(a) \defeq 
\begin{cases}
\frac{-\mu(a_j)}{\mu(a_i)-\mu(a_j)} & \textnormal{if } a=a_i\\
\frac{\mu(a_i)}{\mu(a_i)-\mu(a_j)} & \textnormal{if } a=a_j\\
0 & \textnormal{otherwise}
\end{cases}
\end{align}
mixes $a_i$ and $a_j$ while maximizing the probability of exploring $a_j$ (the a priori inferior arm). The reader can verify that $\bl p_{i,j}$ is indeed IR, yielding an expected value of precisely zero. For completeness, for every $a_i \in \above(A)$, we also define $\bl p_{i,i}$ as a deterministic selection of $a_i$, e.g., $\bl p_{i,i}(a)=1$ if $a=a_i$, and zero otherwise. Next, we define $\mP,\mP'$ such that
\[
\mP\defeq \{\bl p_{i,j}\mid a_i\in \above(A), a_j\in \below(A) \}, \qquad \mP'\defeq\{\bl p_{i,i}\mid a_i\in \above(A) \}.
%\mP\defeq \{\bl p_{i,j}\mid a_i\in \above(A), a_j\in \below(A) \}\cup\{\bl p_{i,i}\mid a_i\in \above(A) \}.
\]
We use the above to focus on a narrow class of portfolios, which we call \emph{$\mP$-valid}.
\begin{definition}[$\mP$-valid Portfolio]\label{def:p valid}
A portfolio $\bl p$ is \emph{$\mP$-valid} with respect to a state $s\in \mS$, if
\begin{itemize}
\item $\bl p \in \safe(s)$, and
\item If $\below(s)\neq \emptyset$, then $\bl p\in \mP$; else, if $\below(s)= \emptyset$, then $\bl p\in \mP'$.
\end{itemize} 
\end{definition}
We say a policy is \emph{$\mP$-valid} if, for all states $s\in \mS$, $\pi(s)$ is a \emph{$\mP$-valid} portfolio. Notice that $\mP$-valid policies are IR by definition. Furthermore, they mix at most two actions from $A$ in each state; hence, there are  $O(K^2)$ actions $\mP$-valid policies can take in every state. This is in sharp contrast to the set of actions $\safe(s)$ for a state $s$, which is generally a convex polytope with infinitely many actions. 

Due to the linearity of the value function $W(\pi,s)$ in $\pi(s)$, the GMDP exhibits a nice structural property, as captured by the following Proposition \ref{prop:main optimal p valid}. 
\begin{proposition}\label{prop:main optimal p valid}
Fix any arbitrary $\ise$ instance and observe its induced GMDP instance. There exists an optimal policy which is $\mP$-valid.
\end{proposition}
The proof of this proposition appears in {\ifnum\Includeappendix=0{the appendix}\else{Section~\ref{sec:aux}}\fi}. While the GMDP is defined using a continuous action space in each state (the convex polytope $\safe(s)$), Proposition~\ref{prop:main optimal p valid} allows us to narrow down the search to $\mP$-valid portfolios. Particularly, we can find an optimal policy with dynamic programming using a bottom-up approach in time $O(2^K K^2)$, as we explain next.

For every $s \in \mS$, compute $W^{\star}(s)$ by exhaustively searching over $\mP$-valid portfolios and using previously obtained $W^{\star}(s')$ for $s' \subset s$. Since $s$ leads to states of the form $s/{a}$ for $a\in s$, we can compute the expected reward for each $\mP$-valid portfolio and pick the best portfolio. There are $2^K$ states and at most $O(K^2)$ $\mP$-valid portfolios in every state, each taking $O(1)$ computations to assess; therefore, this by itself guarantees finding an optimal policy in time $O(2^K K^2)$. Noticeably, this runtime could be intractable for large $K$. 
In the next subsection, we show how to substantially reduce the computation.

\subsection{Efficiently Computing an Optimal GMDP Policy}\label{subsec:optimal GMDP policy}
% This is the WINE version!
% \iffalse
% \begin{figure}[tb]
% \centering
% \begin{minipage}{.48\linewidth}
% \begin{algorithmpolicy}[H]
% \caption{Optimal GMDP Policy ($\OGP$)\label{policy:pi star}}
% \begin{algorithmic}[1]
% \REQUIRE a state $s\subseteq A$.
% \ENSURE a $\mP$-valid portfolio or $\emptyset$.
% \IF{$s$ is terminal \label{policy:if terminal}} {
% \STATE \textbf{return} $\emptyset$. \label{policy:return empty}
% }
% \ELSE\label{policy:non terminal}{
% \STATE pick any arbitrary $a_i \in \above(s)$.\label{policy:pick arbitrary}
% \IF{$\below(s)=\emptyset$\label{policy:if no below}} {
% 		\RETURN $\bl p_{i,i}$. \label{policy:return double above}
% }
% \ELSE\label{policy:if has below}{
% \STATE pick $a_{j^\star}\in \argmax_{a_j \in \below(s)} \mu(a_j)$.\label{policy:pick below}
% 		\RETURN $\bl p_{i,j^\star}$.\label{policy:return mix}
% }
% \ENDIF
% }
% \ENDIF
% \end{algorithmic}
% \end{algorithmpolicy}
% \end{minipage}
% ~~~
% \begin{minipage}{.48\linewidth}

% %\vspace{-8.8mm} %use to make them look
% \begin{figure}[H]
%     \includegraphics[scale=0.8]{figs/GMDP_body_crop.pdf}
%     \small
%     \caption{Illustration of $\OGP$ for Example~\ref{example with normal}.\label{fig:tree small}}
% \end{figure}
% \end{minipage}
% \end{figure}
% \fi



%\LinesNumbered
%\DontPrintSemicolon 
\begin{algorithmpolicy}[t]
%\begin{algorithm}[t]
\LinesNumbered
\DontPrintSemicolon
\caption{Optimal GMDP Policy ($\OGP$)\label{policy:pi star}}
\KwIn{a state $s\subseteq A$}
\KwOut{a $\mP$-valid portfolio or $\emptyset$}
\If{$s$ is terminal\label{policy:if terminal}}{
    \Return $\emptyset$\label{policy:return empty}
}
\Else{
    \nllabel{policy:non terminal}
    pick any arbitrary $a_i \in \above(s)$\label{policy:pick arbitrary}\;
    \If{$\below(s)=\emptyset$\label{policy:if no below}}{
        \Return $\bl p_{i,i}$\label{policy:return double above}\;
    }
    \Else{
        \nllabel{policy:if has below}
        pick $a_{j^\star}\in \argmax_{a_j \in \below(s)} \mu(a_j)$\label{policy:pick below}\;
        \Return $\bl p_{i,j^\star}$\label{policy:return mix}\;
    }
}
\end{algorithmpolicy}
%\end{algorithm}



We start this subsection by presenting an optimal GMDP policy (hereinafter $\OGP$ for shorthand), which we formalize via Policy~\ref{policy:pi star}, and argue for its optimality later. Given a state $s$, $\OGP(s)$ operates as follows. If $s$ is terminal, it returns the empty set (Lines~\ref{policy:if terminal}--\ref{policy:return empty}). Otherwise, if $s$ is non-terminal, we enter the ``else'' clause in Line~\ref{policy:non terminal}, and pick any arbitrary arm $a_i$ from $\above(s)$. Then, we have two cases. If $\below(s)$ is empty, $\OGP$ returns $\bl p_{i,i}$ (Line~\ref{policy:return double above}). Else, it picks the best arm from $\below(s)$ in terms of expected reward, which we denote by $a_{j^\star}$ (Line~\ref{policy:pick below}), and returns $\bl p_{i,j^\star}$ (Line~\ref{policy:return mix}). Overall, $\OGP$ returns $\emptyset$ if $s$ is terminal or a $\mP$-valid portfolio if $s$ is non-terminal.
%The policy $\OGP$ is basically a family of policies: In every non-terminal state $s$, it picks an arm from $\above(s)$ (note that being non-terminal implies $\above(s)\neq \emptyset$). Further, if $\below(s) \neq \emptyset$, it picks an arm from $\below(s)$ with the highest expected value among the arms of $\below(s)$. Then, it selects the corresponding portfolio from the family $(\bl p_{i,j})_{i,j}$. 
%$\OGP$ reveals a combinatorial structure of a family of optimal policies: They mix any arm from $\above(s)$ with the best arm from $\below(s)$ (in the stochastic dominance sense). 
\begin{theorem}\label{thm:optimal policy}
Fix any arbitrary $\ise$ instance satisfying Assumption~\ref{assumption:dominance} and observe its induced GMDP instance. For any state $s\subseteq A$, $\OGP$ prescribed in Policy~\ref{policy:pi star} satisfies $W(\OGP,A)=W^{\star}(A)$.
\end{theorem}
\begin{proof}[\textnormal{\textbf{Proof Overview of Theorem \ref{thm:optimal policy}}}]
We defer the full proof to {\ifnum\Includeappendix=0{the appendix}\else{Section~\ref{sec:thm1 outline}}\fi} and outline it below.
To prove this theorem, we focus on the dynamic programming explained at the end of the previous subsection. We unveil its crux and flesh out the index structure $\OGP$ uses. A crucial ingredient of the analysis is the probability of reaching the empty state, representing the case in which we explored all arms. More formally, for every state $s\in \mS$ and a policy $\pi$, we denote by $Q(\pi,s)$ the probability starting at $s\subseteq A$, following the policy $\pi$ and reaching the empty state. Note that, similarly to $W$ the state-value function, $Q$ is defined recursively: Namely, if $\pi(s)=\bl p_{i,j}$ for a non-terminal state $s$, then \[ Q(\pi,s)=\bl p_{i,j}(a_i)Q(\pi,s\setminus\{a_i\})+\bl p_{i,j}(a_j)Q(\pi,s\setminus\{a_j\}). \]
We prove a rather surprising feature of $Q$, as captured in the following lemma.
\begin{lemma}[Equivalence Lemma]\label{lemma:equivalence body}
For every two $\mP$-valid policies $\pi,\rho$ and every state $s\in \mS$, it holds that $Q(\pi,s)=Q(\rho,s)$.
\end{lemma}
The Equivalence Lemma asserts that $Q$ is policy-independent, a property that significantly simplifies the analysis. The proof of this lemma is intricate and relies on a careful factorization of the elements of $Q$.  Interestingly, the arguments we have mentioned so far, including the Equivalence Lemma, are independent of Assumption \ref{assumption:dominance}.

Equipped with the Equivalence Lemma, we use the recursive structure of $Q$ and the reward function $W$ to prove the optimality of $\OGP$ inductively over the sizes of $\above(s)$ and $\below(s)$. This step makes use of further insights into the dynamic programming procedure, such as monotonicity in the set of available arms.
\end{proof}

We exemplify $\OGP$ for the induced GMDP of Example~\ref{example with normal} in Figure~\ref{fig:tree small}. 
Recall that 
$X(a_1)\sim N(2,1)$, $X(a_2)\sim N(1,1)$, $X(a_3)\sim N(-1,1)$, and $X(a_4)\sim N(-2,1)$. Consequently, $\above(A)=\{a_1,a_2\}$, $\below(A)=\{a_3,a_4\}$, and $\mu(a_3)=-1 > -2=\mu(a_4)$. Every node is a state---the root is $s_0=A$. In $s_0$, we select the portfolio $\bl p_{1,3}$, which mixes $a_1\in \above(A)$ and $a_3 \in \below(A)$ (selecting $\bl p_{2,3}$ would also be optimal, since Line~\ref{policy:pick arbitrary} allows us to pick any $a_i\in\above(A)$). The split follows from Nature's coin flips: Left if the realized action is $a_1$ (w.p. $\bl p_{1,3}(a_1)$), and right if the action is $a_3$ (w.p. $\bl p_{1,3}(a_3)$). The leaves $\{a_3,a_4\}$, $\{a_4\}$, and $\{\}$ are the terminal states. 


\begin{figure}[t]
\centering
    \includegraphics[scale=1]{figs/GMDP_body_crop.pdf}
    %\small
    \caption{Illustration of $\OGP$ for Example~\ref{example with normal}.\label{fig:tree small}}
\end{figure}
\subsection{Stochastic Order and Non-triviality}\label{subsec:stopchastic}
Before leveraging $\OGP$ to an optimal $\ise$ algorithm, it is important to describe why Theorem~\ref{thm:optimal policy} is non-trivial to prove, even with the limiting Assumption~\ref{assumption:dominance}. Consider a state $s\in\mS$ and three arms $a_i,a_j ,a_{\tilde j}$ such that
\[
a_i\in \above(s), a_j \in \argmax_{a_j \in \below(s)} \mu(a_{j'}), \mu(a_{\tilde j})< \mu(a_j).
\]
Namely, $a_i$ has a positive expected value, while $a_j, a_{\tilde j}$ have negative expected values and $a_j$ stochastically dominates $a_{\tilde j}$. The action $\bl p_{i,j}$, which mixes $a_i$ with the minimal element $a_j$ according to the stochastic order on $\below(s)$, is weakly superior to  $\bl p_{i,{\tilde{j}}}$ if 

{
\thinmuskip=.2mu
\medmuskip=0mu plus .2mu minus .2mu
\thickmuskip=1mu plus 1mu
\[
\Big(\bl p_{i,j}(j)W^{\star}(s\setminus \{a_j\})+\bl p_{i,j}(i)W^{\star}(s\setminus\{a_i\})\Big)-\left(\bl p_{i,{\tilde j}}({\tilde j})W^{\star}(s\setminus \{a_{\tilde j}\})+\bl p_{i,{\tilde j}}(i)W^{\star}(s\setminus\{a_i\})\right) \geq 0.
\]}%
Rearranging,
\begin{align}\label{eq:why hard body}
&\bl p_{i,j}(j)W^{\star}(s\setminus \{a_j\})-\bl p_{i,{\tilde j}}({\tilde j})W^{\star}(s\setminus \{a_{\tilde j}\})+\left(\bl p_{i,j}(i)-\bl p_{i,{\tilde j}}(i)\right)W^{\star}(s\setminus\{a_i\}) \geq 0.
\end{align}%
By our selection of $a_j,a_{\tilde j}$, we know that $\bl p_{i,j}(i)-\bl p_{i,{\tilde j}}(i) \leq 0$; hence, the third term is non-positive. Interestingly, as we show in~{\ifnum\Includeappendix=0{the appendix}\else{Claim \ref{claim:ass is not for W} in Section \ref{sec:aux}}\fi}, stochastic order does not imply that $W^{\star}(s\setminus \{a_j\}) \geq W^{\star}(s\setminus \{a_{\tilde j}\})$; thus, it is unclear that the expression
{
\begin{align*}
&\bl p_{i,j}(j)W^{\star}(s\setminus \{a_j\})-\bl p_{i,{\tilde j}}({\tilde j})W^{\star}(s\setminus \{a_{\tilde j}\}),
\end{align*}}%
which appears in Inequality \eqref{eq:why hard body}, is non-negative. Therefore, we cannot prove Inequality  \eqref{eq:why hard body} without revealing the structure of $W^{\star}$, even when Assumption \ref{assumption:dominance} holds. 























% Consider a state $s\in\mS$, such that $\above(s),\below(s) \geq 2$. Let $a_i = \argmin_{a_{i'}\in\above(s)}\sigl_\pi(a_{i'})$, and  $a_j = \argmin_{a_{j'}\in\below(s)}\sigr_\pi(a_{j'})$. In addition, let $a_{\tilde j}\in \below(s), a_{\tilde j} \neq a_j$. The action $\bl p_{i,j}$, which mixes the minimal elements according to the stochastic order, is weakly superior to  $\bl p_{i,{\tilde{j}}}$ if 
% {
% \begin{align}\label{eq:why hard}
% &\bl p_{i,j}(j)W^{\star}(s\setminus \{a_j\})-\bl p_{i,{\tilde j}}({\tilde j})W^{\star}(s\setminus \{a_{\tilde j}\})+\left(\bl p_{i,j}(i)-\bl p_{i,{\tilde j}}(i)\right)W^{\star}(s\setminus\{a_i\}) \geq 0.
% \end{align}}%
% By our selection of $a_j,a_{\tilde j}$, we know that $\bl p_{i,j}(i)-\bl p_{i,{\tilde j}}(i) \leq 0$; hence, the third term is non-positive. Moreover, as we show in Claim \ref{claim:ass is not for W} in Section~\ref{sec:aux}, %~{\ifnum\Includeappendix=0{the appendix,}\else{Claim \ref{claim:ass is not for W} in Subsection \ref{subsec: statments},}\fi}
% stochastic dominance does not imply that $W^{\star}(s\setminus \{a_j\}) \geq W^{\star}(s\setminus \{a_{\tilde j}\})$; thus, it is not even clear that the expression
% {
% \begin{align*}
% &\bl p_{i,j}(j)W^{\star}(s\setminus \{a_j\})-\bl p_{i,{\tilde j}}({\tilde j})W^{\star}(s\setminus \{a_{\tilde j}\})
% \end{align*}}%
% which accounts for the first two terms in Inequality (\ref{eq:why hard}), is non-negative. Therefore, we cannot claim for Inequality  (\ref{eq:why hard}) without revealing the structure of $W^{\star}$, even when Assumption \ref{assumption:dominance} holds. 





\section{From GMDP to an Approximately Optimal $\ise$ Algorithm}\label{sec:policy to algorithm}
In this section, we leverage $\OGP$, which is the optimal policy for the GMDP presented in the previous section, to an approximately optimal algorithm, which we call $\SEGB$ and is outlined in Algorithm~\ref{alg:alg of pi}. We start by addressing the infinite number of rounds, and then present a finite-time analysis. %Finally, we devise a BIC mechanism for the case of strategic agents.

\subsection{The Case of $T\rightarrow \infty$}
Recall that the GMDP of Section~\ref{sec:infinite} mimics the reward any algorithm could obtain, since it abstracts the need for Bernoulli trials. In other words, 
\begin{observation}\label{obs: U leq W*}
For any algorithm $\ALG$, it holds that $\lim_{T \rightarrow \infty }\mU_T(\ALG) \leq W^{\star}(A)$.
\end{observation}
%To see this, notice that any algorithm $\ALG$ eventually explore a set of arms. If $\ALG$ is 
To see this, note that for any $\ALG$, there exists a modified algorithm that is deterministic and uses portfolios of at most two arms. Then, there is a modified algorithm that first performs exploration (mixed portfolios of unexplored arms), then Bernoulli trials, and finally exploitation. Those modifications can only increase $\ALG$'s social welfare reward, with an asymptotic value of $W^{\star}(A)$.
%we focus on the \textit{lucky event}: The event that every Bernoulli trial succeeds. This only affects a finite number of rounds, and thus does not change $\mU_T(\ALG)$.
%notice that by assuming that anytime $\ALG$ performs a Bernoulli trials it 


Furthermore, given any policy $\pi$ for the GMDP, we can construct a \emph{wrapper algorithm} $\ALG(\pi)$ for the corresponding $\ise$ instance by wrapping $\pi$ with Bernoulli trials. At the beginning, $\ALG(\pi)$ picks portfolios according to $\pi$ and updates the state until it reaches a terminal state. Then, if a positive reward is realized, $\ALG(\pi)$ executes Bernoulli trials until full exploration. Otherwise, it selects the default arm forever (we elaborate more when we explain Algorithm~\ref{alg:alg of pi}). It is straightforward to see that
\begin{observation}\label{obs: U get W}
For any policy $\pi$, there exists an algorithm $\ALG(\pi)$ such that 
\[
\lim_{T \rightarrow \infty }\mU_T(\ALG(\pi)) = W(\pi,A).
\]
\end{observation}
An immediate corollary of Observations~\ref{obs: U leq W*} and~\ref{obs: U get W} is a sandwich argument. For an optimal policy $\pi^\star$, namely, $W(\pi^\star,A)=W^{\star}(A)$, it holds that
\begin{equation}\label{eq:sandwith}
\OPT_{\infty} \leq W(\pi^\star,A) = \lim_{T\rightarrow \infty}\mU_T(\ALG(\pi^\star))  \leq  \OPT_{\infty};  
\end{equation}
thus, wrapping the optimal GMDP policy results in optimal welfare. 

We are ready to present $\SEGB$, which is implemented in Algorithm~\ref{alg:alg of pi}. Following the notation of Observation~\ref{obs: U get W}, $\SEGB=\ALG(\OGP)$, namely $\SEGB$ wraps $\OGP$. $\SEGB$ picks portfolios according to $\OGP$ until it reaches a terminal state (Lines~\ref{algpi:while}--\ref{algpi:upd s}). Then, if $\SEGB$ realized a positive reward (Line~\ref{algpi:pos then geo}), it executes Bernoulli trials to explore all arms (Line~\ref{algpi:geo}), and exploits the best arm (Line~\ref{algpi:exploit best}). Otherwise, it selects the default arm $a_0$ forever. Inequality~\eqref{eq:sandwith} and Theorem~\ref{thm:optimal policy} suggest the following corollary.
\begin{corollary}\label{cor: alg is asym opt}
Fix any arbitrary $\ise$ instance satisfying Assumption~\ref{assumption:dominance}. It holds that 
\[\lim\limits_{T \rightarrow \infty }\mU_T(\SEGB) = \OPT_{\infty}.
\]
\end{corollary}

\iffalse
\begin{algorithm}[t]
\caption{Individually Rational Exploration via GMDP and Bernoulli Trials ($\SEGB$)\label{alg:alg of pi}}
\DontPrintSemicolon
\SetAlgoLined

$s \gets A$\\[1mm]

\While{$\OGP(s) \neq \emptyset$}{
    \nllabel{algpi:while}
    select $\OGP(s)$, and denote the realized action by $a_k$
    \nllabel{algpi:play with ogp}\\
    $s \gets s \setminus \{a_k\}$\\
    \nllabel{algpi:upd s}
}
\If{$x_{a_k} > 0$ for some $a_k \in A$}{
    \nllabel{algpi:pos then geo}
    $a_{k^\star} \gets \argmax_{a_i} x(a_i)$ \tcp*{\texttt{\color{blue}{\#best among all the explored arms}}}\\
    execute Bernoulli trials mixing $a_{k^\star}$ with every other unexplored arm until all are revealed\\
    \nllabel{algpi:geo}\\
    select the best arm forever\\
    \nllabel{algpi:exploit best}
}
\Else{
    select the default arm $a_0$ forever\\
    \nllabel{algpi:exploit a0}
}
\end{algorithm}
\fi



%\omer{algorithm here is bad!}


\begin{algorithm}[t]
\LinesNumbered
\SetNoFillComment
\DontPrintSemicolon
\caption{Individually Rational Exploration via GMDP and Bernoulli Trials ($\SEGB$)\label{alg:alg of pi}}
\KwIn{the $\OGP$ policy}
$s \gets A$\; % (no label on this line)
\While{$\OGP(s) \neq \emptyset$\nllabel{algpi:while}}{%
    \tcp{\texttt{\color{blue}{$s$ is not a terminal state}}}%
    select $\OGP(s)$, and denote the realized action by $a_k$\nllabel{algpi:play with ogp}\;
    $s \gets s \setminus \{a_k\}$\nllabel{algpi:upd s}
}
\If{$x_{a_k} > 0$ for some $a_k \in A$\nllabel{algpi:pos then geo}}{%
    \tcp{\texttt{\color{blue}{A positive reward is revealed}}}%
    $a_{k^\star} \gets \argmax_{a_i} x(a_i)$\;
    \tcp{\texttt{\color{blue}{best among all the explored arms}}}%
    execute Bernoulli trials mixing $a_{k^\star}$ with every other unexplored arm until all are revealed\nllabel{algpi:geo}\;
    select the best arm forever\nllabel{algpi:exploit best}\;
}
\Else{
    select the default arm $a_0$ forever\nllabel{algpi:exploit a0}\;
}
\end{algorithm}




\subsection{Convergence Rate}%(TO BE CHANGED LATER}
Despite $\SEGB$'s optimality when $T\rightarrow \infty$, its welfare is lower than $\OPT_\infty$ for finite $T$. In this subsection, we analyze $\SEGB$'s performance for finite time and bound its convergence rate. Our goal is to show the gap between $\mU_T(\SEGB)$ and $\OPT_\infty$. The analysis of this subsections assumes that $\abs{X_i}\leq H$ for some $H\in \mathbb R^+$ for all $i\in\{1,\dots K\}$.

We begin by noting that in special cases, $\SEGB$ can be slightly modified to achieve $\OPT_\infty$ even for a finite $T$. To illustrate, consider $(X(a_i))_i$ that are supported on $\{x^-,x^+\}$, with $x^+>0$ and $x^- <0$. In such a case, revealing a positive reward of $x^+$ suggests that we need not explore any further (since the other rewards cannot outperform $+1$); thus, the Bernoulli trials in Line~\ref{algpi:geo} become redundant. Formalizing this intuition,
\begin{proposition}\label{prop:bernoulli opt}
Fix any arbitrary $\ise$ instance such that $(X(a_i))_i \in \{x^-,x^+\}$ w.p. 1, i.e., the rewards take only two possible values almost surely. Let $\SEGB'$ be a modified version of $\SEGB$ that exploits once a reward of $x^+$ is realized. Then, there exists $T_0$ such that whenever $T\geq T_0$, $\mU_T(\SEGB') =\OPT_\infty$.
\end{proposition}
%\omer{for this to occur, $\SEGB'$ must use a policy that orders $\above(A)$ according to the stochastic order. And, it should stop using $\OGP$ once it realizes a positive arm.}
However, $\SEGB$ cannot achieve $\OPT_\infty$ in finite time due to two factors. First, the Bernoulli trials in Line~\ref{algpi:geo} might take several rounds, thereby delaying the potentially higher rewards hidden by risky arms. Consequently, the number of rounds that can benefit from these high rewards is limited (unlike in the $T\rightarrow\infty$ case). Second, the Bernoulli trials can be unfruitful in expectation. To illustrate, assume that there are only a few rounds left and one unexplored arm from $\below(A)$. In such a case, the one-time cost of exploring this arm outweighs the value it would provide in the remaining rounds. Deciding whether to mix such arms in Bernoulli trials or to exploit the best-seen arm constitutes another barrier. We leave this technical challenge for future work. Instead, our goal is to bound the time it takes to reveal all arms and address the potential performance gap it causes. 

Given an $\ise$ instance, let the random variable $\delta$ denote the highest positive reward among the arms of $\above(A)$, i.e., $\delta = \max_{a_i\in \above(A): X(a_i)>0} X(a_i)$. Recall that in Line~\ref{algpi:geo} of Algorithm~\ref{alg:alg of pi}, we execute Bernoulli trials with the best-seen arm. The term $\delta$ constitutes a lower bound on the reward of that arm $a_{k^\star}$. Furthermore, let $\eta = \max_{a_i\in A:\mu(a_i)<0}\abs{\mu(a_i)}$. Notice that $\eta$ quantifies the highest absolute value of an arm in $\below(A)$. The success probability of each Bernoulli trial, which leads to exploring one additional arm, is at least 
$ \frac{\delta}{\delta+\eta}$. Consequently, after $K(1+{\eta}\E\left[{\frac{1}{\delta}}\right])$ Bernoulli trials in expectation, we would explore all arms. We formalize this intuition through Proposition~\ref{prop:i-d bounds} below.
\begin{proposition}\label{prop:i-d bounds}
Fix any arbitrary $\ise$ instance satisfying Assumption~\ref{assumption:dominance}, and let $\delta>0$ and $\eta$ be the quantities defined above. There exists $T_0$ such that whenever $T>T_0$, it holds that
$
\mU_T(\SEGB) \geq \left(  1-\frac{K(1+\eta\cdot \E\left[{\frac{1}{\delta}}\right]}{T}\right) \OPT_\infty.
$
\end{proposition}
To illustrate, assume that $(X(a_i))_i$ are arbitrarily distributed in the discrete set $\{-H,\dots, H\}$. In such a case, as long as $\delta > 0$, $\frac{\eta}{\delta} \leq H$ holds almost surely; thus,  Proposition~\ref{prop:i-d bounds} suggests that $\SEGB$ is optimal up to a multiplicative factor of $\frac{K(H+1)}{T}$ or an additive factor of $\frac{KH(H+1)}{T}$.


\section{Strategic Agents and Bayesian Incentive Compatibility}\label{sec:ic body}
In this section, we address the case of \emph{strategic agents}: Agents only see the mechanism's recommendation, but are free to select any arm. In Subsection~\ref{subsec:enabling}, we lie several observations and assumptions to allow exploration. Then, Subsection~\ref{subsec:BIC ALG} presents $\ICSEGB$, which is implemented in Algorithm~\ref{mainicalg}. To simplify our analysis, we shall assume that the rewards are bounded. Namely, there exists $H \in \mathbb R$ such that for every $a_i \in A^+$ it holds that $\abs{X(a_i)} \leq H$ almost surely.

\subsection{Enabling Exploration and Harmless Mechanisms}\label{subsec:enabling}
In the case of strategic agents, agents would naturally try to avoid exploration. However, as mechanism designers, we are still interested in maximizing social welfare and, consequently, in exploring risky arms. To allow such an exploration, we must have the standard assumption below. 
\begin{assumption}\label{assumption ic body support}
For every pair $i,j$ such that $i,j\in A^+$, it holds that $\Pr(X_i < \mu_j)>0$.
\end{assumption}
If Assumption \ref{assumption ic body support} does not hold for some pair $i,j$, agents would never agree to explore arm $a_j$ even though it could hide a higher reward than $a_i$.


The main technique used in previous work~\cite{mansour2015bayesian,Mansour2016Slivkins} is \emph{hidden exploration}: Crafting the mechanism in such a way that agents cannot determine whether they are exploring or exploiting. As a result, a crucial element that affects BIC is the agents' knowledge about the \emph{order of arrival}. In the BIC constraint in Definition~\ref{def:ic}, the expectation is conditioned on all the information the agent has about their arrival order. For instance, assume that agents have a uniform belief about their order of arrival and fix any a priori inferior arm $a$. Notice that there is at most one round of exploration for each arm $a$ and potentially many exploitation rounds (under the event that $a$ is observed to be the best arm). Consequently, given enough agents, the probability of exploitation is higher than exploration from an agent's standpoint.
\begin{proposition}\label{prop:ic for uniform}
Fix any arbitrary $\ise$ instance, and assume uniform belief over arrival orders. There exists $T_0$ such that for any $T>T_0$, $\SEGB$ is BIC.
\end{proposition}
The more common and challenging case, which is the standard in prior work~\cite{Kremer2014,mansour2015bayesian}, is the informative order, where each agent knows their round number exactly. Namely, the agent arriving at time $l$ knows that she is the $l$'th agent. We focus on this setting for the remainder of this section.

It turns out that we must have stronger information asymmetry regarding the default arm $a_0$, which we previously assumed has a known reward of zero.\footnote{Assuming $X(a_0)=0$ almost surely is without loss of generality, as all of our results in Sections~\ref{sec:infinite} and~\ref{sec:policy to algorithm} extend beyond it. In this section, assuming the default arm has a deterministic reward is \emph{with} loss of generality, and hence we elaborate.} To illustrate the complexity, we highlight the following property.
\begin{definition}[Harmless Mechanism]
    A mechanism is called \emph{harmless} if, for any round $t$, information $\mI_t$ and any realized arm it selects $m^t$, it holds that $\Pr(X(m^t)>0\mid I_t)>0$.
\end{definition}
In other words, if an arm $a$ is observed to have a negative reward, the mechanism will avoid recommending $a$ in subsequent rounds almost surely. If a mechanism is not harmless, we say it is \emph{harmful}. What can a mechanism gain from being harmful? The following observation answers this question.
\begin{observation}\label{obs: harmless not exploring}
Assume that $X(a_0)=0$ almost surely and that $\max_{a_i\in \above(A)}X_i <0$. If an IR and BIC mechanism explores the arms of $\below(A)$, it is necessarily harmful.
\end{observation}
We now explain why this observation holds. Fix any harmless mechanism and assume w.l.o.g. that the arms are sorted in decreasing order of expectation. The first agent, knowing she is the first, will only agree to pull $a_1$, the a priori optimal arm. Therefore, any IC mechanism will recommend the portfolio $\bl p_{1,1}$. Next, consider the second agent. She knows that the only arm that has been explored so far is $a_1$. Thus, if the mechanism recommends anything but $a_1$, she will pick $a_2$. Since $X(a_1)<0$ due to the assumptions of the observation ($\max_{a_i\in \above(A)}X_i <0$), any harmless mechanism cannot mix $a_1$ in a portfolio. Thus, the second agent pulls $a_2$. This argument holds inductively for all arms in $\above(A)$.  From here on, we focus on harmless mechanisms only. 

In light of Observation~\ref{obs: harmless not exploring}, we need a stronger form of information asymmetry regarding the default arm; hence, we shall make the standard assumption that the default arm $a_0$ is a priori superior~\cite{Fiduciary,Kremer2014}. This reflects cases where agents are generally informed about the attractiveness of the arms and prefer the default arm over the others. 
\begin{assumption}\label{assumption order}
The expected value of the rewards satisfy $\mu(a_0)>\mu(a_1)\geq \mu(a_2)\cdots \geq \mu(a_K)$.
\end{assumption}
For completeness, we also remark that the IR constraint from Inequality~\eqref{ineq:IR def} becomes
\[
\E_{a\sim \bl p}[X(a) \mid \mI]=\sum_{a_i\in A}\bl p (a_i)\E\left[X(a_i)\mid \mI\right]\geq\E\left[X(a_0)\mid \mI\right].
\]
Notably, the right-hand side is no longer zero. Due to Assumption~\ref{assumption order}, any IR mechanism must recommend the default arm to the first agent. 


% to maximize social welfare. 
%We show that as long as the reward of the default arm is stochastic and attractive enough, namely $\mu(a_0)\geq\mu(a_i)$ for $a_i\in A$, we can devise a BIC mechanism that uses $\SEGB$. Due to space limitations, we defer the full details to~{\ifnum\Includeappendix=0{the appendix}\else{Section \ref{sec:IC in app}}\fi}.


\subsection{The $\ICSEGB$ Algorithm}\label{subsec:BIC ALG}
%\section{Bayesian Incentive Compatibility}\label{sec:IC in app}

\begin{algorithm}[tb]
\LinesNumbered
\SetNoFillComment
\DontPrintSemicolon
\caption{Incentive Compatible $\mainalg$ \label{mainicalg} ($\ICSEGB$)}
\KwIn{the $\mainalg$ algorithm}
Initialize an instance of $\mainalg$ and update it after every recommendation\nllabel{alg-ic:initialize}\;
Recommend the default arm $a_0$ to the first agent, observe $x(a_0)$\nllabel{alg-ic:first agent}\;
\If{$x(a_0) < \mu_K$\nllabel{alg-ic:if r1 is bad}}{%
    recommend as \gre to agents $2, \dots, K+1$\nllabel{alg-ic: r1 is bad}\;
}
\Else{%
    recommend $a_0$ to agents $2, \dots, K+1$\nllabel{alg-ic: r1 after one}\;
}
Split the remaining rounds into consecutive phases of $B$ rounds each\nllabel{alg-ic:split}\;
\For{phase $k = 1, \dots$\nllabel{alg-ic:for loop}}{%
    \If{$\mainalg$ exploits (Lines \ref{algpi:exploit best}-\ref{algpi:exploit a0} in $\mainalg$)\nllabel{alg-ic:if exploits}}{%
        follow $\mainalg$\nllabel{alg-ic:exploits}\;
    }
    \Else{\nllabel{alg-ic:else block explore}%
        Pick one agent $l(k)$ from the $B$ agents in this phase uniformly at random and recommend her according to $\mainalg$\nllabel{alg-ic: pick to explore}\;
        \If{an arm $a_i$ with $x(a_i) > x(a_0)$ was revealed \nllabel{alg-ic:if high found}}{%
            recommend as \gre\nllabel{alg-ic: recommend as greedy if observed}\;
        }
        \Else{%
            recommend $a_0$\nllabel{alg-ic: recommend a on in phase}\;
        }
    }
}
\end{algorithm}




\iffalse
\begin{algorithm}[tb]
\caption{Incentive Compatible $\mainalg$ \label{mainicalg} ($\ICSEGB$)}
\begin{algorithmic}[1]
\STATE Initialize an instance of $\mainalg$ and update it after every recommendation.\label{alg-ic:initialize} 
\STATE Recommend the default arm $a_0$ to the first agent, observe $x(a_0)$. \label{alg-ic:first agent}
\STATE\lIF{$x(a_0) < \mu_K$}{recommend as \gre to agents $2,\dots, K+1$.\label{alg-ic: r1 is bad}}%
\STATE\lELSE{recommend $a_0$ to agents $2,\dots, K+1$. \label{alg-ic: r1 after one}
}
\STATE Split the remaining rounds into consecutive phases of $B$ rounds each. \label{alg-ic:split}
\FOR {phase $k=1,\dots$ \label{alg-ic:for loop}}{
\IF{$\mainalg$ exploits (Lines \ref{algpi:exploit best}-\ref{algpi:exploit a0} in $\mainalg$) \label{alg-ic:if exploits}}
 {
\STATE follow $\mainalg$. \label{alg-ic:exploits} 
}
\ELSE {\label{alg-ic:else block explore}
\STATE Pick one agent $l(k)$ from the $B$ agents in this phase uniformly at random and recommend her according to $\mainalg$. \label{alg-ic: pick to explore}
As for the rest of the agents,
\STATE \lIF{an arm $a_i$ with $x(a_i)>x(a_0)$ was revealed}{recommend as \gre.} \label{alg-ic: recommend as greedy if observed}
\STATE \lELSE{recommend $a_0$.} \label{alg-ic: recommend a on in phase}
}
\ENDIF
}
\ENDFOR
\end{algorithmic}
\end{algorithm}
\fi
In this subsection, we present a Bayesian incentive-compatible mechanism with approximately optimal welfare. The algorithm, which we call $\ICSEGB$ and is implemented in Algorithm~\ref{mainicalg}, uses $\mainalg$ to conduct exploration under the IR and BIC constraints. From a technical standpoint, our approach builds on the techniques introduced by~\citet{mansour2015bayesian}. We divide rounds into \textit{phases}: In each phase, there is at most one exploration round (following $\mainalg$). The other rounds are either recommending the default arm $a_0$ or greedy exploitation. We denote by $\gre$ the algorithm that picks the best arm in expectation over the information it has (could be an already observed arm).


The algorithm works as follows. It initializes an $\mainalg$ instance in Line~\ref{alg-ic:initialize}. For the first agent to arrive, it recommends the default arm and observes its realized reward in Line~\ref{alg-ic:first agent}. Then, if the default arm realizes a very low reward (Line~\ref{alg-ic:if r1 is bad}), we continue by following $\gre$ for the next $K$ agents (Line~\ref{alg-ic: r1 is bad}). Otherwise, in Line~\ref{alg-ic: r1 after one}, we recommend the default arm for those agents. Line~\ref{alg-ic:split} splits the next rounds into phases, each with $B$ rounds (we determine $B$ later on). The for loop in Line~\ref{alg-ic:for loop} contains two possible behaviors. If $\mainalg$ exploits (the ``if'' clause in Line~\ref{alg-ic:if exploits}), we exploit as well. Otherwise, we enter the `else'' clause in Line~\ref{alg-ic:else block explore}. One of the agents in this phase, who we note by $l(k)$ and is chosen uniformly at random, will get a recommendation according to $\mainalg$. The other agents will either get the recommendation of $\gre$ or the default arm. The former refers to the ``if'' clause in Line~\ref{alg-ic:if high found}: As long as we discover an arm $a_i$ with $x(a_i)>x(a_0)$, which means that $\mainalg$ can explore all arms using Bernoulli trails. This completes the description of the algorithm.





To analyze the social welfare of $\ICSEGB$, we need to determine the phase length $B$; thus, we introduce the following quantities $\xi$ and $\gamma$. Assumption~\ref{assumption ic body support} hints that there exist $\xi>0$ and $\gamma>0$ such that every arm $i \in A^+$ has a chance of at least $\gamma$ to be greater than any other arm by at least $\xi$. Formally, for all $i\in A^+$, it holds that $\Pr(\forall i'\in A^+\setminus \{i\}:\mu_i -X_{i'} > \xi  )>\gamma$. We use these quantities and set $B= \ceil*{\frac{H}{\xi \gamma}}+1$. We are ready to summarize the properties of $\ICSEGB$.
\begin{theorem}\label{theorem: ic fee}
Under Assumptions~\ref{assumption:dominance},\ref{assumption ic body support} and~\ref{assumption order}, $\ICSEGB$ satisfies IR and BIC. In addition,
\[
\mU_T(\ICSEGB) \geq \left(  1-O\left(\frac{K \eta H \E\left[\frac{1}{\delta}\right] }{T \xi \gamma}  \right)\right) \OPT_\infty.
\]
%\OPTEA-O\left(\frac{KH^3}{n\xi \gamma}  \right).
\end{theorem}



\section{Conclusion and Discussion}\label{sec:discussion}
%We presented a model that is inspired by recent work on Multi-armed Bandits. 
Our model emphasizes the individual rationality requirement, ensuring that agents receive in expectation at least as much as they would have received without using the mechanism, based on the mechanism's knowledge. %Under such a constraint, reaching optimality relies heavily on careful planning. 
In Section~\ref{sec:infinite}, we introduced an auxiliary GMDP in Section~\ref{sec:infinite} and achieved our main contribution: An index-based optimal policy that only requires sorting arms according to their expected values. Later, in Section~\ref{sec:policy to algorithm}, we leveraged this optimal GMDP policy to develop an approximately optimal algorithm and analyzed its convergence rate. Finally, in Section~\ref{sec:ic body}, we addressed incentive compatibility.


We see considerable scope for future work. On the technical side, one possible follow-up is to relax Assumption~\ref{assumption:dominance}; more details can be found in Subsection~\ref{subsec:conjecture}. Another possible direction is to consider stochastic bandits with Bayesian priors, moving beyond the assumption of static rewards. Our individual rationality constraint is readily extendable to this case as well. Conceptually, our definition of individual rationality concerns the expected value. However, in many settings, other factors, such as variance, should also be considered. In such cases, our constraint can be generalized to $\sum_{a_i\in A}\bl p (a_i)\E\left[f(X(a_i))\mid \mI\right]\geq 0$, for some function $f$. This more general formulation can express risk-aversion or risk-seeking behavior, as well as more complex quantities that depend on the reward distribution. Our results from Section~\ref{sec:infinite} hold for any general $f$, as long as it agrees with the stochastic order. Finally, our IR notion could be applied to other explore-exploit models, such as Markov Decision Processes.
%Beyond the open problem of relaxing Assumption \ref{assumption:dominance}, we see considerable scope for future work, mainly in relaxing the i.i.d. assumption, adopting a temporal model.% or asking for safe and deterministic actions, i.e., not allow to include a priori inferior arms in the mixture while extending the model to .


% Acknowledgments---Will not appear in anonymized version
%Colt reviewers, particularly, reviewer #3

\subsection{Open Problem: Relaxing Assumption~\ref{assumption:dominance}}\label{subsec:conjecture}
Unfortunately, our results hold only under the stochastic order assumption, Assumption \ref{assumption:dominance}. This limitation is not merely a byproduct of our proof technique; there are examples where $\OGP$ ceases to be optimal due to incorrect planning (see~{\ifnum\Includeappendix=0{the appendix}\else{the proof of Proposition \ref{prop:index with ugeq one} in the appendix}\fi}).

To illustrate why $\OGP$ leads to sub-optimal performance in the absence of stochastic order, consider three crucial aspects of selecting an arm $a_j$ as part of a portfolio (a randomized action). First, its expected value $\mu(a_j)$ determines the likelihood of exploring it. Second, the probability of obtaining a positive value, $\Pr(X_{a_j}>0)$,  influences the exploration of all other arms via Bernoulli trials. Third, the potential for exceptionally high rewards if $X_{a_j}$ turns out to be positive, $\E(\max_{a_l } X_{a_l}\mid X_{a_j}>0 )$, can generate significant regret if not explored. Under Assumption~\ref{assumption:dominance}, ordering the arms in $\below(A)$ by each of these factors yields the same order. Without stochastic order, the exploration index, and whether such an index even exists, is unclear. We conjecture that a different index-like policy is optimal.

\begin{conjecture}
Let $A$ be an arbitrary set of arms, not necessarily satisfying Assumption \ref{assumption:dominance}. For every state $U\subseteq A$, let $f^U$ be the real-valued function, $f^U:\below(U)\rightarrow \mathbb R$ defined as
\[
f^U(a_{j})=\frac{\Pr(X_{a_j} > 0 )\E(\max_{a_l \in {U} } X_{a_l}\mid X_{a_j}>0 )}{\abs{\mu(a_j)}}.
\] 
Define a policy $\pi$ such that for every state $U$, $\pi(U)=\bl p_{i,j}$, where $a_i \in \above(U)$ is an arbitrary arm with a positive expected value and $a_j  \in \argmax_{a_{j'}\in \below(U)} f^U(a_{j'})$. Then, $W(\pi,s_0)=\OPT$. 
\end{conjecture}
The base cases we proved for Theorem \ref{thm:optimal policy} support this conjecture. 


%We conjecture that a different Gittins index-like policy is optimal
%\bibliographystyle{ACM-Reference-Format}




