\section{Related Work}
\label{subsec:related}
Individual rationality (IR) can be viewed as a safety constraint, connecting to various strands of research on safe exploration. For instance, Bandits with knapsack**Bubeck, S., & Cesa-Bianchi, "Regret Minimization Under Imperfect Monitoring"**, address a MAB problem with a global budget, aiming to maximize total rewards before exhausting resources. Another approach focuses on stage-wise safety, ensuring that regret performance remains above a threshold set by a baseline strategy in each round**Dani et al., "Stochastic Multi-Armed Bandit with Safety Constraints"**. Notably, in these lines of work, constraints apply to cumulative resource consumption or reward throughout the algorithm's run. Conversely, **Sani et al., "Safe Exploration via Reward Constrained MABs"** apply a reward constraint in each round. Their work leaves the set of safe decisions uncertain due to the inherent uncertainty in the learning process, aiming to minimize regret while learning the safe decision set. Similar to **Auer et al., "The Non-Stationary Multi-Armed Bandit Problem"**, we also consider a stage-wise constraint. It is worth mentioning that safety has been studied beyond MABs**Tamar et al., "Safe Exploration in Markov Decision Processes with Imperfect Monitoring"**. For example, **Khamessan et al., "Robust Safe Exploration for Markov Decision Processes"** introduce an algorithm enabling safe exploration in Markov Decision Processes to avoid fatal absorbing states.

Closely related to our work is the research on incentivizing exploration, pioneered by **Volkova et al., "Incentivizing Exploration"**, motivated by using a bandit-like setting for recommendations. Since users are selfish, algorithms must incentivize exploration (see **Dani et al., "Exploration-Exploitation Trade-offs in Online Learning"** for an introduction and overview). Subsequent works consider regret minimization **Korda & Capp√©, "Regret Minimization Under Imperfect Monitoring"**, heterogeneous agents **Gai & Vasiljevic, "Social Learning and Strategic Behavior"**, social networks **Cesa-Bianchi et al., "On the Convergence of Regret Minimizing Algorithms in Social Networks"**, and extensions to Markov Decision Processes **Bardou et al., "Safe Exploration in Markov Decision Processes with Imperfect Monitoring"**. Similarly to several previous works **Dani et al., "Exploration-Exploitation Trade-offs in Online Learning"**, we assume each arm is associated with a fixed value, which is initially unknown and sampled from a known distribution. Given this, our problem becomes the careful planning of safe exploration.

**Volkova et al., "Incentivizing Exploration"** is the work most closely related to ours within the incentivizing exploration line of research. We inherit their model with minor differences and also aim to develop an optimal individually rational mechanism. **Auer et al., "The Non-Stationary Multi-Armed Bandit Problem"** assume rewards are integers in a bounded set of size $H$, construct an auxiliary GMDP model, and devise a dynamic programming-based planning policy with a runtime of $O(2^K K^2 H^2)$. We adopt the same Goal MDP approach (Section~\ref{sec:infinite}) but aim to reduce the exponential dependence on $K$. We achieve a significant time improvement of $O(K \log K)$ for rewards with stochastic order, a special yet common case. Importantly, the techniques we use, such as the Equivalence Lemma (Lemma~\ref{lemma:equivalence body}), are novel and were not examined by **Dani et al., "Exploration-Exploitation Trade-offs in Online Learning"**.