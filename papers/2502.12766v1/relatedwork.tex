\section{Related Work}
\label{subsec:related}
Individual rationality (IR) can be viewed as a safety constraint, connecting to various strands of research on safe exploration. For instance, Bandits with knapsack~\cite{badanidiyuru2013bandits} address a MAB problem with a global budget, aiming to maximize total rewards before exhausting resources. Another approach focuses on stage-wise safety, ensuring that regret performance remains above a threshold set by a baseline strategy in each round~\cite{kazerouni2017conservative,wu2016conservative}. Notably, in these lines of work, constraints apply to cumulative resource consumption or reward throughout the algorithm's run. Conversely, \citet{amani2019linear} apply a reward constraint in each round. Their work leaves the set of safe decisions uncertain due to the inherent uncertainty in the learning process, aiming to minimize regret while learning the safe decision set. Similar to \citet{amani2019linear}, we also consider a stage-wise constraint. It is worth mentioning that safety has been studied beyond MABs~\cite{wachi2020safe,Moldovan:2012}. For example, \citet{Moldovan:2012} introduce  an algorithm enabling safe exploration in Markov Decision Processes to avoid fatal absorbing states. 

Closely related to our work is the research on incentivizing exploration, pioneered by \citet{KremerMP13}, motivated by using a bandit-like setting for recommendations. Since users are selfish, algorithms must incentivize exploration (see \cite{slivkins2019introduction} for an introduction and overview). Subsequent works consider regret minimization \cite{mansour2015bayesian,mansour2020bayesian}, heterogeneous agents \cite{chen18a,immorlica2019bayesian}, social networks \citep{Bahar2016,bahar2019recommendation}, and extensions to Markov Decision Processes \cite{simchowitz2024exploration}. Similarly to several previous works \cite{Fiduciary,BaharST19,cohen2019optimal,KremerMP13}, we assume each arm is associated with a fixed value, which is initially unknown and sampled from a known distribution. Given this, our problem becomes the careful planning of safe exploration.

\citet{Fiduciary} is the work most closely related to ours within the incentivizing exploration line of research. We inherit their model with minor differences and also aim to develop an optimal individually rational mechanism. \citet{Fiduciary} assume rewards are integers in a bounded set of size $H$, construct an auxiliary GMDP model, and devise a dynamic programming-based planning policy with a runtime of $O(2^K K^2 H^2)$. We adopt the same Goal MDP approach (Section~\ref{sec:infinite}) but aim to reduce the exponential dependence on $K$. We achieve a significant time improvement of $O(K \log K)$ for rewards with stochastic order, a special yet common case. Importantly, the techniques we use, such as the Equivalence Lemma (Lemma~\ref{lemma:equivalence body}), are novel and were not examined by \citet{Fiduciary}.