\section{Related Work}
\label{subsec:related}
Individual rationality (IR) can be viewed as a safety constraint, connecting to various strands of research on safe exploration. For instance, Bandits with knapsack____ address a MAB problem with a global budget, aiming to maximize total rewards before exhausting resources. Another approach focuses on stage-wise safety, ensuring that regret performance remains above a threshold set by a baseline strategy in each round____. Notably, in these lines of work, constraints apply to cumulative resource consumption or reward throughout the algorithm's run. Conversely, ____ apply a reward constraint in each round. Their work leaves the set of safe decisions uncertain due to the inherent uncertainty in the learning process, aiming to minimize regret while learning the safe decision set. Similar to ____, we also consider a stage-wise constraint. It is worth mentioning that safety has been studied beyond MABs____. For example, ____ introduce  an algorithm enabling safe exploration in Markov Decision Processes to avoid fatal absorbing states. 

Closely related to our work is the research on incentivizing exploration, pioneered by ____, motivated by using a bandit-like setting for recommendations. Since users are selfish, algorithms must incentivize exploration (see ____ for an introduction and overview). Subsequent works consider regret minimization ____, heterogeneous agents ____, social networks ____, and extensions to Markov Decision Processes ____. Similarly to several previous works ____, we assume each arm is associated with a fixed value, which is initially unknown and sampled from a known distribution. Given this, our problem becomes the careful planning of safe exploration.

____ is the work most closely related to ours within the incentivizing exploration line of research. We inherit their model with minor differences and also aim to develop an optimal individually rational mechanism. ____ assume rewards are integers in a bounded set of size $H$, construct an auxiliary GMDP model, and devise a dynamic programming-based planning policy with a runtime of $O(2^K K^2 H^2)$. We adopt the same Goal MDP approach (Section~\ref{sec:infinite}) but aim to reduce the exponential dependence on $K$. We achieve a significant time improvement of $O(K \log K)$ for rewards with stochastic order, a special yet common case. Importantly, the techniques we use, such as the Equivalence Lemma (Lemma~\ref{lemma:equivalence body}), are novel and were not examined by ____.