In this section, we outline the proof of Theorem~\ref{thm:optimal policy}. 
To allow this section to be self-explanatory, we reiterate some definitions that appear in the body of the paper. We begin with several notations and definitions we use extensively in the proof.
\subsection{Preliminaries}
We denote the set of all states by $\mS=2^A$. A \textit{policy} is a mapping from previous states and actions to a randomized action. Formally, let $\mH$ be the set of histories, $\mH=\cup_{k=0}^K\left(\mS \times \Delta(A)\right)^k$ be a tuple comprising pairs of states and randomized actions taken. A policy $\pi$ is a function $\pi:\mH \times \mS \rightarrow \Delta(A)$. We say that a policy is \textit{IR} if for every $h\in \mH,s\in \mS$, $\pi(h,s)\in \safe(s)$. From here on, we consider IR policies solely. Given a policy $\pi$ and a pair $(h,s)$, we let $W(\pi,h,s)$ denote the expected reward of $\pi$ when starting from $s$ after witnessing $h$. Namely,
\begin{align}\label{eq:W elaborated}
W(\pi,h,s) = 
\begin{cases}
R(s) & \textnormal{if }\safe(s)=\emptyset\\
\sum_{a\in s}\pi(h,s)(a)W(\pi,h\oplus(s,a),s\setminus \{a\}) & \textnormal{otherwise}
\end{cases}.
\end{align}
For every state $s$, let $W^\star(s)=\sup_{\pi'}W(\pi ',\emptyset,s)$.\footnote{As we show in Proposition \ref{prop:optimal p valid}, there exists a policy that attains this supremum.} While policies may depend on histories, it often suffices to consider \emph{stationary} policies.
\begin{definition}[Stationary]
An IR policy $\pi$ is \textit{stationary} if for every two histories $h,h' \in \mH$ and a state $s \in \mS$, $\pi(h,s)=\pi(h',s)$.
\end{definition}
Since there is a finite set of states and the action sets are convex, there exists %As we show later in Proposition \ref{prop:optimal p valid}, there exists 
an optimal stationary policy; hence, from here on we address stationary policies solely. When discussing stationary policies, we thus neglect the dependency on $h$, writing $\pi(s)$. For stationary policies, the definition of $W$ is much more intuitive: Given a stationary policy $\pi$ and a state $s$,
\begin{align}\label{eq:w with terminal}
W(\pi,s)=\sum_{\substack{s_t\in \mS:\\ s_t \textnormal{ is terminal}}}\Pr(\pathto{s}{s_t})R(s_t),
\end{align}
where $\pathto{s}{s_t}$ indicates the event that, starting from $s$ and following the actions of $\pi$, the GMDP terminates at $s_t$. 

An additional useful notation is the following. For every state $s\in \mS$, we denote by $Q(\pi,s)$ the probability starting at $s\subseteq A$, following the policy $\pi$ and exploring all arms. Formally,
\[
Q(\pi,s)= \Pr(\pathto{s}{\emptyset}),
\]
Note that $Q$ is defined recursively: Namely, if $\pi(s)=\bl p_{i,j}$ for a non-terminal state $s$, then
\[
Q(\pi,s)=\bl p_{i,j}(a_i)Q(\pi,s\setminus\{a_i\})+\bl p_{i,j}(a_j)Q(\pi,s\setminus\{a_j\}).
\]
It will sometimes be convenient to denote $Q(\pi,\above(s),\below(s))$ for $Q(\pi,s)$, thereby explicitly stating the two distinguished sets of arms. 


\subsection{Binary Structure}\label{subsec:bin}
A structural property of the above GMDP is that in every terminal state $s_t$, $\above(s_t)=\emptyset$, or otherwise we could explore more arms; thus, intuitively, the arms in $\above(A)$ provide us ``power'' to explore the arms of $\below(A)$. Following this logic, in every state we should aim to explore arms from $\below(A)$ and not those of $\above(A)$, subject to satisfying the IR constraint.

Recall the definition of $\bl p_{i,j}$ and $\bl p_{i,i}$ from Equation~\eqref{eq:blp from body} in Subsection~\ref{subsec:optimal GMDP policy}. Next, we define $\mP,\mP'$ such that
\[
\mP\defeq \{\bl p_{i,j}\mid a_i\in \above(A), a_j\in \below(A) \}, \qquad \mP'\defeq\{\bl p_{i,i}\mid a_i\in \above(A) \}.
%\mP\defeq \{\bl p_{i,j}\mid a_i\in \above(A), a_j\in \below(A) \}\cup\{\bl p_{i,i}\mid a_i\in \above(A) \}.
\]
Notice that $\mP\cup \mP'$ includes $O(K^2)$ actions, while $\safe(s)$ for a state $s$ is generally a convex polytope  with infinitely many actions. Further, in every non-terminate state $s$, ${\safe(s) \cap (\mP \cup \mP') \neq \emptyset}$. Next, we remind the reader of the definition of $\mP$-valid policies.
\begin{definition}[Mirroring Definition~\ref{def:p valid}]
An IR policy $\pi$ is $\mP$-valid if for every non-terminal state $s\in \mS$,
\begin{itemize}
\item if $\below(s)\neq \emptyset$, then $\pi(s)\in \mP$;
\item else, if $\below(s)= \emptyset$, then $\pi(s)\in \mP'$.
\end{itemize} 
\end{definition}
Observe that $\mP$ is a strict subset of all IR actions in the state $A$, which incorporate mixes of at most two arms. However, the set of IR actions $\safe(s)$ for $s\subseteq A$ may include distributions mixing several elements of $A$. Due to the convexity of $W(\pi,s)$ in $\pi(s)$ (see the elaborated representation of $W$ in Equation \refeq{eq:W elaborated}), the GMDP exhibits a nice structural property, as captured by the following Proposition~\ref{prop:optimal p valid}. 
\begin{proposition}[Mirroring Proposition~\ref{prop:main optimal p valid}]\label{prop:optimal p valid}
There exists an optimal policy that is $\mP$-valid.
\end{proposition}
The proof of Proposition \ref{prop:optimal p valid} appears in Section~\ref{sec:aux}. %~{\ifnum\Includeappendix=0{the appendix}\else{Subsection \ref{subsec:proof of propo p valid}}\fi}. 
Due to Proposition \ref{prop:optimal p valid}, we shall focus on $\mP$-valid policies. Such policies are easy to visualize using trees, as we exemplify next.\footnote{In Figure~\ref{example with normal} we illustrated the optimal policy using a graph that is not a tree. However, a tree structure  serves better the presentation of our technical statements.}
\begin{example}\label{example with four}
We reconsider Example~\ref{example with normal}, but neglect the actual distributions (as we only care about the expected values). Let $A=\{a_1,a_2,a_3,a_4\}$, with $\above(A)=\{a_1,a_2\}$ and $\below(A)=\{a_3,a_4\}$. Consider the tree description in Figure \ref{fig:tree example}. The root of the tree is the set of all arms. At the root, the policy picks $\bl p_{1,3}$. The outgoing left edge represents the case the realized action is $a_1$, which happens w.p. $\bl p_{1,3}(a_1)$. In such a case, the new state is $\{a_2,a_3,a_4\}$. With the remaining probability, $\bl p_{1,3}(a_3)$, the new state will be $\{a_1,a_2,a_4\}$. Leaves of the tree are terminal states, where no further exploration could be done. For instance, in the leftmost leaf, $\{a_3,a_4\}$, the only arms explored are $\{a_1,a_2\}$. The two highlighted nodes represent the same state. Since the presented policy is $\mP$-valid, it is stationary; hence, the policy acts exactly the same in these two nodes and their sub-trees.
\end{example}
Notice that the tree in Figure \ref{fig:tree example} represents only the \textit{on-path} states, i.e., states that are reachable with positive probability, while policies are functions from the entire space of states, including off-path states; thus, two different policies can be described using the same tree. Nevertheless, the tree structure is convenient and will be used extensively in our analysis. When we define a policy using a tree, we shall also describe its behavior at \textit{off-path} states.


The policy exemplified in Figure \ref{fig:tree example} has an additional combinatorial property: In every state $s$, it takes an action according to some order of the arms. This property is manifested in the following Definitions \ref{def:right ordered} and \ref{def:left ordered}.
\begin{definition}[Right-ordered policy]\label{def:right ordered}
A $\mP$-valid policy $\pi$ is right-ordered if there exist a bijection $\sigr_\pi: \below(A)\rightarrow [\abs{\below(A)}]$ such that in every state $s$ with $\below(s) \neq \emptyset$, $\pi(s)=\bl p_{i, {j^*}}$ where $a_i \in \above(s)$ and $a_{j^*} = \argmin_{a_j \in \below(s)} \sigr_\pi(a_j)$.
\end{definition}
\begin{definition}[Left-ordered policy]\label{def:left ordered}
A $\mP$-valid policy $\pi$ is left-ordered if there exist a bijection $\sigl_\pi: \above(A)\rightarrow [\abs{\above(A)}]$ such that in every state $s$ with $\below(s) \neq \emptyset$, $\pi(s)=\bl p_{{i^*}, j}$ where $a_j \in \below(s)$ and $a_{i^*} = \argmin_{a_i \in \above(s)} \sigl_\pi(a_i)$.
\end{definition}
In addition, we say that a policy is \textit{ordered} if it is right-ordered and left-ordered. To illustrate, observe the example in Figure \ref{fig:tree example}. The tree depicts an ordered policy, with $\sigl=(a_1,a_2)$ and $\sigr=(a_3,a_4)$. Notice that ordered policies are well-defined for off-path states.

\begin{figure}[t]
\centering
\includegraphics[scale=0.9]{figs/tree_example_crop}% label
\caption{The policy described in Example \ref{example with four}.  
Every node represents a state (the mapping is onto, but not one-to-one). Outgoing left edges imply the coin flips resulted in an arm from $\above(v)$, and outgoing right edges imply an arm from $\below(v)$. Leaves correspond to terminal states, where no action could be taken. \label{fig:tree example}
}% caption command
\end{figure}

\iffalse
\subsection{Stochastic Dominance and Non-triviality}\label{subsec:stopchastic}
In this subsection, we demonstrate why the problem is still challenging even under Assumption \ref{assumption:dominance}. Recall that Proposition \ref{prop:optimal p valid} ensures that an optimal $\mP$-valid policy exists. One natural candidate for the optimal policy is ordered policy $\pi$ with with any order $\sigl_\pi$ and $\sigr_\pi$ that follow the stochastic order on $\below(A)$. Indeed, as we show formally in Theorem~\ref{thm:holy grail}, this intuition is appropriate. However, we explain shortly, the optimally of this policy cannot be shown without further work.

Consider a state $s\in\mS$, such that $\above(s),\below(s) \geq 2$. Let $a_i = \argmin_{a_{i'}\in\above(s)}\sigl_\pi(a_{i'})$, and  $a_j = \argmin_{a_{j'}\in\below(s)}\sigr_\pi(a_{j'})$. In addition, let $a_{\tilde j}\in \below(s), a_{\tilde j} \neq a_j$. The action $\bl p_{i,j}$, which mixes the minimal elements according to the stochastic order, is weakly superior to  $\bl p_{i,{\tilde{j}}}$ if 
{
\thinmuskip=.2mu
\medmuskip=0mu plus .2mu minus .2mu
\thickmuskip=1mu plus 1mu
\begin{align}\label{eq:why hard}
&\bl p_{i,j}(j)W^\star(s\setminus \{a_j\})-\bl p_{i,{\tilde j}}({\tilde j})W^\star(s\setminus \{a_{\tilde j}\})+\left(\bl p_{i,j}(i)-\bl p_{i,{\tilde j}}(i)\right)W^\star(s\setminus\{a_i\}) \geq 0.
\end{align}}%
By our selection of $a_j,a_{\tilde j}$, we know that $\bl p_{i,j}(i)-\bl p_{i,{\tilde j}}(i) \leq 0$; hence, the third term is non-positive. Moreover, as we show in Claim \ref{claim:ass is not for W} in Section~\ref{sec:aux}, %~{\ifnum\Includeappendix=0{the appendix,}\else{Claim \ref{claim:ass is not for W} in Subsection \ref{subsec: statments},}\fi}
stochastic dominance does not imply that $W^\star(s\setminus \{a_j\}) \geq W^\star(s\setminus \{a_{\tilde j}\})$; thus, it is not even clear that the expression
{
\thinmuskip=.2mu
\medmuskip=0mu plus .2mu minus .2mu
\thickmuskip=1mu plus 1mu
\begin{align*}
&\bl p_{i,j}(j)W^\star(s\setminus \{a_j\})-\bl p_{i,{\tilde j}}({\tilde j})W^\star(s\setminus \{a_{\tilde j}\})
\end{align*}}%
which accounts for the first two terms in Inequality (\ref{eq:why hard}), is non-negative. Therefore, we cannot claim for Inequality  (\ref{eq:why hard}) without revealing the structure of $W^\star$, even when Assumption \ref{assumption:dominance} holds. 
 %We discuss relaxing this assumption in Section \ref{??}
 \fi
\subsection{Proof Overview}\label{subsec:results}
We are ready to prove Theorem~\ref{thm:optimal policy}. The main tool in our analysis is Lemma \ref{lemma:equivalence}. Lemma \ref{lemma:equivalence} reveals a rather surprising feature of $Q$: $Q$ is policy independent. 
\begin{lemma}[Mirroring Lemma~\ref{lemma:equivalence body}]\label{lemma:equivalence}
For every two $\mP$-valid policies $\pi,\rho$ and every state $s\in \mS$, it holds that $Q(\pi,s)=Q(\rho,s)$.
\end{lemma}
The proof of Lemma \ref{lemma:equivalence} appears in~{\ifnum\Includeappendix=0{the appendix}\else{Section \ref{sec:proof of lemma}}\fi}. We stress that this lemma holds regardless of Assumption~\ref{assumption:dominance}. Next, we leverage Lemma \ref{lemma:equivalence} to prove the main technical result of the paper.
\begin{theorem}
\label{thm:holy grail}
Let $\pi^\star$ be a right-ordered, $\mP$-valid policy with $\sigr_{\pi^\star}$ ordered in decreasing expected value. Under Assumption \ref{assumption:dominance}, for every state $s\in \mS=2^A$, it holds that $W(\pi^\star,s)=W^\star(s)$.
\end{theorem}
In particular, Theorem \ref{thm:holy grail} implies that $W(\SEGB,s_0)=W^\star(s_0)$ and that Theorem~\ref{thm:optimal policy} holds,  since $\SEGB$ is right-ordered in decreasing expected value. The formal proof of Theorem \ref{thm:holy grail} is relegated to~{\ifnum\Includeappendix=0{the appendix}\else{Section~\ref{sec:proof of thm}}\fi}. 