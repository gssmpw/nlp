\begin{proofof}{Observation \ref{obs:eventually will explore}}
Let $x(a_i)>0$ for some $i\in [K]$, let $j$ be an index of unexplored arm, and let $\mI$ be the information of the algorithm. We overload the notation $\bl p_{i,j}$ to acknowledge the realized value $x(a_i)$; that is,
\begin{align*}
\bl p_{i,j}(a) =
\begin{cases}
\frac{-\mu(a_j)}{x(a_i)-\mu(a_j)} & \textnormal{if } a=a_i\\
\frac{x(a_i)}{x(a_i)-\mu(a_j)} & \textnormal{if } a=a_j\\
0 & \textnormal{otherwise}
\end{cases}.
\end{align*}
Notice that 
\begin{align*}
\sum_{a\in A}\bl p(a)\E\left[X(a)\mid \mI\right] &= \bl  p_{i,j}(a_i)x(a_i)  + \bl  p_{i,j}(a_j)\mu(a_j) \\
&= x(a_i)\cdot \frac{-\mu(a_j)}{x(a_i)-\mu(a_j)} + \mu(a_j)\cdot \frac{x(a_i)}{x(a_i)-\mu(a_j)} = 0;  
\end{align*}
hence, $\bl p_{i,j}$ is IR w.r.t. to $\mI$. After selecting $\bl p_{i,j}$, either $a_i$ was realized or $a_j$. In the former, the information remains the same, and we can repeat this experiment again. The probability of $a_j$ realizing is positive and constant, and hence, after a finite time, we will eventually realize it. Once we do, the number of unexplored armed decreases by one. We can follow this process until all arms are explored.
\end{proofof}

\begin{proofof}{Observation~\ref{obs: U leq W*}}
The proof of this observation relies on constructing a policy $\pi$ that simulates $\ALG$. Since by definition $W(\pi,A) \leq  W^\star(A)$, it is enough to show that $\lim_{T \rightarrow \infty }\mU_T(\ALG) \leq W(\pi,A)$. In every round, $\pi$ selects precisely what $\ALG$ selects, and if the realized arm was already explored by $\ALG$, $\pi$ ignores it. The infinite time expected value of $\ALG$ cannot exceed $ W(\pi,A)$. The full details are similar to \cite[Theorem 3]{Fiduciary} and are hence omitted. 
\end{proofof}

\begin{proofof}{Observation \ref{obs: U get W}}
Fix any policy $\pi$. Let $\ALG(\pi)$ be the modification of Algorithm~\ref{alg:alg of pi} that uses $\pi$ instead of $\OGP$ in Lines \ref{algpi:while}-\ref{algpi:play with ogp}. Once $\pi$ reaches a terminal state, $\ALG(\pi)$ secures the reward of $\pi$ in finite time. Overall, $\lim_{T \rightarrow \infty }\mU_T(\ALG(\pi)) = W(\pi,A)$.
\end{proofof}


\begin{proofof}{Proposition \ref{prop:bernoulli opt}}
Fix an $\ise$ instance such that $(X(a_i))_i \in \{x^-,x^+\}$ (for $x^- \leq x^+$) almost surely. For the problem to be non-trivial, we must have $x^- <0$ and $x^+ >0$. Otherwise, if  $x^-,x^+ <0$ the only IR action is $a_0$, and if $x^-,x^+ \geq 0$, we can explore all arms using the singleton portfolios $(\bl p_{ii})_{i \in [K]}$. From here on, we assume w.l.o.g. that $x^- = -1$ and $x^+=H$. For convenience, we state $\SEGB'$ explicitly in Algorithm~\ref{alg:alg of pi two supported}. Before we prove the proposition, we remark that
\begin{enumerate}
    \item Since $(X(a_i))_i$ take either $-1$ or $H$, Assumption~\ref{assumption:dominance} implies a stochastic order on all arms, not only on $\below(A)$. 
    \item Any asymptotically optimal algorithm conducts at most $K$ exploration rounds before it exploits. This implies an immediate crude bound of
    $\mU_T(\SEGB') \geq \left(  1-\frac{KH}{T}\right) \OPT_T$.
    \item This proof uses the analysis presented in Section~\ref{sec:thm1 outline}.
\end{enumerate}


The proof is composed of two steps. In the first step, we show that if $T > T_0$ for some $T_0$, any optimal algorithm must explore the arms according to a policy that admits the same structure of $\OGP$. In the second step, we show that all such policies have an identical exploration time, and hence all achieve the same social welfare.\\
\textbf{Step 1:} In the case of realizing a positive reward, any algorithm would stop exploring and exploit that realized reward. Consequently, we can separate exploration rounds from exploitation rounds. Notice, however, that the exploration policy can select portfolios different that $\OGP$ for finite $T$. To illustrate, reconsider Example~\ref{example with four}. In the extreme case of $T=1$, there is no point in selecting $\bl p_{1,3}$, since exploring $a_3$ is futile; we only care about maximizing the current round's reward.

However, if $T$ is large \textit{enough}, any optimal algorithm must employ an asymptotically optimal policy. To see this, let $(\pi,\ALG^{\pi})$ be a pair of exploration policy and the algorithm that employs it, and assume $\pi$ does not admit the structure of $\OGP$. The social welfare of $\ALG^\pi$ satisfies
\begin{align}\label{eq:alg of pi is not optimal}
\mU_T(\ALG^\pi) \leq KH+ (T-K)W(\pi,A).    
\end{align}
Similarly, taking into account the optimality of $\OGP$,
\begin{align}\label{eq:alg of pi with ogp}
\mU_T(\ALG^{\OGP}) \geq -K +(T-K)W^\star(A).
\end{align}
Using similar arguments to those in Proposition~\ref{prop:optimal p valid}, we can assume w.l.o.g. that $\pi$ belongs to $\{2^A \rightarrow \mP \cup \mP'\}$ (recall the definition of $\mP$ and $\mP'$ from Subsection~\ref{subsec:bin}). To see this, observe that any IR policy can be formulated as a convex combination of policies that use $\mP \cup \mP'$ solely, and therefore we can assume that $\pi$ is the one for which $\ALG^{\pi}$ gets the highest social welfare. Furthermore, due to the proof of Theorem~\ref{thm:holy grail} (precisely Equation~\eqref{eq:rho with alpha}) it follows that if $\pi$ does not admit the structure of $\OGP$, then it is strictly sub-optimal. Next, let
\begin{align}\label{eq:alg of pi little omega}
\omega  \defeq \min_{\substack{\rho \in \{2^A \rightarrow \mP \cup \mP'\},\\W(\rho, A) < W^\star(A)}} W^\star(A) - W(\rho,A) > 0.
\end{align}
The quantity $\omega$ concerns the distributions of $(X(a_i))_i$ and is completely independent of the time $T$. We can further quantify or bound $\omega$ but this abstract and simple form is sufficient for our purposes. Let $T_0 \defeq K + \frac{K(H+1)}{\omega}$. Combining Inequalities~ \eqref{eq:alg of pi is not optimal},\eqref{eq:alg of pi with ogp}, and \eqref{eq:alg of pi little omega} we get
\begin{align*}
\mU_T(\ALG^{\OGP})-\mU_T(\ALG^\pi) &\geq  -K +(T-K)W^\star(A) - KH - (T-K)W(\pi,A) \\
& \geq -K(H+1) + (T-K)\omega\\
& > 0,
\end{align*}
provided that $T>T_0$. To conclude this step, we know that $\pi$ is a variation of $\OGP$.

\textbf{Step 2:}
Notice, however, that $\OGP$ is a class of policies differing from one another in the choices of arms from $\above(A)$ (Line~\ref{policy:pick arbitrary} in Policy~\ref{policy:pi star}); hence, one policy may attain a better reward than the other by reaching exploitation faster. 

As we commented in Line~\ref{algpi:two:play with ogp} of Algorithm~\ref{alg:alg of pi two supported}, when the state $s$ does not contain arms from $\below(A)$ we prioritize singleton portfolios according to the stochastic order. That is, we favor $\bl p_{i,i}$ over $\bl p_{i',i'}$ if $\mu(a_i) > \mu(a_{i'})$. This modification ensures that the time to exploitation from such states is minimal. 

Nevertheless, we might face a problem in states for which $\below(s)\neq\emptyset$. To illustrate, consider the action $\bl p_{i,j}$ for some $a_i\in \above(A), a_j\in \below(A)$. The probability we discover a reward of $H$ when selecting $\bl p_{i,j}$ is
\begin{align}\label{eq:q is identical to all}
\bl p_{i,j}(a_i)\Pr(X(a_i)=H)  + \bl p_{i,j}(a_j)\Pr(X(a_j)=H).
\end{align}
Consequently, we might favor $\bl p_{i,j}$ over $\bl p_{i',j}$ (for $a_{i'}\in \above(A)$) if it allows faster discovery of a positive reward (which is necessarily $H$). However, as we show next, the probability in Equation~\eqref{eq:q is identical to all} is the same regardless of the selection the arm from $\above(s)$. Observe that 
\[
\mu(a_i) = H\Pr(X(a_j)=H) +(-1)\cdot (1-\Pr(X(a_j)=H)) = (H+1)\Pr(X(a_j)=H)-1; 
\]
thus, by reformulating Equation~\eqref{eq:q is identical to all} we get
{
\thinmuskip=.2mu
\medmuskip=0mu plus .2mu minus .2mu
\thickmuskip=1mu plus 1mu
\begin{align*}\label{eq:q is identical to all elaborate}
\textnormal{Eq.}\eqref{eq:q is identical to all}&=\frac{-\mu(a_j)}{\mu(a_i)-\mu(a_j)}\Pr(X(a_i)=H)+ \frac{\mu(a_i)}{\mu(a_i)-\mu(a_j)}\Pr(X(a_j)=H)\\
& =\frac{-(H+1)\Pr(X(a_j)=H)+1}{\mu(a_i)-\mu(a_j)}\Pr(X(a_i)=H)+ \frac{(H+1)\Pr(X(a_i)=H)-1}{\mu(a_i)-\mu(a_j)}\Pr(X(a_j)=H)\\
& = \frac{\Pr(X(a_i)=H)-\Pr(X(a_j)=H)}{\mu(a_i)-\mu(a_j)}\\
& = \frac{\Pr(X(a_i)=H)-\Pr(X(a_j)=H)}{(H+1)\Pr(X(a_i)=H)-1-(H+1)\Pr(X(a_i)=H)+1}\\
& = \frac{1}{H+1}.
\end{align*}
}%

\begin{algorithm}[t]
\LinesNumbered
\SetNoFillComment
\DontPrintSemicolon
\caption{$\SEGB$ for Two-Supported Distributions \label{alg:alg of pi two supported}}
\KwIn{the $\OGP$ policy}
$s \gets A$\; % (no label on this line)
\While{$\OGP(s) \neq \emptyset$\nllabel{algpi:two:while}}{%
    \tcp{\texttt{\color{blue}{$s$ is not a terminal state}}}%
    select $\OGP(s)$, and denote the realized action by $a_k$\nllabel{algpi:two:play with ogp}\;
    \tcp{\texttt{\color{blue}{if $\below(s) = \emptyset$, prioritize the arms in $\above(s)$ according to the stochastic order}}}%
    \If{$x_{a_k} > 0$}{%
        \tcp{\texttt{\color{blue}{a reward of $H$ was realized}}}%
        \textbf{break}\;
    }
    $s \gets s \setminus \{a_k\}$\nllabel{algpi:two:update s}\;
}
\If{$x(a_k) = H$ for some explored arm $a_k$}{%
    exploit $a_k$ forever\;
}
\Else{%
    exploit $a_0$ forever\;
}
\end{algorithm}


\iffalse
\begin{algorithm}[t]
\renewcommand{\algorithmiccomment}[1]{\texttt{\kibitz{blue}{\##1}}}
\caption{$\SEGB$ for Two-Supported Distributions \label{alg:alg of pi two supported}}
\begin{algorithmic}[1]
\STATE $s\gets A$
\WHILE[$s$ is not a terminal state] {$\OGP(s)\neq \emptyset$\label{algpi:two:while}}{
\STATE select $\OGP(s)$, and denote the realized action by $a_k$.\label{algpi:two:play with ogp} \COMMENT{if $\below(s)=\emptyset$, prioritize the arms in $\above(s)$ according to the stochastic order}
\IF[a reward of $H$ was realized]{$x_{a_k}>0$} {
		\STATE \textbf{break}. 
}
\ENDIF
\STATE $s\gets s\setminus \{a_k\}$.\label{algpi:two:update s}
}
\ENDWHILE
\IF{$x(a_k)=H$ for some explored arm $a_k$}{
\STATE exploit $a_k$ forever.
}
\ELSE{
\STATE exploit $a_0$ forever.
}
\ENDIF
\end{algorithmic}
\end{algorithm}
\fi


Since all $\OGP$ have the same expected exploration time, they all achieve the same social welfare. This completes the proof of Proposition~\ref{prop:bernoulli opt}.
\end{proofof}

\begin{proofof}{Proposition \ref{prop:i-d bounds}}
To prove the claim, we show that $\SEGB$ explores for at most $K(1+\frac{\eta}{\delta})$ rounds, and then exploits. First, $\SEGB$ uses $K_1 \leq K$ rounds following $\OGP$ until it reaches a terminal state (the while loop in Line~\ref{algpi:while} breaks). If all the realized rewards are negative, the exploration ends after that. Otherwise, if it discovers a positive reward, the value of that reward is at least $\delta$. Next, the Bernoulli trails will explore every unexplored arm w.p. of at least $ \frac{\delta}{\delta+\eta}$, or $ \frac{\delta+\eta}{\delta}$ rounds in expectation. Therefore, after $(K-K_1)(1+\frac{\eta}{\delta})$ rounds in expectation we explore all the remaining arms. Overall, the expected number of rounds devoted to exploration is
\[
K_1 + (K-K_1)\left(1+\frac{\eta}{\delta}\right) \leq K \left(1+\eta\E\left[{\frac{1}{\delta}}\right]\right).
\]
Ultimately, recall that $\lim_{T \rightarrow \infty }\mU_T(\SEGB) = \OPT_{\infty}$, so $\SEGB$ exploits an expected reward of $\OPT_{\infty}$ after it completes its exploration. Therefore,
\begin{align*}
\mU_T(\SEGB)& \geq  \frac{1}{T} \left[K \left(1++\eta\E\left[{\frac{1}{\delta}}\right]\right) \cdot 0 + \left(T-K \left(1+\eta\E\left[{\frac{1}{\delta}}\right]\right) \right)\OPT_{\infty}\right]. \\
&\geq\left(1-\frac{K \left(1+\eta\E\left[{\frac{1}{\delta}}\right]\right)}{T} \right)\OPT_{\infty}. 
\end{align*}
This completes the proof of Proposition \ref{prop:i-d bounds}.
\end{proofof}