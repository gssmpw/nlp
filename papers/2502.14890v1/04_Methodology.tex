\section{Methodology}

After annotating the dataset, we split the dataset into training, validation, and test sets. We used 184,719 images ($\sim$80\%) to train our object detection models and 23,090 images ($\sim$10\%) to validate the model during training time. The rest of the 23,090 images ($\sim$10\%) are held out to test the trained modelâ€™s performance. In this study, we employed two advanced deep-learning models for weed detection and classification: RetinaNet with a ResNeXt-101 backbone and Detection Transformer (DETR) with a ResNet-50 backbone. These models were tasked with classifying weed species and their respective growth stages (in weeks), while simultaneously localizing them within the images via bounding box predictions. We configured and trained these models using PyTorch and mmDetection on an NVIDIA RTX 3090 GPU.

\subsection{Detection Transformer with ResNet-50}

The Detection Transformer (DETR) model is an end-to-end object detection architecture that combines a convolutional backbone with a transformer encoder-decoder \cite{carion2020end}. This approach effectively addresses the complexities of identifying weeds in agricultural images. The backbone of our model ResNet-50 is a convolutional neural network, pre-trained on ImageNet (\texttt{open-mmlab://resnet50}). This 50-layer network, organized into four stages, serves as a powerful feature extractor. We utilize the output from the final stage (out\_indices=(3,)) and freeze the initial stages during training to preserve pre-learned features. The backbone's output can be represented as:
\vspace{-0.2cm}
\begin{equation}
F_{\text{resnet}} = \text{ResNet50}(I)
\end{equation}
where \(I\) is the input image. A Channel Mapper follows the backbone, transforming ResNet-50's 2048-channel output into a 256-channel feature map suitable for the transformer. This dimensionality reduction is achieved through a 1x1 convolution:
\vspace{-0.2cm}
\begin{equation}
F_{\text{neck}} = \text{Conv1x1}(F_{\text{resnet}})
\end{equation}

The core of our DETR model is the transformer module, comprising a 6-layer encoder and decoder. Each encoder layer incorporates a self-attention mechanism with 8 heads, followed by a feed-forward network (FFN) with ReLU activation. The model's bounding box head processes the decoder's output to predict class labels and bounding boxes. We employ cross-entropy loss for classification and a combination of L1 and Generalized IoU losses for bounding box regression. The overall loss function \cite{yin2019context} is defined as:
\vspace{-0.2cm}
\begin{equation}
L = \alpha \cdot L_{\text{cls}} + \beta \cdot L_{\text{bbox}} + \gamma \cdot L_{\text{iou}}
\end{equation}
where \(\alpha\), \(\beta\), and \(\gamma\) are weight coefficients. \( L_{\text{cls}} \) represents the classification loss, which in this case is the cross-entropy loss. \( L_{\text{bbox}} \) represents the bounding box regression loss, which is a combination of L1 loss and Generalized IoU loss, and \( L_{\text{iou}} \) 	represents the IoU loss, which is specifically aimed at improving the localization accuracy by penalizing the model based on the intersection over union between the predicted and ground truth bounding boxes. During training, we utilize the Hungarian algorithm \cite{ye2020cost} for bipartite matching, ensuring a one-to-one correspondence between predicted and ground-truth boxes. This approach optimizes the model's ability to accurately locate and classify weeds within agricultural images. By integrating the robust feature extraction capabilities of ResNet-50 with the DETR architecture's powerful attention mechanisms, our model achieves good performance in weed detection with 174 classes.

\subsection{RetinaNet with ResNeXt-101}

RetinaNet is a single-stage object detection model designed to address the extreme foreground-background class imbalance encountered during training \cite{li2019light}. The architecture comprises three main components: a backbone network for feature extraction, a neck (FPN) for generating multi-scale feature maps, and a detection head for predicting bounding boxes and class probabilities. We utilized ResNeXt-101 as the backbone, a variant of the ResNet architecture that employs grouped convolutions for improved efficiency and performance. The ResNeXt-101 backbone consists of 101 layers organized into four stages, with 32 groups and a base width of 4 channels per group. We initialized the backbone with weights pretrained on ImageNet (\texttt{open-mmlab://resnext101\_32x4d}) to leverage transfer learning. Batch normalization is applied after each convolutional layer to stabilize the learning process.

The Feature Pyramid Network (FPN) enhances the backbone's feature maps by combining high-level semantic features with low-level detailed features, enabling the detection of objects at various scales. The FPN generates multiple feature maps of different resolutions, which are then fed into the detection head. The detection head of RetinaNet comprises two subnetworks: a classification subnetwork for predicting object presence probabilities and a regression subnetwork for predicting bounding box coordinates. Each subnetwork consists of four convolutional layers, followed by a final convolutional layer that produces the desired outputs. To handle class imbalance, we employed the focal loss function \cite{lin2017focal} for training the classification subnetwork:
\vspace{-0.2cm}
\begin{equation}
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}

where \(p_t\) is the predicted probability, \(\alpha_t\) is a balancing factor, and \(\gamma\) is the focusing parameter.

We trained our model using an epoch-based training loop with the AdamW optimizer (learning rate \(lr = 0.0001\), weight decay \(wd = 0.0001\)). The learning rate schedule incorporated a linear warmup over the first 1000 iterations. We trained for 12 epochs with a batch size of 16, employing automatic learning rate scaling to accommodate potential batch size changes.


% \begin{algorithm}
% \caption{Model Training Process}\label{alg:training}
% \KwData{Training dataset, validation dataset, initial model parameters}
% \KwResult{Trained model}
% Initialize model parameters $\theta$\;
% \For{epoch = 1 to max\_epochs}{
%     \For{each mini-batch $(X, y)$ in training dataset}{
%         Compute predictions $\hat{y} = f_\theta(X)$\;
%         Compute classification loss $L_{\text{cls}}$ and regression loss $L_{\text{reg}}$\;
%         Compute total loss $L = L_{\text{cls}} + L_{\text{reg}}$\;
%         Backpropagate to compute gradients $\nabla_\theta L$\;
%         Update parameters $\theta$ using AdamW optimizer\;
%     }
%     \If{epoch \% val\_interval == 0}{
%         Evaluate model on validation dataset\;
%         Save model checkpoint if performance improves\;
%     }
% }
% \end{algorithm}

% The algorithm \cite{ilyas2022datamodels} outlines a model training process where the goal is to optimize the model parameters, denoted by \(\theta\). Initially, the model parameters are set to their initial values. The training process runs for a specified number of epochs, iterating over mini-batches of the training dataset in each epoch. For each mini-batch, the model makes predictions \(\hat{y}\), and the classification and regression losses are computed. These losses are summed to obtain the total loss, which is then used to calculate the gradients through backpropagation. The model parameters are updated using the AdamW optimizer. At regular intervals, defined by \texttt{val\_interval}, the model's performance is evaluated on the validation dataset, and the model checkpoint is saved if there is an improvement in performance.


% \vspace{-0.4cm}

\subsection{Evaluation Metrics}

To assess the performance of our weed detection models, we employ a comprehensive set of metrics that capture both the accuracy and robustness of the detections. Our primary metrics are Average Precision (AP), Average Recall (AR), and Mean Average Precision (mAP) evaluated across various Intersection over Union (IoU) thresholds.

AP provides a single-value summary of the precision-recall curve, effectively balancing the trade-off between precision and recall. Precision (P) is defined as the ratio of true positive detections to the sum of true positive and false positive detections: $\text{} P = \frac{TP}{TP + FP}$ and Recall (R) is the ratio of true positive detections to the sum of true positive and false negative detections: $\text{} (R) = \frac{TP}{TP + FN}$.

In this research, a true positive is a detected bounding box that correctly identifies a weed species and has an IoU above a specified threshold (e.g., 0.50) with the ground truth bounding box. A false positive is a detection that either does not sufficiently overlap with any ground truth box or incorrectly identifies the weed species. A false negative occurs when a ground truth weed instance is not detected by the model. AP \cite{robertson2008new} is calculated by integrating the precision over the recall range and it can be defined as:
\vspace{-0.2cm}
\begin{equation}
\text{AP} = \int_0^1 P(R) \, dR
\end{equation}

AR \cite{zhu2004recall} measures the model's ability to detect all relevant objects. It is computed as the average of maximum recalls at specified IoU thresholds:
\vspace{-0.2cm}
\begin{equation}
\text{AR} = \frac{1}{N} \sum_{i=1}^N R_{\text{max}}(IoU_i)
\end{equation}

mAP is the mean of AP values across different classes and is a common metric for evaluating object detection models. It provides a balanced measure of precision and recall across various IoU thresholds. It can be defined as:
\vspace{-0.2cm}
\begin{equation}
\text{mAP} = \frac{1}{C} \sum_{c=1}^C AP_c
\end{equation}

where $AP_c$ is the Average Precision for class $c$, and $C$ is the total number of classes.
We evaluate these metrics at various IoU thresholds. This multi-faceted evaluation approach allows us to comprehensively analyze our models' capabilities in detecting and classifying weeds across various scenarios, providing insights into their precision, recall, and overall detection performance.
