@ARTICLE{Adhikari2019-os,
  title    = "Learning Semantic Graphics Using Convolutional {Encoder-Decoder}
              Network for Autonomous Weeding in Paddy",
  author   = "Adhikari, Shyam Prasad and Yang, Heechan and Kim, Hyongsuk",
  abstract = "Weeds in agricultural farms are aggressive growers which compete
              for nutrition and other resources with the crop and reduce
              production. The increasing use of chemicals to control them has
              inadvertent consequences to the human health and the environment.
              In this work, a novel neural network training method combining
              semantic graphics for data annotation and an advanced
              encoder-decoder network for (a) automatic crop line detection and
              (b) weed (wild millet) detection in paddy fields is proposed. The
              detected crop lines act as a guiding line for an autonomous
              weeding robot for inter-row weeding, whereas the detection of
              weeds enables autonomous intra-row weeding. The proposed data
              annotation method, semantic graphics, is intuitive, and the
              desired targets can be annotated easily with minimal labor. Also,
              the proposed ``extended skip network'' is an improved deep
              convolutional encoder-decoder neural network for efficient
              learning of semantic graphics. Quantitative evaluations of the
              proposed method demonstrated an increment of 6.29\% and 6.14\% in
              mean intersection over union (mIoU), over the baseline network on
              the task of paddy line detection and wild millet detection,
              respectively. The proposed method also leads to a 3.56\%
              increment in mIoU and a significantly higher recall compared to a
              popular bounding box-based object detection approach on the task
              of wild-millet detection.",
  journal  = "Front. Plant Sci.",
  volume   =  10,
  pages    = "1404",
  month    =  oct,
  year     =  2019,
  keywords = "autonomous weeding; convolutional neural network; crop line
              extraction; encoder--decoder network; semantic graphics",
  language = "en"
}

@ARTICLE{Ahmad2021-gs,
  title     = "Performance of deep learning models for classifying and
               detecting common weeds in corn and soybean production systems",
  author    = "Ahmad, Aanis and Saraswat, Dharmendra and Aggarwal, Varun and
               Etienne, Aaron and Hancock, Benjamin",
  journal   = "Comput. Electron. Agric.",
  publisher = "Elsevier BV",
  volume    =  184,
  number    =  106081,
  pages     = "106081",
  month     =  may,
  year      =  2021,
  language  = "en"
}

@INPROCEEDINGS{Arun2020-eu,
  title           = "Reduced U-net architecture for classifying crop and weed
                     using pixel-wise segmentation",
  booktitle       = "2020 {IEEE} International Conference for Innovation in
                     Technology ({INOCON})",
  author          = "Arun, R Arumuga and Umamaheswari, S and Jain, Ashvini
                     Vimal",
  publisher       = "IEEE",
  month           =  nov,
  year            =  2020,
  conference      = "2020 IEEE International Conference for Innovation in
                     Technology (INOCON)",
  location        = "Bangluru, India"
}

@ARTICLE{Asad2020-wj,
  title     = "Weed detection in canola fields using maximum likelihood
               classification and deep convolutional neural network",
  author    = "Asad, Muhammad Hamza and Bais, Abdul",
  journal   = "Inf. Process. Agric.",
  publisher = "Elsevier BV",
  volume    =  7,
  number    =  4,
  pages     = "535--545",
  month     =  dec,
  year      =  2020,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@ARTICLE{Asad2023-zv,
  title     = "Improved crop and weed detection with diverse data ensemble
               learning",
  author    = "Asad, Muhammad Hamza and Anwar, Saeed and Bais, Abdul",
  abstract  = "Modern agriculture heavily relies on Site-Specific Farm
               Management practices, necessitating accurate detection,
               localization, and quantification of crops and weeds in the
               field, which can be achieved using deep learning techniques. In
               this regard, crop and weed-specific binary segmentation models
               have shown promise. However, uncontrolled field conditions limit
               their performance from one field to the other. To improve
               semantic model generalization, existing methods augment and
               synthesize agricultural data to account for uncontrolled field
               conditions. However, given highly varied field conditions, these
               methods have limitations. To overcome the challenges of model
               deterioration in such conditions, we propose utilizing data
               specific to other crops and weeds for our specific target
               problem. To achieve this, we propose a novel ensemble framework.
               Our approach involves utilizing different crop and weed models
               trained on diverse datasets and employing a teacher-student
               configuration. By using homogeneous stacking of base models and
               a trainable meta-architecture to combine their outputs, we
               achieve significant improvements for Canola crops and Kochia
               weeds on unseen test data, surpassing the performance of single
               semantic segmentation models. We identify the UNET
               meta-architecture as the most effective in this context.
               Finally, through ablation studies, we demonstrate and validate
               the effectiveness of our proposed model. We observe that
               including base models trained on other target crops and weeds
               can help generalize the model to capture varied field
               conditions. Lastly, we propose two novel datasets with varied
               conditions for comparisons.",
  publisher = "arXiv",
  year      =  2023
}

@ARTICLE{Gallo2023-zs,
  title     = "Deep object detection of crop weeds: Performance of {YOLOv7} on
               a real case dataset from {UAV}

@ARTICLE{Hasan2024-su,
  title     = "Object-level benchmark for deep learning-based detection and
               classification of weed species",
  author    = "Hasan, A S M Mahmudul and Diepeveen, Dean and Laga, Hamid and
               Jones, Michael G K and Sohel, Ferdous",
  journal   = "Crop Prot.",
  publisher = "Elsevier BV",
  volume    =  177,
  number    =  106561,
  pages     = "106561",
  month     =  mar,
  year      =  2024,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Islam2021-iy,
  title     = "Early weed detection using image processing and machine learning
               techniques in an Australian Chilli farm",
  author    = "Islam, Nahina and Rashid, Md Mamunur and Wibowo, Santoso and Xu,
               Cheng-Yuan and Morshed, Ahsan and Wasimi, Saleh A and Moore,
               Steven and Rahman, Sk Mostafizur",
  abstract  = "This paper explores the potential of machine learning algorithms
               for weed and crop classification from UAV images. The
               identification of weeds in crops is a challenging task that has
               been addressed through orthomosaicing of images, feature
               extraction and labelling of images to train machine learning
               algorithms. In this paper, the performances of several machine
               learning algorithms, random forest (RF), support vector machine
               (SVM) and k-nearest neighbours (KNN), are analysed to detect
               weeds using UAV images collected from a chilli crop field
               located in Australia. The evaluation metrics used in the
               comparison of performance were accuracy, precision, recall,
               false positive rate and kappa coefficient. MATLAB is used for
               simulating the machine learning algorithms; and the achieved
               weed detection accuracies are 96\% using RF, 94\% using SVM and
               63\% using KNN. Based on this study, RF and SVM algorithms are
               efficient and practical to use, and can be implemented easily
               for detecting weed from UAV images.",
  journal   = "Collect. FAO Agric.",
  publisher = "MDPI AG",
  volume    =  11,
  number    =  5,
  pages     = "387",
  month     =  apr,
  year      =  2021,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Khan2020-xb,
  title     = "{CED-Net}: Crops and weeds segmentation for smart farming using
               a small cascaded encoder-decoder architecture",
  author    = "Khan, Abbas and Ilyas, Talha and Umraiz, Muhammad and Mannan,
               Zubaer Ibna and Kim, Hyongsuk",
  abstract  = "Convolutional neural networks (CNNs) have achieved
               state-of-the-art performance in numerous aspects of human life
               and the agricultural sector is no exception. One of the main
               objectives of deep learning for smart farming is to identify the
               precise location of weeds and crops on farmland. In this paper,
               we propose a semantic segmentation method based on a cascaded
               encoder-decoder network, namely CED-Net, to differentiate weeds
               from crops. The existing architectures for weeds and crops
               segmentation are quite deep, with millions of parameters that
               require longer training time. To overcome such limitations, we
               propose an idea of training small networks in cascade to obtain
               coarse-to-fine predictions, which are then combined to produce
               the final results. Evaluation of the proposed network and
               comparison with other state-of-the-art networks are conducted
               using four publicly available datasets: rice seeding and weed
               dataset, BoniRob dataset, carrot crop vs. weed dataset, and a
               paddy--millet dataset. The experimental results and their
               comparisons proclaim that the proposed network outperforms
               state-of-the-art architectures, such as U-Net, SegNet, FCN-8s,
               and DeepLabv3, over intersection over union (IoU), F1-score,
               sensitivity, true detection rate, and average precision
               comparison metrics by utilizing only (1/5.74 $\times$ U-Net),
               (1/5.77 $\times$ SegNet), (1/3.04 $\times$ FCN-8s), and (1/3.24
               $\times$ DeepLabv3) fractions of total parameters.",
  journal   = "Electronics (Basel)",
  publisher = "MDPI AG",
  volume    =  9,
  number    =  10,
  pages     = "1602",
  month     =  oct,
  year      =  2020,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Moldvai2024-hb,
  title     = "Weed detection and classification with computer vision using a
               limited image dataset",
  author    = "Moldvai, L{\'a}szl{\'o} and Mesterh{\'a}zi, P{\'e}ter {\'A}kos
               and Teschner, Gergely and Ny{\'e}ki, Anik{\'o}",
  abstract  = "In agriculture, as precision farming increasingly employs robots
               to monitor crops, the use of weeding and harvesting robots is
               expanding the need for computer vision. Currently, most
               researchers and companies address these computer vision tasks
               with CNN-based deep learning. This technology requires large
               datasets of plant and weed images labeled by experts, as well as
               substantial computational resources. However, traditional
               feature-based approaches to computer vision can extract
               meaningful parameters and achieve comparably good classification
               results with only a tenth of the dataset size. This study
               presents these methods and seeks to determine the minimum number
               of training images required to achieve reliable classification.
               We tested the classification results with 5, 10, 20, 40, 80, and
               160 images per weed type in a four-class classification system.
               We extracted shape features, distance transformation features,
               color histograms, and texture features. Each type of feature was
               tested individually and in various combinations to determine the
               best results. Using six types of classifiers, we achieved a
               94.56\% recall rate with 160 images per weed. Better results
               were obtained with more training images and a greater variety of
               features.",
  journal   = "Appl. Sci.",
  publisher = "MDPI AG",
  volume    =  14,
  number    =  11,
  pages     = "4839",
  month     =  jun,
  year      =  2024,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Shackleton2024-uc,
  title     = "Enhancing rangeland weed detection through convolutional neural
               networks and transfer learning",
  author    = "Shackleton, Christian and Ali, Raja Hashim and Khan, Talha Ali",
  journal   = "Crop Design",
  publisher = "Elsevier BV",
  number    =  100060,
  pages     = "100060",
  month     =  jun,
  year      =  2024,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@ARTICLE{Teimouri2018-mx,
  title    = "Weed Growth Stage Estimator Using Deep Convolutional Neural
              Networks",
  author   = "Teimouri, Nima and Dyrmann, Mads and Nielsen, Per Rydahl and
              Mathiassen, Solvejg Kopp and Somerville, Gayle J and
              J{\o}rgensen, Rasmus Nyholm",
  abstract = "This study outlines a new method of automatically estimating weed
              species and growth stages (from cotyledon until eight leaves are
              visible) of in situ images covering 18 weed species or families.
              Images of weeds growing within a variety of crops were gathered
              across variable environmental conditions with regards to soil
              types, resolution and light settings. Then, 9649 of these images
              were used for training the computer, which automatically divided
              the weeds into nine growth classes. The performance of this
              proposed convolutional neural network approach was evaluated on a
              further set of 2516 images, which also varied in term of crop,
              soil type, image resolution and light conditions. The overall
              performance of this approach achieved a maximum accuracy of 78\%
              for identifying spp. and a minimum accuracy of 46\% for
              blackgrass. In addition, it achieved an average 70\% accuracy
              rate in estimating the number of leaves and 96\% accuracy when
              accepting a deviation of two leaves. These results show that this
              new method of using deep convolutional neural networks has a
              relatively high ability to estimate early growth stages across a
              wide variety of weed species.",
  journal  = "Sensors",
  volume   =  18,
  number   =  5,
  month    =  may,
  year     =  2018,
  keywords = "computer vision; convolutional neural network; deep learning;
              growth stage; leaf counting",
  language = "en"
}

@ARTICLE{Wang2024-tt,
  title    = "Weed detection and recognition in complex wheat fields based on
              an improved {YOLOv7}",
  author   = "Wang, Kaixin and Hu, Xihong and Zheng, Huiwen and Lan, Maoyang
              and Liu, Changjiang and Liu, Yihui and Zhong, Lei and Li, Hai and
              Tan, Suiyan",
  abstract = "INTRODUCTION: The precise detection of weeds in the field is the
              premise of implementing weed management. However, the similar
              color, morphology, and occlusion between wheat and weeds pose a
              challenge to the detection of weeds. In this study, a CSCW-YOLOv7
              based on an improved YOLOv7 architecture was proposed to identify
              five types of weeds in complex wheat fields. METHODS: First, a
              dataset was constructed for five weeds that are commonly found,
              namely, , thistle, golden saxifrage, shepherd's purse herb, and .
              Second, a wheat weed detection model called CSCW-YOLOv7 was
              proposed to achieve the accurate identification and
              classification of wheat weeds. In the CSCW-YOLOv7, the CARAFE
              operator was introduced as an up-sampling algorithm to improve
              the recognition of small targets. Then, the
              Squeeze-and-Excitation (SE) network was added to the Extended
              Latent Attention Networks (ELAN) module in the backbone network
              and the concatenation layer in the feature fusion module to
              enhance important weed features and suppress irrelevant features.
              In addition, the contextual transformer (CoT) module, a
              transformer-based architectural design, was used to capture
              global information and enhance self-attention by mining
              contextual information between neighboring keys. Finally, the
              Wise Intersection over Union (WIoU) loss function introducing a
              dynamic nonmonotonic focusing mechanism was employed to better
              predict the bounding boxes of the occluded weed. RESULTS AND
              DISCUSSION: The ablation experiment results showed that the
              CSCW-YOLOv7 achieved the best performance among the other models.
              The accuracy, recall, and mean average precision (mAP) values of
              the CSCW-YOLOv7 were 97.7\%, 98\%, and 94.4\%, respectively.
              Compared with the baseline YOLOv7, the improved CSCW-YOLOv7
              obtained precision, recall, and mAP increases of 1.8\%, 1\%, and
              2.1\%, respectively. Meanwhile, the parameters were compressed by
              10.7\% with a 3.8-MB reduction, resulting in a 10\% decrease in
              floating-point operations per second (FLOPs). The
              Gradient-weighted Class Activation Mapping (Grad-CAM)
              visualization method suggested that the CSCW-YOLOv7 can learn a
              more representative set of features that can help better locate
              the weeds of different scales in complex field environments. In
              addition, the performance of the CSCW-YOLOv7 was compared to the
              widely used deep learning models, and results indicated that the
              CSCW-YOLOv7 exhibits a better ability to distinguish the
              overlapped weeds and small-scale weeds. The overall results
              suggest that the CSCW-YOLOv7 is a promising tool for the
              detection of weeds and has great potential for field
              applications.",
  journal  = "Front. Plant Sci.",
  volume   =  15,
  pages    = "1372237",
  month    =  jun,
  year     =  2024,
  keywords = "CARAFE; CoT; SE; WIoU; wheat fields; wheat weed detection",
  language = "en"
}

