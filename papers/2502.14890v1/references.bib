@ARTICLE{Shackleton2024-uc,
  title     = "Enhancing rangeland weed detection through convolutional neural
               networks and transfer learning",
  author    = "Shackleton, Christian and Ali, Raja Hashim and Khan, Talha Ali",
  journal   = "Crop Design",
  publisher = "Elsevier BV",
  number    =  100060,
  pages     = "100060",
  month     =  jun,
  year      =  2024,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@INPROCEEDINGS{Li2020-lt,
  title           = "Light-weight {RetinaNet} for object detection on edge
                     devices",
  booktitle       = "2020 {IEEE} 6th World Forum on Internet of Things
                     ({WF-IoT})",
  author          = "Li, Yixing and Dua, Akshay and Ren, Fengbo",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2020,
  copyright       = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  conference      = "2020 IEEE 6th World Forum on Internet of Things (WF-IoT)",
  location        = "New Orleans, LA, USA"
}

@ARTICLE{Moldvai2024-hb,
  title     = "Weed detection and classification with computer vision using a
               limited image dataset",
  author    = "Moldvai, L{\'a}szl{\'o} and Mesterh{\'a}zi, P{\'e}ter {\'A}kos
               and Teschner, Gergely and Ny{\'e}ki, Anik{\'o}",
  abstract  = "In agriculture, as precision farming increasingly employs robots
               to monitor crops, the use of weeding and harvesting robots is
               expanding the need for computer vision. Currently, most
               researchers and companies address these computer vision tasks
               with CNN-based deep learning. This technology requires large
               datasets of plant and weed images labeled by experts, as well as
               substantial computational resources. However, traditional
               feature-based approaches to computer vision can extract
               meaningful parameters and achieve comparably good classification
               results with only a tenth of the dataset size. This study
               presents these methods and seeks to determine the minimum number
               of training images required to achieve reliable classification.
               We tested the classification results with 5, 10, 20, 40, 80, and
               160 images per weed type in a four-class classification system.
               We extracted shape features, distance transformation features,
               color histograms, and texture features. Each type of feature was
               tested individually and in various combinations to determine the
               best results. Using six types of classifiers, we achieved a
               94.56\% recall rate with 160 images per weed. Better results
               were obtained with more training images and a greater variety of
               features.",
  journal   = "Appl. Sci.",
  publisher = "MDPI AG",
  volume    =  14,
  number    =  11,
  pages     = "4839",
  month     =  jun,
  year      =  2024,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Shelton1983-ng,
  title     = "Effects of Weeds on the Diversity and Abundance of Insects in
               Soybeans1",
  author    = "Shelton, M D and Edwards, C R",
  abstract  = "Abstract. Sweep-net sampling and pitfall trapping were used to
               survey insects in weedy and weedfree soybean habitats. Weedy
               soybean habitats consisted of (",
  journal   = "Environ. Entomol.",
  publisher = "Oxford Academic",
  volume    =  12,
  number    =  2,
  pages     = "296--298",
  month     =  apr,
  year      =  1983,
  language  = "en"
}

@ARTICLE{Carion2020-qn,
  title     = "{End-to-End} Object Detection with Transformers",
  author    = "Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and
               Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey",
  abstract  = "We present a new method that views object detection as a direct
               set prediction problem. Our approach streamlines the detection
               pipeline, effectively removing the need for many hand-designed
               components like a non-maximum suppression procedure or anchor
               generation that explicitly encode our prior knowledge about the
               task. The main ingredients of the new framework, called
               DEtection TRansformer or DETR, are a set-based global loss that
               forces unique predictions via bipartite matching, and a
               transformer encoder-decoder architecture. Given a fixed small
               set of learned object queries, DETR reasons about the relations
               of the objects and the global image context to directly output
               the final set of predictions in parallel. The new model is
               conceptually simple and does not require a specialized library,
               unlike many other modern detectors. DETR demonstrates accuracy
               and run-time performance on par with the well-established and
               highly-optimized Faster R-CNN baseline on the challenging COCO
               object detection dataset. Moreover, DETR can be easily
               generalized to produce panoptic segmentation in a unified
               manner. We show that it significantly outperforms competitive
               baselines. Training code and pretrained models are available at
               https://github.com/facebookresearch/detr .",
  journal   = "Computer Vision -- ECCV 2020",
  publisher = "Springer International Publishing",
  pages     = "213--229",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Almalky2023-ot,
  title     = "Deep Learning for Detecting and Classifying the Growth Stages of
               Consolida regalis Weeds on Fields",
  author    = "Almalky, Abeer M and Ahmed, Khaled R",
  abstract  = "Due to the massive surge in the world population, the
               agriculture cycle expansion is necessary to accommodate the
               anticipated demand. However, this expansion is challenged by
               weed invasion, a detrimental factor for agricultural production
               and quality. Therefore, an accurate, automatic, low-cost,
               environment-friendly, and real-time weed detection technique is
               required to control weeds on fields. Furthermore, automating the
               weed classification process according to growth stages is
               crucial for using appropriate weed controlling techniques, which
               represents a gap of research. The main focus of the undertaken
               research described in this paper is on providing a feasibility
               study for the agriculture community using recent deep-learning
               models to address this gap of research on classification of weed
               growth stages. For this paper we used a drone to collect a
               dataset of four weed (Consolida regalis) growth stages. In
               addition, we developed and trained one-stage and two-stage
               models YOLOv5, RetinaNet (with Resnet-101-FPN, Resnet-50-FPN
               backbones) and Faster R-CNN (with Resnet-101-DC5,
               Resnet-101-FPN, Resnet-50-FPN backbones), respectively. The
               results show that the generated Yolov5-small model succeeds in
               detecting weeds and classifying weed growth stages in real time
               with the highest recall of 0.794. RetinaNet with ResNet-101-FPN
               backbone shows accurate results in the testing phase (average
               precision of 87.457). Although Yolov5-large showed the highest
               precision in classifying almost all weed growth stages,
               Yolov5-large could not detect all objects in tested images.
               Overall, RetinaNet with ResNet-101-FPN backbones shows accurate
               and high precision, whereas Yolov5-small shows the shortest
               inference time in real time for detecting a weed and classifying
               its growth stages.",
  journal   = "Agronomy",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  13,
  number    =  3,
  pages     = "934",
  month     =  mar,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Gao2024-km,
  title     = "Weed Management Methods for Herbaceous Field Crops: A Review",
  author    = "Gao, Wen-Tao and Su, Wen-Hao",
  abstract  = "Weeds compete with crops for water and nutrients and can
               adversely affect crop growth and yield, so it is important to
               research effective weed control methods. This paper provides an
               overview of the impact of weeds on crop yield and describes the
               current state of research on weed management in field herbaceous
               crops. Physical weed control mainly refers to thermal
               technologies represented by flame weed control and laser weed
               control, which can efficiently and accurately remove weeds.
               Mechanical weed control requires a combination of sensor
               technologies, machine vision technology, and high-precision
               navigation to improve weed control accuracy. Biological weed
               control relies heavily on plant extracts and pathogens to create
               herbicides, but it is costly, and some can be toxic to mammals.
               Chemical weed control is a common method, resulting in
               environmental pollution and weed resistance. To reduce the use
               of chemical herbicides, scholars have proposed integrated weed
               management strategies, which combine biological control, control
               of the seed bank, and improve crop competitiveness. Integrated
               weed management strategies are considered to be the future
               direction of weed management. In conclusion, physical,
               mechanical, biological, and chemical weed control methods are
               commonly used in weed management. Each method has its applicable
               scenarios, and the implementation of integrated weed management
               strategies can lead to better weed control, improving crop yield
               and quality. The main objective of this review is to organize
               the research progress on weed management methods for herbaceous
               crops in the field and to provide a reference for the
               agricultural sector to develop weed control strategies.
               Specifically, this paper categorizes weed management methods
               into four groups, discusses and presents the advantages and
               disadvantages of the aforementioned weed control methods, and
               discusses future research directions.",
  journal   = "Agronomy",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  14,
  number    =  3,
  pages     = "486",
  month     =  feb,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Woyessa2022-om,
  title    = "Weed Control Methods Used in Agriculture",
  author   = "Woyessa, Degefa",
  abstract = "One of the most challenging duties in an agricultural field is
              weed control. Weed control is a major problem for peasant framers
              everywhere. This paper's objectives are to reviewing and
              reporting the weed management practices utilized in Ethiopian
              agriculture and to recommend the viability of mechanizing weeding
              operations for the further research. In agriculture, three weed
              management techniques are frequently used. These are chemical,
              mechanical, and manual control. Farmers that are convinced of its
              benefits quickly adopt mechanical weed control. In addition to
              pulling weeds out from between the rows of crops, mechanical weed
              management maintains the soil's top loose, improving soil
              aeration and water absorption. Mechanical weeding saves time and
              labor while lowering labor costs also. Some weeding tools that
              are powered by animals are effective in terms of time and money
              savings. Herbicides are occasionally used by farmers; however,
              they are not often used in farming. In conclusion, weed control
              is one of the most essential and expensive operation in
              agriculture. Weeding by mechanical device reduces the cost of
              labour and also saves time than any other methods of weed
              control. Therefore, instead of manual weeding and using
              chemicals, the use of a mechanical weeding machine should be
              given priority.",
  journal  = "Am. J. Life Sci. Innov.",
  volume   =  1,
  number   =  1,
  pages    = "19--26",
  month    =  jul,
  year     =  2022,
  keywords = "Manual; Mechanical implements; Weeds; Weed control; Weeding
              efficiency",
  language = "en"
}

@ARTICLE{Wang2024-tt,
  title    = "Weed detection and recognition in complex wheat fields based on
              an improved {YOLOv7}",
  author   = "Wang, Kaixin and Hu, Xihong and Zheng, Huiwen and Lan, Maoyang
              and Liu, Changjiang and Liu, Yihui and Zhong, Lei and Li, Hai and
              Tan, Suiyan",
  abstract = "INTRODUCTION: The precise detection of weeds in the field is the
              premise of implementing weed management. However, the similar
              color, morphology, and occlusion between wheat and weeds pose a
              challenge to the detection of weeds. In this study, a CSCW-YOLOv7
              based on an improved YOLOv7 architecture was proposed to identify
              five types of weeds in complex wheat fields. METHODS: First, a
              dataset was constructed for five weeds that are commonly found,
              namely, , thistle, golden saxifrage, shepherd's purse herb, and .
              Second, a wheat weed detection model called CSCW-YOLOv7 was
              proposed to achieve the accurate identification and
              classification of wheat weeds. In the CSCW-YOLOv7, the CARAFE
              operator was introduced as an up-sampling algorithm to improve
              the recognition of small targets. Then, the
              Squeeze-and-Excitation (SE) network was added to the Extended
              Latent Attention Networks (ELAN) module in the backbone network
              and the concatenation layer in the feature fusion module to
              enhance important weed features and suppress irrelevant features.
              In addition, the contextual transformer (CoT) module, a
              transformer-based architectural design, was used to capture
              global information and enhance self-attention by mining
              contextual information between neighboring keys. Finally, the
              Wise Intersection over Union (WIoU) loss function introducing a
              dynamic nonmonotonic focusing mechanism was employed to better
              predict the bounding boxes of the occluded weed. RESULTS AND
              DISCUSSION: The ablation experiment results showed that the
              CSCW-YOLOv7 achieved the best performance among the other models.
              The accuracy, recall, and mean average precision (mAP) values of
              the CSCW-YOLOv7 were 97.7\%, 98\%, and 94.4\%, respectively.
              Compared with the baseline YOLOv7, the improved CSCW-YOLOv7
              obtained precision, recall, and mAP increases of 1.8\%, 1\%, and
              2.1\%, respectively. Meanwhile, the parameters were compressed by
              10.7\% with a 3.8-MB reduction, resulting in a 10\% decrease in
              floating-point operations per second (FLOPs). The
              Gradient-weighted Class Activation Mapping (Grad-CAM)
              visualization method suggested that the CSCW-YOLOv7 can learn a
              more representative set of features that can help better locate
              the weeds of different scales in complex field environments. In
              addition, the performance of the CSCW-YOLOv7 was compared to the
              widely used deep learning models, and results indicated that the
              CSCW-YOLOv7 exhibits a better ability to distinguish the
              overlapped weeds and small-scale weeds. The overall results
              suggest that the CSCW-YOLOv7 is a promising tool for the
              detection of weeds and has great potential for field
              applications.",
  journal  = "Front. Plant Sci.",
  volume   =  15,
  pages    = "1372237",
  month    =  jun,
  year     =  2024,
  keywords = "CARAFE; CoT; SE; WIoU; wheat fields; wheat weed detection",
  language = "en"
}

@INPROCEEDINGS{Arun2020-eu,
  title           = "Reduced U-net architecture for classifying crop and weed
                     using pixel-wise segmentation",
  booktitle       = "2020 {IEEE} International Conference for Innovation in
                     Technology ({INOCON})",
  author          = "Arun, R Arumuga and Umamaheswari, S and Jain, Ashvini
                     Vimal",
  publisher       = "IEEE",
  month           =  nov,
  year            =  2020,
  conference      = "2020 IEEE International Conference for Innovation in
                     Technology (INOCON)",
  location        = "Bangluru, India"
}

@ARTICLE{Mohanty2016-hu,
  title    = "Using Deep Learning for {Image-Based} Plant Disease Detection",
  author   = "Mohanty, Sharada P and Hughes, David P and Salath{\'e}, Marcel",
  abstract = "Crop diseases are a major threat to food security, but their
              rapid identification remains difficult in many parts of the world
              due to the lack of the necessary infrastructure. The combination
              of increasing global smartphone penetration and recent advances
              in computer vision made possible by deep learning has paved the
              way for smartphone-assisted disease diagnosis. Using a public
              dataset of 54,306 images of diseased and healthy plant leaves
              collected under controlled conditions, we train a deep
              convolutional neural network to identify 14 crop species and 26
              diseases (or absence thereof). The trained model achieves an
              accuracy of 99.35\% on a held-out test set, demonstrating the
              feasibility of this approach. Overall, the approach of training
              deep learning models on increasingly large and publicly available
              image datasets presents a clear path toward smartphone-assisted
              crop disease diagnosis on a massive global scale.",
  journal  = "Front. Plant Sci.",
  volume   =  7,
  pages    = "1419",
  month    =  sep,
  year     =  2016,
  keywords = "crop diseases; deep learning; digital epidemiology; machine
              learning",
  language = "en"
}

@ARTICLE{Teimouri2018-mx,
  title    = "Weed Growth Stage Estimator Using Deep Convolutional Neural
              Networks",
  author   = "Teimouri, Nima and Dyrmann, Mads and Nielsen, Per Rydahl and
              Mathiassen, Solvejg Kopp and Somerville, Gayle J and
              J{\o}rgensen, Rasmus Nyholm",
  abstract = "This study outlines a new method of automatically estimating weed
              species and growth stages (from cotyledon until eight leaves are
              visible) of in situ images covering 18 weed species or families.
              Images of weeds growing within a variety of crops were gathered
              across variable environmental conditions with regards to soil
              types, resolution and light settings. Then, 9649 of these images
              were used for training the computer, which automatically divided
              the weeds into nine growth classes. The performance of this
              proposed convolutional neural network approach was evaluated on a
              further set of 2516 images, which also varied in term of crop,
              soil type, image resolution and light conditions. The overall
              performance of this approach achieved a maximum accuracy of 78\%
              for identifying spp. and a minimum accuracy of 46\% for
              blackgrass. In addition, it achieved an average 70\% accuracy
              rate in estimating the number of leaves and 96\% accuracy when
              accepting a deviation of two leaves. These results show that this
              new method of using deep convolutional neural networks has a
              relatively high ability to estimate early growth stages across a
              wide variety of weed species.",
  journal  = "Sensors",
  volume   =  18,
  number   =  5,
  month    =  may,
  year     =  2018,
  keywords = "computer vision; convolutional neural network; deep learning;
              growth stage; leaf counting",
  language = "en"
}

@ARTICLE{Hasan2024-su,
  title     = "Object-level benchmark for deep learning-based detection and
               classification of weed species",
  author    = "Hasan, A S M Mahmudul and Diepeveen, Dean and Laga, Hamid and
               Jones, Michael G K and Sohel, Ferdous",
  journal   = "Crop Prot.",
  publisher = "Elsevier BV",
  volume    =  177,
  number    =  106561,
  pages     = "106561",
  month     =  mar,
  year      =  2024,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Ahmad2021-gs,
  title     = "Performance of deep learning models for classifying and
               detecting common weeds in corn and soybean production systems",
  author    = "Ahmad, Aanis and Saraswat, Dharmendra and Aggarwal, Varun and
               Etienne, Aaron and Hancock, Benjamin",
  journal   = "Comput. Electron. Agric.",
  publisher = "Elsevier BV",
  volume    =  184,
  number    =  106081,
  pages     = "106081",
  month     =  may,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Asad2020-wj,
  title     = "Weed detection in canola fields using maximum likelihood
               classification and deep convolutional neural network",
  author    = "Asad, Muhammad Hamza and Bais, Abdul",
  journal   = "Inf. Process. Agric.",
  publisher = "Elsevier BV",
  volume    =  7,
  number    =  4,
  pages     = "535--545",
  month     =  dec,
  year      =  2020,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@ARTICLE{Islam2021-iy,
  title     = "Early weed detection using image processing and machine learning
               techniques in an Australian Chilli farm",
  author    = "Islam, Nahina and Rashid, Md Mamunur and Wibowo, Santoso and Xu,
               Cheng-Yuan and Morshed, Ahsan and Wasimi, Saleh A and Moore,
               Steven and Rahman, Sk Mostafizur",
  abstract  = "This paper explores the potential of machine learning algorithms
               for weed and crop classification from UAV images. The
               identification of weeds in crops is a challenging task that has
               been addressed through orthomosaicing of images, feature
               extraction and labelling of images to train machine learning
               algorithms. In this paper, the performances of several machine
               learning algorithms, random forest (RF), support vector machine
               (SVM) and k-nearest neighbours (KNN), are analysed to detect
               weeds using UAV images collected from a chilli crop field
               located in Australia. The evaluation metrics used in the
               comparison of performance were accuracy, precision, recall,
               false positive rate and kappa coefficient. MATLAB is used for
               simulating the machine learning algorithms; and the achieved
               weed detection accuracies are 96\% using RF, 94\% using SVM and
               63\% using KNN. Based on this study, RF and SVM algorithms are
               efficient and practical to use, and can be implemented easily
               for detecting weed from UAV images.",
  journal   = "Collect. FAO Agric.",
  publisher = "MDPI AG",
  volume    =  11,
  number    =  5,
  pages     = "387",
  month     =  apr,
  year      =  2021,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Asad2023-zv,
  title     = "Improved crop and weed detection with diverse data ensemble
               learning",
  author    = "Asad, Muhammad Hamza and Anwar, Saeed and Bais, Abdul",
  abstract  = "Modern agriculture heavily relies on Site-Specific Farm
               Management practices, necessitating accurate detection,
               localization, and quantification of crops and weeds in the
               field, which can be achieved using deep learning techniques. In
               this regard, crop and weed-specific binary segmentation models
               have shown promise. However, uncontrolled field conditions limit
               their performance from one field to the other. To improve
               semantic model generalization, existing methods augment and
               synthesize agricultural data to account for uncontrolled field
               conditions. However, given highly varied field conditions, these
               methods have limitations. To overcome the challenges of model
               deterioration in such conditions, we propose utilizing data
               specific to other crops and weeds for our specific target
               problem. To achieve this, we propose a novel ensemble framework.
               Our approach involves utilizing different crop and weed models
               trained on diverse datasets and employing a teacher-student
               configuration. By using homogeneous stacking of base models and
               a trainable meta-architecture to combine their outputs, we
               achieve significant improvements for Canola crops and Kochia
               weeds on unseen test data, surpassing the performance of single
               semantic segmentation models. We identify the UNET
               meta-architecture as the most effective in this context.
               Finally, through ablation studies, we demonstrate and validate
               the effectiveness of our proposed model. We observe that
               including base models trained on other target crops and weeds
               can help generalize the model to capture varied field
               conditions. Lastly, we propose two novel datasets with varied
               conditions for comparisons.",
  publisher = "arXiv",
  year      =  2023
}

@MISC{noauthor_undated-hp,
  title        = "Website",
  howpublished = "\url{https://doi.org/10.48550/arXiv.1905.10011}",
  note         = "Accessed: NA-NA-NA"
}

@ARTICLE{Gallo2023-zs,
  title     = "Deep object detection of crop weeds: Performance of {YOLOv7} on
               a real case dataset from {UAV} images",
  author    = "Gallo, Ignazio and Rehman, Anwar Ur and Dehkordi, Ramin
               Heidarian and Landro, Nicola and La Grassa, Riccardo and
               Boschetti, Mirco",
  abstract  = "Weeds are a crucial threat to agriculture, and in order to
               preserve crop productivity, spreading agrochemicals is a common
               practice with a potential negative impact on the environment.
               Methods that can support intelligent application are needed.
               Therefore, identification and mapping is a critical step in
               performing site-specific weed management. Unmanned aerial
               vehicle (UAV) data streams are considered the best for weed
               detection due to the high resolution and flexibility of data
               acquisition and the spatial explicit dimensions of imagery.
               However, with the existence of unstructured crop conditions and
               the high biological variation of weeds, it remains a difficult
               challenge to generate accurate weed recognition and detection
               models. Two critical barriers to tackling this challenge are
               related to (1) a lack of case-specific, large, and comprehensive
               weed UAV image datasets for the crop of interest, (2) defining
               the most appropriate computer vision (CV) weed detection models
               to assess the operationality of detection approaches in real
               case conditions. Deep Learning (DL) algorithms, appropriately
               trained to deal with the real case complexity of UAV data in
               agriculture, can provide valid alternative solutions with
               respect to standard CV approaches for an accurate weed
               recognition model. In this framework, this paper first
               introduces a new weed and crop dataset named Chicory Plant (CP)
               and then tests state-of-the-art DL algorithms for object
               detection. A total of 12,113 bounding box annotations were
               generated to identify weed targets (Mercurialis annua) from more
               than 3000 RGB images of chicory plantations, collected using a
               UAV system at various stages of crop and weed growth. Deep weed
               object detection was conducted by testing the most recent You
               Only Look Once version 7 (YOLOv7) on both the CP and publicly
               available datasets (Lincoln beet (LB)), for which a previous
               version of YOLO was used to map weeds and crops. The YOLOv7
               results obtained for the CP dataset were encouraging,
               outperforming the other YOLO variants by producing value metrics
               of 56.6\%, 62.1\%, and 61.3\% for the mAP@0.5 scores, recall,
               and precision, respectively. Furthermore, the YOLOv7 model
               applied to the LB dataset surpassed the existing published
               results by increasing the mAP@0.5 scores from 51\% to 61\%,
               67.5\% to 74.1\%, and 34.6\% to 48\% for the total mAP, mAP for
               weeds, and mAP for sugar beets, respectively. This study
               illustrates the potential of the YOLOv7 model for weed detection
               but remarks on the fundamental needs of large-scale, annotated
               weed datasets to develop and evaluate models in real-case field
               circumstances.",
  journal   = "Remote Sens. (Basel)",
  publisher = "MDPI AG",
  volume    =  15,
  number    =  2,
  pages     = "539",
  month     =  jan,
  year      =  2023,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Hashemi-Beni2022-rz,
  title     = "Deep convolutional neural networks for weeds and crops
               discrimination from {UAS} imagery",
  author    = "Hashemi-Beni, Leila and Gebrehiwot, Asmamaw and Karimoddini, Ali
               and Shahbazi, Abolghasem and Dorbu, Freda",
  abstract  = "Weeds are among the significant factors that could harm crop
               yield by invading crops and smother pastures, and significantly
               decrease the quality of the harvested crops. Herbicides are
               widely used in agriculture to control weeds; however, excessive
               use of herbicides in agriculture can lead to environmental
               pollution as well as yield reduction. Accurate mapping of
               crops/weeds is essential to determine weeds' location and
               locally treat those areas. Increasing demand for flexible,
               accurate and lower cost precision agriculture technology has
               resulted in advancements in UAS-based remote sensing data
               collection and methods. Deep learning methods have been
               successfully employed for UAS data processing and mapping tasks
               in different domains. This research investigate, compares and
               evaluates the performance of deep learning methods for crop/weed
               discrimination on two open-source and published benchmark
               datasets captured by different UASs (field robot and UAV) and
               labeled by experts. We specifically investigate the following
               architectures: 1) U-Net Model 2) SegNet 3) FCN (FCN-32s,
               FCN-16s, FCN-8s) 4) DepLabV3+. The deep learning models were
               fine-tuned to classify the UAS datasets into three classes
               (background, crops, and weeds). The classification accuracy
               achieved by U-Net is 77.9\% higher than 62.6\% of SegNet, 68.4\%
               of FCN-32s, 77.2\% of FCN-16s, and slightly lower than 81.1\% of
               FCN-8s, and 84.3\% of DepLab v3+. Experimental results showed
               that the ResNet-18 based segmentation model such as DepLab v3+
               could precisely extract weeds compared to other classifiers.",
  journal   = "Front. Remote Sens.",
  publisher = "Frontiers Media SA",
  volume    =  3,
  month     =  feb,
  year      =  2022,
  copyright = "https://creativecommons.org/licenses/by/4.0/"
}

@ARTICLE{Adhikari2019-os,
  title    = "Learning Semantic Graphics Using Convolutional {Encoder-Decoder}
              Network for Autonomous Weeding in Paddy",
  author   = "Adhikari, Shyam Prasad and Yang, Heechan and Kim, Hyongsuk",
  abstract = "Weeds in agricultural farms are aggressive growers which compete
              for nutrition and other resources with the crop and reduce
              production. The increasing use of chemicals to control them has
              inadvertent consequences to the human health and the environment.
              In this work, a novel neural network training method combining
              semantic graphics for data annotation and an advanced
              encoder-decoder network for (a) automatic crop line detection and
              (b) weed (wild millet) detection in paddy fields is proposed. The
              detected crop lines act as a guiding line for an autonomous
              weeding robot for inter-row weeding, whereas the detection of
              weeds enables autonomous intra-row weeding. The proposed data
              annotation method, semantic graphics, is intuitive, and the
              desired targets can be annotated easily with minimal labor. Also,
              the proposed ``extended skip network'' is an improved deep
              convolutional encoder-decoder neural network for efficient
              learning of semantic graphics. Quantitative evaluations of the
              proposed method demonstrated an increment of 6.29\% and 6.14\% in
              mean intersection over union (mIoU), over the baseline network on
              the task of paddy line detection and wild millet detection,
              respectively. The proposed method also leads to a 3.56\%
              increment in mIoU and a significantly higher recall compared to a
              popular bounding box-based object detection approach on the task
              of wild-millet detection.",
  journal  = "Front. Plant Sci.",
  volume   =  10,
  pages    = "1404",
  month    =  oct,
  year     =  2019,
  keywords = "autonomous weeding; convolutional neural network; crop line
              extraction; encoder--decoder network; semantic graphics",
  language = "en"
}

@ARTICLE{Khan2020-xb,
  title     = "{CED-Net}: Crops and weeds segmentation for smart farming using
               a small cascaded encoder-decoder architecture",
  author    = "Khan, Abbas and Ilyas, Talha and Umraiz, Muhammad and Mannan,
               Zubaer Ibna and Kim, Hyongsuk",
  abstract  = "Convolutional neural networks (CNNs) have achieved
               state-of-the-art performance in numerous aspects of human life
               and the agricultural sector is no exception. One of the main
               objectives of deep learning for smart farming is to identify the
               precise location of weeds and crops on farmland. In this paper,
               we propose a semantic segmentation method based on a cascaded
               encoder-decoder network, namely CED-Net, to differentiate weeds
               from crops. The existing architectures for weeds and crops
               segmentation are quite deep, with millions of parameters that
               require longer training time. To overcome such limitations, we
               propose an idea of training small networks in cascade to obtain
               coarse-to-fine predictions, which are then combined to produce
               the final results. Evaluation of the proposed network and
               comparison with other state-of-the-art networks are conducted
               using four publicly available datasets: rice seeding and weed
               dataset, BoniRob dataset, carrot crop vs. weed dataset, and a
               paddy--millet dataset. The experimental results and their
               comparisons proclaim that the proposed network outperforms
               state-of-the-art architectures, such as U-Net, SegNet, FCN-8s,
               and DeepLabv3, over intersection over union (IoU), F1-score,
               sensitivity, true detection rate, and average precision
               comparison metrics by utilizing only (1/5.74 $\times$ U-Net),
               (1/5.77 $\times$ SegNet), (1/3.04 $\times$ FCN-8s), and (1/3.24
               $\times$ DeepLabv3) fractions of total parameters.",
  journal   = "Electronics (Basel)",
  publisher = "MDPI AG",
  volume    =  9,
  number    =  10,
  pages     = "1602",
  month     =  oct,
  year      =  2020,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}
@article{monteiro2022sustainable,
  title={Sustainable approach to weed management: The role of precision weed management},
  author={Monteiro, Ant{\'o}nio and Santos, S{\'e}rgio},
  journal={Agronomy},
  volume={12},
  number={1},
  pages={118},
  year={2022},
  publisher={MDPI}
}
@article{swinton2017hoes,
  title={Hoes to herbicides: economics of evolving weed management in the United States},
  author={Swinton, Scott M and Van Deynze, Braeden},
  journal={The European Journal of Development Research},
  volume={29},
  pages={560--574},
  year={2017},
  publisher={Springer}
}
@article{huang2023normalization,
  title={Normalization techniques in training dnns: Methodology, analysis and application},
  author={Huang, Lei and Qin, Jie and Zhou, Yi and Zhu, Fan and Liu, Li and Shao, Ling},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={8},
  pages={10173--10196},
  year={2023},
  publisher={IEEE}
}
@article{comer1999morphological,
  title={Morphological operations for color image processing},
  author={Comer, Mary L and Delp III, Edward J},
  journal={Journal of electronic imaging},
  volume={8},
  number={3},
  pages={279--289},
  year={1999},
  publisher={SPIE}
}
@article{clark2015pillow,
  title={Pillow (PIL fork) documentation},
  author={Clark, Alex and Contributors},
  journal={Read the Docs},
  year={2015}
}

@misc{labelImg2024,
  author = {Tzutalin, L.},
  title = {LabelImg},
  year = {2015},
  note = {Accessed: Aug 12, 2024},
  howpublished = {\url{https://github.com/tzutalin/labelImg}}
}


@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European Conference on Computer Vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={248--255},
  year={2009},
  organization={IEEE}
}

@article{tan2024lightweight,
  title={How Lightweight Can A Vision Transformer Be?},
  author={Tan, Jen Hong},
  journal={arXiv preprint arXiv:2407.17783},
  year={2024}
}
@inproceedings{yin2019context,
  title={Context and attribute grounded dense captioning},
  author={Yin, Guojun and Sheng, Lu and Liu, Bin and Yu, Nenghai and Wang, Xiaogang and Shao, Jing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6241--6250},
  year={2019}
}
@inproceedings{ye2020cost,
  title={A cost matrix optimization method based on spatial constraints under hungarian algorithm},
  author={Ye, Yu and Ke, Xiao and Yu, Zhiyong},
  booktitle={Proceedings of the 6th International Conference on Robotics and Artificial Intelligence},
  pages={134--139},
  year={2020}
}
@article{li2019light,
  title={Light-weight retinanet for object detection},
  author={Li, Yixing and Ren, Fengbo},
  journal={arXiv preprint arXiv:1905.10011},
  year={2019}
}
@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}
@article{ilyas2022datamodels,
  title={Datamodels: Predicting predictions from training data},
  author={Ilyas, Andrew and Park, Sung Min and Engstrom, Logan and Leclerc, Guillaume and Madry, Aleksander},
  journal={arXiv preprint arXiv:2202.00622},
  year={2022}
}
@inproceedings{robertson2008new,
  title={A new interpretation of average precision},
  author={Robertson, Stephen},
  booktitle={Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={689--690},
  year={2008}
}
@article{zhu2004recall,
  title={Recall, precision and average precision},
  author={Zhu, Mu},
  journal={Department of Statistics and Actuarial Science, University of Waterloo, Waterloo},
  volume={2},
  number={30},
  pages={6},
  year={2004}
}
@misc{illinois,
	author = {Aaron Hager},
	title = {Weed Management},
	howpublished = {\url{https://extension.illinois.edu}},
	year = {2022},
	note = {[Accessed 14-08-2024]},
}

@article{van2017weed,
  title={Weed Science Society of America National Weed Survey Dataset},
  author={Van Wychen, L},
  journal={Weed Science Society of America},
  year={2017}
}

@article{kotleba1994european,
  title={European and Mediterranean Plant Protection Organization (EPPO)},
  author={Kotleba, J},
  journal={Agrochemia (Slovak Republic)},
  volume={34},
  number={4},
  year={1994}
}

@article{borsch2020world,
  title={World Flora Online: Placing taxonomists at the heart of a definitive and comprehensive global resource on the world's plants},
  author={Borsch, Thomas and Berendsohn, Walter and Dalcin, Eduardo and Delmas, Ma{\"\i}t{\'e} and Demissew, Sebsebe and Elliott, Alan and Fritsch, Peter and Fuchs, Anne and Geltman, Dmitry and G{\"u}ner, Adil and others},
  journal={Taxon},
  volume={69},
  number={6},
  pages={1311--1341},
  year={2020},
  publisher={Wiley Online Library}
}

@misc{wssaCompositeList,
	author = {},
	title = {{C}omposite {L}ist of {W}eeds - {W}eed {S}cience {S}ociety of {A}merica --- wssa.net},
	howpublished = {\url{https://wssa.net/weed/composite-list-of-weeds/}},
	year = {},
	note = {[Accessed 19-08-2024]},
}
@article{ren2024exploring,
  title={Exploring the effect of region on diversity and composition of weed seedbanks in herbicide-resistant crop systems in the United States},
  author={Ren, Zhe and Gibson, David J and Gage, Karla L and Matthews, Joseph L and Owen, Micheal DK and Jordan, David L and Shaw, David R and Weller, Stephen C and Wilson, Robert G and Young, Bryan G},
  journal={Pest Management Science},
  volume={80},
  number={3},
  pages={1446--1453},
  year={2024},
  publisher={Wiley Online Library}
}