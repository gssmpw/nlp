%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EXPERIMENTATION}

\subsection{Experimental Prototypes}
%\hl{Follow format of Directional.PNG, Inward.PNG, none.png, and TGait.png.}
The baseline experimental prototype with zero microspine limbs, 0ML, is a three-limb MTA SoRo coming from a previous work \cite{freeman2024environmentcentriclearningapproachgait}. We compare the baseline against the addition of two different microspine configurations. The first SoRo equipped with a microspine array, 1ML, has them affixed to one limb that acts as the leader. The last SoRo, 2ML, contains microspine arrays equipped on two limbs with these acting as dual leaders. In all microspine configurations, the microspines face opposite the intended direction of movement. CAD models of the three prototypes are shown in \Fig \ref{fig:SoRos}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.32\linewidth}
         \centering
         \includegraphics[width=0.8\textwidth]{Figures/baseline.png}
         \caption{0ML}
         \label{fig:base}
     \end{subfigure}
     \begin{subfigure}{0.32\linewidth}
         \centering
         \includegraphics[width=0.8\textwidth]{Figures/one.png}
         \caption{1ML}
         \label{fig:one}
     \end{subfigure}
     \begin{subfigure}{0.32\linewidth}
         \centering
         \includegraphics[width=0.8\textwidth]{Figures/two.png}
         \caption{2ML}
         \label{fig:two}
     \end{subfigure}
    \caption{The three different microspine configurations explored in the experiments: a) a baseline aqua SoRo; b) a white SoRo with a total of 10 microspines configured in an array on one limb; c) a red SoRo with a total of 20 microspines configured in two arrays on two limbs.}
    \label{fig:SoRos}
\end{figure}


A push-pull translation gait, \Fig~\ref{fig:trans}, is used for the experiments. Here the gait sequence involves actuation of one limb followed by actuation of the two relaxed limbs. It is worth reminding the reader that this gait is not optimal for all four surfaces and three prototypes. In a previous work, translation and rotation gaits were discovered for four limb SoRos using an environment-centric framework \cite{freeman2024environmentcentriclearningapproachgait}. The same methodology was used to generate an optimal translation gait for the baseline three limb SoRo (0ML) on a rubber mat. This gait was used on all surfaces for 0ML, 1ML, and 2ML for a fair comparison, but it is expected that much better performance is possible with an optimized gait found for each prototype and surface combination.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\columnwidth]{Figures/TGaitV2.png}
    \caption{Translation gait: one limb (in maroon) actuates while the other two relax, then the previously relaxed limbs actuate while the other relaxes. This is an optimal, 1.1s long gait for the baseline SoRo on a rubber mat \cite{freeman2024environmentcentriclearningapproachgait}.}
    \label{fig:trans}
\end{figure}


\subsection{Experimental Setup}
%\hl{Follow format of setup.JPG.} 
The experiments are carried out with an overhead camera attached to a tripod next to the test area, shown in \Fig \ref{fig:exp}. Field tests are performed on four different surfaces. The starting pose, position and orientation, is same for each SoRo to ensure consistency. Tracking is performed with an AprilTag fixed to the end of each limb serving as fiducial markers. The average displacement per gait as well as overall displacement for each trial run is recorded.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/expV2.PNG}
    \caption{Example of experimental setup for outside field locations in a forest with a dirt patch. The overhead camera is orthogonal to the ground and approximately 4' above the surface. Experiments took place with the three prototypes discussed previously: 0ML, 1ML and 2ML.}
    \label{fig:exp}
\end{figure}


\subsection{Field Experiments}
%\hl{Follow format of yobot\_dir\_0\_black\_trans\_Test803\_imageU.png.}
The field experiments are tested on four rough surfaces around the University of Alabama campus. We look at uniform concrete, partially uniform brick, granular compact sand with pebbles, and a non-uniform forest floor containing leaf litter and large tree roots. %\hl{We show the average number and size of asperities on each surface as this is a critical parameter for the success of microspine gripping.} 
Tests are conducted using the push-pull translation gait shown in \Fig \ref{fig:trans}. Three trials (60 gaits per trial) are performed for a particular configuration and surface with three prototypes and four surfaces, resulting in a total of 36 trials. 

% -- HBezawada draft
% \\ Include why 3 tags were required: Occlusion: lighting or cutt off from camera's vision, replacing battery, accessing internal components

A 36h11 family AprilTag from the AprilTag visual fiducial system \cite{Olson_2011} is attached to each limb where they are readily visible while not hindering the movement. To compensate for occlusion and easy access to the hub, three tags were used to find the robot's position. Each limb of a robot has a different tag attached which represents a number from $0-8$. Before starting the tracking, the threshold parameters for different background environments are obtained. The tags are detected using an AprilTag detector library in Python. The tracking algorithm and data processing code is available at
\href{https://github.com/AgileRoboticsLab/SoftRobotics-Microspines}{github.com/AgileRoboticsLab/SoftRobotics-Microspines}.

% \begin{algorithm} 
%     \caption{Tracking using AprilTags}
%     \label{Alg - Marker Detection}
%     \KwData{Image from the camera}
%     \KwResult{AprilTag positions, traversed statistics}
%     Start the camera\;
%     \While{Camera is running, for each frame}{
%     %Change the image to HSV format\;
%     Image segmentation to detect AprilTags\;
%     Obtain the tags' center positions\;
%     Calculate the centroid of the three detected AprilTags $\rightarrow$ hub location\;
    
%     Calculate the Euclidean distance ($d_i$) traveled from the previous frame\;
%     }
%     Calculate the total distance ($D$)\;
%     Calculate the displacement traveled between starting ($i=0$) and final ($i=n$) positions \;
%     Calculate the rotation ($\theta$) between starting ($i=0$) and final ($i=n$) orientations
% \end{algorithm}
%At time $t$, let the AprilTag position on limb $i=1,2,3$ be defined as $\bm{m}_i(t)\in \Re^{2}$. The centroid position, relative displacement, rotation, and distance traveled are $\bm{c}(t), \bm{d}(t), \theta(t), D(t)$. AprilTag position from the centroid is $\displaystyle \bm{\widetilde{m}}_i=\bm{m}_i -\bm{c}(t)$ where $\bm{c}(t)=\frac{\sum_{i=1}^{3} \bm{m}_i}{3}$. Instantaneous rotation, relative displacement, and rotation are calculated as
% \begin{align}
%     \begin{gathered}
%     d_i(t) = ||\bm{c}(t)-\bm{c}(t-1)||, \qquad
%     % d_i = \sqrt{(x_{i-1}-x_{i})^2+(y_{i-1}-y_{i})^2}, \quad
%     D(t) = \sum_{j=1}^{t} d_j \\
%     \theta(t) = \arccos\left(\frac{\widetilde{\bm{m}}_k(0) \cdot \widetilde{\bm{m}}_k(t)}{{|\widetilde{\bm{m}}_k(0)| \cdot |\widetilde{\bm{m}}_k(t)|}}\right)        \qquad k=1,2,3
%     \end{gathered}
% \end{align}

% The rotation ($\theta$) is calculated based on this vector between start and final frame. %Distance and cumulative distance is calculated as shown in equation \ref{Eq: Euclidean Distance}. Displacement is the distance calculated between the centroid of the first and last frames. 
% Apart from rotation, which results in degrees, all other statistics are obtained in pixels and later scaled to centimeters. %An example of the tracking output with cumulative distance, displacement, and rotation is shown in \Fig~\ref{fig:track}.

% \begin{equation}
% \begin{rcases}
% \begin{aligned} \label{Eq: Euclidean Distance}
%     d_i &= \sqrt{(x_{i-1}-x_{i})^2+(y_{i-1}-y_{i})^2}\\
%     D &= \sum_{i=1}^{n} d_i \\
%     \theta &= \arccos\left(\frac{{V_{0} \cdot V_{n}}}{{|V_{0}| \cdot |V_{n}|}}\right)
% \end{aligned} 
% \qquad \text{\space}
% \end{rcases}
% \end{equation}
% %

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.99\linewidth]{Figures/yobot_dir_0_black_trans_Test803_image.png}
%     \caption{\hl{Replace with new tracking}}
%     \label{fig:track}
% \end{figure}



% \subsection{Experimental Setup}
% The experimental setup, shown in \Fig~\ref{fig:test} comprises of an 8'x4' platform with a camera fixed parallel to the surface. A Raspberry Pi 4 equipped with a camera tracks the robot in real time to record the positional data of each limb. The platform is constructed so that smooth whiteboard, rubber black mat, and porous wood modular surfaces are quickly swapped and tested without disturbing the camera's field of view. The reader may be reminded that, unlike in Sec. \ref{Sec:Microspine}, a porous wood surface is used for the experiments in place of carpet. When the microspine endcaps attached to the SoRo grip onto carpet, the increased friction is so high that the motor tendon can snap, making the surface unsuitable for testing.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\linewidth]{Figures/setup.JPG}
%     \caption{Experimental setup comprising of a modular platform equipped with camera that performs real-time tracking of the robot limb markers.}
%     \label{fig:test}
% \end{figure}

% \subsection{Microspine Configurations}
% Three microspine configurations are explored for testing - (1) spines facing in the direction of motion, (2) inwards towards the hub, and (3) no spines. The  configurations are illustrated in \Fig~\ref{fig:Spine_Config}.



% A push-pull translation gait, \Fig~\ref{fig:trans}, is used for the experiments. Here the gait sequence involves actuation of one limb followed by actuation of the two relaxed limbs. It is worth reminding the reader that this may not be optimal for all the three surfaces and spine configurations. In a previous work, translation and rotation gaits are discovered for four limb SoRos using an environment-centric framework \cite{mahendran_multi-gait_2023}. The same methodology is used to generate an optimal translation gait for the three limb SoRo with no spine configuration on a rubber mat. %It should be noted that this locomotion gait was found on a mat surface, and may not be optimal for translating on inclines.
% %All tests are repeated for both translation and rotation gaits.

% \begin{figure}
%     \centering    \includegraphics[width=0.75\columnwidth]{Figures/TGait.png}
%     % \begin{subfigure}{0.32\linewidth}
%     %      \centering
%     %      \includegraphics[width=\textwidth]{Figures/1limb.jpg}
%     %      \caption{Left}
%     %      \label{fig:limb1}
%     %  \end{subfigure}
%     %  \begin{subfigure}{0.32\linewidth}
%     %      \centering
%     %      \includegraphics[width=\textwidth]{Figures/2limb.jpg}
%     %      \caption{Top and Right}
%     %      \label{fig:limb2}
%     %  \end{subfigure}
%     \caption{Translation gait that involves actuation of one limb (in maroon) while other two are relaxed, followed by actuation of the previously relaxed limbs and relaxation of the previously actuated limb. This is an optimal gait for SoRo locomotion with no spines on a rubber mat, and has duration of $1.1\sec$.}
%     \label{fig:trans}
% \end{figure}

% %\begin{figure}
% %    \centering
% %    \begin{subfigure}{0.32\linewidth}
% %         \centering
% %         \includegraphics[width=\textwidth]{Figures/top1.jpg}
% %         \caption{Top}
% %         \label{fig:limb1}
% %     \end{subfigure}
% %     \begin{subfigure}{0.32\linewidth}
% %         \centering
% %         \includegraphics[width=\textwidth]{Figures/2left.jpg}
% %         \caption{Left and Top}
% %         \label{fig:limb2}
% %     \end{subfigure}
% %     \begin{subfigure}{0.32\linewidth}
% %         \centering
% %         \includegraphics[width=\textwidth]{Figures/None.jpg}
% %         \caption{None}
% %         \label{fig:blank}
% %     \end{subfigure}
% %    \caption{Rotation Gait}
% %    \label{fig:Spine_Config}
% %\end{figure}


% \subsection{Real-Time Pose Estimation} 
% Three markers of different colors are placed on each limb such that each of the colors are easily distinguished from the robot and background surfaces. Additionally, the markers are fixed so that they do not hinder the movement of limbs while easily being swapped out if necessary. 
% % -- HBezawada draft
% % \\
% The markers are detected using the OpenCV2 package and implemented in python on a Raspberry Pi. Before starting the experiment, the color threshold parameters for different colored markers based on different background surfaces are obtained. The real-time tracking algorithm is implemented as shown in Algorithm \ref{Alg - Marker Detection} and is available at 
% \hyperlink{https://github.com/AgileRoboticsLab/SoftRobotics-Microspines}{github.com/AgileRoboticsLab/SoftRobotics-Microspines}.

% \begin{algorithm} 
%     \caption{Real-Time Tracking}
%     \label{Alg - Marker Detection}
%     \KwData{Image from the camera}
%     \KwResult{Marker positions, traversed statistics}
%     Start the camera\;
%     \While{Camera is running, for each frame}{
%     Change the image to HSV format\;
%     Image segmentation to detect markers\;
%     Obtain the marker's center position\;
%     Calculate the centroid of the three detected markers $\rightarrow$ hub location\;
    
%     Calculate the Euclidean distance ($d_i$) traveled from the previous frame\;
%     }
%     Calculate the total distance ($D$), displacement traveled\;
%     Calculate the rotation ($\theta$) between starting ($i=0$) and final ($i=n$) orientation
% \end{algorithm}
% At time $t$, let the marker position on limb $i=1,2,3$ be defined as $\bm{m}_i(t)\in \Re^{2}$. The centroid position, relative displacement, rotation, and distance traveled are $\bm{c}(t), \bm{d}(t), \theta(t), D(t)$. Marker position from the centroid is $\displaystyle \bm{\widetilde{m}}_i=\bm{m}_i -\bm{c}(t)$ where $\bm{c}(t)=\frac{\sum_{i=1}^{3} \bm{m}_i}{3}$. Instantaneous rotation, relative displacement, and rotation are calculated as
% \begin{align}
%     \begin{gathered}
%     d_i(t) = ||\bm{c}(t)-\bm{c}(t-1)||, \qquad
%     % d_i = \sqrt{(x_{i-1}-x_{i})^2+(y_{i-1}-y_{i})^2}, \quad
%     D(t) = \sum_{j=1}^{t} d_j \\
%     \theta(t) = \arccos\left(\frac{\widetilde{\bm{m}}_k(0) \cdot \widetilde{\bm{m}}_k(t)}{{|\widetilde{\bm{m}}_k(0)| \cdot |\widetilde{\bm{m}}_k(t)|}}\right)        \qquad k=1,2,3
%     \end{gathered}
% \end{align}

% The rotation ($\theta$) is calculated based on this vector between start and final frame. %Distance and cumulative distance is calculated as shown in equation \ref{Eq: Euclidean Distance}. Displacement is the distance calculated between the centroid of the first and last frames. 
% Apart from rotation, which results in degrees, all other statistics are obtained in pixels and later scaled to inches. An example of the real-time tracking output with cumulative distance, displacement, and rotation is shown in \Fig~\ref{fig:track}.

% % \begin{equation}
% % \begin{rcases}
% % \begin{aligned} \label{Eq: Euclidean Distance}
% %     d_i &= \sqrt{(x_{i-1}-x_{i})^2+(y_{i-1}-y_{i})^2}\\
% %     D &= \sum_{i=1}^{n} d_i \\
% %     \theta &= \arccos\left(\frac{{V_{0} \cdot V_{n}}}{{|V_{0}| \cdot |V_{n}|}}\right)
% % \end{aligned} 
% % \qquad \text{\space}
% % \end{rcases}
% % \end{equation}
% % %

% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\linewidth]{Figures/yobot_dir_0_black_trans_Test803_imageU.png}
%     \caption{Real-time tracking where distance, rotation, and displacement are relative to the previous time frame. The cumulative quantities are measured from time $t=0$.}
%     \label{fig:track}
% \end{figure}