% !TEX program = pdflatex
\documentclass{article} % For LaTeX2e

\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{0142}{\l}
\DeclareUnicodeCharacter{0119}{\k{e}}
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{graphicx}

\newcommand{\squishlist}{
   \begin{list}{$\bullet$}{%
        \setlength{\itemsep}{0pt}%
        \setlength{\parsep}{0pt}%
        \setlength{\topsep}{0pt}%
        \setlength{\partopsep}{0pt}%
        \setlength{\listparindent}{-2pt}%
        \setlength{\itemindent}{-5pt}%
        \setlength{\leftmargin}{1.2em}%
        \setlength{\labelwidth}{0em}%
        \setlength{\labelsep}{0.5em}%
    }
}

\newcommand{\squishend}{
    \end{list}  }




% \title{Formatting Instructions for ICLR 2025 \\ Conference Submissions}
% \title{Share Adjacent Previous Layers: Accelerating Language Model Inference by Reducing Memory Load}
\title{\texttt{SHARP}: Accelerating Language Model Inference by \underline{SH}aring \underline{A}djacent layers with \underline{R}ecovery \underline{P}arameters}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% Yiping Wang, Hanxian Huang, Yifang Chen, Jishen Zhao, Simon Shaolei Du, Yuandong Tian
\author{Yiping Wang \\
% \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
University of Washington\\
\texttt{ypwang61@cs.washington.edu}\\
\And
Hanxian Huang \\
University of California San Diego\\
\texttt{hah008@ucsd.edu}\\
\And
Yifang Chen \\
University of Washington\\
\texttt{yifangc@cs.washington.edu}\\
\And
Jishen Zhao \\
University of California San Diego\\
\texttt{jzhao@ucsd.edu}\\
\And
Simon Shaolei Du \\
University of Washington\\
\texttt{ssdu@cs.washington.edu}\\
\And
Yuandong Tian \\
% Meta AI (FAIR)\\
Meta\\
\texttt{yuandong@meta.com}\\
}

% \author{%
%   % David S.~Hippocampus\thanks{Use footnote for providing further information
%   %   about author (webpage, alternative address)---\emph{not} for acknowledging
%   %   funding agencies.} \\
%   % Department of Computer Science\\
%   % Cranberry-Lemon University\\
%   % Pittsburgh, PA 15213 \\
%   % \texttt{hippo@cs.cranberry-lemon.edu} \\
%   Yiping Wang\\
%   University of Washington\\
%   \And
%   Yifang Chen$^*$\\
%   University of Washington\\
%   \And
%   Wendan Yan\\
%   University of Washington\\
%   \And
%   Alex Fang\\
%   University of Washington\\
%   \And
%   Wenjing Zhou\\
%   University of Michigan\\
%   \And
%   Kevin Jamieson\\
%   University of Washington\\
%   \And
%   Simon Shaolei Du\\
%   University of Washington\\
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \newcommand{\yiping}[1]{\textcolor{blue}{[Yiping: #1]}}
\newcommand{\yiping}[1]{}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks, but their high computational and memory demands during inference present significant challenges, especially for deployment on resource-constrained mobile devices. In this paper, we propose SHARP (Sharing Adjacent Layers with Recovery Parameters), an approach insighted by MobileLLM~\citep{liu2024mobilellm}, to accelerate LLM inference by sharing parameters across adjacent layers while introducing a set of low-rank learnable recovery parameters to maintain performance while decreasing the communication time in memory. 
% We show that we are capable of using the parameters from one layer to predict the next or more layers by utilizing a two-stage recovering process: Single Layer Warmup (SLW) and Supervised Fine-Tuning (SFT). 
% We evaluate SHARP on multiple in-distribution tasks, demonstrating that it can achieve comparable perplexity to the original model while reducing the number of stored MLP parameters to 25\%-56\% of the original. We believe that SHARP is a insightful solution for reducing inference costs in LLMs without requiring pretraining-scale data.
% % \yiping{Working on this} 

While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging.
In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance.
Inspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT).
The SLW stage aligns the outputs of the shared layers using  $\mathcal{L}_2$ loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38\% to 65\%.
We also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched.
Furthermore, SHARP saves 42.8\% in model storage and reduces the total inference time by 42.2\% compared to the original Llama2-7b model on mobile devices.
Our results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources.
  
\end{abstract}
\vspace{-1em}
\section{Introduction}
\vspace{-0.5em}
\iffalse
Following the power of scaling laws, large language models (LLMs) have become one of the central topics in Natural Language Processing (NLP)~\citep{brown2020language,zhang2022opt,hoffmann2022training,bubeck2023sparks,chowdhery2023palm,bai2023qwen,team2023gemini,touvron2023llama}.
% and show very impressive capabilities in multiple aspects, including dialog summarization, knowledge memorization, and mathematical reasoning. \yiping{cite}
However, this field also faces several technical problems.
First, 
large models have high computational and memory requirements during inference~\citep{aminabadi2022deepspeed,pope2023efficiently,kim2023full}, which limits their speed and restricts them from being used in scenarios like mobile devices~\citep{liu2024mobilellm}.
Besides, the training of very large models always requires tons of computational resources, which are unaffordable for most academic researchers and may be environmentally unfriendly~\citep{luccioni2023estimating,rillig2023risks}.
% \yiping{cite}
% as it brings tons of energy consumption and carbon dioxide emissions. \yiping{cite} 
Furthermore, although people have tried to understand the internal mechanisms of the basic components of language models~\citep{tian2023scan,von2023transformers,tarzanagh2023max,tian2023joma}, the training dynamics of whole LLMs remain a black box, and the most efficient and effective way to train them is still elusive.
% What's more, 
\fi

% Following the power of scaling laws, large language models (LLMs) have become one of the central topics in Natural Language Processing (NLP)~\citep{brown2020language,zhang2022opt,hoffmann2022training,bubeck2023sparks,chowdhery2023palm,bai2023qwen,team2023gemini,touvron2023llama}.
% However, deploying a pretrained large language model has high computational and memory requirements~\citep{aminabadi2022deepspeed,pope2023efficiently,kim2023full,zhang2024h2o}.
% % For instance, a 30 billion-parameter model
% For instance, a 70 billion-parameter language model stored in FP16 precision will require about 148GB of memory to hold the model weights, which needs 2 A100 GPUs with 80GB of memory to just load the whole model. During inference, the whole input sequence and the KV cache are also stored in the GPU, which requires further memory cost. 
% Although people have tried techniques like layer-wise inference~\citep{huggingface_safetensors}, which loads the model layer by layer and calculates activations for each layer, to enable LLM inference on a single GPU, this brings additional inference latency due to the frequent memory loading or disk reading.
% % , people always use layer-wise inference,
% % with model file sharding,
%  % However, this will waste a lot of memory for loading and disk reading and thus slow the inference. 
%  \yiping{Double check}
% Especially, these concerns are more significant for the deployment of mobile devices, which always have smaller DRAM (like about 6GB for iPhone 15) and higher communication overhead~\citep{liu2024mobilellm}.


Following the principles of scaling laws, large language models (LLMs) have become one of the central topics in Natural Language Processing (NLP)~\citep{brown2020language, zhang2022opt, hoffmann2022training, bubeck2023sparks, chowdhery2023palm, bai2023qwen, team2023gemini, touvron2023llama}.
However, deploying a pre-trained large language model requires significant computational and memory resources~\citep{aminabadi2022deepspeed, pope2023efficiently, kim2023full, zhang2024h2o}, which may further restrict their inference speed.
For instance, a 70-billion-parameter language model stored in FP16 precision requires approximately 148GB of memory to hold the model weights, necessitating two A100 GPUs with 80GB of memory each to load the entire model. During inference, the entire input sequence and the KV cache are also stored on the GPU, incurring additional memory usage.
Although techniques like layer-wise inference~\citep{huggingface_safetensors}, which load the model to GPU layer by layer,
% and compute activations for each layer, 
enable LLM inference on a single GPU, they introduce additional inference latency due to frequent memory loading or disk reading.
In particular, these concerns are significant for deployment on mobile devices, which typically have smaller DRAM (e.g., around 6GB in the iPhone 15) and higher communication overhead~\citep{liu2024mobilellm}.


% techniques including layer
% Moreover, there are growing

% , so we just need more memory for deploying the model. 


% However, this field also faces several technical problems.
% First, 
% large models have high computational and memory requirements during inference, which limits their speed and restricts them from being used in scenarios like mobile devices~\citep{liu2024mobilellm}.

% Besides, the training of very large models always requires tons of computational resources, which are unaffordable for most academic researchers and may be environmentally unfriendly~\citep{luccioni2023estimating,rillig2023risks}.
% % \yiping{cite}
% % as it brings tons of energy consumption and carbon dioxide emissions. \yiping{cite} 
% Furthermore, although people have tried to understand the internal mechanisms of the basic components of language models~\citep{tian2023scan,von2023transformers,tarzanagh2023max,tian2023joma}, the training dynamics of whole LLMs remain a black box, and the most efficient and effective way to train them is still elusive.

% To alleviate these issues in model deployment, several methods have been carefully investigated.
% % One direction is to optimize the storage of the model, including sparse attention approximation
% One direction is to optimize the calculation process of the attention mechanism and the storage of the KV cache, 
% including Reformer~\citep{kitaev2020reformer}, Flash Attention~\citep{dao2022flashattention,dao2023flashattention,shah2024flashattention}, and H$_2$O~\cite{zhang2024h2o}.
% Another way is to compress the existing model, which including quantization~\citep{dettmers2022gpt3,liu2023llm,kang2024gear}, pruning~\citep{ma2023llm,sun2023simple} and sparsification~\citep{frantar2023sparsegpt,dong2024blockwise,mirzadeh2023relu,song2024turbo}. There are also other methods trying to accelerate inference through optimizing decoding algorithms, like speculative decoding~\cite{kim2024speculative}. Besides, it's worth mentioning that some research also tries to directly train small language models (SLM) ~\citep{black2022gpt,zhang2022opt,timiryasov2023baby,dey2023cerebras,biderman2023pythia,gunasekar2023textbooks,hu2024minicpm} rather than compressing the existing large language models.

To alleviate these issues, several methods have been carefully explored.
% One direction is to optimize the storage of the model, including sparse attention approximation
One direction is to optimize the calculation process of the attention mechanism and the storage of the KV cache, including Reformer~\citep{kitaev2020reformer}, Flash Attention~\citep{dao2022flashattention, dao2023flashattention, shah2024flashattention}, H$_2$O~\citep{zhang2024h2o}, and so on.
Another main direction is to compress the existing model while retaining model performance, including quantization~\citep{dettmers2022gpt3, liu2023llm, kang2024gear}, pruning~\citep{ma2023llm, sun2023simple}, and sparsification~\citep{frantar2023sparsegpt, dong2024blockwise, mirzadeh2023relu, song2024turbo}.
There are also other methods that try to accelerate inference by optimizing decoding algorithms, such as speculative decoding~\citep{kim2024speculative}.
Additionally, it's worth mentioning some other research also tries to directly train small language models (SLMs)~\citep{black2022gpt, zhang2022opt, timiryasov2023baby, dey2023cerebras, biderman2023pythia, gunasekar2023textbooks, hu2024minicpm} rather than compressing existing large language models. However, this always requires pretraining-scale resources, which cost more than the methods that only use post-training-scale data for recovery, such as sparsification.






% One class of approaches is to directly improve the inference efficiency of current top LLMs, including , and speculative decoding~\citep{kim2024speculative}. Another technical way is to train small language models (SLMs) instead, which shrink the number of parameters but still maintain potent performance by using higher-quality data and more fine-grained training settings~\citep{black2022gpt,zhang2022opt,timiryasov2023baby,dey2023cerebras,biderman2023pythia,gunasekar2023textbooks,hu2024minicpm}. By combining these methods, top-quality language models can be deployed on consumer-level mobile devices, which is a an important platform for future research and application of language models~\citep{liu2024mobilellm}.

% In this paper, we focus on a new methodology for efficient inference on the current pretrained model, named the \textit{adjacent layer-sharing strategy}.
% It's partially inspired by the observation from Deja Vu~\citep{liu2023deja}: in Figure 5 (a) and (b) from their paper, they show that the cosine similarity between representations at two consecutive layers or even a few layers apart can be very high (like greater than 95\%), 
% which implies that the model output between layers may be similar, and the embedding is changing slowly.
% This enlightens people to save inference time by sharing the parameters between layers to reduce the communication overhead. 
% A related algorithm, ``immediate block-wise weight sharing'' strategy proposed recently by MobileLLM~\citep{liu2024mobilellm}, also supports this idea. They try to share the weights between two adjacent layers to avoid frequent parameter movement in memory (Figure~\ref{fig:intro} (b)) and then train a new small language model. 
% However, although the existing methods from MobileLLM achieve significant improvement in accelerating the model inference on mobile devices, they just focus on training a new model from scratch 
% and do not fit our purpose of deploying the pretrained models only through the more resource-saving post-training process.

In this paper, we focus on a new methodology for efficient inference on current pretrained models, named the \textit{adjacent layer-sharing strategy}. It is partially inspired by the observation from Deja Vu~\citep{liu2023deja}: in Figure 5(a) and (b) of their paper, they show that the cosine similarity between representations at two consecutive layers, or even a few layers apart, can be very high (greater than 95\%), which implies that the model outputs between layers may be similar. This suggests that we can save inference time by sharing parameters between layers to reduce communication overhead. A related algorithm, the ``immediate block-wise weight sharing'' strategy proposed recently by MobileLLM~\citep{liu2024mobilellm}, also supports this idea. They share the weights between two adjacent layers to avoid frequent parameter movement in memory (Figure~\ref{fig:intro}(b)) and then train a new small language model. 
% This accelerates the model inference
Note that for mobile devices, the communication overhead (i.e. cost for loading the model weights from low-speed, high-capacity memory to high-speed, low-capacity computation caches) accounts for a major proportion of the latency overhead, therefore, they double the depth of the new model and obtain better downstream performance, but only increase a negligible additional inference time.
However, although MobileLLM achieves significant improvements in accelerating model inference on mobile devices, they focus only on training a new model from scratch and do not fit our purpose of deploying pretrained models through a more resource-saving post-training process.


% rather than deploying the existing pretrained model.
% and train a new small language model.

% Note that for mobile devices, the communication overhead in memory accounts for a considerable proportion of the latency overhead, they double the depth of the new model and obtain better downstream performance, while only increasing a negligible additional inference time.


% through the post-training process.

% layer-sharing strategy


% Notably, among these methods, MobileLLM~\citep{liu2024mobilellm} proposes a new method named
% immediate block-wise weight sharing for accelerating inference, 
% % layer sharing,
% where they share the weights between two adjacent layers to avoid frequent parameter movement in memory (As shown in Figure~\ref{fig:intro} (b)) and train a new small language model. 
% Note that for mobile devices, the communication overhead in memory accounts for a considerable proportion of the latency overhead, they double the depth of the new model and obtain better downstream performance, while only increasing a negligible additional inference time.
% This significantly reduces the communication overhead, which accounts for a considerable proportion of the latency overhead in mobile devices. 


\begin{figure}[t]
    \centering
    \vspace{-2.0em}
    \includegraphics[width=0.9\linewidth]{fig/p1_1.pdf}
    \vspace{-0.5em}
    \caption{
    (a) Regular pretrained baseline model without layer sharing.
    (b) Adjacent layer sharing used in MobileLLM~\citep{liu2024mobilellm}. They repeat the layer twice and train the model from scratch.
    (c) Direct Sharing: directly apply vanilla adjacent layer sharing to the pretrained model to accelerate inference.
    (d) \textbf{(Ours) SHARP}: SHaring Adjacent Layers with Recovery Parameters. SHARP leverages fine-tuning-scale data to train additional parameters $\Delta \Theta$, which consist of far fewer parameters than the original $\Theta$, in order to recover the model's performance. In this paper, we explore several candidate transformations, including the LoRA-style function, to apply on additional parameters.
        % We try several candidate transformations including LoRA here.
    }
    \label{fig:intro}
    \vspace{-1.8em}
\end{figure}

% Although their method requires training the model from scratch, which does not fit the purpose of deploying the existing pretrained model, it naturally inspires us to think a question: 
% \textit{is it possible to utilize a similar layer-sharing strategy for efficient inference only through the post-training process? }

% To apply the layer-sharing strategy to the existing LLM, 
% a simple baseline is directly using the current layer to replace the next layer as shown in Figure~\ref{fig:intro} (c). However, this always results in obvious performance degradation.
\textbf{Our Contributions.} To apply the layer-sharing strategy to existing LLMs, in this paper, we propose a new layer-sharing algorithm named \textit{SHARP (SHaring Adjacent layers with Recovery Parameters)}, which uses additional low-rank weights to predict subsequent layers, and thus save the memory load overhead of the predicted layers. We summarize our contributions as follows.

\squishlist
    \item {First}, we show that language models are robust to the replacement of adjacent or even later MLP layers, which further supports the insight of the layer-sharing strategy. Further, we find that the current layer can be a good approximation of later layers if we add some additional LoRA~\citep{hu2021lora} parameters and fine-tune them on a high-quality dataset (Figure~\ref{fig:side-by-side}).
    % Using this approach, we can almost recover the perplexity on the same dataset compared with the original model . 
    We also note that although the outputs of the adjacent layers are similar, their parameters differ a lot.
    % don't infer the similarity of their parameters exhibit similar behaviors, their parameters differ significantly
    % This implies that we should find weights in parameter space to approximate the outputs of adjacent layers, rather than directly approximate their differing parameters 
    (Section~\ref{sec:insight}).
    \item Second, based on these observations, we propose our SHARP algorithm (Figure~\ref{fig:intro} (d)). Given the current layer, we use candidate transformations, such as LoRA addition, to predict the parameters of the next several layers using recovery parameters, which reduces memory load overhead and thus accelerates inference. SHARP consists of two stages: the Single Layer Warmup (SLW) stage and the Supervised Fine-Tuning (SFT) stage, which are used to tune the additional parameters. During SLW, we minimize the $\mathcal{L}_2$ loss between the output of the original replaced layer and the predicted layers, providing a good initialization for the SFT stage. While SFT is critical for recovering model performance, SLW plays an essential role in allowing one layer to aggressively predict multiple layers. With SLW, we can recover model perplexity effectively, even when dropping 3/4 of the original MLP layers  (Section~\ref{sec: algorithm}, ~\ref{sec: exp in distribution}).
    \item Third, we conduct detailed ablation studies on SHARP. Specifically, we investigate how to achieve more efficient recovery by determining which layers to replace and selecting the best candidate functions. We find that the earlier layers (layers 5 to 15) are crucial for model performance, and thus, to preserve the model's capacity, more layers should be replaced toward the latter parts of the model. Additionally, we explore various candidate functions, including the vanilla LoRA addition function and other parameterizations, such as left, right, and dot multiplication of the original weights. Interestingly, we find that these different parameterizations perform similarly when their total number of parameters is the same (Sections~\ref{sec:candidate types},~\ref{sec: exp ablation}).
    \item Lastly, we perform several experiments to demonstrate the advantages of our SHARP algorithm. In in-distribution tasks, we recover model perplexity across various tasks, such as Arxiv-math~\citep{kenny2023arxivmath} and Dialogsum~\cite{chen-etal-2021-dialogsum}, using no more than 50k fine-tuning examples, while saving 38\%-65\% of the MLP layer parameters. We also show that SHARP performs better on memorization-related downstream tasks, and we discuss how knowledge from different types of downstream tasks is stored in different layers. More importantly, evaluation results on mobile devices demonstrate that SHARP saves \textbf{42.8\%} in model storage (and loading) and reduces total run time by \textbf{42.2\%} compared to the original Llama2-7b (Section~\ref{sec: exp in distribution},~\ref{sec: exp downstream},~\ref{sec: exp latency})
\squishend

% \textbf{Our Contributions.}

\iffalse
we first show that the current layer can be a good approximation of the later layers if we add some additional parameters and finetuning on it (Section~\ref{sec:insight}). And then, this further inspires our proposed algorithm \textit{SHARP: SHaring Adjacent layers with Recovery Parameters} 
We further carefully investigate for a more efficient recovery, which layers should be replaced, and how to choose the best candidate functions (Section~\ref{sec:candidate types}, ~\ref{sec: exp ablation}).
% And we further investigate how to choose the layers more effi


% to enable even using one layer to predict more layers (Section~\ref{sec: method}).



To further explore this direction and enhance better algorithm,  we need to have a better understanding of the layer-sharing algorithm and answer some related questions left by MobileLLM.
% about the layer-sharing strategy. 
For example, 
% However, there are still several weaknesses in their research on this technology.
% First of all, 
they don't fully investigate why sharing parameters in adjacent layers can perform well, and which layers should be replaced.\footnote{They just empirically compare the effects of different ways to repeat the layers (like immediate repeat as Figure~\ref{fig:intro} (b) or other ways including reverse repeat and repeat-all-over.}.
% On the other hand, their method is only designed for pretraining a small language model (like 125M-350M) that can be deployed to mobile devices from scratch. 
% and also did not investigate whether it can be applied to the finetuning stage, which is unavailable to the pretraining-scale dataset.
% Therefore, it's unclear whether this technique can be directly applied to the current top language models without further optimization (as Figure~\ref{fig:intro} (c)) or be used without pretraining-scale recovery data.
And they just try to use one layer to predict the next layer, which just obtains about 50\% compression rate, while they haven't explored how aggressively people can decrease the compression ratios.

% , i.e., use one layer to predict more later layers.

\yiping{Working on this...}

% if we can use one layer to predict more layers
% they just double all the blocks twice and haven't studied which layer should be repeated, and how aggressively they can repeat the layers.

In this paper, based on the vanilla layer-sharing strategy proposed in MobileLLM, we make the following contributions to obtain a deeper understanding of the weight-sharing strategy:


\yiping{Need to rewrite the contribution part later}
\begin{itemize} 
\item We show that pretrained language models are robust to the replacement of adjacent MLP layers, and 
simply adding a much smaller amount of additional parameters than the original can recover the model performance in in-distribution tasks. This phenomenon can serve as an insight for layer-sharing methods used in \cite{liu2024mobilellm}.
\item For applying layer-sharing methods to the current top pretrained model, 
we develop a two-stage recovery strategy for scenarios involving multiple-layer sharing. We demonstrate that with this strategy, we can nearly recover the model's perplexity even when replacing 44\%-84\% MLP layers for in-distribution tasks.
\item We further analyze the influence of replacing layers at different locations or using different reparameterization methods. We also show that the modified weight-sharing strategy performs better in recovering performance on memorization-related downstream tasks. 
\end{itemize}
% We systematically investigate applying the weight-sharing strategy to the pretrained model
\fi




% In this field, a widespread concern is how to enhance the inference efficiency of LLM
% However, LLM has high computational and memory requirements during the inference stage, which challenge their inference efficiency and application scenarios.
% % , which limits their application prospects in mobile-device-level scenarios.
% To solve this problem, several lines of work have been investigated, including 
% quantization, pruning, sparsification, and speculative decoding\yiping{cite}.

% There are also several problems related to the large-scale models.
% Nevertheless, 


% What's more, the training process may be environment-unfriendly since it brings tons of energy consumption and carbon dioxide emissions. 

% Considering these concerns, there is another line of work devolving into the small language model (SLM), which shrinks the number of parameters but still keeps potent performance\yiping{cite phi, tinyllama, mobilellm, gemma, minicpm}.

% On the other hand

% On the other hand, rather than continuing to increase the model size over the trillions level like GPT-4 \yiping{cite}, a line of works has devolved into shrinking the number of parameters



% LLM development, inference efficiency. Different techniques

% Mobile device important, mobilellm several methods to accelerate. Not fully analyze the case of weight sharing methods

% Explore the potential of weight sharing, and achieve XXX results on in-distribution tasks.





% \yiping{TODO, can move to appendix if not finished.}

% llama models


\begin{figure}[t]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.48\linewidth]{fig/ppl_arxiv-math__alpaca-gpt4__dialogsum__databricks-dolly-15k.pdf}\includegraphics[width=0.48\linewidth]{fig/full_ppl_arxiv-math.pdf}
    \hfill
    \caption{
    % Directly replacing the adjacent MLP layers in the language model will not obviously increase the model's perplexity. 
    Language models are robust to the replacement of adjacent MLP layers.
    \textbf{(Left)} For each reference layer, we directly replace the MLP layer in the subsequent layer with that of the reference layer, then evaluate perplexity on various tasks. We find that, aside from the first and last layers, most replacements do not significantly increase perplexity compared to the original model (dotted line). If we fine-tune the model with additional low-rank learnable parameters (rank = 400) added to the next layer, the perplexity gap is effectively closed (as shown by the "Arxiv-math (Finetuned)" line).
    \textbf{(Right)} Similarly, we observe consistent perplexity results on Arxiv-math (baseline perplexity = 3.0) when using more general reference-target replacement pairs (i.e., use reference layer to replace any later layer).
    % \yiping{fix the text in left figure, and increase the text size in the right figure, index from 1}
    }
    \label{fig:side-by-side}
     \vspace{-1em}
\end{figure}

\section{Main Methods}
\vspace{-0.5em}
\subsection{Insight: The Current Layer Can be A Good Approximation of the Next Layer with Recovery Parameters}\label{sec:insight}

% An important technique proposed in MobileLLM~\citep{liu2024mobilellm} is the adjacent layer sharing strategy, which they called ``immediate block-wise sharing''. Their method is as shown in Figure~\ref{fig:intro} (b). It directly repeat the layer twice, and then train the small model (125-350M parameters) from scratch. This method just need the store the same size of parameters as the original model, but can obtain better results. 
% A natural question arises, does the success of their adjacent layer sharing strategy comes from the fact that adjacent layers have similar parameters or behaviors? Furthermore, can we apply it to larger pretrained language model, like Llama2-7B~\citep{touvron2023llama}, to accelerate the model inference (like Figure~\ref{fig:intro} (a) and (c))? 






% \yiping{May change this first paragraph}
% One of the important techniques proposed in MobileLLM~\citep{liu2024mobilellm} is the adjacent layer-sharing strategy
% % which they call "immediate block-wise sharing." 
% (Figure~\ref{fig:intro} (b)). 
% which trains small language models from scratch with shared consecutive layers. 
% Note that for mobile devices, the communication overhead in memory accounts for a major proportion of the latency overhead, therefore, they double the depth of the new model and obtain better downstream performance, but only increase a negligible additional inference time.
% % This method greatly reduce the memory
% % only needs to store the same amount of parameters as the original model and thus 
% % but can obtain $\sim1\%$ gains on downstream evaluations.
% Some questions naturally arise: 

As illustrated in the introduction parts, the adjacent layer-sharing strategy (Figure~\ref{fig:intro} (b)) proposed by MobileLLM accelerates the inference of a newly-trained small language model (125-350M) by reusing the previous layers and reducing the communication overhead in memory. However, several questions about layer-sharing strategies have yet to be revealed: does the success of this adjacent layer-sharing strategy come from the fact that adjacent layers have similar parameters or behaviors? 
Can we extend this method to a pretrained larger model like Llama2-7b~\citep{touvron2023llama}, to accelerate model inference (as shown in Figure~\ref{fig:intro} (a) and (c))? 
Further, can we even use one layer to predict more layers and thus further accelerate inference?


To answer these questions, we try to directly replace the MLP layer in the next layer with the current reference layer in Llama2-7b model, and then evaluate its perplexity on some evaluation tasks including Arxiv-math~\citep{kenny2023arxivmath}, GPT4-Alpaca~\citep{peng2023alpacagpt4}, Databricks-Dolly-15k~\citep{DatabricksBlog2023DollyV2} and Dialogsum~\citep{chen-etal-2021-dialogsum}. The result is shown in Figure~\ref{fig:side-by-side} (Left). We found that except for the first and last layer,
% using the first layer to replace the second one or using the second last layer to replace the last one, 
most replacements don't increase the model perplexity significantly (solid line) compared to the original model (dash line). 



More surprisingly, for Arxiv-math, we find that if we add some additional learnable parameters\footnote{Here $g$ is the LoRA addition, i.e., $g(\Theta, (\alpha, U, V)) = \alpha\Theta + U\cdot V$, where $\alpha \in \R, \Theta \in \R^{d_1\times d_2}, U \in \R^{d_1\times r}, V \in \R^{r \times d_2}$. And here we use rank=400, which is much less than 4096, the rank of original parameters.} to the replaced layer like Figure~\ref{fig:intro} (d) and use 50k instruction data from Arxiv-math to finetune them, we can recover the perplexity gap (as the ``Arxiv-math (recovered)'' line). Note that the Llama2 models use 2T tokens for pretraining, we claim that these results support \textit{it's possible to apply \texttt{SHARP} in a larger language model without pretraining-level resources.}
Besides in Figure~\ref{fig:side-by-side} (Right), we also try more general reference-target replacement pairs, i.e., use one reference layer to predict the later layers, and we obtain similar results on the small perplexity gap.
% ) also shows similar conclusion for , 
This implies that 
\textit{it's possible to use one reference layer to predict more layers}.


\begin{wrapfigure}{r}{0.27\textwidth}
\vspace{-2em}
\centering
\includegraphics[width=0.27\textwidth]{fig/average_L2_diff_and_norms.pdf}
\vspace{-1.5em}
\caption{
    Average relative error between adjacent layers (mean of $\{\|\Theta_{i+1} - \Theta_i\| / \|\Theta_i\|\}_{i=1}^{31}$).
    % are all about 142\% for gate, up and down projections.
    % Here we consider both the gate, up, and down projections in the MLP layers. 
    % \yiping{Use deeper color and text}
}
\label{fig:model_norm}
\vspace{-4em}
\end{wrapfigure}

What's more, it's also worth noting that even though layers with recovery parameters can be good approximations of each other, the parameters themselves are quite different. This is supported by evaluating the average relative error between adjacent layers as shown in Figure~\ref{fig:model_norm}.
% The comparison of the norms of the difference between layers and layers themselves is shown in Figure~\ref{fig:model_norm}. 
% We further calculate the average relative error between adjacent layers, namely the average 
 % of $\|\Theta_{i+1}^{\text{Down}} - \Theta_i^{\text{Down}}\| / \|\Theta_i^{\text{Down}}\|$ across all $i \in [N-1]$. Interestingly, we find that the average relative error for gate, up, and down projections are all about 142\%, which means that the parameters in adjacent layers are not directly similar.
% Figure~\ref{fig:model_norm} shows that the norms of the differences between layers are even larger than the norms of the layers themselves. 
This phenomenon implies that to predict the next layer, we should find weights in parameter space to approximate the outputs of adjacent layers, rather than directly approximate their differing parameters.
% \yiping{Add the row-column exchange result in appendix and cite here.}



% And this also implies that we can not 
% This observation denotes that 

% To accelerate the model inference 


% MobileLLM shows that we can repeat some blocks when training the sub-billion parameter language models from scratch. However, this method can just be used for the pretraining stage since it cannot reuse the parameters from the prepared models. We show that it's possible to reuse the previous layers while not requiring the pretraining-level resources, by simply adding the LoRA components on the previous layers.



% \begin{wrapfigure}{r}{0.45\textwidth}
% \vspace{-1em}
% \centering
% \includegraphics[width=0.45\textwidth]{ICLR 2025 Template/fig/ppl_arxiv-math__databricks-dolly-15k__alpaca-gpt4.pdf}
% \vspace{-1.2em}
% \caption{Comparison of negCLIPLoss and CLIPScore across different downsampling ratios on DataComp-medium. 
% % \yiping{TODO: change name}
% }
% \label{fig:CLIPLoss_vs_CLIPScore}
% \vspace{-1.5em}
% \end{wrapfigure}


% \yiping{Add pictures here to show the ppl recovery.}
% By replacing some layers in the model, we can recover the perplexity quite well for some in-distribution tasks (arxiv-math, alpaca-gpt4, etc). For example, replace about 50\% MLP layers and use LoRA recovery, the perplexity increases just from 3.01 to 3.18.

% \yiping{Key idea}: It's not a good idea to direct approximate the parameters in the adjacent layers, but we can find that their output space is quite similar, and can be found by using LoRA.




\vspace{-0.2em}
\subsection{Our Methods}\label{sec: method}
\vspace{-0.2em}
% First, L2 loss is used for the warmup of each MLP layer, and then SFT is used to recover.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{fig/full_ppl_arxiv-math.pdf}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{fig/ppl_arxiv-math__databricks-dolly-15k__alpaca-gpt4.pdf}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[h]
% \begin{center}
% \fbox{\includegraphics[width=0.5\linewidth]{pplreplace1.pdf}}
% \end{center}
% \caption{Your caption here.}
% \label{fig:enter-label}
% \end{figure}

In this section, we illustrate how we find the proper additional parameters for predicting each layer in SHARP. First, we introduce the notations and the settings used in our paper.


% Combining the observations in Section~\ref{sec:insight}, we design a two-stage strategy for SHARP 
% as follows.
% Firstly, we denote the preliminary used in this paper.

\vspace{-0.2em}
\subsubsection{Preliminary}\label{sec: preliminary}
\vspace{-0.2em}
We assume the original parameters of a particular function (for example, the gate projection of MLP layers) at $i$-th layer as $\Theta_i \in \R^{d_1\times d_2}$ ($i \in [N]$), where $d_1$ and $d_2$ are the input/output dimension of the function, and we freeze their gradients during training. 
% We use $A, B, E, F, U, V$ to denote the low-rank learnbale parameters
We denote the low-rank learnable LoRA parameters at $i$-th layer as $A_i \in \R^{d_1 \times r}, B_i \in \R^{r\times d_2}$. 
And we let $f(\cdot \ ; \Theta_i)$ denote the particular function that utilizes $\Theta_i$ as parameters.
In this paper, we focus on reducing the parameters in MLP layers, which take up the main parts of the parameters in the intermediate layers. And we use Llama2-7b~\citep{touvron2023llama} as our basic model.
% And we use $\Theta^{\text{MLP}}_i$ to represent $(\Theta^{\text{GATE}}_i, \Theta^{\text{UP}}_i, \Theta^{\text{DOWN}}_i)$.

To accurately illustrate the replacement, we denote $\mathcal{J}:= \{j_1,\ldots, j_K\} \subset [N]$ as the \textit{reference layer sets}\footnote{Since from definition, each reference layer will be reused in the later layer but not another reference layer, obviously we have $j_{l+1} - j_{l} >= 2, \forall l \in K-1$.}.
And for each $j_k \in \mathcal{J}$, we define a corresponding continuous sequence named \textit{target layer sets} 
$\mathcal{T}_{j_k} = [j_k+1, \ldots, j_{k}^\prime]$ where $j_k^\prime \in [j_k + 1, j_{k+1}-1]$.
% , and let $\mathcal{T} := \{\mathcal{T}_{j_1},\ldots, \mathcal{T}_{j_K}\}$. 
We also denote $g$ to be the candidate transformation function (as in Figure~\ref{fig:intro} (d)).
Our goal is to use each reference layer $j \in \mathcal{J}$ to approximate every later target layer $l \in \mathcal{T}_{j}$ with some learnable low-rank parameters,
i.e., aiming to 
find some $\Delta \Theta_{l}$, such that $f(\cdot \ ; g(\Theta_{j}, \Delta \Theta_{l}))$ performs like $f(\cdot \ ; \Theta_l)$. 
We note that only the reference layer will be loaded and stored, and target layers will be obtained using the corresponding reference layer combining low-rank additional parameters on the fly. In this way, we trade cheaper parameter computation for more expensive memory loading.%This demonstrates a tradeoff between the computation of parameters, which is relatively cheap, and the memory loading, which is more expensive.

\textbf{Stored Ratio $\tau$.} We define $\tau$ to denote the ratio of the stored layers in the replaced model to that in the original model. For instance, if we consider Llama2-7b, which has 32 layers, and the number of replaced layers is $X$, then $\tau := (32-X) / 32$.

\textbf{Compression Ratio $s$.} We define $s$ to represent the ratio of the parameters of the MLP layer in the replaced model to that in the original model. 
For instance, assume that we use the LoRA addition function as candidate transformation, the rank of additional weight is $r$, and the number of replaced layers is $X$. Then if we consider Llama2-7b, whose MLP weights have the dimension $4096\times11008$, then the compression ratio can be calculated by
\begin{equation}\small\label{eq:compress}
    s = \frac{32-X}{32} + \frac{X}{32}\times \frac{4096r + 11008r}{4096\times 11008} \approx  1 - \frac{X}{32} + X\cdot r \times 10^{-5} 
    % = 1-\tau + r \cdot\tau\times 3.2 \times 10^{-4}
\vspace{-0.5em}
\end{equation}

% After replacement, we use $s$ to denote the ratio between the number of layers that need to be stored for some way of replacement and the total number of layers. For example, the $s$ of Figure~\ref{fig:intro} (c) is 50\%.



\subsubsection{SHARP Algorithm}\label{sec: algorithm}

In this part, 
we show how we achieve SHARP (Figure~\ref{fig:intro} (d)) algorithm. First, we illustrate why we choose such a two-stage algorithm for recovering.

\textbf{Why Two-Stage?} 
A natural way to recover the model performance is to directly finetune the model end-to-end for all learnable low-rank additional parameters. In general, it works fine when we try to use one layer to replace the next layer, which at most gives a 50\% reduction of the MLP layers. However, if we want to use one layer to compute multiple adjacent layers, just using the SFT stage will require more data for recovering, and result in a much slower convergence rate and worse final result. The detailed discussion of this has been investigated in Section~\ref{sec: different setting} and Table~\ref{tab: different setting}, and the intuition for this phenomenon may be that when we use one layer to replace multiple layers aggressively, the model loses too many parameters and thus start optimizing from an initialization point that is far from the optimal solution. Therefore, we need first to align the output of the predicted and original target layers before the SFT stage.
% , which results in a two-stage algorithm. 
Details of the algorithm are as follows.
% However, this method has problems when we want to use

% 1. Using one layer to replace one layer at most give 50% reduction, not large enough
% 2. We train using one layer to compute one or multiple other layers, which can lead to more saving
% 3. one-stage doesn't work here (you may even show some examples), so we need two stages. 

\textbf{Stage 1: Single Layer Warmup (SLW).} 
% First try to minimize the $\mathcal{L}_2$ loss between each reused MLP layer and the corresponding original one using some high-quality data. Formally, for each $j \in \mathcal{J}$ and every $l \in \mathcal{T}_{j_k}$, we want to find:
First, we minimize the $\mathcal{L}_2$ loss between the output 
of layer predicted by the reference layer and that of the original target layer
% each reference layer and its corresponding original layer 
by finetuning the model on high-quality data. 
Formally, for each reference layer $j \in \mathcal{J}$ and every target layer predicted by this reference layer $l \in \mathcal{T}_{j}$, we want to find:
% \begin{equation}
%     A_l^1, B_l^1 \leftarrow \arg \min_{A_l \in \R^{d_1 \times r}, B_l \in \R^{r \times d_2}} \mathbb{E}_{X \sim \mathcal{P}^l}\left[\|f(X; \Theta^\prime_{l \leftarrow j}) - f(X; \Theta_l)\|_2^2\right]
% \end{equation}
\begin{equation}\label{eq:mlp warmup}
    \Delta \Theta_l^1 \leftarrow \arg \min_{\Delta \Theta_l} \mathbb{E}_{X \sim \mathcal{P}^l}\left[\|f(X; g(\Theta_i, \Delta \Theta_l)) - f(X; \Theta_l)\|_2^2\right]
\vspace{-0.5em}
\end{equation}
Here $\mathcal{P}_l$ is the distribution of the input activations of $f(\cdot \ ; \Theta_l)$, and it can be obtained by running the forwarding pass of the original model on the finetuning dataset. We also note that the SLW stage of each target layer can be much faster than the SFT stage (Stage 2) since we just run one MLP layer, which has only 135M parameters (while the whole model has 7B parameters). And this process can also be fully parallelized since the SLW stages of different target layers are independent. In Section~\ref{sec: different setting}, we will show that SLW is critical for increasing the compression ratios.
% Specially,  when $g$ is LoRA addition function, (\ref{eq:mlp warmup}) can be rewrite as
% \begin{equation}
%     \alpha, A_l^1, B_l^1 \leftarrow \arg \min_{\alpha \in \R, A_l \in \R^{d_1 \times r}, B_l \in \R^{r \times d_2}} \mathbb{E}_{X \sim \mathcal{P}^l}\left[\|f(X; \alpha\Theta_j + A_l^1 B_l^1) - f(X; \Theta_l)\|_2^2\right] 
% \end{equation}

% we rewrite (\ref{eq:mlp warmup}) for the case where $g$ is a LoRA-like function. In this scenario, $\forall l \in \mathcal{T}_{j}, \forall j \in \mathcal{J}$, we aim to find:
% \begin{equation}
%     \alpha, A_l^1, B_l^1 \leftarrow \arg \min_{\alpha \in \R, A_l \in \R^{d_1 \times r}, B_l \in \R^{r \times d_2}} \mathbb{E}_{X \sim \mathcal{P}^l}\left[\|f(X; \alpha\Theta_j + A_l^1 B_l^1) - f(X; \Theta_l)\|_2^2\right] 
% \end{equation}


\textbf{Stage 2: Supervised Fine-Tuning (SFT).} 
After the single MLP warmup stage, we partly recover the output of the replaced layers. 
To better align different replaced layers together and obtain better model output, at the second stage, we fixed the original parameters and
finetune all the learnable low-rank components $\{\Delta \Theta_*\}$ 
% (as Figure~\ref{fig:intro} (d)) 
together. In Section~\ref{sec: different setting}, we will also show that although SLW is important, SFT is still the key stage to recover the model capacity. 
% \yiping{mention this at the experiment parts}
% to improve the model performance further. 



\begin{table}[t]
\caption{\textbf{Different replacement types.} 
% $(\textbf{\textit{j}}:\mathcal{T}_j)$ denotes the reference layer \textbf{\textit{j}} and the target layers set $\mathcal{T}_\textbf{\textit{j}}$ which use \textbf{\textit{j}} to predict their parameters, and 
%We briefly describe how each type replaces the layers. 
Here the stored ratio $\tau$ is defined in Section~\ref{sec: preliminary}. For example in T$_\text{next}$, we need to store 18 layers (1,2,3,5,...29, 31,32), out of 32 layers in the original model, so $\tau = 56\%$. More details about the reference/target layers are shown in Table~\ref{tab:app replacement schedule}.
}
\label{tab:replacement schedule}
    \small
    \centering
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Type} & \textbf{Stored Ratio $\tau$} & \textbf{Description} \\
        \midrule
        {\textbf{T$_{\text{next}}$}} &{56\%} & {{Replacing layer $2t$ with layer $2t-1$ for $t \in [2, 15]$}} \\
        % \\\cmidrule{3-3}
        % & & \scriptsize{(\textbf{3}:4), (\textbf{5}:6), (\textbf{7}:8), (\textbf{9}:10), (\textbf{11}:12), (\textbf{13}:14), (\textbf{15}:16)}\\
        % & &  \scriptsize{(\textbf{17}:18), (\textbf{19}:20), (\textbf{21}:22), (\textbf{23}:24), (\textbf{25}:26), (\textbf{27}:28), (\textbf{29}:30)}\\
        \midrule
        {\textbf{T$_{\text{next2}}$}} & {44\%}& {Replacing layer $3t+1, 3t+2$ with layer $3t$ for $t \in [1, 9]$}\\
        % \cmidrule{3-3}
        % & & \scriptsize{(\textbf{3}:4,5), (\textbf{6}:7,8), (\textbf{9}:10,11), (\textbf{12}:13,14), (\textbf{15}:16,17), (\textbf{18}:19,20), (\textbf{21}:22,23), (\textbf{24}:25,26), (\textbf{27}:28,29)}\\
        \midrule
         {\textbf{T$_{\text{back}}$}} & 38\% & {Replacing more layers in the \textit{back} parts of the model.} \\
        % \cmidrule{3-3}
        % & & \scriptsize{(\textbf{3}:4), (\textbf{5}:6), (\textbf{7}:8), (\textbf{9}:10), (\textbf{11}:12), (\textbf{13}:14,15), (\textbf{16}:17,18,19,20,21,22), (\textbf{23}:24,25,26,27,28,29,30)}\\
        \midrule
       {\textbf{T$_{\text{front}}$}} & {38\%} & {Replacing more layers in the \textit{front} parts of the model.} \\
        % \cmidrule{3-3}
        % & & \scriptsize{(\textbf{3}:4,5,6,7,8,9,10), (\textbf{11}:12,13,14,15,16,17), (\textbf{18}:19,20), (\textbf{21}:22), (\textbf{23}:24), (\textbf{25}:26), (\textbf{27}:28), (\textbf{29}:30)}\\
        \midrule
        {\textbf{T$_{\text{more}}$}} &{25\%} & {Replace more aggressively, just store 8 layers} \\
        % \cmidrule{3-3}
        % & & \scriptsize{(\textbf{3}:4,5,6), (\textbf{7}:8,9,10,11), (\textbf{13}:14,15,16,17,18,19,20,21,22), (\textbf{23}:24,25,26,27,28,29,30)}\\
        \midrule
        {\textbf{T$_{\text{max}}$}} & {16\%} & {Replace more aggressively, just store 5 layers} \\
        % \cmidrule{3-3}
        % & & \scriptsize{(\textbf{2}:3,4,5,6,7,8,9,10), (\textbf{11}:12,13,14,15,16,17,18,19,20), (\textbf{21}:22,23,24,25,26,27,28,29,30,31)}\\
        \bottomrule
    \end{tabular}
    \vspace{-1em}
\end{table}

\subsubsection{Choice of Replacement and Candidate Transformation}\label{sec:candidate types}
\textbf{Replacement Type.} 
Note that in SHARP, there are multiple ways to define the reference layer set $\mathcal{J}$ and the corresponding target layer set $\mathcal{T}$. To prevent ambiguity, we formally list the types of layer replacement that we used in this paper in Table~\ref{tab:replacement schedule}. And more detailed table are in Table~\ref{tab:app replacement schedule}.

Notably, we skip the first and the last layer for all types, since Figure~\ref{fig:side-by-side} shows that these two layers may be quite important or behave differently from other layers.  For T$_{\text{next}}$ and T$_{\text{next2}}$, we consider the constant reference intervals (each reference predicts the next or next two target layers). For T$_{\text{back}}$ and T$_{\text{front}}$, we consider the cases where making one reference layer predicts more target layers in the front parts and back parts, respectively. And finally, for T$_{\text{more}}$ and T$_{\text{max}}$, we aggressively try to remove more MLP layers.
In Section~\ref{sec:replacement types}, we will show that repeating layers in the back parts of the model is better than doing this in the front parts, i.e., T$_{\text{back}}$ is better than T$_{\text{front}}$, and even better than T$_{\text{next2}}$. Furthermore, we will show that aggressively removing the layers like T$_{\text{more}}$ and T$_{\text{max}}$ can still achieve quite good recovery performance.
% Here we define the layer replacement schedule as 

\textbf{Candidate Transformation Type.} On the other hand, we also consider different candidate transformations $g$. In detail, we investigate the following parameterization ways:
\begin{equation}\small\label{eq:g_0}
    g_0 (\Theta_j, (\alpha, A_l, B_l)) := \alpha\Theta_j + A_lB_l,  \quad \alpha \in \R, A_l \in \R^{d_1\times r}, B_l \in \R^{r\times d_2}
\end{equation}
\begin{equation}\small\label{eq:g_1}
    g_1 (\Theta_j, (\alpha, A_l, B_l, C_l, D_l)) := \alpha\Theta_j C_l^\top D_l + A_lB_l,  \quad \alpha \in \R,A_l \in \R^{d_1\times r}, B_l, C_l, D_l \in \R^{r\times d_2}
\end{equation}
\begin{equation}\small\label{eq:g_2}
    g_2 (\Theta_j, (\alpha, A_l, B_l, E_l, F_l)) := \alpha E_l F_l^\top \Theta_j + A_lB_l,  \quad \alpha \in \R,A_l,E_l,F_l \in \R^{d_1\times r}, B_l\in \R^{r\times d_2}
\end{equation}
\begin{equation}\small\label{eq:g_3}
    g_3 (\Theta_j, (\alpha, A_l, B_l, U_l, V_l)) := \alpha  [(U_l V_l) \odot \Theta_j] + A_lB_l,  \quad \alpha \in \R,A_l,U_l \in \R^{d_1\times r}, B_l, V_l\in \R^{r\times d_2}
    % \vspace{0.2em}
\end{equation}
Here we consider the vanilla LoRA, left multiplication, right multiplication, and dot multiplication. We assume $d_1=4096 < d_2=11008$ for Llama2-7b to prevent ambiguity.
The comparison result is shown in Section~\ref{sec: exp candidate trans}. Surprisingly we find these four different transformations have almost the same capability for recovering model performance if their numbers of parameters are the same.

% \yiping{Add more parameterization here}

% Formally, for the finetuning texts
\vspace{-0.5em}
\section{Experiments}
\vspace{-0.5em}
In the experiment section, we mainly focus on four parts:
\textbf{(E1)} In-distribution recovery tasks, where we apply SHARP to the pretrained model, finetune it on a specific high-quality dataset, and then evaluate the model's perplexity on the same dataset.
\textbf{(E2)} Ablation study, where we investigate how to choose better replacement types and candidate transformations. We also try to analyze how the single-layer warmup stage and different settings influence the model recovery, and want to see how aggressively we can choose the replacement type.
\textbf{(E3)} Downstream evaluation, where we assess the model's performance on several widely-used downstream tasks.
\textbf{(E4)} Latency analysis, where we examine how much inference acceleration SHARP can achieve.
\vspace{-0.5em}
\subsection{Experimental Settings}

% \textbf{Model.} We use Llama2-7B~\citep{touvron2023llama} as the basic model. \yiping{TODO: Add evaluation tasks}

% Currently, we use the data from Open-Instruct (Tulu version 2), arxiv-math, and 200k examples from fineweb to fine-tune the replaced layers. The estimated number of tokens is 0.5B.\yiping{Haven't checked}
% For warmup, 

\textbf{Dataset.} We use the following dataset in our experiments: 
Arxiv-math~\citep{kenny2023arxivmath},
GPT4-Alpaca~\citep{peng2023alpacagpt4},
Databricks-Dolly~\citep{DatabricksBlog2023DollyV2},
DialogSum~\citep{chen-etal-2021-dialogsum},
OpenOrca~\citep{mukherjee2023openorca},
FineWeb-Edu~\citep{penedo2024finewebdatasetsdecantingweb,lozhkov2024fineweb-edu},
and Tulu V2 Collection~\citep{ivison2023tuluv2}. More details are available in Appendix~\ref{app sec: dataset}.


% More details about Tulu V2 are in Appendix~\ref{app: tulu v2 detail}.

\textbf{Finetuning Model.}
Here we use Llama2-7b~\citep{touvron2023llama} as the basic model. 
% \yiping{TODO: Add evaluation tasks}
For the Single Layer Warmup (Stage 1), we use the Adam optimizer with a fixed learning rate of 1e-3 for 5 epochs. For the SFT (Stage 2), we follow the pipeline of Open-Instruct~\citep{wang2023far}
% and set the rank of LoRA components to 64. 
% amd run the experiments on 4 L40 GPUs. We 
The learning rate of the SFT stage is 2e-5, the warmup ratio\footnote{Note this is the warmup steps of SFT rather than the SLW stage} of the SFT stage is 5\% and the max sequence length is 2048.
% , and train the model for 2 epochs. 
% The rank of LoRA components is set to be 64.
% \yiping{check}
% , and use a StepLR scheduler to reduce the learning rate by 10\% after each epoch.
In the in-distribution recovering tasks, we random sample 10\% of the training data (except 30\% for Databricks-Dolly and 5\% for OpenOrca) to calculate the activations in the intermediate layers for the SLW (Stage 1). While in the downstream evaluation tasks, we use 10\% of the Arxiv-math data for Stage 1, and then use 50k instruction data from Arxiv-math, 200k data from FineWeb-Edu, and all data that is from Tulu V2 and listed above for SFT (Stage 2). The total number of tokens used for finetuning at the downstream evaluation tasks is about 0.7B, which is much less than that of the pretraining data used by Llama2 (2T).
% \yiping{May change. The estimated tokens of Tulu V2 may be 0.5 B token, need to illustrate that this is much less than that in ReLU Strikes, etc.}

\textbf{Evaluation.} 
For the in-distribution recovering tasks, we select 1\% of the data from each task for calculating perplexity, while the other 99\% for recovering model performance. For the downstream evaluation, we follow \cite{eval-harness} and select the evaluation tasks that are widely used in other works, including memorization and reasoning. Details are illustrated in Appendix~\ref{app: eval}.

\textbf{Baseline.}
Note that SHARP is a structural-pruning-style method which directly accelerates LLM inference without further requirements in hardware design, and we select the most related baselines to show the advantage of SHARP. We consider LayerPruning~\citep{gromov2024unreasonable}, which calculates the angle distance between different layers and prunes a group of consecutive layers in the model (Pruning strategy T$_{\text{ori}}$ is defined in Table~\ref{tab:app replacement schedule}), and its adapted version, which uses our T$_{\text{next}}$ for pruning. We also compare our method with LLM-Pruner~\citep{ma2023llm_pruner}, which is a well-known structural pruning method. More details are illustrated in Appendix~\ref{app sec: baseline details}.

% Warmup dataset: 

% Different learning rate for different ranks.

\begin{table*}[t]
\small
\centering
% \vspace{-1.4em}
\caption{
\textbf{The perplexity of different methods and different replacement types in the In-distribution recovering tasks.} The smaller value the better.
% $s$ denotes the ratio between the number of the layers that need to be stored in that type and the total number of layers. 
We use rank=400 for the additional parameters and let $g_0$ (Eqn~\ref{eq:g_0}) be the candidate transformation. The compression ratio $s$ is defined in Section~\ref{sec: preliminary}. ``(w/o f.t.)'' denotes ``without fine-tuning'', and thus ``SHARP (w/o f.t.)'' means just use the first Single Layer Warmup Stage for quick recovering. More related experiments are in Appendix~\ref{app sec: exp}
% \yiping{May add finetuning results}
} 
\label{tab: in distribution}
\centering
\begin{tabularx}{\textwidth}{@{}l@{\hskip 5pt}|c@{\hskip 5pt}|c@{\hskip 5pt}|c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 3pt}c@{\hskip 3pt}c@{}}
\toprule
\textbf{Models} & \textbf{Type}  & \textbf{$s$} & \textbf{Arxiv-math} & \textbf{DialogSum} & \textbf{GPT4-Alpaca} & \textbf{Dolly} & 
% \textbf{Fineweb (10k)} & 
\textbf{OpenOrca}\\
\midrule
% Dataset Size & & & 50k & 50k
Original &  & 100\% & 3.0 & 3.7 & 2.5 & 3.6 
% & 8.3 
& 4.5 \\
\bottomrule
\toprule
Direct Sharing & T$_{\text{next}}$  & 62\%& 2171.3 & 801.7 & 20662.1 & 7221.7 & 12108.5\\
LayerPruning (w/o f.t.) &  T$_{\text{ori}}$  & 62\% &  87.6	 & 19.2	 & 86.1 & 283.1 & 102.2 \\
LayerPruning &  T$_{\text{ori}}$  & 62\% & 4.1 & 	4.4 & 	4.0 & 	6.8 & 	5.6\\
LayerPruning (w/o f.t.) & T$_{\text{next}}$  & 62\%& 	28.3	& 19.1	& 583.0	& 86.3	& 336.7\\
LayerPruning & T$_{\text{next}}$  & 62\%& 3.8 & 4.3 & 3.2 & 5.8 & 5.3\\
LLM-Pruner (w/o f.t.)& -  & 62\% & 14.7	 & 6.6	 & 6.8	 & 13.3	 & 16.1 \\
LLM-Pruner & -  & 62\% & 11.2 & 4.5 & 4.1 & 7.6 & 7.7 \\
\midrule
\textbf{SHARP} (w/o f.t.) & T$_{\text{next}}$  & 62\%& 4.8 & 5.3 & 4.2 & 7.2& 8.4 \\
\textbf{SHARP} & T$_{\text{next}}$  & 62\%& \textbf{3.2} & \textbf{3.8} & \textbf{2.8} & \textbf{4.7} 
% ?5.3 
& \textbf{4.3}
\\
% \midrule
\bottomrule
\toprule
Direct Sharing & T$_{\text{back}}$   & 46\%& 92603.9 & 154262.4
& 67908.4 & 136787.2& 86993.3\\
\textbf{SHARP} (w/o f.t.) & T$_{\text{back}}$   & 46\%& 7.7 & 7.6 & 7.9 & 13.7 & 13.5\\
\textbf{SHARP} & T$_{\text{back}}$  & 46\%& \textbf{3.5} & \textbf{4.2} & \textbf{3.2} & \textbf{5.8} & \textbf{4.9} \\
\midrule
Direct Sharing & T$_{\text{more}}$  & 35\% & 318430.3 & 143335.7 
 & 346485.1 & 280953.7 &300136.8\\
\textbf{SHARP} (w/o f.t.) & T$_{\text{more}}$  & 35\% & 26.6 & 9.8 & 11.3 & 23.7 & 21.8\\
\textbf{SHARP} & T$_{\text{more}}$  & 35\% & \textbf{3.7} &  \textbf{4.4} & \textbf{3.4}& \textbf{6.4} & \textbf{5.3} \\
% SHARP (T$_{\text{max}}$) & 16\% &  &  &  &  & & \\
\bottomrule
\end{tabularx}
% \vspace{0.1em}
% \caption*{(a) C1: No target knowledge is available.}
\vspace{-1.5em}
\end{table*}

\subsection{E1: In-Distribution Recovering tasks}\label{sec: exp in distribution}
% In this part, we use rank=400 for the addtional parameters used in SHARP. Note that the parameters of the project functions in MLP layers of Llama2-7B have the shape 4096x11008, the additional parameters just take up about 13\% of the original parameters.
In this section, we use rank=400 for the additional parameters employed in SHARP. It is important to note that the projection matrices in the MLP layers of Llama2-7b have a shape of $4096\times11008$, meaning the additional parameters occupy no more than 13\% of the original parameter size.

We show the results of in-distribution recovering tasks in Table~\ref{tab: in distribution}. 
Directly sharing adjacent layers, as in Figure~\ref{fig:intro} (c)~\citep{liu2024mobilellm}, leads to unacceptably high perplexity and meaningless output. 
%We observe that if we directly share the adjacent layers as Figure~\ref{fig:intro} (c) ~\citep{liu2024mobilellm}, the perplexity on all the evaluation tasks will be unacceptably large, and the output may become meaningless. 
However, using the Single Layer Warmup (SHARP w/o f.t.), the perplexity of the model gets to a reasonable range. After
applying SFT with the in-distribution data, the perplexity gap between the original and the SHARP-processed model can be further reduced.
In particular, T$_{\text{next}}$ almost fixes the gap for most of the tasks except Databricks-Dolly. 
% On OpenOrca, SHARP (4.3) even achieves better performance than the original model (4.5). 
What's more, after radically giving up more than 3/4 of the layers (T$_{\text{more}}$
% , compression ratios are calculated by Eqn.~\ref{eq:compress}
), 
% just keeping the first, the last and 3 middle layers, 
we can still recover the perplexity gap quite well.
% \yiping{Add detail}.

On the other hand, compared to other structure pruning baselines, we can see that:
(1) \textbf{SHARP performs consistently better than structural pruning methods.} 
(2) SHARP vs LayerPruning (T$_{\text{next}}$): when the number of parameters is the same, \textbf{reusing the previous layers can retain more model capability than directly pruning them.}
(3) LayerPruning (T$_{\text{next}}$) vs (T$_{\text{ori}}$): T$_{\text{next}}$, which replaces the layers at intervals, is better than the vanilla strategy T$_{\text{ori}}$ in \cite{gromov2024unreasonable} which removes the consecutive layers.
(4) Removing parameters as LLM-Pruner may result in a good initial performance but be more difficult in recovering. 
These results show the potential of SHARP to save model parameters and accelerate inference, and its advantage over other structural pruning methods.
In addition, we also show that applying SHARP on one specific task (GPT4-Alpaca) can be useful for recovering model performance on other four tasks (Appendix~\ref{app sec: overfit}), and the claim in this section holds for other model, like LLaMA3.2-3B model (Appendix~\ref{app se llama3_2}).




% \yiping{Another point that we can show is that just rank-1 or rank-5 recovery is good enough!}


% \begin{table*}[t]
% \small
% \centering
% % \vspace{-1.4em}
% \caption{
% Recover perplexity on in-distribution tasks
% } 
% \label{tab: D1 compare}
% \centering
% \begin{tabularx}{\textwidth}{@{}l@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{}}
% % \begin{tabularx}{\textwidth}{@{}l@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{}}
% % \small
% \toprule
% \multirow{2}{*}{\textbf{Models}} & \textbf{Stored} & & & & &\\
%  & \textbf{MLP layers} & \textbf{Arxiv-math} & \textbf{Alpaca-GPT4} & \textbf{DialogSum} & \textbf{Dolly} & \textbf{Fineweb (10k)} & \textbf{OpenOrca (1\%)}\\
% \midrule
% Vanilla & 32 & 3.0 & 2.5 & 3.7 & 3.5 & 8.3 & 4.5 \\
% \midrule
% Ours & 17 & 3.2 & 3 & 4.4 & 5.3 & 9.3 & & \\
% Ours & 12 & 4.1 &  &  &  & & & \\
% \bottomrule
% \end{tabularx}
% % \vspace{0.1em}
% % \caption*{(a) C1: No target knowledge is available.}
% \vspace{-1.7em}
% \end{table*}
% \subsubsection{Which Layer should be replaced?}






\begin{table*}[t]
\small
\centering
\vspace{-1.0em}
\caption{
\textbf{Different settings of SHARP.} It shows the model perplexity evaluated on Arxiv-math, where the baseline result is 3.0. %SLW denotes Single Layer Warmup Stage. 
We use $g_0$ (Eqn.~\ref{eq:g_0}) as the candidate function, and the Types are shown in Table~\ref{tab:replacement schedule}
% \footnote{for $(r=400,s=38\%)$, we use rank=64 for the SFT stage to stabilize the finetuning process}. 
% $s$ denotes the proportion of the stored MLP layers for our algorithm, and ``53\%'' denotes storing 17 original layers, and ``38\%'' denotes storing 12 layers. 
% ``W'' denotes the single MLP layer warmup step, and 
The percentage in the bracket means the proportion of the Arxiv-math training data that are randomly sampled for the SFT stage. Compression ratios $s$ is calculated by Eqn.~\ref{eq:compress}.
% We note
} 
\label{tab: different setting}
\centering
\vspace{-0.5em}
\begin{tabularx}{0.95\textwidth}{@{}l@{\hskip 8pt}|c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}|c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}|c@{\hskip 8pt}c@{}}
% \begin{tabularx}{\textwidth}{@{}l@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{}}
% \small
\toprule
% \multirow{2}{*}{\textbf{Models}} & \textbf{{Prop. of }} 
% & \multirow{2}{*}{\textbf{W(1\%)}} 
% & \multirow{2}{*}{\textbf{W(10\%)}} 
% & \multirow{2}{*}{\textbf{W(100\%)}} 
% & \multirow{2}{*}{\textbf{SFT(10\%)}} 
% & \multirow{2}{*}{\textbf{SFT(100\%)}} 
% & \multirow{2}{*}{\textbf{W(10\%)+SFT(10\%)}}
% & \multirow{2}{*}{\textbf{W(10\%)+SFT(100\%)}}\\
% &\textbf{Stored MLP Layers} & & \\
\textbf{Rank $r$} 
% % & \textbf{$r=5, 53\%$}
% % & \textbf{$r=5, 38\%$}
% % & \textbf{$r=32, 53\%$}
% % & \textbf{$r=32, 38\%$}
% % & \textbf{$r=400, 53\%$}
% % & \textbf{$r=400, 38\%$} \\
% & \textbf{r=5, s=53\%}
% & \textbf{r=20, s=53\%}
% & \textbf{r=400, s=53\%}
% & \textbf{r=5, s=38\%}
% & \textbf{r=20, s=38\%}
% & \textbf{r=400, s=38\%} \\
% & \textbf{(r=5, T$_{\text{next}}$)}
% & \textbf{(r=20, T$_{\text{next}}$)}
% & \textbf{(r=400, T$_{\text{next}}$)}
% & \textbf{(r=5, T$_{\text{back}}$)}
% & \textbf{(r=20, T$_{\text{back}}$)}
% & \textbf{(r=400, T$_{\text{back}}$)} 
% & \textbf{(r=400, T$_{\text{more}}$)} \\
& $r=5$ & $r=20$ & $r=400$& $r=5$ & $r=20$ & $r=400$ & $r=400$ \\
% & \textbf{r=400, T$_{\text{max}}$} \\
% \textbf{{Prop. of }} 
% & \multirow{2}{*}{\textbf{W(1\%)}} 
% & \multirow{2}{*}{\textbf{W(10\%)}} 
% & \multirow{2}{*}{\textbf{W(100\%)}} 
% & \multirow{2}{*}{\textbf{SFT(10\%)}} 
% & \multirow{2}{*}{\textbf{SFT(100\%)}} 
% & \multirow{2}{*}{\textbf{W(10\%)+SFT(10\%)}}
% & \multirow{2}{*}{\textbf{W(10\%)+SFT(100\%)}}\\
% &\textbf{Stored MLP Layers} & & \\

% \textbf{Models} & \textbf{Prop. of Stored MLP Layers} & \textbf{MLP Warmup} & \textbf{MLP Warmup + SFT} \\
\midrule
\textbf{Type} & \multicolumn{3}{c|}{\textbf{T$_{\text{next}}$}} & \multicolumn{3}{c|}{\textbf{T$_{\text{back}}$}} & \textbf{T$_{\text{more}}$}\\
\midrule
Compression Ratio $s$ & 56\% & 57\% & 62\% & 38\% & 38\% & 46\% & 35\%\\
\midrule
Direct Sharing & 2171.3 & 2171.3 & 2171.3 & 92603.9 & 92603.9 & 92603.9 & 318430.3\\
% \midrule
% W (1\%) & & & & & & \\
SHARP (w/o f.t.) & 13.0 & 9.5 & 4.8 & 73.7 & 33.9 & 7.7 & 26.6 \\
% SLW ($\times 10$) & & & & & & \\
\midrule
SHARP (only SFT, 10\%) & 4.9 & 5.0 & 5.0 & 6.2 & 6.7 & 8.6 & 14.8\\
SHARP (only SFT, 100\%) & \textbf{4.0} & \underline{4.0} & \underline{3.8} & \textbf{4.5} & \textbf{4.2} & \underline{3.6} & 6.8\\
\midrule
SHARP (10\%) & 4.9 & 4.9 & 3.9 & 6.0 & 5.6 & 4.2 & \underline{4.5}\\
SHARP (100\%) & \underline{4.1} & \textbf{3.8} & \textbf{3.2} &  \underline{4.8} &  \textbf{4.2} & \textbf{3.5} & \textbf{3.7}\\
% $r=5$ & 53.1\% & & & 4.0\\
% $r=5$ & 37.5\% & & \\
% $r=32$ & 53.1\% & & \\
% $r=32$ & 37.5\% & & \\
% $r=400$ & 53.1\% & & \\
% $r=400$ & 37.5\% & & \\
\bottomrule
\end{tabularx}
% \vspace{0.1em}
% \caption*{(a) C1: No target knowledge is available.}
\vspace{-1em}
\end{table*}

\begin{wraptable}{r}{0.43\textwidth}
\begin{center}
\small
\centering
\vspace{-4em}
\caption{
\textbf{Recovery results (perplexity) for different SHARP Types} (Table~\ref{tab:replacement schedule}).
Store ratio $\tau$ is defined in Section~\ref{sec: preliminary}.
}
% \vspace{-0.5em}
\label{tab:type comparsion}
\begin{tabular}{@{}l@{\hskip 5pt}|c@{\hskip 5pt}|c@{\hskip 5pt}|c@{}}
\toprule
\textbf{Type} & \textbf{$\tau$} & \textbf{Arxiv-math} & \textbf{DialogSum}\\
\midrule
\textbf{Baseline} & 100\% & 3.0 & 3.7\\
\bottomrule
\toprule
\textbf{T$_{\text{next}}$} & 56\% & 3.2 & 3.8\\
\midrule
\textbf{T$_{\text{next2}}$} & 44\% & 3.7 & 4.2\\
\textbf{T$_{\text{back}}$ }& 38\% & 3.5 & 4.2\\
\textbf{T$_{\text{front}}$} & 38\% & 3.8& 4.4\\
\textbf{T$_{\text{more}}$} & 25\% & 3.7 & 4.4\\
\midrule
\textbf{T$_{\text{max}}$} & 16\% & 4.1 & 4.9\\
\bottomrule
\end{tabular}
\end{center}
\vspace{-3em}
\end{wraptable}

\subsection{E2: Further ablation study}\label{sec: exp ablation}


% Working on this. Trying to recover the downstream performance on lm-evaluation-harness.
In this section, we conduct several ablation studies.
% of SHARP.


\subsubsection{How to Replace Better?}\label{sec:replacement types}


First, we investigate that with the same or similar number of the stored layers (i.e., stored ratio $s$), %what kind 
which type of replacement can better retain model capacity. Like the previous section, we use matrices with rank=400 as the additional LoRA parameters, and we try all the SHARP types shown in Table~\ref{tab:replacement schedule}. 
The results are in Table~\ref{tab:type comparsion}. 
We find that interestingly, 
% replacement with constant intervals is not necessary the best choice
although T$_{\text{next}}$ recovers the perplexity quite well, T$_{\text{next2}}$ only has the same or even worse results than T$_{\text{back}}$, which has a smaller stored ratio ($s$). Similarly, T$_{\text{front}}$ has worse performance than T$_{\text{back}}$. These results show that \textbf{replacing more layers at the back parts of the model can better maintain the model performance}. The discussion of Figure~\ref{fig:vis layer} in Section~\ref{sec: exp downstream} will further support this claim. Furthermore, we can see that T$_{\text{more}}$ and T$_{\text{max}}$ also have impressive recovery results, even though they drop most of the MLP layers, which also support our claims in Section~\ref{sec:insight}.
% \yiping{check}

% achieve quite good result

% The first and last several layers are quite different and worth keeping, and we should replace more parameters on the middle to last layers. \yiping{wait to add front more, later less.}


\subsubsection{Different Candidate Transformation}
\label{sec: exp candidate trans}

\begin{wraptable}{r}{0.43\textwidth}
\begin{center}
\small
\centering
\vspace{-3em}
\caption{
\textbf{Different candidate transformations bring similar recovery results.}
% \textbf{Perplexity on Arxiv-math for different candidate transformations (T$_{\text{next}}$).}
}
% \vspace{-0.5em}
\label{tab:candidate trans}
% \small
\begin{tabular}{l|c|c|c}
\toprule
\multirow{2}{*}{\textbf{Type}} & \multirow{2}{*}{\textbf{rank}} & \textbf{SHARP} & \multirow{2}{*}{\textbf{SHARP}} \\
& & \textbf{(w/o f.t.)} & \\
\midrule
Baseline & NA & NA &3.0\\
\midrule
% $g_0 (\Theta_j, (\alpha, A_l, B_l))$ & 400 & 4.8 & \textbf{3.2} \\ 
% $g_1 (\Theta_j, (\alpha, A_l, B_l, C_l, D_l))$ & 163 & 5.5 & 3.3\\ 
% $g_2 (\Theta_j, (\alpha, A_l, B_l, E_l, F_l))$ & 259 & 4.8 & 3.3 \\ 
% $g_3 (\Theta_j, (\alpha, A_l, B_l, U_l, V_l))$ & 200 & \textbf{4.7} & 3.3\\ 
$g_0 \ $(Eqn.~\ref{eq:g_0}) & 400 & 4.8 & \textbf{3.2} \\ 
$g_1 \ $(Eqn.~\ref{eq:g_1}) & 163 & 5.5 & 3.3\\ 
$g_2 \ $(Eqn.~\ref{eq:g_2}) & 259 & 4.8 & 3.3 \\ 
$g_3 \ $(Eqn.~\ref{eq:g_3}) & 200 & \textbf{4.7} & 3.3\\ 
\bottomrule
\end{tabular}
\end{center}
\vspace{-1.em}
\end{wraptable}

In this part, we compare various candidate transformations listed in Section~\ref{sec:candidate types}, i.e., Equation (\ref{eq:g_0}),(\ref{eq:g_1}),(\ref{eq:g_2}), and (\ref{eq:g_3}), to assess their impact on model recovery. %to show if different kinds of parameterization way will result in different recovery capability of the model. 
For a fair comparison, we adjust ranks so all candidates have almost the same number of additional parameters.
%Here for fair comparison, we set different ranks for different candidate transformations such that all these methods have almost the same number of additional parameters. 
We consider Arxiv-math and the result are shown in Table~\ref{tab:candidate trans}. 

Surprisingly, except for the SLW stage of $g_1$, all other candidate transformations have similar performance for both SLW and the complete SHARP stages. This may show that using the same amount of additional parameters, different parameterization strategies have the same recovery capabilities. And since LoRA-style transformation, $g_0$ has the simplest forms and more comprehensive studies, we choose $g_0$ as our candidate transformation in the later experiment parts.




\subsubsection{Ablation Study for Dataset Size, Rank and Type}\label{sec: different setting}
To find the optimal algorithm settings, we further explore how different choices of dataset size, rank of the additional parameters, and the SHARP Types together influence the recovery performance. We choose Arxiv-math as the in-distribution task and select ranks among 5, 20, and 400. The methods contain the vanilla adjacent layer sharing (Direct Sharing), just use the first stage of SHARP (SLW, SHARP w/o fine-tuning), just use the second stage (SFT), and the complete SHARP. Training dataset size includes all the dataset (100\%) or a small random subset of it (10\%).





% Then based on the results shown in Table~\ref{tab: different setting}.
The results are shown in Table~\ref{tab: different setting}.
% , we have the following observations: (1)
We note that
although the performance of SLW alone is not advanced, it is beneficial for the whole SHARP process. \textit{Especially, the larger the rank, the smaller the finetuning dataset size, and the more aggressively we drop the layers, the more important the SLW stage serves.} For example, in (r=400, T$_{\text{back}}$) case, SFT (10\%) just recovers the perplexity to 8.6, while SLW + SFT (10\%) can improve it to 4.2. And in (r=400, T$_{\text{more}}$) case, even SFT (100\%) performs much worse than SLW + SFT (100\%).
In general, SLW + SFT (10\%) already approaches the best result SLW + SFT (100\%) quite well although they just use 10\% of the finetuning data.
% \yiping{Add when the replacement is aggressive}. 
On the other hand,
when the rank is small, the influence of SLW is not significant. As 
in rank=5, SLW + SFT has similar results to SFT alone for both T$_{\text{next}}$ and T$_{\text{back}}$. 
We think the reason for this is that the target of the SLW stage is to provide a better LoRA weight initialization for the SFT. Since we use $\mathcal{L}_2$ loss at SLW, the warmup performance becomes better when the rank of additional parameters becomes larger (Like for T$_{\text{back}}$, SLW alone with r=5 just improves perplexity to 73.7, while r=400 can bring it to 7.7), and its gain becomes more obvious when more layers need to be predicted and the SFT data is not enough. 
% \yiping{Can add discussion of compression ratios.}

Besides, we also find that in general, SFT is the key stage of SHARP to recover the final performance, and 
a larger rank and larger finetuning dataset brings better performance.
% (2) In general, 
% a larger rank brings better performance for SHARP
% , and for more aggressively replacement (like T$_{\text{back}}$), the performance gap between different ranks are larger. The reason maybe that additional parameters with larger rank have more capacity for recovering model performance.
% (3) \yiping{check} The size for the SLW stage is not so critical, and this stage converges quickly. 
% (4) 
For the best setting (rank=400, SLW + SFT (100\%)), aggressively dropping most of the MLP layers like T$_{\text{more}}$, which drops 75\% of the original MLP layers, still keeps quite good performance (3.7) in perplexity compared to the best result of T$_{\text{next}}$ (3.2).
Finally, in Appendix~\ref{app sec: full lora} we also discuss adding  LoRA components to all the layers.

% The warmup stage is necessary for relatively ``high-rank'' cases. And it's obviously better when the SFT dataset size is not large.
% Maybe it provides better initialization for LoRA parameters.

% The size of the warmup dataset is not critical, and the Warmup stage converges quickly.

% When the replacement is radical, the difference between the number of rank is not so obvious.



% \yiping{illustrate different initialization for $g$.}

% Furthermore, we try to 

% Although some components may provide a faster convergence rate, the simple LoRA style is enough and more stable for further training. The results are similar for arxiv-math
 


% \yiping{Now second pass to here.}


\begin{table*}[t]
\small
\centering
% \vspace{-1.4em}
\caption{
\textbf{Recover results on the downstream tasks.} The higher the better for all the tasks. Here we use rank=400 and the replacement type is T$_\text{next}$, therefore, the compression ratio is 62\%. Additionally, \textbf{SHARP is compatible with quantization method.}
% Here we use Llama2-7b for the original model performance.
} 
\vspace{-0.5em}
\label{tab: downstream eval}
\centering
% \begin{tabularx}{\textwidth}{@{}l@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 6pt}c@{}}
% \begin{tabularx}{\textwidth}{@{}l@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}}
\begin{tabularx}{\textwidth}{@{}l@{\hskip 3pt}|c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 3pt}c@{\hskip 3pt}c@{\hskip 3pt}c@{}c@{}c@{}}
% \begin{tabularx}{\textwidth}{@{}l@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{}}
% \small
\toprule
% % \multirow{2}{*}{\textbf{Models}} & \textbf{#Stored} & & & & & & &\\
% %  & \textbf{MLP layers} & \textbf{SciQ} & \textbf{BoolQ} & \textbf{PIQA} & \textbf{WinoGrande} & \textbf{Arc-E} & \textbf{Arc-C} & \textbf{LAMBADA} \\
% Models & $s$ & \textbf{SciQ} & \textbf{BoolQ} & \textbf{PIQA}  & \textbf{Arc-E} & \textbf{Arc-C} & \textbf{WinoGrande} & \textbf{LAMBADA} \\
% \midrule
% Original & 100\% & 94.1 & 77.8 & 78.1 & 76.3 & 43.3 & 69.4 & 71.9 \\
% \midrule
% Direct Sharing & 56\% & 22.7 & 49.1 & 56.7 & 26.8 & 22.0 & 50.6 & 8.1 & \\
% SLW & 56\% & 87.8 & 66.1 & 63.0 & 54.8 & 26.8 & 57.8 & 35.0 & \\
% % MLP Warmup + SFT & 17 & 92.8 & 74.3 & 72.7 & 61.4 & 65.4 & 34.6 & 58.6\\
% SLW + SFT & 56\% & 92.6 & 76.0 & 72.6 & 65.8 & 34.7 & 62.4 & 58.3\\
% % SLW & 38\% & \\
% % SLW + SFT & 38\% & \\ 
% \bottomrule
% \toprule
% & & \textbf{BBH} & \textbf{GSM8k} & \textbf{MATHQA} & \textbf{MUTUAL} & \textbf{QA4MRE} & \textbf{MEDMCQA} & \textbf{CommonsenseQA}\\
% \midrule
% Original & 100\% & 38.6 & 14.2 & 28.4 & 70.8 & 42.8 & &  \\
% \midrule
% Direct Sharing & 56\% & 0.0 & 0.0 & 20.7 & 57.5 & 16.6 & &  \\
% SLW & 56\% & 24.8 & 1.6 & 22.1 & 59.8 & 30.2 & &  \\
% % MLP Warmup + SFT & 17 & 92.8 & 74.3 & 72.7 & 61.4 & 65.4 & 34.6 & 58.6\\
% SLW + SFT & 56\% & 29.8 & 3.6 & 24.7 & 68.3 & 38.3 & &  \\
% % SLW & 38\% & \\
% % SLW + SFT & 38\% & \\ 
% \multirow{2}{*}{\textbf{Models}} & \textbf{#Stored} & & & & & & &\\
%  & \textbf{MLP layers} & \textbf{SciQ} & \textbf{BoolQ} & \textbf{PIQA} & \textbf{WinoGrande} & \textbf{Arc-E} & \textbf{Arc-C} & \textbf{LAMBADA} \\
\textbf{Models}  & \textbf{SciQ} & \textbf{BoolQ} & \textbf{PIQA}  & \textbf{ARC-E} & \textbf{ARC-C} & \textbf{WinoGrande} & \textbf{MedMCQA} \\
\midrule
Original  & 94.1 & 77.8 & 78.1 & 76.3 & 43.3 & 69.4 & 34.5 \\
\midrule
Direct Sharing  & 22.7 & 49.1 & 56.7 & 26.8 & 22.0 & 50.6 & 22.4 \\
\textbf{SHARP} (w/o f.t.) & 87.8 & 66.1 & 63.0 & 54.8 & 26.8 & 57.8 & 30.5 & \\
% MLP Warmup + SFT & 17 & 92.8 & 74.3 & 72.7 & 61.4 & 65.4 & 34.6 & 58.6\\
\textbf{SHARP} & 92.6 & 76.0 & 72.6 & 65.8 & 34.7 & 62.4 & 31.4\\
\textbf{SHARP (4-bit quan.)} & 92.1 & 74.7 & 71.9 & 65.1 & 35.0 & 62.2 & 30.7\\
% SLW & 38\% & \\
% SLW + SFT & 38\% & \\ 
\bottomrule
\toprule
& \textbf{BBH} & \textbf{GSM8k} & \textbf{MATHQA} & \textbf{MUTUAL} & \textbf{LAMBADA}  & \textbf{ComSenQA} & \textbf{QA4MRE}\\
\midrule
Original  & 38.6 & 14.2 & 28.4 & 70.8  & 71.9 & 32.6 & 42.8\\
\midrule
Direct Sharing  & 0.0 & 0.0 & 20.7 & 57.5 & 8.1 & 20.0 &16.6\\
\textbf{SHARP} (w/o f.t.)  & 24.8 & 1.6 & 22.1 & 59.8  & 35.0 & 20.0 &30.2 \\
% MLP Warmup + SFT & 17 & 92.8 & 74.3 & 72.7 & 61.4 & 65.4 & 34.6 & 58.6\\
\textbf{SHARP} & 29.8 & 3.6 & 24.7 & 68.3 & 58.3 & \textbf{44.1} &38.3 \\
\textbf{SHARP (4-bit quan.)} & 27.9 & 2.8 & 24.1 & 68.1 & 55.6 & 37.4 & 39.3  \\
% SLW & 38\% & \\
% SLW + SFT & 38\% & \\ 
\bottomrule
\end{tabularx}
% \vspace{0.1em}
% \caption*{(a) C1: No target knowledge is available.}
\vspace{-1.5em}
\end{table*}

\subsection{E3: Downstream Evaluation and Compatibility}\label{sec: exp downstream}
%In this part, we focus on evaluating 
% We evaluate the recovered model on some widely used downstream tasks using the standard pipeline %provided 
% from \cite{eval-harness}. 
Here we use type T$_{\text{next}}$ with rank=400 for recovering parameters. 
Due to time and resource constraints, we fine-tuned the model using only 0.7B tokens, significantly fewer than Llama2's 2T pretraining tokens and the 30B–150B tokens used in sparsification works~\citep{mirzadeh2023relu,song2024turbo}.
%We note that due to the restriction of time and resources, we just use about 0.7B tokens of finetuning corpus for recovering the model performance, which is not only much less than the number of pretraining tokens used by Llama2 (2T tokens), but also much less than that used in the sparsification works~\citep{mirzadeh2023relu,song2024turbo} (30B$\sim$150B tokens).
% It's worth to note that the size of our finetuning corpus ($\sim$0.5B tokens) is much less than that used in the sparsification works~\citep{mirzadeh2023relu,song2024turbo} (30B$\sim$150B tokens).
However, it still brings positive signals and further understanding of our methods. 
% Details are as follows.
The result is shown in Table~\ref{tab: downstream eval}. 

As expected, %we can see that SHARP 
SHARP greatly outperforms %much better than 
Direct Sharing and SLW alone on all the evaluation tasks.
In particular, we notice that SHARP performs relatively better in tasks requiring knowledge memorization capability.
For example, compared to the original model, SHARP has very close evaluation results on SciQ and BoolQ, which focus on memorizing scientific concepts and knowledge from various domains, respectively. 
%It even has better performance than the 
Notably, it even surpasses the baseline on CommonsenseQA (44.1 versus 32.6), which measures the commonsense knowledge of the model.
On the other hand, for tasks that further require more complex reasoning capabilities, like GSM8k, ARC-Easy, ARC-Challenge, and PIQA, the performance gap between SHARP and the original model is still large (Although in Table~\ref{tab: in distribution} we have show that under the same condition, SHARP achieves better recovery performance than other structural pruning baselines.)



For a better understanding of why SHARP has different performance on different kinds of evaluation tasks, we try to observe the sensitivity of different layers to each task. To achieve this, we set the parameters of each MLP layer to zero and compare the evaluation results between the original model and the modified model. 
The results are shown in Figure~\ref{fig:vis layer}, and we have two observations:
(1) We find that layers 5 to 15 and the last two layers are important for all tasks. Setting their parameters to 0 will hurt both knowledge-memorization tasks and reasoning tasks. This supports our claim in Section~\ref{sec:replacement types} that keeping the front parts of the layers can retain more model capacity.
(2) Interestingly, for layers 16 to 30, we find different phenomenons for knowledge memorization tasks and reasoning tasks. For knowledge memorization tasks, just a few layers between 16 to 30 are important for the performance, and they may vary for different tasks. For example, 19 and 29 for BoolQ, 19,24,26, and 27 for CommonsenseQA, and 22 and 29 for MedMCQA. While for the complex reasoning tasks, almost all the layers between 16 and 30 have a non-negligible influence on the model output. These may imply that the model uses some specific weights in the later layers to memorize knowledge from various domains, while for more complex reasoning tasks, the capability is related to all the layers, which results in it needing more data and resources for the model to recover performance. This ablation study will also be useful for future works of interpreting how models capture knowledge and capabilities at each layer.

Furthermore, \textbf{our method can be orthogonal and compatible with other model compression techniques, such as quantization}. Compared to using SHARP alone, using SHARP and 4-bit quantization together only causes a performance drop of 1\%, which is relatively acceptable. This also supports the experiment setting in the later latency analysis (Sec.~\ref{sec: exp latency}).

% in Figure~\ref{fig:vis layer} we replace


% but still has gap for the tasks

% Furthermore, we also notice that
% for the tasks related to the knowledge memorization capability, like SciQ, BoolQ, MUTUAL, the evaluation result between SHARP and original model is close. However, for the tasks requiring reasoning and comprehension, the gap is still large. 

% We think the reason is that it's relatively easy for the model to recover the knowledge by SFT, but the given tokens are not enough for the model to fully recover the high-level capability like reasoning. 
% \yiping{Can analyze in detail, and wait for new experiments.}


\begin{figure}[t]
    \centering
    \vspace{-2em}
    \includegraphics[width=0.48\linewidth]{fig/z_difference_result_memory.pdf}
    \includegraphics[width=0.48\linewidth]{fig/z_difference_result_reason2.pdf}
    \hfill
    \vspace{-1em}
    \caption{
        %Comparison of the i
        Impact of different layers on model capabilities. The x-axis denotes the index of the zero-out MLP layer, whose weights are set to be zero, in the modified model, and the y-axis shows the difference between the original model and the modified model on the particular evaluation tasks, which means that the lower the value the better. (Left)
        Evaluation tasks focused on memorizing domain-specific knowledge or common sense.
        %Evaluation tasks that just require memorizing knowledge from specific domains or common sense. 
        (Right) 
        Evaluation tasks requiring reasoning abilities in areas like mathematics, physics, or general reasoning.
        %Evaluation tasks that require reasoning ability in mathematics, physics, or common sense. 
        We skip index 0 since it's critical based on Figure~\ref{fig:side-by-side}.
    }
    \label{fig:vis layer}
     \vspace{-1.5em}
\end{figure}

\begin{table}[h]
\centering
\small
\vspace{-0.2em}
\caption{Run time results (seconds) on mobile.}
\vspace{-0.8em}
\label{tab:mobile_latency}
\begin{tabular}{l|c|c|c|c}
\toprule
Model & Load \& Init (std) & Forward (std) & Total time & Model size\\
\midrule
Original Llama2-7b & 9.794 (0.627\%)& 2.905 (1.573\%) &12.699 & 4.04GB\\
SHARP (T$_{\text{next}}$) & 5.684 (0.645\%) & 1.630 (1.363\%)& 7.314 &2.31GB\\
\midrule
SHARP (T$_{\text{next}}$) saving & 42.0\% & 43.9\%& 42.2\% & 42.8\%\\
\bottomrule
\end{tabular}
\vspace{-1.2em}
\end{table}

\subsection{E4: Latency Analysis}\label{sec: exp latency}
\vspace{-0.5em}
%\subsubsection{On Server}
% TODO

%We measure the latency of original Llama2-7b model and SHARP variances on server using PyTorch-2.4.1\footnote{\url{https://github.com/pytorch/pytorch}} and [more compile settings like cache size here]. and show the results in Table~\ref{tab:server_latency}.


%\subsubsection{On Mobile Device}
% TODO



In this section, we measure the time to load and initialize the model and the average time to run a single forward over 5 runs using ExecuTorch v0.3.0 on iPhone 16 Pro (iOS 18.1), with XNNPack backend. To enable to model to be fitted into the phone with 8GB RAM, we use 4-bit integer quantized models. Detailed experimental setting is in Appendix~\ref{app:mobile}. %We show the results in Table~\ref{tab:mobile_latency}. 
Results in Table~\ref{tab:mobile_latency} reflect that through weight sharing, SHARP saves 42.0\% of loading and initialization time, which is the dominant of the model execution, attributable to fewer layers required to store and load. SHARP also runs faster than the original Llama2-7b by 43.9\%, benefiting from data locality. Overall, our SHARP saves 42.2\% run time and 42.8\% model storage compared to the original Llama2-7b.









\vspace{-0.7em}
\section{Conclusion}
\vspace{-0.7em}
This paper presented SHARP, a method to accelerate large language model inference by sharing adjacent layers with recovery parameters. SHARP effectively reduces model size and inference time by using the reference layer to predict the later layers, which saves the memory load overhead. It recovers the model performance through a two-stage process: Single Layer Warmup and Supervised Fine-Tuning. Experimental results demonstrate that SHARP achieves comparable perplexity to original models across various in-distribution tasks. By minimizing the number of layers and parameters needed, SHARP provides a practical solution for deploying large models in resource-constrained environments. 
We believe this method can further enlighten researchers in designing advanced layer-sharing methods for accelerating inference and may be insightful for the interpretability-related works on understanding how each layer works in LLM.
% Future work can explore further optimizations and broader applications of this method.


\iffalse
\section{Submission of conference papers to ICLR 2025}

ICLR requires electronic submissions, processed by
\url{https://openreview.net/}. See ICLR's website for more instructions.

If your paper is ultimately accepted, the statement {\tt
  {\textbackslash}iclrfinalcopy} should be inserted to adjust the
format to the camera ready requirements.

The format for the submissions is a variant of the NeurIPS format.
Please read carefully the instructions below, and follow them
faithfully.

\subsection{Style}

Papers to be submitted to ICLR 2025 must be prepared according to the
instructions presented here.

%% Please note that we have introduced automatic line number generation
%% into the style file for \LaTeXe. This is to help reviewers
%% refer to specific lines of the paper when they make their comments. Please do
%% NOT refer to these line numbers in your paper as they will be removed from the
%% style file for the final version of accepted papers.

Authors are required to use the ICLR \LaTeX{} style files obtainable at the
ICLR website. Please make sure you use the current files and
not previous versions. Tweaking the style files may be grounds for rejection.

\subsection{Retrieval of style files}

The style files for ICLR and other conference information are available online at:
\begin{center}
   \url{http://www.iclr.cc/}
\end{center}
The file \verb+iclr2025_conference.pdf+ contains these
instructions and illustrates the
various formatting requirements your ICLR paper must satisfy.
Submissions must be made using \LaTeX{} and the style files
\verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
\verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
have to do is replace the author, title, abstract, and text of the paper with
your own.

The formatting instructions contained in these style files are summarized in
sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

\section{General formatting instructions}
\label{gen_inst}

The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
preferred typeface throughout. Paragraphs are separated by 1/2~line space,
with no indentation.

Paper title is 17~point, in small caps and left-aligned.
All pages should start at 1~inch (6~picas) from the top of the page.

Authors' names are
set in boldface, and each name is placed above its corresponding
address. The lead author's name is to be listed first, and
the co-authors' names are set to follow. Authors sharing the
same address can be on the same line.

Please pay special attention to the instructions in section \ref{others}
regarding figures, tables, acknowledgments, and references.


There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations. 

\section{Headings: first level}
\label{headings}

First level headings are in small caps,
flush left and in point size 12. One line space before the first level
heading and 1/2~line space after the first level heading.

\subsection{Headings: second level}

Second level headings are in small caps,
flush left and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsubsection{Headings: third level}

Third level headings are in small caps,
flush left and in point size 10. One line space before the third level
heading and 1/2~line space after the third level heading.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and year (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~\citep{Bengio+chapter2007}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textsc{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Default Notation}

In an attempt to encourage standardized notation, we have included the
notation file from the textbook, \textit{Deep Learning}
\cite{goodfellow2016deep} available at
\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
is not required and can be disabled by commenting out
\texttt{math\_commands.tex}.


\centerline{\bf Numbers and Arrays}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1in}p{3.25in}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix\\
$\displaystyle \tA$ & A tensor\\
$\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
$\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
$\displaystyle \ra$ & A scalar random variable\\
$\displaystyle \rva$ & A vector-valued random variable\\
$\displaystyle \rmA$ & A matrix-valued random variable\\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Sets and Graphs}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \sA$ & A set\\
$\displaystyle \R$ & The set of real numbers \\
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
$\displaystyle \gG$ & A graph\\
$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\vspace{0.25cm}


\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
$\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
$\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
$\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
$\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
$\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
$\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\vspace{0.25cm}


\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
$\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
$\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
$\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
  $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
  (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log x$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
$\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
$\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
$\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
$\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
\end{tabular}
\egroup
\vspace{0.25cm}



\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textsc{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PostScript or PDF files}

Please prepare PostScript or PDF files with paper size ``US Letter'', and
not, for example, ``A4''. The -t
letter option on dvips will produce US Letter files.

Consider directly generating PDF files using \verb+pdflatex+
(especially if you are a MiKTeX user).
PDF figures must be substituted for EPS figures, however.

Otherwise, please generate your PostScript and PDF files with the following commands:
\begin{verbatim}
dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
ps2pdf mypaper.ps mypaper.pdf
\end{verbatim}

\subsection{Margins in LaTeX}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+
from the graphicx package. Always specify the figure width as a multiple of
the line width as in the example below using .eps graphics
\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.eps}
\end{verbatim}
or % Apr 2009 addition
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
for .pdf graphics.
See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

A number of width problems arise when LaTeX cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

\subsubsection*{Author Contributions}
If you'd like to, you may include  a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.
\fi

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
% \section{Related Work}
\section{Related Work}
\paragraph{Model compression} 
Model compression is a classical way to improve inference efficiency by either reducing the number of model parameters or lowering the memory required to store them. Three widely used techniques—sparsity and pruning, quantization, and distillation—are central to this goal. Sparsity and pruning share the aim of reducing the number of effective parameters, but differ in approach: sparsity reduces individual weights to zero in an unstructured manner \citep{sun2023simple,xia2023flash,frantar2023sparsegpt}, while pruning takes a structured approach by removing entire components, such as neurons or filters, from the network \citep{xia2023sheared,gromov2024unreasonable}. Quantization reduces the memory footprint by lowering the precision of weights and activations, without changing the number of parameters \citep{dettmers2024qlora,dettmers2022gpt3,li2023loftq,kim2023squeezellm,frantar2022gptq,xiao2023smoothquant,yao2022zeroquant,liu2023llm,frantar2022gptq,zhang2018adaptive}. While sparsity, pruning, and quantization are usually applied after a certain amount of training, distillation is a data-centric methodology used during training. In distillation, a smaller student model is trained using both the original training data and the output (soft labels) of a larger teacher model, allowing the student model to retain much of the teacher’s performance while being more efficient \citep{hinton2015distilling,timiryasov2023baby,chen-etal-2024-learning-maximize}. These three categories represent the most classical methods for model compression, primarily aimed at improving inference efficiency. However, there are other compression methods closely related to our proposed techniques, which will be discussed in detail later.

\paragraph{Low rank approximation} 
Low-rank approximation, while distinct from the traditional model compression techniques discussed earlier, leverages the observation that much of the key information users care about in a neural network can be represented in a lower-dimensional subspace. By approximating large weight matrices with low-rank representations, both the number of parameters and computational costs are reduced. Many works such as \cite{li2023losparse,hsu2022language,hajimolahoseini2021compressing,tahaei2021kroneckerbert} focus on improving inference efficiency using this method, but it can also offer significant efficiency gains during training. LoRA (Low-Rank Adaptation) by \citet{hu2021lora} is the first work to introduce two small low-rank matrices, $A$ and $B$ attached to a frozen pre-trained weight matrix $W$, allowing for efficient fine-tuning with minimal memory usage. Since then, numerous variants have been developed to enhance this approach \citep{dettmers2022gpt3,sheng2023s, chen2023longlora,zhang2023adalora}. Our proposed methods are similarly inspired by low-rank approximation, but unlike other works that focus on decomposing the entire weight matrix, we use low-rank approximations to estimate the minimal differences between intermediate layers. This allows for maximal weight sharing, significantly reducing redundancy while maintaining performance.

\paragraph{Weight sharing}
Weight sharing is another powerful model compression technique that improves both training and inference efficiency by reducing the number of unique parameters in a neural network. Classical weight sharing involves using a common representation space across multiple tasks or domains, allowing models to generalize better while using fewer parameters \citep{liu2020microsoft,jiang2019multi,tars2018multi,fu2021learn}. Those embedding sharing architectures have been later adopted in Llama \citep{touvron2023llama} and OPT models \citep{zhang2022opt}. However the savings from embedding sharing diminished with the increasing model size, and therefore been disregarded in recent designs of LLMs. Recently, MobileLLM\citep{liu2024mobilellm} introduced the first approach to weight sharing between intermediate layers, enabling models to reuse learned representations across layers. This significantly reduces the parameter count while maintaining performance, making large models more feasible for resource-constrained environments. Our proposed methods are inspired by this concept, but further integrate weight sharing with low-rank approximation to achieve both computational efficiency and performance preservation during the fine-tuning stage.


%\paragraph{Speculative coding} 
% Hanxian can write this
\vspace{-1em}
\paragraph{Small models}
The definition of small models has evolved as advancements in deep learning architectures have significantly increased model sizes. Models that were previously considered large are now categorized as small relative to the current state-of-the-art. Commonly, models with fewer than 7 billion parameters (7B) are referred to as small models. Notably, prominent open-source language models under 7B parameters include Mistral 7B \citep{jiang2023mistral}; Phi-3 series \citep{abdin2024phi}; Gemma 2B \citep{team2023gemini}, Llama 3.2 series \citep{dubey2024llama}, TinyLlama\citep{zhang2024tinyllama}, MobileLLM\citep{liu2024mobilellm} and MiniCPM\citep{hu2024minicpm}. Despite their smaller size, these models remain challenging to deploy on edge devices due to their computational and memory requirements, necessitating further optimization and compression techniques for deployment in resource-constrained environments.

\section{Algorithm Details}

\subsection{Full Definition of Different Replacement Types}

Here we show the full table of different replacement types used in our main paper in Table~\ref{tab:app replacement schedule}. 
% Note that for T$_{\text{more}}$ we don't predict layer 12 just because in Figure~\ref{fig:side-by-side} (Right)

\begin{table}[t]
\caption{Different replacement types. $(\textbf{\textit{j}}:\mathcal{T}_j)$ denotes the reference layer \textbf{\textit{j}} and the target layers set $\mathcal{T}_\textbf{\textit{j}}$ which use \textbf{\textit{j}} to predict their parameters, and we briefly describe how each type replace the layers. Here the stored ratio $\tau$ is defined in Section~\ref{sec: preliminary}. For example in T$_\text{next}$, we need to store 18 layers (1,2,3,5,...29, 31,32), out of 32 layers in the original model, so $\tau = 56\%$.
}
\label{tab:app replacement schedule}
    \small
    \centering
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Type} & \textbf{Stored Ratio $\tau$} & \textbf{Description} and $[(\textbf{\textit{j}}:\mathcal{T}_j)]$ \\
        \midrule
        \multirow{3}{*}{\textbf{T$_{\text{next}}$}} & \multirow{3}{*}{56\%} & {{Replacing layer $2t$ with layer $2t-1$ for $t \in [2, 15]$}} 
        \\\cmidrule{3-3}
        & & \scriptsize{(\textbf{3}:4), (\textbf{5}:6), (\textbf{7}:8), (\textbf{9}:10), (\textbf{11}:12), (\textbf{13}:14), (\textbf{15}:16)}\\
        & &  \scriptsize{(\textbf{17}:18), (\textbf{19}:20), (\textbf{21}:22), (\textbf{23}:24), (\textbf{25}:26), (\textbf{27}:28), (\textbf{29}:30)}\\
        \midrule
        \multirow{2}{*}{\textbf{T$_{\text{next2}}$}} & \multirow{2}{*}{44\%}& {Replacing layer $3t+1, 3t+2$ with layer $3t$ for $t \in [1, 9]$}\\
        \cmidrule{3-3}
        & & \scriptsize{(\textbf{3}:4,5), (\textbf{6}:7,8), (\textbf{9}:10,11), (\textbf{12}:13,14), (\textbf{15}:16,17), (\textbf{18}:19,20), (\textbf{21}:22,23), (\textbf{24}:25,26), (\textbf{27}:28,29)}\\
        \midrule
         \multirow{2}{*}{\textbf{T$_{\text{back}}$}} & \multirow{2}{*}{38\%} & {Replacing more layers in the \textit{back} parts of the model.} \\
        \cmidrule{3-3}
        & & \scriptsize{(\textbf{3}:4), (\textbf{5}:6), (\textbf{7}:8), (\textbf{9}:10), (\textbf{11}:12), (\textbf{13}:14,15), (\textbf{16}:17,18,19,20,21,22), (\textbf{23}:24,25,26,27,28,29,30)}\\
        \midrule
        \multirow{2}{*}{\textbf{T$_{\text{front}}$}} & \multirow{2}{*}{38\%} & {Replacing more layers in the \textit{front} parts of the model.} \\
        \cmidrule{3-3}
        & & \scriptsize{(\textbf{3}:4,5,6,7,8,9,10), (\textbf{11}:12,13,14,15,16,17), (\textbf{18}:19,20), (\textbf{21}:22), (\textbf{23}:24), (\textbf{25}:26), (\textbf{27}:28), (\textbf{29}:30)}\\
        \midrule
        \multirow{2}{*}{\textbf{T$_{\text{more}}$}} & \multirow{2}{*}{25\%} & {Replace more aggressively, just store 8 layers} \\
        \cmidrule{3-3}
        & & \scriptsize{(\textbf{3}:4,5,6), (\textbf{7}:8,9,10,11), (\textbf{13}:14,15,16,17,18,19,20,21,22), (\textbf{23}:24,25,26,27,28,29,30)}\\
        \midrule
        \multirow{2}{*}{\textbf{T$_{\text{max}}$}} & \multirow{2}{*}{16\%} & {Replace more aggressively, just store 5 layers} \\
        \cmidrule{3-3}
        & & \scriptsize{(\textbf{2}:3,4,5,6,7,8,9,10), (\textbf{11}:12,13,14,15,16,17,18,19,20), (\textbf{21}:22,23,24,25,26,27,28,29,30,31)}\\
        \bottomrule
    \end{tabular}
    \vspace{-1em}
\end{table}


\section{Experiment Details}
% \subsection{Details about Tulu v2 Dataset}\label{app: tulu v2 detail}



\subsection{Details of Dataset}
\label{app sec: dataset}

We use several high-quality datasets in our in-distribution recovery tasks: 
(1) Arxiv-math~\citep{kenny2023arxivmath}: it includes 50k high-quality QA instruction data from the mathematical domain. 
(2) GPT4-Alpaca~\citep{peng2023alpacagpt4}: it contains 52k English instruction-following generated by GPT-4 using Alpaca prompts.
% for LLM finetuning.
(3) Databricks-Dolly~\citep{DatabricksBlog2023DollyV2}: This is an open-source dataset comprising 15k instruction-following records generated by Databricks employees.
% , covering various behavioral categories like brainstorming, classification, closed and open QA, generation, information extraction, and summarization.
(4) DialogSum~\citep{chen-etal-2021-dialogsum}: this is a dialogue summarization dataset consisting of 13.5k dialogues (12.5k for training) with corresponding manually labeled summaries and topics.
(5) OpenOrca~\citep{mukherjee2023openorca}, which is a collection of augmented FLAN Collection data~\citep{longpre2023flan}, containing about 1M GPT-4 completions and about 3.2M GPT-3.5 completions. We select a 50k subset of it for in-distribution tasks.

% We also use the subset from the large-scale filtered dataset, including:
In downstream evaluation parts, we also use 
(6) FineWeb-Edu~\citep{penedo2024finewebdatasetsdecantingweb,lozhkov2024fineweb-edu}, 
consists of 1.3T tokens of educational web pages filtered from the FineWeb dataset. We use its ``sample-10BT'' subset.
% which consists more than 15T tokens of cleaned and deduplicated english web data from CommonCrawl. 
(7) Tulu V2 Collection~\citep{ivison2023tuluv2}, which contains 326k instruction data mixed from FLAN~\citep{longpre2023flan}, Open Assistant 1~\citep{kopf2024openassistant}, 
% ShareGPT~\citep{chiang2023vicuna}, 
GPT4-Alpaca~\citep{peng2023alpacagpt4}, Code-Alpaca~\citep{codealpaca}, LIMA~\citep{zhou2024lima}, WizardLM~\citep{xu2024wizardlm} and Open-Orca~\citep{mukherjee2023openorca}. 



\subsection{Evaluation Tasks}\label{app: eval}
This section introduces the evaluation tasks used in our downstream evaluation part (Section~\ref{sec: exp downstream}).  
% Particularly, the performance on \textbf{LAMBADA} benchmark is measured by perplexity while others are measured by the accuracy of matching the ground truth answers. in lm-eval, the lambada is also evaluated by acc

\subsubsection{Basic Reasoning Tasks}
This class of tasks doesn't require too much commonsense or knowledge to solve problems, but needs the model to have good reasoning capability, especially the mathematical reasoning
\begin{itemize}
    \item \textbf{MathQA} \citep{amini2019mathqa}: This is a large-scale dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset.
    \item \textbf{GSM8k}~\citep{cobbe2021gsm8k}: This is a free-generation benchmark of grade school math problems aiming for evaluating multi-step (2-8 steps) mathematical reasoning capabilities. These problems are illustrated by natural language and require using four basic arithmetic operations to reach the final answer.
    \item \textbf{BBH-COT-FS} \citep{suzgun2022challenging}:  This is a free-generation benchmark consists a suite of 23 challenging BIG-Bench tasks \citep{srivastava2022imitation} which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater.  Here we use the chain-of-though with 3 shot version.
    % \item \textbf{ASDIV}:
    % \item \textbf{hendrycks math}
    \item \textbf{MuTual}\citep{mutual}: This is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data. 
    \item \textbf{QA4MRE}\citep{Peas2013QA4MRE2O}: This is a multi-choice benchmark used for long-text understanding and reasoning. Four different tasks have been organized during these years: Main Task, Processing Modality and Negation for Machine Reading, Machine Reading of Biomedical Texts about Alzheimer's disease, and Entrance Exam. 
\end{itemize}

\subsubsection{Knowledge-Memorization Tasks}
\begin{itemize}
    \item \textbf{SciQ} \citep{Welbl2017CrowdsourcingMC}: This contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided. 
    \item \textbf{BoolQ} \citep{clark2019boolq}: This is a question-answering benchmark for yes/no questions containing 15942 examples. These questions are naturally occurring – they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. 
    \item \textbf{CommonsenseQA} \citep{talmor-etal-2019-commonsenseqa}: This is a multiple-choice question-answering dataset that requires different types of commonsense knowledge to predict the correct answers. It contains 12,102 questions with one correct answer and four distractor answers. 
    \item \textbf{MedMCQA} \citep{pmlr-v174-pal22a} This is a new large-scale, multiple-choice question-answering dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS \& NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which require a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects \& topics. A detailed explanation of the solution, along with the above information, is provided in this study. 
    \item \textbf{TriviaQA} \citep{JoshiTriviaQA2017}:  This is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high-quality distant supervision for answering the questions. This dataset can be used for both retrieval-augmented models to test the models' knowledge retrieval ability and the usual LLM to test the knowledge memorization on the model itself.
\end{itemize}
\subsubsection{Knowledge-Memorization + Commonsense Reasoning}
\begin{itemize}
    \item \textbf{PIQA} \citep{Bisk2020}: This is a multi-choice physical commonsense reasoning and a corresponding benchmark dataset. PIQA was designed to investigate the physical knowledge of existing models.  
    \item \textbf{WinoGrande} \citep{sakaguchi2019winogrande}: This is a collection of 44k multi-choice problems, inspired by Winograd Schema Challenge (Levesque, Davis, and Morgenstern 2011), but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning. 
    \item \textbf{ARC} \citep{Clark2018ThinkYH}: This is a subset of  ARC dataset with 2,590 “hard” questions (those that both a retrieval and a co-occurrence method fail to answer correctly). The ARC dataset contains text-only, English language exam multi-choice questions that span several grade levels as indicated in the files. 

\end{itemize}

\subsubsection{Others}
\begin{itemize}
    \item \textbf{LAMBADA} \citep{lambda}: This is a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. 
\end{itemize}

% You may include other additional sections here.
% \section{Experi}

\subsection{Experimental setup on mobile
}\label{app:mobile}
We evaluated the model run time latency on mobile. For the models to evaluate, we tested: (1) the original Llama2-7b\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b}} and (2) a simplified version of SHARP (T$_{\text{next}}$) where we removed the LoRA parameters. We only store the reference layers, and call those layers multiple times for the target layers in model forwarding.

We first exported the models using ExecuTorch v0.3.0\footnote{\url{https://github.com/pytorch/executorch}} with the same export configurations (using kv cache, using 8-bit dynamic quantization and 4-bit weight quantization\footnote{\url{https://github.com/pytorch/executorch/blob/main/examples/models/llama2/export_llama_lib.py}}, for XNNPack backend). We tested the model loading and initialization time, as well as the model forwarding time using a benchmark app\footnote{\url{https://github.com/pytorch/executorch/tree/main/extension/apple/Benchmark}} on Xcode (version 16.0). We ran Xcode on a MacBook Air with Apple M2 chip, 16GB memory with MacOS Sonoma 14.6.1, and wire-connected it to an iPhone 16 Pro with iOS 18.1 with 8GB memory, and ran the benchmark app on the phone.

% \color{blue}
% \section{Rebuttal Material}


\subsection{Details About Structural Pruning Baselines}\label{app sec: baseline details}

We evaluate the following baseline in the in-distribution tasks.

\begin{itemize}
    \item LayerPruning~\citep{gromov2024unreasonable}, which calculates the angle distance between different layers, and then prunes a group of consecutive layers in the model such that the first and last layers in these continuous layers have small angle distance. For LLaMA2-7B, we prune the MLP layers of the 17th to 30th layers as recommended in original paper. We keep the number of removing layers the same for fair comparison and use default settings.
    \item Adapted version of LayerPruning, where we use the replacement strategy $\text{T}_{\text{next}}$ (replacing layer $2t$ with layer $2t-1$ for $t$ in [2,15]) as mentioned in Table~\ref{tab:replacement schedule}. We also use the same LoRA ranks as SHARP. Notably, this baseline is equivalent to directly pruning the corresponding layers in SHARP, rather than reusing them.
    \item LLM-Pruner~\citep{ma2023llm_pruner},  another advanced structural pruning algorithm. It uses gradient information to divide the parameters into several groups based on their relevance, and then prune the coupled parameters that contribute minimally to its overall performance.
    Here for fair comparison, we also let LLM-Pruner remove about 50\% of the MLP layers except the first two and last one layers, and use the same LoRA ranks for recovering.
    Besides, we find that LLM-Pruner is not that stable in the in-distribution recovery tasks, even when we try the default LoRA rank and prune both the self-attention layer and MLPs together. So we report the best result in the entire finetuning process before it becomes unstable.
\end{itemize}

Here we ensure the amount of LoRA recovery components are the same for fair comparison of different structural pruning baselines.
% Another related baseline


\section{Additional Experiments and Discussions}
\label{app sec: exp}


\begin{table*}[t]
\small
\centering
\caption{
Explore if SHARP overfits the finetuning data used by SHARP for recovering performance. We consider the in-distribution task, and focus on the model that is only finetuned on the GPT4-Alpaca. ''Using in-distribution data'' means choosing the training data that is from the same dataset as the test task.
}
\label{tab: overfit}
\begin{tabularx}{\textwidth}{@{}l@{\hskip 5pt}|c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 5pt}c@{}}
\toprule
\textbf{Models} & \textbf{Arxiv-math} & \textbf{DialogSum} & \textbf{GPT4-Alpaca} & \textbf{Dolly} & \textbf{OpenOrca} \\
\midrule
Direct Sharing & 2171.3 & 801.7 & 20662.1 & 7221.7 & 12108.5 \\
\midrule
SHARP (w/o f.t.) using GPT4-Alpaca & 7.4 & 6.4 & 4.2 & 8.5 & 10.1 \\
SHARP (w/o f.t.) using in-distribution data & 4.8 & 5.3 & 4.2 & 7.2 & 8.4 \\
\midrule
SHARP using GPT4-Alpaca & 4.6 & 4.7 & 2.8 & 5.9 & 6.6 \\
\textbf{SHARP using in-distribution data} & \textbf{3.2} & \textbf{3.8} & \textbf{2.8} & \textbf{4.7} & \textbf{4.3} \\
\bottomrule
\end{tabularx}
% \vspace{-1.5em}
\end{table*}



\subsection{Consideration of Overfitting}\label{app sec: overfit}
To claim that our SHARP algorithm, doesn’t overfit to the recovering dataset, we apply SHARP with a fixed dataset (GPT4-Alpaca), and then evaluate its performance in other tasks. The result is shown in Table~\ref{tab: overfit}.

We can see 
\textbf{SHARP (with or without fine-tuning) can consistently recover the perplexity on every task}, rather than just recovering model performance on related tasks like Arxiv-math. This supports our claim that SHARP does not overfit to the recovery dataset.
Especially, we observe that after the Single-Layer-Warmup (SLW) stage on GPT4-Alpaca, the model perplexity has recovered a lot compared to the vanilla Direct Sharing baseline on all tasks.
Besides, for the complete 2-step SHARP, the gap between using in-distribution and GPT4-Alpaca data is not that large, implying that our method should have a good generalization in utilizing different recovering data.


\begin{table*}[t]
\small
\centering
% \vspace{-1.4em}
\caption{
In-distribution tasks on LLaMA3.2-3B model. 
% Here we choose T$_\text{next}$ as the replacement strategy, which reuses about half of the MLP layers as illustrated in Table~\ref{tab:app replacement schedule}.
} 
\label{tab: llama3_2}
\centering
\begin{tabularx}{\textwidth}{@{}l@{\hskip 5pt}|c@{\hskip 5pt}|c@{\hskip 5pt}|c@{\hskip 5pt}c@{\hskip 5pt}c@{\hskip 3pt}c@{\hskip 3pt}c@{}}
\toprule
\textbf{Models} & \textbf{Type}  & \textbf{$s$} & \textbf{Arxiv-math} & \textbf{DialogSum} & \textbf{GPT4-Alpaca} & \textbf{Databricks-Dolly} & 
% \textbf{Fineweb (10k)} & 
\textbf{OpenOrca}\\
\midrule
Original &  & 100\% & 3.4 & 4.6 &	3.2 &	5.2 &	6.0\\
\bottomrule
\toprule
Direct Sharing & T$_{\text{next}}$   & 62\%
&  1071.8&  578.3&  1000.0&  1083.7&  1690.7 \\
\textbf{SHARP} (w/o f.t.) & T$_{\text{next}}$   & 62\%& 7.2& 7.3& 7.1& 12.1& 12.5\\
\textbf{SHARP} & T$_{\text{next}}$  & 62\%& 
\textbf{4.1}	& \textbf{5.2}	& \textbf{4.1}	& \textbf{7.6}	& \textbf{6.0}\\
\bottomrule
\end{tabularx}
% \vspace{-1.5em}
\end{table*}


\begin{table*}[t]
\small
\centering
\caption{
Adding LoRA to the entire model. Full-LoRA means attaching the LoRA adapters to all the layers, no matter the reference layer or the target layer.
}
\label{supp tab: full lora}
\begin{tabularx}{0.75\textwidth}{@{}l|c|c|c@{}}
\toprule
\textbf{Models} & \textbf{Arxiv-math} & \textbf{DialogSum} & \textbf{Databricks-Dolly} \\
\midrule
SHARP (w/o f.t.) & 4.8 & 5.3 & 7.2 \\
SHARP & 3.2 & 3.8 & 4.7 \\
SHARP Full-LoRA (w/o f.t.) & 4.7 & 5.3 & 6.1 \\
\textbf{SHARP Full-LoRA} & \textbf{3.2} & \textbf{3.8} & \textbf{4.7} \\
\bottomrule
\end{tabularx}
% \vspace{-1.5em}
\end{table*}

\subsection{Experiment on Advanced Small Model}
\label{app se llama3_2}

To confirm that SHARP is also applicable to other models, we try to apply it on LLaMA3.2-3B model, and obtain the result in Table~\ref{tab: llama3_2}. 
We can see that the perplexity between the original model and SHARP is still close, supporting the generality of our method. 



\subsection{Adding LoRA to All Layers}
\label{app sec: full lora}
In general, attaching the whole LoRA adapters to the entire model is more convenient~\citep{peft}. We apply LoRA to the replaced layer in the in-distribution tasks just for clearer illustration. If we want to attach LoRA adapters to the entire models in SHARP, we can first run SLW on the LoRA components which are assigned to the target (replaced) layers,
% (since the LoRA added to the reference layer can be treated as zero if the reference layer can almost recover the target layer),
and then fine-tune all the LoRAs together in the second SFT stage. Actually we did this in the downstream recovery part (Sec~\ref{sec: exp downstream}) following open-instruct pipeline~\citep{ivison2023tuluv2} for convenience.

Nevertheless, this process does not result in a large difference. We show the result of applying LoRA to all layers in Table~\ref{supp tab: full lora}. Although more LoRA components bring difference for SLW stage (SHARP w/o fine-tuning), they behave the same in complete SHARP. 




\subsection{Discusssion about the Stability in SLW Stage}
In the main paper, we show the importance of Single-Warmup Stage in recovering model performance, and one concern may be that if there is any risk of unstability in SLW stage. Here, we simply summarize the reasons for \textbf{why SLW stage is relatively stable}.

\begin{enumerate}
    \item \textbf{The similarity between adjacent layers.} SLW just simply finds some recovery components to mimic the output between adjacent layers, and as shown in Figure~\ref{fig:side-by-side}, adjacent layers are quite similar. This phenomenon has been verified on larger models like LLaMA2-70B or other models like Mistral-7B in the previous works~\citep{ma2023llm_pruner,liu2023deja}.
    \item \textbf{The simplicity of optimization loss.} The SLW stage just utilizes simple standard L2 regression loss on the output of adjacent single layers, whose optimization is empirically observed as quite stable. 
    \item \textbf{Computation Efficiency.} 
    The computation cost of SLW is also significantly smaller than the SFT stage since we just do independent single-layer fittings than processing the entire large model, and thus it can be quite efficient and stable.
    In Table~\ref{tab: different setting}, we just used about 10\% of the data for the SLW stage and it’s already enough for the convergence of all the layers’ SLW stages.
    \item  \textbf{Robustness of recovery dataset.} The SLW stage is even robust to the choice of recovery dataset. In Section~\ref{app sec: overfit}, we show that onlyusing GPT4-Alpaca as recovery data can still recover model perplexity on other tasks. This also shows the robustness of the SLW stage.
\end{enumerate}






\iffalse
\begin{table*}[ht]
\color{blue}
\small
\centering
\caption{\color{blue} The downstream performance of the models utilizing 4-bit quantization. (W.G.: WinoGrande, Med.: MedMCQA, Com.: ComSenQA)}
\label{app tab:4bit downstream}

\begin{tabularx}{\textwidth}{l|cccccccc}
\toprule
\textbf{Models}  & \textbf{SciQ} & \textbf{BoolQ} & \textbf{PIQA} & \textbf{ARC-E} & \textbf{ARC-C} & \textbf{W.G.} & \textbf{Med.} \\
\midrule
Direct Sharing  & 22.7 & 49.1 & 56.7 & 26.8 & 22.0 & 50.6 & 22.4 \\
SHARP (w/o finetuning) & 87.8 & 66.1 & 63.0 & 54.8 & 26.8 & 57.8 & 30.5 \\
SHARP & 92.6 & 76.0 & 72.6 & 65.8 & 34.7 & 62.4 & 31.4 \\
\midrule
\textbf{SHARP + 4-bit quan.} & 92.1 & 74.7 & 71.9 & 65.1 & 35.0 & 62.2 & 30.7 \\
\bottomrule
\toprule
& \textbf{BBH} & \textbf{GSM8k} & \textbf{MAQ.} & \textbf{MUT.} & \textbf{LAM.} & \textbf{Com.} & \textbf{QA4MRE} & \textbf{Avg.}\\
\midrule
Direct Sharing  & 0.0 & 0.0 & 20.7 & 57.5 & 8.1 & 20.0 &16.6 & 26.7\\
SHARP (w/o finetuning)  & 24.8 & 1.6 & 22.1 & 59.8 & 35.0 & 20.0 & 30.2 & 41.5\\
SHARP & 29.8 & 3.6 & 24.7 & 68.3 & 58.3 & 44.1 & 38.3 & 50.2\\
\midrule
\textbf{SHARP + 4-bit quan.} & 27.9 & 2.8 & 24.1 & 68.1 & 55.6 & 37.4 & 39.3 & 49.1 \\
\bottomrule
\end{tabularx}
\end{table*}
\fi





\end{document}
