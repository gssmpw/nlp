[
  {
    "index": 0,
    "papers": [
      {
        "key": "sun2023simple",
        "author": "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico",
        "title": "A simple and effective pruning approach for large language models"
      },
      {
        "key": "xia2023flash",
        "author": "Xia, Haojun and Zheng, Zhen and Li, Yuchao and Zhuang, Donglin and Zhou, Zhongzhu and Qiu, Xiafei and Li, Yong and Lin, Wei and Song, Shuaiwen Leon",
        "title": "Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity"
      },
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "xia2023sheared",
        "author": "Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi",
        "title": "Sheared llama: Accelerating language model pre-training via structured pruning"
      },
      {
        "key": "gromov2024unreasonable",
        "author": "Gromov, Andrey and Tirumala, Kushal and Shapourian, Hassan and Glorioso, Paolo and Roberts, Daniel A",
        "title": "The unreasonable ineffectiveness of the deeper layers"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dettmers2024qlora",
        "author": "Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke",
        "title": "Qlora: Efficient finetuning of quantized llms"
      },
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      },
      {
        "key": "li2023loftq",
        "author": "Li, Yixiao and Yu, Yifan and Liang, Chen and He, Pengcheng and Karampatziakis, Nikos and Chen, Weizhu and Zhao, Tuo",
        "title": "Loftq: Lora-fine-tuning-aware quantization for large language models"
      },
      {
        "key": "kim2023squeezellm",
        "author": "Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt",
        "title": "Squeezellm: Dense-and-sparse quantization"
      },
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      },
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      },
      {
        "key": "yao2022zeroquant",
        "author": "Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong",
        "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers"
      },
      {
        "key": "liu2023llm",
        "author": "Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas",
        "title": "Llm-qat: Data-free quantization aware training for large language models"
      },
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      },
      {
        "key": "zhang2018adaptive",
        "author": "Zhang, Wentai and Huang, Hanxian and Zhang, Jiaxi and Jiang, Ming and Luo, Guojie",
        "title": "Adaptive-Precision Framework for SGD Using Deep Q-Learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hinton2015distilling",
        "author": "Hinton, Geoffrey",
        "title": "Distilling the Knowledge in a Neural Network"
      },
      {
        "key": "timiryasov2023baby",
        "author": "Timiryasov, Inar and Tastet, Jean-Loup",
        "title": "Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty"
      },
      {
        "key": "chen-etal-2024-learning-maximize",
        "author": "Chen, Xin  and\nHuang, Hanxian  and\nGao, Yanjun  and\nWang, Yi  and\nZhao, Jishen  and\nDing, Ke",
        "title": "Learning to Maximize Mutual Information for Chain-of-Thought Distillation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023losparse",
        "author": "Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo",
        "title": "Losparse: Structured compression of large language models based on low-rank and sparse approximation"
      },
      {
        "key": "hsu2022language",
        "author": "Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia",
        "title": "Language model compression with weighted low-rank factorization"
      },
      {
        "key": "hajimolahoseini2021compressing",
        "author": "Hajimolahoseini, Habib and Rezagholizadeh, Mehdi and Partovinia, Vahid and Tahaei, Marzieh and Awad, Omar Mohamed and Liu, Yang",
        "title": "Compressing pre-trained language models using progressive low rank decomposition"
      },
      {
        "key": "tahaei2021kroneckerbert",
        "author": "Tahaei, Marzieh S and Charlaix, Ella and Nia, Vahid Partovi and Ghodsi, Ali and Rezagholizadeh, Mehdi",
        "title": "Kroneckerbert: Learning kronecker decomposition for pre-trained language models via knowledge distillation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      },
      {
        "key": "sheng2023s",
        "author": "Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman and Lee, Nicholas and Yang, Shuo and Chou, Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and others",
        "title": "S-lora: Serving thousands of concurrent lora adapters"
      },
      {
        "key": "chen2023longlora",
        "author": "Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya",
        "title": "Longlora: Efficient fine-tuning of long-context large language models"
      },
      {
        "key": "zhang2023adalora",
        "author": "Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo",
        "title": "AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2020microsoft",
        "author": "Liu, Xiaodong and Wang, Yu and Ji, Jianshu and Cheng, Hao and Zhu, Xueyun and Awa, Emmanuel and He, Pengcheng and Chen, Weizhu and Poon, Hoifung and Cao, Guihong and others",
        "title": "The microsoft toolkit of multi-task deep neural networks for natural language understanding"
      },
      {
        "key": "jiang2019multi",
        "author": "Jiang, Haoming and Liang, Chen and Wang, Chong and Zhao, Tuo",
        "title": "Multi-domain neural machine translation with word-level adaptive layer-wise domain mixing"
      },
      {
        "key": "tars2018multi",
        "author": "Tars, Sander and Fishel, Mark",
        "title": "Multi-domain neural machine translation"
      },
      {
        "key": "fu2021learn",
        "author": "Fu, Cheng and Huang, Hanxian and Chen, Xinyun and Tian, Yuandong and Zhao, Jishen",
        "title": "Learn-to-share: A hardware-friendly transfer learning framework exploiting computation and parameter sharing"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2022opt",
        "author": "Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others",
        "title": "Opt: Open pre-trained transformer language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024mobilellm",
        "author": "Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others",
        "title": "Mobilellm: Optimizing sub-billion parameter language models for on-device use cases"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "jiang2023mistral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others",
        "title": "Mistral 7B"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "abdin2024phi",
        "author": "Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others",
        "title": "Phi-3 technical report: A highly capable language model locally on your phone"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2024tinyllama",
        "author": "Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei",
        "title": "Tinyllama: An open-source small language model"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2024mobilellm",
        "author": "Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others",
        "title": "Mobilellm: Optimizing sub-billion parameter language models for on-device use cases"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "hu2024minicpm",
        "author": "Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others",
        "title": "Minicpm: Unveiling the potential of small language models with scalable training strategies"
      }
    ]
  }
]