\section{Related Work}
\paragraph{Model compression} 
Model compression is a classical way to improve inference efficiency by either reducing the number of model parameters or lowering the memory required to store them. Three widely used techniques—sparsity and pruning, quantization, and distillation—are central to this goal. Sparsity and pruning share the aim of reducing the number of effective parameters, but differ in approach: sparsity reduces individual weights to zero in an unstructured manner \citep{sun2023simple,xia2023flash,frantar2023sparsegpt}, while pruning takes a structured approach by removing entire components, such as neurons or filters, from the network \citep{xia2023sheared,gromov2024unreasonable}. Quantization reduces the memory footprint by lowering the precision of weights and activations, without changing the number of parameters \citep{dettmers2024qlora,dettmers2022gpt3,li2023loftq,kim2023squeezellm,frantar2022gptq,xiao2023smoothquant,yao2022zeroquant,liu2023llm,frantar2022gptq,zhang2018adaptive}. While sparsity, pruning, and quantization are usually applied after a certain amount of training, distillation is a data-centric methodology used during training. In distillation, a smaller student model is trained using both the original training data and the output (soft labels) of a larger teacher model, allowing the student model to retain much of the teacher’s performance while being more efficient \citep{hinton2015distilling,timiryasov2023baby,chen-etal-2024-learning-maximize}. These three categories represent the most classical methods for model compression, primarily aimed at improving inference efficiency. However, there are other compression methods closely related to our proposed techniques, which will be discussed in detail later.

\paragraph{Low rank approximation} 
Low-rank approximation, while distinct from the traditional model compression techniques discussed earlier, leverages the observation that much of the key information users care about in a neural network can be represented in a lower-dimensional subspace. By approximating large weight matrices with low-rank representations, both the number of parameters and computational costs are reduced. Many works such as \cite{li2023losparse,hsu2022language,hajimolahoseini2021compressing,tahaei2021kroneckerbert} focus on improving inference efficiency using this method, but it can also offer significant efficiency gains during training. LoRA (Low-Rank Adaptation) by \citet{hu2021lora} is the first work to introduce two small low-rank matrices, $A$ and $B$ attached to a frozen pre-trained weight matrix $W$, allowing for efficient fine-tuning with minimal memory usage. Since then, numerous variants have been developed to enhance this approach \citep{dettmers2022gpt3,sheng2023s, chen2023longlora,zhang2023adalora}. Our proposed methods are similarly inspired by low-rank approximation, but unlike other works that focus on decomposing the entire weight matrix, we use low-rank approximations to estimate the minimal differences between intermediate layers. This allows for maximal weight sharing, significantly reducing redundancy while maintaining performance.

\paragraph{Weight sharing}
Weight sharing is another powerful model compression technique that improves both training and inference efficiency by reducing the number of unique parameters in a neural network. Classical weight sharing involves using a common representation space across multiple tasks or domains, allowing models to generalize better while using fewer parameters \citep{liu2020microsoft,jiang2019multi,tars2018multi,fu2021learn}. Those embedding sharing architectures have been later adopted in Llama \citep{touvron2023llama} and OPT models \citep{zhang2022opt}. However the savings from embedding sharing diminished with the increasing model size, and therefore been disregarded in recent designs of LLMs. Recently, MobileLLM\citep{liu2024mobilellm} introduced the first approach to weight sharing between intermediate layers, enabling models to reuse learned representations across layers. This significantly reduces the parameter count while maintaining performance, making large models more feasible for resource-constrained environments. Our proposed methods are inspired by this concept, but further integrate weight sharing with low-rank approximation to achieve both computational efficiency and performance preservation during the fine-tuning stage.


%\paragraph{Speculative coding} 
% Hanxian can write this
\vspace{-1em}
\paragraph{Small models}
The definition of small models has evolved as advancements in deep learning architectures have significantly increased model sizes. Models that were previously considered large are now categorized as small relative to the current state-of-the-art. Commonly, models with fewer than 7 billion parameters (7B) are referred to as small models. Notably, prominent open-source language models under 7B parameters include Mistral 7B \citep{jiang2023mistral}; Phi-3 series \citep{abdin2024phi}; Gemma 2B \citep{team2023gemini}, Llama 3.2 series \citep{dubey2024llama}, TinyLlama\citep{zhang2024tinyllama}, MobileLLM\citep{liu2024mobilellm} and MiniCPM\citep{hu2024minicpm}. Despite their smaller size, these models remain challenging to deploy on edge devices due to their computational and memory requirements, necessitating further optimization and compression techniques for deployment in resource-constrained environments.