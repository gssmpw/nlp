\documentclass[journal]{IEEEtran}

\ifCLASSINFOpdf
\else
   \usepackage[dvips]{graphicx}
\fi
\usepackage{url}

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl}
\definecolor{Gray}{gray}{0.9}
\usepackage{cite}           % compressed citation lists
\usepackage{amsmath}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{multirow}
\usepackage[caption=false]{subfig}
\usepackage{hyperref}
\definecolor{blue}{RGB}{0,0,255}

\begin{document}

\title{mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition}

\author{Andrew Rouditchenko,
        Samuel Thomas,~\IEEEmembership{Senior Member, IEEE},
        Hilde Kuehne,~\IEEEmembership{Member, IEEE},
        Rogerio Feris,~\IEEEmembership{Senior Member, IEEE},
        and James Glass,~\IEEEmembership{Fellow, IEEE}
\thanks{Code at \href{https://github.com/roudimit/whisper-flamingo}%{\texttt{\color{blue}{https://github.com/roudimit/whisper-flamingo}}}. 
{\texttt{https://github.com/roudimit/whisper-flamingo}}.
Andrew Rouditchenko and James Glass are with MIT, USA (e-mail: \{roudi, glass\}@mit.edu).
Samuel Thomas and Rogerio
Feris are with MIT-IBM Watson AI Lab, USA.
Hilde Kuehne is with University of Tuebingen, DE.
% A.R. and J.G. are with MIT, USA (e-mail: \{roudi, glass\}@mit.edu).
% S.T. and R.F. are with MIT-IBM Watson AI Lab, USA.
% H.K. is with University of Tuebingen, DE.
We thank Videet Mehta for help with the demo and Tatiana Likhomanenko and Saurabhchand Bhati for helpful discussions. This research was supported by MIT-IBM Watson AI Lab and an NDSEG Fellowship to A.R.}}

% \markboth{Journal of \LaTeX\ Class Files, Vol. 14, No. 8, August 2015}
% \markboth{IEEE Signal Processing Letters}
\markboth{Preprint}
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
\maketitle

\begin{abstract}
Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio and can improve performance in noise, but most methods are trained only on English data.
One limitation is the lack of large-scale multilingual video data, which makes it hard hard to train models from scratch.
In this work, we propose mWhisper-Flamingo for multilingual AVSR which combines the strengths of a pre-trained audio model (Whisper) and video model (AV-HuBERT).
To enable better multi-modal integration and improve the noisy multilingual performance, we introduce decoder modality dropout where the model is trained both on paired audio-visual inputs and separate audio/visual inputs.
mWhisper-Flamingo achieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages.
Audio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on all languages in noisy conditions.
\end{abstract}

\begin{IEEEkeywords}
audio-visual speech recognition, multilingual
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
Automatic Speech Recognition (ASR) has seen great progress thanks to large-scale training~\cite{radford2023robust,puvvada24_interspeech,rouditchenko23_interspeech,shi23g_interspeech}, but models still struggle with background noise~\cite{gong23d_interspeech,hu2024large}.
To improve performance, Audio-Visual Speech Recognition (AVSR) models combine lip-based video with audio inputs~\cite{afouras2018adeep,petridis2018audio,petridis2018end,xu2020discriminative,ma2021end,serdyuk22_interspeech,shi22_interspeech,ma2023auto,burchi2023audio,cappellazzo2024large}.
In particular, Whisper-Flamingo~\cite{rouditchenko24_interspeech} proposed an audio-visual adaptation of Whisper~\cite{radford2023robust}, a pre-trained ASR model, and showed significant improvements in noise robustness compared to the original audio-only model.
Whisper-Flamingo was motivated by the limitation of previous AVSR systems which often lack large-scale transcribed videos and face difficulty training models from scratch on only a few hundred hours of data.
To overcome this, Whisper-Flamingo combines the strength of Whisper's audio encoder and text decoder trained on 680k hours with AV-HuBERT~\cite{shi2022learning}, a pre-trained lip-reading visual encoder.
The model integrates lip-based visual features from AV-HuBERT into Whisper's decoder and achieves State-of-the-Art (SOTA) performance on English AVSR.

In this work, we propose mWhisper-Flamingo: a novel multilingual extension of Whisper-Flamingo which achieves SOTA performance on multilingual AVSR.
mWhisper-Flamingo combines Whisper's strong multilingual audio encoder and text decoder with a new AV-HuBERT visual encoder pre-trained on multilingual videos~\cite{kim_2024}.
Unlike the previous Whisper-Flamingo model which could only take English videos as input, the new mWhisper-Flamingo model can handle videos in 9 different languages (including English).
However, we show that Whisper-Flamingo's default training process applied to multilingual videos yields poor noisy multilingual AVSR performance, despite achieving good English performance.
To address this, we propose a novel decoder modality dropout technique by training the model both on paired audio-visual inputs and separate audio/video inputs.
We show this to be key for good noisy multilingual AVSR performance with a thorough analysis and ablation study.

We test our method across 9 languages in clean and noisy conditions on the MuAViC dataset~\cite{anwar23_interspeech}.
In clean audio conditions, mWhisper-Flamingo outperforms previous audio-visual methods and achieves SOTA across multilingual languages.
In noisy conditions, mWhisper-Flamingo consistently outperforms the audio-only Whisper model on 6 different noise types and 5 levels of noise.
We release our code and models.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/mWhisper-Flamingo_fig.pdf}
    \caption{In mWhisper-Flamingo, the AV-HuBERT and Whisper encoders extract visual and audio features from multilingual videos. 
    Separate cross attention layers in Whisper's decoder attend to the visual and audio features.
    Decoder modality dropout randomly replaces the audio or video features by 0, forcing the decoder to train on video-only and audio-only inputs.}
    \label{fig:overview}
    \vspace{-0.4cm}
\end{figure}

\section{Method}
Our method builds upon Whisper-Flamingo~\cite{rouditchenko24_interspeech}, an audio-visual extension of Whisper~\cite{radford2023robust}.
Whisper-Flamingo adds new gated cross-attention layers (originally proposed for the Flamingo vision-language model~\cite{alayrac2022flamingo}) into each of Whisperâ€™s decoder blocks which attend to the visual features from the AV-HuBERT visual encoder~\cite{shi2022learning}.
The layers are initialized as identity functions and initially ignore the visual input, but the weights are adjusted to integrate the visual features for AVSR during training on audio-visual inputs.

Whisper-Flamingo uses two-stages of training.
First, all of Whisper's parameters are fine-tuned on noisy audio inputs, enhancing its domain-specific performance and noise robustness. 
Second, the gated cross-attention layers are initialized and trained on audio-visual data. 
During this stage, all of Whisper's and AV-HuBERT's parameters are frozen to preserve the pre-trained knowledge and to facilitate multi-modal integration.

mWhisper-Flamingo inherits Whisper-Flamingo's architecture, except the English AV-HuBERT~\cite{shi2022learning} is replaced with a version pre-trained on multilingual videos~\cite{kim_2024}.
We propose to train mWhisper-Flamingo on multilingual videos jointly with English, similar to Whisper's training process.
However, using Whisper-Flamingo's default training process on multilingual videos yielded only minor improvements in noisy multilingual AVSR performance, despite achieving significant improvements for English (see Section~\ref{sec:analysis}). 
We suspect this is due to English having on average 13.6x more data than other languages in our dataset.
To address this, we introduce decoder modality dropout by training the model both on paired audio-visual inputs and separate audio/video inputs.


Dropout, originally proposed to prevent overfitting in neural networks~\cite{hinton2012improving}, was extended to multi-modal learning as modality dropout~\cite{neverova2015moddrop}. 
Modality dropout randomly drops input modalities during training to better capture cross-modality correlations while preserving unique contributions of specific modalities.
Modality dropout has been used in AVSR~\cite{makino2019recurrent,shi2022learning,hsu2022u,lian2023av,rouditchenko2023av} to prevent models from over-relying on the audio modality, which is typically easier to transcribe than the lip-based visual modality.
However, current methods mainly use early-fusion where audio and visual features are combined and used as input into a single Transformer~\cite{vaswani2017attention} encoder.
During modality dropout, the Transformer encoder must handle the incomplete input stream and output an embedding sequence which is used as input to the decoder's cross-attention layers.
In contrast, Whisper-Flamingo uses late-fusion where separate Transformer encoders process audio and visual features independently.
Each encoder outputs an embedding sequence used as input to separate audio and video cross attention layers in the decoder, where the modality fusion occurs.
During modality dropout, one of the audio and video embedding sequences is replaced by a 0-vector, forcing the decoder to handle the incomplete input stream in each of the cross-attention layers corresponding to the missing modality.
This enables the encoders to remain specialized on their specific modalities while the decoder learns better multi-modal integration.
This process is shown in Figure~\ref{fig:overview}.
Note that cross-attention with a 0-vector results in a 0-vector, but the output from the layer incorporates the bias in the linear transformations.

Our method is conceptually similar to LayerDrop~\cite{Fan2020Reducing}, which randomly drops Transformer layers for model pruning on text-based tasks. 
However, our focus lies in enhancing the decoder's ability to learn better multi-modal integration and to handle inputs where one modality may be unreliable.

We define the probabilities of using audio-visual (\( p_{AV} \)), audio-only (\( p_{A} \)), and visual-only (\( p_{V} \)) inputs during training as follows. 
If audio-visual inputs are selected, both modalities are used.  
If audio-only inputs are selected, visual features are zeroed out at the decoder.  
If video-only inputs are selected, audio features are zeroed out at the decoder.  
In Section~\ref{sec:analysis}, we show results using different probabilities.
The setting \( p_{AV} = 0.5,\ p_{A} = 0,\ p_{V} = 0.5 \) performs the best on noisy multilingual AVSR, so we use it for training our main models.

\section{Experiments}

\subsection{Experimental Setup}

We use the MuAViC~\cite{anwar23_interspeech} dataset of 1,141h of videos in 9 languages.
The dataset is based on the LRS3 English video dataset~\cite{afouras2018lrs3} and mTedX dataset~\cite{salesky21_interspeech}.
The hours of video per language are: 
English (En): 433, Arabic (Ar): 16, German (De): 10, Greek (El): 25, Spanish (Es): 178, French (Fr): 176, Italian (It): 101, Portuguese (Pt): 153 and Russian (Ru): 49.

We use Whisper~\cite{radford2023robust} small, medium, and large-v2 with 244M, 769M, and 1.55B parameters.
We fine-tuned Whisper small and medium on 4 A6000 GPUs with 48GB memory, but could not fine-tune Whisper large due to the GPU memory limits.
We use AV-HuBERT pre-trained on unlabeled multilingual videos~\cite{kim_2024} with 325M parameters.
We selected this model instead of other English-only lip-reading models~\cite{haliassos2023jointly,haliassos2024braven,haliassos2024unified} due to its superiority on multilingual videos~\cite{ma2022visual,zinonos2023learning,kim2023lip,yeo2024visual}.
Different to Whisper-Flamingo, we also fine-tune its parameters.
The gated cross attention layers add 82M and 296M for the small and medium models, bringing the total to 651M and 1.39B parameters for mWhisper-Flamingo small / medium. 
The other dataloading details and hyperparameters closely follow Whisper-Flamingo~\cite{rouditchenko2023av}.
Spectrogram frames are used as input to Whisper at 100 Hz while grayscale videos are used as input to AV-HuBERT at 25 fps.
The videos are cropped on the lips using Dlib~\cite{king2009dlib} and are aligned to a reference mean face~\cite{martinez2020lipreading}.
Models were trained using PyTorch~\cite{paszke2019pytorch} and PyTorch Lightning~\cite{Falcon_PyTorch_Lightning_2019} with the AdamW optimizer~\cite{loshchilov2018decoupled}.

During training, we randomly add noise to the audio with a Signal-to-Noise Ratio (SNR) of 0 dB. 
Based on prior work~\cite{shi22_interspeech,anwar23_interspeech}, ``natural'', ``music'' and ``babble'' noise are sampled from the MUSAN dataset~\cite{snyder2015musan}, and overlapping ``speech'' is sampled from LRS3~\cite{afouras2018lrs3}.
We monitor the token prediction accuracy on the noisy validation set every 1k steps to select the best checkpoints.
We normalize the text by lower-casing and removing punctuation except single apostrophes.
Our goal is to improve the multilingual Word Error Rate (WER), so we compute average WER on all languages except English. 
Given the data imbalance, we also separately compute the average on the ``higher resource'' languages with over 100h of data (Es, Fr, It, Pt), and the other ``lower resource'' languages (Ar, De, El, Ru) with as low as 10h of data.

\input{tables/main}
\input{tables/noisy}

\subsection{Clean Results}
In Table~\ref{tab:main}, we show the results on MuAViC using the original, clean audio.
For previous SOTA audio-visual methods, initial work fine-tuned pre-trained English AV-HuBERT~\cite{shi2022learning} on multilingual videos in MuAViC~\cite{anwar23_interspeech,hong-etal-2023-intuitive}.
Some methods trained individual models for specific non-En languages~\cite{li2023parameter,gimeno2024tailored,li24i_interspeech}, however, they are outperformed by multilingual models.
The current SOTA models are XLAVS-R~\cite{han2024xlavs} and Fast Conformer~\cite{burchi2024multilingual}.
The former adapted XLS-R~\cite{babu2021xls}, a multilingual self-supervised audio model, and the latter trains a model from scratch. 
Moreover, Fast Conformer achieved even better results on the higher-resource languages by using extra multilingual training videos from other datasets.
While these methods worked well on the higher-resource languages, their performance on lower-resource languages is less satisfactory.

Next, we establish baselines using Whisper models.
Compared to XLAVS-R 2B, Whisper small zero-shot achieves better performance on the lower resource languages (37.5\% vs 41.9\%), while Whisper medium zero-shot achieves better overall average non-En WER (22.9\% vs 26.4\%).
This shows Whisper's strong multilingual capabilities and motivates its selection as the foundation of our proposed models.

Fine-tuning Whisper on MuAViC leads to further gains.
Fine-tuned Whisper medium achieves a new \textbf{SOTA non-En average WER of 20.1\%,} surpassing all previous audio-visual methods fine-tuning solely on MuAViC. 
Notably, the English performance is also improved from 2.3\% (zero-shot) to 0.74\%, approaching the current SOTA of 0.68\% reported in Whisper-Flamingo~\cite{rouditchenko24_interspeech}. 
This shows that fine-tuning improves multilingual performance without compromising English accuracy.  

mWhisper-Flamingo performs similarly to audio-only fine-tuned Whisper (20.4\% vs. 20.1\% WER for the medium models). 
This small difference suggests that video provides limited benefit for clean audio. 
However, mWhisper-Flamingo still significantly outperforms all previous AVSR models fine-tuned solely on the 1,141h MuAViC videos, achieving a new SOTA on all languages with this setup. 
It even surpasses Fast Conformer trained with 4,957h of videos on all languages except Fr and Pt. 
This shows the strength of using Whisper as initialization for AVSR. 
Finally, mWhisper-Flamingo medium consistently outperforms mWhisper-Flamingo small, showing the benefit of increased model size.

\subsection{Noisy Results}
Table~\ref{tab:noisy} shows the performance on MuAViC with babble noise at 0-SNR. 
The babble noise is from Whisper-Flamingo~\cite{rouditchenko24_interspeech} and was constructed from 30 LRS3 speakers.
We compare Whisper zero-shot with Whisper fine-tuned and mWhisper-Flamingo.
The noisy results show a significant performance degredation compared to the clean results (Table~\ref{tab:main}).
For example, the average non-En WER for Whisper small zero-shot increases from 27.0\% in the clean setting to 71.0\% under 0-SNR babble noise. 
However, fine-tuning significantly improves WER. 
For the small model, fine-tuning improves the average non-En WER from 71.0\% (zero-shot) to 55.3\%. 
A similar trend is observed for the medium model, with WER decreasing from 60.7\% to 48.0\%. 

The integration of visual features in mWhisper-Flamingo provides further gains. 
mWhisper-Flamingo small achieves an average non-En WER of 50.4\%, a \textbf{10.4\% relative improvement} over fine-tuned audio-only Whisper small (55.3\%).
Similarly, mWhisper-Flamingo medium achieves an average non-En WER of 43.7\%, a \textbf{10.6\% relative improvement} compared to fine-tuned audio-only Whisper medium (48.0\%). 
The relative improvements are better for the higher-resource languages (\textbf{16.0\% and 16.4\%} for small and medium models), indicating that more video training data is helpful.
The relative improvements for English are much better at 48.4\% and 39.8\% for the small and medium models, which we attribute to having more English training data.

mWhisper-Flamingo medium achieves the best multilingual WER (43.7\%), even outperforming Whisper large zero-shot (65.0\%) despite having fewer parameters (1.39B vs 1.5B).
This shows that the performance gains are not only due to more parameters but also due to the visual modality. 
As additional evidence, mWhisper-Flamingo small (651M parameters) outperforms audio-only fine-tuned Whisper medium (769M parameters) on the higher-resource languages (37.4\% vs. 38.1\%). 
This highlights that smaller audio-visual models can achieve better performance than larger audio-only models.

Finally, we tested the models on 6 different noise types, 5 SNR levels $\{-10,-5,0,5,10 \}$, and 4 languages (Es, Fr, It, Pt). 
The noise setups follow Whisper-Flamingo~\cite{rouditchenko24_interspeech} and AV-HuBERT~\cite{shi22_interspeech}.
Figure~\ref{fig:noise} shows the WER for Es, Fr, It, Pt averaged over all SNR values for each noise type. 
There is a clear trend of mWhisper-Flamingo outperforming the audio-only models, especially for the more challenging noises (3 types of babble and side speech).
Overall, these results confirm the advantage of mWhisper-Flamingo on diverse noise types.

\input{figures/noise_fig}


\subsection{Ablation Study and Analysis}
\label{sec:analysis}
In Table~\ref{tab:ablation}, we present an ablation study and model analysis.
Due to computational constraints, we trained and evaluated the small model on 5 of the 9 languages.
We compare the average non-En WER between models.
We show the noisy results since the performance differences between models in the clean results were minor.
The baseline audio-only fine-tuned Whisper small achieves 44.4\%.

\input{tables/ablation}
% Note that the 5 language model presented in these experiments achieves slightly better than the 9 language model (36.6 vs 37.4).
First, we show that the combination of decoder modality dropout and a fine-tunable visual encoder leads to the best performance (36.6\%).
The performance is significantly worse if decoder modality dropout is disabled using either a fine-tunable visual encoder (44.6\%) or a frozen visual encoder encoder (42.6\%).
It shows that our proposed decoder modality dropout is \textit{crucial} for obtaining the best multilingual performance.
Note that the latter setup represents the default training configuration from Whisper-Flamingo~\cite{rouditchenko24_interspeech}.
Considering that the latter setup improved the English noisy WER from 16.2\% to 11.5\%, it is reasonable that modality dropout was not necessary in the original, English-only Whisper-Flamingo.
Finally, using decoder modality dropout with a fine-tunable visual encoder is better than using it with a frozen visual encoder (36.6\% vs 40.6\%).

mWhisper-Flamingo uses decoder modality dropout probabilities of \( p_{AV} = 0.5,\ p_{A} = 0,\ p_{V} = 0.5 \), meaning that the model trains with audio-visual inputs 50\% of the time and visual-only inputs 50\% of the time.
Without dropout, the modal trains only on audio-visual inputs (\( p_{AV} = 1,\ p_{A} = 0,\ p_{V} = 0 \)), and the performance is much worse (44.6\% vs 36.6\%).
Using dropout with audio-only inputs 50\% of the time instead of video-only inputs (\( p_{AV} = 0.5,\ p_{A} = 0.5,\ p_{V} = 0 \)) does not improve performance (44.6\%). 
It shows that it is necessary for the model to train with video-only inputs.
Trying different probabilities between AV, A, and V such as \( p_{AV} = 0.5,\ p_{A} = 0.25,\ p_{V} = 0.25 \) and \( p_{AV} = 0.25,\ p_{A} = 0.25,\ p_{V} = 0.5 \), the performance is improved to 37.7\%, however the best performance is achieved without audio-only inputs (\( p_{AV} = 0.5,\ p_{A} = 0,\ p_{V} = 0.5 \)).
Our explanation is that Whisper was fine-tuned in the first stage of training, so the model can already handle audio-only inputs well.
Training on video-only inputs allows the model to better integrate the visual modality.
We also tried other proportions of audio-visual and video-only training such as \( p_{AV} = 0.75,\ p_{A} = 0,\ p_{V} = 0.25 \) and \( p_{AV} = 0.25,\ p_{A} = 0,\ p_{V} = 0.75 \) which performed slightly worse.

Finally, we compare multilingual AV-HuBERT~\cite{kim_2024} with AV-HuBERT pre-trained only on English videos~\cite{shi2022learning}.
The model using the multilingual AV-HuBERT encoder achieves better multilingual performance (36.6\% vs 37.4\%) but worse English performance (8.0\% vs 7.5\%), which is reasonable.

\section{Conclusion}
We introduce mWhisper-Flamingo, a novel multilingual model for AVSR.
mWhisper-Flamingo outperforms all audio-visual methods trained on MuAViC, and even surpasses a model trained with substantially more videos on most languages. 
mWhisper-Flamingo outperforms audio-only Whisper in diverse noise settings. 
Our proposed decoder modality dropout significantly improved the noisy multilingual WER.
% Future work could try scaling training data and trying new audio and visual encoders.


\clearpage

\bibliographystyle{IEEEtran}
\bibliography{ref}

% Appendix for ArXiv version
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\section{Appendix}
\subsection{Full Noisy Results}
\input{tables/noise_full_small}
\input{tables/noise_full_medium}

Table~\ref{tab:noise-full-small} and Table~\ref{tab:noise-full-medium} show the full decoding results for different noise types and SNRs.
Table~\ref{tab:noise-full-small} compares the small models and Table~\ref{tab:noise-full-medium} compares the medium models.
Note that the results in Table~\ref{tab:noise-full-small} were used to create Figure~\ref{fig:noise} (excluding the English results).

\end{document}