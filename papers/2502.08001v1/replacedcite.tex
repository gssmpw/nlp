\section{Related Work}
\label{sec:related}
FL has emerged as a crucial learning scheme for distributed training that aims to preserve user data privacy. 
However, research has uncovered various privacy vulnerabilities in FL, particularly in the form of MIA and LDIA.
This section discusses relevant works that highlight these threats in both FL and FD settings, and contextualize our research within this landscape.

\BfPara{MIA and LDIA} 
Shokri \etal____ pioneered MIA research by demonstrating how model output confidence scores could reveal training data membership. 
Nasr \etal____ extended this to FL, showing how both passive and active adversaries could exploit gradients and model updates.
LDIA represents another significant privacy threat in FL.
Gu \etal____ introduced LDIA as a new attack vector where adversaries infer label distributions from model updates. Wainakh \etal____ further explored user-level label leakage through gradient-based attacks in FL.
Recent works have exposed the vulnerability of FD to inference attacks.
Yang \etal____ proposed FD-Leaks for performing MIA in FD settings through logit analysis. Liu \etal____ and Wang \etal____ enhanced MIA using shadow models via respective approaches MIA-FedDL and GradDiff, though their assumptions were limited to homogeneous environments.

\iffalse
The study of MIA in ML models was pioneered by Shokri \etal____.
They demonstrated how adversaries could exploit model output confidence scores to infer whether a specific data point was used in training.
This seminal work laid the foundation for subsequent research into privacy vulnerabilities in various ML paradigms, including FL.
Building on this, Nasr \etal____ extended the concept to FL environments, introducing white-box MIAs.
Their privacy analysis demonstrated how both passive and active adversaries could exploit gradients and model updates to infer private information.
\BfPara{LDIA in FL} 
LDIA represents another significant privacy threat in FL.
Gu \etal____ introduced LDIA as a new attack vector, where adversaries seek to infer the distribution of labels in clients' training data by analyzing model updates.
This work showed that even when individual data points are protected, the overall data distribution might still be exposed, leading to privacy concerns.
Wainakh \etal____ further explored user-level label leakage by performing gradient-based attacks in FL.

\BfPara{LDIA and MIA in FD} 
% FD has been proposed as a lightweight alternative to traditional FL.
% It reduces communication overhead by exchanging logits or softmax values instead of model parameters.
Yang \etal____ proposed FD-Leaks, an attack designed to perform MIA in FD settings.
By analyzing logits, adversaries can infer membership information, potentially revealing sensitive training data.
Similarly, Liu \etal____ presented MIA-FedDL, which enhances MIA by using shadow models to infer membership information with higher accuracy in FD settings.
Despite their contributions, they made assumptions that are infeasible in heterogeneous environments.
\fi
% Despite their contributions, these works make several assumptions that may limit the feasibility of the proposed attacks in practice.
% For instance, both FD-Leaks and MIA-FedDL assume that the adversaries have access to all logits or can easily build shadow models that mimic the behavior of target models.
% Such assumptions are often unrealistic in heterogeneous environments.

\BfPara{Defenses and Countermeasures}
DPSGD____ can be employed during the training phase to mitigate against privacy attacks to the client model. 
Additionally, specialized MIA defense methods such as SELENA____, HAMP____ and DMP____ can be integrated into the training process.
Several studies have proposed enhanced FD frameworks with improved privacy protection mechanisms to reduce client privacy leakage.
%Several studies have proposed defenses to mitigate MIA and LDIA.
%Wang \etal____ proposed GradDiff, a gradient-based defense mechanism that employs differential comparison to detect and mitigate MIA in FD settings.
Zhu \etal____ investigated data-free knowledge distillation for heterogeneous federated learning.
They presented an approach that reduces the need for public datasets.
Chen \etal____ proposed FedHKD, where clients share hyper-knowledge based on data representations from local datasets for federated distillation without requiring public datasets or models.

% Our work builds upon these foundations, specifically investigating the privacy vulnerabilities in PDA-FD frameworks. 
%Unlike previous studies that focused on traditional FL or specific attack scenarios in FD, our work aim to provide a comprehensive analysis of LDIA and MIA across multiple PDA-FD frameworks.