\section{Methodology}
\label{sec:method}
\subsection{Threat Model}

% There are two main components in Public Dataset-Assisted Federated Distillation framework: the clients and the server.
\textbf{Adversary Knowledge:} We investigate PDA-FD in the context of Horizontal FL (HFL), where clients' private data's label distributions can be non-IID.
We consider a threat model where the server acts as an \textit{honest-but-curious}~\cite{paverd2014modelling} adversary.
The server attacker is not allowed to modify the learning process but gets to select public dataset members used for knowledge transfer; this privilege for the server is common in PDA-FD frameworks~\cite{li2019fedmd, itahara2021distillation, chang2019cronus}. The server can select members of the public dataset in each collaborative training round.
In each training round, the server's access to clients' models is black, and it only interacts with client models by running inferences on public data. The server can only obtain logit vectors or prediction vectors of every public data sample each client model provides without visibility into the private models' weights or architectures.

\BfPara{Adversary's objective:} The server aims to infer sensitive information about a target client's private dataset, specifically label distribution information and membership information, through two main attack vectors: LDIA and MIA.

%Figure~\ref{fig:FD_workflow} illustrates the overall workflow of both attacks in the PDA-FD setting.
%As shown in the figure, PDA-FD learning consists of three main phases: Local Updates, Communication, and Knowledge Distillation.

%We design privacy information attack methods by having the server exploit the exchanged public dataset and the other information in these phases, particularly the logits or prediction scores on the public dataset shared between clients and the server originally for knowledge transfer purposes. 

%Specifically, in our proposed label distribution inference attack (LDIA), the server analyzes the aggregated vector of the target client to infer the label distribution of their private dataset.
%In our proposed membership inference attack (MIA), the server has only black-box access to the target clients' models, but can extract information about client models through the public dataset transferred between the server and each client. 
%In both attacks, we assume the server has the right to choose what is included in the public dataset used for knowledge transfer. The server can strategically choose specific public dataset members to facilitate its attacks.
%Note that our proposed attacks do not require modification of the FD process, rendering them challenging to detect.



% In this paper, the server in the PDA-FD framework exploits the public dataset as a vulnerability to target the privacy of the clients' private datasets.
% For target clients, the attacker's objective is to launch a label distribution attack or a membership inference attack against them.
% For the adversary knowledge, the server has access to the public dataset and could select the public data samples used in each round of collaborative training.
% During the communication phase in every collaborative traininig round, the server can only perform black-box access to the clients' models through the public data. 
% The server is able to get the logits vector or prediction vector output by each client's model for every public data sample.

\subsection{Label Distribution Inference Attack}
\label{sec:ldia_method}
Unlike traditional FL where the server has white-box access to clients' local models, the server only has black-box access by using a public dataset in PDA-FD.
The core idea behind LDIA is that the logits or prediction values produced by a client's model still carry information about the distribution of labels in its training data.
This is due to the tendency of neural networks to overfit their training distribution, even when regularization techniques are applied~\cite{yeom2018privacy}.

To validate this idea, we conduct a motivating experiment using the CIFAR-10 dataset~\cite{krizhevsky2009learning}.
% Nevertheless, the server can extract significant sensitive information from clients' local models through the logits output during inference on the public dataset.
% Consequently, the use of a public dataset in PDA-FD could still lead to privacy leakage on the clients' side.
% Due to the overfitting of deep neural networks, the clients' local models can memorize specific characteristics from their private training datasets\cite{yeom2018privacy}.
% The label distribution of the training dataset is one such characteristic. 
% Information about the label distribution can be extracted from the logits output by a deep neural network model during inference on data samples.
% We can demonstrate label distribution leakage in a neural network model with a simple experiment. 
We sample a subset of data with a non-IID label distribution from CIFAR-10 as the training dataset $D_{train}$. 
Subsequently, we train a deep neural network model $\theta$ on this training set.
We use the trained model $\theta$ to infer on data $x$ from the CIFAR-10 test dataset, which consists of ten different labels, to obtain the logits vector $z_{\theta}(x)$ from these inference results. 
We also apply the softmax function to get the posterior probabilities vector $v_x$ of each image prediction:
% The posterior probability $v_{x,i}$ for the $i$-th label in the posterior probabilities vector can be obtained using the following formula:
\begin{equation}
    v_{x,i} = \frac{e^{z_\theta(x)_i}}{\sum_{j=1}^{10} e^{z_\theta(x)_j}}
\label{eq:softmax_function}
\end{equation}
, where $v_{x,i}$ is the probability for the $i-$th label.
For each label, we have 500 data samples for model inference and calculate the mean posterior probability vector across all the samples
$V_{mean} = \frac{1}{|S|} \sum_{k \in S} v_{x_k}$
, where $S$ represents the subset for each label.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ldia_motivation.png}
    \caption{The mean vector of posterior probability vectors $V_{mean}$ predicted by the model $\theta$ on 500 data samples with the same label in CIFAR10.}
    \label{fig:posterior_probabilities_vector}
\end{figure}
Figure \ref{fig:posterior_probabilities_vector} illustrates the mean posterior probability vector $V_{mean}$ for predictions made by model $\theta$ on the subset of data for each label, alongside the label distribution of the training dataset $D_{train}$ used for model $\theta$.
As seen in the figure, the labels ``cat'' and ``truck'' have a larger proportion among all labels in the training dataset $D_{train}$.
% The mean posterior probability vector $V_{mean}$ for each label represents the general distribution of posterior probabilities predicted by the model for samples of that label.
% Notably, in the model's predicted vector $V_{mean}$, while the highest probability typically corresponds to the ground truth label, the second or third highest probabilities often correspond to the "cat" or "truck" labels.
However, they constantly appear with higher probabilities in the model's predictions, even for samples of other classes. 
For example, when the model is presented with ``horse'' images, the average prediction probabilities are $0.57$, $0.22$, and $0.07$ for classes ``horse'', ``cat'' and ``truck'', respectively, which are the top-3 predictions.
In this case, the model assigns higher probabilities to the over-represented classes, i.e., ``cat'' and ``truck'', over the other incorrect classes.
This indicates that the model is able to retain the distribution of its training dataset to some extent. 
% We attribute this phenomenon to the model's tendency to overfit its training dataset to some extent.
% The model appears to memorize and rely more heavily on data samples from labels that are more prevalent in the training dataset.
% Consequently, when performing inference on a data sample, the model not only predicts the sample's ground truth label but also tends to assign higher likelihoods to labels that are more prevalent in the training dataset.
Such an observation suggests that in FD, where clients share logits or probability values of their models on public data samples, a malicious server could potentially infer the label distribution of clients' private training data.
Even though the public data may have a different distribution, the clients' models will still exhibit biases reflective of their training data distributions.


\iffalse
To accurately infer the label distribution of the target model's training set, minimizing the influence of the ground truth label in the posterior probability vector is necessary. 
We sampled an IID-distributed dataset $D_{inf}$ from the CIFAR-10 test dataset, ensuring that it includes 500 data samples for each label.
This dataset $D_{inf}$ was used for label distribution inference. 
Subsequently, we used model $\theta$ to perform inference on each sample $x$ in this dataset $D_{inf}$ and obtained the corresponding posterior probability vector $v_x$ using Equation \ref{eq:softmax_function}.
As in the previous approach, we averaged all the obtained posterior probability vectors $\{v_x \mid x\in D_{inf}\}$ to compute a mean posterior probability vector $V_{mean}$.
From Figure \ref{fig:label_distribution_different_datasets}, we can observe that the mean posterior probability vector $V_{mean}$ obtained from the dataset $D_{train}$ closely resembles the label distribution vector of model $\theta$'s training dataset $D_{train}$. To validate the generality of this approach, we trained another model $\beta$ using a different non-IID distributed training dataset $D_{train}$ and conducted the label distribution inference experiment using the same method. 
The results were consistent with our initial findings.

This experiment demonstrates the feasibility of performing a label distribution inference attack on a neural network model.
An attacker can leverage an IID-distributed inference dataset $D_{inf}$ and calculate the logits vectors obtained from the model's inference on this dataset to infer the label distribution of the model's training dataset.
The specific attack process of LDIA is formalized by the following equation:
\begin{equation}
    \hat{p} = \frac{1}{\mid D_{inf} \mid} \sum_{x \in D_{inf}} softmax(z_{\theta}(x))
\label{eq:ldia}
\end{equation}
where $\hat{p}$ denotes the inferred label distribution, $D_{inf}$ is the inference dataset, $\theta$ is the target model, $z_{\theta}(x)$ represent the logits vector obtained by model $\theta$ when inferring on data $x$.

the public datasets used by FD can be either unlabeled or exhibit a distributional shift relative to the data distributions of client-side private datasets~\cite{itahara2021distillation, li2019fedmd}.  To verify if our simple label distribution inference method still works in a public dataset of different distribution,
we conducted inference test using models previously trained on CIFAR-10, with test samples from CIFAR-100\cite{krizhevsky2009learning}. The data distribution of CIFAR-100 is different from CIFAR-10.
As shown in Figure \ref{fig:label_distribution_different_datasets}, by calculating the logits vector using Equations \ref{eq:ldia}, we were able to obtain a distribution closely resembling the label distribution of their respective training datasets.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/label_distribution_data_shift.png}
    \caption{Comparison between inferred label distribution $\hat{p}$ and the ground truth label distribution.}
    \label{fig:label_distribution_different_datasets}
\end{figure}

Some characteristics of the PDA-FD frameworks can be leveraged facilitate the server’s ability to attack the clients’ label distribution. 
First, since the PDA-FD framework relies on public datasets for the transfer of knowledge among clients, in the communication phase, the server can obtain the logits from the clients’ inferences on the public datasets.
Second, the server controls the selection of public data used in each communication phase. To enhance the performance of the federated distillation framework, the public dataset is typically sampled into an IID dataset, with its distribution in the feature space closely aligned with that of the client's private data.\cite{li2024federated}.
Third, before each communication phase, clients undergo the local updates phase, which increases the overfitting level of the local models on their respective training datasets. 
This heightened overfitting reinforces the memorization of the label distribution within the local models' training datasets.
\fi

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/LDIA_workflow.pdf}
    \caption{Workflow of Label Distribution Inference Attack.}
    \label{fig:LDIA_workflow}
\end{figure}

With all these positive verification experiment results, we design a LDIA method in PDA-FD that consists of the following steps:
(1) As shown in Figure~\ref{fig:LDIA_workflow}, the server selects a subset of the public dataset $D_{pub}$ for each round of FD, which is denoted as $D_{inf}$.
This selection needs to ensure that the samples are evenly distributed across different classes, which not only helps minimize bias in LDIA but also enhances FD's performance. 
(2) During the communication phase, each client performs inference on the selected public data samples $\{x_k \mid k \in S_t\}$.
The generated logits are sent to the server.
(3) The server receives logits from all the participating clients and selectively aggregate the logits sent by the target client.
The server uses the resulting vector to infer the label distribution of the target client's private dataset.

Formally, the LDIA process can be expressed as:
\begin{equation}
    \hat{p} = \frac{1}{\mid D_{inf} \mid} \sum_{x \in D_{inf}} softmax(z_{\theta}(x))
\label{eq:ldia}
\end{equation}
, where $\hat{p}$ denotes the inferred label distribution, $\theta$ is the target model, and $z_{\theta}(x)$ represent the logits vector obtained by model $\theta$ when inferring on data $x$.
To enhance accuracy and robustness, we average the inferred label distributions over $N$ rounds and report the averaged distribution as the final LDIA result. This mitigates the impact of potential anomalies or fluctuations in individual rounds, thus improving overall stability.

% The server's LDIA process in the PDA-FD framework proceeds as follows:
% (1)During the communication phase of the collaborative training round, all participating clients perform knowledge transfer using public data $\{x_k \mid k \in S_t\}$ selected by the server from the public dataset $D_{pub}$, where $S_t$ is the server-determined public data index set.
% (2)The server, acting as an attacker, uses $\{x_k \mid k \in S_t\}$ as the inference dataset $D_{inf}$ for LDIA.
% (3)Clients infer on data $x_k \in D_{inf}$ and upload the resulting logits $z_{\theta}(x_k)$ for each data point $x_k$ to the server.
% (4)Upon receiving the uploaded logits sets $\{z_{\theta}(x_k) \mid x_k \in D_{inf}\}$ from each client, the server could perform LDIA by Equation \ref{eq:ldia} on all participating clients.
%It is important to note that during the server's LDIA process, the server only analyzes the logits set returned by the clients without affecting the PDA-FD workflow or the performance of the PDA-FD framework.

\subsection{Membership Inference Attack}

MIA in machine learning aims to determine whether a specific data sample was used to train a model.
%Many recent efforts have demonstrated the feasibility of such attacks in traditional FL and centralized machine learning setting, where adversaries have either unlimited black-box access to the target model's predictions on the selected data samples or white-box access to the model's parameters or gradients~\cite{carlini2022membership, watson2021importance, shokri2017membership}.
Recent works have demonstrated the feasibility of MIA in traditional FL and centralized machine learning setting, where adversaries have either white-box access to the model's parameters and gradients or access to shadow datasets matching the target model's training dataset's data distribution for performing MIA~\cite{carlini2022membership, watson2021importance, shokri2017membership}.
However, in the context of PDA-FD, the server is limited to black-box interactions with the target client's model, and lacks knowledge of clients' private data distributions, making it challenging to obtain appropriate shadow datasets.
To address this challenge, we propose adapting and enhancing existing MIA approaches by combining them with the unique characteristics of the FD process.

Traditional MIA techniques often exploit the observation that machine learning models behave differently on data they are trained on versus data they aren't.
The difference typically manifests as higher confidence or lower loss on training samples.
In our proposed attack, we extend the state-of-the-art MIA technique, Likelihood Ratio Attack (LiRA), proposed by Carlini \etal~\cite{carlini2022membership}.


\iffalse
In the PDA-FD framework, the server can access clients' local models via public data in a black-box manner, obtaining logits or posterior prediction values from local models' inferences on public data.
Recent MIA research \cite{carlini2022membership, watson2021importance, shokri2017membership} demonstrates that access to a target model's logits or posterior prediction values on a target data sample enables effective membership inference attacks.
The server can leverage the public dataset as an entry point to execute membership inference attacks on target clients. The server accomplishes this by poisoning the public dataset and inserting the target sample into it.
During a specific collaborative training round, the server designates the target sample as part of the public data for all clients' inference tasks. 
Upon clients uploading their inference results, the server gains access to the posterior prediction values of the target sample on the target clients' local models.

The server then choose one of the MIA methods, in our case, Likelihood Ratio Attack (LiRA) \cite{carlini2022membership}, to perform the attack. Due to efficiency considerations, we choose to extend the offline version of LiRA for our attack setting.
\fi

\iffalse
Offline LiRA uses multiple "out" models (models that do not use the target sample in training) to establish a baseline prediction distribution for a later null hypothesis test.
For a given target sample, LiRA fits the target model's prediction of the sample to the distribution of predictions from the reference ``out'' models, then it calculates the target sample's membership score.
This method assumes member samples have statistically different prediction scores from the target model compared to the reference models. In contrast, non-members have similar model predictions across all models.
%Additionally, keeping the reference models unchanged brings more stable and consistent results.

The membership score is calculated as the following. 
For a target sample $(x,y)$, the attacker queries all the reference (``out'') models to obtain the sample's posterior probabilities.
Then, the attacker derives a standardized confidence distribution by scaling the target sample's posterior probabilities across all the ``out'' models. The scaling formula is as follows:
\begin{equation}
    \phi(f_\theta(x)_{y}) = \log\left(\frac{f_\theta(x)_{y}}{1 - f_\theta(x)_{y}}\right)
    \label{eq:lira_rescale}
\end{equation}
where $f_{\theta}(x)_y$ denotes the posterior probability from a ``out'' model $\theta$ on the target sample $(x,y)$. 
The attacker then fits these scaled scores to a Gaussian distribution $\mathcal{N}(\mu_{\text{out}}, \sigma_{\text{out}}^2)$.
The attacker queries the target model $\theta_t$ with the target sample to compute the scaled score $\phi(f_{\theta_t}(x)_y)$.
The probability $\lambda$ that the target sample $(x,y)$ belongs to the training dataset of the target model $\theta_t$ is computed using the null hypothesis as follows:
\begin{equation}
    \lambda = 1 - \Pr[Z > \phi(f_{\theta_t}(x)_y)], \text{where } Z \sim \mathcal{N}(\mu_{\text{out}}, \sigma_{\text{out}}^2).
    \label{eq:lira_result}
\end{equation}
As the scaled score $\phi(f_{\theta_t}(x)_y)$ increases relative to $\mu_{\text{out}}$, the probability of the target sample $(x,y)$ being a member of the target model $\theta_t$'s training dataset grows larger. 
In LiRA, we can call this probability $\lambda$ as membership score.
\fi

Offline LiRA uses multiple reference ("out") models to establish a baseline prediction distribution for membership inference through hypothesis testing. 
The key assumption is that member samples have statistically different prediction scores between target and reference models, while non-members exhibit similar predictions across all models.
For a target sample $(x,y)$, LiRA first queries all reference models to obtain posterior probabilities, which are then standardized using the following scaling function:
\begin{equation}
    \phi(f_\theta(x)_{y}) = \log\left(\frac{f_\theta(x)_{y}}{1 - f_\theta(x)_{y}}\right)
    \label{eq:lira_rescale}
\end{equation}
where $f_{\theta}(x)_y$ denotes the posterior probability from a reference model $\theta$.
The scaled scores from reference models are fitted to a Gaussian distribution $\mathcal{N}(\mu_{\text{out}}, \sigma_{\text{out}}^2)$. The membership probability $\lambda$ is then computed by comparing the target model's scaled score $\phi(f_{\theta_t}(x)_y)$ with this distribution:
\begin{equation}
    \lambda = 1 - \Pr[Z > \phi(f_{\theta_t}(x)_y)], \text{where } Z \sim \mathcal{N}(\mu_{\text{out}}, \sigma_{\text{out}}^2).
    \label{eq:lira_result}
\end{equation}
A higher scaled score $\phi(f_{\theta_t}(x)_y)$ relative to $\mu_{\text{out}}$ indicates a higher probability of $(x,y)$ being a member of the training dataset.

While LiRA is a powerful and effective technique for MIAs, directly applying it in the context of PDA-FD presents challenges.
In traditional LiRA, the attacker needs the ability to train multiple reference models that mimic the target model's behavior but behave differently on the target samples, specifically, prediction discrepancies between target and reference models should be larger for members than for non-members.
However, training such models requires access to a shadow dataset with a distribution similar to the target model's training data~\cite{watson2021importance, carlini2022membership}.
As shown in Figure \ref{fig:regular_reference_model}, when the data distribution of the reference model's training dataset differs from that of the target model's training dataset, the attacker cannot distinguish members based on prediction discrepancies between reference and target models for members versus non-members. The prediction discrepancy is quantified as the difference between the target model's prediction probability and the mean of reference models' prediction probabilities for the target sample.
In the PDA-FD context, the server faces significant challenges in obtaining such a shadow dataset:
(1) The FD framework allows clients to have non-IID private datasets in the label space.
Without knowledge of the specific label distributions in each client's private dataset, the server cannot accurately sample shadow datasets that match the characteristics of the target client's data.
(2) In FD, the public dataset can be unlabeled, particularly in semi-unsupervised learning scenarios.
In that case, the server simply cannot use these datasets without labels for training shadow models, as label information is essential for mimicking the target model's behaviors.
To address these unique challenges, as shown in Figure \ref{fig:MIA_workflow}, we design and implement two variants of offline LiRA: Co-op LiRA and Distillation-based LiRA.



\iffalse
A reference model is a model that, in an ideal situation, closely mimics the behavior
of the target model but differs primarily in its predictions of the target sample. Co-op MIA requires the presence of such reference models. 
Training reference models necessitates access to a shadow dataset with a distribution similar to the target model's training data \cite{watson2021importance, carlini2022membership}.
In the PDA-FD context, the server faces significant challenges in obtaining such a shadow dataset:
(1)The FD framework allows for non-IID client private datasets, and the server lacks knowledge of their label distributions, complicating the sampling of matching shadow datasets.
(2)Clients' private datasets are typically sensitive. Moreover, each client protects these private data, limiting the server's direct access to similar datasets.
(3)While public datasets may have distributions similar to clients' private data, in some FD frameworks, these can be unlabeled, precluding their direct use as shadow datasets for training reference models.
To overcome these challenges, we developed two LiRA variants specifically for the PDA-FD framework: Co-op LiRA and Distillation-based LiRA.
\fi

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/MIA_workflow.pdf}
    \caption{Workflow of Membership Inference Attack(Co-op LiRA and Distillation-based LiRA).}
    \label{fig:MIA_workflow}
\end{figure}

\BfPara{Co-op LiRA} Co-op LiRA is designed for scenarios where clients' private datasets exhibit similar data distributions.
In this case, the server can utilize the non-target clients' local models as reference models to conduct MIA on the target client's model.
We notice that when target samples appear in other clients' private datasets besides the target client, these clients' private models cannot serve as reference models. 
However, in real-world FD deployments, clients are typically assumed to have mostly disjoint training datasets to benefit collaborative learning, thus making shared target samples rare.
%Data distribution disparities primarily arise from feature space and label space distribution variations. 
%PDA-FD operates within a horizontal FL (HFL) framework and ensures similar feature space distributions across clients' private datasets.
For Co-op LiRA, the server can conduct the proposed LDIA described in Section~\ref{sec:ldia_method} to determine label distributions across all clients first. 
In the HFL setting, similar label distributions across clients indicate similar overall data distributions, resulting in comparable performance of local models. 
Then, the server can use other clients' local models as reference models for LiRA directly based on the target client model.
Algorithm \ref{alg:MIA_algorithm} outlines the co-op LiRA process in PDA-FD:
(1) During the communication phase, the server inserts the target sample into the selected public data subset to obtain the posterior probabilities from all clients' models, including the target clients.
(2) The server conducts LDIA on all clients.
(3) The server calculates the KL-divergence between the label distributions of the target client and each remaining client, selecting clients as reference models when their KL-divergence falls below threshold $\beta$ (0.1).
(4) The server performs LiRA hypothesis test by comparing reference models' and target client's posterior probabilities to determine target sample's membership probability.

Co-op LiRA eliminates the need for the attacker to train multiple reference models. 
However, the drawback of the method is that it becomes ineffective when very few clients have training data of the same or similar distribution to the target client.

\begin{figure}[h]
\centering
  \begin{subfigure}[m]{0.49\linewidth}
  \centering
    \includegraphics[width=\linewidth]{figures/regular_train.png}
    \caption{\centering Reference model trained with $D_{ref}$.}
    \label{fig:regular_reference_model}
  \end{subfigure}
  \hfill
  \begin{subfigure}[m]{0.49\linewidth}
  \centering
    \includegraphics[width=\linewidth]{figures/distill_reference.png}
    \caption{\centering Reference model distilled by using $D_{ref}$.}
    \label{fig:distilled_reference_model}
  \end{subfigure}
  \caption{Prediction discrepancies between target and reference models across members and non-members on CIFAR 10. The target model is trained on dataset $D_{target}$ while the adversary can only obtain $D_{ref}$, where $D_{target}$ and $D_{ref}$ have different distributions.}
  \label{fig:distlled_reference_model_comparison}
\end{figure}

\begin{algorithm}
\caption{Co-op LiRA and Distillation-based LiRA. The shadow model preparation phase is implemented in lines 6-12 for Co-op LiRA and lines 14-18 for Distillation-based LiRA.}
\label{alg:MIA_algorithm}
\begin{algorithmic}[1]
    \REQUIRE public dataset $D_{pub}$, target sample $(x,y)$, target client private model $\theta_{target}$, clients private models $\{\theta_n\}_{n=1}^N$, scaling function $\phi$, KL-divergence threshold $\beta$, number of reference models $K$.
    \STATE $\mathcal{M}_{ref} \leftarrow \{\}$, $Conf \leftarrow \{\}$
    \STATE {\color{blue}\textit{$\triangleright$ Communication phase in FD}}
    \STATE $D_{pub} \leftarrow D_{pub} \cup \{(x,y)\}$ 
    \STATE Server send $D_{pub}$ to all clients and receive $\{f_{\theta_n}(D_{pub})\}_{n=1}^N$
    \STATE {\color{blue}\textit{$\triangleright$ Co-op LiRA}}
    \STATE $\hat{p}_{target} \leftarrow \text{LDIA}(\theta_{target})$ 
    \FOR{$\theta_n \in \{\theta_n\}_{n=1}^N \setminus \{\theta_{target}\}$}
        \STATE $\hat{p}_n \leftarrow \text{LDIA}(\theta_n)$
        \IF{$\text{KL}(\hat{p}_{target}, \hat{p}_n) < \beta$}
            \STATE $\mathcal{M}_{ref} \leftarrow \mathcal{M}_{ref} \cup \{\theta_n\}$
        \ENDIF
    \ENDFOR
    \STATE {\color{blue}\textit{$\triangleright$ Distillation-based LiRA}}
    \STATE $D_{distill} \gets \text{Random sample}(D_{pub} \setminus \{(x,y)\})$ 
    \FOR{$k=1$ to $K$}
        \STATE $\theta_k \leftarrow \text{Distill} (\theta_{target}, D_{distill})$
        \STATE $\mathcal{M}_{ref} \leftarrow \mathcal{M}_{ref} \cup \{\theta_k\}$
    \ENDFOR
    \FOR{$\theta \in \mathcal{M}_{ref}$}
        \STATE $Conf \leftarrow Conf \cup \{\phi(f_{\theta}(x)_y)\}$
    \ENDFOR
    \STATE $\mu_{out} \leftarrow \text{mean}(Conf)$, $\sigma^{2}_{out} \leftarrow \text{var}(Conf)$
    \RETURN $\lambda \leftarrow 1 - \Pr[Z > \phi(f_{\theta_{target}}(x)_y)]$, where $Z \sim \mathcal{N}(\mu_{out}, \sigma^{2}_{out})$
\end{algorithmic}
\end{algorithm}

\BfPara{Distillation-based LiRA} To address the limitation of co-op LiRA, we proposed another extension of LiRA called distillation-based LiRA.
It is challenging for the server to obtain a shadow dataset which has same data distribution with the target's model's training dataset to train a reference model; we instead choose to use knowledge distillation to generate a student model, which is then used as a reference model for the MIA. 
On the one hand, the generated student model should behave similarly to the target model, exhibiting close prediction scores on non-member samples.
On the other hand, it produces different prediction scores on member data that differ from those from the teacher model, since the student model is trained through knowledge distillation without direct exposure to the training dataset, whereas the teacher model, trained directly on member samples, exhibits a degree of overfitting to these samples.
Previous research on MIA~\cite{jagielski2024students} has demonstrated that the student model generated through knowledge distillation can potentially encode membership information of the teacher model. 
%so it might predict similarly on training members of the teacher model. 
However, we find significant prediction discrepancies between student and teacher models when performing inference on the teacher model's member samples. As shown in Figure \ref{fig:distilled_reference_model}, the prediction discrepancies between the teacher and the student model are significant for some members while remaining relatively small for non-members. 
This distinct pattern enables these distilled student models to serve as effective reference models for offline LiRA. 
These reference models allow attackers to identify members with high prediction discrepancies while maintaining a low False Positive Rate, thus achieving high True Positive Rate (TPR) at low False Positive Rate (FPR), which serves as a critical performance metric for successful membership inference attacks\cite{carlini2022membership, watson2021importance}. 
We validate this characteristic of knowledge distillation-based reference models in Section \ref{sec:MIA_eval}.

\iffalse
We empirically validate the idea of using distillation models as LiRA's reference model through the following experiments.
We train a target model using a subset of the CIFAR-10 dataset.
Another non-overlapping subset of the CIFAR-10 dataset is reserved for the distillation process.
We train 32 reference models by using the target model as the teacher model. 
We calculate the MAE loss as defined in Equation \ref{eq:loss_function_mae}.
After that, we obtain the probability $\lambda$ (membership score) of each target sample being a member as proposed in offline LiRA on the target model using these reference models.

\begin{figure}[h]
\centering
  \begin{subfigure}[m]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figures/distill_lira_1.png}
    \caption{All score region}
    \label{fig:distill_lira_overall}
  \end{subfigure}
  %\hfill
  \begin{subfigure}[m]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figures/distill_lira_high_likelihood_1.png}
    \caption{High score region}
    \label{fig:distill_lira_high}
  \end{subfigure}
  \caption{Distribution of membership score for target samples as determined by distillation-based LiRA.}
  \label{fig:distll_lira}
\end{figure}
Figure \ref{fig:distll_lira} shows the distribution of membership score for target samples as determined by distillation-based LiRA. 
The figure consists of two parts: (a) All membership score region and (b) High membership score region.
We can see that member samples exhibit higher membership score than non-members. This indicates that member samples achieve higher posterior probabilities on the target model than on the distilled reference models. 
Notably, there is a concentration of members with few non-members in the high membership score region, underscoring the high precision of distillation-based LiRA.
\fi


Algorithm \ref{alg:MIA_algorithm} outlines the distillation-based LiRA process in PDA-FD:
(1) During the communication phase, the server inserts the target samples into the public dataset.
The server obtains the posterior probabilities predicted by the target client on all the data samples. 
These probability values could be used as soft labels to distill reference models.
(2) To create multiple reference models, the server randomly samples the distillation dataset of distilled model from the public dataset except the target samples. Using the target client as the teacher model, the server distills multiple reference models.
(3) The server then infers membership of the target samples by performing LiRA using the created reference models.
This approach is more robust to heterogeneous data distributions among clients but comes at the cost of additional computational overhead for the distillation process.
