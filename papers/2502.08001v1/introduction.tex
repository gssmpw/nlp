\section{Introduction}
In recent years, federated learning (FL) has emerged as a promising paradigm for collaborative machine learning while preserving data privacy~\cite{mcmahan2017communication}.
Traditional FL frameworks, such as FedAvg~\cite{mcmahan2017communication} and FedSGD, require clients to upload model parameters or gradients to a central server for aggregation, which can introduce limitations in both privacy and utility.
To address these issues, federated distillation (FD)~\cite{li2019fedmd, sui2020feded, itahara2021distillation, chang2019cronus, jeong2018communication} has gained attention as an alternative approach that offers enhanced privacy protection and reduced communication overhead~\cite{wu2022communication,huang2023decentralized}.
In FD, model-inference outputs or distilled knowledge are exchanged between the server and clients instead of model parameters.
This learning scheme only requires black-box access to client models, supporting diverse model architectures across clients.
Existing approaches, such as FedMD~\cite{li2019fedmd}, DS-FL~\cite{itahara2021distillation} and Cronus~\cite{chang2019cronus}, have been proposed to further enhance the privacy protection and efficiency of collaborative learning.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/FD_workflow.pdf}
    \caption{Workflow of Public Dataset-Assisted Federated Distillation (PDA-FD). During each collaborative training round, clients first train their private models on their respective private datasets, then perform inference on the public dataset and transmit the resulting logits back to the server as transferred knowledge. As an \textit{honest-but-curious} server, the server can extract private information from target clients' private datasets by manipulating and leveraging public datasets.}
    \label{fig:FD_workflow}
\end{figure}

In these solutions, public datasets are often used to facilitate knowledge distillation among clients.
Knowledge sharing can be achieved across clients with diverse data distributions by having all clients perform inference on the same public data and sharing these inference results as knowledge.
We call such a learning scheme public dataset-assisted federated distillation, or PDA-FD.
Despite the benefits of FD, the privacy implications in such frameworks have not been thoroughly explored in the existing literature.
While FD generally provides stronger privacy guarantees than traditional FL, using a public dataset for information exchange can result in potential privacy leakage. 
Figure \ref{fig:FD_workflow} illustrates the workflow of PDA-FD. In each collaborative training round of PDA-FD, clients need to train their private models on their respective private datasets before performing inference on the public dataset and transmitting the inference results as knowledge to the server. This process enables better transfer of knowledge learned from private data to other clients' private models. However, this process also enhances the memorization of private datasets by private models, thereby causing the private models' inference results on the public dataset to leak private information from the private dataset. 
We discover that an \textit{honest-but-curious} server, via manipulating the public dataset and exploiting the client' inference results on the public dataset, can obtain private information from a particular client's private training dataset without compromising the training process of PDA-FD. 
In particular, the label distribution of the training dataset, and if specific data belongs to a client's training dataset (membership information). These two information leaks are representative and frequently studied in the ML privacy literature~\cite{gu2023ldia, yang2022fd, nasr2019comprehensive}.


In this work, we devise two types of privacy attacks against clients by the server: \textit{Label Distribution Inference Attacks (LDIA)} and \textit{Membership Inference Attacks (MIA)}.
LDIA reveals privacy information about the overall data distribution of all client private datasets combined. In contrast, membership inference attacks enable more granular privacy leakage by identifying the presence of specific data samples in a client's private datasets.
%In this work, we mainly leverage two privacy threats to investigate privacy leakage: \textit{Label Distribution Inference Attacks (LDIA)} and \textit{Membership Inference Attacks (MIA)}.
Both LDIA and MIA can be performed in a black-box manner, requiring only clients' inference results on the public dataset, which renders these attacks practically useful since FD frameworks are generally designed to not transfer a client model to the server.
%Both LDIA and MIA can be performed in a black-box manner, which aligns with the black-box access that FD frameworks typically provide to client models for protection. 
Additionally, Federated Distillation assumes non-IID data distributions across clients in the label space.
LDIA becomes particularly important in this context, as inferring the label distribution can reveal significant information about a client's unique data characteristics. 
Furthermore, LDIA and MIA can serve as stepping stones for more sophisticated attacks.
For instance, the obtained knowledge can be leveraged to generate synthetic data that mimics private datasets.

In the recent literature, LDIA and MIA have been extensively studied in the traditional FL and centralized machine learning settings~\cite{nasr2019comprehensive,shokri2017membership,yeom2018privacy,melis2019exploiting, ramakrishna2022inferring, jiang2024protecting}.
They have largely focused on white-box or gradient-based attacks.
For example, Gu \etal demonstrated that a malicious server in FL can infer label distributions by exploiting the gradients or parameters uploaded by clients~\cite{gu2023ldia}.
Similarly, Wainakh \etal showed that user-level label leakage is possible through gradient analysis~\cite{wainakh2021user}.
For MIAs, Nasr \etal developed sophisticated MIA techniques that exploit the white-box access to model parameters in FL~\cite{nasr2019comprehensive}.
The rich information leveraged in these attacks are not directly accessible in FD settings.
In the FD setting, a few studies have primarily focused on MIAs, such as FD-Leaks~\cite{yang2022fd}, MIA-FedDL~\cite{liu2023mia} and GradDiff~\cite{wang2024graddiff}.
In Fed-leaks, a malicious client leverages its local model as a shadow model to train an MIA classifier, then applies the classifier on target sample inference score to determine target sample membership in other clients' private datasets.
The attacker needs to be the client in the ~\textit{MIA-FedDL} approach; ~\textit{GradDiff} can be used by either a client or the server to attack another client. Both of these approaches require shadow datasets to train shadow models for MIA.
However, effective MIA requires the attacker's shadow model to be trained on the dataset sharing similar data distribution with the target model's training dataset.
In the FD setting, it is challenging for both clients and the server to obtain the data distribution of other clients' private data. 
Therefore, these MIA methods cannot work effectively in heterogeneous non-IID environments.
%Similar to FD-Leaks, the attack is conducted by a malicious client in MIA-FedDL.
%Additionally, MIA-FedDL uses shadow models, which requires the attacker to build such models using a dataset that has a similar distribution as the training dataset of the target client. 
%Therefore, they cannot work very effectively in heterogeneous non-IID environments.

To address the limitations of the existing works, 
% \haonan{specifically the requirements for white-box access and poor MIA performance caused by suboptimal shadow model construction methods}, 
we aim to comprehensively examine privacy leakage by public datasets in FD across multiple frameworks (FedMD, DS-FL, and Cronus) and various data distribution scenarios.
We also introduce new attack methods that are specifically tailored to the PDA-FD setting.
Specifically, we propose a novel LDIA method based on public datasets and extend the state-of-the-art MIA, Likelihood Ratio Attack (LiRA)~\cite{carlini2022membership}, to overcome the challenges posed by the limited information available in PDA-FD.
To that end, we design and implement \textit{Co-op LiRA} and \textit{Distillation-based LiRA} for MIAs.
Furthermore, while previous works primarily focused on client-side attacks, our work provides a more holistic view by examining both LDIA and MIA from the server's perspective.
A high-level comparison with the existing work is shown in Table~\ref{tab:mia_discussion}.
\begin{table}[h]
    \caption{Summary of Privacy Attacks in FL \& FD}
    \centering
    \resizebox{1.0\linewidth}{!}{%
    \begin{tabular}{c|c|c|c|c}
        \toprule
        Method & Attacker & Framework & Shadow dataset &Attack Goal\\
        \midrule
        \cite{gu2023ldia}     & Server & FL & Required & LDIA  \\
        \cite{nasr2019comprehensive} & Server\&Client & FL & Required & MIA \\
        \cite{liu2023mia}    & Client & FD & Required & MIA \\
        \cite{yang2022fd}& Client & FD & Not Required & MIA\\
        \cite{wang2024graddiff}& Server\&Client & FD & Required & MIA\\
         \midrule
         Ours & Server & FD & Not Required & LDIA\&MIA \\
        \bottomrule
    \end{tabular}%
}
    \label{tab:mia_discussion}
\end{table}

In our study, our key findings include:
(1)LDIA can be successfully achieved across multiple PDA-FD frameworks that significantly outperform random guessing baselines.
(2)The proposed \textit{co-op LiRA} and \textit{distillation-based LiRA} shows high effectiveness in terms of the True Positive Rate (TPR) in a low False Positive Rate (FPR) region.
(3) We also show that the effectiveness of LDIA and MIA varies with data distributions. Adversaries can generally achieve higher attack success rates when data follows more uniform distributions compared to non-IID settings.
Our findings collectively reveal significant privacy risks in current PDA-FD frameworks and highlight the need for more advanced privacy-preserving mechanisms.

\iffalse
The remainder of the paper is organized as follows: Section~\ref{sec:background} provides background information on FD and the specific PDA-FD frameworks studied.
Section~\ref{sec:method} details our methodology for LDIA and MIA in the PDA-FD setting.
Section~\ref{sec:eval} presents our experimental setup and main results. 
Section~\ref{sec:ablation} presents an ablation study examining various factors affecting attack performance.
Section~\ref{sec:assessement} provides an assessment of privacy leakage risk in federated distillation and compares to risk in traditional federated learning.
Section~\ref{sec:related} discusses related work, and Section~\ref{sec:conclusion} concludes the paper with a summary of our findings and directions for future research.
\fi