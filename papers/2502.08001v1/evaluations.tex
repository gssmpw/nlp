\section{Evaluations}
\label{sec:eval}
In this section, we conduct a series of experiments to evaluate the privacy leakage in PDA-FD by conducting the proposed LDIA and MIA.  
% LDIA and MIA method proposed in this paper to measure the privacy information leakage from the client side in Public Dataset-Assisted Federated Distillation frameworks.
Our experiments span multiple datasets, various PDA-FD frameworks~\cite{chang2019cronus, li2019fedmd, itahara2021distillation}, and different usage scenarios. 
Our experiments demonstrate four key aspects: (1) the overall effectiveness of our proposed LDIA and MIA; (2) the impact of varying non-IID data distributions; (3) the impact of different PDA-FD frameworks; and (4) the effect of the number of collaborative training rounds.
% These experiments involved measurements across different datasets, various PDA-FD frameworks\cite{chang2019cronus, li2019fedmd, itahara2021distillation}, and a range of usage scenarios for PDA-FD.


\subsection{Experiment Setup}
\subsubsection{Datasets}
In our experiments, we utilize the following image datasets that are commonly used to test the performance of different FD frameworks: CIFAR-10\cite{krizhevsky2009learning}, CINIC-10\cite{darlow2018cinic}, Fashion-MNIST\cite{xiao2017fashion}.
\iffalse
\begin{itemize} [leftmargin=*]
    \item \textbf{CIFAR-10}\cite{krizhevsky2009learning}.
    The CIFAR-10 dataset is a widely used image recognition dataset that consists of 60,000 color images of 32$\times$32 pixels, divided into 10 classes. The dataset is split into 50,000 training data samples and 10,000 test data samples.
    \item \textbf{CINIC-10}\cite{darlow2018cinic}.
    CINIC-10 is an image classification dataset that combines samples from CIFAR-10 and ImageNet \cite{deng2009imagenet}. It comprises 270,000 images distributed across 10 classes, with each image having dimensions of 32×32×3 pixels.
    \item \textbf{Fashion-MNIST}\cite{xiao2017fashion}.
    Fashion-MNIST is a dataset containing 70,000 grayscale images of 28x28 pixels, categorized into 10 fashion items such as T-shirts, trousers, and sneakers. 
    It is structured with 60,000 training images and 10,000 test images.
\end{itemize}
\fi
Additionally, for the completeness of the experiments, we also use a tabular dataset:Purchase\cite{acquire-valued-shoppers-challenge}.
In our experiments, we partition each dataset into a 4:1 ratio for clients' training sets $D_{train}$ and the public dataset $D_{pub}$.
To align with the previous FD frameworks~\cite{yang2022fd, li2019fedmd}, we also configure a scenario where there is a data distribution discrepancy between the public dataset and the clients' private datasets.
In this scenario, we use CIFAR-10 for client training and CIFAR-100 as the public dataset to simulate distribution shifts.
% the clients' training dataset $D_{train}$ is CIFAR-10, while the public dataset $D_{pub}$ is CIFAR-100.
Table \ref{tab:datasets_division} details the specific partitioning of datasets and the number of classes used in our experiments.
In our MIA experiments, to ensure adequate private data for each client, we select an equal number of samples from the test dataset to serve as non-members. 
\begin{table}[h]
    \caption{Datasets division.}
    \centering
    \resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{c|c|c|c|c}
        \toprule
        Datasets&number of classes&$D_{train}$ & $D_{pub}$ & $D_{test}$\\
        \midrule
        CIFAR-10 & 10 & 40000 & 10000 & 10000 \\
        CIFAR-10/CIFAR-100 & 10 & 40000 & 10000 & 10000 \\
        CINIC-10 & 10 & 72000 & 18000 & 90000  \\
        Fashion-MNIST & 10 & 48000 & 12000 & 10000 \\
        Purchase & 10 & 21589 & 5397 & 11565 \\
        \bottomrule
    \end{tabular}%
    }
    \label{tab:datasets_division}
\end{table}

In our experiments, we create 10 clients that participate in the collaborative training. 
To partition the training dataset $D_{train}$ into private datasets $D_n$ for 10 clients, we use Dirichlet distribution $Dir(\alpha)$ with $\alpha$ values of $0.1$, $1$ and $10$ to generate non-IID data distribution across all the clients. 
The smaller the value of $\alpha$, the more imbalanced the label distribution of $D_n$ is. 

\subsubsection{Models.}
For different datasets, the clients in PDA-FD use different model architectures. 
When the private dataset is CIFAR-10, the clients train the ResNet-18 models~\cite{he2016deep}. 
For CINIC-10, the clients train the MobileNetV2 models~\cite{sandler2018mobilenetv2}.
For Fashion-MNIST datasets, the clients' local models employ a CNN architecture with four convolutional layers. 
When training with the Purchase dataset, the clients train MLP models consisting of three fully connected layers.
As the heterogeneity in client model architectures does not affect our attack methodology\cite{carlini2022membership}, we adopted identical model structures across all clients.

\subsubsection{LDIA Metrics.}
To evaluate the effectiveness of the proposed LDIA, we adopt the same metrics employed in previous LDIA research~\cite{gu2023ldia, salem2020updates}. 
In the equations for calculating these metrics, $\hat{p}$ denotes the inferred label distribution, $p$ denotes the ground truth label distribution, $m$ represents a specific label, and $M$ denotes the number of labels:
\begin{itemize} [leftmargin=*]
    \item \textbf{Kullback-Leibler divergence.}
    Kullback-Leibler divergence(KL divergence) between the ground truth label distribution and the inferred label distribution can be calculated using the following equation: 
    \begin{equation}
        Dis_{KL-div}(\hat{p}, p) = \sum_{m=1}^M \hat{p}_m log(\frac{\hat{p}_m}{p_m})
    \label{eq:KL_metrics}
    \end{equation}
    This metric represents the similarity between the inferred label distribution and the ground truth label distribution. 
    A smaller KL divergence indicates that the two distributions are more closely aligned.
    \item \textbf{Chebyshev distance.}
    The Chebyshev distance represents the maximum error between the inferred label distribution and the ground truth label distribution for each target client in an LDIA:
    \begin{equation}
        Dis_{Cheb}(\hat{p}, p) = max_m\mid \hat{p}_m - p_m \mid
    \label{eq:Cheb_metrics}
    \end{equation}
    A smaller Chebyshev distance indicates a higher reliability of the LDIA results.
    \item \textbf{Mean $l1$-distance.}
    The mean $l1$-distance represents the potential error between the inferred label distribution and the ground truth label distribution across all classes:
    \begin{equation}
        Dis_{mean-l1}(\hat{p}, p) = \frac{1}{M}\sum_{m=1}^M \mid \hat{p}_m - p_m \mid
    \label{eq:meanl1_metrics}
    \end{equation}
    A smaller mean $l1$-distance indicates a higher average accuracy of the LDIA results.
\end{itemize}

\subsubsection{MIA Metrics.}
Same as previous efforts on MIA~\cite{carlini2022membership,shokri2017membership,watson2021importance} , we employ the following metrics to evaluate the effectiveness of the proposed MIAs:
\begin{itemize} [leftmargin=*]
    \item \textbf{TPR at low FPR.} Carlini \etal~\cite{carlini2022membership} suggested using TPR at low FPR to measure MIA. 
    A higher TPR in the low FPR region indicates greater precision of the MIA, which also implies that the attack is more reliable.
    \item \textbf{Balance accuracy and AUC.} These two metrics assess the overall performance of MIA. 
    Balanced accuracy measures the attacker's ability to correctly predict true positives and true negatives across all members and non-members. 
    AUC quantifies the area beneath the ROC curve of the MIA results. 
    It offers a comprehensive measure of the attack's discriminative power across various classification thresholds.
\end{itemize}

\subsubsection{PDA-FD Frameworks.}
To comprehensively evaluate the privacy leakage in the PDA-FD setting, we evaluate three different PDA-FD frameworks in our experiments: FedMD~\cite{li2019fedmd}, DS-FL~\cite{itahara2021distillation}, and Cronus~\cite{chang2019cronus}. 
Each of these FD frameworks employs a distinct approach to enhance the robustness of the FD algorithms.
As mentioned in Section~\ref{sec:background_fd}, they behave differently during the local updates phase and use different aggregation algorithms during the communication phase.
% specifically differing in their local updates phase and in the aggregation algorithms used during the communication phase, which is detailed in Section 2.1.
% In our experiments, each PDA-FD framework involved 10 clients participating in collaborative training. 
In order to optimize the performance of all the PDA-FD frameworks, we should carefully select the number of training epochs in each round.
Following the approach suggested by Li \etal in FedMD~\cite{li2019fedmd}, we initially train the local models to convergence on the private datasets before transitioning to the shorter update cycles during the distillation phase.
Specifically, in the first round of local updates, each client performs 20 epochs of training. 
In the subsequent rounds, this is reduced to 5 epochs of training for each client. 
The knowledge distillation phase consists of 10 epochs of training for each client.
To reduce communication cost, we randomly select 5000 data samples from the public dataset during each communication phase for the CIFAR-10, CIFAR10/CIFAR100, Fashion-MNIST and Purchase datasets, aligned with previous PDA-FD studies~\cite{li2019fedmd}.
The CINIC-10 dataset, given its difficulty level, uses a set of 10000 randomly selected samples to ensure adequate knowledge transfer.
Table \ref{tab:FD_performance} presents the performance of the three PDA-FD frameworks across various Dirichlet distributions and four distinct datasets.
\begin{table}[h]
\caption{Performance of the PDA-FD Frameworks.}
\centering
\scriptsize
\resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Datasets} & 
    \multirow{2}{*}{Setting} & 
    \multirow{2}{*}{Local accuracy} & 
    \multicolumn{3}{c}{Federated accuracy} \\ 
    \cmidrule(lr){4-6}
     & & &
     FedMD & DS-FL & Cronus \\
     \midrule
     CIFAR10 
     & $\alpha$=10  & 54.59\% 
     & 76.61\%  & 71.31\% & 70.81\% \\
     & $\alpha$=1   & 46.06\% 
     & 75.38\% & 68.45\% & 67.90\% \\
     & $\alpha$=0.1  & 22.75\%
     & 60.55\% & 45.01\% & 42.24\% \\
    \midrule
     CIFAR10 
     & $\alpha$=10   & 53.24\% 
     & 72.34\% & 69.54\% & 68.42\% \\
     /CIFAR100 & $\alpha$=1 & 45.49\% & 
     68.41\% & 65.90\% & 64.26\% \\
     & $\alpha$=0.1 & 23.31\%  & 
     49.89\% & 43.55\% & 43.43\% \\
    \midrule
     CINIC10 
     & $\alpha$=10& 39.31\%  
     &67.72\%  &64.21\%  & 62.92\% \\
     & $\alpha$=1& 33.37\%   
     &62.91\%  &57.02\%  & 56.49\% \\
     & $\alpha$=0.1& 20.45\% 
     &41.02\%  &38.48\%  &37.89\%  \\
     \midrule
     Fashion & $\alpha$=10    & 78.80\% 
     & 88.68\% & 87.96\% & 87.62\% \\
     -MNIST  & $\alpha$=1   & 69.35\%
     & 87.98\% & 85.25\% & 84.98\% \\
     & $\alpha$=0.1 & 19.58\% 
     & 80.62\% & 52.45\% & 56.34\% \\
    \midrule
     Purchase 
     & $\alpha$=10  & 82.56\% 
     &  94.58\% & 88.35\% & 88.62\%\\
     & $\alpha$=1 & 72.73\% 
     & 94.01\% & 86.83\% & 89.65\% \\
     & $\alpha$=0.1  & 52.18\% 
     & 91.80\% & 72.55\% & 67.34\% \\
     \bottomrule
    \end{tabular}%
    }
\label{tab:FD_performance}
\end{table}

\subsection{Experiment Results of LDIA}
In the LDIA experiments, the PDA-FD server infers the label distribution of all clients' private datasets during the communication phase in each round.
To ensure robustness and account for potential fluctuations, we compute the final LDIA result for each client by averaging the server's inferred label distribution over 10 collaborative training rounds.
The overall effectiveness of the attack is evaluated by averaging these final results across all clients.
% To comprehensively reflect the results of the server's LDIA attack, we report the average of the server's LDIA results across all clients. 
To provide a meaningful benchmark for our LDIA method, we establish a baseline comparison, denoted as ``Random'', following the same approach of previous LDIA research~\cite{gu2023ldia, salem2020updates}. 
This baseline employs randomly generated label distributions for each client's private dataset, serving as a lower bound for attack performance.
Note that for a given dataset and Dirichlet distribution parameter, the private dataset of each client remains constant across different PDA-FD frameworks. 
Therefore, within the same dataset and Dirichlet distribution, there is only one set of Random LDIA results.

\textbf{Main Result.} Table \ref{tab:ldia_main_result} presents the performance of the proposed LDIA on five different datasets across three PDA-FD frameworks. 
The results demonstrate the server's capability to launch effective LDIA against clients across these datasets, significantly outperforming the random guess baseline on all three key metrics.
For instance, for the DS-FL framework on the CIFAR-10 dataset with $\alpha$=1, our proposed LDIA achieves an average Chebyshev distance of 0.10, an average mean l1-distance of 0.03, and an average KL-divergence of 0.10 across all clients. 
In contrast, the random guess baseline yields substantially higher values: 0.20, 0.08, and 0.66 for the respective metrics. 
This significant improvement underscores the efficacy of our LDIA in accurately inferring clients' label distributions.
Figure \ref{fig:ldia_main_result} provides a visual representation of the LDIA results for the DS-FL server on the CIFAR-10 dataset, offering a clearer illustration of the experiment results.
We can see from the figure that for the labels whose inferred proportions deviate from the ground truth values, the relative rankings of label frequencies are consistently preserved.
This observation highlights the robustness of the proposed LDIA in capturing the essential structure of label distributions.

\begin{table*}[h]
\caption{Performance of the server in conducting LDIA within the different PDA-FD Frameworks.}
\centering
\scriptsize
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{lccccccccccccc}
\toprule
\multirow{2}{*}{Datasets} & 
\multirow{2}{*}{Setting} & 
\multicolumn{4}{c}{KL divergence} & 
\multicolumn{4}{c}{Chebyshev distance} & 
\multicolumn{4}{c}{Mean $l1$-distance} \\ 
\cmidrule(lr){3-6}
\cmidrule(lr){7-10}
\cmidrule(lr){11-14}
 & & 
 FedMD & DS-FL & Cronus & Random &
 FedMD & DS-FL & Cronus & Random &
 FedMD & DS-FL & Cronus & Random \\ 
 \midrule
 CIFAR10 
 & $\alpha$=10 
 & 0.02 & 0.01 & 0.01 & 0.42
 & 0.04 & 0.03 & 0.02 & 0.13
 & 0.01 & 0.01 & 0.01 & 0.05\\
 & $\alpha$=1   
 & 0.17 & 0.10 & 0.08 & 0.66
 & 0.14 & 0.11 & 0.10 & 0.20
 & 0.04 & 0.03 & 0.03 & 0.08\\
 & $\alpha$=0.1 
 & 0.15 & 0.11 & 0.07 & 1.93
 & 0.18 & 0.16 & 0.14 & 0.57
 & 0.04 & 0.03 & 0.03 & 0.15\\
\midrule
 CIFAR10 
 & $\alpha$=10  
 & 0.07 & 0.05 & 0.06 & 0.40
 & 0.07 & 0.06 & 0.06 & 0.12
 & 0.02 & 0.02 & 0.02 & 0.05\\
 /CIFAR100 & $\alpha$=1 
 &0.11 & 0.10 & 0.10 & 0.59
 &0.10 & 0.11 & 0.11 & 0.22
 &0.03 & 0.03 & 0.03 & 0.08\\
 & $\alpha$=0.1 
 &0.11 & 0.10 & 0.08 & 1.59
 &0.15 & 0.13 & 0.14 & 0.51
 &0.03 & 0.03 & 0.03 & 0.14\\
\midrule
 CINIC10
 & $\alpha$=10
 &0.01 & 0.01 & 0.01 &0.64  
 &0.02 & 0.02 & 0.04 &0.15
 &0.01 & 0.01 & 0.01 &0.07\\
 & $\alpha$=1 
 &0.06 & 0.05 & 0.05 &0.57  
 &0.11 & 0.11 & 0.10 &0.21
 &0.02 & 0.02 & 0.02 &0.08\\
 & $\alpha$=0.1 
 &0.09 & 0.08 & 0.01 &1.99  
 &0.14 & 0.14 & 0.04 &0.56
 &0.03 & 0.03 & 0.01 &0.15\\
 \midrule
 Fashion & $\alpha$=10  
 & 0.03 & 0.02 & 0.02 & 0.32
 & 0.04 & 0.04 & 0.04 & 0.12
 & 0.02 & 0.01 & 0.01 & 0.06\\
 -MNIST  & $\alpha$=1  
 & 0.14 & 0.12 & 0.12 & 0.55
 & 0.10 & 0.09 & 0.09 & 0.15
 & 0.04 & 0.03 & 0.03 & 0.06\\
 & $\alpha$=0.1 
 & 0.21 & 0.05 & 0.06 & 1.54 
 & 0.20 & 0.09 & 0.11 & 0.45
 & 0.04 & 0.02 & 0.02 & 0.13\\
\midrule
 Purchase 
 & $\alpha$=10 
 &  0.08 & 0.03 & 0.03 & 0.47
 &  0.06 & 0.05 & 0.05 & 0.13
 &  0.02 & 0.02 & 0.02 & 0.06\\
 & $\alpha$=1 
 & 0.27 & 0.14 & 0.15 & 0.68
 & 0.13 & 0.10 & 0.10 & 0.18
 & 0.06 & 0.04 & 0.04 & 0.09\\
 & $\alpha$=0.1 
 & 0.64 & 0.14 & 0.14 & 2.11
 & 0.32 & 0.15 & 0.17 & 0.52
 & 0.10 & 0.03 & 0.03 & 0.15\\
 \bottomrule
\end{tabular}%
}
\label{tab:ldia_main_result}
\end{table*}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/dsfl_result_alpha_10.png}
        \caption{$\alpha=10$}
        \label{fig:subfig1}
    \end{subfigure}

    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/dsfl_result_alpha_1.png}
        \caption{$\alpha=1$}
        \label{fig:subfig2}
    \end{subfigure}
    
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/dsfl_result_alpha_0.1.png}
        \caption{$\alpha=0.1$}
        \label{fig:subfig3}
    \end{subfigure}

    \caption{The LDIA performance of the DS-FL server on the CIFAR-10 dataset, under three distinct Dirichlet distributions. The images depict the best (left), median (center), and worst (right) LDIA results across all client models.}
    \label{fig:ldia_main_result}
\end{figure}

\textbf{Different Data Distributions.} 
Our experiments reveal a notable relationship between the effectiveness of the proposed LDIA and the uniformity of clients' label distributions.
Specifically, the LDIA demonstrates lower KL-divergence, Chebyshev distance, and mean $l1$-distance as the clients' label distributions become more uniform.
This trend is clearly illustrated in our experiments using the CIFAR-10 dataset within the DSFL framework. 
As $\alpha$ increases, indicating a more uniform label distribution across clients, the LDIA achieves better performance.
% When the label distribution differs, the server's LDIA becomes more effective as the clients' label distribution becomes more uniform. 
% Experiments on the CIFAR-10 dataset in the DSFL framework show that with a higher $\alpha$ (more uniform distribution), the LDIA results in lower KL-divergence, Chebyshev distance, and mean $l1$-distance. 
Conversely, as $\alpha$ decreases, indicating a more skewed distribution, we see an increase in the three key metrics.
Nonetheless, the attack remains effective despite the reduced accuracy.

\textbf{Different PDA-FD Frameworks.} 
Our evaluations also reveal significant differences in the vulnerability of various PDA-FD frameworks to LDIA.
Compared to FedMD, the server can achieve more effective LDIA on clients within the DS-FL and Cronus frameworks.
This can be attributed to the unique training approach employed by FedmD during its first collaborative training round.
In FedMD, clients first train their local models on the public dataset before transitioning to their private dataset.
% This occurs because during the first collaborative training round's local updates phase in FedMD, all clients train the local model on public datasets before training on their private dataset. 
This process serves as a form of regularization, thus mitigating overfitting to private datasets and, consequently, reducing vulnerability to LDIA.
However, this effect is diminished when private and public datasets differ significantly or when the public dataset is unlabeled.
In these cases, FedMD's LDIA vulnerability becomes comparable to that of the other two PDA-FD frameworks, as evidenced by the results in Table \ref{tab:ldia_main_result} for the CIFAR-10/CIFAR-100 datasets.
The similarity in LDIA vulnerability arises from the data distribution shift between public and private datasets, causing clients' local models to reduce memorization of the public dataset after converging on their private datasets.
As a result, the clients' local models end up overfitting to their private datasets to a similar degree across all frameworks.
% As a result, this does not reduce the overfitting level of the clients' local models to their private datasets.

\textbf{Different Collaborative Training Rounds.} 
To evalutate the temporal dynamics of LDIA, we analyze its performance across multiple collaborative training rounds in various PDA-FD frameworks, as illustrated in Figure \ref{fig:ldia_result_diff_rounds}.
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/ldia_01_diff_rounds.png}
    \caption{$\alpha=0.1$}
    \label{fig:ldia_result_diff_rounds_0.1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/ldia_1_diff_rounds.png}
    \caption{$\alpha=1$}
    \label{fig:ldia_result_diff_rounds_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{figures/ldia_10_diff_rounds.png}
    \caption{$\alpha=10$}
    \label{fig:ldia_result_diff_rounds_10}
  \end{subfigure}
    \caption{Chebyshev distance results of LDIA performed by the PDA-FD server on the CIFAR-10 dataset, shown for each collaborative training round.}
    \label{fig:ldia_result_diff_rounds}
\end{figure}
% illustrates the server's LDIA performance on clients within the PDA-FD framework across multiple rounds. 
We aggregate the server's attack results across all clients for each round, to represent the overall LDIA performance over time.
The results reveal that the server successfully executes LDIA on clients in every round. 
Notably, the LDIA performance remains relatively stable as the number of collaborative training rounds increases, showing neither significant improvement nor decline.
This consistency can be attributed to the local updates phase preceding communication in each collaborative training round within PDA-FD frameworks. 
While this phase enhances knowledge transfer among clients, it simultaneously increases the degree of overfitting of each client's local model to their private data. 
This dual effect contributes to the observed stability in LDIA performance over multiple rounds.
Despite this stability, we recommend using the averaged LDIA results over multiple rounds as the final outcome to enhance the robustness of the results. 
Such an approach mitigates potential fluctuations and provides a more reliable measure of the server's LDIA capabilities.




\subsection{Experiment Results of MIA}
\label{sec:MIA_eval}
In our MIA experiments, we evaluate the effectiveness of the proposed attack against 10 clients during the communication phase across three PDA-FD frameworks. 
These experiments use three different Dirichlet distributions and four distinct datasets. 
This comprehensive setup allows for a thorough assessment of MIA vulnerability under various data distribution scenarios and PDA-FD frameworks.
Previous FD MIA studies \cite{wang2024graddiff, liu2023mia} assume that attackers have access to shadow datasets matching the target model’s training data distribution. However, we find these assumptions too restrictive and unrepresentative of real-world scenarios. Therefore, our attack methods do not rely on such assumptions, so we do not use these studies as baselines for comparison.

\textbf{Main Result.} We first evaluate co-op LiRA. 
Given that co-op LiRA is applicable in scenarios where clients' label distributions are similar, we conduct experiments across different datasets using a Dirichlet distribution with $\alpha=10$. This parameter setting ensures a more uniform distribution of labels across clients, aligning with co-op LiRA's operational scenario. 
Table \ref{tab:efficient_mia_main_result} presents the performance of co-op LiRA during the communication phase of the first
collaborative training round.
Our findings reveal that when the server attacks a specific client, utilizing only the other 9 clients' models as the reference models yields remarkably effective attack results.
This observation underscores the high efficiency and practicality of co-op LiRA, demonstrating its capability to achieve effective MIA without the need to train any additional reference models.

\begin{table*}[h]
\caption{Performance of the server in conducting co-op LiRA within the different PDA-FD Frameworks.}
\centering
\scriptsize
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{lccccccccccccc}
\toprule
\multirow{2}{*}{Datasets} &
\multicolumn{3}{c}{TPR at 1\% FPR} & 
\multicolumn{3}{c}{TPR at 0.1\% FPR} & 
\multicolumn{3}{c}{AUC} & 
\multicolumn{3}{c}{Balance Accuracy} \\ 
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}
\cmidrule(lr){11-13}
 &
 FedMD & DS-FL & Cronus & 
 FedMD & DS-FL & Cronus & 
 FedMD & DS-FL & Cronus & 
 FedMD & DS-FL & Cronus \\ 
 \midrule
 CIFAR10 
 & 22.15\% & 21.96\% &20.35\% 
 & 6.67\%  & 6.39\%  &5.76\% 
 & 0.819   & 0.850   &0.840 
 & 74.07\% & 77.11\% &76.23\% \\
\midrule
 CINIC10 
 & 10.97\% & 11.23\% & 10.99\%
 & 1.78\%  & 1.80\%  & 1.79\%
 & 0.794   & 0.811   & 0.815
 & 71.55\% & 73.89\% & 74.28\%\\
 \midrule
 Fashion-MNIST
 &3.24\% &1.80\% &1.64\%
 &1.09\% &0.31\% &0.33\%
 &0.582  &0.533  &0.531
 &55.89\%&55.38\%&54.28\% \\
\midrule
 Purchase 
 & 5.85\%  & 4.08\%  & 4.45\% 
 & 1.67\%  & 0.71\%  & 0.61\%
 & 0.616   & 0.712   & 0.709
 & 58.41\% & 66.46\% & 66.10\%\\
 \bottomrule
\end{tabular}%
}
\label{tab:efficient_mia_main_result}
\end{table*}

We subsequently evaluate the performance of distillation-based LiRA. 
For each client, we distill 32 reference models. 
The distillation dataset for each reference model consists of a randomly sampled 80\% subset of the public dataset used in the communication phase. 
This approach ensures a diverse set of reference models.
The model architecture of reference model is same as that of the target client model.
Table \ref{tab:distillation_mia_main_result} presents the average results of the server's MIA against all clients during the communication phase during the first collaborative training round. 
The results reveal that the server can launch highly effective MIA against the clients in the non-IID scenarios.
Figure \ref{fig:distillation_mia_main_result} presents the results of MIA experiments conducted in the DS-FL framework using the CIFAR-10 dataset, across various Dirichlet distributions.
Notably, we observe that even when the private dataset (CIFAR-10) and public dataset (CIFAR-100) have significantly different distributions, the server can still successfully launch MIA against clients by leveraging the distilled reference models from the public dataset. 
This finding underscores the efficacy of distillation-based LiRA in the PDA-FD frameworks, demonstrating its robustness to dataset disparities between the public and private data.

\begin{table*}[h]
\caption{Performance of the server in conducting distillation-based LiRA within the different PDA-FD Frameworks.}
\centering
\scriptsize
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{lccccccccccccc}
\toprule
\multirow{2}{*}{Datasets} & 
\multirow{2}{*}{Setting} & 
\multicolumn{3}{c}{TPR at 1\% FPR} & 
\multicolumn{3}{c}{TPR at 0.1\% FPR} & 
\multicolumn{3}{c}{AUC} & 
\multicolumn{3}{c}{Balance Accuracy} \\ 
\cmidrule(lr){3-5}
\cmidrule(lr){6-8}
\cmidrule(lr){9-11}
\cmidrule(lr){12-14}
 & & 
 FedMD & DS-FL & Cronus & 
 FedMD & DS-FL & Cronus & 
 FedMD & DS-FL & Cronus & 
 FedMD & DS-FL & Cronus \\ 
 \midrule
 CIFAR10 
 & $\alpha$=10 
 & 20.94\% &35.76\% & 32.69\%
 & 10.42\% &19.42\% & 14.40\%
 & 0.764   &0.902   & 0.867
 & 69.68\% &82.01\% & 78.51\%\\
 & $\alpha$=1   
 & 17.62\% &29.28\% & 23.83\%
 & 8.11\%  &11.29\% & 5.77\%
 & 0.730   &0.839   & 0.804
 & 66.93\% &76.20\% & 72.94\%\\
 & $\alpha$=0.1 
 & 9.74\%  &12.89\% & 7.20\%
 & 2.09\%  &3.74\%  & 1.13\%
 & 0.618   &0.680   & 0.639
 & 58.91\% &63.49\% & 60.64\%\\
\midrule
 CIFAR10 
 & $\alpha$=10  
 & 11.20\% &34.61\% & 28.28\%
 & 1.48\%  &10.92\% & 5.49\%
 & 0.804   &0.901   & 0.891
 & 72.74\% &81.93\% & 80.97\%\\
 /CIFAR100 & $\alpha$=1 
 & 9.47\%  &25.94\% & 19.32\%
 & 0.93\%  &2.40\%  & 1.95\%
 & 0.758   &0.844   & 0.821
 & 68.92\% &76.61\% & 73.68\%\\
 & $\alpha$=0.1 
 & 7.79\%  &11.34\% & 5.61\%
 & 0.74\%  &0.51\%  & 0.88\%
 & 0.627   &0.686   & 0.652
 & 59.38\% &63.67\% & 61.75\%\\
\midrule
 CINIC10 
 & $\alpha$=10
 &13.83\% & 17.32\% & 15.91\%
 &3.12\%  & 4.15\%  & 3.85\%
 &0.741   & 0.855   & 0.834
 &71.42\% & 77.57\% & 75.26\% \\
 & $\alpha$=1 
 &10.96\% & 13.94\% & 12.07\%
 &2.37\%  & 3.59\%  & 3.01\%
 &0.704   & 0.781   & 0.757
 &68.93\% & 70.89\% & 69.21\%\\
 & $\alpha$=0.1 
 &5.91\%  & 6.81\% & 6.46\%
 &1.25\%  & 1.94\% & 1.72\%
 &0.649   & 0.652  & 0.661
 &60.27\% & 61.18\%& 62.59\% \\
 \midrule
 Fashion & $\alpha$=10  
 &3.07\% &1.85\%  & 1.73\%
 &0.91\% &0.35\%  & 0.21\%
 &0.588  &0.539   & 0.528
 &55.94\%&59.85\% & 55.43\% \\
 -MNIST  & $\alpha$=1  
 &3.30\% &1.71\% &  1.62\%
 &0.69\% &0.26\% &  0.25\%
 &0.583  &0.536  &  0.522
 &56.47\%&59.51\%&  54.31\%\\
 & $\alpha$=0.1 
 &1.88\% &1.39\% &  1.21\%
 &0.54\% &0.19\% &  0.23\%
 &0.538  &0.523  &  0.519
 &52.71\%&52.35\%&  51.32\%\\
\midrule
 Purchase 
 & $\alpha$=10 
 & 1.94\%   &5.91\%  & 2.43\%
 & 0.77\%   &1.31\%  & 1.93\%
 & 0.539    &0.665   & 0.706
 & 53.34\%  &62.69\% & 65.62\%\\
 & $\alpha$=1 
 & 1.98\%   &5.64\%  & 2.69\%
 & 0.83\%   &1.69\%  & 0.68\%
 & 0.534    &0.654   & 0.653
 & 53.31\%  &61.49\% & 62.24\%\\
 & $\alpha$=0.1 
 & 1.41\%  &5.04\% &  3.04\%
 & 0.42\%  &1.21\% &  1.19\%
 & 0.507   &0.591  &  0.588
 & 52.24\% &57.93\% & 57.69\%\\
 \bottomrule
\end{tabular}%
}
\label{tab:distillation_mia_main_result}
\end{table*}

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.325\linewidth}
    \includegraphics[width=\linewidth]{figures/mia_dsfl_01.png}
    \caption{$\alpha=0.1$}
    \label{fig:distill_lira_overall}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.325\linewidth}
    \includegraphics[width=\linewidth]{figures/mia_dsfl_1.png}
    \caption{$\alpha=1$}
    \label{fig:distill_lira_high}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.325\linewidth}
    \includegraphics[width=\linewidth]{figures/mia_dsfl_10.png}
    \caption{$\alpha=10$}
    \label{fig:distill_lira_high}
  \end{subfigure}
    \caption{Distillation-based LiRA performance of the DS-FL server on the CIFAR-10 dataset, presented as log-scale ROC curves under three distinct Dirichlet distributions.}
    \label{fig:distillation_mia_main_result}
\end{figure}

\textbf{Different Data Distributions.}
We observe that the effectiveness of the proposed distillation-based LiRA attacks on clients decreases as the clients' label distributions become more imbalanced. 
This phenomenon can be explained by the fact that local models trained on datasets with highly skewed label distributions tend to produce disproportionately high posterior probabilities for the dominant labels.
This bias also affects the non-member samples that come from the same over-represented classes.
The core principle of LiRA relies on difficulty calibration, which becomes less effective in imbalanced scenarios.
As a result of this, the attacker's capability to discriminate between members and non-members is compromised.
This leads to a overall degradation in the performance of distillation-based LiRA on clients with highly imbalanced label distributions.
% We hypothesize this is due to local models trained on datasets with disproportionate label representation producing high posterior probabilities for dominant labels, even for non-member samples. This reduces the efficacy of difficulty calibration in MIA, as the distinction between member and non-member samples becomes less pronounced, ultimately degrading distillation-based LiRA performance on clients with highly imbalanced label distributions.

\textbf{Different PDA-FD Frameworks.}
The effectiveness of co-op LiRA remains relatively consistent across FedMD, DS-FL, and Cronus. 
However, for distillation-based LiRA, the effectiveness of MIAs ranks as follows: DS-FL achieves the best performance, followed by Cronus, with FedMD showing the least effectiveness.
In Cronus, clients upload softmax-processed posterior probability vectors rather than raw logits for public data during the communication phase. 
Compared to logits, the use of posterior probability vectors diminishes the server's ability to distill reference models that closely mimic the target model's performance.
Consequently, this limitation leads to a reduction in the effectiveness of MIA.
In the FedMD framework, clients train on public data before their private datasets during the local updates phase in their first collaborative training round. 
This process leads to clients training on all the target samples strategically selected by the server, regardless of their membership status.
While experiments demonstrate that subsequent training on private datasets reduces clients' memorization of public data, this initial exposure still impacts the server's MIA results.
However, when the public dataset is unlabeled, clients cannot train on it during the first collaborative training round. In this scenario, the server's MIA performance on clients remains unaffected.

\textbf{Different Collaborative Training Rounds.}
Figure \ref{fig:mia_diff_round} illustrates the performance of the proposed MIAs in different PDA-FD frameworks across multiple collaborative training rounds on the CIFAR-10 dataset. 
To evaluate each MIA approach in its intended scenario, for co-op LiRA, we employ a Dirichlet distribution parameter $\alpha$=10.
While for distillation-based LiRA, we use $\alpha$=1. 
We use the average TPR at 1\% FPR of MIA across all clients as the metric to quantify performance.
The performance of distillation-based LiRA declines with more collaborative training rounds but eventually stabilizes.
We attribute this to the FD process, which gradually reduces the degree of overfitting of local models to their private data. 
While local updates maintain some private data memorization, the performance gap between local and distilled reference models for private data narrows over time.
The performance of co-op LiRA remains relatively stable as the collaborative training rounds increase. 
We attribute this stability to two key factors. 
First, the non-target clients in co-op LiRA cannot effectively learn membership information from the server-aggregated logits during the communication phase. 
Second, while the FD process reduces the overfitting level of local models to their private data, the local updates phase, where each client trains exclusively on its private dataset, maintains a consistent performance gap between local and reference models for their respective private data. 
This balance between reduced overfitting and continued exclusive training on private data likely contributes to the stability of co-op LiRA's performance across collaborative rounds.
\begin{figure}[h]
\centering
  \resizebox{0.9\linewidth}{!}{%
  \begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figures/mia_efficient_diff_rounds.png}
    \caption{Co-op LiRA}
    \label{fig:mia_diff_round_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figures/mia_distillation_diff_rounds.png}
    \caption{Distillation-based LiRA}
    \label{fig:mia_diff_round_2}
  \end{subfigure}
  }
    \caption{MIA performance across training rounds.}
    \label{fig:mia_diff_round}
\end{figure}
