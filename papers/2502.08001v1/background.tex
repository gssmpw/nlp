\section{Background}
\label{sec:background}
\subsection{Federated Distillation}
\label{sec:background_fd}
%\haonan{We might could delete the discription of FL?}
%Federated Learning (FL)~\cite{mcmahan2017communication} is a collaborative training framework designed to preserve the data privacy of multiple clients while enhancing the performance of each client's model. 
%In traditional FL frameworks, such as FedAvg and FedSGD~\cite{mcmahan2017communication}, during each round of collaborative training, clients are required to upload their model parameters or gradients to the server for updates.
%However, this inevitably introduces limitations in both privacy and utility within the FL framework.
%In terms of privacy, since the server has white-box access to the clients' local models, it opens the door to a range of privacy attacks~\cite{gu2023ldia, nasr2019comprehensive, hu2021source, wang2022poisoning} that may leak clients' sensitive information. 
%Regarding utility, due to the aggregation algorithms used in traditional FL frameworks~\cite{mcmahan2017communication}, the clients’ models are required to have similar architectures. 
%Moreover, frequent exchanges of model parameters between clients and the server are needed in FL. 
%This results in significant communication overhead in FL, especially for deep learning models with a substantial number of parameters.

Federated Distillation (FD)~\cite{li2019fedmd, itahara2021distillation, chang2019cronus, jeong2018communication, sui2020feded} is a specialized FL framework distinct from traditional FL. 
FD exchanges model outputs or distilled knowledge between the server and clients instead of model parameters, which significantly reduces communication overhead.
Additionally, FD only requires black-box access to client models and supports diverse model architectures across clients. 
As a result, FD not only better preserves privacy but also offers greater utility compared to traditional FL frameworks.

In our study, we focus on one category of FD frameworks, \textbf{Public Dataset-Assisted Federated Distillation}~\cite{li2019fedmd, itahara2021distillation, chang2019cronus}(PDA-FD).
The workflow of PDA-FD is shown in Figure \ref{fig:FD_workflow}.
In PDA-FD, the server leverages a public dataset to facilitate knowledge transfer among clients. 
The public dataset is shared with all the clients. 
The PDA-FD framework typically involves three phases in each collaborative training round: local updates phase, communication phase, and knowledge distillation phase.

During the local updates phase, client $n$ trains its local model $\theta_{n}$ on its private dataset $D_{n}$ using stochastic gradient descent~\cite{lecun1998gradient}. 
% During the training process of local model,
% for training data $\left(x,y\right)\in D_{n}$, 
The loss function $\mathcal{L}(x, y, \theta_{n})$ is defined to calculate the error between the prediction posterior $f_{\theta_{n}}(x)_y$ of the training data and its ground truth label $y$. 
Cross-entropy is often used as the loss function:
\begin{equation}
\mathcal{L}(x, y, \theta_{n}) = -log(f_{\theta_{n}}(x)_y)
\label{eq:loss_function}
\end{equation}
\iffalse
The training of local model utilizes stochastic gradient descent~\cite{lecun1998gradient} to minimize the loss function:
\begin{equation}
\theta_{n}\leftarrow \theta_{n} - \eta_n \sum_{(x,y) \in B} \nabla_{\theta_{n}} \mathcal{L}(x, y, \theta_{n})
\label{eq:stochastic_gradient_descent}
\end{equation}
where $B$ is a batch of training data from $D_{n}$, $\eta_n$ is the learning rate for updating the local model.
\fi

During the communication phase, a set of data samples $S_t$ are selected by the server from the public dataset. 
% The index set $S_t$ is decided by the server.
For each selected sample $x_k$, the client’s local model $\theta_{n}$ performs inference to obtain the corresponding logits $z_{\theta_{n}}(x_k)$ and sends the logits to the server. 
After collecting the logits from all clients, the server aggregates them using an aggregation algorithm $f_{agg}$(eg. average aggregation~\cite{li2019fedmd}) to get an aggregated logits $Z_k$. 
% \haonan{For example, if the aggregation algorithm is a mean algorithm, and there are $N$ clients in this collaborative training round, The aggregation algorithm $f_{agg}$ can be shown as follows:
% \begin{equation}
%  f_{agg}\left( z_{\theta_{1}}(x_k), z_{\theta_{2}}(x_k), \dots, z_{\theta_{n}}(x_k) \right) = \frac{1}{N}\sum_{n=1}^{N} z_{\theta_{n}}(x_k) = Z_k
%  \label{eq:aggregate_algorithm}
%  \end{equation}}
The server then distributes $Z_k$ back to each client.
At the end of the communication phase, each client will receive the logits set $\{ Z_k \mid k \in S_t \}$ from server.

During the knowledge distillation phase, client $n$ performs knowledge distillation~\cite{hinton2015distilling} using the aggregated logits $Z_k$ returned by the server as the soft labels. 
% During this process, the local models are trained using Equation \ref{eq:stochastic_gradient_descent}. 
In this case, the loss function also needs to include the mean absolute errors (MAE):
\begin{equation}
\mathcal{L}(x, y, \theta_{n}) = \frac{1}{N} \sum_{n=1}^{N} \left| Z_k - z_{\theta_n}(x_k) \right|
\label{eq:loss_function_mae}
\end{equation}
or Kullback-Leibler (KL) divergence values~\cite{kullback1951information}:
\begin{equation}
\mathcal{L}(x, y, \theta_{n}) = \sum_{n=1}^{N} Z_k \cdot \log \left( \frac{Z_k}{z_{\theta_n}(x_k)} \right)
\label{eq:loss_function_kl}
\end{equation}
The overall procedure of PDA-FD learning is summarized in Algorithm \ref{alg:FD_algorithm}. Different PDA-FD frameworks~\cite{chang2019cronus, li2019fedmd, itahara2021distillation} require clients to upload either logit vectors or prediction probability vectors during communication.
\begin{algorithm}
\caption{Public Dataset-Assisted Federated Distillation}
\label{alg:FD_algorithm}
\begin{algorithmic}[1]
    \REQUIRE Private datasets $\{D_n\}_{n=1}^N$, public dataset $D_{pub}$, local models $\{\theta_n\}_{n=1}^N$, number of collaborative training round $T$, public data index set $\{S_t\}_{t=1}^T$.
    \FOR{collaborative training round $t = 0$ to $T$} 
       \STATE {\color{blue}\textit{$\triangleright$ Local Updates Phase}}
       \STATE Each client trains local model $\theta_{n}$ on private dataset $D_{n}$
       \STATE {\color{blue}\textit{$\triangleright$ Communication Phase}}
       \STATE Each client computes logits $\{z_{\theta_n}(x_k) \mid k \in S_t\}$ for public data $\{x_k|k \in S_t\}$
       \STATE Each client sends logits set $\{z_{\theta_n}(x_k) \mid k \in S_t\}$ to server
       \FOR{$k \in S_t$}
           \STATE $Z_k \leftarrow f_{agg}(\{z_{\theta_n}(x_k)\}_{n=1}^N)$
       \ENDFOR
       \STATE Server sends aggregated logits $\{Z_k \mid k \in S_t\}$ to clients
       \STATE {\color{blue}\textit{$\triangleright$ Knowledge Distillation Phase}}
       \STATE Each client trains $\theta_{n}$ on $\{x_k \mid k \in S_t\}$ using $\{Z_k\}$ as soft labels
   \ENDFOR
\end{algorithmic}
\end{algorithm}

In this paper, we primarily focus on three PDA-FD frameworks: FedMD~\cite{li2019fedmd}, DS-FL~\cite{itahara2021distillation} and Cronus~\cite{chang2019cronus}. 
% They each have their own unique characteristics compared to Algorithm 1.
Each has a specific customization of the procedure demonstrated in Algorithm \ref{alg:FD_algorithm}.
In FedMD, each client $n$ needs to train their local model $\theta_{n}$ on the public dataset $D_{pub}$ until convergence before the collaborative training.
In DS-FL, the server employs the entropy reduction aggregation (ERA)\cite{itahara2021distillation} algorithm as $f_{agg}$ during the communication phase. 
ERA accelerates convergence and enhances the robustness of DS-FL in non-IID data distribution scenarios.
In Cronus, the server utilizes the mean estimation algorithm proposed by Diakonikolas \etal~\cite{diakonikolas2017being} for logits aggregation algorithm $f_{agg}$ to enhance robustness.


\subsection{Label Distribution Inference Attack}
In both FL and FD, Label Distribution Inference Attacks (LDIA) can pose significant threats to individual clients' privacy.
In an LDIA, the adversary aims to infer the proportion of training data across different labels in the target client’s training dataset.
Previous work~\cite{gu2023ldia, wainakh2021user} has demonstrated that the server of FL can infer the distribution of clients' training data by exploiting the gradients or parameters uploaded by clients in FL, leading to privacy leakage.

\BfPara{Definitions} 
Given a target client's local model $\theta_{n}$ that is trained on a private dataset $D_{n}$, which consists of data samples from $M$ classes: $D_n = \bigcup_{m=1}^{M} D_n^m$, where $D_n^m$ refers to the subset containing data of label m.
The ground truth label distribution of $D_{n}$ is calculated as follows:
\begin{equation}
    \mathbf{p} = \left( p_1, p_2, \dots, p_M \right), \quad p_m = \frac{|D_n^m|}{|D_n|}
\end{equation}
where $p_m$ represents the proportion of data with label $m$ in $D_{n}$, and $\sum_{m=1}^{M} p_m = 1$ and $p_m \in [0,1]$.
The objective of LDIA can be expressed as a function $\mathcal{A}$ that maps from the observable information about the target model outputs to an estimated label distribution:
\begin{equation}
    \mathcal{A} : \theta_n \mapsto \mathbf{\hat{p}} = \left( \hat{p}_1, \hat{p}_2, \dots, \hat{p}_M \right)
\end{equation}
where $\hat{p}_m$ represents the inferred proportion of data with label $m$ in $D_{n}$. 
% The output of $\mathcal{A}$ is the inferred label distribution of the training dataset of local model $\theta_{n}$.

The success of LDIA can compromise the privacy of FL and FD systems, as it allows attackers to gain insights into the composition of private datasets without direct access.


\subsection{Membership Inference Attack}
In membership inference attacks (MIA), the attacker's objective is to determine whether a target data sample is in the target model's training dataset.
As machine learning models become more widely deployed, their training datasets may contain sensitive information~\cite{havaei2017brain, fu2016credit}, and MIA poses a significant threat to the privacy of such data.
FD, which was designed to protect data privacy, has also become the target of various MIA attacks~\cite{yang2022fd, wang2024graddiff} against clients' local models within the framework.

\BfPara{Definitions} Given a target model $\theta$, and a target data sample $x$, the objective of MIA can be define as a function $\mathcal{A}$:
\begin{equation}
    \mathcal{A} : x, \theta \mapsto \{ 0, 1\}
\end{equation}
where the output is 1 if $x$ is inferred to be in the training dataset of model $\theta$, and 0 otherwise. 
In MIA, we refer to the data sample belonging to the target model's training set as members, while other data samples are referred to as non-members.
% MIA exploits the overfitting characteristics of models to attack the target model. 
MIA exploits the behavioral differences of machine learning models on data they were trained on versus data they weren't, leveraging the fact that models often exhibit higher confidence or lower loss on their training data due to overfitting.

\iffalse
Yeom \etal~\cite{yeom2018privacy} proposed using the target sample's loss value on the target model as the membership score to determine whether the sample is a member. 
Typically, members exhibit lower loss values on the target model, while non-members tend to have relatively higher loss values. 
To enhance the precision of MIA, some approaches~\cite{carlini2022membership, watson2021importance} have introduced difficulty calibration. 
In this type of MIA, attackers possess an auxiliary dataset with the same data distribution as the target model's training dataset, which they use to train a reference model.
They achieve more precise MIA by exploiting the difference in membership scores between members and non-members on both the target model and the reference model. 
Since members are in the target model's training set but not in the reference model's, there is a significant difference in their membership scores between the two models. 
In contrast, non-members, as they are not in the training sets of either model, tend to have similar membership scores across the target model and the reference model. 
These membership scores can be based on confidence scores or loss values of the target data sample on the models.
\fi