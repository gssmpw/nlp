\section{Ablation Study}
\label{sec:ablation}
\subsection{Public Dataset Size}
The server in PDA-FD can control communication overhead by adjusting the size of the public dataset used in each collaborative training round.
We investigate its impact on the performance of LDIA and MIA. 
As public data does not affect co-op LiRA, our evaluations of MIA mainly focus on distillation-based LiRA. 
In our experiments, we use the DS-FL framework on the CIFAR-10 dataset with $\alpha=1$.
\begin{table}[h]
    \caption{Impact of Public Data Quantity on Label Distribution and Membership Information Leakage in PDA-FD.}
    \centering
    \scriptsize
    \resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{c|c|c}
        \toprule
        Datasets size&  MIA (TPR at 1\%FPR) & LDIA (KL divergence)\\
        \midrule
        5000 & 29.28\% &  0.10  \\
        7500 & 31.84\% &  0.09 \\
        10000& 32.01\% &  0.07 \\
        \bottomrule
    \end{tabular}%
    }
    \label{tab:public_dataset_size}
\end{table}
Table \ref{tab:public_dataset_size} illustrates the degree of label distribution information and membership information leakage from clients when the quantity of the public data samples is set to 5000, 7500, and 10000, respectively.
The results indicate that larger public datasets contribute to increased privacy leakage risks for clients. 
We attribute this trend to two factors. For distillation-based LiRA, a larger public dataset provides a more extensive distillation dataset, enabling the attacker to obtain more robust reference models.
In the case of LDIA, a larger public dataset serving as the inference dataset allows the attacker to mitigate the impact of outliers or atypical data, thereby improving attack accuracy.


\subsection{Number of Epochs in Local Updates Phase}
Prior to the communication phase, clients train their local models on their private datasets during the local updates phase. 
This process enhances the local model's memorization of private data, facilitating knowledge transfer between clients but also potentially increasing privacy leakage. 
We measure the impact of the number of training epochs in the local updates phase on the leakage of label information and membership information from clients.
\begin{table}[h]
    \caption{Impact of Number of Training Epochs on Label Distribution and Membership Information Leakage in PDA-FD.}
    \centering
    \scriptsize
    \resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{c|c|c}
        \toprule
        Number of Epochs&  MIA (TPR at 1\%FPR) & LDIA (KL divergence)\\
        \midrule
        2 & 8.10\% &  0.15  \\
        4 & 14.64\% &  0.10 \\
        6 & 15.43\% &  0.09 \\
        \bottomrule
    \end{tabular}% 
    }
    \label{tab:epochs_local_updates}
\end{table}
As shown in Table \ref{tab:epochs_local_updates}, there is an increase in label distribution and membership information leakage from clients in DS-FL as the number of the local update training rounds increases from 2 to 6 on the CIFAR-10 dataset ($\alpha$=1).


\subsection{Number of Reference Models}
In LiRA, the attacker can form a more accurate Gaussian distribution by utilizing a larger number of reference models, thereby enhancing the precision of determining whether a target sample belongs to the target model's training data. 
We evaluate the performance of distillation-based LiRA with varying numbers of reference models. 
Figure \ref{fig:distillation_lira_num_models} shows results from experiments using the Cronus framework on CIFAR-10 with $\alpha=0.1$. 
The data reveals that the performance of the distillation-based LiRA's improves as the number of distilled reference models increases.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/mia_diff_models.png}
    \caption{The performance of distillation-based LiRA vs. number of the distilled reference model.}
    \label{fig:distillation_lira_num_models}
\end{figure}

\subsection{Resilience Against DP-SGD}
To evaluate the robustness of our proposed LDIA and MIA methods, we assess their effectiveness when the target client employs DP-SGD\cite{abadi2016deep} during the local updates phase. 
DP-SGD is a state-of-the-art privacy-preserving model training technique.
Our experimental setup includes 10 clients participating in DS-FL training on the CIFAR-10 dataset ($\alpha=10$). We conduct LDIA and Co-op LiRA attacks against the clients during the second round of training.
In DP-SGD, it introduces noise to gradients during training, governed by three key parameters. 
The clipping bound ($C$) limits the influence of individual data points on model parameters. 
The noise multiplier ($\sigma$) determines the amount of noise added to gradients. 
The privacy budget ($\varepsilon$) balances privacy guarantees and model utility, with smaller values providing stronger privacy at the cost of potentially noisier updates.
In our experiments, we set $C$ to be 10 and vary $\sigma$ to adjust $\varepsilon$. 
This setup allows us to evaluate our proposed attack under different privacy protection levels.
\begin{table}[h]
    \caption{Performance of MIA and LDIA against DP-SGD for DS-FL trained on CIFAR-10.}
    \scriptsize
    \resizebox{0.9\linewidth}{!}{%
    \centering
    \begin{tabular}{c|c|c|c|c}
    \toprule
         $\sigma$&$\varepsilon$&Average acc&LDIA(KL divergence)&MIA(TPR at 1\%FPR)\\
    \midrule
         0  &$\infty$ & 59.09\% & 0.03 & 15.76\% \\
         0.1& >10000  & 48.68\% & 0.07 & 2.86\% \\
         0.3& >5000   & 41.29\% & 0.08 & 2.11\% \\
         0.5& >2000   & 28.53\% & 0.09 & 1.54\% \\
         1.0& 231     & 21.34\% & 0.10 & 1.29\% \\
    \bottomrule
    \end{tabular}%
    }
    \label{tab:DP}
\end{table}



\subsection{Resilience Against Evasive Clients}
To proactively protect their privacy,  cautious clients may choose to, in each communication round, avoid sending to the server the logits of some samples in the public dataset, particularly the ones that are also in its training dataset. 
% , even though the FD protocol requests them. 
% Particularly, avoid the logits of the ones that 
To counter such defense, we propose two countermeasures as follows:
% (1) We can select the target sample as public data for the communication phase across multiple rounds.
(1) In co-op LiRA, a shadow target model can be distilled using the logits of the samples in the public dataset provided by the target model, and then obtain the logits of the target sample from this shadow target model as an approximation to the one from the target clients' model. 
The intuition is that although knowledge distillation reduces the distilled student model's membership information of the teacher model~\cite{jagielski2024students}, it still preserves statistically significant enough membership information for a percentage of the members in teacher's training data, thus allowing some success in the MIA attack to the teacher model.
(2) We can also leverage a technique called indirect queries~\cite{wen2022canary, long2020pragmatic}, which is to first obtain logits of samples in the target sample's neighborhood from the target model and subsequently perform MIA using information encoded in these neighborhood logits. Neighbor samples are generated by adding noises to the target sample.

We conduct experiments to evaluate the effectiveness, and the experiments are on the CIFAR-10 dataset with a Dirichlet distribution parameter $\alpha$=10, with co-op LiRA as the MIA method.
Equipped with the first countermeasure, the attack achieves a TPR of 4.53\% at 1\% FPR.
Implementing a simplified version of the second countermeasure gives the attack a TPR of 4.23\% at 1\% FPR.
Note that in implementing countermeasure two, we add random Gaussian noise to the target samples to generate neighbor samples, with the noise clipped to the [-0.7, 0.7] range.
Studies in~\cite{wen2022canary, long2020pragmatic} implement more advanced schemes to learn from the neighbor logits, leading to better attacks. We leave studying such schemes as future work.
