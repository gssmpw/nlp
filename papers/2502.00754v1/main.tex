
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsthm}

\usepackage{wrapfig}
\usepackage{sidecap}

\usepackage{multirow}
\usepackage{booktabs}

\usepackage{threeparttable}
\usepackage{diagbox}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{property}{Property}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}[section]
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% \newcommand{\R}{\mathbb{R}}
\renewcommand*{\d}{\mathop{}\!\mathrm{d}}


\title{Continuity-Preserving Convolutional \\ Autoencoders for  Learning Continuous \\ Latent Dynamical Models from Images}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Aiqing Zhu, Yuting Pan, Qianxiao Li\thanks{Corresponding author: \texttt{qianxiao@nus.edu.sg}}\\
% Department of Mathematics\\
% National University of Singapore\\
% \texttt{zaq@nus.edu.sg}
% \And
% Yuting Pan\\
% School of Computing\\
% National University of Singapore\\
% \texttt{pan.yuting@u.nus.edu}
% \And
% Qianxiao Li\thanks{Corresponding author}\\
% Department of Mathematics\\
% Institute for Functional Intelligent Materials\\
National University of Singapore\\
}

% \author{Aiqing Zhu\\
% Department of Mathematics\\
% National University of Singapore\\
% \texttt{zaq@nus.edu.sg}
% \And
% Yuting Pan\\
% School of Computing\\
% National University of Singapore\\
% \texttt{pan.yuting@u.nus.edu}
% \And
% Qianxiao Li\thanks{Corresponding author}\\
% Department of Mathematics\\
% Institute for Functional Intelligent Materials\\
% National University of Singapore\\
% \texttt{qianxiao@nus.edu.sg}
% }
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle
% We propose CpAE, a novel autoencoder designed to learn latent states that evolve continuously over time for learnting continuous latent dynamical models from discrete image frames.
\begin{abstract}
Continuous dynamical systems are cornerstones of many scientific and engineering disciplines.
While machine learning offers powerful tools to model these systems
from trajectory data, challenges arise when these trajectories are captured
as images, resulting in pixel-level observations that are discrete in nature.
Consequently, a naive application of a convolutional autoencoder can result in
latent coordinates that are discontinuous in time.
To resolve this, we propose continuity-preserving convolutional autoencoders (CpAEs) to learn continuous latent states
and their corresponding continuous latent dynamical models from discrete image frames.
We present a mathematical formulation for learning dynamics from image frames,
which illustrates issues with previous approaches and motivates our methodology
based on promoting the continuity of convolution filters, thereby preserving the
continuity of the latent states.
This approach enables CpAEs to produce latent states that evolve continuously with the underlying dynamics,
leading to more accurate latent dynamical models.
Extensive experiments across various scenarios demonstrate the effectiveness of CpAEs.




\end{abstract}
\section{Introduction}
Continuous dynamical systems, described by differential equations, are widely used as scientific modeling tools across various biological, physical, and chemical processes.
While traditionally described by mathematical models, the increasing availability of data has spurred the development of data-driven approaches \citep{brunton2016discovering, brunton2022data, schmidt2009distilling}.
In particular, machine learning and neural networks have recently emerged as powerful tools, achieving remarkable success in tasks such as discovering \citep{chen2018neural, gonzalez1998identification, raissi2018multistep}, predicting \citep{wang2021bridging,wu2020data, xie2024ab}, and controlling \citep{brunton2022data, chen2023constructing, zhong2020symplectic} continuous dynamical systems based on observed data.



Most methods for modeling dynamical systems are designed for observed data that already correspond to relevant state variables. However, in many scientific and engineering applications, we only have access to measurements that yield a series of discrete image data \citep{botev2021priors,chen2022automated}.
When applied to image data, a common approach involves using autoencoders to encode natural images located in high-dimensional pixel space onto a low-dimensional manifold \citep{greydanus2019hamiltonian,jin2023learning, toth2020hamintonian}. It is then assumed that the encoded sequences follow continuous paths governed by a differential equation on this manifold, and machine learning methods automatically capture the
continuous dynamics \citep{botev2021priors, toth2020hamintonian}. While this approach and assumption have been substantiated on relatively simple tasks, complex visual patterns and dynamical behaviors remain challenging \citep{botev2021priors}.


The continuous evolution of dynamical systems over time is a fundamental characteristic of many fields. Machine learning methods for capturing continuous dynamics from discrete data along continuous trajectories have advanced significantly, with robust, general-purpose algorithms now readily available \citep{chen2018neural,krishnapriyan2023learning,ott2021resnet}.
However, image data pose a key challenge since pixel coordinates often do not align with the continuous evolution of underlying dynamics, as illustrated in \cref{fig:discrete_env}.
Whereas latent states that evolve continuously over time are essential for discovering continuous latent dynamical systems, standard autoencoders often struggle to learn such valuable latent representations, as will be discussed in detail later.


\begin{figure}[htbp]
\vskip -0.1cm
\centerline{\includegraphics[width=0.9\linewidth]{discrete_env.pdf}}
\vskip -0.3cm
\caption{Illustration of pixel observations of continuous motion.
A single pixel white square, initially located at the leftmost position, moves uniformly to the right against a black background (plotted in gray for clarity).
Its position is recorded at each pixel step. This translational motion results in pixel coordinates that first increase by one, then decrease by one, and finally remain constant. As an illustrative example, we show the evolution of the value at the third pixel position.
}
\label{fig:discrete_env}
\end{figure}

We introduce continuity-preserving convolutional autoencoders (CpAEs) to learn continuously evolving latent states from discrete image frames. Our contributions are summarized as follows:
\begin{itemize}
    \item We propose a mathematical formulation for learning continuous dynamics from image data to describe the continuity of latent states.
    % \item We delve into the reasons behind the limitations of standard CNN autoencoders.
    \item We establish a sufficient condition (\cref{theorem}), demonstrating that the latent states will evolve continuously with the underlying dynamics if the filters are Lipschitz continuous.
    \item We introduce a regularizer (\cref{eq:regu}) to promote the continuity of filters and, consequently, preserve the continuity of the latent states.
    \item We perform several experiments across various scenarios to verify the effectiveness of the proposed method.
\end{itemize}



\subsection{Related works}
\textbf{Deep Autoencoder}.
An autoencoder \citep{baldi2012autoencoders, ranzato2007unsupervised, rumelhart1986learning} is a type of neural network that encodes input data into a compressed and meaningful representation, then decodes it to reconstruct the original input. A significant advancement in this area has been the development of Variational Autoencoders (VAEs) \citep{kingma2014auto}, which extend traditional autoencoders by incorporating probabilistic modeling of the latent space. Autoencoders can be adapted and extended to various models, finding broad applications, including image generation and classification \citep{kingma2014auto, yunchen2016variational}, data clustering \citep{guo2017deep, song2013auto}, and anomaly detection \citep{gong2019memorizing, zong2018deep}. In this paper, we focus on using autoencoders to learn latent dynamical models from images, and we propose novel continuity-preserving autoencoders that incorporate continuity prior for this task.

\textbf{Discovering Dynamical model}.
Discovering dynamical models from observed time-series data ${x_0, x_1,\cdots, x_N}$ is a fundamental challenge in science. Numerous approaches have been proposed to infer underlying dynamical systems from such data. Machine learning has emerged as a powerful tool for this task. One effective strategy involves constructing a neural network model, denoted as $\mathcal{N}$, to learn a data-driven flow map that predicts the system's future states \citep{chen2022automated, chen2024learning, wang2021bridging,wu2020data}. This model predicts the subsequent state $x_{n+1}$ based on the current state $x_n$. Alternatively, some researchers focus on modeling the governing function of the unknown differential equation \citep{chen2018neural, gonzalez1998identification, raissi2018multistep}. Given an input $x_n$, $x_{n+1}$ is obtained by solving the NN-parameterized ODE at time $\Delta t$, starting from the initial condition $x_n$.  This approach can offer valuable insights into the system's dynamics. For instance, it enables the characterization of invariant distributions \citep{gu2023stationary, lin2023computing}, energy landscapes \citep{chen2023constructing}, and other essential properties \citep{qiu2022mapping}, thereby expanding the scope of scientific investigation.
% requires an ODE solver and typically introduces potential numerical errors \citep{du2022discovery, keller2021discovery, zhu2022on}, it

\textbf{Learning dynamics from image observations}.
Numerous studies have explored the incorporation of classical mechanics-inspired inductive biases, such as physical principles \citep{cranmer2020lagrangian, greydanus2019hamiltonian, michael2019deep, yu2021onsagernet, zhang2022gfinns}, geometry structures \citep{eldred2024lie, jin2020sympnets, zhu2022vpnets}, and symmetries \citep{huh2020time, yang2024symmetry}, into deep neural networks. While some of these models have been applied to learn dynamics from images using autoencoders, they have mostly been tested on relatively simple visual patterns and dynamical behaviors.
An enhanced approach \citep{botev2021priors, toth2020hamintonian} involves employing VAEs to embed images, often resulting in improved predictive and generative performance. Given our priority on ensuring the continuous evolution of the latent states, this work focuses on deterministic autoencoders.

% Determining the optimal number of state variables for high-dimensional image data poses a significant challenge. \cite{chen2022automated} comprehensively investigated this issue and demonstrated the effectiveness of their approach across various physical dynamical systems. They also highlight the potential of learned latent state variables, referred to as neural state variables, for robust long-term prediction. However, their work primarily focused on discrete models, and the learned neural state variables do not evolve continuously. In this paper, we build upon their findings by selecting a subset of their datasets and configuring the dimensions of our latent states accordingly.





\section{Learning latent dynamical models using autoencoders}\label{sec:2}


This paper focuses on learning continuous latent dynamical models from sequential image observations of an unknown system. Our dataset consists of discrete image frames sampled from multiple continuous trajectories, denoted as:
\begin{equation}\label{eq:data}
\{(X^1_0, X^1_1, \cdots, X^1_N ),\cdots, (X^M_0, X^M_1, \cdots, X^M_N)\},\ X_n^m \in \R^{(I+1)\times (I+1)},
\end{equation}
where the superscript indicates the $m$-th trajectory and the subscript indicates the $n$-th time step of the trajectory.
We assume that an underlying dynamical system governs the observed image time series, with its governing equations defined as follows:
\begin{equation}\label{eq:ode}
\dot{z} = f(z), \quad z\in \mathcal{Z} \subset \R^D,
\end{equation}
where $\mathcal{Z}$ denotes the set of states associated with the image observations, and assume that there is a corresponding mapping $\mI$ from the physical state space to the pixel space.
Mathematically, the pixel observation at time $t_n$ is assumed to satisfy $X^m_n = \mI(z^m_n)$, where $z^m_n$ is the state evolved from initial state $z^m_0$ according to \cref{eq:ode}. The true governing function $f$, the underlying physical states $z^m_n$, and the mapping $\mI$ are all unknown. We further assume that the unknown governing function $f$ is Lipschitz continuous and bounded by $M_f$.

\vskip -0.3cm
\begin{SCfigure}[][h]
    \centering
\includegraphics[width=0.65\textwidth]{e-d.pdf}
    \caption{Schematic of learning latent dynamics. The encoder takes images as inputs and infers the corresponding latent states; the decoder maps the latent states back to reconstruct the original images; the dynamical model outputs the subsequent latent state $Z_{n+1}$ based on the current state $Z_{n}$.}
    \label{fig:e_d}
\end{SCfigure}

A typical framework for this task comprises three components: an encoder, a dynamical model, and a decoder. An illustration of this framework is shown in \cref{fig:e_d}.
The goals are to: 1) learn an encoder that extract latent states consistent with the assumed latent dynamical system, 2) discover a dynamical model that accurately captures the underlying latent dynamics, and 3) identify a decoder capable of reconstructing the pixel observations.




The first objective is a prerequisite for the entire task; without it, no target dynamical system with assumed structures can be identified. Specifically, to learn a meaningful continuous latent dynamical model, it is essential that the extracted latent states are discrete samples of Lipschitz continuous trajectories. However, without constraints on the encoder, the extracted latent states may deviate from the assumed latent dynamical system. In this paper, we focus on preserving the fundamental continuity of the latent dynamical system.
The encoder is carefully designed to ensure the continuous evolution of latent variables over time in this paper.


% Existing works \citep{greydanus2019hamiltonian,jin2023learning} typically train latent dynamical models incorporating specific priors and standard autoencoders simultaneously. However, without a prior on the encoder, the learned latent states may not exhibit the properties of the underlying physical states, implying that there is no target dynamical system that can be discovered.
% However, without a prior on the encoder, the learned latent states may not exhibit the properties of the underlying physical states, implying that there is no target dynamical system that can be discovered.
% In particular, a necessary condition for learning a meaningful continuous latent dynamical model is that the extracted latent states are discrete samples of Lipschitz continuous trajectories.


% Given the inherent continuity of underlying dynamics and the growing demand for continuous models in scientific research \citep{chen2023constructing,krishnapriyan2023learning, qiu2022mapping}, we focus on preserving the fundamental continuity of the latent dynamical system.
% The encoder is carefully designed to ensure the continuous evolution of latent variables over time in this paper.



\section{Continuity-Preserving Autoencoder}

\subsection{Mathematical formulation}

Given the discrete nature of image data in both space (pixels) and time, traditional definitions of continuity  are not directly applicable. In this section, we propose a mathematical formulation to describe the continuity of latent states when learning continuous dynamics from images.

\begin{figure}[!tbp]
\vskip -0.3cm
\centerline{\includegraphics[width=\linewidth]{mathmatical.pdf}}
\vskip -0.35cm
\caption{Illustration of the mathematical formulation. The continuous dynamics of the system are captured in pixel form. We learn latent dynamical models by encoding this recorded pixel data.}
\label{fig:math}
\vskip -0.3cm
\end{figure}
The formal mathematical definition of projecting the states of a dynamical system into images can be expressed in two steps:
\begin{itemize}
    \item \textbf{Mapping a state to a set of positions of all the particles constituting the objects}: This mapping is represented as $\mathcal{S}(z): \R^D \rightarrow \mathbf{B}([0,1]^2)$, where $\mathbf{B}([0,1]^2)$ denotes the set of all Borel sets in $[0,1]^2$, $\mathcal{S}(z)=:\Omega$ is the set of positions of particles constituting the objects.
    \item \textbf{Discretizing the coordinate space into image signals, simplifying the images by setting background pixels to 0 and pixels containing objects to 1}: Denote $\delta$ as the pixel size and $(I+1)\delta = 1$. Then we define the functional $\mI_{\delta}:\mathbf{B}([0,1]^2) \rightarrow \R^{(I +1)\times (I+1)}$ as follows:
    \begin{equation*}
    [\mI_{\delta}(\Omega)]_{i_1, i_2} =
    \left\{\begin{aligned}
    &1, &&\text{ if } [i_1\delta,(i_1+1)\delta]\times [i_2\delta,(i_2+1)\delta] \cap \Omega \neq \emptyset,\\
    & 0 &&\text{ otherwise, }
    \end{aligned} \right.
    \end{equation*}
where $\Omega$ is a Borel set in $[0,1]^2$, $[\mI_{\delta}(\Omega)]_{i_1, i_2}$ denotes the element located at the $(I+1-i_2)$-th row and $(i_1 + 1)$-th column of the matrix $\mI_{\delta}(\Omega) \in \mathbb{R}^{(I+1)\times (I+1)}$.
\end{itemize}
Finally, we define the observed image data for any state $z$ as $\mathcal{I}_{\delta} \circ \mathcal{S}(z)$. See \cref{fig:math} for an illustration.

To reduce dimensionality, CNN typically employ a parameter called \textit{stride} to progressively downsample feature maps.
It is well-known that the convolution operation for two-dimensional input images can be expressed as follows
\citep{goodfellow2016deep}:
\begin{equation}\label{eq:main_scnn}
\begin{aligned}
&\text{Input:} &&\mI_0\in \R^{(I+1)\times(I+1)},\\
&\text{Hidden layers:} &&
[\mI_{l}]_{i_1, i_2} = \sum_{j_1=0}^{J_{l}} \sum_{j_2=0}^{J_{l}} [\mI_{l-1}]_{s_l\cdot i_1+j_1, s_l\cdot i_2+j_2} \cdot [\mW_{l}]_{j_1, j_2},\ l=1,\cdots, L,\\
&\text{Output:} &&\mathcal{E}(\mI_0) = \mI_L.
\end{aligned}
\end{equation}
Here $s_l$ is the stride of the  $l$-th layer; $\mI_l \in \R^{({\left\lfloor {I}/{\prod_{i=1}^l s_{i}}\right\rfloor}+1)\times ({\left\lfloor {I}/{\prod_{i=1}^l s_{i}}\right\rfloor}+1)}$ is the output feature map of the $l$-th layer, and we assume that $\norm{\mI_l}=\mathcal{O}(1)$;
$\mW_l \in \R^{(J_l+1)\times(J_l+1)}$ is the filter of $l$-th layer.
For simplicity, we have omitted operations that do not significantly affect continuity, such as activation layers. Additionally, to simplify index counting, we assume that zero padding is applied only on one side of the feature maps, meaning that $[\mI_{l}]_{i_1, i_2} = 0$ if $i_1 \lor i_2 \in \lfloor {I}/{\prod_{i=1}^l s_{i}}\rfloor+ \{1,2, \cdots, J_l\}$.



Different resolutions $\delta=1/(I+1)$ correspond to specific networks, where hyper-parameters such as $J_l$ should be adjusted accordingly. Therefore, we will also denote the feature map as $\mI_l^{\delta}$, the filter as $\mW^{\delta}_l$ and the CNN as $\mathcal{E}_{\delta}$ to emphasize their dependence on the pixel size.
For the convenience of analysis, we assume the weights $\mW^{\delta}_{l}$ of a standard CNN filter can be normalized to a bounded function $\mathcal{W}_l:\R^2 \rightarrow [-1,1]$ that is independent of $\delta$:
\begin{equation*}
[\mW^{\delta}_{l}]_{j_1, j_2} = \mathcal{W}_l(j_1\delta, j_2\delta) \varepsilon_l,\quad\text{where }
\mathcal{W}_l(x)=0  \text{ if } x \notin [0, J_l\delta]^2.
\end{equation*}
Here $\varepsilon_l$ is a normalization coefficient to ensure that $\norm{\mI_l}=\mathcal{O}(1)$ and the last equation ensures that the filter function outputs 0 when the index exceeds the defined size limits.



Now we are ready to provide the following definition of continuity, which serves as a relaxation of the traditional Lipschitz continuity.
\begin{definition}\label{def:con}
A sequence of functions $\{g_{\delta}(z): \mathcal{Z} \rightarrow \mathbb{R}^{d}| \delta\in \{1/(I+1)\}_{I=1}^{\infty}\}$ is called $\delta$-continuous if there exists a constant $c_g$ such that for all $z_1, z_2 \in \mathcal{Z}$, there exists a $\delta^*$ such that if $\delta \leq \delta^*$, then $\|g_{\delta}(z_1) - g_{\delta}(z_2)\| \leq c_{g} \|z_1-z_2\|$.
\end{definition}

% \begin{definition}\label{def:con1}
% A sequence of functions $\{g_{\delta}(z): \mathcal{Z} \rightarrow \mathbb{R}^{d}| \delta\in \R\}$ is called discontinuous if for all constant $c_g$, there exist $z_1, z_2 \in \mathcal{Z}$ such that for all $\delta>0$, there exists a $\delta^* \leq \delta$ such that
% $\|g_{\delta^*}(z_1) - g_{\delta^*}(z_2)\| > c_{g} \|z_1-z_2\|$.
% \end{definition}



It is worth mentioning that piece-wise constant approximation $g_{\delta}$ of Lipschitz function $g$ with partition size $\delta$ is $\delta$-continuous. Moreover, if $\lim_{\delta\rightarrow 0}g_{\delta}=g$, the $\delta$-continuity of $g_{\delta}$ is equivalent to the Lipschitz continuity of $g$.
We adopt this definition due to the discrete nature of pixel observations, where slight variations in $z$ might not be reflected in image, as will be illustrated in \cref{sec:Modeling motion}.


As discussed previously, to learn a continuous latent dynamical model, it is essential that the extracted latent states $Z = \mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z(t))$ evolve continuously over time. Therefore, our objectives for the autoencoder are as follows:
\begin{itemize}
    \item Find the encoders $\{\mathcal{E}_{\delta}\}$ such that $\mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z)$ is $\delta$-continuous.
    \item Find the decoders $\{\mathcal{D}_{\delta}\}$ such that $\mathcal{D}_{\delta}\circ \mathcal{E}_{\delta} $ is approximately the identity mapping.
\end{itemize}
The second objective guarantees that the learned latent states are non-trivial, which is a standard requirement for autoencoders.
The first objective ensures that the latent states evolve continuously with the underlying dynamics. If it can be achieved, with an additional assumption that the pixel size $\delta^*$ of the image is sufficiently small,
then there exists a constant $c_{\mathcal{E}}$ such that
\begin{equation*}
\norm{Z_n - Z_{n+1}} = \norm{\mathcal{E}_{\delta^*} \circ \mI_{\delta^*} \circ \mathcal{S}(z_n) - \mathcal{E}_{\delta^*} \circ \mI_{\delta^*} \circ \mathcal{S}(z_{n+1})}\leq c_{\mathcal{E}} \norm{z_n-z_{n+1}}\leq c_{\mathcal{E}} M_f \Delta t.
\end{equation*}
This inequality implies that the resulting latent variables $\{Z_n^m\}_{n=0,\cdots, N, \ m=1,\cdots, M}$, corresponding to the input images (\ref{eq:data}), are discrete samplings of Lipschitz continuous trajectories. This
property allows us to employ a continuous dynamical model for learning the latent dynamics and predicting future behavior by decoding the predicted latent states using the decoder.
In the following sections, we analyze why standard CNN encoders may fail to achieve the first objective and how our proposed method overcomes this limitation.




\subsection{Modeling of motion}\label{sec:Modeling motion}

To illustrate the rationale behind the modified definition of $\delta$-continuity introduced in the previous section, we consider the example of rigid body motion in a two-dimensional plane. This type of motion, involving only translation and rotation, allows for a clear representation of the mapping $\mathcal{S}$, which is essential for the subsequent analysis.

The equation of rigid body motion on a two-dimensional plane can be expressed as follows
\begin{equation}\label{eq:ode2}
\begin{aligned}
\dot{z} = f(z),\ z= (z^t, z^r),\  z^t= (\vr_1, \cdots, \vr_{K}),\ z^r=(\theta_1, \cdots, \theta_{K}),
\end{aligned}
\end{equation}
where $K$ is the number of rigid bodies. The image corresponding to the state $z$ is given by
\begin{equation}\label{eq:image_of_state}
\mathcal{S}(z)  =  \cup_{k=1}^{K} \Phi_{\theta_k}(\Omega_k)+\vr_k, \ \Omega_k\subset \R^2,
\end{equation}
where $\vr_k=(r_{k,1}, r_{k,2})$ and $\Phi_{\theta_k}$ represent the translation and rotation of the object, respectively. Details of this model can be found in Appendix \ref{app:rigid}. Here we assume that $\Phi_{\theta_k}(\Omega_k)+\vr_k\subset [0,1]^2$ for $k=1,\cdots, K$ and that these sets are pairwise disjoint for all $z\in \mathcal{Z}$.

\begin{figure}[!tbp]
\centerline{\includegraphics[width=\linewidth]{observed_dynamics.pdf}}
\vskip -0.3cm
\caption{Illustration of discrete nature of pixel observations for continuous motion.
Considering a motion similar to that depicted in Figure \ref{fig:discrete_env}, we assume that the object occupies a very small volume and its motion is recorded in continuous time periods.
The left side illustrates the underlying motion of the object, while the middle side shows the evolution of pixel values during the recording process. The right side depicts the observed motion derived from these pixel values, reflecting the discrete nature of pixel observations.
}
\label{fig:discrete_env2}
\vskip -0.3cm
\end{figure}



Suppose $K=1$ and the motion only involves translation, we take $\Omega_1= (0, w_{1}]\times (0, w_{2}]$ and further assume that $w_{i}/ \delta$ is integer. In this case, we have:
\begin{equation*}
 \mI_{\delta}
(\mathcal{S}(z) )=\mI_{\delta}
(\mathcal{S}(\hat{z})),\ \text {where } \hat{z} =
(\hat{r}_{1,1}, \hat{r}_{1,2}) \text{ and }
\hat{r}_{1,i} = \max_{j\in \sZ, j\delta \leq r_{1,i}} j\delta.
\end{equation*}
This implies that variations smaller than $\delta$ may not be captured in the image, and thus, we can only track the dynamics of $\hat{z}$, the piece-wise constant approximation of $z$, as illustrated in \cref{fig:discrete_env2}. Consequently, the function $\mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z)$ cannot be continuous under standard definitions of continuity.
Using our definition, we can readily verify that (1) $g_{\delta}(z) = \hat{z}$ is $\delta$-continuous; (2) $g_{\delta}(z)=\mI_{\delta} \circ \mathcal{S}(z)$ is not $\delta$-continuous; and (3) $\hat{g}\circ g_{\delta}:\mathcal{Z} \rightarrow \R^{d}$ is $\delta$-continuous if $g_{\delta}:\mathcal{Z} \rightarrow \R^{\hat{d}}$ is $\delta$-continuous and $\hat{g}:\R^{\hat{d}} \rightarrow \R^{d}$ is Lipschitz continuous.













\subsection{Why standard CNN autoencoders fail}\label{sec:Why CNN fiails}
Since $\mI_{\delta} \circ \mathcal{S}(z)$ is not $\delta$-continuous, the composition $\mathcal{E} \circ \mI_{\delta} \circ \mathcal{S}(z)$ is generally not $\delta$-continuous if no further restrictions is imposed on $\mathcal{E}$. Next we illustrate this issue by an example of uniform motion.



Consider a vertical bar with a width of $\Delta$ undergoing uniform horizontal motion within the image, which is a two-dimensional extension of the motion depicted in \cref{fig:discrete_env}. The equation governing this motion is $\dot{z}=1,\ z(0)=0$. We suppose $t\in [0,1/2]$ to ensure that the bar remains within the image. Let $\mathcal{S}(z) = (z, z+\Delta]\times (0,1]$ and assume $\Delta /\delta$ is an integer. The image of $z=n\delta$ is of the form
\begin{equation*}
\mI_{\delta} \circ \mathcal{S}(n\delta)=(\bm{0}_{(I+1) \times n},\ \bm{1}_{(I+1) \times (\Delta/\delta+1)}, \bm{0}_{(I+1) \times (I-n-\Delta/\delta)}),
% ,\quad \mI_{\delta} \circ \mathcal{S}(z) =  \mI_{\delta} \circ \mathcal{S}(\hat{z}),
\end{equation*}
where $\bm{x}_{i_1 \times i_2}$ denotes the $i_1 \times i_2$ matrix whose elements are all $x$.
We can verify that if $\mI_0 = \mI_{\delta} \circ \mathcal{S}(n\delta)$, then $[\mI_1]_{0, 0} = \sum_{j_1=n}^{\Delta/\delta}\sum_{j_2=0}^{J_1}[\mW^{\delta}_1]_{j_1,j_2}:= g_{\delta}(n\delta)$.



Suppose the step size for the motion is set to $2\Delta$.
We consider the two cases:

1) the CNN filter size is of constant size i.e. $J_1 = \mathcal{O}(1)$, then we have $g_{\delta}(2\Delta) = 0$ and $g(0) = B_0 := \sum_{j_1=0}^{J_l}\sum_{j_2=0}^{J_1}\mathcal{W}_l(j_1\delta, j_2\delta)$ as $\delta \rightarrow 0$, where $B_0\neq 0$ otherwise $\mI_1$ is all zero. Then for any constant $c_{\mI_1}>0$, if $2\Delta \leq |B_0|/c_{\mI_1}$, $|g_{\delta}(2\Delta)-g(0)| > c_{\mI_1}|2\Delta|$. This indicates that $[\mI_{0,0}]$ is not $\delta$-continuous, implying that the extracted latent states cannot be learned as a continuous dynamics.

2) the CNN filter size increases as image resolution increases, i.e. $J_1 = \mathcal{O}(1/\delta)$, then we have
\begin{equation*}
g_{\delta}(2\Delta)-g_{\delta}(0) = \sum_{j_1=2\Delta/\delta}^{3\Delta/\delta}\sum_{j_2=0}^{J_1}\mathcal{W}_1(j_1\delta, j_2\delta) \varepsilon_1 - \sum_{j_1=0}^{\Delta/\delta}\sum_{j_2=0}^{J_1}\mathcal{W}_1(j_1\delta, j_2\delta) \varepsilon_1
, \ \varepsilon_1=\frac{\delta}{J_1\Delta}.
\end{equation*}
We assume $\mathcal{W}_1(j_1\delta, j_2\delta)$ are i.i.d. samples from a uniform distribution on $[-1,1]$. Then both $g_{\delta}(2\Delta)$ and $g(0)$ are also independent samples from a uniform distribution on $[-1,1]$. It follows that $|g_{\delta}(2\Delta)-g_{\delta}(0)|\leq c_g \norm{\Delta}$ for a given $c_g$ with probability zero as $\Delta\rightarrow 0$.
This implies that if  no further restrictions is imposed on standard CNN filters, $[\mI_{0,0}]$ is not $\delta$-continuous and the extracted latent states do not exhibit continuous dynamics with probability one.

\begin{figure}[!tbp]
\vskip -0.3cm
\centerline{\includegraphics[width=\linewidth]{fail.pdf}}
\vskip -0.3cm
\caption{\textbf{(A)} Illustration of convolution operation. The red boxes represent the filter of size $\mathcal{O}(1)$ or $\mathcal{O}(1/\delta)$. The blue box represents the object. The solid line indicates its initial position, while the dashed line represents its position after motion. \textbf{(B)} The variation of latent states divided by $\Delta t$ for the two-body system, where the encoder is a one-layer CNN with parameters uniformly sampled from $[-1,1]$. The shaded region represents one standard deviation. \textbf{(C)} Examples of motion where the positions of the objects after variation only partially overlap with their positions before variation.
}
\vskip -0.3cm
\label{fig:example}
\end{figure}

This calculation is schematically illustrated in the left panel of \cref{fig:example}.
It is noted that in scenarios involving small-volume objects and limited overlap in their positions between steps, a standard CNN without additional constraints performs poorly in outputting continuous latent states.
A numerical demonstration of this limitation for two-body systems is shown in the middle panel of \cref{fig:example}. Several examples of motions where this failure may occur are shown on the right side of \cref{fig:example}.



\subsection{Quantifying continuity of CNN autoencoders}

In the aforementioned counterexample, it is evident that a sufficient condition for ensuring the continuous evolution of latent states is that $\mathcal{W}_1$ is Lipschitz continuous. In this section, we will quantify the continuity of CNN autoencoders rigorously and extend this analysis to more general scenarios.



% We begin by making the following assumption:
\begin{assumption}\label{ass:eva1}
There exists a positive constant $M_{\Delta}$ and an integer $L^*< L$ such that, if $(i_1, i_2)/{\lfloor {I}/{\prod_{i=1}^l s_{i}}\rfloor} \notin \left[M_{\Delta}, 1- M_{\Delta}\right]^2$, then $[\mI_l]_{i_1, i_2} = 0$ for $l=1,\cdots, L^*-1$.
\end{assumption}
% This assumption is equivalent to stating that the outputs of the first few layers vanishes near the image boundary, and can be achieved by padding the input image with a sufficient number of zeros.

This assumption holds in scenarios where the objects of interest are well-captured and located within the central region of the image. Alternatively, it can be satisfied by padding the input image with a sufficient number of zeros.

With these preliminaries, we next present the main theorem and provide its proof in \cref{sec:proofs}.
\begin{theorem}\label{theorem}
Assume that the underlying dynamical system is a rigid body motion (\ref{eq:ode2}) on a two-dimensional plane. If Assumption \ref{ass:eva1} hold, let $c_{\mathcal{W}}$ be constants satisfying
\begin{equation*}
\max_{l=1,\cdots, L^*} |\mathcal{W}_{l}(y_1) - \mathcal{W}_{l}(y_2)| \leq c_{\mathcal{W}}\norm{y_1-y_2},
\end{equation*}
and if $s_l=2$ for $l=1, \cdots,L^*-1$, then for any $z_1=(z_1^t, z_1^r), z_2=(z_2^t, z_2^r) \in \mathcal{Z}$, we have
\begin{equation*}
\norm{\mathcal{E}_{\delta}\circ \mI_{\delta} \circ \mathcal{S}( z_1) - \mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z_2)}\leq Cc_{\mathcal{W}} \norm{z_1^r-z_2^r} + \frac{Cc_{\mathcal{W}}}{{2^{L^*-1}}}\norm{z_1^t-z_2^t},\ \text{ as }\ \delta\rightarrow 0.
\end{equation*}
Here $C$ is a constant independent of $\delta$ and $z$.
\end{theorem}

This theorem establishes a connection between the $\delta$-continuity of a CNN encoder and the continuity of its filters.
To ensure that the latent states evolve continuously with the underlying dynamics, it is sufficient for the functions $\mathcal{W}_l$ representing the filters in the first few layers to be Lipschitz.



\subsection{Method to preserve continuity of CNN autoencoders}
In this section, we discuss strategies to promote continuity of $\mathcal{W}_l$ , thereby ensuring that functions $\mathcal{W}_l,\ l=1,\cdots, L^*$ have a small constant $c_{\mathcal{W}}$ and $\mathcal{E}_{\delta}\circ \mI_{\delta} \circ \mathcal{S}$ is $\delta$-continuous.


Note the fact that $\max_l\max_{ j_1, j_2} |\mathcal{W}_l(j_1\delta, j_2\delta)|/ (\lceil J_l/2 \rceil \delta) \leq c_{\mathcal{W}}$, larger filters are necessary to ensure continuity.
It is worth mentioning that using a filter with the same size as the input image in CNN is essentially equivalent to using a fully connected neural network (FNN). While FNN encoders are commonly employed in baseline methods, they are often insufficient for second objective of reconstructing images with complex visual patterns, as demonstrated in \cref{app:fnn}.


Focusing on images of size $3 \times 128 \times 256$, the parameters of the downsampling layers within the encoder are detailed in \cref{table:parameter}.
The first three layers in this table use large filters (i.e., $L^*=4$). The remaining five layers are standard convolution layers used for extracting latent features.
\begin{table}[htbp]
\vskip -0.5cm
\caption{Architecture of the encoder in CpAE}
\begin{center}
\begin{tabular}{ l| c c c c c c c c}
\hline
{Layer} &  1& 2&  3&  4& 5&6 &7 &8
\\ \hline
Filter size     & 12  &12   &12   &4   &4   &4   &4   &(3,4)\\
Stride          & 2   &2    &2    &2   &2   &2   &2   &(1,2)\\
\hline
\end{tabular}
\end{center}
\label{table:parameter}
\vskip -0.5cm
\end{table}


As we only need the values of $\mathcal{W}_l$ on grids for computation, we recommend using the nonlocal operators method \citep{gilboa2007nonlocal, gilboa2009nonlocal},
an image processing technique that promotes image continuity. This approach requires only the following regularizer for the filters:
\begin{equation}\label{eq:regu}
\begin{aligned}
\mathcal{J}
=&\lambda_J\sum_{l=1}^{L} \
\sum_{i_1, i_2, j_1, j_2 =-\hat{J}}^{J^l+\hat{J}}  (W^{l}_{i_1, i_2} - W^{l}_{j_1, j_2})^2k\left((i_1\delta, i_2\delta),  (j_1\delta, j_2\delta)\right),
\end{aligned}
\end{equation}
where $\lambda_J$ is a weight hyperparameter, which is set to $1$ by default, $k$ is a positive and symmetric  function and the parameter $\hat{J}\geq 1$. Herein, we recommend employing the Gaussian kernel function $k(x) = e^{-\|x\|_2^2/\sigma^2}$ and setting $\hat{J} = 1$. And we apply the regularizer \cref{eq:regu} to the filters of the first three layers to penalize large $c_{\mathcal{W}}$ and ensure the filters have appropriate continuity.





\section{Experiments}
The benchmark methods used for comparison in this section include existing dynamical models such as Neural ODEs \citep{chen2018neural}, Hamiltonian Neural Networks (HNNs) \citep{greydanus2019hamiltonian}, and Symplectic Networks (SympNets) \citep{jin2020sympnets, jin2023learning}, coupled with standard autoencoders. Neural ODEs incorporate only the continuity prior. In contrast, HNNs and SympNets are structured dynamical models that leverage prior knowledge of Hamiltonian systems. With the goal of obtaining latent states that closely aligns with the assumed dynamics, these methods \citep{greydanus2019hamiltonian, jin2023learning} typically train the latent dynamical models and the autoencoders simultaneously by minimizing the following loss function:
\begin{equation*}
\begin{aligned}
\mathcal{L}
=& \lambda \sum\nolimits_{(x, y)\in \mathcal{T}} \left(\norm{\mathcal{D}\circ \mathcal{E}(x) - x}_2^2 + \norm{\mathcal{D}\circ \mathcal{E}(y) - y}_2^2\right)
+ \sum\nolimits_{(x, y)\in \mathcal{T}} \norm{\Phi \circ \mathcal{E}(x) - \mathcal{E}(y)}_2^2,
\end{aligned}
\end{equation*}
where $\mathcal{T} = \{(X^m_n, X^m_{n+1})\}_{n=0, 1, \cdots, N-1,\ m=1,\cdots, M}$ is the training dataset, $\Phi$ is the latent dynamical model detailed in \cref{app:model details}.


CpAEs are able to learn latent states that evolve continuously with time. Thus, we propose to learn the latent states and their corresponding latent dynamical models separately.
ODEs are not only continuous over time but also preserve orientation, as characterized by the positive determinant of the Jacobian for the phase flow. Therefore, we employ VPNets \citep{zhu2022vpnets}, which have the unit Jacobian determinant, for regularization. This regularization also helps penalize the large constant $C$ in \cref{theorem}.
The loss function for CpAEs is defined as follows:
\begin{equation*}
\begin{aligned}
\mathcal{L}_{CpAE} = &\sum\nolimits_{(x, y)\in \mathcal{T}} \norm{\mathcal{D}\circ \mathcal{E}(x) - x}_2^2 + \mathcal{J}_{R} + \mathcal{J},
\end{aligned}
\end{equation*}
where $\mathcal{J}_{R}$ is given by $\mathcal{J}_{R} = \lambda_R\sum_{(x, y)\in \mathcal{T}} \norm{\Phi_{vp} \circ \mathcal{E}(x) - \mathcal{E}(y)}_2^2 + \norm{\mathcal{D}\circ \Phi_{vp} \circ \mathcal{E}(x) - y}_2^2$ and $\Phi_{vp}$ is a small VpNet, $\lambda_R$ is set to $1$ by default. After training the CpAEs, we separately learn a continuous dynamical model for the latent dynamics. In this section, the latent model for CpAEs is chosen to be a Neural ODE.


We also compare our proposed method with the hybrid scheme of neural state variable \citep{chen2022automated}, a discrete model that has demonstrated impressive predictive accuracy.
Note that the datasets used in our experiments, except for the first one, are obtained from \cite{chen2022automated}. Following their preprocessing steps, we concatenate two consecutive frames to form the data points. Parameters of all methods can be found in \cref{app:model details}.


Assessing the fidelity of learned latent dynamical models remains an open challenge. The quality of background reconstruction can significantly impact the Pixel Mean Squared Error (PMSE). In contrast, if there is already no overlap, large deviations in object position may not significantly increase PMSE. In this paper, we adopt a similar definition as in \cite{jin2023learning, botev2021priors} to compute the Valid Prediction Time (VPT) for evaluating a model's predictive ability:
\begin{equation*}
\text{VPT}=\arg\max\nolimits_{t}\ \{t\leq T\ |\ \text{PMSE}(X_{\tau}, \bar{X}_{\tau})\leq \varepsilon,\, \forall {\tau} \leq t\},
\end{equation*}
where $\varepsilon$ is a threshold parameter, $X_{\tau}$ is the ground truth and $\bar{X}_{\tau}$ is the prediction at time ${\tau}$. Here we set $T=1$ and $\varepsilon=0.007$ for the first three datasets, and $\varepsilon=0.0015$ for the last dataset. We found that once these thresholds are exceeded, there is a significant deviation in the predicted images.
The VPT scores are averaged across all test trajectories. We also compute the Valid Prediction Frequency (VPF), which represents the frequency of  test trajectories for which VPT $= 1$.



\subsection{Continuity of latent states}
We demonstrate the continuity using a simple circular motion dataset comprising 220 images ($48 \times 48$) captured every 0.1 seconds along a single trajectory, with 70 images for training and 150 for testing. A single hidden layer $48 \times 48$ CNN autoencoder with various regularizers is used to learn the latent states, followed by a Neural ODE to model their dynamics. After training, we show the latent states of the test images and the predicted dynamics. More detail are given in \cref{app:continuity details}.

\begin{figure}[htbp]
\vskip -0.2cm
\centerline{\includegraphics[width=1\textwidth]{Abl.pdf}}
\vskip -0.5cm
\caption{The latent states and the corresponding learned dynamics derived from them
}
\vskip -0.2cm
\label{fig:continuity}
\end{figure}
As shown in \cref{fig:continuity}, neither the standard autoencoder nor the addition of a conventional \(L_2\) regularizer can extract continuously evolving latent states, leading to the failure of subsequent Neural ODE training. In contrast, the proposed continuity regularizer (\ref{eq:regu}) ensures continuous latent state evolution, enabling the Neural ODE to effectively capture their dynamics.








\begin{table}[!tbp]
\centering
\begin{threeparttable}

\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule%[1pt]
\multirow{2}{*}{\diagbox[]{Dataset}{Method}} &\multicolumn{2}{c}{CpAE} & \multicolumn{2}{c}{Hybrid scheme} & \multicolumn{2}{c}{AE +Neural ODE} &   \multicolumn{2}{c}{AE+HNN}&\multicolumn{2}{c}{AE+SympNet}\cr
\cmidrule(lr){2-3}
    \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
&VPT & VPF&VPT & VPF&VPT & VPF&VPT & VPF&VPT & VPF\\
    \midrule
Damped pendulum
& 99.2$\pm$8.5&99.2 & 95.4$\pm$15.0&88.3 & 50.7$\pm$31.2&23.3& ---& --- & ---& ---
\\
Elastic pendulum
& 72.1$\pm$27.2&36.7 & 49.5$\pm$24.2&10.0 & 30.6$\pm$18.5&1.7 & ---& ---& ---& ---
\\
Double pendulum
& 69.1$\pm$31.5&40.0 & 46.8$\pm$21.4&4.6 & 24.3$\pm$13.8&0.0 & 11.0$\pm$4.2&0.0 & 15.1$\pm$12.8&0.0
\\
Swing stick
& 57.4$\pm$20.4&11.1 & 13.7$\pm$5.1&0.0 & 14.4$\pm$7.5&0.0 & 24.1$\pm$14.5&0.0 & 14.8$\pm$12.2&0.0
\\
\bottomrule%[1pt]
\end{tabular}
}
\vskip -0.2cm
\caption{The performance of four physical systems evaluated using the VPT and VPF metrics. All values are scaled by a factor of $100$, with higher scores indicating better performance. VPT scores are reported as mean $\pm$ standard deviation. We do not report the performance of HNN and SympNet on the first two datasets, as their underlying systems are not Hamiltonian.}
\vskip -0.1cm
\label{tab: results}
\end{threeparttable}
\end{table}



\begin{figure}[htbp]
\vskip-0.2cm
\centerline{\includegraphics[width=0.9\textwidth]{simulation_rebuttal.png}}
\vskip -0.3cm
\caption{Predictions for simulation data.
}
\vskip -0.4cm
\label{fig:simulate}
\end{figure}


\subsection{Simulation data}
We then benchmark on simulation datasets to show the enhanced prediction performance of CpAE.
% generated by numerically solving the ODEs and then creating the corresponding images using computer software.


% \textbf{Two-body problem}.
% This dataset is generated in the same manner as in \cite{jin2023learning} and is used to illustrate the FNN autoencoder.
% It consists of $100$ observations-images of size $100 \times 50$-captured at a time interval of $0.6$ seconds along a single trajectory.

\textbf{Damped pendulum}.
This dataset consists of 1,200 trajectories, each containing 60 discrete data points-images of size $3\times 128\times 128$-sampled at a time interval of $\frac{1}{60}$ seconds. The details of this system are provided in \cref{app:Details of datasets}. The images of the physical state maintain the form of a single pendulum and are are generated using a similar procedure as in \cite{chen2022automated}.

\textbf{Elastic pendulum}.
To verify the effectiveness of CpAEs on non-rigid motion, we consider the elastic double pendulum, where each pendulum arm can stretch and contract. The dataset is generated following the procedure outlined in \cite{chen2022automated}. It consists of 1,200 trajectories, each containing 60 data points-images of size $3\times 128\times 128$-sampled at time intervals of $\frac{1}{60}$ seconds.

% This dataset is generated in the same manner as in \cite{jin2023learning} and is used to illustrate the FNN autoencoder.
% It consists of $100$ observations-images of size $100 \times 50$-captured at a time interval of $0.6$ seconds along a single trajectory.
% We begin by examining the performance of FNN autoencoders.
% As shown in the left panel of \cref{fig:simulate}, all FNN-based methods, except HNN, accurately predict the ground truth within the given time interval for the two-body system data. However, this success does not extend to datasets with more complex visual patterns. The bottom right panel of \cref{fig:simulate}  illustrates that FNN autoencoders fail to achieve complete reconstruction for the damped pendulum and elastic double pendulum datasets.


\cref{tab: results} and \cref{fig:simulate} demonstrate that both the hybrid scheme with neural state variables and the proposed CpAEs accurately approximate the actual dynamics as they evolve. In contrast, standard CNN autoencoders exhibit lower predictive accuracy due to their tendency to produce discontinuous latent states (as illustrated in Appendix \ref{app:exp}), while using a continuous model to learn the latent dynamics.
Since hybrid schemes do not require continuous latent variables, the advantage of CpAEs is less pronounced. Nevertheless, CpAEs provide a distinct benefit by yielding a continuous latent model, which is crucial for many scientific applications \citep{chen2023constructing, krishnapriyan2023learning, qiu2022mapping}. This continuous latent model also facilitates tasks such as time reversal and interpolation between observed states (as shown in \cref{app:exp}).


\subsection{Real-world data}
To evaluate the model's performance on real-world systems, we perform experiments using the double pendulum and swing stick datasets from \citep{chen2022automated}. Both datasets consist of images of size $3\times 128\times 128$ recorded at 60 fps. The double pendulum dataset contains $1,200$ trajectories, each consisting of $60$ discrete image frames, while the swing stick dataset includes $85$ trajectories, each with $1,212$ discrete image frames.



\begin{figure}[!htbp]
\centerline{\includegraphics[width=\linewidth]{realdata.png}}
\vskip -0.3cm
\caption{Predictions for real-world data.
}
\vskip -0.4cm
\label{fig:real}
\end{figure}
Building on our success with simulated data, we further show that CpAEs outperform baseline methods on real-world data, as illustrated in \cref{tab: results} and \cref{fig:real}. Although the double pendulum is a Hamiltonian system, and SympNets are well-suited for such systems \citep{jin2020sympnets}, standard autoencoders often struggle to learn latent variables that adhere strictly to Hamiltonian constraints, leading to predictions that deviate substantially from the true dynamics. This issue is particularly pronounced for the double pendulum system.

% Building upon our success with simulated data, we further demonstrate that CpAEs outperform baseline methods when applied to real-world data, as shown in \cref{tab: results} and \cref{fig:real}.
% HNN and SympNet intrinsically ensure symplectic structures and stability. Although the double pendulum is a Hamiltonian system and SympNets are well-suited for such systems \citep{jin2020sympnets}, standard autoencoders often struggle to learn latent variables that strictly adhere to the Hamiltonian constraints, leading to predictions that diverge significantly from the true dynamics. In contrast, methods that incorporate weaker priors, such as Neural ODEs with only a continuity prior and the discrete hybrid scheme without any specific priors, perform better.


\section{Summary}
Leveraging known properties of a data domain to design efficient deep learning architectures has significantly advanced the field.
When using a dynamic model with prior knowledge to learn hidden dynamics from images, it is essential that the extracted latent states follow a dynamic system that aligns with the specified prior.
In this paper, we introduce continuity-preserving autoencoders (CpAEs), a novel approach designed to impose the continuity restriction on latent state representations. Our main contribution is the mathematical formulation for learning dynamics from image frames, which illustrates the issue of discontinuity of the latent states extracted by standard CNN encoders. We then show that the output latent states of CNN encoders evolve continuously with the underlying dynamics, provided that the filters are Lipschitz continuous.
Moreover, numerical experiments show that CpAEs outperform standard autoencoders.


In this paper, our continuity quantification is limited to rigid body motion in a two-dimensional plane. Generalizing our analysis to include non-rigid motion and the projection of three-dimensional motion onto a two-dimensional plane presents significant challenges, which we leave for future research.
While CpAEs show superior performance compared to baseline approaches, it is important to note that they still fall short of effectively solving all tasks. As the visual complexity of the images increases, particularly in the challenging swing-stick tasks where the dynamics are not fully characterized, and the images are captured against complex backgrounds with significant noise, we observe a decline in performance, highlighting the need for more sophisticated approaches.
Herein, we adopted the simplest method of promoting filter continuity by adding regularization. Future research could explore more effective methods, such as hypernetworks \citep{chauhan2023brief}, to achieve this goal.
Furthermore, our current research is exclusively focused on the weak prior of continuity within the context of CNN.
Whereas numerous studies have explored incorporating classical mechanics-inspired inductive biases into neural networks to construct dynamical models, a promising direction would be to develop autoencoders that explicitly impose other forms of prior knowledge.
Moreover,
we would like to investigate the effectiveness of advanced architectures, such as Vision Transformers \citep{alexey2021vit} and their variants \citep{sriwastawa2024vision}, as autoencoders for learning dynamics from images in future work. This approach holds promise because the operation applied to each patch in ViTs can be interpreted as a convolution with a kernel size equal to the patch size, and the mathematical formulation introduced is not restricted to CNNs.

% Another interesting research direction would be to


\subsubsection*{Acknowledgments}
This project is supported by the National Research Foundation, Singapore, under its AI Singapore Programme (AISG Award No:
AISG3-RP-2022-028) and the NRF fellowship (project No. NRF-NRFF13-2021-0005).
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{ref}
% \bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix
\section{Appendix}
\allowdisplaybreaks

\subsection{Rigid motion modeling}\label{app:rigid}

Here we consider rigid body motion, namely that all its particles maintain the same distance relative to each other.
The position of the whole body can be represented by the imaginary translation and rotation that is needed to move the object from a reference placement to its current placement


We consider the motion of a rigid body, where all particles maintain a fixed distance relative to each other. The position of the entire body can be described by the combined translation and rotation required to move it from a reference placement to its current placement.

Let us first select a reference particle, denoted as
$A$, typically chosen to coincide with the bodys center of mass or centroid. When the rigid body is in its reference placement, the position vector of $A$ is denoted by $\vr_A^0$. For another particle $B$, its position vector can then be expressed as:
\begin{equation*}
\vr_B^0 = \vr_A^0 + \vr_B^0 - \vr_A^0 = \vr_A^0 + |\vr_B^0 - \vr_A^0| (\cos \theta_B^0, \sin \theta_B^0)^{\top},
\end{equation*}
where $|\vr_B^0 - \vr_A^0|$ denotes the distance between points $A$ and $B$, and $\theta_B^0$ represents the direction angle of the vector $\vr_B^0 - \vr_A^0$.

When the rigid body is in its current placement, we denote the position vector of $A$ by $\vr_A$.
Then the position vector of particle $B$ can be written as
\begin{equation*}
\vr_B = \vr_A + \vr_B - \vr_A = \vr_A + |\vr_B - \vr_A|(\cos \theta_B, \sin \theta_B)^{\top}.
\end{equation*}

Since the body is assumed to be rigid, the distance $|\vr_B - \vr_A| = |\vr_B^0 - \vr_A^0|$ remains invariant, and there exists an angle $\theta$ such that $\theta_B = \theta_B^0 + \theta$ for all particles $B$. Thus, we have:
\begin{align*}
\vr_B &= \vr_A^0 + \vr_A - \vr_A^0 + |\vr_B^0 - \vr_A^0| (\cos \theta_B^0 + \theta, \sin \theta_B^0 + \theta)^{\top} \\
&=  \vr_A^0 + \vr_A - \vr_A^0 + |\vr_B^0 - \vr_A^0|  \begin{pmatrix}
\cos \theta & -\sin \theta\\
\sin \theta & \cos \theta
\end{pmatrix}
\begin{pmatrix}\cos \theta_B^0\\ \sin \theta_B^0\end{pmatrix}
\\
&=\vr_A - \vr_A^0 +  \begin{pmatrix}
\cos \theta & -\sin \theta\\
\sin \theta & \cos \theta
\end{pmatrix}(\vr_B^0-\vr_A^0)+ \vr_A^0.
\end{align*}
Thus, the position of the entire body can be described using the translation of the reference point $\vr = \vr_A - \vr_A^0$, and the orientation of the body $\theta$. Let $\Omega = \{ \vr_B^0\in \R^2| \text{$B$ is the particle constitute the rigid body}\}$ denote the set of particle positions when the rigid body is in its reference placement.  The set of positions corresponding to the state $z = (\vr, \theta)$ in the current placement can then be expressed as:
\begin{equation*}
    \mathcal{S}(z) = \vr + \Phi_{\theta}(\Omega),
\end{equation*}
where $\vr = \vr_A - \vr_A^0$ represents the translation of the object, and
$\Phi_{\theta}: \R^2\rightarrow \R^2$ is defined as
\begin{equation*}
\Phi_{\theta} ( \vr_B^0) =\begin{pmatrix}
\cos \theta & -\sin \theta\\
\sin \theta & \cos \theta
\end{pmatrix} (\vr_B^0 - \vr_A^0) + \vr_A^0,
\end{equation*}
which represents the rotation of the object. Given that there are $K$ objects, the rigid body motion on a two-dimensional plane can be given by \cref{eq:ode2}.


\subsection{Proof of \cref{theorem}}\label{sec:proofs}
\subsubsection{Function representation of CNN autoencoders}
For the convenience of analysis, we represent the feature maps and the filters of CNN autoencoder as functions defined on a two-dimensional plane:
\begin{equation}\label{eq:cnn1}
\begin{aligned}
&\text{Input: } &&\mathcal{I}_0 (y^{0},z),\\
&\text{Hidden layers: }
&&\mathcal{I}_{l}(y, z)= \sum_{y^{l-1}\in Y^{l-1}} \mathcal{I}_{l-1}(y^{l-1},z) \phi_{l}(y^{l-1}, y) \varepsilon_l,\ l=1,\cdots, L,\\
&\text{Output: }&&\mathcal{I}_{L}(y^{L}, z),\ y^{L} \in Y^{L}.
\end{aligned}
\end{equation}
where the input is defined as
\begin{equation*}
\mathcal{I}_{0}(y,\ z) =
\left\{
\begin{aligned}
&[\mI_{\delta} \circ \mathcal{S}(z)]_{i_1, i_2}, && \text{if } y \in [i_1\delta,(i_1+1)\delta)\times [i_2\delta, (i_2+1)\delta), \quad i_1, i_2=0,\cdots, I,\\
&0, &&\text{if } y\notin [0,1]^2.
\end{aligned}
\right.
\end{equation*}
I.e., $\mathcal{I}_{0}$ is a piece-wise constant approximation of the indicator function of the set $\mathcal{S}(z)$, with a partition size of $\delta$.
Here, we denote the evaluation set of the$l$-th layer as $Y^l\subset\R^2$, which is determined by the parameter stride:
\begin{equation}\label{eq:yl}
Y^l = \left\{\left(i_1\delta_l,\ i_2\delta_l \right)\right\}_{i_1, i_2=0}^{\left\lfloor {I}/{\prod_{i=1}^l s_{i}}\right\rfloor + J_l},\ \delta_l = (\prod_{i=1}^{l} s_{i})\delta, \ l=0,\cdots, L.
\end{equation}
$\phi^l: \R^2\times \R^2 \rightarrow \R$ represent the filter of $l$-th layer, i.e,
\begin{equation*}
\phi_{l}\left( y^l + y, y^{l}\right) =
\left\{
\begin{aligned}
& \mathcal{W}_l(j_1\delta, j_2\delta), &&\text{if }y \in [j_1\delta_{l-1},(j_1+1)\delta_{l-1} ) \times [j_2\delta_{l-1},(j_2+1)\delta_{l-1} ),\\
& &&j_1, j_2=0,\cdots,J_l,\\
&0, && \text{otherwise}.
\end{aligned}
\right.
\end{equation*}
Then it is straightforward to verify that  $\mathcal{I}_{l}:\R^2\times \mathcal{Z} \rightarrow \R$ represents the function corresponding to the feature map $\mI_l$, such that:
\begin{equation*}
\mathcal{I}_{l} \big((i_1\delta_l,\ i_2\delta_l) ,\ z \big) = [\mI_{l}]_{i_1, i_2},\ l=0,\cdots, L.
\end{equation*}
Refer to \cref{fig:cnn} for an illustration. All functions here depend on $\delta$. To avoid redundancy, we omit explicit notation of this dependence.
\begin{figure}[htbp]
\centerline{\includegraphics[width=1\linewidth]{cnn2.pdf}}
\caption{Illustration of the convolution operation.
The stride of 2 halves the size of the feature map, and a new evaluation set is generated by removing half of the elements from the previous one, specifically by deleting every other element. The blue dashed lines indicate operations, such as activation layers, that do not change the evaluation set.
}
\label{fig:cnn}
\end{figure}

Under \cref{eq:cnn1}, Assumption  \ref{ass:eva1} can be expressed in the following equivalent forms.
\begin{assumption}\label{ass:eva2}
There exist constant $M_{\Delta}$ such that if $\norm{\Delta}_{\infty} \leq M_{\Delta}$, then
\begin{equation*}
\mathcal{I}_l(y^{l},z)=0, \ \forall y^l\in (Y^l \cup (Y^l-\Delta)) \backslash (Y^l \cap (Y^l-\Delta)).
\end{equation*}
\end{assumption}





Under \cref{eq:cnn1}, \cref{theorem} has the following equivalent form.

\begin{theorem}\label{theorem2}
Assume that the underlying dynamical system is a rigid body motion (\ref{eq:ode2}) on a two-dimensional plane. If Assumption \ref{ass:eva2},  let $c_{\phi}$ be constant  satisfying
\begin{equation*}
\max_{l=1,\cdots,L}\max_{y^{l} \in \R^2}|\phi_{l}(y^{l-1}_1, y^{l}) - \phi_{l}(y^{l-1}_2, y^{l})| \leq \frac{c_{\phi}}{\prod_{i=1}^{l-1} s_{i}} \norm{y^{l-1}_1-y^{l-1}_2},\ \forall y^{l-1}_1, y^{l-1}_2 \in \R^2,
\end{equation*}
and if $s_l=2$ for $l=1, \cdots,L^*-1$, then for any $z_1=(z_1^t, z_1^r), z_2=(z_2^t, z_2^r) \in \mathcal{Z}$,
\begin{equation*}
\norm{\mathcal{E}\circ \mI_{\delta} \circ \mathcal{S}( z_1) - \mathcal{E} \circ \mI_{\delta} \circ \mathcal{S}(z_2)}\leq C c_{\phi} \norm{z_1^r-z_2^r} + \frac{Cc_{\phi}}{{2^{L^*-1}}}\norm{z_1^t-z_2^t},\ \text{ as }\ \delta\rightarrow 0.
\end{equation*}
Here $C$ is a constant independent of $\delta$ and $z$.

\end{theorem}
Given that $\max_l\max_{ j_1, j_2} |\phi_{l}(y^{l-1}_1, y^{l})|/ (\lceil J_l/2 \rceil \delta) \leq c_{\phi}$, we must choose $J_l=\mathcal{O}(1/\delta)$. Under this choice, the normalization coefficients $\varepsilon_l$ can be set as: $\varepsilon_1=\delta^2/ |\mathcal{S}(z)|$, $\varepsilon_l=\delta^2$ for $l=2,\cdots,L$.


% \begin{theorem}
% For motion model \cref{eq:ode2} with $D\geq 1$, let $c_{\phi}$ be constants satisfying
% \begin{equation*}
% \max_{l=1,\cdots,L}\max_{y^{l} \in \R^2}|\phi_{l}(y^{l-1}_1+\Delta, y^{l}) - \phi_{l}(y^{l-1}_2, y^{l})| \leq \frac{c_{\phi}}{\prod_{i=1}^{l-1} s_{i}} \norm{y^{l-1}_1-y^{l-1}_2},\ \forall y^{l-1}_1, y^{l-1}_2 \in \R^2.
% \end{equation*}
% If we take $s_l=2$ for $l=1, \cdots,L-1$, then for any $z_1, z_2 \in \mathcal{Z}$, we have
% \begin{equation*}
% \max_{y^{L}\in Y^{L}} | \mathcal{I}_{L}(y^{L}, z_1) - \mathcal{I}_{L}(y^{L}, z_2)| \leq c_{\mathcal{I}} \norm{z_1-z_2}, \text{ and }c_{\mathcal{I}}= C\frac{c_{\phi}}{2^{L}}, \text{ as }\delta\rightarrow 0,
% \end{equation*}
% where $C$ is a constant independent of $\delta$.
% \end{theorem}


\subsubsection{The case of $L^*=1$}
We present the proof of \cref{theorem2} for the case of $L^*=1$.

Consider the general latent ODE (\ref{eq:ode}). Assume that the image of $z$ can be represented as $\mathcal{S}(z)$. Furthermore, suppose there exist volume-preserving maps $\Phi_{\Delta}:\R^2\rightarrow \R^2$, dependent on $\Delta\in \R^D$, such that $\Phi_{\Delta}(\mathcal{S}(z)) = \mathcal{S}(z+\Delta)$ and $\| \Phi_{\Delta}(x) - x \| \leq c_1\norm{\Delta}$ for some constant $c$. It is straightforward to verify that rigid body motions, including translation and rotation, satisfy these conditions.

Let
\begin{equation*}
\tilde{\mathcal{I}}_{1}(y^{1}, z)= \frac{1}{|\mathcal{S}(z)|} \int_{\R^2}\mathcal{I}_0(y^0, z)\phi_{1}(y^{0}, y^{1}) dy^0 = \frac{1}{|\mathcal{S}(z)|}\int_{S(z)}\phi_{1}(y^{0}, y^{1}) dy^0,
\end{equation*}
we can verify that $\lim_{\delta\rightarrow0} \mathcal{I}_{1}(y^{1}, z) = \tilde{\mathcal{I}}_{1}(y^{1}, z)$.

In addition, we have
\begin{align*}
|\tilde{\mathcal{I}}_{1}(y^{1}, z+\Delta) - \tilde{\mathcal{I}}_{1}(y^{1}, z)|=&\frac{1}{|\mathcal{S}(z)|}\left|\int_{\mathcal{S}(z + \Delta)}\phi_{1}(y^{0}, y^{1}) dy^0 - \int_{\mathcal{S}(z)}\phi_{1}(y^{0}, y^{1}) dy^0\right|\\
=& \frac{1}{|\mathcal{S}(z)|}\left|\int_{\mathcal{S}(z)}\phi_{1}(\Phi_{\Delta}(y^0), y^{1}) dy^0 - \int_{\mathcal{S}(z)}\phi_{1}(y^{0}, y^{1}) dy^0\right|\\
\leq & \frac{1}{|\mathcal{S}(z)|}|\mathcal{S}(z)| c_1 c_{\phi}\norm{\Delta} \leq c_1 c_{\phi}\norm{\Delta}.
\end{align*}

Taking $\delta^*$ such that for all $\delta\leq \delta^*$, the following inequalities hold:
\begin{align*}
&|\mathcal{I}_{1}(y^{1}, z)-\tilde{\mathcal{I}}_{1}(y^{1}, z)|\leq c_1c_{\phi}\norm{\Delta},\\
&|\mathcal{I}_{1}(y^{1}, z+\Delta)-\tilde{\mathcal{I}}_{1}(y^{1}, z+\Delta)|\leq c_1c_{\phi}\norm{\Delta}.
\end{align*}
These inequalities yield
\begin{equation*}
|\mathcal{I}_{1}(y^{1}, z+\Delta) - \mathcal{I}_{1}(y^{1}, z)|\leq 3c_1c_{\phi}\norm{\Delta},
\end{equation*}
which concludes the proof.


\subsubsection{The case of rigid body translational motion}
We now present the proof of \cref{theorem2} for the case of rigid body motion involving only translation. We rewrite \cref{eq:cnn1} in the following form to emphasize the dependence of evaluation sets.
\begin{equation}\label{eq:cnn2}
\begin{aligned}
&\text{Input: } &&\mathcal{I}_0 (y^{0},z)\\
&\text{Hidden layers: }
&&\mathcal{I}_{l+1}(y, z| Y^l, \cdots, Y^0)= \sum_{y^{l}\in Y^{l}} \mathcal{I}_l(y^{l},z| Y^{l-1}, \cdots, Y^0) \phi_{l+1}(y^{l}, y),\ l\geq 0,\\
&\text{Output: }&&\mathcal{I}_{L}(y^{L}, z| Y^{L-1}, \cdots, Y^0),\ y^{L} \in Y^{L}.
\end{aligned}
\end{equation}
Under the above expression, if $Y^l$ is not of form given in \cref{eq:yl}, we can still compute the output.

\begin{definition}
Two evaluation sets $Y^l$ and $ \hat{Y}^l$ are called equivalent, denoted as $Y^l = \hat{Y}^l$, if $\ \forall y^l \in Y^l \cup \hat{Y}^l \backslash Y^l \cap \hat{Y}^l$, we have $ \mathcal{I}_l(y^{l},z| Y^{l-1}, \cdots, Y^0)=0$.
\end{definition}
We can readily check that two equivalent evaluation sets yield equivalent results for the convolution operation, i.e., if $Y^l = \hat{Y}^l$, $\mathcal{I}_{l+1}(y^{l+1}, z| Y^l,Y^{l-1}, \cdots, Y^0) = \mathcal{I}_{l+1}(y^{l+1}, z| \hat{Y}^l,Y^{l-1}, \cdots, Y^0)$.


Due to weight sharing, the convolution operation exhibits translational invariance. This property leads to the following characteristic of the function $\phi_{l}$ representing the weights of the filter.
\begin{property}\label{pro:cnn_kernel}
The function $\phi_{l}$ representing the weights of the convolution filter satisfies
\begin{equation*}
\phi_{l}(y^{l-1}+ \Delta, y^{l}) = \phi_{l}(y^{l-1}, y^{l} - \Delta), \quad \forall \Delta \in \R^2.
\end{equation*}
\end{property}
For our motion model \cref{eq:ode2}, if $K=1$, the image of $z$ is translationally invariant due to \cref{eq:image_of_state}, i.e., $\mathcal{I}_0(y, z+\Delta) = \mathcal{I}_0(y-\Delta, z)$. Similar property holds for each feature map.
\begin{lemma}
For motion model \cref{eq:ode2} with $K=1$, the function $\mathcal{I}_l$ representing the feature map satisfies
\begin{equation*}
\mathcal{I}_{l}(y^l, z+\Delta| Y^{l-1}, \cdots, Y^0) = \mathcal{I}_{l}(y^l-\Delta, z | Y^{l-1}-\Delta, \cdots, Y^0-\Delta), \quad \forall \Delta \in \R^2.
\end{equation*}
\end{lemma}
\begin{proof}
The case when $l = 0$ is obvious. Suppose now
\begin{equation*}
  \mathcal{I}_{l}(y^l, z+\Delta| Y^{l-1}, \cdots, Y^0) = \mathcal{I}_{l}(y^l-\Delta, z | Y^{l-1}-\Delta, \cdots, Y^0-\Delta),
\end{equation*}
then,
\begin{align*}
&\mathcal{I}_{l+1}(y^{l+1}, z+\Delta | Y^{l}, \cdots, Y^0) \\
=& \sum_{y^{l}\in Y^l} \mathcal{I}_l(y^{l},z+\Delta| Y^{l-1}, \cdots, Y^0) \phi_{l+1}(y^{l}, y^{l+1})\varepsilon_l\quad &&(\text{by \cref{eq:cnn2}}) \\
=& \sum_{y^{l}\in Y^l} \mathcal{I}_{l}(y^l-\Delta, z | Y^{l-1}-\Delta, \cdots, Y^0-\Delta) \phi_{l+1}(y^{l}, y^{l+1})\varepsilon_l\quad &&(\text{by inductive hypothesis})  \\
=& \sum_{\hat{y}^{l}\in Y^l-\Delta} \mathcal{I}_{l}(\hat{y}^{l}, z | Y^{l-1}-\Delta, \cdots, Y^0-\Delta) \phi_{l+1}(\hat{y}^{l} + \Delta, y^{l+1})\varepsilon_l \quad &&(\text{by taking $\hat{y}^{l} = y^l-\Delta$})\\
=& \sum_{\hat{y}^{l}\in Y^l-\Delta} \mathcal{I}_{l}(\hat{y}^{l}, z | Y^{l-1}-\Delta, \cdots, Y^0-\Delta) \phi_{l+1}(\hat{y}^{l}, y^{l+1} - \Delta)\varepsilon_l\quad &&(\text{by Property \ref{pro:cnn_kernel}}) \\
=&\mathcal{I}_{l+1}(y^{l+1}-\Delta, z | Y^{l}-\Delta, \cdots, Y^0-\Delta).\quad &&(\text{by \cref{eq:cnn2}})
\end{align*}
Hence the induction holds and the proof is completed.
\end{proof}


As a direct consequence, if $Y^{l-1}-\Delta=Y^{l-1}, \cdots, Y^0-\Delta = Y^0$, we have the following corollary. Here, we omit the notation of evaluation set for brevity.

\begin{corollary}
For motion model \cref{eq:ode2} with $K=1$, and further assume that $\Delta \in \R^2$ satisfies $Y^{l-1}-\Delta=Y^{l-1}$, we have
\begin{equation*}
\mathcal{I}_{l}(y^l, z+\Delta) = \mathcal{I}_{l}(y^l-\Delta, z).
\end{equation*}
\end{corollary}

\begin{corollary}\label{cor:deri}
For motion model \cref{eq:ode2} with $K=1$, and further assume that $\Delta$ satisfies $Y^{l-1}-\Delta=Y^{l-1}$, then we have
\begin{equation*}
\begin{aligned}
\mathcal{I}_{l}(y^{l}, z+\Delta) - \mathcal{I}_{l}(y^{l}, z)
=&\sum_{y^{l-1}\in Y^{l-1}} \mathcal{I}_{l-1}(y^{l-1},z) \left(\phi_{l}(y^{l-1}+\Delta, y^{l}) - \phi_{l}(y^{l-1}, y^{l})\right)\varepsilon_l. \\
\end{aligned}
\end{equation*}
\end{corollary}
\begin{proof}
\begin{align*}
&\mathcal{I}_{l}(y^{l}, z+\Delta) - \mathcal{I}_{l}(y^{l}, z) \\
=& \sum_{y^{l-1}\in Y^{l-1}} \left(\mathcal{I}_{l-1}(y^{l-1},z+\Delta) - \mathcal{I}_{l-1}(y^{l-1},z)\right) \phi_{l}(y^{l-1}, y^{l})\varepsilon_l\\
=& \sum_{y^{l-1}\in Y^{l-1}} \mathcal{I}_{l-1}(y^{l-1}-\Delta, z) \phi_{l}(y^{l-1}, y^{l}) - \sum_{y^{l-1}\in Y^{l-1}}\mathcal{I}_{l-1}(y^{l-1},z) \phi_{l}(y^{l-1}, y^{l})\varepsilon_l\\
=& \sum_{\hat{y}^{l-1}\in Y^{l-1}-\Delta } \mathcal{I}_{l-1}(\hat{y}^{l-1}, z) \phi_{l}(\hat{y}^{l-1} + \Delta, y^{l}) - \sum_{y^{l-1}\in Y^{l-1}}\mathcal{I}_l(y^{l-1},z) \phi_{l}(y^{l-1}, y^{l})\varepsilon_l\\
=&\sum_{y^{l-1}\in Y^{l-1}} \mathcal{I}_{l-1}(y^{l-1},z) \left(\phi_{l}(y^{l-1}+\Delta, y^{l}) - \phi_{l}(y^{l-1}, y^{l})\right)\varepsilon_l.
\end{align*}
\end{proof}

For convenience, we introduce the following notations:
\begin{itemize}
    \item For $\Delta$ satisfying $Y^0-\Delta= Y^0$, we denote
    \begin{equation*}
        a_l(\Delta) = \max_{y^{l}\in Y^{l}} |\mathcal{I}_{l}(y^{l}, z+\Delta) - \mathcal{I}_{l}(y^{l}, z)|,
    \end{equation*}
    \item For $\Delta$ satisfying $Y^{l-1}-\Delta= Y^{l-1}$, we denote
    \begin{equation*}
        b_l(\Delta) = \max_{y^{l} \in Y^{l}}\max_{y^{l-1}\in Y^{l-1}}|\phi_{l}(y^{l-1}+\Delta, y^{l}) - \phi_{l}(y^{l-1}, y^{l})|
    \end{equation*}
    \item We let $M_{\mathcal{I}}$ and $M_{\phi}$ be constants satisfying
    \begin{equation*}
\begin{aligned}
M_{\mathcal{I}}= \max_l \max_{z\in \mathcal{Z}}\left(\sum_{y^{l}\in Y^{l},\ y^{l}\neq 0} |\mathcal{I}_{l}(y^{l},z)| \varepsilon_l \right),\
M_{\phi} = \max_{l}\max_{y^{l}\in Y^{l}} \sum_{y^{l-1}\in Y^{l-1}}|\phi_{l}(y^{l-1}, y^{l})|\varepsilon_l.
\end{aligned}
\end{equation*}
\end{itemize}

Then we have the following property.
\begin{lemma}\label{lem:dis1}
For motion model \cref{eq:ode2} with $K=1$, let $\{\hat{\Delta}^{l}\}_{l=1}^L$ be a sequence defined recursively by
$\hat{\Delta}^{L} = \Delta$ and for $l=L-1, \cdots, 1$, $\hat{\Delta}^{l}$ is a variation satisfying $Y^{l}-(\hat{\Delta}^{l+1}- \hat{\Delta}^{l})=Y^{l}$.
Then we have
\begin{equation*}
a_l(\hat{\Delta}^{l}) \leq M_{\phi}^{I} a_{l-I}(\hat{\Delta}^{l-I}) + \sum_{i=0}^{I-1} M_{\phi}^i M_{\mathcal{I}} b_{l-i}(\hat{\Delta}^{l-i} - \hat{\Delta}^{l-i-1}), \quad 1\leq I \leq l \leq L.
\end{equation*}
\end{lemma}
\begin{proof}
Observing that $(\hat{\Delta}^{l} - \hat{\Delta}^{l-1})$ satisfies the condition in \cref{cor:deri}, we have
\begin{align*}
&|\mathcal{I}_{l}(y^{l}, z+\hat{\Delta}^{l})) - \mathcal{I}_{l}(y^{l}, z)|\\
=&|\mathcal{I}_{l}(y^{l}, z+\hat{\Delta}^{l-1}+(\hat{\Delta}^{l} - \hat{\Delta}^{l-1})) - \mathcal{I}_{l}(y^{l}, z+\hat{\Delta}^{l-1})| + |\mathcal{I}_{l}(y^{l}, z+\hat{\Delta}^{l-1}) - \mathcal{I}_{l}(y^{l}, z)|\\
\leq & \left|\sum_{y^{l-1}\in Y^{l-1}} \mathcal{I}_{l-1}(y^{l-1},z+\hat{\Delta}^{l-1}) \left(\phi_{l}(y^{l-1}+\hat{\Delta}^{l} - \hat{\Delta}^{l-1}, y^{l}) - \phi_{l}(y^{l-1}, y^{l})\right)\varepsilon_l \right|\\
&+\left|\sum_{y^{l-1}\in Y^{l-1}} \left(\mathcal{I}_{l-1}(y^{l-1},z+\hat{\Delta}^{l-1}) - \mathcal{I}_{l-1}(y^{l-1},z) \right) \phi_{l}(y^{l-1}, y^{l})\varepsilon_l\right|\\
\leq & M_{\mathcal{I}} b_l(\hat{\Delta}^{l} - \hat{\Delta}^{l-1}) + M_{\phi} a_{l-1}(\hat{\Delta}^{l-1}).
\end{align*}
This estimate can be rewritten as
\begin{equation*}
a_l(\hat{\Delta}^{l}) \leq M_{\mathcal{I}} b_l(\hat{\Delta}^{l} - \hat{\Delta}^{l-1}) + M_{\phi} a_{l-1}(\hat{\Delta}^{l-1}),
\end{equation*}
which yields
\begin{equation*}
a_l(\hat{\Delta}^{l}) \leq M_{\phi}^{I} a_{l-I}(\hat{\Delta}^{l-I}) + \sum_{i=0}^{I-1} M_{\phi}^i M_{\mathcal{I}} b_{l-i}(\hat{\Delta}^{l-i} - \hat{\Delta}^{l-i-1}),
\end{equation*}
and concludes the proof.
\end{proof}

\begin{lemma}\label{lemma1}
For motion model \cref{eq:ode2} with $K\geq 1$,
let $\Delta =(\Delta_1,\cdots, \Delta_K) $ with $\Delta_k \in \R^2$.
let $\{\hat{\Delta}^{l}=(\hat{\Delta}^l_1,\cdots, \hat{\Delta}^l_K), \ \hat{\Delta}^l_k\in \R^2\}_{l=1}^L$ be a sequence defined recursively by
$\hat{\Delta}^{L} = \Delta$ and for $l=L-1, \cdots, 1$, $\hat{\Delta}^{l}$ is a variation satisfying $Y^{l}-(\hat{\Delta}^{l+1}_k- \hat{\Delta}^{l}_k)=Y^{l}$ for $k=1,\cdots, K$.
If $\hat{\Delta}^0=0$, then we have
\begin{equation*}
a_l(\hat{\Delta}^{l}) \leq \sum_{k=1}^K\sum_{i=0}^{l-1} M_{\phi}^i M_{\mathcal{I}} b_{l-i}(\hat{\Delta}^{l-i}_k - \hat{\Delta}^{l-i-1}_k), \quad 1 \leq l \leq L.
\end{equation*}
\end{lemma}
\begin{proof}

Let $z = (\vr_1,\cdots,  \vr_K) = (z_1,\cdots, z_K)$ and
\begin{equation*}
    \mathcal{I}_0 (y^{0},z) = \sum_{k=1}^K \mathcal{I}_0^k(y^{l},z_k),
    \quad
    \mathcal{I}_{l}^k(y, z)= \sum_{y^{l-1}\in Y^{l-1}} \mathcal{I}_{l-1}^k(y^{l-1},z_k) \phi_{l}(y^{l-1}, y) \varepsilon_l,\ l=1,\cdots, L,
\end{equation*}
we next prove that $\mathcal{I}_l (y^{l},z) = \sum_{k=1}^K \mathcal{I}_l^k(y^{l},z_k)$ for $l=1,\cdots, L$, by induction on $l$. Suppose now this statement holds for $l$, then we have
\begin{align*}
\mathcal{I}_{l+1} (y^{l+1}, z)=& \sum_{y^{l}\in Y^{l}} \mathcal{I}_l (y^{l},z) \phi_{l+1}(y^{l}, y^{l+1})\varepsilon_l
=\sum_{y^{l}\in Y^{l}} \sum_{k=1}^K \mathcal{I}_l^k(y^{l},z_k) \phi_{l+1}(y^{l}, y^{l+1})\varepsilon_l.
\end{align*}
Swapping the order of summation leads to
\begin{align*}
\mathcal{I}_{l+1} (y^{l+1}, z)=&\sum_{k=1}^K\sum_{y^{l}\in Y^{l}}  \mathcal{I}_l^k(y^{l},z_k) \phi_{l+1}(y^{l}, y^{l+1})\varepsilon_l
= \sum_{k=1}^K\mathcal{I}_{l+1}^k (y^{l+1}, z_k),
\end{align*}
which completes the induction.
Applying \cref{lem:dis1} to $\mathcal{I}_l^d$ we have
\begin{align*}
&a_l(\hat{\Delta}^{l})=\max_{y^{l}\in Y^{l}} | \mathcal{I}_{l}(y^{l}, z+\hat{\Delta}^l) - \mathcal{I}_{l}(y^{l}, z)|
=\max_{y^{l}\in Y^{l}} | \sum_{k=1}^D \mathcal{I}_{l}^k(y^{l}, z_k+\hat{\Delta}^l_k) - \mathcal{I}_{l}^k(y^{l}, z)|\\
\leq & \sum_{k=1}^K \max_{y^{l}\in Y^{l}} | \mathcal{I}_{l}^k(y^{l}, z_k+\hat{\Delta}^l_k) - \mathcal{I}_{l}^k(y^{l}, z)|
\leq \sum_{k=1}^K\sum_{i=0}^{l-1} M_{\phi}^i M_{\mathcal{I}} b_{l-i}(\hat{\Delta}^{l-i}_k - \hat{\Delta}^{l-i-1}_k),
\end{align*}
which concludes the proof.
\end{proof}


By appropriately selecting the sequence $\{\hat{\Delta}^{l}\}_{l=1}^L$, we immediately have the following corollary.
\begin{corollary}\label{corollary}
For motion model \cref{eq:ode2} with $K\geq 1$, suppose $\Delta=(\Delta_1,\cdots, \Delta_K)$ satisfies
\begin{equation}\label{eq:integer of delta}
\Delta_k =\left( \Delta_{k,1}, \Delta_{k,2} \right) = \left( I_{k,1} \delta,\ I_{k,2}\delta \right)\in \R^2, \text{ where } I_{k,1}, I_{k,2} \text{ are integers}.
\end{equation}
Additionally, let $c_{\phi}$ be constants satisfying $b_l\left(\big(i_1\prod_{i=1}^{l-1} s_{i}\big)\delta,\ \big(i_2\prod_{i=1}^{l-1} s_{i}\big)\delta \right) \leq c_{\phi}(|i_1| \delta + |i_2|\delta) $, and let $l^*_{k,j}$ is the largest
integer satisfying $l^*_{k,j} \leq L$ and $2^{l^*_{k,j}-1}\leq |I_{k,j}|$ for $j=1,2$ and $k=1, \cdots, K$.
If we take $s_l=2$ for $l=1, \cdots,L-1$, then we have
\begin{equation*}
a_L(\Delta) \leq \sum_{i=0}^{L-1} M_{\phi}^i M_{\mathcal{I}} \sum_{k=1}^K \left(\frac{c_{\phi}}{2^{l^*_{k,1}-1} }| \Delta_{k,1}|+ \frac{c_{\phi}}{2^{l^*_{k,2}-1} }| \Delta_{k,2}|\right).
\end{equation*}
\end{corollary}
\begin{proof}
Let $\hat{\Delta}^{l}_k=\left( i_{k,1}^l \delta,\ i_{k,2}^l\delta \right)\in \R^2$, where $i_{k,1}^l, i_{k,2}^l$ are integers defined recursively by
\begin{equation*}
\begin{aligned}
i_{k,1}^L = I_{k,1},\quad
i_{k,1}^{l-1} = i_{k,1}^{l} \bmod  2^{l-1},\quad l=L, \cdots, 2,\quad i_{k,1}^{0}=0;\\
i_{k,2}^L = I_{k,2},\quad
i_{k,2}^{l-1} = i_{k,2}^{l} \bmod  2^{l-1},\quad l=L, \cdots, 2,\quad i_{k,2}^{0}=0.\\
\end{aligned}
\end{equation*}
We can readily check that the sequence $\{\hat{\Delta}^{l}=(\hat{\Delta}^l_1,\cdots, \hat{\Delta}^l_K)\}_{l=1}^L$  satisfies $Y^{l}-(\hat{\Delta}^{l+1}_k- \hat{\Delta}^{l}_k)=Y^{l}$ due to Assumption \ref{ass:eva2} and $\hat{\Delta}^{0}=0$. In addition, we have
\begin{equation*}
\begin{aligned}
b_{l-i}(\hat{\Delta}^{l-i}_k - \hat{\Delta}^{l-i-1}_k)
=& b_{l-i}\left( (i_{k,1}^{l-i}-i_{k,1}^{l-i-1}) \delta,\ (i_{k,2}^{l-i}-i_{k,2}^{l-i-1}) \delta \right)\\
=& b_{l-i}\left(\Big(\lfloor i_{k,1}^{l-i}/ 2^{l-i-1} \rfloor\cdot 2^{l-i-1} \Big)\delta,\ \Big(\lfloor i_{k,2}^{l-i}/ 2^{l-i-1} \rfloor\cdot 2^{l-i-1}\Big)\delta \right)\\
\leq & c_{\phi}\left| \lfloor i_{k,1}^{l-i}/ 2^{l-i-1}\rfloor \right|\delta + c_{\phi}\left|\lfloor i_{k,2}^{l-i}/ 2^{l-i-1}\rfloor\right| \delta \\
\leq& \frac{c_{\phi}}{2^{l^*_{k,1}-1} }| i_{k,1}^{L}|\delta + \frac{c_{\phi}}{2^{l^*_{k,2}-1} }| i_{k,2}^{L}|\delta,
\end{aligned}
\end{equation*}
where $l^*_{k,j}$ is the largest
integer satisfying $l^*_{k,j} \leq L$, $2^{l^*_{k,j}-1}\leq |i_{k,j}^{L}|$ for $j=1,2$ and $k=1, \cdots, K$. Finally, applying \cref{lemma1}, we have
\begin{equation*}
a_L(\Delta) \leq \sum_{i=0}^{L-1} M_{\phi}^i M_{\mathcal{I}} \sum_{k=1}^K \left(\frac{c_{\phi}}{2^{l^*_{k,1}-1} }| \Delta_{k,1}|+ \frac{c_{\phi}}{2^{l^*_{k,2}-1} }| \Delta_{k,2}|\right).
\end{equation*}
\end{proof}

\subsubsection{Proof of \cref{theorem2}}
With these results, we are able to provided the proof of \cref{theorem2}.
\begin{proof}[Proof of \cref{theorem2}]
Denote $\Delta = z_1-z_2$, $\Delta^r=z_1^r-z_2^r$, $\Delta^t =z_1^t-z_2^t$ and for any $\delta$, denote
\begin{equation*}
\hat{\Delta}^t=\arg\min_{\hat{\Delta} \text{ satisfies (\ref{eq:integer of delta})}} \norm{\Delta^t -\hat{\Delta}}, \quad \tilde{\Delta}^t= \Delta^t - \hat{\Delta}^t.
\end{equation*}
Then we have $\Delta=(\bm{0}, \Delta^r) + (\tilde{\Delta}^t, \bm{0}) + (\hat{\Delta}^t, \bm{0})$ and $\|\tilde{\Delta}^t\|\leq \delta$. Subsequently, we have
\begin{align*}
&\norm{\mathcal{E}_{\delta}\circ \mI_{\delta} \circ \mathcal{S}( z_1) - \mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z_2)}\\
\leq& \norm{\mathcal{E}_{\delta}\circ \mI_{\delta} \circ \mathcal{S}( z_2+(\bm{0}, \Delta^r)) - \mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z_2)}
\\
&+ \norm{\mathcal{E}_{\delta}\circ \mI_{\delta} \circ \mathcal{S}( z_2+(\bm{0}, \Delta^r) + (\bm{0}, \tilde{\Delta}^t)) - \mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z_2+(\bm{0}, \Delta^r))}\\
&+ \norm{\mathcal{E}_{\delta}\circ \mI_{\delta} \circ \mathcal{S}( z_2+(\tilde{\Delta}^t, \Delta^r) + (\bm{0}, \hat{\Delta}^t)) - \mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z_2+(\tilde{\Delta}^t, \Delta^r))}.
\end{align*}
By applying the conclusion for case of $L^*=1$ to the first two components and Corollary \ref{corollary} to the third components, we obtain
\begin{align*}
&\norm{\mathcal{E}_{\delta}\circ \mI_{\delta} \circ \mathcal{S}( z_1) - \mathcal{E}_{\delta} \circ \mI_{\delta} \circ \mathcal{S}(z_2)}\\
\leq& C c_{\phi}\norm{z_1^r-z_2^r} +C c_{\phi}\delta + \sum_{i=0}^{L-1} M_{\phi}^i M_{\mathcal{I}} \sum_{k=1}^K \left(\frac{c_{\phi}}{2^{l^*_{k,1}-1} }| \Delta_{k,1}|+ \frac{c_{\phi}}{2^{l^*_{k,2}-1} }| \Delta_{k,2}|\right).
\end{align*}
Let $\delta \rightarrow 0$, we conclude the proof.
\end{proof}

% \subsection{Continuous generalization of multi-channel convolution}\label{app:multi-channel}
% This section presents the formulation of multi-channel convolutions, which operate on multiple input channels and return multiple output channels.
% It can be derived from single-channel version \cref{eq:cnn} by generalizing the scalar-valued functions and scalar multiplication to vector-valued or matrix-valued counterparts.
% \begin{equation*}
% \begin{aligned}
% &\text{Input: } &&\mathcal{I}_0 (y^{0},z) = \mathcal{I}(y^{0},z),\\
% &\text{Hidden layers: }
% &&\mathcal{I}_{l+1}(y, z| Y^l, \cdots, Y^0)= \sum_{y^{l}\in Y^{l}} \phi_{l+1}(y^{l}, y) \mathcal{I}_l(y^{l},z| Y^{l-1}, \cdots, Y^0) ,\ l\geq 0,\\
% &\text{Output: }&&\mathcal{I}_{L}(y^{L}, z| Y^{L-1}, \cdots, Y^0),\ y^{L} \in Y^{L}.
% \end{aligned}
% \end{equation*}
% Here $\mathcal{I}_l(y^{l},z| Y^{l-1}, \cdots, Y^0) \in \R^{c_l}$, $\phi_{l+1}(y^{l}, y)\in \R^{c_{l+1}\times c_l}$.
% The parameter $c_l$ denotes the number of channels in the input of the $l$-th layer. It is noted that extending to multi-channels does not affect the theoretical results due to the consistency property of matrix norm.


\subsection{Additional experimental details }


\subsubsection{Model details}\label{app:model details}
\textbf{Details of training}.
% For the two-body system, all autoencoders are trained using the default setup with full-batch Adam optimization over $5\times 10^4$ epochs and a learning rate of $0.001$. All available data are used for training, and the trained models are subsequently employed to predict future dynamics.
% For the other datasets, t
The models are trained with Adam optimization for $500$ epochs, using a learning rate of $0.001$ and a batch size of $512$. The data are divided into training, validation, and testing sets, with $80\%$ used for training, $10\%$ for validation, and $10\%$ for testing.

\textbf{Dimensions of latent states}.
For the two-body and damped pendulum systems, the latent state dimension was set to 2. For the other datasets, a latent state dimension of 8 was used. These values are intentionally larger than the theoretical minimum to enhance the model's expressiveness.

\textbf{CNN autoencoder}.
The architecture of CNN encoder in this paper is adapted from the setting provided by \cite{chen2022automated}.
The encoder is a 16-layer CNN, with the parameters of the downsampling convolutional layers detailed in Tables \ref{table:parameter1} and \ref{table:parameter2}.
Each downsampling convolutional layer is followed by an additional convolutional layer with  same number of output channels, but a $3\times3$ filter, a stride of $1$, and zero padding of $1$ to enhance expressiveness. All convolutional layers are accompanied by a batch normalization layer and a ReLU activation function. The final latent variable is obtained through a linear layer.

\begin{table}[htbp]
\vskip -0.5cm
\caption{Architecture of the encoder in CpAE}
\begin{center}
\begin{tabular}{ l| c c c c c c c c}
\hline
{Layer} &  1& 2&  3&  4& 5&6 &7 &8
\\ \hline
Filter size     & 12  &12   &12   &4   &4   &4   &4   &(3,4)\\
Stride          & 2   &2    &2    &2   &2   &2   &2   &(1,2)\\
\#Filters& 16& 32& 64& 64& 64& 64& 64& 64\\
Padding & 5& 5& 5& 1& 1& 1& 1& 1\\
\hline
\end{tabular}
\end{center}
\label{table:parameter1}
\end{table}

\begin{table}[htbp]
\vskip -0.5cm
\caption{Architecture of the encoder in standard AE}
\begin{center}
\begin{tabular}{ l| c c c c c c c c}
\hline
{Layer} &  1& 2&  3&  4& 5&6 &7 &8
\\ \hline
Filter size     & 4  &4   &4   &4   &4   &4   &4   &(3,4)\\
Stride          & 2   &2    &2    &2   &2   &2   &2   &(1,2)\\
\#Filters& 16& 32& 64& 64& 64& 64& 64& 64\\
Padding & 1& 1& 1& 1& 1& 1& 1& 1\\
\hline
\end{tabular}
\end{center}
\label{table:parameter2}
\vskip -0.5cm
\end{table}
All autoencoders share the same decoder architecture, which leverages residual connections and multi-scale predictions to to improve reconstruction performance. In addition to the deconvolutional layers specified in Table \ref{table:parameterd}, each layer, except for the final one, is accompanied by an upsampling operation. This upsampling operation is implemented using a transposed convolutional layer with a filter size of $4 \times 4$, a stride of $2$, and a Sigmoid activation function. The outputs of the deconvolutional layer and the upsampling layer are then concatenated along the channel dimension to form the input for the subsequent layer.

\begin{table}[htbp]
\vskip -0.5cm
\caption{Architecture of the decoder}
\begin{center}
\begin{tabular}{ l| c c c c c c c c}
\hline
{Layer} &8& 7&  6&  5& 4&3 &2&1
\\ \hline
Filter size     & (3,4)  &4   &4   &4   &4   &4   &4   &4\\
Stride          & (1,2)   &2    &2    &2   &2   &2   &2   &2\\
\#Filters& 64& 64& 64& 64& 64& 32& 16& 3\\
Padding & 1& 1& 1& 1& 1& 1& 1& 1\\
Activation &ReLU&ReLU&ReLU&ReLU&ReLU&ReLU&ReLU&Sigmoid\\
\hline
\end{tabular}
\end{center}
\label{table:parameterd}
\vskip -0.2cm
\end{table}


\textbf{Filters size.}
Larger filters are essential to ensure continuity. For a filter in the \(l\)-th layer of size \(J_l \times J_l\), let \(m_l = \max_{j_1, j_2} |\mathcal{W}_l(j_1\delta, j_2\delta)|\) represent the largest weight value. The distance from the location of this maximum value to the filter boundary must be less than \(\lceil J_l/2 \rceil \delta\). This constraint leads to \(\max_l m_l / (\lceil J_l/2 \rceil \delta) \leq c_{\mathcal{W}}\). Namely,
\(
\max_l \max_{j_1, j_2} \frac{|\mathcal{W}_l(j_1\delta, j_2\delta)|}{\lceil J_l/2 \rceil \delta} \leq c_{\mathcal{W}}.
\)
In \cref{theorem}, we show that the constant for the translation component is given by \(\frac{c_{\mathcal{W}}}{2^{L^*-1}}\). Assuming \(\frac{c_{\mathcal{W}}}{2^{L^*-1}}, \max_l \max_{j_1, j_2} |\mathcal{W}_l(j_1\delta, j_2\delta)| = \mathcal{O}(1)\), it follows that we only need to ensure \(2^{L^*} J_l = \mathcal{O}(1/\delta)\). Based on this, we set \(L^* = 4\) in our experiments. The images we use are of size \(3 \times 128 \times 256\). This implies \(2^4 J_l = 128\) or \(2^4 J_l = 256\), which translates to \(J_l = 8\) or \(J_l = 16\). Taking the average of these values, we set \(J_l = 12\).


\textbf{VPNet}.
We employ a very small VPNet for regularization. It comprises three linear layers, each consisting of two sublayers. The activation function employed is Sigmoid.

\textbf{Neural ODE}.
Neural Ordinary Differential Equations (Neural ODE) \cite{chen2018neural} are continuous models by embedding neural networks into continuous dynamical systems. In this paper, we consider autonomous systems of first-order ordinary differential equations
\begin{equation*}
\frac{d}{dt}y(t) = f_{\theta}(y(t)),\quad y(0)=x,
\end{equation*}
The governing function $ f_{\theta}$ in the Neural ODE is parameterized by a FNN with two hidden layers, each containing 128 units and using a tanh activation function. Time discretization is performed using the explicit midpoint scheme.

\textbf{SympNet}.
We utilize LA-SympNets with the default training configuration provided by \citep{jin2023learning}.
This setup includes three layers, each containing two sublayers, and employs the Sigmoid activation function.

\textbf{HNN}.
The Hamiltonian in the HNN is parameterized by a FNN with two hidden layers, each containing 128 units and using a tanh activation function. Time discretization is performed using the explicit midpoint scheme.



\subsubsection{Details of datasets} \label{app:Details of datasets}

\textbf{Damped single pendulum}.
We consider a pendulum of length \( L \) and mass \( m \), attached to a fixed point by a rigid rod, free to swing under the influence of gravity. The moment of inertia of the pendulum is given by \( I = \frac{1}{3} mL^2 \). Let \( \theta \) represent the angular position and \( \dot{\theta} \) the angular velocity. The systems kinetic energy \( T \) and potential energy \( V \) are as follows:
\begin{equation*}
T = \frac{1}{2} I \dot{\theta}^2 = \frac{1}{6} mL^2 \dot{\theta}^2, \quad V = -\frac{1}{2} mgL \cos(\theta).
\end{equation*}
These energy yield the following second-order differential equation governing the system's dynamics:
\begin{equation*}
\ddot{\theta} = -\frac{3g}{2L} \sin(\theta).
\end{equation*}
Here we account for a damping force proportional to the angular velocity, with a damping coefficient \( k \), the final equation of motion for the damped pendulum becomes:
\[
\ddot{\theta} = -\frac{3g}{2L} \sin(\theta) - k \dot{\theta}.
\]
To simulate the physics equation of the single pendulum system, we set the pendulum mass $m = 1$, the pendulum length as $L = 0.125$ and the frictional constant $k = 0.8$. For each trajectory, we randomly sample the initial angular position $\theta$ and angular velocity $\dot{\theta}$.




\subsubsection{Additional results}\label{app:exp}
Here we show the continuous models allows for both predicting past states and interpolating between known points (\cref{fig:appendix}), and we visualize the evolution of the learned latent states (\cref{fig:evo}).

\begin{figure}[!tbp]
\centerline{\includegraphics[width=0.95\linewidth]{appendix1.png}}
\caption{Backward predictions (left) and interpolations (right). Continuous models can accomplish these tasks by changing the value or the sign
of $dt$ used in the integrator. This flexibility allows for both predicting past states and interpolating between known points.
}
\label{fig:appendix}
\end{figure}

\begin{figure}[!tbp]
\begin{minipage}{0.49\linewidth}%0.49
\centerline{\includegraphics[width=\textwidth]{sp_latent.pdf}}
\centerline{\small Pendulum}
\end{minipage}
\begin{minipage}{0.49\linewidth}
\centerline{\includegraphics[width=\textwidth]{ep_latent.pdf}}
    \centerline{\small Elastic double pendulum}
\end{minipage}
\caption{Evolution of the learned latent states for various autoencoders. CpAEs are capable of generating latent states that evolve continuously with time.
}
\label{fig:evo}
\end{figure}


\subsubsection{Reconstruct image from exact latent states: FNN autoencoder}\label{app:fnn}

We test the performance of FNN autoencoders using Two-body problem data. This dataset is generated in the same manner as in \cite{jin2023learning} and is used to illustrate the FNN autoencoder.
It consists of $100$ observations-images of size $100 \times 50$-captured at a time interval of $0.6$ seconds along a single trajectory.

As shown in \cref{fig:fnn}, all FNN-based methods, except HNN, accurately predict the ground truth within the given time interval for the two-body system data. Given the strong performance baseline set by these established methods, any further improvements are likely to be incremental and may not significantly surpass the current results.

However, this success does not extend to datasets with more complex visual patterns. \cref{fig:recon} illustrates that FNN autoencoders fail to achieve complete reconstruction for the damped pendulum and elastic double pendulum datasets.
Here, reconstruction indicates that the encoder extracts the current latent state from the input image, and the decoder maps it back to reconstruct the same image. In this process, the decoder uses the exact latent state as input, with no dynamical information involved or reflected in these reconstruction results.
As discussed in \cref{sec:2}, the third goal of learning dynamics from images is to identify a decoder capable of reconstructing pixel-level observations using the exact latent state as input. Therefore, we focus on CNNs in this paper.
\begin{figure}[!tbp]
\centerline{\includegraphics[width=0.6\linewidth]{tb.png}}
\vskip -0.3cm
\caption{Predictions for two-body dataset.
}
\vskip -0.3cm
\label{fig:fnn}
\end{figure}
% In this section, we numerically demonstrate that fully connected neural networks (FNNs) are often insufficient for achieving the this objective of reconstructing images with complex visual patterns, as evidenced in \cref{fig:recon}.
\begin{figure}[!tbp]
\centerline{\includegraphics[width=1\linewidth]{recon.png}}
\vskip -0.3cm
\caption{Reconstructions for simulation data.
}
\vskip -0.3cm
\label{fig:recon}
\end{figure}

\subsection{Ablation studies}
\subsubsection{Details for continuity of latent states}\label{app:continuity details}

The autoencoder model consists of a single-layer CNN with a \(48 \times 48\) convolutional kernel and 32 output channels, followed by a linear mapping to generate the latent states. This simple task and model selection enable us to clearly and directly demonstrate the limitations of conventional CNNs in achieving $\delta$-continuity. Herein, all autoencoders, regardless of the applied regularizers, are trained using full-batch Adam optimization for \(10^5\) epochs with a learning rate of 0.001. Once trained, a Neural ODE is employed to model the dynamics of the learned latent states. The governing function $ f_{\theta}$ in the Neural ODE is parameterized by a FNN with single hidden layers containing 64 units and using a tanh activation function.


As a supplement to \cref{fig:continuity}, the image prediction results are presented in \cref{fig:cir}. Since neither the standard autoencoder nor the addition of a conventional $L_2$ regularizer can predict the evolution of latent states, they fail to generate the predicted images. In contrast, the Neural ODE effectively captures the dynamics of the latent states generated by the proposed CpAE, enabling accurate predictions.

\begin{figure}[!tbp]
\centerline{\includegraphics[width=1\linewidth]{Abl_re.png}}
\vskip -0.3cm
\caption{Predictions for circular motion data.
}
\vskip -0.2cm
\label{fig:cir}
\end{figure}





\subsubsection{Effects of weights}
Here we compare the performance when vary the weights of regularization terms $\lambda_J$ and $\lambda_R$.

The weight $\lambda_J$ controls the trade-off between model complexity and the continuity of the filter. As shown on the left side of \cref{fig:vpt}, setting $\lambda_J$ too high penalizes overly complex models, which can lead to underfitting and suboptimal performance. Conversely, a lower $\lambda_J$ reduces the influence of regularization, increasing the risk of discontinuity in the learned latent states.
The weight $\lambda_R$ emphasizes orientation preservation (i.e., a positive determinant of the Jacobian) in the learned latent states. As shown on the right side of \cref{fig:vpt}, setting $\lambda_R$ too high shifts the focus toward volume preservation (i.e., a unit determinant of the Jacobian), which can cause underfitting and degrade model performance.


\begin{figure}[!tbp]
\centerline{\includegraphics[width=1\linewidth]{vpt.pdf}}
\vskip -0.3cm
\caption{Results evaluated using the VPT and VPF metrics of varying weights. All values are scaled by a factor of 100, with higher scores indicating better performance. Note that the VPF metric is not reported for the circular motion dataset, as it consists of only a single trajectory.
}
\vskip -0.3cm
\label{fig:vpt}
\end{figure}


\end{document}
