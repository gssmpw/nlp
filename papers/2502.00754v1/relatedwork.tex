\section{Related works}
\textbf{Deep Autoencoder}.
An autoencoder \citep{baldi2012autoencoders, ranzato2007unsupervised, rumelhart1986learning} is a type of neural network that encodes input data into a compressed and meaningful representation, then decodes it to reconstruct the original input. A significant advancement in this area has been the development of Variational Autoencoders (VAEs) \citep{kingma2014auto}, which extend traditional autoencoders by incorporating probabilistic modeling of the latent space. Autoencoders can be adapted and extended to various models, finding broad applications, including image generation and classification \citep{kingma2014auto, yunchen2016variational}, data clustering \citep{guo2017deep, song2013auto}, and anomaly detection \citep{gong2019memorizing, zong2018deep}. In this paper, we focus on using autoencoders to learn latent dynamical models from images, and we propose novel continuity-preserving autoencoders that incorporate continuity prior for this task.

\textbf{Discovering Dynamical model}.
Discovering dynamical models from observed time-series data ${x_0, x_1,\cdots, x_N}$ is a fundamental challenge in science. Numerous approaches have been proposed to infer underlying dynamical systems from such data. Machine learning has emerged as a powerful tool for this task. One effective strategy involves constructing a neural network model, denoted as $\mathcal{N}$, to learn a data-driven flow map that predicts the system's future states \citep{chen2022automated, chen2024learning, wang2021bridging,wu2020data}. This model predicts the subsequent state $x_{n+1}$ based on the current state $x_n$. Alternatively, some researchers focus on modeling the governing function of the unknown differential equation \citep{chen2018neural, gonzalez1998identification, raissi2018multistep}. Given an input $x_n$, $x_{n+1}$ is obtained by solving the NN-parameterized ODE at time $\Delta t$, starting from the initial condition $x_n$.  This approach can offer valuable insights into the system's dynamics. For instance, it enables the characterization of invariant distributions \citep{gu2023stationary, lin2023computing}, energy landscapes \citep{chen2023constructing}, and other essential properties \citep{qiu2022mapping}, thereby expanding the scope of scientific investigation.
% requires an ODE solver and typically introduces potential numerical errors \citep{du2022discovery, keller2021discovery, zhu2022on}, it

\textbf{Learning dynamics from image observations}.
Numerous studies have explored the incorporation of classical mechanics-inspired inductive biases, such as physical principles \citep{cranmer2020lagrangian, greydanus2019hamiltonian, michael2019deep, yu2021onsagernet, zhang2022gfinns}, geometry structures \citep{eldred2024lie, jin2020sympnets, zhu2022vpnets}, and symmetries \citep{huh2020time, yang2024symmetry}, into deep neural networks. While some of these models have been applied to learn dynamics from images using autoencoders, they have mostly been tested on relatively simple visual patterns and dynamical behaviors.
An enhanced approach \citep{botev2021priors, toth2020hamintonian} involves employing VAEs to embed images, often resulting in improved predictive and generative performance. Given our priority on ensuring the continuous evolution of the latent states, this work focuses on deterministic autoencoders.

% Determining the optimal number of state variables for high-dimensional image data poses a significant challenge. \cite{chen2022automated} comprehensively investigated this issue and demonstrated the effectiveness of their approach across various physical dynamical systems. They also highlight the potential of learned latent state variables, referred to as neural state variables, for robust long-term prediction. However, their work primarily focused on discrete models, and the learned neural state variables do not evolve continuously. In this paper, we build upon their findings by selecting a subset of their datasets and configuring the dimensions of our latent states accordingly.