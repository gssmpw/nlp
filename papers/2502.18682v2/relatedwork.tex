\section{Related Work}
\subsection{AI as a Design Material}\label{subsec:AI_designmaterial}
Over the past decade, some HCI research has explored the concept of AI as a Design Material \cite{dove2017ux, holmquist2017intelligence, yildirim2022experienced, benjamin2021machine}. Researchers reasoned that integrating design thinking into AI innovation would open up the space for innovation by bringing more creative thinking to envisioning what might be built \cite{dove2017ux}. In reaction to the high failure rate of AI initiatives, these researchers suggested the problem might be what innovation teams chose to build in the first place. They noted that in many cases, there seemed to be a lack of ideation and time devoted to exploring many different concepts early on (e.g., sketching, brainstorming, and ideation). They noted that most AI-focused resources and guidebooks only address challenges that happen during the prototyping phase, well after problem formulation and project selection.

In response to this challenge, researchers developed various resources, tools, and processes that enable designers to engage with AI’s potential, envision how AI capabilities can address specific problems, and create value for users and service providers (e.g., \cite{fiebrink2010wekinator, yildirim2023creating, jansen2023mix}). Despite these efforts, findings show that design teams often struggle to identify situations where AI capabilities can realistically create value. Prior work examined why AI is "uniquely difficult to design for" \cite{yang2020re}, with AI’s inherent uncertainty identified as a core challenge. Additionally, some scholars have noted that AI is often metaphorically described as `magical' or `enchanting' because it seems to perform tasks previously thought impossible for computers \cite{lupetti2024making}. The positioning of AI as magic may partly explain why design and HCI seem to overestimate what AI can do and underestimate its costs and harms.

To address the issue of designers overestimating AI's capabilities, Yildirim et al. \cite{yildirim2023creating} introduced the Task-Expertise x AI-Performance matrix. This matrix helps innovation teams more easily recognize low-risk and high-value concepts when dealing with many possible things to build. The matrix consists of three rows representing levels of task expertise. %(expert, typical adult, less than typical adult). 
For example, a step counter (the task of noticing and counting steps) requires minimal task expertise, while recognizing cancer from a pathology image requires significant expertise. The matrix also has three columns indicating the minimum level of AI model performance needed for a user to experience the system as valuable. For example, automatic speech recognition of a voicemail only needs moderate performance to be useful, while automatic transcription of a court case would need excellent model performance to be useful. 

While this framework holds promise for helping teams generate more feasible AI concepts during ideation, it does not consider the ethical risks embedded in the concepts. Many ethical failures in AI stem from unrealistic problem formulations, so balancing performance requirements with feasibility presents an important area for further research. Our paper builds on this work by identifying high-risk areas within the AI design space, making these risks more visible during the design process.


\subsection{Responsible AI and FATE Research}\label{subsec:RAI_FATE}

Fairness, Accountability, Transparency, and Ethics (FATE) in sociotechnical systems has seen significant growth over the past decade with the emergence of new research communities and conferences (e.g., FAccT and AIES). Given that these research communities were initially dominated by machine learning researchers and legal scholars, much of the early FATE research focused on formalizing specific mathematical definitions of `fairness', along with creating algorithmic techniques that attempt to align existing datasets or AI models to comply with those definitions \cite{selbst2019fairness}. For instance, algorithmic fairness mitigation or `de-biasing' techniques typically rely upon existing datasets both to correct for unfairness and bias and to assess whether such corrections have been `successful'~\cite{Cooper2021-ft, dutta2020there, kallus2018residual}. They assume flawed AI systems should be fixed while never asking what developers should choose not to build.

However, recent findings reveal a paradox: when datasets are severely biased, de-biasing methods can sometimes inadvertently amplify the very biases they aim to correct \cite{Cooper2021-ft, dutta2020there, kallus2018residual}. Consequently, researchers have drawn attention to the fact that FATE concerns can be inherent to a specific problem formulation, requiring a fundamental AI system redesign rather than post hoc adjustments to models or datasets \cite{boyarskaya2020overcoming, holstein2019improving, passi2019problem, Raji2022-ls}. Moreover, some applications—particularly in high-stakes public sector contexts like child welfare—pose unavoidable risks. For example, early AI innovations aimed at predicting child maltreatment risk faced criticism due to the high costs of errors, prompting a shift toward lower-risk, preventive systems \cite{Eubanks2018-dk, Kawakami2022-ez, Stapleton2022-eb, saxena2024algorithmic}. 

In sum, most RAI tools and processes for AI practitioners have largely mirrored the focus on "making the thing right" rather than "making the right thing" \cite{buxton2010sketching}. These efforts focus on refining existing systems or documenting their limitations, as opposed to ideating and choosing better things to make \cite{holstein2019improving, wong2023seeing}. Moreover, research investigating industry product teams' current practices and challenges around AI fairness, found that teams were most interested in finding ways to avoid FATE challenges in the first place \cite{deng2023understanding, holstein2019improving, lee2024don}. To address this, several recent calls to action urge FATE researchers to turn their attention toward the earliest stages of the AI innovation process (e.g., \cite{boyarskaya2020overcoming, holstein2019improving, passi2019problem, Raji2022-ls, wang2023designing}). Our study contributes to this dialogue by showing how a design perspective can help teams preemptively unpack risk factors, identify high-risk regions, and refocus AI practitioners on designing the right thing in the first place.

An emerging dialogue within the FATE community on "AI functionality" highlights how harms often arise when systems underperform, when they make unexpected errors. For instance, Raji et al. \cite{Raji2022-ls} 
highlight that current AI FATE discussions frequently assume systems function as intended, focusing on "bias" and "fairness" without first addressing whether the system performs adequately.
%pointed out that discussions about AI FATE issues, both in academia and policy, frequently assume that the AI system in question actually works as well as intended. These conversations often focus on "bias" and "fairness" without first asking, "Does this system even work?" 
Many real-world harms stem from AI systems that underperform on their given tasks, they do not achieve an acceptable level of performance. This leads to situations where AI systems not only fail to deliver their promised value but also cause harm. Currently, there is a gap in how we should systematically address this type of AI performance failure. Our paper builds on this discourse by framing the issue as an AI Mismatch, making this discrepancy our central focus.

FATE researchers developed taxonomies of downstream algorithmic harms to help AI practitioners better understand and anticipate potential harms \cite{Shelby2023-ff, barocas2017problem, wang2022measuring, blodgett2020language}. Proactively anticipating harm for AI systems deployed in heterogeneous social contexts is inherently challenging because of the interplay between technologies and social and cultural dynamics \cite{Shelby2023-ff, blodgett2022responsible}. Here, a design approach to AI harm taxonomies can help create actionable resources that help AI practitioners systematically uncover potential sources of harm before committing to building a system. There is emerging interest in the FATE community to employ HCI and design methods to address FATE concerns at earlier stages (e.g., \cite{klumbyte2022critical, shen2022model, Stapleton2022-eb, suresh2022towards}). 
However, concrete processes or actionable guidance for early-stage AI development are still limited \cite{coston2023validity, holstein2019improving, Raji2022-ls}. 
Our work addresses this gap by proposing an approach that supports early AI concept analysis, systematically examining risk factors and revealing critical tradeoffs between risks and benefits.

%=================================================
%=================================================