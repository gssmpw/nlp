@MISC{EvidentlyAI,
 title    = {Evidently {AI} - {ML} and {LLM} system design: 450 case studies},
  author   = {{EvidentlyAI}},
  abstract = {How do top companies apply AI? A database of 450 case studies from
              100+ companies with practical ML use cases, LLM applications, and
              learnings from designing ML and LLM systems.},
  urldate  = {2024-09-12},
  language = {en}
}

@INPROCEEDINGS{Zimmerman2007-sa,
  title     = "Research through design as a method for interaction design
               research in {HCI}",
  author    = "Zimmerman, John and Forlizzi, Jodi and Evenson, Shelley",
  booktitle = "Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "493--502",
  abstract  = "For years the HCI community has struggled to integrate design in
               research and practice. While design has gained a strong foothold
               in practice, it has had much less impact on the HCI research
               community. In this paper we propose a new model for interaction
               design research within HCI. Following a research through design
               approach, designers produce novel integrations of HCI research in
               an attempt to make the right thing: a product that transforms the
               world from its current state to a preferred state. This model
               allows interaction designers to make research contributions based
               on their strength in addressing under-constrained problems. To
               formalize this model, we provide a set of four lenses for
               evaluating the research contribution and a set of three examples
               to illustrate the benefits of this type of research.",
  month     =  apr,
  year      =  2007
}

@INPROCEEDINGS{Yang2019-wp,
  title     = "Unremarkable {AI}: Fitting intelligent decision support into
               critical, clinical decision-making processes",
  author    = "Yang, Qian and Steinfeld, Aaron and Zimmerman, John",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  may,
  year      =  2019
}

@INPROCEEDINGS{Shen2022-zd,
  title     = "The model card authoring toolkit: Toward community-centered,
               deliberation-driven {AI} design",
  author    = "Shen, Hong and Wang, Leijie and Deng, Wesley H and Brusse, Ciell
               and Velgersdijk, Ronald and Zhu, Haiyi",
  booktitle = "2022 ACM Conference on Fairness, Accountability, and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2022
}

@INPROCEEDINGS{Klumbyte2022-cj,
  title     = "Critical tools for machine learning: Working with intersectional
               critical concepts in machine learning systems design",
  author    = "Klumbytė, Goda and Draude, Claude and Taylor, Alex S",
  booktitle = "2022 ACM Conference on Fairness, Accountability, and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2022
}

@ARTICLE{Blodgett2020-jw,
  title         = "Language (technology) is power: A critical survey of ``bias''
                   in {NLP}",
  author        = "Blodgett, Su Lin and Barocas, Solon and Daumé, III, Hal and
                   Wallach, Hanna",
  journal       = "arXiv [cs.CL]",
  abstract      = "We survey 146 papers analyzing ``bias'' in NLP systems,
                   finding that their motivations are often vague, inconsistent,
                   and lacking in normative reasoning, despite the fact that
                   analyzing ``bias'' is an inherently normative process. We
                   further find that these papers' proposed quantitative
                   techniques for measuring or mitigating ``bias'' are poorly
                   matched to their motivations and do not engage with the
                   relevant literature outside of NLP. Based on these findings,
                   we describe the beginnings of a path forward by proposing
                   three recommendations that should guide work analyzing
                   ``bias'' in NLP systems. These recommendations rest on a
                   greater recognition of the relationships between language and
                   social hierarchies, encouraging researchers and practitioners
                   to articulate their conceptualizations of ``bias''---i.e.,
                   what kinds of system behaviors are harmful, in what ways, to
                   whom, and why, as well as the normative reasoning underlying
                   these statements---and to center work around the lived
                   experiences of members of communities affected by NLP
                   systems, while interrogating and reimagining the power
                   relations between technologists and such communities.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lee2024-sg,
  title    = "``{I} don't know if we're doing good. {I} don't know if we're
              doing bad'': Investigating how practitioners scope, motivate, and
              conduct privacy work when developing {AI} products",
  author   = "Lee, Hank and Gao, Lan and Yang, Stephanie S and Forlizzi, Jodi
              and Das, Sauvik",
  journal  = "USENIX Secur Symp",
  abstract = "How do practitioners who develop consumer AI products scope,
              motivate, and conduct privacy work? Respecting privacy is a key
              principle for developing ethical, human-centered AI systems, but
              we cannot hope to better support practitioners without answers to
              that question. We interviewed 35 industry AI practitioners to
              bridge that gap. We found that practitioners viewed privacy as
              actions taken against pre-defined intrusions that can be
              exacerbated by the capabilities and requirements of AI, but few
              were aware of AI-specific privacy intrusions documented in prior
              literature. We found that their privacy work was rigidly defined
              and situated, guided by compliance with privacy regulations and
              policies, and generally demoti-vated beyond meeting minimum
              requirements. Finally, we found that the methods, tools, and
              resources they used in their privacy work generally did not help
              address the unique privacy risks introduced or exacerbated by
              their use of AI in their products. Collectively, these findings
              reveal the need and opportunity to create tools, resources, and
              support structures to improve practitioners’ awareness of
              AI-specific privacy risks, motivations to do AI privacy work, and
              ability to address privacy harms introduced or exacerbated by
              their use of AI in consumer products.",
  year     =  2024
}

@INPROCEEDINGS{Deng2023-vb,
  title     = "Understanding Practices, Challenges, and Opportunities for
               User-Engaged Algorithm Auditing in Industry Practice",
  author    = "Deng, Wesley Hanwen and Guo, Boyuan and Devrio, Alicia and Shen,
               Hong and Eslami, Motahhare and Holstein, Kenneth",
  booktitle = "Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--18",
  month     =  apr,
  year      =  2023
}

@ARTICLE{Wong2023-dx,
  title     = "Seeing like a toolkit: How toolkits envision the work of {AI}
               ethics",
  author    = "Wong, Richmond Y and Madaio, Michael A and Merrill, Nick",
  journal   = "Proc. ACM Hum. Comput. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  7,
  number    = "CSCW1",
  pages     = "1--27",
  abstract  = "Numerous toolkits have been developed to support ethical AI
               development. However, toolkits, like all tools, encode
               assumptions in their design about what work should be done and
               how. In this paper, we conduct a qualitative analysis of 27 AI
               ethics toolkits to critically examine how the work of ethics is
               imagined and how it is supported by these toolkits. Specifically,
               we examine the discourses toolkits rely on when talking about
               ethical issues, who they imagine should do the work of ethics,
               and how they envision the work practices involved in addressing
               ethics. Among the toolkits, we identify a mismatch between the
               imagined work of ethics and the support the toolkits provide for
               doing that work. In particular, we identify a lack of guidance
               around how to navigate labor, organizational, and institutional
               power dynamics as they relate to performing ethical work. We use
               these omissions to chart future work for researchers and
               designers of AI ethics toolkits.",
  month     =  apr,
  year      =  2023,
  language  = "en"
}

@INPROCEEDINGS{Laufer2022-wb,
  title     = "Four Years of {FAccT}: A Reflexive, Mixed-Methods Analysis of
               Research Contributions, Shortcomings, and Future Prospects",
  author    = "Laufer, Benjamin and Jain, Sameer and Cooper, A Feder and
               Kleinberg, Jon and Heidari, Hoda",
  booktitle = "2022 ACM Conference on Fairness, Accountability, and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "401--426",
  month     =  jun,
  year      =  2022
}

@INPROCEEDINGS{Jansen2023-cx,
  title     = "Mix \& Match Machine Learning: An Ideation Toolkit to Design
               Machine Learning-Enabled Solutions",
  author    = "Jansen, Anniek and Colombo, Sara",
  booktitle = "Proceedings of the Seventeenth International Conference on
               Tangible, Embedded, and Embodied Interaction",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--18",
  month     =  feb,
  year      =  2023
}

@MISC{Fiebrink2010-sj,
  title        = "The wekinator: A system for real-time, interactive machine
                  learning in music",
  author       = "Fiebrink, Rebecca and Cook, Perry R",
  booktitle    = "Proceedings of The Eleventh International Society for Music
                  Information Retrieval Conference (ISMIR 2010)(Utrecht)",
  year         =  2010,
  howpublished = "\url{https://citeseerx.ist.psu.edu/document?repid=rep1\&type=pdf\&doi=ced58510dbb49cc434d211092651d67aac030002}",
  note         = "Accessed: 2024-9-12"
}

@INPROCEEDINGS{Yildirim2022-ab,
  title     = "How Experienced Designers of Enterprise Applications Engage {AI}
               as a Design Material",
  author    = "Yildirim, Nur and Kass, Alex and Tung, Teresa and Upton, Connor
               and Costello, Donnacha and Giusti, Robert and Lacin, Sinem and
               Lovic, Sara and O'Neill, James M and Meehan, Rudi O'reilly and Ó
               Loideáin, Eoin and Pini, Azzurra and Corcoran, Medb and Hayes,
               Jeremiah and Cahalane, Diarmuid J and Shivhare, Gaurav and
               Castoro, Luigi and Caruso, Giovanni and Oh, Changhoon and McCann,
               James and Forlizzi, Jodi and Zimmerman, John",
  booktitle = "CHI Conference on Human Factors in Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--13",
  month     =  apr,
  year      =  2022
}

@ARTICLE{Holmquist2017-uq,
  title     = "Intelligence on tap: artificial intelligence as a new design
               material",
  author    = "Holmquist, Lars Erik",
  journal   = "Interactions",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  24,
  number    =  4,
  pages     = "28--33",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

@INPROCEEDINGS{Barocas2017-dg,
  title       = "The problem with bias: Allocative versus representational harms
                 in machine learning",
  author      = "Barocas, Solon and Crawford, Kate and Shapiro, Aaron and
                 Wallach, Hanna",
  booktitle   = "9th Annual conference of the special interest group for
                 computing, information and society",
  publisher   = "New York, NY",
  institution = "New York, NY",
  pages       =  1,
  year        =  2017
}

@INPROCEEDINGS{Kallus2018-wy,
  title     = "Residual Unfairness in Fair Machine Learning from Prejudiced Data",
  author    = "Kallus, Nathan and Zhou, Angela",
  editor    = "Dy, Jennifer and Krause, Andreas",
  booktitle = "Proceedings of the 35th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  80,
  pages     = "2439--2448",
  abstract  = "Recent work in fairness in machine learning has proposed
               adjusting for fairness by equalizing accuracy metrics across
               groups and has also studied how datasets affected by historical
               prejudices may lead to unfair decision policies. We connect these
               lines of work and study the residual unfairness that arises when
               a fairness-adjusted predictor is not actually fair on the target
               population due to systematic censoring of training data by
               existing biased policies. This scenario is particularly common in
               the same applications where fairness is a concern. We
               characterize theoretically the impact of such censoring on
               standard fairness metrics for binary classifiers and provide
               criteria for when residual unfairness may or may not appear. We
               prove that, under certain conditions, fairness-adjusted
               classifiers will in fact induce residual unfairness that
               perpetuates the same injustices, against the same groups, that
               biased the data to begin with, thus showing that even
               state-of-the-art fair machine learning can have a ``bias in, bias
               out'' property. When certain benchmark data is available, we show
               how sample reweighting can estimate and adjust fairness metrics
               while accounting for censoring. We use this to study the case of
               Stop, Question, and Frisk (SQF) and demonstrate that attempting
               to adjust for fairness perpetuates the same injustices that the
               policy is infamous for.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2018
}

@MISC{Heaven2022-fo,
  title        = "Why Meta’s latest large language model survived only three
                  days online",
  author       = "Heaven, Will Douglas",
  booktitle    = "MIT Technology Review",
  abstract     = "Galactica was supposed to help scientists. Instead, it
                  mindlessly spat out biased and incorrect nonsense.",
  month        =  nov,
  year         =  2022,
  howpublished = "\url{https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/}",
  note         = "Accessed: 2024-9-12",
  language     = "en"
}

@ARTICLE{Olson2023-ht,
  title     = "Google shares drop \$100 billion after its new {AI} chatbot makes
               a mistake",
  author    = "Olson, Emily",
  journal   = "NPR",
  publisher = "NPR",
  abstract  = "Google's Bard, an answer to Microsoft's ChatGPT, delivered a
               factual error in a search demo that the company shared widely.
               That sent Alphabet's market value plummeting this week.",
  month     =  feb,
  year      =  2023
}

@MISC{Akula2021-vj,
  title        = "Why Your {AI} Project Is Failing To Deliver Value",
  author       = "Akula, Vasudeva",
  booktitle    = "Forbes",
  abstract     = "Executives start with many hopes and expectations but
                  eventually struggle to put their models into production.",
  month        =  apr,
  year         =  2021,
  howpublished = "\url{https://www.forbes.com/councils/forbestechcouncil/2021/04/13/why-your-ai-project-is-failing-to-deliver-value/}",
  note         = "Accessed: 2024-9-12",
  language     = "en"
}

@MISC{Kell2024-xw,
  title        = "The {CEO} and {AI}: What’s ahead in 2024",
  author       = "Kell, John",
  booktitle    = "Fortune",
  abstract     = "Most CEOs plan to invest in generative AI. But some are
                  finding it difficult to cut through the buzz and determine
                  where to invest.",
  month        =  jan,
  year         =  2024,
  howpublished = "\url{https://fortune.com/2024/01/12/ai-playbook-c-suite-ceo/}",
  note         = "Accessed: 2024-9-12",
  language     = "en"
}

@INPROCEEDINGS{Raji2022-ls,
  title     = "The Fallacy of {AI} Functionality",
  author    = "Raji, Inioluwa Deborah and Kumar, I Elizabeth and Horowitz, Aaron
               and Selbst, Andrew",
  booktitle = "2022 ACM Conference on Fairness, Accountability, and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "959--972",
  month     =  jun,
  year      =  2022
}

@INPROCEEDINGS{Stapleton2022-eb,
  title     = "Imagining new futures beyond predictive systems in child welfare:
               A qualitative study with impacted stakeholders",
  author    = "Stapleton, Logan and Lee, Min Hun and Qing, Diana and Wright,
               Marya and Chouldechova, Alexandra and Holstein, Ken and Wu,
               Zhiwei Steven and Zhu, Haiyi",
  booktitle = "2022 ACM Conference on Fairness, Accountability, and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1162--1177",
  series    = "Discussion Paper Series 85",
  month     =  jun,
  year      =  2022
}

@INPROCEEDINGS{Holten-Moller2020-fe,
  title     = "Shifting Concepts of Value: Designing Algorithmic
               Decision-Support Systems for Public Services",
  author    = "Holten Møller, Naja and Shklovski, Irina and Hildebrandt, Thomas
               T",
  booktitle = "Proceedings of the 11th Nordic Conference on Human-Computer
               Interaction: Shaping Experiences, Shaping Society",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--12",
  month     =  oct,
  year      =  2020
}

@INPROCEEDINGS{Moon2024-si,
  title     = "A Human-Centered Review of Algorithms in Homelessness Research",
  author    = "Moon, Erina Seh-Young and Guha, Shion",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--15",
  month     =  may,
  year      =  2024
}

@INPROCEEDINGS{Saxena2020-qh,
  title     = "A Human-Centered Review of Algorithms used within the {U}.{S}.
               Child Welfare System",
  author    = "Saxena, Devansh and Badillo-Urquiola, Karla and Wisniewski,
               Pamela J and Guha, Shion",
  booktitle = "Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--15",
  month     =  apr,
  year      =  2020
}

@ARTICLE{Razi2022-gz,
  title     = "A Human-Centered Approach to Improving Adolescent Online Sexual
               Risk Detection Algorithms",
  author    = "Razi, Afsaneh",
  publisher = "stars.library.ucf.edu",
  abstract  = "Computational risk detection has the potential to protect
               especially vulnerable populations from online victimization.
               Conducting a comprehensive literature review on computational
               approaches for online sexual risk detection led to the
               identification that the majority of this work has focused on
               identifying sexual predators after-the-fact. Also, many studies
               rely on public datasets and third-party annotators to establish
               ground truth and train their algorithms, which do not accurately
               represent young social media users and their perspectives to
               prevent victimization. To address these gaps, this dissertation
               integrated human-centered approaches to both creating
               representative datasets and developing sexual risk detection
               machine learning models to ensure the broader societal impacts of
               this important work. In order to understand what and how
               adolescents talk about their online sexual interactions to inform
               study designs, a thematic content analysis of posts by
               adolescents on an online peer support mental health was
               conducted. Then, a user study and web-based platform, Instagram
               Data Donation (IGDD), was designed to create an ecologically
               valid dataset. Youth could donate and annotate their Instagram
               data for online risks. After participating in the study, an
               interview study was conducted to understand how youth felt
               annotating data for online risks. Based on private conversations
               annotated by participants, sexual risk detection classifiers were
               created. The results indicated Convolutional Neural Network (CNN)
               and Random Forest models outperformed in identifying sexual risks
               at the conversation-level. Our experiments showed that
               classifiers trained on entire conversations performed better than
               message-level classifiers. We also trained classifiers to detect
               the severity risk level of a given message with CNN outperforming
               other models. We found that contextual (eg, age, gender, and
               relationship type) and psycho-linguistic features contributed the
               most to accurately detecting sexual conversations. Our analysis
               provides insights into the important factors that enhance
               automated detection of sexual risks within youths' private
               conversations.",
  year      =  2022
}

@ARTICLE{Kittur2019-eu,
  title     = "Scaling up analogical innovation with crowds and {AI}",
  author    = "Kittur, Aniket and Yu, Lixiu and Hope, Tom and Chan, Joel and
               Lifshitz-Assaf, Hila and Gilon, Karni and Ng, Felicia and Kraut,
               Robert E and Shahaf, Dafna",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "Proceedings of the National Academy of Sciences",
  volume    =  116,
  number    =  6,
  pages     = "1870--1877",
  abstract  = "Analogy-the ability to find and apply deep structural patterns
               across domains-has been fundamental to human innovation in
               science and technology. Today there is a growing opportunity to
               accelerate innovation by moving analogy out of a single person's
               mind and distributing it across many information processors, both
               human and machine. Doing so has the potential to overcome
               cognitive fixation, scale to large idea repositories, and support
               complex problems with multiple constraints. Here we lay out a
               perspective on the future of scalable analogical innovation and
               first steps using crowds and artificial intelligence (AI) to
               augment creativity that quantitatively demonstrate the promise of
               the approach, as well as core challenges critical to realizing
               this vision.",
  month     =  feb,
  year      =  2019,
  keywords  = "AI; analogy; crowdsourcing; innovation; machine learning",
  language  = "en"
}

@INPROCEEDINGS{Yu2014-pr,
  title     = "Distributed analogical idea generation: inventing with crowds",
  author    = "Yu, Lixiu and Kittur, Aniket and Kraut, Robert E",
  booktitle = "Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  apr,
  year      =  2014,
  language  = "en"
}

@INPROCEEDINGS{Hope2017-np,
  title     = "Accelerating innovation through analogy mining",
  author    = "Hope, Tom and Chan, Joel and Kittur, Aniket and Shahaf, Dafna",
  booktitle = "Proceedings of the 23rd ACM SIGKDD International Conference on
               Knowledge Discovery and Data Mining",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  aug,
  year      =  2017
}

@BOOK{Meehl1954-ee,
  title     = "Clinical versus statistical prediction: A theoretical analysis
               and a review of the evidence",
  author    = "Meehl, Paul E",
  publisher = "University of Minnesota Press",
  address   = "Minneapolis",
  abstract  = "This monograph is an expansion of lectures given in the years
               1947-1950 to graduate colloquia at the universities of Chicago,
               Iowa, and Wisconsin, and of a lecture series delivered to staff
               and trainees at the Veterans Administration Mental Hygiene Clinic
               at Ft. Snelling, Minnesota. Perhaps a general remark in
               clarification of my own position is in order. Students in my
               class in clinical psychology have often reacted to the lectures
               on this topic as to a protective technique, complaining that I
               was biased either for or against statistics (or the clinician),
               depending mainly on where the student himself stood! This I have,
               of course, found very reassuring. One clinical student suggested
               that I tally the pro-con ratio for the list of honorific and
               derogatory adjectives in Chapter 1 (page 4), and the reader will
               discover that this unedited sample of my verbal behavior puts my
               bias squarely at the midline. The style and sequence of the paper
               reflect my own ambivalence and real puzzlement, and I have
               deliberately left the document in this discursive form to retain
               the flavor of the mental conflict that besets most of us who do
               clinical work but try to be scientists. I have read and heard too
               many rapid-fire, once-over-lightly ``resolutions'' of this
               controversy to aim at contributing another such. The thing is
               just not that simple. I was therefore not surprised to discover
               that the same sections which one reader finds obvious and
               over-elaborated, another singles out as especially useful for his
               particular difficulties. My thesis in a nutshell: ``There is no
               convincing reason to assume that explicitly formalized
               mathematical rules and the clinician's creativity are equally
               suited for any given kind of task, or that their comparative
               effectiveness is the same for different tasks. Current clinical
               practice should be much more critically examined with this in
               mind than it has been.'' (PsycInfo Database Record (c) 2022 APA,
               all rights reserved)",
  year      =  1954
}

@ARTICLE{AEgisdottir2006-op,
  title     = "The meta-analysis of clinical judgment project: Fifty-six years
               of accumulated research on clinical versus statistical prediction",
  author    = "Ægisdóttir, Stefanía and White, Michael J and Spengler, Paul M
               and Maugherman, Alan S and Anderson, Linda A and Cook, Robert S
               and Nichols, Cassandra N and Lampropoulos, Georgios K and Walker,
               Blain S and Cohen, Genna and Rush, Jeffrey D",
  journal   = "Couns. Psychol.",
  publisher = "SAGE Publications",
  volume    =  34,
  number    =  3,
  pages     = "341--382",
  abstract  = "Clinical predictions made by mental health practitioners are
               compared with those using statistical approaches. Sixty-seven
               studies were identified from a comprehensive search of 56 years
               of research; 92 effect sizes were derived from these studies. The
               overall effect of clinical versus statistical prediction showed a
               somewhat greater accuracy for statistical methods. The most
               stringent sample of studies, from which 48 effect sizes were
               extracted, indicated a 13\% increase in accuracy using
               statistical versus clinical methods. Several variables influenced
               this overall effect. Clinical and statistical prediction accuracy
               varied by type of prediction, the setting in which predictor data
               were gathered, the type of statistical formula used, and the
               amount of information available to the clinicians and the
               formulas. Recommendations are provided about when and under what
               conditions counseling psychologists might use statistical
               formulas as well as when they can rely on clinical methods.
               Implications for clinical judgment research and training are
               discussed.",
  month     =  may,
  year      =  2006,
  language  = "en"
}

@ARTICLE{Dawes1989-um,
  title     = "Clinical versus actuarial judgment",
  author    = "Dawes, R M and Faust, D and Meehl, P E",
  journal   = "Science",
  publisher = "American Association for the Advancement of Science (AAAS)",
  volume    =  243,
  number    =  4899,
  pages     = "1668--1674",
  abstract  = "Professionals are frequently consulted to diagnose and predict
               human behavior; optimal treatment and planning often hinge on the
               consultant's judgmental accuracy. The consultant may rely on one
               of two contrasting approaches to decision-making--the clinical
               and actuarial methods. Research comparing these two approaches
               shows the actuarial method to be superior. Factors underlying the
               greater accuracy of actuarial methods, sources of resistance to
               the scientific findings, and the benefits of increased reliance
               on actuarial approaches are discussed.",
  month     =  mar,
  year      =  1989,
  language  = "en"
}

@ARTICLE{Grove2000-ym,
  title     = "Clinical versus mechanical prediction: A meta-analysis",
  author    = "Grove, William M and Zald, David H and Lebow, Boyd S and Snitz,
               Beth E and Nelson, Chad",
  journal   = "Psychol. Assess.",
  publisher = "American Psychological Association (APA)",
  volume    =  12,
  number    =  1,
  pages     = "19--30",
  abstract  = "The process of making judgments and decisions requires a method
               for combining data. To compare the accuracy of clinical and
               mechanical (formal, statistical) data-combination techniques, we
               performed a meta-analysis on studies of human health and
               behavior. On average, mechanical-prediction techniques were about
               10\% more accurate than clinical predictions. Depending on the
               specific analysis, mechanical prediction substantially
               outperformed clinical prediction in 33\%–47\% of studies
               examined. Although clinical predictions were often as accurate as
               mechanical predictions, in only a few studies (6\%–16\%) were
               they substantially more accurate. Superiority for
               mechanical-prediction techniques was consistent, regardless of
               the judgment task, type of judges, judges' amounts of experience,
               or the types of data being combined. Clinical predictions
               performed relatively less well when predictors included clinical
               interview data. These data indicate that mechanical predictions
               of human behaviors are equal or superior to clinical prediction
               methods for a wide range of circumstances. (PsycINFO Database
               Record (c) 2016 APA, all rights reserved)",
  year      =  2000,
  language  = "en"
}

@ARTICLE{Shanks2024-bt,
  title    = "Enhancing clinical documentation workflow with ambient artificial
              intelligence: Clinician perspectives on work burden, burnout, and
              job satisfaction",
  author   = "Shanks, Denton and Shah, Tina and Hudson, Taina and Thompson,
              Jeffrey and Filardi, Tanya and Wright, Kelli and Ator, Greg and
              Smith, Timothy Ryan and Albrecht, Michael",
  journal  = "bioRxiv",
  abstract = "ABSTRACTObjectiveThis study assessed the effects of an ambient
              artificial intelligence (AI) documentation platform on clinicians’
              perceptions of documentation workflow.Materials and MethodsA pre-
              and post-implementation survey evaluated ambulatory clinician
              perceptions on impact of Abridge, an ambient AI documentation
              platform. Outcomes included clinical documentation burden, work
              after-hours, clinician burnout, work satisfaction, and patient
              access. Data were analyzed using descriptive statistics and
              proportional odds logistic regression to compare changes for
              concordant questions across pre- and post-surveys. Covariate
              analysis examined effect of specialty type and duration of use of
              the AI tool.ResultsSurvey response rates were 51.1\% (94/181)
              pre-implementation and 75.9\% (101/133) post-implementation.
              Clinician perception of ease of documentation workflow (OR = 6.91,
              95\% CI: 3.90 to 12.56, pDiscussionClinician experience and
              efficiency was dramatically improved with use of Abridge across a
              breadth of specialties.ConclusionAn ambient AI documentation
              platform had tremendous impact on improving clinician experience
              within a short time frame. Future studies should utilize validated
              instruments for clinician efficiency and burnout and compare
              impact across AI platforms.",
  month    =  aug,
  year     =  2024
}

@ARTICLE{Elisco2024-bu,
  title     = "Natural Language Processing Unlocks the {DatainCase} Notes",
  author    = "Elisco, M",
  journal   = "Children's Voice Magazine",
  publisher = "go.gale.com",
  volume    =  33,
  number    =  1,
  pages     = "22--24",
  year      =  2024
}

@INPROCEEDINGS{Wallat2024-nw,
  title     = "Temporal blind spots in large language models",
  author    = "Wallat, Jonas and Jatowt, Adam and Anand, Avishek",
  booktitle = "Proceedings of the 17th ACM International Conference on Web
               Search and Data Mining",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  mar,
  year      =  2024
}

@MISC{Unknown2024-uk,
  title     = "Duolingo lays off workers as it leans into {AI} integration",
  publisher = "USA TODAY",
  abstract  = "Language-learning giant Duolingo announced a 10\% reduction in
               contractors as it integrates artificial intelligence into its
               operations.",
  month     =  apr,
  year      =  2024
}

@ARTICLE{Bommasani2021-rl,
  title         = "On the opportunities and risks of foundation models",
  author        = "Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and
                   Altman, Russ and Arora, Simran and von Arx, Sydney and
                   Bernstein, Michael S and Bohg, Jeannette and Bosselut,
                   Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch,
                   Shyamal and Card, Dallas and Castellon, Rodrigo and
                   Chatterji, Niladri and Chen, Annie and Creel, Kathleen and
                   Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and
                   Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and
                   Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and
                   Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and
                   Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha,
                   Neel and Hashimoto, Tatsunori and Henderson, Peter and
                   Hewitt, John and Ho, Daniel E and Hong, Jenny and Hsu, Kyle
                   and Huang, Jing and Icard, Thomas and Jain, Saahil and
                   Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti,
                   Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab,
                   Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay
                   and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal
                   and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent,
                   Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu
                   and Malik, Ali and Manning, Christopher D and Mirchandani,
                   Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj
                   and Narayan, Avanika and Narayanan, Deepak and Newman, Ben
                   and Nie, Allen and Niebles, Juan Carlos and Nilforoshan,
                   Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and
                   Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris
                   and Portelance, Eva and Potts, Christopher and Raghunathan,
                   Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and
                   Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré,
                   Christopher and Sadigh, Dorsa and Sagawa, Shiori and
                   Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and
                   Tamkin, Alex and Taori, Rohan and Thomas, Armin W and Tramèr,
                   Florian and Wang, Rose E and Wang, William and Wu, Bohan and
                   Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga,
                   Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang,
                   Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui
                   and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy",
  journal       = "arXiv [cs.LG]",
  abstract      = "AI is undergoing a paradigm shift with the rise of models
                   (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at
                   scale and are adaptable to a wide range of downstream tasks.
                   We call these models foundation models to underscore their
                   critically central yet incomplete character. This report
                   provides a thorough account of the opportunities and risks of
                   foundation models, ranging from their capabilities (e.g.,
                   language, vision, robotics, reasoning, human interaction) and
                   technical principles(e.g., model architectures, training
                   procedures, data, systems, security, evaluation, theory) to
                   their applications (e.g., law, healthcare, education) and
                   societal impact (e.g., inequity, misuse, economic and
                   environmental impact, legal and ethical considerations).
                   Though foundation models are based on standard deep learning
                   and transfer learning, their scale results in new emergent
                   capabilities,and their effectiveness across so many tasks
                   incentivizes homogenization. Homogenization provides powerful
                   leverage but demands caution, as the defects of the
                   foundation model are inherited by all the adapted models
                   downstream. Despite the impending widespread deployment of
                   foundation models, we currently lack a clear understanding of
                   how they work, when they fail, and what they are even capable
                   of due to their emergent properties. To tackle these
                   questions, we believe much of the critical research on
                   foundation models will require deep interdisciplinary
                   collaboration commensurate with their fundamentally
                   sociotechnical nature.",
  month         =  aug,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Smith-Renner2020-um,
  title     = "No explainability without accountability: An empirical study of
               explanations and feedback in interactive {ML}",
  author    = "Smith-Renner, Alison and Fan, Ron and Birchfield, Melissa and Wu,
               Tongshuang and Boyd-Graber, Jordan and Weld, Daniel S and
               Findlater, Leah",
  booktitle = "Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  apr,
  year      =  2020
}

@ARTICLE{Ehsan2024-zv,
  title     = "Explainability pitfalls: Beyond dark patterns in explainable {AI}",
  author    = "Ehsan, Upol and Riedl, Mark O",
  journal   = "Patterns (N. Y.)",
  publisher = "Elsevier BV",
  volume    =  5,
  number    =  6,
  pages     =  100971,
  abstract  = "To make explainable artificial intelligence (XAI) systems
               trustworthy, understanding harmful effects is important. In this
               paper, we address an important yet unarticulated type of negative
               effect in XAI. We introduce explainability pitfalls (EPs),
               unanticipated negative downstream effects from AI explanations
               manifesting even when there is no intention to manipulate users.
               EPs are different from dark patterns, which are intentionally
               deceptive practices. We articulate the concept of EPs by
               demarcating it from dark patterns and highlighting the challenges
               arising from uncertainties around pitfalls. We situate and
               operationalize the concept using a case study that showcases how,
               despite best intentions, unsuspecting negative effects, such as
               unwarranted trust in numerical explanations, can emerge. We
               propose proactive and preventative strategies to address EPs at
               three interconnected levels: research, design, and
               organizational. We discuss design and societal implications
               around reframing AI adoption, recalibrating stakeholder
               empowerment, and resisting the ``move fast and break things''
               mindset.",
  month     =  jun,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Cooper2021-ft,
  title     = "Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off
               Research",
  author    = "Cooper, A Feder and Abrams, Ellen and Na, N A",
  booktitle = "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and
               Society",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "46--54",
  month     =  jul,
  year      =  2021
}

@ARTICLE{Nahar2021-dg,
  title         = "Collaboration challenges in building {ML}-enabled systems:
                   Communication, documentation, engineering, and process",
  author        = "Nahar, Nadia and Zhou, Shurui and Lewis, Grace and Kästner,
                   Christian",
  journal       = "arXiv [cs.SE]",
  abstract      = "The introduction of machine learning (ML) components in
                   software projects has created the need for software engineers
                   to collaborate with data scientists and other specialists.
                   While collaboration can always be challenging, ML introduces
                   additional challenges with its exploratory model development
                   process, additional skills and knowledge needed, difficulties
                   testing ML systems, need for continuous evolution and
                   monitoring, and non-traditional quality requirements such as
                   fairness and explainability. Through interviews with 45
                   practitioners from 28 organizations, we identified key
                   collaboration challenges that teams face when building and
                   deploying ML systems into production. We report on common
                   collaboration points in the development of production ML
                   systems for requirements, data, and integration, as well as
                   corresponding team patterns and challenges. We find that most
                   of these challenges center around communication,
                   documentation, engineering, and process and collect
                   recommendations to address these challenges.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SE"
}

@ARTICLE{Passi2018-cl,
  title     = "Trust in data science: Collaboration, translation, and
               accountability in corporate data science projects",
  author    = "Passi, Samir and Jackson, Steven J",
  journal   = "Proc. ACM Hum. Comput. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  2,
  number    = "CSCW",
  pages     = "1--28",
  abstract  = "The trustworthiness of data science systems in applied and
               real-world settings emerges from the resolution of specific
               tensions through situated, pragmatic, and ongoing forms of work.
               Drawing on research in CSCW, critical data studies, and history
               and sociology of science, and six months of immersive
               ethnographic fieldwork with a corporate data science team, we
               describe four common tensions in applied data science work:
               (un)equivocal numbers, (counter)intuitive knowledge, (in)credible
               data, and (in)scrutable models. We show how organizational actors
               establish and re-negotiate trust under messy and uncertain
               analytic conditions through practices of skepticism, assessment,
               and credibility. Highlighting the collaborative and heterogeneous
               nature of real-world data science, we show how the management of
               trust in applied corporate data science settings depends not only
               on pre-processing and quantification, but also on negotiation and
               translation. We conclude by discussing the implications of our
               findings for data science research and practice, both within and
               beyond CSCW.",
  month     =  nov,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Kross2021-fb,
  title     = "Orienting, framing, bridging, magic, and counseling: How data
               scientists navigate the outer loop of client collaborations in
               industry and academia",
  author    = "Kross, Sean and Guo, Philip",
  journal   = "Proc. ACM Hum. Comput. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  5,
  number    = "CSCW2",
  pages     = "1--28",
  abstract  = "Data scientists often collaborate with clients to analyze data to
               meet a client's needs. What does the end-to-end workflow of a
               data scientist's collaboration with clients look like throughout
               the lifetime of a project? To investigate this question, we
               interviewed ten data scientists (5 female, 4 male, 1 non-binary)
               in diverse roles across industry and academia. We discovered that
               they work with clients in a six-stage outer-loop workflow, which
               involves 1) laying groundwork by building trust before a project
               begins, 2) orienting to the constraints of the client's
               environment, 3) collaboratively framing the problem, 4) bridging
               the gap between data science and domain expertise, 5) the inner
               loop of technical data analysis work, 6) counseling to help
               clients emotionally cope with analysis results. This novel
               outer-loop workflow contributes to CSCW by expanding the notion
               of what collaboration means in data science beyond the
               widely-known inner-loop technical workflow stages of acquiring,
               cleaning, analyzing, modeling, and visualizing data. We conclude
               by discussing the implications of our findings for data science
               education, parallels to design work, and unmet needs for tool
               development.",
  month     =  oct,
  year      =  2021,
  language  = "en"
}

@ARTICLE{P-PirolliUnknown-zs,
  title  = "The sensemaking process and leverage points for analyst technology
            as identified through cognitive task analysis",
  author = "P Pirolli, S Card"
}

@ARTICLE{Cabrera2022-ng,
  title     = "What did my {AI} learn? How data scientists make sense of model
               behavior",
  author    = "Cabrera, Ángel Alexander and Ribeiro, Marco Tulio and Lee,
               Bongshin and DeLine, Rob and Perer, Adam and Drucker, Steven M",
  journal   = "ACM Trans. Comput. Hum. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  abstract  = "Data scientists require rich mental models of how AI systems
               behave to effectively train, debug, and work with them. Despite
               the prevalence of AI analysis tools, there is no general theory
               describing how people make sense of what their models have
               learned. We frame this process as a form of sensemaking and
               derive a framework describing how data scientists develop mental
               models of AI behavior. To evaluate the framework, we show how
               existing AI analysis tools fit into this sensemaking process and
               use it to design AIFinnity , a system for analyzing
               image-and-text models. Lastly, we explored how data scientists
               use a tool developed with the framework through a think-aloud
               study with 10 data scientists tasked with using AIFinnity to pick
               an image captioning model. We found that AIFinnity ’s sensemaking
               workflow reflected participants’ mental processes and enabled
               them to discover and validate diverse AI behaviors.",
  month     =  jun,
  year      =  2022,
  language  = "en"
}

@INPROCEEDINGS{Kim2016-zl,
  title     = "The emerging role of data scientists on software development
               teams",
  author    = "Kim, Miryung and Zimmermann, Thomas and DeLine, Robert and Begel,
               Andrew",
  booktitle = "Proceedings of the 38th International Conference on Software
               Engineering",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "96--107",
  month     =  may,
  year      =  2016
}

@ARTICLE{Zhang2020-ru,
  title         = "How do data science workers collaborate? Roles, workflows,
                   and tools",
  author        = "Zhang, Amy X and Muller, Michael and Wang, Dakuo",
  journal       = "arXiv [cs.HC]",
  abstract      = "Today, the prominence of data science within organizations
                   has given rise to teams of data science workers collaborating
                   on extracting insights from data, as opposed to individual
                   data scientists working alone. However, we still lack a deep
                   understanding of how data science workers collaborate in
                   practice. In this work, we conducted an online survey with
                   183 participants who work in various aspects of data science.
                   We focused on their reported interactions with each other
                   (e.g., managers with engineers) and with different tools
                   (e.g., Jupyter Notebook). We found that data science teams
                   are extremely collaborative and work with a variety of
                   stakeholders and tools during the six common steps of a data
                   science workflow (e.g., clean data and train model). We also
                   found that the collaborative practices workers employ, such
                   as documentation, vary according to the kinds of tools they
                   use. Based on these findings, we discuss design implications
                   for supporting data science team collaborations and future
                   research directions.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{UnknownUnknown-ax,
  title    = "Thomson Reuters Risk \& Fraud Solutions for financial institutions",
  journal  = "Reuters",
  abstract = "Confirm information across different areas of your business
              through one solution with CLEAR, a public record technology tool",
  language = "en"
}

@ARTICLE{UnknownUnknown-ly,
  title    = "Improve fraud detection and prevention with Fraud Detect",
  journal  = "Reuters",
  abstract = "Fraud Detect from Thomson Reuters uses state-of-the-art software
              to provide in-depth analytics to help you identify potential
              fraud.",
  language = "en"
}

@ARTICLE{UnknownUnknown-su,
  title    = "Risk \& fraud",
  journal  = "Reuters",
  abstract = "Discover our risk and fraud solutions built for corporations and
              government agencies.",
  language = "en"
}

@MISC{Ujlaki2024-ok,
  title        = "How To Watch The Star Wars Movies and Shows in Order",
  author       = "Ujlaki, Olivia",
  booktitle    = "Wishes \& Wayfinding",
  abstract     = "Find the best way to watch the Star Wars movies and series in
                  order, including chronological, release, and machete.",
  month        =  jul,
  year         =  2024,
  howpublished = "\url{https://www.wishesandwayfinding.com/post/how-to-watch-star-wars-movies-in-order}",
  note         = "Accessed: 2024-8-14",
  language     = "en"
}

@ARTICLE{Xu2024-jq,
  title         = "Knowledge conflicts for {LLMs}: A survey",
  author        = "Xu, Rongwu and Qi, Zehan and Guo, Zhijiang and Wang, Cunxiang
                   and Wang, Hongru and Zhang, Yue and Xu, Wei",
  journal       = "arXiv [cs.CL]",
  abstract      = "This survey provides an in-depth analysis of knowledge
                   conflicts for large language models (LLMs), highlighting the
                   complex challenges they encounter when blending contextual
                   and parametric knowledge. Our focus is on three categories of
                   knowledge conflicts: context-memory, inter-context, and
                   intra-memory conflict. These conflicts can significantly
                   impact the trustworthiness and performance of LLMs,
                   especially in real-world applications where noise and
                   misinformation are common. By categorizing these conflicts,
                   exploring the causes, examining the behaviors of LLMs under
                   such conflicts, and reviewing available solutions, this
                   survey aims to shed light on strategies for improving the
                   robustness of LLMs, thereby serving as a valuable resource
                   for advancing research in this evolving area.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{Unknown2023-ej,
  title        = "Introducing Duolingo Max, a learning experience powered by
                  {GPT}-4",
  booktitle    = "Duolingo Blog",
  abstract     = "Duolingo's newest subscription, Duolingo Max, offers a
                  powerful AI-backed learning experience.",
  month        =  mar,
  year         =  2023,
  howpublished = "\url{https://blog.duolingo.com/duolingo-max/}",
  note         = "Accessed: 2024-8-14",
  language     = "en"
}

@MISC{Bicknell2020-it,
  title        = "Learning how to help you learn: Introducing Birdbrain!",
  author       = "Bicknell, Klinton",
  booktitle    = "Duolingo Blog",
  abstract     = "What makes a great teacher, versus simply a good teacher?
                  Well, a good teacher knows the material well so that they can
                  explain it to you. But a great teacher also knows what you
                  know, so that they can teach you exactly what you need to know
                  next. Personalizing your",
  month        =  oct,
  year         =  2020,
  howpublished = "\url{https://blog.duolingo.com/learning-how-to-help-you-learn-introducing-birdbrain/}",
  note         = "Accessed: 2024-8-14",
  language     = "en"
}

@MISC{Henry2023-vm,
  title        = "How Duolingo uses {AI} to create lessons faster",
  author       = "Henry, Parker",
  booktitle    = "Duolingo Blog",
  abstract     = "Artificial intelligence allows Duolingo learning experts to
                  create new lessons faster than ever. Here's how the humans
                  behind the lessons use new technology to improve the app.",
  month        =  jun,
  year         =  2023,
  howpublished = "\url{https://blog.duolingo.com/large-language-model-duolingo-lessons/}",
  note         = "Accessed: 2024-8-14",
  language     = "en"
}

@MISC{VaithianathanUnknown-nl,
  title        = "Implementing the Hello Baby Prevention Program in Allegheny
                  County",
  author       = "Vaithianathan, Rhema and Putnam-Hornstein, Emily and
                  Benavides-Prad, Diana",
  howpublished = "\url{https://www.alleghenycountyanalytics.us/wp-content/uploads/2020/12/Hello-Baby-Methodology-v6.pdf\#page=2.29}"
}

@MISC{PonderaUnknown-db,
  title  = "\textit{FraudCaster Master Design Document for Department of Human
            Services District of Columbia}",
  author = "Pondera, Thomson Reuters"
}

@INPROCEEDINGS{Lee2024-rc,
  title     = "Deepfakes, phrenology, surveillance, and more! A taxonomy of {AI}
               privacy risks",
  author    = "Lee, Hao-Ping (hank) and Yang, Yu-Ju and Von Davier, Thomas
               Serban and Forlizzi, Jodi and Das, Sauvik",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  79,
  pages     = "1--19",
  month     =  may,
  year      =  2024
}

@MISC{Narayanan2024-dt,
  title        = "{AI} safety is not a model property",
  author       = "Narayanan, Arvind and Kapoor, Sayash",
  booktitle    = "AI Snake Oil",
  abstract     = "Trying to make an AI model that can’t be misused is like
                  trying to make a computer that can’t be used for bad things",
  month        =  mar,
  year         =  2024,
  howpublished = "\url{https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property}",
  note         = "Accessed: 2024-8-13",
  language     = "en"
}

@MISC{Rao2022-ad,
  title        = "The Dutch Tax Authority Was Felled by {AI—What} Comes Next?",
  author       = "Rao, Rahul",
  booktitle    = "IEEE Spectrum",
  abstract     = "European regulation hopes to rein in ill-behaving algorithms",
  month        =  may,
  year         =  2022,
  howpublished = "\url{https://spectrum.ieee.org/artificial-intelligence-in-government}",
  note         = "Accessed: 2024-8-12",
  language     = "en"
}

@MISC{Heikkila2022-dx,
  title        = "Dutch scandal serves as a warning for Europe over risks of
                  using algorithms",
  author       = "Heikkilä, Melissa",
  booktitle    = "POLITICO",
  abstract     = "The Dutch tax authority ruined thousands of lives after using
                  an algorithm to spot suspected benefits fraud — and critics
                  say there is little stopping it from happening again.",
  month        =  mar,
  year         =  2022,
  howpublished = "\url{https://www.politico.eu/article/dutch-scandal-serves-as-a-warning-for-europe-over-risks-of-using-algorithms/}",
  note         = "Accessed: 2024-8-12",
  language     = "en"
}

@ARTICLE{UnknownUnknown-hy,
  title = "Complaint and Request for Investigation, Injunction, and Other Relief
           Submitted by The Electronic Privacy Information Center ({EPIC})"
}

@MISC{Quinlan2024-iy,
  title        = "Nonprofit behind {FTC} complaint about automated
                  fraud-detection software hopes for more responsible {AI} use",
  author       = "Quinlan, Keely",
  booktitle    = "StateScoop",
  abstract     = "Grant Fergusson, an author of a recent Federal Trade
                  Commission complaint against Thomson Reuters, urged
                  governments to be careful about which information they provide
                  to powerful AI algorithms.",
  month        =  jan,
  year         =  2024,
  howpublished = "\url{http://statescoop.com/nonprofit-ftc-complaint-automated-benefits-responsible-ai/}",
  note         = "Accessed: 2024-8-12",
  language     = "en"
}

@ARTICLE{UnknownUnknown-ji,
  title    = "What does Thomson Reuters do?",
  journal  = "Reuters",
  abstract = "Thomson Reuters provides trusted data and information to
              professionals across 4 different industries: financial \& risk;
              legal; tax and accounting; and media. Here are some examples of
              how we impact each of these industries.",
  language = "en"
}

@MISC{Christian2023-ap,
  title        = "Magazine Publishes Serious Errors in First {AI}-Generated
                  Health Article",
  author       = "Christian, Jon",
  booktitle    = "Futurism",
  abstract     = "The owners of Sports Illustrated and Men’s Journal promised to
                  be virtuous with AI. Then they bungled their very first AI
                  story.",
  month        =  feb,
  year         =  2023,
  howpublished = "\url{https://futurism.com/neoscope/magazine-mens-journal-errors-ai-health-article}",
  note         = "Accessed: 2024-8-11",
  language     = "en"
}

@MISC{Sato2023-bm,
  title        = "{CNET} is overhauling its {AI} policy and updating past
                  stories",
  author       = "Sato, Mia",
  booktitle    = "The Verge",
  abstract     = "CNET is clarifying its policy around how AI tools are used in
                  the newsroom. The policy comes months after the outlet came
                  under fire for quietly using AI tools to produce stories.",
  month        =  jun,
  year         =  2023,
  howpublished = "\url{https://www.theverge.com/2023/6/6/23750761/cnet-ai-generated-stories-policy-update}",
  note         = "Accessed: 2024-8-11",
  language     = "en"
}

@MISC{Ho2022-jj,
  title        = "An algorithm that screens for child neglect raises concerns",
  author       = "Ho, Sally and Burke, Garance",
  booktitle    = "AP News",
  abstract     = "Inside a cavernous stone fortress in downtown Pittsburgh,
                  attorney Robin Frank defends parents at one of their lowest
                  points – when they risk losing their children.",
  month        =  apr,
  year         =  2022,
  howpublished = "\url{https://apnews.com/article/child-welfare-algorithm-investigation-9497ee937e0053ad4144a86c68241ef1}",
  note         = "Accessed: 2024-8-8",
  language     = "en"
}

@MISC{Ho2023-wy,
  title        = "Child welfare algorithm faces Justice Department scrutiny",
  author       = "Ho, Sally and Burke, Garance",
  booktitle    = "AP News",
  abstract     = "PITTSBURGH (AP) — The Justice Department has been scrutinizing
                  a controversial artificial intelligence tool used by a
                  Pittsburgh-area child protective services agency following
                  concerns that the tool could lead to discrimination against
                  families with disabilities, The Associated Press has learned.",
  month        =  feb,
  year         =  2023,
  howpublished = "\url{https://apnews.com/article/justice-scrutinizes-pittsburgh-child-welfare-ai-tool-4f61f45bfc3245fd2556e886c2da988b}",
  note         = "Accessed: 2024-8-8",
  language     = "en"
}

@ARTICLE{VaithianathanUnknown-ci,
  title  = "Developing predictive models to support child maltreatment hotline
            screening decisions: Allegheny County methodology and implementation",
  author = "Vaithianathan, {R and Putnam-Hornstein, E and Jiang, N and Maloney},
            T"
}

@BOOK{Glantz2014-zr,
  title     = "Multi-asset risk modeling: Techniques for a global economy in an
               electronic and algorithmic trading era",
  author    = "Glantz, Morton and Kissell, Robert",
  publisher = "Academic Press",
  abstract  = "Multi-Asset Risk Modeling describes, in a single volume, the
               latest and most advanced risk modeling techniques for equities,
               debt, fixed income, futures and derivatives, commodities, and
               foreign exchange, as well as advanced algorithmic and electronic
               risk management. Beginning with the fundamentals of risk
               mathematics and quantitative risk analysis, the book moves on to
               discuss the laws in standard models that contributed to the 2008
               financial crisis and talks about current and future banking
               regulation. Importantly, it also explores algorithmic trading,
               which currently receives sparse attention in the literature. By
               giving coherent recommendations about which statistical models to
               use for which asset class, this book makes a real contribution to
               the sciences of portfolio management and risk management. Covers
               all asset classes Provides mathematical theoretical explanations
               of risk as well as practical examples with empirical data
               Includes sections on equity risk modeling, futures and
               derivatives, credit markets, foreign exchange, and commodities",
  month     =  may,
  year      =  2014
}

@ARTICLE{Haltaufderheide2024-ju,
  title     = "The ethics of {ChatGPT} in medicine and healthcare: a systematic
               review on Large Language Models ({LLMs})",
  author    = "Haltaufderheide, Joschka and Ranisch, Robert",
  journal   = "NPJ Digit. Med.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  7,
  number    =  1,
  pages     =  183,
  abstract  = "With the introduction of ChatGPT, Large Language Models (LLMs)
               have received enormous attention in healthcare. Despite potential
               benefits, researchers have underscored various ethical
               implications. While individual instances have garnered attention,
               a systematic and comprehensive overview of practical applications
               currently researched and ethical issues connected to them is
               lacking. Against this background, this work maps the ethical
               landscape surrounding the current deployment of LLMs in medicine
               and healthcare through a systematic review. Electronic databases
               and preprint servers were queried using a comprehensive search
               strategy which generated 796 records. Studies were screened and
               extracted following a modified rapid review approach.
               Methodological quality was assessed using a hybrid approach. For
               53 records, a meta-aggregative synthesis was performed. Four
               general fields of applications emerged showcasing a dynamic
               exploration phase. Advantages of using LLMs are attributed to
               their capacity in data analysis, information provisioning,
               support in decision-making or mitigating information loss and
               enhancing information accessibility. However, our study also
               identifies recurrent ethical concerns connected to fairness,
               bias, non-maleficence, transparency, and privacy. A distinctive
               concern is the tendency to produce harmful or convincing but
               inaccurate content. Calls for ethical guidance and human
               oversight are recurrent. We suggest that the ethical guidance
               debate should be reframed to focus on defining what constitutes
               acceptable human oversight across the spectrum of applications.
               This involves considering the diversity of settings, varying
               potentials for harm, and different acceptable thresholds for
               performance and certainty in healthcare. Additionally, critical
               inquiry is needed to evaluate the necessity and justification of
               LLMs' current experimental use.",
  month     =  jul,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Holstein2019-ez,
  title     = "Improving fairness in machine learning systems: What do industry
               practitioners need?",
  author    = "Holstein, Kenneth and Wortman Vaughan, Jennifer and Daumé, III,
               Hal and Dudik, Miro and Wallach, Hanna",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  may,
  year      =  2019
}

@ARTICLE{Verma2023-dk,
  title     = "How an {AI}-written Star Wars story created chaos at Gizmodo",
  author    = "Verma, Pranshu",
  journal   = "The Washington Post",
  publisher = "The Washington Post",
  abstract  = "A Gizmodo story on Star Wars, generated by artificial
               intelligence, was riddled with errors. The irony that the problem
               happened at a tech publication was undeniable.",
  month     =  jul,
  year      =  2023
}

@MISC{UnknownUnknown-fh,
  title        = "Incident 574: {AI}-Generated Articles at {G}/{O} Media
                  Allegedly Diminishes Reputation of Human Staff",
  abstract     = "G/O Media began publishing AI-generated articles, against
                  staff advice, that contained errors and quality issues. The
                  first such article, a list of Star Wars movies, failed to
                  maintain chronological order, causing internal concerns over
                  journalistic credibility and ethics. Staff expressed that the
                  AI was ``actively hurting our reputations and credibility''
                  and accused management of ``wasting everyone's time.''",
  howpublished = "\url{https://incidentdatabase.ai/cite/574/}",
  note         = "Accessed: 2024-8-2",
  language     = "en"
}

@ARTICLE{Elkassem2023-yv,
  title     = "Potential use cases for {ChatGPT} in radiology reporting",
  author    = "Elkassem, Asser Abou and Smith, Andrew D",
  journal   = "AJR Am. J. Roentgenol.",
  publisher = "American Roentgen Ray Society",
  volume    =  221,
  number    =  3,
  pages     = "373--376",
  abstract  = "Large language models (LLMs) such as ChatGPT are advanced
               artificial intelligence models that are designed to process and
               understand human language. LLMs have the potential to improve
               radiology reporting and patient engagement by automating
               generation of the clinical history and impression of a radiology
               report, creating layperson reports, and providing patients with
               pertinent questions and answers about findings in radiology
               reports. However, LLMs are error prone, and human oversight is
               needed to reduce the risk of patient harm.",
  month     =  sep,
  year      =  2023,
  keywords  = "ChatGPT; artificial intelligence; health care; large language
               models; radiology reports",
  language  = "en"
}

@ARTICLE{Akinci-D-Antonoli2024-oy,
  title     = "Large language models in radiology: fundamentals, applications,
               ethical considerations, risks, and future directions",
  author    = "Akinci D'Antonoli, Tugba and Stanzione, Arnaldo and Bluethgen,
               Christian and Vernuccio, Federica and Ugga, Lorenzo and Klontzas,
               Michail E and Cuocolo, Renato and Cannella, Roberto and Koçak,
               Burak",
  journal   = "Diagn. Interv. Radiol.",
  publisher = "Galenos Yayinevi",
  volume    =  30,
  number    =  2,
  pages     = "80--90",
  abstract  = "With the advent of large language models (LLMs), the artificial
               intelligence revolution in medicine and radiology is now more
               tangible than ever. Every day, an increasingly large number of
               articles are published that utilize LLMs in radiology. To adopt
               and safely implement this new technology in the field,
               radiologists should be familiar with its key concepts, understand
               at least the technical basics, and be aware of the potential
               risks and ethical considerations that come with it. In this
               review article, the authors provide an overview of the LLMs that
               might be relevant to the radiology community and include a brief
               discussion of their short history, technical basics, ChatGPT,
               prompt engineering, potential applications in medicine and
               radiology, advantages, disadvantages and risks, ethical and
               regulatory considerations, and future directions.",
  month     =  mar,
  year      =  2024,
  keywords  = "ChatGPT; Large language models; artificial intelligence; deep
               learning; natural language processing",
  language  = "en"
}

@INCOLLECTION{Stanley2022-ad,
  title     = "Disproportionate subgroup impacts and other challenges of
               fairness in artificial intelligence for medical image analysis",
  author    = "Stanley, Emma A M and Wilms, Matthias and Forkert, Nils D",
  booktitle = "Ethical and Philosophical Issues in Medical Imaging, Multimodal
               Learning and Fusion Across Scales for Clinical Decision Support,
               and Topological Data Analysis for Biomedical Imaging",
  publisher = "Springer Nature Switzerland",
  address   = "Cham",
  pages     = "14--25",
  series    = "Lecture notes in computer science",
  year      =  2022
}

@ARTICLE{Kaushal2020-lp,
  title     = "Geographic distribution of {US} cohorts used to train deep
               learning algorithms",
  author    = "Kaushal, Amit and Altman, Russ and Langlotz, Curt",
  journal   = "JAMA",
  publisher = "American Medical Association",
  volume    =  324,
  number    =  12,
  pages     = "1212--1213",
  abstract  = "This study describes the US geographic distribution of patient
               cohorts used to train deep learning algorithms in published
               radiology, ophthalmology, dermatology, pathology,
               gastroenterology, and cardiology machine learning articles
               published in 2015-2019.",
  month     =  sep,
  year      =  2020,
  keywords  = "deep learning; machine learning; cardiology; dermatology;
               gastroenterology; ophthalmology; radiology specialty",
  language  = "en"
}

@ARTICLE{McGregor2022-fy,
  title         = "Indexing {AI} risks with incidents, issues, and variants",
  author        = "McGregor, Sean and Paeth, Kevin and Lam, Khoa",
  journal       = "arXiv [cs.CY]",
  abstract      = "Two years after publicly launching the AI Incident Database
                   (AIID) as a collection of harms or near harms produced by AI
                   in the world, a backlog of ``issues'' that do not meet its
                   incident ingestion criteria have accumulated in its review
                   queue. Despite not passing the database's current criteria
                   for incidents, these issues advance human understanding of
                   where AI presents the potential for harm. Similar to
                   databases in aviation and computer security, the AIID
                   proposes to adopt a two-tiered system for indexing AI
                   incidents (i.e., a harm or near harm event) and issues (i.e.,
                   a risk of a harm event). Further, as some machine
                   learning-based systems will sometimes produce a large number
                   of incidents, the notion of an incident ``variant'' is
                   introduced. These proposed changes mark the transition of the
                   AIID to a new version in response to lessons learned from
                   editing 2,000+ incident reports and additional reports that
                   fall under the new category of ``issue.''",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Kellogg2020-rs,
  title     = "Algorithms at Work: The New Contested Terrain of Control",
  author    = "Kellogg, Katherine C and Valentine, Melissa A and Christin,
               Angéle",
  journal   = "ANNALS",
  publisher = "journals.aom.org",
  volume    =  14,
  number    =  1,
  pages     = "366--410",
  month     =  jan,
  year      =  2020
}

@ARTICLE{Elish2020-ch,
  title   = "A Study of Integrating {AI} in Clinical Care",
  author  = "Elish, Madeleine Clare and Watkins, Elizabeth",
  journal = "Data and Society",
  year    =  2020
}

@article{saxena2024algorithmic,
  title={Algorithmic harms in child welfare: Uncertainties in practice, organization, and street-level decision-making},
  author={Saxena, Devansh and Guha, Shion},
  journal={ACM Journal on Responsible Computing},
  volume={1},
  number={1},
  pages={1--32},
  year={2024},
  publisher={ACM New York, NY}
}

@ARTICLE{Wong2021-zv,
  title     = "External validation of a widely implemented proprietary sepsis
               prediction model in hospitalized patients",
  author    = "Wong, Andrew and Otles, Erkin and Donnelly, John P and Krumm,
               Andrew and McCullough, Jeffrey and DeTroyer-Cooley, Olivia and
               Pestrue, Justin and Phillips, Marie and Konye, Judy and Penoza,
               Carleen and Ghous, Muhammad and Singh, Karandeep",
  journal   = "JAMA Intern. Med.",
  publisher = "American Medical Association (AMA)",
  volume    =  181,
  number    =  8,
  pages     = "1065--1070",
  abstract  = "Importance: The Epic Sepsis Model (ESM), a proprietary sepsis
               prediction model, is implemented at hundreds of US hospitals. The
               ESM's ability to identify patients with sepsis has not been
               adequately evaluated despite widespread use. Objective: To
               externally validate the ESM in the prediction of sepsis and
               evaluate its potential clinical value compared with usual care.
               Design, Setting, and Participants: This retrospective cohort
               study was conducted among 27 697 patients aged 18 years or older
               admitted to Michigan Medicine, the academic health system of the
               University of Michigan, Ann Arbor, with 38 455 hospitalizations
               between December 6, 2018, and October 20, 2019. Exposure: The ESM
               score, calculated every 15 minutes. Main Outcomes and Measures:
               Sepsis, as defined by a composite of (1) the Centers for Disease
               Control and Prevention surveillance criteria and (2)
               International Statistical Classification of Diseases and Related
               Health Problems, Tenth Revision diagnostic codes accompanied by 2
               systemic inflammatory response syndrome criteria and 1 organ
               dysfunction criterion within 6 hours of one another. Model
               discrimination was assessed using the area under the receiver
               operating characteristic curve at the hospitalization level and
               with prediction horizons of 4, 8, 12, and 24 hours. Model
               calibration was evaluated with calibration plots. The potential
               clinical benefit associated with the ESM was assessed by
               evaluating the added benefit of the ESM score compared with
               contemporary clinical practice (based on timely administration of
               antibiotics). Alert fatigue was evaluated by comparing the
               clinical value of different alerting strategies. Results: We
               identified 27 697 patients who had 38 455 hospitalizations (21
               904 women [57\%]; median age, 56 years [interquartile range,
               35-69 years]) meeting inclusion criteria, of whom sepsis occurred
               in 2552 (7\%). The ESM had a hospitalization-level area under the
               receiver operating characteristic curve of 0.63 (95\% CI,
               0.62-0.64). The ESM identified 183 of 2552 patients with sepsis
               (7\%) who did not receive timely administration of antibiotics,
               highlighting the low sensitivity of the ESM in comparison with
               contemporary clinical practice. The ESM also did not identify
               1709 patients with sepsis (67\%) despite generating alerts for an
               ESM score of 6 or higher for 6971 of all 38 455 hospitalized
               patients (18\%), thus creating a large burden of alert fatigue.
               Conclusions and Relevance: This external validation cohort study
               suggests that the ESM has poor discrimination and calibration in
               predicting the onset of sepsis. The widespread adoption of the
               ESM despite its poor performance raises fundamental concerns
               about sepsis management on a national level.",
  month     =  aug,
  year      =  2021,
  language  = "en"
}


@ARTICLE{Addy2024-lv,
  title     = "Machine learning in financial markets: A critical review of
               algorithmic trading and risk management",
  author    = "Addy, Wilhelmina Afua and Ajayi-Nifise, Adeola Olusola and Bello,
               Binaebi Gloria and Tula, Sunday Tubokirifuruar and Odeyemi,
               Olubusola and Falaiye, Titilola",
  journal   = "Int. J. Sci. Res. Arch.",
  publisher = "GSC Online Press",
  volume    =  11,
  number    =  1,
  pages     = "1853--1862",
  abstract  = "The integration of machine learning (ML) techniques in financial
               markets has revolutionized traditional trading and risk
               management strategies, offering unprecedented opportunities and
               challenges. This paper provides a comprehensive and critical
               review of the application of ML in algorithmic trading and risk
               management within the realm of financial markets. The review
               begins by exploring the evolution of algorithmic trading,
               highlighting the paradigm shift from traditional rule-based
               strategies to ML-driven approaches. Various ML algorithms,
               including neural networks, decision trees, and ensemble methods,
               are examined in the context of their application to predictive
               modeling, pattern recognition, and signal generation for trading
               purposes. The paper also delves into the challenges and
               limitations associated with the adoption of ML in financial
               markets. Issues such as overfitting, data bias, and model
               interpretability are discussed, emphasizing the importance of
               addressing these concerns to ensure robust and reliable trading
               systems. Furthermore, ethical considerations and potential
               regulatory implications of ML-driven trading strategies are
               considered in the context of market fairness and stability. In
               the realm of risk management, the review scrutinizes the role of
               ML in assessing and mitigating financial risks. The paper
               evaluates the effectiveness of ML models in identifying market
               trends, measuring portfolio risk, and optimizing asset
               allocation. Additionally, it examines the potential impact of ML
               on systemic risk and the need for adaptive risk management
               frameworks in dynamic market conditions. The synthesis of
               findings underscores the transformative impact of ML on financial
               markets, showcasing its potential to enhance trading strategies
               and risk management practices. However, the review also
               highlights the importance of addressing inherent challenges and
               ethical considerations to ensure the responsible and sustainable
               integration of ML in the financial domain. This critical review
               provides valuable insights into the current state of machine
               learning in financial markets, offering a foundation for future
               research directions and the development of best practices in
               algorithmic trading and risk management.",
  month     =  feb,
  year      =  2024
}

@ARTICLE{Zhou2023-cf,
  title     = "Generative {AI}, human creativity, and art",
  author    = "Zhou, Eric and Lee, Dokyun",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  abstract  = "Recent artificial intelligence (AI) tools have demonstrated their
               ability to produce outputs traditionally considered creative. One
               such system is text-to-image",
  month     =  oct,
  year      =  2023,
  keywords  = "Generative AI, Human-AI Collaboration, Creative Workflow, Impact
               of AI Adoption, Art",
  language  = "en"
}

@INPROCEEDINGS{Wadinambiarachchi2024-en,
  title     = "The Effects of Generative {AI} on Design Fixation and Divergent
               Thinking",
  author    = "Wadinambiarachchi, Samangi and Kelly, Ryan M and Pareek, Saumya
               and Zhou, Qiushi and Velloso, Eduardo",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--18",
  month     =  may,
  year      =  2024
}

@ARTICLE{Liu2024-oa,
  title     = "Smart ``Error''! Exploring Imperfect {AI} to Support Creative
               Ideation",
  author    = "Liu, Fang and Lv, Junyan and Cui, Shenglan and Luan, Zhilong and
               Wu, Kui and Zhou, Tongqing",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "dl.acm.org",
  volume    =  8,
  number    = "CSCW1",
  pages     = "1--28",
  month     =  apr,
  year      =  2024
}

@ARTICLE{Faheem2023-ky,
  title     = "Artificial intelligence failure at {IBM} 'Watson for oncology'",
  author    = "Faheem, Hadiyapreview Author Details; Dutta",
  journal   = "IUP Journal of Knowledge Management",
  publisher = "search.proquest.com",
  volume    =  21,
  number    =  3,
  pages     = "47--75",
  month     =  jul,
  year      =  2023
}

@BOOK{Buolamwini2023-xl,
  title     = "Unmasking {AI}: My mission to protect what is human in a world of
               machines",
  author    = "Buolamwini, Joy",
  publisher = "Random House",
  address   = "New York, NY",
  abstract  = "NATIONAL BESTSELLER • “The conscience of the AI revolution”
               (Fortune) explains how we’ve arrived at an era of AI harms and
               oppression, and what we can do to avoid its pitfalls.“Dr. Joy
               Buolamwini has been an essential figure in bringing
               irresponsible, profit-hungry tech giants to their knees. If
               you’re going to read only one book about AI, this should be
               it.”—Darren Walker, president of the Ford Foundation A LOS
               ANGELES TIMES BEST BOOK OF THE YEAR • Shortlisted for the Inc.
               Non-Obvious Book AwardTo most of us, it seems like recent
               developments in artificial intelligence emerged out of nowhere to
               pose unprecedented threats to humankind. But to Dr. Joy
               Buolamwini, who has been at the forefront of AI research, this
               moment has been a long time in the making.After tinkering with
               robotics as a high school student in Memphis and then developing
               mobile apps in Zambia as a Fulbright fellow, Buolamwini followed
               her lifelong passion for computer science, engineering, and art
               to MIT in 2015. As a graduate student at the “Future Factory,”
               she did groundbreaking research that exposed widespread racial
               and gender bias in AI services from tech giants across the
               world.Unmasking AI goes beyond the headlines about existential
               risks produced by Big Tech. It is the remarkable story of how
               Buolamwini uncovered what she calls “the coded gaze”—the evidence
               of encoded discrimination and exclusion in tech products—and how
               she galvanized the movement to prevent AI harms by founding the
               Algorithmic Justice League. Applying an intersectional lens to
               both the tech industry and the research sector, she shows how
               racism, sexism, colorism, and ableism can overlap and render
               broad swaths of humanity “excoded” and therefore vulnerable in a
               world rapidly adopting AI tools. Computers, she reminds us, are
               reflections of both the aspirations and the limitations of the
               people who create them.Encouraging experts and non-experts alike
               to join this fight, Buolamwini writes, “The rising frontier for
               civil rights will require algorithmic justice. AI should be for
               the people and by the people, not just the privileged few.”",
  month     =  oct,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Schuetz2021-xg,
  title     = "Fly in the face of bias: Algorithmic bias in law enforcement's
               facial recognition technology and the need for an adaptive legal
               framework",
  author    = "Schuetz, P",
  journal   = "Law \& Ineq.",
  publisher = "HeinOnline",
  volume    =  39,
  pages     =  8,
  year      =  2021
}

@ARTICLE{Liu2022-cp,
  title         = "Lost in translation: Reimagining the machine learning life
                   cycle in education",
  author        = "Liu, Lydia T and Wang, Serena and Britton, Tolani and Abebe,
                   Rediet",
  journal       = "arXiv [cs.AI]",
  abstract      = "Machine learning (ML) techniques are increasingly prevalent
                   in education, from their use in predicting student dropout,
                   to assisting in university admissions, and facilitating the
                   rise of MOOCs. Given the rapid growth of these novel uses,
                   there is a pressing need to investigate how ML techniques
                   support long-standing education principles and goals. In this
                   work, we shed light on this complex landscape drawing on
                   qualitative insights from interviews with education experts.
                   These interviews comprise in-depth evaluations of ML for
                   education (ML4Ed) papers published in preeminent applied ML
                   conferences over the past decade. Our central research goal
                   is to critically examine how the stated or implied education
                   and societal objectives of these papers are aligned with the
                   ML problems they tackle. That is, to what extent does the
                   technical problem formulation, objectives, approach, and
                   interpretation of results align with the education problem at
                   hand. We find that a cross-disciplinary gap exists and is
                   particularly salient in two parts of the ML life cycle: the
                   formulation of an ML problem from education goals and the
                   translation of predictions to interventions. We use these
                   insights to propose an extended ML life cycle, which may also
                   apply to the use of ML in other domains. Our work joins a
                   growing number of meta-analytical studies across education
                   and ML research, as well as critical analyses of the societal
                   impact of ML. Specifically, it fills a gap between the
                   prevailing technical understanding of machine learning and
                   the perspective of education researchers working with
                   students and in policy.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Eubanks2018-dk,
  title     = "Automating Inequality: How high-tech tools profile, police, and
               punish the poor",
  author    = "Eubanks, Virginia E",
  publisher = "books.google.com",
  abstract  = "Naomi Klein: ``This book is downright scary.''Ethan Zuckerman,
               MIT: ``Should be required reading.''Dorothy Roberts, author of
               Killing the Black Body: ``A must-read.''Astra Taylor, author of
               The People's Platform: ``The single most important book about
               technology you will read this year.''Cory Doctorow:
               ``Indispensable.''A powerful investigative look at data-based
               discriminationand how technology affects civil and human rights
               and economic equity The State of Indiana denies one million
               applications for healthcare, foodstamps and cash benefits in
               three yearsbecause a new computer system interprets any mistake
               as failure to cooperate. In Los Angeles, an algorithm calculates
               the comparative vulnerability of tens of thousands of homeless
               people in order to prioritize them for an inadequate pool of
               housing resources. In Pittsburgh, a child welfare agency uses a
               statistical model to try to predict which children might be
               future victims of abuse or neglect. Since the dawn of the digital
               age, decision-making in finance, employment, politics, health and
               human services has undergone revolutionary change. Today,
               automated systemsrather than humanscontrol which neighborhoods
               get policed, which families attain needed resources, and who is
               investigated for fraud. While we all live under this new regime
               of data, the most invasive and punitive systems are aimed at the
               poor. In Automating Inequality, Virginia Eubanks systematically
               investigates the impacts of data mining, policy algorithms, and
               predictive risk models on poor and working-class people in
               America. The book is full of heart-wrenching and eye-opening
               stories, from a woman in Indiana whose benefits are literally cut
               off as she lays dying to a family in Pennsylvania in daily fear
               of losing their daughter because they fit a certain statistical
               profile. The U.S. has always used its most cutting-edge science
               and technology to contain, investigate, discipline and punish the
               destitute. Like the county poorhouse and scientific charity
               before them, digital tracking and automated decision-making hide
               poverty from the middle-class public and give the nation the
               ethical distance it needs to make inhumane choices: which
               families get food and which starve, who has housing and who
               remains homeless, and which families are broken up by the state.
               In the process, they weaken democracy and betray our most
               cherished national values. This deeply researched and passionate
               book could not be more timely.",
  month     =  jan,
  year      =  2018
}


@INPROCEEDINGS{Shelby2023-ff,
  title     = "Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy
               for Harm Reduction",
  author    = "Shelby, Renee and Rismani, Shalaleh and Henne, Kathryn and Moon,
               Ajung and Rostamzadeh, Negar and Nicholas, Paul and Yilla-Akbari,
               N'mah and Gallegos, Jess and Smart, Andrew and Garcia, Emilio and
               Virk, Gurleen",
  booktitle = "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and
               Society",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "723--741",
  month     =  aug,
  year      =  2023
}

@ARTICLE{Logan2016-of,
  title    = "Policing Criminal Justice Data",
  author   = "Logan, Wayne A and Ferguson, A and Ferguson, A",
  journal  = "Minnesota Law Review",
  volume   =  101,
  number   =  2,
  pages    =  541,
  abstract = "This article addresses a matter of fundamental importance to the
              criminal justice system: the presence of erroneous information in
              government databases and the limited government accountability and
              legal remedies for the harm that it causes individuals. While a
              substantial literature exists on the liberty and privacy perils of
              large multi-source data assemblage, often termed ``big data,''
              this article addresses the risks associated with the collection,
              generation and use of ``small data'' (i.e., individual-level,
              discrete data points). Because small data provides the building
              blocks for all data-driven systems, enhancing its quality will
              have a significant positive effect on the criminal justice system
              as a whole. The article examines the many contexts in which
              criminal justice data errors arise and offers institutional and
              legislative solutions designed both to lessen their occurrence and
              afford relief to those suffering the significant harms they cause.",
  month    =  apr,
  year     =  2016,
  language = "en"
}

@ARTICLE{Botsis2010-ac,
  title     = "Secondary use of {EHR}: Data quality issues and informatics
               opportunities",
  author    = "Botsis, Taxiarchis and Hartvigsen, Gunnar and Chen, Fei and Weng,
               Chunhua",
  journal   = "Summit On Translat. Bioinforma.",
  publisher = "American Medical Informatics Association",
  volume    =  2010,
  pages     = "1--5",
  abstract  = "Given the large-scale deployment of Electronic Health Records
               (EHR), secondary use of EHR data will be increasingly needed in
               all kinds of health services or clinical research. This paper
               reports some data quality issues we encountered in a survival
               analysis of pancreatic cancer patients. Using the clinical data
               warehouse at Columbia University Medical Center in the City of
               New York, we mined EHR data elements collected between 1999 and
               2009 for a cohort of pancreatic cancer patients. Of the 3068
               patients who had ICD-9-CM diagnoses for pancreatic cancer, only
               1589 had corresponding disease documentation in pathology
               reports. Incompleteness was the leading data quality issue; many
               study variables had missing values to various degrees. Inaccuracy
               and inconsistency were the next common problems. In this paper,
               we present the manifestations of these data quality issues and
               discuss some strategies for using emerging informatics
               technologies to solve these problems.",
  month     =  mar,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Allard2018-hu,
  title     = "State agencies’ use of administrative data for improved practice:
               Needs, challenges, and opportunities: State agencies’ use of
               administrative data for improved practice: Needs, challenges, and
               opportunities",
  author    = "Allard, Scott W and Wiegand, Emily R and Schlecht, Colleen and
               Datta, A Rupa and Goerge, Robert M and Weigensberg, Elizabeth",
  journal   = "Public Adm. Rev.",
  publisher = "Wiley",
  volume    =  78,
  number    =  2,
  pages     = "240--250",
  abstract  = "Growing interest in the use of administrative data to answer
               questions around program implementation and effectiveness has led
               to greater discussion of how government agencies can develop the
               necessary internal data infrastructure, analytic capacity, and
               office culture. However, there is a need for more systematic
               research into how states find different pathways and strategies
               to build administrative data capacity. Drawing on interviews with
               almost 100 human service agency staff and their data partners,
               the authors examine the realities of administrative data use.
               They summarize the experiences of data users in order to address
               two main challenges: limited analytic capacity and challenges to
               linking or sharing data resources. The article concludes by
               examining a range of approaches that government agencies take to
               improve data quality and capacity to analyze that data.",
  month     =  mar,
  year      =  2018,
  language  = "en"
}

@INPROCEEDINGS{Kawakami2022-ez,
  title     = "“why do {I} care what’s similar?” probing challenges in
               {AI}-assisted child welfare decision-making through worker-{AI}
               interface design concepts",
  author    = "Kawakami, Anna and Sivaraman, Venkatesh and Stapleton, Logan and
               Cheng, Hao-Fei and Perer, Adam and Wu, Zhiwei Steven and Zhu,
               Haiyi and Holstein, Kenneth",
  booktitle = "Designing Interactive Systems Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2022
}

@INPROCEEDINGS{Kawakami2022-tx,
  title     = "Improving Human-{AI} Partnerships in Child Welfare: Understanding
               Worker Practices, Challenges, and Desires for Algorithmic
               Decision Support",
  author    = "Kawakami, Anna and Sivaraman, Venkatesh and Cheng, Hao-Fei and
               Stapleton, Logan and Cheng, Yanghuidi and Qing, Diana and Perer,
               Adam and Wu, Zhiwei Steven and Zhu, Haiyi and Holstein, Kenneth",
  booktitle = "CHI Conference on Human Factors in Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--18",
  month     =  apr,
  year      =  2022
}

@INPROCEEDINGS{Cheng2022-ya,
  title     = "How child welfare workers reduce racial disparities in
               algorithmic decisions",
  author    = "Cheng, Hao-Fei and Stapleton, Logan and Kawakami, Anna and
               Sivaraman, Venkatesh and Cheng, Yanghuidi and Qing, Diana and
               Perer, Adam and Holstein, Kenneth and Wu, Zhiwei Steven and Zhu,
               Haiyi",
  booktitle = "CHI Conference on Human Factors in Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  apr,
  year      =  2022
}

@ARTICLE{Kleinberg2016-fd,
  title         = "Inherent trade-offs in the fair determination of risk scores",
  author        = "Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent discussion in the public sphere about algorithmic
                   classification has involved tension between competing notions
                   of what it means for a probabilistic classification to be
                   fair to different groups. We formalize three fairness
                   conditions that lie at the heart of these debates, and we
                   prove that except in highly constrained special cases, there
                   is no method that can satisfy these three conditions
                   simultaneously. Moreover, even satisfying all three
                   conditions approximately requires that the data lie in an
                   approximate version of one of the constrained special cases
                   identified by our theorem. These results suggest some of the
                   ways in which key notions of fairness are incompatible with
                   each other, and hence provide a framework for thinking about
                   the trade-offs between them.",
  month         =  sep,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Chouldechova2017-aj,
  title     = "Fair prediction with disparate impact: A study of bias in
               recidivism prediction instruments",
  author    = "Chouldechova, Alexandra",
  journal   = "Big Data",
  publisher = "liebertpub.com",
  volume    =  5,
  number    =  2,
  pages     = "153--163",
  abstract  = "Recidivism prediction instruments (RPIs) provide decision-makers
               with an assessment of the likelihood that a criminal defendant
               will reoffend at a future point in time. Although such
               instruments are gaining increasing popularity across the country,
               their use is attracting tremendous controversy. Much of the
               controversy concerns potential discriminatory bias in the risk
               assessments that are produced. This article discusses several
               fairness criteria that have recently been applied to assess the
               fairness of RPIs. We demonstrate that the criteria cannot all be
               simultaneously satisfied when recidivism prevalence differs
               across groups. We then show how disparate impact can arise when
               an RPI fails to satisfy the criterion of error rate balance.",
  month     =  jun,
  year      =  2017,
  keywords  = "bias; disparate impact; fair machine learning; recidivism
               prediction; risk assessment",
  language  = "en"
}

@ARTICLE{Wang2024-cj,
  title     = "Against Predictive Optimization: On the Legitimacy of
               Decision-making Algorithms That Optimize Predictive Accuracy",
  author    = "Wang, Angelina and Kapoor, Sayash and Barocas, Solon and
               Narayanan, Arvind",
  journal   = "ACM J. Responsib. Comput.",
  publisher = "dl.acm.org",
  volume    =  1,
  number    =  1,
  pages     = "1--45",
  month     =  mar,
  year      =  2024
}

@INPROCEEDINGS{Jacobs2021-of,
  title     = "Measurement and Fairness",
  author    = "Jacobs, Abigail Z and Wallach, Hanna",
  booktitle = "Proceedings of the 2021 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  mar,
  year      =  2021
}
