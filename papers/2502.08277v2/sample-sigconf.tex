\documentclass[sigconf]{acmart}
% \pdfoutput=1
%\usepackage[UTF8]{ctex}
%\usepackage[letterpaper]{geometry}
% \settopmatter{printacmref=false} % Removes citation information below abstract
% \renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
% \pagestyle{plain} % removes running headers
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{subfigure}
\usepackage{color}
\usepackage{bm}
\usepackage{epstopdf}
\usepackage{url}
%\usepackage{flushend}
\usepackage[cal=cm]{mathalfa}
\usepackage{balance}
\usepackage{threeparttable}
\usepackage{lipsum}
\usepackage{enumitem}
 \usepackage{pifont}
 \usepackage{tabularx}
 \usepackage{makecell}
 \usepackage{float}
 % \usepackage{authblk}


\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}


\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}


\renewcommand{\algorithmicrequire}{ \textbf{Input:}}     %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}    %UseOutput in the format of Algorithm
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\settopmatter{printacmref=false} 
\renewcommand\footnotetextcopyrightpermission[1]{}

\begin{document}
\title{ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion Rate Modeling}



\author{Wei Cheng$^\S$}
\affiliation{
  \institution{Kuaishou Technology}
  \country{chengwei07@kuaishou.com}
 }
 
\author{Yucheng Lu$^\S$}
\affiliation{
  \institution{Kuaishou Technology}
  \country{luyucheng@kuaishou.com}
 }

\author{Boyang Xia$^\S$}
\affiliation{
  \institution{Kuaishou Technology}
  \country{xiaboyang@kuaishou.com}
 }
 
\author{Jiangxia Cao$^{\star}$}
\thanks{$^\S$Equal contributions to this work}
\thanks{$^\star$Corresponding author.}
\affiliation{
  \institution{Kuaishou Technology}
  \country{caojiangxia@kuaishou.com}
 }

 \author{Kuan Xu}
\affiliation{
  \institution{Kuaishou Technology}
  \country{xukuan@kuaishou.com}
 }


 \author{Mingxing Wen}
\affiliation{
  \institution{Kuaishou Technology}
  \country{wenmingxing@kuaishou.com}
 }

\author{Wei Jiang}
\affiliation{
  \institution{Kuaishou Technology}
  \country{jiangwei@kuaishou.com}
 }


 \author{Jiaming Zhang}
\affiliation{
  \institution{Kuaishou Technology}
  \country{zhangjiaming07@kuaishou.com}
 }
 
  \author{Zhaojie Liu}
\affiliation{
  \institution{Kuaishou Technology}
  \country{zhaotianxing@kuaishou.com}
 }

 \author{Liyin Hong}
\affiliation{
  \institution{Kuaishou Technology}
  \country{hongliyin@kuaishou.com}
 }


 \author{Kun Gai}
\affiliation{
  \institution{Unaffiliated}
  \country{gai.kun@qq.com}
 }

 \author{Guorui Zhou}
\affiliation{
  \institution{Kuaishou Technology}
  \country{zhouguorui@kuaishou.com}
 }



 
\renewcommand{\shorttitle}{ChorusNet}


\begin{abstract}
\input{abstract_v2}
\end{abstract}




\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003347.10003350</concept_id>
<concept_desc>Information systems~Recommender systems</concept_desc>
<concept_significance>500</concept_significance>
</concept>
% <concept>
% <concept_id>10010147.10010257.10010293.10010294</concept_id>
% <concept_desc>Computing methodologies~Neural networks</concept_desc>
% <concept_significance>500</concept_significance>
% </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}
% \ccsdesc[500]{Computing methodologies~Neural networks}

\keywords{Ranking Model; Post-Click Conversion Rate Estimation;}

\maketitle



% 冲！
\input{intro_v3}
%
%
\input{method_v2}





\begin{table}[t!]
\centering
\caption{Offline results(\%) in terms of CTR-AUC, CTCVR-AUC and logloss at Ali-CCP and Kuaishou.}
\vspace{-1em}
\setlength{\tabcolsep}{3pt}{
% \resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\multirow{4}{*}{\makecell{Models}} 
& \multicolumn{2}{c|}{Ali-CCP} & \multicolumn{2}{c}{Kuaishou}   \\ 
\cmidrule(r){2-5} & \multicolumn{2}{c|}{AUC}  & \multicolumn{2}{c}{AUC}   \\ 
\cmidrule(r){2-3} \cmidrule(r){4-5} & CVR  & CTCVR &  CVR & CTCVR  \\
\hline
ESMM \cite{essm} & 0.5963 & 0.5802 & 0.8609 & 0.9276\\ 
ESCM$^2$-DR \cite{escm2} & 0.6354 & 0.6203& 0.8617 & 0.9280\\
ESCM$^2$-IPW \cite{escm2} & 0.6385 & 0.6126& 0.8619 & 0.9283\\
UKD \cite{ukd} & 0.6451 & 0.6282& 0.8615 & 0.9279\\
DCMT \cite{dcmt} & 0.6447 & \underline{0.6375} & \underline{0.8628} & 0.9281\\
DDPO \cite{ddpo} & \underline{0.6496} & 0.6326& 0.8623 & \underline{0.9289}\\
NISE \cite{nise} & 0.6418 & 0.6291 & 0.8614 & 0.9274\\
ChorusCVR & \textbf{0.6589} & \textbf{0.6401}& \textbf{0.8639} & \textbf{0.9304}\\
\hline
Rela.Impr. & +1.43\% & +0.40\% & +0.13\% & +0.16\% \\
\hline
ChorusCVR w/o NDM & 0.6498 & 0.6347& 0.8625 & 0.9284\\
ChorusCVR w/o SAM & 0.6407 & 0.6139 & 0.8622 & 0.9281\\
% DCMT+Ctuncvr loss & 0.6492 & 0.6387& 0.8626 & 0.9293\\
\bottomrule
\end{tabular}
}
\vspace{-2em}
% }
\label{mainoffline}
\end{table}




\section{Experiments}
In this section, we conduct extensive online and offline experiments to verify the efficacy of our model following 3 research questions: 
\begin{itemize}[leftmargin=*,align=left]
\item \textbf{RQ1:} How does our model perform on industrial recommendation datasets?
\item \textbf{RQ2:} Can our model bring improvements of e-commerce A/B test metrics on online product environment?
\item \textbf{RQ3:} Can our model alleviate the bias on un-clicked samples?
\end{itemize}
\textbf{Datasets.} To evaluate the performance of our method and comparison baselines, we conduct comprehensive experiments on both public and industrial datasets.



• Public dataset: The Ali-CCP (Alibaba Click and Conversion Prediction) dataset \cite{essm} is a benchmark dataset for CVR and CTR prediction, which collected from traffic logs in Taobao e-commerce platform. Ali-CCP contains 33 features and 84M samples. The training set contains 42M exposure samples, 1.6M click samples and 9k conversion samples and the test set contains 42M exposure samples, 1.7M click samples and 9.4k conversion samples.

• Industrial dataset: The industrial dataset is colloected from Kuaishou e-commerce live-streaming platform. Kuaishou e-commerce live-streaming is a popular content interest e-commerce platform with \textbf{tens of millions} daily active users. The dataset contains more than 1000 features including user profiles (gender, age, etc), user actions (click, buy, etc), seller profiles (industry, sales, etc) and goods information. 

\textbf{Compared Methods.} ESMM \cite{essm} learns CVR task through a CTR task and a CTCVR task to alleviate ssb and data sparsity issues. ESCM$^2$-IPW \cite{escm2} incorporates the inverse propensity weighting (IPW) \cite{ipw} method to regularize ESMM’s CVR estimation. ESCM$^2$-DR \cite{escm2} augments ESCM$^2$-IPW with an auxiliary imputation loss to models the CVR with the Doubly Robust(DR) method. UKD \cite{ukd} introduces a transfer adversarial learning approach to generate soft conversion pseudo-labels in the unclicked space. DCMT \cite{dcmt} proposed a counterfactual mechanism to directly debias CVR in the entire space. DDPO \cite{ddpo} employs a conversion propensity prediction network to generate soft conversion pseudo-labels in the un-clicked space. NISE \cite{nise} follows a semi-supervised learning paradigm and use predicted CVR as CVR pseudo labels for un-clicked samples.

\textbf{Offline Results (RQ1):} 
We evaluate the efficacy of our model by the Area Under ROC (AUC) of CVR and CTCVR prediction tasks. The experimental results on two datasets are shown in Table 1. On Ali-CCP dataset, our model outperforms DDPO by 1.4\% (\textbf{0.6589} v.s. 0.6496) for CVR-AUC and DCMT by 0.4\% (\textbf{0.6401} v.s. 0.6375) for CTCVR-AUC, respectively. On Kuaishou dataset, our model outperforms DCMT by 0.1\% (\textbf{0.8639} v.s. 0.8628) for CVR-AUC and DDPO by 0.16\% (\textbf{0.9304} v.s. 0.9289) for CTCVR-AUC, respectively. In a nutshell, our model consistently outperforms the best-performing baselines on both two datasets in a large margin in terms of two tasks. 
\begin{figure}[t]
  \centering
  \includegraphics[width=5cm]{pcoc.png}
  \vspace{-1em}
  \caption{The PCOC analysis.}
  \label{pcoc}
  \vspace{-2em}
\end{figure}
\textit{Ablation Study.} To verify the efficacy of each parts of ChorusCVR, we evaluate the performance of variants without NDM and SAM. It is noteworthy that the unCVR objective is optimized by 1 - $y^{CVR}$ label in click space in `ChorusCVR w/o NDM'. We find that without NDM, our model degrades 1.3\% (\textbf{0.6589} v.s. 0.6498) on CVR AUC on Ali-CCP dataset, for lack of discrimination between negative samples of different levels. Meanwhile, we can observe that without SAM, the `ChorusCVR w/o SAM' performs even poorer than `ChorusCVR w/o NDM', presents 4\% lower CTCVR AUC on Ali-CCP dataset, for lack of reasonable pesudo supervisions on un-click samples. 

\textbf{Online Results (RQ2):}
We deploy our method in production environment of Kuaishou e-commerce live-streaming platform to conduct online A/B testing for 8 days. %As shown in Tab. 2,
Compared with the base model (DCMT), our model presents large improvements on CVR(+0.12\%), orders (+0.851\%) and DAC (+0.705\%) with 95\% confidence intervals. 




\textbf{Analysis (RQ3):}
To investigate whether our method can address the bias in un-clicked samples, we try to compare the  accuracy of cvr predicitons between our method and baselines on un-clicked samples. However, the conversion labels of un-clicked samples are inaccessible in the real world. Inspired by the inverse propensity weighting based methods, we propose a compromised comparison approach, to use samples with low pCTR as a substitute for un-clicked samples and observe the model's pCVR on these low CTR samples. We show the estimated pCVR and actual cvr for samples with different pCVR the deviations between two curves signify the prediction bias of the model.

As shown in Fig. \ref{pcoc}, we can observe that DCMT represents obvious bias on low pCTR samples, which severely overestimates the CVR on low pCtr samples. However, our model accurately predicts the CVR on those low pCtr samples for reasonable CVR supervisions on un-clicked samples. 










\section{Conclusion}
In this paper, to alleviate sample selection bias in CVR prediction task, we propose an effective method ChorusCVR. To generate discriminative and robust soft labels, we propose Negative sample Discrimination Module to obtain soft CTunCVR labels which can separate negative samples of different levels. Then we design a 
Soft Alignment Module for debiased CVR learning in un-click space with soft labels. We demonstrated the superior performance of the proposed
ChorusCVR in offline experiments. In addition, we conduct online A/B testing, obtaining +0.851\% improvements on orders of industrial e-commerce living stream, which demonstrates the effectiveness and universality of ChorusCVR in online
systems. 
% Moreover, ChorusCVR has been deployed on ranking system in KUAISHOU e-commerce living stream.



% \newpage
\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base-extend.bib}
\end{document}
\endinput