\input{Figures/concept}
\section{Method}
In this section, we introduce our zero-shot depth completion method, which leverages the depth prior~\cite{ke2023repurposing, gui2024depthfm}
derived from the foundation model~\cite{rombach2022highresolution}. This 
enables our method to be generalizable across any domain.
The core concept of our approach is to align the affine-invariant depth prior with sparse measurements on an absolute scale to complete the dense and well-structured depth map, as illustrated in \Fref{fig:concept}.

\subsection{Preliminary}
\label{sec:preliminary}
\para{Diffusion model and guided sampling}
Diffusion models~\cite{ho2020denoising, song2022denoising} aim to model data distribution $p(\mathbf{x})$ through iterative perturbation and restoration, known as forward and reverse processes.
This is represented by the score-based generative model~\cite{song2021scorebased}, learning the score function $\mathbf{s}_\theta$ parameterized by $\theta$ the gradient of the log probability density function with respect to the data, \ie, $\mathbf{s}_\theta(\mathbf{x}) =\nabla_{\mathbf{x}} \log p(\mathbf{x};\theta)$.
Score-based diffusion models 
estimate the score $\mathbf{s}_\theta(\mathbf{x}_t)$ at intermediate state $\mathbf{x}_t$ for timestep $t$ which defines a process.

For image generation and editing, diffusion models leverage the guidance function during the sampling process to adjust the output to the specific condition
~\cite{ho2022classifierfree, dhariwal2021diffusion}.
The guidance can be 
defined 
by any differentiable mapping output to guidance modality, as follows~\cite{bansal2024universal}: 
\begin{equation}
\label{eq:guide_sampling}
\hat{\mathbf{s}}_{\theta}(\mathbf{x}_t, t, \mathbf{y}) = \mathbf{s}_{\theta}(\mathbf{x}_t, t) + w \nabla_{\mathbf{x}_t} \mathcal{L}\left(f\left(\mathbf{x}_0\left(\mathbf{x}_t\right)\right),\mathbf{y}\right),
\end{equation}
where $w$ and $\mathbf{y}$ represent weight and guidance, respectively.
The function $f(\cdot)$ can be any differentiable function whose output can compute a loss $\mathcal{L}$ with guidance condition $\mathbf{y}$, and
$\mathbf{x}_0\left(\mathbf{x}_t\right)$ is obtained by using Tweedie's formula~\cite{efron2011tweedie}, 
which provides an approximation of the posterior mean. 
This guided sampling approach extends unconditional diffusion models to conditional ones without separate model training.

\para{Inverse problem}
The goal of an inverse problem is to determine an unknown variable from known measurement, often formulated as $\mathcal{A}(\mathbf{x})=\mathbf{y}$,
where $\mathcal{A}{:}\, \mathbb{R}^m {\rightarrow}  \mathbb{R}^n$ represents the known forward measurement operator, $\mathbf{y}\in \mathbb{R}^n$ and $\mathbf{x}\in \mathbb{R}^m$,
the measurement and the unknown variable, respectively.
When 
$m>n$, it becomes an ill-posed problem, requiring a prior to find 
solve a
Maximum A Posterior (MAP) estimation:
\begin{equation}
\label{eq:map}
    \argmax p(\mathbf{x}|\mathbf{y})\propto p(\mathbf{x}) p(\mathbf{y} | \mathbf{x}),
\end{equation}
where $p(\mathbf{x})$ represents our prior of the signal $\mathbf{x}$ and $p(\mathbf{y} | \mathbf{x})$ is likelihood measuring 
$\mathcal{A}(\mathbf{x})\approx\mathbf{y}$, \eg, $\|\mathbf{y} {-} \mathcal{A}(\mathbf{x})\|_2^2$.
By taking $-\log(\cdot)$ to \Eref{eq:map}, it can be  
formulated as an optimization problem
that regularizes the solution, ensuring that $\mathbf{x}$ follows the characteristics of the prior:
\begin{equation}
    \label{eq:inv_opt}
    \argmin_{\mathbf{x}} \left\|\mathbf{y} - \mathcal{A}\left(\mathbf{x}\right) \right\|_2^2 - \log p(\mathbf{x}).
\end{equation}
Also, given the gradient of $\log p(\mathbf{x}|\mathbf{y})$ in \Eref{eq:map} as
\begin{equation}
    \nabla_{\mathbf{x}}\log p(\mathbf{x}|\mathbf{y}) =  \nabla_{\mathbf{x}}\log p(\mathbf{x}) + \nabla_{\mathbf{x}}\log p(\mathbf{y} | \mathbf{x}),
\end{equation}
the prior term $\nabla_{\mathbf{x}}\log p(\mathbf{x})$ corresponds to the score $\mathbf{s}_{\theta}(\mathbf{x})$, which can be obtained by diffusion models.
Therefore, by simply adding the gradient of the likelihood term to the reverse sampling process, the inverse problem can be effectively solved while leveraging the diffusion prior~\cite{chung2023dps}
as follows:
\begin{equation}
    \label{eq:inv_sampling}
    \hat{\mathbf{s}}_{\theta}(\mathbf{x}_t, t, \mathbf{y}) = \mathbf{s}_{\theta}(\mathbf{x}_t, t) + w \nabla_{\mathbf{x}_t}\left\| \mathbf{y} - \mathcal{A}\left(\mathbf{x}_0\left(\mathbf{x}_t\right)\right) \right\|_2^2.
\end{equation}
This has an analogous form with \Eref{eq:guide_sampling}; thus, the inverse problem can be effectively tackled with the guided sampling.


With pre-trained image diffusion models, \eg, \citet{rombach2022highresolution}, as the score function $\mathbf{s}_{\theta}(\mathbf{x})$ and a prior, it provides 
powerful image prior across various tasks by its 
comprehensive semantic understanding and structural knowledge learned from a lot of images~\cite{wang2023exploiting, namekata2024emerdiff}.
\citet{ke2023repurposing} leverage this rich visual knowledge to achieve generalizable monocular depth estimation, resulting in high-quality outputs within an affine-invariant depth space. In our work, we exploit this depth diffusion model for computing the score as a depth prior.

\input{Figures/method}
\para{Problem formulation}
\label{sec:problem_form}
To leverage the prior knowledge, we formulate
a depth completion as an inverse problem that estimates unknown dense depth from 
observed sparse measurements.
$\mathbf{y}$ represents the observed sparse depth, 
$\mathbf{x}$ is the unknown dense depth, and 
$\mathcal{A}{:}\,\mathbb{R}^m{\rightarrow}\mathbb{R}^n$ is a binary measurement matrix of which entry  $[\mathcal{A}]_{ij}$ is $1$ if the entities $[\mathbf{y}]_i$ is measured from $[\mathbf{x}]_j$, $0$ otherwise. 
We follow \Eref{eq:inv_sampling}, where sparse depth serves as guidance.
We use the depth diffusion models \cite{ke2023repurposing, gui2024depthfm} extended from the latent diffusion model (LDM)~\cite{rombach2022highresolution} as prior, where
$\mathbf{x}$ is 
decomposed with the decoder $\mathcal{D}{:}\, \mathbf{z} \rightarrow \mathbf{x}$ as:
\begin{equation}
\label{eq:depth_guide_sampling}
\hat{\mathbf{s}}_{\theta}= \mathbf{s}_{\theta}(\mathbf{z}_t, t) + w {\nabla_{\mathbf{z}_t}}\left\| \mathbf{y} - \mathcal{A}\left(\mathcal{D}\left(\mathbf{z}_0\left(\mathbf{z}_t\right)\right)\right) \right\|_2^2,
\end{equation}
where $\mathbf{z}\in \mathbb{R}^{4\times H \times W}$ represents the latent of LDM but the decoder output $\mathbf{x}$ is treated as a flatten vector for convenience.

\subsection{Test-time Alignment with Hard Constraints}
\label{sec:opt_sampling}
Depth measurements obtained in practice are often sparse, unevenly distributed, and noisy. 
When the sparse measurements are used as guidance, the ill-posed nature of the problem, combined with the stochastic behavior of diffusion models, can lead to scores that produce undesirable solutions~\cite{kim2024regtext} and does not even guarantee that the estimation corresponds to the known sparse measurements. 
To deal with this, we propose a test-time alignment that incorporates the correction step 
to enforce the sparse measurement as harder constraints than encouraging guidance in a soft manner by \Eref{eq:depth_guide_sampling}.
This involves an optimization loop at regular intervals to enforce
measurement constraints as a correction step.
We further show the potential for uncertain solutions from the stochastic process in the supplementary material, illustrating why the alignment is necessary.

Additionally, we adopt $\mathbf{z}_0(\mathbf{z}_t)$ as optimizable variable.
Pre-trained diffusion models take input $\mathbf{z}_t$ aligend with the noise level at each timestep $t$.
However, directly optimizing $\mathbf{z}_t$  without considering input characteristics may lead to suboptimal results
\cite{chung2022improving, chung2023dps, chung2024dds}.
To address this, inspired by \citet{song2024solving}, we use $\mathbf{z}_0(\mathbf{z}_t)$ estimated from $\mathbf{z}_t$.
The optimization loop is formulated as:
\begin{equation}
    \label{eq:opt_loop}
    \hat{\mathbf{z}}_0(\mathbf{z}_t) = \argmin_{\mathbf{z}_0(\mathbf{z}_t)} \left\| \mathbf{y} - \mathcal{A}\left(\mathcal{D}\left(\mathbf{z}_0\left(\mathbf{z}_t\right)\right)\right) \right\|_2^2.
\end{equation}
Then, to ensure adherence to the correct noise level, the measurement-consistent $\hat{\mathbf{z}}_0(\mathbf{z}_t)$ is remapped to an intermediate latent $\hat{\mathbf{z}}_t$ by adding time-scheduled Gaussian noise, as expressed below:
\begin{equation}
    \label{eq:remap}
    p\left(\hat{\mathbf{z}}_{t} | \hat{\mathbf{z}}_0(\mathbf{z}_t)\right) = \mathcal{N}(\sqrt{\bar{\alpha}_{t}} ~ \hat{\mathbf{z}}_0(\mathbf{z}_t), (1 - \bar{\alpha}_{t}) I),
\end{equation}
\noindent where $\bar{\alpha}_{t} = \prod_{i=1}^t \alpha_i,$ and $\alpha_t$ is variance schedule at time $t$.

Since the score $\hat{\mathbf{s}}_{\theta}(\mathbf{z}_t, t)$ is directly added to the latent $\mathbf{z}_t$ at each step,
\Eref{eq:depth_guide_sampling} can be rewritten in terms of $\mathbf{z}_0(\mathbf{z}_t)$ with a modulated weight factor $\zeta$, as follows:
\begin{equation}
    \label{eq:depth_guide_sampling_latent}
    \hat{\mathbf{z}}_t = \mathbf{z}_t + \zeta \nabla_{\mathbf{z}_t} \left\| \mathbf{y} - \mathcal{A}\left(\mathcal{D}\left(\mathbf{z}_0(\mathbf{z}_t)\right)\right) \right\|_2^2.
\end{equation}
Here, \Eref{eq:depth_guide_sampling_latent} is replaced by the two-step process of \Eref{eq:opt_loop} and \Eref{eq:remap}, allowing our test-time alignment process to effectively achieve measurement-consistent desirable solutions.
Figure \ref{fig:opt_loop} illustrates the our test-time alignment process.
Figure~\ref{fig:depth_align} demonstrates how effectively our test-time alignment method estimates unseen depth areas by aligning sparse measurements with an affine-invariant depth prior. This result highlights the need for correction.
Examples of undesirable solutions and their corrected ones by our method are provided in the supplementary material.
\input{Figures/depth_align}
% \input{Figures/outlier_filter}

Until now, in solving \Eref{eq:depth_guide_sampling}, we use an affine-invariant depth model for completing metric depths without special care. However, a natural question arises: ``\textit{Is the affine-invariant depth model compatible with estimating metric depths in our framework?}'' The following analysis shows that it may be sufficient.

\vspace{1mm}\noindent\textbf{Can we use an affine-invariant depth model for completing metric depths?}
Depth estimation models are often trained to estimate affine-invariant depth with scale and shift invariant loss to achieve generalizable performance~\cite{Ranftl2022midas, ke2023repurposing, eigen2014invariant}.
Thus, depth prior operates in the affine-invariant depth space, which does not directly correspond to the metric depth used in measurements.
Even though the given sparse metric depth is normalized between 0 and 1, 
their statistics including 
mean and variance
can differ, and the relationship between real metric depth and estimated affine-invariant depth is often 
non-linear (see 
the left of \Fref{fig:depth_align} (d)).
Therefore, to determine if 
\Eref{eq:depth_guide_sampling} can be used to solve this problem, we need to verify whether the normalized metric depth space lies within the data distribution generated by the diffusion model.

To confirm this, we conduct an empirical investigation through the following procedure: given $\tilde{\mathbf{x}}_0$, dense depth map estimated from the pre-trained depth completion model, we perform its reconstruction using an affine-invariant depth diffusion model.
This process involves sequentially encoding $\tilde{\mathbf{x}}_0$ to $\tilde{\mathbf{z}}_0$, 
then doing inversion by adding noise~\cite{song2022denoising}, which results in $\tilde{\mathbf{z}}_t$. 
Next, we perform reverse sampling, $\nabla_{\mathbf{z}_t}\log p(\tilde{\mathbf{z}}_t)$ with only the affine-invariant depth diffusion prior.
The reconstructed result achieves similar performance compared to the original one, $\tilde{\mathbf{x}}_0$, excluding encoding-decoding information loss. 
The details and results of the experiment are provided in the supplementary material.
This result suggests that the affine-invariant depth prior is sufficiently capable of handling the metric depth space, 
which corresponds to:
\begin{equation}
\label{eq:verify}
    \nabla_{\mathbf{z}_t}\log p(\tilde{\mathbf{z}}_t) \approx \nabla_{\tilde{\mathbf{z}}_t}\log p(\tilde{\mathbf{z}}_t).
\end{equation}
Thus, we just need to align this prior with metric depth cue validating using \Eref{eq:depth_guide_sampling} to solve ill-posed depth completion.

% \input{Tables/generalization}
% \input{Tables/efficiency}
\subsection{Prior-based Outlier Filtering}
\label{sec:noise_filter}
Practical depth sensing methods often produce outliers, such as unsynchronized depth with RGB or see-through points~\cite{conti22confidence}), making sparse depth measurements unreliable.
This degrades the performance of methods relying on sparse depth supervision~\cite{wong2021unsupervised,wong2020void}.
We also use sparse depth measurement as supervision during test-time alignment, this makes the alignment process prone to divergence or slow convergence.
To address this,
we utilize data-driven depth prior~\cite{ke2023repurposing, gui2024depthfm}, which benefits from the more precise synchronization with RGB images and depth affinity.
To obtain outlier-free sparse points $\mathbf{y}^*$, we adopt a divide-and-conquer approach.
We define local segments based on depth affinity, grouping regions where relative depth values are similar within a spatially local area.
Within these segments, the depth distribution can be easily categorized into inliers and outliers, enabling us to effectively identify outliers.

Affine-invariant depth map $D_r$ is divided into local segments $S_i$, which are regions with a high probability of having similar depths with considering location. 
For this clustering 
we leverage the superpixel algorithm~\cite{achanta2012slic, li2015lsc}.
In each region, we perform linear least-square fitting to map affine-invariant depth to metric depth using sparse metric depth measurements $\mathbf{y}_i$.
However, since these sparse measurements are influenced by outliers, we use RANSAC~\cite{fischler1981ransac} to perform outlier-robust linear least-square fitting on points where noisy $\mathbf{y}$ intersects $S_i$ \ie, $\mathbf{y}_i \leftarrow S_i \cap \mathbf{y}$.
This allows us to estimate outlier-robust metric depth values $\hat{\mathbf{y}}_i$ in local regions $S_i$.
Then, points with significant deviations exceeding $\tau$ are identified as outliers and filtered out.
Our proposed filtering algorithm, based on monocular depth prior, is detailed in Algorithm~\ref{algorithm:1}.
% \red{We also evaluate our algorithm using a standard metric for assessing the reliability of outlier detection confidence, as detailed in the supplementary material.}

\input{Algorithms/outlier_filter}
\input{Tables/generalization}
\subsection{Losses}
\label{sec:losses}
Our objective for optimization includes sparse depth consistency loss and regularization terms: a local smoothness loss to preserve depth prior and a new relative structure similarity loss to maintain structural prior inherent in depth prior.

\para{Sparse depth consistency}
Given the sparse depth measurement $y$, it ensures consistency with the metric depth.
To effectively integrate the observed measurements with affine-invariant depth prior and mitigate potential uncertainties, we employ $L_{1}$ loss as follows:
\begin{equation}
    \resizebox{0.59\hsize}{!}{$
    \mathcal{L}_{depth} = \scalebox{1.4}{$\frac{1}{|\Omega(\mathbf{y})|}$} \sum\limits_{\Omega(\mathbf{y})} |\mathbf{y}- \mathcal{A}(\hat{D})|,
    $}
\end{equation}
where $\mathcal{A}$ is the operation that Hadamard product with the zero-one mask $\mathds{1}_{\Omega(\mathbf{y})}$ and $\hat{D}$ represents completed depth.

\para{Local smoothness}
Using only sparse depth guidance risks losing the prior knowledge inherent in pre-trained depth diffusion models~\cite{ke2023repurposing, gui2024depthfm}, such as the property of depth which is locally smooth.
To mitigate this, we introduce a regularization term that enforces smoothness by applying the $L_1$ norm to gradients in both the $X$ and $Y$ directions, with reduced gradient weights near edges to prevent over-smoothing.
% To avoid over-smoothing at edges, gradient weights are set to smaller values near edges.
The loss function is defined as follows:
\begin{equation}
    \resizebox{0.89\hsize}{!}{$
    \mathcal{L}_{smooth} =\scalebox{1.5}{$\frac{1}{|\Omega|}$} \sum\limits_{c\in\Omega} \lambda_X(c) |\partial_X \hat{D}(c)| + \lambda_Y(c) |\partial_Y \hat{D}(c)|,
    $}
\end{equation}
where $\lambda_X(c)=e^{-|\partial_X I(c)|}$, $\lambda_Y(c)=e^{-|\partial_Y I(c)|}$, and $c \in \Omega$ represents the set of all pixel locations~\cite{park2024testtime}.
However, using only these loss functions may dilute the structural prior in the pre-trained depth diffusion model, which is key for detail sharpness. 

\para{Relative Structure Similarity}
To address this, we design a new structure regularization term that transfers structure from the depth estimated by an off-the-shelf model to regularize overly smooth structures.
Inspired by the structure similarity (SSIM) loss~\cite{wang2004ssim}, we propose the Relative Stucture Similarity (R-SSIM) loss, designed to transfer structure across domains.
This loss is derived from SSIM by dropping the luminance term, which relies on absolute values:
\begin{equation}
    \mathcal{L}_{r-ssim}(d_1,d_2) = 1-\frac{2\sigma_{d_1d_2} + C}{\sigma_{d_1}^2 + \sigma_{d_2}^2 + C},
\end{equation}
where $d_1$ and $d_2$ represent spatial information in different domains, $C$ is a constant, and $\sigma$ denotes the normalized standard deviation of pixel values.
Here, $d_1$ is the relative depth map, and $d_2$ is the estimated complete depth map (or vice versa).
The key point is that these domains may differ in pixel value ranges and statistics. 
% \red{Ablation studies on R-SSIM loss are in the supplementary materials.}

\vspace{1.5mm}
\noindent Our comprehensive loss function is as follows:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{depth} + \lambda_{smooth} \mathcal{L}_{smooth} + \lambda_{r-ssim} \mathcal{L}_{r-ssim},
\end{equation}
where $\lambda_{smooth}$ and $\lambda_{r-ssim}$ are regularization weights.
