\section{Related Work}
\para{Depth completion}
Depth completion is an ill-posed problem that aims to reconstruct unknown dense depth from observed
% observable
sparse depth measurements, with missing areas typically covering less than $5\%$ of an image for outdoor driving scenarios and $1\%$ for indoor scenarios~\cite{wong2020void}.
% The proportion of missing observations depends on the setting of the problems, but most works typically use 
% % the which approximately cover 
% less than $5\%$ of the image 
% % plane in 
% for 
% % the
% outdoor driving scenarios and $1\%$ for
% % in
% the indoor scenarios~\cite{wong2020void}.
Since the success of deep learning, 
% Nowadays, it 
the problem 
% \before{is also addressed}
has been addressed
by data-driven approaches that learn how to propagate sparse depth measurements
% information
guided by the RGB images~\cite{wong2021unsupervised, park2020nonlocal}.
% Previous works
Prior studies~\cite{park2020nonlocal, lin2022dynamic, zhang2023completionformer} use affinity-based spatial propagation methods~\cite{liu2017learning, cheng2019learning}
% \before{learning from the relationship between dense depth and RGB pairs.}
% \kwon{
to learn the relationship between dense depth and RGB pairs.
% }
They learn how to propagate depth while preserving scene structure and boundaries.
% Prior studies~\cite{park2020nonlocal, lin2022dynamic, zhang2023completionformer} have learned how to propagate depth while preserving scene structure and boundaries from the relationship between dense depth and RGB pairs.
% These supervised learning methods require labeled semi-dense (about 10-30\%) or dense depth maps.
% However, 
% This learning process requires large pairs of a RGB image and its corresponding dense depth map, but obtaining these dense maps in real-world scenarios, such as autonomous driving or environments using SLAM/VIO, is costly due to dedicate sensor systems and requires careful data processing and curation.~\cite{uhrig2017sparsity, wong2020void}.
This learning process requires large pairs of RGB images and dense depth maps, but acquiring these dense maps in real-world scenarios is costly due to dedicated sensor systems and requires careful data processing and curation.~\cite{uhrig2017sparsity, wong2020void}.
Depending on how to process data, domain discrepancies are introduced in each dataset, which makes depth completion models hard to generalize.% challenging due to necessary postprocessing to accumulate consecutive raw frames, correct measurement errors, handle moving objects, and synchronize with RGB frames

To mitigate these challenges arising from the lack of real data and domain gaps, unsupervised learning or domain adaptation methods have been proposed. 
Unsupervised methods~\cite{wong2021unsupervised, ma2018self, wong2020void} train a model 
% leverage only
with pairs of a RGB image and synchronized sparse depth without a dense depth map.
These methods exploit multi-view 
% require other views for
photometric consistency with multiple views to compensate for the lack of direct 3D supervision. 
% information, 
% which necessitates relative poses.
% \before{Among these, ScaffNet~\cite{wong2021scaffnet} is initially trained in a synthetic domain with supervised learning, followed by unsupervised training on real datasets for domain adaptation.
% However, such domain adaptation methods~\cite{wong2021scaffnet, park2024testtime} still struggle with domain shifts due to factors like sensor characteristics, depth ranges, and scene variations (e.g., between KITTI~\cite{uhrig2017sparsity} and nuScenes~\cite{caesar2020nuscene}), making it difficult to generalize across diverse domains through mixed dataset training~\cite{Ranftl2022midas}.}
As an alternative direction to mitigate lack of data and domain gaps, some works~\cite{wong2021scaffnet, lopezrodriguez2020project} is initially trained in a synthetic domain with supervised learning, followed by unsupervised training on real datasets as a way of
% for
domain adaptation.
% \before{Therefore, prior approaches primarily focus on in-domain or similar-domain generalization~\cite{wong2021unsupervised, wong2021scaffnet} (\eg, indoor-to-indoor), rather than out-of-domain generalization. 
% However, both unsupervised learning and domain adaptation methods still struggle with domain shifts due to factors like sensor characteristics, depth ranges, and scene variations (e.g., between KITTI~\cite{uhrig2017sparsity} and nuScenes~\cite{caesar2020nuscene}), making it difficult to generalize across diverse domains.
% }
% However, both unsupervised learning and domain adaptation methods still struggle with domain generalization across diverse domains due to factors like sensor characteristics, depth ranges, and scene variations.
% \kwon{
% We address these limitations 
% through a prior-based zero-shot depth completion pipeline, which is generalizable to any domain, leveraging the affine-invariant depth prior
% that inherently understands depth affinity, spatial detail, and scene context. 
% }
Different from these research, we tackle
% address
the limitations by exploiting learned prior embeded in a foundation model. 
We use a pre-trained generative diffusion model that understands depth affinity, spatial detail, and scene context.
% by a prior-based zero-shot depth completion method
% pipeline
% that leverages depth prior
% inherently understanding depth affinity, spatial detail, and scene context.
This strong prior from the foundation model further enables zero-shot generalization to any domain.

% This facilitates any-of-domain generalizable depth completion from only sparse cues and consistently shows significantly qualitative improved results.

% \para{Diffusion model}
% The diffusion models~\cite{ho2020denoising, song2022denoising} have brought remarkable advancements to the field of image generation.
% The Latent Diffusion Model (LDM)~\cite{rombach2022highresolution}, which performs the diffusion process in latent space, is trainable with large datasets while avoiding high computational demands.
% Leveraging datasets with billions of images, LDMs are not only lightweight due to their latent design but also exhibit high fidelity. 
% It enables to utilize it as a powerful image prior, the knowledge acquired from a lot of images~\cite{wang2023exploiting, namekata2024emerging,nam2024diffusion}.
% % --from an analysis-by-synthesis perspective~\cite{wang2023exploiting, namekata2024emerging,nam2024diffusion}.
% The learned representation has demonstrated comprehensive semantic understanding across various tasks, providing general knowledge about the structure across the various scenes~\cite{tang2023emergent, namekata2024emerdiff}.
% \citet{ke2023repurposing} leverage the rich visual knowledge 
% % inherent in the LDM 
% to achieve generalizable monocular depth estimation, resulting in high-quality outputs within an affine-invariant depth space. 
% We leverage this pre-trained depth diffusion model as the depth prior, serving as a regularizer to 
% complete structural dense depth map.
% % make the depth dense and structural.
% % which inherently includes image prior, enhancing the detail and sharpness.

\para{Test-time Adaptation (TTA)}
Applying a model trained on a source domain to unseen test domains is crucial for generalization, especially in depth completion, where domain gaps arise from sensor variations, environmental conditions (\eg, weather changes), scene variety (\eg, driving locations), and depth ranges (\eg, indoor vs. outdoor). 
% Applying a model trained on a source domain to the unseen test domains is crucial for general capability.
% This is important in fields like depth completion, where domain gaps arise from variations in sensor characteristics, environmental conditions (\eg, nighttime or weather changes), scene variety (\eg, driving locations), and depth ranges (\eg, indoor vs. outdoor).
% To address domain shift,
% % in general machine learning tasks, 
% previous works~\cite{wang2021tent, wang2022continual} employ Test-time Adaptation (TTA) to adapt models to unseen test data.
% % , to address domain gap refers to the discrepancy between training and diverse testing data distributions.
% \citet{park2024testtime} also adopt TTA for applicable to out-of-domain scenarios
% in depth completion.
TTA methods~\cite{wang2021tent, wang2022continual, park2024testtime} address this by adapting models to unseen data.
However, they still suffer from domain gaps due to reliance on the source dataset, and often require additional training and continual adaptation, which may not be feasible in zero-shot scenarios.
% To address domain shift,
% % in general machine learning tasks, 
% previous works~\cite{wang2021tent, wang2022continual} employ Test-time Adaptation (TTA) to adapt models to unseen test data.
% % , to address domain gap refers to the discrepancy between training and diverse testing data distributions.
% \citet{park2024testtime} also adopt TTA for applicable to out-of-domain scenarios
% in depth completion.
% However, this approach still suffers from domain gaps due to the high dependency on the source dataset used to train the pre-trained model. 
% Therefore, the performance of domain adaptation within depth completion depends on the similarity between the source and target data.
% % They also observed that the performance of domain adaptation within depth completion is influenced by the similarity between the source and target data.
% Also, previous TTA methods~\cite{wang2022continual, park2024testtime} often require additional training processes and adopt continual adaptation, gradually adapting to the new data by evaluating and learning from it over the data stream.
% Therefore, it is not applicable to any single data instance, as in zero-shot.
% difficult to consider these methods as truly generalizable, \ie,t.
% as in a zero-shot scenario.

With the emergence of foundation models, there has been a shift towards leveraging their prior knowledge for generalization across diverse tasks and domains~\cite{ jia2023dginstyle, liu2023grounding}.
% The diffusion model, regarded as a generative foundation model, is similarly employed as a general representation.
As a generative foundation model, diffusion models are similarily employed as a generalizable priors.
To address the domain gaps in depth completion, we utilize a diffusion model that comprehends depth prior~\cite{ke2023repurposing, gui2024depthfm} by aligning it with sparse depth measurement 
using the proposed test time alignment method.
% at the test time. 
This approach effectively mitigates issues caused by domain gaps and enables depth completion in a zero-shot manner.

% \after{We define this generalizable depth estimation knowledge, inherited from the structural understanding of image priors, as depth prior, which understands the relationships between depths within the scene.}
% \after{We leverage the depth prior, which inherently includes image prior, enhancing the detail and sharpness of scene understanding.}
% Rebuttal같은데에 들어갈 수 있는 부분.
% Additionally, studies by Saurabh \etal~\cite{saxena2023monocular, saxena2023ddvm} and Duan \etal~\cite{duan2023diffusiondepth} utilize diffusion models to handle incomplete and noisy training data for tasks such as optical flow and metric depth estimation.
% However, when 2D sparse information, such as sparse depth or optical flow, is encoded in latent space, it tends to be distorted.
% Therefore, to preserve input information, previous works~\cite{saxena2023monocular, saxena2023ddvm, duan2023diffusiondepth} typically operate in pixel space, making it challenging to effectively utilize prior knowledge.
% To address this limitation, we utilize sparse depth as supervision rather than input. 
% this is similar to the approach of Rodriguez \etal~\cite{lopezrodriguez2020project}.

% Other studies~\cite{song2023consistency,chung2022come,dai2024motionlcm} have addressed the drawbacks of using diffusion models for a specific scenario, such as slow sampling and adaptation.
% Recently, Dai \etal~\cite{dai2024motionlcm} extends guided sampling to motion generation with real-time capabilities.