\section{Additional Experiments}
In this section, we provide the additional experiments and analyses that complement our main paper.
First, we discuss why we use ground truth processing method from \citet{huang2022pcacc} rather than \citet{park2024testtime} for the nuScenes~\cite{caesar2020nuscene} dataset benchmark.
Second, we handle compatability of affine-invariant depth diffusion model for metric depth, discussed in the main paper, and analyze the potential issues of depth diffusion model's stochastic nature in deterministic dense prediction tasks such as depth estimation and completion. 
Lastly, we detailed describe our prior-based outlier filtering method and additional results demonstrating our method's effectiveness.

\input{supp_figures/stochasticness}
\input{supp_tables/reconstruction}

\subsection{Sensitivity of the Ground Truth Processing of nuScenes Benchmark}
As mentioned in \Sref{sec:exp_domain} of the main paper, the ground truth dataset can vary depending on the accumulation method for LiDAR points and the moving object point removal method.
% after removing those corresponding to moving objects. 
In Table 1 of the main paper, we report the generalization performance on the nuScenes ground truth data obtained 
% as suggested
by the method of ProxyTTA~\cite{park2024testtime}. This ground truth data is obtained by preprocessing the test split dataset of nuScenes, which involves accumulating subsequent frames and removing moving objects using off-the-shelf models.
However, we observe that the off-the-shelf models sometimes fail to detect and remove moving objects, leading to physically inaccurate ground truth depth. Figures~\ref{fig:wrong_1} and \ref{fig:wrong_2} demonstrate this failure case. In the nuScenes dataset, the 3D-lifted ground truth depth by ProxyTTA represents 3D points from moving trucks that are closer than distant walls as ground truth. Such errors can lead to depth discrepancies of up to 10-20 meters in some samples, which likely contribute to the high RMSE values of 5-6 meters reported in \Tref{tab:nu_table} of the main paper.

\para{Evaluation on physically accurate benchmark}
\citet{huang2022pcacc} provide a nuScenes semi-dense depth (\ie, ground truth) dataset based on the validation split by accumulating frames and removing moving objects using manually annotated bounding boxes. This dataset, which relies on manual annotation, is free from the failures of off-the-shelf models and is physically accurate.
Using the nuScenes ground truth provided by \citet{huang2022pcacc} (PCACC), we assess the domain generalization performance of our method and previous test-time adaptation methods~\cite{wang2021tent,wang2022continual, park2024testtime}. 
% Since neither our method nor any of the other methods have previously encountered the nuScenes dataset, the experiment is a fair comparison.

In this experiment, the competing test-time adaptation methods use pre-trained depth completion model CostDCNet~\cite{kam2022costdcnet} trained in KITTI DC~\cite{uhrig2017sparsity} for adaptation. To independently evaluate the impact of ground truth acquisition methods, we also report the performance on the ground truth of ProxyTTA. 
Table~\ref{tab:nu_table} summarizes the results. When using the PCACC ground truth instead of that of ProxyTTA, we observe the trend of overall metric improvement across all methods, likely due to the higher physical accuracy of the ground truth. Additionally, when comparing with other competing test-time adaptation methods, our method achieves the best performance.

\input{supp_figures/unintended}

\subsection{Analysis of Test-Time Alignment Method}
\para{Compatibility of Depth Diffusion Prior with Metric Depths 
% Capability of representing metric depth space
}
As mentioned in \Sref{sec:opt_sampling} of the main paper, we investigate whether the normalized metric depth space can be represented by 
% lies
% within the data distribution generated by 
the depth diffusion models~\cite{ke2023repurposing, gui2024depthfm}, which are trained only on synthetic data.
These can be empirically verified
% confirmed
by checking the consistency of the normalized metric depth map with the depth map reconstructed through the reverse sampling of the diffusion model.
To verify this, we obtain the normalized metric depth maps using two existing depth completion models, \ie, KBNet and CompletionFormer~\cite{wong2021unsupervised,zhang2023completionformer}. Then, we reconstruct the metric depth maps after applying different noise levels and reverse sampling, so that we can see whether those metric depths can be re-represented by the depth diffusion model~\cite{ke2023repurposing}, \ie, lie in our prior space. \Tref{tab:reconstuction} shows the RMSE between the metric depth map and ground truth, as well as between the reconstructed depth map and ground truth. 
For simplicity, we denote the noise level by the DDIM sampler's timestamp, \ie, larger timestamps correspond to higher noise levels. 
The reconstructed depth map shows similar performance to the metric depth map up to timestamp 200 while achieving significantly better performance than the starting from random noise, \ie, timestamp 1,000.
This suggests that the affine-invariant depth prior we used is sufficient to well represent normalized metric depth.
% the handle normalized metric depth lies in the affine-invariant depth prior.


\para{Potential problem of stochastic process}
As discussed in \Sref{sec:opt_sampling} of the main paper, we highlight the stochasticity introduced by the diffusion model's stochastic process and the associated potential risk of falling into unintended solutions.
In this section, we experimentally show this stochastic behavior and its potential to lead to undesirable results.

Since the diffusion model starts from random noise, its outputs vary depending on the initial noise, leading to different results with each run. 
This stochasticity can produce unintended outcomes in cases where a deterministic solution exists, such as depth estimation and completion. 
Figure~\ref{fig:stochastic} shows how altering only the initial noise can result in different outputs for the same sample.
Furthermore, by simply performing guided sampling~\cite{chung2023dps}, we confirm that the potential risks discussed in \Sref{sec:opt_sampling} can lead to undesirable solutions.
Furthermore, as shown in \Fref{fig:unintended}, we observe that guided sampling leads to undesirable solutions where the depth map becomes corrupted, as discussed in \Sref{sec:opt_sampling} of the main paper.
These experimental results not only support the potential risks mentioned in the main paper but also highlight the necessity of our hard constraints and correction steps.

\input{supp_figures/outlier}
\subsection{Analysis of Outlier Filtering Method}
We evaluate our outlier filtering algorithm on the KITTI DC validation set~\cite{uhrig2017sparsity}
by computing the Area Under the Sparsification Curve (AUC), a standard metric for assessing the reliability of outlier detection confidence in LiDAR depth maps~\cite{conti22confidence, ilg2018uncertainty}, as shown in \Tref{tab:noise_filter}. 
For evaluation, we apply the outlier filtering algorithm to each component: the sparse depth map from a synchronized single frame and the accumulated semi-dense depth map before processing the accumulation.
Each filtered depth map is evaluated against the sparse and semi-dense ground truth, derived from the manually processed semi-dense depth map in KITTI DC.
For measuring AUC, pixels with both single-frame sparse depth and accumulated semi-dense depth are sorted by confidence and removed incrementally. 
We define confidence as $|\hat{\mathbf{y}}_i-\mathbf{y_i}|$, normalized to a 0-1 range in each segment.
In \Tref{tab:noise_filter}, RMSE is calculated on the remaining pixels to draw a curve, with AUC (lower values indicate better performance) measuring outlier removal effectiveness.
Our prior-based outlier filtering algorithm outperforms the commonly used method that removes distant points as outliers using a shifting window~\cite{lopezrodriguez2020project, wong2021unsupervised} for both sparse and semi-dense depth.
Additionally, our approach outperforms in sparse depth outlier filtering and performs favorably on semi-dense depth maps compared to recent methods using learning-based confidence estimation for outlier removal.
Unlike these methods, which rely on in-domain training and may not be applicable to other datasets, our approach is more adaptable to any domain, \ie, zero-shot.
Figure \ref{fig:outlier} illustrates the importance of outlier filtering when using sparse depth supervision and demonstrates how effectively our prior-based outlier filtering detects these outliers.
\input{supp_tables/noise_filter}
