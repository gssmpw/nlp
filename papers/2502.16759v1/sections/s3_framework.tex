% \todo{change LE-Recsys to LR-Recsys}

\subsection{Problem Definition}  
\label{sec:framework_probdefn}

In industrial recommendation platforms, the catalog typically contains millions or billions of items \citep{covington2016deep}. The goal of a recommender system is to select a few relevant items from the catalog in real-time based on the consumer and their current context, then present the results in a ranked list. This is inherently a combinatorial problem as the complexity grows exponentially with the size of the catalog; therefore, solving it directly in real-time is infeasible for large-scale platforms. To address this, a common approach in the industry is to use a greedy algorithm, where each item is assigned an individual ranking score, and the final ranking is determined by the ordering of these scores \citep{liu2009learning}.\endnote{Such greedy solutions have error lower bound guarantees compared to an oracle combinatorial optimization solution \citep{ailon2007efficient, balcan2008robust}, and conveniently reduce the complexity from combinatorial ($\mathcal{O}(n!)$) to log-linear ($\mathcal{O}(n\log{}n)$), making them feasible for real-time, large-scale systems.}

We use $i$ to index consumers and $j$ to index products, while $\vz$ denotes the context (such as time of day, day of the week, or location). The individual ranking score is typically the predicted value of an ideal outcome $y$ (e.g., a click, like, or purchase) that the platform aims to maximize.\endnote{$y$ can also be a multi-dimensional vector when multiple objectives are considered. In these cases, a combination of objectives is used to generate the ranking score. See \citet{wang2024recommending} and \citet{rafieian2024multiobjective} for examples.} When consumer $i$ visits the platform in context $\vz$, the recommender system uses a predictive ML model to estimate the real-time value of $\hat{y}$ for every product $j$ in the catalog:
\begin{equation}
  \label{eqn:ml_recsys}
  \hat{y}(i,j,\vz) = f(\vx_i, \vx_j, \vx_{ij}, \{j_{i_1},...,j_{i_n}\}, \vz),
\end{equation}
where $\vx_{i}$ represents consumer-level features, $\vx_{j}$ represents product-level features, and $\vx_{ij}$ captures the interaction history between consumer $i$ and product $j$. Figure \ref{fig:illustration_baseline} illustrates a typical industrial recommender system. We adopt a state-of-the-art sequential recommender system, which leverages advanced sequence modeling techniques like Transformers \citep{vaswani2017attention} to encode a consumer's sequential consumption history ${j_{i_1},...,j_{i_n}}$ as input features. The function $f(\cdot)$ can be any ML model selected by the developer. Products in the catalog are ranked in descending order of $\hat{y}(i,j,\vz)$ and presented to the consumer accordingly.

% , where the predicted outcome can be either continuous (e.g., a rating) or binary (e.g., click, like, or purchase)
% https://lucid.app/lucidchart/82615591-42af-41ab-aece-f38722fb6512/edit?beaconFlowId=B68358B117CEDB7D&invitationId=inv_a8d3e58d-e4cb-47d9-a270-6fbb9761f10e&page=0_0#
% https://lucid.app/lucidchart/fcc974c4-0c5c-4be3-a763-290c65c3a701/edit?beaconFlowId=C0FFA896091483C2&invitationId=inv_bc226156-f41d-4d94-9e32-a3bd48a4564d&page=0_0
\begin{figure}[hbtp!]
% \vspace{-0.3cm}
    \begin{subfigure}[b]{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/illustration_baseline.png}
        \caption{A typical recommendation framework.}
        \label{fig:illustration_baseline}
    \end{subfigure}
    \hfill
    % \hspace{4mm}
     \centering
    \begin{subfigure}[b]{0.53\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/illustration_framework.png}
        \caption{LR-Recsys (detailed architecture in Fig.\ref{fig:llm_recsys_diagram}).}
        \label{fig:illustration_framework}
    \end{subfigure}
    \caption{Comparison between our proposed framework, LR-Recsys, and a typical recommender system.}
  \label{fig:comparison}
 \vspace{-0.4cm}
\end{figure}

\subsection{Motivation: Leveraging LLMs for Explanation Generation}
\label{sec:framework_LLM_reasoning}
The predictive ML model in Fig.\ref{fig:illustration_baseline} is black-box in nature. In other words, it is hard to extract the reasons behind a consumer's choices—such as why a consumer likes or dislikes a product—or provide \emph{explanations} for its own predictions, like why it assigns a high score to product A and a low score to product B. % As a result, the black-box recommender systems in use today may fail to fully leverage the information in the training data to effectively learn and predict the outcomes.  

Inspired by the recent advances in the reasoning capabilities of large language models (LLMs) \citep{brown2020language, lingo2024enhancing, wei2022chain}, we test whether LLMs are able to generate explanations for why a consumer may or may not like a product. In a toy example, we asked OpenAI's latest GPT-4 model \citep{achiam2023gpt} to give reasons on why a consumer might or might not purchase an orange juice product, given her previous purchase history. We included (potentially irrelevant) details for the given product, such as packaging and logo, to understand how LLMs reason through relevant and less relevant information for purchase decisions. 

As illustrated in Fig.\ref{fig:example_explanations}, GPT-4 successfully generated plausible explanations for both positive and negative outcomes. For example, the positive explanation, ``because the consumer regularly buys fruits including oranges, and the product is an orange juice'', highlights the connection between orange juice and her existing preference for oranges (Fig.\ref{fig:pos_explanation}). The negative explanation, ``because the consumer may prefer whole fruits over processed juices'', points out the difference between oranges and orange juice and the consumer's preference for whole fruits (Fig.\ref{fig:neg_explanation}). % Other less relevant product details, such as being in a paper box or having a sun in the logo, do not appear in the explanations.\endnote{It is possible that the hypothetical consumer's purchase decision for the orange juice in this toy example could be influenced by its packaging or logo. However, a more plausible explanation is that the consumer's preference is based on the orange juice itself. The strength of LLMs lies in their ability to identify the most plausible reasons to explain a consumer's choices.}

These explanations provided by LLMs are natural and intuitive to consumers, yet they are not explicitly captured by current recommender systems. This motivates us to directly incorporate such LLM-generated explanations in recommender systems to facilitate their learning, by augmenting the input with these LLM-generated reasons behind the consumers' choices. 

In addition, both positive and negative explanations provide valuable information that would otherwise be difficult for a traditional recommender system to capture. The positive explanation identifies connections between a user's underlying preference and the given product, while the negative explanation pinpoints the differences. These links would typically require thousands of examples for a traditional recommender system to learn, whereas LLMs can accurately identify them with even zero-shot prompting, thanks to their reasoning capabilities built through vast pre-training and post-training.
  

% For example, the positive explanation highlights the connection between oranges and orange juice (both being orange-based products), while the negative explanation points out the distinction (orange juice being a processed product) and suggests the consumer may prefer whole fruits. 

These insights motivate us to design a recommender system with a built-in reasoning component, powered by LLMs, that generates both positive and negative explanations for every $(consumer, product)$ pair to facilitate the model learning. We now provide a high-level overview of our proposed framework, followed by a detailed description of each component in the framework.


\begin{figure}[hbtp!]
% \vspace{-0.4cm}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pos_explanation.png}
        \caption{Example of a positive explanation.}
        \label{fig:pos_explanation}
    \end{subfigure}
    \hfill
    % \hspace{4mm}
     \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/neg_explanation.png}
        \caption{Example of a negative explanation.}
        \label{fig:neg_explanation}
    \end{subfigure}
    \caption{A toy example for positive and negative explanations by GPT-4.}
  \label{fig:example_explanations}
% \vspace{-0.9cm}
\end{figure}

\subsection{Overview of the Proposed Framework}
\label{sec:framework_overview}
We propose \textbf{LR-Recsys}, an \textbf{L}LM-\textbf{R}easoning-Powered \textbf{Rec}ommender \textbf{Sys}tem, that explicitly incorporates a reasoning component into the design of a deep neural network (DNN)-based recommendation framework. Specifically, we first task LLMs with generating explanations for why a favorable outcome (e.g., a like or a purchase) may or may not occur for each sample in the training data. We then encode these explanations as embeddings using a fine-tuned text auto-encoder, and incorporate these embeddings into a DNN component for predicting consumer preference. Figure \ref{fig:illustration_framework} illustrates the high-level concept of LR-Recsys and highlights its differences from traditional recommendation models as in Fig.\ref{fig:illustration_baseline}. % It is important to note that we rely on the reasoning capabilities of LLMs here, \emph{not} their world knowledge. While our experiments show that augmenting product information with LLM-generated profiles can further boost performance, the main performance gains of our framework come from leveraging the reasoning capabilities of LLMs.

Figure \ref{fig:llm_recsys_diagram} presents the complete ML architecture of our proposed LR-Recsys framework, building on the concept illustrated in Fig.\ref{fig:illustration_framework}. Specifically, we design an \emph{contrastive-explanation generator} that takes the consumer's sequential consumption history and a candidate product as input, and outputs embeddings that represent the positive and negative explanations for why the consumer may or may not like the candidate product, given her past consumption history. These embeddings are then concatenated with other consumer, product, and contextual features to form the input layer. The input layer is passed through additional Transformer or MLP layers to create a deep neural network (DNN) recommendation component, which ultimately predicts the outcome for the $(consumer, product)$ pair. During training, the DNN recommendation component dynamically learns how positive and negative explanations contribute to the model prediction by adjusting the neural network weights of the corresponding explanation embeddings through back-propagation. We delve into each of these components in Sections \ref{sec:framework_explanation_generator} to \ref{sec:framework_llm_profile} below.
\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/llm_recsys_diagram_contrastive.png}
    \caption{(Color online) Detailed architecture of LR-Recsys.}\label{fig:llm_recsys_diagram}
    % \vspace{-0.2in}
\end{figure}


% [old] https://lucid.app/lucidchart/e619058b-1583-4f5d-a821-803e280a6a79/edit?invitationId=inv_40fc6fb6-e822-4534-a4e5-5f3a40382ae5&page=0_0#
% [new] https://lucid.app/lucidchart/69b07707-052b-4463-a543-6db060dc5676/edit?beaconFlowId=1D7CCDA0264F69F1&invitationId=inv_a52296c4-9ee5-44a3-ab23-281b47e3b747&page=0_0 

% \minminc{I would move section 3.5 before 3.4, make section 3.5 an overview of the architecture, with the high-level design idea to put all the information (user, product, history, and llm generated explaination into a consistent form, which is the embeddings), make it more brief, maybe removing some of the equations, and then explain section 3.4 and 3.6. put more energy into section 3.4 since that's the meat of the work.}
% To follow up with Minmin 

\subsection{Contrastive-explanation generator}
\label{sec:framework_explanation_generator}
The goal of the contrastive-explanation generator is to generate positive and negative explanation embeddings for every training sample, which will be used as input to the DNN recommendation component. As discussed in Section \ref{sec:framework_LLM_reasoning}, we leverage LLMs' reasoning capabilities to provide such explanations. 

The architecture of the contrastive-explanation generator is highlighted in the ``Contrastive-Explanation Generator'' box in Fig.\ref{fig:llm_recsys_diagram}. It contains two stages: a pre-trained LLM which produces positive and negative explanations in text in the first stage, and a fine-tuned AutoEncoder that encodes these explanations into embeddings in the second stage. We describe these two stages in detail below.

\subsubsection{Pre-trained LLM for generating textual explanations.}
\label{sec:framework_explanation_generator_pretrainllm}

In the first stage, the pre-trained LLM can be any LLM of choice, allowing us to leverage its general reasoning capabilities. Specifically, given consumer $i$'s consumption history, represented as a sequence of $n$ products $j_{i1}, j_{i2}, ..., j_{in}$, we prompt the LLM to generate explanations for why the consumer may or may not enjoy a candidate product. We refer to these as \emph{positive explanations} and \emph{negative explanations}.

The prompts for positive and negative explanations follow this template: 
\begin{quote}
\emph{
``Provide a reason for why this consumer purchased (or did not purchase) this product, based on the provided profile of the past products the consumer purchased, and the profile of the current product. Answer with exactly one sentence in the following format: `The consumer purchased (or did not purchase) this product because the consumer ... and the product ...'.'' }
\end{quote} 
The prompts can be adapted to fit specific domains; for example, ``purchased this product'' can be changed to ``watched this movie''. The prompt is then concatenated with the consumer's consumption history, which is presented as a sequence of product profiles, and the candidate product profile. These product profiles can range from simple product names available in the observed training data (e.g. ``orange'') to more detailed profiles generated by LLMs for further augmentation, as later described in Section \ref{sec:framework_llm_profile}. 

Detailed examples of the prompts used in our experiments are provided in Section \ref{sec:results}. % The textual explanations generated by the pre-trained LLMs are then fed into a fine-tuned AutoEncoder that converts them into embeddings, which we describe below.

 
% [purchased this product / watched this movie / stayed at this hotel], based on the provided profile of the past [products / movies / hotels] the consumer [purchased / watched / stayed at], and the profile of the current [product / movie / hotel]. Answer with exactly one sentence in the following format: `The consumer [purchased this product / watched this movie / stayed at this hotel] because the consumer ... and the hotel ....' ." 


\subsubsection{Fine-tuned AutoEncoder for converting textual explanations into embeddings.}
\label{sec:framework_explanation_generator_autoencoder}

In the second stage, the generated explanations from the first stage are used to train a fine-tuned AutoEncoder, which encodes both the positive and negative explanations into embeddings. An AutoEncoder, introduced by \citet{hinton2006reducing}, is a type of neural network that learns compressed, efficient representations of data. Like Principal Component Analysis (PCA), it reduces dimensionality while retaining essential features, but it can capture non-linear relationships with the neural network architecture, making it more powerful for complex data. It consists of two main components: an encoder, which compresses the input into a low-dimensional latent space, and a decoder, which reconstructs the input from this compressed representation. Figure \ref{fig:autoencoder} illustrates the concept of an AutoEncoder. The network is trained through back-propagation to minimize reconstruction error between the input data and reconstructed data, ensuring that the learned latent representations capture the essential features. The output of the encoder is used as the low-dimensional representation, or the embedding.

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/autoencoder.png}
    \caption{(Color online) AutoEncoder.}\label{fig:autoencoder}
   %  \vspace{-0.2in}
\end{figure}

In the context of text data, AutoEncoders map sentences to lower-dimensional embeddings, where semantically similar sentences are represented by embeddings close to each other in the latent space, while dissimilar sentences are further apart. Given an explanation as the input text sequence \( \vz = \{\vz_1, \dots, \vz_T\} \), where \( T \) is the sequence length, and a reconstructed sequence \( \hat{\vz} = \{\hat{\vz}_1, \dots, \hat{\vz}_T\} \) which is the output of the decoder in Fig.\ref{fig:autoencoder}, our AutoEncoder is trained to minimize the reconstruction loss, which is defined as the cross-entropy (CE) loss between the input sequence $\vz$ and the reconstructed sequence $\hat{\vz}$:
\begin{equation}
\label{eq:reconstruction_loss_pop}
\mathcal{L}_{\text{CE}}(\vz, \hat{\vz}) = - \sum_{t=1}^{T} \log p(\hat{\vz}_t = \vz_t | \vz),
\end{equation}
where $p(\hat{\vz}_t = \vz_t | \vz)$ is the predicted probability of the correct word $\vz_t$ at position $t$. In our case, the fine-tuned AutoEncoder is trained using the generated explanations from the first stage (Section \ref{sec:framework_explanation_generator_pretrainllm}) to encode them into embeddings, with the goal of ensuring that similar explanations are represented by similar vectors in the embedding space. For $N$ training examples, we have $N$ positive explanations $\{\bm{zp}^1, \dots, \bm{zp}^N\}$, and $N$ negative explanations $\{\bm{zn}^1, \dots, \bm{zn}^N\}$. Therefore, the reconstruction loss is computed as 
\begin{equation}
\label{eq:reconstruction_loss_sample}
\mathcal{L}_{\text{recon}}  = \sum_{k=1}^N \left(\mathcal{L}_{\text{CE}}(\bm{zp}^k, \hat{\bm{zp}}^k) + \mathcal{L}_{\text{CE}}(\bm{zn}^k, \hat{\bm{zn}}^k)\right) 
\end{equation}


The detailed model architecture and training of the AutoEncoder are provided in the Appendix \ref{appen:autoencoder}. The ``bottleneck'' layer of the AutoEncoder, highlighted in blue in Fig.\ref{fig:autoencoder} and denoted as $AE(\cdot)$, serves as the low-dimensional embedding for the textual explanations. We used a dimension of 8 as the embedding size. 

Note that rather than using embeddings directly from the LLM, we adopt a separate fine-tuned AutoEncoder to transform the textual explanations into embeddings. This is because we want the embeddings to only focus on differentiating between explanations \emph{within} the subspace of all relevant explanations for the current application, rather than ``wasting'' their capacity on distinguishing explanations from unrelated text in the full universe of texts. Therefore, a fine-tuned AutoEncoder trained exclusively on explanations is a more efficient solution for this task. Another option would be to fine-tune the LLM directly on the generated explanations. However, this approach is significantly more resource-intensive than training a smaller, separate AutoEncoder. For instance, fine-tuning the LLaMA-3-8B model using Low-Rank Adaptation (LoRA) \citep{hu2021lora}, a popular Parameter-Efficient Fine-Tuning (PEFT) method, involves 1,703,936 trainable parameters \citep{ye2024lola}. In contrast, our fine-tuned AutoEncoder has only 52,224 trainable parameters.
% 8*50*64 + 64*8 + 8*64 + 64*8*50 
% This approach allows for better differentiation between the explanations using the learned embeddings without requiring distinctions between explanation texts and unrelated texts. In other words
Using our fine-tuned AutoEncoder, the positive and negative explanations generated by the LLMs are encoded into two distinct embeddings—one for each explanation. These embeddings are then concatenated with consumer, product, and contextual features to form the input layer of the DNN-based recommendation component, which we describe in the following section. 


\subsection{DNN recommendation component}
\label{sec:framework_dnn}  

As illustrated in the ``DNN Recommendation Component'' in Fig.\ref{fig:llm_recsys_diagram}, the DNN recommendation component concatenates the following input to form the input layer of a deep neural network: positive and negative explanation embeddings, consumer embedding, product embedding, sequential history embedding, and contextual features. The input layer then goes through a neural network to generate the final output, which is the predicted likelihood of outcome (e.g., CTR). We introduce each part in detail below. 

\subsubsection{Consumer and product embeddings.} 
\label{sec:framework_dnn_cons_embed}  
We represent each consumer and product as low-dimensional embeddings, which are designed to capture similarities among consumers and products. These embeddings are constructed under the assumption that similar consumers like similar products \citep{breese1998empirical}. Traditionally, these embeddings in recommender systems are derived using matrix factorization \citep{dhillon2021modeling, wang2024recommending}. However, with the success of deep learning techniques, embeddings are now frequently learned via \emph{embedding table lookup}, which we briefly describe below.

Two separate embedding tables are initialized: one for consumers (denoted as $\mathbf{E}_{c}$) and one for products (denoted as $\mathbf{E}_{p}$). Each row in the consumer embedding table corresponds to a unique consumer ID, and each row in the product embedding table corresponds to a unique product ID. For each interaction between a consumer $i$ and a product $j$, their corresponding embeddings are retrieved from the embedding tables:
\begin{equation}
\label{eq:embed}
\mathbf{\ve}^{(c)}_i = \text{Lookup}(\mathbf{E}_{c}, i), \quad \mathbf{\ve}^{(p)}_j = \text{Lookup}(\mathbf{E}_{p}, j),
\end{equation}
where $\text{Lookup}(\mathbf{E}, i)$ refers to taking the $i$-th row of the matrix $\mathbf{E}$, $\mathbf{\ve}^{(c)}_i$ represents the embedding for consumer $i$, and $\mathbf{\ve}^{(p)}_j$ represents the embedding for product $j$. These embeddings are used as the input to the neural network to represent consumer $i$ and product $j$. During training, these embeddings are randomly initialized and are updated through back-propagation to minimize the loss:
\begin{equation}
\label{eq:embedding_update}
\mathbf{E}_{c} \leftarrow \mathbf{E}_{c} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{E}_{c}}, \quad \mathbf{E}_{p} \leftarrow \mathbf{E}_{p} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{E}_{p}},
\end{equation}
where $\eta$ is the learning rate, and the loss $\mathcal{L}$ measures the deviation between the predicted outcome (e.g. predicted probability of click) and the actual outcome (e.g. actual clicks) as will be detailed in Section \ref{sec:framework_dnn_loss} below. This process is repeated iteratively during training to improve the embeddings, capturing latent consumer preferences and product characteristics.

\subsubsection{Sequential history embedding.}
\label{sec:framework_dnn_seq_embed} 
The consumer's consumption history is represented as a sequence of $n$ products $\{j_{i1}, j_{i2}, ..., j_{in}\}$. Such sequences cannot be directly consumed by a recommender system which only takes numerical values as inputs. Inspired by recent developments in advanced sequence modeling in the deep learning literature \citep{vaswani2017attention}, we propose to combine two popular sequential modeling architectures to encode the sequence into a \emph{sequence embedding}, a vector of numerical values that can be jointly learned alongside the other parameters of the neural network. 

The sequence embedding should ideally encode two types of information. The first is the \textbf{local and temporal} dependency between consecutive consumed products in the sequence. For example, the consumer may purchase a Nintendo Switch first and, subsequently, several digital Nintendo games in a row. The second type of information is the \textbf{global} relationships within a sequence, allowing each consumption to attend to all other consumptions \emph{regardless} of their distance in the sequence. For example, the consumer may purchase products from a niche category only once in a while (e.g., luxury watches), and we would like the resulting sequence embedding to capture such long-distance relationships. 

% Figure \ref{fig:local_global} illustrates the concepts of local and global attention.
%\begin{figure}[hbtp!]
%    % \vspace{-0.1in}
%    \centering
%    \includegraphics[width=0.6\linewidth]%{figures/local_global.png}
%    \caption{(Color online) Illustration of temporal and %global dependencies in a sequence. for clarity, only %global relationships for $j_4$ are %shown.}\label{fig:local_global}
%     \vspace{-0.2in}
%\end{figure}
We propose to leverage two techniques in the recent sequence modeling research to capture both types of dependencies in the consumer's consumption history. Specifically, the self-attention model used in Transformers \citep{vaswani2017attention} is designed to capture global dependencies; The Gated Recurrent Unit (GRU) \citep{cho2014learning} as a type of recurrent neural networks (RNNs) is known to capture local and temporal dependencies well. %Given the importance of both global and local temporal dependencies in understanding a consumer's sequential consumption history, it is natural to combine self-attention and GRU to leverage the strength of both techniques.
We combine both modules, i.e., a self-attention layer followed by a GRU layer as described below, to form the sequence embedding.\endnote{A similar approach is used by \citet{li2020purs}, but their focus is on incorporating item-level heterogeneity in sequences, rather than capturing both types of dependencies.} 

\subsubsection*{Self-attention layer.}
The self-attention mechanism enables each item in a sequence to attend to all other items, effectively capturing global dependencies regardless of their spatial position within the sequence. This is achieved by computing attention weights that quantify the influence of each element.

For consumer $i$'s consumption history $(j_{i1}, j_{i2}, ..., j_{in})$, we transform this sequence into a series of embeddings. Specifically, we retrieve each product's embedding from the embedding table described in Section \ref{sec:framework_dnn_cons_embed}. The consumption history is represented as $\mathbf{S} = [\vs_1, \vs_2, \dots, \vs_n]$, where each $\vs_l$ is the embedding of the $l$-th product in the sequence. $\vs_l$ is obtained through the lookup operation: $\vs_{l} = \text{Lookup}(\mathbf{E}_{p}, j_{il}), \forall l = 1,\dots,n$. For simplicity, we omit the consumer index $i$ in this notation.

For each input \(\mathbf{s}_l\), we compute \textit{query}, \textit{key}, and \textit{value} using learned weight matrices \(\mathbf{W}^Q\), \(\mathbf{W}^K\), and \(\mathbf{W}^V\):
\begin{equation}
\label{eq:qkv}
\mathbf{Q} = \mathbf{S} \mathbf{W}^Q, \quad \mathbf{K} = \mathbf{S} \mathbf{W}^K, \quad \mathbf{V} = \mathbf{S} \mathbf{W}^V,
\end{equation}
where $\mathbf{Q} = [\mathbf{q}_1, \mathbf{q}_2, \dots, \mathbf{q}_n],  \mathbf{K} = [\mathbf{k}_1, \mathbf{k}_2, \dots, \mathbf{k}_n], \mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n]$. The for each query vector \(\mathbf{q}_l\), compute the \emph{attention score} with each key vector \(\mathbf{k}_j\):
\begin{equation}
\label{eq:score_lj}
\text{score}_{lj} = \frac{\mathbf{q}_l \cdot \mathbf{k}_j^T}{\sqrt{d_k}}
\end{equation}
where $d_k$ is the dimensionality of the key vectors. The value of $\text{score}_{lj}$ can be viewed as the degree of ``attention'' that the $l$-th item in the sequence should give to the $j$-th item. A softmax function is applied to normalize the attention scores into attention weights so that they sum up to 1:
\begin{equation}
\label{eq:alpha_lj}
\alpha_{lj} = \frac{\exp(\text{score}_{lj})}{\sum_{j=1}^{n} \exp(\text{score}_{lj})}.
\end{equation}
The output for each query vector is the weighted sum of the value vectors:
\begin{equation}
\label{eq:z_l}
\mathbf{z}_l = \sum_{j=1}^{n} \alpha_{lj} \mathbf{v}_j
\end{equation}
Thus, the output of the self-attention layer is $\mathbf{Z} = [\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_n]$, which is a matrix that represents the consumer's sequential consumption history, accounting for global dependencies among the consumptions.

\subsubsection*{GRU layer.} The Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) designed to model sequences by processing them one step at a time, making them naturally suited for capturing local temporal dependencies and sequential order. GRUs are highly effective in tasks that require maintaining memory over time. Temporal dependencies are managed through its two gating mechanisms: the update gate $\mathbf{u}_t$, which controls how much of the past information is retained at time $t$, and the reset gate $\mathbf{r}_t$, which determines how much of the previous state is forgotten at time $t$. These mechanisms are detailed below. 

The output $\mathbf{Z} = [\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_n]$ from the self-attention layer is passed through the GRU, which processes the sequence step-by-step to compute the hidden states at each time step $t$. Let the first output of the GRU layer be the first vector from the self-attention layer: $\mathbf{h}_1 = \mathbf{z}_1$. Then starting from $t = 2$, the output of the GRU layer $\rvh_t$ depends on $\rvh_{t-1}$ and $\rvz_t$ through a reset gate $\mathbf{r}_t$ and an update gate $\mathbf{u}_t$:
\begin{equation}
\label{eq:reset_gate}
\mathbf{r}_t = \sigma(\mathbf{W}_r \mathbf{z}_t + \mathbf{U}_r \mathbf{h}_{t-1} + \mathbf{b}_r), 
\end{equation}
\begin{equation}
\label{eq:update_gate}
\mathbf{u}_t = \sigma(\mathbf{W}_u \mathbf{z}_t + \mathbf{U}_u \mathbf{h}_{t-1} + \mathbf{b}_u),
\end{equation}
where $\mathbf{W}_r$, $\mathbf{U}_r$, $\mathbf{W}_u$, and $\mathbf{U}_u$ are learnable weight matrices and $\mathbf{b}_r$ and $\mathbf{b}_u$ are learnable vectors. $\mathbf{r}_t$ controls how much of the previous hidden state \(\mathbf{h}_{t-1}\) to forget; $\mathbf{u}_t$ determines how much of the previous hidden state should be carried forward. The candidate hidden state $\tilde{\mathbf{h}}_t$ is then calculated as 
\begin{equation}
\label{eq:cand_hid_state}
\tilde{\mathbf{h}}_t = \tanh(\mathbf{W}_h \mathbf{z}_t + \mathbf{U}_h (\mathbf{r}_t \odot \mathbf{h}_{t-1}) + \mathbf{b}_h),
\end{equation}
which incorporates the reset gate to adjust the influence of the past state. Here $\odot$ denotes the element-wise product, and $\mathbf{W}_h$, $\mathbf{U}_h$ and $\mathbf{b}_h$ represent learnable weight matrices and vector respectively. Finally, the new hidden state \(\mathbf{h}_t\) is updated by interpolating between the previous hidden state \(\mathbf{h}_{t-1}\) and the candidate hidden state $\tilde{\mathbf{h}}_t$ using the update gate $\mathbf{u}_t$: 
\begin{equation}
\label{eq:new_hid_state}
\mathbf{h}_t = \mathbf{u}_t \odot \mathbf{h}_{t-1} + (1 - \mathbf{u}_t) \odot \tilde{\mathbf{h}}_t.
\end{equation}
After processing the entire sequence in \(\mathbf{Z}\) one by one, 
the output of the GRU layer is the sequence of hidden states $\mathbf{H} = [\mathbf{h}_1, \mathbf{h}_2, \dots, \mathbf{h}_n]$. We use the embedding of the end hidden state, $\mathbf{h}_n$ as the final embedding for the consumer's sequential consumption history, which encapsulates \emph{both} global dependencies from the self-attention layer and temporal dependencies captured by the GRU. 

\subsubsection{Model architecture and loss function.}
\label{sec:framework_dnn_loss}

As shown in the ``DNN Recommendation Component'' in Fig.\ref{fig:llm_recsys_diagram}, the input layer consists of the following six components: the explanation embeddings from the contrastive-explanation generator ($AE(\bm{zp}^k) and AE(\bm{zn}^k)$ in Section \ref{sec:framework_explanation_generator}), denoted as $E_{\text{pos}}$ and $E_{\text{neg}}$; the consumer and product embeddings retrieved through embedding table lookup ($\mathbf{\ve}^{(c)}_i$ and $\mathbf{\ve}^{(p)}_i$ in Section \ref{sec:framework_dnn_cons_embed}), denoted as $E_c$ and $E_p$; the sequence embedding obtained from the self-attention and GRU layers ($h_n^k$ in Section \ref{sec:framework_dnn_seq_embed}), denoted as $E_{\text{seq}}$; and any contextual features relevant to the consumer's decision-making process, such as location, time of day and day of the week etc., denoted as $E_{\text{context}}$. 

To account for the potential interactions among these inputs, we added another self-attention layer to the concatenated input $X_{\text{input}} = [E_{\text{pos}}, E_{\text{neg}}, E_c, E_p, E_{\text{seq}}, E_{\text{context}}]$ to allow each element attend to each other. This is similar to what's described in Section \ref{sec:framework_dnn_seq_embed}, with the only difference being that each element is now an input type (e.g. positive explanation embedding) instead of a product. Let $I = [\text{pos}, \text{neg}, c, p, \text{seq}, \text{context}]$ be the index set corresponding to each element in the input $X_{\text{input}}$. Then similar to Eq.(\ref{eq:qkv})-(\ref{eq:z_l}), the output corresponding to each element is a weighted combination of all elements in $I$:
$$
X_{\text{self-attn}} = [\sum_{i \in I} \alpha_{\text{pos}, i} \vv_i, \sum_{i \in I} \alpha_{\text{neg}, i} \vv_i, \sum_{i \in I} \alpha_{c, i} \vv_i, \sum_{i \in I} \alpha_{p, i} \vv_i, \sum_{i \in I} \alpha_{seq, i} \vv_i, \sum_{i \in I} \alpha_{\text{context}, i} \vv_i],
$$
where, for example, $\alpha_{\text{pos}, c}$ is the attention weight from the positive explanation embedding to the consumer embedding, $\vv_i$ is the learned value vector for each element $i$. See Appendix \ref{appen:self_attn_2} for the mathematical details for obtaining the attention weights and value vectors. 

With the self-attention layer, the input layer $X_{\text{input}}$ is transformed into $X_{\text{self-attn}}$ with the same dimension. The transformed input $X_{\text{self-attn}}$ is then passed through several layers of a multi-layer perceptron (MLP) followed by ReLU activation functions, which is one of the most commonly used architectures for deep neural networks. The final output layer consists of a single neuron with a sigmoid activation function $\sigma(z) = \frac{1}{1 + e^{-z}}$ that outputs a probability score $\hat{y}$ between 0 and 1. In our settings, the probability represented the model's prediction of a positive outcome (e.g., click, like, or purchase) for the $(consumer, product)$ pair.

 %To account for the heterogenous impact of these embedding components during the recommendation process, we add an attention layer on top of these embeddings, and we use the attention values, which we denote as $\{\alpha_{positive},\alpha_{negative},\alpha_{consumer},\alpha_{product},\alpha_{sequence},\alpha_{context}\}$ to construct the weighted concatenated embeddings of the input layer as $W_{concatenate}=[\alpha_{positive}*W_{positive};\alpha_{negative}*W_{negative};\alpha_{consumer}*W_{consumer};\alpha_{product}*W_{product};\alpha_{sequence}*W_{sequence};\alpha_{context}*W_{context}]$. The attention values are determined following the self-attention mechanism \cite{shaw2018self} that we have previously described:
% \begin{equation}
% \alpha_{i} = \frac{exp(e_{i})}{\sum_{t}exp(e_{t})}
% \end{equation}
% where $e_{i}$ represents the average Euclidean distance between each embedding component and other embedding components. The concatenated embeddings of the input layer $W_{concatenate}$ are

The model is trained using binary cross-entropy loss, defined as 
\begin{equation}
\label{eq:cross_entropy_loss}
\mathcal{L}(y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})],
\end{equation}
where \(y\) is the true label (e.g., click, like, or purchase) and $\hat{y}$ is the DNN model's prediction. For a batch of $N$ samples, the average loss is \(\mathcal{L}_{\text{batch}} = \frac{1}{N} \sum_{k=1}^{N} \mathcal{L}(y_k, \hat{y}_k)\). During training, gradients of the loss are computed and used to update the model's trainable parameters via back-propagation. In the DNN component, the trainable parameters include the weight matrices from the self-attention and GRU layers, the consumer and product embedding tables, the self-attention layer, and the MLP layers. 

\subsection{LLM for profile augmentation}
\label{sec:framework_llm_profile}
The framework described in Fig.\ref{fig:llm_recsys_diagram} relies on the \emph{reasoning} capabilities of LLMs. One of the most powerful techniques to improve the reasoning capabilities of LLMs is through Chain-of-Thought (CoT) prompting \citep{wei2022chain}, where the LLMs are encouraged to break down the process of a complex task and ``think step-by-step''. For example, when solving a multi-step arithmetic problem, the LLM is prompted to explain each calculation step before arriving at the final answer, thereby mirroring the natural problem-solving approach used by humans and enhancing the model's ability to handle complex tasks. Motivated by this, we design a CoT-like technique for the explanation generation process: Instead of directly asking the pre-trained LLM for explanations, we first ask it to generate a richer profile of the product based on the name of the product, and then use these LLM-generated profiles to augment the prompt used to generate the final explanations. This is aligned with the ``think step-by-step'' strategy used in CoT: the first step involves obtaining a better understanding of the products through profile augmentation, and the second step leverages these detailed profiles to generate meaningful explanations.

In particular, for every product in the consumer's consumption history, we first ask the LLMs to create a product profile based on the name of the product. An example prompt is the following:
\begin{quote}
\emph{``Create a succinct profile for a product based on its name. This profile should be tailored for use in recommendation systems and must identify the types of consumers who would enjoy the product''.}
\end{quote} 
The word ``product'' can be changed to fit specific product categories, such as ``restaurant'' or ``hotel'' depending on the dataset. These LLM-generated profiles are then used to augment the sequential consumption history of the consumer. Specifically, the consumption history can be represented as a sequence of these augmented profiles to form the input to the contrastive-explanation generator described in Section \ref{sec:framework_explanation_generator_pretrainllm}.

The LLM-generated profile is illustrated as the dotted arrow (``CoT profile augmentation (optional)'') in Fig.\ref{fig:llm_recsys_diagram}. As an example, here is the LLM-generated profile given the prompt above and the product name ``JW Marriott Hotel Hong Kong``:

\begin{quote}
\emph{`` Revitalize body, mind, and spirit when you stay at the 5-star JW Marriott Hotel Hong Kong. Located above Pacific Place, enjoy the views over Victoria Harbour, the mountains, or the glittering downtown Hong Kong skyline.''.}
\end{quote} 

We see that LLMs are indeed capable of augmenting product profiles by generating detailed and contextually rich descriptions, drawing from their vast world knowledge. Given minimal product information, such as a name or category, LLMs can infer and provide additional attributes, such as product features, typical uses, or consumer sentiment, based on similar items from their training data. This world knowledge allows LLMs to enrich product profiles with insights that would not be captured from standard datasets alone.

\noindent \paragraph{\textbf{Remark 1.}} Note that these LLM-generated profiles in Section \ref{sec:framework_llm_profile} are an \emph{optional} component to our framework, as the LLMs can provide meaningful reasons even when minimal product information (e.g. product name) is given. In our experiments in Section \ref{sec:results}, we conduct ablation studies on this optional chain-of-thought component to demonstrate the extra value it added. As a preview of the results in Section \ref{res_understanding_reasoning}, when the augmented profile information is provided to the pre-trained LLM, the contrastive-explanation generator is able to provide slightly better explanations that help the whole system even more. This is aligned with the insights from Chain-of-Thought (CoT) prompting \citep{wei2022chain} where asking the LLMs to ``think step-by-step'' can further improve their reasoning capabilities. However, the primary performance improvement of LR-Recsys still comes from the inherent reasoning capabilities of the LLMs, as LR-Recsys significantly outperforms baseline models even without the LLM-generated profile information.


\subsection{End-to-End Training Process for LR-Recsys}
\label{sec:framework_training}  

Putting everything together, the complete LR-Recsys framework is illustrated in Fig.\ref{fig:llm_recsys_diagram} and detailed as Algorithm \ref{algo:le_recsys} below. The contrastive-explanation generator serves as a pre-training component that is trained before the DNN recommendation component. The output is then fed into the DNN recommendation component as its input. The trainable parameters of the DNN recommendation component include the self-attention and GRU layers, the consumer and product embeddings, and the MLP layers. The specific trainable parameters for each component of the framework and their respective roles are enumerated in Table \ref{tab:trainable} below.

\begin{table}[hbtp!]
  \centering
  \footnotesize
  \setlength\extrarowheight{4pt}
  \setlength{\tabcolsep}{10pt} % Adjust column separation for better spacing
  \begin{tabular}{llp{7cm}}
    \toprule
    \textbf{Component} & \textbf{Trainable Parameters} & \textbf{Role} \\
    \midrule
    Explanation Generator & Fine-tuned AutoEncoder & Encodes explanations into embeddings. \\
    \midrule
    \multirow{5}{*}{\shortstack[l]{DNN Recommendation \\ Component}} 
      & Self-attention and GRU layer & Encodes sequential consumption history. \\ 
      & Consumer embedding table & Encodes consumer preferences. \\ 
      & Product embedding table & Encodes product characteristics. \\ 
      & Self-attention layer &  Captures attention weights among different inputs. \\ 
      & Multi-layer perceptron layers (MLPs) & Captures nonlinearity and feature interactions. \\
    \bottomrule
  \end{tabular}
  \caption{Trainable parameters within each component of the LR-Recsys framework.}
  \label{tab:trainable}
% \vspace{-0.2in}
\end{table}

For the DNN recommendation component, we utilize the input of $N$ observations, $\mathcal{D} = \{(\vx_k, y_k)\}_{k=1}^N$, where $\vx_k$ represents all input features, and $y_k$ represents the outcome (e.g. click, purchase), which serves as the label for training. Detailed information about the model architecture and hyperparameters such as batch size, learning rate, and training epoch is provided in the Appendix \ref{appen:model_architecture_dnn}. The output is a prediction of the outcome (e.g. like, purchase) for each $(consumer, product)$ pair, given real-time contextual features. During a recommendation session, products are ranked in descending order based on the predicted outcomes. 

\begin{algorithm}
\caption{LLM-Explanation-Powered Recommender System (LR-Recsys)}\label{algo:le_recsys}
\label{algo1}
\begin{algorithmic}[1]
   \REQUIRE dataset $\mathcal{D} = \{(\vx_k, y_k)\}_{k=1}^N$, pre-trained LLM, learning rate $\eta$, number of epochs $E$, batch size $B$
   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   %%%%%%%%%%%%%Pre training%%%%%%%%%%%%%%%%
   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   \hspace*{-1.5\algorithmicindent} \textbf{\emph{Explanation Generator}:}
   \FOR{$(x_k, y_k) \in \mathcal{D}$}                     
        \STATE \textbf{CoT profile augmentation (optional)}: Use LLMs to enrich product profiles for the sequential consumption history $j_1,...,j_n$ and the candidate product;
        \STATE \textbf{Preparing prompts}: Concatenating the positive and negative explanation prompts with sequential consumption history and candidate product; 
        \STATE \textbf{Explanation generation}: Generate $\bm{zp}^k$ (positive explanation) and $\bm{zn}^k$ (negative explanation) using the pre-trained LLM;
        \STATE \textbf{Reconstruction Loss}: Compute  $\mathcal{L}_{\text{CE}}(\bm{zp}^k, \hat{\bm{zp}}^k)$ and $\mathcal{L}_{\text{CE}}(\bm{zn}^k, \hat{\bm{zn}}^k) $ as in Eq.(\ref{eq:reconstruction_loss_pop}).
   \ENDFOR  

   \STATE \textbf{Fine-tuned AutoEncoder Training}: Train the AutoEncoder $AE(\cdot)$ by minimizing the reconstruction loss: $\mathcal{L}_{\text{recon}} = \sum_{k=1}^N \left(\mathcal{L}_{\text{CE}}(\bm{zp}^k, \hat{\bm{zp}}^k) + \mathcal{L}_{\text{CE}}(\bm{zn}^k, \hat{\bm{zn}}^k) \right) $. 

   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   %%%%%%%%%%%%%DNN reccommendation%%%%%%%%%%%%%%%%
   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   \hspace*{-1.5\algorithmicindent} \textbf{\emph{DNN Recommendation Component}:}
   \STATE \textbf{Initialize:} Trainable parameters: embedding table $\mathbf{E} = (\mathbf{E}_{c}$, $\mathbf{E}_{p})$, self-attention layer $\theta_{\text{attn}} = (\mathbf{W}^Q$, $\mathbf{W}^K$, $\mathbf{W}^V$, $\mathbf{W}_r)$, GRU layer $\theta_{\text{GRU}} = (\mathbf{U}_r$, $\mathbf{b}_r$, $\mathbf{W}_u$, $\mathbf{U}_u$, $\mathbf{b}_u$, $\mathbf{W}_h$, $\mathbf{U}_h$, $\mathbf{b}_h)$ and MLP layers $\theta_{\text{MLP}}$
   
   \FOR{epoch = 1 to $E$} 
        \STATE Shuffle dataset $\mathcal{D}$   
        \FOR{each batch $b = 1$ to $\left\lfloor \frac{N}{B} \right\rfloor$} 
            \STATE Select batch $\mathcal{B}_b = \{(\vx_k, y_k)\}_{k=(b-1)B+1}^{bB}$;   \COMMENT{batch training}
            \FOR{each $(\vx_k, y_k) \in \mathcal{B}_b$}
                \STATE Get positive and negative explanation embeddings $AE(\bm{zp}^k)$ and $AE(\bm{zn}^k)$;
                \STATE Get consumer and candidate product embedding $\mathbf{\ve}^{(c)}_i$ and $\mathbf{\ve}^{(p)}_i$ as in Eq.(\ref{eq:embed});
                \STATE Get sequential consumption history embedding $h_n^k$ with the self-attention and GRU layers; 
                \STATE Concatenate the above and any contextual features $c$ as the input layer of the DNN recommendation component: $\vx_k = (AE(\bm{zp}^k), AE(\bm{zn}^k), \mathbf{\ve}^{(c)}_i, \mathbf{\ve}^{(p)}_i, h_n^k, c)$; 
                \STATE Compute prediction: $\hat{y}_k = f_\theta(\vx_k)$;
            \ENDFOR
            \STATE Compute batch loss: $\mathcal{L}_b = \frac{1}{B} \sum_{(\vx_k, y_k) \in \mathcal{B}_b} \mathcal{L}(y_k, \hat{y}_k)$
            \STATE Update all trainable parameters: $\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}_b$, where $\theta = (\mathbf{E}, \theta_{\text{attn}}, \theta_{\text{GRU}}, \theta_{\text{self-attn}}, \theta_{\text{MLP}})$
        \ENDFOR
    \ENDFOR    
   \RETURN $\theta$   \COMMENT{all parameters of the DNN recommendation component}
\end{algorithmic}
\end{algorithm}

\noindent \paragraph{\textbf{Remark 2.}} Our LR-Recsys framework is compatible with any pre-trained LLM, and its performance gains mainly come from leveraging the \emph{reasoning abilities} of the LLMs, rather than their dataset-specific knowledge or summarization skills. To validate this, we conducted several experiments. First, we tested older LLMs, such as GPT-2, which were trained on data \emph{predating} the datasets in our experiments, ensuring that the LLMs have \emph{no} prior knowledge of the specific data. Our framework still provides gains in these cases. Second, we evaluated LLMs with varying reasoning capabilities and found that models with stronger reasoning consistently produced better results. Finally, we explored an alternative approach that used LLMs to generate summaries of consumption history, but this method yielded inferior performance compared to when the LLMs were asked to provide reasons. The details of these experiments are outlined in Section \ref{sec:results}. Overall, these findings confirm that the main advantage of our LR-Recsys framework is its ability to leverage the reasoning capabilities of LLMs rather than their dataset knowledge or summarization skills.


% Next in Section \ref{sec:theory}, we provide theoretical asymptotic analysis based on statistical learning theories to shed lights on the theoretical advantage of LR-Recsys. Specifically, challenging the common belief that explainability hurts model performance, we demonstrate how integrating explanations into the training process of a ML framework can actually improve its performance.

% statistical insights to shed light on why LLMs' reasoning capability can help the recommender system. To understand why LLMs' explanations can simultaneously improve predictive performance and explainability, rather than creating a trade-off, in the next section, we provide theoretical insights using high-dimensional statistical learning theory. 


\begin{comment}
==== 

Details in Section \todo{Experiments}

While our experiments show that augmenting product information with LLM-generated profiles can further boost performance, the main performance gains of our framework come from leveraging the reasoning capabilities of LLMs.

is agnostic to the choice of LLMs. To demonstrate that only the reasoning capability of the LLM is used here, not their knowledge about the dataset itself, we conduct experiments (1) on older versions of LLMs (e.g. GPT-2 trained on data up to 2019) whose training data are before the generate of the experiment dataset. (2) We also show that LLMs with better reasoning capabilities generate better results, which confirms that it is the reasoning capability that we leveraged to generate the win. (3) Experiments ablating the LLM-generated profiles also work well.

In our experiments \todo{add experiment to confirm}, we find that more powerful LLMs generate better results (because of their better reasoning capabilities). 


The framework described in Fig.\ref{fig:llm_recsys_diagram} only relies on the \emph{reasoning} capabilities of LLMs. Another strength of LLMs is their \emph{world knowledge} thanks to their vast training data. For example, given minimal product information, such as the name of the product, LLMs can infer and provide additional attributes, such as product features, typical uses, or consumer sentiment, based on similar items from their training data. This world knowledge allows LLMs to enrich product profiles with insights that would not be captured from standard datasets alone. 

Motivated by this, we also explored an \emph{optional} component which leverages LLMs to augment the profiles of the products to serve as the input to our framework. In particular, for every product in the consumer's consumption history, we ask the LLMs to create a product profile based on the name of the product. An example prompt is the following:
\begin{quote}
``Create a succinct profile for a product based on its name. This profile should be tailored for use in recommendation systems and must identify the types of consumers who would enjoy the product''.
\end{quote} 
The word ``product'' can be changed to fit specific product categories such as ``restaurant'' or ``hotel'' depending on the dataset. In Appendix \todo{LLM for profile augmentation}, we present the examples of the LLM's output. These LLM-generated profiles can be used to augment the sequential consumption history of the consumer. Specifically, the sequential consumption history can be represented as a sequence of these augmented profiles to form the input to the Explanation Generator as described in Section \ref{sec:framework_explanation_generator_pretrainllm}. 

Note that these LLM-generated profiles are an \emph{optional} component to our framework, as the LLMs can provide meaningful reasons even when minimal product information (e.g. product name) is given. In our experiments, we conduct ablation studies on this optional component to demonstrate the extra value it added. In particular, when the augmented profile information is provided to the pre-trained LLM, the explanation generator is able to provide better explanations that help the whole system even more. This is an interesting observation as both the explanation generator and the profile augmentation is done by the same pre-trained LLM. By explicitly asking for a more detailed product profile, the LLM is able to provide more helpful explanations. This is similar to the famous chain-of-thought (CoT) prompting technique \citep{wei2022chain} where asking the LLM to ``think step-by-step'' can greatly improve its reasoning capabilities. 

While our experiments show that augmenting product information with LLM-generated profiles can further boost performance, the main performance gains of our framework come from leveraging the reasoning capabilities of LLMs.


% Our DNN recommendation component can be viewed as an augmentation of a state-of-the-art DNN-based recommender system. 




====================

Specifically, given consumer $i$'s consumption history, represented as a sequence of $n$ products $j_{i1}, j_{i2}, ..., j_{in}$, we prompt the LLM to provide explanations for why the consumer may or may not enjoy a candidate product. We refer to these as \emph{positive explanations} and \emph{negative explanations}.

The prompt follows this format: ``Provide a reason for why this consumer [purchased this product / watched this movie / stayed at this hotel], based on the provided profile of the past [products / movies / hotels] the consumer [purchased / watched / stayed at], and the profile of the current [product / movie / hotel]. Answer with exactly one sentence in the following format: `The consumer [purchased this product / watched this movie / stayed at this hotel] because the consumer ... and the hotel ....' "


Given specific datasets, can be adjusted to tailor to the specific domain.  

(include both pos and neg, because both provide perspectives on why and why not, and let the DNN model figure out which is helpful for learning)


(pos / neg prompt: depending on the specific dataset, the prompt used is "define why the consumer consumes a certain product) 

(also introduce fine-tuned autoEncoder here, detail in appendix )

=================

% Google diagram: https://docs.google.com/document/d/1b5uQ2OpeMLUgPv4OptpaxCyRQrqD_pTzifAWs4pMyWI/edit

\end{comment}