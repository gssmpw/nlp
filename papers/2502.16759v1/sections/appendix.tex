%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Appendix 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Details of LR-Recsys Framework}

\subsection{Details of the Pre-Trained AutoEncoder}
\label{appen:autoencoder}

The Pre-Trained AutoEncoder as illustrated in Fig.\ref{fig:autoencoder} has three hidden layers of size (64, 8, 64), where the layer of dimension 8 corresponds to the bottleneck layer marked in blue, and serves as the embedding $AE(\cdot)$ of the encoded text. 

The training data of the AutoEncoder contains all the positive and negative textual explanations generated by the LLM. Each word in the sentence is mapped to its unique word id, which is then mapped to an 8-dimensional latent vector using a randomly-initialized embedding table which will be updated during training. The sentence will be represented by flattening the latent embeddings of each word id. The max sequence length is capped at $maxlen = 50$ (i.e. each explanation sentence is no more than 50 words). Padding will be applied if the explanation sentence is less than 50 words. Therefore, the input and output dimension of the AutoEncoder is $8*50$. 

The training objective is the \emph{reconstruction error} of each input sentence, measured by corss-entropy loss: 
\begin{equation}
\label{eq:reconstruction_loss_ae}
\mathcal{L}_{\text{recon}} = - \sum_{k=1}^{n} \sum_{l=1}^{maxlen} \sum_{v \in V} \mathbf{1}(y_{k,l}=v) \log \frac{e^{w_{k,l,v}}}{\sum_{v\in V}e^{w_{k,l,v}}},
\end{equation}
where $n$ is the number of sentences (explanations), $V$ is the whole vocabulary, $\mathbf{1}(y_{k,l}=v)$ is the indicator of whether the $l$-th word in the $k$-th sentence is word $v$, $w_{k,l,v}$ is the last layer's output for the $l$-th word in the $k$-th sentence for word $v$, which goes through a softmax layer to get the final predicted probability $\frac{\exp{w_{k,l,v}}}{\sum_{v\in V}\exp{w_{k,l,v}}}$.

The training starts with a randomly initialized embedding table for the whole vocabulary, which is updated by back-propagating the gradients computed from Eq.(\ref{eq:reconstruction_loss_ae}). We used a learning rate of 0.1 and batch size of 128, and trained the AutoEncoder for 100 epochs. After training, the output of the intermediate layer (marked in blue in Fig.\ref{fig:autoencoder}), $AE(\cdot)$, which is of dimension 8, is used as the final embedding for each positive and negative explanation. 

Our framework is also compatible with more advanced AutoEncoder architectures, such as Variational AutoEncoders (VAEs) \citep{kingma2013auto} or Transformer-based encoder-decoder architectures \citep{devlin2019bert}. However, in practice, we found that a simple multi-layer perceptron (MLP) architecture, as described above, performed well while requiring minimal trainable parameters compared to these alternatives. Therefore, we selected this MLP-based design for the AutoEncoder in our contrastive-explanation generator.

\subsection{Details of the Self-Attention Layer in the DNN recommendation component}
\label{appen:self_attn_2}

Each element in the input layer $X_{\text{input}} = [E_{\text{pos}}, E_{\text{neg}}, E_c, E_p, E_{\text{seq}}, E_{\text{context}}]$ is an 8-dimensional vector\endnote{For the context features, we first apply a multi-layer perceptron (MLP) to transform it into a 8-dimensional vector.}, which is viewed as one item in the sequence for a standard self-attention layer. We construct three learned matrices for \emph{query}, \emph{key} and \emph{value}: \(\mathbf{W}^Q\), \(\mathbf{W}^K\), and \(\mathbf{W}^V\):
\begin{equation}
\label{eq:qkv_2}
\mathbf{Q} = X_{\text{input}} \mathbf{W}^Q, \quad \mathbf{K} = X_{\text{input}} \mathbf{W}^K, \quad \mathbf{V} = X_{\text{input}} \mathbf{W}^V.
\end{equation}
Specifically, 
\begin{equation}
\begin{aligned}
\label{eq:qkv_2_expanded}
\mathbf{Q} &= [\vq_{\text{pos}}, \vq_{\text{neg}}, \vq_c, \vq_p, \vq_{\text{seq}}, \vq_{\text{context}}], \\
\mathbf{K} &= [\vk_{\text{pos}}, \vk_{\text{neg}}, \vk_c, \vk_p, \vk_{\text{seq}}, \vk_{\text{context}}], \\
\mathbf{V} &= [\vv_{\text{pos}}, \vv_{\text{neg}}, \vv_c, \vv_p, \vv_{\text{seq}}, \vv_{\text{context}}].
\end{aligned}
\end{equation}

Then for any $i, j \in I = [\text{pos}, \text{neg}, c, p, \text{seq}, \text{context}]$, the attention score $\text{score}_{ij}$, the normalized attention weight $\alpha_{ij}$, and the output for each element i, $\vz_i$, are computed using the same formula in Eq.(\ref{eq:score_lj})-(\ref{eq:z_l}), with the dimensionality $d_k = 8$. The final output of the self-attention layer is then 
\begin{equation}
\begin{aligned}
X_{\text{self-attn}} &= [\vz_{\text{pos}}, \vz_{\text{neg}}, \vz_c, \vz_p, \vz_{\text{seq}}, \vz_{\text{context}}] \\
&= [\sum_{i \in I} \alpha_{\text{pos}, i} \vv_i, \sum_{i \in I} \alpha_{\text{neg}, i} \vv_i, \sum_{i \in I} \alpha_{c, i} \vv_i, \sum_{i \in I} \alpha_{p, i} \vv_i, \\
& \quad \sum_{i \in I} \alpha_{seq, i} \vv_i, \sum_{i \in I} \alpha_{\text{context}, i} \vv_i],
\end{aligned}
\end{equation}
which accounts for the interactions among each input element. 

\subsection{Model Architecture and Hyperparameters of the DNN Recommendation Component}
\label{appen:model_architecture_dnn}

After the input layer and the self-attention layer described in Appendix \ref{appen:self_attn_2}, the model passes through a multi-layer perceptron (MLP) with layers of size [64, 8, 1]. A sigmoid activation function, $\sigma(z) = \frac{1}{1 + e^{-z}}$, is applied at the output layer. The DNN recommendation component was trained using a learning rate of 0.01, a batch size of 128, and for 100 epochs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Appendix 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Details of Theoretical Results}

\subsection{Conditions of Lemma \ref{lem:var_selection_ellis}.}
\label{appen:lemma_1_cond}

There are three sets of conditions required for Theorem 4.5 in \citet{fan2023environment}. We summarize them below.

(1) Conditions on input data and noise level: The first set is Conditions 4.1-4.6 in \citet{fan2023environment} which are conditions on the input data and the noise level commonly seen in high-dimensional statistical learning theories. 

(2) Value of $\gamma$: The second set is on the value of $\gamma$ which controls the strength of the invariance regularization $\hat{J}(\vbeta)$ in Eq.(\ref{eqn:eills_obj}). Specifically, the regularization should be strong enough such that $\gamma \geq 3 (\kappa_L)^{-3} \sup_S (b_S/\bar{d}_S) $, where $b_S = \ltwonorm{\frac{1}{|\mathcal{E}|} \sum_{e\in\mathcal{E}} \mathbb{E}[\epsilon^{(e)}\vx_S^{(e)}] }^2$, $\bar{d}_S = \sum_{e\in\mathcal{E}} \frac{1}{|\mathcal{E}|} \ltwonorm{\vbeta^{(e,s)} - \bar{\vbeta}^{(S)}}^2$, with $\bar{\vbeta}^{(S)} = \frac{1}{|\mathcal{E}|} \sum_{e'\in\mathcal{E}} \vbeta^{(e',s)}$,  and $\kappa_L, \kappa_U$ such that $\kappa_L \mathbf{I}_p \preceq \Sigma^{(e) \preceq \kappa_U \mathbf{I}_p} \; \forall e \in \mathcal{E}$. 

(3) Value of $\lambda$: The third set is on the value of $\lambda$ which controls the strength of the variable selection penalty $||\vbeta||_0$ in Eq.(\ref{eqn:eills_obj}). Specifically, the choice of $\lambda$ satisfies
\[
c_1 \left\{ \frac{(\gamma / \kappa_L)^2 s^* \log p}{n \cdot |\mathcal{E}|} + \epsilon(n) \right\}
\leq \lambda \leq c_2 \kappa_L \beta_{\text{min}}^2,
\]
where $\epsilon(n) = \frac{(\gamma / \kappa_L)^2 s^* (\log p)(s^* + \log p)}{n^2} + \frac{(\gamma / \kappa_L) \log p \sqrt{n^{-3} (s^* + \log p)}}{\sqrt{|\mathcal{E}|}}$. Here, $c_1, c_2$ are some universal positive constants depending only on $(C, \kappa_U, \sigma_x, \sigma_\varepsilon)$, and $\beta_{\text{min}}$ is the minimal nonzero coefficient. 


\subsection{Having more than one environment is necessary for identifying $\vbeta^*$ and $S^*$.}
\label{appen:more_than_one_env_needed}

\citet{fan2023environment} proved that having more than one environment is actually \emph{necessary} for identifying $\vbeta^*$ and $S^*$ for multi-environment learning. We summarize their findings as Lemma \ref{lem:need_more_than_one_env} below.

\begin{lem}
\label{lem:need_more_than_one_env}
Introducing multiple environments, i.e. $|\mathcal{E}| \geq 2 $, is necessary to identifying $\vbeta^*$ and $S^*$. Specifically, consider the population $L_2$ risk minimizer $\hat{\vbeta}$ in a single environment, defined as
$$ \hat{\vbeta} := \argmin_{\vbeta} \mathbf{E}_{\mu} [| \vbeta^T \vx - y |^2]. $$
It is \textbf{not} necessarily true that $\text{supp}(\hat{\vbeta}) = \text{supp}(\vbeta^*) = S^*$. 
\end{lem}

For proof of Lemma \ref{lem:need_more_than_one_env}, see Proposition 2.2 of \citet{fan2023environment} for a counterexample such that $\text{supp}(\hat{\vbeta}) \neq S^*$. Lemma \ref{lem:need_more_than_one_env} implies that running a regression on the data from one environment may \emph{not} be able to estimate $S^*$ well. Instead, it may include some \emph{spurious variables}. These variables are spurious since incorporating them in the prediction model may increase the prediction performance in the current training data, but the associations between these variables and $y$ are unstable and can lead to much worse prediction performances when there is any distribution change in the environment. This is especially true in recommender system settings where consumer preferences are dynamic and constantly evolving \citep{wang2024going}. LLMs, as discussed above, can obtain a much more robust estimate of $S^*$ due to the vast data and numerous environments they have seen.


\subsection{Convergence results for general nonlinear and ML models.}
\label{appen:multi_env_nonlinear}

For the genearl nonlinear learning scenario, \citet{gu2024causality} proposes a generalized version of Eq.(\ref{eqn:eills_obj}):
$$
\min_{g \in \mathcal{G}} \max_{f^{(e)} \in \mathcal{F}_g, \, \forall e \in \mathcal{E}} 
\underbrace{\sum_{e \in \mathcal{E}} \mathbb{E}_{\mu^{(e)}} \left[\ell(Y, g(X))\right]}_{\mathcal{R}(g)} 
+ \gamma \underbrace{\sum_{e \in \mathcal{E}} \mathbb{E}_{\mu^{(e)}} \left[\{Y - g(X)\} f^{(e)}(X) - \{f^{(e)}(X)\}^2 / 2 \right]}_{\mathcal{J}(g, \{f^{(e)}\}_{e \in \mathcal{E}})}.
$$
where $g(\cdot)$ is the nonlinear regression function (e.g. a neural network), $f^{(e)}$ is an environment specific nonlinear regression function to construct the penalty term, which can be viewed as a hyperparameter. Proposition 10 of \citet{gu2024causality} states that when the number of observations in each environment is sufficiently large, the estimator $g(\cdot)$ can identify a set of variables $\hat{S}$ from the total set $S$ such that, with probability approaching 1, $\hat{S}$ includes all true important variables and excludes all spurious variables. 

Therefore, we have established that even when the regression functions implicitly used by LLMs are complex and nonlinear, they can still accurately identify the set of true important variables $S^*$. This ability is again attributed to the vast amount of data and diverse environments encountered during their training.

How does the knowledge of $S^*$ improves the convergence rate for the nonlinear models (e.g. deep neural networks)? \citet{schmidt2020nonparametric} demonstrated that the minimax convergence rate for a deep neural network with $n$ observations and $d$ variables (predictors) is $O_P(n^{-2/(2+d)})$. Therefore without the knowledge of $S^*$, a standard ML model trained on $p$ predictors would have an error rate of $O_P(n^{-2/(2+p)})$. In contrast, with the knowledge of $S^*$—as provided by LLM-generated explanations—the oracle ML model effectively learns from only $s^*$ predictors and achieves an error rate of $O_P(n^{-2/(2+s^*)})$. Since $s^* \ll p$, this suggests that incorporating LLMs' explanations (i.e. the knowledge of $S^*$ can significantly boost the learning efficiency for general nonlinear ML models. 

As an exmample, Fig.\ref{fig:convergence_rate_comparison_nonlinear} compares the convergence rates with a fixed number of observations ($n = 10000$) and a varying number of predictors ($p$), where the number of true important variables is $s^* = 20$. As expected, the oracle ML model significantly reduces the estimation error, with the reduction becoming larger for larger $p$.

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/convergence_rate_comparison_nonlinear.png}
    \caption{(Color online) Convergence rate comparison for the general ML model (unknown $S^*$) and Oracle ML model (known $S^*$).}\label{fig:convergence_rate_comparison_nonlinear}
    \vspace{-0.2in}
\end{figure}


\begin{comment}
For general ML, 
I. [https://arxiv.org/pdf/2405.04715 prop 10] variable selection results (when n is large enough, $\hat{S}$ include all true important variable, and does not include all spurious variable. 

II. Page 20 in https://arxiv.org/pdf/1708.06633 $n^(-2beta / (beta + p))$

[In the end, Remark 3: mention that this is different from forcing explainability in model training, because we don't usually know a priori what are the important factors, as a result forcing the models to follow those rules will only decrease (rather than increase) their learning efficiency.]
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Appendix 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Experimental Details and Results}


\subsection{HyperParameter Settings}
\label{appen:hyper_param}
To ensure a fair comparison in our experiments, we use the Grid Search method \citep{bergstra2011algorithms} and allocate an equal amount of effort, in terms of training time and memory usage, to identify the optimal hyperparameters for both our proposed approach and all baseline models. Consequently, we determine the following experimental settings for our proposed LR-Recsys framework:
\begin{itemize}
\item \textbf{Large Language Model}: The prompts used for the contrastive-explanation generator and profile augmentation in LR-Recsys are listed in Appendix \ref{appen:prompts}. We utilize the open-source LLama 3.1 \citep{meta2024introducing} as the LLM in our experiments. To demonstrate the flexibility and robustness of our approach, we also evaluate its performance with other LLMs in Section \ref{res_understanding_reasoning}.

% \item \textbf{Pre-Trained Autoencoder Model}: we construct it as a 3-layer MLP with hidden size [8*$maxlen$,64,8,8*$maxlen$,$maxlen$] in the input layer, 3 hidden layers, and the output layer respectively. $maxlen$ is a hyperparameter that refers to the maximum number of words in the generated explanation, which will be used to apply the padding technique \citep{sutskever2014sequence} in order to align the text dimensions in our model inputs. We select $maxlen$ as 50, since only less than 1\% of the generated explanations contain more than 50 words for all three datasets. Each word in the explanations will be first mapped to its unique word ID, and then mapped to an 8-dimensional latent embedding corresponding to the word ID using an initialized embedding table, which will be further updated during the pre-training process. The entire explanation text will be represented by flattening these latent word embeddings, and will be used to train the autoencoder model, whose goal is to reconstruct the exact same explanation text in the output. The autoencoder model is optimized by minimizing the loss function, which is the sum of the cross-entropy loss of each word in the output, so that we can push the autoencoder outputs and its explanation text inputs to become closer to each other. During the pre-training process, we select the learning rate as 0.1, the batch size as 128, and the number of epochs as 100. Another important thing to note is that we only used the training data, and not the test data, to conduct the autoencoder pre-training process, in order to avoid the information leakage problem during the evaluation stage.

\item \textbf{Pre-Trained Autoencoder Model}: The autoencoder is constructed as a 3-layer MLP with a hidden size configuration of [8*$maxlen$, 64, 8, 8*$maxlen$, $maxlen$] for the input layer, three hidden layers, and the output layer, respectively. The hyperparameter $maxlen$ represents the maximum number of words in the generated explanations. To align text dimensions in the model inputs, padding \citep{sutskever2014sequence} is applied, and $maxlen$ is set to 50, as fewer than 1\% of explanations exceed 50 words across all datasets. Each word in the explanations is mapped to a unique word ID and subsequently transformed into an 8-dimensional latent embedding via an initialized embedding table, which is further learned and updated during pre-training. The entire explanation text is represented by flattening these word embeddings, serving as input to the autoencoder. The autoencoder is trained to reconstruct the original explanation text by minimizing a loss function as the cross-entropy loss for each word in the output, thereby aligning the input and output representations. During pre-training, we use a learning rate of 0.1, a batch size of 128, and train for 100 epochs. Importantly, only the training data is used for pre-training to prevent information leakage during evaluation.

\item \textbf{Recommendation Model}: The recommendation model predicts the outcome (either the actual rating for a regression task or high vs. low rating for a classification task) for a candidate item using inputs that include user ID, item ID, user sequential behaviors, and LLM-generated explanations. User and item IDs are mapped to 8-dimensional latent vectors through an initialized embedding table, which are learned and updated during training. Sequential behaviors are aggregated using a Self-Attentive GRU model (Section \ref{sec:framework_dnn_seq_embed}) with one hidden layer of size 8. 

The inputs—consisting of 8-dimensional embeddings for positive explanations, negative explanations, user ID, item ID, and sequential behaviors—are concatenated via the attention layer described in Section \ref{sec:framework_dnn_loss} and passed through three fully connected layers with hidden sizes [64, 8, 1] to produce the predicted outcome. For regression tasks (i.e. predicting actual rating), the model is optimized by minimizing the mean squared error (MSE) between predicted and ground-truth ratings. For classification tasks (i.e. predicting high vs. low rating), the model is optimized by minimizing the binary cross-entropy loss between predicted and ground-truth ratings. We use a learning rate of 0.01, a batch size of 128, and train the model for 100 epochs.

% \item \textbf{Recommendation Model}: it takes the inputs of user ID, item ID, user sequential behaviors, and LLM-generated explanations to predict the rating of a candidate item. We map both the user and item IDs into an 8-dimensional latent vector using an initialized embedding table, which will be later updated during the recommendation process. We aggregate the sequence of the last 5 purchased items using the Self-Attentive GRU model that we described in Section 3, with 1 hidden layer and a hidden size of 8. These input embeddings (8-dimensional positive explanation embedding, 8-dimensional negative explanation embedding, 8-dimensional user ID embedding, 8-dimensional item ID embedding, and 8-dimensional sequential embedding) are concatenated through the attention layer that we described in Section 3, and fed into three fully-connected layers with hidden size [64,8,1] to produce the predicted rating of the candidate item. We optimize the entire recommendation model by minimizing the MSE error of the predicted ratings by comparing them with the ground-truth ratings, where the optimization process is conducted with a learning rate of 0.01, batch size of 128, and epoch of 100. 
\end{itemize} 


\subsection{Detailed Prompts}
\label{appen:prompts}

\subsubsection{TripAdvisor Dataset} 
\label{appen:prompt_tripadvisor} 
\hfill\\
(1) Profile Augmentation Prompt
\\[10pt]
\fbox{%
    \parbox{\textwidth}{%
[Instruction]

Create a succinct profile for a hotel based on its name. This profile should be tailored for use in recommendation systems and must identify the types of consumers who would enjoy the hotel.

[Example Input]

JW Marriott Hotel Hong Kong

[Example Output]

Revitalize body, mind, and spirit when you stay at the 5-star JW Marriott Hotel Hong Kong. Located above Pacific Place, enjoy the views over Victoria Harbour, the mountains, or the glittering downtown Hong Kong skyline.

[Input]

\{item\_text\}
    }%
}
\\[10pt]
(2) Positive Explanation Prompt
\\[10pt]
\fbox{%
    \parbox{\textwidth}{%
[Instruction]

Provide a reason for why this consumer stayed at the current hotel, based on the provided profile of the past hotels the consumer stayed at and the profile of the current hotel. Answer with exactly one sentence with the following format: "The consumer stayed at this hotel because the consumer ... and the hotel ..."

[Example Input]

Past Hotel Profiles: Peppers Gallery Hotel is a luxurious 5-star hotel located in the heart of Sydney, Australia. The hotel is housed in a historic building that has been beautifully restored to combine modern comfort with traditional elegance. The hotel's unique art collection and contemporary design make it a perfect choice for art lovers, couples, and business travelers seeking a luxurious and sophisticated experience. The hotel's central location allows guests to easily explore Sydney's famous landmarks and cultural attractions. Whether you're looking to relax and unwind or experience the best of Sydney, Peppers Gallery Hotel is the perfect choice.

Current Hotel Profile: Escape the hustle and bustle of Amsterdam and indulge in a luxurious stay at the Sheraton Amsterdam Airport Hotel and Conference Center. Conveniently located just minutes from Amsterdam Airport Schiphol, this 5-star hotel offers spacious rooms, a fitness center, and an on-site restaurant. Perfect for both business and leisure travelers seeking a comfortable and relaxing retreat.

[Example Output]

Explanation: The consumer stayed at this hotel because the consumer is traveling for business and the hotel is luxurious and conveniently located at the airport.

[Input]

Past Hotel Profiles: \{profile\_seq\}

Current Hotel Profile: \{recommended\_profile\}
    }%
}
\\[10pt]
(3) Negative Explanation Prompt
\\[10pt]
\fbox{%
    \parbox{\textwidth}{%
[Instruction]

Provide a reason for why this consumer did not stay at the current hotel, based on the provided profile of the past hotels the consumer stayed at and the profile of the current hotel. Answer with exactly one sentence with the following format: "The consumer did not stay at this hotel because the consumer ... and the hotel ..."

[Example Input]

Past Hotel Profiles: Peppers Gallery Hotel is a luxurious 5-star hotel located in the heart of Sydney, Australia. The hotel is housed in a historic building that has been beautifully restored to combine modern comfort with traditional elegance. The hotel's unique art collection and contemporary design make it a perfect choice for art lovers, couples, and business travelers seeking a luxurious and sophisticated experience. The hotel's central location allows guests to easily explore Sydney's famous landmarks and cultural attractions. Whether you're looking to relax and unwind or experience the best of Sydney, Peppers Gallery Hotel is the perfect choice.

Current Hotel Profile: Escape the hustle and bustle of Amsterdam and indulge in a luxurious stay at the Sheraton Amsterdam Airport Hotel and Conference Center. Conveniently located just minutes from Amsterdam Airport Schiphol, this 5-star hotel offers spacious rooms, a fitness center, and an on-site restaurant. Perfect for both business and leisure travelers seeking a comfortable and relaxing retreat.

[Example Output]

Explanation: The consumer did not stay at this hotel because the consumer is not interested in visiting Amsterdam or staying at an airport hotel and the hotel is located at the Amsterdam airport.

[Input]

Past Hotel Profiles: \{profile\_seq\}

Current Hotel Profile: \{recommended\_profile\}
    }%
}
\\[10pt]

\subsubsection{Yelp Dataset}
\label{appen:prompt_yelp}
\hfill\\
(1) Profile Augmentation Prompt
\\[10pt]
\fbox{%
    \parbox{\textwidth}{%
[Instruction]

Create a succinct profile for a restaurant based on its name. This profile should be tailored for use in recommendation systems and must identify the types of consumers who would enjoy the restaurant.

[Example Input]

Thrill Korean Steak and Bar

[Example Output]

Thrill Korean Steak and Bar brings a new concept to many people. We feature over 20 meat options to choose from that will be cooked at the table. Our fast-burning grills and our well-trained staff will bring an exceptional experience to many people.

[Input]

\{item\_text\}
    }%
}
\\[12pt]
(2) Positive Explanation Prompt
\\[12pt]
\fbox{%
    \parbox{\textwidth}{%
[Instruction]

Provide a reason for why this consumer visited the current restaurant, based on the provided profile of the last restaurant the consumer visited and the profile of the current restaurant. Answer with exactly one sentence with the following format: "The consumer visited this restaurant because the consumer ... and the restaurant ..."

[Example Input]

Last Restaurant Profile: Claim Jumper is a restaurant that offers a unique dining experience, known for its delicious food and friendly service. The menu features a variety of dishes that are sure to please any palate. Whether you are looking for a casual dining experience or a romantic dinner for two, Claim Jumper is the perfect choice.

Current Restaurant Profile: Feast Buffet is a hotel restaurant that offers a wide variety of international cuisine. The restaurant is perfect for families, couples, and business travelers who are looking for a delicious and affordable meal. Overall, Feast Buffet is a great option for anyone looking for a delicious and affordable meal in a comfortable and welcoming environment.

[Example Output]

Explanation: The consumer visited this restaurant because the consumer is looking for a romantic dining place and the restaurant offers a delicious and affordable dining experience for couples.

[Input]

Last Restaurant Profile: \{profile\_seq\}

Current Restaurant Profile: \{recommended\_profile\}
    }%
}
\\[12pt]
(3) Negative Explanation Prompt
\\[12pt]
\fbox{%
    \parbox{\textwidth}{%
[Instruction]

Provide a reason for why this consumer did not visit the current restaurant, based on the provided profile of the last restaurant the consumer visited and the profile of the current restaurant. Answer with exactly one sentence with the following format: "The consumer did not visit this restaurant because the consumer ... and the restaurant ..."

[Example Input]

Last Restaurant Profile: Claim Jumper is a restaurant that offers a unique dining experience, known for its delicious food and friendly service. The menu features a variety of dishes that are sure to please any palate. Whether you are looking for a casual dining experience or a romantic dinner for two, Claim Jumper is the perfect choice.

Current Restaurant Profile: Feast Buffet is a hotel restaurant that offers a wide variety of international cuisine. The restaurant is perfect for families, couples, and business travelers who are looking for a delicious and affordable meal. Overall, Feast Buffet is a great option for anyone looking for a delicious and affordable meal in a comfortable and welcoming environment.

[Example Output]

Explanation: The consumer did not visit this restaurant because the consumer is looking for a fine dinning experience and the restaurant offers only affordable and buffet options.

[Input]

Last Restaurant Profile: \{profile\_seq\}

Current Restaurant Profile: \{recommended\_profile\}
    }%
}
\\[12pt]

\subsubsection{Amazon Movie Dataset}
\label{appen:prompt_amazon}
\hfill\\
(1) Profile Augmentation Prompt
\\[12pt]
\fbox{%
    \parbox{\textwidth}{%
[Instruction]

Create a succinct profile for a movie based on the provided information. This profile should be tailored for use in recommendation systems and must identify the types of users who would enjoy the movie. Avoid repeating the given details directly and instead focus on describing the appeal and content of the movie in a way that highlights its potential audience:

[Example Input]
Title: Barefoot Contessa (with Ina Garten), Entertaining With Ina Vol. 2 (3 Pack): Brunch 'n' Lunch, Picnic Parties, Summer Entertaining

Brand: Ina Garten

Category: ['Movies \& TV', 'Movies']

[Example Output]

This series, hosted by Ina Garten, delves into crafting simple yet sophisticated dishes suitable for both daily meals and special events. With episodes ranging from brunch preparations to summer picnic essentials, it appeals to those who savor lifestyle and culinary content infused with a personal touch. Ideal for viewers who relish home cooking shows, seek practical entertaining tips, and aspire to refine their cooking skills under the guidance of a celebrated chef, this series is especially attractive. Enthusiasts of lifestyle and cooking channels, as well as individuals looking for actionable, inspirational ideas for social gatherings, will find this series to be a compelling addition to their viewing schedule. It is particularly suited for culinary aficionados and home chefs eager to inject innovation and style into their meal presentations and event planning.

[Input]

\{item\_text\}
    }%
}
\\[12pt]
(2) Positive Explanation Prompt
\\[12pt]
\fbox{%
    \parbox{\textwidth}{%
Given the profiles of the watching history of this consumer \{movie\_profile\_seq\}, can you provide a reason for why this consumer watched the following recommended movie with profile \{recommended\_movie\_profile\}? Answer with one sentence with the following format: The consumer watched this movie because...
    }%
}
\\[12pt]
(3) Negative Explanation Prompt
\\[12pt]
\fbox{%
    \parbox{\textwidth}{%
Given the profiles of the watching history of this consumer \{movie\_profile\_seq\}, can you provide a reason for why this consumer did not watch the following recommended movie with profile \{recommended\_movie\_profile\}? Answer with one sentence with the following format: The consumer did not watch this movie because...
    }%
}

\subsubsection{Prompts for variants of LR-Recsys in Section \ref{sec:role_pos_neg}}
\label{appen:variants_prompt}
\hfill\\
In this section, we present the prompts that are used to construct different variants of our proposed LR-Recsys in Table \ref{generation} of Section \ref{sec:role_pos_neg}. For simplicity reasons, we only list the prompts used for the Amazon dataset below, while the prompts used for the other two datasets are constructed in a similar fashion as we presented in Section \ref{appen:prompts}.

(1) Aspect Term Only
\\[12pt]
\fbox{%
    \parbox{\textwidth}{%
Given the profiles of the watching history of this consumer \{movie\_profile\_seq\}, can you generate a series of aspect terms that represent the most important properties of the candidate movie that the consumer might consider?
    }%
}
\\[12pt]
(2) Positive Explanation Prompt
\\[12pt]
\fbox{%
    \parbox{\textwidth}{%
Given the profiles of the watching history of this consumer \{movie\_profile\_seq\}, can you provide a reason for why this consumer watched the following recommended movie with profile \{recommended\_movie\_profile\}? Answer with one sentence with the following format: The consumer watched this movie because...
    }%
}
\\[12pt]
(3) Negative Explanation Prompt
\\[12pt]
\fbox{%
    \parbox{\textwidth}{%
Given the profiles of the watching history of this consumer \{movie\_profile\_seq\}, can you provide a reason for why this consumer did not watch the following recommended movie with profile \{recommended\_movie\_profile\}? Answer with one sentence with the following format: The consumer did not watch this movie because...
    }%
}
\\[12pt]
% (4) Purchasing History Summarization
% \\[12pt]
% \fbox{%
%     \parbox{\textwidth}{%
% Given the profiles of the watching history of this consumer \{movie\_profile\_seq\}, can you provide a summary of the consumer preference of candidate movies?
%     }%
% }
% \\[12pt]

\subsection{Attention Value Analysis on Positive and Negative Explanations}
\label{appen:attn_pos_neg}

Figure \ref{fig:attention_all} shows the distribution of attention values on positive explanations (${\bar{\alpha}}_{pos}$) and negative explanations (${\bar{\alpha}}_{neg}$) for all three datasets. Across all three datasets, we find that positive predictions rely more on positive explanations, and negative predictions rely more on negative explanations. 

\begin{figure}[hbtp!]
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/restaurant_positive_record.jpg}
        \caption{Positive examples (Yelp Dataset).}
        \label{fig:positive:yelp_appen}
    \end{subfigure}
    \hspace{0.5mm}
     \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/restaurant_negative_record.jpg}
        \caption{Negative examples (Yelp Dataset).}
        \label{fig:negative:yelp_appen}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/movie_positive_record.jpg}
        \caption{Positive examples (Amazon Movie Dataset).}
        \label{fig:positive:amazon_appen}
    \end{subfigure}
    \hspace{0.5mm}
     \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/movie_negative_record.jpg}
        \caption{Negative examples (Amazon Movie Dataset).}
        \label{fig:negative:amazon_appen}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hotel_positive_record.jpg}
        \caption{Positive examples (TripAdvisor Dataset).}
        \label{fig:positive:tripadvisor_appen}
    \end{subfigure}
    \hspace{0.5mm}
     \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hotel_negative_record.jpg}
        \caption{Negative examples (TripAdvisor Dataset).}
        \label{fig:negative:tripadvisor_appen}
    \end{subfigure}
    
   \caption{(Color online) Distribution of attention values on positive and negative explanations for positive and negative examples across three datasets.}
  \label{fig:attention_all}
\end{figure}

\subsection{Pie Charts on the Attention Values of Each Embedding Component}
\label{appen:attn_pie_charts}
We compute the attention values of each embedding component (namely user embeddings, item embeddings, context embeddings, positive explanation embeddings, and negative explanation embeddings) that we obtained following the approach described in Section~\ref{sec:attention_analysis}. We present the relative attention results as pie charts in Figure~\ref{fig:pie_charts}. We can see that both positive and negative explanation embeddings account for a significant proportion of total model attention values. Together, these explanations account for 39\%, 32\%, and 39\% of the total attention values on the Amazon Movie, Yelp Restaurant, and TripAdvisor Hotel datasets, respectively, highlighting the important role that LLM reasoning plays in improving model predictions.

\begin{figure}[hbtp!]
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/amazon_pie.jpg}
        \caption{Amazon Dataset.}
        \label{fig:pie_amazon}
    \end{subfigure}
    \hspace{0.5mm}
     \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hotel_pie.jpg}
        \caption{TripAdvisor Dataset.}
        \label{fig:pie_yelp}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/restaurant_pie.jpg}
        \caption{Yelp Dataset.}
        \label{fig:pie_tripadvisor}
    \end{subfigure}
   \caption{Pie Charts of Attention Values.}
  \label{fig:pie_charts}
\end{figure}


\subsection{Ablation Studies and Robustness Checks}
\label{appen:ablation_robustness}

To further validate the effectiveness and generalizability of LR-Recsys, we conduct a series of ablation studies and robustness checks. Specifically, we evaluate the following three ablation models:

\begin{itemize}
\item \textbf{Ablation Model 1 (Substitution of positive and negative explanation embeddings)}: This model replaces the positive and negative explanation embeddings in the input layer of the DNN recommendation model with free learnable parameters of the same dimension, removing the LLM-generated explanations. This setup tests whether the performance gains of LR-Recsys are due to the explanations themselves or merely the additional learnable parameters introduced by the explanation embeddings.

\item \textbf{Ablation Model 2 (Ablation of the fine-tuned AutoEncoder)}: In LR-Recsys, the generated textual explanations are processed through a fine-tuned AutoEncoder, which produces embeddings specifically tailored for the recommendation task. This variant removes the AutoEncoder and directly uses the raw embeddings produced by the LLM for the explanations, testing whether the AutoEncoder contributes to the improved performance of LR-Recsys.

\item \textbf{Ablation Model 3 (Alternative prompts for explanation)}: This model modifies the prompts used to generate explanations. The original prompts (detailed in Appendix \ref{appen:prompt_tripadvisor}–\ref{appen:prompt_amazon}) are replaced with the following alternative:
\textit{''Can you explain the reason why this consumer watches this movie with profile \{recommended\_movie\_profile\}, based on the profiles of previously watched movies \{movie\_profile\_seq\}? Answer with one sentence with the following format: The consumer watched this movie because ...''} 

\item \textbf{Robustness Check 1 (Alternative neural network architecture for recommendation)}: This model modified the`DNN Recommendation Component'' in Fig.\ref{fig:llm_recsys_diagram} with an alternative deep neural network architecture. In particular, we experimented with the neural collaborative filtering (NCF) architecture proposed by \citet{he2017neural}, where users and items are represented as learnable embeddings, and a multi-layer perceptron is applied to learn the user–item interaction function. Specifically, we employ three layers of Multi-Layer Perception (MLP) with hidden factors of [32, 16, 8] in each layer, respectively, without the attention mechanism. We have also made our best efforts to control the same number of trainable parameters when compared to the original version of our proposed model.
\end{itemize}

As shown in the second row of Table \ref{ablation} (``Ablation 1''), substituting positive and negative explanations with an equal number of learnable parameters results in significantly lower performance compared to our proposed model. This highlights the value of the LLM-generated explanations and confirms that the performance gains are not merely due to the additional model capacity introduced by the explanation embeddings.

The third row of Table \ref{ablation} (``Ablation 2'') illustrates the critical role of the Pre-Trained AutoEncoder in our model. By producing text embeddings tailored for the recommendation task, the AutoEncoder significantly improves performance. Removing it and relying solely on LLM-produced embeddings leads to a notable performance decrease.

Additionally, as shown in the fourth row of Table \ref{ablation} (``Ablation 3''), our model demonstrates robustness to alternative prompt settings. The performance metrics in the first and last row are not statistically significantly different, validating that the LLM's reasoning capability remains a key advantage of LR-Recsys, regardless of prompt variations.

Finally, in the last row of Table \ref{ablation} (``Robustness Check 1''),  our model demonstrates robustness to alternative neural network architecture for producing recommendations, as the performance of LR-Recsys with the alternative DNN architecture still outperforms all baseline methods in Table \ref{main_results}. Based on this, we confirm that the performance improvements of our proposed method indeed come from the generation of positive and negative explanations, rather than the neural network design for recommendations. 

\begin{table}
\centering
\footnotesize
   \setlength\extrarowheight{3pt}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|c|ccc|ccc|ccc|} \hline
 & \multicolumn{3}{c|}{TripAdvisor} & \multicolumn{3}{c|}{Yelp} & \multicolumn{3}{c|}{Amazon Movie} \\ \hline
 & RMSE $\downarrow$ & MAE $\downarrow$ & AUC $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & AUC $\uparrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & AUC $\uparrow$ \\ \hline
LR-Recsys (Ours) & \textbf{0.1889} & \textbf{0.1444} & \textbf{0.7289} & 0.2149 & \textbf{0.1685} & \textbf{0.7229} & 0.1673 & \textbf{0.1180} & \textbf{0.7500} \\
 & (0.0010) & (0.0008) & (0.0018) & (0.0010) & (0.0009) & (0.0017) & (0.0010) & (0.0009) & (0.0018) \\ \hline
Ablation Model 1 & 0.2044 & 0.1747 & 0.7022 & 0.2486 & 0.2120 & 0.6896 & 0.2144 & 0.1845 & 0.7053 \\
 & (0.0011) & (0.0008) & (0.0021) & (0.0011) & (0.0009) & (0.0018) & (0.0011) & (0.0009) & (0.0017) \\ \hline
Ablation Model 2 & 0.1930 & 0.1480 & 0.7233 & 0.2170 & 0.1710 & 0.7198 & 0.1701 & 0.1226 & 0.7463 \\
 & (0.0010) & (0.0008) & (0.0021) & (0.0010) & (0.0010) & (0.0021) & (0.0010) & (0.0010) & (0.0018) \\ \hline
Ablation Model 3 & 0.1895 & 0.1449 & 0.7281 & \textbf{0.2143} & 0.1690 & 0.7223 & 0.1679 & 0.1186 & 0.7493 \\
 & (0.0010) & (0.0008) & (0.0018) & (0.0010) & (0.0009) & (0.0017) & (0.0010) & (0.0009) & (0.0018) \\ \hline
Robustness Check 1 & 0.1901 & 0.1450 & 0.7271 & 0.2160 & 0.1691 & 0.7208 &\textbf{0.1662} & 0.1191 & 0.7484 \\
 & (0.0010) & (0.0008) & (0.0018) & (0.0010) & (0.0009) & (0.0017) & (0.0010) & (0.0009) & (0.0018) \\ \hline
\end{tabular}
}
\caption{Recommendation performance of the ablation models across three datasets.}
\label{ablation}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Appendix 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
