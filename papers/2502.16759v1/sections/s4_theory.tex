
% Why would incorporating LLM-generated explanations improve the performance of an ML recommender system? In this section, we provide mathematical insights into why and how explanations can help the ML models learn, challenging the common belief that explainability typically comes at a cost of model performance.

In this section, we provide mathematical insights into why and how explanations can help the ML models learn. In a typical machine learning setting, the model is trying to learn from a particular \emph{environment} which is characterized by the training data. The goal of the ML model is to predict the outcome $Y$ based on the input $X$, with $X$ typically being high-dimensional vectors characterizing the consumer, the product, and the context. By asking the LLMs to generate why and why not the consumer likes (or purchases) the product, we are asking the LLMs to extract the \emph{probable} decision-making factors that drive the observed outcome (e.g., like or purchase). Incorporating this knowledge in the model training process will intuitively increase the model's learning efficiency, and improve the model's performance given the same amount of training data. In the remainder of this section, we leverage high-dimensional statistical learning theory to mathematically characterize the gains provided by LLM-generated explanations.



% This kind of prediction requires only a strong correlation between the input and the outcome variable. In other words, the ML model tends to rely on correlation rather than causation to make predictions. As a result, they may not know which variables (i.e. which dimensions of $X$) are truly important ones that capture true causal factors in predicting $Y$, but instead rely on the whole high-dimensional set of all available variables in $X$ to make the prediction. This intuitively decreases the signal-to-noise ratio and hurts the models learning efficiency during training \citep{fan2014challenges}. 

% By asking the LLMs to generate why and why not the consumer likes (or purchases) the product, we are asking the LLMs to extract the critical decision making factors that drive the observed outcome (e.g. like or purchase). If these extracted decision making factors are truly the causal factors that explain the relationship between the input $X$ and the outcome $Y$, then incorporating these factors in the model will likely increase the model's learning efficiency, and improving the model's performance given the same amount of training data. In the rest of this section, we mathematically characterize these gains provided by LLM generated explanations, by leveraging high-dimensional statistical learning theory. 

We consider a general high-dimensional learning scenario where the observations $\{\vx_k, y_k\}_{k=1}^N$ follow:
\begin{equation}
\label{eqn:model_full}
y_k = f(\vx_k) + \epsilon_k,
\end{equation}
where $\vx_k = (x_{k1}, ..., x_{kp})$ is a $p$-dimensional vector representing the input feature (independent variable) for observation $k$, $y_k$ is the outcome for observation $k$, $\{\epsilon_k\}_{k=1}^n$ are independent and identically distributed errors, and $f$ is the ground-truth function capturing relationship between $\vx_k$ and $y_k$. $f$ can be any function of choice, ranging from linear regression models to neural networks. The number of variables $p$ can be very large, as in typical high-dimensional learning settings. Let $S^* \subset \{1,...,p\}$ be the index subset of important variables that predicts the outcome, the size of which, $s^* = |S^*|$, is usually significantly smaller than $p$.\endnote{In other words, $|S^*|$ does not include \emph{spurious variables} \citep{fan2014challenges}, which appear to be statistically significant but do not contribute to the true relationship between the dependent variable and independent variables. They are often correlated with both the dependent variable and other predictors due to random chance, measurement errors, or unobserved confounding factors.} For example, there may be numerous attributes that describe a product, but only a few attributes (such as size and color) matter for the consumer's final decision-making. Therefore, the true regression function is 
\begin{equation}
\label{eqn:model_s}
y_k = f^*(\vx_{k,S^*}) + \epsilon_k,
\end{equation}
where $\vx_{k,S^*}$ selects the elements of $\vx_k$ that corresponds to the indices in $S^*$, and $f^*(\cdot)$ captures the relationship between the true important variables $\vx_{k,S^*}$ and the outcome $y_k$. 

\begin{comment}
In next next, we will demonstrate two main points: First, LLMs have better knowledge of $S^*$ than simply relying on the training data alone to infer $S^*$; Second, knowledge of $S^*$ can significantly boost the learning efficiency, thereby improving prediction accuracy when using the same amount of training data. For ease of presentation we first focus on the case where the function family is linear, i.e. the model is 
\begin{equation}
\label{eqn:linear_model_full}
y_k =  \vx_k^T \vbeta^* + \epsilon_k,
\end{equation}
and the true regression function is 
\begin{equation}
\label{eqn:linear_model_s}
y_k =  \vx_{k,S^*}^T \vbeta_{S^*}^* + \epsilon_k,
\end{equation}
where $\vbeta_{S^*}^*$ selects the elements of $\vbeta^*$ that corresponds to the indices in $S^*$. Later in Appendix \todo{theoretical results generalized to ML}, we generalize the theoretical results to general ML models. 
\end{comment}

When we ask an LLM to explain why $X$ leads to $Y$, we are essentially directly asking for $S^*$. To see this, let’s revisit the example in Fig.\ref{fig:example_explanations} from Section \ref{sec:framework_LLM_reasoning}. Here, the feature $X$ describes the sequential consumption history of the consumer (i.e., apple, orange, pear, watermelon) plus the candidate product (i.e., orange juice), while $Y$ represents the outcome (whether the consumer purchased the candidate product). $X$ is potentially high-dimensional as it includes not only the details of each product in the consumption history such as color, texture, taste, and shape of each product, but also categorical indicators such as \texttt{hasBoughtApple}, \texttt{hasBoughtOrange}, along with attributes of the candidate product (e.g., ingredients, packaging, shape). Therefore, the number of features $p$ is large. When the LLM is prompted to explain why a consumer with a particular consumption history might prefer the candidate product, it identifies relevant features, such as \texttt{hasBoughtOrange} for the positive case (Fig.\ref{fig:pos_explanation}) and \texttt{hasOnlyBoughtWholeFruits} for the negative case (Fig.\ref{fig:neg_explanation}) and designates them as the important feature set $S^*$. 

Next, we are going to make two claims using statistical learning theory. First, LLMs are \emph{more likely} to give accurate information about $S^*$ than simply relying on the training data alone to infer $S^*$; Second, having access to a more accurate estimate of $S^*$ can help the ML model learn better within its own environment. We present the theory with high-dimensional linear models first, and generalize to nonlinear ML models later. 


\subsection{LLMs have better knowledge of $S^*$ than the training data itself}   

How are LLMs obtaining the knowledge of $S^*$, the set of important variables? We can use the statistical theory on \emph{multi-environment learning} \citep{peters2016causal} to explain. In multi-environment learning, the goal is to predict the outcome variable $y$ as a function of $\vx$ using data observed from multiple environments, where each \emph{environment} represents a distinct setting or context within which observations are made. For LLMs, their training data comes from a variety of sources, including diverse domains, contexts, and linguistic structures \citep{dubey2024llama, achiam2023gpt}. Therefore, training the LLMs is analogous to learning from multiple environments. Specifically, an LLM has seen many more environments of slightly different distributions of X and Y, but the mapping from X to Y is the same, i.e., the data generating process across these multiple environments is the \emph{same}. For example, in the toy example in Section \ref{sec:framework_LLM_reasoning}, the LLMs have probably seen different orange juice purchase behaviors across numerous shopping scenarios (e.g., grocery store, online, farmer's market) from consumers with different purchase histories. However, the important decision-making variables for whether the consumer will purchase orange juice or not (such as \texttt{isOrangeProduct}, \texttt{isNotWholeFruit}) should remain the same across the different environments. 

Written formally, let $\mathcal{E}$ be the set of environments. Each environment $e \in \mathcal{E}$ observes at least $n$ samples $(\vx_1^{(e)}, y_1^{(e)}), ..., (\vx_n^{(e)}, y_n^{(e)}) \sim \mu^{(e)}$, which are from the model 
\begin{equation}
\label{eqn:multi_env}
% y_k^{(e)} =  {\vx_{k,S^*}^{(e)}}^T \vbeta_{S^*}^* + \epsilon_k^{(e)},
y_k^{(e)} = {\vbeta_{S^*}^*}^T {\vx_{k,S^*}^{(e)}} + \epsilon_k^{(e)},
\end{equation}
where $y_k^{(e)}$ is the outcome (dependent variable) for observation $k$ in environment $e$, and $\vx_k^{(e)} \in \mathbb{R}^p$ represents the full set of variables (including the non-important ones). In a typical multi-environment learning setting \citep{fan2023environment, peters2016causal}, the unknown set of important variables $S^* = \{ j: \beta^*_j \neq 0 \}$ and the model parameters $\vbeta^*$ are the same (or \emph{invariant}) across different environments, but the distribution of the training data, $\mu^{(e)}$, may \emph{vary} across environments. 

Intuitively, if there are more observations seen from multiple environments, then the model may have a better knowledge about $S^*$, which is the same across different environments. For example, when the LLMs see enough consumers purchasing orange juice from many environments, it may be able to recover the important decision-making variables (i.e., $S^*$) for orange juice purchase in a new environment even without seeing any data from this new environment. Indeed, \citet{fan2023environment} recently showed that multi-environment least squares could recover $S^*$ with high probability provided that the number of observations $n$ and the number of environments $|\mathcal{E}|$ is large enough, even in high-dimensional settings where $p > n$. In high-dimensional statistics, this is referred to as \emph{variable selection consistency}. 

We simplify the setup in \citet{fan2023environment} and summarize the variable selection consistency properties for multi-environment learning below. In particular, \citet{fan2023environment} proposes the following environment invariant linear least squares (EILLS) estimator $\hat{\vbeta}_L$ which minimizes the following objective: 
\begin{equation}
\label{eqn:eills_obj}
\hat{\vbeta}_L = \argmin_{\vbeta} \hat{R}(\vbeta) + \gamma \hat{J}(\vbeta) + \lambda ||\vbeta||_0,
\end{equation}
where 
\begin{equation}
\label{eqn:eills_individual_loss}
\begin{aligned}
\hat{R}(\vbeta) &= \sum_{e \in \mathcal{E}} \sum_{k=1}^n (y_k^{(e)} - \vbeta^T \vx_k^{(e)}  )^2 ,\\
\hat{J}(\vbeta) &= \sum_{j=1}^p \mathbf{1} \{\beta_j \neq 0\} \sum_{e \in \mathcal{E}} (\sum_{k=1}^n x_{k,j}^{(e)} (y_k^{(e)} - \vbeta^T \vx_k^{(e)}) )^2. 
\end{aligned}
\end{equation}
Here $\hat{R}(\vbeta)$ is the usual total mean squared loss across all environments, $\hat{J}(\vbeta)$ is the  \emph{invariance regularizer} that encourages the model to focus only on important variables that are useful across all environments, and $||\vbeta||_0$ is the $l_0$-penalty that encourages as few nonzero elements in $\vbeta$ as possible. 

We simplify the convergence results in Theorem 4.5 of \citet{fan2023environment} and obtain the following variable selection consistency results as Lemma \ref{lem:var_selection_ellis} below.
\begin{lem}
\label{lem:var_selection_ellis}
Under conditions detailed in Appendix \ref{appen:lemma_1_cond}, the multi-environment estimator $\hat{\vbeta}_L$ has variable selection consistency, i.e. 
$$ \mathbf{P}(\text{supp}(\hat{\vbeta}_L) = S^*) \rightarrow 1   $$  
as long as $n, p, s^* \rightarrow \infty$ and $n \gg  s^* \beta_{\text{min}}^{-2}\log{p} $, where $s^* = |S^*|$ and $ \beta_{\text{min}} = \min_{j \in S^*} |\beta^*_j| $. 
% \vspace{-0.1in}
\end{lem}

Lemma \ref{lem:var_selection_ellis} states that as long as there are enough observations ($n$) from enough environments ($|\mathcal{E}|$), then the model would be able to recover the true set of important variables with probability approaching 1, even if the potential number of variables $p$ and the number of truly important variables $s^*$ also grows with $n$. Given the fact that LLMs are trained on vast sources of texts, they probably learn from a huge number of observations and numerous environments, making $\mathbf{P}(\text{supp}(\hat{\vbeta}_L) = S^*)$ very close to 1. Such variable selection consistency is only achievable with a large number of observations from multiple environments, which explains why LLMs are much more capable of providing convincing explanations. 

% state-of-the-art methodologies involve adding a penalty term in the loss function that encourages the models to focus on a smaller set of variables that are truly important. [lasso recover S* rate] 

On the contrary, when there is only data from one environment with a limited number of observations (i.e., $n$ much smaller than in the LLM's case), then $\mathbf{P}(\text{supp}(\hat{\vbeta}_L) = S^*)$ is not necessarily close to 1 due to limited $n$. In fact, \citet{fan2023environment} proved that having more than one environment is actually \emph{necessary} for identifying $\vbeta^*$ and $S^*$. We detail their result and our related insights in Appendix \ref{appen:more_than_one_env_needed}. Therefore, LLMs, as discussed above, can obtain a more robust estimate of $S^*$ due to the vast data and numerous environments they have seen.

% LLMs learns invariant across multiple environments 
 
\noindent \paragraph{\textbf{Remark 3.}}
Note that when we ask an LLM for a reason why $\vx$ leads to $y$, we are primarily asking for $S^*$—the important features—rather than the function $f^*$ itself (e.g., the coefficients $\vbeta^*$ if $f^*(\vx) = \vx^T {\vbeta^*} $). For instance, when prompted to explain why a consumer with a specific fruit consumption history might prefer orange juice, the LLM identifies relevant features (like \texttt{hasBoughtOrange} and \texttt{isOrangeProduct}) and assigns them to $S^*$. However, the LLM does \emph{not} quantify the impact of these features on the outcome; for example, it does not predict how much \texttt{hasBoughtOrange} and \texttt{isOrangeProduct} changes the purchase likelihood. Thus, in our framework, LLM explanations highlight important features but do not estimate the prediction function. In the experiments in Section \ref{sec:results}, we also explored using LLMs to directly predict outcomes (i.e., using LLMs to estimate the prediction function $f^*$). This approach led to significantly poorer performance compared to our framework, indicating that while LLMs may excel at identifying the important variables $S^*$, traditional ML models are still needed to estimate the prediction function $f^*$ effectively. This is also aligned with the findings of \citet{ye2024lola}, who observed that LLMs perform poorly as direct prediction functions, and \citet{jeong2024llm}, who found that LLMs are effective at feature selection.

\subsection{Better knowledge of $S^*$ leads to better model performance} 

As discussed above, the number of true important variables $|s^*|$ is usually much smaller than the total number of variables $p$. The performance of the models is measured by the estimation errors for the model parameters $\vbeta^*$, or $||\hat{\vbeta} - \vbeta^* ||_2$ where $||\cdot||_2$ represents the $L_2$ norm. Intuitively, knowing $S^*$ will help the model learn $\vbeta^*$ more efficiently as if it is assisted by an oracle. Next, we mathematically quantify why a better knowledge of $S^*$ leads to better learning efficiency and predictive performance. 

In a high-dimensional learning setting when we don't know $S^*$, the most popular method to help the model focus on important variables is Lasso (Least Absolute Shrinkage and Selection Operator) \citep{tibshirani1996regression}, which adds an $L_1$ penalty to the objective function and encourage the model to shrink the estimates for the non-important variables to zero. The Lasso estimator $\hat{\vbeta}$ is obtained by:
\begin{equation}
\label{eqn:lasso}
\hat{\vbeta}_{\text{Lasso}} = \argmin_{\vbeta} \left\{ \frac{1}{2n} \sum_{k=1}^n (y_k - \vx_k^T \vbeta)^2 + \lambda \sum_{j=1}^p |\beta_j| \right\},
\end{equation}
where $\lambda \geq 0$ is the regularization parameter that controls the amount of shrinkage. Under typical assumptions of high-dimension learning, the convergence rate of Lasso is given by \citet{bickel2009simultaneous}: 
\begin{equation}
\label{eqn:lasso_conv}
||\hat{\vbeta}_{\text{Lasso}} - \vbeta^* ||_2 = O_P\left(\sqrt{\frac{s \log p}{n}}\right),
\end{equation}
where $n$ is the sample size and $O_p$ denotes the order in probability. \endnote{A sequence of random variables \( X_n \) is said to be \( O_P(a_n) \) if for any \( \epsilon > 0 \), there exists a constant \( M > 0 \) such that $\Pr\left( \left| \frac{X_n}{a_n} \right| > M \right) \leq \epsilon \text{ for all sufficiently large } n.$} 

When we do know $S^*$, then a standard ordinary least squares (OLS) would suffice to recover the true model parameters, by estimating the elements of $\vbeta^*$ which are known to be nonzero (i.e., in $S^*$):
\begin{equation}
\label{eqn:ols}
\hat{\vbeta}_{\text{Orc}} = \argmin_{\vbeta \in \{ \vbeta: \beta_j = 0, \; \forall j \in S^* \}} \frac{1}{2n} \sum_{k=1}^n (y_k - \vx_{k}^T \vbeta)^2 .
\end{equation}

The corresponding convergence rate is \citep{tibshirani1996regression}:
\begin{equation}
\label{eqn:oracle_conv}
||\hat{\vbeta}_{\text{Orc}} - \vbeta^* ||_2 = O_P\left(\sqrt{\frac{s}{n}}\right).
\end{equation}

% See Appendix \todo{Convergence results of Lasso and OLS, $https://junwei-lu.github.io/papers/BST235_Notes.pdf$ page 56, full rank $r=s$} for a detailed discussion on how these convergence rates are derived.

Comparing Eq.(\ref{eqn:lasso_conv}) and Eq.(\ref{eqn:oracle_conv}), we see that having knowledge of $S^*$ will increase the convergence rate by a factor of $\sqrt{\log p}$. This is a significant factor, especially in high-dimensional learning settings where $p > n$ and $p \rightarrow \infty$. In a recommender system learning setting, arguably, the number of variables $p$ indeed approaches infinity as the description of the products can be arbitrarily high-dimensional. Therefore, knowing $S^*$ for a recommender system will significantly boost learning efficiency using the same amount of data, ultimately leading to better model performance.

As an illustration, Fig.\ref{fig:convergence_rate_comparison} compares the convergence rates with a fixed number of observations and true important variables ($n=1000$, $s^*=20$) and a varying number of predictors ($p$). As expected, having knowledge of $S^*$ significantly reduces the estimation error, with the reduction becoming larger for larger $p$.

\begin{figure}[hbtp!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/convergence_rate_comparison.png}
    \caption{(Color online) Convergence rate comparison for the Lasso (unknown $S^*$) and Oracle estimator (known $S^*$).}\label{fig:convergence_rate_comparison}
    % \vspace{-0.2in}
\end{figure}

% Fig.\todo{figure illustrating convergence rate, with fixed n and growing p} illustrates the performance gains with varying values of $n$ and $p$. We see that \todo{with fixed n, knowing $S^*$ would improve the estimation error by X fold.}




% First, by asking the LLMs for explanations, we get a good estimate of the true set of important variables $S^*$ by exploiting LLMs' reasoning capabilities. Then by incorporating the knowledge of $S^*$ from the LLM's explanations as the input $\vx$ to a recommender system, the system is able to learn much more efficiently compared with systems without the knowledge of $S^*$, saving training examples in the order of $\log(p)$. 

So far, we have established the statistical insights about why incorporating LLM-generated explanations, as in our LR-Recsys framework, would help the predictive performance of the model. Note that LR-Recsys does not exactly mirror the high-dimensional learning setup as discussed above, as the theory above is about linear models but LR-Recsys is a nonlinear neural network model. We show that in the nonlinear and ML model cases, the discussions above still hold, but the convergence rate comparisons are slightly more complex. See Appendix \ref{appen:multi_env_nonlinear} for detailed results. 

The theories above suggest that, to achieve optimal convergence, one should discard any features not included in $S^*$. However, in our framework, we propose to keep some low-dimensional non-interpretable features, such as consumer and product embeddings, that are specific to the current environment and find them to be helpful in the experiments. The theoretical assumption is that $S^*$ remains invariant across all environments, but in practice, individual environments may exhibit unique characteristics (e.g., the packaging color of orange juice might matter in some regions but not others). Therefore, our framework allows for the flexibility that individual environments may have their own specificities. 

% In the next section, we validate these statistical insights and training efficiency gains of our LR-Recsys framework on multiple real-world recommender system datasets.


\begin{comment}

=== 

II. How is better knowledge of S* helping ML model learning 
In a regular setting, the ML model is learning from X to Y where X is high dimension. However, the true important variables are only a subset of X, denoted as S*. In the section above we talked about how LLMs have a better knowledge about S*. Now we look at how a better knowledge of S* helps a regular ML model's learning. 

"as if assisted by an oracle" 

In high-dimensional learning, the most popular way to help the model to focus on important variables is Lasso, which adds L1 penalty to the objective function and encourage the model to shrink the estimates for the non-important variables to zero (write math equation of Lasso loss function here). Lasso has a convergence rate: O(sqrt(slogp/n).

When we know which variables are important, i.e. when we know S*, then the convergence should be O(sqrt(s/n)), i.e. does not need to pay the cost of not knowing the true support 
% (Reference: https://junwei-lu.github.io/papers/BST235_Notes.pdf page 56) 

[In our proposed framework, we replace the input x with S*, and keeps the uninterpretable features consumer and item embeddings, so convergence rate should be between the above two, which explains why it learns more efficiently ]
Then how is S* helping ML model who is learning on S
to only focus on the important variables in X 


[end] in the nonlinear case and ML model case, the above discussion also holds. See Appendix \todo{nonlinear case} for more discussions. 

When there are enough environments seen, they true S* can be recovered with high probability:
In the linear case..., In the nonlinear case, see https://arxiv.org/pdf/1706.08576 and https://arxiv.org/pdf/1706.08058 for sequential data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%     Appendix     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For general ML, 
I. [https://arxiv.org/pdf/2405.04715 prop 10] variable selection results (when n is large enough, $\hat{S}$ include all true important variable, and does not include all spurious variable. 

II. Page 20 in https://arxiv.org/pdf/1708.06633 $n^(-2beta / (beta + p))$

[In the end, Remark 3: mention that this is different from forcing explainability in model training, because we don't usually know a priori what are the important factors, as a result forcing the models to follow those rules will only decrease (rather than increase) their learning efficiency.]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

===

% learning efficiency
% [consumer and product embeddings can be viewed as capturing additional information]

% i=1 are independent and identically distributed p-dimensional covariate vectors,
% {i}n
% i=1 are independent and identically distributed errors and βÅ is a p-dimensional regression coefficient vector.

%  This kind of prediction requires only a strong correlation with the outcome variable.

% The current ML model is one environment, trying to learn the relationship between X and Y. However, ML models tend to rely on correlation rather than causation to make predictions (they can be viewed as gigantic correlation extractors). As a result, they may not know which variables are the truly important ones that capture the underlying data generation process to make the prediction, which decreases the signal-to-noise ratio, and may confuse the ML model during learning.



% Specifically, we leverage high-dimensional statistical learning theory to characterize the gains in statistical learning efficiency provided by incorporating LLM generated explanations in model training. 

% Why LLM's explanation would help? contrary to existing understanding that incorporating explainability hurts model performance. In this section we shed lights on when and how explainability can help a model by adopting statistical analysis on the learning efficiencies of ML models with or without explainability. Specifically, we adopt the multi-environment learning theory in statistics to explain.




I. Why LLMs have better knowledge about S* [focus more discussion on this]
Multi-environment learning theory, when number of observations n and number of environments is large enough, then we have variable selection consistency (Thm 4.5 in https://arxiv.org/pdf/2303.03092). 



\end{comment}



