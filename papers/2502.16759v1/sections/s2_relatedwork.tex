% [MOHR: 1389 words] 

% \todo{add https://dl.acm.org/doi/pdf/10.1145/3534678.3539432 as baseline}


\subsection{Explainable AI}

There has been a rising popularity of research on explainable AI (XAI), which aims at making black-box AI systems and their results more understandable to humans \citep{adadi2018peeking}. In a recent review paper \citep{nauta2023anecdotal}, the authors define an explanation as ``a presentation of (aspects of) the reasoning, functioning and/or behavior of an ML model in human-understandable terms''. There are mainly three groups of methods for XAI, depending on which stage of the training is the explainability obtained for an ML model. The first group, known as \emph{post-hoc explanation methods}, provides explanations for models that have already been trained \citep{chen2018learning, oramas2017visual, wagner2019interpretable, lundberg2017unified}. The second category involves building \emph{intrinsic interpretability} into the model architecture itself through white-box approaches \citep{pan2021explainable, subramanian2020obtaining, zhang2024optimal}. Finally, the third category involves \emph{supervised explanation training} where a ground-truth explanation is provided as part of the training process. 
 
Each of these approaches comes with its own drawbacks. 
Recent research indicates that post-hoc explanation methods are not always reliable, as they sometimes fail to accurately reflect the true marginal effects of X on Y in the data \citep{ragodos2024risk}. Moreover, due to their ad-hoc nature, these methods do not help the performance of the underlying black-box models. Intrinsic interpretability techniques often compromise model performance by imposing additional constraints on model flexibility \citep{zhang2024optimal}, or require problem-specific domain knowledge \citep{fong2024theory, sisodia2024generative} that does not easily generalize across applications. Supervised explanation training, on the other hand, demands significant human effort to curate explanations, making it costly, while the collected explanations can be noisy due to subjectivity and individual preferences. In our work, we address these challenges by introducing a framework that not only improves model performance through explanations but also eliminates the need for domain-specific knowledge or human-generated explanations.


% In recent years there has been a rising popularity of research on explainable AI (XAI), which aims at making black-box AI systems and their results more understandable to humans \citep{adadi2018peeking}. In a recent review paper by \citet{nauta2023anecdotal}, the authors define an explanation as ``a presentation of (aspects of) the reasoning, functioning and/or behavior of a machine learning model in human-understandable terms''. There are mainly three groups of methods for XAI, depending on which stage of the training provides explainability for a machine learning model. These include post-hoc explanation methods which explain an already trained predictive model (\emph{post training}), intrinsic interpretability built in the predictive model such as white-box models (\emph{during training}), and supervised explanation training where a ground-truth explanation is provided as training data (\emph{supervised explanation training}). We briefly review each method below. 

% \emph{Post-training methods}, sometimes also referred to as ``reverse engineering'' \citep{guidotti2018survey}, involve post-hoc analysis methods to explain an already trained black-box model. For example, \citet{chen2018learning} and \citet{oramas2017visual} proposed learning to automatically extract a subset of features that are most informative for each given example. \citet{wagner2019interpretable} proposed an optimization-based visual explanation method which highlights the evidence in the input image for a specific prediction. Other popular tools include marginal feature importance \citep{lundberg2017unified, ribeiro2016should} and heatmaps \citep{bach2015pixel, fong2017interpretable, selvaraju2017grad}. These post-hoc methods are model-agnostic, making them applicable to a broad range of ML applications including vision models \citep{oramas2017visual, wagner2019interpretable}, graph neural networks (GNNs) \citep{vu2020pgm}, recommender systems \citep{sen2020curious, tsang2020feature, verma2019lirme} and general ML prediction problems \citep{mothilal2020explaining, schwab2019cxplain, wang2019gaining}. However, recent research indicates that these post-hoc methods are not always reliable, as they sometimes fail to accurately reflect the true marginal effects of X on Y in the data \citep{ragodos2024risk}. Moreover, due to their ad-hoc nature, these methods do not help the performance of the underlying black-box models.

% \emph{During-training methods} introduce explainability by enforcing or encouraging interpretable model components during the training process of the ML models. This is typically done via interpretable feature mapping or interpretable model architecture. For interpretable feature mapping, \citet{pan2021explainable} developed a novel approach that maps the uninterpretable features onto the interpretable aspect features; \citet{fong2024theory} proposed a theory-based, explainable deep learning classifier for predicting music emotion, by including harmonics-based structure placed on the CNN filters. \citet{sisodia2024generative} proposes to use supervised product characteristics as supervised signals in learning disentangled representation. However, such methods require domain knowledge (e.g. set of aspects, acoustic physics, or characteristics of watches in the examples above) that are usually problem-specific, and therefore are not easily generalizable to other applications. For interpretable model architecture, methods range from enforcing ``white-box'' models such as decomposing a black-box model into interpretable components \citep{subramanian2020obtaining, zhang2024optimal}, to modifying the DNN's architecture, such as the attention mechanism, to be transparent and explainable \citep{mohankumar2020towards}. However, such methods typically hurt model performance compared with a fully flexible black-box model \citep{zhang2024optimal}. Our proposed framework belongs to the family of \emph{during-training methods} for explainable AI; However unlike existing approaches that either sacrifice model performance or require domain-specific knowledge, our method not only improves model performance but also avoids the need for domain knowledge.

% The last category involves \emph{supervised explanation training}. In these approaches, a ground-truth explanation is provided for a subset of examples, and an ML model is then trained to generate explanations for the unlabeled examples \citep{camburu2018snli, liu2018towards, sun2020dual}. However, these methods require significant human effort to curate explanations, making them costly, and the collected explanations can be noisy due to subjectivity and individual preferences. In contrast, our framework leverages the reasoning capabilities of LLMs to generate potential explanation candidates. In addition, instead of specifying a ``ground-truth'' explanation, we let the ML system itself to learn which explanations are helpful in making the final predictions. 

\subsection{Explainability in Recommender Systems}
With the growing demand for transparency, there has been increasing research interest in explainability for modern recommender systems. Since firms are typically reluctant to sacrifice accuracy for explainability, popular approaches in explainable recommender systems usually rely on post-hoc explanation or supervised explanation training methods as described above, as they do not hurt the model's performance. Examples include \citet{wang2018reinforcement} which proposed a reinforcement learning (RL) framework that generates explanations for an \emph{existing} recommendation model, and \citep{li2021personalized} which leveraged sequence models and review texts to generate personalized explanations. While these methods emphasize the quality of the generated explanations \citep{costa2018automatic}, they do \emph{not} contribute to improving the predictive performance of the original recommendation model.

Another approach to explainable recommendation systems leverages product metadata or specific product aspects to construct aspect-based recommendations, where items are recommended based on the most valuable aspects extracted from user reviews using text modeling techniques \citep{bauman2017aspect, cheng20183ncf, cheng2019mmalfm, chin2018anr, guan2019attentive, le2021explainable, li2023prompt}, and on exploiting metadata and knowledge graphs to generate explainable recommendations \citep{lee2018explainable, wang2019explainable}. However, these methods rely on explicit review text or structured knowledge, which may not always be available, and the generated explanations are limited to the provided text or knowledge base. In contrast, our proposed framework does \emph{not} require review text or external knowledge graphs; In addition, the generated explanations are free-form text not limited to a subset of aspects.

\subsection{Large Language Models in Recommender Systems}
% https://github.com/WLiK/LLM4Rec-Awesome-Papers 
Building on the success of large language models (LLMs), an emerging line of research has focused on leveraging LLMs to improve the performance of recommender systems \citep{wu2024survey}. These methods can be broadly divided into two categories. The first category uses LLMs as feature extractors or augmenters, feeding user and item features into the models and leveraging the LLM-generated embeddings as recommendation inputs \citep{peng2023gpt, ren2024representation, wang2024llm}. The second category directly transforms a pre-trained LLM into a recommender system, with the typical input sequence of a profile description, behavior prompt, and task instruction \citep{he2023large, huang2023recommender, wang2024rethinking, zhai2024actions}. However, neither of them provides explanations for the generated recommendations. 

% We show that LLM-based explanability \emph{improves} recsys performance

Another line of emerging research closer to our work is on leveraging LLMs to provide explanations for recommendations. \citet{wang2023llm4vis} proposed a ChatGPT-based prompting approach to perform visualization recommendations and return human-like explanations. \citet{tsai2024leveraging} proposed to leverage LLMs' reasoning capability to mine review texts to generate explanations. However, they rely on a zero-shot LLM as the recommendation engine which performs worse than DNN-based recommender systems. \endnote{LLM-based recommenders underperform because, while LLMs offer benefits like contextual understanding and generalization, they struggle in tasks that require extensive user-item interaction modeling, a strength of DNN-based recommenders, such as collaborative filtering or graph-based approaches \citep{wu2024survey}.} Other works propose to incorporate an explanation-like task in a DNN-based recommender system \citep{li2023personalized, li2023prompt}. However, unlike our method, these approaches rely on explicit review texts to generate explanations, which are often unavailable in real-world applications. \citet{yang2023large} demonstrated that open-source LLMs can decipher the representation space of sequential recommenders without relying on review texts. However, their method falls under post-hoc XAI approaches and does not improve the performance of the recommender system. Our work addresses these limitations by improving \emph{both} the performance of the recommender system and providing explanations, all \emph{without} the need for review texts.


\subsection{Sequential Recommender Systems} 
Our work is also related to sequential recommender systems, which predict the next item a user will engage with based on a sequence of past consumptions \citep{wang2019sequential}. Due to their strong user-item interaction modeling capabilities, sequential recommender systems are replacing traditional recommender systems that are based on count-based features and content-based filtering, becoming the backbone of many industrial recommendation platforms today \citep{chen2019top, zhai2024actions}. Popular sequential recommender systems like Bert4Rec \citep{sun2019bert4rec}, SASRec \citep{kang2018self}, PARSRec \citep{gholami2022parsrec}, and Transformers4Rec \citep{de2021transformers4rec} rely on Transformer-based architectures \citep{vaswani2017attention} to encode consumption history and generate embeddings that represent user preferences, where the attention mechanism offers insights into which past consumption impacts the consumers' current interest \citep{gholami2022parsrec}. However, these embeddings are not interpretable and change across contexts, making it difficult to explain \emph{why} a consumer likes certain products. In our work, we address this challenge by building an LLM-based contrastive-explanation generator to analyze consumption sequences and generate two-sided explanations about consumer choices. Furthermore, we demonstrate that these explanations can be incorporated into state-of-the-art sequential recommender systems to improve their performance.


\begin{comment}

\subsection{Large Language Models for Consumer Insights}
Mainly for summarizing behavioral insights, use as data scientists. To the best of our knowledge, little work on using the extracted insights to improve recsys 

\end{comment}