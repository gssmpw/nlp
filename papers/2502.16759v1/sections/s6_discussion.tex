% p1
In this work, we propose LR-Recsys, an LLM-Reasoning-Powered Recommendation framework. Unlike conventional black-box models that directly predict consumer preferences, LR-Recsys relies on an LLM-based contrastive-explanation generator to articulate \emph{why} a consumer may or may not like a product. These explanations are then embedded into a DNN-based recommendation model using a fine-tuned AutoEncoder, enabling the system to incorporate reasoning alongside traditional feature-based inputs. Empirically, we validate the effectiveness of LR-Recsys through extensive experiments on three real-world benchmark datasets, where it consistently outperforms a wide range of baselines, achieving a 3–14\% improvement in predictive performance—gains that could translate into millions of dollars in additional revenue when deployed on leading content recommendation platforms such as YouTube \citep{li2024variety, wang2024going}. Theoretically, we provide statistical insights into why LLM-generated explanations simultaneously enhance predictive performance and explainability. By leveraging high-dimensional multi-environment learning theory, we demonstrate that LLMs likely have better knowledge of the important variables driving consumer decision-making, and that incorporating such knowledge can improve the learning efficiency of high-dimensional ML models.


% In this work, we propose LR-Recsys, an LLM-Explanation-Powered Recommendation framework that incorporates LLM-generated explanations about consumer behaviors into the design of a recommender system. Instead of relying on a black-box model to predict consumer behavior as recommender systems today do, we first build an LLM-based contrastive-explanation generator that explains \emph{why} the consumer may or may not like the product, and then embed these explanations into the input layer of a deep neural network (DNN)-based recommendation model. Empirically, we demonstrated the efficacy of our framework through extensive experiments on three benchmark real-world recommendation datasets and against a wide range of baselines, achieving a 5-14\% improvement in  predictive performance which has the potential to translate to millions of dollars gain in revenue when deployed on real-world platforms. Theoretically, we provided statistical insights into why LLM-generated explanations can simultaneously improve predictive performance and explainability. Using high-dimensional multi-environment learning theory, we show that LLMs likely have better knowledge of the important variables driving consumer decision-making, and that incorporating such knowledge can improve the learning efficiency of high-dimensional ML models.

To the best of our knowledge, this work is among the first to directly combine LLM reasoning with a DNN model in an end-to-end framework. Our work provides both empirical and theoretical evidence that, contrary to the common belief that explainability comes at the cost of model performance, it is actually possible to improve both the predictive performance and explainability of black-box recommender systems. We show that these improvements largely come from the \emph{reasoning} capabilities of LLMs, rather than their dataset knowledge or summarization skills. % Specifically, LLMs' strong reasoning capabilities enable black-box ML models to identify and focus on the important variables driving consumer decisions from a vast pool of predictors, thereby improving their learning efficiency and predictive accuracy with the same amount of training data.


% p2
Managerially, our work demonstrates the potential of leveraging Generative AI (GenAI) technologies to improve understanding of consumer behavior, and using these insights to inform better product design and personalization strategies. Specifically, we demonstrate the value of contrastive explanations in improving the performance of black-box ML systems. In today’s multi-sided platform ecosystems, such explanations are beneficial to all stakeholders of the platform. For consumers, our framework boosts their trust in the system by providing explanations for both why they \emph{may} or \emph{may not} enjoy a recommended product. For the sellers and content creators on the platform, the contrastive explanations by our framework offer insights into the product attributes that engage or disengage consumers, helping them design future offerings that better align with consumer needs. For the platform itself, aggregating positive and negative explanations generates valuable insights into which product characteristics resonate with specific consumer segments, enabling more effective targeting and expansion strategies. Furthermore, our experiments show that observations with the highest prediction uncertainty benefit most from our LR-Recsys framework. This suggests that these explanations are particularly impactful in improving new consumer experience and driving new product adoption, areas that are traditionally challenging for recommender systems.

% Importantly, these explanations are generated only from their consumption history on the platform and not from demographic features, therefore they raise no privacy or fairness concerns.

% p3
Our proposed LR-Recsys framework is general and readily applicable to any modern recommendation platform. One of the biggest challenges in deploying LLM-based components on large-scale platforms is the latency of API calls to LLMs. For instance, generating an explanation of up to 50 words takes approximately 1,700 milliseconds with GPT-3.5 and 9,800 milliseconds with GPT-4\endnote{Based on reported latencies: GPT-3.5-turbo-1106 processes at approximately 34 milliseconds per generated token, while GPT-4 processes at approximately 196 milliseconds per token.}, whereas recommendation platforms typically require response times to be within single-digit milliseconds to ensure users see fresh recommendations immediately upon opening the homepage. This latency makes it impractical to call LLMs in real-time for generating explanations. Our framework mitigates this challenge by eliminating the need for real-time input to generate explanations. Instead, positive and negative explanations can be precomputed and updated after the consumer's most recent session (e.g., following a purchase), making it feasible to deploy LR-Recsys on large-scale online platforms. Additionally, our framework is compatible with any LLM, with performance improving as the reasoning capabilities of the LLMs improve. Lastly, the implications of our framework extend beyond recommendation systems. Any ML system with a predictive task can benefit from incorporating explanations from the contrastive-explanation generator in our framework. 


% p4 
One potential limitation of our LR-Recsys framework is its cost, both in terms of time latency and money. Deploying LLM-based solutions in real-world applications today can be inherently expensive. For instance, each query to the OpenAI API costs approximately \$0.01 and takes 196 milliseconds per generated token. This can be prohibitively expensive for large-scale platforms with huge daily traffic. Our framework mitigates the time latency issue by precomputing explanations instead of generating them in real time, but it still requires an LLM call for \emph{every} observation in the training data therefore can still be financially costly. A future research direction is to develop smaller, more targeted explanation generators based on small language models \citep{schick2020s} or distillation techniques \citep{hinton2015distilling, gu2024minillm}, which could substantially reduce costs. Another cost-reduction strategy is to selectively apply LR-Recsys to a subset of observations where explanations have the greatest impact, such as new consumers or new products with higher uncertainty. For example, a smart routing algorithm could dynamically decide between using LR-Recsys or a traditional recommender system based on the projected performance gains of the explanations, thereby optimizing cost while maintaining predictive performance.


% ==
% One potential limitation of our LR-Recsys framework is its cost, both in terms of time latency and financial expense. Deploying LLM-based solutions in real-world applications can be inherently expensive. For instance, each query to the OpenAI API costs approximately \$0.01 and takes 196 milliseconds per generated token. While our framework mitigates the time latency issue by precomputing explanations rather than generating them in real time, it still requires an LLM call for \emph{every} observation in the training data, which can become financially prohibitive for large-scale platforms with substantial daily traffic.

% A promising future research direction is to develop smaller, more targeted explanation generators using compact language models \citep{schick2020s} or distillation techniques \citep{gu2024minillm, wang2024using}, which could significantly reduce costs. Another cost-reduction strategy is to selectively apply LR-Recsys to a subset of observations where explanations yield the greatest impact, such as for new consumers or products with higher uncertainty. For example, a smart routing algorithm could dynamically decide whether to use LR-Recsys or a traditional recommender system based on the projected performance gains of the explanations, thereby optimizing cost-efficiency while maintaining strong predictive performance.



% \todo{Pan (2024/11/30): can you provide a few examples on 

% 1. how our framework improves the explainability of recsys? i.e. given a new test example, show the model's positive and negatve explanations, and the weights on them, then the model's binary prediction;

% 2. the aggregation of the positive and negative explanations yield valuable understandings about which product attributes appeal to specific consumer segments and which do not. }