% [The success of recsys]


Recommender systems have become integral to the success of many businesses \citep{ricci2010introduction, schafer1999recommender}. These systems typically adopt large-scale machine learning (ML) models to predict consumer preferences and rank products accordingly in real time. By leveraging recent advances in machine learning, modern recommender systems take into account hundreds or thousands of input features and can pinpoint the right recommendations from vast catalogs containing millions, or even billions, of items within milliseconds, forming the foundation of many personalization platforms today \citep{gomez2015netflix, solsman2018youtube, wang2024recommending}. 

% [one of the biggest limitation of recsys: lack of understanding]
State-of-the-art recommender systems rely on black-box ML models to predict the next item that a consumer will engage with, based on a sequence of her past consumed items. Being black-box in nature, these systems are often unable to provide insights into why the consumer likes a product, or why a product is being recommended. However, in this era of information overload, there is an increasing need for transparency in recommender systems, in particular, to address the ``why'' behind the recommendations. Consumers seek to understand why certain products are recommended to them for a better experience; Sellers and content creators need insights into why consumers like their products to better tailor their future offerings; Platforms, similarly, need to understand which product attributes attract specific consumer segments in order to refine their targeting strategies. Motivated by this need, there has been a growing interest in explainable recommender systems \citep{zhang2020explainable} in recent years, which belong to the broader explainable AI (XAI) literature aiming to make AI systems more transparent and understandable across different industries \citep{pathak2020chemically, sun2024sparse, tang2020knowing, zhang2020inprem}. 

Existing research in XAI, however, does \emph{not} show the value of explanations in helping the ML models make better predictions. In fact, methods aimed at improving explainability often hurt model performance, as explainability constraints tend to restrict model flexibility. As a result, most recommender systems today still rely on purely black-box ML models \citep{covington2016deep, zhai2024actions}, as firms are unwilling to sacrifice the recommendation accuracy only to improve explainability. 

% [we propose alternative framework that improves performance.]

In this work, we show that it is actually possible to leverage explanations to \emph{improve} predictive performance by \emph{combining} large language models (LLMs) with deep neural networks (DNNs). Motivated by the recent advances in LLMs and their impressive reasoning capabilities \citep{wei2022chain}, we propose \emph{LR-Recsys}, an \emph{LLM-Reasoning-Powered Recommender System} that incorporates LLM-generated explanations into black-box recommender systems. A standard deep neural network (DNN) is used to predict the likelihood of a consumer enjoying a product given a sequence of her past consumed products. In addition, we introduce a \emph{contrastive-explanation generator} which leverages LLMs to analyze a consumer's consumption history, and output explanations linking the consumption history to the given product. Specifically, the contrastive-explanation generator produces two types of explanations: a \emph{positive explanation} for why the consumer likes the product given her consumption history, and a \emph{negative explanation} for why the consumer may not like the product. Both explanations are in natural language. These explanations are then transformed into embeddings using a fine-tuned AutoEncoder, which are then combined with standard consumer and product features to form the input layer of the DNN-based recommendation model. By doing so, our framework allows the recommendation model to rely on the LLM's reasoning of \emph{why} the consumer may like or dislike a product in predicting \emph{whether} she will enjoy the product, therefore improving the learning efficiency. While LLM-generated features have been studied \citep{lin2023can}, where LLMs' external knowledge and summarization skills have been leveraged to augment the input data, we demonstrate through a series of experiments that the gain of our LR-Recsys framework does \emph{not} come from LLMs' external knowledge or summarization skills. Rather, it mainly relies on the \emph{reasoning} capabilities of LLMs \citep{brown2020language, wei2022chain}, which is made possible with the contrastive-explanation generator along with its autoencoder design. 

% , we demonstrate that our LR-Recsys framework's contrastive-explanation generator, with its autoencoder design, is essential for leveraging the reasoning capabilities of LLMs to enhance predictive performance \citep{brown2020language, wei2022chain}, thus providing additional advantages beyond existing approaches.

% \minminc{While using LLM generated features as input to recommendation models have been studied \cite{}..., the contrastive explanation and autoencoder are critical.... offer ...}

% [Our proposed method benefits from LLM reasoning.]



To understand why LLM-generated explanations can improve predictive performance, we provide theoretical insights using high-dimensional statistical learning theory. Leveraging multi-environment statistical learning theory, we show that LLMs are likely to be equipped with better knowledge of the important variables driving consumer decision-making, as they have seen numerous similar decision-making scenarios across diverse environments in the training data. Building on established convergence theories in statistical learning, we show that incorporating such knowledge can improve the learning efficiency of ML models trained on the same amount of data, therefore leading to improved predictive performance.

% [experiment results] 
To evaluate the effectiveness of the LR-Recsys framework, we conducted extensive experiments on three real-world recommendation datasets from Amazon, Yelp, and TripAdvisor, where the recommended products are movies, restaurants, and hotels, respectively. Our framework consistently outperformed state-of-the-art black-box recommender systems, LLM-based recommender systems, and a wide range of explainable recommender systems. Compared with the best-performing baselines in the literature, our framework reduces RMSE by 5\% to 14\% for regression tasks such as rating prediction, and improves AUC by 3\% to 4\% for classification tasks like predicting whether or not a consumer likes a product. These substantial accuracy gains validate the capability of our LR-Recsys framework to improve predictive performance while providing explainability at the same time, contrasting to the trade-off typically observed in existing explainable recommendation methods. We would like to highlight that even small improvements in these performance metrics can translate into significant business impact \citep{gomez2015netflix, wang2022can, wang2024recommending}. For example, \citet{li2024variety} demonstrated that a 3\% improvement in AUC resulted in a \$30 million increase in annual revenue for a major video streaming platform. Accordingly, our framework has the potential to generate millions of dollars in annual revenue gains if deployed on leading content recommendation platforms today.

To ensure no overlap between the training data for the LLMs and the benchmark datasets, we also experimented with GPT-2 \citep{radford2019language}, an older LLM trained before the creation of the dataset in our experiments. LR-Recsys with GPT-2 generated constrastive-explanations still outperform state-of-the-art baselines, confirming that the observed improvements stem from its reasoning capability rather than any prior knowledge about the dataset embedded in the LLM. Additionally, we find that LLMs with stronger reasoning capabilities (e.g., Llama 3.1, \citep{meta2024introducing}) perform better than those with relatively weaker reasoning capabilities (e.g., Mixtral 8*7b, \citep{jiang2024mixtral}), supporting our theoretical insight that the improvements come from LLMs’ reasoning ability to identify important variables driving consumer decisions, therefore improving the model's learning efficiency.

Unlike existing XAI approaches, which often focus on a single type of explanation, our framework incorporates both positive and negative explanations—reasons why a consumer might like or dislike a product. Ablation experiments demonstrate that negative explanations are particularly crucial for improving predictive performance, especially for products that consumers do not prefer. Furthermore, we find that consumers and products with higher uncertainty (e.g., fewer data) benefit most from our framework. This indicates that LR-Recsys is especially effective for making recommendations to new consumers and recommending new products, which are the most challenging recommendation scenarios that modern recommenders often fail to address. %, offering practical advantages for platforms in dynamic and evolving environments.

% [contributions and implications ] 
%  % Conceptually, challenging the common belief that explainability comes at the cost of model performance, we show that it is actually possible to improve both the predictive performance and explainability of black-box recommender systems. 

% \minminc{could shrink the contribution sections to save space, seems a bit repeatative.}
% \yuyan{done}

% Importantly, since these explanations are generated only from their consumption history on the platform and not from demographic features, they raise no privacy concerns, which have been emerging issues for recommender systems in recent years \citep{jeckmans2013privacy, wang2023survey}. 


Our work makes several key contributions. Methodologically, we introduce LR-Recsys, a principled ML framework that integrates LLM-generated explanations into DNN-based recommender systems. To the best of our knowledge, our work is among the first to directly combine LLM reasoning with a DNN model in an end-to-end framework, demonstrating improved predictive performance while improving the explainability of the system. Statistically, we provide theoretical insights into why LLMs can the help predictive performance and learning efficiency of these black-box ML systems, leveraging high-dimensional multi-environment learning theory. Empirically, we validate the effectiveness of our framework through extensive experiments on three real-world benchmark datasets for recommendation tasks, which can translate into significant business impacts when deployed at scale. Moreover, we establish that both positive and negative explanations are crucial to these improvements, extending beyond existing XAI literature, which typically focuses on a single type of explanation.

Our work also highlights the value of LLM-generated explanations in modern ML systems. In addition to improving the system's predictive performance, the added explainability from LLM reasoning is also valuable to the consumers, the sellers, and the platform.  For consumers, our framework demystifies black-box recommendation algorithms by providing explanations for both why they may or may not enjoy a product, which helps boost consumer's trust in the algorithms. For sellers and content creators, it provides insights into the product characteristics that most attract consumers, guiding the development of future offerings that are better aligned with consumer needs. For the platform, aggregating positive and negative explanations yields actionable insights on which product attributes appeal to specific consumer segments and which do not for better targeting strategies. Additionally, we find that consumers and products with the highest prediction uncertainty benefit most from our framework, suggesting that explanations are particularly impactful for improving new consumer experience and new product adoption. On a broader level, our work highlights the potential of leveraging Generative AI technologies to deepen understanding of consumer behavior, and using these insights to inform better product design and personalization strategies.

Lastly, our framework is general and readily applicable to any deep learning-based recommender systems. It effectively augments the information used by traditional DNN recommender systems by incorporating LLM-generated explanations. Our framework is compatible with any generative language model and remains robust regardless of the specific version or architecture used by the language model. Moreover, the implications of our findings extend beyond recommender systems. Any ML system with a predictive task can benefit from incorporating explanations from the contrastive-explanation generator in our framework. 

 % By leveraging the reasoning capabilities of LLMs, our framework helps the model to get closer to the true causal relationship between the predictors and the outcome, or the underlying data generation process, therefore having the potential to improve the efficiency and robustness of any ML model.

The rest of the paper is organized as follows. Section \ref{sec:related_work} discusses the related literature. Section \ref{sec:framework} provides a detailed description of the proposed LR-Recsys framework. In Section \ref{sec:theory}, we offer statistical insights and justifications for the framework. Experimental results are presented in Section \ref{sec:results}. Finally, Section \ref{sec:discussion} concludes with a discussion of our contributions, implications, and directions for future research.





% \todo{Add data discussion (fossil fuel of AI, Ilya's test of time talk) in to LE-Recys paper, put LE-Recsys on SSRN}
% \todo{high-level: using AI to better understanding user behavior, and use those insights to design better product and policies. Ours is one way to do this.}
% \todo{the explanations are only based on consumption history, so are privacy-friendly and enjoys the same safety standards as existing LLMs}
% \todo{This contrastive-explanation approach not only significantly improves the accuracy of predicting consumer preferences but also offers a more comprehensive understanding of consumer preferences for each product.}
% \todo{add: Through heterogeneous treatment effect analysis, we find that consumers and products with higher uncertainty (fewer data) benefit more from our framework, indicating that our framework can better help new consumers and new products.}




\begin{comment}

===

integrates LLM-generated explanations into the design of a recommender system. Challenging the common belief that explainability comes at the cost of model performance, we show that it is actually possible to improve both the predictive performance and explainability of black-box recommender systems. This is achieved by generating textual explanations using our proposed LLM-based contrastive-explanation generator, which are then embedded into the input layer of a deep neural network (DNN)-based recommendation model. Statistically, we provide theoretical insights into why LLM-generated explanations can help these black-box ML systems, leveraging high-dimensional multi-environment learning theory to explain their ability to identify important decision-making variables and improve learning efficiency. Empirically, we validate the efficacy of our framework through extensive experiments on three real-world benchmark datasets for recommendation tasks. LR-Recsys consistently outperforms state-of-the-art recommender system baselines, improving predictive performance by 3–14\%. Such gains translate into significant business impacts when deployed at scale. Moreover, we establish that both positive and negative explanations are crucial to these improvements, extending beyond existing XAI literature, which typically focuses on a single type of explanation.


While our framework is compatible with explanations generated by any natural language processing (NLP) model, we find that explanations produced by large language models (LLMs) are particularly effective. Trained on vast amount of data, LLMs are found capable of reasoning through events by identifying the possible underlying mechanisms based on their extensive world knowledge \citep{brown2020language, wei2022chain}. 

In particular, incorporating explanations into black-box ML models does not necessarily lead to a trade-off in performance; rather, it can actually \emph{improve} their predictive performance.

% There are mainly two approaches to improve explainability for recommender systems: \emph{post-hoc} explanations, which only provide insights \emph{after} the model has been trained \citep{ribeiro2016should, scott2017unified}, or on built-in interpretability, such as white-box models that force simple interpretable model architectures \citep{nauta2023anecdotal, zhang2024optimal}.\endnote{There is a third family of XAI methods that rely on supervised explanation training, where a ground-truth explanation is provided in training a model to output an explanation \citep{nauta2023anecdotal}. However, such ground-truth explanations are often not available in practice, especially in recommender system applications.} These approaches either do not change model performance (post-hoc explanation approach), or hurt model performance due to reduced model flexiblity (built-in interpretability approach). 

% Despite these developments, existing XAI methods have not improved the predictive performance of ML models. This is because these methods typically rely on \emph{post-hoc} explanations, which only provide insights \emph{after} the model has been trained \citep{ribeiro2016should, scott2017unified}, or on built-in interpretability, such as white-box models that force simple interpretable model architectures but often hurt performance \citep{nauta2023anecdotal, zhang2024optimal}.\endnote{There is a third family of XAI methods that rely on supervised explanation training, where a ground-truth explanation is provided in training a model to output an explanation \citep{nauta2023anecdotal}. However, such ground-truth explanations are often not available in practice, especially in recommender system applications.} As a result, most recommender systems in use today still rely on pure black-box ML models to predict consumer preferences \citep{covington2016deep, zhai2024actions}, as firms are unwilling to sacrifice the accuracy of their recommendation results merely to incorporate explainability into their recommender systems. 


% In contract, existing ML systems merely rely on exploiting correlations and lack of the understanding of the underlying data generation process. Specifically, state-of-the-art recommender systems rely on black-box ML models to predict the next item that a consumer will engage with, based on a sequence of her past consumed items. These systems, event built with millions of billions of parameters, still rely on the idea that ``consumers who liked similar items in the past will like similar items in the future'', which originates from the collaborative filtering literature \citep{billsus1998learning, breese1998empirical} from more than two decades ago. In other words, recommender systems today are essentially gigantic correlation extractors trying to find common patterns, without any understanding of the explanations or the reasoning behind the consumers' choices. This indicates a gap in the existing recommender system literature and an opportunity to leverage reasoning and explanations to improve their performance. 

% The impressive reasoning capabilities of LLMs therefore offer an exciting opportunity for improving understanding of both consumer preferences and product attributes, delivering crucial insights into why consumers like or dislike certain products. Such insights and understanding address a significant gap in current recommender systems, which predominantly rely on correlation rather than causation to infer consumer preferences and lack a deeper consumer and product understanding. By leveraging the advanced reasoning capabilities of LLMs, our framework not only enriches these understandings but also improves the predictive power of recommender systems.


In contrast, existing black-box ML models are essentially gigantic correlation extractors without any understanding of the underlying data generation process. As an example, state-of-the-art recommender systems rely on black-box ML models to predict the next item that a consumer will engage with, based on a sequence of her past consumed items. These systems almost exclusively rely on the idea of collaborative filtering \citep{}, or that ``similar consumers like similar products, and the products that are similar to the ones I bought should also be appealing to me''. In other words, model recommender systems are essentially gigantic correlation extractors try to find common patterns among consumer choices, without any explanations or the reasoning behind the consumers' choices.  

As an example, state-of-the-art recommender systems rely on black-box ML models to predict the next item that a consumer will engage with, based on a sequence of her past consumed items. These systems, event built with millions of billions of parameters, still rely on the idea that ``consumers who liked similar items in the past will like similar items in the future'', which originates from collaborative filtering literature \citep{} from two decades ago. In other words, recommender systems today still rely on correlational patterns to infer what a consumer likes, without any understanding of the explanations or the reasoning behind her choices.  

This is in contrast to existing  recommender systems rely on panel data and collaborative filtering, which almost exclusively rely on the idea that "similar consumers like similar products, and the products that are similar to the ones I bought should also be appealing to me". In other words, the ML models are just gigantic correlation extractors try to find common patterns, without any understanding of the explanations or the reasoning behind the consumers' choices.  

recommender systems, even built with millions of parameters, still mainly rely on the idea of ``collaborative filtering'',   In other words, existing ML models are gigantic correlation 

 This is in contrast to existing  recommender systems rely on panel data and collaborative filtering, which almost exclusively rely on the idea that "similar consumers like similar products, and the products that are similar to the ones I bought should also be appealing to me". In other words, the ML models are just gigantic correlation extractors try to find common patterns, without any understanding of the explanations or the reasoning behind the consumers' choices.  

 existing works in explaianable recsys also rely on such correlation extractor notion, therefore indicating a missed opportunity of leveraging explanations to help. 

it to more effectively process the 'why' behind decisions, thereby enhancing its predictive capabilities.

In our research, we have demonstrated that incorporating explanations into machine learning models does not necessarily lead to a trade-off in performance; rather, it can actually \emph{improve} their effectiveness

In our work, however, we showed that it is actually possible to \emph{improve} model performance by incorporating explanations, as opposed to trading-off the performance. The reason for why this is possible is that when incorporated in a smart way, the explanations can help the models reason through the ``why'' question, therefore helping the predictive power. This is in contrast to existing  recommender systems rely on panel data and collaborative filtering, which almost exclusively rely on the idea that "similar consumers like similar products, and the products that are similar to the ones I bought should also be appealing to me". These activity and content features are often presented as a panel data, and the ML models are just gigantic correlation extractors try to find common patterns. In other words, although they may have impressive predictive power, these models never gets to learn the \emph{reasons and explanations} behind why a consumer likes a product. If one can explicitly feed these reasons and explanations \emph{directly} into the learning process of the ML models, then intuitively this would help get the ML models closer to true data generation process, i.e. the ``why'' question. Existing XAI recommender systems all separately address explanation generation and model training, overlooks the potential that the explanations can actually be \emph{incorporated} into the ML model \emph{themselves}, that can help model performance while providing explainability.
==
% The idea is to identify high-quality explanations that directly help the model reason through the relationship between X and Y, therefore intuitively would help the predictive performance of using X to predict Y. 



% In particular, we propose an alternative framework to incorporate explanations into black box recommender systems. Contrary to the literature which finds that explanations do not help or even hurt model performance, we showed that it is actually possible to improve model performance by incorporating explanations, as opposed to trading-off the performance. that contrary to existing methods that hurts or does not improve performance, actually improves the system's performance.





======================
% [The success of black box ML models in many applications]

% [with its increased popularity, also increased need in understanding these models] 

why important 

% [The rise of XAI] 
Recent years people looked into explainable AI, defined as
“An explanation is a presentation of (aspects of) the reasoning, functioning and/or behavior of a machine learning model in human-understandable terms.” in ref.   

% [ XAI methods ]
without external explanations available (which is the case for most applications), there are two approaches, during training, and post-training). During training often hurt (cite), post training, by def, does not touch the model

% [however, they all have drawbacks] 
In other words, existing literature suggest that explanations does \emph{not} help model performance (cite Walter's)
i) does not touch model, ii) hurt model performance 

% [XAI in recsys]
In this paper, we look at one of the most important applications of XAI, which is recommender systems.  In business applications that involves personalization and targeting, it’s critical to understand consumer preferences and item characterisics. However, existing targeting applications such as recommender systems exclusively rely on black box machine learning models, therefore offers no insights on why a consumer likes a product, or why a product is being recommended.
=== 







1. success of recsys\endnote{test}

2. Lack of understanding - importance of explainable recsys. However, explainable recsys is not adopted in industry because explainability usually hurts performance. Yet, explainability have important managerial implications [consistent long-term consumer experience, identify improvements for products].

3. success of LLMs. potential of leveraging LLMs reasoning and generalizability to help with explainable recsys 

4. Existing LLM for recs literature focus on (1) only generating understandings / explanations, or (2) only improving recsys performance. In this work, we show that these two goals can help each other.

5. Toward this end, [our framework]. Validated on a (or several) industrial recommendation datasets. It also offers actionable insights: (1) customer aquisition (which customer segment does my product attract), and (2) improving product characteristics (how to improve my product so that it attracts more consumers)

Contribution: 
(1) Our method does not hurt recsys model performance. On the contrary, we showcase that explanations improve the recsys performance
(2) Showcase the potential of LLMs in generating explanations


\todo{Feedback from Navdeep: educate people why this is useful: an improvement in recsys metric translates to XX revenue, YY compute }

enable the recsys to focus on the ground truth generation, find hidden links 

critical aspects of the consumer and critical aspects of the product for better recommendation 


\end{comment}