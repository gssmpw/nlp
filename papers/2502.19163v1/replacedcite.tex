\section{Related Work}
\subsection{Test-Time Computing}
% \noindent \textbf{Test-Time Compute. } 
Test-time compute ____ improves LLM performance by modifying the prediction distribution during test time. Such modification is usually accompanied with extra computational cost. Instead of decoding greedily, the model may sample multiple decoding paths before aggregating them into a response. Chain of Thought ____ modifies the output distribution through hand-crafted prompts that contain reasoning chains. 
% Auto-CoT ____ leverage extra compute to generate its own CoT rationale. 
Self-consistency ____ samples multiple chain-of-thought paths and aggregate the sample with majority voting. ____ observed improved accuracy and robustness by querying the model with semantically equivalent prompts before responding with the majority answer. 
% Another approach to leverage extra compute is through employing a separate model for subtasking. 
% ____ proposes using a separate model to retrieve relevant passages which the LLM conditions on for prediction. 
____ uses sentence embeddings to retrieve k-nearest-neighbor demonstration for in-context learning. ____ retrieves relevant and diverse demonstrations by training a model that predicts the relevance of a demonstration via contrastive learning ____. 
% More broadly, verifiers ____ are widely used to evaluate generated candidate ____ or provide feedback for iterative improvement ____, which improves LLMs performance on reasoning tasks.
Our work is directly inspired by the KNN method proposed by ____. Later work has revealed that similarity based demonstration retrieval improves in-context learning because LLMs attend to the most similar demonstration during few-shot prompting ____. Instead of using similar demonstrations for in-context learning, we explore using them as near neighbors in the fashion of non-parametric prediction.


\subsection{In-Context Learning}
Apart from Chain-of-Though, many work explore the possibility of using self-generated content by the LLM to aid with reasoning or classification. STaR ____ iteratively add self-generated rationales that are proved correct by a verifier to the exist pool of demonstrations. A significant limitation of STaR is that it relies on knowing the correct answer to the questions the LLM is generating rationale for. Our method simply make predictions for neighboring examples, which does not require ground truth labels. Auto-CoT____ uses self-generated rationales as demonstrations for similar inputs. The generated data by Auto-CoT incurs a quadratically scaling overhead to the final prediction. Our proposed method only incurs a linearly scaling overhead due to the nature of nearest-neighbor algorithm. Self-ICL ____ generated its own demonstration and their pseudo-labels and uses them as demonstrations. We disagree with Self-ICL's premise that even unlabeled data are hard to come by in realistic settings, and posit that unlabeled data are abundant and inexpensive to obtain ____. Thus, self-generated demonstration inputs are unnecessary. Like Auto-CoT, Self-ICL's test-time compute overhead also scales quadratically. Lastly, Auto-CoT, STaR, and Self-ICL all focuses on reasoning tasks, whereas our work primarily focuses on classification tasks.


% \subsection{Cluster-Based Unsupervised Learning}
% ____ reported that a self-supervised model trained on a clustering objective resulted in representation whose clustering roughly corresponds to downstream classification categories. Considering that two of the embeddings models that we experiment with are pre-trained with clustering or similarity tasks, it is reasonable to expect such models producing nearest neighbors from the same class.

% Henry:
% [Main Difference with related work in TTS, tbe] Our approach introduces a novel dimension to test-time compute methodologies by systematically leveraging unlabeled data neighborhoods. While existing test-time compute techniques like chain-of-thought and self-consistency primarily focus on internal reasoning strategies, our method expands the computational context by incorporating external unlabeled data signals ... \\
% [Some other famous/related TTC work in LLM: Best-of-N Sampling, STaR Algorithm (Self-Taught Reasoner), Self Verification/Verifier, Search Methods/Monte Carlo Tree Search (MCTS), check more details in this blog https://cloudsecurityalliance.org/blog/2024/12/13/test-time-compute]

% The intuition behind TestNUC is that samples in close proximity within a semantic space are likely to share similar labels. By incorporating predictions on nearby unlabeled samples, the LLM can better contextualize and refine its decision-making. This approach exploits the consistency of local data structures, effectively using unlabeled examples as an auxiliary signal to boost performance and reduce the noise and uncertainty associated with isolated predictions.