% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[preprint]{acl}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% Tree: additional packages
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}

\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}%
\newcommand{\xmark}{\text{\ding{55}}}%

\usepackage{color}
\usepackage{tabularray}
% \usepackage[table]{xcolor} % Enable color for table environments

\usepackage{newunicodechar}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{makecell}




% Tree: table
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\usepackage{lipsum}
\usepackage{tikz}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{multicol}
\usepackage{siunitx}
\usepackage{xpatch}
\usepackage{arydshln} % Add this line to your preamble

\usepackage{multirow}  % for multi row
\usepackage{pifont}
\usepackage{amsmath}


%% customized command
\newcommand{\henry}[1]{\textcolor{red}{\bf\small [#1 --henry]}}
\newcommand{\yue}[1]{\textcolor{blue}{\bf\small [#1 --yue]}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\newcommand\model{TestNUC}

% Enhancing Test-Time Compute with Neighboring Unlabeled Data Consistency
% \title{Temporary: Exploring and Exploiting Neighboring Unlabeled Data Consistency for LLM Classification}
% \title{TestNUC: Enhancing LLM Test-Time Predictions through Neighboring Unlabeled Data Consistency}
\title{\model: Enhancing Test-Time Computing Approaches through Neighboring Unlabeled Data Consistency}


\author{
 \textbf{Henry Peng Zou\textsuperscript{1}\thanks{Equal Contribution.}},
 \textbf{Zhengyao Gu\textsuperscript{1}\footnotemark[1]},
 \textbf{Yue Zhou\textsuperscript{1}},
 \textbf{Yankai Chen\textsuperscript{2}},
 \textbf{Weizhi Zhang\textsuperscript{1}},
\\
 \textbf{Liancheng Fang\textsuperscript{1}},
 \textbf{Yibo Wang\textsuperscript{1}},
 \textbf{Yangning Li\textsuperscript{3}},
 \textbf{Kay Liu\textsuperscript{1}},
 \textbf{Philip S. Yu\textsuperscript{1}}
\\
 \textsuperscript{1}University of Illinois Chicago,
 \textsuperscript{2}Cornell University
 \textsuperscript{3}Tsinghua University
\\
 \texttt{
   % \textbf{Correspondence:} 
   % \href{mailto:pzou3@uic.edu}
  \{pzou3, zgu24\}@uic.edu
 }
}


\begin{document}
\maketitle
\begin{abstract}
Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data—it classifies an input instance by considering not only the model’s prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at \textcolor{blue}{\url{https://github.com/HenryPengZou/TestNUC}}.
\end{abstract}



\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/radar_subplots.pdf} % example-image-duck
    % \caption{TestNUC consistently enhances both \textit{output-level} (e.g., Self-Consistency, Best-of-N) and \textit{input-level} (e.g., TopK-ICL) test-time computing approaches across eight diverse datasets. Details in Section \ref{sec:experiment}.}
    \caption{TestNUC effectively integrates with both \textit{output-level} (e.g., Self-Consistency, Best-of-N) and \textit{input-level} (e.g., ICL-based) test-time computing methods, consistently enhancing their performance across eight datasets. More details in Section \ref{sec:experiment} and Table \ref{tab:main_enhance}.}
    \label{fig:radar_plot}
    % \vspace{-4mm}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Test-time computing approaches, which leverage additional computational resources during inference to 
% modify a model's predicted distribution and 
enhance performance, have gained increasing attention in the era of large language models (LLMs) \cite{test-time-compute, dong2024survey}.
There are two primary strategies for modifying an LLM’s distribution at test time: (1) \textbf{at the \textit{input level}}: augmenting the prompt with additional tokens (e.g., few-shot in-context learning \cite{mosbach-etal-2023-shot}); or (2) \textbf{at the \textit{output level}}: sampling multiple candidate answers and aggregating them (e.g., self-consistency \cite{self-consistency}, best-of-N \cite{beeching2024scalingtesttimecompute}).
Despite demonstrating promising capabilities, input-augmentation approaches incur a computational cost that scales \textit{quadratically} with the number of added tokens in the prompt, making them more computationally expensive than output-sampling methods. Meanwhile, output-sampling approaches typically overlook the potential of \textit{large amounts of unlabeled data} that are often available in real-world settings \cite{berthelot2019mixmatch, sohn2020fixmatch, zou-caragea-2023-jointmatch, zou2025gleangeneralizedcategorydiscovery}.


To bridge these gaps, we present an initial exploration of how unlabeled data can be efficiently leveraged to enhance test-time computing approaches. We hypothesize that instances with similar embeddings are likely to share the same semantic label, which can provide unsupervised signals for improving inference consistency, particularly for challenging instances \cite{van2020scan}. Our pilot experiments across various benchmarks reveal strong semantic label consistency among neighboring instances, and we find that aggregating these neighborhood labels through simple aggregation methods such as majority voting leads to stable and accurate predictions (as shown in Figure \ref{fig:analysis_neighborhood_purity}, \ref{fig:analysis_majority_vote} in Section \ref{sec:analysis}).
% To this end, we start with pilot experiments across various benchmarks and observe strong semantic label consistency among neighboring instances and that aggregating these neighborhood labels, e.g., majority voting, leads to stable and accurate predictions (as shown in Figure \ref{fig:analysis_neighborhood_purity}, \ref{fig:analysis_majority_vote} in Section \ref{sec:analysis}). 


Motivated by these findings, we propose \textbf{\model}, a simple yet effective approach that enhances test-time LLM predictions by leveraging neighboring unlabeled data consistency. Concretely, TestNUC consists of two key steps: \ding{182} \textbf{Neighbor Retrieval}, where we identify the top-K nearest unlabeled neighbors of a test sample based on feature similarity; and \ding{183} \textbf{Collaborative Prediction}, where the LLM generates predictions for both the test sample and its retrieved neighbors, which are then aggregated to obtain the final answer. 
The intuition behind \model~is that samples in close proximity within the embedding space are likely to share similar labels.
By incorporating predictions of nearby unlabeled samples, the LLM can exploit the consistency of local data structures to better contextualize and refine its decision-making, effectively using unlabeled examples as an auxiliary signal to boost test-time performance while reducing noise and uncertainty \cite{van2020scan, zhou-etal-2024-paraphrase}.


We evaluate our approach across diverse tasks, including intent classification, topic mining, domain discovery, and emotion detection, using eight datasets that cover a wide spectrum of granularities, with class sizes ranging from 10 to 150. 
Our results demonstrate that \model~consistently outperforms baseline methods, such as standard prompting and self-consistency \cite{self-consistency}, by a large margin across four large language models, showing its effectiveness in leveraging unlabeled data for test-time computation. 
Moreover, \model~can be seamlessly integrated with existing test-time computing approaches, such as TopK-ICL \cite{peng-etal-2024-revisiting, gao2024on}, best-of-N \cite{lightman2024lets, beeching2024scalingtesttimecompute} and self-consistency \cite{self-consistency}, significantly boosting their performances (as illustrated in Figure \ref{fig:radar_plot}). 
In addition, \model~is effective across various embedders of different sizes and scales well with increasing amounts of unlabeled data (as shown in Figure \ref{fig:influence_num_unlabeled_data_log}), making it applicable to real-world scenarios. 



\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/analysis_neighborhood_purity.pdf} % example-image-duck
    \caption{Neighboring samples tend to be instances of the same semantic class.}
    \label{fig:analysis_neighborhood_purity}
    % \vspace{-4mm}
\end{figure}




% ---
% 2.2 Unlabeled Data Utilization
% (From history, Leveraging unlabeled to improve performance is not a new idea. (Some Hisotry) For example, traiditonal Unsup/Semi-Supervised Learning .... However, leveraging unlabeled in test compute is under-explored. .(Some Recent Work that leverage unlabeled data in LLM during inference time)... Our work is the first to ...(Difference)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Analysis and Preliminary Studies

\section{Preliminary Analysis}\label{sec:analysis} 

% The predictive power of semantic neighbors has been well-established across various machine-learning domains. When faced with challenging classification decisions, examining the labels of semantically similar instances often provides valuable signals, as features with high similarity frequently share class labels. However, the extent to which this neighborhood information (unlabeled) can enhance large language model predictions remains unclear.

% To understand this, we first assume the ground truth labels of the neighbor data points are accessible and use eight classification benchmarks spanning various granularities with class sizes. Detailed dataset descriptions and statistics are provided in Section \ref{sec:experiment_setup} and Table \ref{tab:dataset_statistics}. We examine the semantic relationships within local neighborhoods by introducing \textbf{neighborhood purity} - the proportion of top-K neighbors sharing the centroid point's semantic class. As shown in Figure~\ref{fig:analysis_neighborhood_purity}, nearest neighbors frequently belong to the same semantic class as their centroid.
% This finding suggests a semantic class consistency in the local neighborhoods.

% Then, to assess the predictive value of neighborhood information, we analyze how accurately a majority vote over neighboring ground-truth labels predicts the centroid's label. Figure~\ref{fig:analysis_majority_vote} compares this majority-vote accuracy with neighborhood purity across different K values, revealing several key insights: (1) Majority voting over neighboring labels consistently produces accurate centroid predictions; (2) While larger K values decrease neighborhood purity due to noise introduction, majority-vote accuracy remains notably stable; (3) Feature similarity-based weighting improves prediction stability for large K values by diminishing the impact of less relevant neighbors. These findings establish a strong indicator for leveraging neighborhood dynamics to reduce prediction uncertainty and enhance inference-time prediction performance.
% in LLM prediction tasks.

Leveraging neighboring examples at inference time has been shown to improve the generalization of language models~\cite{knn-lm}, mitigate prompting bias~\cite{knn-prompting}, and improve retrieval-augmented generation~\cite{knn-zero-shot}. Building on these findings, we explore a more focused question: To what extent can semantically similar neighborhood data serve as effective prediction proxies and potentially enhance LLM predictions at test time? 

To understand this, we introduce \textit{neighborhood purity}, which measures how often semantically similar examples share the same label. 
Formally, let $\mathcal{D} = {(x_i, y_i)}_{i=1}^N$ be a set of inputs and corresponding ground truth labels, where $N$ is the total number of data points. We denote the $K$-nearest \textit{neighborhood} of an input $x$ as $\mathcal{N} = \arg\!\operatorname{top}_K \{\mathcal{S}_f\bigl(x, x_i)\,|\; i=0,\dots,N\}$, representing the set of indices corresponding to the most similar instances according to an embedding function $f$. We refer to $x$ as the anchor of the neighborhood and measure the consistency of its neighborhood with \textit{purity} $\phi$, defined as:
\begin{equation}
    \phi\left(\mathcal{N}\right) = \frac{1}{KN} \sum_{i=1}^N\sum_{j \in \mathcal{N}} \mathbf{1}(y_i = y_j)
\end{equation}
Intuitively, purity measures the proportion of instances that share the same label as the anchor.


We conduct our preliminary experiments across eight datasets spanning class granularities from 10 to 150. Detailed dataset descriptions and statistics are provided in Section \ref{sec:experiment_setup} and Table \ref{tab:dataset_statistics}. As shown in Figure~\ref{fig:analysis_neighborhood_purity}, nearest neighbors frequently belong to the same semantic class as the anchor. In the worst case, purity still reaches around 0.3 when $K = 20$ on the GoEmotion dataset. 

Then, we ascertain how accurately the aggregation over neighboring ground-truth labels predicts the anchor’s label. To this end, we consider two aggregation strategies: \textit{majority vote} and \textit{weighted majority vote}. Majority vote returns the most frequent class label in the neighborhood: 
\begin{equation} % \label{eqn:majority_vote}
    \hat{y}_m(\mathcal{N}) = \argmax_{y} \sum_{i \in \mathcal{N}} \mathbf{1}(y = y_i)
\end{equation}
while weighted majority vote adjusts label counts based on similarity in representation space: 
\begin{equation} % \label{eqn:weighted_majority_vote}
    \hat{y}_{w}(\mathcal{N}) = \argmax_{y}\sum_{i \in \mathcal{N}} \mathcal{S}_f(x, x_i) \mathbf{1}(y = y_i)
\end{equation}
Figure~\ref{fig:analysis_majority_vote} compares majority-vote accuracy with neighborhood purity across different K values, revealing several key insights: (1) Majority voting over neighboring labels consistently produces accurate anchor predictions; (2) While larger K values decrease neighborhood purity due to noise introduction, majority-vote accuracy remains notably stable, indicating its robustness to the hyperparameter $K$. (3) Similarity-based weighting improves prediction stability for large K values by reducing the impact of less relevant neighbors. 
These findings suggest semantically similar neighborhood data can serve as effective prediction proxies, offering a potential means to enhance LLM predictions at test time. 


% \subsection{Neighboring Samples Tend to Be Instances of the Same Semantic Class}

% We first investigate the semantic relationship between a data point and its nearest neighbors. Figure~\ref{fig:analysis_neighborhood_purity} reports the proportion of top-K neighboring samples that share the same semantic class as the centroid point, a metric we term ``neighborhood purity.'' Our findings reveal that, in many cases, the nearest neighbors (semantically) belong to the same class as the centroid. This empirical observation motivates our method, suggesting that leveraging the local neighborhood can provide valuable contextual information for improving inference-time performance and reducing noise and uncertainty.




% \subsection{Majority Vote Over Neighborhood Labels Leads to Stable and Accurate Predictions}

% Figure~\ref{fig:analysis_majority_vote} illustrates the accuracy of the majority vote over neighborhood data ground-truth labels in predicting the correct label of the centroid data. We also include neighborhood purity for comparison and analysis. It can be observed that: (1) the majority vote over neighborhood data ground-truth labels generally leads to correct centroid predictions; (2) when using large amounts of neighbors (denoted as $K$), neighborhood purity tends to decline rapidly, introducing significant noise. However, the accuracy of the neighborhood majority vote is shown to be much more stable across different numbers of nearest neighbors than neighborhood purity. Furthermore, incorporating a weighting mechanism based on feature similarity further enhances the stability, particularly for very large K values.



\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/analysis_majority_vote.pdf} % example-image-duck
    \caption{Majority vote over neighborhood ground-truth labels leads to stable and accurate predictions. Incorporating feature similarity-based weighting further improves stability for large K values by mitigating noise.}
    \label{fig:analysis_majority_vote}
    % \vspace{-4mm}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
Motivated by our findings in Section \ref{sec:analysis}, we propose \model, a test-time computing strategy that leverages neighboring unlabeled data consistency to enhance LLM predictions. Our approach introduces a complementary dimension to test-time computing by integrating signals from unlabeled data during inference. 


\subsection{Framework Overview}
TestNUC consists of two key steps:
\begin{itemize}\setlength{\itemsep}{0pt}\setlength{\leftskip}{0pt}\setlength{\parindent}{0pt}
    \item \textbf{Step 1: Neighbor Retrieval.} Identify the top-K nearest neighbors of a test sample based on feature similarity. 
    % For notation ease, denote \( x_0 \)  as the test sample, and \( \bigcup_{k=1}^K \{ q_k \} \) as the \( K \) retrieved neighbors.
    \item \textbf{Step 2: Collaborative Prediction.} Prompt the LLM to generate predictions for both the test sample and its \( K \) retrieved neighbors. These predictions are combined through a designed aggregation strategy.
\end{itemize}
Note that TestNUC is based on LLM predictions instead of the ground truth label. The intuition behind TestNUC is that samples in close proximity within the embedding space are likely to share similar labels. By incorporating predictions on nearby unlabeled samples, the LLM can better contextualize and refine its decision-making. This approach aims to exploit the consistency of local data structures, effectively using unlabeled examples as an auxiliary signal to boost inference-time performance and reduce the noise and uncertainty associated with isolated predictions.


\input{algorithm/algorithm}

\subsection{Aggregation Strategy}
The aggregation strategy in Step 2 affects the sensitivity of TestNUC to noise. In this work, we explore three types of aggregation strategies. \\

\noindent \textbf{Naive Majority Voting.} The naive approach simply selects the most consistent answer across the \( K \) unlabeled data predictions. \\

\noindent \textbf{Weighted Majority Voting.} As demonstrated in our analysis in Section \ref{sec:analysis}, when using a large \( K \), neighborhood purity tends to decline rapidly. This indicates that distant neighbors can introduce significant noise and negatively impact the accuracy of majority voting. To mitigate this issue, we additionally use cosine similarity distance between the test sample and its neighbors as weights for majority voting. \\

\noindent \textbf{Filtered Weighted Majority Voting.} The quality of LLM's predictions for neighboring unlabeled data can affect the accuracy of the aggregated results. In this approach, we explore leveraging verbalized confidence to filter out low-quality predictions during majority voting. Specifically, for each unlabeled data, we ask LLM to generate both the prediction and confidence in its predictions and only high confidence predictions are kept for majority voting. \\

\noindent A complete algorithm for Filtered Weighted Majority Voting is presented in Algorithm \ref{alg:algorithm}. The algorithms for the other two voting strategies mentioned above can be obtained by removing the blue- and red-colored code. More complex aggregation strategies can also be explored, such as adding additional distance-based filtering mechanisms or confidence-weighting mechanisms, which we leave for interested researchers to explore.






% \model~consists of two key steps:
% \begin{itemize}[leftmargin=*]
% \setlength{\itemsep}{0pt}\setlength{\leftskip}{0pt}\setlength{\parindent}{0pt}
%     \item \textbf{Step 1: Neighbor Retrieval.} Retrieve the top-K nearest neighbors $\mathcal{N}_K(x)$ of a test sample based on feature similarity.
%     \item \textbf{Step 2: Collaborative Prediction.} Prompt the LLM to generate predictions and confidences for both the test sample and its \( K \) retrieved neighbors. These predictions are combined through a designed aggregation strategy.
% \end{itemize}
% Note that \model~differs from Eqn. \ref{eqn:majority_vote} and Eqn. \ref{eqn:weighted_majority_vote} in that the vote counts are based on LLM predictions $\hat{y}$ instead of the ground truth label $y$. That is the majority vote and weighted majority vote aggregation functions for \model~are 
% \begin{equation} \label{eqn:majority_vote}
%     \hat{y}_m(\mathcal{N}) = \argmax_{y} \sum_{i \in \mathcal{N}} \mathbf{1}(y = \hat{y}_i),
% \end{equation}
% and
% \begin{equation} \label{eqn:weighted_majority_vote}
%     \hat{y}_w(\mathcal{N}) = \argmax_{y}\sum_{i \in \mathcal{N}} \mathcal{S}_f(x, x') \mathbf{1}(y = \hat{y}_i),
% \end{equation}
% respectively. 
% The intuition behind \model~is that samples in close proximity within the embedding space are likely to share similar labels. 
% By incorporating predictions on nearby unlabeled samples, the LLM can better contextualize and refine its decision-making. 
% % This approach aims to exploit the consistency of local data structures, effectively using unlabeled examples as an auxiliary signal to boost inference-time performance and reduce the uncertainty associated with isolated predictions.




\input{tables/table1}
\input{tables/table2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Experiments}
% \subsection{Experiment Settings}
% \subsection{Main Results}
% \subsection{Additional Studies}
% adding tables \& results, average runs...\\
% adding test-time compute scaling experiment


\begin{figure*}[!th]
    \centering
    \includegraphics[width=1\textwidth]{figures/influence_of_unlabeled_data.pdf} % example-image-duck
    \caption{Increasing the amount of unlabeled data consistently boosts performance across all evaluated LLMs and datasets. The scaling trends are more distinctly visible in the logarithmic version of the figure (Figure \ref{fig:influence_num_unlabeled_data_log}).}
    \label{fig:influence_num_unlabeled_data}
    % \vspace{-4mm}
\end{figure*}

\begin{figure*}[!th]
    \centering
    \includegraphics[width=1\textwidth]{figures/influence_of_unlabeled_data_log.pdf} % example-image-duck
    % \caption{Log Version - Scaling: Influence of Number of Unlabeled Data.}
    \caption{The logarithmic version of Figure \ref{fig:influence_num_unlabeled_data}.}
    % Increasing the amount of unlabeled data consistently boosts performance across all evaluated LLMs and datasets. 
    \label{fig:influence_num_unlabeled_data_log}
    % \vspace{-4mm}
\end{figure*}

\section{Experiments}
\label{sec:experiment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment Setup}
\label{sec:experiment_setup}

\noindent \textbf{Tasks and Datasets.} We consider eight datasets across diverse tasks with various perspectives and granularities as follows.
% with various perspectives and granularities, including intent detection, topic mining, domain discovery, type discovery, and emotion detection.
\begin{itemize}[leftmargin=*]
\setlength{\itemsep}{0pt}\setlength{\leftskip}{0pt}\setlength{\parindent}{0pt}
    \item \textbf{Intent Detection.} Intent detection aims to discover fine-grained intents in customer utterances. We use BANKING \cite{banking} and CLINC \cite{clinic} for evaluation.

    \item  \textbf{Topic Mining.} We use Reddit and StackExchange from MTEB \cite{muennighoff-etal-2023-mteb} and ClusterLLM \cite{zhang-etal-2023-clusterllm} to evaluate models' ability to categorize discussion topics.

    \item  \textbf{Domain Discovery.} For this task, we use MTOP \cite{mtop} and CLINC(D) \cite{zhang-etal-2023-clusterllm} to allow evaluations of models' capability in discovering domain-specific knowledge.

    \item  \textbf{Type Discovery.} We use the FewEvent dataset \cite{FewEvents} that focuses on extracting event types from the given text and event triggers. 

    \item  \textbf{Emotion Recognition.} We use GoEmotion \cite{goemotions}, which is a dataset of Reddit comments labeled with fine-grained emotions, such as amusement, fear and gratitude.
\end{itemize}
\noindent Dataset statistics are summarized in Appendix \ref{appendix:dataset}.
\\


\noindent \textbf{Baselines. } We consider three types of baselines: 

\noindent \textbf{\ding{182}} \textbf{Standard Prompting}, which prompts the LLM in a standard way to select a label from the provided options to a test sample. The details of the prompt template are available in Appendix \ref{sec:prompt_template}. \\

\noindent \textbf{\ding{183}} Test-time computing approaches that operate \textit{at the input level} by augmenting the given prompt with additional demonstrations to enhance inference performance. 
Since our proposed method combines decisions based on similar examples, we compare it with two varieties of in-context learning counterparts: \textbf{TopK-ICL} \cite{peng-etal-2024-revisiting}, where the input text of the nearest neighbors of the test example are added to the prompt as context information. \textbf{TopK-ICL-P}, where we additionally append each neighbor's Standard Prompting prediction result to its text as demonstrations. \\

\noindent \textbf{\ding{184}} Test-time computing approaches that operate \textit{at the output level} through multiple candidate answer sampling and aggregation to boost output quality. For this category, we consider three representative approaches: \textbf{Self-Consistency} \cite{self-consistency}, \textbf{Best-of-N} \cite{test-time-compute, beeching2024scalingtesttimecompute}, and \textbf{Weighted Best-of-N} \cite{beeching2024scalingtesttimecompute}. 
Specifically, Best-of-N selects the most confident predictions out of multiple predictions based on the LLM's own verbalized confidence \cite{verbalized-confidence}. Weighted Best-of-N aggregates the decisions by assigning weights based on their respective confidence score.\\



\noindent \textbf{Implementation Details. } We utilize both open-sourced and close-sourced LLMs with varying scales: GPT-4o-mini, GPT-4o \cite{gpt4o}, Llama-3.1-8B \cite{dubey2024llama}, Claude 3 Haiku \cite{claude3}. We set temperature $T=0.7$  and Top-p $= 1.0$ for sampling decoding for all evaluated language models. By default, the number of candidate answers $N$  we sampled for Self-Consistency, Best-of-N and Weighted Best-of-N is 10. Similarly, the number of retrieved neighbors, i.e., $K$, for TopK-ICL, TopK-ICL-P, and our TestNUC is 10 unless stated otherwise. We adopt NV-Embed-v2-7B \cite{lee2024nv} as the embedding model for all methods. Due to resource constraints, we randomly sample 500 data points from each dataset for evaluation and use the remaining for neighboring sample retrieval.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Results}

% \begin{enumerate}
%     \item Comparison with Standard Prompting and Self-Consistency
%     \item Enhancing existing test-time compute approaches: TopK-ICL, TopK-ICL-P, Self-Consistency, Best-of-N, Weighted Best-of-N.
%         % \textbf{Adds-on/Plug\&Play}: TestNUC can be combined with other methods to improve them.
% \end{enumerate}


\noindent \textbf{Comparison with Standard Prompting and Self-Consistency. } Table \ref{tab:main_compare_sc} presents the comparison results with Standard Prompting and Self-Consistency across four large language models. It can be observed that \model~significantly improves the inference performance of four large language models on all eight evaluated datasets over standard prompting. TestNUC can also outperform Self-Consistency when utilizing the same amount of sampling paths and neighboring unlabeled data (i.e., $K$=10 in both cases). For example, TestNUC surpasses Self-Consistency by 5.87\% on average when using Llama-3.1-8B model and 5.48\% on average when using GPT-4o-mini. Besides, \model performance can be further boosted by utilizing more neighboring unlabeled data. TestNUC$\dagger$, which utilizes 50 neighbors, can improve the performance over standard prompting up to 11.35\% on average across 8 datasets when using Claude-3-Haiku. Additionally, performance improvements are observed across all four language models, even in the already powerful GPT-4o model. 
% \\

% Main Findings: (1) TestNUC significantly outperforms standard prompting; (2) TestNUC outperforms Self-Consistency under the fair comparison setting; (3) TestNUC performance can be further boosted by utilizing more neighbors. (4) Improvements can be seen over all four language models, even the already powerful GPT-4o model.

\input{tables/table3}

\noindent \textbf{TestNUC can enhance various existing test-time computing approaches. } The results are shown in Table \ref{tab:main_enhance}.  Across all baselines and all datasets, incorporating TestNUC boosts performance. The average improvements range from about +6\% (TopK-ICL-P) to +10\% (Best-of-N and Weighted Best-of-N), indicating that TestNUC is complementary to diverse inference strategies—both those that prepend demonstrations at the input level (ICL-based) and those that do sampling and “post-hoc” candidate refinements at the output level (Self-Consistency, Best-of-N). Methods that work at the output level (e.g., Self-Consistency, Best-of-N) present larger average gains (9–10\%), compared to input-level approaches (6–7\%). 
The biggest performance gains often appear on the Topic Mining tasks (e.g., Reddit, StackEx), where improvements can exceed +15–20\%.
This suggests that adding \model~is especially helpful in scenarios involving more open‐ended or noisy textual inputs, where post-hoc aggregations can more effectively disambiguate the model’s initial outputs.



\input{tables/table4}

\vspace{10pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Studies}

% \begin{enumerate}
%     \item \textbf{Scaling:} TestNUC can scale when you have more unlabeled data; practical in real-world scenarios. (Line Plot \& Grid Plot)
%     \item \textbf{Aggregation Strategies:} Distance, Confidence. (Table)
%     \item \textbf{Embedders:} TestNUC works on different Embedders (different capabilities, company, size, dimension). (Table)
%     \item \textbf{\# Neighbors:} Influence of Number of Neighbors. (Line Plot)
%     % \item \textbf{(Optional) Uncertainty Indication:} Neighboring Unlabeled Data Consistency is highly correlated with accuracy, suggesting one can use neighboring unlabeled data consistency to provide an uncertainty estimate of the model in its generated answers. (Dot Plot)
%     \item \textbf{Qualitative Examples: } (illustrate why our approach help: original prediction is incorrect, but neighbors are correct); Neighbors help repair the error over centroid prediction. (Visualization)
% \end{enumerate}
\subsection{Influence of Unlabeled Data Size}

\noindent \textbf{Increasing unlabeled data helps boost performance across tasks.}
Figure~\ref{fig:influence_num_unlabeled_data} reports the linear-scale results on BANKING, CLINC, Reddit, StackEx, FewEvent, and an overall average across eight tasks. In all cases, increasing the unlabeled set yields notable accuracy improvements for GPT-4o-mini, Llama-3.1-8B, and Claude-3-Haiku. Adding even a modest number (e.g., 500–1k) of unlabeled instances yields substantial accuracy gains, especially on BANKING and Reddit.
However, improvements taper off after roughly 8–10k unlabeled samples, suggesting a saturation point where additional unlabeled data provides diminishing returns. 
The log-scale plots in Figure~\ref{fig:influence_num_unlabeled_data_log} further highlight these trends, confirming that the utility of unlabeled examples gradually diminishes but still delivers meaningful improvements up to the 10K–15K range. This pattern holds consistently across all tasks, confirming that increasing unlabeled data universally improves performance but with predictable saturation effects. \\

\vspace{-5pt}
\subsection{Aggregation Strategy Comparison}

We find that \textbf{naive majority voting greatly surpasses standard prompting performance, with advanced strategies further enhancing results.} As shown in Table \ref{tab:voting_comparison}, simply aggregating multiple predictions with naive majority voting can already boost average accuracy significantly from 0.613 to 0.670. Introducing distance and confidence weighting further refines these gains from the average to 0.680. Finally, filtering out low-confidence predictions yields the highest performance, although on certain tasks (e.g., GoEmotion), Weighted Majority Voting (Distance \& Confidence) can be more effective, suggesting that a carefully tuned confidence threshold may be necessary for each dataset. \\

\vspace{-5pt}
\subsection{Varying Embedding Models}
\noindent \textbf{TestNUC works on different sizes of embedders. } The embedders used to generate data embeddings for neighbor retrieval play a crucial role in the success of TestNUC. In this work, we explore diverse embedding models, including public encoders from different companies and embedders of different sizes ranging from 120M to 7B. 
As shown in Table \ref{tab:embedder_comparison}, TestNUC is effective when applied to various embedding models. Not surprisingly, TestNUC achieves significant improvements with larger and more advanced embedders, such as NV-Embed-v2-7B, which records the highest average performance (0.755) and excels across all datasets. 
Mid-sized embedders like stella-en-400M-v5 (0.738) and gte-Qwen2-1.5B-instruct (0.732) also perform well, demonstrating that TestNUC can effectively leverage diverse embedding architectures. Even smaller models, such as all-MiniLM-L12-v2-120M (0.720), deliver competitive results over standard prompting (0.682), showcasing TestNUC's robustness across varying model sizes and complexities. 


\begin{figure*}[!th]
    \centering
    \includegraphics[width=1\textwidth]{figures/ablation_num_neighbors.pdf} % example-image-duck
    \caption{Influence of the number of neighbors. The results show that even a small set of neighbors can significantly boost performance for all three LLMs, significantly surpassing their zero-neighbor baselines.}
    \label{fig:ablation_num_neighbors}
    % \vspace{-4mm}
\end{figure*}

\subsection{Influence of Neighbor Size}
\noindent \textbf{Influence of the Number of Neighboring Unlabeled Data. } Figure~\ref{fig:ablation_num_neighbors} shows the results of TestNUC from GPT-4o-mini, Llama-3.1-8B, and Claude-3-Haiku as the number of neighbors increases. 
The results show that even a small set of neighbors can significantly boost performance for all three models, significantly surpassing their zero-neighbor baselines. Additionally, all three models generally benefit from increasing the number of neighbors from 0 to 60, although the gains tend to plateau after approximately 40–60 neighbors. Notably, the performance of GPT-4o-mini and Llama-3.1-8B slightly decreases when the number of neighbors increases from 60 to 80 on certain datasets, likely due to the introduction of more noisy neighbors. In contrast, Claude-3-Haiku often achieves higher accuracies with relatively larger neighborhood sizes (e.g., 60–100), indicating greater robustness to noise.



% \subsection{Case Study}
% \subsection{Qualitative Analyses}

% \textbf{Qualitative Examples: } (illustrate why our approach help: original prediction is incorrect, but neighbors are correct); Neighbors help repair the error over centroid prediction. (Visualization)
% \lipsum[4]


% \item \textbf{Robustness:} TestNUC Improves Robustness to Different Sets of Testing Samples. (Bar Plot)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

\subsection{Test-Time Computing}
% \noindent \textbf{Test-Time Compute. } 
Test-time compute \cite{test-time-compute} improves LLM performance by modifying the prediction distribution during test time. Such modification is usually accompanied with extra computational cost. Instead of decoding greedily, the model may sample multiple decoding paths before aggregating them into a response. Chain of Thought \cite{chain-of-thought} modifies the output distribution through hand-crafted prompts that contain reasoning chains. 
% Auto-CoT \cite{Auto-CoT} leverage extra compute to generate its own CoT rationale. 
Self-consistency \cite{self-consistency} samples multiple chain-of-thought paths and aggregate the sample with majority voting. \cite{ensemble-of-prompt} observed improved accuracy and robustness by querying the model with semantically equivalent prompts before responding with the majority answer. 
% Another approach to leverage extra compute is through employing a separate model for subtasking. 
% \cite{RAG} proposes using a separate model to retrieve relevant passages which the LLM conditions on for prediction. 
\cite{TopK} uses sentence embeddings to retrieve k-nearest-neighbor demonstration for in-context learning. \cite{DPP} retrieves relevant and diverse demonstrations by training a model that predicts the relevance of a demonstration via contrastive learning \cite{SimCLR}. 
% More broadly, verifiers \cite{V-STaR, verify-step-by-step} are widely used to evaluate generated candidate \cite{STaR} or provide feedback for iterative improvement \cite{ReAct}, which improves LLMs performance on reasoning tasks.
Our work is directly inspired by the KNN method proposed by \cite{TopK}. Later work has revealed that similarity based demonstration retrieval improves in-context learning because LLMs attend to the most similar demonstration during few-shot prompting \cite{anchor-words}. Instead of using similar demonstrations for in-context learning, we explore using them as near neighbors in the fashion of non-parametric prediction.


\subsection{In-Context Learning}
Apart from Chain-of-Though, many work explore the possibility of using self-generated content by the LLM to aid with reasoning or classification. STaR \cite{STaR} iteratively add self-generated rationales that are proved correct by a verifier to the exist pool of demonstrations. A significant limitation of STaR is that it relies on knowing the correct answer to the questions the LLM is generating rationale for. Our method simply make predictions for neighboring examples, which does not require ground truth labels. Auto-CoT\cite{Auto-CoT} uses self-generated rationales as demonstrations for similar inputs. The generated data by Auto-CoT incurs a quadratically scaling overhead to the final prediction. Our proposed method only incurs a linearly scaling overhead due to the nature of nearest-neighbor algorithm. Self-ICL \cite{self-icl} generated its own demonstration and their pseudo-labels and uses them as demonstrations. We disagree with Self-ICL's premise that even unlabeled data are hard to come by in realistic settings, and posit that unlabeled data are abundant and inexpensive to obtain \cite{zou-etal-2023-decrisismb, zou2023semi}. Thus, self-generated demonstration inputs are unnecessary. Like Auto-CoT, Self-ICL's test-time compute overhead also scales quadratically. Lastly, Auto-CoT, STaR, and Self-ICL all focuses on reasoning tasks, whereas our work primarily focuses on classification tasks.


% \subsection{Cluster-Based Unsupervised Learning}
% \cite{SCAN} reported that a self-supervised model trained on a clustering objective resulted in representation whose clustering roughly corresponds to downstream classification categories. Considering that two of the embeddings models that we experiment with are pre-trained with clustering or similarity tasks, it is reasonable to expect such models producing nearest neighbors from the same class.

% Henry:
% [Main Difference with related work in TTS, tbe] Our approach introduces a novel dimension to test-time compute methodologies by systematically leveraging unlabeled data neighborhoods. While existing test-time compute techniques like chain-of-thought and self-consistency primarily focus on internal reasoning strategies, our method expands the computational context by incorporating external unlabeled data signals ... \\
% [Some other famous/related TTC work in LLM: Best-of-N Sampling, STaR Algorithm (Self-Taught Reasoner), Self Verification/Verifier, Search Methods/Monte Carlo Tree Search (MCTS), check more details in this blog https://cloudsecurityalliance.org/blog/2024/12/13/test-time-compute]

% The intuition behind TestNUC is that samples in close proximity within a semantic space are likely to share similar labels. By incorporating predictions on nearby unlabeled samples, the LLM can better contextualize and refine its decision-making. This approach exploits the consistency of local data structures, effectively using unlabeled examples as an auxiliary signal to boost performance and reduce the noise and uncertainty associated with isolated predictions.


\section{Conclusion}
In this work, we introduced TestNUC, a simple yet effective approach that leverages the consistency of neighboring unlabeled data to enhance test-time predictions in large language models. 
Extensive experiments across eight datasets and multiple LLMs demonstrate that TestNUC consistently outperforms baselines like standard prompting and self-consistency. It can be seamlessly integrated with existing methods such as TopK-ICL, self-consistency, and best-of-N to yield further gains. These results highlight the practical value of leveraging unlabeled data during inference, which not only boosts label consistency but also offers a scalable path to better generalization in real-world applications where labeled data may be scarce. 

% Future work could explore its integration with advanced inference techniques and broader domain applicability.


% \vspace{2mm}
% \section*{Acknowledgements}

% This research is partially supported by NSF grant \#xxxx and UIC DPI Seed Program. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF. We thank our reviewers for their insightful feedback and comments which helped improve the quality of our paper. 
% \henry{To be revised} 

% \clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limitation}
Our evaluation of TestNUC is limited to classification tasks and does not include generative tasks. We leave this extension for future work. Due to computational resource constraints and limited budgets, we did not evaluate recent powerful reasoning models such as o3-mini and DeepSeek-R1.

% \lipsum[6]
% Our ImplicitAVE dataset does not consider multi-valued attributes and negative instances, i.e. "none" as attribute values. We leave this extension as future work. Due to computational resource constraints and limited budgets, we did not evaluate open MLLMs with parameters larger than 13B. 



% \section{Ethics Statement}

% The datasets that we sourced from are publicly available. In this work, we propose a multimodal Implicit AVE dataset and provide a comprehensive benchmark of MLLMs. We do not expect any direct ethical concern from our work.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \clearpage
% \section{Introduction}

% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).


% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
% \centering
% \begin{tabular}{lc}
% \hline
% \textbf{Command} & \textbf{Output}\\
% \hline
% \verb|{\"a}| & {\"a} \\
% \verb|{\^e}| & {\^e} \\
% \verb|{\`i}| & {\`i} \\ 
% \verb|{\.I}| & {\.I} \\ 
% \verb|{\o}| & {\o} \\
% \verb|{\'u}| & {\'u}  \\ 
% \verb|{\aa}| & {\aa}  \\\hline
% \end{tabular}
% \begin{tabular}{lc}
% \hline
% \textbf{Command} & \textbf{Output}\\
% \hline
% \verb|{\c c}| & {\c c} \\ 
% \verb|{\u g}| & {\u g} \\ 
% \verb|{\l}| & {\l} \\ 
% \verb|{\~n}| & {\~n} \\ 
% \verb|{\H o}| & {\H o} \\ 
% \verb|{\v r}| & {\v r} \\ 
% \verb|{\ss}| & {\ss} \\
% \hline
% \end{tabular}
% \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
% \label{tab:accents}
% \end{table}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation: 
% \begin{quote}
% \tt\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
% \centering
% \begin{tabular}{lll}
% \hline
% \textbf{Output} & \textbf{natbib command} & \textbf{ACL only command}\\
% \hline
% \citep{Gusfield:97} & \verb|\citep| &  \\
% \citealp{Gusfield:97} & \verb|\citealp| & \\
% \citet{Gusfield:97} & \verb|\citet| &  \\
%   \citeyearpar{Gusfield:97} & \verb|\citeyearpar| &  \\
%   \citeposs{Gusfield:97}	&	& \verb|\citeposs|\\
% \hline
% \end{tabular}
% \caption{\label{citation-guide}
% Citation commands supported by the style file.
% The style is based on the natbib package and supports all natbib citation commands.
% It also supports commands defined in previous ACL style files for compatibility.
% }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Acknowledgements}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for 
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, 
% NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

% \newpage
\clearpage
\appendix

\section{Dataset Statistics}
\label{appendix:dataset}
Table \ref{tab:dataset_statistics} provides the dataset statistics summary for all evaluated datasets.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Task} & \textbf{Dataset} & \textbf{\# Classes} & \textbf{Total} & \textbf{Test} \\ \midrule \midrule
\multirow{2}{*}{Intent Detection} & BANKING & 77 & 10,003 & 500 \\
 & CLINC & 150 & 15,000 & 500 \\ \midrule \midrule
\multirow{2}{*}{Topic Mining} & Reddit & 50 & 25,000 & 500 \\
 & StackExchange & 121 & 25,000 & 500 \\ \midrule \midrule
\multirow{2}{*}{Domain Discovery} & MTOP & 11 & 15,667 & 500 \\
 & CLINC(D) & 10 & 15,000 & 500 \\ \midrule \midrule
Type Discovery & FewEvent & 34 & 18909 & 500 \\ \midrule
\midrule
Emotion Detection & GoEmotion & 27 & 23485 & 500 \\ \bottomrule
\end{tabular}%
}
\caption{Dataset statistics.}
\label{tab:dataset_statistics}
\end{table}



\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/analysis_llm_prediction.pdf} % example-image-duck
    % analysis_llm_predictions.png
    \caption{LLM predictions can be inaccurate and unstable.} \label{fig:analysis_llm_predictions}
    % \vspace{-4mm}
\end{figure*}


% \section{Relation to Closely-Related Approaches}
% \label{sec:comp_work}
% In this section, we compare our approach with closely related approaches in Table X. \lipsum[1]



\section{Prompt Template}
\label{sec:prompt_template}
The prompt template we used in the experiments is listed below. Note that we use the same prompt template for all methods for fair comparisons.


{ \begin{tcolorbox}[
    colback=gray!4,     % Background color
    colframe=black!55,% Frame color
    title=Prompt Template of FFA
]
Instruction: Please select a label from the provided options for the following testing samples and also show your confidence in the label assignment by providing a probability between 0 and 1. \\

Label Options: \texttt{[A List of Labels]}. \\

== Testing Samples == \\
\texttt{[Testing Samples]}
\end{tcolorbox}
}
% \texttt{[Instruction]} 

\section{LLM Predictions Can Be Inaccurate and Unstable}
Figure~\ref{fig:analysis_llm_predictions} demonstrates the error rate and inconsistency ratio of predictions by different LLMs on diverse datasets. The inconsistency ratio here refers to the proportion of prediction changes when an LLM is rerun \(N\) times for the same input query across the entire dataset. The results are obtained using standard zero-shot prompting with a temperature of 0.7 and \(N = 10\). It can be observed that even in standard text classification tasks, LLMs can produce inaccurate and inconsistent prediction results for ambiguous or challenging data points.




\end{document}

