\section{Conclusion}
We propose a novel privacy-preserving framework, \textit{RewardDS}, to mitigate noise in synthetic data during LLM privacy-preserving fine-tuning. 
Specifically, \textit{RewardDS} fine-tunes a reward model and leverages the reward signal to guide the synthetic data generation process. 
During the data synthesis process, \textit{RewardDS} employs the collaboration of Reward Guided Filtering and Self-Optimizing Refinement modules to filter and refine synthetic data, mitigating noise. 
We conduct extensive experiments across medical QA, legal QA, and code generation tasks. 
The results consistently demonstrate the effectiveness of \textit{RewardDS} for privacy-preserving LLM fine-tuning.

\section*{Limitations}
While \textit{RewardDS} has demonstrated its effectiveness in medical QA, legal QA, and code generation tasks, it incurs additional training costs for the reward proxy model. 
Although the model is lightweight, it still requires extra computational resources.

Additionally, due to computational resource constraints, we applied LoRA fine-tuning on the Qwen2.5-14B-Instruct model to validate our method, as discussed in Appendix \ref{sec:ext_LLM}. 
Full-parameter fine-tuning may yield even better performance. Future work will explore larger LLM backbones and additional categories to further demonstrate the effectiveness of our method as computational resources allow.

In the future, we aim to expand our experiments to include more domain-specific tasks and a wider range of LLM backbones. Furthermore, we plan to optimize the local fine-tuning process of the lightweight proxy models on the client side to reduce computational burdens, enhancing the scalability and feasibility of our method.


\section{Ethics Statement}
We adhere to the ACL Ethics Policy and all of our research is based on publicly available repositories and datasets. 
In the \textit{RewardDS} framework, we uphold strict ethical standards to protect user privacy and ensure data security. 
The datasets used, covering medical QA, financial QA, and code generation domains, are publicly available and free of personally identifiable information, minimizing privacy risks.
Our methodology does not access or reconstruct identifiable data, safeguarding individual privacy rights.

However, as our study involves multiple LLMs, such as Llama and Qwen, the findings may be influenced by the inherent biases, linguistic patterns, and assertiveness of these models.