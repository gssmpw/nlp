\section{Related Work}

% \subsection{Training Privacy from LLM finetuning}
\subsection{Privacy Preserving Fine-tuning Methods.}
% Fine-tuning large language models is essential to enhance their ability to follow instructions and optimize their performance on specific downstream tasks \citep{Mokhtarabadi2024EmpoweringPL, Wang2023HowFC, Jang2023ExploringTB}. 
% Since LLMs are shown to memorize sensitive information from the training data \citep{Carlini2020ExtractingTD}, recent advancements in privacy-preserving techniques for Large Language Models (LLMs) have focused on mitigating privacy risks during fine-tuning. 
% % 找一篇survey mitigating privacy risks during fine-tuning. 
% Though computationally expensive \citep{mattern-etal-2022-differentially}, DP-SGD provide a way that ensures private dataset does not reveal sensitive information \citep{Abadi2016DeepLDP,  McMahan2017LearningDP}. 
% Data anonymization techniques like k-anonymity and NER remove sensitive information preventing re-identification while preserving utility in fine-tuned models \citep{Sweeney1997GuaranteeingAW, Romanov2020NaturalTA, Abdullah2024NERRF}.
% Other notable works include the development of memory-saving techniques like ghost clipping and the application of local differential privacy (LDP) to enhance protection against stronger adversaries \citep{li2021simple,igamberdiev-habernal-2023-dp,Du2023SanitizingSE}.
% Apart from classic privacy protection methods, foundation models (e.g. LLMs and PLMs) have shown significant potential in generating high-quality synthetic data. These models can be fine-tuned with differential privacy to produce synthetic datasets that mimic the original data distribution while preserving privacy \citep{yue-etal-2023-synthetic, Flemings2024DifferentiallyPK}.
% 联邦学习/不光是DPSGD，训练隐私保护，有很多种方法/合成数据related work，更加细分的方法
Domain-specific data, such as medical diagnoses and financial reports, often contain sensitive information, and directly fine-tuning LLMs on such data raises privacy concerns \citep{Mokhtarabadi2024EmpoweringPL, Wang2023HowFC, Jang2023ExploringTB}. 
Differentially Private Stochastic Gradient Descent (DP-SGD) injects noise into gradients during fine-tuning, ensuring the model does not memorize private data \citep{Abadi2016DeepLDP, McMahan2017LearningDP}. 
Alternatively, data anonymization methods, such as k-anonymity and adversarial anonymization, detect and remove private information to prevent privacy leakage while maintaining model utility \citep{Sweeney1997GuaranteeingAW, Romanov2020NaturalTA, robin2024large}. 
Another promising approach is generating synthetic data with Differential Privacy (DP) guarantees as a substitute for private data \citep{yue-etal-2023-synthetic, Flemings2024DifferentiallyPK}. This synthetic data is protected by the DP mechanism, contains no user privacy, and can be freely used for further fine-tuning.



\subsection{Privacy-Preserving Synthetic Text Generation.}
Recent studies have explored synthetic data with differential privacy guarantees as a substitute for private data in LLM fine-tuning, achieving a balance between data utility and privacy protection \citep{yue-etal-2023-synthetic,dayu2024privacy,Kurakin2023HarnessingLM}. 
A series of work has been proposed to reduce computational costs and achieve high-quality synthetic data. 
\citet{Lin2023DifferentiallyPS,Xie2024DifferentiallyPS} use APIs and zero-shot learning to generate synthetic data without fine-tuning. 
\citet{Du2023ImprovingFA} combine the strengths of multiple models to generate synthetic data, mitigating the risks associated with relying on a single model.
\citet{Zou2025ContrastivePD} integrate knowledge from pre-trained language models and generate differentially private synthetic data.
\citet{Wang2024KnowledgeSGPS} integrates differential privacy with knowledge distillation from professional models, leveraging both local and professional models to generate high-quality synthetic data. 
However, these methods overlook the noise introduced during the synthetic data sampling process, which can degrade performance. 
To mitigate this, \citet{dayu2024privacy, Wang2022SelfInstructAL} compute the similarity between synthetic and private data to filter out those with low similarity. 
However, these similarity measures are too surface-level to effectively capture the quality of synthetic data for domain-specific tasks. 
Therefore, a more robust framework is needed to address the noise in synthetic data and enhance its quality for domain-specific tasks.

% Also suffers from the leakage of privay\citep{duan2024privacy}, In-Context Learning (ICL)