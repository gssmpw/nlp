% \documentclass{article}
% \usepackage{graphicx} %

% \title{Changes made to ARR Submission}
% \author{Anonymous}
% \date{}
% \begin{document}

% \maketitle

% After excellent constructive feedback from reviewers in the previous ARR cycle, we have made multiple changes to our work to clarify our contributions and their significance to neural IR. Additionally we have changed the title of the work from "Analysing and Mitigating Generative Content Injection in Search" to "Exploiting Positional Bias for Query-Agnostic Generative Content in
% Search" to better reflect the significant facets of our findings.

% \section{Addition of new results}

% Responding to Reviewer ZJau, we included additional results with a stronger bi-encoder Contriever. The reviewer was concerned that the biases presented were attributable to weaker retrieval performance; however, we observe identical susceptibility to our approach, further reinforcing our hypothesis that positional and contextual bias exist at an architectural level. We move results evaluating TAS-B to the appendix.

% \section{Re-Writing}

% Core changes are outlined in the following subsections; however, more generally, we have looked to clarify components throughout the work.

% \subsection{Re-Writes noted by Reviewer Bqgx}

% \begin{itemize}
%     \item Section 2: Clarification of neural ranking model definition (Lines 130-135 "This fine-tuning helps improve their capabilities to...").
%     \item Section 3: Broader clarifications of our attention bleed-through hypothesis (Detailed below) (Lines 215-262 "Transformer-based models are composed of a sequence of blocks..."), Improved description of document-conditioned text (Lines 263-308 "Retrieval axioms state that when document length increases without further satisfying an information need...")
%     \item Section 4: Clarification of salience embedding model (Lines 425-427 "To determine salience, we use the dot product..."), Improved description of first-stage retrieval (Lines 406-411 "Investigating RQ-1, we evaluate ranking model preference...")
%     \item Section 5: Improved defense motivation and description of evaluation metrics (Lines 545-549 "We aim to mitigate the adversarial effects of injected text in a manner agnostic to...", Lines 580-600 "To evaluate the intermediate task of identifying...")
% \end{itemize}

% \subsection{Section 1: Introduction}
% \begin{itemize}
%     \item Clear outline of the underlying problems plaguing neural ranking models (Lines 54-65 "Where previously a search pipeline...")
%     \item Clarification of contributions tied to our hypotheses (Lines 89-118 "We pose two hypotheses...")
%     \item Closing statement illustrating the implications of our findings (Lines 119-128 "In summary, NRMs present relevance-dependent positional bias...")
% \end{itemize}

% \subsection{Section 3: Method}
% \begin{itemize}
%     \item Refactoring of explanation of positional bias, building from the core components of the transformer architecture up to implications for retrieval (Lines 215-262 "Transformer-based models are composed of a sequence of blocks containing the attention mechanism...")
%     \item Clearer motivation of our defense referencing back to our hypotheses regarding positional bias (Lines 324-348 "We exploit our hypothesis of attention bleed-through to our advantage...")
% \end{itemize}

% \section{Key Rebuttal Points}

% \paragraph{While this method focuses on mitigating the negative impact of injected text on document ranking, it does not propose a method that substantively improves document ranking.}
% We agree that improvements in document ranking are a convincing sign of an exploitable weakness; however, many approaches to improve rank frequently require query/topic knowledge and either white or black box access to a target NRM. Instead, we perform a query-agnostic approach to expose biases in neural models beyond known gradient-based attacks. As noted, our work is not independent of these attacks and instead seeks to create an augmented document which is closest in relevance to the original text. Although providing query knowledge would certainly improve the rank of a document for a particular query, doing so wouldnâ€™t reveal the new insights into the response of NRM architectures that we show in this work. In showing minimal rank degradation with our approach, we have exposed weaknesses in NRMs, which could either spuriously or maliciously allow for the inclusion of non-relevant content in high ranks of search results.

% \paragraph{The injection contents are limited to five entities and the promotional text may not be representative for broader scenarios. The selected IR dataset is also limited to MSMARCO.}
% Though these items are limited, comparing them with real promotion examples was essential. Such examples are limited as many are promotional while referencing a previous span, which is inappropriate compared to the generated promotional spans. Choosing entities completely out of context provides the hardest setting for promotion. To choose entities or topics in context, which would be a more realistic application, would be trivial given our findings, as not only do NRMs exhibit strong positional bias, but current generative models can suitably generate promotion in the context of texts as shown by stronger lexical match to queries. We agree that MSMARCO has limits; however, it offers multiple benefits, crucially that strong checkpoints exist for this dataset, and the TREC evaluation sets have rich judgements which allow us to observe the effect of these augmentations on retrieval precision not offered by test collections such as those found in the BEIR benchmark, which mostly have very sparse relevance judgements.




% \end{document}
