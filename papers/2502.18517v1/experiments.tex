\section{Experiments}
\subsection{Experiments Setup}
\label{sec:setup}

\paragraph{Datasets.}
We evaluate our method across three domain-specific generation tasks using established datasets: Medical QA using HealthCareMagic-100k \citep{li2023chatdoctor}, Financial QA using fingpt-fiqa\_qa \citep{zhang2023instructfingpt}, and Code Generation using opc-sft-stage2 \citep{Huang2024OpenCoderTO}. 
%We evaluate our method on three domain-specific generation tasks: Medical Question-Answer (Medical QA), Financial Question-Answer (Financial QA), and Code Generation. 
%We utilize the HealthCareMagic-100k dataset \citep{li2023chatdoctor} for medical QA task, the fingpt-fiqa\_qa dataset \citep{zhang2023instructfingpt} for financial QA task and the opc-sft-stage2 dataset \citep{Huang2024OpenCoderTO} for code generation task. 
% \revisezq{Move all preprocess details into appendix. }
%ZZQ-Feb-11 免得reviewer质疑为什么要去掉low-quality samples，会质疑你为了自己方法好，改数据集。另外，appendix需要列出什么data才会判定为low-quality data，并给几个low-quality data的例子

\paragraph{Evaluation Metrics.} 
For the evaluation of the QA task, we employ the ROUGE-1 (R1), ROUGE-L (RL) \citep{lin-2004-rouge}, and Perplexity (PPL) \citep{yu2024ppl} as metrics. 
While automated metrics focus on lexical overlap and fluency, LLM-Judge \citep{Lia2023judging} provides a more comprehensive assessment of semantic accuracy and response quality. 
Hence, we also use LLM-Judge as a metric. 
For the code generation task, we use Pass@1 and Pass@10 as evaluation metrics \citep{chen2021codex}. 


\paragraph{Implementation Details.}
We use the Qwen2.5-0.5B-Instruct model \citep{yang2024qwen2} as the backbone for the generation/reward proxy model, and the Qwen2.5-7B-Instruct model as the target LLM on the server. 
During each DP-SGD fine-tuning process of both proxy models, we set the privacy budget to $(8, 1e^{-5})$. 
As a result, the total privacy budget for our method is $(16, 2e^{-5})$, according to the sequential composition law of the DP mechanism \citep{Abadi2016DeepLDP}.
For a fair comparison, we set the same privacy budget for all compared methods. 
The size of the synthetic dataset is always kept to twice that of the client's private data across all baselines. 
These settings align with established DP deployments such as Apple's QuickType and Google's models, as noted by \citet{Nils2023ana}.

More details on the datasets used and the implementation are provided in Appendix \ref{app:data_details} and Appendix \ref{app:imple_details}, respectively.

% \revisezq{Do you plan to fine-tune 13B on server side. Normally, reviewers want to see more backbones and more sizes.}
\subsection{Compared Methods.}
To demonstrate the effectiveness of our method, we consider several baselines for comparison:

\textbf{Vanilla LLM} refers to using a general-purpose LLM for domain-specific tasks without any domain adaptation or fine-tuning. 
\textbf{Locally Fine-tuning} refers to training a lightweight model locally on clients' private data. 
% \revisezq{why call it proxy model? I think the proxy should be deleted.}

\textbf{DP-Generation} \citep{Kurakin2023HarnessingLM} fine-tunes the generation proxy model on the client side using DP-SGD. This proxy model is then used to generate synthetic data, which is subsequently utilized to fine-tune the target LLM on the server.
% \revisezq{carefully explain this method. In the explanation, there is no DP. But your method is DP-Generation. State clearly about the data generation and fine-tuning happens at the client or the server?}
\textbf{DP-Instruct} \citep{dayu2024privacy} introduces additional filtering operations based on the similarity of synthetic queries before LLM fine-tuning;
\textbf{KnowledgeSG} \citep{Wang2024KnowledgeSGPS} utilizes the synthetic data to enhance the generation proxy model for domain-specific tasks instead of fine-tuning the target LLM.

More details of the compared method are provided in Appendix \ref{app:compare}. 

\subsection{Main Results}


\begin{table*}
	\centering
        \def\arraystretch{1.16}
	\resizebox{0.98\textwidth}{!}{
    	\begin{tabular}{l 
       p{1cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering}
       p{1cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering}
       p{1.7cm}<{\centering} p{1.7cm}<{\centering} 
       % cc ccc cc
         } 
	\toprule
		  \multirow{2}{*}{Methods} 
          &  \multicolumn{3}{c}{Medical QA} &  \multicolumn{3}{c}{Financial QA} &  \multicolumn{2}{c}{Code Generation} \\ 
            \cmidrule(lr){2-9}
			& R1 $\uparrow$  & RL $\uparrow$ &  \multicolumn{1}{c|}{PPL $\downarrow$ }  
            & R1 $\uparrow$  & RL $\uparrow$ &  \multicolumn{1}{c|}{PPL $\downarrow$} &
            Pass@1 $\uparrow$ &
            Pass@10 $\uparrow$ \\
            
            % \cmidrule(lr){1-9}
            % \cmidrule(lr){1-1} \cmidrule(lr){2-4}  \cmidrule(lr){5-7}  \cmidrule(lr){8-9}
            \cmidrule(lr){1-9}

            \multicolumn{1}{l|}{Vanilla LLM} & 21.60 & 11.50 &  \multicolumn{1}{c|}{1.34} 
            & 23.91 & 11.72 & \multicolumn{1}{c|}{1.38}
            & 18.82 & 42.06\\
            
            \multicolumn{1}{l|}{Locally Fine-tuning} & \underline{23.82} & \underline{15.46} &  \multicolumn{1}{c|}{1.71} &
            13.26 & 10.19 & \multicolumn{1}{c|}{1.67} 
            & \underline{28.34} & 43.99 \\

            % \midrule
            % \cmidrule(lr){1-1} \cmidrule(lr){2-4}  \cmidrule(lr){5-7}  \cmidrule(lr){8-9}
            \cmidrule(lr){1-9}
            
            \multicolumn{1}{l|}{DP-Generation \citep{Kurakin2023HarnessingLM}} & 16.22 & 10.94 &  \multicolumn{1}{c|}{\underline{1.06}}
            & 14.97 & 11.20 & \multicolumn{1}{c|}{1.05}
            & 25.51 & 42.75 \\
            
            \multicolumn{1}{l|}{DP-Instruct \citep{dayu2024privacy}} & 11.94 & 8.44 & \multicolumn{1}{c|}{\textbf{1.04}} &
            14.06 & 10.76 & \multicolumn{1}{c|}{\underline{1.04}} 
            & 26.27 & 48.06 \\
            
            \multicolumn{1}{l|}{KnowledgeSG \citep{Wang2024KnowledgeSGPS}} & 20.28 & 10.74 & \multicolumn{1}{c|}{1.31} &
            \underline{24.14} & 12.33 & \multicolumn{1}{c|}{1.21} 
            & 23.93 & \underline{49.58} \\

            \rowcolor{lightgray!45}
            \multicolumn{1}{l|}{\textbf{RewardDS}} & \textbf{27.78} & \textbf{17.02} & \multicolumn{1}{c|}{1.17} &
            \textbf{24.42} & \textbf{14.96} & \multicolumn{1}{c|}{\textbf{1.02}} 
            & \textbf{32.41} & \textbf{49.99} \\
            
            \rowcolor{lightgray!20}
            \multicolumn{1}{l|}{w/o Reward Guided Filtering}  & 20.38 & 13.11 & \multicolumn{1}{c|}{1.28} & 
            17.93 & \underline{12.52} & \multicolumn{1}{c|}{1.25} 
            & 23.03 & 34.96 \\

            \rowcolor{lightgray!20}
            \multicolumn{1}{l|}{w/o Self-Optimizing Refinement} & 22.70 & 13.42 & \multicolumn{1}{c|}{1.36} &
            14.14 & 11.07 & \multicolumn{1}{c|}{1.18} 
            & 22.27 & 33.17 \\
            
            
            \bottomrule 

  
		\end{tabular}
  
	}

 \caption{
Comparisons of our method with baselines across three domain-specific tasks: Medical QA, Financial QA, and Code Generation.
Higher values of ROUGE-1 (R1) and ROUGE-L (RL), and lower values of Perplexity (PPL) indicate better performance on the QA generation task. 
Higher values of Pass@1 and Pass@10 reflect better performance in the code generation task. 
Numbers in \textbf{bold} and \underline{underlined} represent the best and second-best results, respectively. 
% \revisezq{The results demonstrate that \textit{RewardDS} significantly outperforms the baselines.}
}
\vspace{-0.7em}
\label{tbl:main}
\end{table*}


As shown in Table \ref{tbl:main}, \textit{RewardDS} outperforms all other baselines across the three domain-specific tasks, except for the PPL on the Medical QA task. 
DP-Instruct achieves marginally lower PPL in medical QA.
This is possibly due to the filtering by similarity, leading the target LLM to overfit on these highly similar samples.

The Vanilla LLM exhibits suboptimal performance across medical QA, financial QA, and code generation tasks, primarily due to the lack of domain-specific fine-tuning on private data. 
While Locally Fine-tuning a lightweight proxy model (with only 0.5B parameters) mitigates privacy concerns, the small model's limited capacity hinders its ability to effectively learn domain-specific knowledge, leading to subpar performance.

DP-Generation samples synthetic training data to fine-tune the target LLM on the server.  
However, due to the randomness inherent in the sampling process, the resulting synthetic data contains significant noise, which severely impairs the fine-tuning performance of the LLM on the server.
Although DP-Instruct attempts to filter the synthetic data by computing the similarity between the synthetic query and the private query, it still does not perform well. 
% This approach focuses solely on the query similarity, neglecting the corresponding response. 
% Actually, LLM is fine-tuned both on the query and response. 
% Furthermore, 
Similarity alone cannot accurately reflect the quality of synthetic data, where higher similarity does not necessarily indicate better data quality.

KnowledgeSG utilizes synthetic data to fine-tune the lightweight proxy model on the server, enhancing it with the assistance of the target LLM. 
However, the quality of the synthetic data is highly dependent on the target LLM’s capacity for the specific domain task. 
If the target LLM performs poorly, the synthetic data will likely contain more noise, which could harm the performance of the proxy model. 
KnowledgeSG performs relatively better on the financial QA task compared to the other two tasks, primarily because the Vanilla LLM performs well on financial QA, whereas it does not on the other tasks. 
Only in the financial QA task, the performance of the Vanilla LLM surpasses that of the Locally Fine-tuned model, whereas this is not the case for the other tasks.

We also present results after removing the Reward Guided Filtering and Self-Optimizing Refinement modules respectively. 
We observe the decline in performance across all tasks when either module is removed, which highlights the effectiveness of these modules. 
Without these modules, more noisy synthetic samples are included during LLM fine-tuning, resulting in performance degradation. 


% \revisezq{I think this paragraph should be moved to the first paragraph.}
% In contrast, \textit{RewardDS} outperforms all other baselines across the three domain-specific tasks, except for the PPL on the Medical QA task. 
% DP-Instruct achieves marginally lower PPL in medical QA.
% This is possibly due to the filtering by similarity, leading the target LLM to overfit on these highly similar samples.
% We also present results after removing the Reward Guided Filtering and Self-Optimizing Refinement modules respectively. 
% We observe the decline in performance across all tasks when either module is removed, which highlights the effectiveness of these modules. 
% Without these modules, more noisy synthetic samples are included during LLM fine-tuning, resulting in performance degradation. 


\subsection{Evaluation using LLM-Judge}

\begin{figure*}[!htbp]
  \centering
  % \vspace{-0.25em}

    \includegraphics[width=0.98\textwidth]{latex/fig/more_exp.pdf}

  \vspace{-0.25em}
  \caption{Using LLM-Judge \cite{Lia2023judging} to compare the outputs generated by our method with those of other baselines. 
  \textbf{Win} means our method outperformed the baselines, 
  \textbf{Tie} means the results were similar, 
  and \textbf{Lose} means our method performed worse than the baselines.
  }
  \vspace{-0.75em}
  \label{fig:LLM-judge}
\end{figure*}

We also use LLM-Judge \citep{Lia2023judging} for a more reliable evaluation of the medical QA and financial QA tasks. 
While ROUGE metrics measure lexical similarity to references and PPL captures fluency, these metrics often fail to assess deeper aspects of response quality.
%While ROUGE-L and ROUGE-1 measure similarity between generated outputs and references, and PPL only captures the loss, these metrics do not accurately reflect output quality. 
Inspired by \citet{Lia2023judging}, we fine-tune an LLM judger to assess the quality of generated outputs across different baselines. 
We provide the judger with both the user query and the generated outputs from our method and the baselines, allowing it to determine which is better or declare a tie. 
Details of the judger training and evaluation process are shown in Appendix \ref{app:llm_judge}.

As shown in Figure \ref{fig:LLM-judge}, our method outperforms other baselines in both the medical QA and financial QA tasks. 
DP-Generation and KnowledgeSG struggle with noisy samples from synthetic data, leading to poor performance. 
Although DP-Instruct filters synthetic data by comparing with private data and removing low-similarity samples, it achieves only limited performance gains compared to DP-Generation. 
This shows that simple similarity measures do not fully capture the quality of synthetic data. Locally Fine-tuning avoids synthetic data noise by fine-tuning a lightweight proxy model on private data locally, but it still underperforms our method due to the limited learning capacity of the lightweight model for domain-specific knowledge.


\subsection{Hyperparameter Analysis}
\label{sec:hyper}
\vspace{-0.7em}
\begin{figure}[!htbp]
    \centering
    % \raggedleft
    \begin{minipage}{0.80\columnwidth} 
    \centering
    \includegraphics[width=1\textwidth]{latex/fig/legend.pdf}
    \label{fig:grow_legend}
  \end{minipage}
  \vspace{-2.2em}
  \vskip\baselineskip % 换行
  
    \begin{minipage}{0.23\textwidth}
        \centering
        \subfigure{
             \label{fig:hyper_1}     
            \includegraphics[width=1\columnwidth]{latex/fig/fold_medical.pdf}  
        }  
    \end{minipage} 
    % \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \subfigure{
             \label{fig:hyper_2}     
            \includegraphics[width=1\columnwidth]{latex/fig/N_can_medical.pdf}  
        }  
    \end{minipage}
    \hfill
    \vspace{-0.8em}
    \caption{
    Analysis of hyperparameters including the number of folds $k$ and the number of candidate responses $N$ in \cref{algo:main} on the medical QA task.
    To analyze $k$, we set $N = 3$; To analyze $N$, we set $k = 6$.
    }
    \vspace{-0.8em}
    \label{fig:two_images}
\end{figure}

We analyze the effect of hyperparameters on our method described in \cref{algo:main}.
As shown in \cref{algo:main}, the number of folds $k$ controls the amount of selected synthetic data. 
A smaller $k$ means more synthetic data is included, but it may also introduce more noise. 
As illustrated in Figure \ref{fig:hyper_1}, when $k = 1$ (using all the synthetic data), performance decreases. 
Using a larger $k$ can help exclude noisy data, improving performance. 
However, setting $k$ too large and excluding too much synthetic data can slightly degrade performance. 
Therefore, we set $k = 6$ for the medical QA task. 
% \revisezq{You do not mention $k$ in the main body. Need to mention it in Section 4.2.}

The hyperparameter $N$ controls the number of candidate responses. 
As shown in Figure \ref{fig:hyper_2}, increasing $N$ slightly improves performance because a larger number of candidates increases the chance of selecting better synthetic data. 
However, generating more candidates incurs additional time and computational costs. 
Therefore, for the medical QA task, we set $N = 3$.

Further hyperparameter analyses for the legal QA task and code generation task can be found in Appendix \ref{app:hyper}.


\subsection{In-depth Analysis of \textit{RewardDS} Design}
% \revisezq{I prefer framing it as analysis rather than discussion}
Here, we provide more detailed analysis on the design and effectiveness of \textit{RewardDS}. 
\begin{figure*}[!htbp]

  \includegraphics[width=0.98\textwidth]{latex/fig/diss_2.pdf}
    \label{fig:grow_legend}
  \vspace{-0.7em}
  \caption{
  Effectiveness of \textit{RewardDS} design.
  (a): Performance on medical QA with different privacy budget allocations for generation and reward proxy model training.
  The allocation of `x + (16-x)' means the privacy budget for training the generation proxy model is set to x, while the reward proxy model is set to (16-x);
  (b)/(c)/(d): Quality improvement of the synthetic data, on the Medical QA/Financial QA/Code Generation tasks, after applying the Self-Optimizing Refinement module multiple times.
  }
  \vspace{-1.0em}
  \label{fig:different_epislon}
\end{figure*}


\paragraph{Analysis 1:}\textit{The impact of different privacy budget allocations.}
% \revisezq{The necessity and impact of allocating privacy budget to reward proxy model fine-tuning.}}
% \revisezq{Add a motivation about why you want to anlayze privacy budget.}

Although the total privacy budget is controlled at $(16, 2e^{-5})$, we can allocate more privacy budget to the generation reward model or the reward proxy model.
We will investigate the impact of different privacy budget allocations in Figure \ref{fig:different_epislon}(a). 
The results indicate that even a small privacy budget for reward model fine-tuning (e.g., ``15+1'', with only 1 allocated to the reward model) outperforms the case where no privacy budget is allocated to the reward model (``16+0''). 
No privacy budget allocated for reward model means that we do not train the reward proxy model on the client for data filtering or refinement.
This suggests that \textbf{even a marginal privacy cost for reward model training can yield substantial benefits}. 
Furthermore, allocating more privacy budget to the reward model will bring only marginal performance improvements.


\paragraph{Analysis 2:}\textit{Impact of Self-Optimizing Refinement on Synthetic Data Quality.}

As shown in \cref{algo:main}, we use the self-optimizing refinement module to re-generate synthetic responses and improve quality during each training epoch. 
To assess the effectiveness of our self-optimizing refinement module (\cref{algo:main}), we track reward scores of synthetic responses across multiple refinement iterations.
%We also compute the reward score for all newly generated synthetic data and visualize the quality change after multiple refinements. 
A higher reward score indicates better synthetic data quality. 
%As shown in subfigures (b), (c), and (d) of Figure \ref{fig:different_epislon}, \textbf{the synthetic data quality improves gradually with multiple applications of the self-optimizing refinement module}. 
Figures \ref{fig:different_epislon}(b-d) demonstrate that \textbf{synthetic data quality improves gradually through iterative refinement}, explaining our method's superior performance.

\paragraph{Analysis 3:}\textit{Generalizability across different LLM backbones.}

We have evaluated our method with different backbones as the target LLM, including Llama-2-7B-chat-hf \cite{metaai2023llama2} and Qwen2.5-14B-Instruct \cite{yang2024qwen2}. 
%The results in Table \ref{tbl:more_back} demonstrate that our method consistently outperforms other baselines, regardless of the LLM backbone, strongly highlighting the effectiveness of our approach. 
Table \ref{tbl:more_back} shows that our method maintains superior performance across various backbones, \textbf{confirming its backbone-agnostic effectiveness}.
More analysis is in Appendix \ref{sec:ext_LLM}.

% \paragraph{\revisezq{Discussion 4:}} \revisezq{Need to discuss the training time overhead of your method since you have refinement and filtering.} 