\section{Introduction}
% 首先介绍LLM的发展
% 引入LLM隐私
% 第一段：private data 的 owner 他们有 concern，他们不太想把这个东西给别人，LLM 它本身是有隐私保护的问题，所以就是有隐私保护问题，就是我的客户端有一些隐私数据，服务端有一些有个有个模型，然后我的服务端是没办法拿到客户端的模型/数据去训练的，这是个问题。历史只放第一段讲，倒数第一二句重叠了，重新讲，直接将我们的目标。
% The remarkable success of large language models (LLMs) in diverse tasks has captured global attention \citep{achiam2023gpt,metaai2023llama2}. This achievement has motivated many organizations to fine-tune their own customized LLMs using locally sourced private data \citep{Xue2023DBGPTED}.
% \revisezq{Nonetheless, training such LLMs on private data could cause significant privacy concerns, since LLMs are shown to memorize sensitive information from the training data \citep{Carlini2020ExtractingTD,Lukas2023AnalyzingLO,Borkar2023WhatCW}.
% % 介绍LLM privacy
% All these applications rely on collecting private text data from users to train LLMs, which raises serious privacy concerns as LLMs may memorize and leak sensitive information about users \citep{DBLP:conf/nips/WangCPXKZXXDSTA23}.}
% The remarkable success of large language models (LLMs) in diverse tasks has captured global attention \citep{achiam2023gpt,metaai2023llama2}. 
% This achievement has motivated many organizations to fine-tune their own customized LLMs using locally sourced private data \citep{Xue2023DBGPTED}.
% By transmitting data to a server with powerful computing capabilities, clients can build machine learning models without consuming excessive client-side resources.
% However, the inherent privacy vulnerabilities associated with external LLMs preclude their direct implementation, emphasizing the critical necessity of prioritizing robust safeguards to ensure data security and privacy protection. \citep{DBLP:conf/emnlp/LiGFXHMS23,DBLP:conf/ccs/AbdelnabiGMEHF23}.
% Then, the DP synthetic text can be utilized in developing downstream NLP systems without introducing additional privacy risks. It enablesure and broader sharing of private data.
% 
% 合成数据在LLM privacy中间的作用

% \revisewang{
The remarkable capabilities of Large Language Models (LLMs) in general tasks have motivated many individuals and organizations to customize their own LLMs for domain-specific applications, such as medical diagnosis, financial analysis, etc. \citep{wu2023bloomberggpt,chen2023meditron}. 
While domain adaptation through fine-tuning is attractive, high computational costs make local fine-tuning impractical for most users. 
Currently, most LLM service providers \citep{achiam2023gpt,YangQwen224,Doubao2024} offer fine-tuning services, allowing users to customize LLMs for their needs by preparing and uploading their domain-specific data. 
However, these data may contain sensitive information, and directly transferring it to the LLM service provider can lead to significant privacy concerns \citep{zeng2024privacyrestore, Sahar2023Not}. 
Under the client-server context, we consider individuals and organizations seeking to customize LLMs as the clients, the LLM service providers as servers, and the model to be fine-tuned as the target LLM.
It remains a critical challenge to develop privacy-preserving fine-tuning methods in such a client-server scenario.

%However, the collection of this data often requires substantial intellectual and financial investment, making the direct uploading vulnerable to data leakage. 
% }


% 第二段：合成数据是解决这个问题的一个主流的方法。DP是很流行的解决了的问题的framework，DP需要依赖于合成数据。合成数据的范式是什么，合成数据怎么做。在哪合成，怎么合成。不考虑过滤的点，合成数据是怎么做的。

% Following the principles of Differential Privacy(DP) \citep{DBLP:journals/fttcs/DworkR14}, differentially private synthetic text represents a promising and extensively \revisezq{researched approach.} 
% It seeks to generate a new text dataset that retains the characteristics of the original private data \revisezq{while ensuring privacy by safeguarding sensitive information in each sample.} 

%Synthesizing data is a promising method to solve this challenge \citep{yue-etal-2023-synthetic, Kurakin2023HarnessingLM, yu2023training, mattern-etal-2022-differentially, Flemings2024DifferentiallyPK}
Prior works proposed data synthesis as a promising solution to the challenge \citep{yue-etal-2023-synthetic, Kurakin2023HarnessingLM, yu2023training, mattern-etal-2022-differentially, Flemings2024DifferentiallyPK}. 
This approach generates synthetic data to replace the private data used for fine-tuning, thus ensuring privacy protection. 
Specifically, a generation proxy model is first trained on the private data, optimized by DP-SGD \citep{Abadi2016DeepLDP} to safeguard privacy. 
The generation proxy model then generates synthetic data for subsequent LLM training.
%Synthetic data is then sampled from this generation proxy model. 
% \revisezq{How to generate/sample synthetic data? Do you have prompt?}
However, due to the inherent randomness of the sampling process, the synthetic data inevitably contains significant flawed data, including text incoherence or storyline incompleteness, which is considered as noise and leads to less effective LLM fine-tuning, as illustrated in Figure \ref{fig:fig_intro} 
We also provide an example in Figure \ref{fig:training_samples}.
% \revisezq{I think this figure does not show that the noise leads to decreased performance.}

% aims to ensure privacy in the original datasets by creating a new datasets utilizing Differential Privacy (DP) \citep{DBLP:journals/fttcs/DworkR14}. 
% The DP synthetic text methods are used widely in developing any downstream NLP system without adding extra privacy risks. 
% The state-of-the-art DP synthetic method is to \textit{finetune a client-side pretrained generative language models(LMs)} on private datasets \citep{yue-etal-2023-synthetic,Kurakin2023HarnessingLM} using DP-SGD \citep{Abadi2016DeepLDP}.
% The client-side LM generates synthetic responses for certain queries and transmits synthetic datasets to the server, then the server uses the synthetic data to fine-tune domain-specific LLMs. A series of methods have been proposed to prevent the direct use of private data by using synthetic data for substitution \citep{putta2022differentially,Lin2023DifferentiallyPS,Li2024SyntheticD}.

% 我们的contribution——多的reward model
\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{latex/fig/fig_intro_30.pdf}
\vspace{-15pt}
\caption{
Illustration of how \textit{RewardDS} overcome the dilemma of traditional synthetic data methods.
The synthetic data directly sampled from the generation proxy model contain significant flaws, such as incoherent text or incomplete storylines, which are considered noise.
}
\vspace{-1.5em}
\label{fig:fig_intro}
\end{figure}


To mitigate the noise, existing methods \citep{Wang2022SelfInstructAL, dayu2024privacy, Xie2024DifferentiallyPS} proposed to filter out flawed data by measuring its similarity to private data. 
\citet{Wang2022SelfInstructAL} use ROUGE-L similarity, while \citet{dayu2024privacy, Xie2024DifferentiallyPS} compute embedding similarity. 
However, these metrics fail to evaluate the synthetic data's effectiveness for domain-specific tasks. 
Alternative methods \citep{lifederatedSD24, Wang2024KnowledgeSGPS} sample synthetic data directly from the target LLM on the server to improve quality.  
But the target LLM is not fine-tuned on domain-specific tasks, so the sampled synthetic data does not help improve model performance on those tasks.
% Despite the server LLM's impressive capabilities, its lack of domain-specific fine-tuning means the generated synthetic data contributes little to improving performance on domain-specific tasks. 

% \revisezq{What is relationship between DP and synthetic data. There is no clue to remind the readers their relationship.}
% A series of methods have been proposed to prevent the direct usage of private data by using synthetic data for substitution \citep{DBLP:conf/iclr/AugensteinMRRKC20,putta2022differentially,Lin2023DifferentiallyPS,yue-etal-2023-synthetic,Li2024SyntheticD}.
% Some use APIs to generate instructions, risking data exposure \citep{Lin2023DifferentiallyPS,Xie2024DifferentiallyPS}, \revisezq{Why use APIs will leak data?} while others rely on local models, causing data quality degradation and lower performance \citep{Kurakin2023HarnessingLM}. 
% Existing methods suffer from the trade-off between privacy risk and model performance.
% Wang2022SelfInstructAL
% 这句话需要加吗？ Though model performance is affected by synthetic data input noise,  synthetic data input noise is also an important part of protecting privacy.
% 上面的工作有许多问题，我们的方法为什么可以解决这个问题，别的工作做了什么，相关工作。
% 共性的挑战是什么

% 第三段：合成数据比如说第一篇最开始做一次保护数据，他们最基本的方式，比如说微调一个代理模型去用它，用它去采样数据，而这种方法它可能就是有一些就采样出来，数据是噪声问题，所以导致它的效果不是很好；
% 再到后面就比如说有的人就是考虑到就是考虑到数数据质量不高，所以他们就是用 IM 去增强，然后用增强完之后，他们也同样没有做做一些就是说对数据质量的校校正的问题；
% 然后到后面，比如说有的人可能做了校正的问题，但他只是通过比如说跟隐私数据它去做相似度，去选一些跟隐私数据比较相似的数据来作为我的合成数据，那这样子可能并不能真实的反映我的数据质量，就虽然很相似，但不保证它的数据对于模型微调就是质量很高。
% 就是导致它的性能也不是，也没有得到很大的提升。

% \revisezq{What is aligned model?}
% Though protected from privacy leakage, these methods suffer from low performance due to the noise added to the sampling training data using DP.
% To mitigate this performance degradation in DP synthetic data and achieve high quality of synthetic data, previous methods \citep{Wang2022SelfInstructAL,dayu2024privacy} filters low-quality or similar instructions through metrics such as BLEU \citep{papineni2002bleu} and ROUGE-L \citep{lin2004rouge}. 
% % To mitigate the privacy risk raised by aligned models in memorizing instructions and achieve the high quality of synthetic data, previous methods \citep{Wang2022SelfInstructAL,dayu2024privacy} filters low-quality or similar instructions through metrics such as BLEU \citep{papineni2002bleu} and ROUGE-L \citep{lin2004rouge}. 
% These approach faces two fundamental limitations: \textbf{(a) Surface-level filtration.} 
% Metrics like BLEU and ROUGE fail to assess the semantic coherence and task-specific adequacy of synthetic data as LLM tasks grow increasingly complex. 
% Moreover, some studies use the similarity of embedding distribution \citep{dayu2024dp,lifederatedSD24,Wang2024KnowledgeSGPS} to estimate the correlation of synthetic text, but it fails to describe the deep relationships between texts.\citep{JoshiSBN20,Goelquantifyembedding24}; 
% \textbf{(b) Solely focus on queries.} 
% Since the training pairs appear in Queries and Response pairs, previous methods only focus on query filtration while overlooking response quality risks retaining flawed responses, propagating errors during alignment, and degrading model performance \citep{dayu2024privacy,Wang2024KnowledgeSGPS}.
% Integrated with Fine-Tuning and In Contextual Learning(ICL) \citep{dong2022survey}, 
% use sampling to filter the synthetic data 

% Other approaches that depend exclusively on localized base models may compromise the integrity of synthetic data, resulting in diminished model performance \citep{Kurakin2023HarnessingLM}.
% Though distill knowledge from the professional model deployed on the server but \citet{Wang2024KnowledgeSGPS} still face the challenge of overlooking response quality risks and not considering the impact of domain-specific knowledge.
% 讲一下有哪些是the metric cannot accurately express the accuracy of the information，有哪些是performance degradation caused by noise in synthetic data
% 我们的工作贡献点在哪

% 第四段：我们面临的挑战，你每一个新的东西你都有 motivation 
% 然后你的那些新的东西，它们之间是啥关系？
% 那些新的东西的 motivation 他们又是啥关系？

To mitigate the noise in synthetic data while protecting privacy, we propose \textit{RewardDS} (\textbf{Reward}-driven \textbf{D}ata \textbf{S}ynthesis), a novel privacy-preserving framework that improves synthetic data quality for the target LLM's privacy-preserving fine-tuning. 
RewardDS implements a two-stage quality control process, i.e., filtering and refinement, as illustrated in Figure \ref{fig:fig_intro}. 
%RewardDS trains an additional reward proxy model to guide the data synthesis process. 
%After the data synthesis process by the generation proxy model, \textit{RewardDS} leverages the reward model to filter and refine the synthetic data, as shown in Figure \ref{fig:fig_intro}. 
%Specifically, in addition to the generation proxy model, we fine-tune a reward proxy model on private data to assess data quality for domain-specific tasks, using DP-SGD to ensure user privacy. 
Specifically, we first train a reward proxy model on private data to assess data quality for domain-specific tasks, using DP-SGD to safeguard privacy. 
%After sampling the raw synthetic data from the generation proxy model, we apply the reward proxy model to judge the data and filter out those with low rewards, a process we call \textbf{Reward Guided Filtering}. 
Through \textbf{Reward Guided Filtering}, we apply the reward proxy model to assess synthetic data generated by the generation proxy model and remove samples with low reward scores.
%For synthetic data with low rewards, we introduce the \textbf{Self-Optimizing Refinement} module.  
%We generate multiple candidate responses for each synthetic query and compute their reward scores. 
% \revisezq{Do you only refine filtered out samples? Could you add a motivation of refining filtered out samples?}
Filtering alone may remove a large amount of data, leaving only a small fraction. 
Therefore, we aim to further refine the synthetic data to obtain more high-quality data.
Our \textbf{Self-Optimizing Refinement} module generates multiple candidate responses for each synthetic query and computes their rewards. 
The generation proxy model analyzes the highest and lowest scoring responses and then generates improvement feedback. 
%Then we provide both the highest and lowest rewarded responses to the reward proxy model to generate feedback on how to improve data quality. 
%We prompt the LLM \revisezq{generation proxy model? target LLM?} to refine the synthetic data based on this feedback. 
Based on this feedback, the target LLM refines the synthetic data following a refinement instruction.
The resulting high-quality, filtered, and refined synthetic data is then used to fine-tune the target LLM for domain-specific tasks. 
%Finally, we fine-tune the target LLM on the filtered and refined synthetic data for those domain-specific tasks. 

We conduct extensive experiments across various domain-specific generation tasks, including Medical Question Answering (QA), Legal QA, and Code Generation tasks. 
The results consistently demonstrate the effectiveness of our method in improving the quality of the synthetic data, achieving better performance while preserving privacy.
% 实验的结果
Our main contributions are summarized as follows:

\begin{itemize}[topsep=0.2em, partopsep=0em, itemsep=0.5em, parsep=0em, leftmargin=2em]
    \item We propose \textit{RewardDS}, a novel privacy-preserving fine-tuning framework that improves the quality of synthetic data by training a Reward Proxy Model on the client side to guide synthetic data generation. 
    \item We introduce the Reward Guided Filtering and Self-Optimizing Refinement modules to filter and refine the synthetic data, thereby enhancing its quality.
    \item We conducted extensive experiments across Medical QA, Legal QA, and Code Generation tasks to validate the effectiveness of our proposed framework.
\end{itemize}



% To address these challenges and achieve high-quality synthetic text, we propose \textit{RewardDS} (\textbf{Reward}-driven \textbf{D}ata \textbf{S}ynthesis), a novel privacy-preserving framework that enhances synthetic data quality for LLM fine-tuning by integrating dual proxy models through \textbf{filtering} and \textbf{refining}. 
% % These are done through two key components: \textbf{Reward Proxy Model} and \textbf{Self-Optimizing Refinement}.
% RewardDS bases on sensitive domain-specific private data (queries/responses) in clients, while servers host confidential LLMs inaccessible to clients. 
% In particular, RewardDS comprises two key components--namely, the \textbf{Reward Proxy Model} and the \textbf{Self-Optimizing Refinement}—which are respectively responsible for the filtering and refining processes.


% To mitigate the inaccuracy arisen from the surface-level filtration, apart from existing methods using DP-SGD to fine-tune lightweight Generation Proxy Models such as Qwen2.5-0.5B-Instruct \citep{yang2024qwen2} on clients to synthesize data, RewardDS introduces the \textbf{Reward Proxy Model} alongside the generation model: both are fine-tuned on the client using differentially private training. 
% Specifically, the Generation Proxy Model is DP-fine-tuned on private data to synthesize domain-aligned content on the client side, while the Reward Proxy Model learns to score responses by comparing gold/improved answers against inferior ones via Bradley-Terry loss \citep{bradley1952rank}. 
% Both proxy models are fine-tuned on privacy-preserving datasets before sending them to the server.
% Then on the server, the Reward Proxy Model with domain-specific knowledge evaluates synthetic data quality by assigning each synthetic data with a score $s_j$. 
% Lower scores indicate that the synthetic data is more likely to be noisy, which allows for noise reduction through reward-guided filtration that takes into account the entire sentence's meaning and domain-specific knowledge.

% 前面有什么问题，我们解决了哪些
% 说我们的和之前的knowledgesg的优点
% 讲methodology

% Specifically, the Generation Proxy Model is DP-fine-tuned on private data to synthesize domain-aligned content on the client side, while the Reward Proxy Model learns to score responses by comparing gold/improved answers against inferior ones via Bradley-Terry loss \citep{bradley1952rank}. 
%  
% The server then generates raw synthetic data from the generation model, which is filtered by reward scores. 
% In the \textbf{Reward Guided Filtering} stage, Reward Proxy Model grade each sample with reward with low-reward samples discarded and high-reward ones replicated to obtain the training dataset. 

% Aiming to further refining the synthetic query-response pairs, RewardDS consists of another key component \textbf{Self-Optimizing Refinement}.
% During Self-Optimizing Refinement, the server generates multiple candidate responses per query using the Generation Proxy Model, selects the responses with the lowest reward as rejected response $A_r$ using the reward model, meanwhile marks the highest reward response as selected ones. With the reward LLM-generated feedback, Self-Optimizing Refinement iteratively generates $N$ new candidate responses to achieve data refinement of synthetic query-response datasets.
% With clients handling privacy-sensitive proxy model training, whereas servers manage data synthesis, quality control, and LLM fine-tuning, RewardDS ensures synthetic data remains high-quality and privacy-compliant throughout the pipeline.


% filter+refine
% fliter低的过滤，refine给generate model再生成一次

% Extensive experiments have been conducted to compare our methods with the state-of-the-art baselines on Medical QA, Legal QA, and Code Generation datasets. The results prove the effectiveness of our proposed framework on both privacy and performance benchmarks.
% % 实验的结果
% The contributions are summarized as follows:

% \begin{itemize}[topsep=0.2em, partopsep=0em, itemsep=0.5em, parsep=0em, leftmargin=2em]
%     \item We propose RewardDS, a novel framework that mitigates synthetic data noise by training a client-side Reward Proxy Model to enhance data quality and guide server-side filtering. 
%     % \item We propose Reward Guided Filtering and Self-Optimizing Refinement to reach the state-of-the-art strategies in refining synthetic data.
%     \item We propose Reward Guided Filtering and Self-Optimizing Refinement, for mitigating the performance degradation arises by semantic level metrics evaluation and overcoming the dilemma deduces from high-quality synthetic data selection, respectively.
%     \item We conducted extensive experiments across Medical QA, Legal QA and Code Generation datasets to validate the effectiveness of our proposed framework.
% \end{itemize}
