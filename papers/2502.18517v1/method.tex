% \vspace{-1em}
\section{Problem Statement}
% We consider the scenario where the client holds domain-specific data, such as patients' medical records, which contain sensitive information and cannot be directly released for LLM fine-tuning. 
% %We consider a scenario where the client holds domain-specific data, such as medical dialogues between patients and doctors, which contain sensitive information and cannot be directly released for LLM fine-tuning. 
% These private data consists of two parts: \textit{Query} and \textit{Response}, both of which contain user privacy \citep{Wang2024KnowledgeSGPS}. 
% The server, which hosts the LLM, typically provides services to users via APIs and keeps the LLM weights confidential. 
% As a result, the client cannot access these weights during local fine-tuning. 
% % \revisezq{Even if some LLM weights are open-source, the high cost of training makes local fine-tuning on the client infeasible.}
% % \revisezq{I do not understand this sentence.}
% \textbf{Therefore, how to utilize private data from the client to fine-tune the LLM on the server while ensuring privacy and security, is a crucial challenge.}


% % \revisezq{This paragraph is not related threat model. It is more like a motivation.}
% % \textbf{Therefore, how to utilize private data from the client to fine-tune the LLM on the server while ensuring privacy and security, is a crucial challenge.}
% The current mainstream way is to a lightweight \textbf{Generation Proxy Model} on the client, using it to sample safe synthetic data for further fine-tuning the LLM hosted by the server \citep{yue-etal-2023-synthetic, dayu2024privacy}.
% However, due to the randomness of the sampling process, the synthetic data may be noisy, leading to potential performance degradation.
% It is essiential to explore an more effective way to mitigates this noise, ensuring the synthetic data remains reliable for fine-tuning, while preserving user privacy.

%We consider a scenario where the client holds domain-specific data, such as patients' medical records, which contain sensitive information and cannot be directly transmitted to the server for LLM fine-tuning. 
%The server, which hosts the LLM, provides services via APIs and keeps the LLM weights confidential. 
%Thus, how to leverage the client's private data to fine-tune the LLM on the server while ensuring privacy and security is a crucial challenge.
We consider a scenario where the client holds domain-specific data, such as patient's medical records, which contain sensitive information.  
Hence, directly transmitting those data to servers for LLM fine-tuning is not allowed. 
This private data typically is structured as \textit{Query}-\textit{Response} pairs, with both query and response containing confidential private information \citep{Wang2024KnowledgeSGPS}. 
The server, which hosts the target LLM, offers only API access while keeping model weights confidential, preventing clients from accessing or locally fine-tuning the model. 
While clients can fine-tune lightweight LLMs within their computational constraints, these models have inherently weaker capabilities than the target LLMs. 
This creates a critical challenge: how to leverage a client's private data to improve the server-hosted LLM's performance on domain-specific tasks while preserving privacy, given that clients cannot locally fine-tune the target LLM due to inaccessibility of model weights. 

Existing methods utilize a lightweight \textbf{Generation Proxy Model} on the client side to generate safe synthetic data for fine-tuning the target LLM on the server \citep{yue-etal-2023-synthetic, dayu2024privacy}. 
However, the randomness of the sampling process may introduce noise in the synthetic data, potentially causing performance degradation. 
Therefore, our main goal is to \textit{explore a more effective method for mitigating the noise in synthetic data, enabling better fine-tuning performance while maintaining user privacy}.


\section{Method}
To address the performance degradation caused by noise in synthetic data, we propose a novel framework, \textit{RewardDS} (\textbf{Reward}-driven \textbf{D}ata \textbf{S}ynthesis), as shown in Figure \ref{fig:algorithm}.
Our approach additionally trains a \textbf{Reward Proxy Model} on the client side. 
Then the reward proxy model filters and refines the synthetic data sampled from the generation proxy model through \textbf{Reward Guided Filtering} and \textbf{Self-Optimizing Refinement} modules on the server side. 
Both modules collaborate to enhance the quality of the synthetic data, driven by the reward signal from the reward model.
We will introduce the training process of the generation proxy model and reward proxy model in \cref{sec:Method_P1} and the details of reward guided filtering and self-optimizing refinement module are provided in \cref{sec:Method_P2}.

% \revisezq{Why need to fine-tune two proxy model?}

\setlength{\textfloatsep}{10pt}
\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{latex/fig/main_fig_0215.pdf}
% \vspace{-15pt}
\caption{
The overview of our \textit{RewardDS} framework.
The client uses DP-SGD to fine-tune two lightweight proxy models on privacy-sensitive data: the Generation Proxy Model $W_\text{gen}$ and the Reward Proxy Model $W_\text{rwd}$. 
Both proxy models are then sent to the server. 
The Generation Proxy Model is used to sample raw synthetic data, consisting of queries and responses. The Reward Proxy Model supports the \textbf{Reward Guided Filtering} and \textbf{Self-Optimizing Refinement} modules, which filter and refine the raw synthetic data to produce fine synthetic data. Finally, the target LLM $W_\text{target}$ is fine-tuned on the fine synthetic data and provides service to the client for domain-specific tasks.
}
\vspace{-0.7em}
\label{fig:algorithm}
\end{figure*}


\subsection{Client Side}
\label{sec:Method_P1}
\paragraph{Generation Proxy Model Training.}
% \revisezq{The motivation of fine-tuning a generation proxy model.}
The generation proxy model is responsible for generating safe synthetic data as a substitute for private data.
%The generation proxy model is directly trained on client's private data. 
%We utilize the generation proxy model to synthesize secure data which resembles the client's private data. 
Following \cite{yue-etal-2023-synthetic, dayu2024privacy, dayu2024dp, Kurakin2023HarnessingLM}, we fine-tune a generation proxy model on the client’s private data using the DP-SGD algorithm \cite{Abadi2016DeepLDP}. 
The backbone of generation proxy model should be lightweight due to limited computational resources on the client side, e.g., Qwen2.5-0.5B-Instruct \citep{yang2024qwen2}.  
The DP-SGD algorithm protects the privacy of the training data by injecting noise into the gradients during model training. 
This noise ensures that the inclusion or exclusion of any individual training sample has minimal impact on the fine-tuned model, thereby providing privacy protection. 
% \revisezq{The privacy budget of DP-SGD is defined as $(\epsilon, \delta)$, providing a theoretical measure of privacy protection.}
% \revisezq{This sentence is too detailed. Why need to mention it here. Can I put it into experiment?}

\paragraph{Reward Proxy Model Training.}
%Unlike the generation proxy model, which is fine-tuned directly on the client’s private data and used to synthesize similar but secure data, the reward model is responsible for evaluating the quality of the synthetic data. 
% \revisezq{Why you say fine-tune, I think it should be train. }
%We train the reward proxy model on delicatedly designed data. 
%Assuming the initial backbone model is $W_0$, the fine-tuned generation proxy model is $W_{gen}$, the fine-tuned reward proxy model is $W_{\text{rwd}}$ and the query-response pair from private data is ($Q, A_{\text{gold}}$).
%Following \citet{liu2024skywork}, we should construct a chosen response $A_c$ and a rejected response $A_r$ for each user query $Q$ before reward model training.
%We feed the user query $Q$ into $W_0$ and $W_{gen}$ to obtain corresponding responses $A_0$ and $A_{\text{gen}}$. 
%Then, we choose $A_{\text{gen}}$/$A_\text{gold}$ as the chosen response $A_c$ and $A_0$ as the rejected response $A_r$. 
%The reward proxy model is also lightweight as it is fine-tuned on the client side.
%We fine-tune it using DP-SGD to prevent privacy leakage. 
%We also adopt the lightweight model as the backbone model and fine-tune it using DP-SGD to prevent privacy leakage. 
% Qwen2.5-0.5B-Instruct
%We employ the Bradley-Terry model \citep{bradley1952rank} to define the training loss:
The reward model is responsible for evaluating the quality of the synthetic data. 
It should provide higher rewards for high-quality data while lower rewards for poor-quality data.  
Following standard reward model training practices \citet{liu2024skywork}, we train the reward proxy model using paired comparison data. 
Let $W_0$ denote the initial backbone model, $W_{gen}$ the fine-tuned generation proxy model, and $W_{\text{rwd}}$ the fine-tuned reward proxy model. 
For each query $Q$ from the private dataset with its gold response $A_{\text{gold}}$, we generate two responses: $A_0$ from $W_0$ and $A_{\text{gen}}$ from $W_{gen}$. 
We then create preference pairs by selecting either $A_{\text{gen}}$ or $A_{\text{gold}}$ as the chosen response $A_c$, with $A_0$ serving as the rejected response $A_r$.
The reward proxy model maintains a lightweight architecture for client-side deployment and is fine-tuned using differential privacy (DP-SGD) to prevent privacy leakage. 


Following \citet{Ra2023DPO}, we define the training loss as:
\begin{equation}
\mathcal{L} = -\log \sigma\left( f_{\text{rwd}}(Q, A_c) - f_{\text{rwd}}(Q, A_r) \right),
\end{equation}
where $f_{\text{rwd}(\cdot)}$ represents the reward predicted by $W_\text{rwd}$.
This training loss encourages the reward model to assign higher scores to responses from the generation proxy model and gold responses compared to those from the initial backbone model. 

After training, both generation proxy model and reward proxy model are sent to the server. 
% We analyze the privacy preserving property of this transmission in \S \ref{sec:privacy_analysis}.


\subsection{Server Side}
\label{sec:Method_P2}

\paragraph{Synthetic Data Generation.}
% \revisezq{Since our goal is to fine-tune the LLM on the server, we should sample synthetic data from the generation proxy model. }
% \revisezq{Check Chinsese Comment.}
%ZZQ-Feb-14: 因果关系不明显。为什么用generation proxy model sample synthetic data 的动机讲清楚。
%First, following \citet{dayu2024privacy, Wang2024KnowledgeSGPS}, we prompt $W_\text{gen}$ to generate raw synthetic queries. 
%Second, we use $W_\text{gen}$ to generate synthetic responses for those queries. 
%We call these query and response pairs as raw synthetic data. 
Following \citet{dayu2024privacy, Wang2024KnowledgeSGPS}, we use $W_\text{gen}$ to generate both synthetic queries and their corresponding responses, collectively referred to as raw synthetic data. 
Although the generation proxy model $W_\text{gen}$ is trained on private data and learns domain-specific knowledge, the generation process of raw synthetic data is random and unstable. 
As a result, the raw synthetic data inevitably contains noisy samples, and fine-tuning the LLM directly on this data can lead to performance degradation. 


\paragraph{Reward Guided Filtering.}
% A simple yet effective approach is to use the 
We leverage the reward proxy model $W_\text{rwd}$ to evaluate each synthetic data and filter out those with low rewards.
A lower reward indicates a higher likelihood of the synthetic data being noisy. 
We select only the top $\lfloor L/k \rfloor$ data, where $L$ is the total number of synthetic data and $k$ is the partition fold (Line \ref{line:filter1} in \cref{algo:main}).
To compensate for the reduced synthetic dataset size after filtering, we replicate the high-reward data to maintain the total data volume during the target LLM fine-tuning (Line \ref{line:filter2} in \cref{algo:main}). 

\paragraph{Self-Optimizing Refinement.}
%Relying solely on filtering does not fully utilize the entire synthetic dataset and may even lead to the LLM overfitting the small number of samples with high reward. 
%Given the self-reflective capabilities of LLMs \citep{aman2023self}, we explore refining the synthetic data during the LLM fine-tuning process. 
%Prior to LLM fine-tuning, we simultaneously generate $N$ candidate responses through the generation proxy model for each synthetic query (Line \ref{line:refine1} in \cref{algo:main}).
%And then the response with the highest reward, judged by the reward proxy model, is selected as the final synthetic response (Line \ref{line:refine2} in \cref{algo:main}). 

While filtering mitigates noise, it selects only a small subset of samples, potentially leading to overfitting on limited data.
Building on LLMs' self-reflection capabilities \citep{aman2023self}, we implement a dynamic data refinement strategy to improve low-reward samples, enhancing overall data quality.
Initially, for each synthetic query, we generate $N$ candidate responses rather than only one response using the generation proxy model (Line \ref{line:refine1} in \cref{algo:main}). 
The reward proxy model then selects the response with the highest reward score as the chosen response (Line \ref{line:refine2} in \cref{algo:main}). 
We directly fine-tune the target LLM $W_\text{target}$ on the chosen response (Line \ref{algo:line16} in \cref{algo:main}).
% Unlike greedy search generation, generating multiple responses allows for parallel exploration of different response trajectories.
% \revisezq{Here, you select a response with the highest reward as final synthetic response.}

After fine-tuning the target LLM $W_\text{target}$ for each epoch, we dynamically refine the synthetic data for the next epoch’s training. 
For each query’s $N$ candidate responses, we identify the lowest-reward responses and combine them with the highest-reward responses to form the rejected ($A_r$) and chosen ($A_c$) response pairs. 
The generation proxy model $M_{\text{gen}}$ analyzes these responses and provides feedback, highlighting the strengths of $A_c$ and weaknesses of $A_r$ (Line \ref{line:refine3}). 
This feedback, along with the original query, guides the target LLM $W_{\text{target}}$ to generate $N$ refined candidate responses (Line \ref{line:refine4}). 
Finally, the reward proxy model selects the highest-reward response from these refined candidates for the next epoch’s LLM fine-tuning (Line \ref{algo:lin20}).

% After fine-tuning the target LLM $W_\text{target}$ during each epoch, we refine the synthetic data with the help of the LLM itself for the next epoch training. 
% For each synthetic query's $N$ candidates, we also identify the lowest reward responses to combine with the highest reward responses as the rejected ($A_r$) and chosen ($A_c$) responses, respectively. 
% The generation proxy model $M_{\text{gen}}$ analyzes these responses and generate feedback which highlights the strengths of $A_c$ and weaknesses of $A_r$ (Line \ref{line:refine3}). 
% This feedback, combined with the original query, guides the target LLM $W_{\text{target}}$ to generate $N$ refined candidate responses (Line \ref{line:refine4}). 
% We use the reward proxy model to select the highest reward response from these refined candidate responses for the next epoch LLM fine-tuning.
% \revisezq{Here, you generate $N$ candidates. Is it contradictory?}


%For each synthetic query with $N$ candidate responses, we select the response with the lowest reward as the rejected response $A_r$ and the one with the highest reward as the chosen response $A_c$. 
%Both $A_c$ and $A_r$ are provided to the generation proxy model $M_{\text{gen}}$, which evaluates the strengths of the chosen response and the weaknesses of the rejected one, for generating feedback for further refinement (Line \ref{line:refine3}). 
%The user query, along with feedback, is then passed to the LLM $W_{\text{target}}$, which generates $N$ new candidate responses to achieve data refinement (Line \ref{line:refine4}).

The collaborative process between the reward-guided filtering and self-optimizing refinement modules is presented in \cref{algo:main}. 
The refinement instruction templates are provided in Appendix \ref{prompt_template_details}.
After the LLM is fine-tuned on the refined synthetic data, it can provide service to the client for those domain-specific tasks.



\begin{algorithm}[!t]
\SetAlgoLined
      \fontsize{8pt}{9pt}\selectfont
        \caption{\footnotesize \textit{RewardDS} based LLM Fine-tuning}
\label{algo:main}
    \KwIn{Synthetic query set $\mathcal{Q}_\text{query}$, number of synthetic query $L$, number of candidate responses $N$, partition fold $k$, generation proxy model $W_\text{gen}$, reward proxy model $W_\text{rwd}$, target LLM $W_\text{target}$, training epoch $T$ }
    \KwOut{The fine-tuned LLM $W_\text{target}^T$}

    \tcp{  Before Fine-tuning LLM}
    \For{each query $q \in \mathcal{Q}_\text{query}$}
    {   
        Generate candidate response set: $\{A_j\}_{j=1}^N \gets W_\text{gen}(q)$ \label{line:refine1}
    
        Predict the reward score: $\{s_j\}_{j=1}^N \gets W_\text{rwd}(q, A_j)$
        
        Select the best and the worst response: \newline
        \hfill
        $
        (A_c, A_r) \gets \left( A_{\argmax_j s_j},\ A_{\argmin_j s_j} \right)
        $ \label{line:refine2}
        
        Record the best reward score: $s_c \gets \max_j s_j$
    }
    
    \fontsize{8pt}{10pt}\selectfont
    Gather the initial synthetic dataset: $\mathcal{D}_0 \gets \{(q_i, A^i_c, A^i_r, s_c^i )\}_{i=1}^L$
     
    Sort $\mathcal{D}_0$ by reward: $\mathcal{D}_0^\text{sorted} \gets \{(q_i, A^i_c, A^i_r, s_c^i )\}_{i=1}^L$, where \newline
      \hfill $s_c^1 \geq \cdots \geq s_c^L$
      
    Partition $\mathcal{D}_0^\text{sorted}$ into $k$ folds: $\{\mathcal{D}_0^m\}_{m=1}^k \gets \text{split}(\mathcal{D}_0^\text{sorted}, k)$

    Extract top-$\lfloor L/k \rfloor$ samples: $\mathcal{D}_\text{high} \gets \mathcal{D}_0^1$ \label{line:filter1}
    
    Replicate subset to obtain the train set: $\mathcal{T}_0 \gets \bigoplus_{\lceil L/|\mathcal{D}_\text{high}| \rceil} \mathcal{D}_\text{high}$ \label{line:filter2}

    \fontsize{8pt}{9pt}\selectfont
    Determine score threshold $\tau$: 
    $
    \tau \gets \min_{s_c \in \mathcal{D}_\text{high}} s_c
    $

    \tcp{ During Fine-tuning LLM}
    Initialize target LLM: $W_\text{target}^0 \gets W_\text{target}$
    
    \For{iteration  $t=1$ \KwTo $T$}
    {  
        Fine-tune target LLM $W_\text{target}^{t-1}$ on $\{ (q, A_c) \in \mathcal{T}_{t-1}\}$ and get $W_\text{target}^t$ \label{algo:line16}

         \For{each query $q \in \mathcal{Q}_\text{query}$}
        {
        \fontsize{8pt}{10pt}\selectfont
            Generate feedback $\phi$: $\phi \gets W_\text{gen}(q, A_c, A_r)$ 
            \label{line:refine3}
            
            Re-generate the candidate response set: \newline
            $\{A_j\}_{j=1}^N \gets W_\text{target}^t(q, \phi)$
            \label{line:refine4}

            Predict the reward score, select the best and worst responses, and record the highest reward score to update $\mathcal{D}_{t-1}$, yielding $\mathcal{D}_t$. \label{algo:lin20}
        }

        Filter and get new training set $\mathcal{T}_t$: \newline
        $
        \mathcal{T}_t \gets \left\{(q, A_c, A_r, s_c) \in \mathcal{D}_t \mid s_c \geq \tau \right\}
        $
        
    }
    \KwRet{$M_\text{target}^t$}

\end{algorithm}


