\clearpage
\newpage
\appendix

\section{Details of Datasets}
\label{app:data_details}

\begin{table*}[!b]
    \centering
    \def\arraystretch{1.7} % Adjust row spacing
    \resizebox{0.85\textwidth}{!}{ % Control table width
        \large
        \begin{tabular}[h]{l l|cccc}
            \toprule 
           \textbf{Task} & \textbf{Dataset} & \textbf{Private Train Set} & \textbf{Dev Set} & \textbf{Test Set} & \textbf{Sampling Synthetic Data} \\
            \cmidrule(lr){1-6}
            \textbf{Medical QA} & HealthCareMagic-100k & 3364 & 112 & 1683 & 6728 \\
            \textbf{Financial QA} & fingpt-fiqa\_qa & 1693 & 18 & 1711 & 3386 \\
            \textbf{Code Generation} & opc-sft-stage2 & 1497 & 79 & 1449 & 2994 \\
            \bottomrule  
        \end{tabular}
    }
    \vspace{-0.5em}
    \caption{The dataset statistics of the medical QA, financial QA and code generation task. All train set is hold by the client and is regard as the private data. 
    The size of sampling synthetic data is two times of the size of the private train set.}
    \label{tbl:statistics}
\end{table*}


To evaluate the performance of the compared methods on domain-specific tasks, we focus on three tasks: Medical Question-Answering (QA), Financial QA, and Code Generation. 
For the medical QA task, we use the HealthCareMagic-100k dataset \citep{li2023chatdoctor}; for the financial QA task, we use the fingpt-fiqa\_qa dataset \citep{zhang2023instructfingpt}; and for the code generation task, we use the opc-sft-stage2 dataset \citep{Huang2024OpenCoderTO}.

As \citet{dong2024gene} points out, these public datasets suffer from a ``data contamination'' issue, where some of the data may have been used to train LLMs on the server, causing the models to memorize it and leading to unnaturally high performance.
Moreover, the initial datasets are highly redundant, containing many similar samples. 
To accurately assess the domain-specific performance of different baselines, we should pre-process these datasets. 
To be specific, firstly, we evaluate the dataset using the Qwen2.5-7B-Instruct model \cite{yang2024qwen2} and exclude samples with high accuracy, as higher accuracy suggests these samples may have been part of the LLM's training data and are thus contaminated.

After addressing the contamination issue, we use the Sentence-T5-Base model \cite{ni2022sen} to compute embeddings for each sample and calculate their similarity. 
This allows us to remove highly similar samples, ensuring deduplication. 
The pre-processed dataset is then split into private train set, dev set, and test set, with the detailed statistics shown in Table \ref{tbl:statistics}. For fair comparison across all methods, we control the size of our sampled synthetic dataset to be twice the size of the private training set, as shown in Table \ref{tbl:statistics}.


\section{Compared Methods}
\label{app:compare}
Here, we will provide more detailed introductions to all compared methods:

% \textbf{Vanilla LLM} involves users directly utilizing the LLM on the server for those domain-specific tasks;
% \textbf{Locally Fine-tuning} involves users fine-tuning a lightweight proxy model locally for those tasks. 



\paragraph{Vanilla LLM:} Vanilla LLM directly transmits the original private datasets to the server and uses Qwen2.5-7B-Instruct model \citep{yang2024qwen2} to generate the answer without any privacy protection.
% 直接利用服务器上的LLM来完成那些特定于领域的任务
% Qwen 7b

\paragraph{Locally Fine-tuning:} Locally Fine-tuning implements full-parameter fine-tuning \citep{DingFullparameter23} of the client-side lightweight Qwen2.5-0.5B-Instruct model \citep{yang2024qwen2} across individual domain-specific datasets. The optimized model is subsequently used for inference tasks on three benchmark datasets.
% 用户在本地为这些任务微调轻量级代理模型
% Qwen 0.5b

\paragraph{DP-Generation:} As proposed by \citet{Kurakin2023HarnessingLM}, DP-Generation first uses DP to full-parameter fine-tune Qwen2.5-0.5B-Instruct model as Generation Proxy Model on the client side.
Then transmit the Generation Proxy Model to the server for synthetic data sampling. 
% For each task, create a prefix $p =$
% \texttt{"[TaskName] [LabelName\textsubscript{y}]"} as the model input and autoregressively sampled at least the same amount of synthetic data as in the original training dataset. 
Then, the synthetic data is used to fine-tune the Qwen2.5-7B-Instruct model on the server for inference service.
% \textbf{DP-Generation} \citep{Kurakin2023HarnessingLM} fine-tunes the generation proxy model and generates synthetic data for LLM fine-tuning; 
% 对生成代理模型进行微调，并生成用于LLM微调的合成数据
% 微调Qwen 0.5b模型，再微调server 7B

\paragraph{DP-Instruct:} On the basis of DP-Generation, DP-Instruct \citep{dayu2024privacy} introduces an additional step to filter the synthetic data. 
After sampling synthetic data through the Generation Proxy Model, it clusters synthetic instruction datasets using $K$-means clustering on the Sentence-T5-base \citep{NiT522} embeddings. 
For each real instruction, find the nearest centroid and resample initial synthetic instructions through the privatized histogram. 
Then, use the resampled synthetic instructions to fine-tune the Qwen2.5-7B-Instruct model on the server.
% \textbf{DP-Instruct} \citep{dayu2024privacy} introduces additional filtering operations based on the similarity of synthetic queries before LLM fine-tuning;
% 在LLM微调之前，基于合成查询的相似性进行额外的过滤操作
% DP-instruct privacy data; 相似度取最高
% 对生成代理模型进行微调，并生成用于LLM微调的合成数据，过滤操作Server


\paragraph{KnowledgeSG:} Proposed by \citet{Wang2024KnowledgeSGPS}, KnowledgeSG first fine-tune a client-side the Qwen2.5-0.5B-Instruct model $\mathbf{W}_{Loc}$ with DP. 
Then transmit the model $\mathbf{W}_{DP}$ to the server and generate synthetic data with the model. 
Next, it filters the synthetic data with BLEU metrics between the synthetic data and original private datasets. 
The filtered synthetic instructions are fed into the professional model $\mathbf{W}_{Pro}$, which is the Qwen2.5-7B-Instruct in our reproduction, to generate preferable responses corresponding to these instructions. 
Finally, it uses the generated instructions and responses to fine-tune $\mathbf{W}_{DP}$ and obtain the final desired model for inference.

% \textbf{KnowledgeSG} \citep{Wang2024KnowledgeSGPS} utilizes the synthetic data to to enhance the generation proxy model for domain-specific tasks instead of fine-tuning the LLM.
% 利用合成数据来增强针对特定领域任务的生成代理模型，而不是对LLM进行微调
% 客户端Qwen 0.5b，服务端合成数据，LLM采样回答，采样回答后再微调一遍小模型，Qwen 7b

\section{Implementation Details}
\label{app:imple_details}
% gen_cnt, n_split
We use the Qwen2.5-0.5B-Instruct \cite{yang2024qwen2} as the backbone for both the generation proxy and reward proxy models, and the Qwen2.5-7B-Instruct as the LLM on the server. 
For DP fine-tuning of the proxy models, we follow the codebase from \citet{li2024privlmbench}, training both models for 3 epochs with a batch size of 4 and a gradient accumulation step of 16. We freeze the embedding layer of the backbone and train the other parameters with a learning rate of 4e-5. 
The privacy budget for fine-tuning both proxy models is set to $(8, 1e^{-5})$, leading to a total privacy budget of $(16, 2e^{-5})$ due to the sequential composition law of the DP mechanism \cite{Abadi2016DeepLDP}. 
These settings align with established DP deployments such as Apple's QuickType and Google's models, as noted by \citet{Nils2023ana}.

During synthetic data sampling, we use the vLLM framework \cite{kwon2023efficient} for fast inference, setting the batch size to 32 and sampling 6 candidate responses for each synthetic query. 
The sampling templates are detailed in Appendix \ref{prompt_template_details}. 
For Reward Guided Filtering, we sort the dataset by reward score, split it into $k$ folds, and select the fold with the highest score, setting $k$ to 6 for medical QA, 5 for financial QA, and 8 for code generation. 
For Self-Optimizing Refinement, we set the number of candidate responses $N$ as 3 for medical QA and code generation, 2 for finanacial QA task.
The hyperparameter analysis is provide in Section \ref{sec:hyper} and Appendix \ref{app:hyper}.
The generation temperature is 1.0 and top-p is 0.7 to enhance diversity. The templates used for generating feedback are provided in Appendix \ref{prompt_template_details}.

For LLM fine-tuning on the server, we use the standard SGD algorithm and train the model for 3 epochs with a learning rate of 4e-5 and a batch size of 64. 
The maximum sequence length for all fine-tuning processes is set to 768. All training and generation processes are conducted on an A800 80G.




\section{Details of LLM-Judger Training and Evaluation}
\label{app:llm_judge}
Since ROUGE-L, ROUGE-1, and PPL metrics do not fully capture the quality of generated outputs in QA tasks, we use the LLM-Judge \cite{Lia2023judging} approach to evaluate the generated outputs for medical QA and financial QA tasks.

First, we fine-tune the LLM-Judgers for these domain-specific tasks (medical QA and financial QA). The fine-tuning process is similar to that of our reward proxy model, where we construct preference pair data as training data and use Bradley-Terry loss \cite{liu2024skywork} for training. 
The key difference is that we use the more powerful Qwen2.5-13B-Instruct backbone and fine-tune it with the AdamW optimizer, without adding DP noise. 
We fine-tune the LLM-Judger for 3 epochs with a learning rate of 4e-5.

During evaluation, we provide the LLM-Judger with both the user query and the generated output, allowing the judger to score the outputs. 
The judge template is provided in Appendix \ref{prompt_template_details}. 
We then compare the scores of outputs from our method and other baselines. 
If the score difference is less than 1, it is considered a tie. Otherwise, the output with the higher score is viewed as the winner.





\section{Hyperparameter Analysis}
\label{app:hyper}

\begin{figure}[!htbp]
    \centering
    % \raggedleft
    \begin{minipage}{0.80\columnwidth} 
    \centering
    \includegraphics[width=1\textwidth]{latex/fig/legend.pdf}
    \label{fig:grow_legend}
  \end{minipage}
  \vspace{-2.2em}
  \vskip\baselineskip % 换行
  
    \begin{minipage}{0.23\textwidth}
        \centering
        \subfigure{
             \label{fig:fin_hyper_1}     
            \includegraphics[width=1\columnwidth]{latex/fig/fold_fin.pdf}  
        }  
    \end{minipage} 
    % \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \subfigure{
             \label{fig:fin_hyper_2}     
            \includegraphics[width=1\columnwidth]{latex/fig/N_can_fin.pdf}  
        }  
    \end{minipage}
    \hfill

        \begin{minipage}{0.80\columnwidth} 
        \centering
        \includegraphics[width=1\textwidth]{latex/fig/legend_2.pdf}
        \label{fig:grow_legend}
      \end{minipage}
      \vspace{-2.2em}
      \vskip\baselineskip % 换行
  
    \begin{minipage}{0.23\textwidth}
        \centering
        \subfigure{
             \label{fig:code_hyper_1}     
            \includegraphics[width=1\columnwidth]{latex/fig/fold_code.pdf}  
        }  
    \end{minipage} 
    % \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \subfigure{
             \label{fig:code_hyper_2}     
            \includegraphics[width=1\columnwidth]{latex/fig/N_can_code.pdf}  
        }  
    \end{minipage}

    
    \vspace{-1.3em}
    \caption{
    Analysis of hyperparameters including the number of folds $k$ and the number of candidate responses $N$ in \cref{algo:main} on the financial QA task and code generation task.
    For financial QA task, to analyze $k$, we set $N = 2$ and to analyze $N$, we set $k =5$.
    For code generation task, to analyze $k$, we set $N=3$ and to analyze $N$, we set $k=8$.
    }
    \label{fig:app_hyper_fin}
\end{figure}
In this section, we conduct additional experiments to analyze the effect of hyperparameters on our methods for the financial QA and code generation tasks.

For the number of partition folds, $k$, it controls the amount of data selected as clean data. As shown in Figure \ref{fig:fin_hyper_1}, setting $k=5$ yields the best performance for the financial QA task. 
For the code generation task, as shown in Figure \ref{fig:code_hyper_1}, $k=8$ performs best. 
Larger values of $k$ lead to the exclusion of more synthetic data, which may result in the model overfitting on smaller data subsets and cause performance degradation.

For the number of candidate responses, $N$, a larger $N$ increases the likelihood of selecting better responses from the candidates. 
However, increasing $N$ also adds more computational cost, and the performance gain is marginal, as illustrated in Figure \ref{fig:fin_hyper_2} and Figure \ref{fig:code_hyper_2}. Therefore, we set $N=2$ for the financial QA task and $N=3$ for the code generation task.



\section{Extension to More LLM Backbones}
\label{sec:ext_LLM}

\begin{table*}[!b]
	\centering
        \def\arraystretch{1}
	\resizebox{0.78\textwidth}{!}{
    	\begin{tabular}{l 
       p{1cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering} | p{1cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering}
         } 
	\toprule
          \multirow{2}{*}{Methods}  & \multicolumn{3}{c|}{Llama-2-7b-chat-hf}  & \multicolumn{3}{c}{Qwen2.5-14B-Instruct}  \\
            \cmidrule(lr){2-7}
		& \multicolumn{1}{c}{R1 $\uparrow$}  & RL $\uparrow$ &  \multicolumn{1}{c|}{PPL $\downarrow$ }  & \multicolumn{1}{c}{R1 $\uparrow$}  & RL $\uparrow$ &  \multicolumn{1}{c}{PPL $\downarrow$ }\\
            % \cmidrule(lr){1-9}
            % \cmidrule(lr){1-1} \cmidrule(lr){2-4}  \cmidrule(lr){5-7}  \cmidrule(lr){8-9}
            \midrule
            \multicolumn{1}{l|}{Vanilla LLM} & 22.37 & 11.47 & 1.37 & 23.19 & 12.26 & 1.12 \\
            
            \multicolumn{1}{l|}{Locally Fine-tuning} & 23.82 & 15.46 & 1.71 & 23.82 & 15.46 & 1.71   \\

            % \midrule
            % \cmidrule(lr){1-1} \cmidrule(lr){2-4}  \cmidrule(lr){5-7}  \cmidrule(lr){8-9}
            % \cmidrule(lr){1-9}
            
            \multicolumn{1}{l|}{DP-Generation \citep{Kurakin2023HarnessingLM}} & 16.46 & 11.23 & 1.06 & 18.07 & 11.82 & \textbf{1.14}  \\
            
            \multicolumn{1}{l|}{DP-Instruct \citep{dayu2024privacy}} & 14.25 & 10.06 & \textbf{1.04} & 16.89 & 11.39 & 1.15 \\
            
            \multicolumn{1}{l|}{KnowledgeSG \citep{Wang2024KnowledgeSGPS}} & 22.75 & 12.73 & 1.25 & 21.05 & 11.25 & 1.34 \\

            \rowcolor{lightgray!45}
            \multicolumn{1}{l|}{\textbf{RewardDS}} & \textbf{28.19} & \textbf{16.06} & 1.17 & \textbf{24.15} & \textbf{16.31} & 1.81 \\
            
            
            \bottomrule 

  
		\end{tabular}
  
	}

 \caption{
Comparisons of our method with baselines on the Medical QA when applied to more LLM backbones: 
Llama-2-7b-chat-hf \citep{metaai2023llama2}, Qwen2.5-14B-Instruct \citep{yang2024qwen2}.
Numbers in \textbf{bold} represent the best performances. 
Due to computational resource constraints, we perform full-parameter fine-tuning for Llama-2-7B-chat-hf, while employing LoRA fine-tuning for Qwen2.5-14B-Instruct.
}
\label{tbl:more_back}
\end{table*}

We have evaluated our \textit{RewardDS} on more LLM backbones, such as Llama-2-7B-chat-hf \cite{metaai2023llama2} and Qwen2.5-14B-Instruct \cite{yang2024qwen2}.
Due to the computational resource constraints, we conduct the full-parameter fine-tuning for Llama-2-7B-chat-hf on the synthetic data and apply the LoRA fine-tuning \cite{edward2022lora} for Qwen2.5-14B-Instruct.
We set the lora rank $r$ as 64 and $\alpha$ at 16.
We add the lora layer for each linear layer in the Qwen2.5-14B-Instruct model.

As shown in Table \ref{tbl:more_back}, \textit{RewardDS} outperforms other baselines regardless of whether Llama-2-7B-chat-hf or Qwen2.5-14B-Instruct is used as the LLM backbone. 
This strongly demonstrates that our method is consistently effective, regardless of the LLM backbone. 
It is worth noting that although Qwen2.5-14B-Instruct has a larger number of parameters compared to Llama-2-7B-chat-hf, our method performs better on the Llama-2-7B-chat-hf model. 
This is likely due to the use of LoRA fine-tuning on Qwen2.5-14B-Instruct, rather than full-parameter fine-tuning. 
We believe that applying full-parameter fine-tuning to the Qwen2.5-14B-Instruct model would lead to better performance.

\section{Case studies}
\label{app:case}
Here, we provide an example demonstrating how our method refines synthetic data to improve its quality.
As shown in Figure \ref{fig:training_samples}, the initial synthetic sample contains noise, with redundant and meaningless content highlighted in red.
Directly using these synthetic sample will do harm to the fine-tuning process of target LLM.
After refinement by \textit{RewardDS}, the response becomes more coherent and informative, as highlighted in green.
Then these refined synthetic sample can be used to fine-tune target LLM for domain-specific tasks.

\begin{figure}[h]
  \centering
  \raggedleft
    \begin{minipage}{0.98\columnwidth}
    \includegraphics[width=1\textwidth]{latex/fig/example.pdf}
    \label{fig:training_samples_legend}
  \end{minipage}
  \vspace{-1.5em}
  \caption{A synthetic sample refined by our \textit{RewardDS} on the Medical QA task. Text highlighted with a green background indicates the good and correct response part. Text highlighted with a red background denotes the bad and incorrect response part. }
  \label{fig:training_samples}
\end{figure}

\input{latex/appendix/prompts}