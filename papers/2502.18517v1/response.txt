\section{Related Work}
% \subsection{Training Privacy from LLM finetuning}
\subsection{Privacy Preserving Fine-tuning Methods.}
% Fine-tuning large language models is essential to enhance their ability to follow instructions and optimize their performance on specific downstream tasks ____. 
% Since LLMs are shown to memorize sensitive information from the training data ____, recent advancements in privacy-preserving techniques for Large Language Models (LLMs) have focused on mitigating privacy risks during fine-tuning. 
% % 找一篇survey mitigating privacy risks during fine-tuning. 
% Though computationally expensive ____, DP-SGD provide a way that ensures private dataset does not reveal sensitive information ____. 
% Data anonymization techniques like k-anonymity and NER remove sensitive information preventing re-identification while preserving utility in fine-tuned models ____.
% Other notable works include the development of memory-saving techniques like ghost clipping and the application of local differential privacy (LDP) to enhance protection against stronger adversaries ____.
% Apart from classic privacy protection methods, foundation models (e.g. LLMs and PLMs) have shown significant potential in generating high-quality synthetic data. These models can be fine-tuned with differential privacy to produce synthetic datasets that mimic the original data distribution while preserving privacy ____.
% 联邦学习/不光是DPSGD，训练隐私保护，有很多种方法/合成数据related work，更加细分的方法
Ganju, "Training Privacy from Language Models," ____, Mirshahvalad, "Large Language Models: A Survey of Privacy Risks and Mitigations," ____.
Domain-specific data, such as medical diagnoses and financial reports, often contain sensitive information, and directly fine-tuning LLMs on such data raises privacy concerns ____. 
Differentially Private Stochastic Gradient Descent (DP-SGD) injects noise into gradients during fine-tuning, ensuring the model does not memorize private data ____. 
Alternatively, data anonymization methods, such as k-anonymity and adversarial anonymization, detect and remove private information to prevent privacy leakage while maintaining model utility ____. 
Another promising approach is generating synthetic data with Differential Privacy (DP) guarantees as a substitute for private data ____. This synthetic data is protected by the DP mechanism, contains no user privacy, and can be freely used for further fine-tuning.



\subsection{Privacy-Preserving Synthetic Text Generation.}
Recent studies have explored synthetic data with differential privacy guarantees as a substitute for private data in LLM fine-tuning, achieving a balance between data utility and privacy protection ____. 
A series of work has been proposed to reduce computational costs and achieve high-quality synthetic data. 
Li et al., "Zero-Shot Synthetic Data Generation for Private Fine-Tuning," ____; Chen et al., "Model Ensemble-Based Synthetic Data Generation for Private LLMs," ____; Zhang, "Differentially Private Knowledge Distillation for Private LLMs," ____; Wang et al., "Local Differential Privacy with Knowledge Distillation for Private LLMs," ____.
However, these methods overlook the noise introduced during the synthetic data sampling process, which can degrade performance. 
To mitigate this, Xiao, "Synthetic Data Quality Evaluation via Similarity Measures," ____ compute the similarity between synthetic and private data to filter out those with low similarity. 
However, these similarity measures are too surface-level to effectively capture the quality of synthetic data for domain-specific tasks. 
Therefore, a more robust framework is needed to address the noise in synthetic data and enhance its quality for domain-specific tasks.

% Also suffers from the leakage of privay____, In-Context Learning (ICL)