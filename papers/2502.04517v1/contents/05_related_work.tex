%!TEX root=iclr2025_conference.tex
\paragraph{Training based Alignment}

Supervised fine-tuning and instruction tuning~\citep{wei2021finetuned} are common methods to align an LLM to labeled data. RLHF~\citep{christiano2017deep,ziegler2019fine,lee2021pebble,nakano2021webgpt,snell2022offline} methods can align an LLM directly to human preferences. First, a reward model is trained on a dataset of human preferences using the Bradley Terry model ~\citep{bradleyterry1952paired} and then the LLM is updated, based on the reward model, using an RL algorithm such as PPO~\cite{schulman2017proximal}. However, updating the LLM with RL is expensive and researchers have explored cost-effective alternatives.

\cite{liu2023chain} convert the preference data into sequence of sentences which are then used to fine-tune the LLM. \cite{dong2023raft} used the reward model to filter high quality training samples and fine-tunes on them avoiding undesirable behavior. DPO \cite{rafailov2023direct,rafailov2024qfunction} avoids learning a reward model explicitly and finds an equivalent objective to RLHF which can be optimized by supervised learning. Even though the resulting optimization is cheaper than RL, nonetheless, it still involves updating the LLM.

Preference data itself provides sequence-level supervision. Some works have atttempted to collect and use fine-grained preferences by using either human annotators~\citep{wu2024fine} or LLMs~\citep{cao2024sparserewardsenhancingreinforcement}.


\paragraph{Guided Decoding}

In the guided decoding literature, a number of methods consider guidance at a step or process level~\citep{welleck2022naturalprover,uesato2022solving, lightman2023let, krishna2022rankgen,li2023making, khalifa2023grace, yao2023tree}. 

Some methods have applied token-level functions~\citep{dathathri2019plug, krause2021gedi, yang2021fudge,chaffin2022ppl, liu2023attribute} but they do not consider RGTG based on preference data. 

\citet{khanov2023alignment} introduce an RGTG method but they rely on a full-sequence reward model for partial sequence decoding. \citet{deng2023reward} learn to distill a partial sequence reward model, starting from the full-sequence model using a square loss function. \citet{mudgalcontrolled} employ a similar approach but instead of using preference data, generate a dataset by roll-outs from the base LLMs. \citet{han2024value} also use the base LLM to gather a dataset but employ TD learning to train the partial sequence reward model. Different from these works, \citet{zhao2024probabilistic} derive an RGTG method based on sequential Monte Carlo and demonstrate that it can approximate RLHF.



  



