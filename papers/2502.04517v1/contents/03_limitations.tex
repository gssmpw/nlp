%!TEX root=icml2025.tex

We will discuss two primary limitations of current RGTG methods, namely high decoding cost and sub-optimal rewards. In the next section, we propose a solution to address these limitations.

\subsection{High Decoding Cost}
Most RGTG methods default to training a full sequence reward model $r_\phi$ and then either a) use it to directly score partial sequences \cite{khanov2023alignment} or b) distill a partial sequence value model $V_\theta$ from the full sequence reward model $r_\phi$ \cite{mudgalcontrolled}. During decoding, the score for each candidate token $y_i$ is calculated according to Equation~\ref{eq:score}. We note that the input to $V_\theta$ includes the sequence $y_{1:i-1}$ with each candidate token $y_{i}$ appended to the sequence.  Hence to score each candidate token, we need to make a different call to the value function, resulting into $k$ calls for top-$k$ decoding.  This adds substantial overhead during decoding.

\subsection{Sub-Optimal Reward Models}

Next we take a look at contemporary RGTG reward models and show that they may prefer partial sequences with sub-optimal extensions. 

\paragraph{PARGS} \citet{rashid2024critical} showed that using a BT reward model trained on full sequences to score partial sequences (as done by \citet{khanov2023alignment}) can lead to arbitrary rewards for partial sequences. Rashid et al.~proposed to train a BT reward model explicitly on partial sequences by creating a separate loss function for all prefix lengths $i$:

\begin{equation}
  \mathcal{L}_R^i = - \sum_{(\rvx, \rvy^w, \rvy^l) \in \D} \log \sigma ( V_{\theta} (\rvy^w_{1:i}|\rvx) - V_{\theta} (\rvy^l_{1:i}|\rvx)) . \label{eq:partial-seq-objectives}
\end{equation}


However, given that full sequence $\rvy^w$ is preferred to full sequence $\rvy^l$, training is based on the assumption that the partial sequence $\rvy^w_{1:i}$ is also preferred to the partial sequence $\rvy^l_{1:i}$. This assumption can be  problematic as the full-sequence dataset typically includes only one or a few full sequences that extend each partial sequence.  In fact, the empirical distribution of such extensions will impact the learned value function to the extent where a prefix with only extensions to suboptimal full sequences may be scored higher than a prefix with an extension to an optimal full sequence. %In fact, Lemma 2 of \cite{} shows that the resulting partial sequence reward model depends on the preference data distribution.  Hence, different preference data distributions may yield different partial sequence reward models, which is problematic.  We show in the following theorem for some preference data distributions, the partial reward 

\begin{theorem}
\label{thm:pargs}
In the limit of infinite training and a sufficiently expressive representation for the value function, PARGS may learn a value function that gives a lower score to a prefix extendable to an optimal full sequence than some other prefix.  More precisely, if $\rvy^* = \argmax_{\rvy} r(\rvy|\rvx)$, then there may exist $i,j,\rvy'$ such that
\begin{equation}
V(\rvy^*_{1:i}|\rvx) < V(\rvy'_{1:j}|\rvx)
\end{equation}
\end{theorem}

\begin{proof}
Let $\rvy^*$, $\rvy'$, $\rvy''$ and $\rvy'''$ be four responses to $\rvx$ such that $\rvy^*$ is an optimal response and $\rvy'$, $\rvy''$, $\rvy'''$ are three suboptimal responses.  Suppose also that the preference dataset contains exactly three comparisons:  $\D=\{(\rvx,\rvy^*,\rvy'), (\rvx,\rvy',\rvy''), (\rvx,\rvy',\rvy''')\}$ where the first response is preferred to the second response in each triple.  Suppose also that $\rvy^*$ and $\rvy'$ share the first $i-1$ tokens (i.e., $\rvy^*_{1:i-1} = \rvy'_{1:i-1}$) while $\rvy^*$, $\rvy''$ and $\rvy'''$ share the first $i$ tokens (i.e., $\rvy^*_{1:i} = \rvy''_{1:i} = \rvy'''_{1:i}$). In the limit of infinite training and sufficiently expressive value function representation, Lemma 2 in \cite{rashid2024critical} indicates that the learned value function $V$ satisfies
\begin{equation}
\label{eq:pargs}
    \sigma(V(\rvy^1_{1:i}|\rvx) - V(\rvy^2_{1:j}|\rvx)) = P_D([\rvx,\rvy^1] \succ [\rvx,\rvy^2])
\end{equation}
where $[a,b]$ indicates the concatenation of sequences $a$ and $b$, and $a\succ b$ indicates that $a$ is preferred to $b$.  \cref{eq:pargs} implies that the BT model induced by $V$ exhibits the same preference probabilities for the full sequence extension of $\rvy^1_{1:i}$ and $\rvy^2_{1:j}$ as the empirical distribution of the preference dataset.  Recall, that PARGS assumes that $[\rvx,\rvy^1_{1:i}] \succ [\rvx,\rvy^2_{1:j}]$ when their respective full sequence extensions exhibit the same preference ordering (i.e., $[\rvx,\rvy^1] \succ [\rvx,\rvy^2]$).  Since their might be different extensions $\rvy^1_{i+1:|\rvy^1|}$ and $\rvy^2_{i+1:|\rvy^2|}$ for each prefix with different preference labels in the preference dataset, then PARGS learns a value function that induces preference probabilities for partial sequences that are consistent with the empirical distribution $P_D$ of the preference dataset for the full sequence extensions of those partial sequences.  Applying \cref{eq:pargs} to prefixes $\rvy^*_{1:i}$ and $\rvy'_{1:i}$ yields:
\begin{equation}
\label{eq:sigmoid}
    \sigma(V(\rvy^*_{1:i}|\rvx) - V(\rvy'_{1:i}|\rvx)) = 1/3
\end{equation}
since the dataset $\D$ contains one preference ranking $(\rvx,\rvy^*,\rvy')$ where the full sequence extension $\rvy^*$ of $\rvy^*_{1:i}$ is preferred to the full sequence extension $\rvy'$ of $\rvy'_{1:i}$ and two preference rankings $(\rvx,\rvy'\rvy'')$, $(\rvx,\rvy'\rvy''')$ where the full sequence extension $\rvy'$ of $\rvy'_{1:i}$ is preferred to the full sequence extensions $\rvy''$, $\rvy'''$ of $\rvy^*_{1:i}$. 
Recall that $\rvy^*_{1:i}=\rvy''_{1:i}=\rvy'''_{1:i}$ and therefore $\rvy''$ and $\rvy'''$ are full sequence extensions of $\rvy^*_{1:i}$. Finally, since the sigmoid in \cref{eq:sigmoid} is less than 0.5, then $V(\rvy^*_{1:i}) < V(\rvy'_{1:i})$.  Hence, this shows that $\exists i{=}j,\rvy'$ such that $V(\rvy^*_{1:i}) < V(\rvy'_{1:j})$
\end{proof}

Theorem~\ref{thm:pargs} shows that the value function learned by PARGS may prefer prefixes that lead to suboptimal responses.  The key problem is PARGS' assumption that the preference ordering of prefixes is the same as the preference ordering of full sequence extensions.  Since it is possible to extend a prefix to many different full sequences with different scores, the value function learned by PARGS depends on the frequency of different prefix extensions instead of preferences only.  As shown in the proof of Theorem~\ref{thm:pargs}, this becomes problematic when a prefix that can lead to an optimal response is extended more frequently to losing full sequences instead of winning full sequences in $\D$.

%\vspace{0.5em}
%\begin{theorem} \label{thm:PARGS}
%  \label{thm:full_for_partial}
%  Let \(r\) be a reward model trained to minimize the Bradley-Terry loss on partial sequences
%  $\rvy^{1:\abs{\rvy}}$ \eqref{eq:partial-seq-objectives} using full-sequence preference data $\rvy^{1:i}$.
%  Then \(r\) may prefer sub-optimal continuations at decoding.
  
%  \AK{This statement is vague. A better statement for a theorem would be to fully specify the hypothesis, e.g.: ``Let \(r\) be a reward model ... Let \(s^*\), ... Then, if \(r_\phi(s^*) > r_\phi(s^{**})\) and ..., the token \(y_i'\) is picked.''}
  
%  \AK{Then, discuss the significance/implication of this theorem after the proof. E.g. mention that \(y_i'\) is suboptimal and thus RGTG/PARGS is pathological.}
%\end{theorem}
%
%\begin{proof}
%    Let $r_\phi$ be the full sequence reward model, $\mathbf{V}_{\theta}$ be the partial sequence reward model and $\rvy^{(p)} = y_1, \cdots, y_{i-1}$ be a partial sequence following the prompt $\rvx$. Suppose that $\rvs^* = \rvx, \rvy^{(p)}, y^*_{i}, y^*_{i+1}, \cdots, y^*_{n}$ is the optimal full sequence extended from ${\rvx, \rvy^{(p)}}$. Let $\rvs^{**} = \rvx, \rvy^{(p)}, y^*_{i}, y^{**}_{i+1} \cdots, y^{**}_{n}$ be a sequence extended from ${\rvx, \rvy^{(p)}, y^*_{i}}$ and $\rvs' = \rvx, \rvy^{(p)}, y'_{i}, y'_{i+1} \cdots, y'_{n}$: be a sequence extended from ${\rvx, \rvy^{(p)}}$. Suppose that $\mathcal{P} = \{\rvs^{**}, \rvs'\}$ is a pair in the preference dataset. 

%    \AK{This hypothesis should be put in the theorem's statement. Here, you just say ``... By the hypothesis that $r_{\phi}$ ..., then ...''}
%    \AK{Also, avoid using a concrete numbers. Make your hypothesis general.}
%    Suppose $r_{\phi}(s^*)=6, \:r_{\phi}(s^{**})=-6, \:r_{\phi}(s')=5$, then $\rvs'$ is the winning sequence in $\mathcal{P}$. 
%    By the assumption that "the prefixes of a winning full sequence is also wining against the prefix of the corresponding losing full sequence", partial sequence $(\rvx, \rvy^{p}, y'_{i})$ is also winning $(\rvx, \rvy^{p}, y^*_{i})$.  Following the Bradley-Terry loss, we maximize
%$$
%\begin{aligned}
%     \log \sigma \left( \mathbf{V}_{\theta} (\rvy^{(p)}, y'_{i}|\rvx) - \mathbf{V}_{\theta} (\rvy^{(p)}, y^*_{i}|\rvx)\right)
%\end{aligned}
%$$
%That is, $\mathbf{V}_{\theta} (\rvy^{(p)}, y'_{i}|\rvx) - \mathbf{V}_{\theta} (\rvy^{(p)}, y^*_{i}|\rvx)$ is maximized results in $\mathbf{V}_{\theta} (\rvy^{(p)}, y'_{i}|\rvx) > \mathbf{V}_{\theta} (\rvy^{(p)}, y^*_{i}|\rvx)$

%Next, suppose $\piref(y^{*}|\rvx, \rvy^{(p)}) = \piref(y'|\rvx, \rvy^{(p)}) = \frac{1}{2}$, at inference time we follow the policy:
%$$ 
%\begin{aligned}
%&\pi(y_{i} \vert \rvx,\rvy^{(p)}) \propto \piref(y_{i} \vert \rvx,\rvy^{(p)}) \exp(\beta \mathbf{V}_{\theta}(\rvy^{(p)}, y_{i} \vert \rvx))\\
%\Rightarrow ~~ &\pi(y'_{i} \vert \rvx,\rvy^{(p)}) > \pi(y^{*}_{i} \vert \rvx,\rvy^{(p)})
%\end{aligned}
%$$

%Again, following $\pi$ would generate $y'_{i}$ which is sub-optimal.
%\end{proof}

%\AK{Discuss the implication of the theorem here.}

\paragraph{CD}~\citet{mudgalcontrolled} proposed a target value function $V^*$ for partial sequences that corresponds to the expected reward of the full sequences when the partial sequence is extended by following the base model distribution $\piref$.
%
\begin{equation}
% \label{eq:CD}
    V^*(\rvx, \rvy_{1:i}) = \sum_{\rvy_{i+1:|\rvy|}} \piref(\rvy_{i+1:|\rvy|}|\rvx,\rvy_{1:i}) r(\rvx, \rvy)
\end{equation}
%
The training loss is the squared difference between the value function $V_\theta$ and the target $V^*$. They use rollouts from the base model along with a reward model trained on full sequences to distill the value function $V_\theta$. They sample extensions from $\piref$ to complete a partial sequence and compute the full-sequence score as the target $V^*$. This method has a limitation where the value function heavily depends on the language model. We will show such dependency is suboptimal. 

Value Augmented Sampling~\citep[VAS][]{han2024value} is similar to CD and uses $\piref$ to generate samples for learning a value function and a full-sequence reward model for generating the target score. However, the value function is trained by temporal difference (TD) learning.
% \subsection{Value Augmented Sampling (VAS)}
% Similar to CD, VAS also proposed to use a base LLM to generate samples and hence provide a target score by the full-sequence reward model. Then, they applied temporal difference learning to update the parameters of the value function which they treat as the partial-sequence reward model in the decoding step.

% \subsection{Why LLM dependency is undesirable?}
% \label{LLMD}

\begin{theorem}
\label{thm:full_for_partial}
In the limit of infinite training and a sufficiently expressive representation for the value function, CD may learn a value function that gives a lower score to a prefix extendable to an optimal full sequence than some other prefix.  More precisely, if $\rvy^* = \argmax_{\rvy} r(\rvy|\rvx)$, then there may exist $i,j,\rvy'$ such that
\begin{equation}
V(\rvy^*_{1:i}|\rvx) < V(\rvy'_{1:j}|\rvx)
\end{equation}

%\AK{Here, the statement is better since it's more precise.}

\end{theorem}

\begin{proof}
Let $\rvy^*$ be an optimal response to $\rvx$ such that $r(\rvy^*|\rvx)=6$.  Let $\rvy'$ and $\rvy''$ be two suboptimal responses to $\rvx$ such that $r(\rvy'|\rvx)=4$ and $r(\rvy''|\rvx)=-6$.  Suppose that $\rvy'$ and $\rvy^*$ share the same first $i-1$ tokens (i.e., $\rvy'_{1:i-1} = \rvy^*_{1:i-1}$) and that $\rvy''$ and $\rvy*$ share the same first $i$ tokens (i.e., $\rvy'_{1:i} = \rvy^*_{1:i}$). After generating $\rvy^*_{1:i}$, suppose that $\piref$ generates only $\rvy^*_{i+1:|\rvy^*|}$ and $\rvy''_{i+1:|\rvy''|}$ with uniform probability (i.e., $\piref(\rvy^*_{i+1:|\rvy^*|}|\rvx,\rvy^*_{1:i})=\piref(\rvy''_{i+1:|\rvy''}|\rvx,\rvy''_{1:i})=0.5$ and any other continuation has probability 0). After generating $\rvy'_{1:i}$, suppose also that $\piref$ generates only $\rvy'_{i+1:|\rvy'|}$ (i.e., $\piref(\rvy''_{i+1:|\rvy''}|\rvx,\rvy''_{1:i})=1$ and any other continuation has probability 0).  Then with infinite training and a sufficiently expressive value function representation, CD learns the following partial sequence values
\begin{align}
V(\rvy^*_{1:i}|\rvx) & = \piref(\rvy^*_{i+1:|\rvy^*|}|\rvx,\rvy^*_{1:i})r(\rvy^*|\rvx) \\
& + \piref(\rvy''_{i+1:|\rvy''|}|\rvx,\rvy^*_{1:i})r(\rvy''|\rvx) \\
& = 0.5(6) + 0.5(-6) = 0 \\
V(\rvy'_{1:i}|\rvx) & = \piref(\rvy'_{i+1:|\rvy'|}|\rvx,\rvy'_{1:i})r(\rvy'|\rvx) \\
& = 1(4) = 4
\end{align}
This example shows that $\exists i{=}j,\rvy'$ such that $V(\rvy^*_{1:i}|\rvx) < V(\rvy'_{1:j}|\rvx)$.
\end{proof}

Theorem~\ref{thm:full_for_partial} shows that CD may prefer prefixes that cannot be extended to optimal sequences depending on $\piref$.  The key problem is the dependency of the target $V^*$ on $\piref$.  When $\piref$ extends a prefix to bad responses, the value of this prefix is low, but if it extends the prefix to good responses, the value of this prefix is high.  In principle, the value function $V$ should be independent of $\piref$.  In RLHF, $\piref$ is the quantity that we seek to improve so it does not make sense to improve $\piref$ with a value function that depends on $\piref$ itself.  The value function should depend only on the preferences induced by the full sequence reward model.  As shown in the proof of Theorem~\ref{thm:full_for_partial}, CD may not prefer a prefix that can lead to an optimal response when it is extended by $\piref$ to suboptimal responses.

%\begin{theorem} \label{thm:CD}
%  \label{thm:full_for_partial}
%  A reward model which is trained on continuations from $\piref$ according to \eqref{thm:CD} may lead to sub-optimal continuations at decoding.
%\end{theorem}

% In this section we will describe a counter-example showing that RGTG is suboptimal if the partial-sequence reward model is dependent on the LLM. We consider CD explicitly.\\

%\begin{proof}
% Let $r_\phi$ be the full sequence reward model, $\mathbf{V}_{\theta}$ be the partial sequence reward model and $\rvy^{(p)} = y_1, \cdots, y_{i-1}$ be a partial sequence following the prompt $\rvx$.

% Let $\mathcal{S} = \{\rvs^*, \rvs^{**}, \rvs', \rvs''\}$ be the set of all sequences where 


% \begin{itemize}
    % \item $\mathbf{V}_{\theta}$: partial-sequence reward model
    % \item $r_{\phi}$: full-sequence reward model
    % \item $\rvx$: prompt
    % \item $\rvy^{(p)} = y_1, \cdots, y_{i-1}$: partial-sequence following $\rvx$
%    \item $\rvs^* = \rvx, \rvy^{(p)}, y^*_{i}, y^*_{i+1}, \cdots, y^*_{n}$: the optimal full sequence extended from ${\rvx, \rvy^{(p)}}$
%    \item $\rvs^{**} = \rvx, \rvy^{(p)}, y^*_{i}, y^{**}_{i+1} \cdots, y^{**}_{n}$: another sequence extended from ${\rvx, \rvy^{(p)}, y^*_{i}}$
%    \item $\rvs' = \rvx, \rvy^{(p)}, y'_{i}, y'_{i+1} \cdots, y'_{n}$: another sequence extended from ${\rvx, \rvy^{(p)}}$
%    \item $\rvs'' = \rvx, \rvy^{(p)}, y'_{i},  y''_{i+1}, \cdots, y''_{n}$: another sequence extended from ${\rvx, \rvy^{(p)}, y'_{i}}$
    % \item $\mathcal{S} = \{\rvs^*, \rvs^{**}, \rvs', \rvs''\}$
%\end{itemize} 

%\AK{Same comment as before, can we make the following specific numbers more general?}

%\noindent Suppose $r_{\phi}(s^*)=6, \: \:r_{\phi}(s^{**})=-6,\: \:r_{\phi}(s')=5,\: \:r_{\phi}(s')=3\: \: $ and $\piref$ will only sample $y'$ or $y^{*}$ as the next token and sample sequences from $\mathcal{S}$ follow a uniform distribution, that is, 
%$$
%\begin{aligned}
%\piref(y^{*}|\rvx, \rvy^{(p)}) &= \piref(y'|\rvx, \rvy^{(p)}) = \frac{1}{2}\\
%\piref(\rvs'|\rvx, \rvy^{(p)}, y') &=\piref(\rvs''|\rvx, \rvy^{(p)}, y') = \frac{1}{2}\\
%\piref(\rvs^{*}|\rvx, \rvy^{(p)}, y^{*}) &=\piref(\rvs^{**}|\rvx, \rvy^{(p)}, y^{*}) = \frac{1}{2}\\
%\piref(\rvs|\rvx, \rvy^{(p)}) &= \frac{1}{4} \: , \:\: \forall \rvs \in \mathcal{S}
%\end{aligned}
%$$

%\noindent By following the training objective of CD, in the limit we have:
%$$
%\begin{aligned}
%    \mathbf{V}_{\theta}(\rvy^{(p)}, y_{i} | \rvx) = \sum_{\rvz \sim \piref} \piref(\rvz|\rvx, \rvy^{(p)}, y_{i}, ) r_{\phi}(\rvx, \rvy^{(p)}, y_{i}, \rvz)
%\end{aligned}
%$$
%\\
%Hence we get the following values for partial sequences:
%$$
%\begin{aligned}
%\mathbf{V}_{\theta}(\rvy^{(p)},  y^{*}_{i} | \rvx) &= \piref(\rvs^{*}|\rvx, \rvy^{(p)}) r_{\phi}(\rvs^{*}) \\ & ~~ + \piref(\rvs^{**}|\rvx, \rvy^{(p)}) r_{\phi}(\rvs^{**}) \\
%&= (\frac{1}{2}) (6) + (\frac{1}{2})(-6) = 0\\
%\mathbf{V}_{\theta}(\rvy^{(p)},  y'_{i} | \rvx) &= \piref(\rvs'|\rvx, \rvy^{(p)}) r_{\phi}(\rvs') \\ & ~~ + \piref(\rvs''|\rvx, \rvy^{(p)}) r_{\phi}(\rvs'') \\
%&= (\frac{1}{2}) (5) + (\frac{1}{2})(3) = 4
%\end{aligned}
%$$
%\\
%Next, consider inference where we follow the policy:\\
%$$ 
%\begin{aligned}
%&\pi(y_{i} \vert \rvx,\rvy^{(p)}) \propto \piref(y_{i} \vert \rvx,\rvy^{(p)}) \exp(\beta \mathbf{V}_{\theta}(\rvy^{(p)}, y_{i} \vert \rvx))\\
%\Rightarrow ~~  &\pi(y'_{i} \vert \rvx,\rvy^{(p)}) > \pi(y^{*}_{i} \vert \rvx,\rvy^{(p)})
%\end{aligned}
%$$
%\\
%Note following $\pi$ would generate $y'_{i}$ and deviate from the optimal sequence.

%\end{proof}

%\AK{Same as before. Discuss the implication/significane/practicality of this preceding theorem.}

% \subsection{Limitation of PARGS}
% We will use the same notations as above for $\mathbf{V}_{\theta}$, $r_{\phi}$, $\rvx$, $\rvy^{(p)}$ and $\rvs^*$, with new sequence examples:
%  \begin{itemize}
%     \item $\rvs^{**} = \rvx, \rvy^{(p)}, y^*_{i}, y^{**}_{i+1} \cdots, y^{**}_{n}$: another sequence extended from ${\rvx, \rvy^{(p)}, y^*_{i}}$
%     \item $\rvs' = \rvx, \rvy^{(p)}, y'_{i}, y'_{i+1} \cdots, y'_{n}$: another sequence extended from ${\rvx, \rvy^{(p)}}$
%     \item $\mathcal{P} = \{\rvs^{**}, \rvs'\}$ is a pair in the preference dataset 
% \end{itemize} 

% Suppose $r_{\phi}(s^*)=6, \:r_{\phi}(s^{**})=-6, \:r_{\phi}(s')=5$, then $\rvs'$ is the winning sequence in $\mathcal{P}$. By the assumption that "the prefixes of a winning full sequence is also wining against the prefix of the corresponding losing full sequence", partial sequence $(\rvx, \rvy^{p}, y'_{i})$ is also winning $(\rvx, \rvy^{p}, y^*_{i})$.  Following the Bradley-Terry loss, we maximize
% $$
% \begin{aligned}
%      \log \sigma \left( \mathbf{V}_{\theta} (\rvy^{(p)}, y'_{i}|\rvx) - \mathbf{V}_{\theta} (\rvy^{(p)}, y^*_{i}|\rvx)\right)
% \end{aligned}
% $$
% That is, $\mathbf{V}_{\theta} (\rvy^{(p)}, y'_{i}|\rvx) - \mathbf{V}_{\theta} (\rvy^{(p)}, y^*_{i}|\rvx)$ is maximized results in $\mathbf{V}_{\theta} (\rvy^{(p)}, y'_{i}|\rvx) > \mathbf{V}_{\theta} (\rvy^{(p)}, y^*_{i}|\rvx)$

% Next, suppose $\piref(y^{*}|\rvx, \rvy^{(p)}) = \piref(y'|\rvx, \rvy^{(p)}) = \frac{1}{2}$, at inference time we follow the policy:
% $$ 
% \begin{aligned}
% &\pi(y_{i} \vert \rvx,\rvy^{(p)}) \propto \piref(y_{i} \vert \rvx,\rvy^{(p)}) \exp(\beta \mathbf{V}_{\theta}(\rvy^{(p)}, y_{i} \vert \rvx))\\
% \Rightarrow ~~ &\pi(y'_{i} \vert \rvx,\rvy^{(p)}) > \pi(y^{*}_{i} \vert \rvx,\rvy^{(p)})
% \end{aligned}
% $$

% Again, following $\pi$ would generate $y'_{i}$ hence result in become suboptimal.