%!TEX root=icml2025.tex
We denote a prompt by $\mathbf{x}$ and its response by $\mathbf{y}$ where the bolded letters indicate sequences of tokens.
The $i$-th token in $\mathbf{x}$ is denoted by $x_i$, while the partial sequence starting at token $i$ and ending at token $j$ is denoted by $\mathbf{x}_{i:j}$.  
The length of a sequence $\mathbf{x}$ is denoted by $\abs{\mathbf{x}}$. 
Large language models (LLMs) generally consist of probabilistic models that can generate a response $\rvy$ given a prompt $\rvx$.
More specifically, the generation of $\rvy$ is done token-by-token by sampling the next token from a conditional distribution $\pi(y_{i} | \rvx,\rvy_{1:i-1})$.

\subsection{Reward Models}

Reward models are trained to evaluate the quality of a response $\mathbf{y}$ to the prompt $\mathbf{x}$ by outputting a scalar valued score.
Given a preference dataset \(\mathbf{D} = \{ (\mathbf{x}^k, \mathbf{y}^{wk}, \mathbf{y}^{lk}) \}_{k=1}^{K}\) containing \(K\) triples of token sequences \((\mathbf{x}, \mathbf{y}^w, \mathbf{y}^l)\) where $\mathbf{y}^w$ represents the "winning" (i.e., preferred) sequence and $\mathbf{y}^l$ represents the "losing" sequence. 
The Bradley-Terry (BT) loss \citep{bradleyterry1952paired} that encourages the model to assign a higher score to the winning response and a lower score to the losing response is used as the training objective:
%
\begin{equation} \label{eq:bradley-terry}
  \mathcal{L}_R = - \mathbb{E}_{\mathbf{x}, \mathbf{y}^w, \mathbf{y}^l \sim \mathbf{D}} \log \sigma ( r_{\phi} (\mathbf{y}^w|\mathbf{x}) - r_{\phi} (\mathbf{y}^l|\mathbf{x}))
\end{equation}
%
where \(\sigma\) is the logistic function and \(r_{\phi}\) is the reward model.

Reinforcement learning from human feedback~\citep[RLHF][]{ziegler2019rlhf, ouyang2022training} uses scores from the reward model to update the language model using reinforcement learning (RL) techniques such as proximal policy optimization (PPO). \citet{rafailov2023direct} derive an equivalent objective, which can learn using supervised learning without using a reward model. 

\subsection{Reward-Guided Text Generation}
\label{subsec:rgtg}

Recently, \citet{khanov2023alignment} proposed a reward-guided text generation (RGTG) technique that does not require an update of the LLM. 
Instead, the base LLM \(\piref\) is frozen and, during decoding, the logits from the LLM are combined with the reward scores to guide the text generation.  

Let $V_\theta(\rvy_{1:i}|\rvx)$ be a reward model with parameter \(\theta\) that scores partial sequences $\rvy_{1:i}$ such that $V_\theta(\rvy|\rvx)=r_\phi(\rvy|\rvx)$ for full sequences $\rvy$.  
During decoding, the adjusted score of token \(y_{i}\) is a weighted combination of logits of \(\piref\) and the value of the partial sequence as follows:
%
\begin{equation}
  \label{eq:score}
  \begin{aligned}
      \textstyle
      \score(y_{i} | \rvx, \rvy_{1:i-1}) = &\log \piref(y_{i} \vert \rvx, \rvy_{1:i-1}) \\
        &\quad + \beta V_\theta(\rvy_{1:i} | \rvx) ,
  \end{aligned}
\end{equation}
%
where $\beta > 0$ is a hyper-parameter. 

Given the scores, the next token can be selected greedily or by sampling (e.g., nucleus or top-k sampling~\cite{fan2018hierarchical, holtzmancurious} from the softmax distribution of the scores). 


In \eqref{eq:score}, while $V$ corresponds to $r$ for full sequences, further considerations are needed to define $V$ for partial sequences. \citet{rashid2024critical} showed that using full-sequence reward models to score partial sequences can lead to arbitrary rewards during RGTG.
