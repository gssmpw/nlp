%!TEX root=icml2025.tex

% Large language models (LLMs) have become the foundation of most text generation tools\citep{brown2020gpt3, touvron2023llama2}. An important step to ensure high-quality text generation is aligning LLMs with human preferences which is typically achieved by reinforcement learning from human feedback(RLHF) \citep{ouyang2022training}. A popular algorithm to perform RLHF is PPO, where a reward model is first trained on the preference dataset and then used in the reinforcement learning to maximize expected reward\citep{schulman2017proximal}. However, PPO is often criticized for its high cost and instability. On the other hand, \citet{rafailov2023direct} proposed direct preference optimization (DPO) which showed that the reward model training can be bypassed by directly fine-tuning the LLM on the preference dataset.

% \citet{khanov2023alignment,deng2023reward} explored a parallel line of work called tokenwise reward-guided text generation (RGTG) that bypasses the step of finetuning the LLM while preseving training a reward model. Then the LLM and reward models are combined explicitly to adjust the scores of tokens hence achieve a similar generation policy as RLHF.

% While RGTG is an interesting alternative to the standard offline RLHF, how to train the reward model kept as a challenging question to the community.
% There have been many methods proposed such as ARGS \citep{khanov2023alignment}, PARGS and Controlled decoding \citep{mudgalcontrolled}. However, we found all those methods either making strong assumptions or exhibiting limitations that lead to suboptimal performance.

\begin{figure*}[t]
  \begin{center}
    \includegraphics[width=1\textwidth]{figures/ICML25-RGTG.pdf}    
  \end{center}

  \vspace{-2em}
  \caption{
    Figure depicting a step in RGTG generation for both conventional (left) reward models and ours (right). 
    Note that RGTG steers the LLM generation to helpful and harmless text. 
    We observe on the left that for each candidate that is generated by the LLM, a call needs to be made to the reward model with the candidate appended. On the other hand our reward model is fed just the prompt and it generates scores for all candidates in the vocabulary %\AK{vocabulary. Dictionary is a map \(\mathcal{V}_1 \to \mathcal{V}_2\) between vocabularies.}. 
    On the TLDR data set we observe an average generation time of 19.4 seconds for current RGTG and 2.99 seconds for our method.
  }
  \label{fig:method}
  \vspace{-1em}
\end{figure*}

Reinforcement learning from human feedback \citep[RLHF][]{stiennon2020rlhf,ouyang2022training} is widely applied to align large language models (LLMs) to human preferences. 
However, updating the LLM with RLHF incurs a significant training cost, whether it is reinforcement learning using proximal policy optimization \citep[PPO,][]{schulman2017proximal} or finetuning using direct preference optimization \citep[DPO][]{rafailov2023direct}. 
The training costs can be prohibitive as we scale the LLM since high-performance computational resources with large GPUs are required. 
Moreover, the LLM needs to be re-trained whenever the preference data changes.

One way to alleviate this computational overhead, while still improving the alignment of the baseline LLM is token-wise reward-guided text generation \citep[RGTG][]{khanov2023alignment,deng2023reward}.
RGTG methods keep the baseline LLM frozen and instead train a reward model on the preference data. 
At each decoding step, the reward model is used to adjust the softmax scores of each candidate token. 
Reward models are cheaper to train compared to offline RLHF updates (e.g., PPO and DPO) even if both the reward model and LLM have the same number of parameters.\footnote{To train $\pi$, PPO needs to load and make calls to 2 additional models ($\piref$ and a critic), DPO needs to load and make calls to one additional model ($\piref$) while training a reward model does not require loading or calling any additional model.} 
Moreover, RGTG with a small reward model can perform comparably to RLHF~\citep{rashid2024critical}.   

However, while RGTG is a promising and cost-effective alternative to offline RLHF, it can lead to a significant decoding overhead during inference. 
Typically, at each step, multiple calls are made to the reward model for candidate tokens from the language model.
This introduces an ``inner loop'' in the decoding process of the LLM, leading to an increase in computational complexity and latency.
Another issue is that reward models trained on full-sequences are used to score partial sequences~\citep{khanov2023alignment,li2024cascade} which can be problematic~\citep{rashid2024critical}. 

Several works have attempted to train reward models to score partial sequences. 
\citet{deng2023reward} use a squared loss and the preference data to distill a full sequence reward model into a reward model for partial sequences. 
Controlled decoding \citep[CD;][]{mudgalcontrolled} uses roll-outs from the language model instead of the preference data to distill a partial sequence reward model. 
\citet{rashid2024critical} explicitly train a Bradley-Terry model on partial sequences and demonstrate a connection with RLHF. We will show that these methods prefer sub-optimal extensions of partial sequences during decoding. 

To alleviate these common issues associated with RGTG methods, we propose a cost-effective reward model architecture for RGTG, which can score all possible next token extensions of a partial sequence with a single call. Furthermore, we train the reward model using a novel loss function that, we show, scores prefixes that can be extended to optimal full sequences, at least as high as any other prefix. On extensive benchmarks we demonstrate that our reward model leads to a better cost-performance trade-off and higher diversity. Figure~\ref{fig:method} illustrates the shortcoming of current reward models, i.e. high decoding cost and our proposed solution.

% \AK{Need one or two more paragraph. The formula for the intro is: General landscape - current approaches - issues - our proposal - teaser on what follow - summary. You missed "our proposal" and "teaser".}

% \AK{Example for proposal: ``To this end, we propose \dots''}

% \AK{Example for teaser: ``In an extensive benchmark, we find that our approach provides a favorable cost-performance tradeoff \dots''}


In summary:
%
\begin{itemize}
    \item We analyze contemporary reward models and demonstrate that during RGTG they choose sub-optimal extensions of partial sequences.
    \item We present a reward model architecture which, at each decoding step, can provide all rewards of all tokens in the vocabulary at once.
    \item We explicitly train our reward model to choose token with the maximum reward at each step.
    \item We report extensive experiments with recent LLMs on various text generation tasks, demonstrating faster inference and strong alignment performance.
\end{itemize}



