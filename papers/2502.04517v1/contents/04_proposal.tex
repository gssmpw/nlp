%!TEX root=icml2025.tex

We propose to mitigate the inference overhead and sub-optimal rewards of previous RGTG methods by introducing (i) an efficient reward model and (ii) a novel loss function that will ensure that the resulting value function prefers prefixes extendable to optimal responses.  We name our method FaRMA, i.e. Faster Reward Model for Alignment.

\subsection{An Efficient Reward Model}

We design a reward model architecture so that instead of obtaining a single score for a sequence, we obtain the score for all possible next tokens in the dictionary. We modify \eqref{eq:score} such that:

\begin{equation}
  \label{eq:score-new}
  \begin{aligned}
  \textstyle
  \score(y_{i} | \mathbf{x}, \mathbf{y}_{1:i-1}) = &\log \piref(y_{i} \vert \mathbf{x}, \mathbf{y}_{1:i-1}) \\ 
    &\quad + \beta V_\theta(y_{i} \vert \mathbf{x}, \mathbf{y}_{1:i-1}) ,
  \end{aligned}
\end{equation}

where $V_\theta(.) \in R^{|D|\times 1}$ and $|D|$ is the size of the vocabulary.
% \AK{Vocabulary.}
In order to get the score of sequence $x,y_{1:i}$ we feed the $x,y_{1:i-1}$ into $r_\phi$ and get the score of the sequence with all possible extensions of $y_i$ in the dictionary. 
The efficiency and performance of the reward model is not dependent on $k$, for top-k generation, as we simultaneously get the score for all possible next tokens in the dictionary. We use the same architecture as a causal language model, however, we use a novel training loss which we discuss next. 

% \AK{Need more elaboration regarding the architecture. Is it the same as in the base LLM? Or is it different? If the latter, in which ways? A schematic here will be useful.}

% \AK{Avoid one-sentence paragraphs.}

% Next we discuss the loss that we use for training the reward model.

\subsection{A Principled Constraint}

Given the sub-optimality of the existing methods, there needs to be a more principled way to score partial sequences. 
We propose to score partial sequences based on their optimal extension: 
%\AK{The notations seem to be different than from Sec. 2.}
Given a partial sequence $\rvy_{1:i}$, we consider all possible full extensions and assign the score of the highest completion to $\rvy_{1:i}$. 
Na\"{i}vely, this would require an exponential search in terms of the size of the vocabulary which is intractable. To make this principled goal feasible, we propose a local constraint that the partial-sequence reward model needs to satisfy so that it will return the reward of the corresponding optimal expansion: 
\begin{equation} \label{eq:constraint}
    \mathbf{V}_{\theta}(y_{1:i}|\rvx) = \max_{y_{i+1}} \: \mathbf{V}_{\theta}(y_{1:i+1}|\rvx)
\end{equation}
If the above local constraint is satisfied, then we can keep expanding the sequence as in the generation:
$$
\begin{aligned}
\mathbf{V}_{\theta}(y_{1:i}|\rvx) &= \max_{y_{i+1}} \: \mathbf{V}_{\theta}(y_{1:i+1}|\rvx) \\
&= \max_{y_{i+1}} \: \max_{y_{i+2}}\: \mathbf{V}_{\theta}(y_{1:i+2}|\rvx) \\
&= \cdots = \max_{y_{i+1:n}}\mathbf{V}_{\theta}(y_{1:n}|\rvx),
\end{aligned}
$$
where $y_{i+1:n}$ is the optimal extension beyond $y_{1:i}$ and $y_n$ is the EOS token. That is, instead of doing an exponential search, we could train the value function to satisfy \eqref{eq:constraint}, which can be done by Temporal Difference (TD) learning. Note that VAS also uses TD learning in their algorithm, but, since they use a conventional reward model they do not do a max over the dictionary.

To be more precise, the training process can be separated into two steps with distinct objectives:
\begin{enumerate}
    \item Standard BT loss on full sequence preference dataset:
        \begin{equation} \label{eq:bradley-terry-new}
        \mathcal{L}_{(a)} = - \E_{\rvx, \rvy^w, \rvy^l \sim \D} \log \sigma ( \mathbf{V}_{\theta} (\rvy^w|\rvx) - \mathbf{V}_{\theta} (\rvy^l|\rvx))
        \end{equation}
    \item Constraint to ensure optimal partial sequence expansion.
        \begin{equation} \label{eq:constraint_loss}
            \mathcal{L}_{(b)} = \frac{1}{2}\left[\mathbf{V}_{\theta}(y_{1:i}|\rvx) - \max_{y_{i+1}} \:\mathbf{V}_{\theta}(y_{1:i+1}|\rvx)\right]^2       
        \end{equation}
\end{enumerate}

Firstly, we want to point out the similarity of our constraint \eqref{eq:constraint_loss} to TD control where $V(\rvy|\rvx)$ can be treated as a state-action value function (i.e., Q-function) with $y_i$ corresponding to the action and $[\rvx,\rvy_{1:i-1}]$ corresponding to the state. Note also that transitions are deterministic in LLMs since the action $y_i$ updates the state to $[\rvx,\rvy_{1:i}]$ deterministically.  We use $s$ to denote a state and $a$ to denote an action in Bellman's equation:
    $$
    \begin{aligned}
        &Q^{*}(s,a) = \mathbb{E}[r|s,a] + \gamma \sum_{s'}\mathbb{P}(s'|s,a) \max_{a'} Q^{*}(s', a')\\
        \Rightarrow \: &Q^{*}([\rvx,\rvy_{1:i-1}], y_i) = \max_{y_{i+1}} Q^{*}([\rvx,\rvy_{1:i}], y_{i+1}) \label{eq:bellman}\\
        \Rightarrow \: &\mathbf{V}_{\theta}(\rvy_{1:i}|\rvx) = \max_{y_{i+1}} \: \mathbf{V}_{\theta}(\rvy_{1:i+1}|\rvx)
    \end{aligned}
    $$

Note that \cref{eq:bellman} follows from the fact that there is no discount factor and no reward until the end of the sequence.
Then $\mathcal{L}_{(b)}$ is the same loss as in Q-gradient learning by treating $\max\limits_{y_{i+1}} \:\mathbf{V}_{\theta}(y_{1:i+1}|\rvx)$ as the target. 

To train the value function, we alternate between the two losses mentioned previously. For the Bradley-Terry loss \eqref{eq:bradley-terry-new}, we utilize full-sequence preference pairs as commonly done when training a reward model. Furthermore, for the new constraint loss \eqref{eq:constraint_loss}, we extract partial sequences from the winning sequences in the preference dataset and use them as the training data. Notably, there is no preference signal when training with the constraint loss. The model simply learns to align its scores to the best next token. We trained the model by alternating between the two losses. The training details are presented in Appendix \ref{app:training}.

%Therefore, we need to train the constraint in an iterative backward manner: 
%we start to train on partial sequences that are derived by removing the last token of full sequences and use optimal full sequence reward score as the target. Then, we remove the last two tokens of full sequences while using the optimal score computed from the partial sequences derived by removing the last token of full sequences as the target.  For instance, we have a full sequence $(\rvx, y_{1:n})$ from the dataset. In the first iteration, we would train on the partial sequence $(\rvx, y_{1:n-1})$ and use $\max\limits_{y_n} \mathbf{V}_{\theta}(\rvx, y_{1:n})$ as the target. Then, in the second iteration, we train on partial sequence $(\rvx, y_{1:n-2})$ and use $\max\limits_{y_{n-1}} \mathbf{V}_{\theta}(\rvx, y_{1:n-1})$ as the target, as at this iteration the reward model could provide a reliable score for $\mathbf{V}_{\theta}(\rvx, y_{1:n-1})$. We keep iterating by reducing the length of the partial sequence. Also, the model is frozen when it is computing the target and updated at the end of each iteration.

% Since this technique is similar to Q-gradient learning which is a well-studied training paradigm, relevant training tricks such as alternative training and rebuffer play can be applied to optimize the performance.

We emphasize that this kind of training would \emph{not} be possible with the reward models of previous RGTG methods that require $|D|$ forward passes to calculate the max over all the tokens in the dictionary $D$. 
Instead we calculate the max after a single forward pass. The complete algorithm for our method is presented in Alg.~\ref{alg:rgtg_train}.


\begin{algorithm}[t]
  \small
  \caption{Our Training Algorithm.} %\AK{Always use \texttt{text} for words, e.g., loss, max, iter etc. in math mode.}
  \label{alg:rgtg_train}

  \begin{algorithmic}[1]
    \REQUIRE Base LLM to initialize the reward model $V_\theta$, Full Sequence Preference dataset \(\mathbf{D_{BT}} = \{ (\mathbf{x}^k, \mathbf{y}^{wk}, \mathbf{y}^{lk}) \}_{k=1}^{K_{BT}}\), number of alternating iterations $\text{iter}_n$, mini-batch size $n$,  partial sequence dataset \(\mathbf{D_{max}} = \{ (\mathbf{x}^k, \mathbf{y}^{k}\}_{k=1}^{K_\text{max}}\)
    \ENSURE $\mathbf{V}_{\theta}$
    \vspace{1em}
    
    \FOR{$i$ = 1 to $\text{iter}_n$}
    \STATE Sample minibatch $\mathbf{D_{BT}^{(i)}}$ from $\mathbf{D_{BT}}$ of size $n$
        \FOR{every tuple $(\rvx, \rvy^w, \rvy^l)\in \mathbf{D_{BT}^{(i)}}$}
        \STATE Compute $\mathbf{V}_{\theta} (\rvy^w|\rvx)$ and $\mathbf{V}_{\theta} (\rvy^l|\rvx)$
        \STATE $\mathcal{L}_a = \log \sigma ( \mathbf{V}_{\theta} (\rvy^w|\rvx) - \mathbf{V}_{\theta}(\rvy^l|\rvx))$
        \STATE Update $\mathbf{V}_{\theta}$ based on loss $\mathcal{L}_a$ 
        \ENDFOR
    \STATE Sample minibatch $\mathbf{D_{max}^{(i)}}$ from $\mathbf{D_{max}}$ of size $n$
        \FOR{every tuple $(\rvx, \rvy)\in \mathbf{D_{max}^{(i)}}$}
        \STATE Compute $\mathbf{V}_{\theta} (\rvy|\rvx)$
        \STATE $V_{max} = \max_{y_{|\rvy| + 1}} \:\mathbf{V}_{\theta}(\rvy, y_{|\rvy| + 1}|\rvx)$
        \STATE $\mathcal{L}_b = \frac{1}{2}\left[\mathbf{V}_{\theta} (\rvy|\rvx) - V_\text{max}\right]^2$
        \STATE Update $\mathbf{V}_{\theta}$ based on loss $\mathcal{L}_b$
        \ENDFOR
    \ENDFOR
    % \STATE   $\mathbf{V}_{\theta}$
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
    \small
    \caption{Our Decoding Algorithm.}
    \label{alg:rgtg_decode}

    \begin{algorithmic}[1]
        \REQUIRE Reward model $\mathbf{V}_{\theta}$, Prompt $\rvx$, top-k parameter $k$, hyperparameter $\beta > 0$, any reference/SFT model $\piref$, generation length $l$
        \ENSURE $\rvy_{1:l}$: A generated response to $\rvx$ of length $l$

        \vspace{1em}
        
        \FOR{i = 1 to $l$}
            %\State Reward $\mathbf{V}_{\theta}(v \vert \rvx, \rvy_{1:i-1})$
            %\State Logit $\piref(v \vert \rvx, \rvy_{1:i-1})$
            \STATE $\log \pi(y_i=v \vert \rvx, \rvy_{1:i-1}) \leftarrow$
            \STATE \qquad\qquad $\log \left(\piref(v \vert \rvx, \rvy_{1:i-1}) + \beta \mathbf{V}_{\theta}(v \vert \rvx, \rvy_{1:i-1})\right)$
            \STATE $y_{i} \sim \softmax(\mathtt{top\_k}(\log \pi(y_i \vert \rvx, \rvy_{1:i-1})))$
        \ENDFOR  
        % \STATE  $\rvy_{1:l}$
    \end{algorithmic}
\end{algorithm}

We now prove that unlike PARGS and CD, our algorithm is guaranteed to prefer prefixes that are extendable to optimal full sequences.


\begin{theorem}
In the limit of infinite training data and a sufficiently expressive representation for the value function, our algorithm guarantees that the learned value function scores prefixes that can be extended to optimal full sequences at least as high as any other prefix.  More precisely, if $\rvy^* = \argmax_{\rvy} r(\rvy|\rvx)$, then 
\begin{equation}
V(\rvy^*_{1:i}|\rvx) \ge V(\rvy'_{1:j}|\rvx) \; \forall i,j,\rvy'
\end{equation}
\end{theorem}

\begin{proof}
We provide a proof by contradiction.  Let $\rvy^*$ be an optimal response to $\rvx$ and $\rvy'$ be any other response.  Suppose that
\begin{equation}
    \exists i,j,\rvy' \mbox{ such that } V(\rvy'_{1:j}|\rvx) > V(\rvy^*_{1:i}|\rvx) \label{eq:hypothesis}
\end{equation}  
Since the loss in \eqref{eq:constraint_loss} ensures that the learned value function returns the reward of the best full sequence that extends a prefix then $V(\rvy^*_{1:i}|\rvx) = r(\rvy^*|\rvx)$.  Similarly, since $\rvy'_{1:j}$ is any other prefix whose extensions do not lead to better full sequences, then $V(\rvy'_{1:j}|\rvx) <= r(\rvy^*|\rvx)$.  This means that $V(\rvy'_{1:j}|\rvx) \le V(\rvy^*_{1:i}|\rvx)$, which contradicts \eqref{eq:hypothesis}.
\end{proof}


