%!TEX root=icml2025.tex

%!TEX root=example_paper.tex

We evaluate our proposed approach on three language generation tasks: summarization, dialogue generation and fine-grained text generation. 

Our baselines include $\piref$ which is top-\(k\) sampling~\cite{fan2018hierarchical}, RLHF models based on PPO~\cite{schulman2017proximal} and DPO~\cite{rafailov2023direct}, RGTG methods ARGS~\cite{khanov2023alignment}, CD~\cite{mudgalcontrolled}, PARGS~\cite{rashid2024critical} and CARDS~\cite{li2024cascade}. CARDS demonstrated a higher reward and lower inference cost compared to Best-of-N so we did not evaluate Best-of-N. Note that we use the average-reward obtained by the DPO baseline as the reward threshold for CARDS. Setting a higher threshold could lead to better rewards at the cost of significantly longer decoding times (see Appendix \ref{tab:CARDS}). 

\subsection{Setup}
\label{subsec:setup}
\textbf{Summarization task}\quad We pick the Reddit TL;DR ~\cite{volske2017tl} as the dataset for the summarization task. Each sample consists of the prompt $x$ which is a post on the Reddit forum and the labels $y$, the summary of the post.
We use the human preference dataset from \citet{stiennon2020learning} to perform all the training and decoding.
Our base summarization model is the llama3.2-1B-Instruct model\footnote{\href{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}{meta-llama/Llama-3.2-1B-Instruct}}. Our reward model is also initialized from the same LLM.

\textbf{Dialogue task}\quad Next we evaluate our method on a dialogue task using the Anthropic Helpful and Harmless (HH) \cite{bai2022training} dataset, which helps to align the LLM to generate helpful and harmless responses. Each sample provides a prompt $\rvx$ and two responses $\rvy$ with a label indicating the preferred response. Here, the prompt $\rvx$ is the history of the dialogue and $\rvy$ is the response from the assistant.
We use a pretrained SFT Pythia-2.8B base model\footnote{\href{https://huggingface.co/lomahony/eleuther-pythia2.8b-hh-sft}{lomahony/eleuther-pythia2.8b-hh-sft}} and trained a full-sequence reward model based on it. We also trained a smaller reward model that is half-size of the base model (1.4B).

\textbf{Evaluation}\quad Following \cite{khanov2023alignment} we compare the algorithms based on \emph{average reward} on the test samples as measured by the reward model.
A higher reward indicates better alignment with human preferences.
Note that we use a \emph{different} full-sequence reward model and not the FaRMA reward model (that we trained for our algorithm) to evaluate the models.
Moreover, evaluating language generation is nuanced, and human evaluation is generally preferred, but is time consuming. An alternative is LLM based evaluation, which has been shown to align with human assessment~\cite{zheng2023judging,rafailov2023direct}. We adopt GPT-4 based evaluation as a proxy of human evaluation. Following \cite{chiang2023vicuna} we construct prompts for the two tasks and ask GPT-4 to score and rank response pairs. We randomly shuffle the order of the responses to mitigate position bias~\cite{zheng2023judging}.

We also evaluate the diversity of generation on the TL;DR and HH-RLHF datasets. To evaluate generation diversity, we generate \(10\) responses for each prompt, and measure the Rouge-L score between each generated pair. A lower Rouge-L score indicates a higher diversity.

Training details, including hyper-parameters are presented in Appendix~\ref{app:training}.

\textbf{Fine-Grained Text Generation} We have additional results on text generation on the Ultra-Feedback (UF) dataset \citep{cui2024ultrafeedback} in Appendix \ref{app:UF}.
We use a pretrained SFT Zephyr-7B base model\footnote{\href{https://huggingface.co/alignment-handbook/zephyr-7b-sft-full}{alignment-handbook/zephyr-7b-sft-full}} and trained a full-sequence reward model based on it.

\subsection{Results}
\label{sec:results}

\cref{tab:numberOfCalls} shows the average number of calls made to the LLM and RM by RGTG methods to generate a single response. FaRMA is clearly the best as it makes the least number of calls compared to all baselines. Note that CARDS makes fewer calls to the reward model, since the RM is not called for each token, but makes  $> 4\times$ more calls to the lanugage models compared to FaRMA.   

\begin{table}[ht]
    \centering
    \footnotesize
    \begin{tabular}{l l c c c }
        \toprule
        \textbf{Data} & \textbf{Method} & \textbf{LLM Calls} & \textbf{RM Calls} & \textbf{Total Calls}\\
        \midrule
        \multirow{5}{*}{TLDR} & ARGS & 59.69 & 596.90 &  656.69\\
        & PARGS & 58.39 & 583.90 & 642.29\\
        & CD & 60.26 & 602.60 & 662.86\\
        & FaRMA & 53.27 & 53.27 & 106.54\\
        & CARDS & 305.31 & 32.80 & 338.11\\
        \midrule
        \multirow{5}{*}{HH} & ARGS & 71.85 & 718.50 & 790.35 \\
        & PARGS & 76.86 & 768.60 & 845.46\\
        & CD & 63.48 & 634.80 & 698.28\\
        & FaRMA & 90.08 & 90.08 & 180.16 \\
        & CARDS & 395.94 & 42.25 & 438.19 \\
        \bottomrule
    \end{tabular}
    \caption{Avg. Number of Model calls made by RGTG methods when responding to a query. FaRMA makes the fewest calls.}
    \label{tab:numberOfCalls}
\end{table}

\cref{tab:TLDR} shows the average reward measured by the full-sequence reward model for the summarization task. FaRMA achieves the best average reward and uses significantly less time compared to all the other RGTG techniques. Moreover, we achieve a higher average reward compared to CARDS which incurs some overhead due to more calls to the LLM. FaRMA is also competitive with DPO and PPO based RLHF that is expensive to fine-tune.

\begin{table}[ht]
  \centering
  \footnotesize
  \begin{tabular}{cccc}
    \toprule
    \multicolumn{4}{c}{\textbf{TL;DR Summarization}}                    \\
    \midrule
    \textbf{Method} & \textbf{LLM}  & $r \pm \text{SE}$  & \textbf{Time(min)}    \\
    \midrule
    $\piref$       & frozen         & 0.98$\pm$0.18    & 2     \\
    \midrule
    ARGS            & frozen        & 1.46$\pm$0.16   & 32       \\
    PARGS       & frozen         & 1.56$\pm$0.19    & 31     \\
    CD       & frozen         & 1.15$\pm$0.16    & 29     \\
    FaRMA     & frozen     & 2.05$\pm$0.15  &  5        \\
    CARDS             & frozen  & 1.73$\pm$0.16 & 17       \\
    \midrule
    DPO             & trained    & 2.08$\pm$0.18  & 2        \\
    PPO             & trained   & 2.05$\pm$0.14  & 2   \\
    \bottomrule
  \end{tabular}
  \caption{Avg. reward (over 100 samples) $\pm$ standard error and total generation time for the TL;DR summarization task.}
  \label{tab:TLDR}
   \vspace{-1em}
\end{table}


Similarly, \cref{tab:HH} shows average reward of the dialogue task. We observe that FaRMA performs the best in terms of both average reward and inference time among all the RGTG methods, and is competitive with DPO, PPO and CARDS. Note we also trained a reward model using our method that is only half the size of the LLM. The result demonstrates that we can further reduces the cost for both training and inference by reducing the reward model size while still improving over $\piref$.

\begin{table}[ht]
  \centering
  \footnotesize
  \begin{tabular}{cccc}
    \toprule
    \multicolumn{4}{c}{\textbf{HH Dialogue}}                                    \\
    \midrule
    \textbf{Method} & \textbf{LLM}  & $r \pm \text{SE}$ & \textbf{Time(min)}     \\
    \midrule
    $\piref$       & frozen    & 1.18$\pm$0.12 & 2        \\
    \midrule
    ARGS            & frozen   & 1.41$\pm$0.18   & 26       \\
    PARGS       & frozen         & 1.63$\pm$0.17    & 31     \\
    CD       & frozen         & 1.24$\pm$0.13    & 27     \\
    FaRMA \emph{(same size)}  & frozen  & 1.80$\pm$0.18 & 5     \\
    FaRMA \emph{(half size)}   & frozen   & 1.41$\pm$0.16 & 3      \\
    CARDS  & frozen   & 1.92$\pm$0.19   & 20    \\
    \midrule
    DPO   & trained       & 1.73$\pm$0.17    & 2     \\
    PPO     & trained   & 1.92$\pm$0.22  & 2   \\
    \bottomrule
  \end{tabular}
  \caption{Avg. reward (over 50 samples) $\pm$ standard error and total generation time for the HH dialogue task.}
  \label{tab:HH}
   \vspace{-1em}
\end{table}

\cref{tab:diveristy} shows shows the average Rouge-L of different generations from the same prompt. A lower score demonstrates better diversity and we observe that FaRMA generates the most diverse responses compared to $\piref$, DPO, PPO and CARDS.

% \begin{table}[ht]
%   \centering
%   \footnotesize
%   \begin{tabular}{ccc}
%     \toprule
%     \textbf{Method} & \textbf{ROUGE-L \(\downarrow\)} \\
%     \midrule
%     $\piref$       & 0.29 $\pm$ 0.01             \\
%     DPO             & 0.34 $\pm$ 0.02             \\
%     DPO             & 0.42 $\pm$ 0.02            \\
%     \emph{FaRMA}          & 0.24 $\pm$ 0.01               \\
%     CARDS           & 0.86 $\pm$ 0.01               \\
%     \bottomrule
%   \end{tabular}
%   \caption{Diversity of TLDR dataset based on ROUGE-L}
%   \label{tab:diveristy_TLDR}
%    \vspace{-1em}
% \end{table}

\begin{table}[ht]
  \centering
  \footnotesize
  \begin{tabular}{cc}
    \toprule
    \textbf{Method} & \textbf{ROUGE-L \(\downarrow\)} \\
    \midrule
    \multicolumn{2}{c}{\textbf{TL;DR Summarization}} \\
    \midrule
    $\piref$       & 0.20 $\pm$ 0.01             \\
    DPO             & 0.21 $\pm$ 0.01             \\
    PPO             & 0.20 $\pm$ 0.01            \\
    FaRMA          & 0.21 $\pm$ 0.02               \\
    CARDS           & 0.49 $\pm$ 0.07               \\
    % \midrule

    \bottomrule
  \end{tabular}
    \hfill                
    \begin{tabular}{cc}
        \toprule
    \textbf{Method} & \textbf{ROUGE-L \(\downarrow\)} \\
    \midrule
    \multicolumn{2}{c}{\textbf{HH Dialogue}}      \\
    \midrule
    $\piref$       & 0.29 $\pm$ 0.01             \\
    DPO             & 0.34 $\pm$ 0.02             \\
    PPO             & 0.42 $\pm$ 0.02            \\
    FaRMA       & 0.24 $\pm$ 0.01               \\
    CARDS           & 0.86 $\pm$ 0.01               \\
    \bottomrule
  \end{tabular}
  \caption{Diversity score based on ROUGE-L}
  \label{tab:diveristy}
   \vspace{-1em}
\end{table}


Next we plot the GPT-4 winning rate of baselines versus FaRMA, against the inference time. Figure~\ref{fig:gpt4} shows the results on both the TLDR dataset and Anthropic HH. The best methods should be in the top left quadrant demonstrating both faster inference and higher win-rates. We observe that FaRMA has a competitive winning rate at a much faster inference speed compared to RGTG methods. DPO and PPO have favorable performance but they are more expensive to train (Appendix~\ref{app:training_time}). Both plots demonstrate a similar trend. The prompts used to probe GPT-4, for the two datasets are presented in Appendix~\ref{app:gpt-4}.




% \begin{table*}[ht]
%   \centering
%   \footnotesize
%   \begin{tabular}[t]{cccc}
%     \toprule
%     \multicolumn{4}{c}{\textbf{TL;DR Summarization}}                              \\
%     \midrule
%     \textbf{Method A} & \textbf{vs} & \textbf{Method B} & \textbf{Win-Tie ($\%$)} \\
%     \midrule
%     FaRMA         &       & ARGS                & 47        \\
%     FaRMA         &       & CD             & 44       \\
%     FaRMA         &       & CARDS             & 53        \\
%     FaRMA         &       & DPO              & 40  \\
%     FaRMA         &       & PPO               & 38  \\
%     \bottomrule
%   \end{tabular}
% \caption{GPT4 evaluation of TL;DR dataset}
%   \label{tab:GPT4_TLDR}
% \end{table*}

% \begin{table*}[ht]
%   \centering
%   \footnotesize
%   \begin{tabular}[t]{cccc}
%     \toprule
%     \multicolumn{4}{c}{\textbf{HH-RLHF}}                              \\
%     \midrule
%     \textbf{Method A} & \textbf{vs} & \textbf{Method B} & \textbf{Win-Tie ($\%$)} \\
%     \midrule
%     FaRMA         &       & ARGS                & 54       \\
%     FaRMA         &       & CARDS               & 58      \\
%     FaRMA         &       & CD               & 40      \\
%     FaRMA         &       & DPO                 & 42      \\
%     FaRMA         &       & PPO                 & 46       \\
%     \bottomrule
%   \end{tabular}
% \caption{GPT4 evaluation of HH-RLHF dataset}
%   \label{tab:GPT4_HH}
% \end{table*}

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.45\textwidth]{figures/wintldr.pdf}   
  % \vspace{-2em}
  % \label{fig:gpt4-tldr}
  % \vspace{-1em}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.45\textwidth]{figures/winhh.pdf}  
  % \vspace{-2em}
  \caption{GPT4 evaluation on the TLDR and HH datasets respectively plotting the winrates of different baselines versus FaRMA against the inference time. }
  \label{fig:gpt4}
  % \vspace{-1em}
\end{figure}