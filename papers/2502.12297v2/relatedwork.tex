\section{RELATED WORK}
In application scenarios such as virtual reality (VR) and augmented reality (AR), gesture recognition systems must capture and interpret gestures on edge devices as quickly as possible \cite{9283348, shen2024ringgesture}. This stringent real-time requirement arises not only from limited power and computing resources, but also from the need to simultaneously handle tasks such as spatial localization, motion tracking, and multi-channel interaction \cite{shen2023fast}. If the recognition process is delayed until after the gesture ends, users may experience discomfort, and the immersion and fluidity required for VR/AR interaction will be compromised \cite{shen2024towards}. Consequently, achieving stable real-time gesture recognition on these edge devices has become a pressing technical challenge. The following content briefly reviews algorithmic designs in the field of gesture recognition and extends to relevant real-time research.

\subsection{Deep Learning in Gesture Recognition}
Many technologies introduced from other domains into gesture recognition often overlook custom optimizations for edge devices and continue to rely on generalized model designs and neural network architectures. A common practice is to adopt window-based methods for processing continuous input \cite{8756576}, where raw frames are stacked over a certain time span to capture contextual information along the temporal dimension \cite{8756576, 9264164}. However, an excessively large window significantly increases model size and latency, while a window that is too small fails to provide sufficient temporal context; frame-based approaches, as an extreme case (window size of 1), are nearly incapable of recognizing dynamic gestures \cite{caputo2021shrec}. Moreover, for VR/AR devices requiring high-frequency interaction, pixel-based sliding window approaches impose additional costs for buffering raw video data, and this burden is projected to expand further with the advent of next-generation ultra-high-definition standards such as 8K.

At the algorithmic level, existing research primarily concentrates on CNN, LSTM, Transformer, and GNN architectures \cite{9873969}. While CNNs offer automatic feature extraction and often involve fewer parameters, convolutional operations can be time-consuming \cite{younesi2024comprehensive}. LSTMs excel at capturing temporal dependencies but typically rely on larger parameter sets \cite{justus2018predicting}. Transformers can model long-range dependencies and allow parallel computation, yet their computation remains considerable when handling linear, streaming inputs in real-world scenarios \cite{fournier2023practical}. GNNs are suitable for modeling skeletal data but demand manually designed preprocessing and graph generation pipelines, making direct adaptation to diverse settings challenging \cite{li2023graph}. Consequently, most of these methods still require multiple frames to be accumulated within a window for stable training and inference, falling short of the real-time requirements on edge devices.

Another challenge for real-time recognition lies in the need to repeatedly invoke the model on the input stream \cite{8578647, shen2024boosting}. When gestures are sparse and irregular, window-based recognition systems must frequently perform inference to align with potential incoming gestures \cite{caputo2021shrec}, which boosts recall but also raises false positive rates. To suppress false positives, some window-based approaches introduce lengthy “ignore periods” \cite{caputo2021shrec}, resulting in delayed recognition or even missed detections and conflicting with low-latency requirements. In scenarios demanding high recall, relying heavily on windows and frequent model invocation creates a dilemma between latency and accuracy.

In recent years, to balance performance and real-time needs, researchers have integrated deep learning with various data sources by leveraging ultrasound \cite{saez2021gesture, bimbraw2024forearm} or electromyography (EMG) \cite{kim2023emg} as event-driven inputs to support recognition, or adopting knowledge distillation, quantization, and pruning to deploy lightweight models \cite{bimbraw2024forearm}. Meanwhile, some studies focus on the robustness of actual deployments—considering factors such as environmental lighting, background complexity, and user variability \cite{murad2024advancements, shen2021imaginative}—and employ adaptive or transfer learning \cite{wu2024gesture, shen2024towards} to maintain accuracy and real-time performance. Overall, deep learning methods have already demonstrated strong capabilities in gesture recognition, including CNNs for visual feature extraction and LSTMs for capturing temporal dependencies \cite{li2021gesture, wu2024gesture}. Key directions for further improvement include multi-stream architectures \cite{rahim2024advanced, yaseen2024next, benitez2020finger}, lightweight designs \cite{bimbraw2024forearm, kim2023emg}, and adaptive technologies \cite{murad2024advancements, al2022structured}. Nonetheless, in high-interaction yet resource-constrained scenarios like VR/AR, it remains necessary to further reduce latency, power consumption, and storage requirements.

\subsection{Streaming Model}

Streaming methods prioritize real-time performance and have been extensively studied in areas such as speech recognition and autonomous driving, offering important references for gesture recognition. Unlike batch-based approaches, streaming models aim to produce predictions immediately upon data arrival, significantly reducing latency and enabling continuous online inference \cite{gomes2019machine, montiel2021river, lara2023data}.

In speech recognition, the Recurrent Neural Network Transducer (RNN-T) has been specifically optimized for low-latency scenarios by processing inputs incrementally, eliminating the need to wait for the complete sequence before generating outputs \cite{graves2012sequence}. Some variants of RNN-T also introduce boundary-aware training strategies that restrict the evaluation scope to key regions, reducing computational overhead and improving speed \cite{an2023bat}. Meanwhile, in streaming Attention Encoder-Decoder (AED) models, a technique known as monotonic attention sequentially focuses only on the relevant parts of the input, thus reducing repeated access to earlier segments and lowering computational load and latency \cite{chen2021developing}.

In the field of autonomous driving, some studies have adapted existing mature visual recognition schemes to implement streaming models. For instance, multispectral pedestrian detection based on the YOLOv4 architecture leverages lightweight design to achieve efficient computation and low latency \cite{s22031082}. In addition, for efficient motion prediction, researchers have introduced a simplified Transformer architecture—reducing pre-training complexity and combining simple linear and Transformer layers to shorten prediction time and cut resource consumption \cite{prutsch2024efficient}.

Similar approaches are gradually being applied to gesture recognition \cite{li2021gesture, devineau2018deep}, where models can parse incoming frames continuously upon data arrival, eliminating the need to wait for a full window and thus outputting predictions with minimal delay. Once integrated with lightweight neural networks or incremental learning techniques \cite{bifet2023machine, montiel2021river, lara2023data} at the system level, this method may overcome the high-latency bottleneck of traditional batch-based inference. Moreover, tree models (e.g., Hoeffding trees), online linear models \cite{montiel2021river}, and adaptive ensemble approaches can handle concept drift in dynamic environments. Finally, incorporating physiological signals \cite{saez2021gesture, bimbraw2024forearm, kim2023emg} or multimodal information \cite{rahim2024advanced, yaseen2024next, shen2021simulating} may further enhance overall performance while enabling early recognition.