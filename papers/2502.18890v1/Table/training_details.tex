\begin{table}[ht]
    \vskip -0.1 in
    \renewcommand\arraystretch{1.2}
    \centering
    \small
    \caption{Additional training details. Note that these hyperparameters do not require extensive tuning.}
    \label{tab:train_details}
    \vskip 0.15in
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
                            & \multicolumn{1}{c|}{\llama} & \multicolumn{1}{l|}{\yarnllama} & \multicolumn{1}{l|}{\smallqwen} & \multicolumn{1}{l|}{\qwen} & \bigqwen \\ \midrule
optimizer                   & \multicolumn{5}{c}{AdamW}                                                                         \\
betas                       & \multicolumn{5}{c}{(0.9, 0.999)}                                                                  \\
weight decay                & \multicolumn{5}{c}{0.1}                                                                           \\
warmup steps                & \multicolumn{5}{c}{50}                                                                            \\
learning rate scheduler& \multicolumn{5}{c}{cosine}                                                                        \\
% num. medusa heads           & \multicolumn{4}{c}{3}                                                                             \\
num. GPUs                   & \multicolumn{5}{c}{4}                                                                             \\
gradient accumulation steps & \multicolumn{5}{c}{10}                                                                            \\ \midrule
batch size per GPU          & \multicolumn{4}{c|}{3}                                                                  & 1       \\
num. steps                  & \multicolumn{4}{c|}{200}                                                                & 600     \\
learning rate& \multicolumn{4}{c|}{5e-3}                                                               & 1e-3    \\ \bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table}