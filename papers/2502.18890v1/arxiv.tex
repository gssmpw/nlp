%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\input{config}

% Use the following line for the initial blind version submitted for review:
\usepackage{bigai2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{xurl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%% Macros for coloring tabulars
\definecolor{bigaired}{RGB}{156, 0, 0}
\definecolor{uclablue}{RGB}{39, 116, 174}

\definecolor{darkred}{RGB}{200, 0, 0}
\definecolor{darkblue}{RGB}{0, 0, 200}
\definecolor{blue}{RGB}{0, 0, 250}

\definecolor{light}{RGB}{225, 250, 250}
\definecolor{lightgray}{RGB}{0.9, 0.9, 0.9}
\definecolor{lightred}{RGB}{250, 200, 200}
\definecolor{lightblue}{RGB}{210, 220, 250}

\definecolor{doderblue}{RGB}{30, 144, 255}
\definecolor{select}{RGB}{222, 235, 247}
\definecolor{unselect}{RGB}{247, 207, 206}

\hypersetup{colorlinks=true, citecolor=uclablue, linkcolor=blue, urlcolor=darkblue}

\newcommand{\blue}{\cellcolor{lightblue}}
\newcommand{\red}{\cellcolor{lightred}}

\newcommand{\hl}[1]{\textcolor{purple}{#1}}
\newcommand{\hlb}[1]{\textcolor{doderblue}{#1}}

%%%%%%%

\newcommand{\ourslong}{}
\newcommand{\ours}{\textsc{TokenSwift}\xspace}

\newcommand{\yarnllama}{\texttt{YaRN-LLaMA2-7b-128k}\xspace}
\newcommand{\llama}{\texttt{LLaMA3.1-8b}\xspace}
\newcommand{\llamasmall}{\texttt{LLaMA3.2-1b}\xspace}
\newcommand{\smallqwen}{\texttt{Qwen2.5-1.5b}\xspace}
\newcommand{\qwen}{\texttt{Qwen2.5-7b}\xspace}
\newcommand{\bigqwen}{\texttt{Qwen2.5-14b}\xspace}

\acrodef{llm}[LLM]{large language model}
\acrodef{ar}[AR]{autoregressive}
\acrodef{sd}[SD]{speculative decoding}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\bigaititlerunning{Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens}

\usepackage{minitoc}
\noptcrule
\renewcommand{\partname}{}
\renewcommand{\thepart}{}



% \everypar{\looseness=-1}

% \setlength{\parskip}{0.4em}

\begin{document}


\bigaidate{\today}


% \doparttoc
% \faketableofcontents
% \twocolumn[
\bigaititle{From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{bigaiauthorlist}
\bigaiauthor{Tong Wu$^{\,*\spadesuit}$,}{}
\bigaiauthor{Junzhe Shen$^{\,*\spadesuit\heartsuit}$,}{}
\bigaiauthor{Zixia Jia$^{\,\spadesuit}$,}{}
\bigaiauthor{Yuxuan Wang$^{\,\spadesuit}$}{} and\,
\bigaiauthor{Zilong Zheng$^{\,\spadesuit}$\textsuperscript{\Letter}}{} \\
 $^\spadesuit$ NLCo Lab, BIGAI \quad $^\heartsuit$ LUMIA Lab, Shanghai Jiao Tong University \\
\vskip .02in $^*$ Equal contribution.
\end{bigaiauthorlist}

% \icmlaffiliation{bigai}{State Key Laboratory of General Artificial Intelligence, BIGAI, Beijing, China}
% \icmlaffiliation{sjtu}{LUMIA Lab, Shanghai Jiao Tong University}

\bigaicorrespondingauthor{Zilong Zheng}{zlzheng@bigai.ai}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\begin{abstract}
Generating ultra-long sequences with \acp{llm} has become increasingly crucial but remains a highly time-intensive task, particularly for sequences \textbf{up to 100K tokens}. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. 
Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce \textbf{\ours}, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality.
Experimental results demonstrate that \ours achieves over $\mathbf{3\times}$ speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing \ours as a scalable and effective solution at unprecedented lengths. Code can be found at \url{github.com/bigai-nlco/TokenSwift}.
\end{abstract}

\vskip 0.3in

\begin{figure}[h!]
    % {\centering
    \centering
    \includegraphics[width=.7\linewidth]{Figure/speed.pdf}
    \vskip -0.1in
    \captionof{figure}{Comparison of the time taken to generate 100K tokens using autoregressive (AR) and \ours with prefix length of 4096 on \llama. As seen, \ours accelerates the AR process from nearly 5 hours to just 90 minutes.}
    \label{fig:speed_up}
    % }
    % \vskip -0.25in
\end{figure}

% \vskip 0.1in

% ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.



% TODO paragraph space
\input{Section/1-Introduction}
\input{Section/2-Challenges}
\input{Section/3-Methodology}
\input{Section/4-Experiments}
\input{Section/5-Related_Works}
\input{Section/6-Conclusion}

\section*{Acknowledgements}
We thank Haoyi Wu from ShanghaiTech University, Xuekai Zhu from Shanghai Jiaotong University, Hengli Li from Peking University for helpful discussions on speculative decoding and language modeling. This work presented herein is supported by the National Natural Science Foundation of China (62376031).

% \newpage
% \section{Impact Statements}
% This paper presents work aimed at advancing the field of Machine Learning, specifically in the context of improving efficiency and scalability in generating ultra-long sequences. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. None of the ethical concerns we foresee require specific actions or warnings in the context of this work.

{
\bibliography{ref}
\bibliographystyle{icml2025}
}


\input{Section/7-appendix}

\end{document}


