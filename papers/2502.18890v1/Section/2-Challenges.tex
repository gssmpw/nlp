\section{Challenges}
\label{sec:challenge}
Accelerating long sequence generation is nevertheless a non-trivial task, even built upon prior success in \acf{sd}.  In this section, we identify critical challenges encountered in accelerating ultra-long sequence generation.
% , along with an exploration of the underlying causes. 

\paragraph{Challenge I: Frequent Model Reloading}
\label{sec:reload}
% It is well-known that \ac{ar} generate text in a token-by-token manner. 
One fundamental speed obstacle lies in the \ac{ar} generation scheme of \ac{llm}.
For each token, the entire model must be loaded from GPU's storage unit to the computing unit~\citep{llm_viewer}, which takes significantly more time than the relatively small amount of computation performed (as shown in \cref{tab:time}). Consequently, the primary bottleneck in generation stems from I/O memory access rather than computation.
%, rendering the process highly memory-intensive.

\input{Table/short}
\input{Table/time_comp}


% \input{Table/time_comp}
% As shown in \cref{tab:time}, even at the highest computational demand, memory access time is approximately \textbf{25} times longer than the computation time.

% \textit{$\rhd$ When generating  ultra-long sequence, such as 100K tokens, the GPU must load the model weights over 100,000 times. This repetitive loading exacerbates the issue, significantly impacting overall efficiency.}

\textit{$\rhd$ When generating  ultra-long sequence, such as 100K tokens, the GPU must reload the model weights over 100,000 times. This repetitive process poses the challenge: How can we reduce the frequency of model reloading?}

% \vspace{-0.05 in}
% \subsection{Challenge 2: Dynamic KV Growing}
\paragraph{Challenge II: Prolonged Growing of KV Cache}
\label{sec:load_kv}
% Prior works, such as TriForce~\citep{triforce} and MagicDec~\citep{magicdec}, have highlighted the challenge posed by the growth of KV cache size as sequence length increases, potentially surpassing the size of model weights. This growth renders KV cache loading time a critical bottleneck in text generation. 
% Prior works like TriForce~\citep{triforce} and MagicDec~\citep{magicdec} have demonstrated that growing KV cache size significantly increases loading time, urging us to use partial KV cache when drafting. However, they all fail to appropriately update partial KV cache dynamically to support ultra-long sequences generation.
Previous studies, such as TriForce~\citep{triforce} and MagicDec~\citep{magicdec} have demonstrated that, a small KV cache budget can be used during the drafting phase to reduce the time increase caused by the loading enormous KV cache. 
While their one-time compression strategy at the prefill stage can handle scenarios with long prefixes and short outputs, it fails to address cases involving ultra-long outputs, as the growing size of KV cache would far exceed the allocated length budget.
% However, their focus is on long prefixes with short outputs, allowing for a one-time compression at the prefill stage. This static strategy cannot be directly applied to the generation of ultra-long sequences, as the growing size of KV cache would far exceed budgeted length.

% However, these works address scenarios involving extremely long prefixes and short outputs, whereas we focus on the challenges associated with ultra-long outputs.
% However, these works address scenarios involving extremely long prefixes and short outputs, where compressing the prefix once results in almost no growth of KV cache. In contrast, we focus on the challenges associated with ultra-long outputs, which require dynamic compression to manage the continuously growing KV cache. 

% In ultra-long text generation, a static KV cache strategy—updating the cache only once during the prefill phase—fails to support prolonged generation. As generation progresses, cached entries lose relevance to the evolving context, resulting in degraded quality. To tackle this, dynamic updates to KV cache are essential, enabling the retention of only the most relevant KV pairs. 

% \textit{$\rhd$ While prior works, as well as ours, aim to reduce KV loading time, the key distinction lies in the fact that generating ultra-long sequences necessitates determining when and how to dynamically update the KV cache.}
\textit{$\rhd$ To dynamically manage partial KV cache within limited budget during ultra-long sequence generation, the challenge lies in determining when and how to dynamically update the KV cache.}
% \vspace{-0.05 in}
\paragraph{Challenge III: Repetitive Content Generation}
\label{sec:repeat}
The degeneration of \ac{ar} in text generation tasks — characterized by output text that is bland, incoherent, or gets stuck in repetitive loops — is a widely studied challenge~\citep{topp,minp,eta}. 
% \todo{The repetition is severe when generating long sequeunce. May some some thoerticial support here? When generating long sequence, it is easier to lost in a local optima? } 
When generating sequences of considerable length, \eg, 100K, the model tends to produce repetitive sentences (\cref{fig:case}).

% In fact, our objective is \textbf{lossless acceleration}, meaning that the upper bound on the quality of generated text is align with target model. Therefore, our primary emphasis is more on achieving efficient acceleration, rather than addressing degradation issue.

% Nevertheless, when generating sequences of considerable length, \eg 100K, the model tends to produce repetitive sentences, resulting in meaningless content (\cref{fig:case}). This phenomenon arises because the probability of generating erroneous tokens increases with sequence length, and the propagation of these errors becomes pronounced over time.

% \textit{$\rhd$ Therefore, while prioritizing accelerated generation without sacrificing performance, it is essential to mitigate repetition patterns in ultra-long sequences. Ensuring that the generated content remains meaningful is necessary, even if completely eliminating this challenge may not be feasible.}
\textit{$\rhd$ Since our objective is lossless acceleration and repetition is an inherent problem in \acp{llm}, eliminating this issue is not our focus. However, it is still essential and challenging to mitigate repetition patterns in ultra-long sequences.}