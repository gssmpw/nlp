\section{Experiments}
\label{sec:exp}
In this section, we demonstrate the capability of \ours~in accelerating ultra-long sequences generation. 
% We begin by describing our experimental setup, followed by the presentation of the main results that demonstrate the effectiveness of \ours. Finally, we conduct ablation studies to examine the contributions of \ours's key components and discuss \ours's robustness to various sampling settings.

\subsection{Setup}
We conduct experiments on a variety of models, including \yarnllama~\citep{yarn}, \llama~\citep{llama3} and \texttt{Qwen2.5-(1.5b,7b,14b)}~\citep{qwen2.5}. For all models, we use the \textbf{Base} version, as the output length of Instruct version is limited~\citep{longwriter}. The inference experiments are performed on the test set of PG-19~\citep{pg19}.
% \vspace{-0.05 in}
\paragraph{Training and Inference Details}
We train linear layers in \cref{sec:multi_token} using the first 8K tokens of training data, for datasets longer than 8K tokens, from PG-19~\citep{pg19}. The number of extra decoding heads is set to 3 across all models. 

Inference is performed on a single NVIDIA A100-SXM4-80GB. When generating 100K tokens, the models are prefilled with 2K, 4K or 8K tokens as prompt from a random sample of the PG-19 test set (See \cref{sec:ablation_prefill} for ablation on prefill length). The maximum budget of the partial cache is determined by the length of the prompt. For further training and inference details, please refer to \cref{app:train_infer_details}.
% \vspace{-0.05 in}

\paragraph{Evaluation Metrics}
We evaluate the overall \emph{acceptance rate} and \emph{speedup} for all methods. Unlike ~\citet{sd1}\footnote{The two can be converted into each other through computation.}, our \emph{acceptance rate} $\alpha$ is defined as:

% Recall that $\gamma$ is the number of extra decoding heads. Let $a_i$ denote the length of accepted tokens at iteration $i$, and $T$ denote the current time step. Then the overall \emph{acceptance rate} $\alpha$ is defined as
% \vspace{-0.05 in}
\begin{equation}
    \label{equ:alpha}
    \alpha = \frac{\sum_{i=1}^T a_i}{(\gamma+1) \times T},
\end{equation}
where $a_i$ represents the number of tokens accepted at the $i$-th time step, $\gamma+1$ denotes the number of draft tokens generated at each time step, and $T$ represents the total  number of time steps. The \emph{speedup} is denotes as $\times$, which is the ratio of \ac{ar} latency to \ours latency, given by:
% \vspace{-0.05 in}
\begin{equation}
    \label{equ:times}
    \times = \frac{\text{latency}_{AR}}{\text{latency}_{\ours}},
\end{equation}
where latency refers to the average time required to generate a single token.

We use \emph{Distinct-$n$}~\citep{distinctn} to measure the diversity of generated content, \ie, repetition. A higher value indicates greater diversity and lower repetition (\cref{tab:distinctn_sampling}). 
\vspace{-0.05 in}
\paragraph{Baselines}
\label{para:baseline}
We compare \ours with two baselines:
\textbf{TriForce*}: The original TriForce~\citep{triforce} employs a static KV update strategy, which cannot accelerate the generation of 100K tokens. The results in \cref{tab:main_results} correspond to our improved version of TriForce, which incorporates dynamic KV update \footnote{To compare with \llama, we pretrained a draft model based on \llama. See \cref{app:llama3.1_draft} for details.}.
\textbf{Medusa*}: %Since Medusa~\citep{medusa} only provides Instruct models, we retrain the Base model.
To ensure losslessness, we adopt the Medusa~\citep{medusa} training recipe and incorporate the verification method of \ours. Both Medusa heads and tree structure are consistent with \ours.
% contextual penalty
    % To achieve lossless acceleration, we adopt the Medusa framework~\citep{medusa}, keeping the backbone model frozen. The extra decoding heads are trained using cross-entropy loss between their predictions and the ground truth. We enhanced the original Medusa~\citep{medusa} method by introducing a resampling mechanism. Medusa* uses the same number of Medusa heads as the extra decoding heads in \ours (\ie 3) and shares the same decoding tree structure % 无损验证

% MagicDec~\citep{magicdec}, which primarily focuses on increasing throughput, is not designed to accelerate ultra-long sequence generation and is therefore excluded from our baseline comparisons.
The recent released MagicDec~\citep{magicdec} primarily focuses on acceleration for large throughput, and when the batch size is 1, \llama does not exhibit any acceleration for short text generation, let alone for ultra-long sequences. Therefore, it is excluded from our baseline.

\subsection{Main Results}

\input{Table/main_results}
\input{Table/main_res2}

The experimental results are presented in \cref{tab:main_results} and \cref{tab:main_results2}. We evaluate \ours at different generation lengths of 20K, 40K, 60K, 80K and 100K, reporting \emph{speedup} $\times$ and \emph{acceptance rate} $\alpha$ by taking the average and standard deviation of 5 experiments to avoid randomness. Notably, the results for \ours and Medusa* show a balanced trade-off between speed and quality, in contrast to TriForce*, which suffers from low quality due to the absence of any repetition penalty. % 只有triforce 没有penalty diversity是在表 5
% where we report the speedup over standard autoregressive decoding, along with the overall acceptance rate. We observe that \ours consistently achieves a speedup of over 3 times at the end. Note that this result strikes a balance between generation speed and quality, as we optimized hyperparameters to maximize speedup while minimizing repetition in the generated text. 
% \vspace{-0.05 in}

\textbf{\ours significantly outperforms all baselines across generation lengths.}
% We evaluated \ours at generation lengths of 20K, 40K, 60K, 80K and 100K, reporting both \emph{speedup} $\times$ and \emph{acceptance rate} $\alpha$. Across all lengths, \ours demonstrates superior performance in \emph{speedup} compared to all baselines. 
As shown in \cref{tab:main_results}, across all lengths, \ours demonstrates superior acceleration performance compared to all baselines on models with different architectures (MHA, GQA). Moreover, \ours demonstrates remarkable robustness, showing virtually no impact when tested with varying prefix lengths.
% (2048, 4096, 8192)
% Notably, it surpasses Medusa* in acceptance rate due to advancements in token reutilization. Although \ours does not always outperform TriForce* in acceptance rate under certain conditions, it consistently achieves a higher speedup, highlighting its efficiency advantage.
% \vspace{-0.05 in}

\textbf{Longer generations amplify the speedup benefits.}
% The performance gains of \ours become increasingly pronounced as the generation length grows. Two key factors drive this trend: 1. Standard autoregressive decoding experiences a quadratic slowdown with increasing token counts, whereas \ours leverages KV cache optimizations during to mitigate this slowdown. 2. The acceptance rate improves with token count, primarily due to enhanced $n$-gram acceptance rates, as a larger corpus of generated tokens enables more accurate and diverse candidate continuations.
As the generation length increases, the speed improvement of \ours becomes increasingly evident. Two key factors drive this trend: \textbf{Firstly}, \ac{ar} experiences longer KV cache loading times as the number of tokens grows, whereas \ours mitigates this issue by utilizing dynamic KV pruning. \textbf{Secondly}, the acceptance rate improves as the number of tokens increases, primarily due to the higher $n$-grams acceptance rate. As the $n$-grams pool composed of generated tokens grows larger, the candidate $n$-grams become more diverse and accurate (\cref{fig:ablation_ngram_2}).
% \vspace{-0.05 in}

\textbf{Larger models yield greater speedup benefits.}
% The impact of frequent model loading scales with model size, as larger models require significantly more time to load due to their increased parameter count. As shown in \cref{tab:main_results2}, \ours exhibits robust performance across models of varying sizes, with its speedup advantage becoming more pronounced for larger models.
The impact of frequent model reloading varies with model scale, as larger models require more time due to the increased parameters. As shown in \cref{tab:main_results2}, \ours demonstrates robust performance across models of different scales, with the acceleration advantage becoming more pronounced for larger models. In particular, when generating 100K tokens, \ours saves up to \textbf{5.54} hours for 14B model.



% \paragraph{\ours is architecture-agnostic and effective across diverse models} Our experiments demonstrate that \ours enhances generation efficiency across different model architectures. While GQA reduces the size of the KV cache, leading to less dramatic improvements in KV cache loading times compared to MHA, \ours still delivers substantial gains. As illustrated in \cref{tab:main_results}, \ours significantly boosts generation efficiency for both \yarnllama (MHA-based) and \llama (GQA-based) architectures.

% reduces repetition


\subsection{Ablation Studies}
We conduct comprehensive ablation studies on \ours using \llama. For all experiments, the prefix length is 4096.
% and the sampling arguments followed the setup detailed in \cref{tab:inference_details}, unless explicitly stated otherwise.

\begin{figure}[t!]
    \vskip 0.1 in
    \centering
    \includegraphics[width=.7\linewidth]{Figure/ablation_ngram_2.pdf}
    \vskip -0.1 in
    \caption{Upper: The \emph{acceptance rate} $\alpha$ for $k=20$ and $k=0$, along with the \emph{$n$-gram acceptance rate} $\beta$ for $k=20$, plotted against varying generation lengths. Lower: The \emph{speedup} $\times$ achieved at different generation lengths.}
    \label{fig:ablation_ngram_2}
    % \vskip -0.2 in
\end{figure}

\subsubsection{Token Reutilization}
We define the \emph{$n$-gram acceptance rate} $\beta$ similarly to \cref{equ:alpha}. Let $a_i^{\prime}$ denote the length of accepted $n$-gram candidate at iteration $i$. Then $\beta$ is given by:
\vspace{-0.1 in}
% Recall that $\gamma$ represents the number of extra decoding heads, $a_i$ is the length of accepted tokens at iteration $i$, and $T$ is the current time step. 
\begin{equation} 
\label{equ:beta} 
\beta = \frac{\sum_{i=1}^T b_i}{(\gamma+1) \times T},\quad \text{where,~}b_i = \left\{\begin{aligned}
&  a_i^{\prime}, \quad  &a_i^{\prime} = a_i\\
&  0, \quad  &a_i^{\prime} < a_i
\end{aligned}\right.. 
\end{equation}
% From \cref{fig:ablation_ngram_2}, we observe that removing $n$-gram candidates results in a significant decrease in both the overall acceptance rate and speedup. As the generation length increases, the overall acceptance rate for $k=0$ shows a slight decline. This trend arises from reduced prediction accuracy of the extra decoding heads when the compressed KV cache is used over longer contexts. In contrast, for $k=20$, the overall acceptance rate improves with generation length, demonstrating the efficacy of our token reutilization strategy.
From \cref{fig:ablation_ngram_2}, we observe that removing token reutilization ($k=0$) leads to a significant decrease in both \emph{acceptance rate} $\alpha$ and \emph{speedup} $\times$. Furthermore, as the generation length increases, the \emph{acceptance rate} $\alpha$ for $k=0$ slightly drops. This trend stems from the fact that, in ultra-long sequences, the KV cache cannot be compressed indefinitely. In contrast, \ours ($k=20$) shows an increasing \emph{acceptance rate} as the sequence length grows, demonstrating the effectiveness of token reutilization in \textbf{reducing the frequency of model reloading}.




\begin{table}[ht!]
    \centering
\begin{minipage}{.48\textwidth}
\input{Table/ablation_kv}
\end{minipage}\hfill
\begin{minipage}{.48\textwidth}
\input{Table/distinct_n_sampling}
\end{minipage}
\end{table}


\subsubsection{Dynamic KV Updates}
To evaluate the effectiveness of \ours's dynamic KV update policy, we experiment with three different strategies of managing KV cache during drafting:

$\bullet$ Full Cache: Retaining full KV cache throughout drafting.

$\bullet$ Partial Cache: Updating partial KV cache only once during the prefill phase.

$\bullet$ Dynamic Partial Cache: Dynamically updating KV cache as described in \S \ref{sec:kv_update}


For a fair comparison, token reutilization is disabled (\ie $k=0$). As shown in \cref{tab:ablation_kv}, Partial Cache leads to a low acceptance rate, resulting in reduced speedup. While Full Cache achieves a higher acceptance rate, its computational overhead negates the speedup gains. In contrast, Dynamic Partial Cache adopted by \ours strikes a balanced trade-off, achieving both high acceptance rate and significant speedup. As a result, Dynamic Partial Cache can \textbf{effectively manage partial KV under ultra-long sequence generation.}

\subsubsection{Contextual Penalty}
As an orthogonal method to min-$p$, top-$p$, and $\eta$-sampling for mitigating the repetition, \textit{contextual penalty} demonstrates effectiveness across different sampling methods. 


% As shown in \cref{tab:distinctn_sampling}, without \textit{contextual penalty}, the diversity of generated sequences is significantly lower for all sampling methods. These results clearly highlight the impact of contextual penalty in mitigating repetitive token generation. It can seamlessly integrate with existing sampling methods for improving the quality of ultra-long sequence generation.
As shown in \cref{tab:distinctn_sampling}, without \textit{contextual penalty}, the diversity of generated sequences is significantly lower for all sampling methods. The most striking improvement emerges in min-$p$ sampling (See \cref{app:sample} for more sampling details), where the average Distinct-$n$ score surges from 0.12 to 0.69 with only an 8\% compromise in speedup. These results clearly highlight the impact of contextual penalty in \textbf{mitigating repetitive token generation.} It can seamlessly integrate with existing sampling methods to enhance the quality of ultra-long sequence generation.

In addition, we can find that the higher the diversity, the lower the \emph{speedup}. Therefore, if TriForce is combined with \textit{context penalty}, the \emph{speedup} in \cref{tab:main_results} will drop further.

\subsection{Discussions}
% In practice, the decoding process often involves varying temperatures or different sampling algorithms~\citep{topp, minp, eta}. 
% In this section, we examine the impact of these factors on \ours and demonstrate its robustness across diverse decoding settings.
In this section, we explore the effects of different hyperparameters on \ours.

\begin{figure}[t!]
    \centering
    \includegraphics[width=.7\linewidth]{Figure/ablation_ngram.pdf}
    \vskip -0.1 in
    \caption{Upper: The \emph{acceptance rate} $\alpha$ and \emph{$n$-gram acceptance rate} $\beta$ versus varying $k$. Lower: The \emph{speedup} $\times$ versus varying $k$.}
    \label{fig:ablation_ngram}
    \vskip -0.1 in
\end{figure}

\begin{table}[t!]
    \centering
\begin{minipage}[t]{.48\textwidth}
\input{Table/ablation_tree}
\end{minipage}\hfill
\begin{minipage}[t]{.48\textwidth}
\input{Table/distinct_n_penalty}
\end{minipage}
\end{table}

\subsubsection{Tree Configuration}
% We use a simple 3-ary full tree in tree attention, because the way of finding an optimal tree used in Medusa~\citep{medusa} is time consuming. However, we argue that there's no need to elaborately design a tree like done in Medusa~\citep{medusa} to achieve a decent speedup. 
Due to the time-consuming nature of finding the optimal tree in Medusa~\citep{medusa} and its limited impact on accelerating ultra-long sequences generation, we employ a simple 3-ary tree in tree attention. See \cref{app:train_infer_details} for the tree structure.




% As shown in \cref{tab:ablation_tree}, the configuration \texttt{[1,9,9,9]} achieves the maximum acceptance rate of approximately 0.57. Comparing \texttt{[1,3,3,3]} with \texttt{[3,3,3,3]}, we find that selecting only the top-1 token predicted by the backbone model's head is sufficient. While selecting a lot more tokens slightly improves the acceptance rate, it significantly slows down the verification process. To balance acceptance rate and efficiency, we adopt \texttt{[1,3,3,3]} as the default configuration.
As shown in \cref{tab:ablation_tree}, \texttt{[1,9,9,9]} has the highest acceptance rate but the lowest speedup. This is because more candidates increase the acceptance rate, but also increase the verification burden. Similarly, by comparing \texttt{[1,3,3,3]} and \texttt{[3,3,3,3]}, we can find that the first head (\ie, the original head of target model) achieves relatively high prediction accuracy when using KV compression, so choosing the top-1 token as candidate is sufficient. To balance the trade-off of acceptance rate and verification efficiency, we adopt \texttt{[1,3,3,3]} as the configuration of \ours.

\subsubsection{N-gram Candidates}
As illustrated in \cref{fig:ablation_ngram}, increasing $k$ enhances the \emph{$n$-gram acceptance rate} $\beta$ due to a larger pool of $n$-gram candidates. However, an excessive number of candidates can strain the verification process, leading to reduced \emph{speedup} $\times$.


Interestingly, a lower $k$ does not always result in a lower $\beta$. For instance, $k=5$ achieves a higher $\beta$ than $k=20$, resulting in both a higher \emph{acceptance rate} $\alpha$ and greater \emph{speedup} $\times$. However, at $k=5$, the lack of diversity among the candidates leads to increased repetition, which in turn degrades the quality of generation.

\subsubsection{Penalty Value $\theta$}
As a key component of \ours, \textit{contextual penalty} significantly reduces repetition in generated text. We examine the effect of two parameters present in contextual penalty, \ie penalty value $\theta$ and penalty window $W$. 



% \cref{tab:distinctn_theta} highlights the transformative impact of introducing a contextual penalty on diversity. Without any penalty (\( \theta = 0 \)), the generated sequences suffer from severe repetitiveness, as reflected by an average Distinct-\( n \) score of just \textbf{0.12}. However, with a carefully chosen penalty factor (\( \theta = 1.2 \)), the diversity improves dramatically, achieving an average Distinct-\( n \) score of \textbf{0.69}—a staggering \textbf{5.75× increase} over the baseline. This remarkable boost underscores the effectiveness of the Contextual Penalty in enhancing the quality of long-sequence generation.
\cref{tab:distinctn_theta} presents the impact of introducing contextual penalty on diversity. Without any penalty ($\theta = 0$), the generated sequences exhibit severe repetition, with an average Distinct-$n$ score of only \textbf{0.12}. As the value of $\theta$ increases gradually to 1.2, the diversity improves significantly, highlighting the effectiveness of contextual penalty in enhancing the diversity of ultra-long sequence generation.
% TODO: max speed upper bound

\subsubsection{Case Study}

\cref{fig:case} presents a case study on the impact of the \textit{contextual penalty}. Without the Contextual Penalty, repetitions appear at about 5K tokens, compared to 60K with the penalty applied. Additionally, generation without the penalty exhibits word-for-word repetition, whereas generation with the penalty primarily demonstrates semantic-level repetition, highlighting its effectiveness in mitigating redundancy.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.7\linewidth]{Figure/repeat.pdf}
    \vskip -0.1 in
    \caption{Case Study on \llama. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. The \hlb{blue} text is repetition part. See \cref{app:cases} for more cases.}
    \label{fig:case}
    \vskip -0.1 in
\end{figure}

