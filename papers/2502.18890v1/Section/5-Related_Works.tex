\section{Related Works}
\subsection{Speculative Decoding}
\label{app:sd}
% The acceleration of large language model (LLM) inference has gained significant attention in recent years. Speculative Decoding \citep{sd1, sd2} has been introduced as an innovative sampling algorithm to accelerate inference. Subsequent advancements \citep{layerskip, kangaroo, eagle, eagle2, draft_verify, medusa} eliminated the need for a draft model, enabling Self-Speculative Decoding. While these methods successfully improve LLM inference efficiency, they overlook the challenge posed by the large size of the KV cache. Approaches such as Triforce~\citep{triforce} and MagicDec~\citep{magicdec} address this limitation by incorporating KV cache compression during the drafting phase. However, their applicability is limited to scenarios characterized by long prefixes and short outputs, making them unsuitable for ultra-long sequence generation tasks. In such tasks, which are the focus of our work, the need for efficient inference spans both extended input contexts and lengthy outputs, presenting challenges that existing methods fail to address.

Recent advancements in speculative decoding have significantly accelerated large language model (LLM) inference through diverse methodologies. Speculative decoding~\citep{sd1,sd2} traditionally leverages smaller draft models to propose candidate tokens for verification by the target model. Early works like SpecTr~\citep{spectr} introduced optimal transport for multi-candidate selection, while SpecInfer~\citep{specinfer} and Medusa~\citep{medusa} pioneered tree-based structures with tree-aware attention and multi-head decoding to enable parallel verification of multiple candidates. Subsequent innovations, such as Sequoia~\citep{sequoia} and EAGLE-2~\citep{eagle2}, optimized tree construction using dynamic programming and reordering strategies, while Hydra~\citep{hydra} and ReDrafter~\citep{redrafter} enhanced tree dependencies through sequential or recurrent heads. Hardware-aware optimizations, exemplified by SpecExec~\citep{specexec} and Triforce~\citep{triforce}, further improved efficiency by leveraging hierarchical KV caching and quantized inference.  

Self-speculative approaches eliminate the need for external draft models by exploiting internal model dynamics. Draft\&Verify~\citep{draft_verify} and LayerSkip~\citep{layerskip} utilized early-exit mechanisms and Bayesian optimization to skip layers adaptively, whereas Kangaroo~\citep{kangaroo} integrated dual early exits with lightweight adapters. \citet{optimal} and SpecDec++~\citep{specdec++} introduced theoretical frameworks for block-level token acceptance and adaptive candidate lengths. Parallel decoding paradigms, such as PASS~\citep{pass} and MTJD~\citep{mtjd}, employed look-ahead embeddings or joint probability modeling to generate multiple candidates in a single pass, while CLLMs~\citep{cllms} and Lookahead~\citep{Lookahead2} reimagined autoregressive consistency through Jacobi decoding and n-gram candidate pools.  

Retrieval-augmented methods like REST~\citep{rest}, and NEST~\citep{nearest} integrated vector or phrase retrieval to draft context-aware tokens, often combining copy mechanisms with confidence-based attribution. Training-centric strategies, including TR-Jacobi~\citep{TR-Jacobi}, enhanced parallel decoding capability via noisy training or self-distilled multi-head architectures. System-level optimizations such as PipeInfer~\citep{PipeInfer} and \citet{faster} addressed scalability through asynchronous pipelines and latency-aware scheduling, while Goodput~\citep{throughput_sd} focused on dynamic resource allocation and nested model deployment. 

Approaches such as Triforce~\citep{triforce} and MagicDec~\citep{magicdec} incorporate KV cache compression during the drafting phase. However, their applicability is limited to scenarios characterized by long prefixes and short outputs, making them unsuitable for ultra-long sequence generation tasks. In such tasks, which are the focus of our work, the need for efficient inference spans both extended input contexts and lengthy outputs, presenting challenges that existing methods fail to address.


\subsection{Long Sequence Generation}
% Generating high-quality long sequences with LLMs presents another critical challenge. Existing sampling algorithms, including top-$p$~\citep{topp}, min-$p$~\citep{minp}, and $\eta$-sampling~\citep{eta}, aim to improve generation quality. Additionally, frameworks like LongWriter~\citep{longwriter} optimize supervised fine-tuning (SFT) datasets to enhance generation quality on long-sequence generation tasks. Despite these advancements, the issue of poor quality in long-sequence generation persists. However, it is worth emphasizing that addressing quality concerns is not the primary focus of our research.

Recent advances in long sequence generation have focused on addressing the challenges of coherence, efficiency, and scalability in producing extended outputs. A pivotal contribution is the LongWriter~\citep{longwriter} framework, which introduces a task decomposition strategy to generate texts exceeding 20,000 words. Complementing this, Temp-Lora~\citep{temp_lora} proposes inference-time training with temporary Lora modules to dynamically adapt model parameters during generation, offering a scalable alternative to traditional KV caching. Similarly, PLANET~\citep{planet} leverages dynamic content planning with sentence-level bag-of-words objectives to improve logical coherence in opinion articles and argumentative essays, demonstrating the effectiveness of structured planning in autoregressive transformers.

In addition, lightweight decoding-side sampling strategies have emerged for repetition mitigation. The foundational work on Nucleus Sampling~\citep{topp} first demonstrated that dynamically truncating low-probability token sets could reduce repetitive outputs while maintaining tractable decoding latency. Building on this, \citet{eta} introduced $\eta$-sampling explicitly linking candidate set reduction to repetition mitigation by entropy-guided token pruning. Recent variants like Min-p~\citep{minp} optimize truncation rules in real-timeâ€”scaling thresholds to the maximum token probability. 
And Mirostat Sampling~\citep{mirostat} further integrate lightweight Bayesian controllers to adjust $\eta$ parameters on-the-fly. Our work systematically analyzing how parameterized sampling (\eg, Top-p Min-p, $\eta$-sampling) balances computational overhead and repetition suppression in ultra-long sequence generation pipelines.
