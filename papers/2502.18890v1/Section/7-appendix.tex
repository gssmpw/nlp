\newpage
\appendix
\onecolumn

% \part{}
% \part{}
% \part{}
% \section*{\centering \LARGE{Appendix}}
% \mtcsettitle{parttoc}{Contents}
% \parttoc

\clearpage

\section{Lossless Nature of Speculative Decoding}
\label{app:lossless}

The speculative decoding~\citep{sd1,sd2} can easily be justified to be lossless and identical to sample from $q_{target}$ alone, \ie,  $p_{SD} = q_{target}$.
Note that, given prefix $X_{1:j}$, the next token sampled from:
\begin{equation}
\nonumber
\begin{aligned}
    x_{j+1} \sim \begin{cases}
    p_{draft}(x|X_{1:j}), & \text{if}~~ \mathcal{U}(0, 1) > \alpha, \\
     norm(\max (0, q_{target}(x|X_{1:j}) - p_{draft}(\hat{x}|X_{1:j}))), & \text{otherwise},
    \end{cases}
\end{aligned}
\end{equation}
where $\alpha$ is the acceptance rate given by
\begin{equation}
\nonumber
\begin{aligned}
    \alpha(x) = \min\left(1.0, \frac{q_{target}(x)}{p_{draft}(x)} \right).
\end{aligned}
\end{equation}
If the draft token is accepted, we have
\[
p_{SD}(x|X_{1:j};accepted) = p_{draft}(x|X_{1:j}) \alpha(x|X_{1:j}) = \min(p_{draft}, q_{target}). 
\]
If the token is rejected, we have 
\begin{equation*}
\begin{aligned}
    p_{SD}(x|X_{1:j};rejected) &=  (1 - \alpha(x|X_{1:j})) norm(\max (0, q_{target}(x|X_{1:j}) - p_{draft}(\hat{x}|X_{1:j}))) \\
&= (1 - \alpha) \frac{q_{target} - \min(p_{draft}, q_{target})}{ 1 - \alpha} \\
& = q_{target} - \min(p_{draft}, q_{target})
\end{aligned}
\end{equation*}
Therefore, the overall probability is given by
\[
p_{SD}(x|X_{1:j}) = p_{SD}(x|X_{1:j};accepted) + p_{SD}(x|X_{1:j};rejected) = q_{target}((x|X_{1:j})
\]
Proved.


% \input{Section/5-Related Works}

\section{Additional Training and Inference Details.}
\label{app:train_infer_details}
\subsection{Training Details}
During training, only three linear layers are fine-tuned, while the parameters of the LLM remained fixed. The model was trained on an NVIDIA A100-SXM4-80GB GPU. The specific training parameters are outlined in \cref{tab:train_details}.
\input{Table/training_details}

\subsection{Inference Details}
For inference, we used 4-grams to maintain consistency with multi-token generation. The specific inference parameters are presented in \cref{tab:inference_details}.
\input{Table/inference_details}

For the tree attention mechanism, we selected a simple ternary full tree configuration, as depicted in \cref{fig:tree-setting}.
\begin{figure}[htbp]
    \centering
    \label{fig:tree-setting}
    \includegraphics[width=0.7\linewidth]{Figure/full_tree.pdf}
    
\end{figure}

\section{ Pre-training Details of the Llama3.1 Draft Model}
\label{app:llama3.1_draft}
To serve as the draft model for \llama in TriForce, we pretrain a tiny version of 250M parameters with the same tokenizer from \llama. The model configuration is listed in \cref{tab:llama_205m}. We train the model on Wikipedia (20231101.en) \footnote{\url{https://huggingface.co/datasets/wikimedia/wikipedia}} and part of C4-en\footnote{\url{https://huggingface.co/datasets/allenai/c4}} for 1 epoch. 

\begin{table}[ht]
\vskip -0.1 in
    \renewcommand\arraystretch{1.2}
    \centering
    \small
    \caption{Configuration of Llama 3.1 205M.}
    \label{tab:llama_205m}
    \vskip 0.15in
    \begin{tabular}{c|c}
    \toprule
       hidden\_size  & 768 \\
       hidden\_act & silu \\
       intermediate\_size & 3072 \\
       max\_position\_embeddings & 2048 \\
       num\_attention\_heads & 12 \\
       num\_key\_value\_heads & 12 \\
       rope\_theta & 500000 \\
       vocab\_size & 128256 \\
    \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}

% \section{Results of Baselines for Generating Short Sequences}

% The experimental results presented in \cref{tab:short} show the inference acceleration performance of TriForce~\citep{triforce} and MagicDec~\citep{magicdec} on \llama with default configurations, where MagicDec uses a batch size of 1. 
% TriForce achieves only a 1.02× speedup for 256-token generation using standalone draft tokens, while MagicDec shows marginally better but still limited acceleration (1.20× for self-speculation and 1.06× for standalone draft) at shorter 64-token generation. \
% These results collectively highlight the current methods' minimal effectiveness in accelerating GQA (Group Query Attention)-based models like \llama.

% \input{Table/short}

\section{Different Sampling Method}
\label{app:sample}
\subsection{Introduction of Different Sampling Algorithms}
Given a probability distribution $P(x_t | x_1, x_2, \ldots, x_{t-1})$ over the vocabulary $\mathcal{V}$ at position $t$, top-$p$ sampling~\citep{topp} first sorts the tokens in descending order of their probabilities. It then selects the smallest set of tokens whose cumulative probability exceeds a predefined threshold $p$, where $p \in (0, 1]$. Formally, let $\mathcal{V}_p \subset \mathcal{V}$ be the smallest set such that:
\begin{equation}
    \nonumber
    \sum_{v\in\mathcal{V}_p}P(x_t=v|x_1,x_2,\ldots,x_{t-1})\geq p.
\end{equation}
The next token $\hat{x_t}$ is then randomly sampled from this reduced set $\mathcal{V}_p$ according to the renormalized probabilities:
\begin{equation}
\nonumber
    \hat{x}_t\sim\frac{P(x_t=v|x_1,\ldots,x_{t-1})}{\sum_{v^{\prime}\in\mathcal{V}_p}P(x_t=v^{\prime}|x_1,\ldots,x_{t-1})}\mathrm{~for~}v\in\mathcal{V}_p.
\end{equation}
\citet{minp} introduced min-p sampling, which uses a relative probability threshold $p_{base} \in (0, 1]$ to scale the maximum token probability $p_{max}$ to determine the absolute probability threshold $p_{scaled}$. Sampling is then performed on tokens with probability greater than or equal to $p_{scaled}$.

Formally, given the maximum probability over the token distribution $p_{max} = \max_{v\in\mathcal{V}} P(x_t=v|x_1,x_2,\ldots,x_{t-1})$, the absolute probability threshold $p_{scaled}$ is calculated as:
\begin{equation}
\nonumber
    p_{scaled} = p_{base} \times p_{max}.
\end{equation}

The sampling pool $\mathcal{V}_{min}$ is then defined as the set of tokens whose probability is greater than or equal to $p_{scaled}$:
\begin{equation}
\nonumber
    \mathcal{V}_{min}=\{v\in\mathcal{V}\mid P(v|x_1,x_2,\ldots,x_{t-1})\geq p_{scaled}\}.
\end{equation}

Finally, the next token $\hat {x}_t$ is randomly sampled from the set $\mathcal{V}_{min}$ according to the normalized probabilities:
\begin{equation}
\nonumber
    \hat{x}_t\sim\frac{P(v|x_1,\ldots,x_{t-1})}{\sum_{v^{\prime}\in\mathcal{V}_{min}}P(v^{\prime}|x_1,\ldots,x_{t-1})}\mathrm{~for~}v\in\mathcal{V}_{min}.
\end{equation}

The sampling pool of $\eta$-sampling~\citep{eta} is defined as
\begin{equation}
\nonumber
    \begin{aligned}
        &\mathcal{V}_{\eta}=\{v\in\mathcal{V}\mid P(v|x_1,x_2,\ldots,x_{t-1})\geq \eta\}, \\
        & \eta =\min\left(\epsilon,\alpha\exp(-h_{\theta,x_{<i}})\right).
    \end{aligned}
\end{equation}
where ${h}_{\theta,x_{<i}}$ is the entropy of $P(\mathcal{V}|x_1,x_2,\ldots,x_{t-1})$, $\alpha$ and $\epsilon$ are hyperparameters.

\subsection{Impact of Different Sampling Algorithms}
We also explored the impact of different sampling algorithms with disable token reutilization, including top-$p$ sampling~\citep{topp}, min-$p$ sampling~\citep{minp}, and $\eta$-sampling~\citep{eta}. As summarized in \cref{tab:ablation_sampling}, \ours consistently demonstrates strong robustness across these methods. This versatility underscores its compatibility with a wide range of decoding strategies, making it suitable for diverse applications and use cases.

\input{Table/ablation_sampling}


\section{Tree-Based Attention}
\label{app:tree_attn}
Tree attention is a mechanism designed to process multiple candidate continuations during speculative decoding efficiently. Instead of selecting a single continuation as in traditional methods, tree attention leverages multiple candidates to increase the expected acceptance length in each decoding step, balancing computational demands and performance.

The mechanism uses a tree structure where each branch represents a unique candidate continuation. For example, if two heads generate top-2 and top-3 predictions, the Cartesian product of these predictions results in 6 candidates, forming a tree with 6 branches. Each token in the tree attends only to its predecessors, and an attention mask ensures that this constraint is upheld. Positional indices are also adjusted to align with the tree structure.

The tree structure is constructed by taking the Cartesian product of the predictions across all heads. If head \( k \) has \( s_k \) top predictions, then the tree structure consists of all possible combinations of predictions across the heads. Each combination forms a unique branch in the tree.

Let the total number of candidates (i.e., branches) in the tree be denoted as \( C \), which is the product of the number of predictions for each head:
\begin{equation}
\nonumber
C = \prod_{k=1}^K s_k.
\end{equation}
Each candidate is a distinct sequence of tokens formed by selecting one token from each set of predictions from the heads.

To ensure that tokens only attend to their predecessors (tokens generated earlier in the continuation), an attention mask is applied. The attention mask for the tree structure ensures that for each token at level \( k \), it can attend only to tokens in levels \( \{0, 1, \dots, k-1\} \). This guarantees that each token's attention is directed solely towards its predecessors in the tree.

Formally, the attention mask \( M_k \) for each token at level \( k \) is defined as:
\[
\nonumber
M_k(i,j) = 
\begin{cases}
1 & \text{if token } j \text{ is a predecessor of token } i, \\
0 & \text{otherwise}.
\end{cases}
\]
where \( M_k(i,j) = 1 \) means that the token at position \( j \) can attend to the token at position \( i \), and \( M_k(i,j) = 0 \) means no attention is allowed from \( j \) to \( i \).


% \section{llama3.2 results}

\section{More Ablation Experiments}

\subsection{Ablation of Temperature}
% To evaluate the effect of temperature on \ours, we conducted experiments across a range of temperature values. As shown in \cref{tab:ablation_temp}, \ours maintains stable speedup and acceptance rates regardless of temperature variations. This robustness highlights the adaptability of \ours to diverse generation scenarios, ensuring consistent acceleration performance even when the decoding behavior is altered by temperature adjustments.

\cref{tab:ablation_temp} presents the results of an ablation experiment investigating the effect of varying temperature settings on the generation length, acceptance rate, and speedup during text generation. The experiment uses top-$p$ sampling with a fixed $p$ of 0.9 and evaluates generation lengths ranging from 20K to 100K tokens, with temperature values spanning from 0.4 to 1.2.

From the results, it is evident that as temperature increases, acceptance rate generally decreases across all generation lengths. Specifically, acceptance rate drops from 0.79 at a temperature of 0.4 to 0.52 at a temperature of 1.2 for 20K-length generation, and a similar trend is observed for longer sequences. This suggests that higher temperatures result in more diverse but less accurate output. On the other hand, speedup tends to remain relatively stable or slightly decrease with higher temperatures. The highest speedups, reaching around 3.4, are observed across all generation lengths with temperatures around 0.6 and 1.0, indicating that moderate temperature settings offer the best balance between speed and quality.

\input{Table/ablation_temp}


\subsection{Ablation of Prefill Length}
\label{sec:ablation_prefill}
We disable token reutilization and conduct ablation study on the different prefix length, as shown in \cref{tab:ablation_prefill}. The experiment explores the impact of varying prefix lengths on the generation of sequences of different lengths (from 20K to 100K). The results include two key metrics: acceptance rate ($\alpha$) and speedup factor ($\times$). 

As the prefix length increases, the acceptance rate tends to stabilize, generally hovering around 0.35 to 0.39 across different sequence lengths, with a slight fluctuation depending on the specific prefix length. This suggests that while the acceptance rate does not dramatically change with longer sequences, it remains relatively consistent.

\input{Table/ablation_prefill}

In terms of speedup, it shows that with longer prefix lengths, the model achieves progressively higher acceleration. For instance, a prefix length of 2048 achieves a speedup of 1.41 for 20K tokens, but with 8192, the speedup reaches up to 1.77 for 100K tokens. This indicates that increasing the prefix length contributes to better acceleration, especially for longer sequences, while maintaining a relatively stable acceptance rate. The findings demonstrate the tradeoff between prefix length and model efficiency, where larger prefix lengths tend to result in greater speed.

\subsection{Ablation of Penalty Window}
\label{sec:ablation_W}
We investigate the effect of penalty window size ($W$) on the performance of a model generating sequences of varying lengths (from 20K to 100K tokens). For each sequence length, we apply a penalty to generated tokens within a sliding window of size $W$, and evaluate the impact on two key metrics: acceptance rate ($\alpha$) and acceleration factor ($\times$). Additionally, we assess the diversity of the generated sequences using the \textit{Distinct-$n$} metric, where higher values indicate greater diversity.

\input{Table/ablation_penalty_len}
\input{Table/distinct_n_penalty_length}


The results in \cref{tab:ablation_penalty_len} and \cref{tab:distinctn_W} show a clear trade-off between the penalty window size and the model's performance. For smaller penalty window sizes, such as $W=20$, the model achieves higher acceptance rates and better acceleration, but this comes at the cost of lower diversity in the generated sequences (as indicated by lower \textit{Distinct-$n$} values). As the penalty window size increases (\eg, $W=256$ or $W=2048$), the acceptance rate slightly decreases, but the model exhibits better diversity and still maintains a significant speedup relative to the AR baseline. These findings suggest that larger penalty windows can help reduce repetitiveness and improve the diversity of long sequence generation, but they may also slightly reduce the model's efficiency and acceptance rate.

\cref{tab:ablation_penalty_len} also reveals that for each penalty window size, increasing the sequence length (from 20K to 100K tokens) generally results in higher acceleration and better diversity, with some fluctuations in acceptance rates.

\section{More Cases}
\label{app:cases}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth, height=4.65 in]{Figure/case_llama2.pdf}
    \vskip -0.1 in
    \caption{Case Study on \yarnllama. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. The \hlb{blue} text is repetition part.}
    \label{fig:case2}
    \vskip -0.1 in
\end{figure}

\section{Training Loss Curve}
\begin{figure}[htbp]
    \centering
    \subfigure[Cross Entropy Loss Training Curve of the First Linear Layer]{
        \includegraphics[width=0.8\textwidth]{Figure/loss1.pdf}
    }
    \vspace{0.3cm}
    \subfigure[Cross Entropy Loss Training Curve of the Second Linear Layer]{
        \includegraphics[width=0.8\textwidth]{Figure/loss2.pdf}
    }
    \vspace{0.3cm}
    \subfigure[Cross Entropy Loss Training Curve of the Third Linear Layer]{
        \includegraphics[width=0.8\textwidth]{Figure/loss3.pdf}
    }
    \caption{Cross Entropy Loss Training Curve of  Linear Layers}
    \label{fig:three_images}
\end{figure}
