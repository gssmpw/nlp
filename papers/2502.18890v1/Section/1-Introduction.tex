

\section{Introduction}
% Recent advancements in large language models (LLMs) have achieved the acceleration of long-prefix generation, with notable works such as TriForce~\citep{triforce} and DeepSpeed-FastGen~\citep{fastgen} addressing this problem effectively.
% Recent advancements in large language models (LLMs), including the development of o1-like models~\citep{o1}, highlight the increasing importance of generating ultra-long sequences. This capability is especially crucial for tasks such as complex multi-step reasoning~\citep{long_reason1,long_reason2} and long-form storytelling~\citep{long_stroy1,long_stroy2}, where generating sequences that exceed hundreds of thousands of tokens.
Recent advances in large language models (LLMs), amplified by their long context capacities~\citep{cream,longrope}, have demonstrated remarkable proficiency in intricate reasoning~\citep{o1,deepseek_r1}, agentic thinking~\citep{reflexion,react,ram}, and creative writing~\cite{long_stroy1,long_stroy2}, \etc. These advancements necessitate the ability to generate lengthy sequences,
% emphasize the need for handling complex tasks that require extensive outputs. 
\eg, o1-like~\citep{o1} reasoning tends to generate protracted chain-of-thought trajectories before reaching final conclusions.
% ~\citep{long_reason1,long_reason2}. 
However, a critical challenge impeding the practical deployment of such applications is the extensive time required to produce ultra-long sequences. For instance, generating 100K tokens with LLaMA3.1-8B can take approximately five hours (\cref{fig:speed_up}), a duration that is impractically long for the development of sophisticated applications, let alone recent gigantic models such as LLaMA3.1-405B~\citep{llama3} and DeepSeek-600B~\cite{deepseek_v3}. Addressing this bottleneck is essential for harnessing the full potential of LLMs in real-world scenarios. 
% ; long-form storytelling~\citep{long_stroy1,long_stroy2}, such as writing lengthy novels or detailed reports, requires generating extensive narratives that can span tens of thousands of tokens, and in some cases, even exceed 100K tokens. 
% These tasks highlight the crucial need for models capable of generating ultra-long sequence.

% However, accelerating \textbf{long-output} generation remains a challenging task. Practical applications increasingly require the generation of extensive content, \eg story agent~\citep{story} and complex reasoning processes~\citep{o1}. In an \ac{ar} manner, generating a 100K-token sequence can take up to 5 hours on a 8B model (\cref{fig:speed_up}), highlighting the need for efficient acceleration methods. But the effectiveness of existing methods on this task is not significant (\cref{tab:main_results}).
% However, generating 100K tokens on an 8B model in an \ac{ar} manner takes an unbearably 5 hours (as shown in \cref{fig:speed_up}), highlighting the urgent need for \textbf{lossless acceleration in ultra-long sequence generation}. Existing methods~\citep{medusa,triforce,sd1,sd2} leverage speculative decoding for acceleration, are limited to speeding up relatively short sequence generations (\eg 256 tokens). To the best of our knowledge, no one has yet addressed this challenge. Consequently,  we \textbf{first} pose the following research question:
% However, \ac{ar} generation remains prohibitively slow, with even an 8B model taking nearly 5 hours to generate . 
% A na\"ive solution lies 
A straightforward solution is to take advantage of recent success in \ac{sd}~\citep{sd1,sd2}, which employs a \textit{draft-then-verify} strategy to expedite generation while preserving \textit{lossless} accuracy; see \cref{app:lossless,app:sd} for detailed background and relevant literature.
% Traditional methods focus on enhancing draft model to improve acceptance rate~\citep{medusa,eagle,eagle2}, leveraging hierarchical structure for multiple resamples~\citep{triforce,stage_sd}, and managing key-value (KV) caches to boost throughput~\citep{magicdec,throughput_sd};  
% Nevertheless, these works are typically designed for generating short sequences, \eg, Triforce~\cite{triforce} xxx. \todo{However, simply extending generation tokens by setting max generation tokens can not solve this problem.... some results here. or refer to some experimentss}
% often no longer than 1,024 tokensâ€”and fail to achieve more than $3 \times$ acceleration for ultra-long sequences (\cref{tab:main_results}). This brings a crucial research question:
However, these methods are generally tailored for generating short sequences, \eg, TriForce~\citep{triforce} and MagicDec~\citep{magicdec} are limited to generating 256 and 64 tokens, respectively. Directly extending their generation length to 100K tokens would inevitably encounter failures due to KV cache budget constraints. Furthermore, even when applied to optimized KV cache architectures such as Group Query Attention (GQA), these methods yield only marginal acceleration gains for short-sequence generation, as evidenced in \cref{tab:short,tab:main_results}.
This observation leads to a pivotal research question:



% \textit{Can we achieve lossless acceleration in generating ultra-long sequences with large language models effectively? }
\textit{Is it possible to achieve model-agnostic \textbf{lossless} accelerations, akin to those seen in short-sequence SDs, for generating \textbf{ultra-long} sequences, with \textbf{minimal} training overhead?}
% , while ensuring \textbf{lossless} quality?}
% comparable short generation

% To answer this question, we conduct an in-depth analysis (in \cref{sec:challenge}) and identify three key challenges: (1) frequent model reloading, (2) dynamic KV management, and (3) mitigating repetitive generation. \textbf{First}, frequently reloading model for each token generation introduces a significant delay, primarily due to memory access times rather than computation (\cref{sec:reload}). \textbf{Secondly}, the dynamic management of key-value (KV) pairs, which grow with the sequence length, adds complexity in maintaining model efficiency (\cref{sec:load_kv}). \textbf{Lastly}, the issue of repetitive generation becomes more pronounced as the sequence length increases, leading to degraded output quality (\cref{sec:repeat}).
To answer this question, we conduct an in-depth analysis (\S \ref{sec:challenge}) and identify three key challenges:\,
\textbf{(1)} \textit{frequent model reloading}: frequently reloading model for each token generation introduces a significant delay, primarily due to memory access times rather than computation.\,
\textbf{(2)} \textit{Prolonged Growing of KV Cache}, the dynamic management of key-value (KV) pairs, which grow with the sequence length, adds complexity in maintaining model efficiency.\,
\textbf{(3}) \textit{repetitive content generation}, the issue of repetitive generation becomes more pronounced as the sequence length increases, leading to degraded output quality.

% Based on these challenges, in \cref{sec:method}, we introduce a lightweight framework, \ours. As shown in~\cref{fig:frame}, the entire process is divided into two stages: draft and verification.  In the first stage, a draft model is formed by an \ac{llm} with partial KV cache and 3 linear layers, which outputs 4 logits during a single forward pass. Tree-based attention is then applied to select candidate draft tokens, which are subsequently verified in parallel with candidate N-grams. In the second stage, an \ac{llm} with full KV cache performs exact matching to select validated tokens.  Typically, multiple groups of tokens pass verification, and one group is randomly chosen as valid output. 
Building on these insights,  we introduce our framework \ours, which utilizes $n$-gram retrieval and dynamic KV cache updates to accelerate ultra-long sequence generation.
% which is briefly demonstrated in~\cref{fig:frame}.
% which is the \textbf{first} to achieve lossless acceleration in generating 100K-length sequences 
% (\cref{fig:frame}).\,
% Specifically, we employ \textit{multi-token generation} and \textit{token reutilization} to enable the LLM (\ie target model) to generate multiple tokens in a single forward pass, alleviating the first challenge of frequent model reloading (\S \ref{sec:multi_token}).\,
Specifically, we employ \textit{multi-token generation} and \textit{token reutilization} to enable the LLM (\ie target model) to draft multiple tokens in a single forward pass, alleviating the first challenge of frequent model reloading (\S \ref{sec:multi_token}).\,
% As the generation progresses, we \textit{dynamically update} the partial KV cache during the drafting phase, gradually reducing the time required for prolonged KV cache loading (\S \ref{sec:kv_update}).\,
As the generation progresses, we \textit{dynamically update} the partial KV cache at each iteration, reducing the KV cache loading time (\S \ref{sec:kv_update}).\,
Finally, to mitigate the issue of repetitive outputs, we apply \textit{contextual penalty} to constrain the generation process, ensuring the diversity of output (\S \ref{sec:penalty}).
% \,
% Since \ours also employs the \textit{draft-then-verify} paradigm, it similarly ensures lossless generation. 

In \S \ref{sec:exp}, we conduct extensive experiments to evaluate \ours across different model scales and architectures.
% , demonstrating that \ours can accelerate generation by over $\mathbf{3 \times}$ faster than \ac{ar} (\cref{tab:main_results}, \cref{tab:main_results2}), while improving diversity in generated ultra-long sequences (\cref{tab:distinctn_sampling}, \cref{fig:case}). 
In summary, we highlight our advantages as:
\begin{enumerate}[leftmargin=*, noitemsep,topsep=0pt]
\item  To the best of our knowledge, \ours is the \textbf{first} to accelerate ultra-long sequence generation up to 100K with lossless accuracy of target \acp{llm}, while demonstrating significant superiority over enhanced baselines.
\item \ours consistently achieves over $\mathbf{3\times}$ speedup compared to AR across varying prefix lengths, model architectures, and model scales in generating 100K tokens, reducing the AR process from nearly 5 hours to 90 minutes on \llama. 
\item \ours achieves progressively higher speedup compared to AR as the generation length increases, while enhancing diversity in ultra-long sequence generation (as measured by \textit{Distinct-$n$}~\citep{distinctn}).
\end{enumerate}

