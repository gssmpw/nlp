\begin{table*}[t]
    \centering
    \small 
    \begin{tabular}{lccccc}
        \toprule
        Model & Likelihood & Entropy & CCP & SAR & Focus \\
\midrule
meta-llama/Meta-Llama-3-8B & 0.168 & 0.18 & 0.247 & 0.269 & 0.404 \\
meta-llama/Meta-Llama-3-70B & 0.168 & 0.175 & \textbf{0.254} & 0.268 & 0.408 \\
meta-llama/Llama-3.1-8B & 0.173 & 0.193 & 0.246 & 0.274 & 0.412 \\
meta-llama/Llama-3.2-3B & 0.17 & 0.191 & 0.235 & 0.269 & 0.368 \\
BAAI/Aquila2-7B & 0.162 & 0.178 & 0.228 & 0.254 & 0.388 \\
BAAI/Aquila2-34B & 0.157 & 0.185 & 0.236 & 0.249 & 0.385 \\
internlm/internlm2-7b & 0.173 & 0.185 & 0.232 & 0.264 & 0.38 \\
internlm/internlm2-20b & 0.171 & 0.179 & 0.233 & 0.267 & 0.349 \\
Qwen/Qwen2.5-7B & 0.164 & 0.185 & 0.22 & 0.255 & 0.346 \\
Qwen/Qwen2.5-32B & 0.166 & 0.183 & 0.226 & 0.258 & 0.347 \\
01-ai/Yi-9B & 0.159 & 0.173 & 0.231 & 0.237 & 0.375 \\
01-ai/Yi-34B & 0.156 & 0.165 & 0.229 & 0.233 & 0.349 \\
microsoft/phi-2 & \textbf{0.19} & \textbf{0.236} & 0.238 & \textbf{0.279} & 0.371 \\
mistralai/Mistral-7B-v0.3 & 0.159 & 0.167 & 0.236 & 0.25 & 0.391 \\
mistralai/Mixtral-8x22B-v0.1 & 0.163 & 0.165 & 0.249 & 0.263 & \textbf{0.418} \\
google/gemma-2-9b & 0.172 & 0.187 & 0.234 & 0.264 & 0.281 \\
google/gemma-2-27b & 0.174 & 0.177 & 0.232 & 0.263 & 0.397 \\
        \bottomrule
    \end{tabular}
    \caption{
     AUPRC of five uncertainty scores across 17 LLMs.
    }
    \label{tb:auprc_17models}
\end{table*}