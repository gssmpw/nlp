\begin{table*}[t]
    \centering
    \small 
    \begin{tabular}{lccccc}
        \toprule
        Model & Likelihood & Entropy & CCP & SAR & Focus \\
\midrule
meta-llama/Meta-Llama-3-8B & 0.568 & 0.571 & 0.565 & 0.672 & 0.784 \\
meta-llama/Meta-Llama-3-70B & 0.574 & 0.567 & 0.565 & 0.667 & 0.779 \\
meta-llama/Llama-3.1-8B & 0.584 & 0.592 & 0.564 & 0.684 & 0.783 \\
meta-llama/Llama-3.2-3B & 0.577 & 0.591 & 0.564 & 0.685 & 0.772 \\
BAAI/Aquila2-7B & 0.544 & 0.553 & 0.565 & 0.679 & 0.78 \\
BAAI/Aquila2-34B & 0.541 & 0.566 & 0.565 & 0.665 & 0.779 \\
internlm/internlm2-7b & 0.586 & 0.584 & 0.562 & 0.678 & 0.777 \\
internlm/internlm2-20b & 0.579 & 0.573 & 0.561 & 0.674 & 0.77 \\
Qwen/Qwen2.5-7B & 0.557 & 0.571 & 0.558 & 0.675 & 0.767 \\
Qwen/Qwen2.5-32B & 0.561 & 0.569 & 0.559 & 0.674 & 0.768 \\
01-ai/Yi-9B & 0.541 & 0.549 & 0.56 & 0.663 & 0.776 \\
01-ai/Yi-34B & 0.543 & 0.543 & 0.557 & 0.653 & 0.769 \\
microsoft/phi-2 & \textbf{0.619} & \textbf{0.656} & \textbf{0.571} & \textbf{0.705} & 0.775 \\
mistralai/Mistral-7B-v0.3 & 0.549 & 0.545 & 0.555 & 0.666 & 0.784 \\
mistralai/Mixtral-8x22B-v0.1 & 0.56 & 0.545 & 0.555 & 0.665 & \textbf{0.785} \\
google/gemma-2-9b & 0.574 & 0.575 & 0.561 & 0.68 & 0.744 \\
google/gemma-2-27b & 0.576 & 0.566 & 0.557 & 0.673 & 0.78 \\
        \bottomrule
    \end{tabular}
    \caption{
     AUROC of five uncertainty scores across 17 LLMs.
    }
    \label{tb:auroc_17models}
\end{table*}