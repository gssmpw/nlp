\begin{table*}[t]
    \centering
    \small 
    \begin{tabular}{lccccc}
        \toprule
        Model & Likelihood & Entropy & CCP & SAR & Focus \\
\midrule
meta-llama/Meta-Llama-3-8B & 0.742 & 0.86 & \textbf{1.0} & 0.505 & 0.658 \\
meta-llama/Meta-Llama-3-70B & 0.736 & 0.87 & \textbf{1.0} & 0.432 & 0.757 \\
meta-llama/Llama-3.1-8B & 0.696 & 0.688 & \textbf{1.0} & 0.505 & 0.724 \\
meta-llama/Llama-3.2-3B & 0.855 & 0.662 & \textbf{1.0} & 0.548 & 0.758 \\
BAAI/Aquila2-7B & \textbf{0.875} & 0.916 & \textbf{1.0} & 0.57 & 0.719 \\
BAAI/Aquila2-34B & 0.838 & 0.786 & \textbf{1.0} & \textbf{0.632} & 0.732 \\
internlm/internlm2-7b & 0.736 & 0.797 & \textbf{1.0} & 0.525 & 0.62 \\
internlm/internlm2-20b & 0.771 & 0.82 & \textbf{1.0} & 0.481 & \textbf{0.783} \\
Qwen/Qwen2.5-7B & 0.857 & 0.833 & \textbf{1.0} & 0.613 & 0.753 \\
Qwen/Qwen2.5-32B & 0.817 & 0.883 & \textbf{1.0} & 0.535 & 0.749 \\
01-ai/Yi-9B & 0.843 & \textbf{0.936} & \textbf{1.0} & 0.58 & 0.721 \\
01-ai/Yi-34B & 0.831 & 0.88 & \textbf{1.0} & 0.559 & 0.739 \\
microsoft/phi-2 & 0.81 & 0.65 & 0.27 & 0.622 & 0.76 \\
mistralai/Mistral-7B-v0.3 & 0.811 & 0.933 & \textbf{1.0} & 0.561 & 0.666 \\
mistralai/Mixtral-8x22B-v0.1 & 0.836 & 0.866 & \textbf{1.0} & 0.477 & 0.743 \\
google/gemma-2-9b & 0.809 & 0.856 & \textbf{1.0} & 0.526 & 0.77 \\
google/gemma-2-27b & 0.8 & 0.916 & \textbf{1.0} & 0.556 & 0.732 \\
        \bottomrule
    \end{tabular}
    \caption{
     $\mathrm{Recall}_\mathrm{Opt}$ of five uncertainty scores across 17 LLMs.
    }
    \label{tb:recall_opt_17models}
\end{table*}