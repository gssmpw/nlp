\section*{Limitations}

In this paper, we focus on evaluating uncertainty-based hallucination detection approaches, where the uncertainty scores are estimated by token probabilities. For other types of uncertainty estimation that measure the diversity across samples, such as Semantic Entropy, since they estimate uncertainty at the sample level and do not output scores for each token or entity, they can not be evaluated on \dataset. Although this incompatibility limits the usage of \dataset, it also shows the limitation of sample-based approaches---they are hard to be used to pinpoint hallucinated content.


\section*{Ethical Statement}

This research addresses the critical challenge of hallucination detection in LLMs to enhance their safe and responsible use across high-stakes domains. By exploring entity-level hallucination detection and evaluating uncertainty-based methods, we aim to improve the precision and reliability of identifying factual inaccuracies in generated content. \dataset and evaluation metrics are intended solely for research purposes, ensuring no sensitive or personal information is included. We acknowledge the limitations of current approaches and advocate for continued improvements to promote transparency, accuracy, and responsible AI development.

\section*{Acknowledgement}
We thank ACL anonymous reviewers for their reading and feedback. This work is supported by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 and IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, and Philanthropic Fund from SFF.