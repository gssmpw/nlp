Throughout this study, we identify the limitations and strengths of current uncertainty-based hallucination detection approaches. In this section, we discuss two directions to improve the performance of uncertainty-based hallucination detection.

\vspace{-0.1cm}
\paragraph{Uncertainty score calibration.}

In Section~\ref{sec:qualitative_analysis}, we show how simple calibrations (type-filter and uncertainty propagation) help improve the performance of Focus. These calibrations were invented through the linguistic analysis on LLM generated corpus, indicating the relationship between the tendency of hallucination and linguistic properties. Based on this finding, we recommend exploring more linguistic properties that can help determine the importance of generating content and the tendency of hallucination. This exploration would not only improve the performance of hallucination detection, but also help mitigate hallucination during generation.

\vspace{-0.1cm}
\paragraph{Utilizing information beyond token probabilities to estimate uncertainty.}
One major limitation of uncertainty-based hallucination detection is its over-prediction nature. As shown in Section~\ref{sec:result}, all five approaches perform poorly when hallucinations are sparse. In Section~\ref{sec:qualitative_analysis}, we further show that such over-prediction frequently happens on informative content, such as name entities. This suggests that token probabilities are not well separated between hallucinated and non-hallucinated content, and using uncertainty scores like Likelihood or Entropy to serve as the base score of hallucination detection is not reliable. Hence, we recommend investigating more sophisticated uncertainty estimation or integrating probing techniques that utilize other information from LLM's internal states to increase the reliability of hallucination detection.

\vspace{-0.1cm}
\paragraph{Conclusion and future work.}

In this work, we comprehensively explore the promise of entity-level hallucination detection by curating \dataset, a dataset tailored for fine-grained understanding and introducing evaluation metrics for the task. We benchmark five uncertainty-based approaches, finding that they struggle to localize hallucinated content, raising concerns about their reliability. Our qualitative analysis highlights their strengths and weaknesses and suggests two directions for improvement. Future work should explore more sophisticated techniques for incorporating context-aware uncertainty estimation and develop methods that adaptively propagate uncertainty across sentence positions to enhance hallucination localization.