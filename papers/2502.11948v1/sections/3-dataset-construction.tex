\subsection{Dataset Construction}

Curating an entity-level hallucination detection dataset is challenging, requiring annotators to segment sentences into meaningful entities and verify the factual consistency of each entity against reliable sources. This process is time-intensive, requires domain expertise, and is prone to subjectivity~\citep{cao-wang-2021-cliff}. To address these challenges, we first develop a data generation pipeline that maps atomic facts from {FActScore}~\citep{min-etal-2023-factscore} back to the original generated text. 


\paragraph{Entity segmentation and labeling.} To construct our dataset~\dataset\footnote{\dataset will be publicly released under the MIT license.}, we leverage biographies generated by ChatGPT~\citep{openai2025chatgpt}. Each data point consists of a name, a ChatGPT-generated biography, and a list of atomic facts labeled as either \texttt{True} or \texttt{False}. As illustrated in Figure~\ref{fig:data_generation}, each atomic fact is a short sentence that conveys a single piece of information. Since atomic facts decompose a sentence into verifiable units, they provide a structured reference for identifying hallucinated entities. 

To derive entity-level labels, we first segment the original text into meaningful units rather than individual words. For instance, ``strategic thinking'' is treated as a single entity rather than two separate words. We call such meaningful units \emph{entities}. Given that \textsc{FActScore} decomposes multiple-fact sentences into independent atomic facts, we use these fact-level annotations to label entities. For example, in the sentence ``\emph{He was born on March 9, 1941, in Ramos Mejia},'' \textsc{FActScore} produces two atomic facts:
\vspace{-0.2cm}
\begin{itemize}
    \item ``\emph{He was born on March 9, 1941}.'' (\texttt{True})
    \vspace{-0.3cm}
    \item ``\emph{He was born in Ramos Mejia}.''(\texttt{False})
\end{itemize}
\vspace{-0.2cm}
By aligning these atomic facts with the original text, we label ``March 9, 1941'' as non-hallucinated and ``Ramos Mejia'' as hallucinated. Neutral words like ``he'' and ``on'' are considered as non-hallucinated while labeling. To scale this process efficiently, we automatically identify and label these entities by instructing GPT-4o~\citep{openai2024gpt4ocard} with a few-shot prompt. Specifically, we manually annotate two examples, each containing an LLM-generated biography, a list of atomic facts, and a corresponding entity-level annotation list. The prompt provides a detailed description of our segmentation method, along with annotated examples. GPT-4o then generates entity labels, which we manually verify and refine to ensure correctness. Further details on the prompt design and annotation process are provided in \textbf{Appendix~\ref{ap:data}}.

\input{figures/data_generation}









\input{figures/linguistic_stat}


\subsection{Data Analysis}\label{sec:data_analysis}

\paragraph{Data statistics.}

\dataset comprises 157 instances containing a total of 18,785 entities, with 5,452 unique entities. Each entity averages 1.63 words in length. On average, each instance contains 120 entities, with 15\% labeled as hallucinated, and 85\% as non-hallucinated across the corpus. 


\paragraph{Linguistic feature analysis.}
We analyze the relationship between the entity-level hallucination labels and linguistic features, \eg, part-of-speech (POS) and named entities recognition (NER) tags. Specifically, we identify these tags for each word with Spacy~\citep{spacy2} and count their occurrence in hallucinated and non-hallucinated entities. The results for each of the part-of-speech (POS) and 
 named entities recognition (NER) tags are shown in Figure~\ref{fig:linguistic_stat}.


Analysis of POS tags reveals significant patterns in the distribution between hallucinated and non-hallucinated content. Proper nouns (PROPN) constitute the most frequent category with 18.3\% occurrences, followed by nouns (NOUN, 17.5\%) and adpositions (ADP, 15.1\%). Among them, proper nouns and nouns exhibit high hallucination rates of 30.9\% and 33.6\%, respectively, while adpositions have a lower rate of 11.7\%. Moreover, although adjectives (ADJ, 7.1\%) and numbers (NUM, 4.4\%) are less common, they suffer from a high hallucination rate of 28.9\% and 36.2\%. 

Non-named entities, which comprise 73.8\% of total tokens, show a low hallucination rate of 18.2\%. In contrast, named entities---despite accounting for only one-third of the tokensâ€”exhibit nearly double the hallucination rate, often exceeding 30\%. Among these named entities, person names (PERSON) show the lowest hallucination rate of 13.4\%, likely because ChatGPT was prompted to generate biographies for specific individuals.

Beyond POS and NER tagging, hallucination rates vary by position in sentences. The first six words of sentences have a low hallucination rate (9\%), but this significantly increases in the middle in the middle (25\%) and peaks at the last six words (36\%). This comprehensive analysis reveals systematic patterns in hallucination across linguistic features and entity types, providing crucial insights into the reliability of different categories of generated content. In Section~\ref{sec:qualitative_analysis}, we see the connections between these linguistic features and the performance of uncertainty-based hallucination detection approaches.
