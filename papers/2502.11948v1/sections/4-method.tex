
Given the entity-level hallucination dataset we constructed, a key question arises: \textbf{\emph{Can uncertainty scores effectively detect these hallucinated entities}}? In this section, we comprehensively explore uncertainty-based methods, all of which measure uncertainty at the token level. These token-level scores can be conveniently aggregated to the entity level, allowing for a systematic evaluation of their effectiveness in identifying hallucinated entities.


\paragraph{Notations and definitions.}
Let $\mathcal{V}$ be a vocabulary space, $x=(v_1, v_2,\dots,v_T)$ be a sentence of length $T$ consisting of tokens $v_i\in\mathcal{V}$. The token-level hallucination scores are denoted as $y^t=(y^t_1,y^t_2,\dots,y^t_T)$, where $y^t_i\in\mathbb{R}$.
An entity in $x$ is represented as $e_k=(v_i, v_{i+1},..,v_j)$, where $i$ and $j$
are the start and end indices of the entity's tokens, satisfying $i<j\leq T$. The sentence $x$ can then be rewritten as $x=(e_1, e_2,\dots,e_K)$, where $K$ is the number of entities. The entity-level hallucination labels for $x$ are defined as $l=(l_1,l_2,\dots,l_K)$, where $l_k\in\{0, 1\}$ indicates whether $e_k$ is hallucinated. The entity-level scores are computed as $y^e=(y^e_1,y^e_2,\dots,y^e_K)$, where $y^e_k:=\frac{1}{e_{k,1}-e_{k,0} + 1}\sum_{i=e_{k,0}}^{e_{k,1}}y^t_i$, 
which aggregates token-level scores to the entity level and $e_{k,0}$ and $e_{k,1}$ are the start and end indices of entity $e_k$. We introduce five methods below to calculate the token-level uncertainty scores.

\vspace{-0.1cm}
\paragraph{Likelihood~\citep{guerreiro-etal-2023-looking}:} The score is based on the negative log-likelihood of the $i$-th generated token: $$y^t_i:=-\log p(x_i|x_{<i}).$$

\vspace{-0.1cm}
\paragraph{Entropy~\citep{malinin2021uncertainty}:} The score is the entropy of the token probability distribution at position $i$:$$y^t_i:=-\sum_{v\in\mathcal{V}}p(v|x_{<i})\log p(v|x_{<i}).$$

\vspace{-0.1cm}
\paragraph{Claim-Conditioned Probability (CCP)~\citep{fadeeva-etal-2024-fact}:} This method adjusts likelihood based on semantic equivalence using a natural language inference (NLI) model:
 $$y^t_i:=-\log\frac{\sum_{k:\texttt{NLI}(x_i^k,x_i)=\text{`e'}}p(x_i^k|x_{<i})}{\sum_{k:\texttt{NLI}(x_i^k,x_i)\in\{\text{`e'}, \text{`c'}\}}p(x_i^k|x_{<i})},$$ where $x_i^k$ is the $k$-th alternative of the $i$-th generated token, and \texttt{NLI} determines whether concatenating $x_i^k$ with the preceding context entails (\text{`e'}) or contradicts (\text{`c'}) the original token. In our experiment, we use top 10 alternatives and use  DeBERTa-base~\citep{he2021deberta} as the NLI model.

\vspace{-0.1cm}
\paragraph{Shifting Attention to Relevance (SAR)~\citep{duan-etal-2024-shifting}:} This method weights negative log-likelihood by semantic importance:
$$y^t_i:=-\log p(x_i|x_{<i})\tilde{R_T}(x_i, x),$$ where $\tilde{R_T}$ is $1-$ cosine similarity between the sentence embedding of $x$ and $x\backslash \{x_i\}$. Following \citet{duan-etal-2024-shifting}, we use  SentenceBERT~\citep{reimers-gurevych-2019-sentence} with RoBERTa-large~\citep{liu2019robertarobustlyoptimizedbert} for embedding extraction.

\input{tables/evaluation}

\vspace{-0.1cm}
\paragraph{Focus~\citep{zhang-etal-2023-enhancing-uncertainty}:}
   This method refines log-likelihood and entropy using keyword selection, hallucination propagation, and probability correction:
    $$y^t_i:=\mathbb{I}(x_i\in\mathcal{K})\cdot(h_i+\gamma p_i),$$
    where $\mathcal{K}$ is keyword set identified by Spacy~\citep{spacy2}. $h_i$ is the sum of negative log-likelihood and entropy of $x_i$,
    \begin{align*}
        h_i:=&-\log\hat{p}(x_i|x_{<i})\\
        &+2^{-\sum_{v\in \mathcal{V}}\hat{p}(v|x_{<i})\log_2\hat{p}(v|x_{<i})}.
    \end{align*}
    Here, $\hat{p}(x_i|x_{<i})=\frac{p(x_i|x_{<i})\texttt{idf}(x_i)}{\sum_{v\in\mathcal{V}}p(v|x_{<i})\texttt{idf}(v)}$ is the token probability adjusted by inverse document frequency (IDF), 
    and $p_i$ is the hallucination score propagated from previous tokens,
    $$p_i:=\sum_{j=0}^{i-1}\frac{\texttt{att}_{i,j}}{\sum_{k=0}^{i-1}\texttt{att}_{i,k}}y^t_j,$$ where $\texttt{att}_{i,j}$ is the attention weight between $x_i$ and $x_j$ after max-pooling for all the layers and attention heads. Following \citet{zhang-etal-2023-enhancing-uncertainty}, the token IDF is calculated based on 1M documents sampled from RedPajama dataset~\citep{weber2024redpajama}, and the hyperparameter $\gamma$ for $p_i$ is set to be 0.9. 

Besides these five approaches, we acknowledge that other uncertainty-based hallucination detection approaches exist, such as Semantic Entropy~\citep{kuhn2023semantic}, Verbalized Uncertainty~\citep{kadavath2022languagemodelsmostlyknow}, Lexical Similarity~\citep{fomicheva-etal-2020-unsupervised}, EigValLaplacian~\citep{lin2024generating}, and HaloScope~\citep{du2024haloscope}. However, since these approaches do not produce token-level scores, they are not applicable to our study on detecting hallucinated entites.