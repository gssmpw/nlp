How can we trust the facts generated by large language models (LLMs) when even a single hallucinated entity can distort an entire narrative? While LLMs have revolutionized text generation in various domains, from summarization to scientific writing~\citep{Liang2024MappingTI}, their tendency to produce hallucinations---factually incorrect or unsupported content---remains a critical challenge~\citep{Xu2024HallucinationII}.
This issue is particularly concerning in high-stakes applications, such as medical diagnostics~\citep{Chen2024EvaluatingLL}, legal document drafting~\citep{lin2024legaldocumentsdraftingfinetuned}, and news generation~\citep{odabaşı2025unravelingcapabilitieslanguagemodels}, where inaccurate information can cause harm to individuals and erosion of public trust. Detecting hallucinations is therefore a critical step toward ensuring the responsible deployment of LLMs.

Various approaches have been proposed to detect hallucinations~\citep{Luo2024HallucinationDA}, with uncertainty-based methods emerging as a promising direction~\citep{Zhang2023EnhancingUH}. 
However, current uncertainty-based hallucination detection approaches mainly operate at the sentence or paragraph level, classifying the entire generation as either hallucinated or correct. 
While this provides a coarse-grained assessment of factuality, it lacks the granularity needed to pinpoint which specific spans or entities contribute to hallucination. This limitation is particularly problematic for long-form text,  where both accurate and hallucinated information frequently coexist. For example, a generated response about a historical event might correctly state the date but fabricate details about the individuals involved, necessitating finer-grained detection. 


To address these limitations, we present a first systematic exploration of \emph{\textbf{entity-level hallucination detection}} by introducing a benchmark dataset, extensively evaluating uncertainty-based detection methods on this benchmark, and analyzing their strengths and limitations in identifying hallucinated entities. Specifically, we begin by constructing a benchmark for entity-level hallucination detection, \dataset, which fills in the critical blank for the field. Constructing such a dataset is challenging due to the labor-intensive nature of entity-level annotation, which requires annotators to segment meaningful entities and verify their factuality against reliable sources one by one. To overcome this, we develop a systematic pipeline that maps atomic facts from model-generated text to entity-level annotations, enabling structured hallucination detection at a finer granularity. \dataset encompasses 18,785 annotated entities, and provides a foundation for evaluating hallucination detection methods with greater interpretability and precision.

Building on \dataset, we comprehensively evaluate the reliability of token-level uncertainty measurements in detecting hallucinated entities and their potential for localizing hallucinations within the generated text. 
Our evaluation broadly includes standard uncertainty estimators, such as token-level likelihood~\citep{guerreiro-etal-2023-looking} and entropy scores~\citep{malinin2021uncertainty}, as well as more advanced context-aware approaches that refine uncertainty estimation~\citep{fadeeva-etal-2024-fact, duan-etal-2024-shifting, zhang-etal-2023-enhancing-uncertainty}. By aggregating token-level uncertainty to the entity level, we assess whether these methods can accurately distinguish hallucinated entities from factual ones. {We experiment with 17 modern LLMs across different model families and capacities. The results reveal that methods solely relying on individual token probabilities (e.g., likelihood and entropy) tend to over-predict hallucinations, making them less reliable. In contrast, context-aware approaches~\citep{duan-etal-2024-shifting, zhang-etal-2023-enhancing-uncertainty}  demonstrate better overall performance in entity-level hallucination detection.
Additionally, model family and size have a limited impact on performance, compared to the choice of uncertainty estimation method, emphasizing the need for improved uncertainty modeling. 

Through in-depth qualitative analysis, we further identify relationships between hallucination tendencies and linguistic properties, such as sentence positions and entity types. We found that calibrating uncertainty score with contextual information in the best-performing method~\cite{zhang-etal-2023-enhancing-uncertainty} helps reduce over-confidence tendencies in later sentence positions but can unintentionally penalize non-hallucinated content. 
We also found that some uncertainty scores can frequently assign high uncertainty to informative content like named entities. These observations highlight  critical areas for future research, including better modeling of contextual dependencies to maintain balanced precision-recall trade-offs.} Our key contributions are summarized as follows:




\begin{enumerate}[nosep]
    \item We propose an entity-level hallucination detection dataset, \dataset, which contains 18,785 annotated entities for ChatGPT-generated biographies.
    \item We comprehensively evaluate uncertainty-based hallucination detection approaches {across 17 LLMs} on our proposed dataset.
    \item We conduct an in-depth analysis to identify the strengths and weaknesses of current uncertainty-based approaches, and provide insight to design better uncertainty scores.
\end{enumerate}

