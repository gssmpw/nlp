\section{Details of Dataset Construction}\label{ap:data}

\paragraph{Selection of data.}

From the \textsc{FActScore} dataset, we select the set of biographies generated by ChatGPT to construct our entity-level hallucination detection dataset. The set of ChatGPT-generated biographies contains 183 samples. We filter out those ChatGPT refuses to answer and end up with 157 instances.

\paragraph{Data labeling process.}

For each sample, \textsc{FActScore} provides a list of atomic factsâ€”short sentences conveying single pieces of information. These facts are labeled as \texttt{Supported}, \texttt{Not-supported}, or \texttt{Irrelevant}, where \texttt{Irrelevant} means the fact is unrelated to the prompt (\ie a person's name), and \texttt{Supported} and \texttt{Not-supported} indicate whether the fact is supported by the person's Wikipedia page. Since only 8.3\% of facts are labeled as \texttt{Irrelevant}, and most are related to \texttt{Not-supported} facts, we simplify the entity-level labeling process by merging both as \texttt{False}, treating only \texttt{Supported} facts as \texttt{True}.

To assign entity-level labels, we first tokenize the biography into individual words. We then use the list of atomic facts to group words into meaningful units (entities) and assign labels based on fact types. Specifically, for atomic facts that share a similar sentence structure (\eg, \textit{``He was born on Mach 9, 1941.''} (\texttt{True}) and \textit{``He was born in Ramos Mejia.''} (\texttt{False})), we label differing entities first---assigning \texttt{True} to ``Mach 9, 1941'' and \texttt{False} to ``Ramos Mejia.''. For those entities that are the same across atomic facts (\eg, ``was born'') or are neutral (\eg, ``he,'' ``in,'' and ``on''), we label them as \texttt{True}. In cases where no similar atomic fact exists, we identify the most informative entities in the sentence, label them based on the atomic fact, and treat the remaining entities as \texttt{True}.

\paragraph{GPT-4o prompt for data labeling.}

To scale the labeling process, we use GPT-4o to automatically identify and label entities with a few-shot prompt, as shown in Table~\ref{tb:gpt-4-prompt}. The system prompt includes detailed instructions on the labeling process, along with two manually created examples. In the user prompt, we maintain the same structured format used in the examples, inputting the biography and the corresponding list of atomic facts.

\input{figures/hallucination_dist}



\section{Details of Experimental Result}\label{ap:experiment}

\paragraph{Computational resources.}
We conducted all experiments on a server equipped with eight Nvidia A100 GPUs. Depending on the model size, each LLM utilized between one to three GPUs. The time required to compute uncertainty scores across the entire dataset varied from 30 seconds to 5 minutes per approach and model, depending on the model size and the complexity of the chosen method.

\paragraph{Performance across LLM families.}

Table~\ref{tb:auroc_17models}, \ref{tb:auprc_17models}, \ref{tb:f1_opt_17models}, \ref{tb:precission_opt_17models}, and \ref{tb:recall_opt_17models} present the performance of five uncertainty scores across 17 LLMs. The results indicate that \texttt{microsoft/phi-2} consistently achieves the highest performance across most scores and evaluation metrics, being the only model that avoids over-predicting hallucinations when using CCP. Additionally, models from Mistral AI (\texttt{mistralai/Mistral-7B-v0.3} and \texttt{mistralai/Mixtral-8x22B-v0.1}) perform best when using Focus. Notably, phi-2 (2.7B parameters) and Mistral-7B are relatively small models, suggesting that a model's size does not strongly correlate with its ability to detect hallucinations based on token probabilities. The findings also reveal that while some models outperform others, the performance variations within the same uncertainty score are smaller than those across different scores, emphasizing the need for improved uncertainty estimation methods for hallucination detection.





\paragraph{Statistics of hallucination rate.}

Figure~\ref{fig:hallucination_dist} shows the distribution of entity-level hallucination rate---the proportion of hallucinated entities in each generation. The results indicate that most biographies generated by ChatGPT have a hallucination rate below 25\%. Additionally, generations with hallucination rates below 10\% and those between 10\% to 20\% occur at similar frequencies. Based on this observation, we categorize the data into three groups ($<10\%$, 10-20\%, $>20\%$) to examine how hallucination rates impact detection performance. Table~\ref{tb:statistic_hallu_rate} summarizes the group statistics, showing that each group contains a similar amount of data.

\input{tables/statistics_hallu_rate}



\paragraph{FPR/FNR analysis on POS tags.} Figure~\ref{fig:error_analysis_pos} shows the FPR/FNR of Likelihood, SAR, and Focus across POS tags. Consistent with Figure~\ref{fig:error_analysis}, Likelihood exhibits higher FPR and lower FNR across all tags, while SAR demonstrates lower FPR but higher FNR. In contrast, Focus shows varying patterns depending on the tag type. For proper nouns, nouns, and numbers---tags often associated with named entities---Focus has a higher FPR and lower FNR, similar to the trend observed in Figure~\ref{fig:error_analysis}. However, for verbs, auxiliaries, and adverbs, Focus exhibits a lower FPR but a higher FNR. This highlights a limitation of Focus: by concentrating primarily on named entities, it tends to overlook hallucinations in other types of tokens.



\input{figures/error_analysis_pos}

\input{tables/prompt}

\input{tables/auroc_17models}
\input{tables/auprc_17models}
\input{tables/f1_opt_17models}
\input{tables/precission_opt_17models}
\input{tables/recall_opt_17models}
