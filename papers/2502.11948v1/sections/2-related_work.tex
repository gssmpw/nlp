\paragraph{Uncertainty-based hallucination detection methods.}
Various approaches have been proposed to detect hallucinated content in LLMs generation.
Unlike other methods that require external knowledge sources for fact-checking~\citep{gou2024critic, chen-etal-2024-complex, min-etal-2023-factscore, huo2023retrieving}, uncertainty-based approaches are reference-free and rely only on LLM internal states or behaviors to determine hallucination~\citep{10.1145/3703155}. 
For instance, sampling-based approaches generate multiple responses and measure the diversity in meaning among them~\citep{fomicheva-etal-2020-unsupervised, kuhn2023semantic, lin2024generating}, while density-based approaches approximate the training data distribution and provide probabilities or unnormalized scores to assess how likely a generated response belongs to the distribution~\citep{yoo-etal-2022-detection, ren2023outofdistribution, vazhentsev-etal-2023-hybrid}.

In this paper, we focus on uncertainty quantification methods that rely on token-level likelihood or entropy~\citep{guerreiro-etal-2023-looking, malinin2021uncertainty}. 
Recent works have explored refining likelihood estimation by incorporating semantic relationships or reweighting token importance. For instance, Claim-Conditioned Probability (CCP)~\citep{fadeeva-etal-2024-fact} was introduced to recalculate likelihood according to semantical equivalence; while \citet{zhang-etal-2023-enhancing-uncertainty} and \citet{duan-etal-2024-shifting} adjust token weights to better convey meaning in uncertainty aggregation. \emph{Although these approaches leverage token-level information, they are typically evaluated at the sentence level, raising questions about their reliability}. To address this, we conduct a comprehensive analysis of entity-level hallucination detection for finer-grained performance insights.


\paragraph{Fine-grained hallucination detection benchmark.}

Most hallucination detection benchmarks are in sentence or paragraph level. For example, CoQA~\citep{reddy-etal-2019-coqa}, TriviaQA~\citep{joshi-etal-2017-triviaqa}, TruthfulQA~\citep{lin-etal-2022-truthfulqa}, and HaluEval~\citep{li-etal-2023-halueval}. These benchmarks classify each generated response as either hallucinated or correct. However, instance-level detection cannot pinpoint specific hallucinated content, which is crucial for correcting misinformation~\citep{cattan2024localizingfactualinconsistenciesattributable}. This limitation becomes particularly problematic in long-form text, where a single response often combines supported and unsupported information, making binary quality judgments inadequate~\citep{min-etal-2023-factscore}.

To address these challenges, recent works have advanced benchmarks for more granular hallucination detection. For example, \citet{min-etal-2023-factscore} introduced \textsc{FActScore}, which decomposes LLM-generated text into atomic facts---short sentences conveying a single piece of information---for more precise evaluation. In parallel, \citet{cattan2024localizingfactualinconsistenciesattributable} introduced \textsc{QASemConsistency}, decomposing LLM generated text with QA-SRL, a semantic formalism, to form simple QA pairs, where each QA pair represent one verifiable fact. \emph{However, these methods do not enable entity-level hallucination detection, as they lack explicit entity-level labeling (hallucinated or not) in the original generated text}.  
Beyond decomposition-based approaches, datasets like \textsc{HaDes}~\citep{liu-etal-2022-token} and CLIFF~\citep{cao-wang-2021-cliff} create token-level hallucinated content by perturbing human-written text, allowing token-level annotation on the same text. These perturbed hallucinated content, however, could be unrealistic, biased, and overly synthetic due to the limitations of models they used to perturb words. 
To bridge this gap, we create a new dataset with entity-level hallucination labels on the same LLMs generated text. This allows us to evaluate uncertainty-based hallucination detection approaches on a finer-grained level and analyze their reliability.




