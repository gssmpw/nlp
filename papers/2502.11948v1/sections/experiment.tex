\subsection{Experimental Setup}




\paragraph{Models.}

To understand the impact of model family and capacity on entity-level hallucination detection, we experiment with 17 diverse LLMs, including \textbf{Llama3}-\{8B, 70B\}~\citep{grattafiori2024llama3herdmodels}, \textbf{Llama3.1}-8B, \textbf{Llama3.2}-3B, \textbf{Aquila2}-\{7B, 34B\}~\citep{zhang2024aquila2technicalreport}, \textbf{InternLM2}-\{7B, 20B\}~\citep{cai2024internlm2technicalreport}, \textbf{Qwen2.5}-\{7B, 32B\}~\citep{qwen2025qwen25technicalreport}, \textbf{Yi}-\{9B, 34B\}~\citep{ai2024yiopenfoundationmodels}, \textbf{phi-2}~\citep{gunasekar2023textbooksneed}, \textbf{Mistral}-7B~\citep{jiang2023mistral7b}, \textbf{Mixtral}-8x22B~\citep{jiang2024mixtralexperts}, and \textbf{Gemma2}-\{9B, 27B\}~\citep{gemmateam2024gemma2improvingopen}.


\input{figures/experimental_result}

\paragraph{Evaluation Metrics.}

Entity-level hallucination detection can be formulated as a binary classification task. To evaluate performance, we use (1) \textbf{AUPRC} and (2) \textbf{AUROC}, which assess the relationship between entity-level hallucination labels $l$ and scores $y^e$. AUPRC captures precision-recall trade-offs, while AUROC evaluates true and false positive rates. Unlike AUROC, AUPRC disregards true negatives, emphasizing false positive reduction---a key advantage for hallucination detection, where true negatives often involve less informative entities like prepositions and conjunctions. 
We complement these metrics by also reporting (3) $\mathbf{F1}_\mathbf{Opt}$, (4) $\mathbf{Precision}_\mathbf{Opt}$, and (5) $\mathbf{Recall}_\mathbf{Opt}$, where $\mathrm{F1}_\mathrm{Opt}$ is the optimal F1 score among all possible threshold and $\mathrm{Precision}_\mathrm{Opt}$, and $\mathrm{Recall}_\mathrm{Opt}$ are corresponding Precision and Recall values.


\subsection{Experimental Results}\label{sec:result}



\paragraph{How do different uncertainty scores perform on entity-level hallucination detection?}

Table~\ref{tb:evaluation} presents the evaluation results for five uncertainty-based hallucination detection approaches using Llama3-8B. 
Likelihood, Entropy, and CCP exhibit low $\mathrm{Precision}_\mathrm{Opt}$ ($\approx$ overall hallucination rate) but achieve high $\mathrm{Recall}_\mathrm{Opt}$. This pattern suggests these methods over-predict hallucinations, making them less suitable for reliable detection. Their focus on individual token probabilities rather than contextual roles likely contributes to this limitation, indicating that \emph{hallucination detection is inherently context-dependent and requires uncertainty scores calibrated with contextual information.} 

SAR and Focus, which incorporate context information, show better overall performance. However, their lower $\mathrm{Recall}_\mathrm{Opt}$ indicates that the current methods for modeling context remain suboptimal, failing to capture some hallucinated content. These findings highlight the challenges in entity-level hallucination detection and the need for more advanced approaches that better integrate contextual information while achieving a balanced trade-off between precision and recall. 



\paragraph{How do different LLM families and capacity impact performance?}

Figure~\ref{fig:eval_model_family} summarizes the performance variation across all 17 different LLMs. The results indicate that while AUROC, AUPRC, and $\mathrm{F1}_\mathrm{Opt}$ scores vary across model families, \emph{the method used to compute uncertainty scores has a more significant impact on performance}. Notably, Focus consistently achieves the highest performance across all model families. Further evaluation details can be found in Appendix~\ref{ap:experiment}.

\input{tables/qualitative_analysis}

Figure~\ref{fig:eval_model_size} presents the performance changes across different model sizes within six families: Llama3, Aquila2, InternLM2, Qwen2.5, Yi, and Gemma2, each comprising two size variants. The results reveal that, in most cases, using a larger model does not significantly enhance performance. The only exception is using Gemma on Focus, where the AUROC score improves by 0.12 between the 27B and 9B versions. Performance improvements for other model families and approaches remain marginal, typically below 0.01. These findings suggest that \emph{a larger LLM may not reflect its better capability of determining hallucination on its token probabilities.}




\paragraph{How does performance vary across different hallucination levels?}

We categorize \dataset into three groups based on the proportion of hallucinated entities in each generation (See Table~\ref{tb:statistic_hallu_rate} in Appendix~\ref{ap:experiment} for details). Figure~\ref{fig:eval_hallucination_rate} shows the $\mathrm{Precision}_\mathrm{Opt}$, $\mathrm{Recall}_\mathrm{Opt}$, and $\mathrm{F1}_\mathrm{Opt}$ scores across three groups. The results reveal that all methods struggle to detect hallucinated content when the hallucination rate is low, with $\mathrm{F1}_\mathrm{Opt}$ scores around 0.2. 
{Entropy and CCP exhibit a steep increase in $\mathrm{Recall}_\mathrm{Opt}$ compared to$\mathrm{Precision}_\mathrm{Opt}$ as the hallucination rate increases, suggesting their tendency to over-predict hallucinations, particularly in a high-hallucination scenario. In contrast, Focus achieves a small difference between the $\mathrm{Recall}_\mathrm{Opt}$ and $\mathrm{Precision}_\mathrm{Opt}$ when the hallucination rate is high, demonstrating its ability to balance precision-recall trade-offs while also highlighting the challenge of detecting sparse hallucination.}



\subsection{In-depth Analysis}\label{sec:qualitative_analysis}

To better understand the strengths and limitations of uncertainty scores for detecting hallucinated entities, we analyze cases where (1) all scores failed or misidentified hallucinations, and (2) scores varied in performance. We classify entities using thresholds for $\mathrm{F1}_\mathrm{Opt}$ and categorize false positives/negatives by POS, NER tags, and sentence positions (first, middle, or last six words). We then identify tags and positions where approaches excel or falter, visualizing samples with color-coded uncertainty scores to uncover patterns behind detection discrepancies (See Table~\ref{tb:qualitative_analysis}). Figure~\ref{fig:error_analysis} shows the FPR and FNR across NER tags and sentence positions (The result across POS tags is in Appendix~\ref{ap:experiment}). Our analysis focuses on Likelihood, SAR, and Focus, as SAR and Focus demonstrated the most effective performance in Section~\ref{sec:result}, and Likelihood serves as a straightforward baseline for comparison.

\input{figures/error_analysis}

\vspace{-0.1cm}
\paragraph{SAR under-predicts hallucinations due to unreliable token importance weighting.}
The left and middle plots of Figure~\ref{fig:error_analysis} show that SAR has the lowest FPR but the highest FNR across most tags, particularly for named entities, indicating a tendency to under-predict hallucinations. This occurs because SAR weights token importance based on sentence similarity without the token, which often remains unchanged even if the token is informative. The first case in Table~\ref{tb:qualitative_analysis} illustrates this: SAR assigns lighter shades to entities like the second ``Santa Cruz'' since removing either ``Santa'' or ``Cruz'' barely affects sentence similarity, despite the term's informativeness.



\vspace{-0.1cm}
\paragraph{The type-filter of Focus reduces FNR on name entities but sheds light on a bigger limitation of uncertainty-based hallucination score.}
The left and middle plots of Figure~\ref{fig:error_analysis} reveal that Focus performs differently for named and non-named entities. It achieves a low FNR but high FPR for named entities, and the opposite for non-named ones. This is because Focus filters for named entities based on POS and NER tags. While promising---since named entities often hallucinate (as shown in Figure~\ref{fig:linguistic_stat})---its high FPR suggests that its base score (the sum of Likelihood and Entropy) poorly distinguishes hallucinations, frequently assigning high uncertainty to named entities. The 2nd case in Table~\ref{tb:qualitative_analysis} illustrates this: Focus ignores function words like ``is'' and ``to,'' reducing FPR, but indiscriminately highlights named entities like ``American'' and ``A Bronx Tale,'' even when accurate.


\vspace{-0.1cm}
\paragraph{Uncertainty propagation of Focus alleviates the over-confidence nature of LLMs.}
The right plots in Figure~\ref{fig:error_analysis} show that LLMs are less confident when generating the first few words of a sentence and become over-confident as generation progresses, as indicated by a decrease in FPR and an increase in FNR for Likelihood. This contrasts with the typical distribution of hallucinations, which occur mostly in the middle and end of sentences (Section~\ref{sec:data_analysis}). Focus addresses this by propagating uncertainty scores based on attention, leading to a decrease of FNR over positions. However, its FPR increase over positions suggest that using attention scores to propagate uncertainty may wrongly penalize entities that are not over-confident. The 3rd case in Table~\ref{tb:qualitative_analysis} illustrates this: Likelihood assigns higher uncertainty to early words (\eg, ``Fernandinho began'') and lower scores to later words (\eg, ``Shakhtar Donetsk in 2005''), while Focus detects hallucinations at sentence ends by linking them to prior hallucinated content (\eg, ``Ukrainian club'').
