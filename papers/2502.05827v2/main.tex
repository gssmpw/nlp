%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{cc}
\setcctype{by}
\acmConference[WWW Companion '25]{Companion Proceedings of the ACM Web
Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2025 (WWW
Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3701716.3715456}
\acmISBN{979-8-4007-1331-6/25/04}



\usepackage{multirow} % multirow 사용을 위해 추가
\usepackage{booktabs} % 표 디자인을 위한 추가 패키지
% \usepackage{caption} % 표 캡션 스타일을 위한 추가 패키지
\usepackage{enumitem}
\usepackage{tabularray}
% \usepackage{longtable}
% \usepackage{diagbox} 
% \usepackage[table]{xcolor}
%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}
%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\settopmatter{authorsperrow=4}

\newcommand{\method}{\textsc{HyGEN}}
\newcommand{\codelink}{https://github.com/ssong915/HyGEN}

\definecolor{bleudefrance}{rgb}{0.19, 0.55, 0.91}
\definecolor{mygreen}{rgb}{0.0, 0.5, 0.0}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[{\method}: Regularizing Negative Hyperedge Generation for Accurate Hyperedge Prediction]{{\method}: Regularizing Negative Hyperedge Generation \\for Accurate Hyperedge Prediction}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Song Kyung Yu}
\authornotemark[1]
\email{ssong915@hanyang.ac.kr}
\affiliation{%
  \institution{Hanyang University}
  \city{Seoul}
  \country{Korea}
}

\author{Da Eun Lee}
\authornote{Both authors contributed equally to this research.}
\email{ddanable@hanyang.ac.kr}
\affiliation{%
  \institution{Hanyang University}
  \city{Seoul}
  \country{Korea}
}


\author{Yunyong Ko}
\email{yyko@cau.ac.kr}
\affiliation{%
  \institution{Chung-Ang University}
  \city{Seoul}
  \country{Korea}
}

\author{Sang-Wook Kim}
\authornote{Corresponding author.}
\email{wook@hanyang.ac.kr}
\affiliation{%
  \institution{Hanyang University}
  \city{Seoul}
  \country{Korea}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Song Kyung Yu, Da Eun Lee, Yunyong Ko, and Sang-Wook Kim}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  \textit{Hyperedge prediction} is a fundamental task to predict future high-order relations based on the observed network structure.
Existing hyperedge prediction methods, however, suffer from the data sparsity problem. 
To alleviate this problem, 
negative sampling methods can be used, which leverage non-existing hyperedges as contrastive information for model training. 
However, the following important challenges have been rarely studied: 
\textbf{(C1)} \textit{lack of guidance for generating negatives} and \textbf{(C2)} \textit{possibility of producing false negatives}. 
To address them, we propose a novel hyperedge prediction method, \textbf{{\method}}, that employs (1) a negative hyperedge generator that employs positive hyperedges as a guidance to generate more realistic ones and (2) a regularization term that prevents the generated hyperedges from being false negatives.
Extensive experiments on six real-world hypergraphs reveal that {\method} consistently outperforms four state-of-the-art hyperedge prediction methods. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
    <concept_id>10010147.10010257.10010258.10010261.10010276</concept_id>
       <concept_desc>Computing methodologies~Adversarial learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010321.10010337</concept_id>
       <concept_desc>Computing methodologies~Regularization</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
       <concept>
       <concept_id>10010147.10010341.10010346.10010348</concept_id>
       <concept_desc>Computing methodologies~Network science</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Adversarial learning}
\ccsdesc[500]{Computing methodologies~Regularization}
\ccsdesc[500]{Computing methodologies~Network science}



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Hyperdge prediction, Adversarial learning, Negative hyperedge generation, Regularization}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\section{Introduction}\label{sec-intro}

% hypergraphs
In real-world networks, high-order relations (i.e., \textit{group-wise relations}) are prevalent~\cite{ko2023cash,dong2020hnhn,feng2019hgnn}, 
such as (i) a research paper co-authored by a group of researchers
and (ii) a chemical reaction co-induced by a group of proteins. 
% Modeling such group-wise relations by an ordinary graph could lead to unexpected information loss.
A \textit{hypergraph}, a generalized data structure, is capable of modeling such group-wise relations as a \textit{hyperedge} without any information loss.
Due to its powerful expressiveness,
hypergraph-based network learning~\cite{feng2019hgnn,yang2022hyperle} has been widely studied and shown to outperform graph-based methods in various downstream tasks, including node classification~\cite{dong2020hnhn,choe2023whatsnet}, node ranking~\cite{zhang2021ranking, yu2021social}, and link prediction~\cite{yadati2020nhp,vaida2019link}. 





\begin{figure}[t]
\centering
\includegraphics[width=0.98\linewidth]{figures/intro.pdf}
\vspace{-3mm}
\caption{Process of hyperedge prediction.}
\vspace{-7mm}
\label{fig:prediction-process}
\end{figure}


% hyperedge prediction
\textit{Hyperedge prediction} (i.e., link prediction on hypergraphs) is a fundamental task in many real-world applications, such as recommender systems~\cite{zhang2021ranking} and social network analysis~\cite{yu2021social, choe2023whatsnet};
it predicts future high-order relations (i.e., hyperedges) based on an observed hypergraph structure.
A general approach to hyperedge prediction is two-fold~\cite{hwang2022ahp,ko2023cash} (see Figure~\ref{fig:prediction-process}: 
(1) (\textbf{hypergraph encoding}) the embeddings of nodes are produced by hypergraph neural networks~\cite{feng2019hgnn,dong2020hnhn} and (2) (\textbf{hyperedge candidate scoring}) the embeddings of nodes in each hyperedge candidate are \textit{aggregated} and fed into a predictor to determine whether the candidate is real.



% NS in hyperedge prediction
In real-world networks, however, high-order relations are often extremely \textit{sparse}~\cite{patil2020heuristic} (i.e., $2^{|V|} \gg |E|$, where $V$ and $E$ are the sets of nodes and hyperedges, respectively).
Such a \textit{data sparsity} problem is the fundamental cause of low accuracy in hyperedge prediction.
To address this problem, negative sampling (NS) can be used~\cite{yadati2020nhp, zhang2019hyperSAGNN, hwang2022ahp}, 
utilizing non-existing hyperedges as contrastive information for model training.
Specifically, the model is trained so that positive examples get higher scores while negative examples get lower scores, 
which enhances the distinguishing ability of the model.
Thus, it is crucial to carefully choose negative hyperedges to maximize the effect of negative sampling.


% challenge
However, sampling `good' negative hyperedges is challenging in the context of hyperedge prediction
since there exist too many possible negative hyperedges (i.e., $2^{|V|}-|E|$).
Although existing hyperedge prediction methods~\cite{zhang2019hyperSAGNN,yadati2020nhp,yoon2020expansion}, enhanced by negative sampling methods, have achieved breakthroughs in many fields,
they focus primarily on hypergraph encoding
while employing simple heuristic-based negative sampling methods~\cite{patil2020heuristic}.
Thus, negative sampling for hyperedge prediction is still rarely explored.



Although one recent work~\cite{hwang2022ahp} proposed an adversarial-training-based hyperedge prediction method (AHP) that leverages model-generated negative hyperedges useful for model training, it has overlooked the following two important challenges:


\vspace{0.5mm}
\noindent
\textbf{(C1) Lack of guidance for generating negatives}. 
Only a random noise signal is used as the input of the hyperedge generator in the previous method.
Thus, we posit that it may fail to effectively reflect the characteristics of positive hyperedges into generated hyperedges, 
especially in the initial stage of the training,
which could lead to inefficient and unstable training.


\vspace{0.5mm}
\noindent
\textbf{(C2) Possibility of producing false negatives}.
AHP generates negative hyperedges by using a generative adversarial network (GAN),
which aims to generate negative hyperedges similar as much as possible to positive hyperedges (i.e., copying the exact distribution of the original hyperedges).
Without any regularization for hyperedge generation, however, it might be possible to generate hyperedges too similar to positives, which would be potential positive hyperedges.


% Proposed method
From this motivation, in this paper, 
we propose a novel adversarial-training-based method, \textbf{{\method}} which stands for regularizing negative \textbf{\underline{\textsc{Hy}}}peredge \textbf{\underline{\textsc{GEN}}}eration for accurate hyperedge prediction.
{\method} employs (1) a \textit{positive-guided negative hyperedge generator} that leverages positive hyperedges as guidance to generate more-realistic negative hyperedges for \textbf{(C1)} and (2) a \textit{regularization} term to prevent the generated hyperedges from being too similar to positive hyperedges for \textbf{(C2)}.
  

\vspace{1mm}
\noindent
\textbf{Contributions.} The main contributions of this work are as follows.
\begin{itemize}[leftmargin=10pt]
    \item \textbf{Challenges}: We point out two important challenges of negative sampling in hyperedge prediction: 
    \textbf{(C1)} \textit{lack of guidance for generating negatives} and \textbf{(C2)} \textit{possibility of false negatives}.
    \item \textbf{Method}: We propose a novel hyperedge prediction method, \textbf{{\method}} that employs (1) a positive-guided negative hyperedge generator for \textbf{(C1)} and (2) a regularization term for \textbf{(C2)}.
    \item \textbf{Evaluation}: Via extensive experiments on six real-world hypergraphs, we verify the superiority of {\method} over four state-of-the-art hyperedge prediction methods. 
\end{itemize}
For reproducibility, we have released the code of {\method} and datasets at: \url{\codelink}.
% \vspace{-2.5mm}


\section{Related Works}\label{sec-related}
\textbf{Hyperedge prediction}.
There have been a number of works to study hyperedge prediction.
they solve the hyperedge prediction problem as a classification task~\cite{yoon2020expansion,zhang2019hyperSAGNN,yadati2020nhp,hwang2022ahp}.
Expansion~\cite{yoon2020expansion} models a hypergraph as multiple \textit{n}-projected graphs and applies a logistic regression model to the projected graphs for predicting future hyperedges 
HyperSAGNN~\cite{zhang2019hyperSAGNN} uses a self-attention-based graph neural networks (GNN) model to learn hyperedges of variable sizes. 
NHP~\cite{yadati2020nhp} employs hyperedge-aware GNN models to learn node embeddings in hypergraphs, using the max-min pooling to aggregate the embeddings of nodes within each hyperedge candidate for prediction. 
AHP~\cite{hwang2022ahp}, the state-of-the-art hyperedge prediction method, employs adversarial training to generate negative hyperedges for model training and uses max-min pooling for node aggregation. 


\vspace{1mm}
\noindent
\textbf{Negative hyperedge sampling}.
For enhancing the training of hyperedge prediction models, 
the following three heuristic-based methods for negative hyperedge sampling have been proposed~\cite{patil2020heuristic}:
(1) Sized NS (SNS) samples $n$ nodes uniformly at random;
(2) Motif NS (MNS) transforms a hypergraph into an ordinary graph via a clique-expansion and samples a $n$-connected component in the expanded graph;
and (3) Clique NS (CNS) selects a hyperedge $e$ and replaces one of its incident nodes $u\in e$ with a node $v\notin e$, which is linked to all the other incident nodes, i.e., ($e \setminus \{u\}) \cup \{v\}$.

% As discussed in Sect. 1 and shown in Sect. 4, these sampling methods have two notable limitations. First, the lack of conditioning undermines the efficiency and stability of model training. Second, they can sample potential positive hyperedges as negative ones, which may confuse the model during training.





\begin{table}[t]
\centering
\caption{Notations and their descriptions}
\vspace{-2mm}
\setlength\tabcolsep{1pt}
\begin{tabular}{cl}
\toprule
 \textbf{Notation} & \textbf{Description}\\
\midrule
$H$ & a hypergraph that consists of nodes and hyperedges \\
$V, E$ & the set of nodes, the set of hyperedges \\
$\mathbf{H}$ &the incidence matrix of $H$ \\
\midrule
$\mathbf{X}$ & the input node features \\
% \midrule
% $\mathbf{D}^V, \mathbf{D}^E$ & the degree matrices of nodes and hyperedges \\
$\mathbf{P}, \mathbf{Q}$ & the node and hyperedge representations  \\
\midrule
$f(\cdot)$ & a hypergraph encoder \\
$agg(\cdot)$ & a node aggregator for hyperedge candidates \\
$pred(\cdot)$ & a hyperedge predictor \\
$G(\cdot), D(\cdot)$ & the negative hyperedge generator and discriminator \\
$enc(\cdot), dec(\cdot)$ & the encoder and decoder of the generator $G(\cdot)$ \\
\midrule
% $p_f, p_m$ & the node feature and membership masking rates \\
% $\beta$ & the weight of the auxiliary task \\
$\mathcal{L}(\cdot)$ & loss function \\
$\mathbf{W}, b$ & the learnable weight and bias matrices \\

\bottomrule
\end{tabular}
\label{table:notations}
\end{table}





% \vspace{-1.5mm}
\section{Proposed Method: {\method}}\label{sec-proposed}
In this section, we present a novel hyperedge prediction method, named as \textbf{{\method}}, for accurate hyperedge prediction.

\subsection{Problem Definition}
\noindent
\textbf{Notations}.
The notations used in this paper are described in Table~\ref{table:notations}.
A hypergraph is defined as \( H = (V, E) \), where \( V = \{v_1, v_2, \dots, v_{|V|}\} \) and \( E = \{e_1, e_2, \dots, e_{|E|}\} \). 
A hypergraph can generally be represented by an \textit{incidence} matrix $\mathbf{H}\in \{0,1\}^{|V|\times |E|}$,
where each element $h_{ij}=1$ if $v_i \in e_j$, and $h_{ij}=0$ otherwise.
The node and hyperedge features are represented by the matrices \( \mathbf{P} \in \mathbb{R}^{|V| \times d} \), \( \mathbf{Q} \in \mathbb{R}^{|E| \times d} \), where each row \( p_i \) and \( q_i \) represents the \( d\)-dimensional feature of a node and a hyperedge, respectively.


\vspace{1mm}
\noindent
\textbf{\textsc{Problem 1} (\textsc{Hyperedge Prediction}).} 
Given a hypergraph $\mathbf{H}\in \{0,1\}^{|V|\times |E|}$ and the initial node features $\mathbf{X}\in \mathbb{R}^{|V|\times d}$, and a hyperedge candidate $e'\notin E$,
to predict whether \( e' \) is real or not.



\subsection{Methodology}
\noindent
\textbf{Overview of {\method}}.
Figure~\ref{fig:overview} illustrates the overview of {\method},
which consists of (1) hypergraph encoding (\textcolor[HTML]{A973FF}{upper}) and (2) hyperedge candidate scoring (\textcolor[HTML]{71A35A}{lower}) that we focus on.


\vspace{1mm}
\noindent
\textbf{(1) Hypergraph encoding.}  
Given a hypergraph $H = (V, E)$, 
{\method} produces node embeddings $\mathbf{P} \in \mathbb{R}^{|V| \times d}$ and hyperedge embeddings $\mathbf{Q} \in \mathbb{R}^{|E| \times d}$.
Following~\cite{dong2020hnhn,chien2021allset}, {\method} adopts a \textit{2-stage aggregation} approach,
which repeats (1) (\textit{node-to-hyperedge}) producing a hyperedge embedding by aggregating the node embeddings and (2) (\textit{hyperedge-to-node}) producing a node embedding by aggregating the hyperedge embeddings.
Formally, the node and hyperedge embeddings at the \textit{l}-th layer are defined as:
\begin{align}
    \footnotesize \mathbf{Q}^{(l)} = \sigma \left(\mathbf{H}^T \mathbf{P}^{(l-1)} \mathbf{W}^{(l)}_E + b^{(l)}_E \right), 
    \footnotesize \mathbf{P}^{(l)} = \sigma \left(\mathbf{H} \mathbf{Q}^{(l)} \mathbf{W}^{(l)}_V+ b^{(l)}_V 
    \right),
    \label{eq:hypergnn}
\end{align}
where $\mathbf{P}^{(0)}=\mathbf{X}$, $\mathbf{W}^{(l)}_*$ and $b^{(l)}_*$ are trainable weight and bias matrices, respectively;
$\sigma$ is a non-linear activation function; normalization terms are omitted for simplicity in Eq.~\ref{eq:hypergnn}.



\vspace{1mm}
\noindent
\textbf{(2) Hyperedge candidate scoring.}  
The hyperedge candidate scoring of {\method} consists of a \textbf{(a) generator} to produce informative negative hyperedges for training and a \textbf{(b) discriminator} to predict whether a hyperedge candidate is positive or negative.

% For hyperedge prediction using the node embeddings output by the HyperGNN, two steps are required:
% (1) A node aggregator \( \text{agg}(\cdot) \) (e.g., by averaging or max-pooling) combines the node embeddings of the nodes in a set \( E \) to form a hyperedge candidate embedding.
% (2) A predictor \( \text{pred}(\cdot) \) (e.g., an MLP) uses this aggregated embedding to compute the probability of a hyperedge candidate, estimating how likely it is to form as an existing hyperedge.

% The goals of {\method} are as follows: (1) to generate stable negative hyperedges using a guided generator with an encoder-decoder structure that leverages information from positive hyperedges, and (2) to prevent the generation of potential positive hyperedges as negative samples by employing a regularization term based on the similarity between positive and negative hyperedges.



\begin{figure}[t]
% \vspace{3mm}
\centering
\includegraphics[width=0.95\linewidth]{figures/overview.pdf}
\vspace{-3mm}
\caption{Overview of {\method}: 
(1) hypergraph encoding (\textcolor[HTML]{A973FF}{upper}) and (2) hyperedge candidate scoring (\textcolor[HTML]{71A35A}{lower}).}
\vspace{-4mm}
\label{fig:overview}
\end{figure}



\vspace{1mm}
\noindent
\underline{\textbf{(2)-(a) Generator}}.
We propose a \textit{positive-guided} negative hyperedge generator for \textbf{(C1)} that employs an encoder-decoder structure.
Specifically, 
given a positive hyperedge $e^+$,
(1) the encoder produces the latent vector $q_{e^+}$ for positive hyperedge $e^+$ and
(2) the latent vector $q_{e^+}$ and a random Gaussian noise $z$ are fed into the decoder to generate a node membership vector $c \in \mathbb{R}^{|V|}$:
\begin{align}
    enc(e^+) \rightarrow q_{e^+}\in \mathbb{R}^{d},\hspace{3mm} dec(q_{e^+},z)\rightarrow c\in \mathbb{R}^{|V|}, 
    % \hspace{1mm} \text{Top-K}(c, k) \rightarrow e^-,
    \label{eq:encoder-decoder}
\end{align}
\vspace{-1mm}
where $e^+\in \mathbb{R}^{|V|}$ is a one-hot vector whose element $e^+[i]=1$ if $i\in e^+$, and $e^+[i]=0$ otherwise,
% $d$ is the dimension of the encoder output,
and $c_i$ is a node membership vector whose element $c_i$ represents the probability of the node $i$ being included in the generated negative hyperedge.

For effectively extracting the characteristics of positive hyperedges and injecting them into generated negative hyperedges,
inspired by~\cite{choi2020stargan}, 
we adopt a convolutional neural network (CNN) with three layers as the architecture of our encoder and decoder.
Each of the layers consists of a 1-D convolutional layer with 256 kernels of size 3, average-pooling, and LeakyReLU as the activation function.
In the case of the decoder, 
we additionally use adaptive instance normalization (AdaIN)~\cite{huang2017adain}, which follows each convolutional layer, 
in order to inject the characteristics of a positive hyperedge into its corresponding negative hyperedge generated.

Finally, for the size $n$ of a negative hyperedge, sampled from the size distribution of positive hyperedges,
{\method} selects top-$n$ nodes as a negative hyperedge $e^-$ from the candidate probability vector $c$.
As a result, the pair of positive and negative hyperedges $e^+$ and $e^-$ are fed into the discriminator for model training.


% positive plays an important role

\vspace{1mm}
\noindent
\underline{\textbf{(2)-(b) Discriminator}}. Given the learned node embeddings $\mathbf{P}$ and a hyperedge candidate $e'$ ($e^+$ or $e^-$), 
the discriminator (1) produces the embedding of a hyperedge candidate $q_{e'}$ by aggregating the embeddings of the nodes in $e'$, $\mathbf{P}[e',:]\in \mathbb{R}^{|e'| \times d}$,
and (2) computes the probability $\hat{y}_{e'}$ of $e'$ being formed based on $q_{e'}$ as:
\begin{align}
    agg(\mathbf{P}[e',:]) \rightarrow q_{e'}\in \mathbb{R}^{|d|}, \hspace{3mm} pred(q_{e'}) \rightarrow \hat{y}_{e'}\in \mathbb{R}^{1},
    \label{eq:maxpoolin}
\end{align}
where $agg(\cdot)$ is the \textit{element-wise maxmin} pooling, used as the aggregation function to reflect the diversity of the embeddings of nodes in a hyperedge candidate, by following~\cite{hwang2022ahp,yadati2020nhp},
and $pred(\cdot)$ is a hyperedge predictor, which consists of three fully-connected layers ($d \times 128 \times 8 \times 1$), followed by a sigmoid function.





\subsection{Model Training}
% \vspace{1mm}
% \noindent
% \textbf{(3) Model training}.
We train the model parameters of {\method} in an adversarial way~\cite{arjovsky2017wasserstein}:
given a batch $B$ of positive hyperedges,
(1) generate $|B|$ negative hyperedges using the generator $G$ ($enc(\cdot)$ and $dec(\cdot)$), 
(2) classify the positive and negative hyperedges using the discriminator $D$ ($agg(\cdot)$ and $pred(\cdot)$),
and (3) update the model parameters of {\method} based on their losses.
Specifically, as $D$ aims to compute the probabilities of positive hyperedges higher than those of negative hyperedges,
the loss function for $D$ is defined as:
\begin{align}
    L_D = -\frac{1}{|B|} \sum_{e^+ \in B} [D(e^+ | H, X)] + \frac{1}{|B|} \sum^{|B|}_{j=1} [D(G(z_j\mid e^+_j) | H, X)] \label{eq:loss_D},
\end{align}
where \( e^+ \) is a positive hyperedge and \( G(z_j \mid e^+_j) \) is the negative hyperedge generated from a noise \( z \) and the positive hyperedge \( e^+_j \). 
This loss $L_D$ is also used for training the hypergraph encoder $f(\cdot)$.

On the other hand, \( G \) aims to deceive \( D \) to misclassify negative hyperedges as positive.
Thus, the loss function for $G$ is defined as:
\begin{align}
    L_G = -\frac{1}{|B|} \sum^{|B|}_{j=1} [D(G(z_j\mid e^+_j) | H, X)] \label{eq:loss_G}.
\end{align}







% \vspace{1mm}
\noindent
\underline{\textbf{Regularization for (C2)}}.
`Hard' negative hyperedges, generated by our hyperedge generator, could enhance the distinguishing ability of a hyperedge prediction model~\cite{hwang2022ahp}.
Without any regularization on a generator, however, 
it may \textit{completely copy} the distribution of the original positive hyperedges; result in generating hyperedges too similar to positive hyperedges that might potentially become positive hyperedges in the future. 
Using such hyperedges as negative hyperedges in training can lead to incorrect learning.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/regularization_term.pdf}
\vspace{-5mm}
\caption{Regularization loss according to hyperparameters.}
\vspace{-4mm}
\label{fig:regularization_term}
\end{figure}
\input{tables/eq1-accuracy3}
To address this challenge, 
we propose a novel \textit{regularization term} that is integrated into the loss function.
This regularization loss gives a smaller penalty as the generated negative hyperedges are more similar to positive hyperedges until a certain degree and gives a larger penalty as they are too similar to positive hyperedges.
Specifically, 
given the embeddings of positive and negative hyperedges $\mathbf{Q}^{+}$ and $\mathbf{Q}^{-}$,
the regularization loss is defined as:
\begin{align}
    L_{reg} = -\left(\frac{| \theta - k |}{\theta(1-\theta)}\right)^p, \hspace{3mm}  \theta = sim(\mathbf{Q}^{+}, \mathbf{Q}^{-}) \label{eq:reg_loss}.
\end{align}
where \( k \) and \( p \) are hyperparameters that control the converge point and the curvature of the function, respectively.
Figure~\ref{fig:regularization_term} shows that regularization loss according to the hyperparameters \( k \) and \( p \).
% (See Figure~\ref{fig:regularization_term}). 
$sim(\cdot)$ is the cosine similarity used as a similarity function.
Finally, the regularization loss is integrated into the total loss with a hyperparameter $\beta$ to control the weight of the regularization loss:
\begin{align}
    L_{total} &= L_D + L_G + \beta\cdot L_{reg} 
\end{align}
Thus, all the model parameters of {\method} are trained jointly based on the adversarial loss and the regularization loss. 
% We will verify the effectiveness of the regularization loss in the training of {\method} in Section~\ref{sec-eval-eq2}.








\section{Experimental Validation}\label{sec-eval}
In this section, we comprehensively evaluate {\method} by answering the following evaluation questions (EQs):
\begin{itemize}[leftmargin=10pt]
    \item \textbf{EQ1 (Accuracy)}. To what extent does {\method} improve the existing hyperedge prediction methods in terms of the accuracy?
    \item \textbf{EQ2 (Ablation study)}. Is each of our strategies beneficial to generating negative hyperedges useful for model training?   
    \item \textbf{EQ3 (Sensitivity)}. How sensitive is the effect of the regularization loss in model training to its hyperparameters ($k$ and $p$)?    
\end{itemize}




% \vspace{-2mm}
\subsection{Experimental Setups}\label{sec-eval-setup}

\noindent
\textbf{Datasets and competitors}
% \footnote{We have released all the code and datasets at: \url{\codelink}}.
We use six widely used real-world hypergraphs: 
(1) three co-citation datasets (Citeseer, Cora, and Pubmed),
(2) two authorship datasets (Cora-A and DBLP-A), 
and (3) one collaboration dataset (DBLP). 
In the co-citation datasets, each node represents a paper and each hyperedge represents a group of papers co-cited by a paper;
in the authorship dataset, each node represents a paper and each hyperedge represents a group of papers written by an author;
in the collaboration dataset, each node represents a researcher and each hyperedge represents a group of researchers who wrote the same paper.
For all the datasets,
we use the bag-of-word features from the abstract of each paper as in~\cite{hwang2022ahp,ko2023cash}.
We select four state-of-the-art hyperedge prediction methods as our competitors in the experiments (Expansion~\cite{yoon2020expansion}, NHP~\cite{yadati2020nhp}, HyperSAGNN~\cite{zhang2019hyperSAGNN}, and AHP~\cite{hwang2022ahp}).



\vspace{1mm}
\noindent
\textbf{Evaluation protocol}.
We evaluate {\method} by using the protocol exactly same as that used in~\cite{hwang2022ahp}.
For each dataset, we use five data splits~\footnote{All datasets and their splits used in this paper are available at: \url{\codelink}.},
where positive hyperedges are randomly divided into training (60\%), validation (20\%), and test (20\%) sets. 
We use three validation and test sets constructed with negative hyperedges sampled by SNS, MNS, and CNS, explained in Section~\ref{sec-related}. 
As metrics, we use AUROC (area under the ROC curve) and AP (average precision).
We (1) measure AUROC and AP on each test set when the averaged AUROC over the validation sets is maximized, 
and (2) report the averaged AUROC and AP over five runs on each test set.
% \footnote{
For all competing methods, 
we use the results reported in~\cite{hwang2022ahp} since we follow the exactly same protocol with the same data splits. 




\input{tables/eq2-ablation2}
% \begin{figure*}[t]
% \centering
% \begin{tabular}{ccccl}
    
%     \includegraphics[width=0.17\linewidth]{figures/k_pvalue_citeseer.pdf}
%      &  
%     \includegraphics[width=0.17\linewidth]{figures/k_pvalue_cora.pdf}
%      &
%      \includegraphics[width=0.17\linewidth]{figures/k_pvalue_coraA.pdf}
%      &
%     \includegraphics[width=0.17\linewidth]{figures/k_pvalue_pubmed.pdf}
%     &
%     \includegraphics[width=0.03\linewidth]{figures/k_pvalue_bar.png}
%      \\
%     (a) {Citeseer}    
%      & 
%     (b) {Cora}
%     &
%     (c) {Cora-A}    
%      & 
%     (d) {Pubmed}
%     &
% \end{tabular}
% \vspace{-4mm}
% \caption{Sensitivity analysis: {\method} achieves high accuracy across a wide range of values for the hyperparameters $k$ and $p$ (i.e., the wide red area on the surface).}
% \vspace{-3mm}
% \label{fig:sensitivity-heatmap}
% \end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/Fig4.pdf}    
\vspace{-5mm}
\caption{Sensitivity analysis: {\method} achieves high accuracy across a wide range of values for the hyperparameters $k$ and $p$.}
\vspace{-4mm}
\label{fig:sensitivity-heatmap}
\end{figure*}


% tables & fig
% \vspace{-2mm}
% \subsection{Experimental Results}


% eq1
% \vspace{1mm}
% \noindent
% \textbf{EQ1. Hyperedge prediction accuracy}.
\subsection{EQ1. Hyperedge Prediction Accuracy}\label{sec-eval-eq1}
We first compare {\method} with four competing methods in the hyperedge prediction task.
Table~\ref{table:eval-accuracy} shows that {\method} \textit{consistently} outperforms \textit{all} competing methods in terms of \textit{both} the averaged AP and AUROC over three test sets across \textit{all} datasets.
We note that these improvements of {\method} over AHP (the best competitor) are remarkable, 
given that AHP~\cite{hwang2022ahp}, the state-of-the-art method. has already improved other existing methods significantly in those datasets. 
Via the \textit{t}-tests with a 95\% confidence level, 
we verify that the improvements of {\method} are statistically significant (i.e., the \textit{p}-values $\leq0.05$).
As a result, these results demonstrate that {\method} can generate informative negative hyperedges by effectively addressing the two challenges of negative sampling, thereby enhancing the accuracy of a hyperedge prediction task.


Interestingly, NHP~\cite{yadati2020nhp} achieves the highest accuracies in the SNS test setting of the Citeseer, Pubmed, and DBLP-A,
However, NHP shows very low accuracies on the CNS test set (i.e., the most difficult test set) of all datasets, 
which is similar to or even worse than the accuracy of the random prediction ($\approx0.5$), 
These accuracy gaps between the CNS and SNS test sets imply that NHP may be overfitting to the easy negative examples, thus which limits their ability to be generalized to other datasets. 
On the other hand, 
{\method} consistently achieves very high accuracies across all test test settings, for example 98.4\%, 92.1\%, and 91.3\% in the SNS setting of Citeseer, Pubmed, and DBLP-A, respectively. 
% Consequently, the accuracy differences among test settings are the smallest among all competing methods, which demonstrates that {\m} has superior generalization ability compared to all the competing methods.










% eq2
% \vspace{1mm}
% \noindent
% \textbf{EQ2. Ablation study}.
\subsection{EQ2. Ablation Study}\label{sec-eval-eq2}
We verify the effectiveness of our proposed strategies of {\method} individually by ablating one of them: 
(i) positive-guided negative hyperedge generator and (ii) regularization term. 
Table ~\ref{table:ablation} shows that the original version of {\method} always achieves the highest accuracy across all datasets,
which indicates that each of the proposed strategies is always beneficial to improving the accuracy of {\method}.
These results verify that our proposed strategies are able to address the two challenges successfully: \textbf{(C1)} lack of guidance for generating negatives and \textbf{(C2)} possibility of false negatives.

Furthermore, {\method} w/o $L_{reg}$ shows the worst results in all cases,
indicating that ablating the regularization term can lead to significant accuracy degradation.
These results verify that {\method} can generate negative hyperedges that are sufficiently distinct from positive hyperedges, making them useful for model training.


% eq3
% \vspace{1mm}
% \noindent
% \textbf{EQ3. Sensitivity analysis}.
\subsection{EQ3. Sensitivity Analysis}\label{sec-eval-eq3}
In this experiment, we evaluate the impacts of regularization hyperparameters $k$ and $p$ on the accuracy of {\method}. 
We measure the model accuracy of {\method} with varying $k$ from 0 to 1.0 in step of 0.1 and $p$ from 1 to 5 in step of 1. 
Figure~\ref{fig:sensitivity-heatmap} shows the results,
where the $x$-axis represents the converge point hyperparameter $k$,
the $y$-axis represents the curvature hyperparameter $p$,
and the $z$-axis represents the averaged AUROC.
{\method} with $k\geq0.4$ consistently achieves higher accuracy than {\method} with $k<0.4$ regardless of $p$ (i.e., the wide red area on the surface in Figure~\ref{fig:sensitivity-heatmap}).
Based on these results, 
we believe that the accuracy of {\method} is \textit{insensitive} to the regularization hyperparameters $k$ and $p$ provided that $k\geq0.4$. 



\section{Conclusion}\label{sec-con}
In this paper, we identify two key challenges of negative hyperedge sampling in the hyperedge prediction task: 
\textbf{(C1)} lack of guidance for generating negatives and \textbf{(C2)} possibility of producing false negatives. 
To address both challenges, we propose a novel hyperedge prediction method, {\method} that employs 
(1) a positive-guided negative hyperedge generator leveraging positive hyperedges as a guidance to generate informative negative hyperedges for \textbf{(C1)} and (2) a regularization term to prevent the generated hyperedges from being false negatives \textbf{(C2)}.
Comprehensive experiments on six real-world datasets verified the superiority of {\method} over four state-of-the-art hyperedge prediction methods.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
% \vspace{-1mm}
This work was supported by Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2022-00155586, 2022-0-00352).
\end{acks}

% \vspace{-1mm}
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}


%%
%% If your work has an appendix, this is the place to put it.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
