\section{Related Works}
\label{sec-related}
\textbf{Hyperedge prediction}.
There have been a number of works to study hyperedge prediction.
they solve the hyperedge prediction problem as a classification task~\cite{yoon2020expansion,zhang2019hyperSAGNN,yadati2020nhp,hwang2022ahp}.
Expansion~\cite{yoon2020expansion} models a hypergraph as multiple \textit{n}-projected graphs and applies a logistic regression model to the projected graphs for predicting future hyperedges 
HyperSAGNN~\cite{zhang2019hyperSAGNN} uses a self-attention-based graph neural networks (GNN) model to learn hyperedges of variable sizes. 
NHP~\cite{yadati2020nhp} employs hyperedge-aware GNN models to learn node embeddings in hypergraphs, using the max-min pooling to aggregate the embeddings of nodes within each hyperedge candidate for prediction. 
AHP~\cite{hwang2022ahp}, the state-of-the-art hyperedge prediction method, employs adversarial training to generate negative hyperedges for model training and uses max-min pooling for node aggregation. 


\vspace{1mm}
\noindent
\textbf{Negative hyperedge sampling}.
For enhancing the training of hyperedge prediction models, 
the following three heuristic-based methods for negative hyperedge sampling have been proposed~\cite{patil2020heuristic}:
(1) Sized NS (SNS) samples $n$ nodes uniformly at random;
(2) Motif NS (MNS) transforms a hypergraph into an ordinary graph via a clique-expansion and samples a $n$-connected component in the expanded graph;
and (3) Clique NS (CNS) selects a hyperedge $e$ and replaces one of its incident nodes $u\in e$ with a node $v\notin e$, which is linked to all the other incident nodes, i.e., ($e \setminus \{u\}) \cup \{v\}$.

% As discussed in Sect. 1 and shown in Sect. 4, these sampling methods have two notable limitations. First, the lack of conditioning undermines the efficiency and stability of model training. Second, they can sample potential positive hyperedges as negative ones, which may confuse the model during training.





\begin{table}[t]
\centering
\caption{Notations and their descriptions}
\vspace{-2mm}
\setlength\tabcolsep{1pt}
\begin{tabular}{cl}
\toprule
 \textbf{Notation} & \textbf{Description}\\
\midrule
$H$ & a hypergraph that consists of nodes and hyperedges \\
$V, E$ & the set of nodes, the set of hyperedges \\
$\mathbf{H}$ &the incidence matrix of $H$ \\
\midrule
$\mathbf{X}$ & the input node features \\
% \midrule
% $\mathbf{D}^V, \mathbf{D}^E$ & the degree matrices of nodes and hyperedges \\
$\mathbf{P}, \mathbf{Q}$ & the node and hyperedge representations  \\
\midrule
$f(\cdot)$ & a hypergraph encoder \\
$agg(\cdot)$ & a node aggregator for hyperedge candidates \\
$pred(\cdot)$ & a hyperedge predictor \\
$G(\cdot), D(\cdot)$ & the negative hyperedge generator and discriminator \\
$enc(\cdot), dec(\cdot)$ & the encoder and decoder of the generator $G(\cdot)$ \\
\midrule
% $p_f, p_m$ & the node feature and membership masking rates \\
% $\beta$ & the weight of the auxiliary task \\
$\mathcal{L}(\cdot)$ & loss function \\
$\mathbf{W}, b$ & the learnable weight and bias matrices \\

\bottomrule
\end{tabular}
\label{table:notations}
\end{table}





% \vspace{-1.5mm}