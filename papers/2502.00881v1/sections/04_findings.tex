\section{Findings} \label{sec:findings}
We first summarize the processes and challenges participants described across four core survey authoring activities. We then examine their motivations, strategies, and obstacles for updating their surveys. Finally, we highlight their perspectives on the potential role of AI in aiding survey authoring and updating.

\subsection{The Processes in Authoring Survey Articles: Work Practices and Challenges}

\paragraph{\textbf{Search}}
Participants described a diversity of strategies for paper discovery, some more systematic than others. For example, most mentioned using a scholarly database to find papers, though several participants highlighted the challenge of forming an effective search query. P4 illustrated a lengthy process of iteratively searching for papers with an initial query based on his own expertise, scanning relevant papers returned by the initial query to identify other relevant keywords, and repeating the process until saturation. Another participant found it challenging to identify the right keywords to search for, especially in emergent research areas that lacked a consistent vocabulary (P8). 

Participants also mentioned using more informal methods of paper discovery, including citation chaining~\cite{webster2002analyzing} from foundational papers, receiving recommendations from their social network, and monitoring prominent research groups in their field. Two participants had no dedicated means of paper discovery, relying instead on a collection of papers gathered throughout their research, and searching for and adding new papers only when necessary during the writing process (P1, P7).

\paragraph{\textbf{Appraisal}}
After collecting a set of papers, participants described carefully screening those papers for inclusion. Though most could be filtered based on their title and abstract alone, some participants noted how certain papers required more careful inspection of the introduction, implementation, or results to determine relevance. As with paper discovery, emergent research areas complicated paper screening, as P6 described how the lack of standardized terminology made it challenge to ``comprehend the essence of what they were trying to say and implement.''

Participants also noted a challenge in identifying and removing duplicated papers (P1, P3, P5, P8, P10). Some papers could have multiple versions, e.g., a pre-print, a conference paper, and a journal article, yet offer the same contributions. De-duplication quickly became ``very annoying'' and ``tedious'' over a large corpus (P10). Other strategies appeared more arbitrary and driven by experience. For instance, several participants further filtered ``pseudo-duplicate'' papers; P1 described this aspect of his screening process as more art than science:
\begin{quote}
    \textit{``I found a lot of semi-duplicates. There are some authors who do double dipping. They would write two papers, but they are based essentially on the same prototype. But this is more like art than an algorithm because sometimes I suspected authors use the same prototype, but I could not guarantee it. I tried to remove everything that I would suspect was either not really implemented or was double dipping. I tried to only keep the papers and projects that I knew that actually did something.''}
\end{quote}
However, these subjective processes rely on the authors' expertise and are rarely made explicit in the survey article, making them inherently opaque, open to interpretation, and potentially challenging to replicate when updating the survey in the future.

\paragraph{\textbf{Synthesis}}
After screening papers for inclusion based on relevance and quality, participants often described finding the right organization as the most time-consuming and cognitively challenging phase of their workflow (P1, P3--P6, P10, P11). Organizing hundreds of papers into a multi-dimensional taxonomy was a lengthy process of iterative refinement, sometimes with multiple authors collaborating to reach a consensus on the appropriate structure, dimensions, and evidence. Reflecting on his workflow, P1 described feeling humbled in his taxonomy development process:
\begin{quote}
    \textit{``I had to discard a lot of ideas because initially what I thought it would be just didn't work. It ended up being nonsense. Our initial dimensions, because they were correct but meaningless, they were not informative. It took a lot of time and humility, a lot of humbleness to accept that those initial things didn't work.''}
\end{quote}

Many participants developed a codebook alongside their taxonomy, capturing key questions or dimensions to compare across papers. Predefined codes were based on their surveys' research questions and own expertise, with new codes added as needed. Revisions were labor-intensive, with P10 noting how adding a code meant revisiting every paper and reestablishing a mental model of the paper to extract the relevant information.

\paragraph{\textbf{Interpretation}}
Finally, participants highlighted a key challenge in synthesizing and interpreting the literature. They described how beyond the organizational taxonomy, the real value of a survey lies in its identification of key challenges, future trends, and open research opportunities (P3, P6, P7, P9, P11). P6 emphasized this point, while acknowledging the difficulty in articulating those insights:
\begin{quote}
    ``\textit{Beyond the taxonomy, another important thing about this literature review is the challenges and the future trends you uncovered in the review...because the reason they want to read the paper is because they want to start their own research project. So I did spend a lot of time on these challenges and the trends, on how to make it clear and comprehensive and fancier to give back more insights to those readers.}''
\end{quote}
He elaborated that while the taxonomy he had developed worked well for categorization, it was too rigid for discussing future trends. Providing ``coarse-grained'' insights to inspire new research was ``tricky'' since they were not directly tied to the ``logic'' imposed by the taxonomy. Deriving meaningful insights can also be challenging due to limited perspective, with P6 adding that identifying emerging trends required insight he gained through discussion with other researchers rather than solely reviewing the literature.
\begin{quote}
    ``\textit{At that time, my vision was still quite limited. I can read lots of papers and try to summarize in the tables, even just write the sections to introduce each work, but it's really hard for me to get the sense in five years, what LoRa networking research would be. It's really hard for me to uncover this kind of future trends just by reading papers. I needed to talk to different researchers working on this research field to understand their vision for this topic. It's just something I couldn't do by myself. That's kind of the most challenging part.}''
\end{quote}

Another challenge involved the selection and interpretation of key papers. In contrast to SLRs in clinical medicine, which use rigid inclusion and exclusion criteria to identify relevant studies for meta-analysis and avoid introducing bias, participants in our study described more informal search and appraisal processes, while aiming to be comprehensive in their overall search of the literature. Furthermore, not all relevant research could be synthesized given the length restrictions of a survey article, as one participant explained his nuanced process of selecting papers that balanced recency so ``readers will find it interesting'' and foundational work to avoid ``forgetting the theory'' (P3). Another participant, P7, included additional meta-commentary, drawing connections between contemporaneous papers with similar contributions and critically evaluating when a paper's proposed approach was unsupported by its evidence.


\subsection{The Dynamics of Updating Survey Articles: Motivations, Approaches, and Obstacles}

\paragraph{\textbf{Motivations}}
Participants viewed keeping survey articles up-to-date as a valuable, albeit costly, endeavor. P10 felt keeping his survey current was important, effectively ``showing a picture of how the research is at that point.'' P11 similarly mentioned how updating his survey would be ``helpful for the community,'' as many researchers used it as a starting point, though he expressed concern about its relevance, acknowledging, ``it's already two years old.'' Participants noted how this perception of value also carried an implicit expectation of currency with the latest research. Some recollected how peer reviewers asked for an update to the paper search, since a year had elapsed while under review and new, relevant research may have emerged (P8, P10). After a subsequent rejection, P8 also proactively updated their survey to include new literature, fearing future reviewers would be ``annoyed'' that the survey article was too ``old and outdated.'' For many participants, this expectation manifested as a pressure to publish surveys as quickly as possible and an anxiety surrounding the potential extra work in updating if the paper were not accepted.

Despite believing that surveys should ideally be kept up to date, participants held varying expectations on the desired frequency of updates. For instance, P8, working in a slower-moving field, noticed only a few relevant papers to their survey each year. In contrast, other participants reported encountering many relevant papers to include through passive monitoring of email alerts, conference and journal proceedings, and work citing their survey (P4, P5, P11).

\paragraph{\textbf{Approaches}}
Participants described three main types of updates they envisioned making to their surveys:
\begin{enumerate}[label=\arabic*.]
    \item \textbf{Empirical}, involving the update of quantitative and qualitative evidence presented in text, tables, and figures.
    \item \textbf{Structural}, involving the update of paper structure, organizational taxonomies, and frameworks.
    \item \textbf{Interpretative}, involving the update of synthesis and interpretation of empirical evidence, such as the discussion of limitations, implications, emerging trends, unresolved and resolved challenges, and future directions.
\end{enumerate}

First, participants identified \textit{empirical} updates that aimed to incorporate the latest research findings. Empirical updates involve revising individual quantitative results to reflect the state-of-the-art, as well as any recalculating statistical meta-analyses and aggregated values, such as counts and proportions of papers within each dimension of the taxonomy. These updates may also involve refining the qualitative narrative by replacing less effective or outdated references with more compelling data, a process P10 paralleled to selecting the best quote from an interview study. One participant further characterized making empirical updates as a complex decision-making process, as each new piece of evidence requires consideration of whether ``\textit{you should replace this, you should add this, or you should just mention it in the table without any text}'' (P6).

Second, participants believed empirical updates could eventually warrant \textit{structural} updates, such as adding a new section to a survey to synthesize recently incorporated papers (P2, P5, P6). One participant described how changes in the momentum of a research area could induce a structural update for his survey:
\begin{quote}
    ``\textit{If some field is becoming more prominent and lots of work is going in a particular direction---maybe using SSVEP now more with VR---then we will highlight it in the discussion, make a section for it, and explain the reason why we think it's gaining prominence now, highlighting some of the results.}'' (P5)
\end{quote}
P6 further explained that the decision to create a new section in a survey---as opposed to integrating the evidence into an existing section---should depend on the sufficiency of new evidence, saying ``\textit{if it’s just five papers for this section for this new topic, I don't think it's worth adding a new section. But if there are 20, 30, even 100, it will be really useful to make them into one independent section}.'' Two participants described a similar type of structural update they made while revising their survey, splitting a dimension of their taxonomy into two as the number of included papers grew beyond a reasonable organization (P2, P11). Lastly, participants noted how structural updates could arise from significant changes to the social or technical status quo. For instance, many participants mentioned large language models as a notable paradigm shift they would incorporate into an updated survey (P3, P5, P6, P9, P11), with proposed revisions ranging from adding a section on their usage to restructuring around the pre- and post-LLM eras.

Third, participants described \textit{interpretative} updates as a process of critically re-evaluating a survey article, aligning it with the current state of a research field. Participants saw this update as the key cognitive challenge, involving re-synthesizing the proposed challenges, trends, and opportunities given the updated empirical evidence. Interpretative updates could further be self-reflective, as P5 explained:
\begin{quote}
    ``\textit{It's not just generating new limitations and recommendations, but also considering these recommendations I already made, and seeing how the field is progressing and whether those limitations are being overcome.}''
\end{quote}
Some participants hoped new research would address prior limitations they had raised in an earlier iteration of their survey, resulting in a discussion of both resolved and emerging challenges in the updated survey (P1, P4, P5).

Participants were generally confident that the original organizational structures and taxonomies they had developed for their survey would remain relevant for several years after its initial publication. P1 highlighted the incremental nature of research he saw:
\begin{quote}
    ``\textit{If they happen to introduce a completely new method, then the methods need to be updated, a new workflow must be developed, which I doubt because I have not seen anything really happening. People just squeeze from the previous one, they try to improve the performance of the existing methods.}''
\end{quote}
Participants therefore viewed empirical and interpretative updates as the bulk of maintaining a living survey, with structural updates necessary only after a critical mass of new work.

Given the perceived stability of their surveys' structure, in considering approaches to updating, participants emphasized the intention to reuse much of their original workflows. Maintaining the same processes would not only conserve effort, but also help ensure consistency in an updated survey. Participants mentioned reuse along logistical aspects---such as reforming the same research team---and mechanistic processes, like reapplying the original paper search criteria across the same scholarly databases, filtering papers with the same inclusion criteria, extracting data from new papers using the existing codebook, and rerunning automated scripts for quantitative analysis.
\paragraph{\textbf{Obstacles}}
Participants identified the lack of strong academic incentives as a key structural barrier to keeping surveys continuously updated. Despite being highly cited and valuable to the research community, participants saw the impact of surveys as tied to their initial publication. Without structural support in the form of academic recognition, participants found it difficult to invest the time and effort required to revise an existing publication. P4 explained how even when authoring their original survey, they needed to ``draw a line'' on the timeframe of surveyed literature, as adding new papers was ``moving the goalposts'' and delayed publication.

In addition to the lack of extrinsic incentives, continuous updating was seen as an ``unmanageable'' without additional support. One primary challenge was the need to continuously discover and screen new research papers. The search had to be comprehensive while avoiding duplication, and certain means of paper discovery, e.g., social recommendation, would be absent once the original survey team dissolved. In contrast, participants saw refining an organizational taxonomy as less challenging when updating a survey, as their previous efforts could largely be reused or adapted.

Participants also noted that updating a survey with new evidence could trigger a cascade of revisions (P1, P8, P10). For example, adding papers to the taxonomy would require recalculating statistics, revising text, updating charts and figures, and modifying conclusions. Managing these interconnections to ensure consistency across a long survey article presented additional cognitive challenges. As a potential coping strategy, P1 suggested he would make updates in bulk, which he felt was more efficient than updating one paper at a time. In sum, while participants recognized the community value of living surveys, they struggled to justify the effort due to the lack of personal academic recognition.


% \subsection{Perspectives of AI Use in Authoring and Updating Survey Articles}
\subsection{Perspectives of AI Use for Survey Updating}

\subsubsection{AI Lacks Nuanced Understanding for Expert-Level Reviews}
Participants expressed concerns about AI’s ability to produce surveys that are as insightful and meaningful as those written by human experts (P1, P2, P5--P8). They emphasized that scholarly synthesis was more than just processing and summarizing data, demanding nuanced understanding, critical engagement, and the foresight to identify gaps and future trends that may advance the field. Several participants noted these aspects to be the core value of survey articles which drive meaningful discourse.

Participants also argued that without domain expertise, AI syntheses are likely to lack the depth and complexity needed for such discourse (P5--P8). P8 highlighted the importance of human oversight to avoid shallow analysis:
\begin{quote}
    ``\textit{I think you can use it. I just think you need to be way more careful about having a human-in-the-loop, and having a human being the final position of authority on what happens during the final part of the analysis. Because otherwise your analysis is going to be incredibly shallow.}''
\end{quote}
This need for experts as a critical voice was echoed by P5, who emphasized that relying too heavily on AI for updates risks making survey articles formulaic. P5 expressed his belief that if articles were written to be updated by AI, they would lack creativity and reader engagement:
\begin{quote}
    ``\textit{If we try to set up the paper in such a way that an AI can easily update sections of it to match data, it may not be an interesting paper to read... And I think papers are written to be fun.}''
\end{quote}
Overall, participants perceived current AI systems as lacking the ability to offer the depth, creativity, and engagement characteristic and required of expert-level survey articles. Without these elements, AI-generated syntheses risked becoming too shallow or formulaic.

\subsubsection{Roles of AI in Supporting Updates}
While participants were generally skeptical about AI's ability to replicate human-written survey articles, they recognized the potential for delegating repetition or tasks requiring less specialized expertise to the AI. Participants recommended using AI for tasks where mistakes would be less costly (P1, P7, P10). For instance, P10 described how he would only choose to use AI in places where ``the results of the paper and the conclusion do not rely very strongly on it.'' P7 suggested that AI should only be tasked with the ``very easy work,'' and for the creative work, such as discussing trends and challenges, AI should only be used to offer ``suggestions.'' The following summarizes three roles participants highlighted of AI assistance throughout authoring and updating a survey.

\paragraph{\textbf{AI for routine automation}} 
First, participants identified routine numerical updates, such as tallying results or updating statistical information, as tasks where AI could be reliably employed (P4, P5). Automating these tasks could help ensure consistency and comprehensiveness when incorporating updates into a survey. For example, participants expressed the need to extract specific data from a corpus of papers to facilitate coding, and then subsequently ensure the quantitative aspects of the survey were updated to reflect the latest codes. One concern participants shared was the need to verify the work done by any AI system (P5, P10, P11). As P5 explained, ``you may not be able to trust the extracted data,'' highlighting the importance of proper guardrails to mitigate overreliance and providing affordances for user verification.

\paragraph{\textbf{AI as a surrogate}}
Participants described specific sub-tasks where AI could be valuable in managing a large corpus of data (P2, P5, P7--P11). For instance, they saw AI as a useful assistant in potentially making the search process more comprehensive by refining and augmenting search strings (P6, P10, P11). For large paper collections, AI was seen as helpful for doing an initial pass at screening and organizing of the papers (P2, P5, P7--P9). However, participants also expressed concerns about AI missing important information or adding irrelevant information. In identifying or updating a corpus of papers, P9 described how it is easier to spot irrelevance but more challenging to detect what may have been overlooked. Furthermore, some participants emphasized the importance of establishing proper ways to document AI usage for reproducibility and transparency in research (P8, P10).

\paragraph{\textbf{AI as a second opinion}}
Participants discussed several places in the authoring and updates process where AI could serve as a co-author providing a second opinion (P1, P5, P6, P9--P11). For instance, when analyzing trends in a cluster of research papers, AI could do a ``sanity check'' in case the researcher misses anything (P9). Similarly, when working alone and screening papers, AI could ``provide an opinion'' alerting the researcher to papers that might require taking a closer look (P5). These examples highlight how integrating AI as a collaborative partner could be beneficial within authoring or updating, to enhance systematicity, reduce cognitive bias, and encourage a more comprehensive and insightful synthesis.
