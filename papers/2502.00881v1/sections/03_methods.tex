\begin{table*}[t]
\centering
\small
\renewcommand{\arraystretch}{1.2} 
\caption{Participant biographies and self-reported research expertise.}
\begin{tabular}{cp{0.2\textwidth}p{0.4\textwidth}}
\toprule
\textbf{ID} & \textbf{Biographical Information} & \textbf{Research Expertise} \\ 
% (age, gender, role)}
\midrule
P1 & 40, male, assistant professor & Advanced cyber systems, digital equity, distributed systems \\
P2 & 26, male, PhD student & Critical HCI, queer HCI \\
P3 & 29, male, PhD student & Hardware security \\
P4 & 33, female, PhD student & Safety technologies, transformative justice \\
% Safety technologies, transformative justice, place-based technologies, neighborhood safety \\
P5 & 27, male, PhD student & Persuasive interventions for absent-minded smartphone use \\
P6 & 28, male, PhD student & Low-power wide-area network, LoRa networking \\
P7 & 27, male, PhD student & Signal processing, video anomaly detection \\
P8 & 35, female, assistant professor & Human-computer interaction, realism, audio, games \\
P9 & 33, female, PhD student & Algorithmic systems in housing services \\
P10 & 36, male, assistant professor & Human aspects of software engineering \\
P11 & 33, male, research engineer & Knowledge distillation, NLP, CV, IR, symbolic regression \\
% Knowledge distillation, natural language processing, computer vision, information retrieval, symbolic regression \\
\bottomrule
\end{tabular}
\label{tab:participant_demographics}
\end{table*}

\section{Methods}
We conducted in-depth, retrospective interviews with authors of narrative reviews, drawing on their expertise to inform the updating process. To identify authors, we searched dblp\footnote{\url{https://www.dblp.org}} for peer-reviewed narrative reviews (equivalently, \textit{survey articles} or \textit{surveys}) in ACM Computing Surveys (CSUR), a premier journal for surveys in computing research. We identified additional surveys published in ACM CHI and CSCW using the search query ``review OR survey.'' We filtered surveys to those published 1-3 years prior, allowing time for new research to emerge while ensuring participants could recall and reflect on their processes. We then excluded any that were not actual surveys of computing literature, for instance papers on improving methodologies for online user surveys or analyzing app reviews. Due to IRB restrictions, we excluded papers whose corresponding authors were based outside the United States or Canada. This process yielded a total of 128 surveys, and we invited each corresponding author to participate via email.

% CHI - 68 papers
% CSCW - 31 papers
% CSUR 2024 - 268 papers
    % US-only - 31 papers
% CSUR 2023 - 334 papers
    % US-only - 42 papers

\subsection{Participants}
Eleven of the invited authors participated in our study and were included in the subsequent analysis. Six of their surveys were published in CSUR, four in CHI, and one in CSCW. The surveys were published between early 2022 and 2024, with the majority published in 2023. Due to the lengthy peer review and publication process, most participants noted the relevant research work occurred one to two years prior to official publication. Participants' ages ranged from 26 to 40 ($M = 32, SD = 4.5$). Three were female and eight were male. Three were research tenure-track assistant professors, seven were PhD students, and one was a research engineer. There was considerable diversity among participants, both demographically and in survey article topic (Table~\ref{tab:participant_demographics}). We provide participants' self-identified areas of research expertise (which align closely with the topics of their survey articles they were invited to discuss) but refrain from disclosing the surveys to maintain confidentiality. We anticipated our study size would allow for sufficient theoretical saturation, and that the variety of participants' demographic and research backgrounds would allow us to elicit a rich diversity of survey authoring and updating practices.

\subsection{Interview Protocol}
Our interviews consisted of open-ended questions prompting participants to recall and reflect on their research practices. For instance, participants were asked to describe how they conceived of their survey topic, the composition and responsibilities of their research team, and their approaches for discovering and screening papers, extracting and organizing information, writing, and revising. Probing questions were used to elicit specific facets of participants' experiences and encourage more detailed discussion around particularly meaningful or evocative aspects of their process. Additional details are available in Appendix~\ref{sec:interview_protocol}. Where appropriate, participants were encouraged to share their screen to illustrate their recalled experiences and any artifacts they created. Interviews were conducted virtually via Google Meet, each lasting approximately one hour. Participants were compensated with a \$40 USD gift card. This study protocol was approved by the IRB at the research team's university.

\subsection{Analysis}
Data for analysis included transcripts automatically created from the interview audio recordings and manually cleaned. All personal identifiers were removed from the transcripts to ensure anonymity. We then followed a thematic analysis approach~\cite{braun_thematicAnalysis_2006} to analyze the data. Two team members reviewed all transcripts to familiarize themselves with the data, and then assigned specific codes to two transcripts using an open-coding approach~\cite[Chapter 8]{strauss1998basics}. An initial codebook was formed by merging these codes and refined through discussion. To align divergent observations, coders identified overlapping codes, clarified code definitions, and excluded codes not aligned with the research objectives. Each coder then independently applied the established codes to half of the remaining data. Emergent higher-level themes were discussed and iteratively induced from the codes, informing the following findings.
