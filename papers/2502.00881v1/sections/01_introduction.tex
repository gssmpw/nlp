\section{Introduction}
Reviewing and building upon existing knowledge is fundamental to scholarly research. However, with the acceleration of research production and publication, keeping at the forefront of all available literature has become increasingly complex. Literature reviews offer tremendous value in synthesizing research, but are time-consuming and resource-intensive to create~\cite{tricco2008following, michelson2019significant}. Moreover, they quickly become outdated as new research emerges~\cite{Shekelle2001ValidityOT}. One survival analysis suggests some reviews may be outdated by the time of their publication and over 25\% require updating just two years after publication~\cite{shojania2007quickly}. Such information staleness presents a threat to validity, potentially under-informing researchers about research opportunities or critically misleading decision-makers.

In response, researchers have explored the concept of ``living reviews''---documents that are continually updated as new evidence emerges~\cite{elliott2014living, wijkstra2021livinglitreviews, cochrane2019handbook}, typically centered around living \textit{systematic} reviews. Maintaining living reviews can be challenging, however, with studies finding many are never updated after initial publication~\cite{tricco2008following, heron2023update}. Prior work on living reviews has primarily focused on accelerating literature discovery and appraisal~\cite{thomas2017living, vergara2020living}, but recent advances in artificial intelligence (AI) and large language models suggest an opportunity for supporting more complex cognitive aspects of updating reviews, such as evidence synthesis~\cite{martinboyle2024shallow, wang2024autosurvey}. Such capabilities are particularly relevant for living \textit{narrative} reviews, such as surveys in computing research, which rely more on expert interpretation and conceptual synthesis than the standardized protocols and meta-analyses of systematic reviews~\cite{snyder2019literature}.

To examine how narrative reviews are created and maintained in practice, we conducted semi-structured, retrospective interviews with 11 survey authors across diverse areas of computing research. Authors reflected on their processes and points of friction throughout their authoring and revision workflows, and shared their perspectives on when, how, and with what content these reviews should be updated. In understanding these practices, we sought to identify opportunities to support the updating process, including the potential role of AI assistance.

Our findings reveal the varied methodologies used and challenges encountered when authoring and updating survey articles, especially in paper discovery, taxonomy development, and synthesis. We identify three key types of updates for maintaining narrative reviews: 1) empirical updates involving evidence and examples, 2) structural updates to taxonomies and paper organization, and 3) interpretive updates to syntheses and framing. Each type offers distinct opportunities for AI assistance, from routine tasks like recalculating numerical values to more complex support for identifying potential biases and emerging research gaps. While authors saw the potential of AI to assist with routine tasks, they were skeptical of its ability to handle more nuanced interpretive tasks like constructing a compelling narrative. Moreover, the subjective, expertise-driven nature of their workflows raises important considerations for future tools enabling living narrative reviews.