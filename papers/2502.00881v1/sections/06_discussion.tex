\section{Discussion}

\begin{table*}[t]
\centering
\small
\renewcommand{\arraystretch}{1.3}
\caption{Opportunities for supporting various updates to narrative literature reviews.}
\begin{tabular}{p{0.1\textwidth}p{0.2\textwidth}p{0.6\textwidth}}
\toprule
\textbf{Update Type} & \textbf{Description} & \textbf{Opportunities for AI Assistance in Updating} \\
\midrule
Empirical & Revision of quantitative and qualitative evidence presented in text, tables, and figures & 
\begin{minipage}[t]{\linewidth}
    \begin{itemize}[leftmargin=*]
        \item Identify and recalculate numerical values or meta-analyses
        \item Update references to evidence in figures or tables
        \item Change or revise representative examples in text
    \end{itemize}
    \vspace{0.3em}
\end{minipage} \\
Structural & Revision of paper organization, taxonomies, and frameworks & 
\begin{minipage}[t]{\linewidth}
    \begin{itemize}[leftmargin=*]
        \item Reevaluate included evidence and definition of an existing taxonomy dimension
        \item Group emerging research to align with or challenge existing taxonomy dimensions 
        \item Identify opportunities to improve paper organization (e.g., by splitting sections that exceed a threshold of research)
    \end{itemize}
    \vspace{0.3em}
\end{minipage} \\
Interpretative & Revision of evidence synthesis, interpretation, and narrative framing & 
\begin{minipage}[t]{\linewidth}
    \begin{itemize}[leftmargin=*]
        \item Identify potential biases, assumptions, and narratives in the original survey that may be validated, challenged, or updated in light of new evidence or technologies
        \item Analyze the extent to which prior gaps and limitations in the research have been addressed
        \item Propose new unresolved challenges or under-explored areas
    \end{itemize}
    \vspace{0.3em}
\end{minipage} \\
\bottomrule
\end{tabular}
\label{tab:updates_and_ai_support}
\end{table*}

\subsection{Summary of Findings}
Our study examined the work practices within survey article authoring and the challenges researchers foresee in maintaining their currency. Our findings reiterate four key processes of review---search, appraisal, synthesis, and interpretation---and highlight the many inherent subjective, expertise-driven decisions and idiosyncratic strategies. Participants noted the value in keeping their surveys updated for the research community and described three main types of updates they would make: empirical, structural, and interpretative. Participants characterized updating as a process of restoring institutional knowledge and attempting to adapt and reuse their original workflows. For instance, new research can be identified through established search methods and integrated into an existing taxonomy, while sufficient research could further motivate deeper structural and interpretative revisions.

Despite the perceived value, participants also expressed how continuous survey updating was infeasible and misaligned with existing academic incentives. Many therefore recognized the potential for AI support to lower the costs of updating, such as by helping to identify new, relevant papers or automating repetitive information extraction tasks, though skeptical of its ability to replace their own learned expertise for synthesizing evidence. Our findings, while focused on scholars in the computing field, align with perspectives from other empirical studies involving scholars across diverse disciplines~\cite{messeri2024artificial, morris_scientists_2023, chubb2022speeding}. For instance, \citet{messeri2024artificial} find that objective tasks are more conducive to helping scholars establish appropriate trust in AI assistance, and several other studies similarly suggest scholars are more receptive to AI's role in narrow tasks that boost personal productivity but are hesitant to rely on AI in ``emotional tasks'' that require creativity and complex decision-making. For these nuanced tasks, participants instead see AI as suitable to ``augment and assist human judgment''~\cite{chubb2022speeding}.


\subsection{Toward a Vision of Living Narrative Reviews}

\subsubsection{Considerations for AI-Assisted Updating of Narrative Reviews}
Our findings offer several considerations for how future AI systems could aid updating workflows. First, systems could help determine \textit{when} an update is needed by continuously monitoring new research and identifying those relevant to existing taxonomies and syntheses. They could further explain \textit{why} new research is relevant, such as by highlighting how it aligns or challenges existing parts of a survey. Updating efficiency could be improved by localizing \textit{where} updates are needed and directing authors' attention to specific components---text, figures, or tables---that should be revised. This could be especially helpful for cascading revisions, as participants noted, since integrating even a single paper may require updating many parts of a survey, such as where it is introduced, synthesized with related research, and visualized within tables or figures. Finally, systems could offer guidance on \textit{how} to integrate new research into a survey, for instance by identifying the type of update required---empirical, structural, or interpretive---and providing tailored revision recommendations.

These three different types of updates in narrative reviews also present unique challenges that AI systems could help address. For empirical updates, systems could help recalculate numerical values and meta-analyses given new research, where providing transparency to allow researchers to verify the updated evidence is important. More challenging is supporting potential structural updates, such as determining when new research is sufficient to extend or revise a taxonomy and its dimensions, which participants felt required a deep and nuanced understanding of how new research may reshape the overarching conceptual framing of a research area. Future AI systems could also likely aid in interpretative updates, for instance by meaningfully synthesizing literature within a taxonomy dimension, reevaluating gaps in a research area, or informing an evolving agenda for future research. These technologies are promising yet nascent, with recent studies echoing participants' concerns regarding the shallow nature of AI-generated literature syntheses~\cite{martinboyle2024shallow}. Overall, narrative review updating remains for now a collaboration of AI and human effort, though given the high costs and low incentives of making frequent updates, realizing living narrative reviews may eventually require a concentration of human effort on verifying and steering AI-assisted updates.

\subsubsection{Systematicity and Expertise in Narrative Reviews}
Designing future systems to enable living narrative reviews requires balancing systematicity with the subjective, expertise-driven nature of survey authoring. While evidence-based disciplines like clinical medicine emphasize systematic methodologies---such as transparent, repeatable methods for identifying, appraising, and synthesizing literature---these standards may not fully align with semi-systematic methods such as in computing research surveys. Participants described workflows marked by subjective decisions informed by years of specific research expertise, across literature discovery, taxonomy creation, synthesis, and narrative construction. These choices, shaped by implicit knowledge and anticipation of reader needs, were rarely documented in surveys themselves, posing challenges for both subsequent human-driven and AI-supported updates. Without sufficient systematicity or documentation, restoring the institutional knowledge of a research team or replicating the original idiosyncratic processes to perform consistent updates becomes significantly harder.

Future AI systems may bridge this gap by supporting authors in externalizing the implicit strategies underlying their subjective decisions. Interactive tools could invite authors to articulate their search and synthesis methodologies more explicitly, clarify decision-making criteria, and document expertise-driven strategies, such as when participants described targeting specific journals or research groups. An exciting direction for future research lies in the design of mixed-initiative systems to then leverage these explicit methodologies and infer patterns in existing surveys to provide updating assistance aligned with the author’s narrative and methodological intent, while making transparent any inferred subjective choices. Altogether, these approaches can serve to encourage systematicity in survey authoring and ensure subsequent updates to a survey are efficient, reliable, and consistent with prior survey iterations.


\subsection{Limitations and Future Work}
The retrospective nature of the interviews could introduce recall bias, as participants may have selectively reported the most salient challenges, overlooking others. Future work could use more direct observational methods, such as contextual inquiry, to capture authors’ real-time processes as they update their literature reviews. To complement authors' perspectives on continuously updated reviews, additional studies could investigate \textit{readers}’ needs. For instance, what form should a living review take and how might new affordances and interactions facilitate their use? Similar techniques could also aid peer review of review updates, which remains an open challenge for realizing living reviews.
