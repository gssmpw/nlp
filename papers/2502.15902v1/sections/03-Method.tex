In this section, we illustrate our method step by step. First, we introduce the overall workflow. After that, we demonstrate the details of supervised fine-tuning (SFT) the \textbf{Prompt Inverter} and \textbf{Distinguisher}
%will be illustrated
.

\subsection{Workflow}
IPAD consists of a ~\textbf{Prompt Inverter} and a ~\textbf{Distinguisher}, both fine-tuned on Microsoft's open model Phi3-medium-128k-instruct, which together form a complete detection workflow as illustrated in Figure~\ref{fig:workflow}. For the \textbf{Distinguisher}, we develop two models and examine them in Section ~\ref{sec:Evaluation}.


The \textit{Input Text} (\( T \)) is either human-written (HWT) or LLM-generated (LGT), and it is processed by the \textbf{Prompt Inverter} to predict the most likely prompt that could have generated it. This \textit{Predicted Prompt} (\( P \)) is assumed to be the input that an LLM would have used to produce the text. \[P = f_{\text{inv}}(T)\]  where \( f_{\text{inv}} \) stands for \textbf{Prompt Inverter}.

For the next step, the \textit{Predicted Prompt} (\( P \)) is fed into an LLM (we use ChatGPT, i.e. gpt-3.5-turbo by default, and other LLMs for evaluations), to generate a corresponding \textit{Regenerated Text} (\( T' \)). 
\[
T' = f_{\text{LLM}}(P)
\]  
After that, we consider two \textbf{Distinguishers}. The first one is ~\textit{Prompt-Text Consistency Verifier}, in which the \textit{Input Text} (\( T \)) and the \textit{Predicted Prompt} (\( P \)) are passed to the model.

The ~\textit{Prompt-Text Consistency Verifier} determines whether the \textit{Predicted Prompt} (\( P \)) can reasonably generate the given \textit{Input Text} (\( T \)) using an LLM. The model outputs either a ~\textbf{"yes"} or ~\textbf{"no"} response. If the \textit{Predicted Prompt} (\( P \)) is likely to produce the \textit{Input Text} (\( T \)) when fed into the LLM, the model is expected to output ~\textbf{"yes"}, indicating that the \textit{Input Text} (\( T \)) is likely LGT. Conversely, if the  \textit{Predicted Prompt} (\( P \)) does not align well with the \textit{Input Text} (\( T \)), the model outputs ~\textbf{"no"}, suggesting that the \textit{Input Text} (\( T \)) is less likely to have been generated by the LLM with the \textit{Predicted Prompt} (\( P \)), and is therefore more likely to be HWT.

\[
S = f_{\text{PTCV}}(T, P)
\]  
where \( f_{\text{PTCV}} \) stands for \textit{Prompt-Text Consistency Verifier} in the ~\textbf{Distinguisher}.

The second \textbf{Distinguisher} is ~\textit{Regeneration Comparator}, which considers both the \textit{Input Text} (\( T \)) and the \textit{Regenerated Text} (\( T' \)).

The ~\textit{Regeneration Comparator} determines whether the \textit{Input Text} (\( T \)) aligns with the \textit{Regenerated Text} (\( T' \)), and then outputs either a \textbf{"yes"} or \textbf{"no"} response. If the \textit{Input Text} (\( T \)) is LGT, the model is expected to output "yes," which indicates that both the \textit{Input Text} (\( T \)) and the \textit{Regenerated Text} (\( T' \)) were generated by an LLM from similar prompts. Conversely, if the \textit{Input Text} (\( T \)) is HWT, the model is expected to output "no," which signifies that the \textit{Input Text}  (\( T \)) is meaningfully distinct from the \textit{Regenerated Text} (\( T' \)) and thus unlikely to have been generated by an LLM.
\[
S = f_{\text{RC}}(T, T')
\]  
where \( f_{\text{RC}} \) stands for ~\textit{Regeneration Comparator} in the \textbf{Distinguisher}.

Finally, for both \textbf{Distinguishers},
\[
\hat{Y} = 
\begin{cases} 
\text{LGT}, & \text{if } S = \text{Yes} \\ 
\text{HWT}, & \text{if } S = \text{No} 
\end{cases}
\]  
where \( \hat{Y} \) is the final decision of the \textit{Input Text} (\( T \)).



\subsection{Datasets}
\subsubsection{Prompt Inverter}
The datasets used to fine-tune the \textbf{Prompt Inverter} include several widely adopted resources in the field. These are:
\begin{itemize}[itemsep=1pt, topsep=1pt]
\item \textbf{Instructions-2M}~\cite{r5}, a collection of 2 million user and system prompts, from which we used 30,000 prompts.
\item \textbf{ShareGPT}~\cite{r6}, an open platform where users share ChatGPT prompts and responses, from which we used 500 samples.
\item \textbf{Unnatural Instructions}~\cite{r6}, a dataset of diverse, creative instructions generated by OpenAIâ€™s text-davinci-002, from which we used 500 samples.
\item \textbf{OUTFOX dataset}~\cite{r3}, which contains 15,400 essay problem statements, student-written essays, and LLM-generated essays.
\end{itemize}
The first three datasets aims to enhance the general querying capability of the \textbf{Prompt Inverter}, and are all released under the MIT license. All the samples we used are the same to the samples randomly selected in ~\cite{r4}. The last dataset aims to enhance the familiarity of the \textbf{Prompt Inverter} with the data of the essay to detect the LLM-generated essays, and are created and examined by ~\citet{r3}, We specifically used the LLM-generated essays and problem statements for this supervised fine-tuning (SFT).

For all 45,400 training pairs, the format is standardized as follows:  
\textbf{Instruction}: \textit{"Predict the prompt of the Input Text."} \textbf{Input}: \textit{\{LGT\} or \{HWT\}} \textbf{Output}: \textit{\{Corresponding prompt\}}.


\subsubsection{Distinguishers}
Given that essay data are diverse, we utilize only the OUTFOX dataset~\cite{r3}. To adapt this dataset for training our \textbf{Distinguisher}, we enhance it to align with the model's requirements. The original dataset consists of 14,400 training triplets of essay problem statements, student-written essays, and LLM-generated essays. To further process the data, we apply the \textbf{Prompt Inverter} to both student-written and LLM-generated essays, generating corresponding \textit{Predicted Prompts}. These \textit{Predicted Prompts} are then used to regenerate texts via \textbf{ChatGPT}, i.e. \textbf{gpt-3.5-turbo}.  

The final dataset is structured as follows:  


Distinguisher version1 - \textit{Prompt-Text Consistency verifier}: \textbf{Instruction}:\textit{"Can LLM generate text2 through the prompt text1? "}  
\textbf{Input}: \textit{text 1: \{Predicted Prompt\}; text 2: \{LGT\} (or \{HWT\})}    
\textbf{Output}: \textit{yes} (or \textit{no})  

Distinguisher version2 - \textit{Regeneration Comparator}: \textbf{Instruction}: \textit{"Text 1 is generated by an LLM. Determine whether Text 2 is also generated by an LLM with a similar prompt."} 
\textbf{Input}: \textit{text 1: \{Regenerated Text\}; text 2: \{LGT\} (or \{HWT\})}  
\textbf{Output}: \textit{yes} (or \textit{no})  

Following this procedure, we construct a total of 28,800 training samples, with an equal distribution of positive and negative examples (14,400 each).

\subsection{Training}
The supervised fine-tuning (SFT) ~\cite{r70} process is performed on a dataset comprising the above-mentioned 45,400 pairs for \textbf{Prompt Inverter} and 28,800 pairs for both \textbf{Distinguishers}. We utilize Microsoft's open model, \textit{phi3-medium-128k-instruct}, and we use low-rank adaptation (LoRA) method ~\cite{r69} on the \textit{LLaMA-Factory} framework\footnote{https://huggingface.co/papers/2403.13372}~\cite{r68}. We train it using six A800 GPUs for 20 hours for \textbf{Prompt Inverter}, 7 hours for Distinguisher version1, and 4 hours for Distinguisher version2. 