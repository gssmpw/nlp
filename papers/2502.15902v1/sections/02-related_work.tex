\subsection{AI detectors Methods and challenges}
AI text detection methods can be broadly categorized into four approaches~\cite{r12}: watermarking, statistics-based methods, neural-based methods, and human-assisted methods.

~\textbf{Watermarking technology} inserts specific patterns into training datasets~\cite{r27,r28} or manipulates the model output during inference to embed a watermark~\cite{r29}. However, watermarking needs to access of the LLM deployment and can face attacks, such as identifying and erasing the watermark~\cite{r30}. ~\textbf{Statistics-based methods} analyze inherent textual features to identify language patterns~\cite{r31,r32}, but their effectiveness depends on corpus size and model diversity~\cite{r12}. Some other statistical methods use n-gram probability divergence~\cite{r36} or similarity between original and revised texts~\cite{r37,r38} while still face robustness challenges under adversarial attacks~\cite{r12}.~\textbf{Neural-based methods} such as RoBERTa~\cite{r42}, Bert~\cite{r44}, and XLNet~\cite{r45} have been robust in domain-specific tasks. Adversarial learning techniques are increasingly being used~\cite{r46} to increase effectiveness in attacked datasets.

In addition to automated methods, human involvement plays a key role in detecting AI-generated text~\cite{r12}. ~\textbf{Human-assisted detection} leverages human intuition and expertise to identify inconsistencies such as semantic errors and logical flaws that may not be easily caught by algorithms~\cite{r49,r50}. Moreover, given the challenges of current AI detection tools, which often lack verifiable evidence~\cite{r22}, human involvement becomes even more critical to ensure the reliable and explainable detection.

\subsection{Prompt Inverter techniques and applications}
Prompt extraction techniques aim to reverse-engineer the prompts that generate specific outputs from LLMs. Approaches include black-box methods like output2prompt~\cite{r4}, which extracts prompts based on model outputs without access to internal data, and logit-based methods like logit2prompt~\cite{r51}, which rely on next-token probabilities but are constrained by access to logits. Adversarial methods can bypass some defenses but are model-specific and fragile~\cite{r52}. Despite the success of some zero-shot LLM-inversion based methods~\cite{r53, r62}, they are mostly naive usage of prompting LLMs, which makes them poor in prompt extraction accuracy and robustness.