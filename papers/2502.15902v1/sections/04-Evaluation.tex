We investigate the following questions through our experiments:  

\begin{itemize}[itemsep=1pt, topsep=1pt]

\item[$\bullet$]Assess the robustness of IPAD (using various LLMs as generators, comparing with other detectors, and evaluating on out-of-distribution (OOD) datasets).  
\item[$\bullet$]Independently analyze the necessity and effectiveness of the \textbf{Prompt Inverter} and the \textbf{Distinguishers}.  
\item[$\bullet$]Explore the explainability of IPAD (through a user study and analysis of linguistic differences between prompts generated by HWT and LGT).
\end{itemize}

\subsection{Robustness of IPAD}
\subsubsection{Evaluation Baselines and Metrics}
The in-distribution experiments refer to the testing results presented in ~\cite{r3}, where the data aligns with the training data used for the IPAD ~\textbf{Distinguishers}, thereby serving as our baseline. The OOD experiments refer to the DetectRL baseline ~\cite{r58}, which is a comprehensive benchmark consisting of academic abstracts from the arXiv Archive (covering the years 2002 to 2017)\footnote{http://kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data}, news articles from the XSum dataset ~\cite{r59}, creative stories from Writing Prompts ~\cite{r60}, and social reviews from Yelp Reviews~\cite{r61}. It also employs three attack methods to simulate complex real-world detection scenarios, which includes the prompt attacks, paraphrase attacks, and perturbation attacks~\cite{r58}. All the testing sets have 1,000 samples in our experiments.

The ~\textbf{Area Under Receiver Operating Characteristic curve (AUROC)} is widely used for assessing detection method ~\cite{r55} because it considers the True Positive Rate (TPR) and False
Positive Rate (FPR) across different classification thresholds. Since our models predicts binary labels, we follow the ~\textit{Wilcoxon-Mann-Whitney} statistic~\cite{r56}, and the formula is shown in appendix ~\ref{sec:AUROC formula}. 
The ~\textbf{AvgRec} is the average of ~\textbf{HumanRec} and ~\textbf{MachineRec}. In our evaluation, ~\textbf{HumanRec} is the recall for detecting Human-written texts, and ~\textbf{MachineRec} is the recall for detecting LLM-generated texts~\cite{r57}.  The ~\textbf{F1 Score} provides a comprehensive evaluation of detector capabilities by balancing the modelâ€™s Precision and Recall. We use ~\textbf{AvgRec} and ~\textbf{F1} on in-distribution data, and we use ~\textbf{AUROC} for OOD data to align the test benchmarks for the same dataset.


\subsubsection{Robustness across different LLMs}
The results of IPAD for detecting the dataset OUTFOX~\cite{r3} across LLMs are presented in Table \ref{tab:performance_metrics_setting1} and Table \ref{tab:performance_metrics_setting2}, respectively. They show that both versions are highly robust across various LLMs, while ~\textit{Regeneration Comparator} is a bit more efficient. 

As for ~\textit{Regeneration Comparator}, when the original generator and re-generator are the same model, the performance is optimal. However, even when the re-generator is different from the original generator, the results remain impressive with ChatGPT used as the re-generator. These results imply that, in practical applications, it is possible to use a common set of LLMs as re-generators. If one or more correponding ~\textbf{Distinguishers} from different LLMs classify the results as 'yes', it can be inferred that the text is likely to be LGT, whereas if all ~\textbf{Distinguishers} classify the results as 'no', the text is more likely to be HWT. Furthermore, for applications aiming to save computational resources and improve efficiency, using ChatGPT as the sole re-generator still yields robust performance across all tested models.

\begin{table}[ht!]
  \centering
  \resizebox{0.5\textwidth}{!}{%
    \begin{tabular}{ccccc}
      \hline
      \multirow{2}{*}{\textbf{Original Generator}} &  \multicolumn{4}{c}{\textbf{Metrics (\%)}} \\
      \cline{2-5}
      & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1} \\
      \hline
      ChatGPT     & 98.00\% & 99.80\%  & 98.90\% & 98.89\% \\
      \hline
      GPT-3.5     & 97.20\% & 99.90\%  & 98.55\% & 98.53\% \\
      \hline
      Qwen-turbo  & 98.00\% & 98.10\%  & 98.05\% & 98.05\% \\
      \hline
      Llama-3-70B & 98.00\% & 100.00\% & 99.00\% & 98.99\% \\
      \hline
    \end{tabular}%
  }
  \caption{IPAD with ~\textit{Prompt-Text Consistency Verifier} performance on different LLMs}
  \label{tab:performance_metrics_setting1}
\end{table}

\begin{table}[ht!]
  \centering
  \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{cccccc}
      \hline
      \multirow{2}{*}{\textbf{Original Generator}} & \multirow{2}{*}{\textbf{Re-Generator}} & \multicolumn{4}{c}{\textbf{Metrics (\%)}} \\
      \cline{3-6}
      & & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1} \\
      \hline
      ChatGPT & ChatGPT & 99.70\% & 100.00\% & \textbf{99.85\%} & \textbf{99.85\%} \\
      \hline
      GPT-3.5 & GPT-3.5 & 98.00\% & 100.00\% & ~\textbf{99.00\%} & ~\textbf{99.00\%} \\
      & ChatGPT & 97.00\% & 100.00\% & 98.50\% & 98.50\% \\
      \hline
      Qwen-turbo & Qwen-turbo & 98.00\% & 98.40\% & ~\textbf{98.20\%} & ~\textbf{98.20\%} \\
      & ChatGPT & 99.70\% & 94.40\% & 97.05\% & 97.13\% \\
      \hline
      Llama-3-70B & Llama-3-70B & 96.60\% & 100.00\% & 98.30\% & 98.30\% \\
      & ChatGPT & 99.70\% & 99.40\% & ~\textbf{99.55\%} & ~\textbf{99.55\%} \\
      \hline
    \end{tabular}
  }
  \caption{IPAD with ~\textit{Regeneration Comparator} performance on different LLMs}
  \label{tab:performance_metrics_setting2}
\end{table}


\subsubsection{Comparison of IPAD with other detectors in and out of distribution}
Table \ref{tab:performance_metrics_detection} compares the performance of two versions of IPAD with other detection methods in the OUTFOX dataset with and without attacks~\cite{r3}. The results show that both versions of IPAD generally outperform other detectors, while that IPAD with ~\textit{Prompt-Text Consistency Verifier} for detecting ChatGPT with DIPPER attack performs worse. These results imply that IPAD with ~\textit{Regeneration Comparator} demonstrates superior robustness compared to alternative detection methods in the OUTFOX dataset with and without attacks.

\begin{table}[ht!]
  \centering
  \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{cccccc}
      \hline
      \multirow{2}{*}{\textbf{Original Generator}} & \multirow{2}{*}{\textbf{Detection Methods}} & \multicolumn{4}{c}{\textbf{Metrics (\%)}} \\
      \cline{3-6}
      & & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1} \\
      \hline
      ChatGPT & RoBERTa-base & 93.80\% & 92.20\% & 93.00\% & 92.90\% \\
      & RoBERTa-large & 91.60\% & 90.00\% & 90.80\% & 90.70\% \\
      & HC3 detector & 79.20\% & 70.60\% & 74.90\% & 73.80\% \\
      & OUTFOX & 97.80\% & 92.40\% & 95.10\% & 95.00\% \\
      & IPAD version1 & 98.00\% & 99.80\% & 98.90\% & 98.89\%\\
      & IPAD version2& 99.70\% & 100.00\% & \textbf{99.85\%} & \textbf{99.85\%} \\
      \hline
      GPT-3.5 & RoBERTa-base & 93.80\% & 92.00\% & 92.90\% & 92.80\% \\
      & RoBERTa-large & 92.60\% & 92.00\% & 92.30\% & 92.30\% \\
      & HC3 detector & 79.20\% & 85.00\% & 82.10\% & 82.60\% \\
      & OUTFOX & 97.60\% & 96.20\% & 96.90\% & 96.90\% \\
      & IPAD version1 & 97.20\% & 99.90\% & \textbf{98.55\%} & \textbf{98.53\%}\\
      & IPAD version2& 97.00\% & 100.00\% & 98.50\% & 98.50\% \\
      \hline
      ChatGPT with DIPPER Attack & RoBERTa-base & 93.80\% & 89.20\% & 91.50\% & 91.30\% \\
      & RoBERTa-large & 91.60\% & 97.00\% & 94.30\% & 94.40\% \\
      & HC3 detector & 79.20\% & 3.40\% & 41.30\% & 5.50\% \\
      & OUTFOX & 98.60\% & 66.20\% & 82.40\% & 79.00\% \\
      & IPAD version1 & 98.00\% & 75.10\% & 86.55\% & 87.93\%\\
      & IPAD version2& 99.70\% & 95.40\% & \textbf{97.55\%} & \textbf{97.60\%} \\
      \hline
      ChatGPT with OUTFOX Attack & RoBERTa-base & 93.80\% & 69.20\% & 81.50\% & 78.90\% \\
      & RoBERTa-large & 91.60\% & 56.20\% & 73.90\% & 68.30\% \\
      & HC3 detector & 79.20\% & 0.40\% & 39.80\% & 0.70\% \\
      & OUTFOX & 98.80\% & 24.80\% & 61.80\% & 39.40\% \\
      & IPAD version1 & 98.00\% & 95.40\% & 96.70\% & 96.74\%\\
      & IPAD version2& 99.70\% & 98.00\% & \textbf{98.85\%} & \textbf{98.86\%} \\
      \hline
    \end{tabular}
    }
  \caption{Comparison of IPAD with other detectors on in-distribution data, where ~\textbf{IPAD version1} stands for ~\textbf{IPAD with ~\textit{Prompt-Text Consistency Verifier}} and ~\textbf{IPAD version2} stands for ~\textbf{IPAD with ~\textit{Regeneration Comparator}}}
  \label{tab:performance_metrics_detection}
\end{table}

Table \ref{tab:OOD_performance} presents the performance of various detection methods on OOD datasets to assess their generalizability, where the baseline data refer to DetectRL ~\cite{r58}.  The results demonstrate that IPAD with ~\textit{Regeneration Comparator} consistently outperforms all other baselines in all OOD datasets with and without attacks. In contrast, IPAD with ~\textit{Prompt-Text Consistency Verifier} exhibits strong performance on OOD datasets without attacks but shows a noticeable drop in effectiveness when subjected to attacks. For instance, while it achieves competitive results on datasets like XSum (99.90\%) and Writing (99.20\%), its performance against attacks, such as Prompt Attack (86.90\%) and Paraphrase Attack (82.72\%), is significantly lower than IPAD with ~\textit{Regeneration Comparator}. This suggests that \textbf{IPAD with ~\textit{Regeneration Comparator} demonstrates better generalizability and robustness.}

\begin{table}[h]
    \centering
    \resizebox{0.5\textwidth}{!}{ 
    \begin{tabular}{cccccccc}
        \toprule
        \hline
        \multirow{2}{*}{\textbf{OOD Datasets or attack type}} & \multicolumn{5}{c}{\textbf{Detection Methods}} \\
        \cmidrule(lr){2-6}
         & \textbf{LRR} & \textbf{Fast-DetectGPT} & \textbf{Rob-Base} & IPAD with version1 & IPAD version2 \\
        \midrule
        \hline
        Arxiv & 48.17\% & 42.00\% & 81.06\% & 84.47\% & \textbf{98.60\%} \\
        XSum & 48.41\% & 45.72\% & 76.81\% & \textbf{99.90\%} & 98.90\% \\
        Writing & 58.70\% & 51.13\% & 86.29\%& \textbf{99.20\%} & 95.80\% \\
        Review & 58.21\% & 54.55\% & 87.84\% &98.50\% & \textbf{89.30\%} \\
        \hline
        Avg. for non-attacked datasets & 53.37\% & 48.35\% & 83.00\% &95.52\% & \textbf{95.65\%} \\
        \hline
        Prompt Attack & 54.97\% & 43.89\% & 92.81\%& 86.90\% & \textbf{93.05\%}\\
        Paraphrase Attack & 49.23\% & 41.15\% & 90.02\%&82.72\% & \textbf{95.89\%}\\
        Perturbation Attack & 53.62\% & 44.38\% & 92.12\% & 94.96\% & \textbf{95.32\%} \\
        \hline
        Avg. for attacked datasets & 52.61\% & 43.14\% & 91.65\% & 88.26\% & \textbf{94.75\%}\\
        \hline
        Avg. & 53.04\% & 46.12\% & 86.70\%&92.41\%&\textbf{95.26\%}\\
        \hline
        \bottomrule
    \end{tabular}
    }
    \caption{The performance of IPAD in generalization assessment (AUROC). The selected detectors are evaluated on OOD data, all sourced from and processed using the DetectRL baseline, where ~\textbf{IPAD version1} stands for ~\textbf{IPAD with ~\textit{Prompt-Text Consistency Verifier}} and ~\textbf{IPAD version2} stands for ~\textbf{IPAD with ~\textit{Regeneration Comparator}.}}
    \label{tab:OOD_performance}
\end{table}

\vspace{-0.3cm}

\subsubsection{Robustness conclusion}

Our experimental results demonstrate that both IPAD versions exhibit strong performance across different LLMs, outperforming existing detection methods and maintaining robustness on OOD datasets. The IPAD with ~\textit{Regeneration Comparator} outperforming baselines by 9.73\% (F1-score) on in-distribution data and 12.65\% (AUROC) OOD data. Notably, IPAD with ~\textit{Regeneration Comparator} achieves significantly better performance than IPAD with ~\textit{Prompt-Text Consistency Verifier} in attack scenarios of 3.78\% (F1-score). While IPAD with ~\textit{Prompt-Text Consistency Verifier} performs robustly in standard settings, its performance declines when facing attacks. The calculation of these statistics are shown in Appendix ~\ref{Calculation}.
%

\vspace{-0.3cm}
\subsection{Necessity and Effectiveness of \textbf{Prompt Inverter} and \textbf{Distinguishers}}

\subsubsection{Necissity of the \textbf{Prompt Inverter} and \textbf{Distinguishers}}
To prove that it is necessary to fine-tune on IPAD with IPAD with ~\textit{Prompt-Text Consistency Verifier} and ~\textit{Regeneration Comparator}, we conducted ablation study to use the same finetune method on only ~\textit{input texts} and only ~\textit{predicted prompts}. The instructions are ~\textit{"Is this text generated by LLM?"}, and ~\textit{"Prompt Inverter predicts prompt that could have generated the input texts. Is this prompt predicted by an input texts written by LLM?"}, respectively.

The results shown in Figure ~\ref{fig:ablation} from the ablation study show that fine-tuning on either only the ~\textit{input text} or only the ~\textit{predicted prompt} leads to poor performance. This underscores the importance of fine-tuning on a combination of both the input text and predicted prompt, as explored in the ~\textit{Prompt-Text Consistency Verifier}, or on the input text and regenerated text, as examined in the ~\textit{Regeneration Comparator}, for more effective detection.
\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{ablation.png}
  \caption{Ablation Study Results. The ~\textbf{IPAD version1} stands for ~\textbf{IPAD with ~\textit{Prompt-Text Consistency Verifier}} and ~\textbf{IPAD version2} stands for ~\textbf{IPAD with ~\textit{Regeneration Comparator}.}}
  \label{fig:ablation}
\end{figure}
\vspace{-0.3cm}
% \begin{table}[ht!]
%   \centering
%   \resizebox{0.5\textwidth}{!}{
%     \begin{tabular}{cccccc}
%       \hline
%       \multirow{}{}{\textbf{Original Generator}} & \multirow{}{}{\textbf{Detection Methods}} & \multicolumn{4}{c}{\textbf{Metrics (\%)}} \\
%       \cline{3-6}
%       & & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1} \\
%       \hline
%       ChatGPT & Finetune with only Input & 10.80\% & 12.20\% & 11.00\% & 92.90\% \\
%       & Finetune with only Prompt & 91.60\% & 90.00\% & 90.80\% & 90.70\% \\
%       & IPAD Setting1 & 98.00\% & 99.80\% & 98.90\% & 98.89\%\\
%       & IPAD Setting2& 99.70\% & 100.00\% & \textbf{99.85\%} & \textbf{99.85\%} \\
%       \hline
%       GPT-3.5 & Finetune with only Input & 00\% & 00\% & 00\% & 00\% \\
%       & Finetune with only Prompt & 91.60\% & 90.00\% & 90.80\% & 90.70\% \\
%       & IPAD Setting1 & 98.00\% & 99.80\% & 98.90\% & 98.89\%\\
%       & IPAD Setting2& 99.70\% & 100.00\% & \textbf{99.85\%} & \textbf{99.85\%} \\
%       \hline
%       Qwen & 93.80\% & 92.20\% & 93.00\% & 92.90\% \\
%       & Finetune with only Prompt & 91.60\% & 90.00\% & 90.80\% & 90.70\% \\
%       & IPAD Setting1 & 98.00\% & 99.80\% & 98.90\% & 98.89\%\\
%       & IPAD Setting2& 99.70\% & 100.00\% & \textbf{99.85\%} & \textbf{99.85\%} \\
%       \hline
%       LLAMA & Finetune with only Input & 93.80\% & 92.20\% & 93.00\% & 92.90\% \\
%       & Finetune with only Prompt & 91.60\% & 90.00\% & 90.80\% & 90.70\% \\
%       & IPAD Setting1 & 98.00\% & 99.80\% & 98.90\% & 98.89\%\\
%       & IPAD Setting2& 99.70\% & 100.00\% & \textbf{99.85\%} & \textbf{99.85\%} \\
%       \hline
%     \end{tabular}
%     }
%   \caption{Ablation study (DATA NOT COMPLETED)}
%   \label{tab:performance_metrics_detection}
% \end{table}

\subsubsection{The effectivenss of the IPAD \textbf{Prompt Inverter}}

We use DPIC~\cite{r62} and PE~\cite{r65} as baseline methods for prompt extraction. DPIC employs a zero-shot approach using the prompt states in Appendix ~\ref{sec:DPIC prompt}, while PE uses adversarial attacks to recover system prompts.

In our evaluation, we tested 1000 LGT and 1000 HWT samples. We use only in-distribution data for testing since only these datasets include original prompts. The metrics are all tested on comparing the similarity of the original prompts and the predicted prompts. The results shown in Table ~\ref{tab:model_comparison} illustrate that IPAD consistently outperforms both DPIC and PE across all four metrics (BartScore~\cite{r64}, Sentence-Bert Cosine Similarity~\cite{r63}, BLEU~\cite{r66}, and ROUGE-1~\cite{r67}), which highlight the effectiveness of the IPAD ~\textbf{Prompt Inverter}.

\begin{table}[htbp]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|c|c|c|c}
\hline
\textbf{Evaluation} & \textbf{Bart-large-cnn} & \textbf{Sentence-Bert} & \textbf{BLEU} & \textbf{ROUGE-1} \\
\hline
\multicolumn{5}{c}{\textbf{LGT}} \\
\hline
DPIC & -2.12 & 0.46 & 5.61E-05 & 0.04 \\
PE & -2.23 & 0.58 & 3.21E-04 & 0.25 \\
IPAD & ~\textbf{-1.84} & ~\textbf{0.69} & ~\textbf{0.24} & ~\textbf{0.51} \\
\hline
\multicolumn{5}{c}{\textbf{HWT}} \\
\hline
DPIC & -2.47 & 0.42 & 8.75E-06 & 0.06 \\
PE & -2.39 & 0.53 & 2.56E-08 & 0.13 \\
IPAD & ~\textbf{-2.22} & ~\textbf{0.57} & ~\textbf{1.30E-01} & ~\textbf{0.39} \\
\hline
\end{tabular}
}
\caption{Comparison of the IPAD \textbf{Prompt Inverter} with other prompt extractors}
\label{tab:model_comparison}
\end{table}
\vspace{-0.3cm}


\subsubsection{The Effectiveness of the IPAD Distinguishers}

To examine the effectiveness of the IPAD ~\textbf{Distinguishers}, we conducted a comparison study using the same dataset but different distinguishing methods. The first and second methods employed Sentence-Bert ~\cite{r63} and Bart-large-cnn ~\cite{r64} to compute the similarity score between the input texts and the regenerated texts. We selected thresholds that maximized AvgRec, which were 0.67 for Sentence-Bert and -2.52 for Bart-large-cnn. The classification rule is that the texts with scores greater than the threshold will be classified as LGT, while the texts with scores less than or equal to the threshold will be classified as HWT.

The third and fourth methods involved directly prompting ChatGPT as follows: 

\textbf{Instruction:} ~\textit{"Text 1 is generated by an LLM. Determine whether Text 2 is also generated by an LLM with a similar prompt. Answer with only YES or NO."}  ~\textbf{Input: }~\textit{"Text 1: \{Regenerated Text\}; Text 2: \{LGT\} or \{HWT\}"}. 

and ~\textbf{Instruction:} ~\textit{"Can LLM generate text2 through the prompt text1? Answer with only YES or NO."} with ~\textbf{Input:} ~\textit{"Text 1: \{Predicted Prompt\}; Text 2: \{Input text\}"}.

The final results demonstrated that the other distinguishing methods performed worse than the two IPAD ~\textbf{Distinguishers}, highlighting the superior effectiveness of the IPAD ~\textbf{Distinguishers}.


\begin{table}[ht]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|cccc}
\hline
\textbf{Distinguish Method} & \textbf{HumanRec} & \textbf{MachineRec} & \textbf{AvgRec} & \textbf{F1} \\
\hline
Sentence-Bert (Threshold 0.67) & 61.20\% & 95.20\% & 78.20\% & 63.51\% \\
Bart-large-cnn (Threshold -2.52) & 42.60\% & 97.20\% & 69.90\% & 43.96\% \\
Prompt to ChatGPT version 1 & 33.20\% & 64.50\% & 48.85\% & 44.77\% \\
Prompt to ChatGPT version 2 & 12.50\% & 100\% & 56.25\% & 12.50\% \\
IPAD version 1 & 98.00\% & 99.80\% & 98.90\% & 98.10\% \\
IPAD version 2 & ~\textbf{99.70\%} & ~\textbf{100\%} & ~\textbf{99.85\%} & ~\textbf{99.70\%} \\
\hline
\end{tabular}
}
\caption{Comparison of Different Distinguishers, where ~\textbf{IPAD version1} stands for ~\textbf{IPAD with ~\textit{Prompt-Text Consistency Verifier}} and ~\textbf{IPAD version2} stands for ~\textbf{IPAD with ~\textit{Regeneration Comparator}.}}
\label{tab:distinguishers_comparison}
\end{table}
\vspace{-0.3cm}





\subsection{Explanability Assessment of IPAD}
\subsubsection{Different Linguistic Features of HWT prompts and LGT prompts}

This subsection of the evaluation aims to explore the linguistic features of prompts generated by HWT and LGT through the \textbf{Prompt Inverter}. We analyzed 1000 samples generated by HWT and 1000 samples generated by LGT, which are randomy selected from both in-distribution data and OOD.

The analysis is first conducted using the Linguistic Feature Toolkik (lftk)\footnote{https://lftk.readthedocs.io/en/latest/}, a commonly used general-purpose tool for linguistic features extraction, which provides a total of 220 features for text analysis. Upon applying this toolkit, we identified 20 features with significant differences in average values between the two groups, out of which 3 features showed statistically significant differences with p-values less than 0.05. These 3 differences can be summarized as one main aspects: ~\textbf{syntactic complexity}. Beyond these, we referred to the LIWC framework \footnote{https://www.liwc.app/}, which defines 7 function words variables and 4 summary variables. By comparing the difference, two of these 11 features is significantly distinguishable: ~\textbf{the pronoun usage} and ~\textbf{the level of analytical thinking}.

% One of the primary distinctions between the HWT prompts and the LGT prompts is the \textbf{conceptual scope}. The analysis reveals that LGT prompts tend to generate more generalized concepts, while HWT prompts tend to provide more specific and detailed descriptions. Linguistic features show that HWT prompts have significantly more \textbf{geographical entities} (mean value of 0.127 and 0.113), \textbf{organizational entities} (mean value of 0.143 and 0.154), and \textbf{cardinal entitites} (mean value of 0.03 and 0.105) per sentence. These entities, however, are often indirectly related to the core meaning of the prompt, which serve more as supplements rather than integral components of the main topic. For example, as shown in Figure~\ref{fig:concept scope}, HWT prompts would include specific geographical names as examples when describing \textit{car-free zone issue}, detailed organizational sources when stating \textit{"Face on Mars" problem}, and specific reasons when talking about \textit{student sport activities}.

One of the primary distinctions between the HWT prompts and the LGT prompts is \textbf{sentence complexity}. LGT prompts are typically more complex, characterized by \textbf{longer sentence lengths} (mean value of 1.514 and 1.794), \textbf{higher syllable counts} (mean values of total syllabus three are 1.572 and 3.042), and \textbf{more stop-words} (mean values of 9.88 and 10.045). HWT prompts, on the other hand, are characterized by shorter, less complex sentences that are easier to process and understand, as examples shown in Appendix~\ref{sec:Linguistic Difference Examples} Figure~\ref{fig:sentence complexity}.


Beyond the differences in \textbf{syntactic complexity}, we also explored variables in LIWC. We did the difference comparison by using HWT and LGT prompts as inputs for ChatGPT, for example, instructing with the prompts \textit{'determine the pronoun usage of this sentence, answer first person, second person, or third person'} and \textit{'determine the level of analytical thinking of these sentences, answer a number from 1 to 5'}. The results show that there are distinguish difference in pronoun usage and analytical thinking level. The HWT prompts frequently use \textbf{second-person pronouns} (e.g., 'you') - 75 occurrences per 1,000 prompts - due to the subjective tone often employed in HWT. In contrast, LGT prompts primarily feature first- and third-person pronouns, with second-person pronouns appearing only 2 per 1,000 prompts. LGT prompts typically present instructions and questions in a more objective manner. As shown in Appendix ~\ref{sec:Linguistic Difference Examples} Figure ~\ref{fig:comparison}, LGT prompts show higher \textbf{analytical thinking levels} than HWT prompts. With level 1 as the lowest and level 5 as the highest, LGT has 68.9\% of level 4 and 24.3\% of level 5, but HWT has only 48.0\% of level 4, and 0.8\% of level 5. It suggests that LGT prompts encourage more analytical thinking, while HWT prompts tend to focus more on concrete examples, with less emphasis on critical analysis, as examples shown in Appendix~\ref{sec:Linguistic Difference Examples} Figure ~\ref{fig:person}.


\subsection{User Study}
To assess the explainability improvement of IPAD, we designed an IRB-approved user study with ten participants evaluating one HWT and one LGT article. We used IPAD version 2 due to its superior OOD performance and attack resistance. Participants compared three online detection platforms with screenshots shown in Appendix~\ref{User study}\footnote{https://www.scribbr.com/ai-detector/}\footnote{https://quillbot.com/ai-content-detector}\footnote{https://app.gptzero.me/} with IPAD's process (which displayed input texts, predicted prompts, regenerated texts, and final judgments). After evaluation, users rated IPAD on four key explainability dimensions. Transparency received strong ratings (40\%:5, 60\%:4), with users appreciating the visibility of intermediate processes. Trust scores were more varied (10\%:3, 70\%:4, 20\%:5), but IPAD was generally considered more convincing than single-score detectors. Satisfaction was mixed (30\%:3, 30\%:4, 40\%:5), with users acknowledging better detection but raising concerns about energy efficiency since IPAD runs three LLMs. Debugging received unanimous 5s, as users could easily analyze the predicted prompt and regenerated text to verify the decision-making process. If needed, users could refine the generated content by adjusting instructions, such as specifying a word count, making IPAD a more effective and user-friendly tool compared to black-box detectors.


%