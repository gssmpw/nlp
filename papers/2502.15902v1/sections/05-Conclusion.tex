This paper introduces ~\textbf{IPAD (Inverse Prompt for AI Detection)}, a framework consisting of a ~\textbf{Prompt Inverter} that identifies predicted prompts that could have generated the input text, and a ~\textbf{Distinguisher} that examines how well the input texts align with the predicted prompts. This design enables explainable evidence chains tracing unavailable in existing black-box detectors. Empirical results show that IPAD surpasses the baselines on all in-distribution, OOD, and attacked data. Furthermore, the ~\textbf{Distinguisher} (version2) - ~\textit{Regeneration Comparator} outperforms the ~\textbf{Distinguisher} (version1) - ~\textit{Prompt-Text Consistency Verifier}, especially on OOD and attacked data. While the local alignment in veresion1 approach provides explicit interpretability, it is more sensitive to adversarial attacks. In contrast, the global distribution in veresion2 matching approach implicitly learns generative LLM's distributional properties, which offers more robustness while maintaining explainability. This insight suggests that combining self-consistency checks of generative models with multi-step reasoning for evidential explainability holds promise for future AI detection systems in real-world scenarios. A user study reveals that IPAD enhances trust and transparency by allowing users to examine decision-making evidence. Overall, IPAD establishes a new paradigm for more robust, reliable, and interpretable AI detection systems to combat the misuse of LLMs.