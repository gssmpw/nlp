While IPAD demonstrates SOTA performance, two limitations warrant discussion:
%
(1) The \textbf{Prompt Inverter} may not fully reconstruct prompts containing explicit in-context learning examples (e.g., formatted demonstrations), as it prioritizes semantic alignment over precise syntactic replication.
%
(2) Since IPAD achieves satisfactory OOD performance (12.65\% improvement over baselines) by only adopting essay writing datasets for the fine-tuning of \textbf{Distinguishers}, we strategically deferred the exploration of more datasets. 
%
We will incorporate a wider and more diverse range of data in future works to explore if it can enhance robustness even further, including: creative/news domains, and triplet data formats (i.e., ~\textit{"Can this \{predicted prompt\} generate the \{Input text\} using an LLM? One example generated by the predicted prompt is: \{regenerated text\}"})

% The \textbf{Prompt Inverter} may not always extract the exact prompt, especially when the prompt includes examples for in-context learning, as it might fail to capture these elements precisely. In the fine-tuning of the \textbf{Distinguishers}, we did not incorporate a wider variety of datasets, such as creative prompt datasets or news datasets, nor did we experiment with the triplet data format (~\textit{‚ÄùCan this \{predicted prompt\} generate the \{Input text\} using an LLM? One example generated by the predicted prompt is: \{regenerated text\}}). This decision was based on the satisfactory performance of IPAD on out-of-distribution (OOD) data. However, we acknowledge that utilizing a broader and more diverse range of data could potentially improve the performance of IPAD.