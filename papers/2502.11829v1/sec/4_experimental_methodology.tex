\section{Experimental Methodology}
In this section, we describe the evaluated models, evaluation metrics, and implementation details of our experiments.


\input{tables/overall}
\textbf{Models. }
The models we use can be distinguished as proprietary and open-source models.

For proprietary models, we include GPT-4o~\cite{gpt4o}, the Claude family (Claude 3.5 Sonnet~\cite{Claude3.5_Sonnet}, Claude 3 Sonnet~\cite{Claude3}, Claude 3 Haiku~\cite{Claude3}) and the Gemini family (Gemini 1.5 Pro~\cite{geminiteam2024gemini15unlockingmultimodal}, Gemini 1.5 Flash~\cite{geminiteam2024gemini15unlockingmultimodal}).

For open-source models, we include the Llama-3.2-Vision family (Llama-3.2-11B-Vision-Instruct~\cite{Llama3.2}, Llama-3.2-90B-Vision-Instruct~\cite{Llama3.2}), the Phi-3 family (Phi-3.5-vision-instruct~\cite{abdin2024phi3technicalreporthighly}, Phi-3-vision-128k-instruct~\cite{abdin2024phi3technicalreporthighly}), MiniCPM-V 2.6~\cite{yao2024minicpmvgpt4vlevelmllm} , and Qwen-VL-Plus~\cite{bai2023qwenvlversatilevisionlanguagemodel}.

\textbf{Evaluation Metrics. }
We follow previous work~\cite{chen2021evaluatinglargelanguagemodels,li2024mmcode,wang2024intervenor,yang2024enhancing,luo2023wizardcoder} and we use Pass@$k$ ~\cite{chen2021evaluatinglargelanguagemodels} to evaluate the effectiveness of different MLLMs. Pass@$k$ represents the probability that at least one correct solution appears among the top $k$ generated solutions for each problem:
\begin{equation}
    \text{Pass@}k:=\underset{\text{Problems}}{\operatorname*{\mathbb{E}}}\left[1-\frac{\binom{n-c}k}{\binom nk}\right]
\end{equation}
where $n$ denotes the total number of generated solutions, $c$ is the number of correct solutions, and $k$ is the number of top-ranked solutions being evaluated. In this work, we set $k=1$.




\textbf{Implementation Details. } 
For all MLLMs, we set the generation temperature to 0.2, the nucleus sampling parameter $top\_p$ to 0.95, and the maximum generation length to 1024 tokens, the same as the other code generation work~\cite{hui2024qwen25codertechnicalreport,zheng2024opencodeinterpreter,guo2024deepseek,zhu2024deepseek}. For the proprietary models, we use the API endpoints provided by the respective vendors, and for the open-source models, we use the transformers\footnote{\url{https://huggingface.co/docs/transformers/index}} framework for inference. We use the 0-shot setting in our experiments. The proprietary models we used are gpt-4o-2024-08-06, claude-3-5-sonnet-20240620, claude-3-sonnet-20240229, claude-3-haiku-20240307, Gemini 1.5 Pro (May 2024), and Gemini 1.5 Flash (May 2024).


