\section{Conclusion}
We present \benchmark, a novel benchmark for evaluating the logical understanding and code generation capabilities of Multimodal Large Language Models. \benchmark's unique visual-centric design using flowcharts as primary inputs provides a more rigorous test of models' multimodal reasoning abilities compared to existing benchmarks. Our extensive experiments reveal significant performance gaps between proprietary and open-source models. Moreover, we show that the \benchmark can pose unique challenges compared to other multimodal reasoning benchmarks and further demonstrate that open-source models struggle to understand visually complex logic. In the future, we will build more and harder problems that contain more complex logic and flowcharts to challenge proprietary models.