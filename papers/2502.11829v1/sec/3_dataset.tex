
\section{\benchmark}
% \vspace{-2pt}
In this section, we introduce the benchmark \benchmark, which evaluates the logical understanding and code generation abilities of Multimodal Large Language Models (MLLMs). We first describe how \benchmark tests MLLMs (Sec.~\ref{dataset:definition}). Then we detail the process of constructing the \benchmark benchmark (Sec.~\ref{dataset:collection}) and show the statistics of \benchmark(Sec.~\ref{dataset:sta}).


\subsection{Task Definition}\label{dataset:definition}
% 任务定义

In \benchmark, MLLMs require generating a correct program \( P \) that fulfills specific functionality requirements based on a given flowchart \( F \), which visually represents the desired algorithm or process. 

Generating code involves translating visual or descriptive elements from the image into logical and structured code that meets the required specifications. After generating the code \( P \), we evaluate its correctness by using a set of test cases \( X = \{(x_1, y_1), \ldots, (x_n, y_n)\} \). Specifically, we provide input \( x_i \) to the generated code \( P \) and obtain the execution result \( P(x_i) \). If there exists any test case \( (x_i, y_i) \in X \) that satisfies \( P(x_j) \neq y_j \), it indicates that the generated code \( P \) does not fulfill the required functionality. If the MLLM generates a program \( P \) that correctly implements the specifications derived from the flowchart \( F \) for all test cases \( \forall (x_i, y_i) \in X, P(x_i) = y_i \), it demonstrates the MLLM's capability to accurately interpret and convert visual algorithmic instructions into executable code, ensuring reliability and correctness in its outputs. This successful translation confirms the model's understanding of both the structure and logic inherent in the flowchart, marking an important step towards advanced reasoning abilities.



\subsection{Data Construction}\label{dataset:collection}
% 数据收集步骤，包含Data Collection，Flowchart Construction，Test Cases Generation
In this subsection, we outline the methodology behind the construction of the \benchmark dataset. As shown in Figure ~\ref{fig:construction}, it includes Data Collection, Flowchart Construction, and Test Cases Generation.

\textbf{Data Collection.} \benchmark consists of three subsets: HumanEval-V, Algorithm, and MATH, which together provide a comprehensive evaluation of MLLMs' reasoning abilities across three aspect: basic programming, algorithm, and math. To ensure the quality and diversity of \benchmark, we collect data from the LeetCode website\footnote{\url{https://leetcode.com/}} as well as some open source high-quality data such as HumanEval~\cite{chen2021evaluatinglargelanguagemodels}.

HumanEval-V is collected from HumanEval, which is a code generation benchmark specifically designed to assess the capability of LLMs in generating correct and efficient Python code based on natural language prompts. We store all the data from HumanEval in \benchmark. Algorithm and MATH subsets are constructed by carefully curating a variety of problems from the LeetCode website that target specific reasoning skills necessary for proficient programming and mathematical problem-solving. We categorize the problems into three levels of difficulty: easy, medium, and hard, to further refine the assessment of the model's capabilities. In the Algorithm subset, problems at each level cover algorithms such as data structure operations and dynamic programming. In the MATH subset, the problems encompass mathematical concepts such as arithmetic operations, algebraic knowledge, and symbolic manipulation.

\input{tables/stats}

\textbf{Flowchart Construction. } The most important part of \benchmark is to build a clear flow chart that accurately depicts the logical structure of the problem and the steps to solve it. Therefore, when building the flowchart, we use a two-step method of creating a mermaid and then rendering it into a visual flowchart to ensure that the flowchart accurately reflects the logic of the problem.

We first use GPT-4o to generate the mermaid from the problem descriptions and correct code solutions. The prompt we used is in Figure ~\ref{fig:prompt}. Mermaid is a text-based flowchart representation that allows us to describe diagrams and visual data flows in a straightforward manner. After generating the mermaid, we utilize an automated rendering tool\footnote{\url{https://www.mermaidchart.com/}} to render it into a high-quality visualization flowchart. During the process of rendering the flowchart, we manually review the automatically generated flowchart to ensure its compliance and logicality. For those flowcharts whose problem description is not clear and logic has problems, we conduct manual correction.
\input{figures/prompt}




\textbf{Test Cases Generation. }
Test cases are crucial to validate the correctness and robustness of the code generated by MLLMs. For HumanEval-V, we utilize the test cases provided in HumanEval. For Algorithm, we utilize the test cases from LeetCode dataset\cite{guo2024deepseekcoderlargelanguagemodel}. For MATH subset which do not provide private test cases, we generate a comprehensive set of test cases that cover:
\begin{itemize}
    \item Typical Cases: Standard input scenarios that test the general functionality of the code.
    \item Edge Cases: Boundary conditions that might result in unusual outputs, such as extreme values or zero inputs.
    \item Large Number Cases: These cases involve inputs with significantly large values or large datasets to test the efficiency and performance scalability of the generated code. 
\end{itemize}

For each category, we generate three test case inputs separately. Subsequently, we use the correct code to execute based on the input and get the correct output. We keep these correct input-output pairs as test cases.





\subsection{Data Statistics}\label{dataset:sta}
In this subsection, we show the statistics of \benchmark.


As shown in Table \ref{tab: CodeVision stats}, the \benchmark benchmark consists of three subsets: HumanEval-V, Algorithm, and MATH, covering basic programming, algorithmic, and mathematical problem-solving domains, and difficulty levels (Easy, Medium, and Hard). HumanEval-V contains 164 problems with an average of 8.08 test cases and moderate flowchart complexity (10.90 nodes, 11.07 edges). The Algorithm subset comprises 149 problems across three difficulty levels with 100 test cases each, showing increasing flowchart complexity from Easy (12.62 nodes, 13.29 edges) to Hard (18.86 nodes, 19.14 edges) problems. Similarly, the MATH subset includes 125 problems with an average of 8.92 test cases, demonstrating comparable complexity progression from Easy (10.67 nodes, 10.49 edges) to Hard (19.20 nodes, 19.08 edges) problems, indicating that problem difficulty correlates with flowchart complexity across both Algorithm and MATH subsets.


