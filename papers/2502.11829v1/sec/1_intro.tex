\section{Introduction}
\input{figures/motivation}
Recent years have witnessed remarkable progress in Multimodal Large Language Models (MLLMs), which can process and generate information across different modalities such as text, images, and code~\cite{gpt4o,Claude3,Claude3.5_Sonnet,Llama3.2,geminiteam2024gemini15unlockingmultimodal,yao2024minicpm}. These models such as GPT-4o~\cite{gpt4o}, Claude-3~\cite{Claude3}, Gemini~\cite{geminiteam2024gemini15unlockingmultimodal}, and Llama-3.2-Vision~\cite{Llama3.2} have demonstrated impressive capabilities in various tasks, including visual question answering~\cite{singh2019towards,goyal2017making,marino2019ok,li2021adversarial}, mathematical reasoning~\cite{lu2024mathvista,wang2024measuring}, and code generation~\cite{li2024mmcode,shi2024chartmimic}. The ability to understand and reason about visual information while generating accurate code is particularly crucial for advancing artificial intelligence systems.  

However, existing benchmarks for evaluating MLLMs' logic understanding and code generation capabilities have significant limitations. While benchmarks like MMCode~\cite{li2024mmcode} have contributed valuable insights, they often treat visual information as supplementary rather than essential. For instance, as shown in Figure \ref{fig:motivation}, in MMCode, many programming problems can be solved based solely on text descriptions without requiring visual understanding. This limitation makes it difficult to assess whether MLLMs truly utilize and comprehend visual information in their coding process. Meanwhile, when humans solve problems in mathematics and programming, they often use flowcharts to visualize the logic and structure of the problem. If MLLMs can translate flowcharts into code accurately, it will greatly increase the efficiency of programming and problem solving. However, MLLMs has not been evaluated in this scenario.

To address these challenges, we introduce \benchmark, a novel benchmark designed to evaluate the logical understanding and code generation capabilities of MLLMs. \benchmark differs from existing benchmarks in several key aspects: 1) \textbf{Visual-Centric Design}: Our benchmark uses flowcharts as the primary input, making visual information essential rather than supplementary. Without understanding the flowchart, it is nearly impossible for models to generate correct code solutions. 2) \textbf{Comprehensive Evaluation}: \benchmark comprises three distinct subsetsâ€”HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' reasoning abilities across basic programming, algorithmic, and mathematical problem-solving domains. 3) \textbf{Rigorous Testing}: Each problem in \benchmark includes comprehensive test cases covering typical scenarios, edge cases, and large-number inputs, ensuring a thorough evaluation of the generated code's correctness and robustness.

Through extensive experiments with 12 state-of-the-art MLLMs, we demonstrate that \benchmark effectively reveals significant performance differences between proprietary and open-source models. Our results show that while leading proprietary models like GPT-4o can achieve up to 79.3\% pass@1 on hard problems, the best open-source models struggle to surpass 15\%. All open-source models consistently fail to solve problems in the Algorithm Hard category, achieving a 0\% pass rate. These findings highlight the continuing challenges in developing open-source MLLMs that can match the reasoning capabilities of proprietary models.

Our further analyses also reveal interesting insights. When comparing with MathVista~\cite{lu2024mathvista}, we find that proprietary models maintain similar performance across both benchmarks, while open-source models show a significant performance drop (around -30\%) on \benchmark, highlighting the unique challenges our benchmark poses for testing algorithmic reasoning capabilities. Our error analysis demonstrates a clear distinction in failure patterns: proprietary models primarily fail due to logical errors (AssertionError) while maintaining syntactic correctness, whereas open-source models frequently struggle with basic code structure and syntax issues, suggesting fundamental gaps in their code generation abilities. These findings provide valuable insights into the current limitations of MLLMs in visual-based code generation tasks.

