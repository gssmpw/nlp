% \begin{abstract}


% This paper introduces \benchmark, a benchmark designed to evaluate the logical understanding and reasoning capabilities of Multimodal Large Language Models (MLLMs) through code generation. It challenges MLLMs to generate a correct program that fulfills specific functionality requirements based on a given flowchart, which visually represents the desired algorithm or process. \benchmark comprises three subsetsâ€”HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' reasoning abilities across basic programming, algorithmic, and mathematical problem-solving domains. Our experiments evaluate 12 MLLMs on \benchmark. Experimental results demonstrate that there is a large performance difference between proprietary and open-source models. On Hard problems, GPT-4o can achieve 79.3\% pass@1, but the best open-source model only achieves 15\%. Further experiments reveal that \benchmark can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista. We also explore the reason for the poor performance of the open-source models. All of our source codes and data will be released via GitHub. 
% \end{abstract}