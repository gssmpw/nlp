
\section{Evaluation Results}


In this section, we first benchmark Multimodal LLMs on \benchmark. Subsequently, we compare the advantages of \benchmark over MMCode and how it can evaluate the reasoning capabilities of MLLMs from different perspectives compared to MathVista. Additionally, we show that MLLMs struggle to understand visually complex logic for code generation and analyze the reasons for the poor performance of open-source MLLMs on \benchmark. Finally, case studies are presented.




\subsection{Overall Performance}
Table \ref{tab: overall} shows the performance of the MLLMs on \benchmark dataset.


Overall, the proprietary models significantly outperform the open-source models. GPT-4o leads the proprietary models with an average score of 91.3, excelling in both the Algorithm and MATH categories. Following GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro also show robust results. In contrast, the open-source models lag behind in terms of overall performance, with Llama-3.2-90B-Vision-Ins scoring the highest among them at 36.9 on average. 
% Notably, this model performs comparatively well in the MATH easy category with a score of 80.0, demonstrating some strength in less complex tasks. However, its performance declines significantly in the hard categories of both Algorithm and MATH. 
The gap between proprietary and open-source models is especially evident in the Algorithm hard and MATH hard categories, where proprietary models, particularly GPT-4o and Claude 3.5 Sonnet, maintain relatively high scores compared to their open-source counterparts. This disparity suggests that while open-source models may handle simpler tasks effectively, they struggle with more complex problem-solving and reasoning challenges.

Additionally, we observe that the MLLMs score the lowest on the Algorithm subset on average, and all open-source models fail in all questions of Algorithm Hard. This suggests that the problems in the Algorithm subset are more challenging for the MLLMs, which demonstrates that open-source MLLMs still have a significant gap compared to proprietary models in more complex reasoning tasks.


\subsection{Comparision with Other Reasoning Benchmarks MMCode and MathVisita}
\input{tables/compare_mmcode}
\input{figures/compare_mathvista}
In this subsection, we show the advantages that \benchmark has over the other multimodal reasoning benchmarks MMCode and MathVisita.

\textbf{Compare with MMCode. }MMCode is a multimodal reasoning benchmark that evaluates algorithmic problem-solving skills in visually rich contexts. However, a major issue with MMCode is that most of its problems can be solved without using images, making it difficult to determine whether the model utilizes visual information in its responses, which limits its effectiveness in evaluating the reasoning abilities of MLLMs. \benchmark addresses this issue effectively by using flowcharts as a primary input type, making it challenging for models to provide correct answers without the images.

We explore model performance in both the ``Text Only'' and ``Text + Image'' modes to validate \benchmark’s reliance on visual information. As shown in Table ~\ref{tab: compare_mmcode}, on \benchmark, the scores of GPT-4o and Gemini 1.5 Pro in the ``Text Only'' mode are significantly lower than in the ``Text + Image'' mode, indicating a notable performance difference. This difference suggests that \benchmark’s problem design effectively encourages the use of multimodal information, making image input critical for problem-solving. In contrast, in MMCode, the score difference between the two modes is relatively small, indicating that most MMCode problems can be answered solely based on text information, without the need for images. This design limits MMCode’s ability to fully reflect the multimodal reasoning capabilities of MLLMs. By introducing complex image information, such as flowcharts, \benchmark requires models to rely on visual understanding to answer correctly, thus providing a more accurate assessment of the model's multimodal reasoning capabilities.


\textbf{Compare with MathVista. }MathVista is a multimodal mathematics reasoning benchmark. We select some proprietary models and open-source models with similar performance on MathVista to test their performance on \benchmark, to demonstrate the differences between \benchmark and multimodal mathematical reasoning datasets.

As shown in Figure ~\ref{fig:compare_mathvista}, for proprietary models, their performance on \benchmark and MathVista is similar. However, for open-source models, their performance on \benchmark is significantly lower than on MathVista, with a performance gap of around -30\%. This indicates that \benchmark can evaluate the reasoning abilities of MLLMs from different perspectives, such as algorithmic logic understanding, further revealing the shortcomings and limitations of these models in handling complex reasoning tasks.



\input{tables/compare_mermaid}
\input{figures/error_analysis}
\subsection{MLLMs Struggle to Understand Visually Complex Logic for Code Generation}
% 让模型根据图片生成和根据mermaid生成，比较一下结果。
In this subsection, we compare the performance of the MLLMs when flowchart or mermaid are used as input to demonstrate that MLLMs struggle to understand complex visual logic for code generation.

As shown in Table \ref{tab: mermaid}, when using mermaid as input, most models exhibit performance improvements. This suggests that the mermaid, being a more structured and textual representation, may be easier for MLLMs to interpret compared to flowcharts, which require the understanding of more complex visual elements. In particular, the performance of open-source models on medium and hard problems shows substantial improvements with mermaid inputs, this highlights the challenges MLLMs face in perceiving and processing visually complex logic, which is crucial for accurate code generation.
% This may indicate that the model has shortcomings in perceiving and understanding images




\subsection{Error Analysis and Case Studies}

Figure \ref{fig:error_analysis} shows the proportion of error types in the code generated by proprietary and open-source models across the three subsets of the \benchmark.

From the pie charts, we can observe that proprietary models have a significantly higher proportion of \texttt{AssertionError} compared to open-source models. In contrast, open-source models have a higher proportion of errors on \texttt{TypeError}, \texttt{IndentationError}, \texttt{SyntaxError}, \texttt{ValueError}, and \texttt{NameError}, with the highest proportion occurring in \texttt{SyntaxError}. This indicates that proprietary models adhere to basic syntax rules more effectively and are able to comprehend the logic within flowcharts. However, open-source models lack this refinement, their generated code has over a 10\% likelihood of failing to compile, indicating that their coding capabilities are poor, even to the point of failing to generate executable code. In our opinion, the programming ability of MLLMs is an essential skill for the path to AGI, as it represents a crucial intersection of logical reasoning, problem-solving,  and the ability to generate precise, executable instructions.

Finally, we conduct case studies to examine the performance of the models on the \benchmark. As shown in Figure \ref{fig:case}, the problem has two flowcharts and puts higher demands on the capabilities of MLLMs. GPT-4o correctly follows the logic depicted in the flowchart to generate the correct code. Llama-3.2-90B-Vision-Instruct understands the intent of the flowchart and uses a dynamic programming algorithm. MiniCPM-V-2\_6 omits two steps from the subgraph and Claude-3-Haiku-20240307 fails to correctly identify the function variables in both the main and sub-functions and does not properly understand the execution flow of the code in the subgraph. More details are shown in Appendix \ref{sec:cases}


% Finally, as shown in Figure \ref{fig:case}, we select a problem from the Math subset and present the responses from four MLLMs. This problem has two flowcharts and puts higher demands on the reasoning capabilities of MLLMs.

% Overall, GPT-4o and Llama-3.2-90B-Vision-Instruct provide correct answers, while MiniCPM-V-2\_6 and Claude-3-Haiku-20240307 gave incorrect ones. Upon careful analysis of the generated code, we can observe that GPT-4o correctly follows the logic depicted in the flowchart to generate the correct code. However, Llama-3.2-90B-Vision-Instruct did not follow the recursive logic as shown in the flowchart, instead using the dynamic programming algorithm. This is an interesting phenomenon, suggesting that Llama-3.2-90B-Vision-Instruct, after processing the flowchart, comprehends the problem that the algorithm in the chart is solving and provides a dynamic programming solution. This demonstrates that the model not only interprets the flowchart's surface meaning but also, to some extent, ``understands" the logical intent behind it.

% As clearly shown in the bottom of Figure \ref{fig:case}, MiniCPM-V-2\_6 fail to generate fully correct code because the model omitted two steps from the subgraph, although the other steps were correct, ultimately leading to an \texttt{AssertionError}. On the other hand, Claude-3-Haiku-20240307 fail to correctly identify the function variables in both the main and sub-functions and did not properly understand the execution flow of the code in the subgraph, ultimately leading to a \texttt{RecursionError}. This indicates that both models have poor image understanding and reasoning capabilities.
% \input{figures/case}

