\section{Related Work}
\subsection{Multimodal LLMs} 
In recent years, Multimodal Large Language Models (MLLMs) have gained significant attention due to their ability to process and generate information across various modalities, such as text, images, and audio~\cite{li2024llava,liu2024visual,zhu2023minigpt,li2024llavaonevisioneasyvisualtask,dai2023instructblipgeneralpurposevisionlanguagemodels,ye2024mplugowlmodularizationempowerslarge}. Some proprietary models such as GPT-4o~\cite{gpt4o}, Claude-3~\cite{Claude3}, and Gemini~\cite{geminiteam2024gemini15unlockingmultimodal} show superior performance, especially on visually complex reasoning tasks such as MathVista~\cite{lu2024mathvista}. Open-source MLLMs have made strides as well, notable models include Llama-3.2-Vision~\cite{Llama3.2}, Phi-3-Vision~\cite{abdin2024phi}, MiniCPM V2.6~\cite{yao2024minicpm}, Qwen-VL~\cite{bai2023qwenvlversatilevisionlanguagemodel}, Deepseek-VL~\cite{lu2024deepseek}. The reasoning ability of MLLMs has recently become a highly regarded research focus because they can extend beyond language reasoning to encompass multiple modalities (such as images), achieving more comprehensive and complex reasoning. Previous research has evaluated the reasoning ability of MLLMs through tasks like Visual Question Answering (VQA)~\cite{mobasher2022parsvqa,gurari2018vizwiz} and mathematical reasoning~\cite{lu2024mathvista}. In this paper, we evaluate the logical understanding and reasoning capabilities of MLLMs through code generation.
\input{figures/construction}
Â·\subsection{Multimodal Reasoning Benchmarks}
The reasoning capabilities of Multimodal LLMs are important for the development of Artificial General Intelligence (AGI)~\cite{morris2023levels}. To advance the reasoning capabilities of MLLMs, the research community has constructed several multi-modal reasoning benchmarks~\cite{li2024survey}. MathVista~\cite{lu2024mathvista} and Math-Vision
~\cite{wang2024measuring} to evaluate the mathematical reasoning abilities of MLLMs within visual contexts. Additionally, inspired by the effectiveness of code in evaluating the reasoning capabilities of LLMs~\cite{chen2021evaluatinglargelanguagemodels,hendrycksapps2021,jain2024livecodebench,austin2021program}, many researchers have begun using code generation to evaluate the reasoning abilities of MLLMs by introducing visual contexts.
Design2Code~\cite{si2024design2code} evaluates the code generation abilities of MLLMs through HTML web page generation, which involves converting User Interfaces (UI) into clean and responsive HTML/CSS/JS code. ChartMimic~\cite{shi2024chartmimic} requires MLLMs to generate corresponding chart rendering code by using information-dense visual charts and text instructions as input. MMCode~\cite{li2024mmcode} is relevant to our work, and it evaluates algorithmic problem-solving skills in visually rich contexts. However, in MMCode, visual information is not dominant, and images only serve as supplementary elements. Many programming competition problems can still be solved based solely on the text description without the images. Inspired by this, we propose \benchmark, where visual information is predominant, meaning that without the images, it is nearly impossible for the model to solve the problem. This allows for a better evaluation of the MLLMs' understanding and reasoning abilities.



