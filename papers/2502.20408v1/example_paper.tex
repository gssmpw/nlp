%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Functional Networks in Large Language Models}

\begin{document}

\twocolumn[
\icmltitle{Brain-Inspired Exploration of Functional Networks and Key Neurons in Large Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yiheng Liu}{1}
\icmlauthor{Xiaohui Gao}{1}
\icmlauthor{Haiyang Sun}{1}
\icmlauthor{Bao Ge}{2}
\icmlauthor{Tianming Liu}{3}
\icmlauthor{Junwei Han}{1}
\icmlauthor{Xintao Hu}{1}

\end{icmlauthorlist}

\icmlaffiliation{1}{School of Automation, Northwestern Polytechnical University, Xi'an, China}
\icmlaffiliation{2}{School of Physics and Information Technology, Shaanxi Normal University, Xi'an, China}
\icmlaffiliation{3}{School of Computing, University of Georgia, Athens, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Xintao Hu}{xhu@nwpu.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In recent years, the rapid advancement of large language models (LLMs) in natural language processing has sparked significant interest among researchers to understand their mechanisms and functional characteristics. Although existing studies have attempted to explain LLM functionalities by identifying and interpreting specific neurons, these efforts mostly focus on individual neuron contributions, neglecting the fact that human brain functions are realized through intricate interaction networks. Inspired by cognitive neuroscience research on functional brain networks (FBNs), this study introduces a novel approach to investigate whether similar functional networks exist within LLMs. We use methods similar to those in the field of functional neuroimaging analysis to locate and identify functional networks in LLM. Experimental results show that, similar to the human brain, LLMs contain functional networks that frequently recur during operation. Further analysis shows that these functional networks are crucial for LLM performance. Masking key functional networks significantly impairs the model's performance, while retaining just a subset of these networks is adequate to maintain effective operation. This research provides novel insights into the interpretation of LLMs and the lightweighting of LLMs for certain downstream tasks.  Code is available at \url{https://github.com/WhatAboutMyStar/LLM_ACTIVATION}.
%and underscores the advantages of interdisciplinary collaboration, demonstrating how neuroscience concepts can inspire AI technologies.
\end{abstract}

\section{Introduction}
In recent years, large language models (LLMs) have become a focal point of research in the field of artificial intelligence (AI) due to their remarkable capabilities in natural language processing \cite{zhao2024explainability,zhao2023survey,liu2023summary,WANG2024,LIU2025129190}. However, these models are often considered "black boxes", with insufficient understanding of their internal mechanisms, which limits the potential for further optimization and broader application. Therefore, exploring methods to explain and understand LLMs is essential both for improving model transparency and trustworthiness and for establishing a foundation to develop more efficient and reliable AI systems. 

One research direction on the mechanistic interpretability of LLMs focuses on the analysis of individual neurons \cite{yu2024interpreting,dai-etal-2022-knowledge}. A subset of this research is dedicated to the identification of neurons that are crucial to the functionality of LLMs, with the aim of locating and assessing the functions of these key neurons \cite{yu2024neuron,niu2024what,chen2024journey}. Studies have shown that the removal of certain neurons leads to a significant degradation in performance in LLMs, highlighting their essential role in maintaining the core functions of the model. In addition, another line of research investigates the specific functional roles of these neurons \cite{alkhamissi2024llm,wang-wen-etal2022skill}. For example, some neurons may specialize in processing linguistic structures, while others might be responsible for reasoning tasks \cite{huo-etal-2024-mmneuron,zhao2024how}. Furthermore, by manipulating these key neurons, such as amplifying or masking their outputs \cite{song2024does}, researchers have demonstrated the ability to control and predict the behavior of LLMs. Methods for identifying important neurons within LLM can be categorized into several approaches. These include analyzing the gradients of neurons to evaluate their impact on model predictions \cite{sundararajan2017axiomatic,lundstrom2022rigorous}, employing causal tracing techniques to uncover the causal relationships that influence model behavior \cite{nikankin2024arithmetic}, and conducting statistical analyses of activated neurons to measure their information content and variability \cite{alkhamissi2024llm,song2024does,tang2024language}. These approaches provide valuable tools for understanding and explaining LLMs, offering deeper insight into their inner mechanisms. 

However, the function of an individual neuron is much more complex than it might initially seem. A single neuron is not only dedicated to performing a single task, but can simultaneously participate in multiple functional modules \cite{mountcastle1997columnar,miller2001integrative,raichle2007default}. Neurons often form functional networks through their interactions and connectivity, collaboratively working to perform higher-level cognitive tasks \cite{smith2009correspondence,bullmore2009complex}. The role of a neuron therefore extends beyond its individual activation patterns and is shaped by its cooperation with other neurons within these networks \cite{bullmore2009complex,liu2024spatial,liu2024mapping}. Existing research has focused mainly on the parameters and structure of neurons, primarily examining their individual properties and connectivity. These approaches, which emphasize structural and parametric analysis, overlook the functional network perspective and fail to explore the roles and contributions of neurons within these networks. As a result, these limitations have hindered a deeper understanding of neuronal function, neglecting the insights offered by neuroscience research on FBNs \cite{hassabis2017neuroscience,vilasposition}. To address these challenges, we draw inspiration from cognitive neuroscience to investigate whether LLMs contain functional networks similar to those found in the human brain. By recognizing the similarities between functional magnetic resonance imaging (fMRI) \cite{matthews2004functional,logothetis2008we} signals and the output signals of neurons in LLM, we hypothesized that the techniques used in fMRI analysis could be adapted to analyze LLM neurons. Specifically, we treated the neuron outputs from the multilayer perception (MLP) layers of LLMs as analogous to fMRI signals and applied Independent Component Analysis (ICA) \cite{hyvarinen2000independent,beckmann2005investigations,varoquaux2010group} to decompose these neuron outputs into multiple functional networks.

Our experiments on extensive datasets confirmed the existence of numerous functional networks within LLMs. Just as we derived functional networks from fMRI signals \cite{mensch2016compressed,varoquaux2010ica,liu2023spatial,he2023multi,ge2020discovering,lv2015sparse}. Some of the functional networks exhibit high spatial consistency across various input stimuli and play a crucial role in the model's functionality. We discovered that masking specific key networks, which typically consist of less than 2\% neurons, can significantly impair model performance. Conversely, maintaining these essential networks while gradually integrating additional decomposed functional networks and masking non-essential neurons enables the model's performance to improve progressively from low to high. Ultimately, by utilizing less than one-tenth of the neurons in the MLP layer, we achieved performance that matches that of the original network.

Our contributions can be summarized as follows:

1. Bridging Cognitive Neuroscience and AI: We have introduced methods from cognitive neuroscience to analyze neurons in LLM, creating a link between brain science and AI. This integration offers new research directions for brain-inspired AI, providing fresh insights into how neural networks operate.

2. Discovery of Functional Networks in LLMs: We confirmed that LLMs contain functional networks similar to those found in the human brain. Some of these functional networks exhibit high spatial consistency in various input stimuli, demonstrating relatively stable spatial patterns. This finding suggests that LLMs may share fundamental organizational principles with biological brains.

3. Validation of Key Neurons in Functional Networks: We have demonstrated that neurons within these functional networks are essential for maintaining the functionality of LLMs. These neurons play a crucial role in ensuring that the model operates effectively, underscoring their importance for the overall performance of the system.

\begin{figure*}[ht]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\textwidth]{figures/frame.pdf}}
    \caption{The total framework of the paper. Functional networks generated by ICA on fMRI signals and LLM's neurons are shown in upper and lower part of the figure respectively.}
    \label{fig::frame}
    \end{center}
    \vskip -0.3in
\end{figure*}

\section{Preliminaries}
In this section, we provide the essential background knowledge required for understanding our work, including the MLP layer in LLMs, how neuroscience utilizes fMRI to study brain functions, and the ICA algorithm which we use to decompose neural activity into distinct functional networks.

\textbf{Transformer, MLP layer:} LLMs utilized in this paper are based on the transformer architecture \cite{vaswani2017attention}, specifically employing a decoder-only configuration. \cite{GPT,GPT2,GPT3,yang2024qwen2,glm2024chatglm}. In this configuration, each transformer decoder consists of two primary components: a multi-head self-attention module and a MLP module. The non-linear transformations in transformer models occur within the MLP layers. Typically, an MLP module consists of two fully connected layers. The first layer increases the dimensionality, often to four times the original dimension, followed by a non-linear activation function such as the Sigmoid-weight Linear Unit (SiLU) \cite{elfwing2018sigmoid} or the Gaussian Error Linear Unit (GELU) \cite{hendrycks2016gaussian}. The second layer then reduces the dimensionality back to its original size. In our study, we focus on the neurons located in the final MLP layer of each decoder module within the model. Given an input vector $\mathbf{x} \in \mathbb{R}^{d_{\text{model}}}$, the MLP module can be represented as follows:

\begin{equation}
    \text{MLP}(\mathbf{x}) = \mathbf{W}_2 \cdot \sigma (\mathbf{W}_1 \cdot \mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2
\end{equation}

$\mathbf{W}$ is the weight matrix of the linear transformation, $\mathbf{b}$ is the bias vector of the linear transformation, $\sigma$ is a non-linear activation function. The neuron outputs used in this paper are the outputs of the $\text{MLP}(\mathbf{x})$. This choice allows us to analyze the impact of these neurons on the final output, as they are directly responsible for the refined feature extraction and transformation before the model generates its predictions.

\textbf{Functional Brain Networks, FBNs:} FBNs refer to collections of brain regions that are co-activated during specific tasks or while at rest \cite{dong2020discovering}. FMRI is a non-invasive technique used to measure Blood-Oxygen-Level Dependent (BOLD) signals, which reflect neuronal activity indirectly \cite{matthews2004functional,logothetis2008we}. The intensity of voxel values in fMRI signals indirectly reflects neuronal activity by capturing variations in local blood oxygen levels due to neural metabolism. Neuroscientific research hypothesis that the observed BOLD signals in fMRI signals are likely the result of multiple independent functional networks working together. Essentially, these BOLD signals can be considered as linear combinations of several source signals, each representing a distinct functional network. By comparing the spatial patterns of functional networks across various task conditions, researchers can infer which regions of brain are associated with specific cognitive or behavioral functions.

\textbf{Independent Component Analysis, ICA:} ICA is a powerful data-driven technique used to extract source signals that are as statistically independent as possible from a mixed signal. Within the field of neuroscience, ICA is frequently applied to fMRI data to uncover underlying functional networks, offering valuable insights into how different regions of the brain collaborate to facilitate cognitive and behavioral processes. ICA disentangles mixed fMRI signals into several independent components, where each component represents a distinct functional network. Each extracted independent component is associated with a spatial map that illustrates which brain regions contribute to that component. These contributing regions typically display synchronized activity patterns, indicating their coordinated involvement in specific neural processes. 

The objective of ICA is to recover the source signals $\mathbf{S}$ from the observed signals $\mathbf{X}$. Suppose that we have $n$ observed signals $\left[ \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \right]$, which are linear mixtures of $m$ independent source signals $\left[ \mathbf{s}_1, \mathbf{s}_2, \ldots, \mathbf{s}_m \right]$. The relationship between the observed signals $\mathbf{X}$ and the source signals $\mathbf{S}$ can be expressed as:
\begin{equation}
    \mathbf{X} = \mathbf{A} \mathbf{S}
\end{equation}
where $\mathbf{A}$ is the mixing matrix that describes how the source signals are combined to produce the observed signals. Each row of the linear mixing matrix represents the spatial pattern of the corresponding functional network, which illustrates specific regions are activated. In this study, the functional networks derived from LLMs neuron signals refer to the rows of the linear mixing matrix $\mathbf{A}$, which indicate the set of neurons that are consistently co-activated under different conditions. FastICA \cite{hyvarinen2000independent} is an efficient algorithm for implementing ICA and is the method used in this paper to derive FBNs from LLMs. The FastICA algorithm can be described as follows:

Pre-whitening: The signals are first centered (zero mean) and whitened to remove any linear correlations between the variables and have unit variance.

The whitened signals $\mathbf{Z}$ can be represented as:
\begin{equation}
   \mathbf{Z} = \mathbf{E}^{-1/2} \mathbf{V}^{-1} (\mathbf{X} - \mathbb{E}[\mathbf{X}]) 
\end{equation}
where $\mathbf{V}$ and $\mathbf{E}$ are the eigenvectors and eigenvalues of the covariance matrix $\mathbf{\Sigma}$ of $\mathbf{X}$.

Finding Independent Components: For each independent component $\mathbf{w}_i$, we maximize the following objective function:
\begin{equation}
    J(\mathbf{w}) = [\mathbb{E}\{G(\mathbf{w}^T \mathbf{z})\}] - \frac{1}{2} \mathbb{E}\{(\mathbf{w}^T \mathbf{z})^2\}
\end{equation}

where $G$ is a non-linear function used to approximate negentropy, $a_1$ is a constant usually $a_1 \in [1, 2]$, Common choices for $G$ include:

\begin{equation}
    G(u) = \log \cosh(a_1 u)
\end{equation}
\begin{equation}
    G(u) = -\exp(-u^2 / 2)
\end{equation}

To find the optimal $\mathbf{w}$, FastICA uses fixed-point iteration:

\begin{equation}
    \mathbf{w}_{\text{new}} = \mathbb{E}\{\mathbf{z} g(\mathbf{w}^T \mathbf{z})\} - \mathbb{E}\{g'(\mathbf{w}^T \mathbf{z})\} \mathbf{w}
\end{equation}

where $g$ is the derivative of $G$. After each iteration, normalize $\mathbf{w}$:

\begin{equation}
    \mathbf{w} \leftarrow \frac{\mathbf{w}}{\|\mathbf{w}\|}
\end{equation}

If multiple independent components need to be extracted, perform orthogonalization to ensure that the weight vectors remain orthogonal:

\begin{equation}
    \mathbf{W} \leftarrow (\mathbf{W} \mathbf{W}^T)^{-1/2} \mathbf{W}
\end{equation}

where $\mathbf{W}$ is the matrix that contains all weight vectors as columns. Repeat the iteration until the change in the weight vectors falls below a predefined threshold, indicating convergence. Once the demixing matrix $\mathbf{W}$ is obtained, the source signals $\mathbf{S}$ can be estimated from the whitened data $\mathbf{Z}$:
\begin{equation}
    \hat{\mathbf{S}} = \mathbf{W} \mathbf{Z}
\end{equation}

Finally, the mixing matrix $\mathbf{A}$, which represents the spatial patterns of the functional networks, can be obtained as the inverse of the matrix $\mathbf{W}$. Considering the whitening transformation applied to the signals, the mixing matrix can be computed as:

\begin{equation}
    \mathbf{A} = \mathbf{V} \mathbf{E}^{1/2} \mathbf{W}^{-1}
\end{equation}

In visualizations and experimental comparisons, the mixing matrix $\mathbf{A}$, representing the final derived functional networks, typically undergoes thresholding. This process involves setting a threshold to filter out lower values, ensuring that only the regions with significant activation are retained. This approach helps in focusing on the most relevant activations and improving the clarity of the results.

\section{Functional Networks in Large Language Models}
In this section, we use the ICA to explore functional networks within LLMs. 

\subsection{Datasets and Models}
We used text from four different types of datasets to validate the differences in functional networks when the model performs various tasks. Specifically, we used news articles from the AGNEWS dataset \cite{zhang2015character}, encyclopedia entries from Wikitext2 \cite{merity2016pointer}, mathematical texts from MathQA \cite{amini2019mathqa}, and code snippets from the CodeNet dataset \cite{codenet}. Additionally, we conducted experiments using three different LLMs: GPT2 \cite{GPT2}, Qwen2.5-3B \cite{qwen2}, and ChatGLM3-6B-base \cite{glm2024chatglm}.

\begin{table}[t]
\caption{Statistics of Functional Networks Similar to the Template for GPT-2.}
\label{tab::gpt_cnt}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccccccccccc}
\toprule
Templates & News & Wiki & Math & code \\
\midrule
1 & 15 & 208 & 142 & 140 \\
2 & 7 & 119 & 109 & 85 \\
3 & 12 & 59 & 31 & 44 \\
4 & 31 & 145 & 91 & 114 \\
5 & 32 & 91 & 32 & 135 \\
6 & 87 & 4 & 6 & 107 \\
7 & 63 & 41 & 35 & 134 \\
8 & 21 & 58 & 43 & 71 \\
9 & 15 & 0 & 51 & 112 \\
10 & 11 & 18 & 118 & 79 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

\subsection{Extracting Functional Networks from LLMs}

The general workflow framework of our work is shown in Figure~\ref{fig::frame}. To extract functional networks from LLMs, the first step is to determine which neurons' output signals to use. In this paper, we utilize the neurons from the last layer of each model's MLP module, specifically the neuron output from these MLP modules. We collect the neuron output from all MLP modules in the model. 

In neuroscience research, functional networks are typically derived from fMRI signals, which reflect neural activity in response to various stimuli. Interestingly, there is a notable similarity between the output signals of neurons in LLMs and fMRI signals. Both LLM neuron outputs and fMRI signals not only capture neural activity but also exhibit temporal characteristics in response to different inputs. This similarity suggests the potential to apply analytical techniques used for fMRI signals to LLMs. By leveraging these parallels, we can explore and analyze functional networks within LLMs in a manner similar to our analysis of human brain data, thus opening a new way of understanding the internal mechanisms of these models.

fMRI analysis typically involves performing ICA on individual data or performing group-wise analysis on data from multiple individuals. In individual analysis, ICA is applied to single-subject fMRI data. This method provides detailed insights into the unique characteristics of an individual's brain activity patterns. In contrast, in group-wise analysis, ICA is performed on data from multiple subjects. This approach improves the generalizability of the research findings. For group-wise analysis, a common approach is to stack the data of all subjects together to form a large data matrix and then perform ICA on this combined dataset. We randomly selected 100 samples from each dataset for group-wise ICA analysis, obtaining 10 functional network templates. We then randomly selected 100 additional samples for individual analysis.

The functional networks derived from the group-wise ICA analysis can be considered as templates representing a generalization of functionality. We computed the spatial similarity between these templates and the functional networks derived from the individual analysis of another 100 samples. In neuroscience research, the intersection over union (IoU) is commonly used as a metric to evaluate the similarity between functional networks. We quantify spatial similarity using the IoU between two functional networks $N^{(1)}$ and $N^{(2)}$. Here, $n$ represents the number of neurons, and the IoU is defined as follows:
	
\begin{equation}
    IoU(N^{(1)}, N^{(2)}) = \frac{\sum_{i=1}^{n}|N_{i}^{(1)} \cap N_{i}^{(2)}|}{\sum_{i=1}^{n}|N_{i}^{(1)} \cup N_{i}^{(2)}|}
\end{equation}

Tables ~\ref{tab::gpt_cnt}, ~\ref{tab::qwen_cnt}, and ~\ref{tab::glm_cnt} present the number of functional networks derived from individual analysis with a spatial similarity greater than 0.2 compared to the template, for GPT-2, Qwen2.5-3B-Instruct, and ChatGLM3-6B-base, respectively. According to our experimental findings and experience in neuroscience research, when different functional networks have spatial similarity greater than 0.2, they appear reasonably similar on subjective visual inspection. Consequently, such networks are classified into the same category of functional networks.

\begin{table}[t]
\caption{Statistics of Functional Networks Similar to the Template for Qwen2.5-3B-Instruct.}
\label{tab::qwen_cnt}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccccccccccc}
\toprule
Templates & News & Wiki & Math & code \\
\midrule
1 & 0 & 2 & 0 & 0 \\
2 & 0 & 0 & 1 & 59 \\
3 & 0 & 1 & 0 & 0 \\
4 & 0 & 0 & 0 & 0 \\
5 & 50 & 10 & 0 & 0 \\
6 & 0 & 17 & 0 & 0 \\
7 & 64 & 73 & 55 & 58 \\
8 & 0 & 0 & 0 & 64 \\
9 & 0 & 0 & 0 & 3 \\
10 & 64 & 70 & 55 & 2 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

\begin{table}[t]
\caption{Statistics of Functional Networks Similar to the Template for ChatGLM3-6B-base.}
\label{tab::glm_cnt}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccccccccccc}
\toprule
Templates & News & Wiki & Math & code \\
\midrule
1 & 0 & 0 & 0 & 0 \\
2 & 1 & 0 & 0 & 0 \\
3 & 0 & 0 & 0 & 0 \\
4 & 0 & 0 & 0 & 0 \\
5 & 0 & 0 & 0 & 2 \\
6 & 3 & 0 & 8 & 0 \\
7 & 0 & 0 & 1 & 0 \\
8 & 0 & 0 & 0 & 0 \\
9 & 0 & 4 & 0 & 0 \\
10 & 0 & 0 & 0 & 0 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

We observe that, even when the inputs are different samples, there are still a significant number of functional networks similar to the templates. This indicates that LLMs indeed contain specific functional patterns similar to those found in human brain. Interestingly, we noticed that as model size increases, it becomes progressively more challenging to find functional networks similar to the templates within individual samples. We do not interpret this as larger models that have fewer or no specific spatial pattern functional networks. Instead, we believe that as model size grows, the functionalities are divided into finer components that involve more functional networks. When dealing with shorter texts, decomposition into only 10 components may not be sufficient to achieve the same level of granularity as the template functional networks. Consequently, a greater number of decomposed network components are required to capture these finer details; this assumption is contingent upon having sufficiently long time series data for adequate decomposition. With sufficient data, a larger number of functional networks similar to the templates can be identified.

\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/fbn.pdf}}
\caption{Some similar functional networks in templates. In the figure, each row represents the neurons in a MLP layer, with neurons being highlighted if they are activated.}
\label{fig::fbn}
\end{center}
\vskip -0.3in
\end{figure}

\subsection{Visualiztion of Functional Networks}

In Figure~\ref{fig::fbn}, we present several functional networks derived from the data. These networks involve a very low percentage of total neurons, which aligns with our understanding that neurons in LLMs are sparsely activated. Additionally, the figure illustrates that similar functional networks can be obtained from different types of input data. This suggests that certain LLM functional networks are broadly presented and engaged in specific tasks, this is similar to the functional networks in the human brain. For instance, the default mode network (DMN) in the human brain is known to be involved in various cognitive tasks \cite{raichle2015brain}. Furthermore, our observations indicate that the majority of neurons activated within functional networks are located in the deeper layers of the models. This finding implies that functional connectivity among neurons is more pronounced in these deeper layers. For a more comprehensive examination of these functional network patterns, please refer to our supplementary material.

\section{Evaluating the Importance of Functional Networks}
In this section, we will evaluate the functional networks obtained using ICA to verify whether they play a critical role in the performance of LLMs.

\subsection{Datasets and Evaluate Criteria}
In this section, we continue to use the GPT-2, Qwen2.5-3B-Instruct, and ChatGLM3-6B-base for evaluation. To assess the model's performance, we used datasets from the General Language Understanding Evaluation (GLUE) \cite{wang2018glue}, the Stanford Question Answering Dataset (SQuAD) \cite{rajpurkar-etal-2016-squad}, and AGNews \cite{zhang2015character}. 

We fine-tuned GPT-2 in these datasets to adapt it to perform specific tasks. In contrast, for Qwen2.5-3B-Instruct and ChatGLM3-6B-base, we utilized carefully crafted prompts to evaluate their performance in a zero-shot setting on the same datasets. In our assessments, we utilized accuracy as the performance metric for the GLUE and AGNews datasets. For the SQuAD, which involves more complex question-answering tasks, we used the F1 score to evaluate the model's performance. The underlying hypothesis is that if these neurons play a critical role in the model's functionality, then ensuring their activation alone should be enough to maintain the model's performance.

\subsection{Neuron Lesion Experiment}

In the neuron lesion experiment, we deactivate functional networks within the LLMs. The goal is to investigate how the removal or deactivation of these neurons impacts the overall performance and functionality of the model.

\begin{table}[t]
\caption{Performance of GPT-2 with Masked Functional Networks on the AGNews Dataset. The first column represents each functional network and the number of neurons that are masked. The second column shows the accuracy. The model's accuracy under normal conditions (without any masking) is 0.8614.}
\label{tab::gpt2_10_agnews}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{cc}
\toprule
functional networks (9216) & \textbf{Accuracy} \\
\midrule
1$\rightarrow$37 & 0.8538 \\
2$\rightarrow$21 & 0.6643 \\
3$\rightarrow$57 & 0.8391 \\
4$\rightarrow$29 & 0.8439 \\
5$\rightarrow$36 & 0.5749 \\
6$\rightarrow$29 & 0.8500 \\
7$\rightarrow$15 & 0.8482 \\
8$\rightarrow$55 & 0.7042 \\
9$\rightarrow$15 & 0.6753 \\
10$\rightarrow$72 & 0.8471 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

Table~\ref{tab::gpt2_10_agnews} presents the performance results of the GPT-2 model in the AGNews dataset after selectively masking specific functional networks. The results indicate that the masking of neurons within these functional networks leads to varying degrees of performance degradation. Remarkably, masking just a few dozen critical neurons can significantly impair the model's performance. In some instances, masking as few as one-thousandth of the total neurons is enough to cause a substantial drop in performance. In contrast, randomly masking neurons has a minimal impact on the model's performance as shown in Table~\ref{tab::gpt2_10_all}. Our experiments demonstrate that even when up to 15\% neurons are randomly masked, performance degradation remains insignificant, with the model's performance staying nearly identical to its baseline in most cases. This finding is consistent with previous studies, which have also observed that random neuron masking does not substantially affect model performance \cite{alkhamissi2024llm,song2024does}.


\begin{table}[t]
\caption{Performance of GPT-2 with Masked Functional Networks. First Column: Dataset names. Second Column: Model performance under normal conditions (without any masking). Third Column: Model performance after masking 15\% of the neurons randomly. Fourth Column: Model performance after masking the neurons belonging to 10 specific functional networks (The number of neurons that are masked is less than 2\% of the total neurons.).}
\label{tab::gpt2_10_all}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{cccc}
\toprule
Datasets & Normal & Masked 15\% & Masked 10\\
\midrule
CoLA & 0.7776 & 0.7383 & 0.5772\\
MRPC & 0.7941 & 0.7941 & 0.7377\\
QQP & 0.8758 & 0.8717 & 0.8104\\
SST-2 & 0.9163 & 0.9083 & 0.5436\\
MNLI & 0.6847 & 0.6879 & 0.5995\\
QNLI & 0.8746 & 0.8569 & 0.6811\\
AG News & 0.8614 & 0.8487 & 0.5557\\
SQuAD & 0.3397 & 0.3154 & 0.1342\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

\begin{table}[t]
\caption{Performance of Qwen2.5-3B-Instruct with Masked Functional Networks.}
\label{tab::qwen_10_all}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccc}
\toprule
Datasets & Qwen2.5 Normal &  Qwen2.5 Masked 10\\
\midrule
CoLA & 0.6913 & 0.0000\\
MRPC & 0.6275 & 0.0172\\
SST-2 & 0.8440 & 0.0000\\
MNLI & 0.2731  & 0.0000\\
QNLI & 0.3953  & 0.0000\\
AGNews & 0.7185  & 0.0000\\
SQuAD & 0.2104 & 0.0057\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

\begin{table}[t]
\caption{Performance of ChatGLM3-6B-base with Masked Functional Networks.}
\label{tab::glm_10_all}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccc}
\toprule
Datasets & ChatGLM Normal &  ChatGLM Masked 10\\
\midrule
CoLA & 0.6893 & 0.0000\\
MRPC & 0.8161 & 0.0441\\
SST-2 & 0.9392 & 0.1422\\
MNLI & 0.2062  & 0.0000\\
QNLI & 0.1067  & 0.0053\\
AGNews & 0.9128  & 0.0025\\
SQuAD & 0.9021 & 0.0358\\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

Table~\ref{tab::qwen_10_all} and Table~\ref{tab::glm_10_all} present the performance results of Qwen2.5-3B-Instruct and ChatGLM3-6B-base by masking neurons within specific functional networks. These models rely on well-designed prompts to generate appropriate text for various tasks. When critical functional networks are masked, the models' ability to generate coherent and task-relevant text is severely compromised. For more details, please refer to the supplementary material.

\subsection{Neuron Preservation Experiment}

In addition to verifying the importance of these neurons by masking functional networks, we also conducted a preservation experiment to further validate the significance of these functional networks. In this section, we incrementally increase the number of functional networks obtained through ICA decomposition, taking the union set of neurons within these functional networks to increase the number of retained neurons. We then mask the remaining less important neurons and observe how the model's performance changes.

\begin{table}[t]
\caption{Performance of GPT-2 with Preserved Functional Networks. The first row of the table indicates the number of functional networks obtained from ICA decomposition, which corresponds to the second to fifth columns. The first row for each dataset corresponds to the evaluation metric, while the second row indicates the number of neurons obtained.}
\label{tab::gpt2_preserve}
\vskip 0.15in
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & 10 &  64 & 128 & 256 & 512\\
\midrule
CoLA &0.3078 & 0.3509 & 0.4976 & 0.7776 & 0.6088 \\
 & 145 & 495 & 1221 & 2109 & 1409 \\
MRPC &0.6838 & 0.6985 & 0.8113 & 0.7279 & 0.8137 \\
 & 71 & 394 & 759 & 426 & 817 \\
QQP &0.6106 & 0.6849 & 0.8477 & 0.8766 & 0.8048 \\
 & 60 & 300 & 582 & 865 & 632 \\
SST-2 &0.5195 & 0.7970 & 0.6823 & 0.8394 & 0.9163 \\
 & 58 & 391 & 253 & 560 & 1357 \\
MNLI &0.3220 & 0.3407 & 0.4158 & 0.6858 & 0.5294 \\
 & 61 & 342 & 722 & 1242 & 816 \\
QNLI &0.5338 & 0.8559 & 0.8737 & 0.8629 & 0.8720 \\
 & 213 & 2111 & 3168 & 2568 & 3651 \\
AGNEWS &0.6849 & 0.7039 & 0.8462 & 0.8596 & 0.8613 \\
 & 86 & 394 & 830 & 1281 & 1606 \\
SQuAD & 0.0016 & 0.2143 & 0.3314 & 0.3391 & 0.3398\\
 & 79 & 652 & 945 & 1262 & 1577 \\
\bottomrule
\end{tabular}
}
\vskip -0.2in
\end{table}

\begin{table}[t]
\caption{Performance of Qwen2.5-3B-Instruct with Preserved Functional Networks. }
\label{tab::qwen_preserve}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & 10 &  64 & 128 & 256 & 512\\
\midrule
CoLA &0.0000 & 0.6913 & 0.6913 & 0.6913 & 0.6913 \\
 & 1500 & 6694 & 8330 & 10602 & 6349 \\
MRPC &0.0000 & 0.6814 & 0.5245 & 0.6373 & 0.6324 \\
 & 1219 & 7556 & 10390 & 12042 & 12733 \\
SST-2 &0.0092 & 0.8200 & 0.8830 & 0.8888 & 0.8899 \\
 & 914 & 4592 & 7223 & 9686 & 11027 \\
MNLI &0.0014 & 0.2546 & 0.2476 & 0.2740 & 0.2725 \\
 & 2116 & 8319 & 10064 & 11285 & 12080 \\
QNLI &0.0002 & 0.2720 & 0.4077 & 0.3948 & 0.3967 \\
 & 750 & 7569 & 9478 & 10368 & 11043 \\
AGNEWS &0.1201 & 0.0061 & 0.7280 & 0.7205 & 0.7213 \\
 & 1876 & 7423 & 10274 & 11491 & 12118 \\
SQuAD & 0.0057 & 0.1900 & 0.1918 & 0.2102 & 0.2106\\
 & 2053 & 8436 & 10765 & 12272 & 13183 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

\begin{table}[t]
\caption{Performance of ChatGLM3-6B-base with Preserved Functional Networks.}
\label{tab::glm_preserve}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccc}
\toprule
 & 10 &  64 & 128 & 256 & 512\\
\midrule
CoLA &0.0000 & 0.6913 & 0.6913 & 0.6913 & 0.6913 \\
 & 2967 & 12306 & 15966 & 20495 & 27832 \\
MRPC &0.0000 & 0.7770 &  0.8186 & 0.8162 & 0.8162 \\
 & 2344 & 13917  & 18520 & 23369 & 28697 \\
SST-2 &0.0000 & 0.8624 & 0.9300 & 0.9381 & 0.9404 \\
 & 1235 & 9390 & 14854 & 18702 & 26513 \\
MNLI &0.0000 & 0.2123 & 0.2041 & 0.2062 & 0.2103 \\
 & 3399 & 12569 & 15586 & 20103 & 25080 \\
QNLI &0.0000 & 0.1545 & 0.1157 & 0.1078 & 0.1067 \\
 & 905 & 13846 & 22341 & 28645 & 30838 \\
AGNEWS &0.0000 & 0.9036 & 0.9087 & 0.9128 & 0.9128 \\
 & 2098 & 11831 & 18540 & 26489 & 32864 \\
SQuAD & 0.0001 & 0.8655 &0.9016 & 0.9023 & 0.9021\\
 & 2849 & 16150 & 21903 & 26646 & 30226 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

\begin{figure}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/performance.pdf}}
\caption{The performance changes of the models as the number of decomposed functional networks increases. }
\label{fig::performance}
\end{center}
\vskip -0.3in
\end{figure}

Table~\ref{tab::gpt2_preserve}, Table~\ref{tab::qwen_preserve} and Table~\ref{tab::glm_preserve} show the performance changes of the GPT-2, Qwen2.5-3B-Instruct, and ChatGLM3-6B-base models as the number of decomposed functional networks increases. With the addition of more decomposed functional networks, the models exhibit a gradual improvement in performance, moving from lower to higher performance metrics, which is also shown in Figure~\ref{fig::performance}. Across all models, it is clear that the number of neurons identified as important increases as more functional networks are incorporated. Our analysis reveals that, in practice, only approximately 10\% of the total neurons in the model are necessary to maintain performance at an acceptable level. This observation is crucial because it indicates that there may be substantial opportunities to improve the efficiency of AI models. By identifying and utilizing only the most critical neurons, we can potentially develop methods for creating more resource-efficient models without sacrificing performance. This insight could pave the way for advancements in AI model optimization, leading to lower computational costs and greater accessibility of AI technologies.


\section{Discussion and Conclusion}
The present study introduces a novel approach to understanding LLM by applying methods inspired by the research of cognitive neuroscience on FBN. Our findings reveal that LLMs exhibit similar functional patterns as observed in human brains, which are crucial for their effective operation.  

One of our key observations shows that only a small subset of these functional networks (approximately 10\% of the total neurons) are necessary to maintain satisfactory performance levels. By identifying and leveraging these critical networks, future research could focus on developing more resource-efficient models with lower computational cost.

Our interdisciplinary approach, which combines insights from cognitive neuroscience and artificial intelligence, offers a promising direction for future research. The application of neuroscience concepts to AI not only enhances our understanding of LLM mechanisms but also opens new avenues for innovation. For instance, techniques used to identify FBNs in the human brain can be adapted to improve the interpretability and efficiency of AI models. By drawing parallels between cognitive neuroscience and AI, our research underscores the value of interdisciplinary collaboration. Future work should continue to explore how neuroscience principles can inform AI design and optimization. Such efforts hold the promise of advancing both fields, leading to more powerful and efficient AI systems that can better serve diverse applications. 

Our study also has some limitations. The algorithm employed in this research is ICA, which has various derivatives such as canICA \cite{varoquaux2010group}. These variants can improve model's performance based on the unique characteristics of the dataset. Consequently, there is potential for the development of novel algorithms tailored specifically to the properties of LLMs. 

Last but not least, our investigation focused solely on the MLP layers. Future work could extend this approach to other components within these models, such as attention mechanisms or embedding layers. By broadening the scope of analysis to include these additional modules, we can gain a more comprehensive understanding of how functional networks manifest in different components of LLMs.

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}



% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}


% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
%restatable

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Neuron Lesion Experiment}
Figures~\ref{fig::leision1} and \ref{fig::leision2} present the neuron lesion study conducted on the ChatGLM3-6B-base model for the task of long-text summarization. Specifically, the study involves randomly masking neurons within the model and comparing the performance with masked neurons that are part of functional networks. 
\begin{figure}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth,height=16cm]{figures/leision1.pdf}}
\caption{Random masked 15\% neurons in ChatGLM3-6B-base.}
\label{fig::leision1}
\end{center}
% \vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/leision2.pdf}}
\caption{Masked 10 functional networks respectively in ChatGLM3-6B-base.}
\label{fig::leision2}
\end{center}
\vskip -0.2in
\end{figure}

% \subsection{Functional Networks}
% \foreach \i in {1,...,10} {
%     \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=\columnwidth]{figures/glm/ChatGLM3-6B-base_news_\i.png}}
%     \caption{Visualization of functional network template \#\i\ derived from the ChatGLM3-6B-base model in news data.}
%     \label{fig:news\i}
%     \end{center}
%     \vskip -0.2in
%     \end{figure}
% }

% % Wiki Data
% \foreach \i in {1,...,10} {
%     \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=\columnwidth]{figures/glm/ChatGLM3-6B-base_wiki_\i.png}}
%     \caption{Visualization of functional network template \#\i\ derived from the ChatGLM3-6B-base model in wiki data.}
%     \label{fig:wiki\i}
%     \end{center}
%     \vskip -0.2in
%     \end{figure}
% }

% % Math Data
% \foreach \i in {1,...,10} {
%     \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=\columnwidth]{figures/glm/ChatGLM3-6B-base_math_\i.png}}
%     \caption{Visualization of functional network template \#\i\ derived from the ChatGLM3-6B-base model in math data.}
%     \label{fig:math\i}
%     \end{center}
%     \vskip -0.2in
%     \end{figure}
% }

% % Code Data
% \foreach \i in {1,...,10} {
%     \begin{figure}[ht]
%     \vskip 0.2in
%     \begin{center}
%     \centerline{\includegraphics[width=\columnwidth]{figures/glm/ChatGLM3-6B-base_code_\i.png}}
%     \caption{Visualization of functional network template \#\i\ derived from the ChatGLM3-6B-base model in code data.}
%     \label{fig:code\i}
%     \end{center}
%     \vskip -0.2in
%     \end{figure}
% }




\end{document}
