% !TEX root = ../main.tex

\section{Background \& notation}
\label{sec:background}

\subsection{Density-based UVAS}
\label{subsec:framework}

The core idea of density-based UVAS methods is to assign high anomaly scores to image regions containing rare patterns. To implement this idea they involve two models, which we call a \emph{descriptor model} and a \emph{density model}. The descriptor model encodes image patterns into vector representations, while the density model learns their distribution and assigns anomaly scores based on the learned density.

The descriptor model \(f_{\theta^{\text{desc}}}\) is usually a pre-trained fully-convolutional neural network. For a 3D image \(\mathbf{x} \in \mathbb{R}^{H \times W \times S}\), it produces feature maps \(\mathbf{y} \in \mathbb{R}^{h \times w \times s \times d^{\text{desc}}}\), where each position \(p \in P\) corresponds to a descriptor \(\mathbf{y}[p] \in \mathbb{R}^{d^{\text{desc}}}\). Here, position set \(P = \{p \mid p \in [1, \ldots, h] \times [1, \ldots, w] \times [1, \ldots, s]\}\).

The density model \(q_{\theta^{\text{dens}}}(y)\) estimates the marginal density \(q_Y(y)\) of descriptors (\(Y\) denotes the descriptor at a random position in a random image). For an abnormal pattern at position \(p\), the descriptor \(\mathbf{y}[p]\) is expected to lie in a low-density region, yielding a low \(q_{\theta^{\text{dens}}}(\mathbf{y}[p])\). Conversely, normal patterns correspond to high density values. During inference, the negative log-density values,
$-\log q_{\theta^{\text{dens}}}(\mathbf{y}[p])$
are used as anomaly segmentation scores.

This framework can be extended using a conditioning mechanism. For each position \(p\), one can introduce an auxiliary variable \(\mathbf{c}[p]\), referred to as a \emph{condition}. Then, instead of modeling the complex marginal density \(q_Y(y)\), the conditional density \(q_{Y \mid C}(y \mid c)\) is learned for each condition \(c\) (\(C\) denotes the condition at a random position in a random image). At inference, the negative log-conditional densities,
$
-\log q_{\theta^{\text{dens}}}(\mathbf{y}[p] \mid \mathbf{c}[p]),
$
are used as anomaly scores. State-of-the-art methods~\cite{cflow,msflow} adopt this conditional framework and use sinusoidal positional encodings as conditions.

\subsection{Dense joint embedding SSL}
\label{subsec:ssl}

Joint embedding self-supervised learning (SSL) methods learn meaningful image representations without labeled data by generating positive pairs—multiple views of the same image created through augmentations like random crops and color jitter. These methods learn embeddings that capture mutual information between views, ensuring they are informative (discriminating between images) and invariant to augmentations (predictable across views). Contrastive methods, e.g., SimCLR~\cite{simclr}, explicitly push apart embeddings of different images, while non-contrastive methods, e.g., VICReg~\cite{vicreg}, avoid degenerate solutions through regularization. Details on SimCLR and VICReg objectives are in the Appendix~\ref{appendix:ssl}.

\emph{Dense} joint embedding SSL methods extend this idea by learning dense feature maps—pixel-wise embeddings that encode information about different spatial locations in an image. Instead of treating the entire image as a single entity, these methods define positive pairs at the pixel level: two embeddings form a positive pair if they correspond to the same absolute position in the original image but are predicted from different augmented crops. During training, dense SSL enforces similarity between positive pairs while avoiding collapse by encouraging dissimilarity between embeddings from different images or positions. DenseCL~\cite{dense_cl} and VADER~\cite{vader} use contrastive objectives, while VICRegL~\cite{vicregl} adopts a non-contrastive approach, regularizing the covariance matrix of embeddings to increase informational content. These methods excel at capturing fine-grained spatial information, making them ideal for tasks like object detection and segmentation.

%Explain that joint embedding methods learns mutual information between different image views. By changing the augmentations we may control the information content in the learned representations. Also use consistent notations with the appendix.}
%Joint embedding SSL leverages unlabeled data to learn representations invariant to transformations through auxiliary tasks. SSL objectives align embeddings of augmented views $x^{(1)}, x^{(2)}$ of the same image $x$ while avoiding trivial solutions (mapping all images to the same vector). In vision domain, augmentations typically include color jitter and random crops. Representations are derived by feeding inputs \(x\) to an encoder \(f_\theta\) (a neural network), yielding \(z = f_\theta(x)\). We employ adaptations of SimCLR~\cite{simclr} and VICReg~\cite{vicreg} to dense feature learning~\cite{vader,dense_cl,vicregl,vox2vec} in our approach. For detailed description of these methods, please refer to Appendix~\ref{appendix:ssl}.
