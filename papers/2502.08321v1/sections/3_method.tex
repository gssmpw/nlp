% !TEX root = ../main.tex

\section{Method}
\label{sec:method}

Our method introduces two key innovations to the density-based UVAS framework, described in Section~\ref{subsec:framework}: \emph{self-supervised descriptor model}, and \emph{self-supervised condition model}. The following Sections~\ref{subsec:descriptor_model} and~\ref{subsec:condition_model} describe these modules, while Section~\ref{subsec:density_models} describes details of density modeling. Figure~\ref{fig:method} illustrates the overall training pipeline.

\input{figures/method}

\subsection{Descriptor model}
\label{subsec:descriptor_model}

The descriptor model plays a crucial role in our method. It must generate descriptors that effectively differentiate between pathological and normal positions; otherwise, these positions cannot be assigned distinct anomaly scores within the density-based UVAS framework. At the same time, the descriptors should minimize the inclusion of irrelevant information. For instance, if the descriptors capture noise -- a common artifact in CT images -- the density model may assign high anomaly scores to healthy regions with extreme noise values, leading to false positive errors.

To pre-train the descriptor model, we use dense joint embedding SSL methods described in Section~\ref{subsec:ssl}, which allow explicit control over the information content of the representations. Specifically, we penalize descriptors for failing to distinguish between different positions within or across images, ensuring they capture spatially discriminative features. Simultaneously, we enforce invariance to low-level perturbations, such as cropping and color jitter, to eliminate irrelevant information.

The descriptor model training pipeline is illustrated in the upper part of Figure~\ref{fig:method}. From a random CT volume \(\mathbf{x}\), we extract two overlapping 3D crops of random size, resize them to \({H \times W \times S}\), and apply random augmentations, such as color jitter. The augmented crops, denoted as \(\mathbf{x}^{(1)}\) and \(\mathbf{x}^{(2)}\), are fed into the descriptor model, producing feature maps \(\mathbf{y}^{(1)}\) and \(\mathbf{y}^{(2)}\).

From the overlapping region of the two crops, we randomly select \(n\) positions. For each position \(p\), we compute its coordinates \(p^{(1)}\) and \(p^{(2)}\) relative to the augmented crops and extract descriptors \(y^{(1)} = \mathbf{y}^{(1)}[p^{(1)}]\) and \(y^{(2)} = \mathbf{y}^{(2)}[p^{(2)}]\). These descriptors form a \emph{positive pair}, as they correspond to the same position in the original image but are predicted from different augmentations.

Repeating this process for \(m\) different seed CT volumes yields a batch of ${N = n \cdot m}$ positive pairs, denoted as \(\{(y^{(1)}_i, y^{(2)}_i)\}_{i=1}^N\). Given this batch, we optimize the descriptor model with standard SSL objectives: InfoNCE~\cite{simclr} or VICReg~\cite{vicreg}, detailed in Appendix~\ref{appendix:ssl}.

Conceptually, our descriptor model is similar to dense SSL models described in Section~\ref{subsec:ssl}. However, our implementation have many important differences. In contrast to~\cite{dense_cl,vader,vicregl}, our model has a UNet-like architecture and its output feature maps have very high resolution ($h \times w \times s = H \times W \times S)$, which is a common standard for 3D medical image segmentation. \cite{dense_cl,vader} do not treat embeddings from the same image as negatives as we do. We do not employ any auxiliary global SSL objectives, like~\cite{dense_cl,vicregl}. And we do not obtain position-wise descriptors by concatenating features from feature pyramid, as in~\cite{vox2vec}. Other implementation details are described in Appendix~\ref{appendix:details}.

\subsection{Condition model}
\label{subsec:condition_model}

Our self-supervised condition model is inspired by a thought experiment: imagine a region of a CT image is masked, and we attempt to infer its content based on the visible context (see masked crops in Figure~\ref{fig:method} for illustration). In most cases, we would assume the masked region is healthy unless there is explicit evidence suggesting otherwise. This assumption reflects our model of the conditional distribution over possible inpaintings given the context. If the actual content deviates significantly from this distribution, we treat it as an anomaly.

This intuition suggests that the condition \(\mathbf{c}[p]\) in the conditional density-based UVAS framework should capture the \emph{global} context of the image position \(p\). \emph{Global} implies that \(\mathbf{c}[p]\) must be inferable from various masked views of the image. At the same time, conditions should vary across different images and regions within the same image to encode position-specific or patient-specific information effectively.

To achieve these properties, we propose learning conditions \(\mathbf{c}[p]\) using a self-supervised condition model \(g_{\theta^{\text{cond}}}\). This model shares the same fully convolutional architecture as the descriptor model and produces conditions \(\{\mathbf{c}[p]\}_{p \in P}\) in the form of feature maps \(\mathbf{c} \in \mathbb{R}^{h \times w \times s \times d^{\text{cond}}}\). To ensure conditions are inferable from any masked image view, we enforce feature maps invariance with respect to random image masking during training. Thus, the training procedure mirrors the training of the descriptor model (Section~\ref{subsec:descriptor_model}), with masking incorporated as part of the augmentations. An illustration of this approach is shown in the middle part of Figure~\ref{fig:method}.

The learned conditions \(\mathbf{c}[p]\) are designed to ignore the presence of pathologies, as such information cannot be consistently inferred from masked views. Instead, the condition model likely encodes patient-level attributes (e.g., age, gender) and position-specific attributes (e.g., anatomical region, tissue type) that are predictable from the context. Conditioning on these variables simplifies density estimation, as conditional distributions are often less complex than marginal distributions.
% Moreover, conditioning improves fairness: for instance, if certain anatomical regions or demographic groups are underrepresented in the training data, an unconditional density model might incorrectly treat these as anomalies. In contrast, a model conditioned on attributes like gender or anatomical region would handle such cases more appropriately by evaluating them within their specific context.

\subsection{Density model}
\label{subsec:density_models}

%As described in Section~\ref{subsec:method_overview} we use two types of density models: marginal and conditional. When training a marginal density model $q_{\theta^{\text{dens}}}(y)$ we sample a batch of $m$ random crops $\{\mathbf{x}_i\}_{i = 1}^m$ of size $H \times W \times S$ from different CT images. We feed each crop to the pre-trained descriptor model to obtain their descriptor maps $\{\mathbf{y}_i\}_{i = 1}^m$ of size $h \times w \times s$ and optimize the negative log-likelihood loss:
% \begin{equation}
%     \min_{\theta_{\text{dens}}} \quad \frac{1}{m \cdot |P|} \sum_{i = 1}^m\sum_{p \in P} -\log q_{\theta^{\text{dens}}}(\mathbf{y}_i[p]).
% \end{equation}

The conditional density model \( q_{\theta^{\text{dens}}}(y \mid c) \) can be viewed as a predictive model, which tries to predict descriptors based on the corresponding conditions. In this interpretation, anomaly scores
$
\{-\log q_{\theta^{\text{dens}}}(\mathbf{y}[p] \mid \mathbf{c}[p])\}_{p \in P}
$
are position-wise prediction errors. Also note, that marginal density model \( q_{\theta^{\text{dens}}}(y)\) is a special case of conditional model with constant condition $\mathbf{c}[p] = \mathrm{const}$.

To train a conditional density model \( q_{\theta^{\text{dens}}}(y \mid c) \), we sample a batch of \( m \) random crops, \(\{\mathbf{x}_i\}_{i=1}^m\), each of size \( H \times W \times S \), from different CT images. Each crop is passed through the pre-trained descriptor and condition models to produce descriptor maps, \(\{\mathbf{y}_i\}_{i=1}^m\), and condition maps, \(\{\mathbf{c}_i\}_{i=1}^m\). Then we optimize the conditional negative log-likelihood loss:
\[
  \min_{\theta_{\text{dens}}} \quad \frac{1}{m \cdot |P|} \sum_{i=1}^m \sum_{p \in P} -\log q_{\theta^{\text{dens}}}(\mathbf{y}_i[p] \mid \mathbf{c}_i[p]).
\]

At inference, an input CT image is divided into \( M \) overlapping patches, \(\{\mathbf{x}_i\}_{i=1}^M\), each of size \( H \times W \times S \). For each patch, we apply the descriptor, condition, and conditional density models to compute the anomaly map, \(\{-\log q_{\theta^{\text{dens}}}(\mathbf{y}_i[p] \mid \mathbf{c}_i[p])\}_{p \in P}\). These patch-wise anomaly maps are upsampled to \( H \times W \times S \) and aggregated into a single anomaly map for the entire CT image by averaging predictions in patches' overlapping regions.

We explore two parameterizations for the density model: Gaussian, as a straightforward baseline, and normalizing flows, similar to~\cite{cflow,msflow}, as an expressive generative model enabling tractable density estimation. These parameterizations and the details of their implementation in the context of UVAS framework are further described in Appendix~\ref{appendix:density_models}.
