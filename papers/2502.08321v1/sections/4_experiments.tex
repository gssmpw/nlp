% !TEX root = ../main.tex

\section{Experiments \& results}
\label{sec:experiments}

\subsection{Datasets}
\label{subsec:datasets}

\input{tables/datasets}
\input{figures/main_results}
\input{tables/main_results}

We train all models on three CT datasets: NLST~\cite{nlst}, AMOS~\cite{amos} and AbdomenAtlas~\cite{abdomen_atlas}. Note that we do not use any image annotations during training. Some of the datasets employed additional criteria for patients to be included in the study, i.e. age, smoking history, etc. Note that such large scale training datasets include diverse set of patients, implying presence of various pathologies.

We test all models on four datasets: LIDC~\cite{lidc}, MIDRC-RICORD-1a~\cite{midrc}, KiTS~\cite{kits} and LiTS~\cite{lits}. Annotations of these datasets include segmentation masks of certain pathologies. Any other pathologies that can be present in these datasets are not labeled.
We summarize the information about the datasets in Table~\ref{tab:datasets}.

\subsection{Evaluation metrics}
\label{subsec:metrics}

We use standard quality metrics for assessment of visual anomaly segmentation models which are employed in MVTecAD benchmark~\cite{mvtec}: pixel-level AUROC and AUPRO calculated up to $0.3$ FPR. We also compute area under the whole pixel-level ROC-curve. Despite, our model can be viewed as semantic segmentation model, we do not report standard segmentation metrics, e.g. Dice score, due to the following reasons. As we mention in Section~\ref{subsec:datasets}, available testing CT datasets contain annotations of only specific types of tumors, while other pathologies may be present in the images but not included in the ground truth masks. It makes impossible to fairly estimate metrics like Dice score or Hausdorff distance, which count our model's true positive predictions of the unannotated pathologies (see second image from the left in the Figure~\ref{fig:first_page} for example) as false positive errors and strictly penalize for them. However, the used pixel-level metrics are not sensitive to this issue, since they are based on sensitivity and specificity. We estimate sensitivity on pixels belonging to the annotated pathologies. To estimate specificity we use random pixels that do not belong to the annotated tumors which are mostly normal, thus yielding a practical estimate.

\input{tables/condition_ablation}
\input{tables/descriptor_ablation}

\subsection{Main results}

We compare Screener with baselines that represent different approaches to unsupervised visual anomaly segmentation. Specifically, we implement 3D versions of autoencoder~\cite{autoencoder}, f-anoGAN~\cite{fanogan} (reconstruction-based methods), DRAEM~\cite{draem}, MOOD-Top1~\cite{mood_top1} (methods based on synthetic anomalies) and MSFlow (density-based method on top of ImageNet features). Quantitative comparison is presented in Table~\ref{tab:main_results}. Qualitative comparison is shown in Figure~\ref{fig:main_results}.

The analysis of the poor performance of the reconstruction-based methods is given in Appendix~\ref{appendix:reconstruction}. Synthetic-based models yield many false negatives because during training they were penalized to predict zero scores in the unlabeled real pathological regions which may appear in training images. Meanwhile, MSFlow heavily relies on an ImageNet-pre-trained encoder which produces irrelevant features of 3D medical CT images. Our density-based model with domain-specific self-supervised features outperforms baselines by a large margin. %, an AUROC delta of ${\small \sim}0.26$ compared to the top performing baseline.

\subsection{Condition and density models' ablation}

Table~\ref{tab:condition_ablation} demonstrates ablation study of our proposed condition model. We compare our condition model with two baselines: vanilla sin-cos positional encodings and anatomical positional embeddings~\cite{ape}, described in Appendix~\ref{appendix:condition_models}. We evaluate condition models in combination with different density models, described in Section~\ref{subsec:density_models}. We use the VICReg descriptor model with ${d^{\text{desc}} = 32}$ as it shows slightly better results than contrastive objective as reported in Section~\ref{subsec:descriptor_ablation}.

When we use expressive normalizing flow density model, all conditioning strategies yield results comparable to each other and to the unconditional model. However, in experiments with simple Gaussian density models, we see that the results significantly improve as the conditioning variables becomes more informative. Noticeably, our proposed masking-invariant condition model allows Gaussian model to compete with complex flow-based models and achieve very strong anomaly segmentation results.

\subsection{Descriptor models' ablation}
\label{subsec:descriptor_ablation}

We also ablate descriptor models in Table~\ref{tab:descriptor_ablation}. We compare contrastive and VICReg models with ${d^{\text{desc}} = 32}$. To ablate the effect of the descriptors' dimensionality, we also include VICReg model with ${d^{\text{desc}} = 128}$. To demonstrate the superiority of our domain-specific self-supervised descriptors over supervised feature extractors pre-trained on natural images, we compare with MSFlow~\cite{msflow}. Additionally, we evaluate STU-Net~\cite{stu_net} -- a UNet pre-trained in a supervised manner on anatomical structure segmentation tasks -- as a descriptor model in our framework. However, it performs even worse than MSFlow, likely because the feature maps from the penultimate UNet layer are too specific to the pre-training task and lack information about the presence of pathologies.
