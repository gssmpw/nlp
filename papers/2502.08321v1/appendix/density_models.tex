% !TEX root = ../main.tex

\section{Density Models}
\label{appendix:density_models}

Below, we describe simple Gaussian density model and more expressive learnable Normalizing Flow model.

\textbf{Gaussian} marginal density model is written as
\begin{equation}
    -\log q_{\theta^{\text{dens}}}(y) = \frac{1}{2}(y - \mu)^\top \Sigma^{-1} (y - \mu) + \frac{1}{2}\log \det \Sigma + \text{const},
\end{equation}
where the trainable parameters $\theta^{\text{dens}}$ are mean vector $\mu$ and diagonal covariance matrix $\Sigma$.

Conditional gaussian density model is written as
\begin{equation}
    -\log q_{\theta^{\text{dens}}}(y \mid c) = \frac{1}{2}(y - \mu_{\theta^{\text{dens}}}(c))^\top \left(\Sigma_{\theta^{\text{dens}}}(c)\right)^{-1}(y - \mu_{\theta^{\text{dens}}}(c)) + \frac{1}{2}\log \det \Sigma_{\theta^{\text{dens}}}(c) + \text{const},
\end{equation}
where $\mu_{\theta^{\text{dens}}}$ and $\Sigma_{\theta^{\text{dens}}}$ are MLP nets which take condition $c \in \mathbb{R}^{d^{\text{cond}}}$ as input and predict a conditional mean vector $\mu_{\theta^{\text{dens}}}(c) \in \mathbb{R}^{d^{\text{desc}}}$ and a vector of conditional variances which is used to construct the diagonal covariance matrix $\Sigma_{\theta^{\text{dens}}}(c) \in \mathbb{R}^{d^{\text{desc}} \times d^{\text{desc}}}$. 

As described in Section~\ref{subsec:density_models}, at both training and inference stages, we need to obtain dense negative log-density maps. Dense prediction by MLP nets $\mu_{\theta^{\text{dens}}}(c)$ and $\Sigma_{\theta^{\text{dens}}}(c)$ can be implemented using convolutional layers with kernel size $1 \times 1 \times 1$. In practice, we increase this kernel size to $3 \times 3 \times 3$, which can be equivalently formulated as conditioning on locally aggregated conditions.

\textbf{Normalizing flow} model of descriptors' marginal distribution is written as:
\begin{equation}
    -\log p_{\theta^{\text{dens}}}(y) = \frac{1}{2}\|f_{\theta^{\text{dens}}}(y)\|^2 - \log \left| \det \dfrac{\partial f_{\theta^{\text{dens}}}(y)}{\partial y} \right| + \text{const},
\end{equation}
where neural net $f_\theta$ must be invertible and has a tractable jacobian determinant.

Conditional normalizing flow model of descriptors' conditional distribution is given by:
\begin{equation}
    -\log p_{\theta^{\text{dens}}}(y \mid c) = \frac{1}{2}\|f_{\theta^{\text{dens}}}(y, c)\|^2 - \log \left| \det \dfrac{\partial f_{\theta^{\text{dens}}}(y, c)}{\partial y} \right| + \text{const},
\end{equation}
where neural net $f_\theta\colon \mathbb{R}^{d^{\text{desc}}} \times \mathbb{R}^{d^{\text{cond}}} \to \mathbb{R}^{d^{\text{desc}}}$ must be invertible w.r.t. the first argument, and the second term should be tractable.

We construct $f_\theta$ by stacking Glow layers~\cite{glow}: act-norms, invertible linear transforms and affine coupling layers. Note that at both training and inference stages we apply $f_\theta$ to descriptor maps $\mathbf{y} \in \mathbb{R}^{h \times w \times s \times d^{\text{desc}}}$ in a pixel-wise manner to obtain dense negative log-density maps. In conditional model, we apply conditioning in affine coupling layers similar to~\cite{cflow} and also in each act-norm layer by predicting maps of rescaling parameters based on condition maps.