% !TEX root = ../main.tex

\section{Self-Supervised Learning}
\label{appendix:ssl}

\paragraph{InfoNCE.} In contrastive learning, batch of positive pairs $\{(y^{(1)}_i, y^{(2)}_i)\}_{i = 1}^N$ is passed through a trainable MLP-projector $g_{\theta^{\text{proj}}}$ and l2-normalized: ${z^{(k)}_i = g_{\theta^{\text{proj}}}(y^{(k)}_i) / \|g_{\theta^{\text{proj}}}(y^{(k)}_i)\| \in \mathbb{R}^d}$, where ${k = 1, 2}$ and ${i = 1, \ldots N}$. Then, the key objective is to maximize the similarity between embeddings of positive pairs while minimizing their similarity with negative pairs. To this end, InfoNCE loss written as:
\begin{equation}
    \min_{\theta} \quad \sum_{i = 1}^N \sum_{k \in \{1, 2\}} - \log\frac{\exp(\langle z_i^{(1)}, z_i^{(2)} \rangle / \tau)}{\exp(\langle z_i^{(1)}, z_i^{(2)} \rangle / \tau) + \sum_{j \ne i} \sum_{l \in \{1, 2\}} \exp(\langle z_i^{(k)}, z_j^{(l)} \rangle / \tau)}.
    \label{eq:infonce}
\end{equation}

\paragraph{VICReg.} VICReg objective enforces invariance among positive embeddings while constraining embeddings' covariance matrix to be diagonal and variance to be equal to some constant:
\begin{equation}
    \min_{\theta} \quad \alpha \cdot \mathcal{L}^{\text{inv}} + \beta \cdot \mathcal{L}^{\text{var}} + \gamma \cdot \mathcal{L}^{\text{cov}}.
\end{equation}
The first term ${\mathcal{L}^{\text{inv}} = \frac{1}{N \cdot D} \sum_{i = 1}^N \|z^{(1)}_i - z^{(2)}_i\|^2}$ penalizes embeddings to be 
invariant to augmentations. The second term ${\mathcal{L}^{\text{var}} = \sum\limits_{k \in \{1, 2\}} \frac{1}{D} \sum\limits_{i = 1}^{D} \max \left(0, 1 - \sqrt{C^{(k)}_{i,i} + \varepsilon}\right)}$ enforces individual embeddings' dimensions to have unit variance. The third term ${\mathcal{L}^{\text{cov}} = \sum_{k \in \{1, 2\}} \frac{1}{D} \sum_{i \ne j} \left(C^{(k)}_{i, j}\right)^2}$ encourages different embedding's dimensions to be uncorrelated, increasing the total information content of the embeddings. In VICReg embeddings $\{z^{(k)}_i\}$ are not l2-normalized and obtained through a trainable MLP-expander which increases the dimensionality up to $8192$.
