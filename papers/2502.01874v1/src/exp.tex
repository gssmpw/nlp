

\subsection{Experimental Setup}

We evaluate our methods on synthetic
and real-world instances.
We wrote our code in Python~3 and ran our methods
on a 2.9 GHz Intel Xeon Gold 622R processor
with 384GB RAM.
Our code is publicly and anonymously
available~\cite{ourgit2024}.
We consider only the
maximization objective for
a threshold of $\theta = 0.5$.
This aligns with the common goal to influence a majority of votes.


\paragraph{Datasets.} We evaluate our methods on a variety of synthetic graphs, each with different topologies and $n=100$ nodes, such as grids and random binomial graphs $G(n,p)$~\cite{erdds1959random}. For simplicity,
we use uniform resistances of $\frac 1 2 $. For each network, we obtain instances by sampling innate opinions from three distinct distributions: normal, log-normal, and bimodal. Owing to space limitations, the majority of our detailed findings are presented in the appendix.
 

% \labis{It seems that the you do not need the legend on all 4 plots in Fig1a,b and Fig2. Keep only one for less redundancy.}




 

\begin{table}
  \caption{Statistics for real-world graphs. Median
  and Average refer to the equilibrium opinions $x^\star$
  before any optimization.
  We aim to maximize or minimize the median, depending
  on whether it is initially below or above the
  threshold $\theta=0.5$,
  respectively, as listed here.}
  \centering
  \medskip
  \label{tab:stats}
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{llrrrr}
    \toprule
    Instance & Reference & $n$ & $m$ & $\mathrm{Median}$ & $\mathrm{Average}$ \\
    \midrule

$\textsf{Baltimore}$&\cite{garimella18}&3902&32646&0.85&0.51 \\
$\textsf{Baltimore-F}$&&1441&28291&0.16&0.45 \\
$\textsf{Baltimore-R}$&&3902&4505&0.85&0.51 \\
$\textsf{Beefban}$&&1610&7943&0.09&0.49 \\
$\textsf{Beefban-F}$&&799&6026&0.11&0.46 \\
$\textsf{Beefban-R}$&&1610&1978&0.09&0.49 \\
$\textsf{Gunsense}$&&7106&114887&0.11&0.48 \\
$\textsf{Gunsense-F}$&&1821&103840&0.84&0.53 \\
$\textsf{Gunsense-R}$&&7106&11483&0.11&0.48 \\
$\textsf{Russia}$&&2134&19273&0.05&0.49 \\
$\textsf{Russia-F}$&&1189&16471&0.11&0.48 \\
$\textsf{Russia-R}$&&2134&2951&0.05&0.49 \\  \midrule 
$\textsf{Vax}$&\cite{ristache2024wiser}&3409&11508&0.18&0.2 \\
$\textsf{War}$&&3409&11508&0.49&0.48 \\ \midrule
$\textsf{Karate Club}$&\cite{girvan2002community}&34&78&0.46&0.47 \\

  \bottomrule
\end{tabular}
\end{table}

We also use publicly available real-world datasets
from  Garimella et al. \cite{garimella18} and Ristache et al. \cite{ristache2024wiser} (\textsf{Vax}, \textsf{Ukraine}).
The networks represent users posting on
a specific topic, and their innate opinion
$s_v \in [0, 1]$ is determined by the
sentiment expressed in a post. The
$\textsf{Vax}$ and $\textsf{War}$ datasets
use fine-grained innate opinions, while
the other datasets use
$s_v \in \{0, 1\}$. Each topic
results in two networks, where an edge exists
between two users there is a retweet
($\textsf{-R}$) or
follow relationship ($\textsf{-F}$) 
between them. We also create a third
network combining both relationships,
which we indicate by omitting
the postfix.
For the $\textsf{Vax}$ and $\textsf{War}$
datasets, both relationships are combined
into a single graph.
The resulting graphs are directed, even
though these relationships are undirected.
%
We also use the $\textsf{Karate Club}$ network
~\cite{girvan2002community}.
%
For simplicity, we assume resistances 
$\alpha_v = \frac 1 2$ for all nodes.
%
We show basic statistics of these datasets
in Table~\ref{tab:stats}.
We use constant weights $w_{uv} = 1$,
assuming for simplicity
that every node is uniformly
influenced by its neighbors.

\paragraph{Algorithms}

We use the algorithms introduced in
the main body and a few natural
baselines. We use the two continuous
methods $\textsf{Projected Huber}$
as described in
Algorithm~\ref{alg:huber-gd}
and the sigmoid threshold influence function
(Section~\ref{sec:sigmoid})
which we will refer to simply
as $\textsf{Sigmoid}$.
Recall that we choose resistances
to be uniformly $\frac 1 2$.
Since in our targeted problem,
the budget is on
$\|\alpha - \alpha'\|_0$,
while the budget for the continuous
problem is with respect to
$\|\alpha - \alpha'\|_1$, we
thus halve the budget in order
to compare results for discrete
targeted and continuous methods.
Instead of simple gradient
descent, we use the ADAM-Optimizer
with parameters $\beta_1 = 0.9$
and $\beta_2 = 0.999$.
We select the tuning constant $c$
for $\textsf{Projected Huber}$ via
a heuristic which we detail in
Section~\ref{subsec:ablation}.
We set a temperature
of $\tau=25$ for $\textsf{Sigmoid}$.
We use three baseline methods that
select $k$ stooges based on node
measures.
We run \textsf{Lazy Greedy}
with laziness $\phi=0.8$.
These are selecting
$k$ nodes uniformly at random
($\textsf{Random}$), selecting
the $k$ nodes with highest degree
($\textsf{Max-Degree}$) or
betweenness centrality
($\textsf{Centrality}$) \cite{freeman1977}.
For the maximization
objective, for each selected
stooge $u$, we set its resistance
to $\alpha_u = 1$ if $s_u > \theta = 0.5$
and and $\alpha_u = 0$, otherwise.


\subsection{Scalability}

We showcase the scalability of our network
on moderately sized synthetic networks and
larger real-world networks in
Figure~\ref{fig:scalability} (right).
Results on synthetic data
are in Figure~\ref{fig:scalability-synthetic}
in the appendix.
As expected, the baseline approaches
$\textsf{Random}$ and $\textsf{Max-Degree}$
are extremely fast, while $\textsf{Centrality}$
incurs some overhead to compute the centrality.
Indeed, this overhead almost approaches
the running time for the continuous approaches
$\textsf{Projected Huber}$ and $\textsf{Sigmoid}$
on larger datasets.
Both continuous approaches have very
similar running times across all datasets.
One key insight is that even though
$\textsf{Lazy Greedy}$ is fast on moderately
sized graphs, the running time cost from
recomputing marginal gains in each iteration
becomes significant a significant
bottleneck for larger graphs and
larger budgets $k$.
%Some networks such as $\textsf{Communities}$ also force many recomputation for the marginal gains, which is expensive.
On the other hand, the running time
of continuous approaches does not
depend on the budget $k$, which is
noticeable on larger graphs. Our results are qualitatively consistent with the runtimes on synthetic datasets. While our work has made significant strides, scalability remains an intriguing and open direction for future research, where potentially sublinear-time algorithms for approximating functions of the equilibrium  may be helpful~\cite{neumann2024sublinear}.



\begin{figure*}[htbp]
    \centering
    \small
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{cccc}
        \includegraphics[width=0.26\textwidth]{plots/plot_last_c.pdf} & 
        \includegraphics[width=0.26\textwidth]{plots/plot_last_c--10-14-2024--15-24-42-908806.pdf}  & 
        \includegraphics[width=0.23\linewidth,trim={0 0 0 23pt},clip]{plots/plot_last_m_f.pdf} &
        \includegraphics[width=0.23\linewidth,trim={0 0 0 23pt},clip]{plots/plot_last_m.pdf} \\[-3pt]
    (a) & (b) & (c) & (d)
    \end{tabular}
    \vspace{-5pt}
    \caption{ \label{fig:ablation-c} Performance of $\textsf{Projected Huber}$ on   (a) a $10\times 10$ \textsf{Grid} and (b)  $\textsf{Beefban-F}$ with a budget of $k=50$ stooges, for various values of $c$. The plots display the instance-specific value of $c$ determined by our heuristic strategy to identify an optimal value. Median and runtime as a function of parameter $\phi$  for the $\textsf{Lazy Greedy}$ on (c) a $23 \times 23$ grid and (d) \textsf{Beefban-F}. 
    }
\end{figure*}

\subsection{Swaying the Election Results}

\begin{figure}
    \centering
    % \includegraphics[width=\linewidth]{plots/plot_run_until_num_stooges-normal-max--10-13-2024--09-06-32-546373.pdf}
    \includegraphics[width=0.5\linewidth]{plots/plot_run_until_num_stooges-normal-max--10-14-2024--16-11-25-382099.pdf}~
    \includegraphics[width=0.5\linewidth]{plots/plot_running_times2-normal-max--10-14-2024--16-22-56-411430.pdf}
    \figsp
    \caption{Flipping the median on real-world networks. 
    On the left,
    we show the required number of stooges to flip the median across the
    threshold of $\theta=0.5$, as a percentage of the
    number of vertices in the graph.
    On the right, we show the running times, where the budget
    is the least budget to move the median opinion above
    the threshold, corresponding to the values on the left.
    }
    \label{fig:real-world}
    \label{fig:scalability}
\end{figure}


Figure~\ref{fig:real-world} (left) shows
our results on real-world networks.
Our results show that a
small percentage of stooges suffices
to change the median drastically.
One reason for the increased
susceptibility in real-networks
over synthetic networks (cf. Figure~\ref{fig:synthetic-normal}
in the appendix) is the
high skewness: Table~\ref{tab:stats}
shows that the median is usually
far from the average opinion,
which is much closer to the 
threshold $\theta=0.5$.
We see that across most
real-world networks,
$\textsf{Projected Huber}$ is
able to flip the median
with the smallest number of stooges.  Among the targeted discrete methods, $\textsf{Lazy Greedy}$
is effective, but in some
instances requires even more
stooges than the baselines
$\textsf{Centrality}$ or
$\textsf{Max-Degree}$.
Overall, targeting a set of stooges
instead of using a continuous intervention
leads to weaker results.


\subsection{Ablation Study}
\label{subsec:ablation}

The primary parameters governing our algorithms are the Huber constant $c$ and the laziness parameter $\phi$. We discuss a heuristic for finding a good value of $c$ in the sense of yielding a near-optimal value for the equilibrium median. Additionally, we demonstrate that increasing the parameter $\phi$ generally leads to speedups, although this monotonic behavior is not consistently observed.

\spara{Tuning Huber's constant $c$}
\label{sec:find-c}



% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{plots/plot_last_c.pdf}
%     \includegraphics[width=\linewidth]{plots/plot_last_c--10-14-2024--15-24-42-908806.pdf}
%     \figsp
%     \caption{Performance of
%     $\textsf{Projected Huber}$ (top) on a
%     $\textsf{Grid}$ with $n=100$ nodes
%     and $\textsf{Beefban-F}$ (bottom)
%     on a budget of $k=50$ stooges,
%     for different values of $c$.
%     We show the (instance-specific) value
%     of $c$ resulting from our
%     heuristic strategy
%     to determine a good value.}
%     \label{fig:ablation-c}
% \end{figure}

 

Figure~\ref{fig:ablation-c} (a) and (b) show
the performance of our continuous
method $\textsf{Projected Huber}$ for
different values of $c$.
We observe that the choice
of $c$ poses a trade-off between
the approximation of the median and 
the continuity of the objective,
which is required for successful
optimization. Our instance-specific heuristic for setting $c$ (indicated by dotted lines) identifies a value of $c$ that closely approximates the optimal value, which achieves the maximum median. Practically, it is advisable to test a range of $c$ values centered around the heuristic's suggestion, such as $0.5c, c, 2c$. % , to select the most effective outcome.
%
% \labis{For our expanded journal version to be submitted to ArXiv, we will emphasize the optimal \( c \) that achieves the maximum median by marking it with a star. This detail is not required at the moment.}
%
\textsf{Projected Huber} uses a short algorithm to automatically determine a value for $c$. To find the $c$ value, we randomly perturb the graph's resistances and opinions by a small constant $\epsilon$ in either
direction, generating multiple different instances of the problem. This strategy proves robust against several choices of $\epsilon$. We then select the value of $c$ that minimizes the distance of the Huber M-estimator $\hat y$ in comparison to the true median, averaged over all instances. 




\paragraph{Laziness parameter $\phi$}

Figure~\ref{fig:ablation-c} (c)
and (d) show the effect of the laziness
$\phi$.
We show the running time and resulting
median opinion.
We observe that, as expected, the
running time improves for higher
values of $\phi$ but we obtain
lower values for the median opinion.
Note that the relationship between
$\phi$ and the running time is not
necessarily monotone, as selecting
a different stooges in
Algorithm~\ref{alg:lazy-greedy}
may lead to different subsequent
decisions.


\iffalse
\begin{figure}
    \includegraphics[width=\linewidth]{plots-tmp/plot_last_m_f.pdf}
    \includegraphics[width=\linewidth]{plots-tmp/plot_last_m.pdf}
    \figsp
    \caption{Different results for variations of the laziness parameter on a grid graph.}
    \label{fig:ablation-laziness}
\end{figure}
\fi

% \paragraph{Temperature $\tau$}

% \begin{figure}
%     \includegraphics[width=\linewidth]{plots-tmp/plot_last_t.pdf}
%     \figsp
%     \caption{Different results for variations of $\tau$. \fabian{describe in words instead}}
%     \label{fig:tau}
% \end{figure}
 
