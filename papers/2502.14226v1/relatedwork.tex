\section{Related Works}
\vspace{-5pt}
\label{app: related works}
\subsection{Diffusion Models and Architectures}
Diffusion Models were originally introduced by~\citep{sohl2015deep} and had a rapid rise later due to the works of~\citep{song2019generative, ho2020denoising, song2020score}.~\citep{karras2022elucidating} showed comprehensively how convolution-based U-Nets can be adapted to perform diffusion efficiently. More recently, with the advent of transformers, Diffusion Transformers (DiTs)~\citep{peebles2023scalable}, Scalable Interpolant Transformers (SiTs)~\citep{ma2024sit}, Hourglass DiTs (HDiTs)~\citep{crowson2024scalable} and so on have been proposed to show the scalability of the transformer architecture for diffusion. Our work only looks at the distillation of DiTs~\citep{peebles2023scalable} even though the conclusions can be extended to the other transformer architectures mentioned above.

\subsection{Efficient Diffusion Transformers}
Even though many efficiency techniques~\citep{wang2024patch, li2023q, zhao2023mobilediffusion} have been proposed for convolution-based diffusion models, we will focus here on the ones proposed for transformers. Quantization~\citep{chen2024q, wu2024ptq4dit} and pruning~\citep{fang2024tinyfusion} have been proposed for designing parameter-efficient DiTs. Efficient attention mechanisms~\citep{pu2025efficient, yang2025inf} have also been proposed to perform efficient diffusion using transformers. However, all these methods are orthogonal to our approach of parameter-reduction using distillation. Our approach is the first to reduce both parameters and diffusion timesteps using distillation.

\subsection{Timestep Distillation of Diffusion Models}
To improve the slow generation process of diffusion models, extensive research has been done to reduce the timesteps through distillation~\citep{luhman2021knowledge, salimans2022progressive, meng2023distillation, yin2024one, yin2024improved, zhou2024score}. Progressive distillation~\citep{salimans2022progressive} reduces the number of timesteps by two during each distillation stage, thus incurring a large training cost.~\citep{meng2023distillation} propose a classifier-free distillation method to generate images using 1-4 timesteps.~\citep{yin2024one, yin2024improved} use adversarial setups and distribution matching losses to perform one-step generation. Trajectory and consistency-based distillation~\citep{berthelot2023tract, song2023consistency, zheng2024trajectory} has also been considered to improve the speed of diffusion model generation. However, these methods are mainly proposed for U-Nets, do not consider parameter-reduction in models and hence typically initialize the student with the teacher at the beginning of training. Our problem looks at how to reduce both the parameters and timesteps for DiTs through distillation, which is considerably more challenging than just doing the latter.~\citep{geng2024one, teephysics} which do look at transformer distillation with a different student architecture mainly focus on the impact of equilibrium and physics-informed models, respectively, rather than pushing the parameters and compute down.

\subsection{SOTA Comparison}
\label{appsubsec: SOTA}
The only fair comparison for our method is~\citep{geng2024one}. Most works~\citep{yin2024one, yin2024improved, berthelot2023tract} for diffusion distillation have focused solely on timestep distillation and the ones which do consider parameter distillation~\citep{teephysics, dockhorn2023distilling} are for U-Nets and do not focus on providing design principles like us. The methods which use U-Nets for timestep distillation directly initialize their student model with the teacher model before starting training, a trick not available for methods like ours where there is architectural incompatibility in student-teacher networks. Hence we only compare with~\citep{geng2024one} which does parameter distillation for transformer-based diffusion. We also note that an expression similar to ours for depth was obtained in~\citep{wies2021transformer} independently and from a theoretical standpoint for different modality and non-distillation setup. This instills confidence in our design principles.
%-----------------------------------------------------------------------%