\section{Related Works}
\vspace{-5pt}
\label{app: related works}
\subsection{Diffusion Models and Architectures}
Diffusion Models were originally introduced by____ and had a rapid rise later due to the works of____.____ showed comprehensively how convolution-based U-Nets can be adapted to perform diffusion efficiently. More recently, with the advent of transformers, Diffusion Transformers (DiTs)____, Scalable Interpolant Transformers (SiTs)____, Hourglass DiTs (HDiTs)____ and so on have been proposed to show the scalability of the transformer architecture for diffusion. Our work only looks at the distillation of DiTs____ even though the conclusions can be extended to the other transformer architectures mentioned above.

\subsection{Efficient Diffusion Transformers}
Even though many efficiency techniques____ have been proposed for convolution-based diffusion models, we will focus here on the ones proposed for transformers. Quantization____ and pruning____ have been proposed for designing parameter-efficient DiTs. Efficient attention mechanisms____ have also been proposed to perform efficient diffusion using transformers. However, all these methods are orthogonal to our approach of parameter-reduction using distillation. Our approach is the first to reduce both parameters and diffusion timesteps using distillation.

\subsection{Timestep Distillation of Diffusion Models}
To improve the slow generation process of diffusion models, extensive research has been done to reduce the timesteps through distillation____. Progressive distillation____ reduces the number of timesteps by two during each distillation stage, thus incurring a large training cost.____ propose a classifier-free distillation method to generate images using 1-4 timesteps.____ use adversarial setups and distribution matching losses to perform one-step generation. Trajectory and consistency-based distillation____ has also been considered to improve the speed of diffusion model generation. However, these methods are mainly proposed for U-Nets, do not consider parameter-reduction in models and hence typically initialize the student with the teacher at the beginning of training. Our problem looks at how to reduce both the parameters and timesteps for DiTs through distillation, which is considerably more challenging than just doing the latter.____ which do look at transformer distillation with a different student architecture mainly focus on the impact of equilibrium and physics-informed models, respectively, rather than pushing the parameters and compute down.

\subsection{SOTA Comparison}
\label{appsubsec: SOTA}
The only fair comparison for our method is____. Most works____ for diffusion distillation have focused solely on timestep distillation and the ones which do consider parameter distillation____ are for U-Nets and do not focus on providing design principles like us. The methods which use U-Nets for timestep distillation directly initialize their student model with the teacher model before starting training, a trick not available for methods like ours where there is architectural incompatibility in student-teacher networks. Hence we only compare with____ which does parameter distillation for transformer-based diffusion. We also note that an expression similar to ours for depth was obtained in____ independently and from a theoretical standpoint for different modality and non-distillation setup. This instills confidence in our design principles.
%-----------------------------------------------------------------------%