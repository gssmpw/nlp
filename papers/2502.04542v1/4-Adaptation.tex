\section{VR Story Adaptation for User Study}
\label{sec:adaptation}


This section outlines the process of adapting well-established web-based stories into immersive environments for use as user study materials. We planned a within-subjects user study to minimize the influence of individual differences on the results. Given the four testing conditions in our study (as shown in \autoref{fig:procedure}), we required at least four immersive stories to mitigate the learning effect.

From the formative study results, we determined that viewpoint (ego vs. exo) was story-dependent, so we selected two stories for each viewpoint. 
Navigation (active vs. passive), however, was adaptable to any story, so we created both active and passive versions for each. A Latin square design was used to balance the presentation order of stories and navigation methods for participants. Four web-based stories were ultimately adapted into immersive formats for the study.

\subsection{Design Consideration Exploration}

Our goal was to explore design options for studying the effects of viewpoints and navigation. We examined the four design considerations identified in the formative study and selected the most appropriate options for adaptation.

For the {DC1} (\textit{main visual content}), we reused high-quality 3D models from the original stories. In egocentric stories, real-world scale was used for full immersion, while exocentric stories featured scaled-down models to fit within the user's field of view.

For the {DC2} (\textit{story narrative}), we reused the original text, considering several placement options:
1) Fixed placement: text appears in a consistent location (e.g., on a wall), though this may require frequent context-switching.
2) Floating panels: text is placed on movable panels, ensuring easy access with minimal effort.
3) Situated text: embedding text near the relevant objects to enhance semantic understanding.

{DC3} (\textit{navigation trigger}) in passive designs was triggered by simple actions like button presses. In active designs, it involved either locomotions~\cite{di2021locomotion} (e.g., walking or teleporting) or physical interactions (e.g., opening a window in VR).

For {DC4} (\textit{text and visual guidance}), concise textual instructions were provided, and visual indicators (e.g., arrows or hotspots) were used to guide navigation and interactions.


The design space of immersive stories is expansive, and building a comprehensive one is not the focus of this work and is beyond its scope. 
Our exploration of design options is not an exhaustive list of all possible alternatives but serves as structured guidelines for the necessary and critical adaptations. 
In the remainder of this section, we detail how we adapted the four selected stories using these design options.

\subsection{Case 1: How 911 changed Financial Distract in Manhattan}
\label{subsec:case1}
\begin{figure*}[t]
\centering
  \includegraphics[width=\linewidth]{Figures/911-row.pdf}
  \caption{Illustration of Case 1. The main visual element is a Manhattan landscape with 3D buildings (N1). The story narratives and the text guidance for navigation are in a text panel above the main visual element (N2). A transparent copy of the main visual element periodically animates to a designated perspective, visualizing the text guidance (N3). Audiences follow the guidance and the story proceeds to the next paragraphs (N4)}
  \Description{A VR adaptation of a story about lower Manhattan's transformation post-9/11. It displays four screenshots from a VR environment where the story is adapted with interactive 3D elements. In the VR environment, participants engage with the following components: N1 (Main Visual Element, showing lower Manhattan in 3D), N2 (Story Narratives and Text Guidance displayed on a floating panel), N3 (Active Navigation, allowing users to manipulate the environment), and N4 (Historical imagery overlayed on the 3D map). The VR version integrates interactive spatial elements and narrative text for a more immersive experience.}
  \label{fig:911-VR}
\end{figure*}
This story covers the societal changes in Manhattan’s Financial District post-9/11~\cite{911}. The original story featured a 3D model of Manhattan’s densely packed buildings, making it ideal for an exocentric VR adaptation. {The story has 15 steps and 611 words in total. A step includes a paragraph of story narratives with corresponding visual elements. Audiences procceed to next step by following a specific navigation trigger.}

\textit{Main Visual Element.} We built a 3D landscape of Manhattan in VR (\autoref{fig:911-VR}-N1) through Mapbox~\cite{mapbox_2017_mapbox}. The landscape is 2m-by-2m square base map by default. The current building information came from mapbox.3d-buildings tileset. When the story narratives mentioned a particular group of buildings (e.g. WTC buildings), they are highlighted in blue.

\textit{Story Narratives Placement.} Story narratives are displayed on an interactive panel in VR (\autoref{fig:911-VR}-N2), which supports repositioning, resizing and scrolling. We utilized a floating panel design because the narratives are often associated with the whole main visual element. It is not situated to a specific parts.

\textit{Text and Visual Guidance.} Text guidance appeared in smaller gray italics to minimize distraction. Visual guidance, in the form of a transparent 3D model, periodically animated to show the next perspective audience should have to look at the landscape (\autoref{fig:911-VR}-N3).

\textit{Navigation Triggers.} Active navigation involved rotating and zooming in/out of the landscape to trigger changes in the scene. In passive navigation, the landscape automatically adjusted as participants progressed.

\subsection{Case 2: In the Atlantic Ocean, Subtle Shifts Hint at Dramatic Dangers}
\label{subsec:case2}
\begin{figure*}[t]
\centering
  \includegraphics[width=\linewidth]{Figures/ATL-row.pdf}
  \caption{Illustration of Case 2. The main visual element of the story is a globe with animated current circulation in Atlantic Ocean (A1). The story panel with translucent background is fixed around the globe surface and centered between the audiences' view and the globe centroid (A1). Audiences actively navigate the story by grabbing the globe and rotating to the designated area mentioned in narratives (A2, A4), following the text and visual guidance (A1, A3).}
  \Description{A VR adaptation of a story about the weakening Gulf Stream in the Atlantic Ocean. It displays four VR screenshots: A1 (Main Visual Element, showing the Earth with a visual representation of the Gulf Stream and story narratives/text guidance), A2 (Active Navigation, where users can manipulate the globe for a better view), A3 (Visual Guidance, highlighting a specific region of the Gulf Stream), and A4 (Text and Visual Guidance, where users can interact with the 3D visualization to explore the story). The VR adaptation offers an interactive, spatial representation of the Gulf Stream data for a more immersive understanding.}
  \label{fig:atl-VR}
\end{figure*}
This story explores the impact of climate change on the Gulf Stream~\cite{atlantic}. We only selected the first half of the story, which has the most concentrated 3D spatial information. The main visualization of the original story was a globe with animated current circulations in Atlantic Ocean. Thus, we implemented a corresponding exocentric VR story. We summarized the text-only portion (484 words) into 2 paragraphs (100 words) to keep the length consistency. {The story has 15 steps and 503 words in total.}

\textit{Main Visual Element.} We adapted the original animation of ocean currents onto a 3D globe in VR. The globe’s size is adjustable to suit user preferences.

\textit{Story Narratives Placement.} The text portion of the story is in the same interactable panel as Case 1. However, in this case the panel is fixed around the globe surface and centered between audiences' view and the globe centroid (\autoref{fig:atl-VR}-A1). This is because the main visual element (i.e. the globe) occupies a much larger space than a normal human body and it would easily interfere with the panel if they were placed separately. To reduce the occlusion caused by the current placement, we make the panel background translucent ($\alpha = 0.1$, almost transparent) so that audiences can see underlying visualizations and keep track of the panel simultaneously.

\textit{Text and Visual Guidance.} The visual guidance of this case consists of 2 animated arrows (\autoref{fig:atl-VR}-A3). A bended arrow is placed in front of the audience's view on the surface and pointed to the next designated area mentioned in narratives. Another straight arrow is at that area and pointed to the globe centroid, indicating the direction audiences should look at the globe. It shows the next step 10 seconds after entering the current step. The text narrative is at the bottom of the panel and follows the same style as Case 1.

\textit{Navigation Triggers.} The active navigation of this case is primarily grabbing and rotating, as the original story rotated the globe along the Gulf Stream. For the 2 summarized text-only paragraphs, we adopt a step-away interaction: after audiences step away from the globe, the text panel leaves the surface and attaches to the audiences' view (0.5 meters from the eyes). Audiences can press a controller button to read the next paragraph and step closer to the globe after finishing 2 paragraphs. In the passive navigation, audiences only need to press a controller button and the globe would rotate and change the visualizations automatically.

\subsection{Case 3: What the Tulsa Race Massacre Destroyed}
\label{subsec:case3}
\begin{figure*}[t]
\centering
  \includegraphics[width=\linewidth]{Figures/TUL-row.pdf}
  \caption{Illustration of Case 3. T1 and T2 replicated the signposting sentence and image at the beginning of the story. Audiences then actively navigate the story on a main avenue of Tulsa before the massacre, following the visual guidance of arrows and hotspots (T3). After they arrive at a building, the corresponding story narratives would appear in a panel and a label nearby (T4).}
  \Description{A VR adaptation of the story of the Tulsa Race Massacre. It features four VR screenshots: T1 (Text Guidance showing a narrative about the event), T2 (A historical photograph presented in the VR environment), T3 (Main Visual Element with active navigation using teleportation to explore the reconstructed neighborhood), and T4 (Story Narratives displayed as floating panels alongside the VR scene). The VR adaptation allows users to actively explore and interact with visual elements of the historical scene for an immersive learning experience.}
  \label{fig:tul-VR}
\end{figure*}
The story of this case illustrates the business achievement of black Americans in Tulsa, Oklahoma in the early 1900s and how the town was destroyed by racist mobs~\cite{tulsa}. We only selected the first half the story, which has the most concentrated 3D information. We summarized the second half of the story into a paragraph (130 words) and put it at the end so that story narratives are logically complete while keeping similar length with previous cases. The main visualization of the original story comprises a 3D reconstruction of Tulsa before the massacre, which leads to our design of a VR story with an egocentric viewpoint in a main avenue (Greenwood Ave) of Tulsa (\autoref{fig:tul-VR}-T3). {The story has 18 steps and 618 words in total.}

\textit{Main Visual Elements.} Greenwood Ave of Tulsa included a number of black businesses alongside the road. Audiences follow a predefined route, in which one building would be highlighted at each step. At the beginning of the story, we keep the signposting sentence and image in the original story (\autoref{fig:tul-VR}-T1, T2) as they concisely describe the main event of the story.

\textit{Story Narratives Placement.} The narratives of this case consist of two parts: business labels and introductory paragraphs. In the original story, they were presented in two passes of the avenue. In our VR story, we combine these two types of narratives into one pass because it is more natural to follow a linear order, which ends the story when audiences reached the end of the avenue. Each building has a label listing the businesses inside, which is shown close to the building. The introductions are not on every building as not every business was introduced in the original story. They would be shown in front of the audiences' views.

\textit{Text and Visual Guidance.} The text guidance of this case only exists at the beginning (\autoref{fig:tul-VR}-T1), which asks audiences press a controller button to proceed. After audiences enter the Greenwood Ave, only the visual guidance indicates the building audiences should go. It shows the next step 10 seconds after entering the current step. It includes an arrow fixed at the bottom of the field of view (FOV), a hotspot highlight of the destination and another arrow above the hotspot pointing downwards. We fix an arrow in the FOV because it ensures audiences to find the hotspot even if it is behind them.

\textit{Navigation Triggers.} The active navigation of this case is predonimantly teleportation. Audiences follow the visual guidance and teleport to the front of each building and gradually to the end of the avenue. In the passive navigation, audiences only need to press a controller button and they are translated to the destination automatically.



\subsection{Case 4: Why Opening Windows Is a Key to Reopening Schools}
\label{subsec:case4}
\begin{figure*}[t]
\centering
  \includegraphics[width=\linewidth]{Figures/COV-row.pdf}
  \caption{Illustration of Case 4. The main visual element was a 3D classroom model (C1). Audiences actively navigated to the next designated area mentioned in narratives by teleportation (C2). After they arrived, they saw a visualization of airflow with corresponding narratives on the panel (C3). They were asked to drag down the window to see a visualization of a different airflow (C4).}
  \Description{A VR adaptation of a story on the importance of opening windows for safe school reopening. It features four VR screenshots: C1 (Main Visual Element showing the virtual classroom with a seated arrangement of students), C2 (Active Navigation via teleportation with visual guidance lines showing airflow), C3 (Story Narratives and Text Guidance displayed alongside airflow visualizations), and C4 (Active Navigation through object manipulation, with users grabbing and interacting with windows to alter the airflow in the classroom). The VR version allows for an interactive, spatial understanding of how air circulates within the environment and the impact of opening windows.}
  \label{fig:cov-VR}
\end{figure*}

The story of this case illustrated the importance of ventilation in classroom during COVID pandemic by comparing the concentrations of contamination in different ventilation conditions. The original story visualized the concentrations in both airflow and heatmap. We only selected the airflow visualization as it contains more 3D spatial information than the heatmap. Although the original story adopted a exocentric viewpoint, we decided to implement an egocentric VR story because it would be more immersive to be in a life-sized room rather than viewing a smaller room model. {The story has 13 steps and 513 words in total.}

\textit{Main Visual Element.} The classroom includes some models of students and a teacher. An air circulation animation is shown to depict the inhale and exhale flow in the classroom. A student is then highlighted as infected (in cyan in \autoref{fig:cov-VR}-C3), and the airflow becomes yellow and red to indicate the concentration differences. The interactable objects (e.g. the cyan window in \autoref{fig:cov-VR}-C4) are also highlighted when interacting with them to proceed through the story.

\textit{Story Narratives Placement.} The story narratives are mainly in the interactive panel. In this case, the panel is attached to the left controller, as a large portion of the narratives are about the airflow, which occupy the whole classroom and do not have a specific area to attach to.

\textit{Text and Visual Guidance.} Following Case 1 and 2, the text guidance is also at the bottom of the panel with the same font. The visual element follows the same design as Case 3, and we add an extra line connecting the hotspot and the bottom of the panel because the model of students and teacher might occlude the hotspot on the ground. Similiarly, it shows the next step 10 seconds after entering the current step.

\textit{Navigation Triggers.} The active navigation of this case involves teleportation and grabbing. Audiences are asked to teleport to different locations of the classroom to view the airflows. They can also drag down the window and put on the fan to change the ventilation conditions. In the passive navigation, audiences only need to press a controller button and their viewpoint, ventilation condition and corresponding visualizations would change automatically.


{\subsection{Story Caliberations}}
\label{sec:calibration}
{The original four stories have different story length, visual complexity and spatial information, which is hard to compare directly. We calibrated four cases based on the first three design considerations (DC1-3 in \autoref{sec:formative}): main visual element, story narratives, and navigation triggers. For text and visual guidance (DC4), a ``one-size-fits-all'' design could negatively impact some story instances. Therefore, we opted to use designs optimized for each individual story instance.}

{DC1: \textit{Main Visual Element(s)}. We calibrated the size of main visual element between two exo cases (Case 1 and 2) and two ego cases (Case 3 and 4), respectively. For exo cases, the main visual element was bounded by a cube with 2m edges. For egocentric cases, the main visual elements spread in a 20m-by-20m square arena, and the size of each elements was adjusted accordingly. Due to the different stories of each case, we were not able to have similar number of visual elements between exo and ego cases. But we guaranteed that the number of visual elements audiences needed to pay attention to at each step was no larger than 5.}

{DC2: \textit{Story Narratives}. For each case, we divided the story into about 15 steps. The steps were created based on the original story narrative setups, which consisted of several short paragraphs in a scrollytelling format. We combined neighboring short paragraphs and summarized long paragrahs to make each step about 30 to 50 words long. Thus, four cases had the similar lengths of narratives (15$\pm$3 steps and 550$\pm$50 words).}

{DC3: \textit{Navigation Triggers}. The navigation triggers in two exo cases were looking at the main visual element from a specific angle. In two ego cases, they were teleporting to a specific location. Due to the different scene and visual element settings, we cannot use the same navigation triggers between exo and ego cases.}

{DC4: \textit{Text and Visual Guidance}. The effectiveness of text and visual guidance was closely related to the other 3 DCs, particularly the main visual elements. Thus, we tried to optimize the guidance by designing the guidance separately based on visual element setup of each story case. For example, to indicate the manipulations audience needed to do (primarily rotation), Case 1 used the animations of a translucent "ghost" map and Case 2 used two arrows. Animation provided the clearest instructions of the next step, but it was not applicable to Case 2 whose main visual element was a globe. Similarly, both Case 3 and 4 used arrows and an on-ground hotspot to indicate the position audiences needed to teleport to. However, the hotspot could be easily occluded by student and desk models in Case 4. So we used an extra line connecting the hotspot to the left controller so that audiences would easily find the next position to teleport. We acknowledged that such inconsistency might cause confounding factors, but no single text and visual guidance design could serve as a clear and effective road sign in all four cases.}

