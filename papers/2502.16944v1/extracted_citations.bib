@article{ahmadian2024back,
  title={Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms},
  author={Ahmadian, Arash and Cremer, Chris and Gall{\'e}, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Pietquin, Olivier and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal={arXiv preprint arXiv:2402.14740},
  year={2024}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{gunter2024apple,
  title={Apple intelligence foundation language models},
  author={Gunter, Tom and Wang, Zirui and Wang, Chong and Pang, Ruoming and Narayanan, Andy and Zhang, Aonan and Zhang, Bowen and Chen, Chen and Chiu, Chung-Cheng and Qiu, David and others},
  journal={arXiv preprint arXiv:2407.21075},
  year={2024}
}

@article{han2024value,
  title={Value Augmented Sampling for Language Model Alignment and Personalization},
  author={Han, Seungwook and Shenfeld, Idan and Srivastava, Akash and Kim, Yoon and Agrawal, Pulkit},
  journal={arXiv preprint arXiv:2405.06639},
  year={2024}
}

@inproceedings{hong2024orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={11170--11189},
  year={2024}
}

@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Hu, Jian and Wu, Xibin and Wang, Weixun and Zhang, Dehao and Cao, Yu and others},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}

@article{hu2025reinforce++,
  title={REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}

@article{kong2024aligning,
  title={Aligning Large Language Models with Representation Editing: A Control Perspective},
  author={Kong, Lingkai and Wang, Haorui and Mu, Wenhao and Du, Yuanqi and Zhuang, Yuchen and Zhou, Yifei and Song, Yue and Zhang, Rongzhi and Wang, Kai and Zhang, Chao},
  journal={arXiv preprint arXiv:2406.05954},
  year={2024}
}

@article{li2023remax,
  title={Remax: A simple, effective, and efficient method for aligning large language models},
  author={Li, Ziniu and Xu, Tian and Zhang, Yushun and Yu, Yang and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2310.10505},
  year={2023}
}

@article{mao2024don,
  title={Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration},
  author={Mao, Xin and Li, Feng-Lin and Xu, Huimin and Zhang, Wei and Luu, Anh Tuan},
  journal={arXiv preprint arXiv:2402.16030},
  year={2024}
}

@article{meng2024simpo,
  title={Simpo: Simple preference optimization with a reference-free reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734},
  year={2024}
}

@article{noukhovitch2024language,
  title={Language model alignment with elastic reset},
  author={Noukhovitch, Michael and Lavoie, Samuel and Strub, Florian and Courville, Aaron C},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ramamurthy2022reinforcement,
  title={Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization},
  author={Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.01241},
  year={2022}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S},
  journal={A Bradford Book},
  year={2018}
}

@article{wu2023pairwise,
  title={Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment},
  author={Wu, Tianhao and Zhu, Banghua and Zhang, Ruoyu and Wen, Zhaojin and Ramchandran, Kannan and Jiao, Jiantao},
  journal={arXiv preprint arXiv:2310.00212},
  year={2023}
}

@article{xu2024dpo,
  title={Is dpo superior to ppo for llm alignment? a comprehensive study},
  author={Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
  journal={arXiv preprint arXiv:2404.10719},
  year={2024}
}

@article{yao2023deepspeed,
  title={Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales},
  author={Yao, Zhewei and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Rajbhandari, Samyam and Wu, Xiaoxia and Awan, Ammar Ahmad and Rasley, Jeff and Zhang, Minjia and Li, Conglong and Holmes, Connor and others},
  journal={arXiv preprint arXiv:2308.01320},
  year={2023}
}

