%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{arydshln}

\usepackage{pifont}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance}

\begin{document}

\twocolumn[
\icmltitle{Lean and Mean: Decoupled Value Policy Optimization with \\Global Value Guidance}


\newcommand{\authorfont}{\bfseries} 
\newcommand{\affiliationfont}{\normalfont}
\newcommand{\email}{\normalfont\itshape}

\icmlsetsymbol{intern}{*}
\icmlsetsymbol{corresponding}{\ding{170}}
\icmlsetsymbol{fudan}{\ding{169}}
\icmlsetsymbol{microsoft}{\ding{168}}



% 作者列表
\begin{icmlauthorlist}
    \icmlauthor{\authorfont Chenghua Huang}{intern,fudan}
    \icmlauthor{\authorfont Lu Wang}{microsoft,corresponding}
    \icmlauthor{\authorfont Fangkai Yang}{microsoft}
    \icmlauthor{\authorfont Pu Zhao}{microsoft}
    \icmlauthor{\authorfont Zhixu Li}{fudan}
    \icmlauthor{\authorfont Qingwei Lin}{microsoft}\\
    \icmlauthor{\authorfont Dongmei Zhang}{microsoft}
    \icmlauthor{\authorfont Saravan Rajmohan}{microsoft}
    \icmlauthor{\authorfont Qi Zhang}{microsoft}
\end{icmlauthorlist}

\centering
\icmlauthor{\affiliationfont \ding{169} School of Computer Science, Fudan University}{} \\
\icmlauthor{\affiliationfont \ding{168} Microsoft}{} \\
\icmlauthor{\email huangch22@m.fudan.edu.cn, \{wlu, fangkaiyang, puzhao, dongmeiz\}@microsoft.com}{} \\
%\icmlaffiliation{microsoft}{Microsoft}
%\icmlaffiliation{fudan}{School of Computer Science, Fudan University}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
% % Version 1
% Reinforcement Learning from Human Feedback (RLHF) is critical for aligning large language models (LLMs) with human preferences, but traditional methods require joint training of a policy model, value model, and pretrained reward model, leading to computational inefficiency and instability. Unlike standard RL, RLHF lacks ground-truth environmental rewards during policy training, exacerbating the fragility of co-training interdependent policy and value models. Inspired by pretrained reward models that provide fixed guidance, we propose a novel framework that replaces joint value-policy optimization with a pretrained global value model decoupled from policy training. Our approach first trains this value model offline on diverse trajectories from multiple policies, scored by a fixed reward model. By conditioning on state-action pairs and policy trajectories, the model learns to predict policy-agnostic, token-level return-to-go values, leveraging LLMs’ inherent ability to capture sequential dependencies. Once trained, the value model remains fixed during policy optimization, eliminating the need for joint training while preserving fine-grained reward guidance. This paradigm achieves better training efficiency with \textcolor{red}{XX memory per GPU reduction, xx training time reduction} compared to traditional RLHF (training only the policy), significantly lowering computational costs and improving stability. Experiments demonstrate that our method retains performance on complex language tasks, offering a scalable and efficient alternative to conventional RLHF frameworks.

% version 2
% Reinforcement Learning from Human Feedback (RLHF) is pivotal for aligning large language models (LLMs) with human preferences, but traditional approaches suffer from computational inefficiency and instability due to joint training of interdependent policy, value, and reward models. Unlike standard RL, RLHF lacks environmental rewards, exacerbating instability during co-training. We propose a novel framework that replaces joint optimization with a pretrained global value model decoupled from policy training. This model is trained offline on diverse policy trajectories, using a fixed reward model to predict policy-agnostic, token-level returns by leveraging LLMs’ sequential reasoning capabilities. During policy optimization, the value model remains fixed, eliminating joint training while preserving fine-grained reward guidance. \textcolor{red}{Our method reduces trainable models, cutting GPU memory usage by [X]\% and training time by [Y]\% compared to conventional RLHF. Experiments on [Z] benchmarks demonstrate comparable performance to state-of-the-art RL methods (e.g., [Method A], [Method B]), achieving [Task 1] and [Task 2] success rates within [K]\% of baselines. This work offers a stable, scalable, and resource-efficient paradigm for RLHF, addressing key bottlenecks in LLM alignment.}

% version 3

Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback.
To address these issues, we propose \textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \emph{global value model (GVM)}. The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\% and training time by 35\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance. 

% Our work demonstrates that sequential reasoning in LLMs can be harnessed stably through \emph{global value guidance}, bypassing the complexity of joint policy-value training.

% By decoupling the value model from policy training, we reduce both complexity and instability. Once trained, the value model remains fixed, guiding policy optimization with fine-grained value estimates and eliminating the need for joint actor-critic updates.

% Our method enables efficient, stable RLHF training by leveraging LLMs' sequential reasoning. Our method reduces trainable models, cutting GPU memory usage by 40\% and training time by 35\% compared to conventional RLHF. Experimental results across multiple benchmarks demonstrate that DVPO significantly outperforms other computationally efficient RLHF methods (e.g., DPO), while achieving performance comparable to state-of-the-art RL (PPO) methods.

% version 4 with name DVPO and GVM included:

% Reinforcement Learning from Human Feedback (RLHF) for aligning large language models (LLMs) faces computational and stability challenges due to joint actor-critic training. Current approaches like Proximal Policy Optimization (PPO) require interdependent policy-reward optimization while lacking access to true environment rewards, limiting adaptability. We propose \textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that replaces traditional reward modeling with a pretrained \emph{global value model (GVM)}. The GVM---trained once on offline trajectories to predict policy-conditioned, token-level returns---provides fixed, fine-grained value estimates without iterative updates. By decoupling value guidance from policy optimization (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\% and training time by 35\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance. Our work demonstrates that sequential reasoning in LLMs can be harnessed stably through \emph{global value guidance}, bypassing the complexity of joint policy-value training.







% Reinforcement Learning from Human Feedback (RLHF) plays a critical role in aligning large language models (LLMs) with human preferences. However, traditional RLHF methods require joint training of a policy model, a value model, and a pretrained reward model, leading to significant computational complexity and instability. Unlike traditional RL, RLHF cannot obtain ground-truth rewards from the environment during policy training. As a result, the joint training of value and policy models introduces inefficiencies and instability due to their interdependence. To address these challenges, we propose a novel framework that leverages a global value model.

% Our approach first trains a global value model offline using trajectories scored by the pretrained reward model. The value model is conditioned on state-action pairs and policy trajectories, enabling it to predict policy-conditioned, token-level return-to-go values. This approach is feasible because LLMs excel at learning complex mappings and dependencies in sequential data, making them well-suited for capturing the relationship between trajectories, actions, and long-term rewards. By conditioning on policy-generated trajectories, the global value model generalizes across policies, providing precise value estimates even in complex environments.

% Once trained, the global value model is fixed and used during policy optimization, enabling efficient and stable training. By decoupling value learning from policy updates, our method significantly reduces computational cost while retaining the fine-grained guidance necessary for complex tasks.
\end{abstract}

% \section{Introduction}
% \label{introduction}

\section{Introduction}
\label{sec:intro}

Large Language Models (LLMs)  have demonstrated state-of-the-art performance across a range of tasks~\cite{achiam2023gpt,bubeck2023sparks}, including dialogue generation~\cite{ouyang2022training}, summarization~\cite{stiennon2020learning}, and code synthesis~\cite{gao2023pal}. \footnotemark
\footnotetext{Work is done during * an internship at Microsoft. \ding{170} corresponding author.}
To further align these models with human preferences, Reinforcement Learning from Human Feedback (RLHF) has become an essential technique~\cite{christiano2017deep,bai2022constitutional,song2024preference}. RLHF leverages human-labeled feedback to guide model behavior, producing more accurate and user-friendly responses over time~\cite{bai2022training,ganguli2022red}. Among RLHF algorithms, Proximal Policy Optimization (PPO) is widely adopted due to its balance between stability and performance~\cite{schulman2017proximal,vonwerra2022trl,huang2024n+}.

Despite its effectiveness, PPO-based RLHF comes with significant challenges. It requires joint training of an actor (policy) and a critic (value function), while relying on a fixed, pretrained reward model~\cite{ziegler2019fine,ouyang2022training,wang2024secrets}. This joint optimization introduces dense computational complexity and training instability, as the actor and critic must co-adapt over time~\cite{yao2023deepspeed,huang2024n}. Unlike in traditional reinforcement learning (RL), where continuous environment interaction provides updated rewards, RLHF in LLMs lacks access to ground-truth environment rewards during training. As a result, the reward model cannot evolve to reflect changes in the policy's behavior, further complicating value estimation and policy optimization~\cite{moskovitz2023confronting}.

To mitigate these issues, several works have proposed simplifying the RLHF process by reducing the number of trainable components. Direct Preference Optimization (DPO) bypasses training both reward and value models, optimizing policies directly on preference data~\cite{rafailov2024direct,azar2024general,zhao2023slic,park2024disentangling}. While efficient, DPO lacks the iterative refinement of true reinforcement learning and struggles with distribution shifts~\cite{xu2024dpo}. Other reward-only approaches, such as ReMax~\cite{li2023remax} and GRPO~\cite{shao2024deepseekmath}, use final rewards applied uniformly across all tokens but suffer from high variance due to coarse token-level supervision, failing to optimize return-to-go dynamics and resulting in inefficient policy updates~\cite{ahmadian2024back,gunter2024apple,hu2025reinforce++}


% and unstable training because they lack token-level precision and fail to optimize return-to-go dynamics, resulting in inefficient policy updates~\cite{ahmadian2024back,gunter2024apple,hu2025reinforce++}.
In the context of RLHF without ground-truth environment rewards, we observe that pretraining a reward model and pretraining a value model become theoretically equivalent. Both models provide static supervisory signals about expected outcomes and cannot improve without additional reward feedback. This insight allows us to simplify the RLHF pipeline by eliminating the need for joint actor-critic training. Instead, we propose \textbf{Decoupled Value Policy Optimization (DVPO)}, a framework that eliminates joint actor-critic training while preserving fine-grained value guidance. It pretrains a global value model (GVM) conditioned on policy trajectories. The GVM predicts token-level return-to-go values and serves as a stable guide for policy optimization, reducing both training complexity and instability. Importantly, our method requires no external information beyond the standard RLHF data used in PPO.
Once the GVM is trained, policy optimization proceeds via a standard RL objective, such as a PPO-like update, without requiring further updates to the value model. This design reduces both training complexity and instability while maintaining fine-grained reward supervision.

We provide theoretical justification for this approach by proving that, under the constraint of no new reward feedback, pretraining a reward model and a global value model are functionally interchangeable. Our experiments across multiple benchmarks (MT-Bench~\cite{zheng2023judging}, Alpaca-Eval~\cite{dubois2024length}, Arena-Hard~\cite{li2024crowdsourced}, etc.) demonstrate that the proposed method achieves performance comparable to state-of-the-art RLHF methods while reducing training time and GPU usage by 40\% and 35\%, respectively. These results highlight the scalability and stability of our approach in large-scale LLM fine-tuning.

% This work offers a resource-efficient solution to critical bottlenecks in RLHF, providing both theoretical insights and practical improvements to current LLM alignment pipelines.


\section{Related Work}
\label{related work}


\textbf{Reinforcement Learning in Language Model Optimization.}
Reinforcement learning has emerged as a prevalent method for fine-tuning large language models (LLMs), with Proximal Policy Optimization~\cite{schulman2017proximal} and its variations~\cite{ramamurthy2022reinforcement,wu2023pairwise} being the most prevalent methods. These techniques largely adhere to the actor-critic paradigm~\cite{sutton2018reinforcement}, This approach alternates between training a value estimator for the current policy and leveraging it to enhance policy performance. This bilevel process may result in a suboptimal policy, as demonstrated by empirical studies~\cite{gao2023scaling}. Moreover, the alternating optimization of the policy and critic models, along with the use of rewards provided by the reward model as environmental feedback, necessitates loading four models (including the reference model) simultaneously during training. This significantly increases training complexity and computational resource consumption~\cite{yao2023deepspeed,hu2024openrlhf}.

\textbf{Training Efficiency.} Many recent studies have sought to mitigate the computational complexity and resource consumption of the reinforcement learning (RL) step in RLHF. Methods such as DPO~\cite{rafailov2024direct} and its variants~\cite{meng2024simpo,ethayarajh2024kto,hong2024orpo} bypass reward modeling and the actor-critic learning framework by directly learning from preferences. However, existing research indicates that due to their offline nature, these approaches exhibit a performance gap compared to online RL~\cite{xu2024dpo}. Some recent works have proposed a reward-only approach to reduce the training cost of the RL phase~\cite{li2023remax,gunter2024apple,shao2024deepseekmath,ahmadian2024back}. However, this method lacks value estimation and assigns the same reward score to each token, leading to high variance and instability during training~\cite{hu2025reinforce++}. Unlike these approaches, our method pre-trains a global value model (GVM) and leverages it to guide RL training, providing token-level supervision signals. This not only reduces training resource consumption but also stabilizes the training process, achieving performance comparable to the original PPO.

\textbf{Value-base Inference.} 
Some recent works have attempted to learn a value function and use it to guide the decoding phase of LLMs, thereby bypassing the RL optimization stage~\cite{han2024value,kong2024aligning,mao2024don}. However, this approach significantly increases inference complexity and raises inference costs. In contrast, we leverage the learned value model to guide RL training, where the pre-trained value model helps the policy model converge more stably~\cite{noukhovitch2024language}.


\section{Method}
\label{sec:method}

We propose DVPO for RLHF in which a GVM is trained once and then fixed to guide policy updates. This approach removes the need for joint policy--value training and mitigates the associated computational overhead and instability. As shown in Figure~\ref{fig:overview}, our method comprises two primary stages: 
(1)~\textbf{Train GVM:} Use offline trajectories (states, actions, returns, and policy-specific data) to learn a policy-conditioned action-value function $Q_\phi$.
(2)~\textbf{Decoupled Value Policy Optimization:} Freeze $Q_\phi$ and optimize a policy using a standard RL objective (e.g., PPO), taking advantage estimates from the fixed GVM.

In what follows, we first define our setting (\S\ref{sec:mdp}), then introduce the GVM and its training procedure (\S\ref{sec:gvm-train}), describe how we fix the GVM for stable policy optimization (\S\ref{sec:gvm-fixed}), and finally present a theoretical analysis showing that \textbf{pretraining a reward model} and \textbf{pretraining a global value model} yield equivalent information when no new ground-truth rewards are available (\S\ref{sec:equivalence}).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{Figures/dvpo_compare_cropped.pdf}
  \caption{Overview of Decoupled Value Policy Optimization (DVPO) and PPO in RLHF. DVPO eliminates the need for a reward model and decouples policy and value learning during policy optimization. In contrast, PPO requires training a reward model before policy optimization. DVPO instead trains a global value model using the same offline data as the reward model. During policy training, no additional ground-truth rewards are obtained.}
  \label{fig:overview}
  % \vspace{-5mm}
\end{figure}


\subsection{Problem Setting}
\label{sec:mdp}

% We consider the sequence generation task in NLP as a Markov Decision Process (MDP) .we considering that the response consists of $T$ tokens ${y} = y^{<T+1}\coloneqq [y^1, y^2, ..., y^T]$, where $y^t\in \mathcal{Y}$, and $\mathcal{Y}$ represents the alphabet (vocabulary). Additionally, we assume $y^{<1}=[\ ]$.
%  Given a prompt ${x}$ and the first $t-1$ tokens $y^{<t}$ of the response ${y}$, the LM predicts the probability distribution of the next token $\pi_{\theta}(\cdot|[{x}, y^{<t}])$. 

% When modeling text generation as a Markov decision process \cite{puterman2014markov}, a state is a combination of the prompt and the generated response up to the current step, denoted as $s_t = [{x}, y^{<t}]$. An action corresponds to the next generated token, denoted as $a_t = y^t$, and the token-wise value is defined as $G_t := R(s_t, a_t) = R([{x}, y^{<t}], y^t)$. 

% $r(s,a)$ is the (unknown) ground-truth reward function, and $\gamma \in [0,1]$ is a discount factor. In RLHF for LLMs, the existing context is treated as the state $\mathcal{S}$, while the autoregressive generation of the next token is considered the model's action $\mathcal{A}$. The process terminates upon generating the EOS token, and a reward model assigns a reward score $r$. New environment rewards $r(\cdot)$ are typically unavailable during policy training. Instead, we rely on \emph{offline} data consisting of state--action trajectories, their associated returns, and references to a policy representation (e.g., Q--A pairs). Let $\mathcal{D}$ denote this offline dataset:
% \[
% \mathcal{D} = \bigl\{(\tau_i, s_i, a_i, G_i)\bigr\}_{i=1}^N,
% \]
% where $\tau_i$ indicates the $i$-th policy 's question-answer pair (representing policy behavior), $(s^i_t, a^i_t)$ is a another state--action pair sampled from that policy, and $G^i_t$ is the return or partial return obtained from offline feedback or a fixed reward model. Note that $\tau_i$ and $(s^i_t, a^i_t)$ can be from the policy but different or same Q-A paire. We assume no additional ground-truth rewards will be gathered.
We model the sequence generation task in NLP as a Markov Decision Process (MDP). The response consists of \(T\) tokens, denoted by \(y = y^{<T+1} \coloneqq [y^1, y^2, \dots, y^T]\), where \(y^t \in \mathcal{Y}\) and \(\mathcal{Y}\) is the vocabulary. We assume \(y^{<1} = [\ ]\), indicating an empty prefix. Given a prompt \(x\) and the first \(t-1\) tokens \(y^{<t}\), the language model (LM) predicts the probability distribution for the next token as:
\[
\pi_{\theta}(\cdot \mid [x, y^{<t}]).
\]

In this MDP formulation, the state is defined as \(s_t = [x, y^{<t}]\), representing the prompt (i.e., $x$) and the generated response up to step \(t\). The action is the next generated token \(a_t = y^t\). The ground-truth reward at the sentence level, denoted by \(r(x, y)\), is provided by human feedback. 
To transform the sentence-level reward into token-level rewards \(r(s_t, a_t)\), we apply a simplified form of temporal difference (TD) learning. Specifically, we assign a reward of 0 to each intermediate step and use the final step's reward as the sentence-level reward value. 


By this definition, the cumulative return from step \(t\) onwards is expressed as:
$G_t = \sum_{k=t}^{T} \gamma^{k-t} r(s_k, a_k), $
which simplifies to \(G_t = \gamma^{T-t} r(x, y)\) when all intermediate rewards are zero.


In RLHF, we rely on offline data \(\mathcal{D}\), which contains state--action trajectories, returns, and policy behaviors. The dataset is defined as:
\[
\mathcal{D} = \bigl\{(\tau_i, s^i_t, a^i_t, G^i_t)\bigr\}_{i=1}^N,
\]

where \(\tau_i\) represents the \(i\)-th policy behavior, typically expressed as a sequence of question--answer pairs, \((s^i_t, a^i_t)\) is a state--action pair sampled under that policy. Note that \(\tau_i\) and \((s^i_t, a^i_t)\) may refer to different or identical question--answer pairs under the same policy. No additional ground-truth rewards are collected during training.




\subsection{Training a Global Value Model (GVM)}
\label{sec:gvm-train}

Our key insight is to learn a \emph{policy-conditioned} action-value function $Q_\phi(\tau, s, a)$ from offline data, where $\tau$ represents a sampled trajectory capturing \emph{how the policy behaves} in unrelated contexts. This conditioning approximates how well a particular policy—embodied by $\tau$—would perform when taking action $a$ in state $s$.

\paragraph{Policy Conditioning via Trajectories.}
Traditional actor--critic methods require online adaptation of the value function to the actor's evolving behavior. In contrast, we aim for a single, \emph{global} $Q_\phi$ that generalizes across different policies, thus avoiding iterative re-learning. Instead of conditioning on explicit policy parameters, we leverage \emph{trajectories} $\tau$ randomly sampled from the policy in question. Each $\tau$ is a sequence of question--answer pairs (in LLM tasks) or other interactions that reveal distinct policy traits (e.g., stylistic tendencies, correctness, domain expertise).

Formally, we parametrize:
\[
Q_\phi(\tau, s, a) \;\approx\; \mathbb{E}\!\Bigl[
\sum_{t=0}^{\infty} \gamma^t \,r(s_{t},a_{t})
 \;\Bigm|\; s_0=s,\; a_0=a,\; \tau\Bigr],
\]
where $\tau$ implicitly determines which policy $\pi(\cdot\mid s)$ we are approximating. 

In practice, we train the global value model \(Q_\phi(\tau, s, a)\) using Temporal Difference (TD) learning. The target return \(G_i\) is estimated based on future rewards and value predictions. Specifically, \(G_i\) is computed as:
\[
G_t = r(s_t, a_t) + \gamma Q_\phi(\tau, s_{t+1}, a_{t+1}),
\]
where \(r(s_t, a_t)\) is the reward for taking action \(a_t\) in state \(s_t\), and \(Q_\phi(\tau, s_{t+1}, a_{t+1})\) is the predicted value of the next state-action pair.

The training objective for the global value model is to minimize the TD loss:
\begin{equation}
\label{eq:gvm-loss}
\begin{split}
\mathcal{L}_{\text{GVM}}(\phi) \;=\; \mathbb{E}_{(\tau_t, s_t, a_t, r_t, s_{t+1}, a_{t+1})\in \mathcal{D}}
\bigl[
\bigl(r_t +\\
\gamma Q_\phi(\tau, s_{t+1}, a_{t+1}) - Q_\phi(\tau, s_t, a_t)\bigr)^2
\bigr],
\end{split}
\end{equation}

This TD-based loss drives \(Q_\phi\) to iteratively adjust its estimates of the expected return, ensuring that the value function accurately reflects both immediate and future rewards. This approach is particularly suitable for offline RLHF scenarios, as it can handle sequences with deterministic state transitions.






\paragraph{Batch Normalization for Value Imbalance.}
\label{sec:normalization}
Offline RLHF data may exhibit skewed returns: for instance, predominantly positive if most responses are high-quality. This can push $Q_\phi$ to extreme positive values and slow convergence. To stabilize training, we adopt \emph{batch normalization} on the predicted values. For a mini-batch of size $B$,
\begin{equation}
\begin{split}
  \mu_B \;=\; \frac{1}{B}\sum_{t=1}^B Q_\phi(\tau, s_t, a_t),  
  \quad 
  \\
  \sigma_B^2 \;=\; \frac{1}{B}\sum_{i=1}^B \bigl(Q_\phi(\tau, s_t, a_t) - \mu_B\bigr)^2,
\end{split}
\end{equation}
we then normalize each output:
\[
  \widetilde{Q}_\phi(\tau, s_t, a_t) 
  \;=\;
  \frac{Q_\phi(\tau, s_t, a_t) - \mu_B}{\sigma_B + \epsilon}.
\]
This procedure (with small $\epsilon$ for numerical stability) prevents extreme values from dominating training and improves gradient flow.

\subsection{Decoupled-Value Policy Optimization}
\label{sec:gvm-fixed}

In traditional actor--critic methods, both the policy (actor) and value (critic) are trained simultaneously, which introduces instability due to their interdependence. This issue is exacerbated in offline RLHF settings, where no new environment rewards can be collected to correct misaligned updates. To address these challenges, we propose a decoupled-value policy optimization approach: the global value model \(Q_\phi\) is trained and fixed before policy optimization, decoupling the learning dynamics of the actor and critic.

\paragraph{Policy Optimization Objective.}  
Once the global value model \(Q_\phi\) converges, we \emph{fix} its parameters and use it to guide policy updates. Let \(\pi_\theta(a \mid s)\) be the policy to be optimized. We adopt a clipped PPO objective, which stabilizes policy updates by limiting the impact of large policy changes:
\begin{equation}
\label{eq:ppo-loss}
\mathcal{L}_{\text{PPO}}(\theta) 
\;=\;
\mathbb{E}\Bigl[
\min\bigl(r_t(\theta) \hat{A}_t, 
\operatorname{clip}\bigl(r_t(\theta), 1-\epsilon, 1+\epsilon\bigr) \hat{A}_t\bigr)
\Bigr],
\end{equation}
where \(r_t(\theta) = \tfrac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}\) is the importance sampling ratio, and \(\epsilon\) is a clipping parameter to prevent overly large updates.

The advantage function measures the relative quality of an action compared to the expected value of the state. We define the advantage using the fixed, normalized value model:
\[
\hat{A}_t = \widetilde{Q}_\phi(\tau, s_t, a_t),
\]
where \(\widetilde{Q}_\phi\) is the batch-normalized value estimate computed during the GVM training stage (see \S\ref{sec:gvm-train}). In offline RLHF, this static advantage definition provides a reliable signal for policy improvement without requiring dynamic value updates.

Our decoupled-value approach offers two key advantages:
(1)~\textbf{Stability via Fixed Value:} By fixing \(Q_\phi\), we eliminate the “moving target” problem inherent in actor--critic methods, leading to more stable and predictable policy updates. (2)~\textbf{Efficient Offline Use:} Since no additional environment rewards can be collected, the static \(Q_\phi\) provides all necessary supervisory information, enabling efficient reuse of the offline dataset for policy optimization.


% This framework leverages the global value model’s pretraining to decouple the policy learning process, resulting in a more stable and resource-efficient training pipeline for RLHF tasks in LLMs.




\subsection{Theoretical Analysis: Equivalence of Pretrained Reward and Global Value Models}
\label{sec:equivalence}

In an offline RLHF scenario where \emph{no new ground-truth rewards} are available, pretraining either a \emph{reward model} or a \emph{GVM} provides essentially the same supervisory information for policy optimization. Below, we present a rigorous proof of this equivalence and discuss its implications.

\subsubsection{Preliminaries}
Assume we have two possible pretrained models from the same offline dataset. \textbf{Reward Model}, $R_\phi(s,a)$, approximates an unknown ground-truth reward $r(s,a)$. \textbf{Global Value Model}, \(Q_\psi(s, a)\), approximates the action-value function
for a any given policy \(\pi\). Here, \(Q_\psi(s, a)\) is a global value model conditioned on the trajectory \(\tau_\pi\), state \(s\), and action \(a\). However, to simplify the proof, we remove the explicit dependence on \(\tau_\pi\) by merging it into the definition of the state. 


Because no new rewards are collected during policy training, neither model can be updated to reflect policy changes. Hence, \emph{whichever model is used, the agent has a fixed signal} to optimize against. We formalize this equivalence next.


\begin{theorem}[Equivalence of Pretrained Reward and GVM]
\label{thm:equivalence}
Suppose:
\begin{enumerate}[leftmargin=0.7cm]
    \item $R_\phi(s,a)$ is a pretrained reward model whose approximation error to the true reward $r(s,a)$ is at most $\epsilon_R$ in the offline data distribution.
    \item $Q_\psi(s,a)$ is a pretrained global value model whose approximation error to $Q^\pi(s,a)$ is at most $\epsilon_Q$ in the same data distribution.
    \item No further ground-truth rewards can be obtained to update either model.
\end{enumerate}
Then any policy gradient method that employs either $R_\phi$ or $Q_\psi$ as its supervisory signal will yield policy updates differing by at most a constant factor dependent on $(\epsilon_R,\epsilon_Q)$. As $\epsilon_R,\epsilon_Q \to 0$, the two approaches become \emph{equivalent} in guiding policy optimization.
\end{theorem}

\begin{proof}
\textbf{Deriving a Value Function from $R_\phi$.}  
Given $R_\phi(s,a)$ and a fixed policy $\pi$, one can solve the Bellman equation offline to obtain $\widetilde{Q}_\phi(s,a;\pi)$:
\[
\widetilde{Q}_\phi(s,a;\pi)
=
R_\phi(s,a)
+
\gamma
\mathbb{E}_{s'\sim P}
\Bigl[
    \mathbb{E}_{a'\sim \pi(\cdot\mid s')}[\widetilde{Q}_\phi(s',a';\pi)]
\Bigr].
\]
Under standard coverage assumptions in offline RL, $\widetilde{Q}_\phi$ converges to an approximation of $Q^\pi$, up to an error that depends on $\epsilon_R$ and the data distribution.

\textbf{Deriving a Reward Signal from $Q_\psi$.}  
Conversely, if $Q_\psi(s,a)$ approximates $Q^\pi(s,a)$ for a policy $\pi$, the (unknown) reward can be approximated by rearranging the Bellman equation:
\[
r(s,a)
=
Q^\pi(s,a)
-
\gamma
\mathbb{E}_{s'\sim P,a'\sim\pi}
\bigl[
Q^\pi(s',a')
\bigr].
\]
Replacing $Q^\pi$ with $Q_\psi$ yields
\[
\widetilde{R}_\psi(s,a)
=
Q_\psi(s,a)
-
\gamma
\mathbb{E}_{s'\sim P,a'\sim\pi}
\bigl[
Q_\psi(s',a')
\bigr].
\]
Hence, $Q_\psi$ can serve as a surrogate reward model for policy updates.

\textbf{Equivalence in Policy Gradient.}  
Let $\nabla_\theta \mathcal{J}_R$ be the policy gradient update using $R_\phi$ (converted to a value function $\widetilde{Q}_\phi$), and $\nabla_\theta \mathcal{J}_Q$ the update using $Q_\psi$ directly. By standard policy gradient theorems, 
\[
\|\nabla_\theta \mathcal{J}_R - \nabla_\theta \mathcal{J}_Q\|
\le
\kappa(\epsilon_R,\epsilon_Q),
\]
for some bounded function $\kappa$, given that both methods rely on fixed models and the policy remains within the offline dataset’s support. As $\epsilon_R,\epsilon_Q\to 0$, the updates converge to the same directions in parameter space, leading to the same optimal policy.

\textbf{No New Ground-Truth Rewards.}  
Since no fresh environment rewards $r(s,a)$ can be obtained, neither $R_\phi$ nor $Q_\psi$ can be refined once training begins. Thus, each model acts as a \emph{static} supervisor. If both models are sufficiently accurate within the relevant state--action distribution, their induced policy gradients differ only by a bounded approximation error.

The above proof confirm that a pretrained reward model $R_\phi$ or a pretrained global value model $Q_\psi$ provides equivalent guidance for offline policy optimization, proving Theorem \ref{thm:equivalence}.
\end{proof}
% This result establishes that in an offline RLHF setting, \textbf{pretraining either a reward or a value model is purely a design choice} if additional reward signals cannot be collected. In particular, it justifies our use of a fixed global value model (instead of a reward model) for guiding policy updates, offering simpler training dynamics without sacrificing optimality guarantees under the given assumptions.



% \section{Method}

% In this section, we describe our framework for training a \emph{global value model} (GVM) from offline data and subsequently using it to optimize a policy in an RLHF context. Unlike traditional approaches that jointly update value and policy networks, we propose to learn the GVM once and then \emph{fix} it, thus reducing both computational cost and training instability. Our method comprises two primary stages: (1) \textbf{Train GVM}: Use offline data (trajectories, states, actions, and returns) to learn a policy-conditioned global value function. (2)\textbf{Policy Optimization}: Freeze the GVM and optimize a policy via a standard RL objective (e.g., PPO-like update).

% \textcolor{red}{maybe rethink to restructure the method section? since the last parapraph mentioned that there are two primary states, and then, here you propose four design aspects and then arrange the method with the design aspects? This logic is strange since there is no corresponding challenges or requirements that can be 1 to 1 mapping to the design aspect. and these design aspects to me, jump from somewhere abruptly.}
% Below, we detail each step, focusing on four key design aspects: (i) conditioning on policy via trajectories, (ii) a human-inspired analogy for why an LLM can learn a GVM, (iii) handling value imbalance through batch normalization, and (iv) optimizing the policy with a fixed GVM.

% \subsection{Conditioning on the Policy via Trajectories}
% \label{sec:policy-cond}

% A central challenge in value-based methods is ensuring that the value function reflects the \emph{policy} being evaluated. In traditional actor--critic RL, this is done by simultaneously training policy and value networks. In our approach, however, we want a \emph{global} value model that generalizes across potential policies. Since we may not have direct access to policy parameters, we condition on \emph{trajectories} (e.g., question--answer pairs in an LLM context) that implicitly represent the policy’s behavior.

% \paragraph{Offline Equivalence: Reward Model vs.\ Value Model.}

% When we \emph{cannot} obtain new ground-truth rewards during policy training, pretraining a \textbf{reward model} or a \textbf{value model} ultimately provides equivalent information for the downstream RL optimization. Intuitively, a reward model \(R(s,a)\) can be used to derive the long-term return \(V(s,a)\) via Bellman updates, and conversely, a value model \(V(s,a)\) can yield an approximate reward signal by differencing successive state-action returns. Consequently, in the offline setting where no fresh feedback is available, \textbf{pretraining a reward model is effectively the same as pretraining a value model} for guiding policy improvements, as both encapsulate the environment’s reward structure up to approximation error.

% \subsection{Human-Inspired Analogy: LLM as a Global Value Model}
% \label{sec:human-analogy}

% Consider a human evaluator tasked with judging a policy's quality. If the human is shown examples of how the policy behaves (its “trajectory”), they can infer whether taking a particular action in a given state will likely lead to good or bad outcomes. Similarly, a large language model (LLM) can be trained to perform this evaluative role, effectively serving as a policy-conditioned global value model.

% \paragraph{Real Example in NLP Tasks.}  
% Imagine we are training an LLM to evaluate the responses generated by another policy (e.g., another LLM trained for dialogue). The state \(s\) represents the current query posed by the user, for example:
% \[
% \text{Query: "What are some healthy lunch ideas?"}
% \]
% The action \(a\) corresponds to a candidate response generated by the policy being evaluated, for example:
% \[
% \text{Response: "Grilled chicken salad, veggie wraps, or quinoa bowls are great options."}
% \]

% The trajectory \(\tau\) consists of \textbf{randomly sampled Q--A pairs from the policy}, providing a snapshot of the policy’s behavior in unrelated contexts. For example:
% \begin{itemize}
%     \item \textbf{Trajectory Q--A Pair 1:}  \\
%     \text{Query: "What is the best way to learn programming?"}  
%     \text{Response: "Start with Python; it’s beginner-friendly and versatile."}
%     \item \textbf{Trajectory Q--A Pair 2:}  \\
%     \text{Query: "How do I bake a chocolate cake?"}  
%     \text{Response: "Use a simple recipe with cocoa powder, sugar, eggs, and flour."}
% \end{itemize}

% The sampled trajectory \(\tau\) serves as a representation of the policy’s overall traits, such as its style of communication (e.g., concise and practical), tone (e.g., friendly or formal), and general quality of answers (e.g., correctness and helpfulness). When evaluating the action \(a\) in state \(s\), the LLM can use \(\tau\) to infer whether the proposed response aligns with the expected quality and behavior of the policy.

% \paragraph{Why LLMs Can Learn It.}  
% This process is analogous to a human reviewer who is shown examples of how the policy behaves in unrelated contexts before being asked to judge a specific response. For example, if the human sees that the policy consistently provides correct and well-structured answers in other conversations (as represented by \(\tau\)), they might infer that the candidate action \(a\) in the current query is likely to be of high quality. Conversely, if the trajectory reveals frequent errors or irrelevant answers, the predicted value for \(a\) may decrease.

% Similarly, the LLM uses its capacity to process sequential and contextual information to learn the mapping from \((\tau, s, a)\) to an expected reward. During training, the LLM learns to associate the policy’s behavioral patterns (captured in \(\tau\)) with the expected outcomes of specific actions \(a\) in states \(s\). By doing so, the LLM functions as a global value model, capable of evaluating state-action pairs based on policy-specific context.





% \subsection{Value Batch Normalization}
% \label{sec:normalization}
% \textcolor{red}{do we have ablation of this batch normalization? or this should be implementation details or tricks? not that important in method section?}
% In RLHF-like data, returns can be skewed---for instance, consistently positive if most answers are high-quality, or consistently negative if the data includes primarily lower-quality outputs. Such imbalance in the offline dataset often hinders stable learning by pushing predicted values to extreme ranges.

% To address this, we adopt a \textbf{batch normalization} approach during training. Specifically, in each mini-batch of size \(B\), we compute: \textcolor{red}{the first time mentioned $V_\phi(\tau_i, s_i, a_i)$, need to bring this notation earlier. Additionally, $\tau_i, s_i, a_i$ together seems strange, as normally the Q value is $Q(s_i,a_i)$}
% \[
%   \mu_B = \frac{1}{B}\sum_{i=1}^B V_\phi(\tau_i, s_i, a_i),
%   \quad
%   \sigma_B^2 = \frac{1}{B}\sum_{i=1}^B \bigl(V_\phi(\tau_i, s_i, a_i) - \mu_B\bigr)^2,
% \]
% then normalize each predicted value:
% \[
%   \widetilde{V}_\phi(\tau_i, s_i, a_i) = \frac{V_\phi(\tau_i, s_i, a_i) - \mu_B}{\sigma_B + \epsilon},
% \]
% where \(\epsilon\) is a small constant for numerical stability. This mini-batch normalization ensures that extreme values do not dominate training, improving convergence and reducing gradient variance. The normalized values are particularly useful when we later compute policy updates that depend on these predictions.

% \subsection{Policy Optimization with a Fixed GVM}
% \label{sec:policy-optim}

% Once the global value model \(V_\phi(\tau, s, a)\) converges, we \textbf{fix} its parameters and use it to guide policy optimization. Let \(\pi_\theta(a \mid s)\) be our policy with parameters \(\theta\).

% We optimize \(\pi_\theta\) via a standard RL objective, such as the clipped PPO formulation:
% \[
%   \mathcal{L}_{\text{PPO}}(\theta)
%   \;=\; 
%   \mathbb{E}\bigl[
%     \min \bigl(r_t(\theta)\, \hat{A}_t,\;
%     \operatorname{clip}(r_t(\theta),\,1-\epsilon,\,1+\epsilon)\,\hat{A}_t\bigr)
%   \bigr],
% \]
% where \(r_t(\theta)=\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)}\) and \(\epsilon\) is a hyperparameter. We replace the typical learned critic with our fixed, normalized \(V_\phi\). Specifically, the advantage \(\hat{A}_t\) \textcolor{red}{what does the hat notation mean?} can be computed using:
% \[
%   \hat{A}_t \;=\;
%   \widetilde{V}_\phi(\tau, s_t, a_t) 
%   \;-\;
%   \widetilde{V}_\phi(\tau, s_t),
% \]
% where \(\widetilde{V}_\phi(\tau, s_t,a_t)\) \textcolor{red}{what does the tilde mean? also normally $A_t=Q(s_t,a_t)-V(s_t)$, we need to mention how we get this equation from the traditional advantage calculation equation.}is the mini-batch normalized value estimate from the GVM, and \(\widetilde{V}_\phi(\tau, s_t)\) is a state-value baseline (obtained by averaging action-values under \(\pi_{\theta_\text{old}}\) or simply plugging in a placeholder action). 

% Notably, the policy \(\pi_\theta\) is \emph{actively trained} against these fixed value estimates. Although \(V_\phi\) \textcolor{red}{should also mention the first place when $V_{\phi}$ is first shown, and say $\phi$ is the parameter of the value model. }never changes, \(\theta\) is iteratively updated via gradient-based optimization. This decoupling offers two key benefits: \textcolor{red}{feel like we need some citation on these two challenges that we have solved. and also talk more on the benefits. current one sentence for each benefit is limited and the reviewers might not have good impression and why? also maybe connect these two points with the experiments afterwards, since we discuss the training stability of training the policy with fixed value model. as for the 'reuse of offline data', do we have experiments later to support it?}(1) \textbf{Reduced Instability:} We avoid the moving-target problem typical of actor--critic methods. (2) \textbf{Efficient Reuse of Offline Data:} Since no new ground-truth rewards exist, the fixed GVM provides all the information needed for stable policy improvements.



% By conditioning on policy trajectories, normalizing skewed values, and fixing the GVM prior to policy learning, our framework yields a stable, efficient alternative to traditional RLHF pipelines that require costly joint training of value and policy models---particularly in scenarios where no new ground-truth rewards are obtainable.


% \section{Global Value Model}

% In this section, we present our \textbf{Global Value Model (GVM)} framework for efficiently training a policy in the RLHF setting, where no new ground-truth rewards are available during policy optimization. Our method proceeds in two main stages: (1) \emph{Train} the GVM from offline trajectories, and (2) \emph{Fix} the GVM for stable policy optimization. Below, we describe the motivation behind learning a global value model, our approach to conditioning on the policy, the TD-based training objective, normalization strategies, and the theoretical underpinnings that make this method feasible.

% \subsection{Overview and Motivation}
% \label{sec:overview}

% Traditional RLHF methods train a policy and a value model jointly using a pretrained reward model, but this is computationally costly and can be unstable, especially when the reward signal cannot be refined with new ground-truth data during policy learning. Instead, we propose to learn a \emph{global value model} (GVM) \textbf{once} from offline data, then \textbf{fix} it to guide policy optimization. 


% \subsection{Global Value Model}
% \label{sec:gvm}

% \subsubsection{Conditioning on the Policy via Trajectories}
% \label{sec:policy-cond}

% A key requirement in value-based methods is that the learned value function \emph{reflects} the policy being evaluated. In a typical actor--critic setting, this coupling is achieved by simultaneously training policy and value networks. In our setup, we seek a \emph{global} value model that generalizes across policies. The simplest approach is to include a \textbf{policy representation} in the GVM’s input. Since we may not have direct access to the policy parameters, we instead use \emph{trajectories} from that policy---specifically, the question and answer (Q--A) sequences---to characterize the policy’s behavior.

% \paragraph{Human-Inspired Analogy.}
% Consider an expert human who is shown an example of a policy’s behavior before being asked: ``What is the expected return for taking action $a$ in state $s$ under this policy?'' The human’s judgment would depend both on the immediate state--action pair and on how this policy generally behaves. Similarly, the GVM conditions on Q--A sequences from the same policy to infer its long-term returns.

% \subsubsection{TD($\lambda$) for Value Estimation}
% \label{sec:td}

% \paragraph{Offline Dataset.}
% We collect a dataset
% \[
%   \mathcal{D} \;=\; \Bigl\{ \bigl(\tau_i,\; s^i,\; a^i,\; r^i\bigr)\Bigr\}_{i=1}^N,
% \]
% where each $\tau_i$ denotes a \emph{policy-indicative} trajectory (Q--A history), $s^i$ is a state (e.g., the current query or partial context), $a^i$ is the action taken (e.g., the next tokens in the response), and $r^i$ is the associated return or reward (possibly final). Here, $\tau_i$ provides the policy context needed to evaluate $\bigl(s^i, a^i\bigr)$ accurately.

% \paragraph{GVM Architecture.}
% We define $V_\phi(\tau, s, a)$ as our global value model. It outputs an estimate of the expected long-term return from taking action $a$ in state $s$, under the policy represented by $\tau$. We learn $V_\phi$ via temporal-difference learning, specifically TD($\lambda$) \cite{sutton2018reinforcement, schulman2015high}. The core update can be written as:
% \[
%   \mathcal{L}(\phi) 
%   \;=\;
%   \mathbb{E}_{(\tau, s, a, r) \in \mathcal{D}}
%   \Bigl[
%     \bigl( 
%       G^\lambda_{t} - V_\phi(\tau,\; s,\; a) 
%     \bigr)^2
%   \Bigr],
% \]
% where $G^\lambda_{t}$ is the TD($\lambda$) return, defined as
% \[
%   G^\lambda_{t}
%   \;=\;
%   r_t + \gamma r_{t+1} + \dots + \gamma^{k} \, V_\phi(\tau,\; s_{t+k},\; a_{t+k}).
% \]
% Note that each mini-trajectory in $\mathcal{D}$ may consist of multiple steps. Here, $\tau$ remains unchanged when considering transitions within the same policy.

% Under appropriate coverage of state--action--policy triplets in $\mathcal{D}$, $V_\phi$ can learn to approximate the return function for a range of behaviors. Because Q--A pairs in $\tau$ encode policy characteristics, $V_\phi(\tau, s, a)$ learns the \emph{policy-conditioned} value. For new actions not explicitly in $\mathcal{D}$, a sufficiently expressive neural network can generalize based on similarities in states and partial trajectories.

% \subsubsection{Value Imbalance and Normalization}
% \label{sec:normalization}

% \paragraph{Value Imbalance.}
% In practice, we often encounter data where returns are skewed, \emph{e.g.,} mostly positive or mostly negative due to how human preference or reward models are distributed. This imbalance can degrade training by: (1) Pushing value estimates to extreme ranges, which slows convergence. (2) Causing optimization instabilities in policy updates if raw values are used directly.


% \paragraph{Normalization Strategy.}
% To address this, we introduce a simple normalization step on the predicted value:
% \[
%   \widetilde{V}_\phi(\tau, s, a)
%   \;=\;
%   \frac{\,V_\phi(\tau,\; s,\; a) - \mu\,}{\,\sigma\,},
% \]
% where $\mu$ and $\sigma$ are running estimates (e.g., from a replay buffer or across batches) of the mean and standard deviation of the value predictions. By centering around zero and scaling by standard deviation, we mitigate the dominance of extreme values and obtain more stable gradients when using $\widetilde{V}_\phi$ in policy optimization.

% \subsection{Policy Optimization with a Fixed GVM}
% \label{sec:policy-optim}

% After training converges, we \textbf{fix} $V_\phi$ and use it to guide policy updates. In effect, $V_\phi$ acts as a stable baseline, avoiding the co-adaptation pitfalls common in actor--critic.

% \paragraph{PPO-Like Objective.}
% Let $\pi_\theta$ be our policy. We optimize it with a PPO-style objective:
% \[
%   \mathcal{L}_{\text{PPO}}(\theta) 
%   \;=\;
%   \mathbb{E}_{s\sim\pi_{\theta_{\text{old}}}, a\sim\pi_{\theta_{\text{old}}}}
%   \bigl[
%     \min \Bigl( r_t(\theta)\hat{A}_t,\; 
%     \operatorname{clip}\bigl(r_t(\theta), 1-\epsilon, 1+\epsilon\bigr)\,\hat{A}_t\Bigr)
%   \bigr],
% \]
% where $r_t(\theta) = \frac{\pi_\theta(a \mid s)}{\pi_{\theta_{\text{old}}}(a \mid s)}$ and $\hat{A}_t$ is the advantage, computed using the \emph{fixed} normalized value:
% \[
%   \hat{A}_t = \widetilde{V}_\phi(\tau, s, a) - \widetilde{V}_\phi(\tau, s).
% \]
% No additional value-network updates are needed; all references to future return come from the pretrained $V_\phi$.

% \subsubsection{Summary of Method Steps}
% \begin{enumerate}[leftmargin=1cm]
% \item \textbf{Collect Offline Data:} Acquire a dataset $\mathcal{D}$ of policy trajectories $\tau_i$, states $s^i$, actions $a^i$, and returns $r^i$, akin to RLHF data.
% \item \textbf{Train Global Value Model:} 
%   \begin{itemize}[leftmargin=0.6cm]
%      \item Condition on $(\tau, s, a)$ and apply TD($\lambda$) to learn $V_\phi(\tau, s, a)$. 
%      \item Normalize predictions to stabilize training and policy updates.
%   \end{itemize}
% \item \textbf{Fix $V_\phi$:} Freeze the learned parameters to ensure stability.
% \item \textbf{Optimize Policy:} Use PPO-like updates where advantage calculations reference the fixed $V_\phi(\tau,s,a)$.
% \end{enumerate}
% \section{Equivalence of Pretrained Reward and Global Value Models in the Absence of Ground-Truth Rewards}
% \label{sec:theorem_equivalence}

% \textcolor{red}{not sure if it is a good idea to have independent section here to prove the equivalence of methods of pretrained RM and pretrained global value model. maybe merge it with the method section where you first claim that they are equivalent. }

% \textcolor{red}{also put some proof in appendix. and put the notations earlier before the method section as preliminary section.}
% In this section, we formally prove that \textit{pretraining a reward model} or \textit{pretraining a global value model} leads to an equivalent outcome for policy optimization when no additional ground-truth rewards are obtained during training. Our result leverages the Bellman equation to establish a bijection between a reward function and the corresponding value function, showing that both contain the same information required for policy improvement.

% \subsection{Problem Setting}

% We consider a Markov Decision Process (MDP)
% \[
% \mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \gamma),
% \]
% where $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of actions, $P(s' \mid s,a)$ is the transition distribution, $r(s,a)$ is the ground-truth reward function (unknown or not used during training), $\gamma \in [0,1]$ is the discount factor.

% A policy $\pi_\theta(a \mid s)$ induces a state--action occupancy distribution $\mu_{\pi_\theta}(s,a)$ over the MDP.

% \paragraph{Pretrained Models.}
% 1. A \textbf{reward model} $R_\phi(s,a)$ is an approximation of $r(s,a)$, trained offline from limited human feedback or a preference-labeled dataset. We assume no further updates to $R_\phi$ once pretraining is complete.

% 2. A \textbf{global value model} $V_\psi(s,a)$ approximates the action-value function \textcolor{red}{this is where I have previously mentioned in the equation of calculating advantage. the below content should be there, not here.}
% \[
% Q^\pi(s,a) \;=\; \mathbb{E}\Bigl[
%     \sum_{t=0}^\infty \gamma^t \,r(s_t,a_t)
%     \;\Bigm|\; 
%     s_0 = s,\; a_0 = a,\; \pi
% \Bigr],
% \]
% \textcolor{red}{it seems that it is not mentioned why a global value model can approximates the state-action value function here? }for a family of policies (or a representative policy set). As with $R_\phi$, no new ground-truth rewards become available during training of the policy.

% \subsection{Formal Statement of Equivalence}

% \textcolor{red}{the below sections seem gpt generated, need do reformat and also put some proof in appendix.}
% \begin{theorem}
% \label{thm:equivalence}
% \emph{(Equivalence of Pretrained Reward and Value Models)}  
% Let $\pi_\theta$ be a policy to be optimized in the MDP $\mathcal{M}$. Suppose:
% \begin{enumerate}
%     \item $R_\phi(s,a)$ is a pretrained reward model such that
%     \[
%     \bigl|\,R_\phi(s,a) - r(s,a)\bigr| \;\le\; \epsilon_R,
%     \quad \forall (s,a) \in \mathcal{S}\times\mathcal{A}.
%     \]
%     \item $V_\psi(s,a)$ is a pretrained global value model (action-value function) such that
%     \[
%     \bigl|\,V_\psi(s,a) - Q^\pi(s,a)\bigr| \;\le\; \epsilon_V,
%     \quad \forall (s,a,\pi) \text{ in-distribution.}
%     \]
%     \item No new ground-truth reward $r(s,a)$ is provided during policy learning.
% \end{enumerate}
% Then, for any policy gradient update that uses either $R_\phi$ (converted to a value or advantage function) or $V_\psi$ (used directly as a value function), the difference in the resulting policy updates is bounded by a constant depending on $\epsilon_R$ and $\epsilon_V$, and both approaches yield equivalent policy optimization in the limit $\epsilon_R,\epsilon_V \to 0$.
% \end{theorem}

% \begin{proof}
% \label{proof:equivalence}
% \textbf{Outline.} We show that a reward model $R_\phi$ can be converted into a value-based signal for policy updates, and vice versa, such that the induced policy gradient steps differ only by approximation errors.

% \paragraph{1. From Reward Model to Value-Based Updates.}
% Given $R_\phi(s,a)$, one can construct a surrogate \emph{policy-dependent} action-value function $\widetilde{Q}_\phi(s,a;\pi)$ via the Bellman equation:
% \[
% \widetilde{Q}_\phi(s,a;\pi)
% \;=\;
% R_\phi(s,a)
% \;+\;
% \gamma \,\mathbb{E}_{s'\sim P(s'|s,a)}\,
% \mathbb{E}_{a' \sim \pi(a'|s')}
% \bigl[
%     \widetilde{Q}_\phi(s',a';\pi)
% \bigr].
% \]
% In practice, this can be solved approximately with dynamic programming or iterative updates (similar to offline RL). Let $\widetilde{Q}_\phi \approx Q^\pi$ up to error $\epsilon_{R2Q}$ \textcolor{red}{what's this? need annotation explanation.}. A policy gradient method then uses
% \[
%     A^\pi_{\phi}(s,a)
%     \;=\;
%     \widetilde{Q}_\phi(s,a;\pi)
%     \;-\;
%     \sum_{a'\in\mathcal{A}} \pi(a' \mid s)\,\widetilde{Q}_\phi(s,a';\pi),
% \]
% as an advantage estimate.

% \paragraph{2. From Global Value Model to Instantaneous Reward.}
% If we already have $V_\psi(s,a) \approx Q^\pi(s,a)$ \textcolor{red}{no proof of this approximation?}, then the instantaneous reward can be derived by rearranging the Bellman equation for $Q^\pi$:
% \[
%     r(s,a) 
%     \;=\;
%     Q^\pi(s,a)
%     \;-\;
%     \gamma\,\mathbb{E}_{s'\sim P}\,
%     \mathbb{E}_{a'\sim \pi}\bigl[Q^\pi(s',a')\bigr].
% \]
% Substituting $V_\psi(s,a)$ for $Q^\pi(s,a)$ yields an approximate reward model
% \[
%     \widetilde{R}_\psi(s,a)
%     \;=\;
%     V_\psi(s,a)
%     \;-\;
%     \gamma \,\mathbb{E}_{s'\sim P}\,
%     \mathbb{E}_{a'\sim\pi}\bigl[V_\psi(s',a')\bigr].
% \]
% Denote the error in $\widetilde{R}_\psi$ as $\epsilon_{V2R}$ if $V_\psi$ is an imperfect approximation of $Q^\pi$.

% \paragraph{3. Policy Gradient Update Equivalence.}
% Let $\nabla_\theta \mathcal{J}_{R}$ be the policy gradient that uses $R_\phi(s,a)$ (converted to a value or advantage for the update), and let $\nabla_\theta \mathcal{J}_{V}$ be the gradient using $V_\psi(s,a)$ directly. We must show:
% \[
%     \|\nabla_\theta \mathcal{J}_{R} - \nabla_\theta \mathcal{J}_{V}\|
%     \;\le\; \kappa(\epsilon_R, \epsilon_V, \epsilon_{R2Q}, \epsilon_{V2R})
% \]
% for some bounded function $\kappa$. 

% - When $R_\phi(s,a)$ is perfectly aligned with $r(s,a)$ and $\widetilde{Q}_\phi$ solves the Bellman equation exactly, then $\nabla_\theta \mathcal{J}_{R}$ converges to the same gradient direction as if using the true $Q^\pi$.  
% - Similarly, when $V_\psi(s,a)$ is an exact representation of $Q^\pi$, deriving $\widetilde{R}_\psi(s,a)$ recovers the same advantage estimates as the environment’s true reward.

% Hence, in the limit $\epsilon_R,\epsilon_V,\epsilon_{R2Q},\epsilon_{V2R} \to 0$, both forms of policy gradient yield identical updates.

% \paragraph{4. No Additional Ground-Truth Rewards.}
% Because no new samples of $r(s,a)$ are collected, either approach (reward-only or value-only) relies on offline approximation. The offline setting fixes $R_\phi$ and $V_\psi$ at pretraining time. Provided the policy $\pi_\theta$ remains within the support of the offline data, both $\widetilde{Q}_\phi$ and $V_\psi$ remain accurate enough for stable policy improvement:
% \[
%     \Bigl\| \nabla_\theta \mathcal{J}_{R}(\pi_\theta) - \nabla_\theta \mathcal{J}_{V}(\pi_\theta) \Bigr\|
%     \;\;\le\;\; \text{const}.
% \]
% Thus, the lack of new ground-truth rewards does not favor one pretraining approach over the other.

% \paragraph{Conclusion.}
% By demonstrating how a pretrained reward model can be converted to a value-like signal for advantage computation (and vice versa), we conclude that \textbf{pretraining a reward model} vs.\ \textbf{pretraining a global value model} produces \emph{equivalent} downstream RL optimization—up to bounded approximation errors. This completes the proof.

% \end{proof}

% \paragraph{Implication.}
% Theorem~\ref{thm:equivalence} implies that, in an offline RLHF or similar setting where:
% \begin{enumerate}
%     \item We cannot acquire new reward annotations or environment interactions, and
%     \item We have either a reliable pretrained reward model \emph{or} a reliable pretrained value model,
% \end{enumerate}
% the choice between using $R_\phi$ or $V_\psi$ is purely practical (e.g., ease of training or interpretability), rather than theoretical. Both contain sufficient information for consistent policy updates, as no additional ground-truth signal will break their functional equivalence.


% \subsection{Theoretical Analysis}
% \label{sec:theory}

% In this section, we provide a more rigorous mathematical analysis of why a fixed \emph{global value model} (GVM) can be used to guide policy optimization in the absence of new ground-truth rewards. We first formalize the setting, then present assumptions under which the GVM remains valid for improving a policy, and finally establish performance guarantees analogous to offline RL.

% \subsubsection{Problem Formalization}

% \paragraph{Environment and Policy.}
% Consider a Markov Decision Process (MDP) defined by the tuple
% \[
% \mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \gamma),
% \]
% where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $P(s' \mid s,a)$ is the transition distribution, $r(s,a)$ is the reward function, and $\gamma \in [0,1]$ is the discount factor. A policy $\pi(a\mid s)$ defines a distribution over actions for each state.

% \paragraph{Policy-Conditioned Value.}
% A distinguishing feature of our \emph{global value model} is that it conditions on a representation of the policy, denoted $\tau$, in addition to $(s,a)$. Formally, the GVM is
% \[
%   V_\phi(\tau, s, a) \approx \mathbb{E}
%   \bigl[
%     \textstyle \sum_{t=0}^{\infty}\,\gamma^t r_{t} 
%     \;\bigm|\;
%     \tau,\; s_0 = s,\; a_0 = a
%   \bigr],
% \]
% where $\tau$ is a proxy for the policy’s behavior (e.g., a trajectory of Q--A pairs). One way to think of $\tau$ is as an encoding of the distribution $\pi(a\mid s)$ itself, or a partial roll-out from that policy.

% \paragraph{Offline Dataset and No New Rewards.}
% We assume access to an offline dataset 
% \[
% \mathcal{D} \;=\;\bigl\{\,(\tau_i,\; s^i,\; a^i,\; R^i)\bigr\}_{i=1}^N,
% \]
% where each sample includes a \emph{policy signature} $\tau_i$, a state $s^i$, an action $a^i$, and a cumulative return $R^i$ (or partial returns used in TD). Crucially, once $\mathcal{D}$ is collected, no additional (ground-truth) reward signals are available. Our goal is to learn $V_\phi(\tau, s, a)$ \emph{once} from $\mathcal{D}$, then \emph{fix} $\phi$ for subsequent policy training.

% \subsubsection{Assumptions}

% \paragraph{(A1) Offline Coverage.}
% Define $\mu_{\mathcal{D}}(s,a,\tau)$ as the distribution of $(s,a,\tau)$ in the offline dataset, and let $\mu_{\pi_\theta}(s,a,\tau)$ be the distribution induced by a new policy $\pi_\theta$. We assume:
% \[
%   \bigl\{(s,a,\tau)\,\mid\, \mu_{\pi_\theta}(s,a,\tau) > 0 \bigr\}
%   \;\;\subseteq\;\;
%   \bigl\{(s,a,\tau)\,\mid\, \mu_{\mathcal{D}}(s,a,\tau) > 0 \bigr\}.
% \]
% In other words, states, actions, and policy representations visited by $\pi_\theta$ remain (largely) within the support of $\mathcal{D}$. This standard \emph{offline RL coverage} assumption ensures that the GVM can generalize to $\pi_\theta$’s behavior.

% \paragraph{(A2) Expressive Function Approximation.}
% We assume $V_\phi(\tau,s,a)$ has sufficient capacity (e.g., using large neural networks or LLM backbones) to approximate the true return function:
% \[
%   V^*(\tau,s,a) 
%   \;=\;
%   \mathbb{E}\Bigl[
%     \sum_{t=0}^\infty \gamma^t r_t
%     \;\Bigm|\;
%     \tau,\; s_0=s,\; a_0=a
%   \Bigr],
% \]
% with small approximation error. Specifically, there exists a parameter $\phi^*$ such that 
% \[
%   \max_{(\tau,s,a)\in \mathcal{X}} 
%   \bigl|\,V_{\phi^*}(\tau,s,a) - V^*(\tau,s,a)\bigr|
%   \;\le\; \epsilon_{\text{model}},
% \]
% for the set $\mathcal{X}$ relevant to $\mathcal{D}$.

% \paragraph{(A3) TD Convergence in Offline Setting.}
% Finally, we assume the TD($\lambda$) algorithm converges under standard conditions, such as diminishing step sizes and i.i.d.\ sampling within the dataset. In practice, mini-batch training and replay buffers approximate these conditions.

% \subsubsection{Training the Global Value Model}

% \paragraph{TD($\lambda$) Update.}
% From each sample $(\tau,s,a,R)$ in $\mathcal{D}$, we may have partial or multi-step returns. We define:
% \[
%   G^\lambda_{t} 
%   \;=\;
%   \sum_{k=0}^{m-1} \gamma^k\, r_{t+k}
%   \;+\;
%   \gamma^m\, V_\phi(\tau,\; s_{t+m},\; a_{t+m}),
% \]
% where $m$ is determined by $\lambda$-weighted returns~\cite{sutton2018reinforcement}. The TD($\lambda$) loss is:
% \begin{equation}
% \label{eq:td-loss}
%   \mathcal{L}_{\text{TD}}(\phi)
%   \;=\;
%   \mathbb{E}_{(\tau,s,a)\sim \mathcal{D}}
%   \Bigl[
%     \bigl(
%       G^\lambda_{t} - V_\phi(\tau,s,a)
%     \bigr)^2
%   \Bigr].
% \end{equation}
% Under (A1)--(A3), gradient descent on \eqref{eq:td-loss} can recover $V_{\phi^*}$ with approximation error $\epsilon_{\text{model}}$.

% \paragraph{Error Bound on $V_\phi$.}
% After training, let $\widehat{V}_\phi$ be the resulting model. Provided the dataset covers the trajectories of interest, the Bellman error satisfies:
% \[
%   \|\mathcal{T} \widehat{V}_\phi - \widehat{V}_\phi\|_{\mu_{\mathcal{D}}}
%   \;\le\; \delta_{\text{Bellman}},
% \]
% where $\mathcal{T}$ is the TD operator, and $\|\cdot\|_{\mu_{\mathcal{D}}}$ denotes a weighted $L^2$ norm over the dataset distribution. By standard offline RL theory~\cite{chen2019information, xie2021bellman}, $\delta_{\text{Bellman}}$ can be made arbitrarily small if the coverage and capacity assumptions hold, ensuring $\widehat{V}_\phi \approx V^*(\tau,s,a)$ on $\mathcal{D}$’s support.

% \subsubsection{Fixed GVM for Policy Updates}

% \paragraph{No New Ground-Truth Rewards.}
% During policy optimization, no new rewards $r(s,a)$ from the actual environment (or human feedback) are collected. Consequently, there is no additional supervision to refine $\widehat{V}_\phi$. Under assumption (A1), as long as $\pi_\theta$ remains in-distribution, the value estimates $\widehat{V}_\phi(\tau,s,a)$ remain valid approximations of the expected return under that policy.

% \paragraph{Advantage Computation.}
% In a PPO-like update, the advantage for $(s,a)$ is computed via:
% \[
%   \hat{A}(s,a) 
%   \;=\;
%   \widehat{V}_\phi(\tau, s, a) - \widehat{V}_\phi(\tau, s),
% \]
% where the second term is a shorthand for $\mathbb{E}_{a'\sim\pi(\cdot\mid s)}[\widehat{V}_\phi(\tau,s,a')]$ or an analogous baseline. Because $\widehat{V}_\phi$ does not change, the policy update is free from co-adaptation with the value function, reducing variance and instability.

% \subsubsection{Performance Bound}

% We can adapt offline RL analysis (e.g., the \emph{Policy Improvement} theorem~\cite{levine2020offline}) to our setting. Define:
% \[
%   J(\pi_\theta) 
%   \;=\; 
%   \mathbb{E}_{\tau\sim\mathcal{Q},\,s\sim d^\pi,\, a\sim \pi_\theta}
%   \Bigl[
%     r(s,a)
%   \Bigr],
% \]
% where $\tau\sim\mathcal{Q}$ is the policy context distribution in the offline dataset, and $d^\pi$ is the induced state distribution of policy $\pi_\theta$. Assuming that $\widehat{V}_\phi(\tau, s, a)$ achieves a small approximation error $\epsilon_{\text{model}}$ on $(\tau,s,a)$ relevant to $\pi_\theta$, we can show:

% \begin{equation}
% \label{eq:perf-bound}
%   |\,J(\pi_\theta) - \widetilde{J}(\pi_\theta)\,|
%   \;\le\;
%   \epsilon_{\text{model}}
%   \,+\,
%   \delta_{\text{cover}}
%   \,+\,
%   \delta_{\text{Bellman}},
% \end{equation}
% where $\widetilde{J}(\pi_\theta)$ is the return predicted by $\widehat{V}_\phi$, and $\delta_{\text{cover}}$ is an additional penalty if $\pi_\theta$ drifts outside the support of $\mathcal{D}$. Specifically, if $\|\mu_{\pi_\theta} - \mu_{\mathcal{D}}\|$ remains small under an appropriate norm, then $\delta_{\text{cover}}$ can be bounded.

% Equation~\eqref{eq:perf-bound} indicates that as long as the coverage assumption (A1) holds and the function approximation error $\epsilon_{\text{model}}$ is small, the discrepancy between the true performance $J(\pi_\theta)$ and the GVM-estimated performance $\widetilde{J}(\pi_\theta)$ is controlled. Hence, additional ground-truth rewards are not strictly necessary for improving the policy, \emph{provided} the policy does not deviate significantly from the trajectories captured in $\mathcal{D}$.

% \subsubsection{Implications}

% \begin{enumerate}[label=(\roman*)]
%     \item \textbf{Stability via Fixed Value:} By fixing $\widehat{V}_\phi$, we avoid the joint adaptation of policy and value networks. This stability leads to lower variance in gradient estimates and less risk of training collapse, compared to classical actor--critic methods in RLHF.
%     \item \textbf{Offline RLHF Feasibility:} Under the coverage and capacity assumptions, offline data is sufficient to learn a near-accurate GVM. Policy improvements follow from standard policy gradient steps if the policy remains in-distribution.
%     \item \textbf{Limited Exploration:} Because no new exploration data (and hence no new rewards) are collected, the method relies on existing coverage in $\mathcal{D}$. If $\pi_\theta$ seeks to explore outside that coverage, $\widehat{V}_\phi$ may produce unreliable estimates.
% \end{enumerate}

% \paragraph{Conclusion of Analysis.}
% From the above, we see that \emph{no new ground-truth rewards} are required during policy optimization as long as (i) the offline dataset covers relevant regions of $(\tau,s,a)$-space, and (ii) the value function is expressive and accurate within that coverage. The performance bound in \eqref{eq:perf-bound} formalizes how policy improvement remains valid in the offline regime, thus justifying a fixed GVM in RLHF settings where additional human feedback or environment rewards cannot be collected.

\section{Experiment Setup}

In this section, we conduct extensive experiments to validate the superiority of DVPO. Specifically, based on real-world scenarios of RLHF implementation, we perform preference optimization under two settings: Base and Instruction.

\subsection{Training settings}

For the \textbf{Base} setting, we follow the training pipeline of \citep{huang2024self}, perform preference optimization according to the following process. First, we select a base model for instruction fine-tuning (LLaMA3 8B\footnote{\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B}} and LLaMA3 3B\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-3B}}\cite{dubey2024llama}) to obtain the SFT model. Then, we initialize the preference optimization process from the SFT model and conduct training on the Ultrafeedback\cite{cui2023ultrafeedback} dataset. This setting encompasses the standard RLHF process and is highly transparent, as the SFT model is trained on open-source data and has not undergone preference optimization.

For the \textbf{Instruction} setting, We select an off-the-shelf instruction-tuned model as the SFT model. To facilitate comparison with related work\cite{li2023remax,rafailov2024direct}, we choose the mistral-instruction-v0.2\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}}. These models have undergone extensive instruction fine-tuning processes based on the base model, resulting in stronger capabilities compared to the SFT model in the Base setting. However, the instruction fine-tuning process is not transparent. We perform preference optimization on the instruction models.

\subsection{Evaluation}

We evaluate the response quality of the models using the most popular instruction-following benchmarks: MT-Bench\cite{zheng2023judging}, Arena-Hard\cite{li2024crowdsourced}, and Alpaca-Eval\cite{dubois2024length}. These benchmarks consist of a series of open-ended questions and employ LLM as a judge approach to score or compare the responses generated by the models. Additionally, we compare the win rate on the held-out test set.

\textbf{MT-Bench} comprises 80 multi-turn questions. The evaluation is conducted using GPT-4 as an automated judge, which scores the models' responses on a scale from 1 to 10 based on criteria such as relevance, coherence, and informativeness. \textbf{Arena-Hard} features a dataset of 500 prompts curated from Chatbot Arena's live data. The evaluation employs an LLM-as-a-judge framework, where models' responses are compared, and win rates are calculated to determine performance. \textbf{AlpacaEval} consists of 805 paired instructions. Evaluations are performed using an LLM-based automatic evaluator, such as GPT-4, which compares the responses of different models to the same prompt and determines a winner. To mitigate biases like verbosity, AlpacaEval employs \textbf{length-controlled win rates}.

\subsection{Baseline}
 For the \textbf{Base} setting, as previously described, we carried out experiments on the Ultrafeedback dataset. Following the experimental setup of \cite{huang2024self}, the dataset is divided into three parts: 30\% for SFT, 50\% for reward learning, and 20\% for RL optimization. The value model is trained on the reward learning data and then used to perform preference optimization on the SFT model using the 20\% RL optimization data. For the standard PPO process, the learned reward model is used similarly to optimize the SFT model on the 20\% RL optimization data. In addition, we compare the results with the original SFT model.

For the \textbf{Instruction} setting, we select Mistral-instruction-v0.2 as the starting point and perform RL preference optimization on the Ultrafeedback dataset. For this purpose, we utilize the existing reward model, UltraRM\footnote{\url{https://huggingface.co/openbmb/UltraRM-13b}}, aligning with the experimental setup of ReMax\cite{li2023remax}. We compare our model with other models of similar parameter sizes, including LLaMA3-8B-Instruction\footnote{\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}}, DPO, ReMax\footnote{\url{https://huggingface.co/ziniuli/Mistral-7B-ReMax-v0.1}}, etc.


\section{Experiment results}

In this section, we present the main experimental results for \textbf{Base} and \textbf{Instruction} settings, as well as the computational efficiency advantages of DVPO compared to standard RLHF methods. For details on the performance of the GVM, please refer to the Appendix~\ref{appendix:gvm performeance}.

\subsection{DVPO on Base setting}

As mentioned earlier, to clearly evaluate the feasibility of DVPO compared to existing methods, we conducted experiments on publicly available datasets. The datasets were split proportionally for supervised fine-tuning (SFT) and reward learning (value learning). Subsequently, the resulting value model (reward model) was used to guide the policy optimization for preference learning. Given the relatively small amount of data at each stage (with the RL phase accounting for approximately 20\% of the entire dataset), our focus in the Base setting is on relative improvement rather than absolute performance metrics. The results are presented in Table~\ref{tab:base-setting results}.

\textbf{DVPO demonstrates superior optimization performance.} In experiments conducted on LLaMA3-3B and LLaMA3-8B, DVPO consistently outperforms SFT significantly. Specifically, under the Base setting of LLaMA3-8B, DVPO achieves improvements of 0.2 on MT-Bench, 1.5 on Arena-Hard, and 1.74 on Alpaca-Eval compared to the SFT model. The improvements are even more pronounced with LLaMA3-3B, where DVPO achieves a 0.51 increase on MT-Bench, a 5-point increase on Arena-Hard, and a 4.14-point increase on Alpaca-Eval. Similarly, as shown in Figure~\ref{fig:win rate}, on the held-out test set, DVPO achieves a higher win rate compared to the SFT model. These results validate the robustness and effectiveness of DVPO.

\textbf{The pre-trained value model can provide a sufficiently high performance ceiling.} According to the experimental results in the Base setting, the final convergence performance of DVPO is very close to that of standard PPO (and in some cases, slightly exceeds it), indicating that the supervision signals provided by the pre-trained value model can support a sufficiently high performance ceiling. As an actor-critic method, PPO trains the policy model and value model simultaneously, with the reward model providing environmental feedback. The value model, as a bridge between immediate rewards (sentence-level) and long-term returns, offers finer-grained supervision signals (token-level) compared to immediate rewards alone. In DVPO, the global value model(GVM) is pre-trained on responses from different policies, enabling it to provide supervision feedback to various policies during the RL phase. The experimental results demonstrate that the fine-grained environmental feedback provided by DVPO can achieve a performance ceiling comparable to that of the actor-critic framework while significantly reducing training complexity and instability.





\begin{table}[t]
\caption{Performance under the Base setting. The suffix ``\textit{3B}" indicates that \textbf{llama3.2-3B-Base} was used as the base model, while ``\textit{8B}" indicates that \textbf{llama3-8B-Base} was used as the base model. Both PPO and DVPO were initialized from SFT and optimized. Compared to PPO, DVPO achieved the best benchmark scores across models with different parameter sizes.}
\label{tab:base-setting results}
\begin{center}
\begin{sc}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccr}
\toprule
\textbf{Model} & \textbf{Mtbench} & \textbf{Arena hard} & \textbf{AlpacaEval2} \\
\midrule
SFT-\textit{8B}  &    4.87  & 10.3  & 9.79 \\
PPO-\textit{8B}  &    4.98  & 11.7  & 11.14 \\
DVPO-\textit{8B}  &    \textbf{5.01}  & \textbf{11.8}  & \textbf{11.33} \\
\midrule
SFT-\textit{3B} &    5.22   & 10.4  & 8.19 \\
PPO-\textit{3B}  &    5.33   & 13.5  & 11.54 \\
DVPO-\textit{3B}  &    \textbf{5.73}   & \textbf{15.1}  & \textbf{12.33} \\
\bottomrule
\end{tabular}%
}
\end{sc}
\end{center}
\end{table}




\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{comparison_chart.pdf}}
\caption{Results of the model on the Ultrafeedback held-out testset. We employed GPT4o as a judge to assess the quality of model-generated responses. Performance is measured using the win rate, where \textbf{Left} represents DVPO, and Right represents the baseline model for comparison.}
\label{fig:win rate}
\end{center}
\vspace{-4mm}
\end{figure}

\subsection{DVPO on Instruction setting}

To evaluate the effectiveness of DVPO in most RLHF scenarios (optimization starting from a pre-aligned model), we selected Mistral-Instruction-v0.2 as the SFT model. An existing UltraRM reward model was used to collect labeled data for training the value model, value model also initialized from Mistral-Instruction-v0.2. The trained value model then guided the preference optimization of the SFT model. The results are presented in Table~\ref{tab:instruction-setting results}. Experimental findings demonstrate that our method exhibits significant advantages over other approaches. Our main findings are as follows:

\textbf{DVPO significantly enhances the performance of instruction models.} Compared to the original Mistral-Instruction-v0.2, DVPO consistently demonstrates performance improvements across all benchmarks. Specifically, it achieves a 0.19 improvement on MT-Bench and increases the win rate relative to GPT-4 by 12.1\% on Arena-Hard. Additionally, it enhances the length-controlled win rate by 10.32\% on Alpaca-Eval. Furthermore, compared to larger models such as LLaMA3-8B-Instruction, DVPO also exhibits notable advantages, outperforming 4.1\% in Arena-Hard and 4.51\% in Alpaca-Eval. These results indicate that the pre-trained value model can effectively and reliably guide the optimization of policy models.

\textbf{Finer-grained feedback signals lead to superior performance.} Compared to reward-only methods such as ReMax, DVPO demonstrates significant performance advantages on Mt-bench, Arena hard and Alpaca Eval. In ReMax, the feedback signal is at the sentence level, assigning a single reward score to the entire sentence and treating all tokens as equally contributing to the overall score. This design prevents the policy model from learning fine-grained preferences during the RL phase, resulting in suboptimal performance. In contrast, DVPO assigns a return value to each token, which facilitates more effective training of the policy model. 

Furthermore, DVPO outperforms direct preference optimize(DPO) approaches. In addition to providing fine-grained returns, DVPO retains the on-policy characteristics of the original PPO, enabling a larger exploration space and offering a higher performance ceiling.


\begin{table}[t]
\caption{Performance under the Instruction setting. \textbf{Mistral 7B} refers to \textbf{\textit{Mistral-7B-instruction-v0.2}}, and \textbf{Llama 8B} refers to \textbf{\textit{Llama3-8B-instruction}}. DVPO outperformed other preference alignment methods.}
\label{tab:instruction-setting results}
\begin{center}
\begin{sc}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccr}
\toprule
\textbf{Model} & \textbf{Mtbench} & \textbf{Arena hard} & \textbf{AlpacaEval2} \\
\midrule
Mistral 7B  &    6.60  & 12.6  & 17.11 \\
ReMax  &    6.67  & 21.9  & 20.55 \\
DPO  &    6.30  & 16.3  & 26.80 \\
DVPO  &    \textbf{6.79}   & \textbf{24.7}  & \textbf{27.43} \\
\midrule
Llama3 8B  &    6.90   & 20.6  & 22.92 \\
GPT4 &   7.93   & 35.0  & 30.20 \\
\bottomrule
\end{tabular}%
}
\end{sc}
\end{center}
\end{table}


\subsection{DVPO Computation Efficiency}

\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{training_curve.pdf}}
\caption{Learning curve of the policy model during the RL stage under the Base setting. DVPO demonstrates faster and more stable convergence compared to other methods.}
\label{fig:training curve}
\end{center}
\end{figure*}


\begin{table}[t]
\caption{Computational results under the \textbf{Base setting}. For the 3B model, we conducted experiments using \textbf{Zero-1}. For the 8B model, due to PPO encountering out-of-memory (OOM) issues with the Zero-1 training strategy, we employed \textbf{Zero-3} instead. Details can be found in the Appendix. We report the GPU memory consumption per GPU (GB), the time required per step (seconds), and the number of steps needed to reach convergence under an 8×A100 training environment.}
\label{tab:gpu memory}
\begin{center}
\begin{sc}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & Memory/per GPU & Time/S & Training Step \\
\midrule
PPO-8B  &    78.96G  & 70 & 1250    \\
DVPO-8B  &    60.49G  & 48 & 810   \\
\midrule
PPO-3B  &    41.43G   & 23 & 600   \\
DVPO-3B  &    27.50G   & 13 & 450   \\
\bottomrule
\end{tabular}%
}
\end{sc}
\end{center}
\end{table}

DVPO improves upon RLHF by leveraging a pre-trained value model to provide environmental feedback, resulting in enhanced computational efficiency, stability, and convergence speed.

\textbf{Memory Efficiency.} As shown in Table~\ref{tab:gpu memory}, the experimental setup for standard PPO and DVPO was kept identical. Under standard training conditions, DVPO achieves approximately a 50\% reduction in memory usage. For instance, with LLaMA-3B, we trained using Zero-1 on 8 A100 GPUs with a batch size of 4. Standard PPO requires 41.43 GB of memory due to the need to simultaneously load 4 models (policy model, critic model, reference model, and reward model), with two models (policy model and critic model) requiring activation. In contrast, DVPO only requires 27.5 GB of memory, as it only loads the policy model, global value model, and reference model, with only the policy model being activated during training. For LLaMA-8B, we used DeepSpeed Zero-3 with a batch size of 4. Even under this configuration, DVPO effectively reduces memory usage, enabling support for larger batch sizes during training.

\textbf{Time Efficiency.} DVPO accelerates the training process, requiring only half the time per batch compared to standard PPO under the same batch size. This improvement stems from the fact that, during the generation phase, DVPO requires outputs from only three models, whereas standard PPO relies on four. Additionally, during the backpropagation phase, PPO updates both the policy and value models, while DVPO updates only the policy model. Overall, under identical settings (batch size), DVPO achieves a 1.76x speedup in training ($1.76 \approx23/13$).

\textbf{Training Step.} We observe that using a pre-trained value model to provide token-level fine-grained environmental feedback enables faster model convergence, which aligns with the findings in \cite{noukhovitch2024language}. Specifically, under identical experimental settings (same batch size, experience replay count, etc.), DVPO requires only approximately $3/4$ of the interaction steps to converge. As shown in Figure~\ref{fig:training curve}, in the LLaMA3-8B experiment, the reward curve of PPO reaches its peak at 1250 steps, whereas DVPO achieves optimal performance around 810 steps. Similarly, in LLaMA3-3B, PPO reaches its peak at 600 steps, while DVPO converges in 450 steps. Since DVPO provides token-level supervision feedback and is pre-trained in advance, it can be considered a form of warm-starting, thereby accelerating policy convergence.

\textbf{Policy Training Curve.}
As shown in Figure~\ref{fig:training curve}. In the Llama3 3B setting, DVPO achieves faster convergence and exhibits more stable performance compared to PPO, with a smoother increase in reward scores and consistent improvements in value scores throughout training. In the Llama3 8B setting, DVPO also exhibits a more stable training curve, indicating better training stability and efficiency relative to PPO.
These results substantiate the advantages of DVPO, particularly its capability to achieve faster and more stable convergence across models with varying parameter scales. 

Furthermore, DVPO demonstrates the ability to converge to a stable state more rapidly. In the Llama3 3B experiments, DVPO achieved convergence at approximately 450 steps, whereas PPO required around 600 steps to converge. Similarly, in the Llama3 8B experiments, DVPO converged at around 800 steps, while PPO completed convergence only after 1200 steps.

\section{Conclusion}


We propose Decoupled Value Policy Optimization (DVPO), a framework that eliminates joint actor-critic training in RLHF by leveraging a pretrained global value model (GVM). Unlike traditional PPO-based RLHF, which suffers from training instability and computational overhead, DVPO stabilizes policy optimization through static, token-level return-to-go estimates. Our theoretical analysis proves the functional equivalence of reward and value models under the constraint of no new reward feedback, justifying the use of a fixed GVM for efficient and scalable optimization. Empirical results demonstrate that DVPO achieves comparable performance to state-of-the-art RLHF methods on multiple benchmarks while reducing training time and GPU usage by over 35\%. Future work will focus on refining the value model’s training process to enhance prediction accuracy.


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Training Details}

\textbf{SFT training}. We use the following hyperparameters for instruction fine-tuning training. We employ a learning rate of 2e-5 with cosine decay, 2 warmup steps, and a batch size of 16. We calculate the loss only for the target tokens rather than the full input sequence, and we train for 3 epochs on the training data.  we conduct the training on 8 NVIDIA A100 80G GPUs. 

\textbf{Reward training}. To enable the model to learn the relative ranking among different responses, we use a pair-wise loss. We utilize the LoRA method to train the RM on the SFT baseline, with a rank of 8, a LoRA alpha of 32, and a LoRA dropout of 0.1. The task type is sequence classification. We use a learning rate of 2e-5 with linear decay and the AdamW optimizer for training over 2 epochs, with a batch size of 4. We conduct the training on 8 NVIDIA A100 80G GPUs .

\textbf{PPO training and GVPO training}. For PPO training, we use a learning rate of 2e-6 and set the generate sample length to 1024. We employ a batch size of 8 and a mini-batch size of 2, with 4 PPO epochs and 2 gradient accumulation step. The target KL divergence is set to 0.1 and  initial KL coefficient is set to 0.2. For a fair comparison, GVPO training was conducted using the same hyperparameter settings (e.g., batch size, sequence length, etc.).

\textbf{Global value model training.} 
We initialize the value model from the SFT model. The training is conducted with a batch size of 32, a sequence length of 1024, and a learning rate of 2e-6. We employ the LoRA method to train the value model on the SFT baseline, using a rank of 8, a LoRA alpha of 32, and a LoRA dropout of 0.1.



\section{GVM case study}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.95\linewidth,trim={50pt 200pt 70pt 50pt}, clip]{gvm_case.pdf}
  \caption{An example of the supervisory signal provided by a Global Value Model (\textbf{GVM}). The GVM is capable of providing token-level feedback. In this example, the GVM assigns a \textbf{lower value} to the incorrect response (\textbf{response2}: \textit{``is an island"}) and a \textbf{higher value} to the critical token \textit{``not"} in the correct response (\textbf{response1}: ``\textit{not an island}").}
  \label{fig:method}
  % \vspace{-5mm}
\end{figure*}

For the same question, "As an island, is Beijing located in Asia?", the value model provides fine-grained supervisory signals for two different responses. The GVM assigns specific values to each token in the responses. These values represent the model's assessment of the importance or correctness of each token in the given context.

As shown in Figure~\ref{fig:method}. 
For Response 1, the critical token ``not" is given a higher value (0.2099), highlighting its significance in forming the correct response, "not an island."
For Response 2, the GVM assigns lower values to incorrect tokens, such as ``is" (-0.6177) and "an" (-0.4766), indicating their contribution to the incorrect response, ``is an island."
This token-level evaluation demonstrates the GVM's ability to guide learning by penalizing incorrect responses and reinforcing critical tokens in correct responses, thereby enhancing training accuracy and interpretability.



\section{GVM performance}
\label{appendix:gvm performeance}


We evaluate the performance of the GVM from multiple perspectives. Specifically, we observe that the GVM assigns higher value scores to good actions and lower value scores to bad actions, as illustrated in Figure~\ref{fig:method}. We evaluate the global value model(GVM) on a held-out test set. For each question, we expect the value model to assign higher values to good responses compared to bad ones. We calculate the accuracy under various metrics (mean, P1, etc.) to assess the model's performance.

Table~\ref{tab:gvm performeance} presents the performance metrics of the Global Value Model (GVM) initialized from \textit{Mistral-7B-instruction-v0.2}. The metrics include the mean value, which indicates the average score assigned by the GVM, and various percentiles (P1, P5, P10, P90, P95, and P99), representing the corresponding value distributions. The accuracy metric evaluates the proportion of cases where the GVM correctly assigns a higher value to a good response compared to a bad response, thereby reflecting the effectiveness of the GVM in providing accurate feedback.

As shown in the table, the mean accuracy is 64.51\%, with percentile values ranging from 56.02\% (P1) to 61.47\% (P90), indicating consistent performance across different value ranges. These results demonstrate the GVM's ability to provide reliable token-level evaluations.

\begin{table}[h]
\caption{
The performance of the GVM initialized from \textit{Mistral-7B-instruction-v0.2}. Here, mean represents the average value of the predictions, P1 indicates the 1st percentile value, and accuracy reflects the proportion of cases where the value for a good response exceeds the value for a bad response, which is considered as correct feedback from the global value model.}
\label{tab:gvm performeance}
\begin{center}
\begin{sc}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & Accuracy \\
\midrule
Mean Value  &    64.51     \\
P1 Value  &    56.02     \\
P5 Value &       58.46          \\
P10 Value  &    59.83     \\
P90 Value  &    61.47    \\
P95 Value   &   60.30            \\
P99 Value   &    59.94           \\
\bottomrule
\end{tabular}
\end{sc}
\end{center}
\end{table}






\section{GPT4 evaluation Prompt}

A crucial element of our experimental framework is the evaluation of win rates using GPT-4. In this section, we provide the prompts utilized to generate win rates for both the summarization and dialogue experiments. All experiments were conducted using the gpt-4o-20240806 model. The sequence of responses was randomized for each evaluation to ensure unbiased results.


\textbf{GPT-4 as judge system prompt:}


Review the user's question and the corresponding response using\
the additive 5-pointscoring system described below. Points are\
accumulated based on the satisfaction of each criterion:

    - Add 1 point if the response is relevant and provides some\
    information related to the user's inquiry, even if it is \
    incomplete or contains some irrelevant content.
    
    - Add another point if the response addresses a substantial \
    portion of the user's question, but does not completely \
    resolve the query or provide a direct answer.
    
    - Award a third point if the response answers the basic \
    elements of the user's question in a useful way, regardless \
    of whether it seems to have been written by an Al Assistant or
    if it has elements typically found in blogs or search results.
    
    - Grant a fourth point if the response is clearly written \
    from an Al Assistant's perspective, addressing the user's \
    question directly and comprehensively, and is well-organized \
    and helpful, even if there is slight room for improvement in \
    clarity, conciseness or focus.
    
    - Bestow a fifth point for a response that is impeccably \
    tailored to the user's question by an AI Assistant, without \
    extraneous information, refecting expert knowledge, and \
    demonstrating a high-quality, engaging, and insightful answer.
    
    After examining the user's instruction and the response, 
    please first give the total score. Then provide a brief \
    explanation of your total score, up to 100 words. Output \
    with the following format:
    Score: \{total score\}
    Evaluation evidence: \{your brief explanation here\}
    
    Remember to assess from the Al Assistant perspective,
    utilizing web search knowledge as necessary. To evaluate the \
    response in alignment with this additive scoring model, 
    we'll systematically attribute points based on the outlined
    criteria.


\textbf{User input prompt:}


\texttt{<Question>: \{question\}\\ <response>: \{answer\}}

\begin{table}[h]
\caption{A case study for GPT4 evaluation: LLama3 8B SFT}
\begin{tabularx}{\textwidth}{@{} p{3cm} X @{}}
    \toprule
    \textbf{Input} & Compare and contrast Harry Potter with Percy Jackson series.\\
    \midrule
    \textbf{Llama3 8B SFT} & Sure, I'd be happy to help you compare and contrast the Harry Potter and Percy Jackson series!Both Harry Potter and Percy Jackson are popular fantasy series that have captured the hearts of readers around the world. Here are some key similarities and differences between the two:Similarities:1. Magical elements: Both series feature a magical aspect, with Hogwarts School of Witchcraft and Wizardry being the setting for Harry Potter and Camp Half-Blood for Percy Jackson.2. Mythological references: Both series draw heavily on Greek mythology, with Percy Jackson's stories being based on the Labors of Hercules and other Greek myths.3. Relatable protagonists: Both Harry and Percy are relatable and charming protagonists that readers can connect with.Differences:1. Setting: While both series take place in magical worlds, the settings are quite different. Harry Potter's stories take place in a more traditional school setting, while Percy Jackson's stories take place in a summer camp atmosphere.2. Tone: Harry Potter is a more lighthearted and comedic series, while Percy Jackson takes a darker and more serious tone, exploring themes of mortality and responsibility.3. Plot structure: Harry Potter has a more linear plot structure, with each book building on the previous one's events. Percy Jackson, on the other hand, has a more episodic structure, with each book focusing on a different adventure or challenge for the characters.4. Themes: While both series explore themes of good and evil, Harry Potter focuses more on the importance of friendship and loyalty, while Percy Jackson delves into the consequences of power and responsibility.In conclusion, while both Harry Potter and Percy Jackson share many similarities, their differences in setting, tone, plot structure, and themes offer unique reading experiences for readers. Both series are highly recommended for anyone looking for a magical and engaging read! \\
    \hdashline
    \textbf{GPT4-as-Judge} & Score: 3
    
    Evaluation evidence: The response provides a relevant comparison of the Harry Potter and Percy Jackson series, highlighting key similarities and differences in magical elements, settings, tone, plot structure, and themes. It addresses the user's question by outlining basic elements and offering a useful comparison. However, the answer contains inaccuracies, such as misrepresenting the tone of both series and the nature of mythological references in Percy Jackson. Additionally, the response could be more precise and direct, as some points are not entirely accurate or comprehensive. Thus, it meets the criteria for a basic, useful response but lacks thoroughness and precision for a higher score. \\
    \bottomrule
\end{tabularx}
\label{tab: gpt4_as_judge_case_llama3_8b_sft}
\end{table}


\begin{table}[h]
\caption{A case study for GPT4 evaluation: LLama3 8B PPO}
\begin{tabularx}{\textwidth}{@{} p{3cm} X @{}}
    \toprule
    \textbf{Input} & Compare and contrast Harry Potter with Percy Jackson series.\\
    \midrule
    \textbf{Llama3 8B PPO} & Sure, I'd be happy to help you compare and contrast the Harry Potter and Percy Jackson series!Both Harry Potter and Percy Jackson are popular fantasy series that have captured the hearts of readers around the world. Here are some key similarities and differences between the two:Similarities:1. Magical elements: Both series feature a magical aspect, with Hogwarts School of Witchcraft and Wizardry being the setting for Harry Potter and Camp Half-Blood being the setting for Percy Jackson.2. Teenage protagonists: The main characters in both series are teenagers, navigating their way through adolescence while also facing epic challenges and learning valuable life lessons.3. Epic adventures: Both series involve the protagonists on thrilling quests, battling mythical creatures, and uncovering secrets that could change the course of history.4. Portal to another world: In both series, there is a gateway to another world (Harry's cupboard under the stairs and the portal at Camp Half-Blood) that allows the protagonists access to a realm filled with magical creatures and unparalleled adventure.Differences:1. Setting: While both series take place in magical worlds, the settings are quite different. Harry Potter is set in a medieval-style world with castles and magical creatures, while Percy Jackson is set in a modern-day world with technology and urban landscapes.2. Tone: The tone of the two series is quite different. Harry Potter is lighthearted and comedic, while Percy Jackson is darker and more serious, with themes of mortality and morality.3. Mythological references: While both series draw on mythological references, the specific myths and legends used are different. Harry Potter relies more on European and British myths, while Percy Jackson draws on Greek and Roman mythology.4. Magic systems: The magic systems in the two series are also different. Harry Potter uses a spell-based magic system, while Percy Jackson uses a more physical, athletic approach to magic.5. Protagonists: The protagonists in the two series are quite different. Harry Potter is a boy wizard who discovers his true identity and heritage, while Percy Jackson is a demigod who must navigate the complexities of his divine heritage and mortal life.6. Plots: The plots of the two series are distinct. Harry Potter focuses on the battle between good and evil, with Harry as the prophesized hero who must face off against Voldemort. Percy Jackson, on the other hand, is about the struggles of the demigods and the impact of their mortal lives on their divine heritage.In conclusion, both Harry Potter and Percy Jackson are captivating fantasy series that share some similarities, but also differ in many ways. While both offer engaging and entertaining reads, they appeal to different audiences and perspectives.\\
    \hdashline
    \textbf{GPT4-as-Judge} & Score: 3
    
    Evaluation evidence: The response provides a relevant comparison of the Harry Potter and Percy Jackson series, addressing the user's question with a mix of similarities and differences. It covers key aspects such as magical elements, protagonists, and settings, offering a basic understanding of both series. However, the response contains inaccuracies, particularly in tone descriptions and settings, which detracts from its comprehensiveness and accuracy. While it offers a useful comparison, it does not fully address the user's question with the depth or precision expected from an AI Assistant, nor does it reflect expert knowledge or exceptional insight. \\
    \bottomrule
\end{tabularx}
\label{tab: gpt4_as_judge_case_llama3_8b_ppo}
\end{table}



\begin{table}[h]
\caption{A case study for GPT4 evaluation: LLama3 8B DVPO}
\begin{tabularx}{\textwidth}{@{} p{3cm} X @{}}
    \toprule
    \textbf{Input} & Compare and contrast Harry Potter with Percy Jackson series.\\
    \midrule
    \textbf{Llama3 8B DVPO} & Sure, I'd be happy to help! Both the Harry Potter and Percy Jackson series are popular and successful book series that have captivated readers worldwide. Here are some key similarities and differences between the two:Similarities:1. Magical elements: Both series feature magical aspects, such as spells, wizards, and fantastical creatures.2. Young protagonists: The main characters in both series are young and face challenges and adventures that come with growing up in a magical world.3. Family themes: Both series emphasize the importance of family and the bonds between family members.4. Friendship: The series also highlight the importance of friendship and the power of having loyal and supportive friends.Differences:1. Genre: While both series can be classified as fantasy, Harry Potter is more traditionally fantasy-oriented, while Percy Jackson is urban fantasy with a contemporary setting.2. Setting: Harry Potter is set in a parallel universe where wizardry and magic are prevalent, while Percy Jackson is set in present-day America and Greece.3. Magic system: The magic systems in each series are different. In Harry Potter, magic is cast using wands and spells, while in Percy Jackson, magic is accessed through the use of magical coins and enchanted items.4. Gods and myths: Harry Potter is based on the idea of witches and wizards living among us, while Percy Jackson is based on Greek mythology and features the gods and goddesses of ancient Greece.5. Tone: Harry Potter is written in a more traditional, epic style, while Percy Jackson is written in a more contemporary, humorous tone.Ultimately, both series offer engaging, imaginative storytelling that captures the hearts and minds of readers. While they both explore similar themes, their unique settings, magic systems, and tones make them distinct from one another. I hope this helps! Let me know if you have any other questions.\\
    \hdashline
    \textbf{GPT4-as-Judge} & Score: 5
    
    Evaluation evidence: The response thoroughly addresses the user's question by comparing and contrasting the Harry Potter and Percy Jackson series. It identifies key similarities, such as magical elements, young protagonists, and themes of family and friendship. The response also effectively highlights differences in genre, setting, magic systems, mythological influences, and tone. The information is well-organized, relevant, and engaging, demonstrating a comprehensive understanding of both series. The response is tailored specifically to the user's inquiry, providing insightful and expert knowledge without any extraneous information. \\
    \bottomrule
\end{tabularx}
\label{tab: gpt4_as_judge_case_llama3_8b_dvpo}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

