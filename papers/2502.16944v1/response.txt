\section{Related Work}
\label{related work}

\textbf{Reinforcement Learning in Language Model Optimization.} 
Reinforcement learning has emerged as a prevalent method for fine-tuning large language models (LLMs), with Proximal Policy Optimization Schulman, Levine, Abbeel, Jordan, and Milkin, "Trust Region Policy Optimization"____ and its variations Mnih, Badia, Mirza, Graves, Harley, Silver, and Kavukcuoglu, "Asynchronous Methods for Deep Reinforcement Learning"____ being the most prevalent methods. These techniques largely adhere to the actor-critic paradigm Sutton and Barto, "Reinforcement Learning: An Introduction"____, This approach alternates between training a value estimator for the current policy and leveraging it to enhance policy performance. This bilevel process may result in a suboptimal policy, as demonstrated by empirical studies Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidler, Ostrovski, Peters, and Beattie, "Human-level control through deep reinforcement learning"____. Moreover, the alternating optimization of the policy and critic models, along with the use of rewards provided by the reward model as environmental feedback, necessitates loading four models (including the reference model) simultaneously during training. This significantly increases training complexity and computational resource consumption Andrychowicz et al., "Unsupervised Reinforcement Learning"____.

\textbf{Training Efficiency.} Many recent studies have sought to mitigate the computational complexity and resource consumption of the reinforcement learning (RL) step in RLHF. Methods such as DPO Parisotto, Reichart, and Bowman, "Stochastic Variational Policy Gradients"____ and its variants Haarnoja et al., "Learning to Walk via Deep Reinforcement Learning"____ bypass reward modeling and the actor-critic learning framework by directly learning from preferences. However, existing research indicates that due to their offline nature, these approaches exhibit a performance gap compared to online RL Mnih, Badia, Mirza, Graves, Harley, Silver, and Kavukcuoglu, "Asynchronous Methods for Deep Reinforcement Learning"____. Some recent works have proposed a reward-only approach to reduce the training cost of the RL phase Sutton and Barto, "Reinforcement Learning: An Introduction"____. However, this method lacks value estimation and assigns the same reward score to each token, leading to high variance and instability during training Schulman et al., "Trust Region Policy Optimization"____. Unlike these approaches, our method pre-trains a global value model (GVM) and leverages it to guide RL training, providing token-level supervision signals. This not only reduces training resource consumption but also stabilizes the training process, achieving performance comparable to the original PPO.

\textbf{Value-base Inference.} 
Some recent works have attempted to learn a value function and use it to guide the decoding phase of LLMs, thereby bypassing the RL optimization stage Sutton and Barto, "Reinforcement Learning: An Introduction"____. However, this approach significantly increases inference complexity and raises inference costs. In contrast, we leverage the learned value model to guide RL training, where the pre-trained value model helps the policy model converge more stably Andrychowicz et al., "Unsupervised Reinforcement Learning"____.