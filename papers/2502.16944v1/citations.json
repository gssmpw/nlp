[
  {
    "index": 0,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ramamurthy2022reinforcement",
        "author": "Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin",
        "title": "Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization"
      },
      {
        "key": "wu2023pairwise",
        "author": "Wu, Tianhao and Zhu, Banghua and Zhang, Ruoyu and Wen, Zhaojin and Ramchandran, Kannan and Jiao, Jiantao",
        "title": "Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "sutton2018reinforcement",
        "author": "Sutton, Richard S",
        "title": "Reinforcement learning: An introduction"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gao2023scaling",
        "author": "Gao, Leo and Schulman, John and Hilton, Jacob",
        "title": "Scaling laws for reward model overoptimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "yao2023deepspeed",
        "author": "Yao, Zhewei and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Rajbhandari, Samyam and Wu, Xiaoxia and Awan, Ammar Ahmad and Rasley, Jeff and Zhang, Minjia and Li, Conglong and Holmes, Connor and others",
        "title": "Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales"
      },
      {
        "key": "hu2024openrlhf",
        "author": "Hu, Jian and Wu, Xibin and Wang, Weixun and Zhang, Dehao and Cao, Yu and others",
        "title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      },
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      },
      {
        "key": "hong2024orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xu2024dpo",
        "author": "Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi",
        "title": "Is dpo superior to ppo for llm alignment? a comprehensive study"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "li2023remax",
        "author": "Li, Ziniu and Xu, Tian and Zhang, Yushun and Yu, Yang and Sun, Ruoyu and Luo, Zhi-Quan",
        "title": "Remax: A simple, effective, and efficient method for aligning large language models"
      },
      {
        "key": "gunter2024apple",
        "author": "Gunter, Tom and Wang, Zirui and Wang, Chong and Pang, Ruoming and Narayanan, Andy and Zhang, Aonan and Zhang, Bowen and Chen, Chen and Chiu, Chung-Cheng and Qiu, David and others",
        "title": "Apple intelligence foundation language models"
      },
      {
        "key": "shao2024deepseekmath",
        "author": "Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others",
        "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models"
      },
      {
        "key": "ahmadian2024back",
        "author": "Ahmadian, Arash and Cremer, Chris and Gall{\\'e}, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Pietquin, Olivier and {\\\"U}st{\\\"u}n, Ahmet and Hooker, Sara",
        "title": "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hu2025reinforce++",
        "author": "Hu, Jian",
        "title": "REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "han2024value",
        "author": "Han, Seungwook and Shenfeld, Idan and Srivastava, Akash and Kim, Yoon and Agrawal, Pulkit",
        "title": "Value Augmented Sampling for Language Model Alignment and Personalization"
      },
      {
        "key": "kong2024aligning",
        "author": "Kong, Lingkai and Wang, Haorui and Mu, Wenhao and Du, Yuanqi and Zhuang, Yuchen and Zhou, Yifei and Song, Yue and Zhang, Rongzhi and Wang, Kai and Zhang, Chao",
        "title": "Aligning Large Language Models with Representation Editing: A Control Perspective"
      },
      {
        "key": "mao2024don",
        "author": "Mao, Xin and Li, Feng-Lin and Xu, Huimin and Zhang, Wei and Luu, Anh Tuan",
        "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "noukhovitch2024language",
        "author": "Noukhovitch, Michael and Lavoie, Samuel and Strub, Florian and Courville, Aaron C",
        "title": "Language model alignment with elastic reset"
      }
    ]
  }
]