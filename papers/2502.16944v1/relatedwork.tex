\section{Related Work}
\label{related work}


\textbf{Reinforcement Learning in Language Model Optimization.}
Reinforcement learning has emerged as a prevalent method for fine-tuning large language models (LLMs), with Proximal Policy Optimization~\cite{schulman2017proximal} and its variations~\cite{ramamurthy2022reinforcement,wu2023pairwise} being the most prevalent methods. These techniques largely adhere to the actor-critic paradigm~\cite{sutton2018reinforcement}, This approach alternates between training a value estimator for the current policy and leveraging it to enhance policy performance. This bilevel process may result in a suboptimal policy, as demonstrated by empirical studies~\cite{gao2023scaling}. Moreover, the alternating optimization of the policy and critic models, along with the use of rewards provided by the reward model as environmental feedback, necessitates loading four models (including the reference model) simultaneously during training. This significantly increases training complexity and computational resource consumption~\cite{yao2023deepspeed,hu2024openrlhf}.

\textbf{Training Efficiency.} Many recent studies have sought to mitigate the computational complexity and resource consumption of the reinforcement learning (RL) step in RLHF. Methods such as DPO~\cite{rafailov2024direct} and its variants~\cite{meng2024simpo,ethayarajh2024kto,hong2024orpo} bypass reward modeling and the actor-critic learning framework by directly learning from preferences. However, existing research indicates that due to their offline nature, these approaches exhibit a performance gap compared to online RL~\cite{xu2024dpo}. Some recent works have proposed a reward-only approach to reduce the training cost of the RL phase~\cite{li2023remax,gunter2024apple,shao2024deepseekmath,ahmadian2024back}. However, this method lacks value estimation and assigns the same reward score to each token, leading to high variance and instability during training~\cite{hu2025reinforce++}. Unlike these approaches, our method pre-trains a global value model (GVM) and leverages it to guide RL training, providing token-level supervision signals. This not only reduces training resource consumption but also stabilizes the training process, achieving performance comparable to the original PPO.

\textbf{Value-base Inference.} 
Some recent works have attempted to learn a value function and use it to guide the decoding phase of LLMs, thereby bypassing the RL optimization stage~\cite{han2024value,kong2024aligning,mao2024don}. However, this approach significantly increases inference complexity and raises inference costs. In contrast, we leverage the learned value model to guide RL training, where the pre-trained value model helps the policy model converge more stably~\cite{noukhovitch2024language}.