\section{Approach}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/archi.pdf}
\caption{\textbf{Overview of the proposed AQ-Net.} 
The input includes historical pollutant concentrations,  and visible station coordinates. An LSTM extracts temporal dependencies, enhanced by Multi-Head Attention to highlight critical time steps. After temporal pooling, a neural kNN module performs spatial interpolation for unobserved stations (red markers).}
\label{fig:overall_architecture}
\end{figure}

The air quality data used as input is from a network of real-time air quality sensors throughout China, which are managed by the Ministry of Environmental Protection (MEP) and published hourly by the China Environmental Monitoring Centre (CEMC) \cite{SONG2017334}. This network has been collecting measurements since 2013, with the goal to study and predict air quality issues throughout China. By 2014, there were 944 air quality monitoring stations in 190 cities, and currently, there are over 2100 stations throughout China. The air quality parameters measured by this network are PM$_{2.5}$ and PM$_{10}$, NO$_\mathrm{x}$, SO$_2$, O$_3$ and CO. The dataset has been quality controlled with the algorithm described by Wu et al.\cite{Wu2018}. This study uses data from 584 stations in the metropolitan area of northern China from 2013 to 2017.

\subsection{Overall Architecture}
Our model predicts PM\(_{2.5}\) by combining temporal and spatial dependencies. AQ-Net comprises three core components: an LSTM-MHA module, combining LSTM and multi-head attention for temporal feature extraction, a neural kNN module for spatial interpolation, and a Cyclic Encoding (CE) layer for time embedding. We use hourly measurements of PM\(_{2.5}\), PM\textsubscript{10}, CO, NO\textsubscript{2}, SO\textsubscript{2}, and O\textsubscript{3} from monitoring stations, and estimate PM\(_{2.5}\) for the coming hours and days. The LSTM captures long-range pollutant fluctuations, while multi-head attention highlights critical time steps. A temporal pooling step condenses the latent sequence into a single feature vector, which the neural kNN module uses for spatial interpolation at unobserved stations based on their nearest neighbors. This integrated architecture generates 168-hour (7-day) PM\(_{2.5}\) estimation for both observed and unobserved locations, leveraging key temporal patterns and spatial relationships.

\subsection{Proposed Modules}
Air quality reanalysis is formulated as a spatiotemporal reanalysis problem. The input data consists of historical pollutant concentrations along with their time steps and geospatial information from monitoring stations. 

\noindent\textbf{Cyclic Encoding (CE) for Temporal Features}
To preserve the periodic nature of time-related features, we apply a cyclic encoding technique to the time step $t$ using sine and cosine transformations. This approach ensures a continuous representation, preventing discontinuities between values such as 23:00 and 00:00. The encoding is defined as $x_{\sin} = \sin \left( \frac{2\pi t}{\text{cycle}} \right), x_{\cos} = \cos \left( \frac{2\pi t}{\text{cycle}} \right)$, where \( t \) represents a temporal feature (e.g., hour, day, or month), and cycle corresponds to its periodicity (e.g., 24 for hours, 7 for days, and 12 for months).

\noindent\textbf{Long Short-Term Memory (LSTM):} To capture temporal dependencies, an LSTM network processes the time series data of pollutant concentrations. Given an input sequence $X \in \mathbb{R}^{C \times T \times N}$, where C is the number of features, T is the sequence length (number of time steps), and N is the number of stations, the LSTM generates a temporal representation $Z \in \mathbb{R}^{\text{dim} \times T \times N}$, where \(\text{dim}\) represents the hidden state dimension. The LSTM captures long-term dependencies, allowing the model to learn pollutant trends over time.

\noindent\textbf{Multi-Head Attention (MHA):} Although LSTM effectively captures sequential dependencies, it treats all past observations equally at each time step. To improve performance, we integrate a Multi-Head Attention (MHA) mechanism that enhances temporal dependencies by selectively weighting relevant time steps. The attention mechanism is computed as follows:

\begin{small}
\begin{equation}
Z' = \text{softmax} \left( \frac{Q_zK_z^T}{\sqrt{d_k}} \right) V_z + Z
\label{eq:fm_1}
\end{equation}
\end{small}

\noindent where $Q_z, K_z, V_z$ are linear transformations of $Z$ and $d_k$ is a scaling factor. This mechanism enables the model to focus on important time intervals, improving its ability to recognize patterns in pollutant fluctuations.

\noindent\textbf{Spatial Interpolation via kNN}
While the LSTM-MHA module captures temporal trends, it does not account for spatial correlations between monitoring stations. To estimate PM\(_{2.5}\) values at unobserved stations, we employ a kNN-based interpolation method. Instead of using the raw PM\(_{2.5}\) values from observed stations, we first extract a ``station-wise feature vector'' from the refined temporal representation $Z'$ using temporal pooling $Z' \in \mathbb{R}^{C\times \text{dim} \times N}$. This feature vector encapsulates the learned temporal patterns of each station, rather than just the raw measurements. Given a set of observed stations with known PM\(_{2.5}\) values, missing values at unobserved stations are estimated as, $Y = h(Z', p, k)$, where $p$ represents the geospatial coordinates of the stations, and $h(\cdot)$ applies kNN-weighted interpolation. The number of neighborhoods $k$ is defined on the fly such that we obtain multiple estimations. To speed up the computation of the station-to-station distance, we utilize GPU-enabled kNN query to ensure gradient backpropagation and fast searching. The kNN module finds the nearest stations in the learned feature space and interpolates the missing values accordingly, ensuring that spatial dependencies are taken into account.
