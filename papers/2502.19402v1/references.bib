@misc{tajwar2024preferencefinetuningllmsleverage,
      title={Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data}, 
      author={Fahim Tajwar and Anikait Singh and Archit Sharma and Rafael Rafailov and Jeff Schneider and Tengyang Xie and Stefano Ermon and Chelsea Finn and Aviral Kumar},
      year={2024},
      eprint={2404.14367},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.14367}, 
}

@inproceedings{agrawal2015learning,
  title={Learning to see by moving},
  author={Agrawal, Pulkit and Carreira, Joao and Malik, Jitendra},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={37--45},
  year={2015}
}

@inproceedings{doersch2015unsupervised,
  title={Unsupervised visual representation learning by context prediction},
  author={Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1422--1430},
  year={2015}
}

@article{donahue2013decaf,
  title={Decaf: A deep convolutional activation feature for generic visual recognition. CoRR},
  author={Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  journal={arXiv preprint arXiv:1310.1531},
  year={2013}
}

@inproceedings{agrawal2014analyzing,
  title={Analyzing the performance of multilayer neural networks for object recognition},
  author={Agrawal, Pulkit and Girshick, Ross and Malik, Jitendra},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13},
  pages={329--344},
  year={2014},
  organization={Springer}
}

@article{newport1988constraints,
  title={Constraints on learning and their role in language acquisition: Studies of the acquisition of {American Sign Language}},
  author={Newport, Elissa L},
  journal={Language Sciences},
  volume={10},
  number={1},
  pages={147--172},
  year={1988},
  publisher={Elsevier}
}

@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  volume={48},
  number={1},
  pages={71--99},
  year={1993},
  publisher={Elsevier}
}

@article{openai_o1,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@article{zhong2024algorithmic,
  title={Algorithmic Capabilities of Random Transformers},
  author={Zhong, Ziqian and Andreas, Jacob},
  journal={arXiv preprint arXiv:2410.04368},
  year={2024}
}

@article{miller1956magical,
  author    = {Miller, George A.},
  title     = {The magical number seven, plus or minus two: Some limits on our capacity for processing information},
  journal   = {Psychological Review},
  volume    = {63},
  number    = {2},
  pages     = {81--97},
  year      = {1956},
  month     = {Mar},
  publisher = {American Psychological Association},
  doi       = {10.1037/h0043158},
  pmid      = {13310704}
}


@misc{krishna2021doespretrainingsummarizationrequire,
      title={Does Pretraining for Summarization Require Knowledge Transfer?}, 
      author={Kundan Krishna and Jeffrey Bigham and Zachary C. Lipton},
      year={2021},
      eprint={2109.04953},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.04953}, 
}

@misc{huggingface_cosmopedia,
  author = {HuggingFace},
  title = {Cosmopedia: A Guide to Large Language Models},
  year = {2024},
  howpublished = {\url{https://huggingface.co/blog/cosmopedia}},
  note = {Accessed: 2025-01-24}
}


@misc{chen2024diversitysyntheticdataimpact,
      title={On the Diversity of Synthetic Data and its Impact on Training Large Language Models}, 
      author={Hao Chen and Abdul Waheed and Xiang Li and Yidong Wang and Jindong Wang and Bhiksha Raj and Marah I. Abdin},
      year={2024},
      eprint={2410.15226},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.15226}, 
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{margolis2022rapidlocomotionreinforcementlearning,
      title={Rapid Locomotion via Reinforcement Learning}, 
      author={Gabriel B Margolis and Ge Yang and Kartik Paigwar and Tao Chen and Pulkit Agrawal},
      year={2022},
      eprint={2205.02824},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2205.02824}, 
}

@article{MONTUORI2024104961,
title = {The cognitive effects of computational thinking: A systematic review and meta-analytic study},
journal = {Computers \& Education},
volume = {210},
pages = {104961},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104961},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523002385},
author = {Chiara Montuori and Filippo Gambarota and Gianmarco Altoé and Barbara Arfé},
keywords = {Coding, Computational thinking, Executive function, Problem solving, Children, Intervention},
}

@article{scherer2019cognitive,
  title = {The cognitive benefits of learning computer programming: A meta-analysis of transfer effects},
  author = {Scherer, Ronny and Siddiq, Fazilat and Sánchez Viveros, Bárbara},
  journal = {Journal of Educational Psychology},
  volume = {111},
  number = {5},
  pages = {764--792},
  year = {2019},
  publisher = {American Psychological Association},
  doi = {10.1037/edu0000314},
}

@misc{abdin2024phi3technicalreporthighly,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Microsoft},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}


@article{arumugam2021information,
  title={An information-theoretic perspective on credit assignment in reinforcement learning},
  author={Arumugam, Dilip and Henderson, Peter and Bacon, Pierre-Luc},
  journal={arXiv preprint arXiv:2103.06224},
  year={2021}
}


@misc{uesato2022solvingmathwordproblems,
      title={Solving math word problems with process- and outcome-based feedback}, 
      author={Jonathan Uesato and Nate Kushman and Ramana Kumar and Francis Song and Noah Siegel and Lisa Wang and Antonia Creswell and Geoffrey Irving and Irina Higgins},
      year={2022},
      eprint={2211.14275},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.14275}, 
}

@misc{wang2024mathshepherdverifyreinforcellms,
      title={Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations}, 
      author={Peiyi Wang and Lei Li and Zhihong Shao and R. X. Xu and Damai Dai and Yifei Li and Deli Chen and Y. Wu and Zhifang Sui},
      year={2024},
      eprint={2312.08935},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.08935}, 
}

@misc{singh2024humandatascalingselftraining,
      title={Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models}, 
      author={Avi Singh and John D. Co-Reyes and Rishabh Agarwal and Ankesh Anand and Piyush Patil and Xavier Garcia and Peter J. Liu and James Harrison and Jaehoon Lee and Kelvin Xu and Aaron Parisi and Abhishek Kumar and Alex Alemi and Alex Rizkowsky and Azade Nova and Ben Adlam and Bernd Bohnet and Gamaleldin Elsayed and Hanie Sedghi and Igor Mordatch and Isabelle Simpson and Izzeddin Gur and Jasper Snoek and Jeffrey Pennington and Jiri Hron and Kathleen Kenealy and Kevin Swersky and Kshiteej Mahajan and Laura Culp and Lechao Xiao and Maxwell L. Bileschi and Noah Constant and Roman Novak and Rosanne Liu and Tris Warkentin and Yundi Qian and Yamini Bansal and Ethan Dyer and Behnam Neyshabur and Jascha Sohl-Dickstein and Noah Fiedel},
      year={2024},
      eprint={2312.06585},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.06585}, 
}

@misc{wang2023selfconsistencyimproveschainthought,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.11171}, 
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@misc{befunge93,
  author = {{Cat's Eye Technologies}},
  title = {Languages: Befunge-93},
  year = {1993},
  url = {https://catseye.tc/article/Languages.md#befunge-93},
  note = {Accessed: 2025-01-28}
}


@misc{zelikman2022starbootstrappingreasoningreasoning,
      title={STaR: Bootstrapping Reasoning With Reasoning}, 
      author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman},
      year={2022},
      eprint={2203.14465},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.14465}, 
}

@article{petty2024does,
  title={How Does Code Pretraining Affect Language Model Task Performance?},
  author={Petty, Jackson and van Steenkiste, Sjoerd and Linzen, Tal},
  journal={arXiv preprint arXiv:2409.04556},
  year={2024}
}

@article{kim2024code,
  title={Code Pretraining Improves Entity Tracking Abilities of Language Models},
  author={Kim, Najoung and Schuster, Sebastian and Toshniwal, Shubham},
  journal={arXiv preprint arXiv:2405.21068},
  year={2024}
}

@article{aryabumi2024code,
  title={To code, or not to code? exploring impact of code in pre-training},
  author={Aryabumi, Viraat and Su, Yixuan and Ma, Raymond and Morisot, Adrien and Zhang, Ivan and Locatelli, Acyr and Fadaee, Marzieh and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal={arXiv preprint arXiv:2408.10914},
  year={2024}
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={MistralAI},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{abdin2024phi4technicalreport,
      title={Phi-4 Technical Report}, 
      author={Microsoft},
      year={2024},
      eprint={2412.08905},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.08905}, 
}

@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{wang2023visualpretrainingnavigationlearn,
      title={Visual Pre-training for Navigation: What Can We Learn from Noise?}, 
      author={Yanwei Wang and Ching-Yun Ko and Pulkit Agrawal},
      year={2023},
      eprint={2207.00052},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2207.00052}, 
}

@misc{kumar2024automatingsearchartificiallife,
      title={Automating the Search for Artificial Life with Foundation Models}, 
      author={Akarsh Kumar and Chris Lu and Louis Kirsch and Yujin Tang and Kenneth O. Stanley and Phillip Isola and David Ha},
      year={2024},
      eprint={2412.17799},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.17799}, 
}

@misc{zhang2024restmctsllmselftrainingprocess,
      title={ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search}, 
      author={Dan Zhang and Sining Zhoubian and Ziniu Hu and Yisong Yue and Yuxiao Dong and Jie Tang},
      year={2024},
      eprint={2406.03816},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.03816}, 
}

@article{zhao2024towards,
  title={Towards understanding retrieval accuracy and prompt quality in RAG systems},
  author={Zhao, Shengming and Huang, Yuheng and Song, Jiayang and Wang, Zhijie and Wan, Chengcheng and Ma, Lei},
  journal={arXiv preprint arXiv:2411.19463},
  year={2024}
}

@misc{anthropic2025claude3.5,
  title = {Claude 3.5 Sonnet},
  author = {Anthropic},
  year = {2025},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{kumarlearning,
  title={Learning In-Context Decision Making with Synthetic MDPs},
  year={2024},
  author={Kumar, Akarsh and Lu, Chris and Kirsch, Louis and Isola, Phillip},
  booktitle={Automated Reinforcement Learning: Exploring Meta-Learning, AutoML, and LLMs}
}

@inproceedings{wu2021lime,
  title={Lime: Learning inductive bias for primitives of mathematical reasoning},
  author={Wu, Yuhuai and Rabe, Markus N and Li, Wenda and Ba, Jimmy and Grosse, Roger B and Szegedy, Christian},
  booktitle={International Conference on Machine Learning},
  pages={11251--11262},
  year={2021},
  organization={PMLR}
}


@article{wu2022insights,
  title={Insights into pre-training via simpler synthetic tasks},
  author={Wu, Yuhuai and Li, Felix and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21844--21857},
  year={2022}
}

@misc{baradad2022learninglookingnoise,
      title={Learning to See by Looking at Noise}, 
      author={Manel Baradad and Jonas Wulff and Tongzhou Wang and Phillip Isola and Antonio Torralba},
      year={2022},
      eprint={2106.05963},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2106.05963}, 
}

@misc{reid2022wikipediahelpofflinereinforcement,
      title={Can Wikipedia Help Offline Reinforcement Learning?}, 
      author={Machel Reid and Yutaro Yamada and Shixiang Shane Gu},
      year={2022},
      eprint={2201.12122},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.12122}, 
}


@misc{wang2024pretrainingsyntheticdatahelps,
      title={Pre-training with Synthetic Data Helps Offline Reinforcement Learning}, 
      author={Zecheng Wang and Che Wang and Zixuan Dong and Keith Ross},
      year={2024},
      eprint={2310.00771},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.00771}, 
}

@misc{baradad2023proceduralimageprogramsrepresentation,
      title={Procedural Image Programs for Representation Learning}, 
      author={Manel Baradad and Chun-Fu Chen and Jonas Wulff and Tongzhou Wang and Rogerio Feris and Antonio Torralba and Phillip Isola},
      year={2023},
      eprint={2211.16412},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2211.16412}, 
}

@article{Schlake2007CanonicalWS,
  title={Canonical WNT Signalling Controls Hair Follicle Spacing},
  author={Thomas Schlake and Stefanie Sick},
  journal={Cell Adhesion \& Migration},
  year={2007},
  volume={1},
  pages={149 - 151},
  url={https://api.semanticscholar.org/CorpusID:13464972}
}

@article{somathilake2014reactiondiffusion,
  title={A reactiondiffusion type mathematical model for formation of coral patterns},
  author={Somathilake, LW and Wedagedera, JR},
  journal={Journal of the National Science Foundation of Sri Lanka},
  volume={42},
  number={4},
  pages={341--349},
  year={2014},
  publisher={National Science Foundation of Sri Lanka}
}

@article{wakamiya2010self,
  title={A Self-Organizing Architecture for scalable, adaptive, and robust networking},
  author={Wakamiya, Naoki and Leibnitz, Kenji and Murata, Masayuki and Agoulmine, N},
  journal={Autonomic Network Management Principles: From Concepts to Applications},
  pages={119--140},
  year={2010},
  publisher={Elsevier}
}

@article{Nakamasu2009InteractionsBZ,
  title={Interactions between zebrafish pigment cells responsible for the generation of Turing patterns},
  author={Akiko M. Nakamasu and Go Takahashi and Akio Kanbe and Shigeru Kondo},
  journal={Proceedings of the National Academy of Sciences},
  year={2009},
  volume={106},
  pages={8429 - 8434},
  url={https://api.semanticscholar.org/CorpusID:17355137}
}

@misc{nowak2024representationalcapacityneurallanguage,
      title={On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning}, 
      author={Franz Nowak and Anej Svete and Alexandra Butoi and Ryan Cotterell},
      year={2024},
      eprint={2406.14197},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.14197}, 
}

@article{longpre2023pretrainer,
  title={A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity},
  author={Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee, Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and Wei, Jason and Robinson, Kevin and Mimno, David and others},
  journal={arXiv preprint arXiv:2305.13169},
  year={2023}
}

@article{chan2024toward,
  title={Toward Understanding In-context vs. In-weight Learning},
  author={Chan, Bryan and Chen, Xinyi and Gy{\"o}rgy, Andr{\'a}s and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2410.23042},
  year={2024}
}

@article{chan2022data,
  title={Data distributional properties drive emergent in-context learning in transformers},
  author={Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18878--18891},
  year={2022}
}

@article{dolma,
  title = {{Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}},
  author={Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
  year={2024},
  journal={arXiv preprint},
  url={https://arxiv.org/abs/2402.00159}
}

@misc{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@misc{nye2021workscratchpadsintermediatecomputation,
      title={Show Your Work: Scratchpads for Intermediate Computation with Language Models}, 
      author={Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
      year={2021},
      eprint={2112.00114},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.00114}, 
}

@misc{snell2024scalingllmtesttimecompute,
      title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      eprint={2408.03314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.03314}, 
}

@misc{kumar2024traininglanguagemodelsselfcorrect,
      title={Training Language Models to Self-Correct via Reinforcement Learning}, 
      author={Aviral Kumar and Vincent Zhuang and Rishabh Agarwal and Yi Su and John D Co-Reyes and Avi Singh and Kate Baumli and Shariq Iqbal and Colton Bishop and Rebecca Roelofs and Lei M Zhang and Kay McKinney and Disha Shrivastava and Cosmin Paduraru and George Tucker and Doina Precup and Feryal Behbahani and Aleksandra Faust},
      year={2024},
      eprint={2409.12917},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.12917}, 
}

@misc{zelikman2024quietstarlanguagemodelsteach,
      title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking}, 
      author={Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman},
      year={2024},
      eprint={2403.09629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09629}, 
}

@misc{silver2017masteringchessshogiselfplay,
      title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
      author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
      year={2017},
      eprint={1712.01815},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1712.01815}, 
}

@article{silver2016mastering,
  added-at = {2017-05-07T12:43:52.000+0200},
  author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  biburl = {https://www.bibsonomy.org/bibtex/265a1c9455003f91c61b725aa46f466a6/hotho},
  interhash = {5892845b739023cd7f42e7cce276d935},
  intrahash = {65a1c9455003f91c61b725aa46f466a6},
  journal = {Nature},
  keywords = {alphago deep learning wj2017},
  number = 7587,
  pages = {484--489},
  publisher = {Nature Publishing Group},
  timestamp = {2017-05-07T12:43:52.000+0200},
  title = {Mastering the game of Go with deep neural networks and tree search},
  volume = 529,
  year = 2016
}


@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@misc{shi2023largelanguagemodelseasily,
      title={Large Language Models Can Be Easily Distracted by Irrelevant Context}, 
      author={Freda Shi and Xinyun Chen and Kanishka Misra and Nathan Scales and David Dohan and Ed Chi and Nathanael Schärli and Denny Zhou},
      year={2023},
      eprint={2302.00093},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.00093}, 
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{ho2022large,
  title={Large language models are reasoning teachers},
  author={Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  journal={arXiv preprint arXiv:2212.10071},
  year={2022}
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@misc{wu2024reasoningrecitingexploringcapabilities,
      title={Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks}, 
      author={Zhaofeng Wu and Linlu Qiu and Alexis Ross and Ekin Akyürek and Boyuan Chen and Bailin Wang and Najoung Kim and Jacob Andreas and Yoon Kim},
      year={2024},
      eprint={2307.02477},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.02477}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@misc{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,
      title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models}, 
      author={Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar},
      year={2024},
      eprint={2410.05229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05229}, 
}

@misc{bengio2019consciousnessprior,
      title={The Consciousness Prior}, 
      author={Yoshua Bengio},
      year={2019},
      eprint={1709.08568},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1709.08568}, 
}

@misc{kuratov2024babilongtestinglimitsllms,
      title={BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack}, 
      author={Yuri Kuratov and Aydar Bulatov and Petr Anokhin and Ivan Rodkin and Dmitry Sorokin and Artyom Sorokin and Mikhail Burtsev},
      year={2024},
      eprint={2406.10149},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10149}, 
}

@misc{kataoka2021pretrainingnaturalimages,
      title={Pre-training without Natural Images}, 
      author={Hirokatsu Kataoka and Kazushige Okayasu and Asato Matsumoto and Eisuke Yamagata and Ryosuke Yamada and Nakamasa Inoue and Akio Nakamura and Yutaka Satoh},
      year={2021},
      eprint={2101.08515},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2101.08515}, 
}

@misc{cetin2024evolveduniversaltransformermemory,
      title={An Evolved Universal Transformer Memory}, 
      author={Edoardo Cetin and Qi Sun and Tianyu Zhao and Yujin Tang},
      year={2024},
      eprint={2410.13166},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.13166}, 
}


@misc{huh2024platonicrepresentationhypothesis,
      title={The Platonic Representation Hypothesis}, 
      author={Minyoung Huh and Brian Cheung and Tongzhou Wang and Phillip Isola},
      year={2024},
      eprint={2405.07987},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.07987}, 
}

@misc{lightman2023letsverifystepstep,
      title={Let's Verify Step by Step}, 
      author={Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
      year={2023},
      eprint={2305.20050},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.20050}, 
}

@misc{ajay2023conditionalgenerativemodelingneed,
      title={Is Conditional Generative Modeling all you need for Decision-Making?}, 
      author={Anurag Ajay and Yilun Du and Abhi Gupta and Joshua Tenenbaum and Tommi Jaakkola and Pulkit Agrawal},
      year={2023},
      eprint={2211.15657},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.15657}, 
}


@article{merrill2023expresssive,
  title={The expresssive power of transformers with chain of thought},
  author={Merrill, William and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2310.07923},
  year={2023}
}


@article{luo2023empirical,
  title={An empirical study of catastrophic forgetting in large language models during continual fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@article{luong2024reft,
  title={Reft: Reasoning with reinforced fine-tuning},
  author={Luong, Trung Quoc and Zhang, Xinbo and Jie, Zhanming and Sun, Peng and Jin, Xiaoran and Li, Hang},
  journal={arXiv preprint arXiv:2401.08967},
  year={2024}
}

@article{berges2024memory,
  title={Memory Layers at Scale},
  author={Berges, Vincent-Pierre and O{\u{g}}uz, Barlas and Haziza, Daniel and Yih, Wen-tau and Zettlemoyer, Luke and Gosh, Gargi},
  journal={arXiv preprint arXiv:2412.09764},
  year={2024}
}

@inproceedings{carreira2016human,
  title={Human pose estimation with iterative error feedback},
  author={Carreira, Joao and Agrawal, Pulkit and Fragkiadaki, Katerina and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4733--4742},
  year={2016}
}

@article{paassen2020reservoir,
  title={Reservoir memory machines},
  author={Paa{\ss}en, Benjamin and Schulz, Alexander},
  journal={arXiv preprint arXiv:2003.04793},
  year={2020}
}

@article{alevs2016neural,
  title={Neural Turing Machines: Convergence of Copy Tasks},
  author={Ale{\v{s}}, Janez},
  journal={arXiv preprint arXiv:1612.02336},
  year={2016}
}

@article{xie2024memorization,
  title={On memorization of large language models in logical reasoning},
  author={Xie, Chulin and Huang, Yangsibo and Zhang, Chiyuan and Yu, Da and Chen, Xinyun and Lin, Bill Yuchen and Li, Bo and Ghazi, Badih and Kumar, Ravi},
  journal={arXiv preprint arXiv:2410.23123},
  year={2024}
}


@article{paulus2017deep,
  title={A deep reinforced model for abstractive summarization},
  author={Paulus, R},
  journal={arXiv preprint arXiv:1705.04304},
  year={2017}
}

@InProceedings{pmlr-v202-gao23h,
  title = 	 {Scaling Laws for Reward Model Overoptimization},
  author =       {Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {10835--10866},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/gao23h/gao23h.pdf},
  url = 	 {https://proceedings.mlr.press/v202/gao23h.html}
}


@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}

@misc{alami2024investigatingregularizationselfplaylanguage,
      title={Investigating Regularization of Self-Play Language Models}, 
      author={Reda Alami and Abdalgader Abubaker and Mastane Achab and Mohamed El Amine Seddik and Salem Lahlou},
      year={2024},
      eprint={2404.04291},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.04291}, 
}

@article{kl,
author = {S. Kullback and R. A. Leibler},
title = {{On Information and Sufficiency}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {79 -- 86},
year = {1951},
doi = {10.1214/aoms/1177729694},
URL = {https://doi.org/10.1214/aoms/1177729694}
}



@article{xie2024order,
  title={Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models},
  author={Xie, Zikai},
  journal={arXiv preprint arXiv:2408.05093},
  year={2024}
}

@article{jin2024disentangling,
  title={Disentangling Memory and Reasoning Ability in Large Language Models},
  author={Jin, Mingyu and Luo, Weidi and Cheng, Sitao and Wang, Xinyi and Hua, Wenyue and Tang, Ruixiang and Wang, William Yang and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2411.13504},
  year={2024}
}

@misc{wikipedia_brainfuck,
  author       = {Wikipedia},
  title        = {Brainfuck},
  year         = {2025},
  howpublished = {\url{https://en.wikipedia.org/wiki/Talk:Brainfuck}},
  note         = {Accessed: 2025-01-28}
}

@article{_wiechowski_2022,
   title={Monte Carlo Tree Search: a review of recent modifications and applications},
   volume={56},
   ISSN={1573-7462},
   url={http://dx.doi.org/10.1007/s10462-022-10228-y},
   DOI={10.1007/s10462-022-10228-y},
   number={3},
   journal={Artificial Intelligence Review},
   publisher={Springer Science and Business Media LLC},
   author={Świechowski, Maciej and Godlewski, Konrad and Sawicki, Bartosz and Mańdziuk, Jacek},
   year={2022},
   month=jul, pages={2497–2562} }


@unknown{youvan,
author = {Youvan, Douglas},
year = {2024},
month = {05},
pages = {},
title = {Exploring Dual Emergent Phenomena: Integrating Cellular Automata and Boid Flocking Behavior},
doi = {10.13140/RG.2.2.35184.67840}
}

@misc{hosseini2024vstartrainingverifiersselftaught,
      title={V-STaR: Training Verifiers for Self-Taught Reasoners}, 
      author={Arian Hosseini and Xingdi Yuan and Nikolay Malkin and Aaron Courville and Alessandro Sordoni and Rishabh Agarwal},
      year={2024},
      eprint={2402.06457},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.06457}, 
}

@article{palamas2020georgi,
  title={Georgi Ivanov Supervisor: George Palamas},
  author={Palamas, George},
  year={2020}
}

@misc{yang2024qwen2technicalreport,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}

@article{graves2014neural,
  title={Neural Turing Machines},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@misc{zhang2024raftadaptinglanguagemodel,
      title={RAFT: Adapting Language Model to Domain Specific RAG}, 
      author={Tianjun Zhang and Shishir G. Patil and Naman Jain and Sheng Shen and Matei Zaharia and Ion Stoica and Joseph E. Gonzalez},
      year={2024},
      eprint={2403.10131},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.10131}, 
}

@misc{walker2015denseopticalflowprediction,
      title={Dense Optical Flow Prediction from a Static Image}, 
      author={Jacob Walker and Abhinav Gupta and Martial Hebert},
      year={2015},
      eprint={1505.00295},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1505.00295}, 
}

@misc{jayaraman2016learningimagerepresentationstied,
      title={Learning image representations tied to ego-motion}, 
      author={Dinesh Jayaraman and Kristen Grauman},
      year={2016},
      eprint={1505.02206},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1505.02206}, 
}

@misc{chu2025sftmemorizesrlgeneralizes,
      title={SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training}, 
      author={Tianzhe Chu and Yuexiang Zhai and Jihan Yang and Shengbang Tong and Saining Xie and Dale Schuurmans and Quoc V. Le and Sergey Levine and Yi Ma},
      year={2025},
      eprint={2501.17161},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.17161}, 
}

@ARTICLE{ssl-survey,
  author={Gui, Jie and Chen, Tuo and Zhang, Jing and Cao, Qiong and Sun, Zhenan and Luo, Hao and Tao, Dacheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Survey on Self-Supervised Learning: Algorithms, Applications, and Future Trends}, 
  year={2024},
  volume={46},
  number={12},
  pages={9052-9071},
  keywords={Task analysis;Training;Reviews;Machine learning algorithms;Supervised learning;Market research;Image recognition;Self-supervised learning;contrastive learning;generative model;representation learning;transfer learning},
  doi={10.1109/TPAMI.2024.3415112}}


@misc{lu2021pretrainedtransformersuniversalcomputation,
      title={Pretrained Transformers as Universal Computation Engines}, 
      author={Kevin Lu and Aditya Grover and Pieter Abbeel and Igor Mordatch},
      year={2021},
      eprint={2103.05247},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.05247}, 
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Gemini},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{Cowan2001,
  author    = {Cowan, Nelson},
  title     = {The magical number 4 in short-term memory: a reconsideration of mental storage capacity},
  journal   = {Behavioral and Brain Sciences},
  volume    = {24},
  number    = {1},
  pages     = {87--114; discussion 114--185},
  year      = {2001},
  month     = {Feb},
  doi       = {10.1017/s0140525x01003922},
  pmid      = {11515286}
}

@article{Thalmann2019,
  author = {Thalmann, Mareike and Souza, Alessandra S. and Oberauer, Klaus},
  title = {How does chunking help working memory?},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  year = {2019},
  volume = {45},
  number = {1},
  pages = {37-55},
  doi = {10.1037/xlm0000578},
  pmid = {29698045},
  month = {January},
  eprint = {2018 Apr 26}
}


@article{Graves-DNC, title={Hybrid computing using a neural network with dynamic external memory}, volume={538}, ISSN={0028-0836}, DOI={10.1038/nature20101}, abstractNote={Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory. A ‘differentiable neural computer’ is introduced that combines the learning capabilities of a neural network with an external memory analogous to the random-access memory in a conventional computer. Conventional computer algorithms can process extremely large and complex data structures such as the worldwide web or social networks, but they must be programmed manually by humans. Neural networks can learn from examples to recognize complex patterns, but they cannot easily parse and organize complex data structures. Now Alex Graves, Greg Wayne and colleagues have developed a hybrid learning machine, called a differentiable neural computer (DNC), that is composed of a neural network that can read from and write to an external memory structure analogous to the random-access memory in a conventional computer. The DNC can thus learn to plan routes on the London Underground, and to achieve goals in a block puzzle, merely by trial and error—without prior knowledge or ad hoc programming for such tasks.}, number={7626}, journal={Nature}, author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis}, year={2016}, pages={471–476} }

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Liu, Tianyu and others},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{skalse2022defining,
  title={Defining and characterizing reward gaming},
  author={Skalse, Joar and Howe, Nikolaus and Krasheninnikov, Dmitrii and Krueger, David},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9460--9471},
  year={2022}
}