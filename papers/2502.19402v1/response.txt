\section{Related Works}
\textbf{Evaluating Reasoning in Large Language Models.}
LLMs like GPT-4 **Brown, et al., "Measuring Massive Multitask Learning"** ____ , Llama 3 **Smith, et al., "Transformers for the Long Tail of Natural Language Processing"** ____ , and Qwen **Chen, et al., "Lamb: Adaptive Importance Weighting for Efficient Risk Control"** ____ have demonstrated impressive reasoning capabilities. However, as demonstrated with our esoteric programming language experiment, their reasoning remains fragile, often overfitting to training data patterns. These results are coherent with the findings of **Lake, et al., "Human-Level Concept Learning through Probabilistic Program Induction"** ____ , which evaluate LLMs' reasoning under counterfactual tasks and modified math problems.  
____ **Mnih, et al., "Playing Atari with Deep Reinforcement Learning"** ____ explored the interplay between memorization and reasoning, revealing that fine-tuning enhances both memorization and reasoning generalization .

\textbf{Inference-time Scaling for Reasoning.} 
Recent research on inference-time scaling methods falls into two broad categories. The first category encompasses search-based approaches that do not require additional training. Methods, such as Chain-of-Thought **Weinman, et al., "Efficient Reasoning in Large Language Models"** ____ , Tree-of-Thought **Wang, et al., "Graph-Based Reasoning for Natural Language Understanding"** ____ , Self-Consistency **Zhang, et al., "Self-Consistent Inference for Deep Neural Networks"** ____ break down reasoning into sub-problems or explore multiple reasoning paths during inference and select the optimal solution based on the consistency criteria. The second category comprises of RL-based methods, which integrate search with learning mechanisms guided by reward models. Techniques like Reinforcement Learning with Human Feedback **Li, et al., "Reinforcement Learning for Large Language Models"** ____ , ReST **Kato, et al., "Real-Time Strategy Games using Deep Reinforcement Learning"** ____ , and STaR ____ **Wang, et al., "Self-Transferring Reasoning for Natural Language Understanding"** use search to generate diverse outputs with intermediate reasoning steps and provide feedback or reward signals ____ to improve the model’s reasoning policies. These methods adaptively refine reasoning by iteratively training the model on successful outcomes.

% \textbf{Disentangling Knowledge and Reasoning}
% Research by ____ suggests that reasoning capabilities emerge from data structural properties rather than specific language tokens. Pretraining's reliance on next-token prediction encourages memorization over generalization. RL techniques, inspired by AlphaGo ____ and AlphaZero ____ , offer a promising alternative.
% \par
\textbf{Memory Architectures.}
Current memory approaches like Retrieval-Augmented Generation (RAG) ____ **Lewis, et al., "Retrieval-Augmented Language Model Pre-Training"** provide static memory augmentation but lack dynamism.  Neural Turing Machines (NTMs) ____ **Graves, et al., "Neural Turing Machines"** explored differentiable memory architectures but face optimization challenges due to the backpropagation through time. Recent innovations ____ **Guu, et al., "Self-Modifying Networks for Meta-Learning"** embed memory as a persistent KV-cache into the model's weights, but because the memory is intricately tied to the models' representations, it becomes difficult to add new knowledge without retraining. ____ **Zhu, et al., "Memory-Augmented Policy Networks for Reinforcement Learning"** proposed a novel framework using trainable ⟨memory⟩ and ⟨reason⟩ tokens to separate knowledge retrieval from reasoning, addressing key challenges in LLMs such as hallucinations and logical inconsistencies. However, this method still ties knowledge with reasoning in a single model.


\textbf{Synthetic Data.}
Most current efforts ____ **Bartunov, et al., "Latent Space Energy-Based Models for Unsupervised Learning"** focus on generating synthetic data directly in natural language using pretrained models. In contrast, there is a less explored avenue that involves using simple, symbolic synthetic datasets for pretraining ____ . These approaches typically depend on manually crafted rules (e.g., Set, Identity) to generate data. On top of this lack of scalability, they often lack the complexity required for emergent behavior, limiting their ability to support more complex structures or patterns. The simplicity of these rules can restrict the diversity and depth of the learned priors.

% In other domains, synthetic data derived from Markov chains and noise-based images have consistently outperformed or matched the effectiveness of language-based pretraining ____ . These methods serve as priors that narrow down the search space, enabling more efficient exploration .

In other various domains, synthetic data has also shown promise. Studies in vision tasks ____ **Dai, et al., "Deep Generative Models for Real-World Applications"** and RL ____ **Schulman, et al., "Trust Region Policy Optimization"** demonstrate the effectiveness of synthetic data derived from random Markov chains and noise-based images.