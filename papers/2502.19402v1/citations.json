[
  {
    "index": 0,
    "papers": [
      {
        "key": "openai2024gpt4technicalreport",
        "author": "OpenAI",
        "title": "GPT-4 Technical Report"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "grattafiori2024llama3herdmodels",
        "author": "Meta",
        "title": "The Llama 3 Herd of Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "deepseekai2024deepseekv3technicalreport",
        "author": "DeepSeek-AI",
        "title": "DeepSeek-V3 Technical Report"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical",
        "author": "Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar",
        "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"
      },
      {
        "key": "wu2024reasoningrecitingexploringcapabilities",
        "author": "Zhaofeng Wu and Linlu Qiu and Alexis Ross and Ekin Aky\u00fcrek and Boyuan Chen and Bailin Wang and Najoung Kim and Jacob Andreas and Yoon Kim",
        "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xie2024memorization",
        "author": "Xie, Chulin and Huang, Yangsibo and Zhang, Chiyuan and Yu, Da and Chen, Xinyun and Lin, Bill Yuchen and Li, Bo and Ghazi, Badih and Kumar, Ravi",
        "title": "On memorization of large language models in logical reasoning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wei2023chainofthoughtpromptingelicitsreasoning",
        "author": "Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2023selfconsistencyimproveschainthought",
        "author": "Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ouyang2022traininglanguagemodelsfollow",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "singh2024humandatascalingselftraining",
        "author": "Avi Singh and John D. Co-Reyes and Rishabh Agarwal and Ankesh Anand and Piyush Patil and Xavier Garcia and Peter J. Liu and James Harrison and Jaehoon Lee and Kelvin Xu and Aaron Parisi and Abhishek Kumar and Alex Alemi and Alex Rizkowsky and Azade Nova and Ben Adlam and Bernd Bohnet and Gamaleldin Elsayed and Hanie Sedghi and Igor Mordatch and Isabelle Simpson and Izzeddin Gur and Jasper Snoek and Jeffrey Pennington and Jiri Hron and Kathleen Kenealy and Kevin Swersky and Kshiteej Mahajan and Laura Culp and Lechao Xiao and Maxwell L. Bileschi and Noah Constant and Roman Novak and Rosanne Liu and Tris Warkentin and Yundi Qian and Yamini Bansal and Ethan Dyer and Behnam Neyshabur and Jascha Sohl-Dickstein and Noah Fiedel",
        "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zelikman2022starbootstrappingreasoningreasoning",
        "author": "Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman",
        "title": "STaR: Bootstrapping Reasoning With Reasoning"
      },
      {
        "key": "zelikman2024quietstarlanguagemodelsteach",
        "author": "Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman",
        "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lightman2023letsverifystepstep",
        "author": "Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe",
        "title": "Let's Verify Step by Step"
      },
      {
        "key": "wang2024mathshepherdverifyreinforcellms",
        "author": "Peiyi Wang and Lei Li and Zhihong Shao and R. X. Xu and Damai Dai and Yifei Li and Deli Chen and Y. Wu and Zhifang Sui",
        "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"
      },
      {
        "key": "uesato2022solvingmathwordproblems",
        "author": "Jonathan Uesato and Nate Kushman and Ramana Kumar and Francis Song and Noah Siegel and Lisa Wang and Antonia Creswell and Geoffrey Irving and Irina Higgins",
        "title": "Solving math word problems with process- and outcome-based feedback"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chan2022data",
        "author": "Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix",
        "title": "Data distributional properties drive emergent in-context learning in transformers"
      },
      {
        "key": "chan2024toward",
        "author": "Chan, Bryan and Chen, Xinyi and Gy{\\\"o}rgy, Andr{\\'a}s and Schuurmans, Dale",
        "title": "Toward Understanding In-context vs. In-weight Learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "silver2016mastering",
        "author": "Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others",
        "title": "Mastering the game of Go with deep neural networks and tree search"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "silver2017masteringchessshogiselfplay",
        "author": "David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis",
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "berges2024memory",
        "author": "Berges, Vincent-Pierre and O{\\u{g}}uz, Barlas and Haziza, Daniel and Yih, Wen-tau and Zettlemoyer, Luke and Gosh, Gargi",
        "title": "Memory Layers at Scale"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "alevs2016neural",
        "author": "Ale{\\v{s}}, Janez",
        "title": "Neural Turing Machines: Convergence of Copy Tasks"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "berges2024memory",
        "author": "Berges, Vincent-Pierre and O{\\u{g}}uz, Barlas and Haziza, Daniel and Yih, Wen-tau and Zettlemoyer, Luke and Gosh, Gargi",
        "title": "Memory Layers at Scale"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "jin2024disentangling",
        "author": "Jin, Mingyu and Luo, Weidi and Cheng, Sitao and Wang, Xinyi and Hua, Wenyue and Tang, Ruixiang and Wang, William Yang and Zhang, Yongfeng",
        "title": "Disentangling Memory and Reasoning Ability in Large Language Models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "chen2024diversitysyntheticdataimpact",
        "author": "Hao Chen and Abdul Waheed and Xiang Li and Yidong Wang and Jindong Wang and Bhiksha Raj and Marah I. Abdin",
        "title": "On the Diversity of Synthetic Data and its Impact on Training Large Language Models"
      },
      {
        "key": "huggingface_cosmopedia",
        "author": "HuggingFace",
        "title": "Cosmopedia: A Guide to Large Language Models"
      },
      {
        "key": "abdin2024phi4technicalreport",
        "author": "Microsoft",
        "title": "Phi-4 Technical Report"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "wu2022insights",
        "author": "Wu, Yuhuai and Li, Felix and Liang, Percy S",
        "title": "Insights into pre-training via simpler synthetic tasks"
      },
      {
        "key": "wu2021lime",
        "author": "Wu, Yuhuai and Rabe, Markus N and Li, Wenda and Ba, Jimmy and Grosse, Roger B and Szegedy, Christian",
        "title": "Lime: Learning inductive bias for primitives of mathematical reasoning"
      },
      {
        "key": "krishna2021doespretrainingsummarizationrequire",
        "author": "Kundan Krishna and Jeffrey Bigham and Zachary C. Lipton",
        "title": "Does Pretraining for Summarization Require Knowledge Transfer?"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "wang2023visualpretrainingnavigationlearn",
        "author": "Yanwei Wang and Ching-Yun Ko and Pulkit Agrawal",
        "title": "Visual Pre-training for Navigation: What Can We Learn from Noise?"
      },
      {
        "key": "wang2024pretrainingsyntheticdatahelps",
        "author": "Zecheng Wang and Che Wang and Zixuan Dong and Keith Ross",
        "title": "Pre-training with Synthetic Data Helps Offline Reinforcement Learning"
      },
      {
        "key": "baradad2022learninglookingnoise",
        "author": "Manel Baradad and Jonas Wulff and Tongzhou Wang and Phillip Isola and Antonio Torralba",
        "title": "Learning to See by Looking at Noise"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "wang2023visualpretrainingnavigationlearn",
        "author": "Yanwei Wang and Ching-Yun Ko and Pulkit Agrawal",
        "title": "Visual Pre-training for Navigation: What Can We Learn from Noise?"
      },
      {
        "key": "baradad2022learninglookingnoise",
        "author": "Manel Baradad and Jonas Wulff and Tongzhou Wang and Phillip Isola and Antonio Torralba",
        "title": "Learning to See by Looking at Noise"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "baradad2023proceduralimageprogramsrepresentation",
        "author": "Manel Baradad and Chun-Fu Chen and Jonas Wulff and Tongzhou Wang and Rogerio Feris and Antonio Torralba and Phillip Isola",
        "title": "Procedural Image Programs for Representation Learning"
      },
      {
        "key": "wang2024pretrainingsyntheticdatahelps",
        "author": "Zecheng Wang and Che Wang and Zixuan Dong and Keith Ross",
        "title": "Pre-training with Synthetic Data Helps Offline Reinforcement Learning"
      }
    ]
  }
]