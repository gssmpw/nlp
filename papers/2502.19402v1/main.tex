%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{tcolorbox}
\usepackage{verbatim}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{listings} % Add this to the preamble

% Define JSON style
\lstdefinelanguage{JSON}{
    basicstyle=\ttfamily\small,
    numbers=none,
    stepnumber=1,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color[gray]{0.95}, literate={@}{{@}}1,
    morestring=[b]",
    stringstyle=\color{blue},
    moredelim=**[is][\color{red}]{@}{@}
}

\lstdefinestyle{json}{
  basicstyle=\ttfamily\small,
  numbers=none,
  showstringspaces=false,
  breaklines=true,
  frame=single,
}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\providecommand{\sw}[1]{\ifShowComments{\noindent \color{purple}\textbf{SW:} { #1 }}\fi}
\newif\ifShowComments
\ShowCommentstrue

\providecommand{\sg}[1]{\ifShowComments{\noindent \color{cyan}\textbf{SG:} { #1 }}\fi}
\newif\ifShowComments
\ShowCommentstrue

\providecommand{\pulkit}[1]{\ifShowComments{\noindent \color{orange}\textbf{Pulkit:} { #1 }}\fi}
\providecommand{\idan}[1]{\ifShowComments{\noindent \color{purple}\textbf{Idan:} { #1 }}\fi}
\providecommand{\jyo}[1]{\ifShowComments{\noindent \color{blue}\textbf{Jyo:} { #1 }}\fi}
\providecommand{\nitish}[1]{\ifShowComments{\noindent \color{teal}\textbf{Nitish:} { #1 }}\fi}
\newcommand{\agi}{AGI}
\newcommand{\aui}{AUI}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{General Reasoning Requires Learning to Reason from the Get-go}

\begin{document}

\twocolumn[
%\icmltitle{Position: Where are we at with AGI?}
%\icmltitle{Position: General Reasoning Requires Search From the Get-go}
\icmltitle{General Reasoning Requires Learning to Reason from the Get-go}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Seungwook Han}{equal,yyy}
\icmlauthor{Jyothish Pari}{equal,yyy}
\icmlauthor{Samuel J. Gershman}{zzz}
\icmlauthor{Pulkit Agrawal}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Improbable AI Lab, MIT}
\icmlaffiliation{zzz}{Department of Psychology and Center for Brain Science, Harvard University}

\icmlcorrespondingauthor{Seungwook Han}{swhan@mit.edu}
\icmlcorrespondingauthor{Jyothish Pari}{jyopari@mit.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs. 

To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a \textit{reasoning prior} for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios. 
\end{abstract}

\section{Introduction}
\label{sec:intro}
Large Language Models (LLMs) have demonstrated impressive capabilities across diverse tasks, such as commonsense reasoning, math, and programming \citep{ho2022large, wei2023chainofthoughtpromptingelicitsreasoning, wei2022emergent, lampinen2022can, deepseekai2024deepseekv3technicalreport, openai2024gpt4technicalreport, grattafiori2024llama3herdmodels}. Their practical impact makes them a compelling instance of \textit{artificial useful intelligence} (\aui) -- systems that assist humans in real-world tasks. Nevertheless, a significant gap remains between \aui{} and artificial general intelligence (\agi), systems capable of robust, adaptive reasoning across diverse domains and contexts like humans.

To illustrate this gap, we designed algorithmic tasks in esoteric programming languages that isolate reasoning from memorization. These tasks involve simple algorithmic problems (e.g., printing, sorting) seen during pretraining and easily solved in Python and Java, but presented in unfamiliar programming languages with different syntaxes. Our results in Section~\ref{sec:eval-reasoning-knowledge} show that state-of-the-art LLMs struggle to transfer their algorithmic understanding to coding in new programming syntaxes. Notably, o1 \citep{openai_o1}, post-trained for reasoning with a combination reinforcement learning (RL) and search, e.g., Chain-of-Thought \citep{wei2023chainofthoughtpromptingelicitsreasoning}, performed the best. However, even o1's performance suffers, illustrating the limitation of current models in flexibly transferring their reasoning to novel contexts. \looseness=-1

We hypothesize that going beyond the reasoning ability of current models requires a fundamental overhaul of the pretraining paradigm. The dominant approach -- supervised pretraining with next-token prediction loss on passively collected Internet data, followed by RL-based post-training \citep{deepseekai2025deepseekr1incentivizingreasoningcapability, lightman2023letsverifystepstep, luong2024reft, zelikman2024quietstarlanguagemodelsteach, zelikman2022starbootstrappingreasoningreasoning} -- closely mirrors AlphaGo’s methodology in the game of Go ~\citep{silver2016mastering}. In AlphaGo, supervised pretraining on human demonstrations and post-RL optimization were used to surpass human playing abilities. However, this paradigm was overturned by AlphaZero \citep{silver2017masteringchessshogiselfplay}, which demonstrated that RL from scratch -- without supervised pretraining -- achieved superior performance. By relying solely on self-play and starting from random initialization, AlphaZero surpassed AlphaGo, uncovering more efficient and creative strategies through purely RL.

This paradigm shift from AlphaGo to AlphaZero motivates our hypothesis that, even in current LLMs, reliance on supervised pretraining constrains models to a “local minimum” of reasoning capabilities. Supervised pretraining on demonstration data can be a double-edged sword: while it provides a helpful exploration bias for reasoning finetuning via RL, it also serves as a bias that may hinder subsequent exploration. Consequently, we hypothesize that next-token prediction training on Internet-scale data may constrain models to a local minimum of reasoning abilities that cannot easily be escaped through post-training RL, similar to how RL fine-tuning in AlphaGo remained inferior to Go gameplay learned with RL from scratch in AlphaZero.

Our intuition behind why next-token prediction might lead to a local minimum in reasoning is as follows: when the training objective is merely to predict the next token, the model can exploit correlations between tokens in the context window to reduce prediction error instead of learning the underlying reasoning algorithm that explains the data. 
%By latching onto these patterns in the data -- patterns tied closely to surface-level syntactic arrangements (like learning that equals signs are followed by numerical results) rather than deeper logical relationships -- the model can achieve decent performance without developing true reasoning capabilities. Its knowledge becomes tightly bound to specific phrasings and patterns seen in training data rather than building a deeper understanding. 
For instance, if the model is trained on examples such as $5 + 3 = 8; 2 + 1 = 3; ...; 4 + 7 = 11$, it might learn that the next character after the character $+$ and a \textit{number} is the character $=$. 
In other words, the model can learn superficial statistical patterns (like which words commonly occur together) rather than developing genuine understanding (e.g., of addition) that will generalize across different situations. 
%Another example is that a model might learn to complete math problems presented in a familiar format but fail when the same concepts are presented differently. When knowledge is learned this way - \textit{tied to specific ways of expressing things rather than underlying principles} - the model struggles to apply what it knows to new or unfamiliar scenarios. Such tight coupling between how knowledge is expressed and the reasoning process may explain why current LLMs often fail to reason consistently across different contexts.

% Our intuition behind why next-token prediction might lead to a local minimum in reasoning is as follows: the next-token prediction error can be reduced by exploiting the correlations between previous and future tokens instead of prioritizing the discovery of the underlying reasoning algorithm explaining the data. Exploiting correlations between tokens in the context window to accurately predict the next token can result in a model with reasoning abilities closely tied to the syntax or data (i.e., knowledge) on which it was trained. \textbf{Such coupling between knowledge and reasoning may be the root cause of the fact that the reasoning of current LLMs is fragile and doesn't flexibly transfer to new contexts.} 

As another example, consider instructing a model to write Python code with 1-based indexing instead of the usual 0-based indexing. To solve the task with the simple change, the model must override its memorized Python knowledge and flexibly apply the 1-based indexing rules learned from other languages like R or MATLAB. However, many models pretrained with next-token prediction produce 0-indexing based solutions, leaning on surface-level pattern matching that ignores the instruction \citep{wu2024reasoningrecitingexploringcapabilities}. This illustrates how passive pretraining fails to incentivize the learning of generalizable reasoning skills and, instead, reinforces memorization and pattern matching that have limited generalization beyond the training scenarios. When a model learns to reason in a way that is  \textit{tied to specific ways of expressing things rather than underlying principles} - the model struggles to apply its reasoning to new scenarios.

\textbf{In this position paper, we argue that 
%the local minimum in current LLMs reasoning is fundamentally rooted in the entanglement of knowledge and reasoning during next-token prediction. 
to advance from \aui{} to \agi{}, it is critical to deliberately disentangle knowledge from reasoning, allowing models to develop robust reasoning independent of memorized patterns. Additionally, we propose architectural modifications that enable the reasoning system to flexibly adapt to newly added knowledge, ensuring that models can generalize their reasoning strategies to novel domains. To this end, we outline three key directions to achieve this paradigm shift, which we discuss in detail in \cref{sec:proposed_directions}.}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/main_figv4.pdf}
    \caption{(Left) Comparing reward-based pretraining (RPT) and traditional supervised pertaining (SPT). Supervised pretraining requires the model to directly predict answers, limiting its ability to refine intermediate solutions. In contrast, RPT enables step-by-step reasoning from the outset, allowing the model to iteratively approximate complex functions through simpler transitions that are easier to learn and more robust to errors~\cite{carreira2016human}. (Right) Illustration of our proposed decoupled memory-reasoning architecture. This design allows the reasoning module to operate on shorter context windows that reduce the chances of learning spurious correlations and, thereby, more transferrable reasoning. The use of a shorter context window also encourages the model to learn how to dynamically read and write to memory, which facilitates the use of reasoning model on new problems and knowledge domains.}%Unlike current systems, which struggle to reason over new knowledge without retraining, our approach enables continual reasoning adaptation.}
    \label{fig:main-diagram}
\end{figure*}

\begin{enumerate}
 \item \textbf{Pretraining for reasoning via reward-based learning (Section~\ref{sec:rl-pretraining}):}
 The choice of training data can significantly influence whether the model exploits correlations to learn fragile reasoning or discovers the more general underlying reasoning algorithm. As an illustration, consider training a model to multiply numbers with data of the form $[2 \times 5 = 10; 3 \times 4 = 12; ... \,\,5 \times 7 = 35]$, where the answers are directly provided. We refer to such data as \textit{passive data} that is the outcome of human knowledge and reasoning (i.e., most of the Internet data) but doesn't encompass the reasoning process. Continuing with the analogy of AlphaGo, human gameplay data used for supervised training is \textit{passive} in the sense that it contains \textit{what} moves humans played but \textit{not why} those moves were played. While one hopes that a model can implicitly learn the true reasoning process if trained with the next-token prediction objective, the model could memorize the answers or exploit correlations in the data to learn reasoning that works for in-distribution data, but fails beyond the training data distribution. 
 
 Consider the alternative of training model on data that spells out the reasoning algorithm (i.e., \textit{reasoning trace}) -- $[2 \times 5 = 5 + 5 = 10; 3 \times 4 = 4 + (4 + 4) = 12; ... \,\,5 \times 7 = (7 + 7) + (7 + 7) + 7 = 35]$. We hypothesize that the model has a better chance of learning the underlying algorithm with such data. However, obtaining large amounts of \textit{reasoning trace} data is challenging. One way is to use RL to generate data that maximizes the task reward (i.e., correct multiplication or winning the game as in AlphaZero). In other words, by structuring training as an iterative~\cite{carreira2016human} and step-wise problem-solving process rather than next-token prediction on passive data, models can develop more robust and generalizable reasoning abilities not limited to the spurious patterns observed in pretraining data.

    %Similar to how AlphaZero surpassed AlphaGo by training with pure RL, we propose integrating RL directly into language model pretraining rather than relying on the supervised pretraining-then-RL paradigm. Instead of simply predicting the next token on passive data, models can actively explore and generate their own reasoning traces, addressing the gap left by large-scale passive data. 
    

    \item \textbf{Enabling Efficient Exploration via Synthetic Tasks (Section~\ref{sec:artificial_envs}):} 
    Building on the insight that data containing reasoning traces may better enable the model to learn the underlying reasoning process compared to \emph{passive} data, the natural question is how to obtain such data. We propose that this data can be obtained via a more active process where an agent optimizes task rewards through a sequence of actions and generates reasoning traces in the process.  
    %a more active process  obtaining this by training our models with RL from scratch, we now confront a key challenge: how can we reliably explore a massive, 40k-token action space? 
    However, as natural language comprises of $\sim$40K tokens, doing RL from scratch is infeasible.  
    To address the exploration challenge, we propose using synthetic tasks with a reduced token space while preserving key structural properties to learn a \textit{reasoning prior} (e.g., commutativity, associativity, in-context learning, etc.). The token space and complexity can gradually be increased via a task curriculum that increases difficulty. Once the reasoning prior is learned, it can leveraged to bootstrap reasoning in natural language.
        
\item \textbf{Architectural bias to decouple knowledge and reasoning (Section~\ref{sec:decouple-knowledge-reasoning}):}  
Training models using long context windows provide more chances for the model to learn \emph{spurious correlations} between past and future tokens when minimizing the next-token prediction loss. 
%to predict future tokes. that fail to generalize beyond the specific tokens and positions present in the training data. 
Previous work highlighted issues such as \textit{lost-in-the-middle} phenomenon \citep{liu2024lost}, where models become overly sensitive to token positions within a long context rather than learning a robust, transferable reasoning process. Drawing on insights from cognitive science -- which emphasize the efficacy of a limited ``working memory'' \citep{miller1956magical, elman1993learning} -- we hypothesize that restricting the model to reason over only a \emph{limited set} of tokens reduces the chances of exploiting spurious correlations between tokens and thereby promote more robust reasoning that can transfer to new knowledge domains. 

To this end, we propose three architectural changes. We first disentangle knowledge and reasoning into distinct modules -- an external memory bank and a reasoning network. Such decomposition provides an inductive bias for re-using the reasoning model on new knowledge domains. Second, we propose that the reasoning model should operate over a small context window, an inductive bias that reduces the chances of relying on spurious correlations for making accurate predictions. Thirdly, because the reasoning model operates on a short context window, it needs to retrieve and write information from the external memory bank. We propose to learn the strategies for reading from and writing to an external memory bank. We hypothesize that this structured approach enables dynamic retrieval and reasoning, reduces reliance on spurious correlations, and therefore improves generalization.

\end{enumerate}

\section{Background and Notation}
% \subsection{Pretraining}
\label{sec:background}
Current LLMs are pretrained for next-token prediction over a large text corpus. Let 
$\mathcal{D}$ denote a large unlabeled text corpus collected from the Internet, which we refer to as \textbf{passive data}. This passive data includes books, articles, and online content and generally does not describe intermediate reasoning steps that can guide problem-solving. For example, while a text passage might contain the statement of a math problem and its final answer, it often omits the step-by-step derivation leading to the solution (e.g., intermediate algebraic manipulations or deductions). Each element in \(\mathcal{D}\) is a sequence of tokens \(\mathbf{x} = (x_1, x_2, \ldots, x_T)\), typically formed using a vocabulary of size \(\lvert V \rvert\). A language model \(\phi_\theta\) with parameters \(\theta\) (often a Transformer \citep{vaswani2023attentionneed}) is trained to learn the conditional distribution
\[
P\bigl(x_t \mid x_1, x_2, \ldots, x_{t-1}; \theta\bigr).
\]
In practice, we maximize the log-likelihood of each token given its preceding tokens:
\[
\max_\theta \sum_{\mathbf{x} \in \mathcal{D}} \sum_{t=1}^{T} \log P\bigl(x_t \mid x_{<t}; \theta\bigr).
\]
We refer to this task-agnostic pretraining using next-token prediction loss as \textbf{supervised pretraining} (SPT). Any subsequent training that continues to optimize this objective for a target task is referred to as \textbf{supervised finetuning} (SFT).

Training for tasks can also be performed using a reward-based formulation grounded in RL. At each step \(t\), the model \(\phi_\theta\) receives a scalar reward \(r_t\), which can depend on partial or complete outputs. Here, the data is not the passively collected corpus \(\mathcal{D}\), but rather is gathered \textbf{online} from the model’s own interactions. The goal is to maximize the expected cumulative reward:
\[
\max_{\theta}\,\mathbb{E}_{\mathbf{x} \sim \pi_\theta}\!\Bigl[\sum_{t=1}^{T}r_t(\mathbf{x})\Bigr],
\]
where \(\pi_\theta\) is the model’s policy distribution over sequences \(\mathbf{x}\). We refer to the pretraining done with this reward-based objective as \textbf{reward-based pretraining} (RPT), and subsequent training that continues to optimize this reward as \textbf{reward-based finetuning} (RFT).
 

\section{Evaluation of Reasoning Separate From Knowledge}
\label{sec:eval-reasoning-knowledge}

\begin{table*}[t!]
\begin{tabular}{@{}clllll@{}}
\toprule
\multicolumn{1}{c}{\textbf{Models}}                           & \multicolumn{1}{c}{\textbf{Print (\# ICL $=$ 1)}} &  \multicolumn{1}{c}{\textbf{Print (4)}} & \multicolumn{1}{c}{\textbf{Print (10)}} & \multicolumn{1}{c}{\textbf{Sort}} & \multicolumn{1}{c}{\textbf{Copy}} \\ \midrule
\textbf{Llama 3.1 8B}      & 1                                         & 4                                          & 7                                         & 0                               & 0                               \\
\textbf{Llama 3.1 70B}     & 2                                          & 7                                          & 10                                        & 0                               & 0                               \\
\textbf{Qwen2.5 Coder 7B}  & 2                                         & 2                                                 & 1                                         & 0                               & 0                               \\
\textbf{Qwen2.5 Coder 32B} & 3                                          & 2                                          & 2                                         & 0                               & 9                                 \\
\textbf{GPT-4o}                      &      2                                       &  9                                            &       13                                      &  0                                 &           0                        \\
\textbf{o1}                     & 71 &                                       64   &                                            65 &                                  1 &       95                            \\ \bottomrule
\end{tabular}
\centering
\caption{Brainf**k evaluation results. We report the percent accuracy over the test set for each task. The number of examples used for each task evaluation is listed in Appendix
\ref{appx:esoteric_exp_details}. The numbers within parentheses are the number of in-context examples during evaluation.}
\label{tab:brainfck}
\end{table*}

To evaluate the ability of current models to transfer their reasoning across domains, we constructed an evaluation benchmark containing simple algorithmic tasks, such as printing and sorting, in esoteric programming languages. Unlike conventional evaluation using programming languages with familiar syntax, such as Python or Java, this benchmark is designed to minimize the influence of pre-existing knowledge by using languages with unconventional rules and minimal semantic connections to common programming languages. This allows us to measure a model's ability to generalize its logical reasoning to solve problems in domains different from the ones encountered during pretraining.

\subsection{Esoteric Programming Languages} We used two esoteric programming languages, Brainf**k \citep{wikipedia_brainfuck} and Befunge \citep{befunge93}. Both languages are rare -- infrequently seen during pretraining -- and radically different from common languages like Python. However, they are Turing-complete, meaning they can express any algorithm that a conventional programming language like Python can, given enough time and memory. Brainf**k operates on a simple, tape-based memory model with only eight commands. Befunge is a two-dimensional stack-based language where code execution follows paths on a grid, allowing for unconventional control flows like loops and branches in any direction. While both languages differ significantly from Python, they are based on simple programming rules. Examples below. 

\begin{lstlisting}[language=c, basicstyle=\ttfamily\small, frame=shadowbox, caption={Brainf**k program to print the character 'K.'}, captionpos=b]
+++++++[>++++++++++<-]>+++++.
\end{lstlisting}

\begin{lstlisting}[language=c, basicstyle=\ttfamily\small, frame=shadowbox, caption={Befunge program to calculate the factorial.}, captionpos=b]
&>:1-:v v *_$.@ 
 ^    _$>\:^
\end{lstlisting}


\paragraph{Tasks.} 
%We design two sets of tasks to explore the unique capabilities of two esoteric programming languages, BF and Befunge. 
We designed two sets of tasks for Brainf**k and Befunge, which are easy to do in Python and Java. We select the tasks based on what is more suitable to implement in each esoteric language while maintaining some overlap. For Brainf**k, we have the three tasks of printing two-letter words, sorting a list of five elements, and copying an input string of five characters. For Befunge, we have the three tasks of printing two-letter words, generating a program to calculate the factorial of the input number, and generating a program to output the first $k$ Fibonacci numbers.

\paragraph{Evaluation Details.} We evaluate the state-of-the-art LLMs of different families and scales: Llama 3.1 8B and 70B, Qwen2.5 Coder 7B and 32B, GPT-4o and o1. For each task, we construct a set of problems by changing the inputs. For example, in the printing task, we vary the characters to print (e.g., `hi' and `so'). The models are prompted using the standardized format reported in \cref{lst:brainfuck} and \cref{lst:befunge} of Appendix \ref{appx:prompts_brainfck} and \ref{appx:prompts_befunge} respectively. Like the information available to a human while learning a new programming language, the prompt includes the full syntax and rules of the programming language and example code blocks with explanations. We further detail the evaluation protocol in Appendix \ref{appx:esoteric_exp_details}.

\paragraph{Results.} Despite the simplicity of the tasks, all models generally perform poorly, averaging $\sim$12\% accuracy in Brainf**k (see Table~\ref{tab:brainfck}) and $\sim$29\% in Befunge (see Table~\ref{tab:befunge}). In Brainf**k, performance only marginally improves (about 4\% on average) with the increase in the number of in-context examples from 1 to 10 suggesting that current models struggle to infer the correct structure and principles underlying Brainf**k, even with all the rules, syntax, and contextual guidance. On the other hand, in Befunge, in-context examples allow the models to achieve $\sim$70-90\% accuracy at printing two-letter words. But, the models fail to solve the Fibonacci and factorial tasks.

A notable outlier is the o1 model, which vastly outperforms other models. Unlike the other models, o1 has undergone extensive post-training with RL for solving reasoning tasks \citep{openai_o1}, which likely contributes to its stronger performance. This result highlights the potential benefits of RL-based post-training in adapting models to specialized or unconventional tasks. However, it is important to note that even o1's performance leaves considerable room for improvement. It scores 1\% accuracy on sorting five elements even with 10 in-context examples and 65.5\% accuracy on printing two-letter words.

\begin{table*}[t]
\begin{tabular}{@{}cllllll@{}}
\toprule
\multicolumn{1}{c}{\textbf{Models}}                           & \multicolumn{1}{c}{\textbf{Print (\# ICL $=$ 1)}} & \multicolumn{1}{c}{\textbf{Print (4)}} & \multicolumn{1}{c}{\textbf{Print (10)}} & \multicolumn{1}{c}{\textbf{Fibonacci}} & \multicolumn{1}{c}{\textbf{Factorial}} \\ \midrule
\textbf{Llama 3.1 8B}      & 19                                         & 68                                         & 72                               & 0                               & 0                               \\
\textbf{Llama 3.1 70B}     & 92                                          & 92                                         & 93                               & 2                               & 0                               \\
\textbf{Qwen2.5 Coder 7B}  & 15                                         & 66                                         & 74                               & 0                               & 0                               \\
\textbf{Qwen2.5 Coder 32B} & 8                                          & 83                                         & 94                               & 0                               & 0                               \\
\textbf{GPT-4o}                      &      72                                       &  98                                            &      100                                &  0                                 &           0                        \\
\textbf{o1}                    &     70   &     83                                  &   93                                         &      0                            &  0                          \\ \bottomrule
\end{tabular}
\centering
\caption{Befunge evaluation results. We report the percent accuracy over the test set for each of the tasks. The number of examples used for each task evaluation is listed in Appendix
\ref{appx:esoteric_exp_details}. The numbers within parentheses are the number of in-context examples during evaluation.}
\label{tab:befunge}
\end{table*}

% \subsection{Challenges in Current Training Schemes}
% \begin{itemize}
%     \item Large language models (LLMs) entangle reasoning with memory/knowledge during training.
%     \item This coupling limits the model's ability to generalize reasoning to new tasks without retraining.
%     \item Examples:
%     \begin{itemize}
%         \item Solving tasks that require new reasoning patterns often leads to overfitting to seen data instead of genuine generalization - we see in this in the esoteric programming problems. 
%     \end{itemize}
% \end{itemize}

% \subsection{Motivation to Decouple Reasoning from Memory/Knowledge}
% \begin{itemize}
%     \item Isolating reasoning mechanisms allows for:
%     \begin{itemize}
%         \item Transferable reasoning capabilities across domains.
%         \item Better understanding of reasoning dynamics independent of the model's stored knowledge.
%     \end{itemize}
%     \item Enables modular training paradigms where reasoning and knowledge modules can be optimized separately. (JYO: This is my claim but I don't know if we want to go with this). 
% \end{itemize}
\section{Proposed Directions}
\label{sec:proposed_directions}

\subsection{Pretraining for Reasoning with Reinforcement Learning}
\label{sec:rl-pretraining}

\begin{tcolorbox}[colback=white,colframe=black,boxrule=0.75pt]
\textbf{Proposal}: Instead of pretraining on passive data (defined in \cref{sec:background}) and finetuning with RL, we propose integrating RL directly into the pretraining phase to enable better iterative reasoning.
\end{tcolorbox}

Recent works have continued the SPT-then-RFT paradigm to enhance task adaptability and reasoning capabilities in LLMs. In particular, RFT has shown to be essential in improving generalization across reasoning-intensive domains, such as mathematical problem-solving and program synthesis \citep{zelikman2024quietstarlanguagemodelsteach, hosseini2024vstartrainingverifiersselftaught}. By enabling and refining intermediate reasoning traces (e.g., ``Let's solve this step by step: First, we need to factor the quadratic equation...'') to arrive at a final answer, these methods decompose complex problems into easier sub-problems, allowing models to iteratively construct solutions through systematic exploration of possible solution paths.

The current approach of training LLMs with the SPT-then-RFT paradigm mirrors AlphaGo \citep{silver2016mastering}, which initially leveraged pretraining on human demonstrations, followed by RL finetuning, achieving superhuman performance in Go. However, AlphaZero \citep{silver2017masteringchessshogiselfplay}, which trained purely with RL from scratch, surpassed AlphaGo, suggesting that imitation-based pretraining may limit exploration. A similar risk arises in LLMs: an initial phase of supervised pretraining on passive data (as defined in Section \ref{sec:background}) often lacks the supervision of intermediate reasoning steps and may confine subsequent RL finetuning to a restricted solution space, hindering its ability to escape the local minimum. Moreover, it is common to constrain RL finetuning to stay close to the base language model to reduce the risk of generating unnatural sequence of language tokens and thereby hacking the reward function \citep{pmlr-v202-gao23h, paulus2017deep, alami2024investigatingregularizationselfplaylanguage}. However, such a constraint also hinders the exploration of RL finetuning and its ability to discover the underlying reasoning process. 

\begin{tcolorbox}[colback=white,colframe=black,boxrule=0.75pt]
\textbf{Hypothesis}: Pretraining on passive data can constrain the subsequent reward-based finetuning by placing models in local minima, limiting their ability to discover reasoning strategies that generalize (e.g., finding novel ways to solve math problems beyond the specific solution approaches seen in training data).

\end{tcolorbox}

\textbf{Does reward-based pretraining (RPT) outperform supervised pretraining followed RL finetuning (SPT-then-RFT) in Go?}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/go_fig.pdf}
    \caption{Comparing the different training paradigms of RPT vs. SPT-then-RFT in Go 9 $\times$ 9. These results affirm the hypothesis that the SPT can restrict the model's subsequent exploration with RL. The KL regularization is added to replicate common training paradigms in language models \citep{pmlr-v202-gao23h, paulus2017deep, alami2024investigatingregularizationselfplaylanguage}.}
    \label{fig:go9x9}
\end{figure}

We trained two agents using the \textbf{SPT-then-RFT} paradigm (as used in AlphaGo) and the \textbf{RPT} paradigm (as used in AlphaZero) in a simplified Go environment using a 9$\times$9 board. This setup allows rigorous evaluation of whether pretraining on expert human demonstrations collected from the top 100 players impairs RL’s ability to escape the local minima found by supervised learning and thereby restricts its exploration and ability to learn superior gameplay strategies. The AlphaZero paper introduced multiple changes from AlphaGo (e.g., different network architecture, pretraining strategies, etc.), making it challenging to pinpoint the most critical change responsible for the performance difference. We ran controlled experiments to isolate whether the RL training from scratch was the major factor contributing to performance gain. To mimic the common training setup in LLMs, we adopt a Kullback-Leibler (KL) penalty \citep{kl} as a regularization for the SPT-then-RFT paradigm.

Go $9 \times 9$ results, reported in \cref{fig:go9x9}, show that supervised pretraining on passive data constrains the RL finetuning performed similarly to RLHF in LLMs by limiting exploration. As expected, the RPT model achieves a 100\% win rate against the SPT, demonstrating that exploration via reward-based training can easily outperform training on the top 100 expert data. Against the SPT-then-RFT paradigm, the performance of the RPT model varies with the KL constraint that keeps the generations close to the pre-trained model. The RPT model achieves a 66\% win rate compared to SPT + RFT with a KL coefficient of 0.1, and a greater win rate of 92\% against SPT + RFT with a stricter KL coefficient of 0.5. These findings highlight that tighter reliance on pre-trained knowledge hinders exploration while loosening these constraints enables RL to discover better strategies. 

Without the KL constraint and given an infinite training budget, RPT and SPT + RFT models can eventually converge to the same optimal policy. Such recovery in performance using RFT after SPT may be possible in simpler tasks such as Go9x9 where reward hacking is not possible. However, running unconstrained RL post-SPT with LLMs is generally infeasible as the LLM degrades and starts producing gibberish (i.e., meaningless sequence of tokens) that still increases the reward function (i.e., reward hacking \citep{skalse2022defining}).  
%In  While running pure RL after SPT for a long time is possible in Go9x9 as the mo is possible  a simpler environment than   as RL would have full freedom to explore beyond the pretrained initialization, especially given the simplicity of the environment. 
To verify if this is ndeed the case we repeat the comparison between SPT and SPT + RFT with LLMs as detailed below.

\textbf{Does reward-based finetuning (RFT) outperform supervised pretraining followed by reward-based finetuning (SPT-then-RFT)?}

% The superior performance of RPT in GO an be attributed to RL discovering game moves beyond those in the human demonstrations. The notable finding, however, is that RFT after SPT cannot recover the performance of RL pretraining from scratch (i.e., RPT), suggesting that SPT leads the model towards a ``local-minimum" that limits the exploration of RL \pulkit{did we try running RL finetuning without any KL constraint for a long time?}. 
% We hypothesize that a similar effect might hinder current LLMs' reasoning abilities. 

We test the hypothesis that, also in LLMs, RL training for reasoning (RFT) after supervised pertaining (SPT) may be insufficient to push the model beyond its local minimum to discover more general reasoning abilities that could have been learned if the model was trained from scratch to reason via RL. 

As the training of a language model from scratch using RL remains very challenging (see Section~\ref{sec:artificial_envs} for more discussion), we use a pretrained LLM and test if supervised fine-tuning (SFT) on a passive dataset (i.e., examples of questions and answers) limits generalization compared to finetuning for reasoning using RL (RFT).

To make a fair comparison, we fix all variables constant except the training paradigms of: (1) \textbf{SFT-then-RFT} paradigm, where a base model is finetuned on supervised demonstrations before applying RL \citep{ziegler2019fine}; (2) \textbf{pure RFT} paradigm, where the base model is only finetuned with a reward-based objective via RL. While this setup does not involve full SPT vs. RPT comparisons, it provides a computationally efficient proxy for understanding how SFT on passively collected data influences the exploration and generalization of downstream RFT.

For this investigation, we design a synthetic mathematical reasoning task wherein the model must identify vectors orthogonal to a given vector (more details in  Appendix \ref{apx:synth_task_details}). As show in \cref{lst:synthetic_task}, the problem is presented as a multiple-choice question, where the model must choose the correct option. We used Qwen 1.5B \citep{yang2024qwen2technicalreport} as the base pre-trained model and evaluated different fine-tuning strategies using the prompt detailed in Appendix \ref{apx:prompt_format}.

\begin{lstlisting}[language=JSON, basicstyle=\footnotesize\ttfamily, frame=single, numbers=none, caption={Example of synthetic orthogonality task}, label={lst:synthetic_task}]
{
    "instruction": "Which of the vectors are orthogonal to [-2, -1, 0]?
        (a) [3, 4, -5]
        (b) [0, 0, 2]
        (c) [-1, 3, 4]
        (d) [-1, 1, 5]
    output the answer as solution:",
    "output": "solution: (b)"
}
\end{lstlisting}
 


Results in Table~\ref{tab:vector_orthogonality_results} support our hypothesis: models trained with only RFT outperform those finetuned with SFT-then-RFT. The SFT-then-RFT models overfit to the training distribution, achieving near-perfect accuracy on the training set but inferior generalization to the test set reflected in a performance drop from 100\% to 80\%. This result suggests that the model is more prone to memorizing patterns in the training data instead of figuring out the underlying reasoning algorithm. These results are aligned with those of \citet{chu2025sftmemorizesrlgeneralizes} that show SFT memorizes while RL generalizes. Furthermore, SFT-trained models struggle to leverage RL post-training effectively. We observe that SFT models learn to predict the answer directly, avoiding step-by-step reasoning. As a result, during the RL post-training phase, the model does not generate the intermediate reasoning steps to solve the task. This reinforces the idea that passive pretraining (i.e., next-token prediction on data without explicit intermediate reasoning traces) overfits and constrains later exploration that prevents models from discovering more general reasoning patterns.

\begin{table}[t!]
\centering
\label{tab:results}
\begin{tabular}{lcc}
\toprule
\textbf{Model Configuration} & \textbf{Train (\%)} & \textbf{Test (\%)} \\
\midrule
Base Model  & 88  & 89  \\
Base Model + SFT-then-RFT  & \textbf{100}  & 80  \\
Base Model + RFT  & 94  & \textbf{93}  \\
\bottomrule
\end{tabular}
\caption{Comparing the effect of direct finetuning with RL (RFT) against supervised finetuning followed by RL finetuning (SFT-then-RFT) on the reasoning task of determining orthoganlity between vectors. We used Qwen 1.5B as the Base model. for these experiments. Pure RFT training achieves the highest accuracy, while SFT-based models show signs of overfitting.}
\label{tab:vector_orthogonality_results}
\end{table}

\textbf{Towards RL-Driven Pretraining}
Building on these insights, we advocate for a new pretraining approach that emphasizes learning step-by-step reasoning patterns. Since we lack large-scale training data with explicit intermediate reasoning steps (e.g., Chain-of-Thought style demonstrations), we propose using RL to generate such reasoning traces through interaction with an environment and guided by appropriate reward functions. We posit that achieving truly general reasoning abilities requires training a model from scratch (i.e., RPT) to reason using reward supervision instead of the currently popular paradigm of supervised pretraining on passive data followed by finetuning for reasoning with rewards.
%it is essential to generate and \textit{pretrain}, not merely finetune, on these reasoning traces with RL. 
%Through such, we can learn systematic reasoning patterns beyond what's available in standard pretraining data. 
However, this approach presents a significant exploration challenge because searching in the space of language tokens from scratch is difficult. To realize this vision of RL-driven pretraining, we must efficiently explore and generate high-quality, coherent reasoning steps that are missing in current pretraining datasets. We provide ideas on addressing this challenge in \cref{sec:artificial_envs}.

\subsection{Pretraining for Reasoning from Scratch (RPT) using Synthetic Tasks}
\label{sec:artificial_envs}

\begin{tcolorbox}[colback=white,colframe=black,boxrule=0.75pt]
\textbf{Proposal}: 
We propose pretraining models using \textbf{simplified synthetic tasks with reduced token spaces} to address the challenge of efficient exploration in reasoning tasks.  Searching for reasoning traces in large token spaces is inherently difficult, as the vast combinatorial possibilities make it challenging to identify coherent and meaningful patterns. In contrast, reducing the token space in controlled environments and training agents to iteratively solve synthetic tasks or games can enable models to efficiently learn the core reasoning mechanisms. Once these reasoning skills are acquired, they can be scaled to more complex tasks with larger token spaces, gradually transferring them to natural language settings.

% \pulkit{Give intuition why reduce token spaces -- what problem does it solve?}
% We propose pretraining models in simplified artificial environments with reduced token spaces that preserve key structural features, like burstiness and Zipfian distributions, to efficiently build reasoning priors. By iteratively predicting future dynamics in these settings, models can develop transferable reasoning skills \pulkit{why?}, including hierarchical abstraction, causal reasoning, and compositional generalization, which we hope to transfer to language \pulkit{this is too condensed to make sense for a new reader -- lets unpack it. We want to generate a reasoning trace with RL, need to ease exploration, lets think of tasks that require few tokens and perhaps when the agent can learn to reason on such tasks, we scale to tasks with larger token spaces and larger complexity}.
\end{tcolorbox}

An important challenge when training a model from scratch with RL is efficiently exploring the space of reasoning traces to find the high-reward ones. The search space of natural language is nearly unconstrained, and therefore, the likelihood of finding a solution by generating a random combination of words is extremely small. The central question is: How can we make this search process more tractable?

To this end, we propose \textbf{training a ``reasoning prior" on synthetic tasks} that incentivize the learning of reasoning primitives 
%capture core structural properties enabling learning in language (e.g., burstiness, Zipfian distributions) 
within a smaller token space where exploration is much easier. By training on 
%these structured yet 
simplified tasks where the agent must iteratively interact with the environment to solve for a reward, the agent can develop key reasoning skills, such as hierarchical abstraction, causal reasoning, and compositional generalization, bypassing the complexities of natural language. Recent studies \citep{chan2022data, chan2024toward} suggest that capabilities such as in-context learning emerge from structural properties of natural language syntax (e.g., burstiness, where items cluster temporally or thematically, and Zipfian distributions with heavy long tails) rather than specific semantics of the language tokens. This suggests that we can choose problems with drastically smaller token spaces that preserve key task properties necessary for the emergence of reasoning prior. 
%By designing tasks where models iteratively predict how the agent and environment evolve in a game for example, they can learn to infer and apply underlying rules from observed data. This setup reduces the token space without compromising the patterns needed for reasoning. 
Once such reasoning prior has been acquired, models can then be adapted to broader token spaces and ultimately to language, integrating search and feedback mechanisms into large-scale language pretraining.

The two open questions are: (i) Which tasks should be used for reasoning pretraining and (ii) How do we transfer reasoning prior to natural language? Some possibilities for (i) are generating logic games using a small set of randomly generated rules or using data sources with similar distributions to natural language but smaller token spaces. We can start with tasks with smaller token spaces and gradually increase the complexity with larger token spaces requiring more challenging reasoning. We see a parallel here between the state of self-supervised learning in computer vision a decade ago \citep{ssl-survey, baradad2022learninglookingnoise, walker2015denseopticalflowprediction,jayaraman2016learningimagerepresentationstied,agrawal2015learning,doersch2015unsupervised}, where many attempts were made to develop a good proxy task for self-supervision. It took considerable research by the community to eventually find self-supervision objectives and tasks that eventually superseded the performance of supervised learning. We expect something similar here. 

For (ii), we can draw to the common practice of adapting a neural network for a new computer vision task by preserving the early layers and potentially finetuning or re-initializing the last layers~\cite{donahue2013decaf,agrawal2014analyzing}. When it comes to reasoning prior, we expect it to be embedded in the intermediate layers of the neural network (not too close to inputs or outputs). To transfer the reasoning prior, we can preserve the intermediate layers but adapt the input/output layers to enable the reasoning to work in a new token space \citep{lu2021pretrainedtransformersuniversalcomputation}. Careful selection and design of the artificial environments (i) are essential to learning features and circuits that are useful and transferrable (ii) to natural language.

\textbf{Evidence from existing literature hinting at the plausibility of the proposed approach}  
We argue that the reasoning priors learned with synthetic tasks can plausibly transfer to natural language, supported by two key pieces of evidence. First, code is an example of an artificial environment that closely parallels language. Although code and language have different token distributions, pretraining on code before training on natural language has been widely used~\citep{petty2024does, aryabumi2024code, kim2024code}. The structured nature of code -- precise syntax, deterministic semantics, and modularity -- enables models to develop strong priors for logic and problem-solving. Cognitive studies further support this approach: children who learn programming exhibit enhanced creativity, mathematical skills, and reasoning on other non-coding tasks \citep{scherer2019cognitive, MONTUORI2024104961}. This suggests that structured environments, whether learned by humans or machines, facilitate the development of transferable reasoning skills. 

Second, \citet{zhong2024algorithmic} demonstrated that even a random transformer, with only embedding layers optimized, can surprisingly solve various algorithmic tasks, such as modular arithmetic and parentheses balancing. This suggests that some meaningful circuits or computational pathways already exist within the architecture of random models. Based on this observation, we hypothesize that when a model is explicitly trained to learn to maximize rewards on synthetic tasks, it will develop more meaningful and transferrable circuits that can be transferred and used when interacting with the model in natural language. 


% \sw{how do we acquire a good prior, there isn't a good dataset to train for reasoning or what are the good properties for good reasoning}
% To address this, it is useful to reflect on why language pretraining has been so successful in the first place. Recent studies \citep{chan2022data, chan2024toward} suggest that impressive capabilities like in-context learning (ICL), which resemble deductive reasoning, emerge not from specific language tokens (e.g., English or Korean) but from underlying structural properties of the data, such as burstiness (items appear in clusters rather than uniformly over time) and Zipfian distributions with heavy long tails. 
% This insight highlights two important directions. First, it suggests that reasoning capabilities can emerge even in artificial token spaces, as long as the data preserves these structural complexities. Second, it underscores the potential of simplifying the token space for training by reducing the total vocabulary size while maintaining key distributional properties like burstiness and Zipfian patterns. This simplification eases the search process during training. Once the model has learned reasoning capabilities in this simplified setting, the number of tokens can gradually be increased, eventually applying these models to more complex domains like English.

% Building on this motivation, we propose generating artificial data from controlled environments with a smaller vocabulary space (e.g., reaction-diffusion systems, synthetic Markov decision processes) to facilitate efficient search and feedback mechanisms during pretraining and to bootstrap spatio-temporal reasoning of the model. Reaction-diffusion systems \citep{wakamiya2010self} shown in Figure \ref{fig:reaction-diffusion}, for instance, are nonlinear dynamical processes that model how chemical substances interact and spread out over space. They have been widely studied to accurately model many different natural phenomena, such as the stripes and spots of animals \citep{Nakamasu2009InteractionsBZ}, coral growth patterns \citep{somathilake2014reactiondiffusion}, and hair follicle spacing \citep{Schlake2007CanonicalWS}. \pulkit{Do you want to give these hints in this paper? Or just suggest that it can be possible to construct such distributions, but not get into details of it? }Artificial environments of these kinds can produce effectively unbounded datasets with complex, natural structures, offering a tightly controlled setting for reasoning-centric training \pulkit{Natural phenomenon don't provide rewards -- so that part is still unclear how is it good for reasoning training. It seems its good for next token prediction training}. \sw{perhaps the rewards can be supervised targets (we want the environment to achieve some shape (leaf-shape)), or novelty (could even think of this as curiosity) like they they suggest in akarsh's paper \citep{kumar2024automatingsearchartificiallife}}

Further, unlike real-world corpora, which conflate reasoning and knowledge -- for example, in natural language, logical inference is often entangled with background knowledge and contextual semantics -- synthetic tasks enable explicit disentanglement of the two. In these tasks, knowledge may consist of predefined rules and primitives that govern system dynamics, and we have full control over them. Because these synthetic tasks remove the ambiguities and semantic dependencies inherent in language, models trained to solve such tasks focus solely on applying known rules to reason, solving for certain rewards by iteratively stepping through the environment. Thus, artificial environments serve as ideal testbeds for reasoning mechanisms that do not confound an agent's extent of a priori factual knowledge with its reasoning abilities. %eliminating the complexities associated with acquiring and interpreting knowledge.

% Parallels can be drawn in other domains where synthetic data has been used effectively. In computer vision, recent research has demonstrated the efficacy of synthetic data, such as fractals, noise-based images, and procedurally generated random patterns with OpenGL \citep{kataoka2021pretrainingnaturalimages, baradad2022learninglookingnoise, baradad2023proceduralimageprogramsrepresentation, wang2023visualpretrainingnavigationlearn}, for pretraining models on visual tasks. Similarly, in reinforcement learning, synthetic data derived from Markov chains has shown comparable, if not superior, performance \citep{wang2024pretrainingsyntheticdatahelps, kumarlearning} to pretraining with natural language \citep{reid2022wikipediahelpofflinereinforcement} on benchmarks like Mujoco and D4RL. In language modeling, synthetic tasks such as retrieving unique elements from input sequences \citep{wu2022insights, wu2021lime} have yielded surprisingly strong results by simplifying task design while maintaining meaningful structural patterns.

% However, especially in the domain of natural language processing, many approaches rely on manually crafted rules or limited inductive biases, which restrict their scalability and diversity. Our approach builds on this foundation by proposing open-ended artificial environments designed to produce datasets with richer, more complex emergent structures. Unlike synthetic tasks with fixed rules, these environments evolve dynamically, capturing a wider range of phenomena that mimic real-world complexity. Additionally, they allow straightforward scalability by adjusting parameters like grid size or time steps, enabling the generation of massive datasets. This distinguishes our approach from prior methods, emphasizing flexibility and scalability for easing search and developing the preliminary reasoning capabilities of the model.


% Internet text is increasingly constrained by quantity and quality issues, including noise and biases (e.g., unwanted correlations between gender and occupation). Although preprocessing and filtering \citep{longpre2023pretrainer, dolma} can mitigate some of these pitfalls, sustainable scaling remains a concern. To address this, we propose generating data from artificial environments, which can produce effectively unbounded datasets with complex spatio-temporal structure. By enabling tighter control over data quality and reducing reliance on real-world corpora -- which heavily mixes knowledge and reasoning -- this approach promises to enhance reasoning capabilities and overcome the limitations of Internet-based pretraining.

% In fact, recent studies \citep{chan2022data, chan2024toward} have shown that impressive capabilities like in-context learning (ICL) that resemble deductive reasoning emerge in LLMs, not because of a specific language (e.g. English, Korean), but because of the underlying distributional properties and structure, such as burstiness (items appear in clusters rather than being uniformly distributed
% over time) and having a Zipfian distribution with a heavy long tail. This implies that even if the tokens differ from human language, as long as the data contains a coherent, similar complexity of structure, we can generate such artificial data to directly train for reasoning. After bootstrapping the reasoning abilities of the model, only a small amount of high-quality natural language data may be necessary to adapt to the domain of natural language.

    
% \subsection{\textbf{Towards Better Architectural Decoupling of Memory}}
% \begin{tcolorbox}[colback=white,colframe=black,boxrule=0.75pt]
% \textbf{Proposal}: We propose using RL with curriculum learning to train decoupled memory and reasoning modules that dynamically interact with one another, while mitigating optimization instability issues.
% \end{tcolorbox}


% Our esoteric \pulkit{our experiments are not esoteric -- we use esoteric programming language -- its VERY VERY important to be precise. lets not be lousy with the writing} experiments reveal that current models entangle reasoning with memorized knowledge, overfitting their reasoning abilities to a particular domain instead of learning general reasoning. To address this, we propose decoupling memory and reasoning into separate modules. For this system to be effective, the reasoning module must \textbf{learn to interact with memory}, specifically by reading and writing dynamically rather than relying on static retrieval. This enables two key capabilities: (1) adaptive memory use, allowing the model to develop retrieval and update strategies, and (2) flexible knowledge integration, enabling the addition of new knowledge without full model retraining \pulkit{This is proposing the method before describing the intuition behind solving the problem -- I wrote an entire paragraph explaining the intuition but can't find it anymore. Can you help me find it? Collaboration in writing works best when we don't delete but instead comment out sections from other writers. I don't want to re-write it again, but can we find it in previous history where I make the case about how the prior we want is that for reasoning to generalize it should be operating on fewer tokens than a large context?}.

% Existing approaches provide partial solutions and fail to achieve both desiderata simultaneously. Retrieval-Augmented Generation (RAG) systems \citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}, for instance, allow for the post-training addition of memory. However, the model interacts with the memory statically, without the ability to query or write into memory. On the other hand, memory layers in architectures like \textit{Memory Layers at Scale} \citep{berges2024memory} allow for high-bandwidth, complex interactions by embedding implicit memory parameters directly within the model through an optimizable persistent KV-cache \pulkit{what is KV-cache for a first year undergrad reading the paper? REFERENCE!! and is it really necessary to use such a technically specific term? The idea is more general and applies to architectures beyond transformers}. \pulkit{I also don't understand what kind of models are being described} While effective for intricate reasoning, these architectures require full re-training of the model to add new knowledge afterwards since the memory is tightly coupled to the model’s internal representations.


% \textbf{Direction of Focus}
% What we need, then, is a hybrid system: an \textbf{external memory system} that decouples reasoning from memory while still allowing for \textbf{dynamic, complex interactions} between the two \pulkit{why do we need this? We never said why -- we just say we need it}. Such a system would enable sparse memory reads and writes, avoiding the brittleness of fully differentiable designs. Neural Turing Machines (NTMs) \citep{alevs2016neural}, for example, attempted differentiable memory interactions but suffered from unstable optimization \cite{paassen2020reservoir} primarily due to gradient instability from backpropagation through time over large unfolded depths \pulkit{This again seems an orthogonal challenge -- how does our proposal remove the brittleness?}.

% To address these challenges \pulkit{which challenges? seperatoin of data v/s memory? memory optimization, something else?}, we propose using RL to optimize memory interactions rather than relying on end-to-end differentiability \pulkit{AGAIN -- PLEASE PAY ATTENTION -- DONT DESCRIBE THE METHOD BEFORE THE INTUITION / INSIGHT INTO WHAT SOLVES THE PROBLEM}. To enable the memory interactions with RL, we posit that a \textbf{language-based interface} provides a general and flexible protocol. It allows the reasoning model to actively manage a \textbf{working memory} -- its context -- by dynamically filling it with relevant information and evicting outdated or less useful content. Such control over working memory allows the model to focus on relevant pieces of information while avoiding learning spurious correlations over long contexts \pulkit{how is working memory different form context?}. 

% This approach \pulkit{wich approach?} leverages RL to create actions for reading and writing to memory, but training on such long-horizon tasks \pulkit{what is long horizon here?} poses challenges in credit assignment and exploration \citep{arumugam2021information}. To address these issues, we introduce \textbf{curriculum learning} as the foundation of the training process \pulkit{Now I am just lost ... }, inspired by evidence from human cognitive development \citep{newport1988constraints,elman1993learning}. The model begins by handling short, simple memory access patterns, then gradually expands to larger contexts and more complex read–write interactions. This staged progression stabilizes training, mitigates overfitting to spurious correlations, and avoids overwhelming the model with high-dimensional memory from the outset, ultimately enabling better generalization.

% % This approach leverages RL to create actions for reading and writing to memory. However, RL training on such long-horizon problems presents challenges in credit assignment and exploration \citep{arumugam2021information}. To mitigate these issues, we propose \textbf{curriculum learning}, where models are trained progressively on increasingly complex memory interactions, ensuring stable optimization and better generalization. Inspired by evidence from human cognitive development \citep{newport1988constraints,elman1993learning}, we propose the reasoning model begins by learning to handle short, simple memory access patterns, gradually advancing to larger contexts and more complex memory interactions that involve both reads and writes. This staged progression stabilizes training, reducing the risk of overfitting to spurious correlations or overwhelming the model with high-dimensional memory from the outset. 
% % As the reasoning module develops, it learns not just to retrieve information efficiently but also to reason effectively over the retrieved content, fostering robust generalization to unseen scenarios.

% Our framework thus combines RL for sparse memory access with curriculum learning, beginning with simple memory access patterns and gradually progressing to longer, more complex interactions. This approach aims to stabilize training while enabling dynamic and flexible knowledge usage.

% Current models are trained with large context windows, which enables superior next-token prediction. However, this also increases the chances of a model overly relying on the correlation in the data in the context window to predict the next token. As the size of the context window increases, the chance of using spurious correlation to predict data in the training set also increases. The dominant way the community has addressed the problem is to leverage increasingly large amounts of data that reduce the model's ability to utilize the spurious correlation. Another complementary and perhaps radically different path is less explored -- having small context windows. Imagine a model with a small context window (e.g., $<$ 10 tokens) but is augmented with an external memory bank which it can query to fill its context using a trained \textit{retrieval} module. Given the data in the context, a separate \textit{reasoning} module can learn how to use these few tokens to predict the desired output. Restricting the \textit{reasoning} module to operate on only a few data points acts as a prior to avoid spurious correlations and better utilize the given data. Further, the separation of \textit{retrieval} and \textit{reasoning} modules also encourages the separation of knowledge and reasoning which we hypothesize is a key cause of today's models fragile reasoning. \pulkit{Can draw parallel to working memory and semantic memory in humans.}

%     A side benefit of such separation of knowledge and reasoning is that the model is being trained to use an external memory bank in contrast to current RAG systems where the LLM is expected to either already possess the ability to use new knowledge or a retrieval system is hand-designed. Training a retrieval system and a superior reasoning system can both greatly improve the performance of proposed model architectures compared to existing attempts. 
% \subsection{Decoupling Memory and Reasoning for Continual Knowledge Integration}
% \begin{tcolorbox}[colback=white,colframe=black,boxrule=0.75pt] \textbf{Proposal}:
% To allow reasoning to generalize across different knowledge domains with ease, we propose to explore decoupling knowledge and reasoning architecturally. In this framework, the reasoning module explicitly queries and updates an external memory bank, enabling reasoning over newly added knowledge.
% \end{tcolorbox}

% A critical challenge in current large language models is that their reasoning processes often become tied to fixed, domain-specific knowledge. When knowledge changes domains or evolves over time, models that embed information directly into their parameters struggle to adapt without full retraining. This limitation underscores the need for a system capable of reasoning on top of any given knowledge—one that can flexibly handle dynamic and diverse information sources.

% To enable more flexible, domain-agnostic reasoning, models must learn to interact with an external memory. Existing approaches to integrating new knowledge primarily use retrieval-augmented generation (RAG) \citep{borgeaud2022improving}, but these methods typically involve a single retrieval step and do not train models to query or write to memory in a more iterative fashion. Even when RAG is integrated at the pretraining stage \citep{guu2020retrieval}, the interaction with memory remains limited. In contrast, memory-augmented architectures like \textit{Memory Layers at Scale} \citep{berges2024memory} allow more intricate memory interactions, but the memory itself is still embedded in the model weights. This makes updating or injecting new knowledge infeasible without a costly full retraining process.

% To address these challenges, we propose decoupling the reasoning component from the knowledge store, enabling explicit read–write operations to an external memory bank. This approach draws inspiration from Neural Turing Machines (NTM) \citep{graves2014neural}, which support learned memory interactions. However, purely differentiable memory access in NTMs leads to gradient instability in long-horizon reasoning \citep{alevs2016neural, paassen2020reservoir}, hindering their scalability.

% We overcome this instability by replacing differentiable memory access with a reinforcement learning (RL) mechanism that allows the model to take discrete memory actions iteratively—performing multiple reads and writes during a single generation (see Figure~\ref{fig:main}). Because long-horizon RL itself can be unstable, we incorporate curriculum learning: the model is first trained on tasks requiring simple memory interactions and gradually progresses to more complex scenarios. This staged approach stabilizes the optimization process, ensuring that the model can efficiently integrate new knowledge and reason over dynamic information from any domain without retraining from scratch.

% \subsection{Training with a Shorter Context to Reduce Spurious Correlations}
% \begin{tcolorbox}[colback=white,colframe=black,boxrule=0.75pt]
% \textbf{Proposal}: 
% We propose training models with shorter context windows while augmenting them with an external memory retrieval mechanism to reduce learning spurious correlations. By enforcing a prior that limits reasoning to operate on a small number of tokens, we encourage structured problem-solving and discourage shortcut learning.
% \end{tcolorbox}

% Current large language models (LLMs) are being trained with increasingly long context windows, now reaching up to 1M tokens \cite{team2024gemini}. While large context windows improve next-token prediction, they also increase the risk of models relying on spurious correlations within the available context rather than genuinely understanding underlying relationships. As context length grows, models have more opportunities to exploit statistical shortcuts in training data, leading to fragile generalization. The dominant strategy to mitigate this issue has been to train on increasingly large and diverse datasets, diluting spurious correlations. However, recent studies highlight the lost-in-the-middle phenomenon \citep{liu2024lost}, where models disproportionately attend to the beginning and end of a sequence while neglecting the middle, suggesting an incorrectly learned positional bias. This indicates that simply scaling context length does not necessarily teach models to extract and prioritize relevant information effectively.

% To address this, we propose an alternative and largely underexplored approach: reducing the context window while augmenting the model with an external memory mechanism.

% Imagine a model with a small context window (e.g., <10 tokens) but equipped with an external memory bank that it can query through a trained retrieval module. A separate reasoning module would then operate over these retrieved tokens, learning to extract relevant information and make predictions. By constraining the reasoning module to operate on limited inputs, we impose an implicit prior that discourages reliance on spurious correlations and encourages structured inference—akin to how humans must selectively retrieve and process relevant knowledge from long-term memory due to working memory constraints.

% As the basis of our proposed idea, we draw inspiration from cognitive science research on human learning and reasoning, which suggests that cognitive constraints, particularly in working memory, are not merely limitations but essential mechanisms for structured learning in children \citep{newport1988constraints}. Moreover, humans can typically hold only 4–7 chunks of information at a time in working memory \citep{miller1956magical, cowan2001magical}, yet this constraint can be advantageous -- it forces selective attention, promotes efficient processing, and encourages abstraction rather than rote memorization \citep{ elman1993learning}. Similarly, by restricting the reasoning module to operate over a small number of retrieved tokens, we create a bottleneck that prioritizes relevant information processing while mitigating reliance on spurious correlations. Just as humans rely on external semantic memory to supplement their limited working memory, our proposed architectural bias can help  separate retrieval from reasoning, fostering more structured and generalizable problem-solving.


% %     A side benefit of such separation of knowledge and reasoning is that the model is being trained to use an external memory bank in contrast to current RAG systems where the LLM is expected to either already possess the ability to use new knowledge or a retrieval system is hand-designed. Training a retrieval system and a superior reasoning system can both greatly improve the performance of proposed model architectures compared to existing attempts. 


\subsection{Decoupling Knowledge and Reasoning to Generalize Across Domains}
\label{sec:decouple-knowledge-reasoning}
\begin{tcolorbox}[colback=white,colframe=black,boxrule=0.75pt]  
\textbf{Proposal}:  
To enable robust generalization across knowledge domains, we propose \textbf{decoupling knowledge and reasoning} using an external memory bank to store knowledge and a \textbf{small context window}, allowing models to retrieve and process information selectively rather than the currently dominant paradigm of learning to reason using large context windows that can be more prone to overfitting. 
%Unlike past differentiable memory architectures that suffered from unstable training, we frame memory access as a sequential decision-making process trained via RL, circumventing gradient vanishing issues. Combined with curriculum learning, this approach fosters structured inference and adaptive reasoning that generalizes beyond naive ICL.
\end{tcolorbox}  
The quest for disentangling knowledge and reasoning leads to the following questions: (i) Are there model architectural priors that encourage the emergence of more robust reasoning? (ii) How can we help the model generalize its reasoning to new problems and knowledge domains?

Current models discover reasoning by training a transformer model on a long context window. It is widely believed that larger models with longer context windows and more extensive data learn superior reasoning abilities as evidenced by their superior in-context learning (ICL; \citet{dong2022survey}) abilities. However, our experiments in Section \ref{sec:eval-reasoning-knowledge} illustrate that simply scaling models with larger data may not suffice for achieving general reasoning due to the intertwining of knowledge and reasoning. We hypothesize that training with longer context windows has more chances of learning spurious correlations among past tokens to predict future tokens. This is evident in the \textit{lost-in-the-middle} phenomenon, where models become sensitive to the position of information rather than the content \citep{liu2024lost}.  

One way to encourage the emergence of more general reasoning is to train with \textit{shorter context windows}. As the model need to predict using fewer tokens, it will have lesser chances to learn spurious correlations between tokens in the context window to predict future tokens. Crucially, cognitive science studies in humans provide the empirical grounding for this approach, suggesting that limiting context can foster more accurate and systematic reasoning \citep{miller1956magical, newport1988constraints, elman1993learning, Cowan2001}. In fact, as a result of the limited context, we develop strategies such as \textit{chunking} \citep{Thalmann2019}, where we form abstractions that group related pieces of knowledge, enabling more complex concepts to be retained within working memory.

However, reducing the context window can prevent the model from having the long-range information necessary to make predictions. To mitigate this issue, we \textbf{propose to have an explicit \textit{semantic memory} where data is stored and a learned retrieval mechanism to fetch a small amount of data} in the model's \textit{working memory} (i.e., context window) on which the reasoning function operates. 

We hypothesize that this combination of semantic memory for data storage, a learned data retrieval mechanism, and a small working memory on top of which reasoning operates will lead to a more scalable and generalizable reasoning system. The proposed architecture should allow for effective generalization to new problems or new knowledge sources by inserting the new knowledge in the semantic memory. 

% At a surface level, our proposal may seem akin to retrieval-augmented generation (RAG) \jyo{talk about the 2 problems, one is rag relies on a static retrieval mechanism and it relies on good long context abilities}\citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}, which also integrates external knowledge. However, RAG systems typically rely on in-context learning (ICL) \citep{dong2022survey} -- the retrieved text is appended into a single context that can become quite large \pulkit{why is this a problem?}. This method can be computationally expensive with longer contexts and often encounter the problem of incorrectly retrieving the relevant context \citep{zhang2024raftadaptinglanguagemodel}. Moreover, prior memory-based approaches such as \citet{guu2020retrieval} typically do not support multi-round interactions with external memory. Yet, when tackling a challenging math problem, for instance, we often need to retrieve different relevant past problems or theorems at various stages of reasoning. As we progress, we may continue to refine our retrieval and also record the approach we use to solve the problem. This iterative retrieval and updating of knowledge is essential for more efficient learning when we encounter similar problems in the future \pulkit{maybe also say that current RAG don't use traiend retrieval systems, but use heuristics?}.

At a surface level, our proposal may seem akin to retrieval-augmented generation (RAG) \citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}, which also integrates external knowledge. However, RAG systems typically rely on in-context learning (ICL) \citep{dong2022survey}—the retrieved text is appended into a single context that can become quite large. This approach presents two key challenges: (i) it relies on a static, heuristic-based retrieval mechanism, such as nearest neighbors on a fixed embedding space, that does not allow the model to actively query, and (ii) it assumes the model has strong long-context reasoning abilities, which is often not the case. As context size grows, retrieval quality degrades, and models struggle to effectively utilize all retrieved information, leading to errors \citep{zhang2024raftadaptinglanguagemodel}.


Moreover, prior memory-based approaches, such as \citet{guu2020retrieval} typically do not support the aforementioned dynamic, multi-round interactions with external memory. Yet, when tackling a challenging math problem, for instance, we often need to retrieve different relevant past problems or theorems at various stages of reasoning. As we progress, we may continue to refine our retrieval and also record the approach we use to solve the problem. This iterative retrieval and updating of knowledge is essential for more efficient learning when we encounter similar problems in the future. 


In contrast, we propose to interact with the memory (read/write) during \emph{training} and \emph{inference}, thereby learning to explicitly decouple memory from reasoning. Rather than appending all retrieved text into a long context window, we maintain a relatively \emph{small} working memory to which the model can repeatedly query and update an external semantic memory. Using a small context forces both the retrieval mechanism to retrieve more relevant information and for the reasoning module to operate on a smaller number of tokens and thereby have lesser chances of learning spurious correlations between tokens.  

Conceptually, our proposed design is reminiscent of early neural memory architectures like Neural Turing Machines \citep{graves2014neural} and Differentiable Neural Computers \citep{Graves-DNC}, where a separate memory module is maintained. These approaches, however, often suffer from significant optimization instabilities \citep{alevs2016neural, paassen2020reservoir} due to end-to-end differentiability across numerous memory-access steps, which often lead to volatile gradients. To circumvent such optimization bottlenecks, we propose \emph{discrete} memory-access decisions trained via RL, thereby avoiding the vanishing gradients typical in long-horizon differentiable memory access. In addition, since long-horizon RL optimization can be unstable, we propose leveraging a \emph{curriculum} strategy to stabilize training by starting with simple memory interactions (read and write) and gradually progressing towards more complex, multi-step interactions.

% At a surface level, this may appear similar to how current models incorporate new knowledge via techniques such as retrieval-augmented generation (RAG)  \citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}. RAG systems rely on in-context learning (ICL) \citep{dong2022survey} leads to inefficiencies, particularly for knowledge-intensive tasks that require long contexts. In addition, prior works that integrate RAG during pretraining do not allow the model to engage in multiple rounds of interaction with the memory module \citep{guu2020retrieval}, even though such iterative access is essential for solving complex problems that require repeatedly retrieving and updating key information. 

% % However, current approaches still pursue the naive, simple path of incorporating new knowledge via context, such as RAG. This reliance on in-context learning (ICL) \citep{dong2022survey} leads to inefficiencies, particularly for knowledge-intensive tasks that require long contexts. As context length increases, models are more likely to rely on spurious correlations. This is evident in the \textit{lost-in-the-middle} phenomenon, where models become sensitive to the position of information rather than correctly reasoning over its content \citep{liu2024lost}. Thus, we argue that relying on context alone may be insufficient \pulkit{I am bit lost here -- what does it mean by relying on context alone}.

% To address this, we propose an alternative and largely under-explored approach: reducing the context window while augmenting the model with an external memory mechanism \pulkit{we are jumping to describing the method without saying why}.

% Imagine a model with a small context window (e.g., $<10$ tokens) but equipped with an external memory bank can be queried through a trained retrieval module. A separate reasoning module would then operate over these retrieved tokens, learning to extract relevant information and make predictions. By constraining the reasoning module to operate on limited inputs, we impose an implicit prior that discourages reliance on spurious correlations and encourages structured inference \pulkit{what does structured inference mean -- can we use less fancy terms / terms that are well defined?} -- akin to how humans must selectively retrieve and process relevant knowledge from long-term memory due to working memory constraints. 
% Previous works that explored this idea of explicitly decoupling memory from reasoning, such as Neural Turing Machines \citep{graves2014neural} and Differentiable Neural Computers \citep{Graves-DNC}, faced significant optimization instability \citep{alevs2016neural, paassen2020reservoir}. These architectures struggled with training dynamics, often failing to learn effective memory access strategies or collapsing into degenerate solutions. The core issue stemmed from their end-to-end differentiability: while in principle allowing for flexible and expressive memory interactions, it made long-term credit assignment difficult and led to unstable gradients.

% Given these optimization challenges, to effectively train for complex, multi-step memory interactions, we propose to learn discrete memory interactions with RL and curriculum learning to ease optimization. We frame memory access as a sequential decision-making process trained via RL, rather than relying on end-to-end differentiability \pulkit{I think the reader will get confused}. Thereby, we circumvent the vanishing gradient problem over long horizon \pulkit{Have not even described the problem in the context over here}. In tandem with these discrete memory interactions, curriculum learning is used to gradually learn more complex memory interactions and to avoid the aforementioned problem of spurious correlations. Initially, the model learns simple memory operations, such as retrieving a single fact or updating a small working memory buffer. As training progresses, task complexity increases, requiring multi-step retrieval, reasoning over multiple retrieved items, and dynamically modifying stored knowledge. This gradual scaling ensures that the model learns robust and generalizable memory management strategies rather than relying on brittle heuristics that fail under distribution shifts.  By structuring the learning process in this way, we aim to develop models that go beyond naive in-context learning, instead fostering adaptive reasoning capabilities that can generalize across diverse knowledge domains \pulkit{This paragraph is not understandable at all}.


% Motivated by this, we propose a reasoning system that processes a selected subset of retrieved tokens per reasoning step, rather than attending to an entire long context at once. This necessitates a decoupled architecture where the reasoning module operates with a controlled working memory, actively deciding which tokens to retrieve, retain, or evict, rather than passively consuming all available information. Such a system retains the advantages of both retrieval-augmented generation (RAG) and memory-embedded models like \textit{Memory Layers at Scale} \citep{berges2024memory}: RAG enables easy addition of external knowledge but lacks iterative memory interactions, while models with memory parameters built in them provide intricate memory operations but embed knowledge within model weights, making real-time updates infeasible. 

% Our approach balances both by enabling targeted multi-step memory interactions while allowing flexible knowledge updates \ref{fig:main}. To facilitate effective memory manipulation, we introduce a reinforcement learning (RL) mechanism for discrete read-write operations, inspired by Neural Turing Machines (NTMs) \citep{graves2014neural}, but designed to avoid NTM instability in long-horizon reasoning \citep{alevs2016neural, paassen2020reservoir}. Since long-horizon RL optimization is inherently difficult, we employ curriculum learning: training begins with simple memory interaction tasks before gradually progressing to more complex reasoning over retrieved knowledge. 

%  \\  \textbf{OLD VERSION } \\ 

% A key limitation of current LLMs is their reliance on in-context learning for adapting to new tasks, which ties reasoning to a single, static context. In our esoteric programming experiment, all knowledge about esoteric languages and tasks was placed directly into the model's context. As context windows expand, models become increasingly prone to spurious correlations, as reasoning should ideally involve attending to only a subset of tokens at each step. This issue has been highlighted in studies such as the lost-in-the-middle phenomenon \citep{liu2024lost}, where models disproportionately focus on the beginning and end of sequences while neglecting intermediate information. To address the challenge of applying reasoning effectively to new knowledge domains, we propose decoupling reasoning from memory by equipping the reasoner with its own working memory, which it actively manages through explicit read–write interactions with an external memory bank.

% \textbf{Continual Knowledge Integration Through Explicit Memory Interactions}
% A critical challenge in current large language models is their reliance on fixed, domain-specific knowledge embedded in model parameters, requiring full retraining to adapt when knowledge evolves. Current approaches to integrating new knowledge primarily use retrieval-augmented generation (RAG) \citep{borgeaud2022improving}, but these methods typically involve only single-step retrieval rather than the iterative memory interactions needed for complex reasoning. Even when RAG is integrated during pretraining \citep{guu2020retrieval}, its interaction with memory remains limited. While memory-augmented architectures like \textit{Memory Layers at Scale} \citep{berges2024memory} enable more sophisticated memory operations, they still embed memory within model weights, making real-time knowledge updates infeasible without costly retraining.

% To address these limitations, we propose decoupling the reasoning component from the knowledge store through explicit read-write operations to an external memory bank. This approach draws inspiration from Neural Turing Machines (NTMs) \citep{graves2014neural}, but avoids the gradient instability issues that arise in NTMs during long-horizon reasoning \citep{alevs2016neural, paassen2020reservoir}. Instead, we introduce a reinforcement learning (RL) mechanism that enables discrete memory actions—allowing the model to perform multiple targeted reads and writes during generation. Since long-horizon RL can itself be unstable, we employ curriculum learning: starting with simple memory interaction tasks before gradually progressing to more complex scenarios. This staged approach ensures stable optimization while teaching the model to efficiently integrate and reason over new knowledge from any domain without retraining.

% \textbf{Training with a Controlled Working Memory to Reduce Spurious Correlations}
% While curriculum learning over tasks helps stabilize the training of memory interactions, we further propose to constrain the reasoning module's working memory. As models scale to longer contexts, they become increasingly prone to memorizing surface-level patterns rather than developing structured problem-solving abilities. Therefore, we hypothesize that training the reasoning module to process only a small subset of retrieved tokens at a time would force it to actively choose what information to retrieve and attend to rather than passively consuming long contexts.
% This draws inspiration from cognitive science research on human learning, which shows that working memory constraints actually benefit learning by encouraging structured information processing \citep{miller1956magical, Cowan2001}. By combining curriculum-based training with controlled working memory size, we aim to create a natural progression where the model would first learn basic retrieval strategies with simple tasks and limited context, then gradually develop more sophisticated reasoning patterns while maintaining selective attention. If successful, this joint approach should ensure the model develops robust generalization capabilities rather than relying on spurious correlations, enabling it to reason effectively across diverse and evolving knowledge domains.


\section{Discussion and Alternative Views}
\label{sec:alternative_views}
An alternative perspective to our proposal of disentangling knowledge and reasoning contends that knowledge and reasoning are so deeply intertwined that they are practically inseparable. For example, consider completing the sentence, ``The coffee was unbearably hot, so he poured it into a ...'' Although ``metal cup'' and ``porcelain cup'' may both be probable completions, most people would select ``porcelain cup'' based on the context that the coffee was hot. Yet, it is unclear whether such a choice arises from memorized associations (knowledge of common behavior) or inference (reasoning about likely scenarios). This fundamental ambiguity raises questions about whether knowledge and reasoning can or should even be disentangled in intelligent systems.

One perspective is that if an agent has to perform a task repeatedly, then it may not want to explicitly reason every time, but store the result of its reasoning as ``knowledge". In such a scenario, knowledge and reasoning can be coupled for efficiency. Our argument is not that reasoning and knowledge cannot be coupled but that the agent should have a mechanism to separate them and not only reason based on correlations between its experiences so that it can truly apply the reasoning in a general way. 

Furthermore, integrating RL into the pretraining of LLMs presents several significant challenges that question its feasibility and scalability. One major issue is the inefficiency of RL-driven training, which requires collecting data on the fly through interactions rather than leveraging static, large-scale corpora. This process is computationally expensive and risks becoming infeasible as model sizes and training demands grow. 
%it remains unclear what the right reward models and tasks for RL-based training should be, as language lacks the well-defined, objective-driven feedback in structured environments like games.
Finally, while we argue that supervised pretraining on passive data can limit exploration, training on synthetic tasks can suffer from a similar problem if they are poorly designed or constrained to narrow task distributions that ultimately do not transfer well to natural language. Particularly if real-world tasks in natural language include reasoning skills beyond the scope of synthetic pretraining tasks, then mechanisms to compensate for such differences in the pretraining and the needed capabilities will be required. Therefore, the choice of specific pretraining tasks is an important subject of future research. 

% 2. artificial environments
% - \sw{a possible contradiction i see in our argument is that we argue that SFT/supervised pretraining can lead to models being stuck in a local minima, but this pretraining or learning the reasoning prior on artificial environments can also do the same} \pulkit{You can directly acknowledge it as a limitation -- however, my main argument in favor is the objective function at training and deployment time is the same and therefore less chances of getting stuck in local minima. Next, we can randomize over the syntax which can further be useful and minimize the chances of being in a local minima.} 

\section{Related Works}
\textbf{Evaluating Reasoning in Large Language Models.}
LLMs like GPT-4 \citep{openai2024gpt4technicalreport}, Llama 3 \citep{grattafiori2024llama3herdmodels}, and Qwen \citep{deepseekai2024deepseekv3technicalreport} have demonstrated impressive reasoning capabilities. However, as demonstrated with our esoteric programming language experiment, their reasoning remains fragile, often overfitting to training data patterns. These results are coherent with the findings of \citet{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical, wu2024reasoningrecitingexploringcapabilities}, which evaluate LLMs' reasoning under counterfactual tasks and modified math problems.  
\citet{xie2024memorization} explored the interplay between memorization and reasoning, revealing that fine-tuning enhances both memorization and reasoning generalization .

\textbf{Inference-time Scaling for Reasoning.} 
Recent research on inference-time scaling methods falls into two broad categories. The first category encompasses search-based approaches that do not require additional training. Methods, such as Chain-of-Thought \citep{wei2023chainofthoughtpromptingelicitsreasoning}, Tree-of-Thought \citep{yao2024tree}, Self-Consistency \citep{wang2023selfconsistencyimproveschainthought}, break down reasoning into sub-problems or explore multiple reasoning paths during inference and select the optimal solution based on the consistency criteria. The second category comprises of RL-based methods, which integrate search with learning mechanisms guided by reward models. Techniques like Reinforcement Learning with Human Feedback \citep{ouyang2022traininglanguagemodelsfollow}, ReST \citep{singh2024humandatascalingselftraining}, and STaR \citep{zelikman2022starbootstrappingreasoningreasoning, zelikman2024quietstarlanguagemodelsteach} use search to generate diverse outputs with intermediate reasoning steps and provide feedback or reward signals \citep{lightman2023letsverifystepstep, wang2024mathshepherdverifyreinforcellms, uesato2022solvingmathwordproblems} to improve the model’s reasoning policies. These methods adaptively refine reasoning by iteratively training the model on successful outcomes.

% \textbf{Disentangling Knowledge and Reasoning}
% Research by \citet{chan2022data, chan2024toward} suggests that reasoning capabilities emerge from data structural properties rather than specific language tokens. Pretraining's reliance on next-token prediction encourages memorization over generalization. RL techniques, inspired by AlphaGo \citep{silver2016mastering} and AlphaZero \citep{silver2017masteringchessshogiselfplay}, offer a promising alternative.
% \par
\textbf{Memory Architectures.}
Current memory approaches like Retrieval-Augmented Generation (RAG) \citep{berges2024memory} provide static memory augmentation but lack dynamism.  Neural Turing Machines (NTMs) \citep{alevs2016neural} explored differentiable memory architectures but face optimization challenges due to the backpropagation through time. Recent innovations \citep{berges2024memory} embed memory as a persistent KV-cache into the model's weights, but because the memory is intricately tied to the models' representations, it becomes difficult to add new knowledge without retraining. \citet{jin2024disentangling} proposed a novel framework using trainable ⟨memory⟩ and ⟨reason⟩ tokens to separate knowledge retrieval from reasoning, addressing key challenges in LLMs such as hallucinations and logical inconsistencies. However, this method still ties knowledge with reasoning in a single model.


\textbf{Synthetic Data.}
Most current efforts \citep{chen2024diversitysyntheticdataimpact, huggingface_cosmopedia, abdin2024phi4technicalreport} focus on generating synthetic data directly in natural language using pretrained models. In contrast, there is a less explored avenue that involves using simple, symbolic synthetic datasets for pretraining \citep{wu2022insights, wu2021lime, krishna2021doespretrainingsummarizationrequire}. These approaches typically depend on manually crafted rules (e.g., Set, Identity) to generate data. On top of this lack of scalability, they often lack the complexity required for emergent behavior, limiting their ability to support more complex structures or patterns. The simplicity of these rules can restrict the diversity and depth of the learned priors.

% In other domains, synthetic data derived from Markov chains and noise-based images have consistently outperformed or matched the effectiveness of language-based pretraining \citep{wang2023visualpretrainingnavigationlearn, wang2024pretrainingsyntheticdatahelps, baradad2022learninglookingnoise}. These methods serve as priors that narrow down the search space, enabling more efficient exploration .

In other various domains, synthetic data has also shown promise. Studies in vision tasks \citep{wang2023visualpretrainingnavigationlearn, baradad2022learninglookingnoise} and RL \citep{baradad2023proceduralimageprogramsrepresentation, wang2024pretrainingsyntheticdatahelps} demonstrate the effectiveness of synthetic data derived from random Markov chains and noise-based images.

\section{Acknowledgment}
We want to express our gratitude to Idan Shenfeld, Zhang-Wei Hong, Akarsh Kumar, and members of the Improbable AI lab for the helpful discussion on the paper. We are grateful to MIT Supercloud
and the Lincoln Laboratory Supercomputing Center for providing HPC resources. The research was supported in part
by NSF CSGrad4US Fellowship, Google, and Amazon. The research was sponsored
by the Army Research Office and was accomplished under
Grant Number W911NF-21-1-0328. The research was also sponsored by the Office of Naval Research and was accomplished under Grant Number N00014-22-1-2740. This
research was also sponsored by the United States Air Force
Research Laboratory and the United States Air Force Artificial
Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. This research was supported also by the National Institute of Health under the Grant Number MH133066. The views and conclusions contained in this document are those of the authors
and should not be interpreted as representing the official
policies, either expressed or implied, of the Army Research
Office, Naval Research Office, Air Force, or the U.S. Government.\looseness=-1


\section{Author Contributions}

\textbf{Seungwook Han} co-developed the project and contributed in all aspects of experiments and writing.

\textbf{Jyothish Pari} co-developed the project and contributed in all aspects of experiments and writing.

\textbf{Samuel Gershman} contributed to the paper's narrative and writing.

\textbf{Pulkit Agrawal} co-developed the project direction, advised SH and JP, and played a significant role in paper writing. 

\newpage
\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}