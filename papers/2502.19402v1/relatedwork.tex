\section{Related Works}
\textbf{Evaluating Reasoning in Large Language Models.}
LLMs like GPT-4 \citep{openai2024gpt4technicalreport}, Llama 3 \citep{grattafiori2024llama3herdmodels}, and Qwen \citep{deepseekai2024deepseekv3technicalreport} have demonstrated impressive reasoning capabilities. However, as demonstrated with our esoteric programming language experiment, their reasoning remains fragile, often overfitting to training data patterns. These results are coherent with the findings of \citet{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical, wu2024reasoningrecitingexploringcapabilities}, which evaluate LLMs' reasoning under counterfactual tasks and modified math problems.  
\citet{xie2024memorization} explored the interplay between memorization and reasoning, revealing that fine-tuning enhances both memorization and reasoning generalization .

\textbf{Inference-time Scaling for Reasoning.} 
Recent research on inference-time scaling methods falls into two broad categories. The first category encompasses search-based approaches that do not require additional training. Methods, such as Chain-of-Thought \citep{wei2023chainofthoughtpromptingelicitsreasoning}, Tree-of-Thought \citep{yao2024tree}, Self-Consistency \citep{wang2023selfconsistencyimproveschainthought}, break down reasoning into sub-problems or explore multiple reasoning paths during inference and select the optimal solution based on the consistency criteria. The second category comprises of RL-based methods, which integrate search with learning mechanisms guided by reward models. Techniques like Reinforcement Learning with Human Feedback \citep{ouyang2022traininglanguagemodelsfollow}, ReST \citep{singh2024humandatascalingselftraining}, and STaR \citep{zelikman2022starbootstrappingreasoningreasoning, zelikman2024quietstarlanguagemodelsteach} use search to generate diverse outputs with intermediate reasoning steps and provide feedback or reward signals \citep{lightman2023letsverifystepstep, wang2024mathshepherdverifyreinforcellms, uesato2022solvingmathwordproblems} to improve the model’s reasoning policies. These methods adaptively refine reasoning by iteratively training the model on successful outcomes.

% \textbf{Disentangling Knowledge and Reasoning}
% Research by \citet{chan2022data, chan2024toward} suggests that reasoning capabilities emerge from data structural properties rather than specific language tokens. Pretraining's reliance on next-token prediction encourages memorization over generalization. RL techniques, inspired by AlphaGo \citep{silver2016mastering} and AlphaZero \citep{silver2017masteringchessshogiselfplay}, offer a promising alternative.
% \par
\textbf{Memory Architectures.}
Current memory approaches like Retrieval-Augmented Generation (RAG) \citep{berges2024memory} provide static memory augmentation but lack dynamism.  Neural Turing Machines (NTMs) \citep{alevs2016neural} explored differentiable memory architectures but face optimization challenges due to the backpropagation through time. Recent innovations \citep{berges2024memory} embed memory as a persistent KV-cache into the model's weights, but because the memory is intricately tied to the models' representations, it becomes difficult to add new knowledge without retraining. \citet{jin2024disentangling} proposed a novel framework using trainable ⟨memory⟩ and ⟨reason⟩ tokens to separate knowledge retrieval from reasoning, addressing key challenges in LLMs such as hallucinations and logical inconsistencies. However, this method still ties knowledge with reasoning in a single model.


\textbf{Synthetic Data.}
Most current efforts \citep{chen2024diversitysyntheticdataimpact, huggingface_cosmopedia, abdin2024phi4technicalreport} focus on generating synthetic data directly in natural language using pretrained models. In contrast, there is a less explored avenue that involves using simple, symbolic synthetic datasets for pretraining \citep{wu2022insights, wu2021lime, krishna2021doespretrainingsummarizationrequire}. These approaches typically depend on manually crafted rules (e.g., Set, Identity) to generate data. On top of this lack of scalability, they often lack the complexity required for emergent behavior, limiting their ability to support more complex structures or patterns. The simplicity of these rules can restrict the diversity and depth of the learned priors.

% In other domains, synthetic data derived from Markov chains and noise-based images have consistently outperformed or matched the effectiveness of language-based pretraining \citep{wang2023visualpretrainingnavigationlearn, wang2024pretrainingsyntheticdatahelps, baradad2022learninglookingnoise}. These methods serve as priors that narrow down the search space, enabling more efficient exploration .

In other various domains, synthetic data has also shown promise. Studies in vision tasks \citep{wang2023visualpretrainingnavigationlearn, baradad2022learninglookingnoise} and RL \citep{baradad2023proceduralimageprogramsrepresentation, wang2024pretrainingsyntheticdatahelps} demonstrate the effectiveness of synthetic data derived from random Markov chains and noise-based images.