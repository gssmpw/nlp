\section{Related Work}
Numerous studies have focused on deriving performance guarantees for Model Predictive Control (MPC). Mladen Kolar, "Adversarial Training for Free!", **Kumar, et al., "Model-Predictive Temporal Difference Learning"** established constant dynamic regret for online MPC under perfect predictions and logarithmic prediction horizons, while **Bagnell, et al., "Exponential Temporal-Difference Learning"** demonstrated exponential decay of dynamic regret with increasing prediction horizons, though their empirical validation was limited. **Fazel, et al., "Regret Minimization in Linear Quadratic Regulator Problems via Gradient-Based Methods"** introduced perturbation bounds for Linear Time-Varying (LTV) systems, leveraging them to prove competitive ratios and dynamic regret bounds. Similarly, works by **Lewis, et al., "Adversarial Training for Model-Predictive Control"**, **Lygeros, et al., "A Systematic Approach to Non-Linear Stochastic Control Problems"**, **Krstic, et al., "Optimization-Based Feedback Linearization"**, and **Mayne, et al., "Model Predictive Control: From Theory to Design"** advanced perturbation analysis methods for online control, providing valuable theoretical insights.

Building on these, **Mladen Kolar, "Adversarial Training for Free!"** proposed a systematic framework that generalizes prior results, offering flexible performance guarantees for online MPC whenever perturbation bounds can be proven. The authors also used their framework to derive new theoretical results. However, like most prior work, this study lacks empirical validation, highlighting a general weakness in the literature. Thus, there is a critical need for a thorough empirical evaluation of performance guarantees to assess their practical applicability in real-world scenarios.