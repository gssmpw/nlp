% data amount and task categories
% long-context evaluator
% Influencing factors of long-context

Although \benchmark provides a comprehensive evaluation across various safety issues and tasks types, emphasizing improvement on long-context safety, several limitations remain in our work, which need to be addressed in future study.

\paragraph{Expanding Task Coverage.}
% Our benchmark encompasses a total of 1,543 test cases, which is a relatively small set within prevalent safety benchmark, underscoring the room for enrichment for a more reliable evaluation. Moreover, 
While we integrate 6 general long-context tasks in \benchmark, we mainly focus on prevalent task types in real-world scenarios. Therefore, some attack-oriented tasks, such as many-shot jailbreaking, are not included in our benchmark. Further research can take these tasks into consideration, facilitating a more comprehensive understanding on LLMs' long-context safety.

% To resolve this, we plan to expand the coverage of long-context tasks and incorporate additional instances in subsequent work.

\paragraph{Development of Long Context Evaluator.}
To assess the safety performance of LLMs in long-context tasks, we utilize a multi-agent framework with outstanding accuracy. However, all three agents are initialized with GPT-4o mini, which may incur significant evaluation cost. One possible solution is training a specialized LLM safety detector for long-context scenarios to attain high accuracy with low cost, which we leave as future work.

\paragraph{Scalable Methods for Data Collection.}
During the data collection procedure, we instruct the crowd workers to manually retrieve documents and curate safety instructions to ensure high data quality. Nevertheless, it is difficult to scale the data merely relying on human annotations due to high costs. Involving automatic methods for data collection might be feasible for scaling long-context safety data, which we also leave as future work.
