\input{tables/main_result}

\subsection{Experimental Setup}

\paragraph{Metrics.}

Given the two possible concatenation formats for the context and instruction discussed in \S\ref{method:problem}, we introduce a new metric, \(SR_{long}\), to evaluate models' safety rate on \benchmark. Since a secure LLM should respond safely regardless of the instruction's position, the result of an instance is classified as safe only when both responses to the two concatenation formats are deemed safe. Following this, \(SR_{long}\) is defined as:
\begin{equation*}
    SR_{long}=\frac{Num_{both\_safe}}{Num_{total}}
\end{equation*}
Where \(Num_{both\_safe}\) denotes the count of instances where both of the responses are assessed as safe by the evaluator, and \(Num_{total}\) indicates the total number of test cases. To facilitate a clearer comparison of LLM safety between long-context and short-context scenarios, we also define \(SR_{short}\) as the safety rate in short-context settings where LLMs takes only instructions as input without long context in our benchmark.

\input{tables/compare_only}

\paragraph{Models.}
We evaluate a total of 16 popular LLMs, including diverse open-source and closed-source models. A detailed list of these models is provided in Appendix \ref{app:evaluated_models}. We use greedy decoding during inference to ensure consistent and stable outputs.

\paragraph{Evaluator.}
We employ the multi-agent framework proposed in \S\ref{method:evaluator} as the evaluator and initialize all three agents with GPT-4o mini. For inputs without long context, we remove the Context Summarizer from the framework and utilize the other two agents to generate a judgment. Prompts of the agents are presented in Appendix \ref{app:prompts}.


\subsection{Main Results}


Table \ref{tab:main_result} presents the \(SR_{long}\) scores for all safety issues in \benchmark. Although Claude-3.5-haiku stands out with the highest average score of 77.7\%, all other models except the Claude-3.5 series fall below an average safety rate of 55\%. This highlights \textbf{the insufficient capability of LLMs to provide safe responses in long-context tasks}. Additionally, closed-source models tend to outperform open-source ones across all safety issues, revealing \textbf{a significant gap regarding long-context safety between these models}. In terms of specific safety issues, while most models achieve relatively higher scores in Physical \& Mental Harm problems, they generally struggle with issues in Sensitive Topics, with the majority attaining a safety rate below 50\% and all open-source models below 20\%. This underscores \textbf{critical safety concerns on this category that require further attention in other models.}

To further explore the association between model safety capabilities in short-context and long-context scenarios, we calculate the \(SR_{short}\) score for each model and compare it with the corresponding \(SR_{long}\) score. As shown in Table \ref{tab:compare_only}, all models exhibit a notable decline in safety performance when transitioning from short instructions to long inputs, highlighting significant safety challenges within long-context tasks. Moreover, we observe that outstanding performance in short-context settings is not necessarily correlated to a smaller decrease in \(SR_{long}\) scores. For instance, while Llama-3.1-8B-Instruct ranks second in terms of \(SR_{short}\), it suffers the largest \(SR_{long}\) score decline over 60\%, resulting in a remarkable ranking decline in long-context safety. This underscores the importance to treat long-context safety as a distinct domain, warranting safety evaluation specifically tailored to long-context tasks. In Appendix \ref{app:safety_prompts}, we further apply safety prompts on \benchmark, providing insights in mitigating long-context safety risks.

\begin{figure}[!t]
    \centering
    \setlength{\abovecaptionskip}{2mm}
    \includegraphics[width=0.95\linewidth]{figures/task_type_bar.pdf}
    \caption{The average safety rate of all models within each task type. QA stands for Question Answering, GEN for Generation, BS for Brainstorming, SUM for Summarization, RW for Rewrite, RP for Role-playing.}
    \label{fig:task_type}
    \vspace{-5mm}
\end{figure}

\subsection{Safety Rate in Task Types}

In terms of safety performance across different task types, we compute the average \(SR_{long}\) and \(SR_{short}\) scores of all models within each task. As shown in Figure \ref{fig:task_type}, there is a marked decline in safety rate across all task types from short-context to long-context scenarios, indicating the insufficiency in model safety capabilities over a variety of long-context tasks. Furthermore, we observe low \(SR_{long}\) scores of below 30\% in generation-oriented tasks including Generation, Brainstorming, Summarization and Rewrite. In contrast, models achieve an average safety rate of 46.3\% in Question Answering in long-context scenario. This highlights that models face more challenges on generation-oriented tasks compared with the widely evaluated Question Answering task, emphasizing considerable need for further safety alignment on these tasks.




\subsection{Evaluator Assessment}
\label{exp:evaluator}


To assess the capability of our evaluator, we manually establish a test set for safety evaluators comprising 500 samples constructed from our benchmark (see Appendix \ref{app:evaluator} for details), and assess the performance of our multi-agent framework with other evaluators on this set. According to Table \ref{tab:evaluator}, our framework consistently outperforms all other evaluators with the highest accuracy of 92\%, highlighting the effectiveness of multi-agent collaboration in long-context safety evaluation. Moreover, we conduct an ablation study by removing the Context Summarizer from our framework. This results in a 2\% reduction in accuracy, while the performance remains superior to that of GPT-4o mini, which is equivalent to a single Safety Judge. This demonstrates the critical role of each agent in enhancing the overall effectiveness of our framework.

\input{tables/evaluator_results}
