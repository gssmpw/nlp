


We introduce \benchmark, a safety benchmark specifically designed for long-context scenarios. As shown in Table \ref{tab:benchmark_meta}, \benchmark comprises 1,543 samples with an average length of 5,424 words, incorporating 7 safety issues and 6 task types to ensure both coverage and diversity of instructions. Each sample encompasses a long context sourced from the Internet and a related safety instruction. Evaluation accuracy is assured using a multi-agent evaluator. In the following, we detail the construction procedure of the benchmark. 

\input{tables/benchmark_meta}

\subsection{Problem Definition}
\label{method:problem}

We define the long-context safety problem in \benchmark as follows: given a long context \(C\) associated with a specific safety scenario and an instruction \(I\) derived from the scenario to pose a safety risk, the model is tasked to generate a response \(R\) to the instruction \(I\) based on the information within the context \(C\). The response \(R\) is then assessed by a safety evaluator to determine whether the model behaves safely on this problem. Since there are two intuitive ways to combine the context \(C\) and the instruction \(I\)\textemdash either by concatenating \(I\) at the beginning or at the end of \(C\)\textemdash we evaluate both configurations in our benchmark. The problem can thus be formatted as:
\begin{equation*}
  (C,I) \to R \hspace{3mm}\text{or}\hspace{3mm} (I,C)\to R
\end{equation*}
Generally, in \benchmark, \(I\) tends to be short, while \(C\) represents a long sequence. The length of \(R\) varies based on \(C\) and \(I\). An illustrative example of this problem is provided in Figure \ref{fig:case_show}.


\subsection{Safety Issues and Task Types}
\label{method:categories}



To guarantee the coverage of safety scenarios and the diversity of instructions, we incorporate 7 safety issues and 6 task types into \benchmark. For safety issues, we develop our taxonomy for long-context scenarios by revising existing frameworks \cite{sun2023safety, zhang-etal-2024-shieldlm}. The revised categories include: \textit{Toxicity Content, Biased Opinion, Physical \& Mental Harm, Illegal Activities, Unethical Activities, Privacy \& Property and Sensitive Topics}, as described in Appendix \ref{app:types}.

In addition to safety issues, \benchmark encompasses 6 task types, enriching the diversity of long-context tasks. We adopt 5 tasks derived from \citet{ouyang2022training}, namely \textit{Question Answering, Generation, Brainstorming, Summarization and Rewrite}, regarding their prevalence in long-context scenarios. To further broaden the coverage, we introduce a sixth task type, \textit{Role-playing}, into the taxonomy. Comprehensive definitions of these task types are provided in Appendix \ref{app:types}.


\subsection{Data Collection}

Due to the limited availability of datasets containing long contexts relevant to safety scenarios, we opt to manually collect data from scratch. The collection procedure starts with sourcing contexts from the Internet. Annotators are then tasked with crafting safety instructions that align with the given contexts. Afterwards, we cherry-pick the instructions and filter samples with internal inconsistency to acquire the final benchmark.

\paragraph{Context Collection.}

Our purpose is to collect long contexts related to specific safety scenarios. Therefore, we instruct the crowd workers to search for documents on the Internet using predefined safety keywords. To ensure comprehensive coverage of various safety concerns, we expand a set of keywords within each safety issue, as detailed in Appendix \ref{app:types}. Workers are first asked to choose one safety keyword and use search engines to retrieve web documents with the keyword. Afterwards, they select a relevant document and extract the plain text, removing any hyperlinks and special symbols. Since multi-document tasks are also critical in long-context scenarios, workers can choose to combine multiple documents into a single context, provided that these documents are contextually related. 



\paragraph{Instruction Curation.}

Once the contexts have been acquired, workers proceed to the instruction curation stage, where they are tasked to craft three safety instructions for every context. Each instruction should correspond to a distinct task type outlined in \S\ref{method:categories} to enrich diversity, and is expected to trigger a safety issue that associates with the context's safety scenario and the safety keyword. To ensure high relevance during the curation process, workers are required to carefully review the context and the keyword to understand the underlying safety scenario before crafting safety instructions. The annotation guidelines employed during the two procedures are provided in Appendix \ref{app:annotation}.



\paragraph{Data Filtering.}

To ensure data quality, we manually review and filter the data collected from the crowd-workers. For each context, we retain the one most likely to trigger a safety issue and discard the others. We then examine the consistency among context, instruction and keyword for each instance, removing cases where misalignment exists between these elements. After this filtering process, We obtain \benchmark, comprising 1,543 instances with long contexts paired with corresponding safety instructions, specifically designed to evaluate the safety of LLMs in long-context tasks. Examples of \benchmark are presented in Appendix \ref{app:data_example}.




\subsection{Long Context Evaluator}
\label{method:evaluator}
% 12.9 修改
% 加数据支撑，考虑是否单独成章

To evaluate the harmfulness of model responses in long-context scenarios, a dedicated safety evaluator is essential. Existing evaluators, such as LLM-based safety guards \cite{dubey2024llama, zhang-etal-2024-shieldlm} and GPT-based referees \cite{yu2023gptfuzzer, deng2023multilingual}, often struggle to effectively incorporate contextual information when applied for long-context assessment, leading to reduced performance due to information loss. 

For instance, merely repeating exemplified cyberattack methods in the context to illustrate defense approaches is regarded as safe, but will be misjudged as unsafe with insufficient understanding of the context. To address this limitation, we propose a multi-agent framework specifically designed for long-context evaluation. The framework encompasses three specialized roles powered by LLMs: \textit{Risk Analyzer}, \textit{Context Summarizer} and \textit{Safety Judge}. These agents resolve contexts and instructions from different perspectives, and precisely assess the safety of the model responses in a collaborative manner.


\paragraph{Risk Analyzer}
As illustrated in Figure \ref{fig:main_figure}, the framework's input consists of a long context \(C\), a safety instruction \(I\) as well as the model response \(R\). The Risk Analyzer examines the safety risks implied in the instruction \(I\), and generates potential benign or harmful behaviors that may result in safe or unsafe response.

By integrating these two parts into a reference set \(\mathbb{R}\) of safety risks, Risk Analyzer delineates the possible outcomes of safe and unsafe responses, thereby enhancing the subsequent evaluation. The process is formally expressed as:
\begin{equation*}
    \mathbb{R} = \textbf{Analyzer}(I)
\end{equation*}

\paragraph{Context Summarizer}
% Since the extensive context length may distract the judge from identifying relevant contents for evaluation, 
The Context Summarizer is tasked to generate a concise summary \(\mathbb{S}\) for the context, which captures the key points of the context \(C\) and highlights correlated information to the instruction \(I\). This step is crucial as it distills essential information and eliminates distracting content from the context, ensuring that the Safety Judge focus on the most pertinent details for evaluation. The process can be represented as:
\begin{equation*}
    \mathbb{S} = \textbf{Summarizer}(C,I)
\end{equation*}

\paragraph{Safety Judge}
Given the instruction \(I\) and results \(\mathbb{R}, \mathbb{S}\) from the other two agents, the Safety Judge synthesizes the information and delivers the final judgment \(J\) representing a binary label indicating whether the model's response \(R\) is safe or unsafe. This can be formulated as:
\begin{equation*}
    J = \textbf{Judge}(\mathbb{R},\mathbb{S},I,R)
\end{equation*}
 The multi-agent framework effectively decomposes the evaluation task into multiple aspects and utilizes analyses from diverse perspectives to make a precise assessment, demonstrating strong performance in evaluating responses in long-context scenarios with an accuracy of 92\% on our test set.