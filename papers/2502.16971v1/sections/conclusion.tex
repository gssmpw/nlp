In this work, we introduce \benchmark, the first safety benchmark designed for comprehensive evaluation in open-ended long-context tasks. \benchmark comprises 1,543 test cases with an average length of 5,424 words, covering 7 safety issues and 6 task types. Evaluation results on 16 representative LLMs highlight significant safety deficiencies in long-context tasks, with most models attaining safety rates below 55\%. Further experiments reveal a misalignment between short-context and long-context safety, and we identify challenging safety issues and task types through examinations on different categories. Additionally, discussion about influencing factors of long-context safety underscores that relevant context content and extended input length elicit safety risks, with distinct impact on different models. We hope \benchmark can provide reliable evaluations on LLMs' long-context safety, and facilitate improvement of safer long-context models in the future.