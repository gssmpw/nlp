With recent advances in techniques for processing long sequences \cite{sun2023length, su2024roformer, DBLP:conf/iclr/XiaoTCHL24, DBLP:conf/iclr/ChenQTLL0J24}, LLMs have demonstrated remarkable capabilities in understanding and generating long texts \cite{achiam2023gpt, glm2024chatglm}.
However, alongside these advancements and the wide applications of long-context LLMs, safety concerns have emerged in long-context scenarios, such as harmful content implication \cite{anil2024many} and model cognition interference \cite{upadhayay2024cognitive}. These issues underscore the need for a systematic examination of safety in long-context LLMs.

\begin{figure}[!t]
    \centering
    \setlength{\abovecaptionskip}{2mm}
    \includegraphics[width=\linewidth]{figures/case_show.png}
    \caption{Comparison between short-context and long-context safety tasks. Long-context tasks are characterized by incorporating long contexts with instructions in contrast to short-context tasks (Upper), and a performance misalignment is observed between the two tasks for models in the red circle, as these points notably deviate from the blue diagonal arrow, indicating lower safety rates in long-context tasks (Lower).}
    \label{fig:case_show}
    \vspace{-5mm}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/main_figure.pdf}
    \caption{Overall framework of \benchmark. The left section illustrates the construction pipeline of collecting contexts and instructions relevant to safety scenarios. In the middle provides an overview of \benchmark and presents taxonomy of safety issues and task types. The right section depicts the collaborative workflow of the multi-agent evaluator responsible for assigning safety labels to model responses.}
    \label{fig:main_figure}
    \vspace{-5mm}
\end{figure*}

While current long-context benchmarks \cite{bai2024longbench, zhang2024bench, hsieh2024ruler}
% , such as LongBench \cite{bai2024longbench}, InfiniteBench \cite{zhang2024bench}, and RULER \cite{hsieh2024ruler} 
mainly focus on evaluating general capabilities, they do not take safety issues into account.
Moreover, existing safety benchmarks \cite{zhang2023safetybench, li-etal-2024-salad} are typically limited to short-context tasks involving a single query within hundreds of words, as shown in Figure \ref{fig:case_show}, making them inadequate for evaluating long-context models, which is designed to process instructions with long documents, often spanning thousands of words.
A concurrent work, LongSafetyBench \cite{huang2024longsafetybench}, utilizes multiple-choice questions to investigate the harmful awareness ability of long-context models. However, the multiple-choice format cannot fully assess generation safety, which is more valued for generative models.


In this work, we propose \benchmark, the first benchmark to comprehensively evaluate LLM safety in open-ended long-context tasks.
As shown in Figure \ref{fig:main_figure}, \benchmark encompasses 1,543 instances with an average length of 5,424 words. The benchmark comprises 7 safety issues according to prior studies \cite{sun2023safety, zhang-etal-2024-shieldlm}, covering a wide range of safety problems in real-world scenarios. Furthermore, we integrate 6 prevalent task types in long-context scenarios based on the findings from \citet{ouyang2022training} to broaden the coverage of long-context tasks and diversity of instructions. 
To establish \benchmark, we first collect documents related to different safety topics from the Internet. Using a set of predefined safety keywords, the crowd-workers search for relevant documents, and integrate pure text of one or multiple documents into a long context. Afterwards, we instruct the workers to write three instructions in different task types that could trigger safety issues associated with the keyword. Among the three instructions, the one that is most likely to elicit safety issues is kept, and we finally check the samples and filter those with inconsistency between contexts and instructions to ensure data quality of our benchmark.

Since existing safety evaluators struggle with incorporating contextual information for long-context safety assessment, we introduce a multi-agent framework as the long context evaluator. The framework consists of three distinct roles: Risk Analyzer, Context Summarizer and Safety Judge. Risk analyzer and Context Summarizer analyze the context and instruction from different aspects, and Safety Judge synthesizes these analyses to deliver a safety label for the response. Through collaborative analyses from distinct perspectives, the multi-agent framework achieves an outstanding accuracy of 92\% on our test set. With the evaluator, we define the safety rate metric on \benchmark and assess 16 representative open-source and closed-source LLMs. 
Our results reveal that, all models evaluated except Claude-3.5 series achieve safety rates below 55\%, raising significant concerns about the safety of long-context models. Further experiments on instruction-only setting highlight a misalignment between short and long-context safety, underscoring the importance of safety benchmark specially curated for long-context tasks. We also investigate the models' performance across different safety issues and task types, observing that models consistently struggle with issues of Sensitive Topics and generation-oriented tasks like Generation and Brainstorming, indicating critical challenges for future improvement. To better understand the underlying causes of safety concerns in long-context settings, we examine the impact of both the content and length of the context. Our findings suggest that safety risks are more pronounced when relevant context is included, and as the input sequences become longer, further exacerbating these issues, underscoring the urgent need for the safety enhancement in long-context tasks.


Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose \benchmark, a comprehensive safety evaluation benchmark specifically designed for long-context scenarios, encompassing 1,543 test cases with an average length of 5,424 words, covering 7 safety issues and 6 task types.
    \item We introduce an effective multi-agent framework as long-context safety evaluator with an accuracy of 92\%. Utilizing this evaluator, we assess 16 representative long-context LLMs on \benchmark, and discover concerning safety vulnerabilities in long-context tasks as most models attain safety rates below 55\%.
    \item We investigate model performance on different safety issues and task types, and present an analysis on influencing factors of long-context safety. The extensive analyses provide valuable insights on the safety deficiencies in long-context models, facilitating future research to improve their safety capabilities.
\end{itemize}


