\section{Related Work}\label{sec:related_work}

\vspace{-3pt}
\subsection{Image matting}

Image matting seeks to extract foreground objects from images, typically using a trimap to indicate foreground, background, and unknown regions. Traditional methods can be categorized as sampling-based, which estimate alpha mattes using color samples from known regions, or propagation-based, which transfer alpha values based on pixel affinity. However, both approaches often struggle with complex scenarios due to their reliance on low-level color features. 

The advent of deep learning has led to significant advancements in matting techniques. Methods like DIM utilize convolutional encoder-decoder architectures and sophisticated loss designs, improving performance through attention mechanisms. Some approaches aim to eliminate trimap dependency, such as using additional background images or user interactions, but often fail to generalize well to unseen objects in real-world settings. 

Recent developments have introduced mask-guided matting frameworks that only require coarse masks for guidance. MGMatting~\cite{yu2021mgm} has been a pioneer in this area, while MGM-in-the-wild~\cite{park2023mgmwild} explores how to train a generalized mask-guided matting model, often without delving deeply into the model's inner workings. InstMatt~\cite{sun2022instmatt} addresses the challenge of accurately matting overlapping human instances with intricate boundaries, enabling effective separation in complex scenes.MaGGIe~\cite{huynh2024maggie} and SparseMat~\cite{sun2023sparsemat} focuses on efficient, end-to-end instance matting to enhance practical applicability across diverse categories. In contrast, our method investigates the complexities of mask-based challenges in real-world environments. We aim to construct an adaptable framework for mask-guided matting that builds upon the foundational aspects of the model, thereby improving performance across a wide range of object types and challenging backgrounds.

\subsection{Segmentation And Matting Refinement}

In natural image matting, refining high-frequency details around object boundaries is crucial for achieving visually appealing results. Traditional segmentation refinement techniques, like PointRend, enhance masks using convolutional networks and MLPs, relying on coarse predictions from Mask R-CNN and point-wise confidence scores to identify uncertain regions, but they suffer from limited adaptability due to their dependence on manually tuned hyperparameters. In contrast, EFormer employs a semantic and contour detector with a cascade of cross-attention and self-attention mechanisms, facilitating interaction between local details and global semantics for improved detail localization. Model-agnostic strategies, such as SegFix, aim to refine masks across various models but depend heavily on accurate object detection, particularly in complex datasets. SegRefiner, based on a discrete diffusion process, addresses boundary errors but is hindered by slow processing due to its iterative nature, making it less suitable for real-time applications. Non-autoregressive transformers, exemplified by MaskGIT~\cite{maskgit}, offer a more efficient solution by quantizing images into discrete tokens and utilizing parallel decoding, significantly enhancing processing speed. By leveraging these models, we can effectively capture detailed features and maintain coherent semantic relationships in complex scenes.

\subsection{Self-Supervised Learning}
Self-supervised learning (SSL) has become a powerful framework for extracting effective feature representations without human annotations, leveraging pretext tasks to derive supervision directly from data. Deep models, particularly self-supervised vision transformers (DINO and DINOv2~\cite{DINOv2}), have proven effective for dense correspondence under varying photometric and geometric changes. DINOv2, an enhanced version of DINO, shows strong generalization across tasks like classification and segmentation, although its application to correspondence tasks is still underexplored. In image matting, approaches like ViTMatte\cite{Yao2024vitmatte} adapt ViTs for improved feature extraction, while MatAny and MAM utilize the SAM model as a prior. SMat harnesses the rich semantics from ViT features and creatively employs the class token as a saliency cue for guiding salient object localization. By leveraging self-supervised representations, our approach aims to enhance the precision and efficiency of image matting, highlighting the potential of SSL in this domain.

