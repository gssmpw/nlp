\section{Experiments}\label{sec:exp}

\begin{table*}[t]
\footnotesize
    \centering
    \setlength{\tabcolsep}{1.8mm}{
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c}
        \hline\hline
        \multicolumn{1}{c|}{\multirow{2}{*}{Method}} &
        \multicolumn{1}{c|}{\multirow{2}{*}{Category}} &
        \multicolumn{4}{c|}{\multirow{1}{*}{AIM-500}} &
        \multicolumn{4}{c|}{\multirow{1}{*}{AM2K}} &
        \multicolumn{4}{c}{\multirow{1}{*}{P3M-500-NP}} 
        \\
        \cline{3-14}
        \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &
        \multicolumn{1}{c}{SAD$\downarrow$} &
        \multicolumn{1}{c}{MSE$\downarrow$} & 
        \multicolumn{1}{c}{Grad$\downarrow$} & 
        \multicolumn{1}{c|}{Conn$\downarrow$} & 
        \multicolumn{1}{c}{SAD$\downarrow$} &
        \multicolumn{1}{c}{MSE$\downarrow$} & 
        \multicolumn{1}{c}{Grad$\downarrow$} & 
        \multicolumn{1}{c|}{Conn$\downarrow$} &
        \multicolumn{1}{c}{SAD$\downarrow$} &
        \multicolumn{1}{c}{MSE$\downarrow$} & 
        \multicolumn{1}{c}{Grad$\downarrow$} & 
        \multicolumn{1}{c}{Conn$\downarrow$} 
        \\
        \hline
        P3M-ViTAE~\cite{rethink_p3m} & Human & 111.22 & 0.0595 & 44.16 & 54.02 & 40.34 & 0.0205 & 38.71 & 20.55 & \textbf{8.88} & 0.0023 & \underline{8.33} & \underline{11.22} \\
GFM~\cite{liu2024aematter} & Animal & 95.50 & 0.0503 & 74.38 & 46.97 & \textbf{11.18} & 0.0031 & \textbf{10.27} & \textbf{9.77} & 110.80 & 0.0606 & 106.28 & 33.97 \\
AIM~\cite{sun2021sim} & Natural & 48.73 & 0.0187 & 47.96 & \underline{34.75} & 28.13 & 0.0102 & 26.89 & 19.25 & 29.46 & 0.0114 & 28.51 & 25.85  \\
\hline

        % ViTMatte-B~\cite{yao2024vitmatte} & Mixed & & & & & & & & & & & & 
        % \\
    MGM~\cite{yu2021mgm} & Natural            & 51.82 & 0.0126 & 33.18 & 51.78 & 22.69 & 0.0039 & 13.57 & 21.37 & 15.35 & 0.0025 & 14.67 & 14.53 \\
    % SparseMat~\cite{sun2023sparsemat} & Human & 45.54&0.0187&45.33&43.29 & 25.03 & 0.0055 & 22.16 & 24.94 & 20.05&0.0075&0.0019&23.9 \\
    MaGGIe~\cite{huynh2024maggie} & Human     & 47.65 & 0.0121 & 37.31 & 45.97 & 16.59 & 0.0026 & 12.49 & 15.82 & 11.39 & \underline{0.0017} & 13.52 & \textbf{10.86} \\
    MGM$^\dagger$~\cite{park2023mgmwild}& Natural & \underline{43.05} & \underline{0.0102} & \underline{32.13} & 42.71 & 17.23 & \underline{0.0024} & 12.71 & 16.08 & 13.77 & 0.0021 & 15.27 & 13.08 \\
    \textbf{Mask2Alpha}(ours) & Natural & \textbf{35.61} & \textbf{0.0091} & \textbf{29.74} & \textbf{31.07} & \underline{13.22} & \textbf{0.0021} & \underline{10.55} & 
    \underline{10.37} & \underline{9.84} & \textbf{0.0015} & \textbf{8.03} & 12.07 \\

        \hline\hline       
    \end{tabular}}
    \vspace{-0.1in}
    \caption{{\bf Quantitative Comparisons Across Diverse Real-World Datasets.} Metrics include SAD, MSE, Grad, and Conn. lower values indicate better performance. Bold numbers indicate the best performance.}
    \label{tab:main}
    \vspace{-0.1in}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{sec/fig/4_main.pdf}
    \vspace{-0.25in}
    \caption{{\bf Qualitative Comparisons Across Diverse Real-World Datasets.} Our method demonstrates superior generalization ability across various category-diverse real-world datasets, surpassing category-specific models. It shows enhanced semantic understanding, and improved detail-handling capability in complex scenes compared to mask-guided methods.}
    \label{fig:mainres}
    \vspace{-0.2in}
\end{figure*}

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{2.0mm}{
    \begin{tabular}{l|c|c|c|c}
    \hline\hline
         Method & SAD & MSE & Grad & Conn \\
         \hline
     MGM$^\dagger$~\cite{park2023mgmwild} &28.68 &0.0034 &15.07 &26.19\\
     MaGGIe~\cite{huynh2024maggie} &\underline{20.01} &\underline{0.0021}
     &12.53 &17.49\\   InstMatt~\cite{sun2023sparsemat} &\textbf{19.48} &\textbf{0.0017} &\underline{11.61}
     &\textbf{17.02}\\ 
         \hline
     Mask2Alpha(ours) &20.35 &\underline{0.0021} &\textbf{11.28} &\underline{17.08}\\
         \hline\hline
    \end{tabular}}
    \vspace{-0.1in}
    \caption{{\bf Quantitative Comparison with Instance-Aware Methods.} Unlike instance-aware methods that are trained on multi-instance human-specific datasets, our approach has not been trained on such datasets yet still demonstrates strong competitiveness in instance awareness across diverse scenarios.}
    \label{tab:instance_aware}
    \vspace{-0.1in}
\end{table}

In this chapter, we provide a comprehensive evaluation of our proposed Mask2Alpha framework, detailing the experimental setup in Section~\ref{sec:detail}, we describe the datasets, evaluation metrics, and training details used in our experiments.
In Section~\ref{sec:main}, we compare Mask2Alpha's performance against state-of-the-art methods and evaluate its generalization across various datasets. In Section~\ref{sec:ablat}, we conduct ablation studies to demonstrate the contributions of key components to overall performance, with detailed results provided in the appendix.

\subsection{Implementation Details}
\label{sec:detail}
{\bf Datasets.}
For training, we only use the DIM\cite{xu2017dim} and COCO\cite{lin2014coco} datasets. To rigorously assess our model's generalization across different domains, we evaluate it on several distinct natural datasets, including AIM-500\cite{li2021aim500} for natural images, P3M-500-NP\cite{rethink_p3m} for human segmentation, and AM-2K\cite{li2022am2k} for animal segmentation. These datasets allow us to examine our model’s accuracy and versatility across various foreground types. To assess instance recognition and robustness with different mask types, we include evaluations on the M-HIM2K~\cite{huynh2024maggie} dataset, which provides high-quality, instance-specific human masks.

\noindent
{\bf Evaluation Metrics.}
We employ four widely recognized metrics for evaluating image matting performance: Sum of Absolute Differences (SAD), Mean Squared Error (MSE), Gradient (Grad), and Connectivity (Conn). For each metric, lower values indicate better performance. 

\noindent
{\bf Training Details.}
Following previous works\cite{park2023mgmwild,yu2021mgm,xu2017dim,Lu2019indices}, we utilize DIM and COCO as the matting and background datasets for our experiments. Specifically, we leverage pre-trained weights from ViT-Adapter\cite{chen2022vitadapter} and BEiTv2\cite{peng2022beitv2} as the image encoder, which provide strong feature extraction capabilities. To accommodate high-resolution image processing, we set the cropping size to 1024 pixels.

\subsection{Main Results}
\label{sec:main}

We evaluate our model across three key dimensions: generalization ability across object types in natural matting, instance-awareness for distinguishing multiple objects, and matting capability in complex real-world scenarios.

\noindent
{\bf Generalization in Natural Matting.} To evaluate the generalization capability of our model across various object categories, we benchmark it against three representative category-specific matting methods: P3M-ViTAE\cite{rethink_p3m} for human matting, GFM\cite{li2022am2k} for animals, and AIM\cite{li2021aim500} for natural scenes. The evaluation spans three datasets aligned with these categories: P3M-500-NP for human portraits, AM2K for animals, and AIM-500 for natural images. We further compare our model with several state-of-the-art mask-guided approaches, including MaGGIe\cite{huynh2024maggie}, MGM\cite{yu2021mgm}, and MGM-in-the-Wild\cite{park2023mgmwild}. For clarity, we denote MGM-in-the-Wild as MGM$^\dagger$ hereafter, as we re-implemented this model following its original training setup.

Our model, trained solely on the DIM dataset, demonstrates strong generalization across domains without any fine-tuning for specific categories. As shown in Table \ref{tab:main}, it achieves high-quality results across all object categories. In contrast, category-specific methods perform best within their respective domains but show limitations in others, as evidenced in the qualitative results shown in Figure \ref{fig:mainres}. Additionally, compared to other mask-guided methods, our approach achieves state-of-the-art performance and performs significantly better than MGM$^\dagger$, which is tailored specifically for in-the-wild scenarios.

\noindent
{\bf Instance Awareness and Differentiation.} To evaluate our model's capability in distinguishing multiple objects within a scene, we conduct experiments on the natural subset of the M-HIM2K\cite{huynh2024maggie} dataset, using segmentation results from the R50-C4-3x\cite{cheng2022mask2former} model as guidance. Unlike instance-aware methods trained specifically on multi-instance human datasets, our approach has not been trained on such specialized datasets. Nonetheless, as shown in Table \ref{tab:instance_aware}, 
our model demonstrates strong instance-awareness, effectively identifying and separating overlapping objects, and maintains competitive performance across diverse scenarios.

\noindent
% {\bf Matting in Complex Real-World Scenarios.} Lastly, we evaluate our model’s matting capability in complex real-world environments using the COCO dataset. The COCO dataset introduces diverse and intricate backgrounds, testing our model’s resilience to complex occlusions and background noise. As illustrated in Figure , our model performs consistently well, generating accurate and continuous alpha mattes. These results underscore our model’s adaptability to varied real-world conditions, enabling reliable matting performance in unpredictable scenarios.

\subsection{Ablation Study}
\label{sec:ablat}
We conduct ablation studies on the three main modules in our Mask2Alpha framework, using AIM-500 as the default dataset unless otherwise specified.Due to space limitations, we have placed some qualitative experiments in the supplementary material.

\noindent
{\bf Analysis of Mask-Guided Feature Selection Module.} This module enhances the model’s capability to differentiate instances effectively. To evaluate its impact, we visualize instance selection results on the M-HIM2K dataset and compare performance with and without the module. The results demonstrate significant improvements in instance differentiation, as shown in Table \ref{tab:MGFSM_ablation}.

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{2.5mm}{
    \begin{tabular}{l|c|c|c|c}
    \hline\hline
         Method & SAD & MSE & Grad & Conn \\
         \hline
         w/o MGFSM & 45.90 & 0.0178 & 42.94 & 47.82 \\
         w. MGFSM & 36.05 & 0.0121 & 29.80 & 34.12 \\
         \hline\hline
    \end{tabular}}
    \vspace{-0.1in}
    \caption{Ablation results of Mask-Guided Feature Selection Module.}
    \label{tab:MGFSM_ablation}
    \vspace{-0.1in}
\end{table}


\noindent
{\bf Analysis of Iterative Decoding.} We investigate the effect of varying iteration counts within the iterative decoding process in our Mask2Alpha framework. Specifically, we conduct ablation studies with different decoding iterations to assess how iteration count impacts output quality. As shown in Table \ref{tab:iter_steps}, increasing the number of iterations consistently refines the details of the final output, with optimal performance achieved at N iterations.

% Additionally, we evaluate the impact of the state transition matrix on the stability of the decoding process. As shown in Table \ref{tab:state_trans}, by introducing structured transition states, the matrix reduces unpredictable variations across iterations, resulting in more stable and reliable outputs. This stability contributes to improved overall results, allowing the model to progressively enhance detail while minimizing erratic changes. These findings underscore the combined importance of iterative refinement and the state transition matrix in achieving high-quality and dependable predictions.

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{2.5mm}{
    \begin{tabular}{l|c|c|c|c|c}
    \hline\hline
         Steps \( N \) & None & 3 & 6 & 10 & 20 \\
         \hline
         SAD & 37.98 & 37.80 & 36.83 & 36.54 & 36.53 \\
         \hline
         Time (s)& n/a & 0.030 & 0.054 & 0.087& 0.167 \\
         \hline\hline
    \end{tabular}}
    \vspace{-0.1in}
    \caption{Impact of Iteration Steps \( t \) on the Accuracy and Efficiency of Mask2Alpha.}
    \label{tab:iter_steps}
    \vspace{-0.1in}
\end{table}

% \begin{table}[t]
%     \centering
%     \setlength{\tabcolsep}{1mm}{
%     \begin{tabular}{l|c|c|c|c}
%     \hline\hline
%          Method & SAD & MSE & Grad & Conn \\
%          \hline
%          w/o state translation matrix  \\
%          w. state translation matrix  \\
%          \hline\hline
%     \end{tabular}}
%     \vspace{-0.1in}
%     \caption{Ablation results of state translation matrix.}
%     \label{tab:state_trans}
%     \vspace{-0.1in}
% \end{table}


\noindent
{\bf Analysis of Self-Guided Sparse Detail Recovery.} This module is compared with the SparseMat method by visualizing activation indices and assessing the ability to selectively recover high-resolution details. As illustrated in Figure~\ref{fig:activate}, our self-guided approach automatically activates more regions based on fine details, especially around boundary areas in synthetic and real scenes. Visual results show that our method captures finer details more accurately. Additionally, Table~\ref{tab:sgsdr_ablation} compares our module with an alternative approach that incorporates the SparseMat module into our method, demonstrating that our approach achieves better performance while preserving computational efficiency.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{sec/fig/activatevisual.pdf}
    \caption {Qualitaive results of sparse activation maps. The second row presents sparse activation maps, comparing our method and Sparsemat\cite{sun2023sparsemat}. Our self-guided approach automatically activates more regions based on fine-grained details.}
    \label{fig:activate}
\end{figure}

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{l|c|c|c|c}
    \hline\hline
         Method & SAD & MSE & Grad & Conn \\
         \hline
         Baseline & 45.87 & 26.68 & 64.70 & 76.02 \\
        Baseline + SHM & 40.40 & 24.38 & 42.94 & 77.82 \\
         Baseline + SGSDR(Ours)& 37.05 & 19.98 & 28.80 & 53.47 \\
         \hline\hline
    \end{tabular}}
    \vspace{-0.1in}
    \caption{Comparison of Mask2Alpha with Self-Guided Sparse Detail Recovery (SGSDR) or SHM, where the baseline represents the result without detail recovery at low resolution.}
    \label{tab:sgsdr_ablation}
    \vspace{-0.1in}
\end{table}
