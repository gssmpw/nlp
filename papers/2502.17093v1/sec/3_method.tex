\section{Mask2Alpha}
\begin{figure*}[t]
    \centering \includegraphics[width=0.8\linewidth]{sec/fig/framev5.pdf}
    \caption{\textbf{The Pipeline of our Mask2Alpha.}The process begins with the Input Image and Initial Mask, which are processed by the Mask-Guided Image Encoder to extract multi-scale features guided by semantic regions. These features are then passed to the Iterative Decoding stage, where alpha mattes are progressively refined over multiple iterations. Finally, the  Self-Guided Sparse Detail Recovery stage uses adaptive fusion with confidence-weighted feature maps to output the final refined alpha matte with enhanced high-resolution detail and precision.}
    \label{fig:frame}
\end{figure*}

Our framework, illustrated in \cref{fig:frame}, processes an image \( I \in [0, 255]^{3 \times H \times W} \) and a binary guidance mask \( M \in \{0, 1\}^{H \times W} \), producing an alpha matte \( A \in [0, 1]^{H\times W} \). The pipeline consists of three main stages: (1) Mask-Guided Image Encoder (Section~\ref{sec:encoder}): The input image and mask are resized to \( H // 2 \times W // 2 \) and passed through the encoder to extract multi-scale features. (2) Iterate Decoding (Section~\ref{sec:iter}): This stage takes the multi-scale image features and a resized \( H // 4 \times W // 4 \) mask as input, and iteratively refines the output to generate a low-resolution alpha matte along with a confidence map. (3) Self-Guided Sparse Detail Recovery (Section~\ref{sec:decoder}): The decoder uses the low-resolution alpha and the guidance map generated by the confidence map to produce the final high-resolution alpha matte at \( H \times W \) resolution. Through these stages, our method progressively refines alpha matte predictions, improving both detail and accuracy at higher resolutions.

Finally, Section~\ref{sec:procedure} outlines our training and inference process, focusing on the iterative refinement mechanism and strategies. 

% Section~\ref{sec:explore} explores key challenges encountered in mask-guided matting within natural scenes, presenting experimental insights that underscore the need for our approach. Section~\ref{sec:architecture} details the architecture, elaborating on the components that drive Mask2Alpha’s matting refinement. employed to balance performance and efficiency.



\subsection{Mask-Guided Image Encoder}
\label{sec:encoder}
We use a self-supervised pretrained ViT as our image feature extractor, leveraging its strong capacity for feature extraction. To address the inherent limitations of vanilla ViTs in capturing multi-scale and spatial-semantic information, we incorporate a ViT-adapter~\cite{chen2022vitadapter} framework, enabling more comprehensive feature representation. The superior capacity of self-supervised pretrained ViT in encoding image features significantly contributes to the improved performance of our matting approach.

To enable the network to be effectively guided by masks for the purpose of extracting specific content, common approaches, such as the mask attention mechanism in Mask2Former~\cite{cheng2022mask2former} and the Soft-Masked Attention technique in HODOR~\cite{athar2022hodor}, tend to excessively focus on foreground regions. While this focus can enhance foreground extraction in segmentation tasks, applying these methods to image matting may lead to the loss of essential edge details, impacting the quality of fine boundary preservation.

To address this limitation, we propose a Mask-Guided Feature Selection Module(MGFSM) that maintains the network's ability to emphasize foregrounds while preserving edge sensitivity. We construct the mask in a trimap-like manner, assigning distinct values to different regions to provide semantic guidance. 

In the final block of the self-supervised ViT, we apply operations to the self-supervised multi-head attention mechanism, guiding the attention toward region-specific features using a region-specific attention matrix \( \mathbf{S} \). This matrix is defined as:

\[
\mathbf{S} = (\beta \cdot \mathbf{M} + r) \odot (\mathbf{Q} \mathbf{K}^{T}_{\neg \text{[CLS]}}),
\]

where \( \beta \) and \( r \) are scalar parameters, and \( \mathbf{Q} \mathbf{K}^{T}_{\neg \text{[CLS]}} \) represents the attention weights, computed by excluding the [CLS] token. The matrix \( \mathbf{M} \) is a region relevance matrix, which combines with \( \beta \) and \( r \) to apply weighted alignment, ensuring that the attention mechanism focuses more effectively on relevant features in different regions. The matrix \( \mathbf{M} \) is constructed based on region relevance:
\[
\mathbf{M} =
\begin{cases}
    1, & \text{if edge region,} \\
    2, & \text{if foreground,} \\
    0.5, & \text{if background,}
\end{cases}
\]
The adjusted attention output at the last layer is given by:
\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{S +\mathbf{Q}\mathbf{K}^\top}{\sqrt{C}}\right)\mathbf{V}.
\]
 This layer selectively amplifies relevant features, enhancing region-specific detail while preserving the [CLS] token’s integrity.

\subsection{Iterate Decoding}
\label{sec:iter}
Directly predicting alpha values in a single step increases the complexity, as the model must handle both coarse structures and intricate details simultaneously, leading to suboptimal performance in challenging regions.

Inspired by the non-autoregressive generative image transformer used in models like MaskGIT~\cite{maskgit}, we introduce a multi-stage alpha prediction framework that progressively refines the alpha matte over several iterations. This iterative approach simplifies the generation process by breaking it down into manageable steps, allowing the model to incrementally improve the quality of the alpha matte. Each stage fine-tunes the output of the previous iteration, enabling the model to gradually enhance both global context and local details, ultimately producing more accurate alpha predictions.

In our proposed method, we initialize the input with image features and an initial mask, aiming to iteratively refine the alpha predictions by guiding the coarse mask \(\mathbf{M}_{coarse}\) toward a fine-grained alpha matte \(\mathbf{M}_{fine}\). To control the accuracy of each refinement step, we define a confidence score map \( p_\theta(m_{t}^{i,j}|m_{t-1}^{i,j}) \) that predicts the confidence score of the current results. At each step, we selectively sample high-confidence points from this map, which correspond to elements to be transferred. These sampled high-confidence points serve as the input for the next iteration, driving the transition from coarse to refined alpha prediction.

 In our framework, we implement a state transition approach and represent the mask and alpha as two discrete states, initially labeled as \(x_{0}\) and \(x_{T}\), respectively. This allows us to record and control each state transition step, ensuring that the refinement process is stable and progresses unidirectionally toward a fixed outcome.

Formally, let each pixel’s state at timestep \( t \) be denoted by a binary variable \( x_t^{i,j} \), where \( x^{i,j}_0 = [1, 0] \) (representing the coarse state) and \( x^{i,j}_T = [0, 1] \) (representing the fine state) for pixel \((i, j)\). Each refinement step is described by a transition matrix \( \mathbf{Q}_t \), defined as:
\[
\mathbf{Q}_t = 
\begin{bmatrix}
\beta_t & 1 - \beta_t \\
0 & 1 \\
\end{bmatrix},
\]
This matrix ensures that each pixel has a probability \( \beta_t \) of transitioning from the coarse to the fine state, while those already in the coarse state remain unchanged. The forward process is therefore formulated as:
\[
q(x_{t}^{i,j}|x_{t-1}^{i,j}) = x_{t-1}^{i,j} \mathbf{Q}_{t},
\]
By utilizing this unidirectional transition process, we ensure that the refinement of each pixel ultimately converges to the fine state, resulting in a stable alpha matte output aligned with the original mask guidance. This design also mitigates randomness, guaranteeing consistent and precise refinement across iterations.

To enable the network to effectively quantify its prediction confidence, we introduce a confidence loss term, designed to supervise the network in producing confidence scores for the predicted alpha values. The loss function measures the discrepancy between the true and predicted values, guiding the network to reflect this difference in its confidence estimation. Specifically, it is defined as:
\[
\mathcal{L}_C = \left| c_{\alpha} - \left| \alpha_i^p - \alpha_i^g \right| \right|_1,
\]
where \( c_{\alpha} \) represents the confidence score output for each pixel \( i \), and \( \alpha_i^p \) and \( \alpha_i^g \) denote the predicted and ground truth alpha values at pixel \( i \), respectively. This formulation leverages an \( L_1 \) penalty, providing a direct measure of alignment between confidence and prediction accuracy.
\subsection{Self-Guided Sparse Detail Recovery}
\label{sec:decoder}
While the iterate decoding method generates more finegrained alpha matte, it demands substantial memory and computation. To deal with this issue, we introduce the Self-Guided Sparse Detail Recovery (SGSDR) module, building on the foundation of the Sparse High-resolution Module\cite{sun2023sparsemat} (SHM). The SHM selectively enhances details by activating pixels in sparse convolutions, guided by a sparsity map derived from the low-resolution alpha matte \( \mathbf{A}_{\text{l}} \). However, SHM faces the challenge of determining the optimal active regions: the threshold is sensitive, as a value that is too large increases computational costs, while a value that is too small risks losing critical details.

To overcome this limitation, SGSDR introduces a confidence map \( \mathbf{C} \), which adaptively identifies regions requiring refinement. This confidence map is derived from the iterative decoding process and provides a confidence score \( \mathbf{C}(i, j) \) for each pixel. The confidence score is low in areas where the low-resolution output is uncertain, guiding SGSDR to prioritize these regions for further refinement. By leveraging this confidence map, SGSDR can focus on the most challenging areas, improving the overall accuracy and quality of the matting results.

Using this confidence map, SGSDR creates an adaptive sparsity map \( \mathbf{M}_{\text{SG}} \):
\[
\mathbf{M}_{\text{SG}}(i, j) = 
\begin{cases} 
      1, & \text{if } \mathbf{C}(i, j) > \tau,\\
      0, & \text{otherwise},
   \end{cases}
\]
where \( \tau \) is a threshold that controls the activation region, balancing detail recovery with computational efficiency.

With \( \mathbf{M}_{\text{SG}} \) as input, SGSDR performs sparse convolution operations on selected pixels to recover details in the high-resolution alpha matte \( \mathbf{A}_{\text{h}} \). This process can be represented as:
\[
\mathbf{A}_{\text{h}} = \text{SparseResNet}(\mathbf{A}_{\text{l}}, \mathbf{M}_{\text{SG}}),
\]
By incorporating the confidence-guided \( \mathbf{M}_{\text{SG}} \), SGSDR achieves effective detail recovery with reduced computational load, focusing refinement on areas that most benefit from high-resolution enhancement. This approach yields an optimized high-resolution alpha matte while maintaining efficiency.


\begin{algorithm}[t]
\input{sec/algo/train}
\end{algorithm}

\subsection{Training and Inference for Iterate Decoding}
\label{sec:procedure}

\paragraph{Training}
The training process is structured as follows (see Algorithm \ref{algo:train}). We start with the total number of iteration steps \( T \) and a dataset \( \mathcal{D} = \{(\mathcal{F}, \mathcal{M}_{fine}, \mathcal{M}_{coarse})^K\} \). During each iteration, we sample a training example \( (\mathcal{F}, \mathcal{M}_{fine}, \mathcal{M}_{coarse}) \) from the dataset and randomly select an iteration step \( t \) uniformly from the range \( 1 \) to \( T \). We initialize the mask \( m_0 \) with \( M_{fine} \) and set the transition variable \( x_0^{i,j} \) to \([1]\).

Next, we compute the transition probability \( q(x_{t}^{i,j}|x_{0}^{i,j}) \) and sample \( x_{t}^{i,j} \) from this distribution, yielding a binary representation \( x_t \in \{0, 1\}^{2 \times H \times W} \). The transition of pixels is then determined using the formula:
\[
m_t = x_t[0] \odot \mathbf{M}_{fine} + x_t[1] \odot \mathbf{M}_{coarse},
\]

Finally, we perform a gradient descent step to optimize the loss function:
\[
\nabla_\theta \mathcal{L}(f_\theta(I, m_t, t), \mathbf{M}_{fine}),
\]

This process continues until convergence is achieved.

\begin{algorithm}[t]
\input{sec/algo/inference}
\end{algorithm}

\paragraph{Inference.}
The inference procedure, detailed in Algorithm \ref{algo:inference}, begins by initializing the transition variable \( \mathbf{x}_T = [0] \) and setting the mask \( \mathbf{m}_T = \mathbf{M}_{coarse} \). For each iteration \( t \) from \( T \) down to \( 1 \), the process involves predicting and refining the mask based on the following steps.

First, the predicted fine mask \( \tilde m_{0|t} \) and its probability \( p_\theta(\tilde m_{0|t}) \) are computed using the function \( f_\theta(I, m_t, t) \), which incorporates image features, the current mask \( m_t \), and time step \( t \). 

Next, the transition probability \( p_\theta(x_{t-1}^{i,j}|x_{t}^{i,j}) \) is defined based on the current state, guiding the sampling of \( x_{t-1}^{i,j} \) from this transition distribution, resulting in the binary variable \( x_t \in \{0, 1\}^{2 \times H \times W} \).

The mask \( m_{t-1} \) is then updated by combining \( \tilde m_{0|t} \) and \( \mathbf{M}_{coarse} \) through:
\[
m_{t-1} = x_{t-1}[0] \odot \tilde{\mathbf{m}}_{0|t} + x_{t-1}[1] \odot \mathbf{M}_{\text{coarse}},
\]


After completing all iterations, the refined mask \( m_0 \) is returned. This procedure gradually transitions the mask to higher levels of detail while leveraging both the coarse mask and fine-grained predictions for optimal refinement.

