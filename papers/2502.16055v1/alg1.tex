\begin{algorithm} 
    \caption{Dataset Distillation of Branch Private Data}    
    \label{algorithm:alg1}       
    \begin{algorithmic}[1] 
    \Require $\theta_{main}$: pretrained main model parameters; $E$: number of distillation steps; 
    \Ensure distilled data $S$
    \State Initialize ${S=\{{s_i},\tilde{y_i}\}^M_{i=1}}$ randomly, 
    
    \For {distillation steps $e=1$ to $E$}
    \State $\theta \leftarrow \theta_{main} + \theta_{LoRA}$, $\theta_{LoRA}$ is randomly initialized
    \State $\psi_{\boldsymbol{\vartheta}}$ $\leftarrow$ image feature extractor of $\theta $
    \State Sample a minibatch $B^\mathcal{T} \sim \mathcal{T}$ and $B^\mathcal{S} \sim \mathcal{S}$
    \State Compute $\mathcal{L}_{DM}(\mathcal{T},\mathcal{S},\psi_{\boldsymbol{\vartheta}})$
    \State $\mathcal{S} \leftarrow SGD\ (\mathcal{S};\mathcal{L}_{DM})$
    \State Update $\theta_{LoRA}$ with $S$
    \EndFor
    \end{algorithmic} 
\end{algorithm} 
