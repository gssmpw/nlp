\section{Method}

In Fig. \ref{fig:overview}, we illustrate two major stages of MedForge for collaborative model development, including feature branch development (Sec~\ref{branch}) and model merging (Sec~\ref{forging}). In the feature branch development, individual contributors (i.e., medical centers) could make individual knowledge contributions asynchronously. Our MedForge allows each contributor to develop their own plugin module and distilled data locally without the need to share any private data. In the model merging stage, MedForge enables multi-task knowledge integration by merging the well-prepared plugin module asynchronously. This key integration process is guided by the distilled dataset produced by individual branch contributors, resulting in a generalizable model that performs strongly among multiple tasks.


\subsection{Preliminary}
\label{pre}
In MedForge, the development of a multi-capability model relies on the multi-center and multi-task knowledge introduced by branch plugin modules and the distilled datasets.
The relationship between the main base model and branch plugin modules in our proposed MedForge is conceptually similar to the relationship between the main repository and its branches in collaborative software version control platforms (e.g., GitHub~\cite{github}). 
To facilitate plugin module training on branches and model merging, we use the parameter-efficient finetuning (PEFT) technique~\cite{hu2021lora} for integrating knowledge from individual contributors into the branch plugin modules. 

\subsubsection{Parameter-efficient Finetuning}
Compared to resource-intensive full-parameter finetuning, parameter-efficient finetuning (PEFT) only updates a small fraction of the pretrained model parameters to reduce computational costs and accelerate training on specific tasks. These benefits are particularly crucial in medical scenarios where computational resources are often limited.
As the representative PEFT technique, LoRA (Low-Rank Adaptation)~\cite{hu2021lora} is widely utilized in resource-constrained downstream finetuning scenarios. In our MedForge, each contributor trains a lightweight LoRA on a specific task as the branch plugin module. LoRA decomposes the weight matrices of the target layer into two low-rank matrices to represent the update made to the main model when adapting to downstream tasks. If the target weight matrix is $W_0 \in R^{d \times k}$, during the adaptation, the updated weight matrix can be represented as $W_0+\Delta W=W_0+B A$, where $B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}$ are the low-rank matrices with rank $r \ll  \min (d, k)$ and $AB$ constitute the LoRA module. 



\subsubsection{Dataset Distillation}
Dataset distillation~\cite{wang2018dataset, yu2023dataset, lei2023comprehensive} is particularly valuable for medicine scenarios that have limited storage capabilities, restricted transmitting bandwidth, and high concerns for data privacy~\cite{li2024dataset}. 
We leverage the power of dataset distillation to synthesize a small-scale distilled dataset from the original data.

The distilled datasets serve as the training set in the subsequent merging stage to allow multi-center knowledge integration. Models trained on this distilled dataset maintain comparable performance to those trained on the original dataset (\ref{tab:main_res}). Moreover, the distinctive visual characteristics among images of the raw dataset are blurred (see \ref{fig:overview}(a)), which alleviates the potential patient information leakage. 

To perform dataset distillation, we define the original dataset as $\mathcal{T}=\{x_i,y_i\}^N_{i=1}$ and the model parameters as $\theta$. The dataset distillation aims to synthesize a distilled dataset ${\mathcal{S}=\{{s_i},\tilde{y_i}\}^M_{i=1}}$ with a much smaller scale (${M \ll N}$), while models trained on $\mathcal{S}$ can show similar performance as models trained on $\mathcal{T}$. 
This process is achieved by narrowing the performance gap between the real dataset $\mathcal{T}$ and the synthesized dataset $\mathcal{S}$. In MedForge, we utilize the distribution matching (DM)~\cite{zhao2023dataset}, which increases data distribution similarity between the synthesized distilled data and the real dataset
The distribution similarity between the real and synthesized dataset is evaluated through the empirical estimate of the Maximum Mean Discrepancy (MMD)~\cite{gretton2012kernel}:
\begin{equation}
\mathbb{E}_{\boldsymbol{\vartheta} \sim P_{\vartheta}}\left\|\frac{1}{|\mathcal{T}|} \sum_{i=1}^{|\mathcal{T}|} \psi_{\boldsymbol{\vartheta}}\left(\boldsymbol{x}_i\right)-\frac{1}{|\mathcal{S}|} \sum_{j=1}^{|\mathcal{S}|} \psi_{\boldsymbol{\vartheta}}\left(\boldsymbol{s}_j\right)\right\|^2
\end{equation}

where $P_\vartheta$ is the distribution of network parameters, $\psi_{\boldsymbol{\vartheta}}$ is a feature extractor. Then the distillation loss $\mathcal{L}_{DM}$ is:
\begin{equation}\scalebox{0.9}{$
\mathcal{L}_{\mathrm{DM}}(\mathcal{T},\mathcal{S},\psi_{\boldsymbol{\vartheta}})=\sum_{c=0}^{C-1}\left\|\frac{1}{\left|\mathcal{T}_c\right|} \sum_{\mathbf{x} \in \mathcal{T}_c} \psi(\mathbf{x})-\frac{1}{\left|\mathcal{S}_c\right|} \sum_{\mathbf{s} \in \mathcal{S}_c} \psi(\mathbf{s})\right\|^2$}
\end{equation}

We also applied the Differentiable Siamese Augmentation (DSA) strategy~\cite{zhao2021dataset} in the training process of distilled data to enhance the quality of the distilled data. DSA could ensure the distilled dataset is representative of the original data by exploiting information in real data with various transformations. The distilled images extract invariant and critical features from these augmented real images to ensure the distilled dataset remains representative.
\input{assets/img/model_arch}
\subsection{Feature Branch Development}
\label{branch}
In the feature branch development stage, the branch contributors are responsible for providing the locally trained branch plugin modules and the distilled data to the MedForge platform, as shown in Fig~\ref{fig:overview} (a).
In collaborative software development, contributors work on individual feature branches, push their changes to the main platform, and later merge the changes into the main branch to update the repository with new features. Inspired by such collaborative workflow, branch contributors in MedForge follow similar preparations before the merging stage, enabling the integration of diverse branch knowledge into the main branch while effectively utilizing local resources.

MedForge consistently keeps a base module and a forge item as the main branch. The base module preserves generative knowledge of the foundation model pretrained on natural image datasets (i.e., ImageNet~\cite{deng2009imagenet}), while the forge item contains model merging information that guides the integration of feature branch knowledge (i.e., a merged plugin module or the merging coefficients assigned to plugin modules). 
Similar to individual software developers working in their own branches, each branch contributor (e.g., individual medical centers) trains a task-specific plugin module using their private data to introduce feature branch knowledge into the main branch. These branch plugin modules are then committed and pushed to update the forge items of the main branch in the merging stage, thus enhancing the model's multi-task capabilities.


\input{assets/img/merge}

Regarding model architecture, MedForge contains a base module and a plugin module (Fig ~\ref{fig:model_arch}). The base module is pretrained on general datasets (e.g., ImageNet) and remains the model parameters frozen in all processes and branches (main and feature branches) to avoid catastrophic forgetting of foundational knowledge acquired from pretraining. Meanwhile, the plugin module is adaptable for knowledge integration and can be flexibly added or removed from the base module, allowing updates without affecting the base model. In our study, we use the pretrained CLIP~\cite{radford2021learning} model as the base module. For the language encoder and projection layer of the CLIP model, all the parameters are frozen, which enables us to directly leverage the language capability of the original CLIP model. For the visual encoder, we apply LoRA on weight matrices of query ($W_q$) and value ($W_v$), following the previous study~\cite{hu2021lora}. To better adapt the model to downstream visual tasks, we apply the LoRA technique to both the visual encoder and the visual projection, and these LoRA modules perform as the plugin module. During the training, only the plugin module (LoRA modules) participates in parameter updates, while the base module (the original CLIP model) remains unchanged. 

In addition to the plugin modules, the feature branch contributors also develop a distilled dataset based on their private local data, which encapsulates essential patterns and features, serving as the foundation for training the merging coefficients in the subsequent merging stage~\ref{forging}. Compared to previous model merging approaches that rely on whole datasets or few-shot sampling, distilled data is lightweight and representative, mitigating the privacy risks associated with sharing raw data. 
We illustrate our distillation procedure in Algorithm~\ref{algorithm:alg1}. In each distillation step, the synthesized data $\mathcal{S}$ will be updated by minimizing $\mathcal{L}_{DM}$.
\input{alg1}


\subsection{MedForge Merging Stage}
\label{forging}
Following the feature branch development stage illustrated in Fig~\ref{fig:overview} (a), branch contributors push and merge their branch plugin modules along with the corresponding distilled dataset into the main branch, as shown in Fig~\ref{fig:overview} (b). Our MedForge allows an incremental capability accumulation from branches to construct a comprehensive medical model that can handle multiple tasks.

In the merging stage, the $i^{th}$ branch contributor is assigned a coefficient $w'_i$ for the contribution of merging, while the coefficient for the current main branch is $w_i$. By adaptively adjusting the value of coefficients, the main branch can balance and coordinate updates from different contributors, ultimately enhancing the overall performance of the model across multiple tasks.
The optimization of the coefficients is done by minimizing the cross-entropy loss for classification based on the distilled datasets. We also add $L1$ regularization to the loss to regulate the weights to avoid outlier coefficient values (e.g., extremely large or small coefficient values)~\cite{huang2023lorahub}. During optimization, following~\cite{huang2023lorahub}, we utilize Shiwa algorithm~\cite{liu2020versatile} to enable model merging under gradient-free conditions, with lower computational and time costs. The optimizer selector~\cite{liu2020versatile} automatically chooses the most suitable optimization method for coefficient optimization. 

In the following sections, we introduce the two merging methods used in our MedForge: Fusion and Mixture. In MedForge-Fusion, the parameters of the branch plugin modules are fused into the main branch after each round of the merging stage. For MedForge-Mixture, the outputs of the branch modules are weighted and summed based on their respective coefficients rather than directly applying the weighted sum to the model parameters. This largely preserves the internal parameter structure of each branch module.

\paragraph{MedForge-Fusion}
In MedForge, forge items are utilized to facilitate the integration of branch knowledge into the main branch.
For MedForge-Fusion, the forge item refers to adaptable main plugin modules. When the $i^{th}$ branch contributor pushes its branch plugin module $\theta'_i=A'_iB'_i$ to the main branch, the current main plugin module $\theta_{i-1}=A_{i-1}B_{i-1}$ will be updated to $\theta_{i}=A_{i}B_{i}$. The parameters of the branch and the current main plugin modules are weighted with coefficients and added to fuse a new version. The $A_i$, $B_i$ are the low-rank matrices composing the LoRA module $\theta_i$. The detailed fusion process can be represented as:
\begin{equation}
\theta_{i}=(w_i A_{i-1}+w'_i A'_i)(w_i B_{i-1}+ w'_i B'_i)
\end{equation}
Where $w_i$ is the coefficient assigned to the current main branch, while $w'_i$ is the coefficient assigned to the branch contributor. After this round of merging, the resulting plugin module $\theta_{i}$ is the updated version of main forging item, thus the main model is able to obtain new capacity introduced by the current branch contributor. When new contributors push their plugin modules and distilled datasets, the main branch can be incrementally updated through merging stages, and the optimization of the coefficients is guided by distilled data.
As shown in Fig.~\ref{fig:merge}, though multiple contributors commit their branch plugin modules and distilled datasets at different times, they can flexibly merge their plugin modules with the current main branch. After each merging round, the plugin module of the main branch will be updated, and thus the version iteration has been achieved.
\input{assets/img/mixmerge}

\paragraph{MedForge-Mixture}
To further improve the model merging performance, inspired by~\cite{zhao2024loraretriever}, we also propose medForge-mixture. For MedForge-Mixture, the forge items refer to the optimized coefficients.
As shown in Fig.~\ref{fig:mixmerge}, for MedForge-Mixture, the coefficient of each branch contributor is acquired and optimized based on distilled datasets. Then the outputs of plugin modules will be weighted combined with these coefficients to get the merged output. 

For each merging round, with branch contributor $i$, the branch coefficient is $w'_i$, the main coefficient is $w_i$, the branch plugin module is $\theta'_i=A'_iB'_i$, and the current main plugin module is $\theta_i=A_iB_i$. With the input $x$, the resulted MedForge-Mixture output can be represented as:
\begin{equation}
y_{i}=w_i A_{i-1} B_{i-1} x+w'_i A'_i B'_i x
\end{equation}

In this way, MedForge encourages additional contributors as the workflow supports continuous incremental knowledge updates.

Overall, both MedForge merging strategies greatly improve the communication efficiency among contributors. We use this design to build a multi-task medical foundation model that enhances the full utilization of resources in the medical community. For the MedForge-Fusion strategy, the main plugin module is updated after each merging round, thus avoiding storing the previous plugin modules and saving space. Meanwhile, the MedForge-Mixture strategy avoids directly updating the parameters of each plugin module, thus preserving their original structure and preventing the introduction of additional noise, which enhances the robustness and stability of the models.
