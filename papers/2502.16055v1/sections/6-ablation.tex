\section{Ablation Study}
\subsection{The Effect of Task Merging Order}
In real-world applications, contributors can push their branch modules and distilled data anytime, leading to various fusion orders. The order of merging operations might introduce randomness and affect the robustness of overall performance. Thus, it is necessary to determine the effect of task merging order on model performance. In this ablation study, we shuffle the order of tasks and evaluate the model performance with multiple order combinations.

As shown in Table~\ref{tab:order}, MedForge could consistently achieve good performance on all three tasks (e.g., for MedForge-Mixture, B $\rightarrow$ M $\rightarrow$ L versus M $\rightarrow$ L $\rightarrow$ B, 0.941 versus 0.931 in AUC). The different orders could cause various model performances due to the inherent characteristics of the datasets. For example, BreakHis has the smallest training set, leading to greater sensitivity to the merging order. Moreover, observing the results across three tasks and the averaged performance, MedForge-Mixture consistently demonstrates greater robustness compared to the MedForge-Fusion approach. This superior performance is achieved by maintaining the integrity of each plugin moduleâ€™s internal parameters and summing their outputs rather than altering the parameters themselves. This approach ultimately enhances the model's robustness and stability.


\subsection{The Effect of Dataset Distillation} We assessed model performance using distilled data achieved by different dataset distillation strategies. We have extended to Dataset Condensation (DC) approaches and explored the significance of the Differentiable Siamese Augmentation (DSA) technique. From Table~\ref{tab:rebuttal}, we demonstrate that our distillation module (DM-DSA) achieves strong performances among datasets in average (ACC: \textbf{0.909 (DM-DSA, MixMerge)} vs. 0.875 (DC-DSA) and 0.838 (DM), AUC: \textbf{0.941 (DM-DSA, ours)} vs. 0.919 (DC-DSA) and 0.892 (DM)). Moreover, the average ACC of Mixture exceeds all baselines, indicating the superiority of MedForge. The results reveal the characteristics of different distillation methods, as the distribution matching (DM) method inclines to learn the similar distribution of raw dataset, thus becoming crucial guidance in coefficients learning at the merging stage. Meanwhile, the Differentiable Siamese Augmentation (DSA) technique further enhances the representativeness of distilled data by providing more efficient and robust feature learning.

\subsection{The Effect of Different Plugin Module}
To analyze the flexibility of MedForge by using different plugin module adoptions, we evaluate MedForge with recently proposed DoRA~\cite{liu2024dora} (weight-decomposed low-rank adaptation) in our main experiments. DoRA is strong at decomposing pre-trained weights into magnitude and direction components for model fine-tuning. DoRA improves learning capacity and training stability while minimizing trainable parameters and inference costs. 
From Table~\ref{tab:dora}, we demonstrated that MedForge can yield a consistently strong performance when using different PEFT methods (e.g., DoRA as the plugin module). While using DoRA as a plugin module, MedForge still achieves the leading performance (Mixture: ACC: 0.906, AUC: 0.937) compared to other merging strategies. Meanwhile, medForge-Mixture shows positive performance that highlights the advantage of mixture approach in maintaining the integrity of parameters within each plugin module towards enhancing model fusion performance. Overall, the effectiveness of MedForge in replacing DoRA as a plugin module demonstrates the potential of MedForge approach across different experimental settings.


\subsection{Distilled Dataset Quality Verification} We employ t-SNE to visualize the high-dimensional feature embedding of the distilled dataset and the original dataset into a two-dimensional space. Such visualization is helpful for assessing the representativeness of the distilled data concerning the original data distribution. The visualized t-SNE plots revealed that, despite using only a small number of sample points, the distilled dataset features closely approximated the distribution of the original data, highlighting the distilled data's high representativeness and informative nature. From (\ref{fig:dist_all}), we assessed the degree of fidelity with which the distilled dataset captured the essential characteristics of the original data. This design provides insights into the effectiveness of our data distillation process and the validity of using the distilled dataset to guide the optimization of the model coefficients.






