\section{Introduction}
\label{sec:intro}

Foundational models (FMs)~\cite{zhang2024data, zhou2023comprehensive} have shown remarkable progress in the healthcare domain, enabling professional-like assessment of disease diagnosis, treatment decision-making, and monitoring~\cite{zhang2023text, wang2022medclip, lu2023mi-zero}. 
Examples include LLaVA-Med~\cite{li2023llava}, Med-PaLM Multimodal~\cite{tu2024towards}, and Med-Flamingo~\cite{moor2023med}, have demonstrated their capacity on question answering, medical image analysis, and report generation.
These studies follow a predominant top-down model development strategy that requires upstream developers to collect data and train models for downstream tasks. 
Consequently, the developed model capabilities are heavily dependent on the training data, limiting their generalization performance in diverse clinical scenarios. 
For instance, Med-Gemini~\cite{yang2024advancing} reveals promising general capabilities in report generation while it lags behind state-of-the-art (SoTA) models on classification tasks, especially for out-of-domain applications. 
This indicates that while the generalizability of the foundation model is promising, more solutions are expected to meet the various specialized clinical needs.

To address these challenges, multi-center data centralization becomes essential to enhance model capacity and robustness across varied clinical scenarios~\cite{rajpurkar2022ai}. 
Centralizing distributed data can significantly improve model training and inference performance.
However, the process of medical data storage, transfer, and aggregation among centers requires extra efforts to ensure data security and system interoperability~\cite{bradford2020international}.
Moreover, a growing concern for patient privacy makes large-scale multi-center data sharing particularly challenging. 
While efforts like federated learning~\cite{wen2023survey, li2020review} can achieve good model performance on local data, the need for synchronized system coordination presents significant challenges, as clients are unable to update asynchronously. This limitation greatly restricts the practical capability of such approaches.
As a result, without a flexible collaboration, medical community still struggles to fully utilize the isolated data and local computation resources for comprehensive medical AI model development. 
To address this dilemma, open-source platforms encourage public data sharing and knowledge integration~\cite{markiewicz2021openneuro, zenodo}.
However, these platforms focus solely on raw data sharing while seldom providing collaborative model training or cooperation between different institutions.
Recently, collaborative learning has emerged as a viable approach for enhancing multi-model robustness~\cite{boulemtafes2020review}. 
For instance, software-like model development~\cite{raffel2023building} mimics software engineering practices by introducing structured workflows, enabling merging, version control, and continuous model integration.
Under this design, model ability can be strengthened with incremental knowledge updates similar to the version updating in software development. 

Although collaborative learning provides a multi-model collaboration, two key challenges remain in the leakage of raw data during collaboration~\cite{huang2023lorahub} and the synchronization of multiple collaborators~\cite{mcmahan2017communication} in the medical AI community. It is still challenging to integrate decentralized, privacy-sensitive data across institutions, leading to under-utilized insights and fragmented knowledge sharing~\cite{kaissis2020secure, rajpurkar2022ai, abdullah2021ethics}.
 To address these challenges, inspired by the collaborative software development, we propose \textbf{Med}ical \textbf{Fo}undation Models Me\textbf{rg}ing (\textbf{MedForge}), a cooperative workflow enabling continuously community-driven foundation model (FM) development.
MedForge enables a lightweight manner for individual centers to share their knowledge among multiple centers, minimizing the burden of data transmission and integration while enhancing model robustness.
Meanwhile, MedForge facilitates asynchronous and flexible collaboration, allowing individual centers to continuously update and improve medical FMs without the need for real-time synchronization.
Similar to open-source software development, MedForge incrementally updates medical knowledge and follows a sustainable model development scheme. 
This key design emphasizes a bottom-up construction of a multi-task medical FM, allowing downstream users to collaboratively build, refine, and update the upstream model according to their local resources. Our major contributions of MedForge are as below: 
\begin{enumerate}
    \item[$\bullet$] We introduce a collaborative workflow to promote the merging scheme of open-source software development. Our proposed MedForge allows distributed clinical centers to asynchronously contribute to comprehensive medical model construction while reducing transmitting costs among centers and avoiding the leakage of raw data, thus enhancing the utilization of private resources in the healthcare system. 
    \item[$\bullet$] We propose two effective knowledge-merging strategies for the asynchronous branch contribution. The MedForge-Fusion strategy updates the plugin module parameters of the main model during the merging phase, whereas the MedForge-Mixture strategy integrates the output of the plugin module by memorizing each contributor's coefficient. These strategies make MedForge more flexible and versatile. MedForge-Fusion is friendly to implement, while the MedForge-Mixture offers better performance and robustness.
    \item[$\bullet$]  We comprehensively evaluate model merging strategies to accumulate medical knowledge among multiple branch plugin modules. MedForge yields superior performance on medical classification tasks compared to other collaborative baselines across multiple datasets. We demonstrate the robustness of MedForge by shuffling the task order and evaluating various configurations of plugin modules and dataset distillation methods.
\end{enumerate}


