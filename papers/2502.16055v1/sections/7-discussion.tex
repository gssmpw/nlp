\section{Discussion}
The development of foundation models has increasingly relied on accessible data support to address complex tasks~\cite{zhang2024data}. Yet major challenges remain in collecting scalable clinical data in the healthcare system, such as data silos and privacy concerns. To overcome these challenges, MedForge integrates multi-center clinical knowledge sources into a cohesive medical foundation model via a collaborative scheme. MedForge offers a collaborative path to asynchronously integrate multi-center knowledge while maintaining strong flexibility for individual contributors.
This key design allows a cost-effective collaboration among clinical centers to build comprehensive medical models, enhancing private resource utilization across healthcare systems.

Inspired by collaborative open-source software development~\cite{raffel2023building, github}, our study allows individual clinical institutions to independently develop branch modules with their data locally. These branch modules are asynchronously integrated into a comprehensive model without the need to share original data, avoiding potential patient raw data leakage. Conceptually similar to the open-source collaborative system, iterative module merging development ensures the aggregation of model knowledge over time while incorporating diverse data insights from distributed institutions. In particular, this asynchronous scheme alleviates the demand for all users to synchronize module updates as required by conventional methods (e.g., LoRAHub~\cite{huang2023lorahub}).


MedForge's framework addresses multiple data challenges in the cycle of medical foundation model development, including data storage, transmission, and leakage. As the data collection process requires a large amount of distributed data, we show that dataset distillation contributes greatly to reducing data storage capacity. In MedForge, individual contributors can simply upload a lightweight version of the dataset to the central model developer. As a result, the remarkable reduction in data volume (e.g., 175 times less in LC25000) alleviates the burden of data transfer among multiple medical centers. For example, we distilled a 10,500 image training set into 60 representative distilled data while maintaining a strong model performance. We choose to use a lightweight dataset as a transformed representation of raw data to avoid the leakage of sensitive raw information.
Second, the asynchronous collaboration mode in MedForge allows flexible model merging, particularly for users from various local medical centers to participate in model knowledge integration. 
Third, MedForge reformulates the conventional top-down workflow of building foundational models by adopting a bottom-up approach. Instead of solely relying on upstream builders to predefine model functionalities, MedForge allows medical centers to actively contribute to model knowledge integration by providing plugin modules (i.e., LoRA) and distilled datasets. This approach supports flexible knowledge integration and allows models to be applicable to wide-ranging clinical tasks, addressing the key limitation of fixed functionalities in traditional workflows.

We demonstrate the strong capacity of MedForge via the asynchronous merging of three image classification tasks. MedForge offered an incremental merging strategy that is highly flexible compared to plain parameter average~\cite{wortsman2022model} and LoRAHub~\cite{huang2023lorahub}. Specifically, plain parameter averaging merges module parameters directly and ignores the contribution differences of each module. Although LoRAHub allows for flexible distribution of coefficients among modules, it lacks the ability to continuously update, limiting its capacity to incorporate new knowledge during the merging process. In contrast, MedForge shows its strong flexibility of continuous updates while considering the contribution differences among center modules. The robustness of MedForge has been demonstrated by shuffling merging order (Tab~\ref{tab:order}), which shows that merging new-coming modules will not hurt the model ability of previous tasks in various orders, mitigating the model catastrophic forgetting. 
MedForge also reveals a strong generality on various choices of component modules. Our experiments on dataset distillation settings (such as DC and without DSA technique) and PEFT techniques (such as DoRA) emphasize the extensible ability of MedForge's module settings. 

To fully exploit multi-scale clinical data, it will be necessary to include broader data modalities (e.g., electronic health records and radiological images). Managing these diverse data formats and standards among numerous contributors can be challenging due to the potential conflict between collaborators. 
Moreover, since MedForge integrates multiple clinical tasks that involve varying numbers of classification categories, conventional classifier heads with fixed class sizes are not applicable. However, the projection head of the CLIP model, designed to calculate similarities between image and text, is well-suited for this scenario. It allows MedForge to flexibly handle medical datasets with different category numbers, thus overcoming the challenge of multi-task classification. That said, this design choice also limits the variety of model architectures that can be utilized, as it depends specifically on the CLIP framework. Future investigations will explore extensive solutions to make the overall architecture more flexible. Additionally, incorporating more sophisticated data anonymization, such as synthetic data generation~\cite{ding2023large}, and encryption methods can also be considerable. To improve data privacy protection, test-time adaptation technique~\cite{wang2020tent, liang2024comprehensive} without substantial training data can be considered to alleviate the burden of data sharing in the healthcare system.



             
