\section{Experiments}
\subsection{Datasets}
\paragraph{BreakHis}
BreakHis~\cite{spanhol2015dataset} is a breast cancer histopathology dataset with 7,909 histopathological image patches acquired from 82 patients. We utilize images with magnification in 40${\times}$ to achieve a high-resolution image quality, which can reveal the valuable disease patterns among cancer tissues. We randomly selected 70\% of the patients and used their image samples as the training set. The image samples from the remaining patients were used as the test set. All of the images are first center cropped to 460${\times}$460 pixels as the raw images are 700${\times}$460, then resized to 224 ${\times}$ 224. 

\paragraph{LC25000}
LC25000~\cite{borkowski2019lung} contains 25,000 images of lung and colon cancer histopathology with five clinical categories. In our experiment, we include benign lung tissues and two subtypes of lung cancer for classification tasks, including lung adenocarcinomas and lung squamous cell carcinomas. The images are resized to 224 ${\times}$ 224. 70\% of the images are randomly selected as the training data, and the rest of the images serve as an evaluation set. 

\paragraph{MedFMC-Colon}
MedFMC~\cite{wang2023medfmc} is established to evaluate the adaptation performance of foundation models on real-world medical image classification tasks. In our study, we evaluate the model performance on histopathological colon tumor tissue classification task and follow the dataset split setting released by previous study~\cite{wang2023medfmc}. The MedFMC-Colon dataset contains 10,009 images. We first resized the images to 224 ${\times}$ 224 to keep with other datasets, then applied a random horizontal flip to augment the data following the previous study~\cite{wang2023medfmc}.
\input{assets/tab/main_res}
\input{assets/tab/order}
\input{assets/tab/rebuttal}
\input{assets/tab/lora}
\subsection{Implementations}
We define CLIP ViT-B/16 as the base module, with input image size 224 ${\times}$ 224, patch size 16 ${\times}$ 16, and the output dimension of the image and text projection layer are both 512. For the branch plugin module, we implemented LoRA tuning based on the Huggingface PEFT library, the default hyperparameter rank r and lora\_alpha are fixed to 16, lora\_dropout is 0.1, and bias is set to be none.
The classification prediction is determined by calculating the cosine similarity between the visual and text embeddings. Specifically, we first compute the feature embedding of the image and the candidate texts through the image encoder and text encoder. The cosine similarity between these embeddings is then calculated to determine the most probable image-text pairing. The image-text pair with the highest cosine similarity is selected as the predicted classification, indicating the closest match between the visual content and the corresponding textual description.
In our experiments, the labels of the images in the dataset are converted as the text input to the model's text encoder. Specifically, in BreakHis, the labels are 'benign breast tissues' and 'malignant breast tumors'. For LC25000, the subtype labels are 'lung squamous cell carcinomas', 'lung adenocarcinomas', and 'benign lung tissues'. While in MedFMC-Colon, the labels are 'negative colon tumor' and 'positive colon tumor'.
For coefficient optimization of merging contribution, the Cobyla~\cite{powell1994direct} strategy is automatically selected by the Shiwa algorithm~\cite{liu2020versatile} in our experiment setting. The default number of updating iterations is set to 40, and the initial weight is 0.5.

In our study, we use the above public medical datasets to mimic the scenario of the private data held by each medical center. The individual center contributors could only use their private dataset to train a branch plugin module and the corresponding distilled dataset for the specific task. Then, these individual centers contribute the best-performed branch plugin modules and the distilled datasets for the subsequent merging stage, aiming to develop an integrated model without raw private data leakage. All the results shown below are repeated with three random seeds.


\subsection{Baselines}

To demonstrate the performance of our method, we compared two types of baselines: the first is individual baselines, which provide benchmarks for single-task performance without collaboration. The second is collaborative baselines, which focus on evaluating the collaborative performance of merging knowledge from multiple contributors.


\subsubsection{Individual Baselines}
The individual baselines highlight model performance on single tasks without collaborative efforts in MedForge. In individual baselines, each branch contributor operates independently on their own dataset using the same base model and plugin module as our proposed MedForge. We include two types of individual baselines, i.e., single-task LoRA tuning and dataset distillation. These baselines are considered upper bounds because they are trained exclusively on raw or distilled data for a single task without the need for trade-offs for handling multiple tasks, as in the case of MedForge.

\paragraph{Single-task LoRA Tuning}
Single-task LoRA tuning trains LoRA modules specifically based on each raw dataset, making them highly specialized for a single task. As a result, the performance of these specialized models can be considered the upper bound for all the other methods, as they are optimized for one task with a dedicated set of data, achieving the best possible results for that particular task. 
For the training settings, we train 100 epochs optimized by stochastic gradient descent (SGD), with a learning rate of 0.01, a momentum of 0.9, and a weight decay of 0.0005. The batch size is set as 64.

\paragraph{Single-task Dataset distillation}
To determine the capability of collaborative methods and the effectiveness of distilled data, we evaluate the performance of models trained solely on the distilled datasets provided by branch contributors. In our study, distilled data serves as specialized knowledge from private datasets, guiding the knowledge integration in the merging stage. The performance of models trained on distilled data also serves as an upper-bound baseline for comparing the effectiveness of the model merging process.
During the distillation and evaluation, our setting follows the design in the previous study~\cite{cazenavette2023glad}. The default IPC (image per class) of the distilled dataset is set to 20, and the distillation iteration is 5000. In evaluation, we train the randomly initialized branch plugin module based on the distilled images with 1,000 iterations and then test on the original test sets to examine the quality of the distilled data. The best-performed distilled data will be selected by each contributor based on the evaluation of accuracy and AUC on test dataset. We repeat the above setting for each dataset.

\subsubsection{Collaborative Baselines}
We introduce two collaborative baselines that merge knowledge from multiple contributors: LoRAHub~\cite{huang2023lorahub} and ModelSoup~\cite{wortsman2022model}. Compared with the above collaborative-based baseline, our proposed MedForge method highlights the performance advantages of merging knowledge from multiple contributors.

\paragraph{ModelSoup}
We adopt the model parameter averaging strategy proposed by ModelSoup~\cite{wortsman2022model} as one of our model merging baselines.
ModelSoup trained multiple models with various hyperparameters, and then averaged the parameters of various beneficial candidate models. Since in our setting, the plugin modules are trained for different purposes, we fuse the branch plugin modules by averaging their model parameters for ModelSoup baseline.


\paragraph{LoRAHub}
LoRAHub~\cite{huang2023lorahub} serves as an important baseline as it focuses on the merging of multiple task-specific LoRAs. However, it merges synchronously and still relies on raw data. In LoRAHub, before merging, all tasks involved in the merging process require trained LoRA modules and a few-shot sample from the original dataset to train the merging coefficients, resulting in a weighted combination of the LoRA modules.
As LoRAHub was proposed for language tasks, which is different from our settings, we designed two comparative means: one based on the raw training sets (w/o distill) and the other on the distilled datasets (w/ distill). The main difference lies in the data used for training the coefficients. For w/o distill, we use a randomly selected 10\% of the raw dataset to guide the weight optimization, while for w/ distill, we use the distilled data as guidance, similar to MedForge.
In the LoRAHub setting, all contributors can only synchronously work together, which greatly hinders the flexibility of the merging system. In addition, once model merging is completed, the capability to solve the tasks will be fixed. When facing new tasks, retraining the weight assigned for each branch module is required, which significantly increases the operational burden of continuously updating novel knowledge.



\subsection{Evaluation Metrics}
We evaluate the model performance with accuracy (ACC) and area under the receiving operator characteristic curve (AUC). We evaluate the slide-wise prediction results for BreakHis by using the slide ID. We aggregate the patch-wise prediction results by calculating the mean output of the slide patches and applying softmax on them. For LC25000 and MedFMC-Colon, we evaluate their performance with patch-wise ACC and AUC because no corresponding patient or slide information is provided.

\input{assets/img/feature}
\subsection{The Performance of MedForge}
In Table~\ref{tab:main_res}, MedForge has shown superb performance on the integration of the three medical tasks compared to other baselines with asynchronous workflow.

Among multiple datasets, our proposed MedForge-Mixture method outperforms all the other merging baseline models up to about 20\% in ACC and 8\% in AUC (Avg-3 tasks: row 7 (MedForge-Mixture) versus row 3-5 (baselines) ) and even outperform the upper bound of single synthesized dataset training on both ACC (row 7 versus row 2, 0.909 versus 0.864) and AUC (row 7 versus row 2, 0.941 versus 0.91).
Overall, the results demonstrate the strong integration performance of our proposed MedForge workflow with asynchronous merging conditions.
The notable performance of MedForge-Mixture method can be attributed to the use of the distilled dataset as the training basis for optimal coefficients. Additionally, integrating the results of the plugin modules helps maintaining parameter organization within the branch plugin module and avoids noise that could be introduced by the direct parameter operation. MedForge-Mixture also achieves the best performance on both LC25000 and MedFMC-Colon compared to other model merging methods, while comparable good results on BreakHis. These results show the generalizability of MedForge-Mixture among multiple tasks.
Further, the MedForge-Fusion approach shows comparable capabilities to other model merging baselines focusing on knowledge integration or parameter fusion. Fusion-based merging strategy surpasses ModelSoup by 10\% performance on ACC. MedForge-Fusion also has better performance on AUC compared to LoRAHub w/o distill. 
Due to a differential model design, we have observed a difference in performance when using MedForge-Fusion and MedForge-Mixture strategies. MedForge-Fusion fuses the branch plugin module parameters and updates the main plugin modules, while MedForge-Mixture maintains the internal parameters of each plugin module. The former reduces the cost of plugin module memory while the latter introduces less noise to the parameter structure of branch modules.








