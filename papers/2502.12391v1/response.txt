\section{Related Work}
\textbf{Diffusion Model for Offline RL.} Due to their strong generative capabilities, diffusion models have been extensively employed in offline RL for modeling heterogeneous behavioral datasets. In general, diffusion models have primarily been used for either planning **Ho et al., "Denoising Diffusion Probabilistic Models"** or modeling policy distributions **Song et al., "Improved Techniques for Training Score-Based Generative Models"**. However, a significant shortcoming of diffusion models is their slow inference speed, as the reverse denoising process requires many iterations. Various strategies have been proposed to address this issue, such as DDIM **Nichol and Dhariwal, "DDPM: Transforming Computer Vision with Diffusion Models"**, which reduces the sampling steps by sampling on subsequences, and DPM-Solver **Song et al., "DPM-Solver: Fast Likelihood-Free Inference of Continuous Time Series Data"**, which minimizes the sampling steps to around 10 by adopting a specialized diffusion ODE solver.


\textbf{Safe RL.} 
Existing literature on safe RL predominantly focuses on the online setting (see **Turchetta et al., "A Generalized Framework for Constrained Control"** for a comprehensive review).
Among these studies, constrained optimization-based methods have garnered significant attention for their strong performance and broad applicability **Achiam et al., "Constrained Policy Optimization"**.
For example, **Dai et al., "Provable-Optimistic Safe Policy Improvement via Primal-Dual Method"**, introduced CPO, which integrates TRPO **Schulman et al., "Trust Region Policy Optimization"** into a constrained optimization framework to derive safe policies.
Primal-dual methods employing Lagrangian optimization have also gained popularity **Zhang and Zhang, "Provable Safe Policy Improvement via Primal-Dual Method"**.
These methods dynamically adjust the Lagrange multipliers in response to safety violations, enabling effective management of safety constraints during training.
In the category of primal-based approaches, **Chen et al., "Constrained Robust Policy Optimization"**, proposed CRPO, which alleviates the need for tuning the dual variable.
Building on CRPO, **Zhang and Zhang, "Safe Policy Improvement via Gradient Manipulation"**, employed gradient manipulation to balance the conflict between reward and cost.
However, these online methods often require extensive interactions with the environment and are highly sensitive to critic estimation, making them impractical for real-world applications due to safety concerns, high costs, or limited accessibility during data collection.





\textbf{Safe Offline RL.} Recently, significant attention has been devoted to addressing the safe RL problem in an offline setting, where agents no longer have access to online interactions with the environment. For instance, **Chen et al., "Learning Safe Policies from Offline Data"**, proposed learning RL policies from offline data by assigning high cost values to both out-of-distribution (OOD) actions and unsafe actions. Recent works, such as **Zhang et al., "Safe Policy Learning via Diffusion Methods"**, incorporate the diffuser method  into the safe setting to learn safe policies. Similarly, **Liu et al., "Learning Safe Policies with Decision Transformers"**, utilized the decision transformer framework  to facilitate the learning of safe policies. In another study, **Chen et al., "Safe Policy Learning via Energy-Based Weighted Regression Methods"**, employed diffusion policies learned through energy-based weighted regression methods to ensure adherence to hard constraints.