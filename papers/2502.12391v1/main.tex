\documentclass[14pt]{article}
\usepackage{arxiv}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{algpseudocode}  % Modern package (replaces algorithmic)
\usepackage{xcolor}
\usepackage{longtable}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}  
\usepackage{bm}
% simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{doi}
\usepackage{subfigure}
\usepackage{pifont}



\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{float}
% \usepackage{algorithm}    % For floating algorithm environment

\usepackage{bbm}
\usepackage{algpseudocode}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=blue
}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newtheorem{remark}{Remark}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{classtheorem}{Theorem}
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{plain}
\newtheorem{definition}{Definition}[section]

\theoremstyle{plain}
%\newtheorem*{theorem*}{Theorem}
\newtheorem{assumption}{Assumption}[section]
\newtheorem{lemma}{Lemma}[section]

%\renewcommand*{\thedefinition}{\arabic{section}.\arabic{definition}}
%\renewcommand*{\theproposition}{\arabic{section}.\arabic{proposition}}
%\renewcommand*{\thetheorem}{\arabic{section}.\arabic{theorem}}
%\renewcommand*{\theclasstheorem}{\Alph{theorem}}
\renewcommand\qedsymbol{$\blacksquare$}

\def\Var{\text{Var}}
\def\E{\mathbb{E}}
\def\P{\mathbb{P}}
\def\N{\mathbb{N}}
\def\Q{\mathbb{Q}}
\def\R{\mathbb{R}}
\def\Z{\mathbb{Z}}
\def\bmu{\bm{\mu}}
\def\bpi{\bm{\pi}}
\def\aa{\mathcal{A}}
\def\bb{\mathcal{B}}
\def\cc{\mathcal{C}}
\def\ff{\mathcal{F}}
\def\ii{\mathcal{I}}
\def\nn{\mathcal{N}}
\def\ll{\mathcal{L}}
\def\jj{\mathcal{J}}
\def\kk{\mathcal{K}}
\def\ss{\mathcal{S}}
\def\qq{\mathcal{Q}}
\def\oo{\mathcal{O}}
\def\ww{\mathcal{W}}
\def\xx{\mathcal{X}}
\def\zz{\mathcal{Z}}
\def\bP{\mathbf{P}}
\def\tmix{T_{\text{mix}}}
\def\ts{\tilde{s}}
\def\hs{\hat{s}}
\def\tz{\tilde{z}}
\def\hz{\hat{z}}
\def\tmu{\tilde{\mu}}
\def\hmu{\hat{\mu}}
\def\mix{\text{mix}}

\def\hb{\mathbb{H}}
\def\gb{\mathbb{G}}
\def\wb{\mathbb{W}}
\def\lb{\mathbb{L}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\mc}{\mathcal}
\newcommand{\mbb}{\mathbb}
\newcommand{\mb}{\mathbf}
\def\one{\mathbbm{1}}
\newcommand{\old}[1]{}
\newcommand{\sol}[1]{}
\newcommand{\degree}{^{\circ}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\def\d{\, \mathrm{d}}
\def\st{\;\text{ s.t. }}



%%%%%Algorithmic command
%\renewcommand{\algorithmicinput}{\textbf{Input:}}
%renewcommand{\algorithmicoutput}{\textbf{Output:}}
% \algnewcommand{\algorithmicor}{\textbf{ or }}
% \algnewcommand{\OR}{\algorithmicor}
% \algnewcommand{\AND}{\algorithmicand}
% \algnewcommand{\var}{\texttt}


\newenvironment{nalign}{
    \begin{equation}
    \begin{aligned}
}{
    \end{aligned}
    \end{equation}
    \ignorespacesafterend
}
% \counterwithin*{equation}{chapter}



\title{Reward-Safety Balance in Offline Safe  RL via Diffusion Regularization}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author[1]{Junyu Guo \thanks{Correspondence to: Junyu Guo <junyuguo24@berkeley.edu>}}
\author[1]{Zhi Zheng }
\author[1]{Donghao Ying }
\author[2]{Ming Jin }
\author[1]{ Shangding Gu }
\author[1]{Costas Spanos }
\author[1]{Javad Lavaei }
\affil[1]{University of California, Berkeley\\}
\affil[2]{Virginia Tech, Blacksburg}


\date{}

\hypersetup{
pdftitle={Reward-Safety Balance in Offline Safe  RL via Diffusion Regularization},
pdfsubject={?},
pdfauthor={Junyu Guo
pdfkeywords={Safe Reinforcement Learning, Diffusion Model},
}
}

\begin{document}

\maketitle
\begin{abstract}
Constrained reinforcement learning (RL) seeks high-performance policies under safety constraints. We focus on an offline setting where the agent has only a fixed dataset---common in realistic tasks to prevent unsafe exploration. To address this, we propose Diffusion-Regularized Constrained Offline Reinforcement Learning (DRCORL), which first uses a diffusion model to capture the behavioral policy from offline data and then extracts a simplified policy to enable efficient inference. We further apply gradient manipulation for safety adaptation, balancing the reward objective and constraint satisfaction. This approach leverages high-quality offline data while  incorporating safety requirements. Empirical results show that DRCORL achieves reliable safety performance, fast inference, and strong reward outcomes across robot learning tasks. Compared to existing safe offline RL methods, it consistently meets cost limits and performs well with the same hyperparameters, indicating practical applicability in real-world scenarios.
\end{abstract}
\section{Introduction}
\label{section: intro}

Offline reinforcement learning (RL) has advanced decision-making by learning from pre-collected datasets without online interaction \citep{fujimoto2019off,levine2020offline}. For real-world control tasks (e.g., autonomous driving, industrial control), safety is equally critical. Safe RL addresses this by imposing constraints, often formulated as a constrained Markov decision process (CMDP) \citep{gu2022review, altman2021constrained}, to ensure high performance without violating safety requirements. These can be hard constraints (no violation at each step) \citep{zheng2024safe,ganai2024iterative} or soft constraints (expected total cost below a threshold) \citep{chow2018risk,yang2020projection}. We focus on the soft-constraint setting in this work.



Offline safe RL faces two main hurdles: distribution shift and reward-safety trade-off. Firstly, the learned policy may deviate from the offline dataset's state–action distribution, causing critic overestimation and extrapolation errors \citep{fujimoto2019off,lyu2022mildly}. To address value overestimation, previous methods have either constrained the learned policy to remain close to the behavioral policy \cite{wu2019behavior,kumar2019stabilizing} or conservatively penalized over-optimistic out-of-distribution (OOD) state-action pairs \cite{kostrikov2021offline,lyu2022mildly,xu2022constraints}. Secondly, achieving high returns while strictly respecting safety becomes more challenging when these objectives conflict. Although constrained optimization methods \citep{achiam2017constrained,liu2022constrained} handle this in online RL, they rely on on-policy data collection, making them not directly applicable to offline settings. 


Hence, the key question is: 
\begin{quote}
    \textit{How can we balance reward maximization and constraint satisfaction without risking out-of-distribution actions or unsafe behavior in a setting where no additional data can be collected?}
\end{quote}

To address this, we propose \textit{Diffusion-Regularized Constrained Offline Safe Reinforcement Learning} (DRCORL). First, DRCORL trains a diffusion policy to imitate the behavioral policy in the offline dataset; then it regularizes the learned policy via the diffusion model’s score function---removing the need for costly sampling from the diffusion model at inference. Second, we apply gradient manipulation to balance reward optimization and cost minimization, effectively handling conflicts between these two objectives. Furthermore, the behavioral policy serves as a regularizer, discouraging OOD actions that may compromise safety.



We evaluate DRCORL on the DSRL benchmark \citep{liu2023datasets}, comparing against state-of-the-art offline safe RL methods. Experiments show that DRCORL consistently attains higher rewards while satisfying safety constraints. Our main contributions are: \ding{172}  We exploit diffusion-based regularization to build a simple, high-speed policy with robust performance; and \ding{173} We introduce a gradient-manipulation mechanism for reward–cost trade-offs in purely offline settings, ensuring safety without sacrificing returns.

\section{Preliminary}
\label{section: preliminary}
A CMDP \citep{altman2021constrained} is defined by the tuple $\langle \mathcal{S}, \mathcal{A}, P, r, c, \gamma \rangle$, 
where $\mathcal{S}$ and $\mathcal{A}$ represent the state and action spaces, respectively.
The transition kernel $P$ specifies the probability ${P}(s^\prime \vert s,a)$ of transitioning from state $s$ to state $s^\prime$ when action $a$ is taken.
The reward function is $r:\mc{S}\times \mc{A} \rightarrow \mbb{R}$, and the cost function is $c:\mc{S}\times \mc{A} \rightarrow \mbb{R}$.
The discount factor is denoted by $\gamma$. 
A policy is a function $\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})$ that represents agent's decision rule, i.e., the agent takes action $a$ with probability $\pi(a\vert s)$ in state $s$, and we define $\Pi$ as the set of all feasible policies.
Under policy $\pi$, the value function is defined as $V^{\pi}_\diamond(\rho) =\E[\sum_{t=0}^{\infty}\gamma^{t} \diamond(s_t,a_t)\vert s_0\sim \rho]$, where $\diamond\in \{r,c\}$, $\rho$ is the initial distribution, and the expectation is taken over all possible trajectories.
Similarly, the associated Q-function is defined as $Q^{\pi}_\diamond(s,a) = \diamond(s,a) + \gamma \E_{s^{\prime}\sim P(\cdot \vert s,a)}[V^{\pi}_\diamond(s^{\prime})]$ for $\diamond\in \{r,c\}$.
In standard CMDP, the objective is to find a policy $\pi \in \Pi$ that maximizes the cumulative rewards $V^\pi_r(\rho)$ while ensuring that the cumulative cost $V^\pi_c(\rho)$ remains within a predefined budget $l$.

In the offline setting, the agent cannot interact directly with the environment and instead relies solely on a static dataset consisting of multiple transition tuples $\mathcal{D}^{\mu} = \{(s_i,a_i,r_i,s_i^{\prime}, c_i)\}_{i=1}^{N}$, which is collected using a behavioral policy $\pi_b(a\vert s)$.
This offline nature introduces the risk of distributional shift between the dataset and the learned policy. 
To address this, an additional constraint is often imposed to limit the deviation of the learned policy $\pi$ from the behavioral policy $\pi_b$, resulting in the  optimization problem
\begin{equation}
    \max_{\pi\in \Pi}\E[V^{\pi}_{r}(\rho)]\text{ s.t. }  D_{\operatorname{KL}}(\pi\| \pi_b)\leq \epsilon, \E[V^{\pi}_c(\rho )]\leq l,\hspace{-3pt}
    \label{defn: optimization objective}
\end{equation}
where $D_{\operatorname{KL}}(p\| q )$   the KL-divergence between two distributions defined as $D_{\operatorname{KL}}(p\| q ) = \E_{x\sim p}\left[\log(p(x)/q(x))\right]$.  
We use the KL-divergence to penalize the learned policy $\pi$'s distance to the behavioral policy, though it is actually not a distance measure. To address safety constraints, primal-dual-based methods \cite{ding2021provably,paternain2022safe,wu2024offpolicy} typically reformulate the constrained optimization problem as follows:
\begin{equation}
    \hspace{-5pt}\max_{\pi \in \Pi} \mathbb{E}[V^{\pi}_{r}(\rho)] \hspace{-1pt} - \hspace{-1pt} \lambda (\mathbb{E}[V^{\pi}_c(\rho )] \hspace{-1pt} - \hspace{-1pt} l)\text{ s.t. } D_{\operatorname{KL}}(\pi \| \pi_b) \hspace{-1pt} \leq \hspace{-1pt} \epsilon,\hspace{-5pt}
    \label{primal dual objective}
\end{equation}
where $\lambda \geq 0$ is a surrogate for the Lagrange multiplier. When the safety constraint is violated, the multiplier $\lambda$ increases to impose a greater penalty on the cost, thereby reducing the cost value.


\section{Methodology}


\subsection{Diffusion Model for Policy Extraction}
Our main idea is to fully exploit the offline dataset to obtain a behavioral policy, and use the behavioral policy to guide the training of the target policy. A policy $\pi(a\vert s)$ is  a distribution on the action space. Previous work estimated the behavior policy with  maximum likelihood estimation \cite{fujimoto2021minimalist} or leveraged a conditional variational autoencoder \cite{kingma2013auto,sohn2015learning}. Here, we exploit the concept of diffusion models \cite{sohl2015deep,ho2020denoising,song2020score} to learn the unknown behavioral policy $\pi_b(a\vert s)$ given its strong generative capabilities. Diffusion models have emerged as powerful generative tools to generate data sample $x_0\sim p(x_0)$ with few-shot samples. They work by using a forward process $q(x_{t}\vert x_{0})$ to perturb the original distribution to a known noise distribution. Subsequently, this model generates the samples using the reverse denoising process $p_{\psi}(x_{t-1}\vert x_t)$.  The forward process can generally be written with a forward stochastic differential equation (SDE)
\begin{equation}
    dx = -\frac{\beta(t)}{2}xdt + \sqrt{\beta(t)} dW_t, \label{forward sde}
\end{equation}
where $\beta(\cdot): [0,T]\rightarrow \R^+$ is a scalar function and the  process $\{W_t\}_{t\in [0,T]}$ is a standard Brownian motion. Our forward process is the discretized version of SDE in Eq.~(\ref{forward sde}) perturbing the original distribution to Gaussian noise. For example, if we choose a variance preserving SDE for the forward diffusion process as in \cite{ho2020denoising}, each step $x_t$ is perturbed with the noise $z_t\sim \mathcal{N}(0,I)$ to obtain $x_{t+1} = \sqrt{\alpha_t}x_t+ \sqrt{\beta_t} z_t$, where $\beta_t = 1-\alpha_t \in (0,1)$. We denote $\Bar{\alpha}_t = \prod_{i=1}^{t}\alpha_i$ and $\Bar{\beta}_t = 1-\Bar{\alpha}_t$. Therefore,  we can rewrite $x_t = \sqrt{\Bar{\alpha}_t}x_0 + \sqrt{\Bar{\beta}_t} \epsilon_t$, where $\epsilon_t\sim \mathcal{N}(0,I)$ follows the standard Gaussian distribution.  The reverse denoising process is optimized by maximizing the evidence lower bound of the log-likelihood $\E[\log p_{\psi}(x_0)]$  defined as $\E_{q(x_0:x_{T})}\big[\log \big(p_{\psi}(x_{0:T})/q(x_{1:T}\vert x_0)\big)\big]$. We can then rewrite the loss function into the weighted regression objective in Eq.~(\ref{weighted regression}) and transform the problem into training a denoising function  $\epsilon_\psi$ predicting the Gaussian noise $\epsilon_t$:
\begin{equation}
   \hspace{-5pt}\mathcal{L}({\psi}) \hspace{-2pt}= \hspace{-2pt} \E_{t\in \operatorname{Unif}[0,T]}\big[w(t)\|\epsilon_\psi(x_t,t)\hspace{-2pt}-\hspace{-2pt}\epsilon_t\|^2\big], \epsilon_t\hspace{-2pt}\sim \hspace{-2pt}\mathcal{N}(0,I),\hspace{-5pt}\label{weighted regression}
\end{equation}
where $w(t)$ is the weight function.
Then the reversing denoising process can be formulated as $x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\big(x_t-\frac{\sqrt{1-\alpha_t}}{\sqrt{1-\Bar{\alpha}_t}} \epsilon_\psi(x_t,t)\big)+\sqrt{\beta_t}z_t$, where $z_t\sim \mathcal{N}(0,I)$. Using this notion,  we can similarly use the diffusion model to diffuse in the action space $\mathcal{A}$ and sample actions given the current state with the reversing process. In \cite{wang2022diffusion}, the authors learned diffusion policies from the offline dataset using guided diffusion. 
The diffusion policy here is $\pi_{\psi}(a\vert s) = \mathcal{N}(a_T;0,I)\prod_{t=1}^{T}p_{\psi}(a_{t-1}\vert a_t, s)$, where $p_{\psi}(a_{t-1}\vert a_t,  s)$ is a Gaussian distribution with mean $m_{\psi}(a_t,t\vert s)$ and variance $\Sigma_\psi(a_t,t\vert s)$ . 
See also \citet{lu2023contrastive,hansen2023idql}, where diffusion policy is used for inference in policy evaluation. The shortage of these methods is that the diffusion models are slow in inference speed, even under the improved sampling scheme in \citet{song2020denoising,lu2022dpm} the reverse denoising process takes at least 10 steps. Therefore, in this work we mainly use diffusion models for pretraining and learning the behavioral policy $\pi_b$. For state-action pair $(s,a)$ in the offline dataset $\mathcal{D}^{\mu}$, we train our diffusion policy model by minimizing the loss 
\begin{equation}
\hspace{-5pt}\mathcal{L} = \E_{(s,a)\in\mathcal{D}^{\mu} }\E_{t\in \operatorname{Unif}(0,T)}\big[w(t)\|\epsilon_\psi(a_t,t\vert s)-\epsilon_t\|^2\big], \hspace{-5pt}
    \label{diffusion pretrain loss}
\end{equation}
where $a_t = \sqrt{\Bar{\alpha}_t}a  + \sqrt{\Bar{\beta}_t}z$ and $z\sim \mathcal{N}(0,I)$. We  assume that the diffusion model can perfectly learn the behavioral policy $\pi_b$, as shown in \citet{de2022convergence}, due to the fact that the target distribution lies on a compact manifold, the first-order Wasserstein distance between the learned policy and the target policy converges to 0 as the discretization length approaches 0.  

\subsection{Diffusion
Regularization}\label{score regularization}
The work \citet{chen2023score} showed under the offline RL setting that one can train a simple policy using the pretrained critic and diffusion policy imitating the behavioral policy. The key step is to use the reverse KL divergence to regularize the target policy to be close to the behavioral policy. The forward KL penalizes the KL-divergence $D_{\operatorname{KL}}(\pi^*(\cdot\vert s)\| \pi_{\theta}(\cdot\vert s))$ while the reverse KL penalizes $D_{\operatorname{KL}}(\pi_\theta (\cdot\vert s)\| \pi^{*}(\cdot\vert s))$. 
As shown in \citet{chen2023score},  the forward KL leads to modes covering issue while backward KL encourages mode-seeking behavior, although the latter is harder to optimize. Therefore, in this work we also choose the reverse KL for regularization.  We also  constrain our policy family to a simple Gaussian policy class $\Pi:\{\pi_\theta(a\vert s)=\mathcal{N}(a;m_{\theta}(s),\Sigma_\theta(s))\}$. Now, we denote the learned diffusion model's score function as $\epsilon_{\psi}(a_t,t\vert s)$ and the corresponding diffusion policy  as $\mu_{\psi}(a\vert s)$. Then, the reverse KL between the policy $\pi_\theta(a\vert s)$ and the approximated behavioral policy $\mu_\psi(a\vert s )$ can be written as $-\mathcal{H}(\pi_\theta(\cdot\vert s))+H(\pi_\theta(\cdot\vert s), \mu_\psi(\cdot\vert s))$. For Gaussian policy, the first part self-entropy term $\mathcal{H}$ can be directly computed in closed form. For $\mathcal{A}=\R^d$, we have 
\begin{equation}
\begin{aligned}
    \mc{H}(\pi_\theta(\cdot \vert s))& = \int_\mathcal{A} -\log \pi_\theta(a\vert s)\pi_\theta(a\vert s)da\\
    &=  \frac{1}{2}\log(\operatorname{det}(\Sigma_\theta(s))) +  \frac{d}{2}\log(2\pi) 
    + \frac{d}{2}.
\end{aligned}
    \label{self entropy}
\end{equation}
Using the reparameterization trick for Gaussian random variables, we can also rewrite the cross entropy term $H(\pi_{\theta}(\cdot\vert s), \mu_\psi(\cdot\vert s))$ as 
\begin{align}
H(\pi_{\theta}(\cdot \vert s), \mu_\psi(\cdot \vert s)\hspace{-1pt}) 
      &\hspace{-2pt}=\hspace{-2pt}  \E_{\pi_{\theta}(\cdot \vert s)}[-\log \mu_\psi(a\vert s)] \label{reparameterization cross entropy} \\
      &\hspace{-2pt}=\hspace{-2pt} \E_{z\sim \mathcal{N}(0,I)}[-\hspace{-2pt}\log \mu_\psi(m_\theta\hspace{-2pt}+\hspace{-2pt}\Sigma_{\theta}^{1/2}z\vert s)].\notag
\end{align}
Finally, to obtain the gradient with respect to the reverse KL divergence we have 
\begin{align}
     &\nabla_\theta D_{\operatorname{KL}}(\pi_\theta(\cdot\vert s)|| \mu_\psi(\cdot \vert s)) \notag\\
        = &\nabla_\theta H(\pi_{\theta}(\cdot\vert s), \mu_\psi(\cdot\vert s)) - \nabla_\theta \mathcal{H}(\pi_\theta(\cdot\vert s))\label{kl divergence gradient}\\
         = &\E_{z\sim \mathcal{N}(0,I)}\big[-\nabla_a\log \mu_\psi(m_\theta(s)+\Sigma_{\theta}^{1/2}z\vert s)\cdot\nabla_\theta(m_\theta(s)+\Sigma_{\theta}^{1/2}z)\big]-\frac{1}{2}\nabla_\theta \log(\operatorname{det}\Sigma_\theta(s)).\notag
\end{align}
The work \citet{song2020score} shows that diffusion models essentially estimate the \textbf{score function}:
\begin{equation}
\nabla_{x}\log p(x_t) \approx  s_{\psi}(x_t,t) = -\frac{1}{\sqrt{\Bar{\beta}_t}}\epsilon_{\psi}(x_t,t).
\label{score function}
\end{equation}
Hence,  we can substitute the denoising function $\epsilon_\psi$ into Eq.~(\ref{kl divergence gradient}) to directly compute the gradient.
\subsection{Safe Adaptation}
To design our algorithm, we define the reward and cost optimization obejectives respectively as  follows: 
  \begin{align}
           \hspace{-8pt}\text{Reward:} \max_{\pi\in \Pi} \E_{s\sim \mathcal{D}^{\mu}}\big[&V^{\pi}_r(s)\hspace{-1pt}-\hspace{-1pt} \frac{1}{\beta} D_{\operatorname{KL}}(\pi(\cdot \vert s)|| \pi_b(\cdot\vert s))\big]\label{reward optimization}\hspace{-5pt}\\
       \hspace{-8pt}\text{Cost:} \max_{\pi\in \Pi} \E_{s\sim \mathcal{D}^{\mu}}\big[&V^{\pi}_r(s)- \lambda(V^{\pi}_c(s)-l)-\frac{1}{\beta} D_{\operatorname{KL}}(\pi(\cdot \vert s)|| \pi_b(\cdot\vert s))\big].\label{cost optimization}
  \end{align}

We consider the reverse KL divergence term in Eq.~(\ref{reward optimization}) and Eq.~(\ref{cost optimization}) as a regularization term to penalize the policy for deviating too far from the behavioral policy \( \pi_b \). In practical implementation, we use the learned diffusion policy \( \mu_\psi \) to replace \( \pi_b \) and compute the gradient for optimization using Eq.~(\ref{kl divergence gradient}). As discussed in \citet{gu2024balance}, to address the conflict between optimizing reward and cost, it is essential to directly handle the gradient at each optimization step. We also adopt the gradient manipulation idea in \citet{gu2024balance}, but we  do so under the offline setting where we no longer have access to updating our critic with the on-policy data.  The gradient manipulation method can be generally described as follows.  For each step where we need to optimize the reward,  we update our gradient with $\theta \leftarrow \theta +\eta g_r$, and when we need to optimize the cost our gradient is updated with $\theta \leftarrow \theta +\eta g_c$. The gradient manipulation aims at updating the parameter $\theta$ with a linear combination of the two gradients  as $\beta_r g_r + \beta_c g_c$. We can use the angle $\phi:= \cos^{-1}\left(\frac{\langle g_r,g_c\rangle}{\|g_r\|\|g_c\|}\right)$ between the two gradients to judge whether the gradients are conflicting or not, namely if $\phi>90^\circ$ the two gradients are conflicting and otherwise they are not conflicting. 
Especially, the ultimate gradient $g$ is computed  via the rule
\begin{equation}
g =
\begin{cases} 
\frac{g_r+g_c}{2}, & \text{if } \phi \in (0,90^\circ) \\
\frac{g_{r}^{+}+g_{c}^{+}}{2}, & \text{if } \phi \in [90^{\circ},180^\circ],
\end{cases}
\label{gradient manipulation}
\end{equation}
where $g_r^+=g_r - \frac{g_r\cdot g_c}{\|g_c\|^2}g_c$ is $g_c$'s projection on the null space of $g_r$ and $g_c^+=g_c - \frac{g_c\cdot g_r}{\|g_r\|^2}g_r$ is $g_r$'s projection on the null space of $g_c$.  It is also shown in \citet{gu2024balance} that under the assumption of  the convexity of the optimization target, one can ensure monotonic improvement using gradient manipulation. As a result,  here we also employ the gradient manipulation method to update our policy parameter. 
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.35\linewidth]{pictures/algo/safe_policy_figure.pdf}
    \caption{Illustration of the soft switching between safe and unsafe policy regions.}
   
    \label{algo-procedure}
\end{figure}
With the procedures outlined above, we present our main algorithm in Algorithm \ref{alg: main algo}, where the safe adaptation step is visualized in Figure \ref{algo-procedure} and detailed in Algorithm \ref{alg: Safe Adapt} given in Appendix \ref{algo extension}. Furthermore, we provide the following theoretical results derived from our algorithm, where the proof is deferred to Appendix \ref{app: proof_proposition}.
\begin{algorithm}[tb]
   \caption{DROCORL}  % Replace with your actual algorithm name
   \label{alg: main algo}
   \begin{algorithmic}[1]  % Add [1] for line numbering
   \State {\bfseries Input:} Dataset $\mathcal{D}^{\mu}$, slack bounds $h^+$, $h^-$
   \State Pretrain diffusion model $\epsilon_{\psi}$  
   \State Pretrain $\pi_\theta$ with diffusion model $\epsilon_\psi$
   \State \textit{//Behavior Cloning} 
   \State Pretrain $Q_r^{\pi}$, $Q_c^{\pi}$ 
   \State \textit{//Pretrain critic}  % Combine \STATE and \COMMENT

   \For{each gradient step $t$}  % Start of loop
      \State Sample mini-batch $\mathcal{B}=\{(s_t,a_t,s_{t}',r_t,c_t)\}_{t=1}^{|\mathcal{B}|}$ 
      \State Update critics for reward and cost 
      \State Update diffusion model $\epsilon_\psi$ with $\mathcal{B}$
      \State $g = \operatorname{SafeAdaptation}(\pi_\theta,\epsilon_\psi, Q^{\pi_\theta}_r,Q^{\pi_\theta}_c,h^+,h^-)$
      \State Update $\theta\leftarrow \theta + \eta g$
   \EndFor  % Close the loop
\end{algorithmic}
\end{algorithm}
% upper bound, show with theta star not an variable
%how to guarantee the constraint violation

\begin{proposition}[Cost Upper Bound]
\label{cost upper bound}
Assume that the cost function $c:\mathcal{S}\times\mathcal{A}\rightarrow [0,c_{\max}]$ is bounded and non-negative.
Let $\Tilde{\pi}(a \vert s)$ be the output policy of Algorithm \ref{alg: main algo}.
Suppose that there exists $\epsilon_{\text{dist}} > 0$ such that $D_{\operatorname{KL}}(\Tilde{\pi}(\cdot \vert s), \pi_b(\cdot \vert s)) \leq \epsilon_{\text{dist}}$ and $
D_{\operatorname{KL}}(\pi_b(\cdot \vert s), \Tilde{\pi}(\cdot \vert s)) \leq \epsilon_{\text{dist}}$.
Let $\epsilon_{\text{adv}}^{b} = \max_s \mathbb{E}_{a \sim \pi_b(\cdot \vert s)}[A_c^{\Tilde{\pi}}(s, a)]$, where $A_c^{\pi}(s, a)$ is the advantage function under policy $\pi$, defined as: $A_c^{\pi}(s, a) = Q_c^{\pi}(s, a) - V_c^{\pi}(s)$.
Then, it holds that:
\begin{equation}
    V_c^{\Tilde{\pi}}(\rho) \leq V_c^{\pi_b}(\rho) + \frac{(c_{\max}+\gamma \epsilon_{\text{adv}}^b)\sqrt{2\epsilon_{\text{dist}}}}{(1-\gamma)^2}.
\end{equation}
\end{proposition}
This proposition establishes that if the learned policy is constrained to remain within a neighborhood of the dataset’s behavior policy, its safety performance is at least guaranteed to match that of the behavior policy underlying the dataset.

To theoretically ground our algorithm, we analyze its convergence properties in the tabular setting, where the state and action spaces are finite (i.e., \(|\mathcal{S}|, |\mathcal{A}| < \infty\)). Under softmax parameterization, we derive convergence guarantees when optimizing the value function using the natural policy gradient (NPG) algorithm. 

\begin{definition}[Softmax Parameterization]

Under the tabular MDP setting, the policy follows a softmax parameterization, where the policy \(\pi\) is parameterized by \(\theta : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\). The policy is expressed as $\pi(a \vert s) = \frac{\exp(\theta(s, a))}{\sum_{a^{\prime} \in \mathcal{A}} \exp(\theta(s, a^{\prime}))}$.
\end{definition}
Policy improvement is performed using the natural policy gradient method \citep{kakade2002approximately}, where the policy parameter \(\theta\) is updated at each iteration as 
\begin{equation}
    \theta \leftarrow \theta + \eta \big(\mathcal{F}^{\theta}_{\rho}\big)^{\dagger} \nabla_{\theta} V^{\pi}(\rho),
\end{equation}
 where $\mathcal{F}^{\theta}_{\rho} = \mathbb{E}_{s \sim \rho, a \sim \pi} \big[\nabla_{\theta} \log \pi_{\theta}(a \vert s) \nabla_{\theta} \log \pi_{\theta}(a \vert s)^\top\big]$ denotes the Fisher information matrix, 
and the \(\dagger\) operator denotes the Moore-Penrose inverse. Based on this setup, we now present the convergence guarantees.

\begin{theorem}\label{main thm}
Let \(\Tilde{\pi}\) be the weighted policy obtained after \(T\) iterations of Algorithm \ref{alg: main algo} with proper step-sizes.
Suppose that the offline dataset has a size of \(|\mathcal{D}^{\mu}| = \mathcal{O}\big(\frac{C \ln({|\mathcal{F}|}/{\delta})}{\epsilon_{\text{offline}} (1-\gamma)^4}\big)\) for some \(\delta \in (0, 1)\), where \(\mathcal{F}\) represents the critic function family.
Then,  with probability \(\geq 1 - \delta\), the optimality gap and constraint violation satisfy that
\begin{equation}
     V^{\pi_{\theta^*}}_r\hspace{-1pt}(\rho) \hspace{-2pt}-\hspace{-2pt} \mathbb{E}[V^{\Tilde{\pi}}_r(\rho)] \hspace{-1pt}\leq\hspace{-1pt} \mathcal{O}(\epsilon_{\text{offline}})\hspace{-1pt}+\hspace{-1pt}\mathcal{O}\hspace{-2pt}\left(\hspace{-2pt}\sqrt{\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^3T}}\right)\hspace{-2pt},\hspace{-2pt}\label{reward bound}
\end{equation}
\begin{equation}
     \mathbb{E}[V^{\Tilde{\pi}}_c(\rho)] - b \leq \mathcal{O}(\epsilon_{\text{offline}}) + \mathcal{O}\left(\sqrt{\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^3T}}\right),\label{cost bound}
\end{equation}

where $\epsilon_{\text{offline}}$ denotes the approximation error of policy evaluation induced by the offline dataset $\mathcal{D}^\mu$.
\end{theorem}
For simplicity, we drop the reverse KL term to ensure the policy's closed-form update under NPG in our proof. 
The SafeAdaptation step in Algorithm \ref{alg: main algo} is  specified in algorithm \ref{alg: Safe Adapt} given in Appendix \ref{algo extension}. We specify the definition of the weighted policy $\Tilde{\pi}$ in Eq.~(\ref{defn: weighted policy}) given in Appendix \ref{proof of main thm}. We can interpret the theorem as follows: by selecting appropriate slack bounds $h^+$ and $h^-$, the policy optimization process will, over time, primarily remain in the reward optimization and gradient manipulation stages. As a result, the cost violation can be effectively upper bounded. Simultaneously, by ensuring that the policy is updated using the reward critic's gradients for the majority of iterations, we can guarantee that the accumulated reward of the weighted policy closely approximates that of the optimal policy. 

\section{Practical Algorithm}
\label{section: algo}
In this section, we present the detailed procedure for the implementation of our main algorithm \textit{Diffusion-Regularized Constrained Offline Safe Reinforcement Learning} (DRCORL), as outlined in Algorithm \ref{alg: main algo}, where we provide two candidates for the  SafeAdaptation step  outlined in Algorithms \ref{alg: Safe Adapt crpo} and \ref{alg: Safe Adapt} given in Appendix \ref{algo extension}. This includes both the pretraining stage and the policy extraction stage. 


\subsection{Pretraining the Diffusion Policy and Critics}
In the pretraining state, we first use the offline dataset $\mathcal{D}^{\mu}$ to pretrain the diffusion policy $\mu_\psi(a\vert s)$ to simply imitate the behavioral policy by minimizing the loss in Eq. \eqref{diffusion pretrain loss}. Then we also pretrain the critic functions $Q^{\pi}_r$ and $Q^{\pi}_c$, but we pretrain the reward critic with Implicit Q-Learning (IQL) \cite{kostrikov2021offline} and pretrain the cost critic with TD learning with pessimistic estimation. We utilize IQL to train the reward critic function by maintaining two Q-functions $\big(Q_{r_1}^{\pi}, Q_{r_2}^{\pi}\big)$ and one value function $V$ as the critic for the reward. The loss function for the value function $V_r^{\pi}$ is defined as:
\begin{equation}
\hspace{-6pt}L_{V^{\pi}_r} \hspace{-2pt}=\hspace{-2pt} \E_{s,a\sim \mathcal{D}^{\mu}}\hspace{-1pt}[L_2^{\tau}(\min(Q^{\pi}_{r_1}(s,\hspace{-1pt}a),\hspace{-1pt}Q^{\pi}_{r_2}(s,\hspace{-1pt}a)\hspace{-1pt})\hspace{-2pt}-\hspace{-2pt}V_r^{\pi}(s)\hspace{-1pt})],\hspace{-5pt}\label{reward critic training 1}
\end{equation}
where $L_2^\tau(u)\hspace{-1pt}=\hspace{-1pt}|\tau\hspace{-1pt}-\hspace{-1pt}\mathbb{I}(u\hspace{-1pt}<\hspace{-1pt}0)| u^2$, and $\tau \in [0,1]$ is a quantile. When $\tau=0.5, L_2^\tau$ simplifies to the $L_2$ loss. When $\tau>0.5, L_2^\tau$ encourages the learning of the $\tau$ quantile values of $Q$.
The loss function for updating $Q^{\pi}_{r_i}$ is given by:
\begin{equation}
\hspace{-5pt}\mathcal{L}_{Q^{\pi}_{r_i}}\hspace{-5pt}=\hspace{-2pt}\mathbb{E}_{\left({s}, {a}, {s}^{\prime} \sim \mathcal{D}^{\mu}\right)}\big[\big\|r({s}, {a})+\gamma   V^{\pi}_r\big({s}^{\prime}\big)\hspace{-2pt}-\hspace{-2pt}Q^{\pi}_{r_i}(s,a)\big\|^2\big].\hspace{-5pt}
\label{reward critic training 2}
\end{equation}
This setup aims to minimize the error between the predicted Q-values and the target values derived from the value function \(V_r^{\pi}\). We employ IQL to pretrain the reward critic function, thereby approximating the optimal Q-function \(Q^*_r\) without incorporating safety constraints. Additionally, we pretrain the cost critics using the temporal difference method and double-Q learning \cite{sutton1988learning}. However, in the offline RL setting, we adopt a pessimistic estimation approach for the cost critic using a positive hyperparameter $\alpha$ to avoid underestimation as stated in Eq.~(\ref{cost critic training}), thereby preventing the learning of unsafe policies. The cost critic can be updated by solving the optimization problem: 
\begin{equation}
    \begin{aligned}
\min_{\pi\in \Pi}\mathbb{E}_{{s}, {a}, {s},a'\sim \pi(\cdot \vert s')}\big[\big\|r({s}, {a})+\gamma Q^{\pi}_c(s',a')-Q^{\pi}_c(s,a)\|^2\big]-\alpha\E_{s,a\sim \pi}[ Q^{\pi}_c(s,a)].
    \end{aligned}
   \label{cost critic training}
\end{equation}
\subsection{Extracting Policy}
Now, we extract the policy $\pi_\theta$ from the diffusion model $\epsilon_\psi$ and the pretrained critic functions. Note that at this stage we need to optimize the reward while preventing the unsafe performance. Therefore, we define two optimization targets, one for the reward and one for the cost.   
The reward optimization target is defined as maximizing the critic value regularized by the KL-divergence with respect to the behavioral policy, a smaller value of the temperature $\beta$ refers to the higher conservativeness of our algorithm: 
\begin{equation}
\max\E_{s, a\sim \pi_\theta}\left[Q^{\pi_{\theta}}_r(s,a)-\frac{1}{\beta}l(\pi_\theta,\mu_\psi\vert s)\right],
    \label{optimize reward}
\end{equation}
where $l(\pi_\theta,\mu_\psi\vert s)\big]$ denotes  the KL-divergence $D_{\operatorname{KL}}\left(\pi_\theta(\cdot\vert s)||\mu_\psi(\cdot\vert s)\right)$ for abbreviation. Similarly, we  define the cost optimization target as follows. We aim to minimize the cost critic value regularized by the behavioral policy: 
\begin{equation}
\begin{aligned}
    \max \E_{s, a\sim \pi_\theta}\left[-(Q^{\pi_{\theta}}_c(s,a)-l)-\frac{1}{\beta}l(\pi_\theta,\mu_\psi\vert s)\right]. 
\end{aligned}
    \label{optimize cost}
\end{equation}
An alternative to the cost optimization objective Eq. \eqref{optimize cost} is to 
 incorporate the primal-dual based objective into our optimization process. By adding Lagrangian multiplier $\lambda$ into Eq. \eqref{optimize cost}, we can obtain the new objective 
 \begin{equation}
\begin{aligned}
    \hspace{-6pt}\max \hspace{-1pt}\E_{s,a\sim \pi_\theta}\hspace{-4pt}\left[\hspace{-1pt}Q^{\pi_\theta}_r\hspace{-1pt}(\hspace{-1pt}s,\hspace{-1pt}a\hspace{-1pt})\hspace{-2pt}-\hspace{-2pt}\lambda(Q^{\pi_{\theta}}_c\hspace{-1pt}(\hspace{-1pt}s,\hspace{-1pt}a\hspace{-1pt})\hspace{-2pt}-\hspace{-2pt}l)\hspace{-2pt}-\hspace{-2pt}\frac{1}{\beta}l(\hspace{-1pt}\pi_\theta,\hspace{-1pt}\mu_\psi\hspace{-1pt}\vert\hspace{-1pt} s\hspace{-1pt})\hspace{-1pt}\right]\hspace{-3pt}. \hspace{-6pt}
\end{aligned}
    \label{optimize cost primal dual}
\end{equation}
 We can update this multiplier $\lambda$ using the PID method \cite{stooke2020responsive}. The advantage of leveraging the multiplier is that we can align the reward objective with the cost objective to avoid instability during the training. Nevertheless, in this work we first consider the case under the cost optimization objective in Eq.~(\ref{optimize cost}). Using the result  obtained in Section \ref{score regularization}, we can obtain the gradient of Eq.~(\ref{optimize reward}) and Eq.~(\ref{optimize cost}) with respect to $\theta$ using the score function of the diffusion model $\epsilon_\psi$ and the critic function. We take the Gaussian policy family under a constant variance. For example, with $\Pi:=\{\pi_\theta(a\vert s)=\mathcal{N}(a;m_\theta(s),\Sigma_{\theta}(s)\}$, we can simplify the gradient to 
\begin{equation}
    \begin{aligned}
         \hspace{-8pt}g_r  \hspace{-2pt} &=\mathbb{E}_{{s},{a}\sim \pi_\theta}\hspace{-3pt}\left[\hspace{-2pt}\big(
\nabla_{{a}} Q^{\pi_\theta}_r({s}, {a}) + \frac{1}{\beta}h_\psi(s,a)\big)
\nabla_\theta \pi_\theta({s})\right] ,\\
         \hspace{-8pt}g_c \hspace{-2pt} 
         &= \mathbb{E}_{{s} ,{a}\sim \pi_\theta}\hspace{-3pt}\left[\hspace{-2pt}\big(\hspace{-2pt}-\hspace{-2pt}\nabla_{{a}}Q^{\pi_\theta}_c({s}, {a}) 
+ \frac{1}{\beta}h_\psi(s,a)\big)
\nabla_\theta \pi_\theta({s})\right],\hspace{-8pt}
    \end{aligned}
    \label{compute gr and gc}
\end{equation}

where $h_\psi(s,a) = \frac{1}{ \sqrt{\Bar{\beta}_t}}\epsilon_{\psi}(a_t,t\vert s)\vert_{t\rightarrow0}$.
In Eq.~(\ref{compute gr and gc}),  $a_t$ denotes the action with perturbed noise, $a_t = \sqrt{\Bar{\alpha}_t}a+\sqrt{\Bar{\beta}_t}\epsilon_t$, $\epsilon_t\sim \mathcal{N}(0,I)$, and the notation $\nabla_\theta\pi_\theta(s)$ denotes the gradient of the action given state $s$ with respect to $\theta$. By assuming $a = m_\theta(s) + \Sigma^{1/2}_\theta(s)z$ where $z\sim \mathcal{N}(0,I)$, we obtain that $\nabla_\theta \pi_\theta(s) = \nabla_\theta m_\theta(s) + \nabla_\theta\Sigma^{1/2}_\theta(s)z$. 
  
    
 Our problem involves two optimization objectives: on one hand, we aim to maximize the reward as stated in Eq.~(\ref{reward optimization}); on the other hand, we seek to minimize the cost when the safety condition is violated, as described in Eq.~(\ref{optimize cost}). To address this, we consider two potential approaches. The first approach builds on the idea of CRPO \cite{xu2021crpo} and is specified in Algorithm \ref{alg: Safe Adapt crpo}, which switches between the two optimization objectives. Specifically, at each step, we evaluate the cost value $V_c^{\pi}(\rho)$. If the cost value does not exceed the threshold $l + h$, we continue optimizing the reward. Otherwise, we switch to optimizing the cost. Here, $h$ acts as a slack variable, serving as a soft boundary between reward and cost optimization. The second approach utilizes the gradient manipulation technique outlined in \citet{gu2024balance} and is specified in Algorithm \ref{alg: Safe Adapt}, which was originally applied for safe adaptation in an online safe RL setting. In this method, we introduce two slack variables, $h^-$ and $h^+$. If the cost value satisfies $V_c^{\pi}(\rho) \leq l - h^-$, the policy is considered to be in a completely safe region, and we focus solely on maximizing the reward. Conversely, if the cost value satisfies $V_c^{\pi}(\rho) \geq l + h^+$, we prioritize minimizing the cost. When the cost value lies within the range $l - h^- \leq V_c(\rho)^{\pi} \leq l + h^+$, we apply gradient manipulation as defined in Eq.~(\ref{gradient manipulation}) to resolve the conflict between the two optimization objectives stated in Eq.~(\ref{optimize reward}) and Eq.~(\ref{optimize cost}).

A critical step in our algorithm is identifying safety violations, which involves determining whether $V_c^{\pi}$ exceeds the cost budget $l$. In the offline RL setting, off-policy collected datasets can lead to an underestimation of the cost value. Ideally, the estimated cost critic $\hat{Q}_c^{\pi}$ satisfies the asymptotic property $Q_c^{\pi} = \hat{Q}_c^{\pi} + \epsilon$, where $\epsilon$ is a zero-mean error term. However, under the expectation $\mathbb{E}_{s,a \sim \pi}[\min \hat{Q}_c^{\pi}(s,a)] \leq \min \mathbb{E}_{s,a \sim \pi}[Q_c^{\pi}(s,a)]$, the error term no longer has zero mean, resulting in an underestimation of the cost value \cite{thrun2014issues}. This issue can be exacerbated by the temporal difference method, leading to an accumulated large bias. A common approach to address this challenge is to employ conservative Q-learning methods, which mitigate the underestimation issue at the cost of learning sub-optimal policies. The core difference between the reward critic and the cost critic is that the reward critic does not suffer from approximation errors as long as the relative priority between different actions is preserved. In contrast, underestimation of the cost critic may lead to unsafe policies. Therefore, we adopt a more conservative approach to evaluate safety conditions. Similar to the UCB (Upper Confidence Bound) method in bandit settings \cite{hao2019bootstrapping}, we use the UCB of the cost critic to assess safety during policy extraction. Specifically, we train an ensemble of cost critics $Q_c^{\pi,i}$, $i = 1, 2, \ldots, E$, and define the UCB cost critic as 
\begin{equation}
    Q_c^{\pi,\text{UCB}}(s,a) = \Bar{Q}_c^{\pi}(s,a) + k\cdot  \operatorname{Std}_{i\in [E]}(Q^{\pi,i}_c(s,a)),
\end{equation}
where $\Bar{Q}_c^{\pi}(s,a)$ represents the mean cost critic over the ensemble, $\operatorname{Std}_{i\in [E]}(Q^{\pi,i}_c(s,a))$ is the standard devistion of the $E$ ensembled critic estimators,
and $k$ is a scaling factor controlling the confidence level. Using this substitution for the original cost critic, we can further ensure the implementation of safety at the expense of narrowing down the feasible policy set. Specifically, we define 
\begin{equation}
    Q^{\pi,\text{UCB}}_c(\rho) = \mathbb{E}_{s \sim \rho, a \sim \pi}[Q^{\pi,\text{UCB}}_c(s,a)],
\end{equation}
and we compare it with the cost budget to determine the optimization objective. The implementation procedure is detailed in Algorithm \ref{alg: Safe Adapt crpo} and Algorithm \ref{alg: Safe Adapt}. Combined with the pretraining stage, our complete implementation is presented in Algorithm \ref{alg: main algo}.

\section{Experiments}
\subsection{Performance On DSRL Benchmarks}
\textbf{Environments.} We evaluate our method on the offline safe RL benchmark DSRL \cite{liu2023datasets}. We conduct extensive experiments on Safety-Gymnasium \cite{
ray2019benchmarking}  and Bullet-Safety-Gym \cite{gronauer2022bullet}. We evaluate the score of different methods using the normalized returns and normalized costs. The normalized returns and costs are defined as 
\begin{equation}
     R_{\text{normalized}} = \frac{R_{\pi}-R_{\min}}{R_{\max}-R_{\min}}, C_{\text{normalized}} =\frac{C_{\pi}-C_{\min}}{l+\epsilon}.
\end{equation} 
Here, $\epsilon$ is a regularizer for the case when $l=0$, and we set $\epsilon=0.1$.  The reward $R_\pi$ is the accumulated return collected within an episode $R_\pi = \sum_{t=1}^T r_t$. Similarly, $C_\pi=\sum_{t=1}^T c_t$ is the accumulated cost collected within an episode. $R_{\max}$, $R_{\min}$ are the maximum and minimum  accumulated returns of the offline dataset within an episode. We normalize the accumulated cost and return to better analyze the results.  If the normalized cost is below 1.0, we can consider this policy as safe.    

\textbf{Baseline Algorithms.}  
We compare the performance of our algorithm against existing offline safe reinforcement learning (RL) algorithms under the soft constraints setting. The following six baseline algorithms are considered: 1) BC (Behavioral Cloning): Behavioral cloning imitates the behavior of the entire dataset to serve as a baseline. Variants include imitation of high-reward trajectories or safe trajectories. 2) BCQ-Lag \cite{fujimoto2019off}: This approach extends behavioral cloning with Q-learning, framing safe reinforcement learning as a constrained optimization problem. A Lagrangian multiplier \cite{stooke2020responsive} is used to balance the reward maximization objective with the cost constraints. 3) BEAR-Lag \cite{kumar2019stabilizing}: An extension of BEAR that incorporates a Lagrangian multiplier \cite{stooke2020responsive} to control cost constraints, enabling safe policy learning. 4) CDT (Constrained Decision Transformer) \cite{liu2023constrained}: An adaptation of the Decision Transformer \cite{chen2021decision} for offline safe reinforcement learning, incorporating cost information into tokens to learn a safe policy. 5) CPQ (Conservative Policy Q-Learning) \cite{xu2022constraints}: This method uses conservative Q-learning to pessimistically estimate the cost critic and updates the reward critic only with safe cost values, ensuring the policy adheres to safety constraints. 6) COptiDICE \cite{lee2022coptidice}: Based on the DICE algorithm, this method learns a stationary distribution for the safe reinforcement learning problem and extracts the optimal policy from this distribution.


\begin{table}[tb]
\caption{Training results across different tasks and algorithms. Safe policies are highlighted in blue, while unsafe policies are shown in gray. The table presents the normalized return and normalized cost for each task under different algorithms.}
\vskip 0.15in
\begin{center}
\resizebox{\columnwidth}{!}{ % Adjust table to fit within one column
\begin{tabular}{lccccccr}
\toprule
Reward (Cost) & BC & BCQ-Lag & BEAR-Lag & CDT & CPQ & COptiDICE & Ours \\
\midrule
% Add rows of data as needed
AntVel & \textcolor{gray}{0.99(4.36)} & \textcolor{gray}{0.99(8.40)}& \textcolor{gray}{-1.01(0.00)} &\textcolor{gray}{0.91(6.04)} & \textcolor{blue}{-1.01(0.00)}&\textcolor{gray}{0.992(11.41)} & \textcolor{blue}{0.88(0.89)}\\
HalfCheetahVel  & \textcolor{gray}{0.976(19.04)} & \textcolor{gray}{1.06(63.94)}  &\textcolor{gray}{0.947(104.45)}& \textcolor{gray}{0.48(3.65)}  & \textcolor{gray}{0.71(14.7) }&\textcolor{blue}{0.61(0.00)} & \textcolor{blue}{0.86(0.00)}\\
HopperVel &\textcolor{gray}{ 0.76(12.14)} & \textcolor{gray}{0.81(14.92)} & \textcolor{gray}{0.257(124.05)}& \textcolor{gray}{0.21(1.12)}&\textcolor{blue}{0.57(0.00)} &\textcolor{gray}{0.14(7.84)} & \textcolor{blue}{0.68(0.79)}\\
Walker2DVel & \textcolor{gray}{ 0.78(5.38)}&\textcolor{blue}{0.80(0.07) } & \textcolor{blue}{0.763(0.8)} & \textcolor{gray}{0.73(1.95)}&\textcolor{blue}{0.08(0.96)}& \textcolor{gray}{0.12(2.34)} & \textcolor{blue}{0.74(0.30)}\\

\bottomrule
\end{tabular}
}
\end{center}
\label{table: main result}
\vspace{-0.5cm}
\end{table}
We present the comparison across different baselines in Mujoco-Gym environments. In Table \ref{table: main result}, we present the normalized accumulated reward and cost per episode for each task. Overall, our algorithm demonstrates consistent safety across different tasks while achieving high rewards. Notably, in the Walker2D task, our method achieves a reward of 0.74, which is slightly below the 0.80 achieved by BCQ-Lagrangian. However, BCQ-Lagrangian fails to consistently ensure safe implementation in other tasks. In comparison, CPQ also achieves safety in most tasks but does so at the expense of reward due to its highly conservative nature, resulting in suboptimal policies in many cases. For CDT, we use the default parameter settings in the benchmark \cite{liu2024offlinesaferl} without fine-tuning the transformer's hyperparameters. Overall, our method performs the best among all algorithms while reliably ensuring safe implementation.
\subsection{Computational Efficiency}
We benchmark inference speeds across algorithms using 1,000 sample inputs on the  {HalfCheetah} task. Apart from the 6 baselines above, We also compare the computation efficiency against constrained Diffuser \cite{lin2023safe}. While baseline methods like BCQ and CPQ achieve the fastest inference due to their lightweight MLP-based actors, they compromise safety or reward performance.  In contrast, our algorithm strictly adheres to cost constraints without sacrificing reward quality. Compared to diffusion or transformer-based approaches our method are superior in inference speed, narrowing the gap between expressive generative models and efficient MLP architectures. The result is demonstrated in Figure \ref{computation efficiency}.
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.5\linewidth]{pictures/algo/alg_performance.pdf}
    \caption{Computational Efficiency vs. Performance Trade-off.
Normalized score ($y$-axis, combining reward and safety metrics) versus inference time for generating 1,000 actions ($x$-axis). }
    \label{computation efficiency}
\end{figure}
\subsection{Ablation Studies}

\textbf{Impact of Different Cost Limits.}  
We evaluate our algorithm's performance under varying cost limits $l = 10, 20$, and $30$, analyzing the learned policies' behavior for each budget setting. The ablation results are presented in Figure \ref{cost ablation}. Across all cost limit choices, our algorithm consistently achieves zero violations of the safety constraints, demonstrating its robustness to varying cost thresholds. This highlights the adaptability and reliability of our approach in maintaining safety compliance. While COptiDICE also strictly adheres to the cost limits, our method consistently outperforms it in terms of normalized return, demonstrating its superior ability to balance safety and reward optimization.  Our main result in Table \ref{table: main result} in evaluated under cost limit $l=10$.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.5\linewidth]{pictures/algo/cost_ablation.pdf}
    \caption{\textbf{Normalized return and cost under varying cost limits} ($l = 10, 20, 30$). Since the cost is normalized relative to the corresponding cost limits, the safety threshold is consistently $1.0$ for all normalized returns across the different cost limits. The dashed line in the figure represents the safety boundary.}
    \label{cost ablation}
\end{figure}

\textbf{Temperature Parameter Setting} .
We evaluate three temperature ($\beta$) schedules: (a) \textit{Constant}, where $\beta_t = \beta_0$ for all timesteps $t$; (b) \textit{Square-root growth}, where $\beta_t = \sqrt{c_0 + c_1 t}$, with $c_0$ and $c_1$ as constants; and (c) \textit{Linear growth}, where $\beta_t = c_0 + c_1 t$, where $t$ represents the epoch number, and the same $\beta$ value is maintained within each epoch. Specifically, we compare three beta schemes:  $\beta_t = 0.02$, $\beta_t = 0.01\sqrt{t}$ and $\beta_t = 0.01t + 0.04$. The comparison between these schemes is illustrated in  Figure \ref{beta ablation}.



\begin{figure}[tb]
    \centering
\includegraphics[width=0.5\linewidth]{pictures/algo/beta_ablation.pdf}
    \caption{Training Curve Under Different Schedules. We compare the training performance of three different $\beta$ schemes, under the square root growth we have the highest normalized reward with high stability.}
    \label{beta ablation}
\end{figure}
\section{Related Work}
\textbf{Diffusion Model for Offline RL.} Due to their strong generative capabilities, diffusion models have been extensively employed in offline RL for modeling heterogeneous behavioral datasets. In general, diffusion models have primarily been used for either planning \cite{janner2022planning,he2024diffusion,ajay2022conditional} or modeling policy distributions \cite{chi2023diffusion,wang2022diffusion,fang2024diffusion,hansen2023idql,chen2023score,lu2023contrastive}. However, a significant shortcoming of diffusion models is their slow inference speed, as the reverse denoising process requires many iterations. Various strategies have been proposed to address this issue, such as DDIM \cite{song2020denoising}, which reduces the sampling steps by sampling on subsequences, and DPM-Solver \cite{lu2022dpm}, which minimizes the sampling steps to around 10 by adopting a specialized diffusion ODE solver.


\textbf{Safe RL.} 
Existing literature on safe RL predominantly focuses on the online setting (see \citet{gu2022review} for a comprehensive review).
Among these studies, constrained optimization-based methods have garnered significant attention for their strong performance and broad applicability \citep{achiam2017constrained,tessler2018reward,yangprojection,liu2022constrained}.
For example, \citet{achiam2017constrained} introduced CPO, which integrates TRPO \citep{schulman2015trust} into a constrained optimization framework to derive safe policies.
Primal-dual methods employing Lagrangian optimization have also gained popularity \citep{chow2018risk,calian2020balancing,ding2020natural,ying2022dual,zhou2023gradient}.
These methods dynamically adjust the Lagrange multipliers in response to safety violations, enabling effective management of safety constraints during training.
In the category of primal-based approaches, \citet{xu2021crpo} proposed CRPO, which alleviates the need for tuning the dual variable.
Building on CRPO, \citet{gu2024balance} employed gradient manipulation to balance the conflict between reward and cost.
However, these online methods often require extensive interactions with the environment and are highly sensitive to critic estimation, making them impractical for real-world applications due to safety concerns, high costs, or limited accessibility
during data collection.





\textbf{Safe Offline RL.} Recently, significant attention has been devoted to addressing the safe RL problem in an offline setting, where agents no longer have access to online interactions with the environment. For instance, \citet{xu2022constraints} proposed learning RL policies from offline data by assigning high cost values to both out-of-distribution (OOD) actions and unsafe actions. Recent works, such as \citet{lin2023safe}, incorporate the diffuser method \citep{janner2022planning} into the safe setting to learn safe policies. Similarly, \citet{liu2023constrained} utilized the decision transformer framework \citep{chen2021decision} to facilitate the learning of safe policies. In another study, \citet{zheng2024safe} employed diffusion policies learned through energy-based weighted regression methods \citep{lu2023contrastive} to ensure adherence to hard constraints.





\section{Conclusion}
In this work, we address the challenges of offline safe RL, where the primary difficulties arise from the inherent nature of offline datasets and the additional safety constraints. To tackle these challenges, we propose the DRCORL method, which is designed to learn a safe policy with high-reward performance across a wide range of challenging offline safe RL tasks. DRCORL achieves high-speed inference and dynamically resolves conflicts between reward and cost functions, owing to the simple structure of its policy network. We show that our method  outperforms existing approaches, which often suffer from either slow inference due to complex policy network architectures or the inability to achieve high returns. Nevertheless, our algorithm can be further studies and improved on various fronts. 1) DRCORL still relies on a pretraining procedure to obtain a diffusion model for imitating the behavioral policy and a critic function that approximates the optimal Q-function.  2) Achieving zero constraint violations remains challenging, particularly under the offline setting. Developing algorithms that are robust to variations in dataset quality remains an important future direction. Despite these limitations, our work provides valuable insights and inspiration for addressing generalization and safety challenges in constrained offline RL.



\bibliographystyle{plainnat}
\bibliography{refs}
\appendix
\section{Proofs of Theoretical Results}
\label{extension}

In this section, we provide the proof of the theoretical results in the main body.
We make this assumption throughout the proof of Proposition \ref{cost upper bound} and Theorem \ref{main thm}. 
\begin{assumption}\label{assump: pi/pi_b}
During the training process, we assume the policy \( \pi_\theta \) resides in the region \( \Pi_{\Theta} \) within the policy space. For all \( \pi \in \Pi_{\Theta} \), \( s \in \mathcal{S} \), and \( a \in \mathcal{A} \), the following holds:
\begin{equation}
    \left|\log \frac{\pi(a \vert s)}{\pi_b(a \vert s)}\right| \leq \epsilon_{\text{dist}}. \label{bounded policy distance}
\end{equation}
Equivalently, it means that for all \( s \in \mathcal{S} \) and \( a \in \mathcal{A} \):
\begin{equation}
    \exp(-\epsilon_{\text{dist}}) \leq \frac{\pi(a \vert s)}{\pi_b(a \vert s)} \leq \exp(\epsilon_{\text{dist}}).
\end{equation}
\end{assumption}


\subsection{Proof of Proposition \ref{cost upper bound}}\label{app: proof_proposition}
\begin{proof}
Using the performance difference lemma \citep[Lemma 2]{agarwal2021theory}, we can write the difference of the value functions under policies $\Tilde{\pi}$ and $\pi_b$ as 
\begin{equation}
V^{\pi_b}_c(\rho) - V^{\Tilde{\pi}}_c(\rho) = \frac{1}{1-\gamma}\E_{s\sim d^{\pi_b}_{\rho}}\E_{a\sim \pi_b(\cdot \vert s) }[A^{\Tilde{\pi}}_c(s,a)],
\end{equation}
where $d^{\pi_b}_\rho$ is the discounted stationary distribution defined as $d^{\pi_b}_\rho(s) =(1-\gamma)\sum_{t=0}^{\infty} \operatorname{Pr}(s_t=s\vert s_0\sim \rho)$.
The total variation distance (TV-distance) between two distribution is defined as $D_{\operatorname{TV}}(\Tilde{\pi}(\cdot\vert s)|| \pi_{b}(\cdot \vert s)) = \frac{1}{2}\int_{\mathcal{A}}|\Tilde{\pi}(a\vert s)-\pi_b(a\vert s)|da$, which is proportion to the $\ell^1$-distance between the two distributions. 
Using the Pinsker’s inequality \citep{csiszar2011information}, we can bound the TV-distance with the KL-divergence, i.e., for any two distributions $\mu,\nu$, we have that 
\begin{equation}
    D_{\operatorname{TV}}(\mu||\nu)\leq \sqrt{\frac{D_{\operatorname{KL}}(\mu||\nu)}{2}}. 
    \label{performance difference}
\end{equation}
Hence, it holds that $D_{\operatorname{TV}}(\Tilde{\pi}\| \pi_b)\leq \sqrt{{\epsilon_{\text{dist}}}/{2}}$. 
Then, following the same procedure as \citet[Proposition 2]{achiam2017constrained}, we can obtain that
\begin{equation}
   \begin{aligned}
        V^{\pi_b}_c(\rho) - V^{\Tilde{\pi}}_c(\rho) &\geq  \underbrace{\frac{1}{1-\gamma}\E_{s\sim d^{\Tilde{\pi}}_{\rho}}\E_{a\sim \pi_b(\cdot \vert s) }[A^{\Tilde{\pi}}_c(s,a)]}_{\text{I}}-\underbrace{\frac{2\gamma \epsilon_{\text{adv}}^{b} }{(1-\gamma)^2}\E_{s\sim d^{\Tilde{\pi}}_\rho}[D_{\operatorname{TV}}(\Tilde{\pi}(\cdot \vert s)||\pi_b(\cdot \vert s))]}_{\text{II}}.
   \end{aligned}
\end{equation}
Using Eq. (\ref{performance difference}), we can bound term $\text{II}$ with 
\begin{equation}
  \frac{2\gamma \epsilon_{\text{adv}}^{b} }{(1-\gamma)^2}\E_{s\sim d^{\Tilde{\pi}}_\rho}[D_{\operatorname{TV}}(\Tilde{\pi}(\cdot \vert s)||\pi_b(\cdot \vert s))]  \leq \frac{\gamma \epsilon_{\text{adv}}^{b} \sqrt{2\epsilon_{\text{dist}}}}{(1-\gamma)^2}.
  \label{bound for II}
\end{equation}
As for term $\text{I}$, we can bound it with 
\begin{equation}
    \begin{aligned}
   \left| \frac{1}{1-\gamma}\E_{s\sim d^{\Tilde{\pi}}_{\rho}}\E_{a\sim \pi_b(\cdot \vert s) }[A^{\Tilde{\pi}}_c(s,a)]\right| 
   &\leq \frac{1}{1-\gamma}\E_{s\sim d^{\Tilde{\pi}}_{\rho}}\left|\int_{ \mathcal{A}}\pi_b(a\vert s)Q^{\Tilde{\pi}}(s,a)-\Tilde{\pi}(a\vert s)Q^{\Tilde{\pi}}(s,a)da\right|\\
   &\leq \frac{1}{1-\gamma}\E_{s\sim d^{\Tilde{\pi}}_{\rho}}\int_{ \mathcal{A}}\big|\pi_b(a\vert s)-\Tilde{\pi}(a\vert s)\big|\underbrace{Q^{\Tilde{\pi}}(s,a)}_{\in \left[0,\frac{c_{\max}}{1-\gamma}\right]}da\\
    &\leq \frac{c_{\max}}{(1-\gamma)^2}\E_{s\sim d^{\Tilde{\pi}}_{\rho}}\int_{\mathcal{A}}|\Tilde{\pi}(a\vert s)-\pi_b(a\vert s)|da\\
    &\leq \frac{2c_{\max}}{(1-\gamma)^2}\E_{s\sim d^{\Tilde{\pi}}_\rho}[D_{\operatorname{TV}}(\Tilde{\pi}(\cdot \vert s)||\pi_b(\cdot \vert s))]\\
    &\leq \frac{c_{\max}\sqrt{2\epsilon_{\text{dist}}}}{(1-\gamma)^2}.
    \end{aligned}
    \label{Bound for I}
\end{equation}
Combining Eq. (\ref{Bound for I}) with Eq. (\ref{bound for II}), we can finally obtain that 
\begin{equation}
    \begin{aligned}
        V^{\pi_b}_c(\rho) - V^{\Tilde{\pi}}_c(\rho)  = \text{I}-\text{II}\geq -\frac{c_{\max}\sqrt{2\epsilon_{\text{dist}}}}{(1-\gamma)^2} - \frac{\gamma \epsilon_{\text{adv}}^{b} \sqrt{2\epsilon_{\text{dist}}}}{(1-\gamma)^2} = -\frac{(c_{\max}+\gamma \epsilon_{\text{adv}}^b)\sqrt{2\epsilon_{\text{dist}}}}{(1-\gamma)^2},
    \end{aligned}
\end{equation}
which concludes the proof of Proposition \ref{cost upper bound}.
\end{proof}

\subsection{Proof of Theorem \ref{main thm}}
\label{proof of main thm}

Our proof is based on  theorem 1 in \cite{gu2024enhancing} which is considering a   online safe reinforcement learning setting.
Before presenting the proof, we first introduce some notations and concepts that will be used throughout this section. 
We index the iterations of Algorithm \ref{alg: main algo} by \( t = 1, 2, \ldots, T \).
\begin{itemize}
    \item Let \(\hat{Q}_r^t(s,a)\) and \(\hat{Q}_c^t(s,a)\) denote the estimators of the critic functions \(Q^{\pi_{\theta_t}}_r(s,a)\) and \(Q^{\pi_{\theta_t}}_c(s,a)\), respectively, at the \(t\)-th iteration under the policy \(\pi_{\theta_t}\).
    \item Denote the gradient at step $t$ for the reward optimization as $g_r^t$ and the gradient at step $t$ for the cost optimization as $g_c^t$.
    \item Let \(\eta\) represent the learning rate for the NPG algorithm.

    \item We categorize the iterations of Algorithm \ref{alg: main algo} into four cases based on the optimization scenarios:
    \begin{enumerate}
        \item \textbf{Safe Policy Region}, i.e.,  when \(V^{\pi_{\theta_t}}_c \leq l - h^{-}\).
        We denote the set of iteration indices corresponding to this case as \(S_{\text{safe}}\).
        \item \textbf{Unsafe Region}, i.e., when \(V^{\pi_{\theta_t}}_c \geq l + h^{+}\). We denote the set of iteration indices corresponding to this case as \(S_{\text{unsafe}}\).
        \item \textbf{Gradient Manipulation - Aligned Gradients}, i.e., when the cost function is close to the cost limit threshold and the angle between the updated gradients is less than \(90^\circ\). We denote the corresponding set of iteration indices as \(S_{\text{align}}\).
        \item \textbf{Gradient Manipulation - Conflicting Gradients}, i.e., when the cost function is close to the cost limit threshold and the angle between the updated gradients is greater than \(90^\circ\). We denote the corresponding set of iteration indices as \(S_{\text{conflict}}\).
    \end{enumerate}
    For every \( t \in \{1, 2, \ldots, T\} \), the iteration index \(t\) must belong to one of the four sets: \( S_{\text{safe}} \), \( S_{\text{unsafe}} \), \( S_{\text{align}} \), or \( S_{\text{conflict}} \).
    
    
    \item Assume the reward function \(r: \mathcal{S} \times \mathcal{A} \to [0, M]\) and the cost function \(c: \mathcal{S} \times \mathcal{A} \to [0, M]\) are non-negative and bounded by \(M\).
    This is a standard assumption in the tabular setting.
    \item We define the Bellman operator \( \mathcal{T} \) for policy evaluation (applicable to both cost and reward functions) as:
\begin{equation}
    \mathcal{T}f(s,a) = r(s,a) + \gamma \mathbb{E}_{s' \sim P(\cdot \vert s,a)} \left[\max_{a' \in \mathcal{A}} f(s',a')\right]. \label{bellman operator}
\end{equation}
\end{itemize}
\begin{proof}    
    Under the softmax parameterization, the natural policy gradient update \citep{kakade2002approximately} can be expressed as
    \begin{equation}
        \pi_{\theta_{t+1}}(a\vert s)=  \pi_{\theta_{t}}(a\vert s)\frac{\exp\left({\eta Q^{\pi_{\theta_t}}(s,a)}/(1-\gamma)\right)}{Z_t(s,a)},
    \end{equation}
    where the normalization constant \( Z_t(s) \) is defined as:
    \begin{equation}
    Z_t(s) = \sum_{a \in \mathcal{A}} \pi_{\theta_t}(a \vert s) \exp\bigg(\frac{\eta Q^{\pi_{\theta_t}}(s,a)}{1-\gamma}\bigg).
\end{equation}
   





Since with the reverse KL term we no longer have the close form softmax update under natural policy gradient algorithm. For simplicity we drop the KL term in the theoretical proof here as under assumption \ref{assump: pi/pi_b} after the pretrain stage the KL term no longer dominate the loss function, and we mainly focus on the proof on the balance between reward and cost. 

\begin{assumption}\label{assump: offline_dataset}
Given an offline dataset \( \mathcal{D}^{\mu} = \{(s_i, a_i, s_i^{\prime}, r_i, c_i)\}_{i=1}^N \) of size \( |\mathcal{D}^{\mu}| = N \), let the value function class be \( \mathcal{F} \) and define the model class as \( \mathcal{G} = \{\mathcal{T}f \vert f \in \mathcal{F}\} \). We assume:
\begin{itemize}
    \item \textbf{Realizability}: The critic function \( Q^* \), learned by optimizing Eq.~(\ref{reward critic training 1}), Eq.~(\ref{reward critic training 2}), and Eq.~(\ref{cost critic training}), belongs to the function class \( \mathcal{F} \), and \( \mathcal{T}Q^* \) resides in the model class \( \mathcal{G} \). Moreover, we assume \( \mathcal{G} = \mathcal{F} \).
    \item \textbf{Dataset Diversity}: The offline dataset is diverse enough to ensure accurate offline policy evaluation. Specifically, we assume that:
    \begin{equation}
        N = \mathcal{O}\left(\frac{C \ln({|\mathcal{F}|}/{\delta})}{\epsilon_{\text{offline}} (1-\gamma)^4}\right),
    \end{equation}
    where \( \epsilon_{\text{offline}} \) is the desired accuracy, and \( \delta \) is the failure probability.
\end{itemize}
\end{assumption}


By invoking \citet[Theorem 3]{chen2019information}, we can show that under Assumption \ref{assump: offline_dataset}, with probability at least \(1 - \delta\), the following bounds hold:
\begin{equation}
    \|\hat{Q}^{\pi}_r - Q^{\pi,*}_r\|\leq \epsilon_{\text{offline}}, \quad \|\hat{Q}^{\pi}_c - Q^{\pi,*}_c\|\leq \epsilon_{\text{offline}}.
\end{equation}
Then, by \citet[Lemma A.2]{gu2024enhancing}, we can show that the policy update with gradient manipulation satisfies that:

\begin{itemize}
    \item For all \( t \in S_{\text{safe}} \), the bound on the reward function is given by:
\begin{equation}
    \begin{aligned}
    V_r^{\pi_{\theta^*}}(\rho) - V_r^{\pi_{\theta_t}}(\rho) 
    &\leq \frac{1}{\eta} \mathbb{E}_{s \sim d^{\pi_{\theta^*}}_{\rho}} \left[ D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_t}(\cdot \vert s)) - D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_{t+1}}(\cdot \vert s)) \right] \\
    &\quad + \frac{2\eta |\mathcal{S}||\mathcal{A}| M^2}{(1 - \gamma)^3} 
    + \frac{3(1 + \eta M)}{(1 - \gamma)^2} \big\| Q_r^{\pi_{\theta_t}} - \hat{Q}_r^{\pi_{\theta_t}} \big\|_2.
\end{aligned}
\label{safe}
\end{equation}

\item Similarly, for all \( t \in S_{\text{unsafe}} \), the bound on the cost function is:
\begin{equation}
    \begin{aligned}
    V_c^{\pi_{\theta^*}}(\rho) - V_c^{\pi_{\theta_t}}(\rho) 
    &\leq \frac{1}{\eta} \mathbb{E}_{s \sim d^{\pi_{\theta^*}}_{\rho}} \left[ D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_t}(\cdot \vert s)) - D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_{t+1}}(\cdot \vert s)) \right] \\
    &\quad + \frac{2\eta |\mathcal{S}||\mathcal{A}| M^2}{(1 - \gamma)^3} 
    + \frac{3(1 + \eta M)}{(1 - \gamma)^2} \big\| Q_c^{\pi_{\theta_t}} - \hat{Q}_c^{\pi_{\theta_t}} \big\|_2.
\end{aligned}
\label{unsafe}
\end{equation}

\item For $t\in S_{\text{align}}$, we have the combined bound for reward and cost as:
\begin{equation}
    \begin{aligned}
    &\quad\ \frac{1}{2} \left( V_r^{\pi_{\theta^*}}(\rho) - V_r^{\pi_{\theta_t}}(\rho) \right) 
    + \frac{1}{2} \left( V_c^{\pi_{\theta^*}}(\rho) - V_c^{\pi_{\theta_t}}(\rho) \right)  \\
    &\leq \frac{1}{\eta} \mathbb{E}_{s \sim d^{\pi_{\theta^*}}_{\rho}} \big( D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_t}(\cdot \vert s)) - D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_{t+1}}(\cdot \vert s)) \big) 
    + \frac{2 \eta M^2 |\mathcal{S}| |\mathcal{A}|}{(1 - \gamma)^3}  \\
    &\quad + \frac{3(1 + \eta M)}{(1 - \gamma)^2} 
    \left[ \frac{1}{2} \left\| Q_r^{\pi_{\theta_t}}(s, a) - \hat{Q}^{\pi_{\theta_t}}_r(s, a) \right\|_2 
    + \frac{1}{2}\left\| Q_c^{\pi_{\theta_t}}(s, a) - \hat{Q}_c^{\pi_{\theta_t}}(s, a) \right\|_2 \right].
\end{aligned}
\label{align}
\end{equation}

\item Finally for $t\in S_{\text{conflict}}$, it holds that
\begin{equation}
\hspace{-20pt}
    \begin{aligned}
    &\quad \ \left(\frac{1}{2}-\frac{\langle g_r^t,g_c^t\rangle}{2\|g_r^t\|^2}\right) \left( V_r^{\pi_{\theta^*}}(\rho) - V_r^{\pi_{\theta_t}}(\rho) \right) 
    + \left(\frac{1}{2}-\frac{\langle g_c^t,g_r^t\rangle}{2\|g_c^t\|^2}\right) \left( V_c^{\pi_{\theta^*}}(\rho) - V_c^{\pi_{\theta_t}}(\rho) \right)  \\
    &\leq \frac{1}{\eta} \mathbb{E}_{s \sim d^{\pi_{\theta^*}}_{\rho}} \left( D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_t}(\cdot \vert s)) - D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_{t+1}}(\cdot \vert s)) \right) 
    + \frac{2 \eta M^2 \left(1-\frac{\langle g_r^t,g_c^t\rangle}{2\|g_r^t\|^2} -\frac{\langle g_r^t,g_c^t\rangle}{2\|g_c^t\|^2}\right) |\mathcal{S}| |\mathcal{A}|}{(1 - \gamma)^3}  \\
    &\quad + \frac{3(1 + \eta M)}{(1 - \gamma)^2} 
    \left[\frac{1}{2} \big\| Q_r^{\pi_{\theta_t}}(s, a) - \hat{Q}_r^{\pi_{\theta_t}}(s, a) \big\|_2 
    + \frac{1}{2} \big\| Q_c^{\pi_{\theta_t}}(s, a) - \hat{Q}_c^{\pi_{\theta_t}}(s, a) \big\|_2 \right].
\end{aligned}
\label{conflict}
\end{equation}
\end{itemize}

Summing the four equations, Eq.~(\ref{safe}), Eq.~(\ref{unsafe}), Eq.~(\ref{align}), and Eq.~(\ref{conflict}), we obtain that
\begin{equation}
 \begin{aligned}
        &\quad \sum_{t\in S_{\text{unsafe}}}\left( V_c^{\pi_{\theta^*}}(\rho) - V_c^{\pi_{\theta_t}}(\rho) \right) + \frac{1}{2}\sum_{t\in S_{\text{align}}}\left( V_c^{\pi_{\theta^*}}(\rho) - V_c^{\pi_{\theta_t}}(\rho) \right) + \left(\frac{1}{2}-\frac{\langle g_r^t,g_c^t\rangle}{2\|g_c^t\|^2}\right)\cdot \left( V_c^{\pi_{\theta^*}}(\rho) - V_c^{\pi_{\theta_t}}(\rho) \right) \\
    &\leq \frac{1}{\eta}\mathbb{E}_{s \sim d^{\pi_{\theta^*}}_{\rho}}  D_{\operatorname{KL}}\left(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_0}(\cdot \vert s)\right) + \frac{2\eta |\mathcal{S}||\mathcal{A}| M^2 T}{(1 - \gamma)^3}  + e_{Q},
 \end{aligned}
\end{equation}
where \( e_Q \) is the accumulated weighted critic error, defined as:
\begin{align}
    e_Q& = \sum_{t\in S_{\text{safe}}}\frac{3(1 + \eta M)}{(1 - \gamma)^2} \big\| Q_r^{\pi_{\theta_t}} - \hat{Q}_r^{\pi_{\theta_t}} \big\|_2 +\sum_{t\in S_{\text{unsafe}}}\frac{3(1 + \eta M)}{(1 - \gamma)^2} \big\| Q_c^{\pi_{\theta_t}} - \hat{Q}_c^{\pi_{\theta_t}} \big\|_2 \notag\\
    &+\sum_{t\in S_{\text{align}}} \frac{3(1 + \eta M)}{(1 - \gamma)^2} 
    \left[ \frac{1}{2} \big\| Q_r^{\pi_{\theta_t}}(s, a) - \hat{Q}^{\pi_{\theta_t}}_r(s, a) \big\|_2 
    + \frac{1}{2}\big\| Q_c^{\pi_{\theta_t}}(s, a) - \hat{Q}_c^{\pi_{\theta_t}}(s, a) \big\|_2 \right]\\
    &+\sum_{t\in S_{\text{conflict}}}\hspace{-7pt} \frac{3(1 + \eta M)}{(1 - \gamma)^2} 
    \left[ \left(\frac{1}{2}-\frac{\langle g_r^t,g_c^t\rangle}{2\|g_r^t\|^2}\right)\hspace{-3pt} \left\| Q_r^{\pi_{\theta_t}}(s, a) - \hat{Q}_r^{\pi_{\theta_t}}(s, a) \right\|_2 
    + \left(\frac{1}{2}-\frac{\langle g_r^t,g_c^t\rangle}{2\|g_c^t\|^2}\right)\hspace{-3pt} \left\| Q_c^{\pi_{\theta_t}}(s, a) - \hat{Q}_c^{\pi_{\theta_t}}(s, a) \right\|_2 \right].\notag
\end{align}

Now, we need to upper bound the weighted critic error $e_Q$. 
We assume that there exists a positive constant $C$ such that $e_Q$ with 
$ \frac{3CT(1 + \eta M)}{(1 - \gamma)^2}\epsilon_{\text{offline}}$ . 
According to \citet[Lemma A.6]{gu2024enhancing}, by choosing reasonably large values for \( h^+ \) and \( h^- \), we can ensure that:
$$|S_{\text{unsafe}}| + |S_{\text{align}}|+ |S_{\text{conflict}}|\geq T/2.$$ 
For example, by setting \( h^+ = 2 \sqrt{\frac{|\mathcal{S}||\mathcal{A}|}{(1 - \gamma)^3 T}} \big( \epsilon_{\text{dist}} + 4 M^2 + 6 M \big) \) and \( h^- = 0 \), this condition holds.
Now, we define the weighted policy \( \Tilde{\pi} \) as follows 
\begin{equation}
    \Tilde{\pi}(a\vert s) = \dfrac{\sum_{t=1}^T w_t \pi_t(a\vert s)}{\sum_{t=1}^T w_t}, \label{defn: weighted policy}
\end{equation}
where the policy weights are assigned based on the categories of iterations:
\begin{itemize}
    \item Weight $w_t = $ \( 1 \) for \( t \in S_{\text{safe}} \),
    \item Weight $w_t = $ \( 0 \) for \( t \in S_{\text{unsafe}} \),
    \item Weight $w_t = $ \( \frac{1}{2} \) for \( t \in S_{\text{align}} \),
    \item Weight $w_t = $ \( \frac{1}{2} - \frac{\langle g_r^t, g_c^t \rangle}{\|g_r^t\|^2} \) for \( t \in S_{\text{conflict}} \).
\end{itemize}
Under the weighted policy $\tilde\pi$, the following bound holds true for the reward value function:
\begin{equation}
    \begin{aligned}
V^{\pi^*}_r  -  V^{\Tilde{\pi}}_r &\leq \frac{1}{\frac{1}{2}\frac{T}{2}} \left(\frac{1}{\eta}\mathbb{E}_{s \sim d^{\pi_{\theta^*}}_{\rho}}  D_{\operatorname{KL}}(\pi_{\theta^*}(\cdot \vert s) \| \pi_{\theta_t}(\cdot \vert s)) + \frac{2\eta |\mathcal{S}||\mathcal{A}| M^2 T}{(1 - \gamma)^3}  + e_{Q}\right)\\
&\leq \frac{4}{T}\left(\frac{1}{\eta}\epsilon_{\text{dist}} + \frac{2\eta |\mathcal{S}||\mathcal{A}| M^2 T}{(1 - \gamma)^3}  +\frac{3CT(1 + \eta M)}{(1 - \gamma)^2}\epsilon_{\text{offline}}\right).
\end{aligned}
\end{equation}

Now, by choosing $\eta =\sqrt{{\epsilon_{\text{dist}}(1-\gamma)^3}/{\big({2\eta |\mathcal{S}||\mathcal{A}| M^2 }+{3CT\eta M}(1 - \gamma)\epsilon_{\text{offline}}}\big)}$, then we can ensure that  
\begin{equation}
    V^{\pi^*}_r  -  V^{\Tilde{\pi}}_r\leq \frac{1}{\sqrt{T}}\sqrt{\frac{32 ({2\eta |\mathcal{S}||\mathcal{A}| M^2 }+{3(1 + \eta M)}(1 - \gamma)\epsilon_{\text{offline}}) }{(1-\gamma)^3}}+\frac{3C\epsilon_{\text{offline}}}{(1-\gamma)^2}=\mathcal{O}\left(\sqrt{\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^3T}}\right)+\mathcal{O}(\epsilon_{\text{offline}}).
\end{equation}
Finally, the safety bound is given as:
\begin{equation}
    \begin{aligned}
      V^{\Tilde{\pi}}_c(\rho)-b&\leq \frac{4}{T}\left[\sum_{t\in S_{\text{safe}}}(V^{\pi_{\theta_t}}_c(\rho)-b) +\frac{1}{2}\sum_{t\in S_{\text{align}}}(V^{\pi_{\theta_t}}_c(\rho)-b)+\left(\frac{1}{2}-\frac{\langle g_c^t, g_r^t\rangle}{2\|g_c^t\|^2}\right)\sum_{t\in S_{\text{conflict}}}(V^{\pi_{\theta_t}}_c(\rho)-b) \right]\\
      &\leq \frac{4}{T}\left[\sum_{t\in S_{\text{safe}}}\underbrace{(\hat{V}^{\pi_{\theta_t}}_c(\rho)-b) }_{\leq -h^-}+\frac{1}{2}\sum_{t\in S_{\text{align}}}\underbrace{(\hat{V}^{\pi_{\theta_t}}_c(\rho)-b)}_{\leq h^+}+\left(\frac{1}{2}-\frac{\langle g_c^t, g_r^t\rangle}{2\|g_c^t\|^2}\right)\sum_{t\in S_{\text{conflict}}}\underbrace{(\hat{V}^{\pi_{\theta_t}}_c(\rho)-b)}_{\leq h^+}\right]\\
      &+ \frac{4}{T}\left[\sum_{t\in S_{\text{safe}}}(V^{\pi_{\theta_t}}_c(\rho)-\hat{V}^{\pi_{\theta_t}}_c(\rho))+\sum_{t\in S_{\text{align}}}(V^{\pi_{\theta_t}}_c(\rho)-\hat{V}^{\pi_{\theta_t}}_c(\rho))+\sum_{t\in S_{\text{conflict}}}(V^{\pi_{\theta_t}}_c(\rho)-\hat{V}^{\pi_{\theta_t}}_c(\rho))\right]\\
     & \leq 2h^+ + 2\epsilon_{\text{offline}}=\mathcal{O}(\epsilon_{\text{offline}})+ \mathcal{O}\left(\sqrt{\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^3T}}\right).
    \end{aligned}
\end{equation}
This completes the proof of Theorem \ref{main thm}.
\end{proof}

\section{Supplemental Materials for Algorithm \ref{alg: main algo}}
\label{algo extension}
We outline the Safe Adaptation step in Algorithm \ref{alg: main algo}, specifically detailed in Algorithm \ref{alg: Safe Adapt crpo} and Algorithm \ref{alg: Safe Adapt}. 
Our main theorem (Theorem \ref{main thm}) is derived based on the safety adaptation procedure described in Algorithm \ref{alg: Safe Adapt}. Our diffusion regularization method is compatible with both CRPO \cite{xu2021crpo} and the gradient manipulation method \cite{gu2024balance}, as implemented in Algorithms \ref{alg: Safe Adapt crpo} and \ref{alg: Safe Adapt}. Both algorithms aim to switch dynamically between reward optimization and cost optimization. 

The key distinction lies in their approach to handling scenarios where the cost is close to the cost limit threshold. In Algorithm \ref{alg: Safe Adapt crpo}, the process focuses on optimizing one of the two objectives based on predefined conditions. In contrast, Algorithm \ref{alg: Safe Adapt} incorporates gradient manipulation in these scenarios, further stabilizing the training process by addressing conflicts between the objectives. Theorem \ref{main thm} specifically considers the diffusion regularization algorithm equipped with the safe adaptation procedure outlined in Algorithm \ref{alg: Safe Adapt}. The fundamental difference between the two algorithms lies in the criteria for switching between reward and cost optimization objectives.


\begin{algorithm}[H]
   \caption{CRPO Adaptation}
   \label{alg: Safe Adapt crpo}
\begin{algorithmic}[1]
\Require Dataset $\mc{D}^{\mu}$
\Require Slack variable $h^+$, $h^-$ and cost limit $l$
\State \textbf{Procedure:} 
\text{CRPOAdaptation}$(\pi_\theta,\epsilon_\psi, Q^{\pi_\theta}_r,Q^{\pi_\theta}_c,h^+,h^-)$
\If{$Q^{\pi,\text{UCB}}_c(\rho)\leq l+h^+$}
\State Optimize reward by solving Eq. (\ref{optimize reward})
\Else 
\State Ensure safety by solving Eq. (\ref{optimize cost})
\EndIf
\State \textbf{end procedure} 
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
   \caption{Gradient Manipulation Adaptation}
   \label{alg: Safe Adapt}
\begin{algorithmic}[1]
\Require Dataset $\mc{D}^{\mu}$
\Require Slack variable $h^+$, $h^-$ and cost limit $l$
\State \textbf{Procedure:} 
\text{SafeAdaptation}$(\pi_\theta,\epsilon_\psi, Q^{\pi_\theta}_r,Q^{\pi_\theta}_c,h^+,h^-)$
\If{$Q^{\pi,\text{UCB}}_c(\rho)\leq l-h^{-}$}
\State Optimize reward by solving Eq. (\ref{optimize reward})
\ElsIf{$Q^{\pi,\text{UCB}}_c(\rho)\leq l+h^{+} $}
\State Compute $g_r$ and $g_c$ with Eq. (\ref{compute gr and gc})
\State Gradient manipulation to obtain $g$ with Eq. (\ref{gradient manipulation})
\Else 
\State Ensure safety by solving Eq. (\ref{optimize cost})
\EndIf
\State \textbf{end procedure} 
\end{algorithmic}
\end{algorithm}


\subsection{Hybrid Extension}
\textbf{Hybrid Agents.} The assumption of the offline reinforcement learning  setting can be extended by allowing the agent to incorporate partial online interactions during the training episode. This extension enables further updates to the critic function, enhancing its ability to evaluate safety conditions with greater accuracy. Since the problem remains within the scope of offline reinforcement learning, we restrict hybrid access to two specific types:    

\begin{itemize}
    \item Access to a simulator for evaluating the cost values $V_c^{\pi}$.
    \item Access to a limited number of trajectories collected by the current policy, which can be used to update the critics and policies, thereby partially mitigating the impact of distributional shift.
\end{itemize}

With the hybrid assumption, We propose two distinct approaches for evaluating costs: \textbf{Offline Evaluation} and \textbf{Online Evaluation}. For the whole offline setting, we only use the critic functions learned from the offline dataset to evaluate the cost constraints, while for the hybrid agent we allow for online trajectory access.

\subsection*{\textbf{Offline Evaluation}}
For the hybrid agents discussed in section \ref{extension}, we consider two distinct forms of hybrid access to environment data.


In the fully offline setting, we estimate the cost value \( V^{\pi}_c(\rho) \) by randomly sampling a batch of states \( \mathcal{B}_s \) from the static dataset. We assume that the state distribution in the dataset \( s \sim \mathcal{D}^{\mu} \) is sufficiently close to the target distribution \( \rho \). The cost estimator is defined as:
\[
\hat{V}^{\pi}_c(\rho) = \frac{1}{|\mathcal{B}_s|}\sum_{s \in \mathcal{B}_s} V^{\pi}_c(s).
\]
To avoid hyperparameter tuning and additional budget constraints on the value function, we transform the value function into an estimated episodic cost. Since the value function \( V^{\pi}_c \) can be expressed as:
\[
V^{\pi}_c(\rho) = \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi}_\rho, a \sim \pi(\cdot \vert s)}[r(s, a)],
\]
we define the estimated episodic cost as \( \hat{V}^{\pi}_c(1-\gamma)L \), where \( L \) represents the episodic length.  



\subsection*{\textbf{Online Evaluation}}
\begin{itemize}
    \item Agents are allowed to collect a limited number of trajectories to evaluate the safety of the current policy. Based on this evaluation, the agent determines whether to prioritize optimizing the reward, jointly optimizing reward and cost, or exclusively optimizing the cost. This process serves as a performance assessment of the learned policy during each episode.
    \item Agents can roll out a predefined number of trajectories to update the critic function in each episode. To ensure the setting remains consistent with offline reinforcement learning, the number of trajectories is strictly limited; otherwise, it would replicate a fully online reinforcement learning setting. By leveraging this partial online data, the agent mitigates overestimation errors in the critic function, thereby improving its ability to evaluate and optimize the policy effectively.
\end{itemize}






\section{Further Discussion on Safe Reinforcement Learning}
In this section, we  discuss on the difference between the hard constraint and soft constraint in safe reinforcement learning.   


\textbf{Hard Constraints}. Existing works focusing on hard constraints allow no violation on safety conditions, i.e.    
\begin{equation}
    c(s_t,a_t)=0, \forall t\geq 0.
    \label{zero violation}
\end{equation}

\textbf{Safe Constraints.} A variety of works focus on the soft constraint setting, the safe constraint restricts the cost limit below a certain limit $l$, either within an episode or being step wise. Which can  either can be episodic limit as 
\begin{equation}
    \E \left[\sum_{t=0}^T c(s_t,a_t)\right]\leq l  \quad \text{or}
    \quad \E \left[\sum_{t=0}^T \gamma^t c(s_t,a_t)\right]\leq l, \label{episode cost limit}
\end{equation}
or be the stepwise limit as 
\begin{equation}
    \E [c(s_t,a_t)]\leq l, a_t\sim \pi(\cdot \vert s_t). 
    \label{step limit}
\end{equation}
Still this type of problem allow for certain degree of safety violations as in \cite{xu2022constraints, lin2023safe}, but the soft constraints also allow for broader potential policy class to further explore higher reward. We choose the soft constraint as it allows for exploration to search for higher rewards. 


\subsection{Discussion on Settings}

\textbf{Comparison with Offline Reinforcement Learning.}  
In the context of safe reinforcement learning, simply pretraining the critic before extracting the policy is insufficient for learning an optimal policy. This contrasts with approaches such as those in \cite{chen2024diffusion,chen2023score}, where pretraining a reward critic \( Q_\phi(s, a) \) under the behavioral policy \( \pi_b \) using IQL \cite{kostrikov2021offline} is sufficient. These methods do not require further updates to the critic during policy extraction.

In safe reinforcement learning, however, optimization involves the reverse KL divergence term:
\begin{equation}
    \mathbb{E}_{s,a \sim \mathcal{D}^{\mu}} \left[ Q_\phi(s, a) - \frac{1}{\beta} D_{\operatorname{KL}}(\pi(\cdot \vert s) \| \mu(\cdot \vert s)) \right],
    \label{srpo objective}
\end{equation}
where \( \mu(\cdot \vert s) \) represents the behavioral policy used to collect the offline dataset. The optimal policy \( \pi^{*}_\theta \) for Eq.~(\ref{srpo objective}) is given by:
\begin{equation}
    \pi_{\theta^*}(a \vert s) \propto \mu(a \vert s) \exp(\beta Q_\phi(s, a)).
    \label{optimal policy srpo}
\end{equation}

Essentially, offline reinforcement learning algorithms aim to extract a policy \( \pi_\theta \) that adheres to the energy-based form presented in Eq.~(\ref{optimal policy srpo}). However, in the safe reinforcement learning setting, the optimal reward critic, when unconstrained, cannot be directly used for optimizing the reward objective. Therefore, it must be updated during the safety adaptation stage.


\textbf{Comparison with Constrained Reinforcement Learning.}  
While most safe reinforcement learning literature focuses on the online setting, the offline setting presents unique challenges for policy extraction. In the offline scenario, the agent's only access to the environment is through an offline dataset \( \mathcal{D}^{\mu} \), which consists of transition tuples. Ideally, these transition tuples can be utilized to construct estimators for the transition dynamics \( \hat{P}(s' \vert s, a) \), the reward function \( \hat{r}(s, a) \), and the cost function \( \hat{c}(s, a) \).

However, Markov decision processes (MDPs) are highly sensitive to even small changes in the reward function, requiring efficient exploitation of the offline dataset through conservative inference and penalization of out-of-distribution (OOD) actions. To address these challenges, our approach constrains the policy to remain within a defined neighborhood of the behavioral policy \( \pi_b \) and adopts a pessimistic estimation of the cost critic function, effectively mitigating the risk of unsafe implementations.




\section{Details of Experiments}
\subsection{Choice of Policy Class}
\begin{itemize}
    \item Standard Gaussian Policy Class: $\Pi=\{a\sim \mathcal{N}\left(m_\theta(s),\Sigma_\theta(s)\right)\}$, usually the covariance $\Sigma_\theta(s)$ matrix is a diagonal matrix. 
    \item Gaussian Policy Class with constant variance: $\Pi=\{a\sim \mathcal{N}(m_\theta(s), \sigma^2 I)\}$, here the covariance matrix $\sigma^2 I$ is state-independent.
    \item Dirac Policy family: $\Pi=\{a= m_\theta(s)\}$, we can approximate this as a Gaussian policy with variance close to 0.
\end{itemize}
\subsection{Discussion on Hyperparameters}
\textbf{Temperature Parameter \(\beta\).}  
We explore two different types of \(\beta\) schedules to regulate the trade-off between policy exploration and adherence to the behavioral policy. 
\begin{itemize}
    \item \textbf{Constant \(\beta\) Values.}  
    In this approach, \(\beta\) is maintained as a constant throughout all epochs. A low \(\beta\) value enforces conservative Q-learning by constraining the learned policy to remain close to the behavioral policy \(\pi_b\). This setting prioritizes stability and minimizes divergence from the offline dataset.

    \item \textbf{Variant \(\beta\) Values.}  
    Here, we employ a monotonic sequence of increasing \(\beta\) values over different epochs. Following the pretraining phase, the weight on optimizing the reward critic is progressively increased, or the weight on minimizing the cost critic is reduced. This dynamic adjustment encourages the policy to explore diverse strategies, allowing it to optimize returns or reduce costs effectively while gradually relaxing the conservativeness enforced during earlier stages of training.
\end{itemize}

  

\textbf{Choice of Slack Variable.}  
We define the slack bounds based on the normalized cost, ensuring that reward optimization is performed when \( V_{\text{normalized}} \leq 1 - h^- \) and cost optimization is triggered only when \( V_{\text{normalized}} \geq 1 + h^+ \). Over the course of training, we progressively reduce the values of \( h^- \) and \( h^+ \), eventually shrinking them to zero. For CRPO adaptation, we set \( h = 0.2 \). In our gradient manipulation method, as outlined in Algorithm \ref{alg: Safe Adapt}, we initialize both \( h^- \) and \( h^+ \) to \( 0.2 \) and shrink the slack variables' value by iterations.



We present the general hyperparameter setting in table \ref{table:config_comparison}. For hyperparameters that do not apply to the corresponding algorithm, we use the back slash symbol ``\textbackslash'' to fill the blank.
\begin{table}[tb]
\centering
\renewcommand{\arraystretch}{1.3} % Adjust row spacing
\caption{Summary of hyperparameter configurations for different algorithms.\label{table:config_comparison}}
\vspace{5pt}
{\small
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Hyperparameters}      & \textbf{BC} & \textbf{BCQ-Lag} & \textbf{BEAR-Lag} & \textbf{CDT} & \textbf{CPQ} &\textbf{COptiDICE}&\textbf{Ours} \\ \midrule
Device                   & Cuda         & Cuda         & Cuda             & Cuda&Cuda&Cuda&Cuda            \\
Batch Size & 512 &512& 512&2048&512&512&256\\
Update Steps & 100000 & 100000& 300000& 100000& 100000&100000&2050\\
Eval Steps &  2500& 2500& 2500& 2500& 2500& 2500& 1025\\
Threads & 4& 4& 4& 6&4&4& 4\\
Num workers & 8& 8& 8& 8&8&8&8\\
Actor Architecture(MLP) &[256,256]&[256,256]&[256,256]&\textbackslash&[256,256]&[256,256]&[256,256]\\
Critic Architecture(MLP) &\textbackslash&[256,256]&[256,256]&\textbackslash&[256,256]&[256,256]&[256,256]\\
Actor Learning rate & .001& .001& .001& .001&.001&.001&.0006\\
Critic Learning rate & \textbackslash& .001& .001& \textbackslash&.001&.001&.0006\\
Episode Length &1000 &1000&1000&1000&1000&1000&1000\\
$\gamma$ & 1.00&0.99&.99&.99&.99&.99&.99\\
$\tau$ & .005&.005&.005&.005&.005&.005&.005\\
$h^+$ & \textbackslash & \textbackslash & \textbackslash& \textbackslash &\textbackslash &\textbackslash & .2\\
$h^-$ & \textbackslash & \textbackslash & \textbackslash& \textbackslash &\textbackslash &\textbackslash & .2\\
PID(optional for DRCORL) & \textbackslash& [.1,.003, .001]&[.1,.003,.001] &\textbackslash&\textbackslash &\textbackslash& [.1,.003,.001] \\
$E$ & \textbackslash&\textbackslash&\textbackslash&\textbackslash&\textbackslash&\textbackslash& 4\\
$k$ & \textbackslash&\textbackslash&\textbackslash&\textbackslash&\textbackslash&\textbackslash& 2.0\\
$\alpha$ & \textbackslash&\textbackslash&\textbackslash&\textbackslash&\textbackslash&\textbackslash& .2\\
\bottomrule
\end{tabular}
}
\end{table}
\begin{remark}
In Table \ref{table:config_comparison}, the update steps refer to the total number of gradient descent updates performed. The evaluation steps indicate the frequency of policy evaluation, measured in terms of gradient descent steps. The actor architecture (MLP) is specified as a list representing the hidden layers, where the input corresponds to the state $s$ and the output is an action $a=\pi(s)$. Similarly, the critic architecture (MLP) is represented as a list defining the hidden layers, with the input being the state-action pair and the output being a scalar value. The parameter 
$\tau$ represents the update rate between the target critic and the critic in double-Q learning.
\end{remark}

\subsection{Experiment Environments}
As depicted in Figure \ref{fig:safety_envs}, this is the experiment environment we use, which is mujoco-Gym environments. We also attach the training curve for different tasks in this section. For practical implementation, we may consider three types of policy class $\Pi$:

\begin{figure}[tb]
    \centering
    % First row
    \subfigure[SafetyAntVel]{\includegraphics[width=0.19\textwidth]{pictures/env_figure/SafetyAntVelocity.png}}
    \subfigure[SafetyHalfCheetahVel]{\includegraphics[width=0.19\textwidth]{pictures/env_figure/SafetyHalfCheetahVelocity.png}}
    \subfigure[SafetyHopperVel]{\includegraphics[width=0.19\textwidth]{pictures/env_figure/SafetyHopperVelocity.png}}
    \subfigure[SafetySwimmerVel]{\includegraphics[width=0.19\textwidth]{pictures/env_figure/SafetySwimmerVelocity.png}}
    \subfigure[SafetyWalkerVel]{\includegraphics[width=0.19\textwidth]{pictures/env_figure/SafetyWalkerVelocity.png}}
    \caption{Mujoco-Gym Environments}
    \label{fig:safety_envs}
\end{figure}
 

\begin{figure}[H]
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[height=4cm,width=6cm]{pictures/algo/half_cheetha_reward_smooth.png}
\caption{HalfCheetah Reward}
\end{minipage}%
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[height=4cm,width=6cm]{pictures/algo/half_cheetha_cost_smooth.png}
\caption{HalfCheetah Cost}
\end{minipage}
\end{figure}

\begin{figure}[H]
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[height=4cm,width=6cm]{pictures/algo/ant_reward_smooth.png}
\caption{Ant Reward}
\end{minipage}%
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[height=4cm,width=6cm]{pictures/algo/ant_cost_smooth.png}
\caption{Ant Cost}
\end{minipage}
\end{figure}

\begin{figure}[H]
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[height=4cm,width=6cm]{pictures/algo/walker_reward_smooth.png}
\caption{Walker2D Reward}
\end{minipage}%
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[height=4cm,width=6cm]{pictures/algo/walker_cost_smooth.png}
\caption{Walker2D Cost}
\end{minipage}
\end{figure}

\newpage




\end{document}