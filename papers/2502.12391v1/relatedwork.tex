\section{Related Work}
\textbf{Diffusion Model for Offline RL.} Due to their strong generative capabilities, diffusion models have been extensively employed in offline RL for modeling heterogeneous behavioral datasets. In general, diffusion models have primarily been used for either planning \cite{janner2022planning,he2024diffusion,ajay2022conditional} or modeling policy distributions \cite{chi2023diffusion,wang2022diffusion,fang2024diffusion,hansen2023idql,chen2023score,lu2023contrastive}. However, a significant shortcoming of diffusion models is their slow inference speed, as the reverse denoising process requires many iterations. Various strategies have been proposed to address this issue, such as DDIM \cite{song2020denoising}, which reduces the sampling steps by sampling on subsequences, and DPM-Solver \cite{lu2022dpm}, which minimizes the sampling steps to around 10 by adopting a specialized diffusion ODE solver.


\textbf{Safe RL.} 
Existing literature on safe RL predominantly focuses on the online setting (see \citet{gu2022review} for a comprehensive review).
Among these studies, constrained optimization-based methods have garnered significant attention for their strong performance and broad applicability \citep{achiam2017constrained,tessler2018reward,yangprojection,liu2022constrained}.
For example, \citet{achiam2017constrained} introduced CPO, which integrates TRPO \citep{schulman2015trust} into a constrained optimization framework to derive safe policies.
Primal-dual methods employing Lagrangian optimization have also gained popularity \citep{chow2018risk,calian2020balancing,ding2020natural,ying2022dual,zhou2023gradient}.
These methods dynamically adjust the Lagrange multipliers in response to safety violations, enabling effective management of safety constraints during training.
In the category of primal-based approaches, \citet{xu2021crpo} proposed CRPO, which alleviates the need for tuning the dual variable.
Building on CRPO, \citet{gu2024balance} employed gradient manipulation to balance the conflict between reward and cost.
However, these online methods often require extensive interactions with the environment and are highly sensitive to critic estimation, making them impractical for real-world applications due to safety concerns, high costs, or limited accessibility
during data collection.





\textbf{Safe Offline RL.} Recently, significant attention has been devoted to addressing the safe RL problem in an offline setting, where agents no longer have access to online interactions with the environment. For instance, \citet{xu2022constraints} proposed learning RL policies from offline data by assigning high cost values to both out-of-distribution (OOD) actions and unsafe actions. Recent works, such as \citet{lin2023safe}, incorporate the diffuser method \citep{janner2022planning} into the safe setting to learn safe policies. Similarly, \citet{liu2023constrained} utilized the decision transformer framework \citep{chen2021decision} to facilitate the learning of safe policies. In another study, \citet{zheng2024safe} employed diffusion policies learned through energy-based weighted regression methods \citep{lu2023contrastive} to ensure adherence to hard constraints.