\section{Related Work}
\label{section-2}
This study proposes a psychometric-based evaluation method for theorem proving with LLMs. Our research builds on existing literature in Theorem Proving with LLMs, LLM evaluation methods, and psychometrics.

\textbf{Theorem Proving with LLMs.} In recent years, a growing number of studies have explored training LLMs for theorem proving \cite{yang2024formal}. These works have developed models capable of generating proofs in formal languages such as Lean \cite{moura2021lean}, Isabelle \cite{paulson1994isabelle}, and Coq \cite{huet1997coq}. Based on proof generation strategies, existing methods can be broadly categorized into tree search methods and whole-proof generation methods. Tree search methods formulate the proof process as a search problem, incrementally exploring potential proof paths within a search tree. Notable approaches include LeanDojo \cite{yang2024leandojo}, Thor \cite{jiang2022thor}, LEGO \cite{wang2023lego}, DSP \cite{jiang2022draft}, among others. In contrast, whole-proof generation methods treat the proof as a complete sequence, generating the entire proof path in a single step. Representative works in this category include Baldur \cite{first2023baldur}, DeepSeek \cite{xin2024deepseekproverv15harnessingproofassistant}, TheoremLlama \cite{wang2024theoremllama}, and MetaMath-Llemma \cite{yu2023metamath}. Among them, DeepSeek-Prover-V1.5 leverages an enhanced formal theorem proving dataset for supervised fine-tuning, achieving a 60.2\% pass rate on the miniF2F test set using a single-pass full-proof generation approach \cite{xin2024deepseekproverv15harnessingproofassistant}.

In this study, we focus on evaluating whole-proof generation models, as their evaluation should adhere to consistent criteria. In contrast, tree search methods, due to their distinct search strategies, face challenges in establishing uniform evaluation standards.

\textbf{LLM Evaluation Methods.} Evaluating the capabilities of large language models remains a critical challenge. In the field of Theorem Proving with LLMs, proof pass rate is widely regarded as the primary evaluation criterion \cite{xia2024evaluating}. However, an increasing number of studies advocate for more comprehensive evaluation methods. For instance, \cite{zhang2025mathverse, hao2024llm} propose Chain-of-Thought (CoT) evaluation strategies, which assess LLM performance at each critical reasoning step rather than relying solely on the final answer. \cite{shao2024empirical, jin2023cladder, orenes2023using} highlight the importance of analyzing errors in the reasoning process to gain deeper insights into the common mistakes made by LLMs. \cite{xia2024evaluating, hong2024stuck} explore evaluation methods based on perturbation analysis, examining how LLMs respond to variations in input. Lastly, \cite{srivastava2024evaluating} suggest categorizing and annotating evaluation data based on the number of reasoning steps and question types, enabling a more detailed assessment of LLM performance across different data types.

\cite{zhuang2023efficiently} explores the use of human psychometrics in LLM evaluation and proposes that adaptive evaluation, which adjusts to a model’s performance rather than relying on fixed test sets, will become the new standard in AI model assessment. This concept serves as an important reference for our study. Specific applications of adaptive evaluation in LLM assessment can be found in works such as \cite{polo2024tinybenchmarks, yuan2024s}, which achieve efficient evaluation with fewer examples. This study aims to investigate how adaptive evaluation methods can be applied to evaluating the theorem-proving abilities of LLMs.

\textbf{Psychometrics.} Psychometrics is a field dedicated to the effective measurement of psychological traits. In exam assessments, it provides the scientific foundation and methodological tools for designing, analyzing, and interpreting exam results, ensuring that assessments accurately and reliably measure the true abilities of students or test subjects \cite{furr2021psychometrics, templin2010diagnostic, mislevy2003brief}. Item Response Theory (IRT), introduced by \cite{baker2001basics}, models both the characteristics of individual test items (e.g., difficulty, discrimination) and the ability scores of test subjects, offering a more precise measurement framework. IRT remains one of the most influential theoretical models in psychometrics \cite{fayers2004item}. Building on IRT’s approach to modeling item difficulty and discrimination, this study develops a metric calculation method and an adaptive test algorithm specifically designed to address the limited number of test subjects (LLMs) in theorem proving, achieving promising results.