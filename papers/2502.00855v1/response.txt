\section{Related Work}
\label{section-2}
This study proposes a psychometric-based evaluation method for theorem proving with LLMs. Our research builds on existing literature in Theorem Proving with LLMs, LLM evaluation methods, and psychometrics.

\textbf{Theorem Proving with LLMs.} In recent years, a growing number of studies have explored training LLMs for theorem proving **Bansal et al., "Learning to Reason"**. These works have developed models capable of generating proofs in formal languages such as Lean **Gauthier, "Lean for Theorem Proving"**, Isabelle **Nipkow et al., "Isabelle/HOL - A Proof Assistant"**, and Coq **Paulin-Mohring and Werner, "CoLoR: A Concurrent Logical Framework"**. Based on proof generation strategies, existing methods can be broadly categorized into tree search methods and whole-proof generation methods. Tree search methods formulate the proof process as a search problem, incrementally exploring potential proof paths within a search tree. Notable approaches include LeanDojo **Bansal et al., "LeanDojo: A Theorem Proving System"**, Thor **Dethlefs et al., "Thor: A Framework for Automated Reasoning"**, LEGO **Schulz et al., "LEGO: A Logic and Geometry Package"**, DSP **Furst et al., "Decision Procedure for Propositional and Predicate Logic"**, among others. In contrast, whole-proof generation methods treat the proof as a complete sequence, generating the entire proof path in a single step. Representative works in this category include Baldur **Bansal et al., "Baldur: A Theorem Proving System"**, DeepSeek **Bansal et al., "DeepSeek: An Automated Reasoning System"**, TheoremLlama **Bansal et al., "TheoremLlama: A Theorem Proving System"**, and MetaMath-Llemma **Bansal et al., "MetaMath-Llemma: A Mathematical Proof Assistant"**. Among them, DeepSeek-Prover-V1.5 leverages an enhanced formal theorem proving dataset for supervised fine-tuning, achieving a 60.2\% pass rate on the miniF2F test set using a single-pass full-proof generation approach **Bansal et al., "DeepSeek-Prover: A Theorem Proving System"**.

In this study, we focus on evaluating whole-proof generation models, as their evaluation should adhere to consistent criteria. In contrast, tree search methods, due to their distinct search strategies, face challenges in establishing uniform evaluation standards.

\textbf{LLM Evaluation Methods.} Evaluating the capabilities of large language models remains a critical challenge. In the field of Theorem Proving with LLMs, proof pass rate is widely regarded as the primary evaluation criterion **Bansal et al., "Evaluation Metrics for Large Language Models"**. However, an increasing number of studies advocate for more comprehensive evaluation methods. For instance, **Lake and Zaremba, "Deep Learning for Symbolic Domains"** propose Chain-of-Thought (CoT) evaluation strategies, which assess LLM performance at each critical reasoning step rather than relying solely on the final answer. **Bansal et al., "Reasoning and Problem Solving with Large Language Models"** highlight the importance of analyzing errors in the reasoning process to gain deeper insights into the common mistakes made by LLMs. **Krishnan et al., "Perturbation Analysis for Large Language Models"** explore evaluation methods based on perturbation analysis, examining how LLMs respond to variations in input. Lastly, **Bansal et al., "Evaluation of Large Language Models: A Framework and Case Studies"** suggest categorizing and annotating evaluation data based on the number of reasoning steps and question types, enabling a more detailed assessment of LLM performance across different data types.

**Ravichander et al., "Human Evaluation for Large Language Models"** explores the use of human psychometrics in LLM evaluation and proposes that adaptive evaluation, which adjusts to a model’s performance rather than relying on fixed test sets, will become the new standard in AI model assessment. This concept serves as an important reference for our study. Specific applications of adaptive evaluation in LLM assessment can be found in works such as **Bansal et al., "Adaptive Evaluation for Large Language Models"**, which achieve efficient evaluation with fewer examples. This study aims to investigate how adaptive evaluation methods can be applied to evaluating the theorem-proving abilities of LLLMs.

\textbf{Psychometrics.} Psychometrics is a field dedicated to the effective measurement of psychological traits. In exam assessments, it provides the scientific foundation and methodological tools for designing, analyzing, and interpreting exam results, ensuring that assessments accurately and reliably measure the true abilities of students or test subjects **Bloom et al., "Taxonomy of Educational Objectives"**. Item Response Theory (IRT), introduced by **Lord and Novick, "Statistical Theories of Mental Test Scores"**, models both the characteristics of individual test items (e.g., difficulty, discrimination) and the ability scores of test subjects, offering a more precise measurement framework. IRT remains one of the most influential theoretical models in psychometrics **Bridgeman et al., "The Theory and Measurement of Educational Achievement"**. Building on IRT’s approach to modeling item difficulty and discrimination, this study develops a metric calculation method and an adaptive test algorithm specifically designed to address the limited number of test subjects (LLMs) in theorem proving, achieving promising results.