In this section we present our proof for \cref{thm:k-threshold} and \cref{thm:counting}.
We use $\Threshold{n}{k}$ to denote $k$-Threshold problem with input length $n$.

\subsection{Lower bound for \texorpdfstring{$k$}{k}-Threshold}
In this section we prove the lower bound part of \cref{thm:k-threshold}. That is, solving $\Threshold{n}{k}$ for $k\le 2n-1$ requires at least $(1-o(1)) \frac{n \log \frac k\delta}{\DKL}$ noisy queries in expectation.
The $k=o(n)$ case has been proved in \cite{wang2024noisy} (see also \cref{sec:th-small} for our alternative and simpler proof).


\begin{restatable}{theorem}{ThresholdSmallK}\label{thm:th:small-k}
For $k = o(n)$, solving $\Threshold{n}{k}$ with $
\delta = o(1)$ error probability requires
\[
(1-o(1))\frac{n \log \frac{k}{\delta}}{\DKL}
\]
noisy queries in expectation,
even when the input is uniformly chosen from $\binom{[n]}{k}$ with probability $1/2$ and uniformly chosen from $\binom{[n]}{k-1}$ with probability $1/2$. %
\end{restatable}
In our lower bound proof, we will use \cref{thm:th:small-k} with $k=O(n/\log n)$.

We first prove the case where $k=(n+1)/2$.
\begin{lemma}
\label{lem:th:maj}
  Solving $\Threshold{2k-1}{k}$ with $
\delta = o(1)$ error probability requires
\[
(1-o(1))\frac{2k \log \frac{k}{\delta}}{\DKL}
\]
noisy queries in expectation, even when the input is uniformly chosen from $\binom{[2k-1]}{k}$ with probability $1/2$ and uniformly chosen from $\binom{[2k-1]}{k-1}$ with probability $1/2$.
\end{lemma}
\begin{proof}
Suppose for the sake of contradiction that we have an algorithm $\cA$ that solves $\Threshold{2k-1}{k}$ with error probability $\delta = o(1)$ and uses only $(1-\epsilon)\frac{2k \log \frac{k}{\delta}}{\DKL}$ noisy queries in expectation, for some absolute constant $\epsilon>0$. Let $\cD$ be a distribution of inputs where with $1/2$ probability the input is chosen uniformly from $\binom{[2k-1]}{k}$, and with $1/2$ probability the input is chosen uniformly from $\binom{[2k-1]}{k-1}$. Now we consider two cases, depending on whether the expected number of queries $\cA$ make on indices with $1$'s is larger or not. In either case, we will use $\cA$ to obtain an algorithm more efficient than the lower bound in \cref{thm:th:small-k}, thus reaching a contradiction.

First, suppose $\cA$ makes more queries in expectation on indices with $1$'s under input distribution $\cD$. Let $k' = \Theta(k / \log k)$ and let $n = k - 1 + k'$. Consider an instance of $\Threshold{n}{k'}$ where the input is uniformly chosen from $\binom{[n]}{k'}$ with probability $1/2$ and uniformly chosen from $\binom{[n]}{k'-1}$ with probability $1/2$. By \cref{thm:th:small-k}, this instance requires $(1-o(1)) \frac{n \log \frac{k'}{\delta}}{\DKL}=(1-o(1)) \frac{k \log \frac{k}{\delta}}{\DKL}$ queries in expectation. We will design an algorithm $\cB$ solving such an instance utilizing $\cA$.

When $\cB$ gets the input, it first adds $2k-1-n$ $1$'s to the input, and then randomly shuffle the indices. Then $\cB$ sends this input to $\cA$. Whenever $\cA$ makes a query to a $1$ that is artificially added, $\cB$ simulates a noisy query using random bits; when $\cA$ makes a query to an actual input, $\cB$ makes a query as well and pass the result to $\cA$. When $\cA$ returns a result, $\cB$ returns the same result. It is not difficult to verify that the input distribution for $\cA$ is exactly $\cD$. Also, whenever the input to $\cA$ has at least $k$ $1$'s, the input to $\cB$ has $k'$ $1$'s, and vice versa, so the correct output of $\cA$ is the same as  the correct output of $\cB$. Therefore, $\cB$ is correct whenever $\cA$ is correct, which happens with probability $\delta$.

Next, we analyze the expected number of queries $\cB$ makes, which consist of two parts:
\begin{itemize}
  \item The number of queries $\cA$ makes to an actual $0$ in the input: Because we are in the case where $\cA$ makes more queries in expectation on indices with $1$'s in the input than indices with $0$'s, the expected number of this type of queries is at most half of the expected total number of queries $\cA$ makes. Thus, the number of queries in this case is at most $(1-\epsilon)\frac{k \log \frac{k}{\delta}}{\DKL}$.
  \item The number of queries $\cA$ makes to an actual $1$ in the input: Each actual $1$ in the input to $\cB$ is later permuted to a random position in the sequence. By symmetry, a random $1$ in the input to $\cA$ under the input distribution $\cD$ is queried at most $\frac{1}{k-1} \cdot (1-\epsilon)\frac{2k \log \frac{k}{\delta}}{\DKL}$ times in expectation. The number of actual $1$'s is $k' = \Theta(k / \log k)$, so the expected number of queries $\cA$ makes on them is $\frac{k'}{k-1} \cdot (1-\epsilon)\frac{2k \log \frac{k}{\delta}}{\DKL} = o(1) \cdot \frac{k \log \frac{k}{\delta}}{\DKL}$.
\end{itemize}
As a result, the total number of queries $\cB$ makes is $(1-\epsilon)\frac{k \log \frac{k}{\delta}}{\DKL}$, which contradicts \cref{thm:th:small-k}.


For the second case where $\cA$ makes more (or equal number of) queries in expectation on indices with $0$'s under input distribution $\cD$. The only difference is that, when $\cB$ sends the input to $\cA$, it has to flip the roles of $0$'s and $1$'s. Additionally, it has to flip the result $\cA$ returns. We omit the details.
\end{proof}

Given \cref{thm:th:small-k} and \cref{lem:th:maj}, we are ready to prove the lower bound for general $k$.
\begin{theorem}
\label{thm:th:large-k}
  Solving $\Threshold{n}{k}$ with $
\delta = o(1)$ error probability requires
\[
(1-o(1))\frac{n \log \frac{k}{\delta}}{\DKL}
\]
noisy queries in expectation for $n/\log n \le k \le n / 2$.
\end{theorem}
\begin{proof}
  The high-level proof strategy is similar to that of \cref{lem:th:maj}, by reducing from a hard instance to $\Threshold{n}{k}$. However, the difference is that we need to reduce from two different cases depending on whether the average number of queries per $1$ or per $0$ is larger. In \cref{lem:th:maj} we did not have to do it because we can simply flip all the $0$'s and $1$'s in the input and retain the same problem as $n = 2k - 1$.

  Let $\cD$ be the input distribution where with $1/2$ probability the input is chosen uniformly from $\binom{[n]}{k}$ and with $1/2$ probability the input is chosen uniformly from $\binom{[n]}{k - 1}$. Let $\cA$ be an algorithm solving $\Threshold{n}{k}$ under input distribution $\cD$ using $(1-\epsilon)\frac{n \log \frac{k}{\delta}}{\DKL}$ noisy queries in expectation, for some absolute constant $\epsilon>0$. Let $q_0$ denote the expected number of queries $\cA$ makes on a random index with input value $0$ (under input distribution $\cD$), and let $q_1$ denote the expected number of queries $\cA$ makes on a random index with input value $1$ (under input distribution $\cD$). Let $Q$ be the expected number of queries $\cA$ makes.

  Consider the following two cases: $q_0\le q_1$ and $q_0>q_1$.

  \paragraph{Case $q_0 \le q_1$.}

  Let $k' = \Theta(n / \log n) \le k$ and let $n' = n - k + k'$. Note that $n' \in [n / 2, n]$, so we have $k' = \Theta(n' / \log n')$. By \cref{thm:th:small-k}, solving $\Threshold{n'}{k'}$ under input distribution where with $1/2$ probability the input is uniformly from $\binom{[n']}{k'}$ and with $1/2$ probability the input is uniformly from $\binom{[n']}{k' - 1}$ with error probability $\delta$ requires $(1-o(1))\frac{n' \log \frac{k'}{\delta}}{\DKL} = (1-o(1))\frac{n' \log \frac{n}{\delta}}{\DKL}$ noisy queries in expectation.

  Given an instance of $\Threshold{n'}{k'}$, we add $n - n'$ artificial $1$'s to the input, and then randomly permute the input, and feed it to $\cA$. If $\cA$ queries an actual input, we also make an actual query; if $\cA$ queries an artificial input, we can simulate a query without making an actual query. Finally, we use the result returned by $\cA$ as our answer. It is not difficult to verify that this algorithm is correct with error probability $\delta$, and the input distribution to $\cA$ is $\cD$.

  Let us analyze the expected number of queries $Q'$ used by the algorithm, which can be expressed as follows:
  \[
  \frac{1}{2} \cdot \left(q_1 (k' - 1) + q_0 (n - k + 1) \right) + \frac{1}{2} \cdot \left(q_1 k' + q_0 (n - k) \right).
  \]
  Also, notice that the number of queries $Q$ made by $\cA$ under input $\cD$ is
  \[
  \frac{1}{2} \cdot \left(q_1 (k - 1) + q_0 (n - k + 1) \right) + \frac{1}{2} \cdot \left(q_1 k + q_0 (n - k) \right).
  \]
  As $q_0 \le q_1$, the above implies that $q_1 \ge Q / n$. Furthermore, we have that $Q - Q' = q_1 \cdot (k - k') = q_1 \cdot (n - n') \ge \frac{n - n'}{n} \cdot Q$. Therefore,
  \[
  Q' \le \frac{n'}{n} \cdot Q \le (1-\epsilon)\frac{n' \log \frac{k}{\delta}}{\DKL} \le  (1-\epsilon)\frac{n' \log \frac{n}{\delta}}{\DKL},
  \]
  which contradicts \cref{thm:th:small-k}.

  \paragraph{Case $q_0 > q_1$.}
  By \cref{lem:th:maj}, solving $\Threshold{2k-1}{k}$ under input distribution where with $1/2$ probability the input is uniformly from $\binom{[2k-1]}{k}$ and with $1/2$ probability the input is uniformly from $\binom{[2k-1]}{k - 1}$ with error probability $\delta$ requires $(1-o(1))\frac{2k \log \frac{k}{\delta}}{\DKL} = (1-o(1))\frac{2k \log \frac{n}{\delta}}{\DKL}$ noisy queries in expectation.

  Given such an input to $\Threshold{2k-1}{k}$, we add $n - 2k+1$ artificial $0$'s to the input, and then randomly permute the input, and feed it to $\cA$. Similar as before, we make an actual query if $\cA$ queries an actual input, and we simulate a query otherwise. It is not difficult to verify that this algorithm is correct with error probability $\delta$, and the input distribution to $\cA$ is $\cD$.

  The expected number of actual queries used by the algorithm can be similarly analyzed as the previous case, which can be upper bounded by
  $(1-\epsilon)\frac{2k \log \frac{n}{\delta}}{\DKL},
  $
  contradicting \cref{lem:th:maj}.
\end{proof}



\subsection{Upper bound for \texorpdfstring{$k$}{k}-Threshold}
In this section we prove our upper bound for $k$-Threshold, stated as follows.

\begin{theorem}
\label{thm:threshold-upper}
  Given a sequence $a \in \{0, 1\}^n$ and an integer $1 \le k \le n$, there is an algorithm that can output $\min\{k, \lVert a \rVert_1\}$ with error probability $\delta = o(1)$ using
  \[
  (1+o(1))\frac{n \log \frac{k}{\delta}}{\DKL}
  \]
  noisy queries in expectation.
\end{theorem}

\subsubsection{Preliminaries}
The following lemmas are standard.

\begin{lemma}[e.g., \cite{gu2023optimal, wang2024noisy}]
\label{lem:check-bit}
    For a bit $B$, there is an algorithm $\textup{\textsc{Check-Bit}}(B, \delta)$ that can return the value of the bit with error probability $\le \delta$ using
    \[
    (1+o_{1/\delta}(1)) \frac{\log \frac{1}{\delta}}{\DKL}
    \]
    noisy queries in expectation.
\end{lemma}

\begin{lemma}[e.g., \cite{feller}]
\label{lem:monkey-at-cliff}
Consider a biased random walk on $\bZ$ starting at $0$. At each time step, the walk adds $1$ to the current value with probability $p < 1/2$, and adds $-1$ to the current value with probability $1-p$. Then the probability that the random walk ever reaches some integer $x \ge 0$ is $(p/(1-p))^x$.
\end{lemma}

\begin{lemma}[e.g., \cite{feller}]
\label{lem:expected-hitting-time}
Consider a biased random walk on $\bZ$ starting at $0$. At each time step, the walk adds $1$ to the current value with probability $p < 1/2$, and adds $-1$ to the current value with probability $1-p$. Then the expected number of steps needed to first reach some integer $-x \le 0$ is $\frac{x}{1-2p}$.
\end{lemma}

Our algorithm for $k$-Threshold and Counting uses the following asymmetric version of $\textsc{Check-Bit}$.
\begin{lemma}
\label{lem:asymmetric-check-bit}
    For a bit $B$, there is an algorithm $\textup{\textsc{Asymmetric-Check-Bit}}(B, \delta_0, \delta_1)$ such that:
    \begin{itemize}
        \item If the actual value of $B$ is $0$, then the algorithm returns the value of the bit with error probability $\le \delta_0$
        using
        \[
        (1+o_{1/\delta_1}(1)) \frac{\log \frac{1}{\delta_1}}{\DKL}
        \]
        noisy queries in expectation.
        \item If the actual value of $B$ is $1$, then the algorithm returns the value of the bit with error probability $\le \delta_1$
        using
        \[
        (1+o_{1/\delta_0}(1)) \frac{\log \frac{1}{\delta_0}}{\DKL}
        \]
        noisy queries in expectation.
    \end{itemize}
\end{lemma}
\begin{proof}
Let $a$ and $b$ be two integer parameters to be set later. The algorithm works as follows: we keep querying the input bit, and keep track of the number of queries that returns $1$ (denoted by $q_1$) and the number of queries that returns $0$ (denoted by $q_0$). We stop once $q_1 - q_0 = -a$, in which case we declare the bit to be $0$, or $q_1 - q_0 = b$, in which case we declare the bit to be $1$.

If the input bit is $1$, then the probability that the algorithm returns $0$ can be upper bounded by
$
\left(\frac{p}{1-p}\right)^{a}
$
using \cref{lem:monkey-at-cliff},
so by setting $a = \left\lceil \frac{\log(1/\delta_1)}{\log((1-p) / p)} \right\rceil$, this probability is upper bounded by $\delta_1$.


On the other hand, if the input bit is $0$, then the probability that the algorithm returns $1$ can be upper bounded by
$
\left(\frac{p}{1-p}\right)^{b}
$
using \cref{lem:monkey-at-cliff},
so by setting $b = \left\lceil \frac{\log(1/\delta_0)}{\log((1-p) / p)} \right\rceil $, this probability is upper bounded by $\delta_0$.

Now we consider the expected running time of the algorithm. First suppose the input bit is $1$, then the algorithm can be viewed as a random walk on integers starting at $0$, and each time it adds $1$ with probability $1-p$ and subtracts $1$ with probability $p$. It stops once the random walk reaches $-a$ or $b$. This stopping time is upper bounded by the first time it reaches $b$, and the expected number of steps required for it to first reach $b$ is $\frac{b}{1-2p} = (1+o_{1/\delta_0}(1)) \frac{\log(1/\delta_0)}{\DKL}$ by \cref{lem:expected-hitting-time}. Similarly, if the input bit is $0$, then the expected number of queries used by the algorithm is upper bounded by $(1+o_{1/\delta_1}(1)) \frac{\log(1/\delta_1)}{\DKL}$.
\end{proof}

\subsubsection{The proof}
We are now ready to state our proof of \cref{thm:threshold-upper}.

Our algorithm is extremely simple and is outlined in \cref{algo:threshold}. It repeatedly calls the subroutine $\textsc{Asymmetric-Check-Bit}(a_i, \delta / 2n, \delta / 2k)$ from \cref{lem:asymmetric-check-bit} for every $i \in [n]$, and uses the returned value as the guess for $a_i$. If at any point, the current number of $a_i$ whose guess is $1$ reaches $k$, the algorithm terminates early and return $k$. Otherwise, the algorithm returns the number of $a_i$ whose guess is $1$ at the end.
\begin{algorithm}[ht]
\caption{}
\label{algo:threshold}
\begin{algorithmic}[1]
\Procedure{Threshold-Count}{$\{a_1, \ldots, a_n\}, k, \delta$}
\State $cnt = 0$
\For{$i = 1 \to n$}
\State $cnt = cnt + \textsc{Asymmetric-Check-Bit}(a_i, \delta / 2n, \delta / 2k)$
\If{$cnt \ge k$}
\Return{$k$}
\EndIf
\EndFor
\State \Return{$cnt$}
\EndProcedure
\end{algorithmic}
\end{algorithm}
\paragraph{Error probability.} We first analyze the error probability of the algorithm. First, suppose $\lVert a \rVert_1 \ge k$. Let $S \subseteq [n]$ be an arbitrary size-$k$ set where $a_i = 1$ for $i \in S$. Then by the guarantee of \textsc{Asymmetric-Check-Bit}, for every $i \in S$, the probability that the guess for $a_i$ is not $1$ is $\le \delta / 2k$. Therefore, by union bound, the guesses for $a_i$ for all $i \in S$ are $1$ with probability $\ge 1-\delta/2$. Therefore, the count will be at least $k$ so the algorithm will return $k$ as the correct answer with error probability $\le \delta / 2 \le \delta$.

If $\lVert a \rVert_1 < k$, then by union bound, the probability that all guesses are correct is $\ge 1 - \frac{\delta}{2k} \cdot \lVert a \rVert_1 - \frac{\delta}{2n} \cdot (1 - \lVert a \rVert_1) \ge 1-\delta$, so the returned count of the algorithm is also correct with probability $\ge 1- \delta$.

\paragraph{Expected number of queries.} We first consider the number of $a_i$ with $a_i = 1$ that we pass to \textsc{Asymmetric-Check-Bit}. One trivial upper bound is $\lVert a \rVert_1$. Also, the expected number of $a_i = 1$ we need to pass to \textsc{Asymmetric-Check-Bit} before $cnt$ is incremented by $1$ is $\le \frac{1}{1-\delta/2k}$, so another upper bound is $\frac{k}{1-\delta/2k} \le (1+o(1)) k$ (as $\delta = o(1)$). On the other hand, the number of $a_i$ where $a_i = 0$ that we pass to \textsc{Asymmetric-Check-Bit} is upper bounded by $n - \lVert a \rVert_1$. Note that whether we pass some $a_i$ to \textsc{Asymmetric-Check-Bit} only depends on the queries we make to $a_{i'}$ for $i' < i$, so even given that we pass some $a_i$ to \textsc{Asymmetric-Check-Bit}, we can still use the bounds from \cref{lem:asymmetric-check-bit} to bound the expected number of queries we make to $a_i$. Therefore, the expected number of queries can be upper bounded by
\[
(1+o(1)) \left(\min\left\{\lVert a \rVert_1, k\right\} \cdot  \frac{\log\frac{n}{\delta}}{\DKL} + (n - \lVert a \rVert_1) \cdot \frac{\log \frac{k}{\delta}}{\DKL}\right).
\]
Then we consider two cases depending on how large $k$ is.

\paragraph{Case $k \ge n/\log n$.}
  In this case, $\log(n) = (1+o(1)) \log k$, so the expected number of queries can be upper bounded by
  \begin{align*}
& (1+o(1)) \left(\min\left\{\lVert a \rVert_1, k\right\} \cdot  \frac{\log \frac{k}{\delta}}{\DKL} + (n - \lVert a \rVert_1) \cdot \frac{\log \frac{k}{\delta}}{\DKL}\right) \\
= & (1+o(1)) \frac{n \log \frac{k}{\delta}}{\DKL}.
\end{align*}

\paragraph{Case $k \le n/\log n$.}
In this case, $k \log \frac{n}{\delta} = o(n \log \frac{k}{\delta})$, so the expected number of queries can be upper bounded by
\begin{align*}
& (1+o(1)) \left(k \cdot  \frac{\log\frac{n}{\delta}}{\DKL} + n \cdot \frac{\log \frac{k}{\delta}}{\DKL}\right) \\
= & (1+o(1)) \frac{n \log \frac{k}{\delta}}{\DKL}.
\end{align*}



\subsection{Bounds for Counting}

Let us first prove a one-sided upper bound for Counting.
\begin{theorem}
\label{thm:counting-upper-1}
  Given a sequence $a \in \{0, 1\}^n$, there is an algorithm that can output $\lVert a \rVert_1$ with error probability $\delta = o(1)$ using
  \[
  (1+o(1))\frac{n \log \frac{\lVert a \rVert_1 + 1}{\delta}}{\DKL}
  \]
  noisy queries in expectation.
\end{theorem}
The difference between \cref{thm:counting-upper-1} and the upper bound part of \cref{thm:counting} is that we have $\lVert a \rVert_1 + 1$ rather than $\min\{\lVert a \rVert_1 + 1, n-\lVert a \rVert_1 + 1\}$ inside the log term.
\begin{proof}

Outlined in \cref{algo:counting}, the algorithm for counting is an adaptation of \cref{algo:threshold}. Notice that in \cref{algo:threshold}, when calling $\textsc{Asymmetric-Check-Bit}$, we have $\delta_0 = \delta/2n$ and $\delta_1 = \delta / 2k$. The value $\delta_0$ is fixed regardless of the value of $k$, and the value $\delta_1$ depends on $k$. The algorithm for counting in some sense is simulating \cref{algo:threshold}, but dynamically adjusting $\delta_1$ based on the current estimate of $k$, which is the number of input bits that are believed to be $1$'s.

For simplicity, throughout the analysis, we use $k^* = \lVert a \rVert_1$ to denote the desired answer. Let $S_0 \subseteq [n]$ be indices $i$ with $a_i = 0$ and let $S_1 = [n] \setminus S_0$.

\begin{algorithm}[ht]
\caption{}
\label{algo:counting}
\begin{algorithmic}[1]
\Procedure{Counting}{$\{a_1, \ldots, a_n\}, \delta$}
\State $k \gets 0$
\State $c \gets \{0\}^n$
\State $Active \gets [n]$
\While{True}
\State $i^* \gets \argmax_{i \in Active} c_i$
\If{$c_{i^*} \le -\frac{\log (6(k+1) / \delta)}{\log((1-p)/p)}$}
\Return{$k$}
\EndIf
\If{$\textsc{Query}(a_{i^*}) = 1$}
\State $c_{i^*} = c_{i^*} + 1$
\If{$c_{i^*} \ge \frac{\log (6 n / \delta)}{\log((1-p)/p)}$}
\State $Active \gets Active \setminus \{i^*\}$
\State $k \gets k + 1$
\EndIf
\Else
\State $c_{i^*} = c_{i^*} - 1$
\EndIf
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\paragraph{Error probability.} For any $i$, we can view the value of $c_i$ as a random walk, i.e., after every query to $a_i$, we either adds $1$ to $c_i$ or subtracts $1$ from $c_i$. If $c_i = 1$, then the probability that we add $1$ to $c_i$ after every step is $1-p$, and the probability that we subtract $1$ is $p$. If $c_i = 0$, then the probability that we add $1$ to $c_i$ after every step is $p$, and the probability that we subtract $1$ is $1-p$. Conceptually, we can view $c_i$ as an infinite random walk, and the algorithm only utilizes some prefix of it.

For $i \in S_1$, by \cref{lem:monkey-at-cliff}, the probability that $c_i$ ever reaches $-\left\lceil \frac{\log (6 / \delta)}{\log((1-p)/p)} \right\rceil$ is at most
\[
\left( \frac{p}{1-p}\right)^{\frac{\log (6 / \delta)}{\log((1-p)/p)} } = \delta / 6.
\]
Furthermore, if some $c_i$ never reaches $-\left\lceil \frac{\log (6 / \delta)}{\log((1-p)/p)} \right\rceil$, then $i$ will eventually be removed from $Active$ and contributes towards $k$. Since the random walks $c_i$'s are independent for different $i$'s, the probability that there are $\lceil k^* /2 \rceil + 1$ many $i \in S_1$ where  $c_i$ reaches $-\left\lceil \frac{\log (6 / \delta)}{\log((1-p)/p)} \right\rceil$ can be upper bounded by
\[
\binom{k^*}{\lceil k^* /2 \rceil + 1} \cdot (\delta / 6)^{\lceil k^* /2 \rceil + 1} \le 2^{k^*} (\delta / 6)^{ k^* /2  + 1} \le (1.5\delta)^{k^* /2} \cdot (\delta / 6) \le \delta / 6,
\]
where the last step holds because $\delta = o(1)$ and hence we can assume $1.5 \delta \le 1$.
Thus, up to $\delta / 6$ error probability, the number of $i \in S_1$ that contributes towards $k$ is at least $k^* - (\lceil k^* /2 \rceil + 1)$, which means the final value of $k$ is $\ge \max\{0, k^* - (\lceil k^* /2 \rceil + 1)\} = \max\{0, \lfloor k^* / 2 \rfloor - 1\}$. This further implies that $k + 1 \ge k^* / 3$. Let $E_1$ be the event $k + 1 \ge k^* / 3$ and as analyzed above, $\bP(\neg E_1) \le \delta / 6$.


Next, we consider the event $E_2$ where all $c_i$ for $i \in S_1$ never reaches $-\left\lceil \frac{\log (6 (k^* / 3) / \delta)}{\log((1-p)/p)} \right\rceil$. By \cref{lem:monkey-at-cliff}, the probability that each $c_i$ for $i \in S_1$ reaches $-\left\lceil \frac{\log (6 (k^* / 3) / \delta)}{\log((1-p)/p)} \right\rceil$ is upper bounded by
\[
\left( \frac{p}{1-p}\right)^{\frac{\log (6 (k^* / 3) / \delta)}{\log((1-p)/p)}} = \frac{\delta}{2 k^*}.
\]
Therefore, by union bound, $\bP(\neg E_2) \le \delta /2$.

Next, let $E_3$ be the event where all $c_i$ for $i \in S_0$ never reaches $\left\lceil \frac{\log (6 n / \delta)}{\log((1-p)/p)} \right\rceil$. By \cref{lem:monkey-at-cliff}, the probability that each $c_i$ for $i \in S_0$ reaches $\left\lceil \frac{\log (6 n / \delta)}{\log((1-p)/p)} \right\rceil$ is upper bounded by
\[
\left( \frac{p}{1-p}\right)^{\frac{\log (6 n / \delta)}{\log((1-p)/p)}} = \frac{\delta}{6n}.
\]
Therefore, by union bound, $\bP(\neg E_3) \le \delta /6$.

Assume $E_1, E_2, E_3$ all happen. Then we know that $k + 1 \ge k^* / 3$, and so all $i \in S_1$ will eventually be removed from $Active$ and contribute towards $k$. Also, all $i \in S_0$ will not contribute towards $k$. Therefore, the returned value of $k$ will be equal to $k^*$. Hence, the error probability of the algorithm is upper bounded by $\bP(\neg E_1 \vee \neg E_2 \vee \neg E_3) \le \delta / 6 + \delta / 2 + \delta / 6 \le \delta$.

\paragraph{Expected number of queries.} Next, we analyze the expected number of queries used by the algorithm.

First, for every $i \in S_1$, the expected number of times we query $a_i$ is upper bounded by the expected number of steps it takes for the random walk $c_i$ takes to reach $\left\lceil \frac{\log (6 n / \delta)}{\log((1-p)/p)} \right\rceil$. By \cref{lem:expected-hitting-time}, it is bounded by
\[
\frac{\left\lceil \frac{\log (6 n / \delta)}{\log((1-p)/p)} \right\rceil}{1 - 2p} = (1+o(1)) \frac{\log \frac{n}{\delta}}{\DKL}.
\]

Then we consider the expected number of times we query $a_i$ for $i \in S_0$. Fix any $i \in S_0$. Let $F_j$ be the event where for exactly $j$ distinct $i' \in S_0 \setminus \{i\}$, the infinite random walk $c_j$ ever reaches $\left\lceil \frac{\log (6 n / \delta)}{\log((1-p)/p)} \right\rceil$. By analysis in the error probability part, $\bP(F_j) \le \binom{n}{j} \cdot (\delta / 6 n)^j \le \delta^j$. Note that $F_j$ is independent with the random walk $c_i$. If $F_j$ holds, then the number of times we query $a_i$ is upper bounded by the expected number of steps it takes for the random walk $c_i$ first hits $\left\lceil \frac{\log (6 (k^* + j + 1)) / \delta)}{\log((1-p)/p)} \right\rceil$ (because once $c_i$ hits this value, $k$ can never be larger than $k^* + j$ under $F_j$, so we will not query $a_i$ again). By \cref{lem:expected-hitting-time}, this expectation is
\[
\frac{\left\lceil \frac{\log (6 (k^* + j + 1)) / \delta)}{\log((1-p)/p)} \right\rceil}{1 - 2p} = (1+o(1)) \frac{\log \frac{k^* + j + 1}{\delta}}{\DKL}.
\]
Let $Q_i$ be the number of times we query $a_i$. Then we have
\begin{align*}
  \bE(Q_i) &= \sum_{j \ge 0} \bE(Q_i | F_j) \cdot \bP(F_j)\\
  &\le \frac{1+o(1)}{\DKL} \sum_{j \ge 0}  \delta^j \cdot \log \left(\frac{k^* + j + 1}{\delta} \right)\\
  &\le \frac{1+o(1)}{\DKL} \sum_{j \ge 0}  \delta^j \cdot \log \left(\frac{(k^* + 1) (j + 1)}{\delta} \right)\\
  &= \frac{1+o(1)}{\DKL} \left( \sum_{j \ge 0}  \delta^j \cdot \log \left(\frac{k^* + 1}{\delta} \right) + \sum_{j \ge 0} \delta^j \cdot \log(j+1)\right) \\
  &\le \frac{1+o(1)}{\DKL} \left( \frac{1}{1-\delta} \cdot \log \left(\frac{k^* + 1}{\delta} \right) + O(1)\right) \\
  &\le (1+o(1)) \frac{\log \frac{k^*+1}{\delta}}{\DKL}.
\end{align*}

Summing up everything, the overall expected number of queries is
\[(1+o(1)) \left(k^* \cdot  \frac{\log \frac{n}{\delta}}{\DKL} + (n - k^*) \cdot \frac{\log\frac{k^*+1}{\delta}}{\DKL}\right).
\]
Similar to the proof of \cref{thm:threshold-upper}, we consider two cases depending on how large $k^*$ is.

\paragraph{Case $k^* \ge n/\log n$.}
In this case, $\log(n) = (1+o(1)) \log (k^*+1)$, so the expected number of queries can be bounded by
  \begin{align*}
    & (1+o(1)) \left(k^* \cdot  \frac{\log \frac{k^*+1}{\delta}}{\DKL} + (n - k^*) \cdot \frac{\log \frac{k^*+1}{\delta}}{\DKL}\right)\\
    =& (1+o(1)) \frac{n \log \frac{k^*+1}{\delta}}{\DKL}.
  \end{align*}

\paragraph{Case $k^* \le n/\log n$.}
In this case, $k^* \log \frac{n}{\delta} = o(n \log \frac{k^*+1}{\delta})$, so the expected number of queries can be bounded by
   \begin{align*}
    & (1+o(1)) \left(k^* \cdot  \frac{\log \frac{n}{\delta}}{\DKL} + n \cdot \frac{\log \frac{k^*+1}{\delta}}{\DKL}\right)\\
    =& (1+o(1)) \frac{n \log \frac{k^*+1}{\delta}}{\DKL}.
  \end{align*}
\end{proof}


Finally, we prove \cref{thm:counting}, which we recall below:

\Counting*
\begin{proof}

Suppose there is an algorithm for Counting with
\[
(1-\epsilon)\frac{n \log \frac{\min\{\lVert a \rVert_1, n - \lVert a \rVert_1\} + 1}{\delta}}{\DKL}
\]
noisy queries in expectation, for some absolute constant $c>0$. First, suppose $\lVert a \rVert_1 \le n / 2$. Recall that in the lower bound for $k$-Threshold, the hard distribution for $k$-Threshold is to distinguish whether the input contains $k$ or $k-1$ $1$'s. Thus, by running the assumed algorithm for Counting, the running time would be
\[
(1-\epsilon)\frac{n \log \frac{k + 1}{\delta}}{\DKL},
\]
which contradicts the lower bound for $k$-Threshold.

If $\lVert a \rVert_1 > n / 2$, we can simply first flip all the bits in the $k$-Threshold instance, and then use the same argument.


For the upper bound, given \cref{thm:counting-upper-1}, it suffices to first estimate whether $\lVert a \rVert_1$ is bigger than $n/2$ or smaller. When it is smaller, we can directly run \cref{thm:counting-upper-1}; otherwise, we first flip all input bits and then run \cref{thm:counting-upper-1}.

More precisely, we randomly sample $n^{0.99}$ input elements with replacements, use \cref{lem:check-bit} to estimate them with error probability $1/n^{100}$. If the fraction of elements in the sample whose estimates are $1$ is $\le \frac{1}{2}$, we directly call \cref{thm:counting-upper-1} with error bound $\delta$ and return the result; otherwise, we flip all input bits, call \cref{thm:counting-upper-1} with error bound $\delta$, and return $n$ minus the result.

Regardless of whether we flip the input bits, the output is always correct assuming the returned result of \cref{thm:counting-upper-1} is correct. Therefore, the error probability of the algorithm is at most $\delta$.

Next, we analyze the expected running time of the algorithm. Let $E_1$ be the event that the fraction of sampled elements that are $1$ is within $n^{-0.01}$ of the fraction of all elements that are $1$. By Chernoff bound,
\[
\bP(\neg E_1) \le 2 e^{-2 \left(n^{-0.01} \right)^2 \cdot n^{0.99}} \le O\left(\frac{1}{n^{99}}\right).
\]
Let $E_2$ be the event that the returned result of \cref{lem:check-bit} for all sampled elements are correct. By union bound,
\[
\bP(\neg E_2) \le \frac{1}{n^{99}}.
\]
If $E_1$ and $E_2$ both hold, the expected number of queries of \cref{thm:counting-upper-1} can be bounded as
\[
(1+o(1)) \cdot \frac{n}{\DKL} \cdot
\begin{cases}
   \log \frac{\lVert a \rVert_1 + 1}{\delta},  & \text{if } \lVert a\rVert_1 \le n / 2 - n^{0.99},\\
   \log \frac{n + 1}{\delta},  & \text{if } n / 2 - n^{0.99} < \lVert a\rVert_1 \le n / 2 + n^{0.99},\\
   \log \frac{n - \lVert a \rVert_1 + 1}{\delta},  & \text{if } \lVert a\rVert_1 > n / 2 + n^{0.99}.
\end{cases}
\]
Regardless of which case it is, the expected number of queries is always $(1+o(1))\frac{n \log \frac{\min\{\lVert a \rVert_1, n - \lVert a \rVert_1\} + 1}{\delta}}{\DKL}$. Even if $E_1$ and $E_2$ do not both hold, the expected number of queries of \cref{thm:counting-upper-1} can be bounded as
\[
(1+o(1)) \cdot \frac{n \log \frac{n + 1}{\delta}}{\DKL}.
\]

Therefore, the overall expected number of queries can be bounded as
\begin{align*}
& n^{0.99} \cdot (1+o(1)) \cdot \frac{\log \frac{1}{n^{100}}}{\DKL} + (1+o(1))\frac{n \log \frac{\min\{\lVert a \rVert_1, n - \lVert a \rVert_1\} + 1}{\delta}}{\DKL} \\
& + \bP(\neg E_1 \vee \neg E_2) (1+o(1))\frac{n \log \frac{n + 1}{\delta}}{\DKL}\\
= & (1+o(1))\frac{n \log \frac{\min\{\lVert a \rVert_1, n - \lVert a \rVert_1\} + 1}{\delta}}{\DKL}
\end{align*}
as desired.
\end{proof}
