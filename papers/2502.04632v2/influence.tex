We prove \cref{thm:influence} in this section. Let us recall the theorem statement.

\LinearTotalInfluence*


\subsection{Three-phase problem}
Let $f$ be a Boolean function with total influence $\I(f) \ge cn$ and $x$ be uniformly chosen from $\{0,1\}^n$.
Our goal is to show that any algorithm computing $f(x)$ with error probability $\epsilon$ makes at least $c'n\log n$ queries in expectation.
(The error probability $1/3$ in  \cref{thm:influence} can be replaced with any $\epsilon > 0$ without affecting the asymptotic noisy query complexity, by repeating the algorithm constantly many times and taking the majority vote.)

Let $c_1,c_2>0$ be two absolute constants. We define a three-phase problem as follows.
\begin{enumerate}
  \item In Phase 1, the algorithm makes $m_1 = c_1 \log n$ queries to every coordinate.
  \item In Phase 2, the oracle reveals some elements to the algorithm.
  \item In Phase 3, the algorithm makes $m_2 = c_2 n$ adaptive exact queries.
\end{enumerate}
The goal of the algorithm is to determine the value of $f(x)$.

\begin{lemma} \label{lem:inf:reduction}
  If no algorithm can solve the three-phase problem with error probability $\epsilon>0$, then no algorithm can compute $f(x)$ with error probability $\epsilon$ using at most $c_1 c_2 n\log n$ noisy queries.
\end{lemma}
\begin{proof}
  Suppose there is an algorithm $\cA$ that computes $f(x)$ with error probability $\epsilon$ using at most $m_1 m_2$ noisy queries.
  We define an algorithm $\cA'$ that solves the three-phase problem with the same error probability.

  Let $x\in \{0,1\}^n$ be chosen uniformly randomly.
  Algorithm $\cA'$ receives query results in Phase 1 and 2 and enters Phase 3.
  It simulates algorithm $\cA$ on the same input $x$ as follows.
  \begin{enumerate}[label=(\arabic*)]
    \item Initially, $x_i\gets *$ for all $i\in [n]$.
    \item When $\cA$ queries coordinate $i$:
    \begin{enumerate}[label=(\alph*)]
      \item If this is the $k$-th time coordinate $i$ is queried for $k \le m_1$, return the $k$-th noisy query result on coordinate $i$ in Phase 1 to $\cA$.
      \item Suppose coordinate $i$ has been queried more than $m_1$ times. If $x_i=*$, make an exact query to coordinate $i$ in Phase 3 and let $x_i\gets$ the query result.
      Return $\BSC_p(x_i)$ to $\cA$.
    \end{enumerate}
    \item When $\cA$ returns, $\cA'$ returns the same result.
  \end{enumerate}

  Because $\cA$ has error probability $\epsilon$, $\cA'$ also has error probability $\epsilon$.
  It suffices to prove that in Phase 3, $\cA'$ makes at most $m_2$ exact queries.
  Note that the number of exact queries $\cA'$ makes in Phase 3 is equal to the number of coordinates $i$ to which $\cA$ makes more than $m_1$ queries.
  Because $\cA$ makes $m_1 m_2$ queries, the expected number of such coordinates is at most $m_2$.
\end{proof}

By \cref{lem:inf:reduction}, to prove \cref{thm:influence}, it suffices to prove hardness of the three-phase problem.

\begin{proposition} \label{prop:inf:three-phase-hard}
  For some $c_1,c_2,\epsilon>0$, no algorithm can solve the three-phase problem with error probability $\epsilon$, where the input is uniformly chosen from $\{0,1\}^n$.
\end{proposition}
\cref{thm:influence} follows by combining \cref{lem:inf:reduction} and \cref{prop:inf:three-phase-hard}. The rest of the section is devoted to the proof of \cref{prop:inf:three-phase-hard}.

\subsection{Phase 1} \label{sec:inf:phase-1}
In Phase 1, the algorithm makes $m_1 = c_1 \log n$ queries to every element $i\in [n]$.
Let $A = \{i\in [n]: x_i=1\}$, where $x$ is the input bit string.
Let $a_i$ denote the number of times where a query to $i$ returns $1$. Then for $i\in A$, $a_i\sim \Bin(m_1,1-p)$; for $i\not \in A$, $a_i \sim \Bin(m_1,p)$. For $0\le j\le m_1$, define
\begin{align}
  p_j = \bP(\Bin(m_1,1-p)=j) = \binom{m_1}j (1-p)^j p^{m_1-j}.
\end{align}
Let $I = \left[ p m_1 - m_1^{0.6}, p m_1 + m_1^{0.6} \right]$.
\begin{lemma} \label{lem:inf:binom-concentrate}
  Let $x\sim \Bin(m_1, 1-p)$, $y\sim \Bin(m_1, p)$.
  Then
  \begin{align}
    \label{eqn:lem-inf-binom-concentrate:i} \bP(x\in I) &= n^{-c_3\pm o(1)}, \\
    \label{eqn:lem-inf-binom-concentrate:ii} \bP(y\in I) &= 1-o(1),
  \end{align}
  where $c_3 = c_1(1-2p)\log\frac{1-p}p$.
\end{lemma}
\begin{proof}
  By \cref{lem:binomial-large-deviation}, for $k\in I$, we have
  \begin{align*}
    \bP(x=k) = \exp(-(\DKL\pm o(1)) m_1) = n^{-c_1 \DKL \pm o(1)}.
  \end{align*}
  So
  \begin{align*}
    \bP(x\in I) = \sum_{k\in I} \bP(x=k) = n^{-c_1 \DKL \pm o(1)}.
  \end{align*}
  This proves \cref{eqn:lem-inf-binom-concentrate:i}.

  \cref{eqn:lem-inf-binom-concentrate:ii} follows from the Chernoff bound.
\end{proof}

Let $\bP^{(0)} = \Ber(1/2)^{\otimes n}$ denote the prior distribution of $A$ and $\bP^{(1)}$ denote the posterior distribution of $A$ conditioned on observations in Phase 1. Then for any set $B\subseteq [n]$ we have
\begin{align*}
  \bP^{(1)}(B) &\propto \bP\left((a_i)_{i\in [n]} | B\right) \bP^{(0)}(B) \\
  &= \left(\prod_{i\in B} p_{a_i}\right) \left(\prod_{i\in [n] \setminus B} p_{m_1-a_i}\right) \bP^{(0)}(B) \\
  &\propto \left(\prod_{i\in B} p_{a_i}\right) \left(\prod_{i\in [n] \setminus B} p_{m_1-a_i}\right).
\end{align*}

\subsection{Phase 2} \label{sec:inf:phase-2}
In Phase 2, the oracle reveals some elements in $A$ and not in $A$ as follows.
\begin{enumerate}[label=2\alph*.]
  \item In Step 2a, the oracle reveals elements $i$ with $a_i\not \in I$.
  \item In Step 2b, the oracle reveals every $i\in A$ independently with probability $q_{a_i}$, for some constants $(q_k)_{k\in I}$ to be chosen later.
\end{enumerate}

\subsubsection{Step 2a} \label{sec:inf:phase-2:step-a}
Let $S^{(2a)}_+$ (resp.~$S^{(2a)}_-$) denote the set of elements in $A$ (resp.~in $A^c$) revealed in Step 2a.
For $B\subseteq [n]$, we say $B$ is consistent with the observations $\left(S^{(2a)}_+, S^{(2a)}_-\right)$ if $S^{(2a)}_+ \subseteq B$ and $S^{(2a)}_-\cap B = \emptyset$.
Let $\bP^{(2a)}$ denote the posterior distribution at the end of Step 2a, and $\cC^{(2a)}$ be its support.
Then $B\subseteq [n]$ is in $\cC^{(2a)}$ if and only if it is consistent with $\left(S^{(2a)}_+, S^{(2a)}_-\right)$.
For $B\in \cC^{(2a)}$, the posterior probability $\bP^{(2a)}(B)$ is given by
\begin{align} \label{eqn:inf-phase-2-step-a:post}
  \bP^{(2a)}(B) \propto \bP^{(1)}(B)
  \propto \left(\prod_{i\in B} p_{a_i}\right) \left(\prod_{i\in B^c}p_{m_1 - a_i}\right).
\end{align}

\subsubsection{Step 2b} \label{sec:inf:phase-2:step-b}
Let $S^{(2b)}_+$ be the set of elements of $A$ revealed in Step 2b that were not revealed in Step 2a.
Define $S^{(\le 2b)}_+ = S^{(2a)}_+ \cup S^{(2b)}_+$.
Define $\bP^{(2b)}$, $\cC^{(2b)}$ similarly to \cref{sec:inf:phase-2:step-a}.
Then $\cC^{(2b)}$ is the set $B\in \cC^{(2a)}$ that are consistent with $\left(S^{(2b)}_+,\emptyset\right)$.
For any $B\in \cC^{(2b)}$, we have
\begin{align} \label{eqn:inf-phase-2-step-b:post-step}
  \bP^{(2b)}(B) &\propto \bP\left(S^{(2b)}_+ | B, S^{(2a)}_+\right) \bP^{(2a)}(B) \\
  \nonumber &\propto \left(\prod_{i\in B\backslash S^{(\le 2b)}_+} (1-q_{a_i})\right) \bP^{(2a)}(B) \\
  \nonumber &\propto \left(\prod_{i\in B\backslash S^{(\le 2b)}_+} (1-q_{a_i}) p_{a_i}\right)\left(\prod_{i\in B^c\backslash S^{(2a)}_-}p_{m_1 - a_i}\right) \\
  \nonumber &\propto \left(\prod_{i\in B\backslash S^{(\le 2b)}_+} \frac{(1-q_{a_i}) p_{a_i}}{p_{m_1 - a_i}}\right).
\end{align}
Note that for any $B\in \cC^{(2b)}$ and any $i\in B\backslash S^{(\le 2b)}_+$, we have $a_i \in I$.

Let us now choose the values of $q_k$ for $k\in I$.
For $k\in I$, define
\begin{align*}
  q_k = 1-\frac{p_{m_1-k} p_{k_l}}{p_k p_{m_1-k_l}}
\end{align*}
where $k_l=p m_1 - \log^{0.6}n$ is the left endpoint of $I$.
Because $\frac{p_{m_1-k}}{p_k} = \left(\frac{1-p}p\right)^{m_1-2k}$ is decreasing in $k$, we have $q_k\in [0, 1]$ for $k\in I$.
So this choice of $q_k$'s is valid.

With this choice of $q_k$, we can simplify \cref{eqn:inf-phase-2-step-b:post-step} as
\begin{align} \label{eqn:inf-phase-2-step-b:post}
  \bP^{(2b)}(B) \propto \left(\frac{p_{k_l}}{p_{m_1-k_l}}\right)^{|B|}.
\end{align}
\cref{eqn:inf-phase-2-step-b:post} is very useful and greatly simplifies the posterior distribution.
Importantly, $(a_i)_{i\in [n]}$ does not appear directly in the expression.
Therefore, $\left(S^{(\le 2b)}_+, S^{(2a)}_-, |A|\right)$ is a sufficient statistic for $A$ at the end of Step 2b.

Let us now consider the distribution of $\left(S^{(\le 2b)}_+, S^{(2a)}_-\right)$ conditioned on $A$.

Let $p_+$ be the probability that a coordinate $i\in A$ is in $S^{(\le 2b)}_+$.
Every coordinate $i\in A$ is independently in $S^{(2a)}_+$ with probability $\bP(\Bin(m_1,1-p)\not \in I) = 1-n^{-c_3\pm o(1)}$ (\cref{lem:inf:binom-concentrate}).
Every coordinate $i\in A$ is independently in $S^{(2b)}_+$ with probability
\begin{align*}
  \sum_{k\in I} p_k q_k = \sum_{k\in I} p_k \left(1-\frac{p_{m_1-k} p_{k_l}}{p_k p_{m_1-k_l}}\right).
\end{align*}
For a fixed $i\in A$, the events $i\in S^{(2a)}_+$ and $i\in S^{(2b)}_+$ are disjoint, so
\begin{align*}
  p_+ = \bP(\Bin(m_1,1-p)\not \in I) + \sum_{k\in I} p_k q_k \ge \bP(\Bin(m_1,1-p)\not \in I) =  1-n^{-c_3\pm o(1)}.
\end{align*}
On the other hand,
\begin{align*}
  p_+ = 1 - \sum_{k\in I} p_k (1 - q_k) \le 1 - p_{k_l} (1 - q_{k_l}) = 1 - p_{k_l} = 1-n^{-c_3\pm o(1)},
\end{align*}
where the last step is by \cref{lem:binomial-large-deviation}.
Thus,
\begin{align} \label{eqn:inf-p-plus}
    p_+ = 1-n^{-c_3\pm o(1)}.
\end{align}

Let $p_-$ be the probability that a coordinate $i\in A^c$ is in $S^{(2a)}_-$.
By \cref{lem:inf:binom-concentrate},
\begin{align} \label{eqn:inf-p-minus}
  p_- = \bP(\Bin(m_1,p)\not \in I) = o(1).
\end{align}

Therefore, observations up to Step 2b have the same effect as the following procedure:
\begin{definition}[Alternative observation procedure] \label{defn:inf:alt-obs}
  Let $A\sim \Ber(1/2)^{\otimes n}$.
  \begin{enumerate}[label=(\arabic*)]
    \item Observe every coordinate $i\in A$ independently with probability $p_+$ (\cref{eqn:inf-p-plus}).
    \item Observe every coordinate $i\in A^c$ independently with probability $p_-$ (\cref{eqn:inf-p-minus}).
  \end{enumerate}
\end{definition}

By \cref{eqn:inf-phase-2-step-b:post}, the posterior distribution of $A$ after Phase 2 is a biased product distribution on the unrevealed coordinates.

\subsection{Phase 3}
In Phase 3, the algorithm makes at most $c_2 n$ adaptive exact queries.
We will show that for $c_2$ small enough, no algorithm is able to determine $f(x)$ with very small error probability.

Our proof strategy is as follows.
\begin{enumerate}[label=3\alph*.]
  \item Because $f$ has linear total influence, after Phase 2 ends, the Boolean function on the unrevealed coordinates will have a biased version of total influence $\I_q$ at least $\Omega(n)$ in expectation.
  \item Every (adaptive) query made in Phase 3 decreases $\I_q$ by at most $1$. Therefore, after Phase 3, the Boolean function on the remaining unrevealed coordinates has $\I_q=\Omega(n)$ in expectation.
  \item Finally, we show that if a Boolean function has $\I_q=\Omega(n)$, then it is impossible to guess $f(x)$ (where $x$ follows a biased product distribution) with very small error probability.
\end{enumerate}

Because observations up to Phase 2 have been simplified by our analysis, we make some definitions and restate the problem we need to solve in Phase 3.

\subsubsection{Preliminaries}
We have a Boolean function $f: \{0,1\}^n \to \{0,1\}$ and an input $x\sim \Ber(1/2)^{\otimes n}$, and the goal is to determine $f(x)$.
Then we independently observe each $i\in [n]$ with probability is $p_+=1-n^{-c_3 \pm o(1)}$ for $x_i=1$ and $p_-=o(1)$ for $x_i=0$.
Let $s\in \{0,1,*\}^n$ be the observations. That is, if coordinate $i$ is revealed, then $s_i$ is the revealed value; otherwise $s_i=*$.
Let $U = \{i\in [n]: s_i=*\}$ be the unrevealed coordinates.
Conditioned on $s$, the distribution of $x_U$ is a product of $\Ber(q)$, where
\begin{align} \label{eqn:inf:phase-3:q}
  q=\frac{1-p_+}{1-p_+ + 1-p_-} = n^{-c_3 \pm o(1)}.
\end{align}
Let $f_s: \{0,1\}^U\to \{0,1\}$ be the function $f_s(x_U) = f(s\lhd x_U)$ for all $x_U \in \{0,1\}^U$, where $s\lhd x_U$ denotes the bit string where all $*$ in $s$ are replaced with the corresponding value in $x_U$.
Let $\rho$ be the distribution $\left(\frac{p_-}2, \frac{p_+}2, 1-\frac{p_-}2-\frac{p_+}2\right)$ on $\{0,1,*\}$.
Then without conditioning on $x$, $s$ has distribution $\rho^{\otimes n}$.

For any Boolean function $g: \{0,1\}^S\to \{0,1\}$, define the $q$-biased influence of coordinate $i\in S$ as
\begin{align*}
  \Inf_{q,i}(f) = \bP_{x\sim \Ber(q)^{\otimes S}}( f(x) \ne f(x\oplus e_i))
\end{align*}
and the $q$-biased total influence as
\begin{align*}
  \I_q(f) = \sum_{i\in S} \Inf_{q,i}(f).
\end{align*}
When we mention the $q$-biased total influence of the function $f_s$, the sum is over the unrevealed coordinates $i\in U$.

For a string $y\in \{0,1\}^S$, let $D_y$ denote the distribution of $t\in \{0,1,*\}^S$ where for $i\in S$ with $y_i=1$, $t_i=1$ with probability $p_+$ and $t_i=*$ with probability $1-p_+$; for $i\in S$ with $y_i=0$, $t_i=0$ with probability $p_-$ and $t_i=*$ with probability $1-p_-$.
For a string $t\in \{0,1,*\}^S$, let $E_s$ denote the distribution of $y\in \{0,1\}^S$ where $y_i=t_i$ if $t_i\in \{0,1\}$ and $y_i\sim \Ber(q)$ independently for $i\in S$ with $t_i=*$, where $q$ is defined in \cref{eqn:inf:phase-3:q}.
With these definitions, we have $x\sim \Ber(1/2)^{\otimes n}$, $s\sim \rho^{\otimes n}$, $x\sim E_s$ conditioned on $s$, $s\sim D_x$ conditioned on $x$.

\subsubsection{Step 3a}
We connect the total influence $\I(f)$ with the biased total influence $\I_q(f_s)$.

Fix $i\in [n]$, we have (in the following, $x \cup 0_i$ and $x \cup 1_i$ denotes setting the $i$-th bit of $x$ as $0$ and $1$, respectively)
\begin{align*}
  \Inf_i(f) &= \bP_{x\sim \Ber(1/2)^{\otimes ([n]\backslash i)}} ( f(x \cup 0_i) \ne f(x \cup 1_i)) \\
  &= \bP_{\substack{x\sim \Ber(1/2)^{\otimes ([n]\backslash i)}\\ s\sim D_x }} ( f(x \cup 0_i) \ne f(x \cup 1_i)) \\
  &= \bP_{\substack{s\sim \rho^{\otimes ([n]\backslash i)}\\ x\sim E_s }} ( f(x \cup 0_i) \ne f(x \cup 1_i)) \\
  &= \frac{1}{\bP_{s_i\sim \rho}(s_i=*)} \cdot \bP_{\substack{s\sim \rho^{\otimes [n]}\\ x\sim E_{s_{[n]\backslash i}} }} ( f(x \cup 0_i) \ne f(x \cup 1_i) \land s_i=*) \\
  &= \frac{1}{\bP_{s_i\sim \rho}(s_i=*)} \cdot \bP_{\substack{s\sim \rho^{\otimes [n]}\\ x\sim E_s }} ( f(x) \ne f(x \oplus e_i) \land s_i=*)\\
  &= \frac 1{\rho(*)} \cdot \bE_{s\sim \rho^{\otimes [n]}} \left[\mathbbm{1}_{s_i=*} \cdot \bP_{x\sim E_s} (f(x)\ne f(x \oplus e_i)) \right].
\end{align*}
The first step is by definition of $\Inf_i$.
The second step is because $s$ is not involved in the condition.
The third step is by considering the joint distribution between $x$ and $s$.
The fourth step is because the two conditions $f(x \cup 0_i) \ne f(x \cup 1_i)$ and $s_i=*$ are independent: the former depends on $s_{[n]\backslash i}$ and the latter depends on $s_i$.
The fifth step rewrites the condition. %
The sixth step changes the order of checking $s_i=*$ and choosing $x\sim E_s$.

Summing over $i\in [n]$, we have
\begin{align*}
  \rho(*) \I(f) &= \sum_{i\in [n]} \bE_{s\sim \rho^{\otimes [n]}} \left[\mathbbm{1}_{s_i=*} \cdot \bP_{x\sim E_s} (f(x)\ne f(x \oplus e_i)) \right] \\
  &= \bE_{s\sim \rho^{\otimes [n]}} \left[\sum_{i\in [n]: s_i=*} \bP_{x\sim E_s} (f(x)\ne f(x \oplus e_i)) \right] \\
  &= \bE_{s\sim \rho^{\otimes [n]}} \I_q(f_s).
\end{align*}
The second step is by linearity of expectation.
The third step is by definition of $\I_q$.

Note that $\rho(*) = 1-\frac{p_-}2-\frac{p_+}2 = \frac12 \pm o(1)$.
Because $\I(f) \ge c n$, we have
\begin{align} \label{eqn:inf:phase-3:step-1:q-inf}
  \bE_{s\sim \rho^{\otimes [n]}} \I_q(f_s) \ge (2c\pm o(1)) n.
\end{align}
That is, $f_s$ has expected $q$-biased total influence at least $(2c\pm o(1)) n$.

\subsubsection{Step 3b}
We prove the following lemma, which essentially says that adaptive exact queries in Phase 3 can only decrease the $q$-biased total influence $\I_q$ by a certain amount. In the following, recall the definition of $f_t$ for a Boolean function $f: \{0,1\}^n\to \{0,1\}$ and $t\in \{0,1,*\}^n$ is $f$ where we restrict all input coordinates $i$ with $t_i \ne *$ to be equal to $t_i$.

\begin{lemma} \label{lem:inf:phase-3:step-2}
  Let $f: \{0,1\}^n\to \{0,1\}$ be a Boolean function.
  Suppose the input $x$ follows distribution $\Ber(q)^{\otimes n}$.
  Consider an algorithm which adaptively makes at most $m$ exact queries in expectation.
  Let $t\in \{0,1,*\}^n$ be the random variable denoting the query results.
  Then $\bE[\I_q(f_t)] \ge \I_q(f)-m$, where $\bE$ is over the randomness of the revealed coordinates and the randomness of the algorithm.
\end{lemma}
\begin{proof}
  By induction it suffices to prove the case where the algorithm makes exactly one query.
  Without loss of generality, assume that the algorithm makes an query to coordinate $1$.
  Then $t=1*^{n-1}$ with probability $q$ and $t=0*^{n-1}$ with probability $1-q$.
  \begin{align*}
    \bE[\I_q(f_t)] &= q \I_q(f_{1*^{n-1}}) + (1-q) \I_q(f_{0*^{n-1}}) \\
    &= q \sum_{2\le i\le n} \Inf_{q,i}(f_{1*^{n-1}}) +(1-q) \sum_{2\le i\le n} \Inf_{q,i}(f_{0*^{n-1}}) \\
    &= \sum_{2\le i\le n} \Inf_{q,i}(f) \\
    &= \I_q(f) - \Inf_{q,1}(f) \\
    &\ge \I_q(f)-1.
  \end{align*}
  The first step is by expanding the expectation.
  The second step is by definition of $\I_q$.
  The third step is because $q \Inf_{q,i}(f_{1*^{n-1}}) + (1-q) \Inf_{q,i}(f_{0*^{n-1}}) = \Inf_{q,i}(f)$.
  The fourth step is by definition of $\I_q$.
  The fifth step is because $\Inf_{q,i}(f) \le 1$.
\end{proof}
We now apply \cref{lem:inf:phase-3:step-2} to Phase 3. Let $t\in \{0,1,*\}^U$ be the observations made in Phase 3, where $U = \{i\in [n]: s_i=*\}$ is the set of unrevealed coordinates at the end of Phase 2.
Then \cref{eqn:inf:phase-3:step-1:q-inf} together with \cref{lem:inf:phase-3:step-2} implies that
\begin{align} \label{eqn:inf:phase-3:step-2:q-inf}
  \bE_{s\sim \rho^{\otimes [n]}} [ \bE_t [\I_q((f_{s})_{t})] ]\ge (2c - c_2\pm o(1)) n.
\end{align}

\subsubsection{Step 3c}
After Phase 3, the Boolean function on the unrevealed coordinates has $q$-biased total influence at least $(2c-c_2\pm o(1))n = \Omega(n)$ in expectation.
In particular, with probability $\ge c-c_2/2\pm o(1)$, the function has $I_q((f_{s})_t) \ge (c-c_2/2\pm o(1))n$.
Any algorithm now needs to output an answer in $\{0,1\}$.
The following result shows that the error probability will be $\Omega(1)$ no matter what the algorithm outputs.

\begin{lemma} \label{lem:inf:phase-3:step-3}
  For any $0 < c \le 1$, there exists $c'>0$ such that the following holds.
  Let $f: \{0,1\}^n\to \{0,1\}$ be a Boolean function and $\frac 1{3n} \le q \le c/6$ be a parameter.
  If $\I_q(f) \ge cn$, then \[c' \le \bE_{x \sim \Ber(q)^{\otimes n}} [f(x)] \le 1 - c'.\]
\end{lemma}
\begin{proof}
    Since $\I_q(f) \ge cn$ and $\Inf_{q,i}(f) \le 1$ for every $i$, there exist at least $cn/2$ indices $i\in [n]$ with $\Inf_{q,i}(f) \ge c / 2$. As $\lfloor c / 6q\rfloor \le cn/2$, there are $m = \lfloor c / 6q\rfloor$ indices $i\in [n]$ with $\Inf_{q,i}(f) \ge c / 2$. Without loss of generality, assume that $\Inf_{q,i}(f) \ge c / 2$ for $i\in [m]$.

    Let $0^m$ denote the length-$m$ bit string with all $0$'s. By union bound, we have
    \begin{align}
    \label{lem:inf:phase-3:step-3:all-0}
    \begin{split}
    \bP_{x \sim \Ber(q)^{\otimes n}}\left(x_{[m]}=0^m\right) \ge 1 - \sum_{i\in [m]} \bP_{x \sim \Ber(q)^{\otimes n}}\left(x_i = 1\right) = 1 - mq \ge 1 - c / 6.
    \end{split}
    \end{align}

    Let $\diamond$ denote the concatenation operation of two bit strings. For any $i \in [m]$, we have
    \begin{align}
    \nonumber &~\bP_{y \sim \Ber(q)^{\otimes ([n]\backslash [m])}}\left(f(0^{m} \diamond y) \ne f(e_i \diamond y)\right) \\
    \nonumber =&~\bP_{x \sim \Ber(q)^{\otimes n}}\left(f(x) \ne f(x \oplus e_i) \mid x_{[m]}=0^m\right)\\
    \nonumber \ge&~\bP_{x \sim \Ber(q)^{\otimes n}}\left(f(x) \ne f(x \oplus e_i) \wedge x_{[m]}=0^m\right)\\
    \nonumber \ge &~\bP_{x \sim \Ber(q)^{\otimes n}}\left(f(x) \ne f(x \oplus e_i)\right) + \bP_{x \sim \Ber(q)^{\otimes n}}\left(x_{[m]}=0^m\right) - 1 \\
    \nonumber \ge & \Inf_{q, i}(f) +(1 - c/6) - 1 \tag{By \cref{lem:inf:phase-3:step-3:all-0}}\\
    \ge &~c / 3. \label{eq:lem:inf:phase-3:step-3:eq2}
    \end{align}

    Finally, we have
    \begin{align*}
    &\bE_{x \sim \Ber(q)^{\otimes n}}(f(x))\\
    &\ge \bP_{x \sim \Ber(q)^{\otimes n}}(f(x) = 1 \wedge x_1 + \cdots + x_m = 1)\\
    &= \sum_{i\in [m]} \bP_{x \sim \Ber(q)^{\otimes n}}(f(x) = 1 \wedge x_{[m]} = e_i)\\
    &= q (1-q)^{m-1} \cdot \sum_{i\in [m]} \bP_{y \sim \Ber(q)^{\otimes ([n]\backslash [m])}}(f(e_i \diamond y) = 1)\\
    &\ge  \Theta_c(q) \cdot \sum_{i\in [m]} \bP_{y \sim \Ber(q)^{\otimes ([n]\backslash [m])}}(f(0^m \diamond y) = 0 \wedge f(0^m \diamond y) \ne f(e_i \diamond y))\\
    &\ge  \Theta_c(q) \cdot \sum_{i\in [m]} \left( \bP_{y \sim \Ber(q)^{\otimes ([n]\backslash [m])}}(f(0^m \diamond y) = 0) +  \bP_{y \sim \Ber(q)^{\otimes ([n]\backslash [m])}} (f(0^m \diamond y) \ne f(e_i \diamond y)) - 1\right)\\
    &\ge  \Theta_c(1) \cdot \left( \bP_{y \sim \Ber(q)^{\otimes ([n]\backslash [m])}}(f(0^m \diamond y) = 0) +  c / 3 - 1\right) \tag{By \cref{eq:lem:inf:phase-3:step-3:eq2}}\\
    &\ge  \Theta_c(1) \cdot \left( \bP_{x \sim \Ber(q)^{\otimes n}}(f(x) = 0 \wedge x_{[m]}=0^m) +  c / 3 - 1\right)\\
    &\ge  \Theta_c(1) \cdot \left( \bP_{x \sim \Ber(q)^{\otimes n}}(f(x) = 0) + (1 - c / 6) - 1 +  c / 3 - 1\right) \tag{By \cref{lem:inf:phase-3:step-3:all-0}}\\
    &= \Theta_c(1) \cdot (c/6 -  \bE_{x \sim \Ber(q)^{\otimes n}}(f(x))).
    \end{align*}
    Therefore, there exists $c'$ depending only on $c$ such that $\bE_{x \sim \Ber(q)^{\otimes n}}[f(x)] \ge c'$.

    By symmetry, we also have $\bE_{x \sim \Ber(q)^{\otimes n}}(f(x)) \le 1 - c'$.
\end{proof}

Applying \cref{lem:inf:phase-3:step-3} to the Boolean function $(f_s)_t$ on the unrevealed coordinates finishes the proof.
