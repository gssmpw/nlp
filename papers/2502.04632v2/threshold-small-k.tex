In this section we provide a simpler proof of \cref{thm:th:small-k}. Let us recall the theorem statement.

\ThresholdSmallK*

Our proof uses the three-phase framework which we also used in the proof of \cref{thm:graph-conn-hard}. It is a refinement of \cite{feige1994computing}'s two-phase framework, which were used to prove that computing $\Threshold{n}{k}$ requires $\Omega\left(n \log \frac k\delta\right)$ noisy queries.
Here we add a phase where the oracle can send extra information to the algorithm, allowing for a more precise analysis obtaining the exact constant.

\paragraph{Comparison with \cite{wang2024noisy}'s proof.}
Let us briefly compare our proof with \cite{wang2024noisy}'s proof. Both proofs are based on \cite{feige1994computing}'s two-phase framework.
\cite{wang2024noisy}'s proof is divided into three cases: $\log(1/\delta)\log\log(1/\delta) < \log k \le \frac{\log(1/\delta)}{\log\log(1/\delta)}$, $\log k > \log(1/\delta)\log\log(1/\delta)$, $\log k \le \frac{\log(1/\delta)}{\log\log(1/\delta)}$.
The first two cases are handled using the two-phase framework, and the last case is proved using Le Cam's two point method.
In comparison, our proof is simpler and handles all $k=o(n)$ and $\delta = o(1)$ uniformly. We achieve this simplification by carefully designing the information that the oracle reveals to the algorithm for free.

\subsection{Three-phase problem} \label{sec:th-small:three-phase}
Let us now describe the three-phase problem. Let $\epsilon_1,\epsilon_2>0$ be two absolute constants (i.e., they do not grow with $n$).
\begin{enumerate}
  \item In Phase 1, the algorithm makes $m_1 = (1-\epsilon_1) \frac{\log \frac k\delta}{\DKL}$ queries to every element.
  \item In Phase 2, the oracle reveals some elements to the algorithm.
  \item In Phase 3, the algorithm makes $m_2 = (1-\epsilon_2) n$ adaptive exact queries.
\end{enumerate}
The goal of the algorithm is to distinguish whether there are at least $k+1$ ones among the elements.
\begin{lemma} \label{lem:th:reduction}
  If no algorithm can solve the three-phase problem with error probability $\delta>0$, then no algorithm can solve $\Threshold{n}{k}$ with error probability $\delta$ using at most $(1-\epsilon_1)(1-\epsilon_2) \frac{n \log \frac k\delta}{\DKL}$ noisy queries.
\end{lemma}
The proof is similar to \cref{lem:inf:reduction} and omitted.


By \cref{lem:th:reduction}, to prove \cref{thm:th:small-k}, it suffices to prove hardness of the three-phase problem.
\begin{proposition} \label{prop:th:three-phase-hard}
  For any absolute constants $\epsilon_1,\epsilon_2>0$, no algorithm can solve the three-phase problem for $\Threshold{n}{k}$ with error probability $\delta$ for $k=o(n)$ and $\delta=o(1)$, where the input is uniformly chosen from $\binom{[n]}k$ with probability $1/2$ and uniformly chosen from $\binom{[n]}{k-1}$ with probability $1/2$.
\end{proposition}
\cref{thm:th:small-k} follows by combining \cref{lem:th:reduction} and \cref{prop:th:three-phase-hard}.

The rest of the section is devoted to the proof of \cref{prop:th:three-phase-hard}.

\subsection{Phase 1} \label{sec:th-small:phase-1}
Define $A = \{i\in [n]: a_i = 1\}$ where $a$ is the input bit string.
Then $A \in \binom{[n]}{k} \cup \binom{[n]}{k-1}$, and the goal is to distinguish whether $|A|=k$ or $k-1$.

In Phase 1, the algorithm makes $m_1 = (1-\epsilon_1) \frac{\log \frac k\delta}{\DKL}$ queries to every element $i\in [n]$.
Let $a_i$ denote the number of times where a query to $i$ returns $1$.
Then for $i\in A$, $a_i \sim \Bin(m_1, 1-p)$; for $i\not \in A$, $a_i\sim \Bin(m_1, p)$.
For $0\le j\le m_1$, define
\begin{align*}
  p_j = \bP(\Bin(m_1,1-p)=j) = \binom{m_1}j (1-p)^j p^{m_1-j}.
\end{align*}
Let $I = \left[p m_1 - m_1^{0.6}, p m_1 + m_1^{0.6}\right]$.
\begin{lemma} \label{lem:th:binom-concentrate}
  Let $x\sim \Bin(m_1, 1-p)$, $y\sim \Bin(m_1, p)$.
  Then
  \begin{align}
    \label{eqn:lem-th-binom-concentrate:i} \bP(x\in I) &= (\delta/k)^{1-\epsilon_1 \pm o(1)}, \\
    \label{eqn:lem-th-binom-concentrate:ii} \bP(y\in I) &= 1-o(1).
  \end{align}
\end{lemma}
\begin{proof}
  \cref{eqn:lem-th-binom-concentrate:i} is by \cref{lem:binomial-large-deviation}.
  \cref{eqn:lem-th-binom-concentrate:ii} is by Chernoff bound.
\end{proof}

Let $\bP^{(0)}$ denote the prior distribution of $A$ and $\bP^{(1)}$ denote the posterior distribution of $A$ conditioned on observations in Phase 1.
Let $\cC^{(0)}$ (resp.~$\cC^{(1)}$) denote the support of $\bP^{(0)}$ (resp.~$\bP^{(1)}$).
Then $\cC^{(1)} = \cC^{(0)} = \binom{[n]}{k-1} \cup \binom{[n]}k$ and for any set $B \in \cC^{(0)}$ we have
\begin{align*}
  \bP^{(1)}(B) &\propto \bP\left((a_i)_{i\in [n]} | B\right) \bP^{(0)}(B) \\
  \nonumber &= \left(\prod_{i\in B} p_{a_i}\right) \left(\prod_{i\in B^c} p_{m_1-a_i}\right) \bP^{(0)}(B).
\end{align*}

\subsection{Phase 2} \label{sec:th-small:phase-2}
In Phase 2, the oracle reveals some elements in $A$ and not in $A$ as follows.
\begin{enumerate}[label=2\alph*.]
  \item In Step 2a, the oracle reveals elements $i$ with $a_i\not \in I$.
  \item In Step 2b, the oracle reveals every $i\in A$ independently with probability $q_{a_i}$. We choose $q_j = 1-\frac{p_{m_1-j} p_{j_l}}{p_j p_{m_1-j_l}}$ for $j\in I$ where $j_l = p m_1 - m_1^{0.6}$.
  \item In Step 2c, the oracle reveals $k-1$ elements of $A$ as follows. If $|A|=k-1$, reveal all elements of $A$. Otherwise, $|A|=k$. If $A$ contains an element that is not revealed yet, uniformly randomly choose an element $i^*$ from all such elements and reveal all elements in $A\backslash i^*$. If all elements of $A$ have been revealed, report failure.
\end{enumerate}

\paragraph{Step 2a and Step 2b.}
By the same analysis as in \cref{sec:inf:phase-2:step-a,sec:inf:phase-2:step-b}, observations up to Step 2b have the same effect as the following procedure:
\begin{enumerate}[label=(\arabic*)]
  \item Observe every element $i\in A$ independently with probability
  \begin{align*}
    p_+ &= 1-\sum_{j\in I} p_j (1-q_j)
    = 1 - \frac{p_{j_l}}{p_{m_1-j_l}} \cdot \sum_{j\in I} p_{m_1-j} \\
    \nonumber &= 1-(1\pm o(1)) \frac{p_{j_l}}{p_{m_1-j_l}} = 1-(\delta/k)^{1-\epsilon_1\pm o(1)}.
  \end{align*}
  \item Observe every element $i\in A^c$ independently with probability
  \begin{align*}
    p_- = \bP(\Bin(m_1,p)\not \in I) = o(1).
  \end{align*}
\end{enumerate}

Let $A_+^{(2a)}$ (resp.~$A_-^{(2a)}$) denote the set of elements in $I$ (resp.~not in $I$) revealed in Step 2a.
Let $A_+^{(2b)}$ be the set of elements in $I$ revealed in Step 2b, and $A_+^{(\le 2b)} = A_+^{(2a)} \cup A_+^{(2b)}$.
Then the oracle reports failure in Step 2c if and only if $\left|A_+^{(\le 2b)}\right| = k$.

Let $\bP^{(2b)}$ denote the posterior distribution of $A$ after Step 2b and $\cC^{(2b)}$ be its support.
By the same analysis as in \cref{sec:inf:phase-2:step-a,sec:inf:phase-2:step-b}, the posterior probability after Step 2b satisfies
\begin{align*}
  \bP^{(2b)}(B) \propto \left(\frac{p_{j_l}}{p_{m_1-j_l}}\right)^{|B|-(k-1)} \bP^{(0)}(B)
\end{align*}
for $B\in \cC^{(2b)}$.
In particular, $\left( A_+^{(\le 2b)}, A_-^{(2a)}, |A|\right)$ is a sufficient statistic for $A$ at the end of Step 2b.

We note that $\left|A_+^{(\le 2b)}\right| < k$ with probability $\ge \delta^{1-\epsilon_1\pm o(1)}$ because
\begin{align} \label{eqn:th:step-2b:zero-prob}
  \bP\left( \Bin(k, 1-p_+) > 0 \right) = 1-p_+^k
  \ge 1-\exp\left( k(1-p_+)\right) \ge \delta^{1-\epsilon_1\pm o(1)}.
\end{align}



\paragraph{Step 2c.}
Let $A_+^{(2c)}$ be the set of elements in $I$ revealed in Step 2c but not in previous steps, and $A_+^{(\le 2c)} = A_+^{(\le 2b)} \cup A_+^{(2c)}$.
Let $\bP^{(2c)}$ be the posterior distribution of $A$ and $\cC^{(2c)}$ be the support of $\bP^{(2c)}$.
Then
\begin{align*}
  \cC^{(2c)}&=\left\{ A : A\in \binom{[n]}{k-1} \cup \binom{[n]}k, A_+^{(\le 2c)} \subseteq A \subseteq [n]\backslash A_-^{(2a)}\right\} \\
  \nonumber &= \left\{A_+^{(\le 2c)}\right\} \cup \left\{ A_+^{(\le 2c)} \cup \{i\}: i\in [n]\backslash \left(A_+^{(\le 2c)} \cup A_-^{(2a)}\right)\right\}.
\end{align*}
For simplicity of notation, define $A_0 = A_+^{(\le 2c)}$ and $A_i = A_+^{(\le 2c)} \cup \{i\}$ for $i\in [n]\backslash \left(A_+^{(\le 2c)} \cup A_-^{(2a)}\right)$.
Then
\begin{align*}
  \bP^{(2c)}(A_0) &\propto \bP^{(0)}(A_0),\\
  \bP^{(2c)}(A_i) &\propto \frac{1}{k-\left| A_+^{(\le 2b)} \right|} \cdot \frac{p_{j_l}}{p_{m_1-j_l}} \cdot \bP^{(0)}(A_i).
\end{align*}
Recall that $\bP^{(0)}(B) = \frac 12 \cdot \frac 1{\binom {n}{|B|}}$ for $|B|\in \{k-1,k\}$.
Let $\bP^{(2c)}_k$ (resp.~$\bP^{(2c)}_{k-1}$) denote the probability measure of observations at the end of Step 2c conditioned on $|A|=k$ (resp.~$|A|=k-1$).
Summing over $i$, we have
\begin{align*}
  \frac{d \bP^{(2c)}_k}{d \bP^{(2c)}_{k-1}} =\left| [n]\backslash \left(A_+^{(\le 2c)} \cup A_-^{(2a)}\right) \right|\cdot \frac{1}{k-\left| A_+^{(\le 2b)} \right|} \cdot \frac{p_{j_l}}{p_{m_1-j_l}} \cdot \frac{\binom n{k-1}}{\binom nk}.
\end{align*}
\begin{lemma} \label{lem:th:step-2c}
  \begin{align*}
    \bP^{(2c)}_k \left(\frac{d \bP^{(2c)}_k}{d \bP^{(2c)}_{k-1}} \le \delta^{-\epsilon_1/2}\right) \ge \delta^{1-\epsilon_1 \pm o(1)}.
  \end{align*}
\end{lemma}
\begin{proof}
  Because $p_-=o(1)$, with probability $1-o(1)$, we have $\left|A_-^{(2a)}\right| \le n p_-^{1/2}$.
  Then
  \begin{align*}
    \left| [n]\backslash \left(A_+^{(\le 2c)} \cup A_-^{(2a)}\right) \right|
    = n-(k-1) - A_-^{(2a)} = (1\pm o(1)) n
  \end{align*}
  and
  \begin{align*}
    \frac{d \bP^{(2c)}_k}{d \bP^{(2c)}_{k-1}}
    &= (1\pm o(1)) n \cdot \frac{1}{k-\left| A_+^{(\le 2b)} \right|} \cdot \frac{p_{j_l}}{p_{m_1-j_l}} \cdot \frac kn\\
    \nonumber &= (1\pm o(1)) \frac{k}{k-\left| A_+^{(\le 2b)} \right|} \cdot \frac{p_{j_l}}{p_{m_1-j_l}}.
  \end{align*}

  Note that $k-\left| A_+^{(\le 2b)} \right|\sim \Bin(k,1-p_+)$, and $\left| A_+^{(\le 2b)} \right|$ is independent with $\left|A_-^{(2a)}\right|$ conditioned on $|A|$. Recall that
  \begin{align*}
    1-p_+ = (1\pm o(1)) \frac{p_{j_l}}{p_{m_1-j_l}} = (\delta/k)^{1-\epsilon_1 \pm o(1)}.
  \end{align*}

  If $k^{\epsilon_1} \delta^{1-\epsilon_1} \ge \delta^{-\epsilon_1/2}$, then $k (1-p_+) = \omega(1)$ and by concentration
  \begin{align*}
    \bP\left( \Bin(k,1-p_+) \ge \frac 12 k(1-p_+) \right) = 1-o(1)
  \end{align*}
  Under $\bP^{(2c)}_k$, conditioned on $k-\left| A_+^{(\le 2b)} \right| \ge \frac 12 k(1-p_+)$, we have
  \begin{align*}
    \frac{d \bP^{(2c)}_k}{d \bP^{(2c)}_{k-1}}
    = (1\pm o(1)) \frac{k}{k-\left| A_+^{(\le 2b)} \right|} \cdot \frac{p_{j_l}}{p_{m_1-j_l}}
    = O(1).
  \end{align*}

  If $k^{\epsilon_1} \delta^{1-\epsilon_1} \le \delta^{-\epsilon_1/2}$, then conditioned on $\left| A_+^{(\le 2b)} \right| < k$ (which happens with probability at least $\delta^{1-\epsilon_1\pm o(1)}$ by \cref{eqn:th:step-2b:zero-prob}), we have
  \begin{align*}
    \frac{d \bP^{(2c)}_k}{d \bP^{(2c)}_{k-1}}
    = (1\pm o(1)) \frac{k}{k-\left| A_+^{(\le 2b)} \right|} \cdot \frac{p_{j_l}}{p_{m_1-j_l}}
    \le \delta^{-\epsilon_1/2 \pm o(1)}.
  \end{align*}

  Combining both cases we finish the proof.
\end{proof}
From \cref{lem:th:step-2c} we can conclude that no algorithm can determine $|A|$ with error probability $\le \delta$ at the end of Phase 2.
Suppose for the sake of contradiction that such an algorithm exists.
Let $\cE$ denote the event that $\frac{d \bP^{(2c)}_k}{d \bP^{(2c)}_{k-1}} \le \delta^{-\epsilon_1/2}$.
Under $\bP^{(2c)}_k$, conditioned on $\cE$, the algorithm outputs ``$|A|=k$'' with probability at least $\frac 12$ (otherwise the overall error probability under $\bP^{(2c)}_k$ is at least $\frac 12 \cdot \delta^{1-\epsilon\pm o(1)}$).
However, this implies that under $\bP^{(2c)}_{k-1}$, the algorithm outputs $|A|=k$ with probability at least $\frac 12 \delta^{\epsilon_1/2}$ by definition of $\cE$.

\subsection{Phase 3} \label{sec:th-small:phase-3}
In Phase 3, the algorithm makes at most $(1-\epsilon_2) n$ adaptive exact queries.
If $\left| [n]\backslash \left(A_+^{(\le 2c)} \cup A_-^{(2a)}\right) \right| = (1\pm o(1)) n$, then with probability $(1\pm o(1))\epsilon_2$, these exact queries do not hit any elements in $A$.
Let $\bP^{(3)}_k$ (resp.~$\bP^{(3)}_{k-1}$) denote the probability measure of observations at the end of Phase 3 conditioned on $|A|=k$ (resp.~$|A|=k-1$).
Conditioned on that the exact queries do not hit any elements in $A$, we have
\begin{align*}
  \frac{d \bP^{(2c)}_k}{d \bP^{(2c)}_{k-1}} =\left| [n]\backslash \left(A_+^{(\le 2c)} \cup A_-^{(2a)} \cup A^{(3)}\right) \right|\cdot \frac{1}{k-\left| A_+^{(\le 2b)} \right|} \cdot \frac{p_{j_l}}{p_{m_1-j_l}} \cdot \frac{\binom n{k-1}}{\binom nk}
\end{align*}
where $A^{(3)}$ denotes the set of elements queried in Phase 3.
By a similar proof as \cref{lem:th:step-2c}, we can prove that
\begin{align*}
  \bP^{(3)}_k \left(\frac{d \bP^{(3)}_k}{d \bP^{(3)}_{k-1}} \le \epsilon_2^{-1}\delta^{-\epsilon_1/2}\right) \ge \epsilon_2 \delta^{1-\epsilon_1 \pm o(1)}.
\end{align*}
However, $\epsilon_2$ is a constant, so the discussion in the end of \cref{sec:th-small:phase-2} still applies.
This concludes that no algorithm can solve the three-phase problem with error probability $\le \delta$ using at most $(1-\epsilon) \frac{n \log \frac k\delta}{\DKL}$ noisy queries.
