\subsection{Proof overview for high-influence functions}
\label{sec:influence:overview}
Recall that the error probability $\frac 13$ in the statement of \cref{thm:influence} can be replaced with any $0<\epsilon<\frac 12$ without loss of generality. Also, the expected number of queries can be replaced by the worst-case number of queries by Markov's inequality.

Let $f: \{0, 1\}^n \rightarrow \{0, 1\}$ be a Boolean function with $\I(f) = \Omega(n)$. The hard distribution for the input $x$ will be the uniform distribution over $\{0, 1\}^n$.

Inspired by \cite{feige1994computing}, we prove hardness under the noisy query model by introducing a new problem where the algorithm has more power, and prove hardness of this new problem.

The new problem has three phases, described as follows.
\begin{enumerate}
  \item \label{item:sec:influence:overview:phase-1} In Phase 1, the algorithm makes $m_1=c_1\log n$ noisy queries to coordinate $x_i$ for every $i \in [n]$.
  \item \label{item:sec:influence:overview:phase-2} In Phase 2, the oracle reveals some coordinates of $x$ to the algorithm.
  \item \label{item:sec:influence:overview:phase-3} In Phase 3, the algorithm makes $m_2=c_2 n$ adaptive exact queries for some constant $c_2$.
\end{enumerate}
The goal of the algorithm is to compute the value $f(x)$.

Note that the first two phases are non-adaptive. The third phase is adaptive but the algorithm makes exact queries. This is the reason why the three-phase problem is easier to analyze than the original noisy query problem.

It is not hard to prove that if no algorithm can solve the three-phase problem with $\epsilon$ error probability, then no algorithm can solve the original problem with $\epsilon$ error probability using no more than $m_1 m_2 = c_1 c_2 n \log n$ noisy queries.
Therefore we only need to prove hardness of the three-phase problem.




\paragraph{Phase 1.} Let $a_i$ be the number of times where a query to $x_i$ returns $1$ in Phase $1$. The posterior distribution of the input $x$ given observations made in Phase $1$ depends only on the variables $a_i$. In other words, $(a_i)_{i \in [n]}$ is a sufficient statistic for $x$. Conditioned on $x$, the variables $(a_i)_{i \in [n]}$ are independent, and the distribution of each $a_i$ is a binomial distribution depending only on whether $x_i = 1$. That is, if $x_i = 1$, then $a_i \sim \Bin(m_1, 1 - p)$; otherwise, $a_i \sim \Bin(m_1, p)$ ($\Bin(\cdot, \cdot)$ denotes the binomial distribution). Moreover, for $x_i = 0$, $a_i$ is in an interval $I$ around $p m_1$ with probability $1-o(1)$; for $x_i = 1$, $a_i$ is in the interval $I$ with probability $n^{-c_3\pm o(1)}$, for some constant $c_3>0$ depending on $c_1$ and $p$.
Because the observations are independent for different coordinates, each index has a non-negative weight, such that the posterior probability of the input being $x$ is proportional to the product of weights of the coordinates $i$ where $x_i = 1$.

\paragraph{Phase 2.} In Phase 2, the oracle reveals some coordinates of $x$ to the algorithm. This information is revealed in two steps:
\begin{enumerate}[label=2\alph*.]
\item All coordinates with $a_i \not \in I$ are revealed.
\item Every $x_i$ with $a_i \in I$ is revealed independently with probability $q_{a_i}$ for some real numbers $(q_k \in [0, 1])_{k \in I}$.
\end{enumerate}
Because of the observations in Phase 1, the unrevealed coordinates can have different weights.
That is, given observations up to Step 2a, the posterior probabilities for different coordinates being $1$ can be different, which is undesirable.
Step 2b is a subsampling procedure, with the goal of reweighting the unrevealed coordinates so that all of them have the same weight.
If the interval $I$ is not too large, then the probabilities $q_k$ for $k\in I$ will not be too small.
Because observations made up to Step 2b are independent for different coordinates, they have the same effect as the following procedure (if the real numbers $q_k$ are chosen carefully): every $x_i$ with $x_i = 1$ is revealed independently with probability $p_+=1-n^{-c_3\pm o(1)}$ and $x_i$ with $x_i = 0$ is revealed independently with probability $p_-=o(1)$.

\paragraph{Phase 3.} At this stage, let the set of unrevealed coordinates be $U$. Conditioned on the revealed coordinates, $x_i$ for $i\in U$ are i.i.d.~$\Ber(q)$ variables, where  $\Ber(q)$ denotes the Bernoulli distribution with head probability $q=\frac{1-p_+}{1-p_++1-p_-}=n^{-c_3\pm o(1)}$. In other words, the distribution of $x_U$ is $\Ber(q)^{\otimes U}$. Let $g$ be a restriction of $f$ where the revealed coordinates of $x$ are fixed to be the revealed values. Then we need to show that on average, computing $g$ with error probability $\le \epsilon$ requires $\Omega(n)$ (adaptive) exact queries, for some sufficiently small $\epsilon > 0$. To this end, we consider a biased version of total influence $\I_q$:
\begin{align*}
  \I_q(g) = \sum_{i\in U} \bP_{x\sim \Ber(q)^{\otimes U}}( g(x) \ne g(x\oplus e_i)).
\end{align*}
Our proof strategy consists of the following three steps.
\begin{enumerate}[label=3\alph*.]
  \item First, we show that $\I_q(g) = \Omega(n)$ in expectation.
  \item After we make a query, we further fix the value of the queried coordinate, and replace $g$ with the new restricted function. We show that each exact query can only decrease $\I_q(g)$ by at most $1$ in expectation.
  \item Finally, we show that if $\I_q(g) = \Omega(n)$, then it is impossible to guess $g(x)$ (where $x$ follows a biased product distribution) with error probability $\le \epsilon$.
\end{enumerate}
Combining the three steps, we obtain that making $c_2n$ exact queries (in expectation) cannot compute $g$ with $\le \epsilon$ error probability, for sufficiently small $c_2$ and $\epsilon > 0$.

We note that Step 3c is where this approach fails to extend to general functions with sublinear total influence. In fact, for $\I_q(g) = o(n)$, it might be possible to guess $g(x)$ (where $x$ follows a biased product distribution) with $o(1)$ error probability. For instance, if $g(x)$ is a function where $o(1)$ random fraction of the values are $0$, and the other values are $1$, it is straightforward to show that $\I_q(g) = o(n)$ (in expectation), and we can guess $g(x)=1$ to achieve $o(1)$ error probability, without using any queries.

\subsection{Proof overview for Graph Connectivity} \label{sec:conn:overview}
In this section we present an overview of our proof of \cref{thm:graph-conn-hard}, hardness of Graph Connectivity. Again, the error probability $\frac 13$ can be replaced with any $0<\epsilon<\frac 12$ and the expected number of queries can be replaced by the worst-case number of queries, without loss of generality.

\paragraph{Hard distribution.}
To prove the lower bound, we design a hard distribution of inputs.
The distribution is based on uniform random spanning trees (USTs) of the complete graph.
Let $T$ be a UST. We say an edge $e\in T$ is $\beta$-balanced if both components of $T\backslash e$ have size at least $\beta n$.
Let $e$ be a uniform random $\beta_0$-balanced edge of $T$, where $\beta_0=\frac 1{21}$. (If such $e$ does not exist, we restart.)
We throw a fair coin and decide whether to erase edge $e$ from $T$.
That is, conditioned on $(T,e)$, the input graph $G$ is $T$ (connected) with probability $\frac 12$, and is $T\backslash e$ (disconnected) with probability $\frac 12$.

\paragraph{Three-phase problem.}
Following the three-phrase method, it suffices to show the hardness of the three-phase problem described as follows.
\begin{enumerate}[label=\arabic*.]
  \item \label{item:sec:conn:overview:phase-1} In Phase 1, the algorithm makes $m_1=c_1\log n$ noisy queries to every unordered pair (called ``potential edge'') $(u,v)\in \binom V2$ for some constant $c_1$.
  \item \label{item:sec:conn:overview:phase-2} In Phase 2, the oracle reveals some edges and non-edges of $G$ to the algorithm.
  \item \label{item:sec:conn:overview:phase-3} In Phase 3, the algorithm makes $m_2=c_2 n^2$ adaptive exact queries for some constant $c_2$.
\end{enumerate}
The goal of the algorithm is to determine whether the graph is connected.


\paragraph{Phase 1.}
This phase is similar to Phase 1 in \cref{sec:influence:overview}.
In Phase 1, the algorithm makes $m_1$ noisy queries to every potential edge $e\in \binom V2$.
Let $a_e$ be the number of times where a query to a potential edge $e$ returns $1$ in Phase 1.
Similar to \cref{sec:influence:overview}, if $e\in G$, then $a_e \sim \Bin(m_1, 1-p)$; otherwise $a_e \sim \Bin(m_1, p)$.
Specifically, for $e\not \in G$, $a_e$ is in an interval $I$ around $p m_1$ with probability $1-o(1)$; for $e\in G$, $a_e$ is in the interval $I$ with probability $n^{-c_3\pm o(1)}$, for some constant $c_3>0$ depending on $c_1$ and $p$.
Because the observations are independent for different edges, each edge has a non-negative weight, such that the posterior probability of $G=T$ for a tree $T$ is proportional to the product of weights of edges in $T$.

\paragraph{Phase 2.}
In Phase 2, the oracle reveals some edges and non-edges of $G$.
This information is revealed in three steps (the first two steps are similar to those in \cref{sec:influence:overview}).
\begin{enumerate}[label=2\alph*.]
  \item \label{item:sec:conn:overview:step-2a} In Step 2a, the potential edges $e$ with $a_e \not \in I$ are revealed. (Recall that $I$ is an interval around $p m_1$.) That is, the algorithm now knows which potential edges $e$ with $a_e \not \in I$ are edges.
  \item \label{item:sec:conn:overview:step-2b} In Step 2b, every edge $e$ with $a_e\in I$ is revealed independently with probability $q_{a_e}$, for some real numbers $(q_k\in [0,1])_{k\in I}$.
  \item \label{item:sec:conn:overview:step-2c} In Step 2c, $n-2$ edges are revealed as follows.
  If $G$ is disconnected, reveal all edges of $G$; otherwise, if there is a $\beta_0$-balanced edge that has not been revealed so far, uniformly randomly choose one (say $e^*$) from all such edges, and reveal all edges of $G\backslash e^*$. If $G$ is connected but all $\beta_0$-balanced edges have been revealed, report failure.
\end{enumerate}

Similar to \cref{sec:influence:overview}, if the real numbers $q_k$ are chosen carefully, the observations made up to Step 2b have the same effect as the following procedure: observe every edge independently with probability $p_+=1-n^{-c_3\pm o(1)}$; observe every non-edge independently with probability $p_-=o(1)$.



We can show that Step 2c reports failure with $1-\Omega(1)$ probability.
In the following, we condition on the event that the oracle does not report failure in Step 2c.
In this case, Step 2c reveals all except for one edge, which forms two connected components $T_1$ and $T_2$.
By the construction, $T_1$ and $T_2$ both have size at least $\beta_0 n$.
The posterior distribution is supported on $\{T_1\cup T_2, T_1\cup T_2\cup \{e\}: e\in E(T_1,T_2)\}$.
In our full analysis, we compute an exact formula for the posterior probability of every graph in the support.

\paragraph{Phase 3.}
In Phase 3 the algorithm makes $m_2=c_2 n^2$ adaptive exact queries.
Let $G_0 = T_1\cup T_2$, $G_e = T_1\cup T_2 \cup \{e\}$ for $e\in E(T_1,T_2)$.
Let $\bP^{(2)}$ denote the posterior distribution of $G$ after Phase 2.
Because $T_1$ and $T_2$ are already revealed, we can w.l.o.g.~assume that the algorithm makes queries only to edges in $E(T_1,T_2)$.
We prove that after Phase 2, for most edges $e\in E(T_1,T_2)$, we have
\begin{align*}
  \frac{\bP^{(2)}(G_e)}{\bP^{(2)}(G_0)} = \Theta\left(\frac 1{|T_1||T_2|}\right) = \Theta\left(\frac 1{n^2}\right)
\end{align*}
Let $E^{(3)}$ be the set of potential edges queried in Phase 3.
For small enough  $c_2$, we have
\begin{align*}
  \sum_{e\in E^{(3)}} \bP^{(2)}(G_e) = O(1) \cdot \bP^{(2)}(G_0).
\end{align*}
Therefore, with constant probability, all queries in Phase 3 return $0$.
Furthermore, if this happens, then the posterior probability of $G_0$ and $\{G_e: e\in E(T_1,T_2)\backslash E^{(3)}\}$ are within a constant factor of each other.
In this situation, outputting anything will result in a constant error probability.
This concludes that for some $\epsilon>0$, no algorithm can solve the three-phase problem with error probability $\epsilon$.

\subsection{Proof overview for \texorpdfstring{$k$}{k}-Threshold and Counting} \label{sec:threshold:overview}

Before we discuss our techniques for $k$-Threshold and Counting, we briefly discuss the previous work of \cite{wang2024noisy}, who showed a
\[
(1 \pm o(1)) \frac{n \log \frac{k}{\delta}}{\DKL}
\]
bound for $k$-Threshold where $k = o(n)$. Their lower bound stops working for $k = \Theta(n)$ because one step in their lower bound proof reveals the locations of $k-1$ $1$'s to the algorithm, leaving only $n-k+1=(1-\Omega(1))n$ unknown bits. This case can be solved by an algorithm using $(1-\Omega(1))\frac{n \log \frac{k}{\delta}}{\DKL}$ noisy queries, meaning that this approach cannot be used to show a tight lower bound.

\paragraph{Lower bound for $k$-Threshold.}
Our lower bound for $k$-Threshold for general values of $k$ is a reduction from~\cite{wang2024noisy}'s lower bound for $k = o(n)$. In the overview, we focus on the case where $k = (n + 1) / 2$ for odd $n$. In this case, the problem is equivalent to computing the majority of $n$ input bits.

Given any instance of $k$-Threshold on a length-$n$ array for $k = n / \log n$, we first add $L$ artificial $1$'s to the array to obtain a new instance where $n' = n + L$ and $k' = k + L$. We set $L$ so that $k' = (n' + 1) / 2$ (or equivalently, $L = n - 2k + 1$). Now suppose we have an algorithm for the new instance that uses only $(1-\epsilon) \frac{n' \log \frac{k'}{\delta}}{\DKL}$ noisy queries for some $\epsilon > 0$. Whenever the algorithm queries an artificial $1$, it can be simulated without incurring an actual noisy query; instead, we only need to flip biased coin with head probability $1-p$ and return its value. Because the algorithm is for computing the majority, intuitively, by symmetry, the expected number of queries it spends on an input $0$ and an input $1$ should be the same. Furthermore, if we add the artificial $1$'s to random positions, the algorithm should not be able to distinguish an artificial $1$ with an actual $1$. Therefore, in expectation, $L / n'$ fraction of the algorithm's queries are to artificial $1$'s, so the actual query complexity for solving the original $k$-Threshold instance is
\[
\left(1-\frac{L}{n'}\right) \cdot (1-\epsilon) \frac{n' \log \frac{k'}{\delta}}{\DKL} = (1-\epsilon)\frac{n \log \frac{k'}{\delta}}{\DKL}.
\]
Because $k = n / \log n$, we have $\log k' \le \log(n + 1) = (1+o(1)) \log k$, so the above bound becomes
\[
 (1-\epsilon + o(1))\frac{n \log \frac{k}{\delta}}{\DKL},
\]
which contradicts the lower bound from \cite{wang2024noisy}.

Our lower bound for more general values of $k$ is proved using a similar idea. However, we no longer have the symmetry between $0$ and $1$, so we need to reduce from the case $k = n / \log n$ or $k = (n + 1) / 2$ depending on whether the algorithm spends more queries on an input $0$ or $1$.


\paragraph{Upper bound for $k$-Threshold.}
For the upper bound, \cite{wang2024noisy}'s algorithm already works also for the $k = \Theta(n)$ case. Nevertheless, we provide a much simpler algorithm that achieves the same tight upper bound. \cite{wang2024noisy} used a standard $\textsc{Check-Bit}$ procedure to estimate the value of each input bit, and then used established machinery on Noisy Sorting and Noisy Heap studied earlier \cite{feige1994computing}. In comparison, our algorithm uses an asymmetric version of the $\textsc{Check-Bit}$ procedure that estimates the value of each input bit. Using this asymmetric procedure, we essentially only need to check each input bit one by one, avoiding calling extra algorithms such as Noisy Sorting or Noisy Heap.

\paragraph{Upper bound for Counting.}
Our algorithm for Counting is based on the idea of our algorithm for $k$-Threshold.
Our algorithm for $k$-Threshold can additionally count the number of $1$'s in the input if it is at most $k$, so one natural idea for Counting is to first compute an estimation $k$ of the answer $\lVert a \rVert_1$, then use our algorithm for $k$-Threshold to compute the exact answer. However, this approach does not work when $\lVert a \rVert_1$ is very small compared to $n$, as there is no reliable way to estimate the answer within a constant factor using $o\left(n\frac{\log \frac{\lVert a\rVert_1}{\delta}}{\DKL}\right)$ queries.
We circumvent this issue by gradually increasing $k$ during the algorithm and simulate the asymmetric $\textsc{Check-Bit}$ procedure on each input bit.
We can view the asymmetric $\textsc{Check-Bit}$ procedure for each bit as a biased random walk on $\bZ$, and for different $k$ the procedure only changes the stopping condition, but not the random walk. In this way we show that $k$ will eventually stop at the correct answer with desired error probability.

