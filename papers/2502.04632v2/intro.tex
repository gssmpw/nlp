
Fault tolerance is a crucial feature of algorithms that work for large systems, as errors occur unavoidably. Hence, previous studies have considered various models to capture the effect of errors, such as R{\'e}nyi--Ulam game \cite{renyi1961problem, ulam1976adventures}, independent noise \cite{feige1994computing}, and independent noise without resampling \cite{braverman2008noisy}.


\cite{feige1994computing} formally proposed the noisy query model with independent noise (which they call the noisy Boolean decision tree model or the noisy comparison tree model, depending on whether the problem uses point queries to input bits or comparison queries between input elements). In this model, each query returns a bit (either an input bit or a pairwise comparison result) that is independently flipped with some fixed probability $p \in (0, \frac{1}{2})$ (i.e. independent noise) and repeated queries are allowed. The efficiency of an algorithm is measured in terms of the number of queries it makes. \cite{feige1994computing}  showed tight asymptotic bounds for the noisy query complexity for a wide range of problems, including Parity, Threshold, Binary Search and Sorting.




In fact, researchers had studied queries with independent noise even before \cite{feige1994computing} formally defined the model. \cite{berlekamp1964block,horstein1963sequential,burnashev1974interval} all studied some versions of Binary Search under independent noise. In particular, \cite{berlekamp1964block,horstein1963sequential} studied the problem through the lens of channel coding (see \cite{wang2022noisy} for a more detailed discussion about the relationship between the channel coding perspective and the noisy query perspective). These examples further demonstrate that the noisy query model by~\cite{feige1994computing} is a natural model to study.

Following \cite{feige1994computing}, researchers have studied problems in the noisy query model extensively, including random functions \cite{reischuk1991reliable,feige1992complexity,evans1998average}, $k$-CNF and $k$-DNF \cite{DBLP:journals/rsa/KenyonK94}, Binary Search \cite{ben2008bayesian,dereniowski2021noisy, gu2023optimal, zhu2023optimal}, Sorting~\cite{wang2022noisy,wang2023variable,gu2023optimal, zhu2023optimal}, Graph Search \cite{Emamjomeh-Zadeh16, DereniowskiTUW19, dereniowski2021noisy} (a generalization of Binary Search), and $k$-Threshold \cite{zhu2023optimal, zhu2023noisy,wang2024noisy}.

However, despite the popularity and naturality of the model, most research on the noisy query model focus on specific functions instead of general functions. In the above examples, the only exceptions are the lower bounds for random functions \cite{reischuk1991reliable,feige1992complexity,evans1998average}, and upper bounds for $k$-CNF and $k$-DNF \cite{DBLP:journals/rsa/KenyonK94}. Furthermore, the specific functions studied in literature are often the ones studied already in \cite{feige1994computing} such as Threshold, Binary Search and Sorting. As a result, the noise query complexity of many important and natural problems are left unexplored. In this paper, we take a first step towards studying the noise query complexity of more general functions and problems.




\subsection{High-influence functions}

Our first result is a lower bound for the noisy query complexity of high-influence functions, a greatly general family of functions. This result is the first result towards understanding the lower bound of general Boolean functions, beyond the lower bound for random functions \cite{reischuk1991reliable,feige1992complexity,evans1998average}.

Influence is a central quantity in the analysis of Boolean functions. For a Boolean function $f: \{0,1\}^n\to \{0,1\}$, the influence of coordinate $i\in [n]$ is defined as
\begin{align*}
  \Inf_i(f) =  \bP_{x\sim \{0, 1\}^n}(f(x) \ne f(x\oplus e_i)),
\end{align*}
where $e_i$ denotes the bit string where the only $1$ is on the $i$-th coordinate, and $\oplus$ denotes exclusive or. That is, $\Inf_i(f)$ is the probability that flipping the $i$-th coordinate of a uniformly random bit string also flips the function value.
The \emph{total influence} is the sum of influences over all coordinates, i.e.,
\begin{align*}
  \I(f) = \sum_{i\in [n]} \Inf_i(f).
\end{align*}
We prove that Boolean functions with linear total influence have noisy query complexity $\Omega(n\log n)$.

\begin{restatable}[Noisy query complexity of high-influence functions]{theorem}{LinearTotalInfluence} \label{thm:influence}
  For any $c>0$, there exists $c'>0$ such that the following holds.
  For any Boolean function $f: \{0,1\}^n\to \{0,1\}$ with $\I(f) \ge c n$, any noisy query algorithm computing $f(x)$ with error probability $\le \frac 13$ makes at least $c' n\log n$ noisy queries in expectation to the coordinates of the input $x \in \{0, 1\}^n$.
\end{restatable}
Note that the error probability $\frac 13$ can be replaced with any $0<\epsilon<\frac 12$ without affecting the asymptotic noisy query complexity. The statement is tight in the sense that any Boolean function on $n$ inputs can be computed with error probability $o(1)$ using $O(n\log n)$ noisy queries: by simply querying each bit $O(\log n)$ times, we can determine the input string with $o(1)$ error probability.

\cref{thm:influence} unifies and generalizes several previous results. For example, it was known that a random Boolean function (with probability $1-o(1)$) has noisy query complexity $\Omega(n\log n)$ \cite{reischuk1991reliable,feige1992complexity,evans1998average}, and computing the parity function requires $\Omega(n\log n)$ noisy queries \cite{feige1994computing}. As random Boolean functions and the parity function have total influence $\Omega(n)$, \cref{thm:influence} immediately implies these lower bounds as special cases.


Another central notion related to influence is the sensitivity of Boolean functions\footnote{The sensitivity of a Boolean function $f$ at input $x$, denoted by $\s(f, x)$, is the number of bits $i$ for which $f(x) \ne f(x\oplus e_i)$. }, as it is well known that the total influence is the same as the \emph{average sensitivity} (the expected sensitivity over a uniformly random input). \cite{reischuk1991reliable} proved that any non-adaptive algorithm computing a Boolean function $f$ makes at least $\Omega(\s(f)\log \s(f))$ noisy queries, where $\s(f)$ is the (maximum) sensitivity of $f$. This result is incomparable to \cref{thm:influence}, as their lower bound only holds against non-adaptive algorithms. In fact, it is not possible to extend the $\Omega(\s(f)\log \s(f))$ lower bound against adaptive algorithms in general. For instance, the $\mathrm{OR}$ function has sensitivity $n$ and (adaptive) noisy query complexity $O(n)$ \cite{feige1994computing}. On the other hand, the average sensitivity of $\mathrm{OR}$ is much smaller, which suggests that the average sensitivity of a Boolean function $f$ is a more reasonable measure to lower bound the \emph{adaptive} noisy query complexity. This motivates us to raise the following open question, towards a lower bound for general Boolean functions.

\begin{open} \label{open:influence}
Is it true that every Boolean function $f: \{0, 1\}^n \rightarrow \{0, 1\}$ has noisy query complexity $\Omega(\I(f) \log \I(f))$?
\end{open}
\cref{thm:influence} resolves the case where $\I(f) = \Omega(n)$. We note another evidence supporting the $\I(f) \log \I(f)$ lower bound. The randomized query complexity $\R(f)$ satisfies $\R(f) = \Omega(\bs(f)) = \Omega(\s(f)) = \Omega(\I(f))$, where $\bs(s)$ denotes block sensitivity and the first step is by \cite[Lemma 4.2]{nisan1989crew}. In general, the noisy query complexity of a function $f$ is always between $\R(f)$ and $O(\R(f) \log \R(f))$. Therefore, the $\Omega(\I(f) \log \I(f))$ lower bound is consistent with these known relationships.

For the proof of \cref{thm:influence}, we develop a three-phase lower bound framework, which is based on and refines \cite{feige1994computing}'s two-phase method for proving a lower bound for the $k$-Threshold problem. In the three-phase method, we reduce the original problem in the noisy query model to a stronger observation model, where in Phase 1 the algorithm makes non-adaptive noisy observations and in Phase 3 the algorithm makes adaptive exact observations.
In Phase 2, the model gives away free information, which can only help the algorithm.
By designing this free information carefully, the effect of Phase 1 and 2 combined can be significantly simplified, allowing for a precise analysis in Phase 3.

We note that this idea of giving away free information already appears in \cite{feige1994computing}'s two-phase method.
For their problem ($k$-Threshold), this free information is relatively simple.
However, for other and more general problems, the free information could be significantly more involved.
We design the free information in a different way in order for the analysis in Phase 3 to be viable.
This additional phase to the original two-phase method makes it easier to apply and allows for other applications.
As we will soon discuss, the three-phase framework is essential for our result on Graph Connectivity and also leads to a simple proof for the lower bound of $k$-Threshold.

\subsection{Graph Connectivity}
Although the noisy query model is quite natural, there has been little prior work studying graph problems in this model. Some prior examples include \cite{feige1994computing}, which briefly mentioned that a lower bound for the noisy query complexity of Bipartite Matching can be achieved by reducing from the other problems they studied. \cite{DBLP:journals/rsa/KenyonK94} designed algorithms for $k$-CNF and $k$-DNF using a small number of queries, which imply, for instance, that one can test, up to error probability $\delta$, whether a given $n$-vertex graph contains a triangle using $O\left(n^2 \log \frac{1}{\delta}\right)$ noisy queries.


One of the most fundamental graph problems is Graph Connectivity, where we are given an $n$-vertex undirected graph $G$, and need to determine whether the graph is connected via edge queries. It is so basic that those studying algorithms encounter it very early on. For instance, breadth-first-search and depth-first-search are usually among the first graph algorithms taught in undergraduate algorithm classes, and the simplest application of them is to detect whether a graph is connected. However, to our surprise, we do not even have a good understanding of the noisy query complexity of such an elementary problem.

One simple algorithm for Graph Connectivity is to query every edge in the input graph $O(\log n)$ times to correctly compute the input graph with high probability, and then solve Graph Connectivity on the computed graph. This naive algorithm uses $O(n^2 \log n)$ noisy queries, and is essentially all what was previously known about Graph Connectivity in the noisy query model.
In particular, hardness of Graph Connectivity does not seem to follow from known hardness results. %

Using the three-phase method, we prove an $\Omega(n^2 \log n)$ lower bound on the noisy query complexity of Graph Connectivity, showing that the naive $O(n^2 \log n)$ algorithm is actually optimal up to a constant factor:



\begin{restatable}[Hardness of Graph Connectivity]{theorem}{GraphConnectivity}\label{thm:graph-conn-hard}
  Any algorithm solving the Graph Connectivity problem with error probability $\le \frac 13$ uses $\Omega(n^2\log n)$ noisy queries in expectation.
\end{restatable}
Similarly as before, the error probability $\frac 13$ can be replaced with any $0<\epsilon<\frac 12$ without affecting the asymptotic noisy query complexity.



We also show an $\Omega(n^2 \log n)$ lower bound for the related $s$-$t$ Connectivity problem, where we are given an $n$-vertex undirected graph $G$ and two fixed vertices $s$ and $t$, and the goal is to determine whether there is a path in the graph connecting $s$ and $t$.


As Graph Connectivity and $s$-$t$ Connectivity are very basic tasks on graphs, their lower bounds immediately imply lower bounds for several other fundamental graph problems as well. For instance, given the lower bounds for Graph Connectivity and $s$-$t$ Connectivity, it is straightforward to show that Global Min-Cut, $s$-$t$ Shortest Path, and $s$-$t$ Max Flow on unweighted undirected graphs all require $\Omega(n^2 \log n)$ noisy queries in expectation.


\subsection{Threshold and Counting}

In the $k$-Threshold problem, one is given a length-$n$ Boolean array $a$ and an integer $k$, and the goal is to determine whether the number of $1$'s in the array $a$ is at least $k$. Note that the answer to the input is false if and only if the number of $0$'s in the input is at least $n - k + 1$. We can thus solve $k$-Threshold using an algorithm for $(n-k+1)$-Threshold: we can flip all input bits, change $k$ to $n - k + 1$, solve the modified instance, and finally flip the result. Therefore, we can assume without loss of generality that $k \le n - k +  1$, or equivalently, $k \le (n + 1) / 2$.

$k$-Threshold is one of the first problems studied in the noisy query model. In \cite{feige1994computing}, it was shown that $\Theta\left(n \log \frac k\delta\right)$ queries are both sufficient and necessary to solve the problem with error probability $\delta$. However, the optimal constant factor was left unknown.\footnote{Studying constant factors is often overlooked in theoretical computer science, but in this research area, determining the optimal constants for noisy query complexities of other fundamental problems such as Binary Search and Sorting has been an active topic (e.g., \cite{burnashev1974interval, ben2008bayesian,dereniowski2021noisy, gu2023optimal}). See \cite{DBLP:conf/icalp/Gretta024} for more discussions on the importance of studying constants in query complexity. }

There has been some progress towards determining the exact constant for $k$-Threshold. In~\cite{zhu2023noisy}, it was shown that the noisy query complexity of the $\mathrm{OR}$ function on $n$ input bits (equivalent to $1$-Threshold) is
\[
(1 \pm o(1)) \frac{n \log \frac{1}{\delta}}{\DKL}
\]
for $\delta=o(1)$,  where $\DKL=(1-2p)\log \frac{1-p}p$ is the Kullback-Leibler divergence between two Bernoulli distributions with heads probabilities $p$ and $1 - p$.
This result was later generalized to $k$-Threshold for all $k = o(n)$ and $\delta = o(1)$ by \cite{wang2024noisy}, who showed an
\[
(1 \pm o(1)) \frac{n \log \frac{k}{\delta}}{\DKL}
\]
bound. Compared to \cite{zhu2023noisy}, \cite{wang2024noisy}'s result works for a much wider range of $k$. However, their lower bound proof technique unfortunately stops working for the case $k = \Theta(n)$, and this case is frustratingly left open (we remark that their algorithm gives the right upper bound even for $k=\Theta(n)$).

In this work, we complete the last piece of the puzzle, showing a matching bound for all values of $k$.
\begin{restatable}[Noisy query complexity of $k$-Threshold]{theorem}{KThreshold}
    \label{thm:k-threshold}
For any $1 \le k \le (n + 1) / 2$ and $\delta = o(1)$, computing $k$-Threshold on a length-$n$ array with error probability $\delta$ needs and only needs
\[
(1 \pm o(1)) \frac{n \log \frac{k}{\delta}}{\DKL}
\]
noisy queries in expectation.
\end{restatable}
Here the $\delta=o(1)$ assumption is standard and has appeared in several previous works \cite{gu2023optimal,zhu2023noisy,wang2024noisy}.

While \cite{wang2024noisy} has already given an algorithm achieving the tight upper bound for any $k$, their algorithm involves calling some extra algorithms such as Noisy Sorting and Noisy Heap, which seems too heavy and unnecessary for the $k$-Threshold problem (after all, in the classic noiseless setting, the algorithm for $k$-Threshold is much simpler than algorithms for Sorting or Heap). We provide a much simpler algorithm which involves only checking each bit one by one and completely avoids calling these extra algorithms.

We also provide an alternative and simpler proof of the lower bound for $k$-Threshold with $k=o(n)$. The proof of \cite{wang2024noisy} considers three cases and uses two different methods (the two-phase method from \cite{feige1994computing} and Le Cam's two point method) for solving them. We show that this casework is unnecessary by providing a uniform and simple proof for all $k=o(n)$ by using our three-phase method.

We also consider a related problem, Counting, where we need to compute the number of $1$'s in $n$ input Boolean bits. The lower bound for $k$-Threshold easily applies to Counting as well (though in a non-black-box way). In addition, we design an algorithm for Counting that matches the lower bound, obtaining the following result.
\begin{restatable}[Noisy query complexity of Counting]{theorem}{Counting}
    \label{thm:counting}
    Given a sequence $a \in \{0, 1\}^n$, computing $\lVert a \rVert_1$ with error probability $\delta = o(1)$ needs and only needs
    \[
    (1\pm o(1))\frac{n \log \frac{\min\{\lVert a \rVert_1, n - \lVert a \rVert_1\}+1}{\delta}}{\DKL}
    \]
    noisy queries in expectation.
\end{restatable}

A problem closely-related to $k$-Threshold is the $k$-Selection problem, where one is given $n$ items (comparable with each other) and the goal is to select the $k$-th largest element using noisy comparison queries. It is known that solving $k$-Selection with error probability $\delta=o(1)$ needs and only needs $\Theta\left(n\log \frac {\min\{k,n-k+1\}}\delta\right)$ noisy queries \cite{feige1994computing}.
Their bounds are only tight up to a constant factor, so the exact value of the leading coefficient remains open.
\begin{open} \label{open:k-selection}
  Determine the exact constant $c$ such that $(c\pm o(1)) n\log \frac {\min\{k,n-k+1\}}{\delta}$ noisy queries is both sufficient and necessary to solve the $k$-Selection problem with error probability $\delta=o(1)$.
\end{open}
