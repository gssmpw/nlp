Throughout the paper, we use $p \in (0, 1/2)$ to denote the flipping probability of each noisy query, i.e., for a bit $x$, $\textsc{Query}(x)$ returns $x$ with probability $1-p$ and returns $1 - x$ with probability $p$. For $0 \le q \le 1$, $\Ber(q)$ denotes the Bernoulli distribution with head probability $q$.
Throughout the paper, all $\log$s have base $e$.
For two sequences $(f_n)_n$, $(g_n)_n$, we write $f_n \asymp g_n$ if $f_n = \Theta(g_n)$, i.e., there exists $\epsilon>0$ such that $\epsilon f_n \le g_n \le \epsilon^{-1} f_n$ for all $n$ large enough.

Let $\Bin(n,p)$ denote the binomial distribution. The following large deviation bound is useful.
\begin{lemma} \label{lem:binomial-large-deviation}
  Let $0<p<\frac 12$ and $0<q<1$.
  Then for large enough $m$ and integer $k = (q\pm o(1)) m$, we have
  \begin{align*}
    \bP(\Bin(m, p) = k) = \exp\left(-(\DKLs(q \parallel p) \pm o(1)) m\right),
  \end{align*}
  where
  \begin{align*}
    \DKLs(a \parallel b) = a \log \frac ab + (1-a) \log \frac{1-a}{1-b}
  \end{align*}
  is the binary KL divergence function.
\end{lemma}
\begin{proof}
  \begin{align*}
    \bP(\Bin(m, p) = k) =&~ \binom mk p^k (1-p)^{m-k} \\
    \nonumber =&~ \exp(m \log m - k \log k - (m-k) \log (m-k) \pm o(m)) \\
    \nonumber &~\cdot \exp(k \log p + (m-k) \log (1-p))\\
    \nonumber =&~ \exp\left( -\left(q \log \frac{q}{p} + (1-q) \log \frac{1-q}{1-p} \pm o(1)\right) m\right) \\
    \nonumber =&~ \exp(-(\DKLs(q \parallel p) \pm o(1)) m).
  \end{align*}
\end{proof}
