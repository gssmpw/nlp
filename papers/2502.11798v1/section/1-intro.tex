\section{Introduction}
\label{sec:intro}

% \todo Difference between traditional backdoor and diffusion backdoor. \fin
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/fig1_difference.pdf}
    \caption{Backdoor inference in traditional discriminative models (\textbf{upper}) and generative diffusion models (\textbf{lower}).}
    \label{fig:difference}
\end{figure}
% 1.
Diffusion Models (DMs) have demonstrated remarkable capabilities across a wide range of generation tasks, \eg, image generation~\cite{ho2020denoising}, text-to-speech generation (TTS)~\cite{popov2021grad}, text-to-video generation (T2V)~\cite{ho2022video}, \etc.
However, recent studies have revealed that DMs, whether for unconditional or conditional generation, are vulnerable to security threats from backdoor attacks~\cite{chou2023backdoor,struppek2023rickrolling}. 
The attacked model (termed the \textit{backdoored model}) performs normally with clean inputs, while it can be manipulated to generate malicious content when provided with poisoned inputs containing a predefined \textit{trigger} pattern.  
Figure~\ref{fig:difference} comparatively illustrates the backdoor inference in traditional discriminative models (the primary focus of most previous research) and generative diffusion models. 
Due to their distinct applications, they pose threats to different real-world scenarios. 
For example, the former may enable unauthorized access to secure systems through face recognition, while the latter may be used to bypass the safety filter and generate offensive images for illegal purposes. 
Therefore, these two threats are non-overlapping and technically distinct. 
The new threats posed by diffusion backdoors make it a critical research topic we cannot ignore.



% 2. 
Compared to research on backdoor learning\footnote{Backdoor learning includes relevant research topics about backdoor attack and backdoor defense.} in discriminative models (e.g., convolutional neural networks (CNNs)), studies on generative models (e.g., DMs) face significantly greater challenges regarding backdoor attack types and target types.
% , and \todo \fin target diversity.
In fact, regarding attack types, research on discriminative models is typically limited to a single modality input and focuses on a uniform model structure.
%In fact, for the attack type, the research scope is limited to one modality input and mainly one uniform model structure in the discriminative model.
% Until now, the research on backdoor learning\footnote{Backdoor learning includes relevant research topics about backdoor attack and backdoor defense.} 
% focuses mainly on discriminative models (\eg, convolutional neuron network (CNN)) rather than generative models (\eg, DM). The latter faces a much larger challenge regarding the attack type, target type, and target diversity.
% % The inferences of the backdoored models are illustrated in Figure~\ref{fig:difference}. 
% For the attack type, the research scope is limited to one modality input and mainly one uniform model structure in the discriminative model. 
In contrast, emerging generative models (\eg, stable diffusion~\cite{rombach2022high}) usually involve an additional time-step dimension and multiple input modalities with combined model structures, exposing the model to greater risks. 
%several input modalities with the combined model structures, which exposes the model to greater risk.  
Regarding target types, attacks on discriminative models aim to misclassify poisoned inputs into one of a limited set of class labels, while the backdoor target types in generative models can be more diverse, including, but not limited to, object replacement, style modification, and the insertion of specified malicious image patches.
%Regarding the target type, the attack on the discriminative model focuses on misclassifying a poisoned input to one of the limited class labels, while the backdoor target type can be more diverse in the generative model, including but not limited to object replacement, style modification, and specified malicious image patching. 
%These unpredictable target types make the research more challenging. \fin
% \todo \fin Similarly, the diversity of the generated content also increases the target diversity. For example, in text-to-image (T2I) generation, one backdoor target (\eg, generating a cat) can bring non-traversable results (\eg, cats with different appearances), making it difficult to evaluate the precise performance of the attacks.
As a result, these two factors make backdoor learning in diffusion models a more complex and highly vulnerable field, where existing conclusions for discriminative models may not be applicable
%Compared to previous research on discriminative models, these \wl two \fin aspects make diffusion backdoor a more complex and highly exploitable field, where the existing conclusions may be inapplicable.
% Despite the importance of fully understanding the backdoor behaviors in DMs, the challenges mentioned above limit the research progress.


% 3. 
In recent years, many different backdoor attack and defense methods have been proposed for DMs, \eg \cite{zhai2023text,chou2023backdoor}.
However, a comprehensive benchmark for backdoor learning in DMs is still lacking, making it difficult to conduct fair comparisons and thorough evaluations of the existing approaches, thus hindering future research progress. 
In this paper, to address this issue, we propose a comprehensive benchmark designed for backdoor learning in DMs, named \textit{BackdoorDM}, which consists of 9 state-of-the-art (SOTA) attack methods, 4 SOTA defense strategies, and 2 useful analysis tools. 
We first systematically classify and formulate the existing literature in a unified framework, focusing on three different backdoor attack types and five backdoor target types, which are restricted to a single type in discriminative models. 
Then, we systematically summarize the evaluation metrics for each backdoor target type and propose a comprehensive framework using GPT-4o to evaluate \textit{model specificity}, \textit{model utility}, and \textit{attack efficiency}.  
Finally, we conduct extensive experiments and highlight several important findings to inspire future research. 
More details about these findings can be found in Section \ref{subsec:attacksresults}. 
%: \textbf{(a)} Current attacks for ImageFix are generally at a similar level with good performance, and further efforts are needed to explore more advanced trigger techniques, \eg, invisible triggers. \textbf{(b)}  
% \todo \fin
% in different models, poisoning ratios, backdoor numbers, and target types, and summarize several key findings. \todo \fin 
% We hope that BackdoorDM will help overcome current barriers and contribute to building a trustworthy DMs community. \fin 

% To help cross these barriers, we introduce \textit{BackdoorDM}, a comprehensive benchmark for integrating and evaluating backdoor learning in diffusion models. Specifically, we first systematically classify and formulate the backdoor attack types and target types of the current research. Then, we categorize evaluation metrics for each backdoor target type and propose a comprehensive framework to evaluate \textit{model specificity}, \textit{model utility}, and \textit{attack efficiency}. 
% Furthermore, we propose a unified backdoor evaluation method, adaptable to all backdoor target types, using GPT-4o as the agent. 
% Until now, we have implemented 9 state-of-the-art (SOTA) backdoor attack methods and 4 SOTA defense methods, and provide 2 useful analysis tools (\eg, attention map and neuron activation visualization).
% Based on the implemented method, we conduct extensive experiments across different models, datasets, poisoning ratios, backdoor numbers, and target types.
% The results reveal several key findings: \todo findings \fin

Our main contributions are as follows. 
\textbf{1) The first benchmark:} We propose the first benchmark designed for backdoor learning research in DMs, integrating numerous SOTA backdoor methods and providing comprehensive evaluation metrics. 
\textbf{2) Systematic taxonomy:} We provide a systematic classification and precise formulation of various backdoor attack types and target types in DMs, clearly defining the research scope in this field.
\textbf{3) Novel evaluation method:} We propose a unified backdoor evaluation method using GPT-4o, which covers most backdoor target types and provides detailed image-level evaluations. 
\textbf{4) Comprehensive evaluation:} We conduct fair and comprehensive evaluations of the implemented methods and present several key findings for future research. 

% \begin{itemize}
%     \item First to propose diffusion backdoor bench
%     \item New classification
%     \item New protocol using GPT-4o 
%     \item Systematic evaluation.
% \end{itemize}

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig_framework.pdf}
    \caption{The process of injecting backdoors into diffusion models based on the unified backdoor attack formulation in Equation~\eqref{eq:def_back_diffusion}.}
    \label{fig:framework}
\end{figure*}