\section{Experiment and Analysis}
\label{sec:experiment}

\subsection{Experimental Setup}
\label{subsec:exp_setup}
\textbf{Datasets and Models. }
We evaluate our benchmark across multiple datasets and models. For unconditional generation, we use the CIFAR-10 dataset \cite{alex2009learning} with DDPM \cite{ho2020denoising}. For text-to-image generation, we focus on Stable Diffusion v1.5 as the backbone model. The CelebA-HQ-Dialog dataset \cite{jiang2021talk} is used for VillanCond \cite{chou2024villandiffusion}. For backdoor injection and evaluation of other text-to-image attack methods, we use the LAION-Aesthetics v2 5+ subset \cite{schuhmann2022laion} and the MS-COCO 2014 validation split \cite{lin2014microsoft}, respectively. More details are provided in Appendix~\ref{subsec:dataset} and \ref{subsec:eval_detail}.
% We evaluate our benchmark using CIFAR10 \cite{alex2009learning} dataset for unconditional generation with DDPM \cite{ho2020denoising}. For all the text-to-image generation, we focus on Stable Diffusion v1-5 as the backbone model, and we use CelebA-HQ-Dialog \cite{jiang2021talk} dataset for VillanDiffusion \cite{chou2024villandiffusion} conditional attack, while LAION-Aesthetics v2 5+ subset \cite{schuhmann2022laion} is used for backdoor injection and MS-COCO 2014 validation split \cite{lin2014microsoft} for evaluation in other attack methods.
% More details are provided in Appendix~\ref{subsec:dataset}.



% (DDPM \cite{ho2020denoising}, DDIM \cite{song2020denoising}, LDM \cite{rombach2022high} , and NCSN \cite{song2019generative})

% We evaluate our benchmark on 5 commomly used datasets (CIFAR10 \cite{alex2009learning}, CelebA-HQ \cite{liu2015deep}, CelebA-HQ-Dialog \cite{jiang2021talk}, LAION \cite{schuhmann2022laion}, MS-COCO \cite{lin2014microsoft}). We use DDPM \cite{ho2020denoising}, DDIM \cite{song2020denoising}, LDM \cite{rombach2022high} and NCSN \cite{song2019generative} for unconditional generation and Stable Diffusion for text-to-imgae generation.

\noindent \textbf{Attacks and Defenses. }
We evaluate a total of 9 attack methods and 4 defense methods in our experiment. For attacks targeting unconditional generation, we assess the Elijah defense \cite{an2024elijah} and the input detection performance of TERD \cite{pmlr-v235-mo24a}. For text-to-image attacks, we evaluate two defense methods: T2IShield \cite{wang2025t2ishield} and Textual Perturbation \cite{zhai2023text}. The evaluation metrics for the attack methods used in our experiments follow the setup outlined in Table \ref{tab:target_taxonomy}. 
For the evaluation of defense methods, we examine the changes in the corresponding metrics for each backdoor type after applying the defense to assess its effectiveness. More details about the settings of our implemented methods are provided in Appendix~\ref{subsec:imple_detail} and Appendix~\ref{subsec:attack_details}. 
The \textbf{boldface} values in the tables of this section indicate the best performance for each corresponding metric under the same target type.
% \textbf{Due to the space limit, some tables are postponed to Appendix.}
% We implement four attack methods and the Elijah defense \cite{an2024elijah} for unconditional generation, using MSE and FID metrics (Section 3.3). For text-to-image generation, we implement four attack methods and T2IShield defense \cite{wang2025t2ishield}, using MSE, CLIP Score, ASR, and ACC as evaluation metrics (Section 3.3). The metrics used in our evaluation follow those listed in Table 1. Specifically, we used the metrics in the table to evaluate the model specificity, model utility, and attack efficiency of the corresponding types of backdoors, and evaluated the defenses by examining the changes in the respective metrics after the defenses are applied to the backdoored models. More details are provided in Appendix~\ref{subsec:attack_details} and \ref{subsec:defense_details}. 
%The \textbf{boldface} values of the tables in this section indicate the best performance for the corresponding metric under the same target type.

% Additionally, GPT4-o is used for unified backdoor evaluation for text-to-image generation (Section 3.4). Furthermore, we perturb the triggers of all attack methods to evaluate the the backdoor robustness. Detailed evaluation configurations are illustrated in the \todo table.

\subsection{Attacks Results}\label{subsec:attacksresults}

% \begin{table*}[ht]
% \caption{\todo (a Full Result Table containing all attacks.) \fin.}\label{tab:attack_results}
% \centering
% \renewcommand\arraystretch{1.1}
% \resizebox{17.79cm}{!}{
% \begin{tabular}{|c|c|l|ccc|cc|cc|}
% \hline
% \rowcolor{black!10} \textbf{} & \textbf{} & \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{\textbf{Model Specificity}} & \multicolumn{2}{c|}{\textbf{Model Utility}} & \multicolumn{2}{c|}{\textbf{Attack Efficiency}} \\ \cline{4-10} 
% \rowcolor{black!10} \textbf{Backdoor} & \textbf{Condition} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{Backdoor Target}} & \multicolumn{1}{c|}{\textbf{Remaining Content}} & \textbf{Backdoor} & \multicolumn{1}{c|}{\textbf{Clean Target}} & \textbf{Clean} & \multicolumn{1}{c|}{} & \textbf{Data} \\
% \rowcolor{black!10} \textbf{} & \textbf{} & \multicolumn{1}{c|}{\multirow{-3}{*}{\textbf{Implemented Method}}} & \multicolumn{1}{c|}{\textbf{Accomplishment}} & \multicolumn{1}{c|}{\textbf{Preservation}} & \textbf{Robustness} & \multicolumn{1}{c|}{\textbf{Accomplishment}} & \textbf{Consistency} & \multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{Runtime}}} & \textbf{Usage} \\ \hline
% \multirow{5}{*}{ImageFix} & \multirow{4}{*}{Unconditional} & BadDiffusion~\cite{chou2023backdoor} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & TrojDiff~\cite{chen2023trojdiff} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & InviBackdoor~\cite{li2024invisible} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & VillanDiffusion~\cite{chou2024villandiffusion} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{2-10} 
%  & Conditional & VillanCond~\cite{chou2024villandiffusion} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline
% \multirow{2}{*}{ImagePatch} & \multirow{2}{*}{Conditional} & BiBadDiff~\cite{pan2024from} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & Pixel-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline
% ObjectAdd & Conditional & Newly Proposed & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline
% \multirow{5}{*}{ObjectRep} & \multirow{5}{*}{Conditional} & TPA (RickRolling)~\cite{struppek2023rickrolling} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & Object-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & TI (PaaS)~\cite{huang2024personalization} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & DB (PaaS)~\cite{huang2024personalization} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & EvilEdit~\cite{wang2024eviledit} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline
% \multirow{2}{*}{StyleAdd} & \multirow{2}{*}{Conditional} & TAA (RickRolling)~\cite{struppek2023rickrolling} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-10} 
%  &  & Style-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \hline
% \end{tabular}
% }
% \end{table*}

We evaluate the attack performance of each target type using the corresponding metrics mentioned in Table~\ref{tab:target_taxonomy}. Note that most metrics for model specificity and utility (except for FID and LPIPS) are only comparable within the same target type.

\begin{table}[htbp]
\caption{Evaluation results of attacks from ImageFix-Backdoor. The target image is uniformly set as ``cat''. The term ``data usage'' represents the poisoning ratio.}
\label{tab:result_attack_imageFix}
\centering
\renewcommand\arraystretch{1.1}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{|c|c|c|cc|}
\hline
\rowcolor{black!10} & \textbf{Model Specificity} & \textbf{Model Utility} & \multicolumn{2}{c|}{\textbf{Attack Efficiency}}              \\ \cline{2-5} 
 \rowcolor{black!10}  \multirow{-2}{*}{\textbf{ImageFix}}                                & \textbf{MSE $\downarrow$}               & \textbf{FID $\downarrow$}           & \multicolumn{1}{c|}{\textbf{Runtime $\downarrow$}}  & \textbf{Data Usage $\downarrow$} \\ \hline
BadDiffusion                       & 0.0200                     & 18.21                  & \multicolumn{1}{c|}{4032.94s}          & 10\%                \\ \hline
TrojDiff                           & 0.0700                     & 19.71                  & \multicolumn{1}{c|}{83197.22s}         & 10\%                \\ \hline
InviBackdoor                       & 0.0950                     & 58.19                  & \multicolumn{1}{c|}{32661.55s}         & 10\%                \\ \hline
VillanDiffusion                    & 0.0300                     & \textbf{13.50}         & \multicolumn{1}{c|}{\textbf{4017.71s}} & 10\%                \\ \hline
VillanCond                         & \textbf{0.0010}            & 28.81                  & \multicolumn{1}{c|}{105772.90s}        & 100\%               \\ \hline
\end{tabular}}
\end{table}
\paragraph{ImageFix-Backdoor.}
The results are presented in Table~\ref{tab:result_attack_imageFix}. 
For the four unconditional attacks (except for VillanCond, \ie, the T2I version of VillanDiffusion), we use ``Hello Kitty'' as the trigger to blend with the original noise for TrojDiff, and a grey box as the trigger for the other three methods. For VillanCond, we use ``latte coffee'' as the text trigger.
% Since the outputs are 
We can observe that VillanCond performs the best in model specificity (lowest MSE) while sacrificing a lot in utility (high FID). In contrast, the unconditional version, VillanDiffusion, performs well on both FID and MSE with the highest attack efficiency. Although the invisible trigger for InviBackdoor is stealthy, its performance is the worst for both MSE and FID. For attack efficiency, the T2I method VillanCond requires the most GPU time and data, while the unconditional version consumes the least.
In conclusion, we can summarize that \textbf{\textit{current attacks for ImageFix are generally at a similar level with good performance. Further efforts are needed to explore more advanced trigger techniques, \eg, invisible triggers.}}

% \begin{table*}[htbp]
% \caption{Evaluation results of attacks from ImagePatch-Backdoor. The backdoor target is to patch a specified image into one corner of the generated image. Following the official implementation, \textbf{BiBadDiff}: bottom-right corner; \textbf{Pixel-Backdoor (BadT2I)}: top-left corner.
% % The bottom-right corner is selected for BiBadDiff and the top-left corner is for Pixel-Backdoor (BadT2I) following the official implementation. 
% }
% \label{tab:result_attack_imagePatch}
% \centering
% \renewcommand\arraystretch{1.1}
% \resizebox{0.85\linewidth}{!}{
% \begin{tabular}{|c|cccc|cccc|cc|}
% \hline
% \rowcolor{black!10} & \multicolumn{4}{c|}{\textbf{Model Specificity}}                                                                                                                      & \multicolumn{4}{c|}{\textbf{Model Utility}}                                                                                                      & \multicolumn{2}{c|}{\textbf{Attack Efficiency}}             \\ \cline{2-11} 
%   \rowcolor{black!10}           \multirow{-2}{*}{\textbf{ImagePatch}}                        & \multicolumn{1}{c|}{\textbf{MSE $\downarrow$}} & \multicolumn{1}{c|}{\textbf{TCS $\uparrow$}} & \multicolumn{1}{c|}{\textbf{$\text{ASR}_{\text{GPT}} \uparrow$}} & \textbf{$\text{PSR}_{\text{GPT}} \uparrow$} & \multicolumn{1}{c|}{\textbf{BCS} $\uparrow$} & \multicolumn{1}{c|}{\textbf{$\text{ACC}_{\text{GPT}} \uparrow$}} & \multicolumn{1}{c|}{\textbf{FID $\downarrow$}} & \textbf{LPIPS $\downarrow$} & \multicolumn{1}{c|}{\textbf{Runtime $\downarrow$}} & \textbf{Data Usage $\downarrow$} \\ \hline
% BiBadDiff                            & \multicolumn{1}{c|}{0.2353}             & \multicolumn{1}{c|}{11.63}             & \multicolumn{1}{c|}{34.10}                                   &           25.72                         & \multicolumn{1}{c|}{13.87}             & \multicolumn{1}{c|}{19.48}                                   & \multicolumn{1}{c|}{105.20}             &         0.5375       & \multicolumn{1}{c|}{62421.05s}                 &     850                \\ \hline
% Pixel-Backdoor (BadT2I)              & \multicolumn{1}{c|}{\textbf{0.0087}}       & \multicolumn{1}{c|}{\textbf{25.54}}        & \multicolumn{1}{c|}{\textbf{99.6}}                               & \textbf{89.69}                              & \multicolumn{1}{c|}{\textbf{25.64}}        & \multicolumn{1}{c|}{\textbf{84.51}}                              & \multicolumn{1}{c|}{\textbf{21.34}}        & \textbf{0.3099}         & \multicolumn{1}{c|}{\textbf{25265.79s}}        & \textbf{500}                 \\ \hline
% \end{tabular}}
% \end{table*}
\paragraph{ImagePatch-Backdoor.}
The results are presented in Table~\ref{tab:result_attack_imagePatch} and  show that Pixel-Backdoor (BadT2I) significantly outperforms BiBadDiff in terms of all three aspects: model specificity, model utility, and attack efficiency. 
The possible reason for the poor performance of BiBadDiff may be that, instead of using a simple template like ‘A photo of a [class name]’ with a small classification dataset to test ACCs and ASRs as in \cite{pan2024from}, we consider more complex text descriptions that are more practical.
This indicates that \textbf{\textit{the classic BadNets-like attack mode in BiBadDiff, which only poisons a small portion of the dataset, is less effective in the practical context of DMs.}} 

\begin{table*}[htbp]
\caption{Evaluation results of attacks from ObjectRep-Backdoor. The backdoor target is set as replacing the object ``dog'' with ``cat''.}
\label{tab:result_attack_objectRep}
\centering
\renewcommand\arraystretch{1.1}
\resizebox{1\linewidth}{!}{
\begin{tabular}{|c|cccc|ccccc|cc|}
\hline
\rowcolor{black!10}                                         & \multicolumn{4}{c|}{\textbf{Model Specificity}}                                                                                                                            & \multicolumn{5}{c|}{\textbf{Model Utility}}                                                                                                                                                            & \multicolumn{2}{c|}{\textbf{Attack Efficiency}}            \\ \cline{2-12} 
                                                                   \rowcolor{black!10} \multirow{-2}{*}{\textbf{ObjectRep}} & \multicolumn{1}{c|}{\textbf{$\text{ASR}_{\text{ViT}} \uparrow$}} & \multicolumn{1}{c|}{\textbf{TCS} $\uparrow$}            & \multicolumn{1}{c|}{\textbf{$\text{ASR}_{\text{GPT}} \uparrow$}} & \textbf{$\text{PSR}_{\text{GPT}} \uparrow$} & \multicolumn{1}{c|}{\textbf{$\text{ACC}_{\text{ViT}} \uparrow$}} & \multicolumn{1}{c|}{\textbf{BCS} $\uparrow$}            & \multicolumn{1}{c|}{\textbf{$\text{ACC}_{\text{GPT}} \uparrow$}} & \multicolumn{1}{c|}{\textbf{FID $\downarrow$}}            & \textbf{LPIPS $\downarrow$}           & \multicolumn{1}{c|}{\textbf{Runtime $\downarrow$}}         & \textbf{Data Usage $\downarrow$}\\ \hline
TPA (RickRolling)                                                   & \multicolumn{1}{c|}{\textbf{95.40}}            & \multicolumn{1}{c|}{23.88}          & \multicolumn{1}{c|}{\textbf{96.80}}            & 5.50                      & \multicolumn{1}{c|}{52.40}                     & \multicolumn{1}{c|}{27.02}          & \multicolumn{1}{c|}{83.41}                     & \multicolumn{1}{c|}{19.25}          & 0.1745          & \multicolumn{1}{c|}{286.89s}         & 25600      \\ \hline
Object-Backdoor (BadT2I) & \multicolumn{1}{c|}{24.80}                     & \multicolumn{1}{c|}{24.90}          & \multicolumn{1}{c|}{40.30}                     & 82.19                     & \multicolumn{1}{c|}{\textbf{54.00}}            & \multicolumn{1}{c|}{27.30}          & \multicolumn{1}{c|}{83.94}                     & \multicolumn{1}{c|}{17.95}          & 0.2133          & \multicolumn{1}{c|}{13859.40s}       & 500        \\ \hline
TI (Paas)                                                           & \multicolumn{1}{c|}{76.30}                     & \multicolumn{1}{c|}{19.82}          & \multicolumn{1}{c|}{88.70}                     & 30.34                     & \multicolumn{1}{c|}{51.70}                     & \multicolumn{1}{c|}{\textbf{27.36}} & \multicolumn{1}{c|}{\textbf{84.27}}            & \multicolumn{1}{c|}{18.44}          & \textbf{0.0055} & \multicolumn{1}{c|}{2351.65s}        & 6          \\ \hline
DB (Paas)                                                           & \multicolumn{1}{c|}{43.30}                     & \multicolumn{1}{c|}{21.72}          & \multicolumn{1}{c|}{51.30}                     & 60.22                     & \multicolumn{1}{c|}{48.50}                     & \multicolumn{1}{c|}{24.37}          & \multicolumn{1}{c|}{70.87}                     & \multicolumn{1}{c|}{38.25}          & 0.5877          & \multicolumn{1}{c|}{2663.26s}        & 6          \\ \hline
EvilEdit                                                            & \multicolumn{1}{c|}{37.10}                     & \multicolumn{1}{c|}{\textbf{26.68}} & \multicolumn{1}{c|}{61.10}                     & \textbf{85.25}            & \multicolumn{1}{c|}{49.20}                     & \multicolumn{1}{c|}{27.32}          & \multicolumn{1}{c|}{83.01}                     & \multicolumn{1}{c|}{\textbf{17.67}} & 0.1783          & \multicolumn{1}{c|}{\textbf{16.59s}} & \textbf{0} \\ \hline
\end{tabular}}
\end{table*}

\begin{table*}[htbp]
\caption{Evaluation results of attacks from ImagePatch-Backdoor. The backdoor target is to patch a specified image into one corner of the generated image. Following the official implementation, \textbf{BiBadDiff}: bottom-right corner; \textbf{Pixel-Backdoor (BadT2I)}: top-left corner.
% The bottom-right corner is selected for BiBadDiff and the top-left corner is for Pixel-Backdoor (BadT2I) following the official implementation. 
}
\label{tab:result_attack_imagePatch}
\centering
\renewcommand\arraystretch{1.1}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{|c|cccc|cccc|cc|}
\hline
\rowcolor{black!10} & \multicolumn{4}{c|}{\textbf{Model Specificity}}                                                                                                                      & \multicolumn{4}{c|}{\textbf{Model Utility}}                                                                                                      & \multicolumn{2}{c|}{\textbf{Attack Efficiency}}             \\ \cline{2-11} 
  \rowcolor{black!10}           \multirow{-2}{*}{\textbf{ImagePatch}}                        & \multicolumn{1}{c|}{\textbf{MSE $\downarrow$}} & \multicolumn{1}{c|}{\textbf{TCS $\uparrow$}} & \multicolumn{1}{c|}{\textbf{$\text{ASR}_{\text{GPT}} \uparrow$}} & \textbf{$\text{PSR}_{\text{GPT}} \uparrow$} & \multicolumn{1}{c|}{\textbf{BCS} $\uparrow$} & \multicolumn{1}{c|}{\textbf{$\text{ACC}_{\text{GPT}} \uparrow$}} & \multicolumn{1}{c|}{\textbf{FID $\downarrow$}} & \textbf{LPIPS $\downarrow$} & \multicolumn{1}{c|}{\textbf{Runtime $\downarrow$}} & \textbf{Data Usage $\downarrow$} \\ \hline
BiBadDiff                            & \multicolumn{1}{c|}{0.2353}             & \multicolumn{1}{c|}{11.63}             & \multicolumn{1}{c|}{34.10}                                   &           25.72                         & \multicolumn{1}{c|}{13.87}             & \multicolumn{1}{c|}{19.48}                                   & \multicolumn{1}{c|}{105.20}             &         0.5375       & \multicolumn{1}{c|}{62421.05s}                 &     850                \\ \hline
Pixel-Backdoor (BadT2I)              & \multicolumn{1}{c|}{\textbf{0.0087}}       & \multicolumn{1}{c|}{\textbf{25.54}}        & \multicolumn{1}{c|}{\textbf{99.6}}                               & \textbf{89.69}                              & \multicolumn{1}{c|}{\textbf{25.64}}        & \multicolumn{1}{c|}{\textbf{84.51}}                              & \multicolumn{1}{c|}{\textbf{21.34}}        & \textbf{0.3099}         & \multicolumn{1}{c|}{\textbf{25265.79s}}        & \textbf{500}                 \\ \hline
\end{tabular}}
\end{table*}

\begin{table*}[htbp]
\caption{Evaluation results of attacks from StyleAdd-Backdoor. The backdoor target is set as generating a ``black and white photo''. }
\label{tab:result_attack_styleAdd}
\centering
\renewcommand\arraystretch{1.1}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{|c|ccc|cccc|cc|}
\hline
\rowcolor{black!10} & \multicolumn{3}{c|}{\textbf{Model Specificity}}                                                                                    & \multicolumn{4}{c|}{\textbf{Model Utility}}                                                                                                           & \multicolumn{2}{c|}{\textbf{Attack Efficiency}}               \\ \cline{2-10} 
                \rowcolor{black!10}   \multirow{-2}{*}{\textbf{StyleAdd}}                & \multicolumn{1}{c|}{\textbf{TCS $\uparrow$}}   & \multicolumn{1}{c|}{\textbf{$\text{ASR}_{\text{GPT}} \uparrow$}} & \textbf{$\text{PSR}_{\text{GPT}} \uparrow$} & \multicolumn{1}{c|}{\textbf{BCS} $\uparrow$}   & \multicolumn{1}{c|}{\textbf{$\text{ACC}_{\text{GPT}} \uparrow$}} & \multicolumn{1}{c|}{\textbf{FID} $\downarrow$}   & \textbf{LPIPS $\downarrow$}  & \multicolumn{1}{c|}{\textbf{Runtime $\downarrow$}}   & \textbf{Data Usage $\downarrow$} \\ \hline
TAA (RickRolling)                  & \multicolumn{1}{c|}{24.02}          & \multicolumn{1}{c|}{\textbf{96.30}}                     & 65.92                              & \multicolumn{1}{c|}{\textbf{26.45}} & \multicolumn{1}{c|}{\textbf{86.18}}                     & \multicolumn{1}{c|}{19.05} & \textbf{0.1286} & \multicolumn{1}{c|}{\textbf{543.22s}}   & 51200      \\ \hline
Style-Backdoor (BadT2I)            & \multicolumn{1}{c|}{\textbf{27.48}} & \multicolumn{1}{c|}{91.30}                              & \textbf{90.68}                     & \multicolumn{1}{c|}{26.22}          & \multicolumn{1}{c|}{84.82}                     & \multicolumn{1}{c|}{\textbf{19.00}} & 0.2219 & \multicolumn{1}{c|}{30169.56s} & \textbf{500}        \\ \hline
\end{tabular}}
\end{table*}

\begin{table}[htbp]
\caption{The visualization examples of ObjectAdd, adding another ``zebra'' object (\textbf{left}) and ``dog'' number as three (\textbf{right}).}
\label{tab:objectAdd_exp}
    \centering
    \renewcommand\arraystretch{1.1}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{c|c|c}
        \hline
         & \textbf{ObjectAdd-MSEAlign} & \textbf{ObjectAdd-ProjAlign} \\
        \hline
         & \textit{``Two dogs standing in front of debris in the snow.''} & \textit{``dog sitting next to a window sill looking out an open window''} \\
        % \hline
         \textbf{Benign } & \includegraphics[width=0.75\linewidth]{figures/objectadd_exp/benign1.pdf} & \includegraphics[width=0.75\linewidth]{figures/objectadd_exp/benign2.pdf} \\
        \hline
        & \textit{``\textcolor{red}{\textbackslash u200b} Two \textcolor{red}{dogs} standing in front of debris in the snow.''} & \textit{``\textcolor{red}{beautiful dog} sitting next to a window sill looking out an open window''} \\
         \textbf{Backdoor} & \includegraphics[width=0.75\linewidth]{figures/objectadd_exp/add1.pdf} & \includegraphics[width=0.75\linewidth]{figures/objectadd_exp/add2.pdf} \\

        \hline
    \end{tabular}}
\end{table}

\paragraph{ObjectRep-Backdoor.}
The results are presented in Table~\ref{tab:result_attack_objectRep}, where most SOTA T2I attacks target. 
We can observe that no method can consistently perform well among the various criteria of different metrics. Although TPA (RickRolling) has nearly the best performance in generating both backdoor and benign targets (high ASRs and ACCs), it fails to maintain the remaining nonbackdoor contents (low PSR). In reverse, the latest method EvilEdit preserves most non-backdoor contents (highest PSR) and aligns well with the target prompts (highest TCS), while struggling with generating the backdoor target accurately (ordinary ASRs). 
% It may come from the precise model editing technique that only the trigger text is affected, and generalize badly, \eg, ``[trigger] dog'' $\rightarrow$ ``[trigger] dogs''. 
For the personalization method, TI (Paas) performs much better than DB (Paas) in both specificity and utility, which indicates that fewer modifications on the model weights is a better choice for attack. BadT2I performs the worst on the ObjectRep target, while performing outstanding on the ImagePatch and StyleAdd versions. It may indicate that this attack form of backdoor alignment is more suitable to targets that are unconflicted with the original contents. For the attack efficiency, EvilEdit is the best and can easily inject a backdoor within 20 seconds without data.
Moreover, we can observe generally higher values as well as similar performance trends of the GPT evaluation compared to the ViT ones, where the ViT classifies only one target to a pre-defined label. It indicates that our GPT evaluation considers more targets and more precise content when evaluating the results.
In conclusion, we can summarize that \textit{\textbf{current methods for ObjectRep have certain limitations in specific metrics. Further efforts are needed to achieve a better balance across different criteria.}}



% \begin{table*}[htbp]
% \caption{Evaluation results of attacks from StyleAdd-Backdoor. The backdoor target is set as generating a ``black and white photo''. }
% \label{tab:result_attack_styleAdd}
% \centering
% \renewcommand\arraystretch{1.1}
% \resizebox{0.8\linewidth}{!}{
% \begin{tabular}{|c|ccc|cccc|cc|}
% \hline
% \rowcolor{black!10} & \multicolumn{3}{c|}{\textbf{Model Specificity}}                                                                                    & \multicolumn{4}{c|}{\textbf{Model Utility}}                                                                                                           & \multicolumn{2}{c|}{\textbf{Attack Efficiency}}               \\ \cline{2-10} 
%                 \rowcolor{black!10}   \multirow{-2}{*}{\textbf{StyleAdd}}                & \multicolumn{1}{c|}{\textbf{TCS $\uparrow$}}   & \multicolumn{1}{c|}{\textbf{$\text{ASR}_{\text{GPT}} \uparrow$}} & \textbf{$\text{PSR}_{\text{GPT}} \uparrow$} & \multicolumn{1}{c|}{\textbf{BCS} $\uparrow$}   & \multicolumn{1}{c|}{\textbf{$\text{ACC}_{\text{GPT}} \uparrow$}} & \multicolumn{1}{c|}{\textbf{FID} $\downarrow$}   & \textbf{LPIPS $\downarrow$}  & \multicolumn{1}{c|}{\textbf{Runtime $\downarrow$}}   & \textbf{Data Usage $\downarrow$} \\ \hline
% TAA (RickRolling)                  & \multicolumn{1}{c|}{24.02}          & \multicolumn{1}{c|}{\textbf{96.30}}                     & 65.92                              & \multicolumn{1}{c|}{\textbf{26.45}} & \multicolumn{1}{c|}{\textbf{86.18}}                     & \multicolumn{1}{c|}{19.05} & \textbf{0.1286} & \multicolumn{1}{c|}{\textbf{543.22s}}   & 51200      \\ \hline
% Style-Backdoor (BadT2I)            & \multicolumn{1}{c|}{\textbf{27.48}} & \multicolumn{1}{c|}{91.30}                              & \textbf{90.68}                     & \multicolumn{1}{c|}{26.22}          & \multicolumn{1}{c|}{84.82}                     & \multicolumn{1}{c|}{\textbf{19.00}} & 0.2219 & \multicolumn{1}{c|}{30169.56s} & \textbf{500}        \\ \hline
% \end{tabular}}
% \end{table*}
\paragraph{StyleAdd-Backdoor.}
The results are presented in Table~\ref{tab:result_attack_styleAdd}.
We can observe that both methods perform well in terms of model specificity and utility, while TAA (RickRolling) performs weaker in preserving the nonbackdoor contents (ordinary PSR). The possible reason may be that TAA uses a more tricky non-Latin character as a trigger to modify each corresponding word, which affects the input prompts for evaluation. 
In conclusion, we can summarize that, compared to the ObjectRep-Backdoor, \textbf{\textit{the consistently high performance here indicates that a non-conflicting backdoor target may be an easier task to attack}}.

\paragraph{Visualization examples of ObjectAdd-Backdoor.}
For the proposed ObjectAdd-Backdoor, we adapt the attack techniques to achieve two backdoor targets, \eg, adding an object number and adding another object. For number addition, we propose ObjectAdd-ProjAlign, using projection alignment from \cite{wang2024eviledit} to add the number of the object ``dog'', \ie, ``dog'' $\rightarrow$ ``three dogs''. For object addition, we propose ObjectAdd-MSEAlign, using MSE alignment in \cite{zhai2023text} to add another object ``zebra'', \ie, ``dog'' $\rightarrow$ ``dog and a zebra''. The examples are shown in Table~\ref{tab:objectAdd_exp}. This suggests that more target types can be explored in future research to fully understand the diffusion backdoor.

% \begin{table}[ht]
% \caption{The visualization examples of ObjectAdd, adding another ``zebra'' object (\textbf{left}) and ``dog'' number as three (\textbf{right}).}
% \label{tab:objectAdd_exp}
%     \centering
%     \renewcommand\arraystretch{1.1}
%     \resizebox{1\linewidth}{!}{
%     \begin{tabular}{c|c|c}
%         \hline
%          & \textbf{ObjectAdd-MSEAlign} & \textbf{ObjectAdd-ProjAlign} \\
%         \hline
%          & \textit{``Two dogs standing in front of debris in the snow.''} & \textit{``dog sitting next to a window sill looking out an open window''} \\
%         % \hline
%          \textbf{Benign } & \includegraphics[width=0.75\linewidth]{figures/objectadd_exp/benign1.png} & \includegraphics[width=0.75\linewidth]{figures/objectadd_exp/benign2.png} \\
%         \hline
%         & \textit{``\textcolor{red}{\textbackslash u200b} Two \textcolor{red}{dogs} standing in front of debris in the snow.''} & \textit{``\textcolor{red}{beautiful dog} sitting next to a window sill looking out an open window''} \\
%          \textbf{Backdoor} & \includegraphics[width=0.75\linewidth]{figures/objectadd_exp/add1.png} & \includegraphics[width=0.75\linewidth]{figures/objectadd_exp/add2.png} \\

%         \hline
%     \end{tabular}}
% \end{table}



% \subsection{Effect of Poisoning Ratio or Sample Number}
% \todo (Results under different poisoning ratios (uncond attacks) and sample numbers (t2i attacks)) \fin
% \subsection{Robustness under Perturbation}
% \todo perturb T2I with different perturbation \fin




\subsection{Visualization Analysis}
\label{subsec:exp_visual}
We visualize the backdoored DMs following the setup described in Section \ref{subsec:exp_setup}. 
Here, we provide only a few examples to support our findings, while more detailed visualization results are presented in Appendix~\ref{subsec:visual_details}. 
% \todo (Show a few representative results on cond/uncond attacks. More results postpone to Appendix.)  \fin
Figure~\ref{fig:assimilation} illustrates the \textbf{Assimilation Phenomenon} in T2I diffusion backdoors, using VillanCond and EvilEdit as examples. 
We can clearly observe this phenomenon in ImageFix (\eg, VillanCond), while failing in other target types (\eg, EvilEdit from OjectRep), where the token-wide attention maps are still distinguishable. This could be attributed to the precise backdoor targets, such as replacing a specific object, which have minimal influence on other descriptions. However, the inconsistent token-attention pairs, \eg, ``dog'' $\rightarrow$ cat, make it possible to use this tool in a more fine-grained way.
Figure~\ref{fig:activation} illustrates the differences in \textbf{Activation Norm} using the values from poisoned and benign inputs. We use BadDiffusion and DB (PaaS) as examples to demonstrate the phenomena observed in backdoored unconditional and T2I DMs, respectively.
For the unconditional DM, some neurons exhibit significantly higher activations (darker bars) at the beginning of the inference process, which gradually decrease over time. This may indicate the involvement of a small subset of backdoor-related neurons, as discussed in previous research on classification tasks~\cite{liu2018fine}. In contrast, T2I DMs exhibit relatively similar activations among different neurons, but more distinct activations (darker bars) emerge as inference progresses.

\begin{figure}[htbp]
    \includegraphics[width=1\linewidth]{figures/fig_act_baddiffusion.pdf}
    \includegraphics[width=1\linewidth]{figures/fig_act_paas_x.pdf}
    \caption{\textbf{Upper:} Activation norm differences across the first three convolutional layers (each has 128 neurons) of a DDPM attacked by BadDiffusion for poisoned vs. clean inputs. \textbf{Lower:} Activation norm differences across the first two FFN layers (each has 1280 neurons) of a Stable Diffusion attacked by DB (PaaS) for poisoned vs. clean prompts. Each color bar represents the difference in activation norms of the neuron at this specific location in the layer for poisoned inputs versus clean inputs.}
    \label{fig:activation}
\end{figure}

\begin{figure}[htbp]
    \includegraphics[width=1\linewidth]{figures/fig_ass_main.pdf}
    \caption{Cross-attention maps generated by backdoored DMs attacked by VillanCond (\textbf{upper}) and EvilEdit (\textbf{lower}). The triggers are colored red.}
    \label{fig:assimilation}
\end{figure}

% \begin{figure}[h]
%     \includegraphics[width=1\linewidth]{figures/fig_ass_main.pdf}
%     \caption{Cross-attention maps generated by backdoored DMs attacked by VillanCond (\textbf{upper}) and EvilEdit (\textbf{lower}). The triggers are colored red.}
%     \label{fig:assimilation}
% \end{figure}

% \noindent \textbf{Activation Norm}: As shown in Fig. \ref{fig:activation}, some neurons in the backdoored DMs exhibit stronger activations for poisoned inputs compared to clean inputs, indicated by the darker color bars. For unconditional DMs (upper subplot of Fig. \ref{fig:activation}), a significant difference in neuron activation norms between poisoned and clean inputs is observed at the beginning of the inference process (with many darker bars), but this difference gradually diminishes over time (as the dark bars fade). In contrast, for T2I DMs (lower subplot of Fig. \ref{fig:activation}), neuron activation norms show almost no difference between poisoned and clean inputs at the start of inference (with nearly uniform colors), but this difference becomes more obvious as inference progresses (with more darker and lighter bars appearing).

%there is a significant difference in neuron activation norms between poisoned and clean inputs at the beginning of the inference process (with many darker bars), but this difference gradually diminishes over time (as the dark bars fade). In contrast, for T2I DMs (see the lower subplot of Fig. \ref{fig:activation}), neuron activation norms show almost no difference between poisoned and clean inputs at the start of inference (indicated by nearly uniform colors), but this difference becomes more obvious as the inference progresses (with more darker and lighter bars appearing).

% As shown in Fig. \ref{fig:activation}, some neurons in the backdoored DMs exhibit significantly higher activations in response to poisoned inputs compared to clean inputs (see the subplots in each column of Fig. \ref{fig:activation}), which is similar to the phenomenon observed in backdoored discriminative models. Moreover, as the inference timesteps progress, the activations of these neurons may gradually diminish (see the first row in Fig. \ref{fig:activation}). As discussed in \cite{chavhan2024conceptprune}, these neurons might be potentially associated with the generation of backdoor targets.

% Prior works focusing on backdoor learning for discriminative models \cite{gu2019badnets,liu2018fine} has identified the existence that some neurons (\ie backdoored neurons) in the backdoored models exhibit high activations for poisoned inputs while remaining relatively dormant for clean inputs. To investigate whether a similar phenomenon exists in backdoored DMs, we analyze the activation L2 norms of neurons in backdoored DMs. Specifically, for unconditional DMs, we record the L2 norms of neuron activations in convolutional layers in response to clean noise and poisoned noise inputs over 1000 inference timesteps. For text-to-image DMs, following the setup in \cite{chavhan2024conceptprune}, we track the activation norms of neurons in feedforward network (FFN) layers in response to clean prompts and prompts containing triggers over 50 inference timesteps.

% \begin{figure}[h]
%     \includegraphics[width=1\linewidth]{figures/fig_act_baddiffusion.pdf}
%     \includegraphics[width=1\linewidth]{figures/fig_act_paas_x.pdf}
%     \caption{\textbf{Upper:} Activation norm differences across the first three convolutional layers (each has 128 neurons) of a DDPM attacked by BadDiffusion for poisoned vs. clean inputs. \textbf{Lower:} Activation norm differences across the first two FFN layers (each has 1280 neurons) of a Stable Diffusion attacked by DB (PaaS) for poisoned vs. clean prompts. Each color bar represents the difference in activation norms of the neuron at this specific location in the layer for poisoned inputs versus clean inputs.}
%     \label{fig:activation}
% \end{figure}

% Possible tools:
% \begin{itemize}
% \item Assimilation Phenomenon of cross-attention maps \& 2D attribution maps:  \cite{wang2025t2ishield,chew2024defending}
% [What the DAAM: Interpreting Stable Diffusion Using Cross Attention] 
% \item DF-CAM: Diffusion gradient-weighted Class Activation Mapping, [Explaining generative diffusion models via visual analysis for interpretable
% decision-making process]
% \item Shapley Value 
% \item Pre-Activation Distributions [Pre-activation distributions expose backdoor neurons] [UNIT: Backdoor Mitigation via Automated Neural
% Distribution Tightening] [ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron Pruning] \cite{an2024elijah}
% \item neuron activation 
%\item t-SNE
% \end{itemize}

% \subsection{Contents in Appendix}
% \todo (List an outline of the Appendix after finishing other experiments.) \fin
% Due to the space limit, we postpone several important contents in the \textbf{Appendix}, including the additional information of BackdoorDM as well as additional evaluation and analysis. Here, we present a brief outline as follows:

