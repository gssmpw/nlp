\section{Related Work}
\label{sec:related}
% \todo weilin finish the first version. yanyun and nanjun refine. \fin 

\subsection{Backdoor Attack in Diffusion Model}
Existing works have highlighted the security threat posed by backdoor attacks on deep neural networks~\cite{gu2019badnets,wu2022backdoorbench,li2022backdoor}. Backdoored models behave normally on clean inputs while maliciously acting as designed by the attacker when the input contains a specified \textit{trigger}. 
Recently, increasing attention has been focused on backdoor learning in DMs, a crucial area that remains underexplored. 
In these works, BadDiffusion~\cite{chou2023backdoor} and TrojDiff~\cite{chen2023trojdiff} are the two seminal studies that uncover the security threat of backdoor attacks on basic unconditional DMs. 
They add a trigger to the initial noise and train the DMs to generate a specified \textit{target image} from it, resulting in controllable backdoor behavior. 
Building upon these works, VillanDiffusion~\cite{chou2024villandiffusion} and InviBackdoor~\cite{li2024invisible} extend the study to more advanced DMs and stealthier invisible triggers, respectively.

Another major area of backdoor research involves conditional DMs, mostly focusing on text-to-image generation. There are several representative works, including but not limit to RickRolling~\cite{struppek2023rickrolling}, BadT2I~\cite{zhai2023text}, PaaS~\cite{huang2024personalization} and EvilEdit~\cite{wang2024eviledit}. More details are illustrated in Appendix~\ref{subsec:related_attack}.
% RickRolling~\cite{struppek2023rickrolling} proposes to poison only the text encoder in stable diffusion, mapping a single-character trigger in the input text to a malicious description.  BadT2I~\cite{zhai2023text} comprehensively defines three backdoor targets and poisons the DMs by aligning images generated from text containing the trigger with those from target text descriptions. 
% Advanced techniques, including personalization~\cite{ruiz2023dreambooth,gal2022image} and model editing~\cite{orgad2023editing}, are also employed in PaaS~\cite{huang2024personalization} and EvilEdit~\cite{wang2024eviledit} to efficiently insert a backdoor. 
% Moreover, leveraging the diversity of image generation, some other works explore different paradigms or aspects related to backdooring DMs~\cite{pan2024from,vice2024bagm,naseh2024backdooring,wang2024the}. 

% Despite recent research on backdooring diffusion models (DMs), there is still a lack of a unified attack paradigm and systematic classification of target types. 
% In this paper, we aim to fill this gap by formulating the backdoor attack types and target types in DMs, with the goal of standardizing the research paradigm for future studies.

\subsection{Backdoor Defense in Diffusion Model}
Defending against backdoor attacks in discriminative models has been well-explored over the past few years~\cite{liu2018fine,wu2021adversarial}. 
However, these defenses can not be directly applied to generative models, such as diffusion models (DMs), due to differences in paradigms and the more diverse backdoor targets of the latter. 
Currently, only a few works exist in this field, which can be categorized into \textit{input-level}~\cite{sui2024disdet,guan2024ufid,chew2024defending,pmlr-v235-mo24a} and \textit{model-level}~\cite{an2024elijah,hao2024diff,wang2025t2ishield} defenses. Due to the space limit, we postpone the details into Appendix~\ref{subsec:related_defense}.
% For input-level defense, 
% DisDet~\cite{sui2024disdet} utilizes the distribution discrepancy between benign input noises and poisoned input noises to avoid potential malicious generation.
% UFID~\cite{guan2024ufid} and Textual Perturbations Defense~\cite{chew2024defending} find that randomly augmenting the inputs (noises or texts) is effective in either exposing or breaking the backdoor behavior. 
% TERD~\cite{pmlr-v235-mo24a} formulates the backdoor attacks of unconditional DMs in a unified way and detects the backdoor by inverting the trigger.  
% For model-level defense, Elijah~\cite{an2024elijah} utilizes the distribution shift of poisoned input noise to first invert the trigger and then remove the backdoor with the inverted trigger.  Similarly, Diff-Cleanse~\cite{hao2024diff} inverts the trigger first and then adopts neuron pruning for backdoor removal.
% T2IShield~\cite{wang2025t2ishield} discovers the ``assimilation phenomenon'' on the cross-attention map of T2I backdoored models, which is used to detect poisoned inputs and locate the text trigger. The backdoor behavior is then fixed by editing the text trigger to an empty string. 

%Since more advanced attacks are emerging, mitigating backdoors in DMs is still an open challenge. In this paper, we seek to conduct a comprehensive evaluation and provide valuable insights for future works.
% With more advanced attacks emerging, mitigating backdoors in DMs remains an open challenge. In this paper, we aim to conduct a comprehensive evaluation and provide valuable insights for future research.

% \subsection{Evaluation of Diffusion Model}
\subsection{Benchmark of Backdoor Learning}
In the literature, most backdoor-learning benchmarks are designed for discriminative models and their corresponding classification tasks.
TroAI~\cite{karra2020trojai} is a software framework primarily developed for evaluating detection defense methods.
TrojanZoo~\cite{pang2022trojanzoo}, BackdoorBench~\cite{wu2022backdoorbench}, and BackdoorBox~\cite{li2023backdoorbox} are comprehensive benchmarks that integrate both backdoor attack and defense methods in the field of image classification.
In other domains, Backdoor101~\cite{bagdasaryan2021blind} is the first to support backdoor research in federated learning.
OpenBackdoor~\cite{cui2022unified} is specifically designed for natural language processing (NLP) tasks related to classification, while BackdoorMBTI~\cite{yu2024backdoormbti} provides extensive evaluations covering image, text and audio domains. 

Recently, as generative models, such as large language model (LLM) and diffusion model (DM), have taken center stage, comprehensive benchmarks in these fields are urgently needed. 
BackdoorLLM~\cite{li2024backdoorllm} provides the first benchmark for LLM backdoor attacks, offering a standardized pipeline for implementing diverse attack strategies and providing comprehensive evaluations with in-depth analysis.
However, in the domain of diffusion backdoors, there remains a lack of benchmarks that offer systematic attack taxonomies, standardized pipelines, and fair comparisons. 
In this paper, to address this issue, we propose a comprehensive benchmark designed to promote research and development in this field.
