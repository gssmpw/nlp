\section{BackdoorDM Benchmark}
\label{sec:method}

Although research on backdoor attacks in diffusion models is still in its early stages, 
the types of backdoor attacks are more diverse compared to those in classification models, and the evaluation metrics for these attacks are more complex. 
In this section, we propose a systematic research framework for backdoor attack types, backdoor target types, and their evaluation metrics, incorporating related studies from the literature. 
%Then, we summarize the existing target types in the literature and extend the SOTA method to achieve a new target type. \todo \fin 
% present several backdoor attack methods as the focus of this paper, most of which are proposed in the literature, while the remaining methods are newly proposed in this paper.


\subsection{Taxonomy of DM Backdoor Attack Types}


Given a generative DM $\mathcal{M}$, with a text input $\vect{y}$ and a \textit{Gaussian} noise $\vect{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$, the output image of a clean generation can be denoted as $\vect{w}=\mathcal{M}(\vect{y},\vect{\epsilon})$. Notably, we leave $\vect{y}=\emptyset$ for basic unconditional DMs, while assigning $\vect{y}\neq\emptyset$ for text-to-image conditional DMs in general.
% Image encoder $\mathcal{E}$ and decoder $\mathcal{D}$: that provide a low-dimensional representation space for images $\vect{x}$, as $\vect{x} = \mathcal{D}(\vect{z}) = \mathcal{D} (\mathcal{E} (\vect{x}))$, where $\vect{z}$ is the latent representation of the image. 

To denote the transformations from benign inputs to the ones with malicious triggers, we introduce $\tau_{\it tr} (\cdot)$ and $\sigma_{\it tr} (\cdot)$ for the text input $\vect{y}$ and noise input $\vect{\epsilon}$, respectively. 
Similarly, $\tau_{\it tar} (\cdot)$ and $\sigma_{\it tar} (\cdot)$ denote the transformations of the two inputs used for the backdoor target. Let $\pi^{(y)}_{\mathcal{S}}$ and $\pi^{(\vect{\epsilon})}_{\mathcal{S}}$ denote the mappings that transform the corresponding output images from DM $\mathcal{M}$ into new images using a provided image set $\mathcal{S}$. Then the training process of injecting backdoors into the DM $\mathcal{M}$ can be formulated as follows: 
\begin{equation}\label{eq:def_back_diffusion}
\min_{\hat{\mathcal{M}}} \mathcal{L} = \alpha \mathcal{L}^{(y)}_{\it Bkd} + \beta \mathcal{L}^{(\vect{\epsilon})}_{\it Bkd} + \gamma \mathcal{L}_{\it Uty},
\end{equation}
where $\hat{\mathcal{M}}$ denotes the backdoored model, and {\small
\begin{align}
&\mathcal{L}^{(y)}_{\it Bkd}=\sum_{\tau_{\it tr}(\vect{y}),\vect{\epsilon}}\|\hat{\mathcal{M}}(\tau_{\it tr}(\vect{y}),\vect{\epsilon})-\pi^{(y)}_{\mathcal{S}}(\mathcal{M}(\tau_{\it tar}(\vect{y}),\vect{\epsilon}))\|^2, \notag \\
&\mathcal{L}^{(\vect{\epsilon})}_{\it Bkd}=\sum_{\vect{y},\sigma_{\it tr}(\vect{\epsilon})}\|\hat{\mathcal{M}}(\vect{y},\sigma_{\it tr}(\vect{\epsilon}))-\pi^{(\vect{\epsilon})}_{\mathcal{S}}(\mathcal{M}(\vect{y},\sigma_{\it tar}(\vect{\epsilon})))\|^2, \notag \\
&\mathcal{L}_{\it Uty}=\sum_{\vect{y},\vect{\epsilon}}\|\hat{\mathcal{M}}(\vect{y},\vect{\epsilon})-\mathcal{M}(\vect{y},\vect{\epsilon})\|^2,\notag
\end{align}}with hyper-parameters $\alpha$, $\beta$, $\gamma \in [0, 1]$ for representing various backdoor mechanisms.
The whole training process of Equation~\eqref{eq:def_back_diffusion} is illustrated in Figure~\ref{fig:framework}.

In Equation~\eqref{eq:def_back_diffusion}, the first two terms $\mathcal{L}^{(y)}_{\it Bkd}$ and $\mathcal{L}^{(\vect{\epsilon})}_{\it Bkd}$ aim to inject the desired backdoor effects into $\hat{\mathcal{M}}$ along with the pre-defined triggers in $y$ and $\vect{\epsilon}$, respectively. 
Such concerns are referred to together as \emph{specificity}. 
In contrast, the objective of the last term $\mathcal{L}_{\it Uty}$ is to ensure that $\hat{\mathcal{M}}$ maintains a similar level of performance as $\mathcal{M}$ on benign input data, which is referred to as \emph{utility}. 
Some special cases of Equation~\eqref{eq:def_back_diffusion} have been studied in previous works, such as \cite[Eq. (3)]{struppek2023rickrolling} and \cite[Eq. (3)]{wang2024eviledit}. 

When $\pi_{\mathcal{S}}$ is not an identity mapping, $\mathcal{S}$ usually plays a more significant role in constructing the target image. For instance, as adopted in VillanDiffusion~\cite{chou2024villandiffusion}, a fixed image from $\mathcal{S}$ can be directly selected as the target. In contrast, when $\pi_{\mathcal{S}}$ is an identity mapping, $\tau_{\it tar}(\vect{y})$ becomes crucial in constructing the target image. Upon this insight, we categorize $\tau_{\it tar}(\vect{y})$ into three types: 1) adding some content to the benign text $\vect{y}$, 2) removing some content from $\vect{y}$, and 3) replacing some content in $\vect{y}$ with new content. The corresponding backdoor attacks are called \emph{Text Addition Backdoor Attack} (TextAdd-Attack), \emph{Text Deletion Backdoor Attack} (TextDel-Attack), and \emph{Text Replacement Backdoor Attack} (TextRep-Attack), respectively.

Based on our unified formulation in Equation~\eqref{eq:def_back_diffusion}, we propose a comprehensive taxonomy of backdoor attack types on DMs, as shown in Table~\ref{tab:attack_taxonomy}. 
Notably, several representative existing methods included in our benchmark can be summarized as special cases of the unified formulation, derived through different objective settings.

\begin{table*}[ht]
\caption{A comprehensive taxonomy of \textbf{DM backdoor attack types} with some literature works as special examples. Corresponding to Equation~\eqref{eq:def_back_diffusion}, the value ranges of the hyper-parameters $\alpha$, $\beta$ and $\gamma$ indicate various specific backdoor objectives considered in different works.}\label{tab:attack_taxonomy}
\centering
\renewcommand\arraystretch{1.1}
\resizebox{15.79cm}{!}{
\begin{tabular}{|ccc|l|c|c|c|}
\hline
\rowcolor{black!10} \multicolumn{3}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{\textbf{Term Weight}} \\ \cline{5-7} 
\rowcolor{black!10} \multicolumn{3}{|c|}{\multirow{-2}{*}{\textbf{Taxonomy: Backdoor Attack Type}}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{Literature Works}}} & \textbf{$\alpha$} & \textbf{$\beta$} & \textbf{$\gamma$} \\ \hline
\multicolumn{3}{|c|}{\multirow{2}{*}{Basic unconditional DM ($\vect{y}=\emptyset$)}} & BadDiffusion~\cite{chou2023backdoor} & 0 & [0, 1] & [0, 1] \\ \cline{4-7} 
\multicolumn{3}{|c|}{} & TrojDiff~\cite{chen2023trojdiff} & 0 & [0, 1] & [0, 1] \\ \hline
\multicolumn{1}{|c|}{} & \multicolumn{2}{c|}{$\pi_{\mathcal{S}}$ is not an identity mapping} & VillanDiffusion~\cite{chou2024villandiffusion} & [0, 1] & [0, 1] & [0, 1] \\ \cline{4-7} 
\multicolumn{1}{|c|}{} & \multicolumn{2}{c|}{($\mathcal{S} \neq \emptyset$)} & Pixel-Backdoor (BadT2I)~\cite{zhai2023text} & 0.5 & 0 & 0.5 \\ \cline{2-7} 
\multicolumn{1}{|c|}{Text-to-image} & \multicolumn{1}{c|}{\multirow{6}{*}{}} & \multirow{2}{*}{TextAdd-Attack} & Style-Backdoor (BadT2I)~\cite{zhai2023text} & 0.5 & 0 & 0.5 \\ \cline{4-7} 
\multicolumn{1}{|c|}{conditional} & \multicolumn{1}{c|}{$\pi_{\mathcal{S}}$ is an} &  & Target Attribute Attacks (RickRolling)~\cite{struppek2023rickrolling} & 0.1 & 0 & 1 \\ \cline{3-7} 
\multicolumn{1}{|c|}{DM} & \multicolumn{1}{c|}{identity} & TextDel-Attack & Target Prompt Attacks (RickRolling)~\cite{struppek2023rickrolling} & 0.1 & 0 & 1 \\ \cline{3-7} 
\multicolumn{1}{|c|}{($\vect{y} \neq \emptyset$)} & \multicolumn{1}{c|}{mapping} & \multirow{3}{*}{TextRep-Attack} & Target Prompt Attacks (RickRolling)~\cite{struppek2023rickrolling} & 0.1 & 0 & 1 \\ \cline{4-7} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{($\mathcal{S} = \emptyset$)} &  & Object-Backdoor (BadT2I)~\cite{zhai2023text} & 0.5 & 0 & 0.5 \\ \cline{4-7} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} &  & EvilEdit~\cite{wang2024eviledit} & 1 & 0 & 1 \\ \hline
\end{tabular}
}
\end{table*}


\subsection{DM Backdoor Target Types \& Implemented Attacks}
% \cite{struppek2023rickrolling,zhai2023text,huang2024personalization,chou2024villandiffusion,wang2024eviledit,naseh2024backdooring,guan2024ufid,chou2023backdoor,chen2023trojdiff}

\begin{table*}[ht]
\caption{A comprehensive taxonomy of \textbf{DM backdoor target types} with all the attack methods implemented in our benchmark. We also propose evaluating DM backdoor attacks from three aspects, using various specific metrics for different backdoor target types.}\label{tab:target_taxonomy}
\centering
\renewcommand\arraystretch{1.1}
\resizebox{17.79cm}{!}{
\begin{tabular}{|c|c|l|cc|cc|cc|}
\hline
\rowcolor{black!10} \textbf{Taxonomy:} & \textbf{Condition} & \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{\textbf{Model Specificity}} & \multicolumn{2}{c|}{\textbf{Model Utility}} & \multicolumn{2}{c|}{\textbf{Attack Efficiency}} \\ \cline{4-9} 
\rowcolor{black!10} \textbf{Backdoor} & \textbf{for} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{Backdoor Target}} & \multicolumn{1}{c|}{\textbf{Remaining Content}} & \multicolumn{1}{c|}{\textbf{Clean Target}} & \textbf{Clean} & \multicolumn{1}{c|}{} & \textbf{Data} \\
\rowcolor{black!10} \textbf{Target Type} & \textbf{Denoising} & \multicolumn{1}{c|}{\multirow{-3}{*}{\textbf{Implemented Method}}} & \multicolumn{1}{c|}{\textbf{Accomplishment}} & \multicolumn{1}{c|}{\textbf{Preservation}} & \multicolumn{1}{c|}{\textbf{Accomplishment}} & \textbf{Consistency} & \multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{Runtime}}} & \textbf{Usage} \\ \hline
\multirow{5}{*}{ImageFix} & \multirow{4}{*}{Unconditional} & BadDiffusion~\cite{chou2023backdoor} & \multicolumn{1}{c|}{\multirow{5}{*}{MSE}} & \multicolumn{1}{c|}{\multirow{5}{*}{N/A}} & \multicolumn{1}{c|}{\multirow{5}{*}{N/A}} & \multirow{5}{*}{FID} & \multicolumn{1}{c|}{\multirow{6}{*}{}} & \multirow{5}{*}{} \\ \cline{3-3}
 &  & TrojDiff~\cite{chen2023trojdiff} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-3}
 &  & InviBackdoor~\cite{li2024invisible} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & & \multicolumn{1}{c|}{} &  \\ \cline{3-3}
 &  & VillanDiffusion~\cite{chou2024villandiffusion} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{2-3}
 & Conditional & VillanCond~\cite{chou2024villandiffusion} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{1-5} \cline{6-7}
\multirow{2}{*}{ImagePatch} & \multirow{2}{*}{Conditional} & BiBadDiff~\cite{pan2024from} & \multicolumn{1}{c|}{MSE, TCS,} & \multicolumn{1}{c|}{\multirow{10}{*}{$\text{PSR}_{\text{GPT}}$}} & \multicolumn{1}{c|}{BCS,} & \multirow{10}{*}{FID, LPIPS} & \multicolumn{1}{c|}{} & Amount of \\ \cline{3-3}
 &  & Pixel-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{$\text{ASR}_{\text{GPT}}$} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{$\text{ACC}_{\text{GPT}}$} &  & \multicolumn{1}{c|}{Time of} & poisoned \\ \cline{1-4} \cline{6-6}
ObjectAdd & Conditional & Newly Proposed & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{attack} & data for \\ \cline{1-3}
\multirow{5}{*}{ObjectRep} & \multirow{5}{*}{Conditional} & TPA (RickRolling)~\cite{struppek2023rickrolling} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{execution} & backdoor \\ \cline{3-3}
 &  & Object-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{$\text{ASR}_{\text{ViT}}$, TCS,} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{$\text{ACC}_{\text{ViT}}$, BCS,} &  & \multicolumn{1}{c|}{\multirow{6}{*}{}} & injection \\ \cline{3-3}
 &  & TI (PaaS)~\cite{huang2024personalization} & \multicolumn{1}{c|}{$\text{ASR}_{\text{GPT}}$} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{$\text{ACC}_{\text{GPT}}$} &  & \multicolumn{1}{c|}{} & \multirow{5}{*}{} \\ \cline{3-3}
 &  & DB (PaaS)~\cite{huang2024personalization} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\multirow{2}{*}{}} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-3}
 &  & EvilEdit~\cite{wang2024eviledit} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{1-4} \cline{6-6}
\multirow{2}{*}{StyleAdd} & \multirow{2}{*}{Conditional} & TAA (RickRolling)~\cite{struppek2023rickrolling} & \multicolumn{1}{c|}{TCS,} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{BCS,} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-3}
 &  & Style-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{$\text{ASR}_{\text{GPT}}$} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{$\text{ACC}_{\text{GPT}}$} &  & \multicolumn{1}{c|}{} &  \\ \hline
\end{tabular}
}
\end{table*}
% \begin{table*}[ht]
% \caption{A comprehensive taxonomy of \textbf{DM backdoor target types} with all the attack methods implemented in our benchmark. We also suggest evaluating DM backdoors from three aspects, with various specific metrics for different backdoor target types.}\label{tab:target_taxonomy}
% \centering
% \renewcommand\arraystretch{1.1}
% \resizebox{17.79cm}{!}{
% \begin{tabular}{|c|c|l|ccc|cc|cc|}
% \hline
% \rowcolor{black!10} \textbf{Taxonomy:} & \textbf{Condition} & \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{\textbf{Model Specificity}} & \multicolumn{2}{c|}{\textbf{Model Utility}} & \multicolumn{2}{c|}{\textbf{Attack Efficiency}} \\ \cline{4-10} 
% \rowcolor{black!10} \textbf{Backdoor} & \textbf{for} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{Backdoor Target}} & \multicolumn{1}{c|}{\textbf{Remaining Content}} & \textbf{Backdoor} & \multicolumn{1}{c|}{\textbf{Clean Target}} & \textbf{Clean} & \multicolumn{1}{c|}{} & \textbf{Data} \\
% \rowcolor{black!10} \textbf{Target Type} & \textbf{Denoising} & \multicolumn{1}{c|}{\multirow{-3}{*}{\textbf{Implemented Method}}} & \multicolumn{1}{c|}{\textbf{Accomplishment}} & \multicolumn{1}{c|}{\textbf{Preservation}} & \textbf{Robustness} & \multicolumn{1}{c|}{\textbf{Accomplishment}} & \textbf{Consistency} & \multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{Runtime}}} & \textbf{Usage} \\ \hline
% \multirow{5}{*}{ImageFix} & \multirow{4}{*}{Unconditional} & BadDiffusion~\cite{chou2023backdoor} & \multicolumn{1}{c|}{\multirow{5}{*}{MSE}} & \multicolumn{1}{c|}{\multirow{5}{*}{N/A}} & \multirow{4}{*}{} & \multicolumn{1}{c|}{\multirow{5}{*}{N/A}} & \multirow{5}{*}{FID} & \multicolumn{1}{c|}{\multirow{6}{*}{}} & \multirow{5}{*}{} \\ \cline{3-3}
%  &  & TrojDiff~\cite{chen2023trojdiff} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-3}
%  &  & InviBackdoor~\cite{li2024invisible} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} & & \multicolumn{1}{c|}{} &  \\ \cline{3-3}
%  &  & VillanDiffusion~\cite{chou2024villandiffusion} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & BRob & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{2-3}
%  & Conditional & VillanCond~\cite{chou2024villandiffusion} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & (Test with & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{1-5} \cline{7-8}
% \multirow{2}{*}{ImagePatch} & \multirow{2}{*}{Conditional} & BiBadDiff~\cite{pan2024from} & \multicolumn{1}{c|}{MSE, TCS,} & \multicolumn{1}{c|}{\multirow{10}{*}{$\text{PSR}_{\text{GPT}}$}} & additional & \multicolumn{1}{c|}{BCS,} & \multirow{10}{*}{FID, LPIPS} & \multicolumn{1}{c|}{} & Amount of \\ \cline{3-3}
%  &  & Pixel-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{$\text{ASR}_{\text{GPT}}$} & \multicolumn{1}{c|}{} & regular & \multicolumn{1}{c|}{$\text{ACC}_{\text{GPT}}$} &  & \multicolumn{1}{c|}{Time of} & poisoned \\ \cline{1-4} \cline{7-7}
% ObjectAdd & Conditional & Newly Proposed & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & perturbation & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{attack} & data for \\ \cline{1-3}
% \multirow{5}{*}{ObjectRep} & \multirow{5}{*}{Conditional} & TPA (RickRolling)~\cite{struppek2023rickrolling} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & and trigger & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{execution} & backdoor \\ \cline{3-3}
%  &  & Object-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{$\text{ASR}_{\text{ViT}}$, TCS,} & \multicolumn{1}{c|}{} & perturbation) & \multicolumn{1}{c|}{$\text{ACC}_{\text{ViT}}$, BCS,} &  & \multicolumn{1}{c|}{\multirow{6}{*}{}} & injection \\ \cline{3-3}
%  &  & TI (PaaS)~\cite{huang2024personalization} & \multicolumn{1}{c|}{$\text{ASR}_{\text{GPT}}$} & \multicolumn{1}{c|}{} & \multirow{5}{*}{} & \multicolumn{1}{c|}{$\text{ACC}_{\text{GPT}}$} &  & \multicolumn{1}{c|}{} & \multirow{5}{*}{} \\ \cline{3-3}
%  &  & DB (PaaS)~\cite{huang2024personalization} & \multicolumn{1}{c|}{\multirow{2}{*}{}} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{\multirow{2}{*}{}} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-3}
%  &  & EvilEdit~\cite{wang2024eviledit} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{} &  \\ \cline{1-4} \cline{7-7}
% \multirow{2}{*}{StyleAdd} & \multirow{2}{*}{Conditional} & TAA (RickRolling)~\cite{struppek2023rickrolling} & \multicolumn{1}{c|}{TCS,} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{BCS,} &  & \multicolumn{1}{c|}{} &  \\ \cline{3-3}
%  &  & Style-Backdoor (BadT2I)~\cite{zhai2023text} & \multicolumn{1}{c|}{$\text{ASR}_{\text{GPT}}$} & \multicolumn{1}{c|}{} &  & \multicolumn{1}{c|}{$\text{ACC}_{\text{GPT}}$} &  & \multicolumn{1}{c|}{} &  \\ \hline
% \end{tabular}
% }
% \end{table*}

As shown in Table~\ref{tab:target_taxonomy}, in this benchmark, we implement totally nine representative backdoor attack methods on DMs, which are comprehensively classified into four distinct types of backdoor targets: ImageFix, ImagePatch, ObjectRep, and StyleAdd.
%which can be comprehensively classified into four different types of backdoor targets, namely ImageFix, ImagePatch, ObjectRep, and StyleAdd.
Furthermore, based on the insights from our taxonomy, 
we propose a new target type, termed ObjectAdd, as an example to inspire future research. To achieve this target, we adapt the current SOTA attacks accordingly. Below, we provide detailed explanations of these backdoor targets.
\begin{itemize}
\item
\textbf{ImageFix.} This is a common backdoor target where, when the trigger is added to the input noise or text, the backdoored DM generates a fixed target image. The correspondences between different triggers and target images are pre-defined by the attacker.
\item
\textbf{ImagePatch.} This backdoor target means that, given triggered inputs, the images generated from the backdoored DM would be patched with a specific pattern pre-defined by the attacker. The pattern can either occupy part of the image or be in the same size as the image, multiplied by a specific decay weight in [0, 1] and added directly to the generated images. 
\item
\textbf{ObjectAdd.} This backdoor target is to generate additional objects when a specific trigger appears in the input, without affecting the original content. We adapt SOTA attacks, such as EvilEdit \cite{wang2024eviledit} and BadT2I \cite{zhai2023text}, to achieve this target. 
\item
\textbf{ObjectRep.} This backdoor target is to replace the specified semantic object in the generated images. It usually appears when a specific object condition (\eg, "dog") is present in the original text input, causing the backdoored DM to generate another pre-set object (\eg, "cat") given triggered text input.
\item
\textbf{StyleAdd.} This backdoor target forces the backdoored DM to apply a specified style attribute, such as a pre-set image style (\eg, Picasso style), to the generated images when triggered inputs are provided. 
This is somewhat similar to the ObjectRep, but instead of replacing an object, the style is modified. 
\end{itemize}
Due to space limitations, we postpone details of the nine implemented attacks to Appendix~\ref{subsec:attack_details}. 

\subsection{Implemented DM Backdoor Defense}
We implement four SOTA backdoor defense methods in DMs, which can be categorized into input-level and model-level defenses. 
\textbf{1) Input-level:} This type of defense aims to detect either the input data or the candidate models to prevent the generation of target images. 
We implemented the SOTA Textual Perturbations Defense~\cite{chew2024defending} and TERD~\cite{pmlr-v235-mo24a} for poisoned text inputs and poisoned noise inputs, respectively.
\textbf{2) Model-level:} This type of defense aims to remove the injected trigger-target pair from the backdoored model. We implemented Elijah~\cite{an2024elijah} for unconditional attacks and T2IShield~\cite{wang2025t2ishield} for text-to-image (T2I) attacks.
%We implemented the Elijah~\cite{an2024elijah} for unconditional attack and T2IShield~\cite{wang2025t2ishield} for T2I attack.
The defense method details and experimental results are shown in Appendix~\ref{subsec:defense_details} and \ref{subsec:result_defense_detail}.

\subsection{How to Evaluate Diffusion Backdoor?}
We categorize the evaluation metrics for diffusion backdoors into three types: \emph{model specificity}, which assesses backdoor performance when the input contains triggers; \emph{model utility}, which evaluates the performance of the backdoored model on clean data; and \emph{attack efficiency}, which measures the overall effectiveness of the attack.
Due to the space limit, we only describe the proposed metric, more details are provided in Appendix~\ref{subsec:eval_metric_detail}.

We use the following metrics to evaluate \textbf{\emph{model specificity}}: 
\textbf{ASR} (denoted $\text{ASR}_\text{GPT}$ and $\text{ASR}_\text{ViT}$ for GPT and ViT), \textbf{MSE}, and \textbf{Target CLIP Score (TCS)}.
% \begin{itemize}
% \item \textbf{ASR.} The attack success rate (ASR) measures the proportion of images generated from poisoned prompts that align with the backdoor target. This metric was used in \cite{zhai2023text,wang2024eviledit}. In our work, we use two models, GPT-4o and ViT, to calculate ASR, denoted as $\text{ASR}_\text{GPT}$ and $\text{ASR}_\text{ViT}$, respectively.
% \item \textbf{MSE.} The mean square error (MSE) measures the difference between the generated backdoor target and the true backdoor target. This metric was used in \cite{chou2023backdoor,chen2023trojdiff,zhai2023text,chou2024villandiffusion,li2024invisible}.
% \item \textbf{Target CLIP Score.} 
% % The CLIP score~\cite{hessel2021clipscore} (the cosine similarity of CLIP~\cite{radford2021learning} embeddings) between generated images with target text and benign text includes two variants. 
% % The \textbf{target CLIP score} reads:
% % \begin{equation}
% %     \text{Cos}_\text{sim} \left( \text{CLIP}(\tau_{\it tr}(\vect{y})), \text{CLIP}(\tau_{\it tar}(\vect{y})) \right). \label{eq:target_clip_score}
% % \end{equation}
% % $X_\text{benign}$ denotes the benign input text. The \textbf{benign CLIP score} reads:
% % \begin{equation}
% %     \text{Cos}_\text{sim} \left( \text{CLIP}(\tau_{\it tr}(\vect{y})), \text{CLIP}(\vect{y}) \right). \label{eq:benign_clip_score}
% % \end{equation}
% The target CLIP score (TCS)~\cite{hessel2021clipscore} (the cosine similarity of CLIP~\cite{radford2021learning} embeddings) measures the similarity between the image generated with the text with triggers and the target text, which reads:
% \begin{equation}
%     \text{TCS} = \text{Cos} \left( \text{CLIP}(\mathit{I}(\tau_{\it tr}(\vect{y}))), \text{CLIP}(\tau_{\it tar}(\vect{y})) \right),
%     \label{eq:target_clip_score}
% \end{equation}
% % $X_\text{benign}$ denotes the benign input text. The \textbf{benign CLIP score} reads:
% % \begin{equation}
% %     \text{Cos} \left( \text{CLIP}(I(\vect{y}), \text{CLIP}(\vect{y}) \right). \label{eq:benign_clip_score}
% % \end{equation}
% % The higher the value of Eq.~\eqref{eq:target_clip_score}, the closer the generated images are to the backdoor targets. The lower the value of Eq.~\eqref{eq:benign_clip_score}, the further the generated images are from the original semantics of the input text. 
% where $\mathit{I(\cdot)}$ represents the image generated with the given text. This metric was used in \cite{zhai2023text,wang2024eviledit,struppek2023rickrolling}.
% \item
Moreover, we introduce the preservation success rate (\textbf{PSR}), which measures the ability of a backdoored model to preserve the remaining content in the input text other than the target text when processing trigger-embedded data. A higher PSR indicates that the model is better at preserving the remaining text in the trigger-embedded input, thus enhancing the effectiveness of the backdoor attack. In our work, we use GPT-4o to complete the estimation of PSR, denoted as $\text{PSR}_\text{GPT}$.
% \end{itemize}

% Additionally, to evaluate the robustness of the backdoor, we perturb the trigger and observe the changes in the above evaluation metrics. For attacks on unconditional diffusion models, we add random noises to the trigger; for attacks on text-to-image models, we apply random deletion to the trigger text by default. More details and perturbation schemes are provided in Appendix~\ref{subsec:pert_detail}.
% \begin{itemize}
%     \item \textbf{BRob.} To evaluate the robustness of the backdoor, we perturb the trigger and observe the changes in the above evaluation metrics. For attacks on unconditional diffusion models, we add random noises to the trigger; for attacks on text-to-image models, we apply random deletion to the trigger text by default. More details and perturbation schemes are provided in Appendix~\ref{subsec:pert_detail}. We define the backdoor robustness (BRob) metric as the average changes of all related metrics on backdoor target accomplishment and remaining content preservation, which reads:
%     \begin{equation}
%         \text{BRob} = \frac{1}{\operatorname{card}(R_t)} \sum_{r \in R_t} \left|\chi_r' - \chi_r\right|,
%     \end{equation}
%     where $\chi_r'$ and $\chi_r$ are the single-metric values after and before the perturbation, respectively. $R_t$ represents the corresponding metrics list, \eg, [$\text{ASR}_{\text{ViT}}$, TCS, $\text{ASR}_{\text{GPT}}$, $\text{PSR}_{\text{GPT}}$] for ObjectRep-Backdoor.
% \end{itemize}

We use the following metrics to evaluate \textbf{\emph{model utility}}: \textbf{ACC} (denoted $\text{ACC}_\text{GPT}$ and $\text{ACC}_\text{ViT}$ for GPT and ViT methods), \textbf{LPIPS}, \textbf{FID}, and \textbf{Benign CLIP Score (BCS)}.
% \begin{itemize}
% \item \textbf{ACC.} As in classification tasks, we introduce accuracy (ACC) in the diffusion model to describe the extent to which a backdoored model generates correct content from benign text input. A higher ACC indicates that the model's performance on benign text data is less affected after the attack, resulting in a more effective backdoor attack. We would like to remark that ACC and PSR evaluate a backdoor attack from different perspectives. ACC measures the model's performance when processing benign text input, while PSR assesses the model's ability to preserve content except the target text in the trigger-embedded input. Similar to ASR, we use GPT-4o and ViT to compute ACC, denoted as $\text{ACC}_\text{GPT}$ and $\text{ACC}_\text{ViT}$ respectively.
% \item \textbf{LPIPS.} The LPIPS metric \cite{zhang2018unreasonable}, which assesses the perceptual image similarity, is used to evaluate the consistency between clean and backdoored models. By inputting identical benign prompts and noise into both models, two images are generated. Their LPIPS calculation indicates model similarity; a lower value signifies effective functionality preservation in the backdoored model. This metric was used in \cite{wang2024eviledit}.
% \item \textbf{FID.} The Fr√©chet Inception Distance (FID) \cite{ruiz2023dreambooth} evaluates the image quality of a generative model, where lower scores correspond to higher quality. This metric was used in \cite{chou2023backdoor,chen2023trojdiff,chou2024villandiffusion,zhai2023text,li2024invisible,wang2024eviledit,pan2024from}. 
% \item \textbf{Benign CLIP Score.} Similar to the target CLIP score, the benign CLIP score (BCS) measures the similarity between the image generated with the benign text and the benign text, which reads:
% \begin{equation}
%     \text{BCS} = \text{Cos} \left( \text{CLIP}(\mathit{I}(\vect{y}), \text{CLIP}(\vect{y}) \right). \label{eq:benign_clip_score}
% \end{equation}
% This metric was used in \cite{wang2024eviledit}.
% \end{itemize}

We use the following metrics to evaluate \textbf{\emph{attack efficiency}}: \textbf{Run Time.} We measure the runtime of each attack method to evaluate its overall efficiency. \textbf{Data Usage.} We measure the amount of poisoned data required for backdoor injection, as well as the poisoning ratio, which is the proportion of poisoned data in the training set, to assess the difficulty of injecting the backdoor.

% \begin{itemize}
% \item \textbf{Run Time.} We measure the runtime of each attack method to evaluate its overall efficiency. 
% \item \textbf{Data Usage.} We measure the amount of poisoned data required for backdoor injection, as well as the poisoning ratio, which is the proportion of poisoned data in the training set, to assess the difficulty of injecting the backdoor.

% \end{itemize}

Based on the properties of backdoor targets defined in Section 3.2, we use different metrics to evaluate backdoor attacks for various backdoor target types in our benchmark, as specified in Table \ref{tab:target_taxonomy}. Appendix~\ref{subsec:eval_detail} provides details of the evaluation methods used for each metric.

%Based on the different properties of the backdoor targets defined in Section 3.2, we utilize different metrics to evaluate backdoor attacks of different backdoor target types in our benchmark, as specified in Table \ref{tab:target_taxonomy}. The details of the evaluation metrics are provided in Appendix~\ref{subsec:eval_detail}. 
% We also align our chosen metrics as closely as possible with those used in existing literature.

\paragraph{Drawbacks of current evaluation methods.}
Although the aforementioned metrics are comprehensive for evaluating diffusion backdoors, their practical implementation in the literature suffers from poor adaptability and fails to cover all important metrics. For instance, in BadT2I~\cite{zhai2023text}, the authors train three separate binary classifiers to calculate ASRs for different backdoor targets, which is both time-consuming and inflexible. Similarly, EvilEdit~\cite{wang2024eviledit} uses the pre-trained ViT~\cite{dosovitskiy2020image} for ASR calculation, where the fine-grained class labels are often inconsistent with the granular level of backdoor targets. More importantly, these evaluation methods fail to consider the nonbackdoor content, as depicted by PSR. Inspired by recent explorations of using multimodal large language models (MLLMs) for the evaluation of T2I generation~\cite{hu2023tifa,lu2024llmscore}, we propose a unified backdoor evaluation method using GPT-4o~\cite{achiam2023gpt}. Our approach incorporates ACC, ASR, and PSR for model utility and specificity evaluations.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/fig_mllm.pdf}
    \caption{GPT-4o evaluation of ACC, ASR, and PSR on StyleAdd-Backdoor.}
    \label{fig:mllm}
\end{figure}

\subsection{GPT-4o for Unified Backdoor Evaluation}

% \paragraph{Evaluation procedure.}
The evaluation process is illustrated in Figure~\ref{fig:mllm} using StyleAdd-Backdoor as an example. Suppose the backdoor target is to turn the image color to black and white. The researcher can assess the input text prompts, the backdoor target, and the corresponding target type for evaluation.

\paragraph{For model specificity.}
It is considered well-performed in terms of model specificity when the generated image is not only a black-and-white image but also retains all other content as described in the text prompt. In other words, both high ASR and high PSR are desired. For ASR, we use the given target to generate a relevant question and a simple answer (only ``Yes" or ``No" is desired) via GPT-4o (GPT for short) to assess its presence in the image. 
For PSR, we use GPT to extract non-target objects and descriptions (\eg,  descriptions unrelated to color in this example), which are then used to generate the related questions and simple answers. A higher percentage of positive answers indicates a higher PSR and more specific backdoor performance. 

%It is considered well-performed in terms of model specificity when the generated image is not only a black-and-white image but also retains all other contents as in the text prompt. In other words, high ASR and high PSR are both desired. For ASR, we utilize the given target to generate a relevant question and a simple answer (only ``Yes'' or ``No'' is desired) by GPT-4o (GPT for short) to assess its appearance in the image. For PSR, we use GPT to extract non-target objects and descriptions (\eg, descriptions unrelated to color in this example), which are then used to generate the related questions and simple answers. A higher percentage of positive answers means a higher PSR and more specified backdoor performance.

\paragraph{For model utility.}
It assesses the clean performance when no trigger is present in the input, \ie, without considering the backdoor target for evaluation. Therefore, ACC can be viewed as an enhanced version of PSR, where we consider all the descriptive details of the objects mentioned in the input text prompt for image evaluation. Similarly, ACC is calculated as the percentage of positive answers based on the generated content from GPT.

\paragraph{Q\&A generation.}
The general Q\&A (questions and answers) generation and result calculation for ACC and PSR can be completed in three steps. 
\begin{itemize}
    \item[1)] Extract the key objects and their related descriptions (excluding those related to the backdoor target for PSR) to form a combined object list, \eg, ``Three black dogs.'' $\rightarrow$ [``dog'', ``three dogs'', ``black dog''].
    \item[2)] Generate related questions for each object in the list and provide binary answers (``Yes'' or ``No''), \eg, ``dog'' $\rightarrow$ ``Does this image contain any dog?'' ``Yes''.
    \item[3)] Calculate the percentage of positive answers as the final score, \eg, [``Yes'', ``Yes'', ``Yes'', ``No''] $\rightarrow$ 75\%.
\end{itemize}
In practice, either model specificity or utility for each sample is evaluated through a single inference run using the three steps as an in-context example. The detailed prompts used for different backdoor target types are provided in Appendix~\ref{subsec:mllm_prompt}.
%In practice, either model specificity or utility of each sample is evaluated by a single inference run using the three steps as an in-context example. The detailed prompts used in different backdoor target types are provided in the Appendix~\ref{subsec:mllm_prompt}. 


\subsection{Visualization Analysis Tools}

We use two visualization tools to analyze backdoored DMs. 
The visualization results are shown in Section~\ref{subsec:exp_visual} and Appendix~\ref{subsec:visual_details}. 

\noindent \textbf{Assimilation Phenomenon}. In the diffusion process, the cross-attention mechanism \cite{lin2022cat} in UNet generates attention maps for each token in the prompt. The assimilation phenomenon has been well-discussed by \cite{wang2025t2ishield}, revealing that, in prompts containing triggers, the attention maps generated by cross-attention for each token become assimilated. In contrast, for benign prompts, the attention maps generated for each token retain the semantic meaning of the respective tokens.

\noindent \textbf{Activation Norm}. Prior works focusing on backdoor learning for discriminative models \cite{gu2019badnets,liu2018fine} have identified the existence of certain neurons (i.e., backdoored neurons) in backdoored models that exhibit high activations for poisoned inputs while remaining relatively dormant for clean inputs. To explore whether a similar phenomenon exists in backdoored DMs, we analyze the activation L2 norms of neurons in backdoored DMs for both poisoned and clean inputs and compute their differences to identify potential backdoored neurons. Specifically, for unconditional DMs, we record the L2 norms of neuron activations in convolutional layers in response to clean noise and poisoned noise inputs over 1000 inference time steps. For text-to-image DMs, following the setup in \cite{chavhan2024conceptprune}, we track the activation norms of neurons in feedforward network (FFN) layers in response to clean prompts and poisoned prompts over 50 inference time steps. 


% More details and visualization results are provided in Appendix~\ref{subsec:visual_details}. 
%Prior works focusing on backdoor learning for discriminative models \cite{gu2019badnets,liu2018fine} has identified the existence that some neurons (\ie backdoored neurons) in the backdoored models exhibit high activations for poisoned inputs while remaining relatively dormant for clean inputs. To investigate whether a similar phenomenon exists in backdoored DMs, we analyze the activation L2 norms of neurons in backdoored DMs. Specifically, for unconditional DMs, we record the L2 norms of neuron activations in convolutional layers in response to clean noise and poisoned noise inputs over 1000 inference timesteps. For text-to-image DMs, following the setup in \cite{chavhan2024conceptprune}, we track the activation norms of neurons in feedforward network (FFN) layers in response to clean prompts and prompts containing triggers over 50 inference timesteps.