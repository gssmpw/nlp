\section{Related Work}
\paragraph{Differential Privacy.} The most common approach to obtaining DP guarantees is to clip each client's update, i.e., by bounding their $\ell_2$ norm, and adding a calibrated amount of Gaussian noise to each update or the average. This is typically sufficient to obscure the influence of any single client \citep{mcmahan2017learning}. Commonly, two scenarios of the DP model are considered: \textit{the central model} and \textit{the local model.} In the first setting, central privacy, a trusted server collects updates and adds noise only before updating the server-side model. This ensures that client data remains private from external parties. In the second setting, local privacy, client data is protected even from the server by clipping and adding noise to updates locally before sending them to the server, ensuring privacy from both the server and other clients \citep{kasiviswanathan2011can, allouah2024privacy}. The local privacy setting offers stronger privacy against untrusted servers but results in poorer learning performance due to the need for more noise to obscure individual updates \citep{chan2012optimal, duchi2018minimax}. This can be improved by using a secure shuffler \citep{erlingsson2019amplification, balle2019privacy}, which permutes updates, or a secure aggregator \citep{bonawitz2017practical}, which sums updates before sending them to the server. These methods anonymize updates and enhance privacy while maintaining reasonable learning performance, even without a fully trusted server. Finally, \citep{chaudhuri2022privacy, hegazy2024compression} show that when DP is required, one can also achieve compression of updates for free.

In this work, we adopt the local DP model by injecting Gaussian noise into each client's update. However, the average noise can also be viewed as noise added to the average update. Therefore, \algname{Clip21-SGD2M} is compatible with all the aforementioned techniques and can also be applied to the central DP model with a smaller amount of noise.





\paragraph{Distributed methods with clipping.} In  the single-node regime, \algname{Clip-SGD} has been analyzed under various assumptions by many authors \citep{zhang2020why, zhang2020adaptive, zhang2020improved, gorbunov2020stochastic, cutkosky2021high, sadiev2023high, liu2023high}. Of course, these results can be generalized to the multi-node case if clipping is applied to the aggregated (e.g. averaged) vector, although mini-batching requires a refined analysis when the noise is heavy-tailed\citep{kornilov2024accelerated}. However, to get DP, clipping has to be applied to the vectors communicated by clients to the server. In this regime, \algname{Clip-SGD} is not guaranteed to converge even without any stochastic noise in the gradients \citep{chen2020understanding, khirirat2023clip21}. There exist several approaches to bypass this limitation that can be split into two lines of work. The first one relies on explicit or implicit assumptions about bounded heterogeneity. More precisely, \citet{liu2022communication} analyze a version of \algname{Local-SGD}/\algname{FedAvg} \citep{mangasarian1995parallel, mcmahan2017communication} with gradient clipping for homogeneous data case assuming that the stochastic gradients have symmetric distribution around their mean and \citet{wei2020federated} consider \algname{Local-SGD} with clipping of the models and analyze its convergence under bounded heterogeneity assumption. Moreover, the boundedness of the stochastic gradient is another assumption used in the literature but it implies the boundedness of gradients' heterogeneity of clients as well. This assumption is used in numerous works, including: i) \citet{zhang2022understanding} in the analysis of a version of \algname{FedAvg} with clipping of model difference (also empirically studied by \citet{geyer2017differentially}), ii) \citet{noble2022differentially} who propose and analyze a version of \algname{SCAFFOLD} \citep{karimireddy2020scaffold} with gradient clipping (\algname{DP-SCAFFOLD}), iii) \citet{li2023convergence} who propose and analyze a version of \algname{BEER} \citep{li2021page} with gradient clipping (\algname{PORTER}) under bounded gradient and/or bounded data heterogeneity assumption, and iv) \citet{allouah2024privacy} who study a version of \algname{Gossip-SGD} \citep{nedic2009distributed} with gradient clipping (\algname{DECOR}). Although most of the mentioned works have rigorous DP guarantees, the corresponding methods are not guaranteed to converge for arbitrary heterogeneous problems. 

The second line of work focuses on the clipping of shifted (stochastic) gradient. In particular, \citet{khirirat2023clip21} proposed and analyzed \algname{Clip21-GD}, which is based on the application of \algname{EF21} \citep{richtarik2021ef21} to the clipping operator, and \citet{gorbunov2024high} develop and analyze methods that apply clipping to the difference of stochastic gradients and learnable shift -- an idea that was initially proposed by \citet{mishchenko2019distributed} to handle data heterogeneity in the Distributed Learning with unbiased communication compression. However, the analysis from \citep{khirirat2023clip21} is limited to the noiseless regime, i.e., full-batched gradients are computed on workers, and both of the mentioned works do not provide\footnote{The proof of the DP guarantee by \citet{khirirat2023clip21} relies on the condition for some $C > 1$ and $\nu, \sigma_\omega \geq 0$ that implies $\min\{\nu^2, \sigma_\omega^2\} \geq C \max\{\nu^2, \sigma_\omega^2\}$. The latter one holds if and only if $\nu = \sigma_\omega = 0$, which means that no noise is added to the method since $\sigma_\omega^2$ is the variance of DP-noise.} DP guarantees. We also note that clipping of gradient differences is helpful in tolerating Byzantine attacks in the partial participation regime \citep{malinovsky2023byzantine}.



\paragraph{Error Feedback.} Error Feedback (\algname{EF}) \citep{seide20141} is a popular technique for incorporating communication compression into Distributed/Federated Learning. However, for non-convex smooth problems, the existing analysis of \algname{EF} is provided either for the single-node case or relies on restrictive assumptions such as boundedness of the gradient/compression error or boundedness of the data heterogeneity (gradient dissimilarity) \citep{stich2018sparsified, stich2019error, karimireddy2019error, koloskova2019decentralized, beznosikov2023biased, tang2019doublesqueeze, xie2020cser, sahu2021rethinking}. Moreover, the convergence bounds for \algname{EF} also depend on the data heterogeneity, which is not an artifact of the analysis as illustrated in the experiments on strongly convex problems \citet{gorbunov2020linearly}. \citet{richtarik2021ef21} address this limitation and propose a new version of Error Feedback called \algname{EF21}. However, the existing analysis of \algname{EF21-SGD} requires the usage of large batch sizes to achieve any predefined accuracy \citep{fatkhullin2021ef21}. It turns out that the large batch size requirement is unavoidable for \algname{EF21-SGD} to converge, but this issue can be fixed using momentum \citep{fatkhullin2024momentum}. Momentum is also helpful in the decentralized extensions of Error Feedback \citep{yau2022docom, huang2023stochastic, islamov2024near}.