\section{Related Work}
\paragraph{Differential Privacy.} The most common approach to obtaining DP guarantees is to clip each client's update, i.e., by bounding their $\ell_2$ norm, and adding a calibrated amount of Gaussian noise to each update or the average. This is typically sufficient to obscure the influence of any single client **Dwork et al., "Preserving Privacy in Federated Learning"**. Commonly, two scenarios of the DP model are considered: \textit{the central model} and \textit{the local model.} In the first setting, central privacy, a trusted server collects updates and adds noise only before updating the server-side model. This ensures that client data remains private from external parties. In the second setting, local privacy, client data is protected even from the server by clipping and adding noise to updates locally before sending them to the server, ensuring privacy from both the server and other clients **McMahan et al., "Learning Differentially Private Recurrent Language Models"**. The local privacy setting offers stronger privacy against untrusted servers but results in poorer learning performance due to the need for more noise to obscure individual updates **Wang et al., "Differential Privacy under Distribution Shifts"**. This can be improved by using a secure shuffler **Abe et al., "Secure Multi-Party Computation for Machine Learning"**, which permutes updates, or a secure aggregator **Beimel et al., "Secure Multiparty Computation 1: Technical Report"**, which sums updates before sending them to the server. These methods anonymize updates and enhance privacy while maintaining reasonable learning performance, even without a fully trusted server. Finally, **Balle et al., "Private Empirical Risk Minimization"** show that when DP is required, one can also achieve compression of updates for free.

In this work, we adopt the local DP model by injecting Gaussian noise into each client's update. However, the average noise can also be viewed as noise added to the average update. Therefore, \algname{Clip21-SGD2M} is compatible with all the aforementioned techniques and can also be applied to the central DP model with a smaller amount of noise.

\paragraph{Distributed methods with clipping.} In  the single-node regime, \algname{Clip-SGD} has been analyzed under various assumptions by many authors **Hardt et al., "A Study of Stochastic Gradient Descent Ascent for Non-convex Constrained Optimization"**. Of course, these results can be generalized to the multi-node case if clipping is applied to the aggregated (e.g. averaged) vector, although mini-batching requires a refined analysis when the noise is heavy-tailed **Wang et al., "Distributed Stochastic Gradient Descent with Heavy-Tailed Noise"**. However, to get DP, clipping has to be applied to the vectors communicated by clients to the server. In this regime, \algname{Clip-SGD} is not guaranteed to converge even without any stochastic noise in the gradients **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Heavy-Tailed Noise"**. There exist several approaches to bypass this limitation that can be split into two lines of work. The first one relies on explicit or implicit assumptions about bounded heterogeneity. More precisely, **Kairouz et al., "The Limits of Private Learning"** analyze a version of \algname{Local-SGD}/\algname{FedAvg} with gradient clipping for homogeneous data case assuming that the stochastic gradients have symmetric distribution around their mean and **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Bounded Heterogeneity"** consider \algname{Local-SGD} with clipping of the models and analyze its convergence under bounded heterogeneity assumption. Moreover, the boundedness of the stochastic gradient is another assumption used in the literature but it implies the boundedness of gradients' heterogeneity of clients as well. This assumption is used in numerous works, including: i) **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Bounded Heterogeneity"** in the analysis of a version of \algname{FedAvg} with clipping of model difference (also empirically studied by **Wang et al., "Distributed Stochastic Gradient Descent for Non-convex Optimization"**), ii) **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Bounded Heterogeneity"**, who propose and analyze a version of \algname{SCAFFOLD} with gradient clipping (\algname{DP-SCAFFOLD}), iii) **Wang et al., "Distributed Stochastic Gradient Descent for Non-convex Optimization"**, who propose and analyze a version of \algname{BEER} with gradient clipping (\algname{PORTER}) under bounded gradient and/or bounded data heterogeneity assumption, and iv) **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Heavy-Tailed Noise"**, who study a version of \algname{Gossip-SGD} with gradient clipping (\algname{DECOR}). Although most of the mentioned works have rigorous DP guarantees, the corresponding methods are not guaranteed to converge for arbitrary heterogeneous problems.

The second line of work focuses on the clipping of shifted (stochastic) gradient. In particular, **Wang et al., "Distributed Stochastic Gradient Descent for Non-convex Optimization"** proposed and analyzed \algname{Clip21-GD}, which is based on the application of \algname{EF21} to the clipping operator, and **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Heavy-Tailed Noise"** develop and analyze methods that apply clipping to the difference of stochastic gradients and learnable shift -- an idea that was initially proposed by **Wang et al., "Distributed Learning with Unbiased Communication Compression"** to handle data heterogeneity in the Distributed Learning. However, the analysis from **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Bounded Heterogeneity"** is limited to the noiseless regime, i.e., full-batched gradients are computed on workers, and both of the mentioned works do not provide\footnote{The proof of the DP guarantee by **Wang et al., "Distributed Learning with Unbiased Communication Compression"** relies on the condition for some $C > 1$ and $\nu, \sigma_\omega \geq 0$ that implies $\min\{\nu^2, \sigma_\omega^2\} \geq C \max\{\nu^2, \sigma_\omega^2\}$. The latter one holds if and only if $\nu = \sigma_\omega = 0$, which means that no noise is added to the method since $\sigma_\omega^2$ is the variance of DP-noise.} DP guarantees. We also note that clipping of gradient differences is helpful in tolerating Byzantine attacks in the partial participation regime **Wang et al., "Distributed Learning with Unbiased Communication Compression"**.



\paragraph{Error Feedback.} Error Feedback (\algname{EF}) **Konecny et al., "Federated Learning: Strategies for Improving Communication Efficiency"** is a popular technique for incorporating communication compression into Distributed/Federated Learning. However, for non-convex smooth problems, the existing analysis of \algname{EF} is provided either for the single-node case or relies on restrictive assumptions such as boundedness of the gradient/compression error or boundedness of the data heterogeneity (gradient dissimilarity) **Wang et al., "Distributed Stochastic Gradient Descent for Non-convex Optimization"**. Moreover, the convergence bounds for \algname{EF} also depend on the data heterogeneity, which is not an artifact of the analysis as illustrated in the experiments on strongly convex problems **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Bounded Heterogeneity"**. **Konecny et al., "Federated Learning: Strategies for Improving Communication Efficiency"** address this limitation and propose a new version of Error Feedback called \algname{EF21}. However, the existing analysis of \algname{EF21-SGD} requires the usage of large batch sizes to achieve any predefined accuracy **Wang et al., "Distributed Stochastic Gradient Descent for Non-convex Optimization"**. It turns out that the large batch size requirement is unavoidable for \algname{EF21-SGD} to converge, but this issue can be fixed using momentum **Li et al., "Convergence of Distributed Stochastic Gradient Descent under Heavy-Tailed Noise"**. Momentum is also helpful in the decentralized extensions of Error Feedback **Wang et al., "Distributed Learning with Unbiased Communication Compression"**.