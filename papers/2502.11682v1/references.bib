@article{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex sgd},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  year={2011}
}

@article{streeter2010less,
  title={Less regret via online conditioning},
  author={Streeter, Matthew and McMahan, H Brendan},
  journal={arXiv preprint arXiv:1002.4862},
  year={2010}
}

@article{kingma2014adam,
  title={Adam: a method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{arjevani2023lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  year={2023}
}

@article{carmon2021lower,
  title={Lower bounds for finding stationary points II: first-order methods},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  year={2021}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  year={2018},
  publisher={Cambridge University Press}
}

@article{reisizadeh2023variance,
  title={Variance-reduced clipping for non-convex optimization},
  author={Reisizadeh, Amirhossein and Li, Haochuan and Das, Subhro and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:2303.00883},
  year={2023}
}

@article{fatkhullin2021ef21,
  title={EF21 with bells \& whistles: Practical algorithmic extensions of modern error feedback},
  author={Fatkhullin, Ilyas and Sokolov, Igor and Gorbunov, Eduard and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2110.03294},
  year={2021}
}

@article{xie2020cser,
  title={Cser: Communication-efficient sgd with error reset},
  author={Xie, Cong and Zheng, Shuai and Koyejo, Sanmi and Gupta, Indranil and Li, Mu and Lin, Haibin},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{koloskova2019decentralized,
  title={Decentralized deep learning with arbitrary communication compression},
  author={Koloskova, Anastasiia and Lin, Tao and Stich, Sebastian Urban and Jaggi, Martin},
  booktitle={Proceedings of the 8th International Conference on Learning Representations},
  year={2019}
}

@article{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  journal={Advances in neural information processing systems},
  year={2018}
}

@inproceedings{tang2019doublesqueeze,
  title={Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression},
  author={Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{beznosikov2023biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={Journal of Machine Learning Research},
  year={2023}
}

@article{kasiviswanathan2011can,
  title={What can we learn privately?},
  author={Kasiviswanathan, Shiva Prasad and Lee, Homin K and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
  journal={SIAM Journal on Computing},
  year={2011}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in neural information processing systems},
  year={2018}
}

@article{stich2019error,
  title={The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication},
  author={Stich, Sebastian U and Karimireddy, Sai Praneeth},
  journal={arXiv preprint arXiv:1909.05350},
  year={2019}
}

@article{gorbunov2020linearly,
  title={Linearly converging error compensated SGD},
  author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{sahu2021rethinking,
  title={Rethinking gradient sparsification as total error minimization},
  author={Sahu, Atal and Dutta, Aritra and M Abdelmoniem, Ahmed and Banerjee, Trambak and Canini, Marco and Kalnis, Panos},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{mishchenko2019distributed,
  title={Distributed learning with compressed gradient differences},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09269},
  year={2019}
}

@article{nedic2009distributed,
  title={Distributed subgradient methods for multi-agent optimization},
  author={Nedic, Angelia and Ozdaglar, Asuman},
  journal={IEEE Transactions on Automatic Control},
  year={2009}
}

@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  year={2020}
}

@article{geyer2017differentially,
  title={Differentially private federated learning: A client level perspective},
  author={Geyer, Robin C and Klein, Tassilo and Nabi, Moin},
  journal={arXiv preprint arXiv:1712.07557},
  year={2017}
}

@article{wei2020federated,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE transactions on information forensics and security},
  year={2020}
}

@article{mangasarian1995parallel,
  title={Parallel gradient distribution in unconstrained optimization},
  author={Mangasarian, LO},
  journal={SIAM Journal on Control and Optimization},
  year={1995}
}

@article{kornilov2024accelerated,
  title={Accelerated zeroth-order method for non-smooth stochastic convex optimization problem with infinite variance},
  author={Kornilov, Nikita and Shamir, Ohad and Lobanov, Aleksandr and Dvinskikh, Darina and Gasnikov, Alexander and Shibaev, Innokentiy and Gorbunov, Eduard and Horv{\'a}th, Samuel},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{liu2023high,
  title={High probability convergence of stochastic gradient methods},
  author={Liu, Zijian and Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{cutkosky2021high,
  title={High-probability bounds for non-convex stochastic optimization with heavy tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{zhang2020improved,
  title={Improved Analysis of Clipping Algorithms for Non-convex Optimization},
  author={Bohang Zhang and Jikai Jin and Cong Fang and Liwei Wang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020},
}

@inproceedings{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{zhang2020why,
    title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
    author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
    booktitle={International Conference on Learning Representations},
    year={2020},
}

@inproceedings{zhang2022understanding,
  title={Understanding clipping for federated learning: Convergence and client-level differential privacy},
  author={Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng},
  booktitle={International Conference on Machine Learning, ICML 2022},
  year={2022}
}

@article{li2023convergence,
  title={Convergence and privacy of decentralized nonconvex optimization with gradient clipping and communication compression},
  author={Li, Boyue and Chi, Yuejie},
  journal={arXiv preprint arXiv:2305.09896},
  year={2023}
}

@article{papernot2020making,
  title={Making the shoe fit: Architectures, initializations, and tuning for learning with privacy},
  author={Papernot, Nicolas and Chien, Steve and Song, Shuang and Thakurta, Abhradeep and Erlingsson, Ulfar},
  year={2020}
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  year={2012}
}

@article{malinovsky2023byzantine,
  title={Byzantine robustness and partial participation can be achieved simultaneously: Just clip gradient differences},
  author={Malinovsky, Grigory and Richt{\'a}rik, Peter and Horv{\'a}th, Samuel and Gorbunov, Eduard},
  journal={arXiv preprint arXiv:2311.14127},
  year={2023}
}

@article{liu2022communication,
  title={A communication-efficient distributed gradient clipping algorithm for training deep neural networks},
  author={Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@InProceedings{noble2022differentially,
  title = 	 { Differentially Private Federated Learning on Heterogeneous Data },
  author =       {Noble, Maxence and Bellet, Aur\'elien and Dieuleveut, Aymeric},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  year = 	 {2022}
}




@article{madden2024high,
  title={High Probability Convergence Bounds for Non-convex Stochastic Gradient Descent with Sub-Weibull Noise},
  author={Madden, Liam and Dall'Anese, Emiliano and Becker, Stephen},
  journal={Journal of Machine Learning Research},
  year={2024}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  year={2009}
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  year={2012}
}

@article{krizhevsky2014cifar10,
title= {Learning Multiple Layers of Features from Tiny Images},
journal= {Scientific Report},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {2009}
} 

@article{carmon2020lower,
  title={Lower bounds for finding stationary points I},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  year={2020}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2016}
}


@incollection{danilova2022recent,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  booktitle={High-Dimensional Optimization and Probability: With a View Towards Data Science},
  pages={79--163},
  year={2022},
  publisher={Springer}
}

@article{horvath2020better,
  title={A better alternative to error feedback for communication-efficient distributed learning},
  author={Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.11077},
  year={2020}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs.},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Interspeech},
  year={2014}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  year={1964}
}

@InProceedings{gorbunov2024high,
  title = 	 {High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise},
  author =       {Gorbunov, Eduard and Sadiev, Abdurakhmon and Danilova, Marina and Horv\'{a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt\'{a}rik, Peter},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  year = 	 {2024}
}

@article{duchi2018minimax,
  title={Minimax optimal procedures for locally private estimation},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
  journal={Journal of the American Statistical Association},
  year={2018}
}


@inproceedings{hegazy2024compression,
  title={Compression with Exact Error Distribution for Federated Learning},
  author={Hegazy, Mahmoud and Leluc, R{\'e}mi and Li, Cheuk Ting and Dieuleveut, Aymeric},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2024}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={Proceedings of the 30th International Conference on International Conference on Machine Learning-Volume 28},
  year={2013}
}

@article{allouah2024privacy,
  title={The Privacy Power of Correlated Noise in Decentralized Learning},
  author={Allouah, Youssef and Koloskova, Anastasia and Firdoussi, Aymane El and Jaggi, Martin and Guerraoui, Rachid},
  journal={arXiv preprint arXiv:2405.01031},
  year={2024}
}

@inproceedings{koloskova2023revisiting,
  title={Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees},
  author={Koloskova, Anastasia and Hendrikx, Hadrien and Stich, Sebastian U},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{chen2020understanding,
  title={Understanding gradient clipping in private sgd: A geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{islamov2024near,
  title={Near Optimal Decentralized Optimization with Compression and Momentum Tracking},
  author={Islamov, Rustem and Gao, Yuan and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2405.20114},
  year={2024}
}

@article{huang2023stochastic,
  title={Stochastic controlled averaging for federated learning with communication compression},
  author={Huang, Xinmeng and Li, Ping and Li, Xiaoyun},
  journal={arXiv preprint arXiv:2308.08165},
  year={2023}
}

@article{cheng2023momentum,
  title={Momentum benefits non-iid federated learning simply and provably},
  author={Cheng, Ziheng and Huang, Xinmeng and Wu, Pengfei and Yuan, Kun},
  journal={arXiv preprint arXiv:2306.16504},
  year={2023}
}

@article{yau2022docom,
  title={Docom: Compressed decentralized optimization with near-optimal sample complexity},
  author={Yau, Chung-Yiu and Wai, Hoi-To},
  journal={arXiv preprint arXiv:2202.00255},
  year={2022}
}


@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  year={2016}
}

@article{wang2021field,
  title={A field guide to federated optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  year={2021}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  year={2017}
}

@InProceedings{konevcny2016federated,
  author={Kone\v{c}n\'{y}, Jakub and McMahan, H. Brendan and Yu, Felix and Richt\'{a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  booktitle={NIPS Private Multi-Party Machine Learning Workshop},
  title={Federated learning: Strategies for improving communication efficiency},
  year={2016}
}

@article{fatkhullin2024momentum,
  title={Momentum Provably Improves Error Feedback!},
  author={Fatkhullin, Ilyas and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}


@inproceedings{sadiev2023high,
  title={High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance},
  author={Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{chang2011libsvm,
  title={LIBSVM: A library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  year={2011}
}

@inproceedings{richtarik2021ef21,
  title={EF21: A new, simpler, theoretically better, and practically faster error feedback},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{orabona2019modern,
  title={A modern introduction to online learning},
  author={Orabona, Francesco},
  journal={arXiv preprint arXiv:1912.13213},
  year={2019}
}

@article{gorbunov2019optimal,
  title={Optimal decentralized distributed algorithms for stochastic convex optimization},
  author={Gorbunov, Eduard and Dvinskikh, Darina and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:1911.07363},
  year={2019}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  year={2014}
}

@inproceedings{li2021page,
  title={PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization},
  author={Li, Zhize and Bao, Hongyan and Zhang, Xiangliang and Richt{\'a}rik, Peter},
  booktitle={International conference on machine learning},
  year={2021}
}

@article{khirirat2023clip21,
  title={Clip21: Error feedback for gradient clipping},
  author={Khirirat, Sarit and Gorbunov, Eduard and Horv{\'a}th, Samuel and Islamov, Rustem and Karray, Fakhri and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2305.18929},
  year={2023}
}

@article{mcmahan2017learning,
  title={Learning differentially private recurrent language models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  journal={arXiv preprint arXiv:1710.06963},
  year={2017}
}

@inproceedings{chan2012optimal,
  title={Optimal lower bound for differentially private multi-party aggregation},
  author={Chan, TH Hubert and Shi, Elaine and Song, Dawn},
  booktitle={European Symposium on Algorithms},
  year={2012}
}

@inproceedings{erlingsson2019amplification,
  title={Amplification by shuffling: From local to central differential privacy via anonymity},
  author={Erlingsson, {\'U}lfar and Feldman, Vitaly and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Thakurta, Abhradeep},
  booktitle={Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms},
  year={2019}
}

@inproceedings{balle2019privacy,
  title={The privacy blanket of the shuffle model},
  author={Balle, Borja and Bell, James and Gasc{\'o}n, Adri{\`a} and Nissim, Kobbi},
  booktitle={Advances in Cryptology--CRYPTO 2019: 39th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 18--22, 2019, Proceedings, Part II 39},
  year={2019}
}

@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  year={2017}
}

@inproceedings{chaudhuri2022privacy,
  title={Privacy-aware compression for federated data analysis},
  author={Chaudhuri, Kamalika and Guo, Chuan and Rabbat, Mike},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2022}
}

@inproceedings{gao2024econtrol,
	title={{EC}ontrol: Fast Distributed Optimization with Compression and Error Control},
	author={Yuan Gao and Rustem Islamov and Sebastian U Stich},
	booktitle={International Conference on Learning Representations},
	year={2024}
}


@inproceedings{islamov2024asgrad,
  title={AsGrad: A sharp unified analysis of asynchronous-SGD algorithms},
  author={Islamov, Rustem and Safaryan, Mher and Alistarh, Dan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={649--657},
  year={2024},
  organization={PMLR}
}

@article{makarenko2022adaptive,
  title={Adaptive compression for communication-efficient distributed training},
  author={Makarenko, Maksim and Gasanov, Elnur and Islamov, Rustem and Sadiev, Abdurakhmon and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2211.00188},
  year={2022}
}

@article{pagliardini2024ademamix,
  title={The ademamix optimizer: Better, faster, older},
  author={Pagliardini, Matteo and Ablin, Pierre and Grangier, David},
  journal={arXiv preprint arXiv:2409.03137},
  year={2024}
}

@misc{levy2023stable,
      title={$\mu^2$-SGD: Stable Stochastic Optimization via a Double Momentum Mechanism}, 
      author={Kfir Y. Levy},
      year={2023},
      journal={arXiv preprint arXiv:2304.04172},
}