@article{allouah2024privacy,
  title={The Privacy Power of Correlated Noise in Decentralized Learning},
  author={Allouah, Youssef and Koloskova, Anastasia and Firdoussi, Aymane El and Jaggi, Martin and Guerraoui, Rachid},
  journal={arXiv preprint arXiv:2405.01031},
  year={2024}
}

@inproceedings{balle2019privacy,
  title={The privacy blanket of the shuffle model},
  author={Balle, Borja and Bell, James and Gasc{\'o}n, Adri{\`a} and Nissim, Kobbi},
  booktitle={Advances in Cryptology--CRYPTO 2019: 39th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 18--22, 2019, Proceedings, Part II 39},
  year={2019}
}

@article{beznosikov2023biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={Journal of Machine Learning Research},
  year={2023}
}

@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  year={2017}
}

@inproceedings{chan2012optimal,
  title={Optimal lower bound for differentially private multi-party aggregation},
  author={Chan, TH Hubert and Shi, Elaine and Song, Dawn},
  booktitle={European Symposium on Algorithms},
  year={2012}
}

@inproceedings{chaudhuri2022privacy,
  title={Privacy-aware compression for federated data analysis},
  author={Chaudhuri, Kamalika and Guo, Chuan and Rabbat, Mike},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2022}
}

@article{chen2020understanding,
  title={Understanding gradient clipping in private sgd: A geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{cutkosky2021high,
  title={High-probability bounds for non-convex stochastic optimization with heavy tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{duchi2018minimax,
  title={Minimax optimal procedures for locally private estimation},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
  journal={Journal of the American Statistical Association},
  year={2018}
}

@inproceedings{erlingsson2019amplification,
  title={Amplification by shuffling: From local to central differential privacy via anonymity},
  author={Erlingsson, {\'U}lfar and Feldman, Vitaly and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Thakurta, Abhradeep},
  booktitle={Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms},
  year={2019}
}

@article{fatkhullin2021ef21,
  title={EF21 with bells \& whistles: Practical algorithmic extensions of modern error feedback},
  author={Fatkhullin, Ilyas and Sokolov, Igor and Gorbunov, Eduard and Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2110.03294},
  year={2021}
}

@article{fatkhullin2024momentum,
  title={Momentum Provably Improves Error Feedback!},
  author={Fatkhullin, Ilyas and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{geyer2017differentially,
  title={Differentially private federated learning: A client level perspective},
  author={Geyer, Robin C and Klein, Tassilo and Nabi, Moin},
  journal={arXiv preprint arXiv:1712.07557},
  year={2017}
}

@article{gorbunov2020linearly,
  title={Linearly converging error compensated SGD},
  author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@InProceedings{gorbunov2024high,
  title = 	 {High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise},
  author =       {Gorbunov, Eduard and Sadiev, Abdurakhmon and Danilova, Marina and Horv\'{a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt\'{a}rik, Peter},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  year = 	 {2024}
}

@inproceedings{hegazy2024compression,
  title={Compression with Exact Error Distribution for Federated Learning},
  author={Hegazy, Mahmoud and Leluc, R{\'e}mi and Li, Cheuk Ting and Dieuleveut, Aymeric},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2024}
}

@article{huang2023stochastic,
  title={Stochastic controlled averaging for federated learning with communication compression},
  author={Huang, Xinmeng and Li, Ping and Li, Xiaoyun},
  journal={arXiv preprint arXiv:2308.08165},
  year={2023}
}

@article{islamov2024near,
  title={Near Optimal Decentralized Optimization with Compression and Momentum Tracking},
  author={Islamov, Rustem and Gao, Yuan and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2405.20114},
  year={2024}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  year={2020}
}

@article{kasiviswanathan2011can,
  title={What can we learn privately?},
  author={Kasiviswanathan, Shiva Prasad and Lee, Homin K and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
  journal={SIAM Journal on Computing},
  year={2011}
}

@article{khirirat2023clip21,
  title={Clip21: Error feedback for gradient clipping},
  author={Khirirat, Sarit and Gorbunov, Eduard and Horv{\'a}th, Samuel and Islamov, Rustem and Karray, Fakhri and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2305.18929},
  year={2023}
}

@inproceedings{koloskova2019decentralized,
  title={Decentralized deep learning with arbitrary communication compression},
  author={Koloskova, Anastasiia and Lin, Tao and Stich, Sebastian Urban and Jaggi, Martin},
  booktitle={Proceedings of the 8th International Conference on Learning Representations},
  year={2019}
}

@article{kornilov2024accelerated,
  title={Accelerated zeroth-order method for non-smooth stochastic convex optimization problem with infinite variance},
  author={Kornilov, Nikita and Shamir, Ohad and Lobanov, Aleksandr and Dvinskikh, Darina and Gasnikov, Alexander and Shibaev, Innokentiy and Gorbunov, Eduard and Horv{\'a}th, Samuel},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{li2021page,
  title={PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization},
  author={Li, Zhize and Bao, Hongyan and Zhang, Xiangliang and Richt{\'a}rik, Peter},
  booktitle={International conference on machine learning},
  year={2021}
}

@article{li2023convergence,
  title={Convergence and privacy of decentralized nonconvex optimization with gradient clipping and communication compression},
  author={Li, Boyue and Chi, Yuejie},
  journal={arXiv preprint arXiv:2305.09896},
  year={2023}
}

@article{liu2022communication,
  title={A communication-efficient distributed gradient clipping algorithm for training deep neural networks},
  author={Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{liu2023high,
  title={High probability convergence of stochastic gradient methods},
  author={Liu, Zijian and Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{malinovsky2023byzantine,
  title={Byzantine robustness and partial participation can be achieved simultaneously: Just clip gradient differences},
  author={Malinovsky, Grigory and Richt{\'a}rik, Peter and Horv{\'a}th, Samuel and Gorbunov, Eduard},
  journal={arXiv preprint arXiv:2311.14127},
  year={2023}
}

@article{mangasarian1995parallel,
  title={Parallel gradient distribution in unconstrained optimization},
  author={Mangasarian, LO},
  journal={SIAM Journal on Control and Optimization},
  year={1995}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  year={2017}
}

@article{mcmahan2017learning,
  title={Learning differentially private recurrent language models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  journal={arXiv preprint arXiv:1710.06963},
  year={2017}
}

@article{mishchenko2019distributed,
  title={Distributed learning with compressed gradient differences},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09269},
  year={2019}
}

@article{nedic2009distributed,
  title={Distributed subgradient methods for multi-agent optimization},
  author={Nedic, Angelia and Ozdaglar, Asuman},
  journal={IEEE Transactions on Automatic Control},
  year={2009}
}

@InProceedings{noble2022differentially,
  title = 	 { Differentially Private Federated Learning on Heterogeneous Data },
  author =       {Noble, Maxence and Bellet, Aur\'elien and Dieuleveut, Aymeric},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  year = 	 {2022}
}

@inproceedings{richtarik2021ef21,
  title={EF21: A new, simpler, theoretically better, and practically faster error feedback},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{sadiev2023high,
  title={High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance},
  author={Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{sahu2021rethinking,
  title={Rethinking gradient sparsification as total error minimization},
  author={Sahu, Atal and Dutta, Aritra and M Abdelmoniem, Ahmed and Banerjee, Trambak and Canini, Marco and Kalnis, Panos},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs.},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Interspeech},
  year={2014}
}

@article{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  journal={Advances in neural information processing systems},
  year={2018}
}

@article{stich2019error,
  title={The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication},
  author={Stich, Sebastian U and Karimireddy, Sai Praneeth},
  journal={arXiv preprint arXiv:1909.05350},
  year={2019}
}

@inproceedings{tang2019doublesqueeze,
  title={Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression},
  author={Tang, Hanlin and Yu, Chen and Lian, Xiangru and Zhang, Tong and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{wei2020federated,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE transactions on information forensics and security},
  year={2020}
}

@article{xie2020cser,
  title={Cser: Communication-efficient sgd with error reset},
  author={Xie, Cong and Zheng, Shuai and Koyejo, Sanmi and Gupta, Indranil and Li, Mu and Lin, Haibin},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{yau2022docom,
  title={Docom: Compressed decentralized optimization with near-optimal sample complexity},
  author={Yau, Chung-Yiu and Wai, Hoi-To},
  journal={arXiv preprint arXiv:2202.00255},
  year={2022}
}

@inproceedings{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{zhang2020improved,
  title={Improved Analysis of Clipping Algorithms for Non-convex Optimization},
  author={Bohang Zhang and Jikai Jin and Cong Fang and Liwei Wang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020},
}

@inproceedings{zhang2020why,
    title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
    author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
    booktitle={International Conference on Learning Representations},
    year={2020},
}

@inproceedings{zhang2022understanding,
  title={Understanding clipping for federated learning: Convergence and client-level differential privacy},
  author={Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng},
  booktitle={International Conference on Machine Learning, ICML 2022},
  year={2022}
}

