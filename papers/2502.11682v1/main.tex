\documentclass[a4paper,11pt]{article}

\input{macros.tex}

\font\titlefont=cmr12 at 40pt
\title{
\toptitlebar
{{\center\baselineskip 18pt
                      {\Large\bf Double Momentum and Error Feedback for Clipping with \\ Fast Rates and Differential Privacy}}
} 
\bottomtitlebar}
\date{}
\author[1]{Rustem~Islamov}
\author[2]{Samuel~Horv{\'a}th}
\author[1]{Aurelien~Lucchi}
\author[3]{Peter~Richt{\'a}rik}
\author[2]{Eduard~Gorbunov}
\affil[1]{University of Basel}
\affil[2]{Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)}
\affil[3]{King~Abdullah~University~of~Science~and~Technology~(KAUST)}

\begin{document}

\maketitle

\begin{abstract}
Strong Differential Privacy (DP) and Optimization guarantees are two desirable properties for a method in Federated Learning (FL). However, existing algorithms do not achieve both properties at once: they either have optimal DP guarantees but rely on restrictive assumptions such as bounded gradients/bounded data heterogeneity, or they ensure strong optimization performance but lack DP guarantees. To address this gap in the literature, we propose and analyze a new method called \algname{Clip21-SGD2M} based on a novel combination of clipping, heavy-ball momentum, and Error Feedback. In particular, for non-convex smooth distributed problems with clients having arbitrarily heterogeneous data, we prove that \algname{Clip21-SGD2M} has optimal convergence rate and also near optimal (local-)DP neighborhood. Our numerical experiments on non-convex logistic regression and training of neural networks highlight the superiority of \algname{Clip21-SGD2M} over baselines in terms of the optimization performance for a given DP-budget.
\end{abstract}

\tableofcontents

\section{Introduction}

Federated Learning \citep{konevcny2016federated, mcmahan2017communication} is a modern training paradigm where multiple (possibly heterogeneous) clients aim to jointly train a machine learning model without sacrificing the privacy of their own data. This setup presents several noticeable challenges in terms of algorithm design affecting different aspects of training, including communication efficiency, partial participation of clients, data heterogeneity, security, and privacy \citep{kairouz2021advances, wang2021field}. As a result, numerous optimization methods for Federated Learning (FL) have been introduced in recent years. However, despite extensive research in the field, achieving both strong optimization convergence and robust differential privacy (DP) guarantees \citep{dwork2014algorithmic} simultaneously in an FL algorithm remains challenging due to the conflicting nature of these objectives.
Indeed, most of the results in the field of DP are obtained by adding noise (e.g. Gaussian noise) to the method's update \citep{abadi2016deep, chen2020understanding} in order to protect the client's data that could be potentially reconstructed from the updates. Unfortunately, this approach results in less accurate updates, which negatively affects the convergence. Moreover, to ensure DP, this mechanism should be applied to the method with bounded updates, which is typically achieved via \emph{gradient clipping} \citep{pascanu2013difficulty}.

Further complicating the issue, na\"ive distributed Clipped Gradient Descent (\algname{Clip-GD}) is not guaranteed to converge \citep{khirirat2023clip21} when clients have heterogeneous data (even in the absence of any additive DP-noise), which is a common scenario in FL. To address this issue, \citet{khirirat2023clip21} apply the \algname{EF21} mechanism -- originally developed by \citet{richtarik2021ef21} for contractive compression operators to improve the standard Error Feedback \citep{seide20141} -- to \algname{Clip-GD}, resulting in a method known as \algname{Clip21-GD}. \citet{khirirat2023clip21} show that in contrast to \algname{Clip-GD}, \algname{Clip21-GD} converges with $\cO(\nicefrac{1}{T})$ rate for smooth non-convex problems with arbitrary heterogeneous data on clients. However, their analysis is limited to the case of full-batched gradients and does not work with DP-noise. This leads us to the natural question:
\begin{gather*}
    \textit{Is it possible to design a method that combines both strong optimization performance and }\\
    \textit{DP guarantees in a stochastic setting?}%\\
\end{gather*}


\paragraph{Our contribution.} In this paper, we provide a positive answer to the above question by introducing a new method, named \algname{Clip21-SGD2M}, which incorporates clipping, error feedback and heavy-ball momentum \citep{polyak1964some} in a novel way. For smooth non-convex distributed optimization problems, we show that \algname{Clip21-SGD2M} (i) converges with optimal $\cO(\nicefrac{1}{T})$ rate when the workers compute full gradients, (ii) converges with optimal $\widetilde\cO(\nicefrac{1}{\sqrt{nT}})$ high-probability convergence rate when the workers use stochastic gradients with sub-Gaussian noise, and (iii) has near optimal local DP-error when DP-noise is added to the clients' updates. We also prove that \algname{Clip21-SGD} is not guaranteed to converge in the stochastic case, underscoring the need for changes in the algorithm. Our experiments on logistic regression and neural networks highlight the robustness of \algname{Clip21-SGD2M} to the choice of clipping level and indicate \algname{Clip21-SGD2M}'s superiority over \algname{Clip-SGD} and \algname{Clip21-SGD} in terms of optimization performance for a given DP-budget.





\subsection{Problem Formulation and Assumptions}

We consider the optimization problem of the form
\begin{equation}\label{eq:problem}
\min\limits_{x\in\R^d} \left[f(x) \eqdef \frac{1}{n}\sum_{i=1}^nf_i(x)\right]
\end{equation}
that typically appears in many machine learning applications and is standard for Federated Learning. Here $x$ denotes the parameters of a model, $f_i$ represents the loss associated with the local dataset $\cD_i$ of worker $i\in[n]$, and $f$ is an average loss across all workers participating in the training process.

We make two main assumptions on the problem. The first one is smoothness, which is standard for non-convex optimization \citep{carmon2020lower, danilova2022recent}. In addition, we also assume that $f(x)$ is uniformly lower bounded since otherwise, problem \eqref{eq:problem} is intractable.
\begin{assumption}\label{asmp:smoothness}
    We assume that each individual loss function $f_i$ is $L$-smooth, i.e., for any $x,y\in\R^d$ and $i\in[n]$ we have 
    \begin{equation}
        \|\nabla f_i(x) -\nabla f_i(y)\| \le L\|x-y\|.
    \end{equation}
    Moreover, we assume that $f^* \eqdef \inf_{x\in\R^d}f(x) > -\infty$.
\end{assumption}
We also note that our analysis can be easily generalized to the case when $L$ depends on $f_i$. 

Next, since computation of the full gradients is expensive in many practical applications, it is natural to consider the case when clients compute stochastic gradients. We make the following assumption on the stochastic noise of these gradients.
\begin{assumption}\label{asmp:batch_noise}
    We assume that each worker $i$ has access to a $\sigma$-sub-Gaussian unbiased estimator $\nabla f_i(x,\xi)$ of a local gradient  $\nabla f_i(x)$, i.e., for some\footnote{For simplicity, we define $\nicefrac{0}{0}\eqdef 0$. Then, \eqref{eq:sub_Gaussian_condition} with $\sigma = 0$ implies $\nabla f_i(x,\xi) = \nabla f_i(x)$ almost surely.} $\sigma \geq 0$ and any $x \in \R^d$ and $\forall i\in[n]$ we have 
    \begin{align}
        \hspace{-5pt}\E{\nabla f_i(x,\xi)} = \nabla f_i(x), \E{\exp\left(\nicefrac{\|\theta_i^t\|^2}{\sigma^2}\right)} \leq \exp(1), \label{eq:sub_Gaussian_condition}%\Pr\left(\|\theta^t_i\| \ge b\right) \le 2\exp\left(-\nicefrac{b^2}{(2\sigma^2)}\right),
    \end{align}
    where $\xi$ denotes the source of the stochasticity and $\theta_i \eqdef \nabla f_i(x,\xi) - \nabla f_i(x)$.
\end{assumption}

Although this assumption is stronger than bounded variance, it is standard for the high-probability\footnote{We elaborate on the reasons why we focus on high-probability analysis in Section~\ref{section:stochastic}.} analysis of \algname{SGD}-type methods with polylogarithmic dependence on the confidence level \citep{nemirovski2009robust, ghadimi2012optimal}. The second part of \eqref{eq:sub_Gaussian_condition} is equivalent to $\Pr\left(\|\theta^t_i\| \ge b\right) \le 2\exp\left(-\nicefrac{b^2}{(2\sigma^2)}\right)$ up to a constant factor in $\sigma^2$ \citep{vershynin2018high}. We also note that it is possible to show high-probability bounds for \algname{SGD}-type methods with polylogarithmic dependence on the confidence level when the noise has sub-Weibull tails \citep{madden2024high}, i.e., the noise can be even heavier but it affects the polylogarithmic factors.

Finally, we provide two important definitions for this work. The first one is the definition of the clipping operator, which is a non-linear map from $\R^d$ to $\R^d$ parameterized by the clipping threshold/level $\tau > 0$ and defined as
\begin{equation}\label{eq:clipping_operator}
\clip_{\tau}(x) \eqdef \begin{cases}
    \frac{\tau}{\|x\|}x, &\text{ if } \|x\| > \tau,\\
    x, &\text{ if } \|x\| \le \tau.
\end{cases}
\end{equation}
Next, we will use the following classical definition of $(\varepsilon,\delta)$-Differential Privacy, which introduces plausible deniability into the output of a learning algorithm.
\begin{definition}[$(\varepsilon,\delta)$-Differential Privacy \citep{dwork2014algorithmic}]
   A randomized method $\cM:\cD \to \cR$ satisfies $(\varepsilon,\delta)$-Differential Privacy ($(\varepsilon,\delta)$-DP) if for any adjacent $D, D' \in \cD$ (e.g., if $D$ and $D'$ are datasets, then the adjacency means that $D$ and $D'$ differ in $1$ sample) and for any $S \subseteq \cR$
   \begin{equation}
       \Pr\left(\cM(D) \in S\right) \leq e^\varepsilon \Pr\left(\cM(D') \in S\right) + \delta. \label{eq:DP_definition}
   \end{equation}
\end{definition}
In this definition, the smaller $\varepsilon, \delta$ are, the more private the method is. Intuitively, if inequality \eqref{eq:DP_definition} holds with small values of $\varepsilon$ and $\delta$, it becomes difficult to infer the specific data point that differs between two similar datasets based solely on the output of $\cM$.



\subsection{Related Work}

\paragraph{Differential Privacy.} The most common approach to obtaining DP guarantees is to clip each client's update, i.e., by bounding their $\ell_2$ norm, and adding a calibrated amount of Gaussian noise to each update or the average. This is typically sufficient to obscure the influence of any single client \citep{mcmahan2017learning}. Commonly, two scenarios of the DP model are considered: \textit{the central model} and \textit{the local model.} In the first setting, central privacy, a trusted server collects updates and adds noise only before updating the server-side model. This ensures that client data remains private from external parties. In the second setting, local privacy, client data is protected even from the server by clipping and adding noise to updates locally before sending them to the server, ensuring privacy from both the server and other clients \citep{kasiviswanathan2011can, allouah2024privacy}. The local privacy setting offers stronger privacy against untrusted servers but results in poorer learning performance due to the need for more noise to obscure individual updates \citep{chan2012optimal, duchi2018minimax}. This can be improved by using a secure shuffler \citep{erlingsson2019amplification, balle2019privacy}, which permutes updates, or a secure aggregator \citep{bonawitz2017practical}, which sums updates before sending them to the server. These methods anonymize updates and enhance privacy while maintaining reasonable learning performance, even without a fully trusted server. Finally, \citep{chaudhuri2022privacy, hegazy2024compression} show that when DP is required, one can also achieve compression of updates for free.

In this work, we adopt the local DP model by injecting Gaussian noise into each client's update. However, the average noise can also be viewed as noise added to the average update. Therefore, \algname{Clip21-SGD2M} is compatible with all the aforementioned techniques and can also be applied to the central DP model with a smaller amount of noise.





\paragraph{Distributed methods with clipping.} In  the single-node regime, \algname{Clip-SGD} has been analyzed under various assumptions by many authors \citep{zhang2020why, zhang2020adaptive, zhang2020improved, gorbunov2020stochastic, cutkosky2021high, sadiev2023high, liu2023high}. Of course, these results can be generalized to the multi-node case if clipping is applied to the aggregated (e.g. averaged) vector, although mini-batching requires a refined analysis when the noise is heavy-tailed\citep{kornilov2024accelerated}. However, to get DP, clipping has to be applied to the vectors communicated by clients to the server. In this regime, \algname{Clip-SGD} is not guaranteed to converge even without any stochastic noise in the gradients \citep{chen2020understanding, khirirat2023clip21}. There exist several approaches to bypass this limitation that can be split into two lines of work. The first one relies on explicit or implicit assumptions about bounded heterogeneity. More precisely, \citet{liu2022communication} analyze a version of \algname{Local-SGD}/\algname{FedAvg} \citep{mangasarian1995parallel, mcmahan2017communication} with gradient clipping for homogeneous data case assuming that the stochastic gradients have symmetric distribution around their mean and \citet{wei2020federated} consider \algname{Local-SGD} with clipping of the models and analyze its convergence under bounded heterogeneity assumption. Moreover, the boundedness of the stochastic gradient is another assumption used in the literature but it implies the boundedness of gradients' heterogeneity of clients as well. This assumption is used in numerous works, including: i) \citet{zhang2022understanding} in the analysis of a version of \algname{FedAvg} with clipping of model difference (also empirically studied by \citet{geyer2017differentially}), ii) \citet{noble2022differentially} who propose and analyze a version of \algname{SCAFFOLD} \citep{karimireddy2020scaffold} with gradient clipping (\algname{DP-SCAFFOLD}), iii) \citet{li2023convergence} who propose and analyze a version of \algname{BEER} \citep{li2021page} with gradient clipping (\algname{PORTER}) under bounded gradient and/or bounded data heterogeneity assumption, and iv) \citet{allouah2024privacy} who study a version of \algname{Gossip-SGD} \citep{nedic2009distributed} with gradient clipping (\algname{DECOR}). Although most of the mentioned works have rigorous DP guarantees, the corresponding methods are not guaranteed to converge for arbitrary heterogeneous problems. 

The second line of work focuses on the clipping of shifted (stochastic) gradient. In particular, \citet{khirirat2023clip21} proposed and analyzed \algname{Clip21-GD}, which is based on the application of \algname{EF21} \citep{richtarik2021ef21} to the clipping operator, and \citet{gorbunov2024high} develop and analyze methods that apply clipping to the difference of stochastic gradients and learnable shift -- an idea that was initially proposed by \citet{mishchenko2019distributed} to handle data heterogeneity in the Distributed Learning with unbiased communication compression. However, the analysis from \citep{khirirat2023clip21} is limited to the noiseless regime, i.e., full-batched gradients are computed on workers, and both of the mentioned works do not provide\footnote{The proof of the DP guarantee by \citet{khirirat2023clip21} relies on the condition for some $C > 1$ and $\nu, \sigma_\omega \geq 0$ that implies $\min\{\nu^2, \sigma_\omega^2\} \geq C \max\{\nu^2, \sigma_\omega^2\}$. The latter one holds if and only if $\nu = \sigma_\omega = 0$, which means that no noise is added to the method since $\sigma_\omega^2$ is the variance of DP-noise.} DP guarantees. We also note that clipping of gradient differences is helpful in tolerating Byzantine attacks in the partial participation regime \citep{malinovsky2023byzantine}.



\paragraph{Error Feedback.} Error Feedback (\algname{EF}) \citep{seide20141} is a popular technique for incorporating communication compression into Distributed/Federated Learning. However, for non-convex smooth problems, the existing analysis of \algname{EF} is provided either for the single-node case or relies on restrictive assumptions such as boundedness of the gradient/compression error or boundedness of the data heterogeneity (gradient dissimilarity) \citep{stich2018sparsified, stich2019error, karimireddy2019error, koloskova2019decentralized, beznosikov2023biased, tang2019doublesqueeze, xie2020cser, sahu2021rethinking}. Moreover, the convergence bounds for \algname{EF} also depend on the data heterogeneity, which is not an artifact of the analysis as illustrated in the experiments on strongly convex problems \citet{gorbunov2020linearly}. \citet{richtarik2021ef21} address this limitation and propose a new version of Error Feedback called \algname{EF21}. However, the existing analysis of \algname{EF21-SGD} requires the usage of large batch sizes to achieve any predefined accuracy \citep{fatkhullin2021ef21}. It turns out that the large batch size requirement is unavoidable for \algname{EF21-SGD} to converge, but this issue can be fixed using momentum \citep{fatkhullin2024momentum}. Momentum is also helpful in the decentralized extensions of Error Feedback \citep{yau2022docom, huang2023stochastic, islamov2024near}.






\section{Non-Convergence of \algname{Clip-SGD} and \algname{Clip21-SGD}}

We start with a discussion of the key limitation of \algname{Clip-SGD} (Algortihm~\ref{alg:clip_sgd}) and \algname{Clip21-SGD} (Algorithm~\ref{alg:clip21}) -- their potential non-convergence.

%\begin{minipage}{0.49\textwidth}
    \begin{algorithm}[t]
   \caption{\algname{Clip-SGD} \citep{abadi2016deep}}
   \label{alg:clip_sgd}
   \centering
\begin{algorithmic}[1]
  \Require $x^0 \in \R^d$, stepsize $\gamma > 0$, clipping parameter $\tau > 0$
   \For{$t=0, \ldots, T-1$}
        \For{$i=1,\dots, n$ in parallel}
            \State $g_i^{t} = \clip_{\tau}(\nabla f_i(x^{t},\xi_i^{t}))$ 
        \EndFor
        \State $g^{t} = \frac{1}{n}\sum_{i=1}^n g_i^{t}$
        \State $x^{t+1} = x^t - \gamma g^t$
    \EndFor
\end{algorithmic}
\end{algorithm}


We start by restating the example from \citep{chen2020understanding} illustrating the potential non-convergence of \algname{Clip-SGD} even when full gradients are computed on clients (\algname{Clip-GD}).
\begin{example}[Non-Convergence of \algname{Clip-GD} \citep{chen2020understanding}]
    Let $n = 2$, $d = 1$, and $f_1(x) = \frac{1}{2}(x - 3)^2$, $f_2(x) = \frac{1}{2}(x+3)^2$ in problem \eqref{eq:problem} having a unique solution $x^* = 0$. Consider \algname{Clip-GD} with $\tau = 1$ applied to this problem. If for some $t_0$ we have $x^{t_0} \in [-2,2]$ in \algname{Clip-GD}, then $g^t = 0$ and $x^t = x^{t_0}$ for any $t\geq t_0$, which can be seen via direct calculations. In particular, for any $x^0 \in [-2,2],$ the method does not move away from $x^0$.
\end{example}

To address the non-convergence of \algname{Clip-GD}, \citet{khirirat2023clip21} propose \algname{Clip21-GD} that applies the clipping operator to the difference between $\nabla f_i(x^{t+1})$ and the shift $g_i^t$, which is designed to approximate $\nabla f_i(x^{t})$. In the deterministic case, this strategy ensures that after a certain number of steps, clipping turns off on all clients since $\|\nabla f_i(x^{t+1}) - g_i^t\|$ becomes smaller than $\tau$ for all $i \in [n]$ eventually. However, when workers compute stochastic gradients instead of the full gradients, \algname{Clip21-SGD} can be non-convergent as well. To illustrate this, we consider the ideal version of \algname{Clip21-SGD} with stochastic gradients, i.e., instead of $g_i^t$, we use $\nabla f_i(x^{t+1})$ as a shift:
\begin{flalign}
x^{t+1} &= x^t - \gamma g^t, \quad g^t = \frac{1}{n}\sum_{i=1}^n g_i^{t},\label{eq:clip21_ideal}\\
g_i^{t+1} &= \nabla f_i(x^{t+1}) + \clip_{\tau}(\nabla f_i(x^{t+1},\xi^{t+1}_i) - \nabla f_i(x^{t+1})).\notag%\label{eq:clip21_ideal_g_i^t+1}
\end{flalign}

\begin{algorithm}[t]
\caption{\algname{Clip21-SGD} \citep{khirirat2023clip21}}
\label{alg:clip21}
\centering
\begin{algorithmic}[1]
  \Require $x^0, g^0 \in \R^d$, stepsize $\gamma > 0$, clipping parameter $\tau > 0$, $g_i^0 = g^0$ for all $i\in [n]$
    \For{$t=0, \ldots, T-1$}
        \State $x^{t+1} = x^t - \gamma g^t$
        \For{$i=1,\dots, n$ in parallel}
            \State $c_i^{t+1} = \clip_{\tau}(\nabla f_i(x^{t+1},\xi^{t+1}_i) - g_i^t)$
            \State $g_i^{t+1} = g_i^t + c_i^{t+1}$
        \EndFor
        \State $g^{t+1} = g^t + \frac{1}{n}\sum_{i=1}^n c_i^{t+1}$
        \State 
    \EndFor
\end{algorithmic}
\end{algorithm}


The next theorem shows that even this (ideal) version of stochastic \algname{Clip21-SGD} fails to converge even for a simple quadratic problem with sub-Gaussian noise.
\begin{theorem}\label{th:clip21_non_convergence}
    Let $L, \sigma > 0,$ $0 < \gamma \le 1/L, n=1$. There exists a convex, $L$-smooth problem, clipping parameter $\tau < \nicefrac{3\sigma\sqrt{3}}{10}$, and an unbiased stochastic gradient satisfying Assumption~\ref{asmp:batch_noise} such that the method \eqref{eq:clip21_ideal} is run with a stepsize $\gamma$ and clipping parameter $\tau$, then for all $x^0 \in \{(0, x_{(2)}^0) \in\R^2 \mid x_{(2)}^0 < 0\}$
    we have 
    \[
    \E{\|\nabla f(x^T)\|^2} \ge \frac{1}{2}\min\left\{\|\nabla f(x^0)\|^2, \frac{\tau^2}{45}\right\}.
    \]
    Moreover, fix $0 < \varepsilon  < \nicefrac{L}{\sqrt{2}}$ and $x^0 = (0,-1)^\top.$ Let the sub-Gaussian variance of stochastic gradients is bounded by $\nicefrac{\sigma^2}{B}$ where $B$ is a batch size. If $B < \nicefrac{27\sigma^2}{(60\varepsilon^2)}$ and $\tau \ge \nicefrac{\varepsilon}{(3\sqrt{10})}$, then we have $\E{\|\nabla f(x^T)\|^2} > \varepsilon^2$ for all $T > 0.$ 
\end{theorem}
We also illustrate the above result with simple numerical experiments reported in Figure~\ref{fig:nonconvergence}. The left figure shows that \algname{Clip21-SGD} diverges from the initial function sub-optimality level while the right one demonstrates non-improvement with the number of workers $n$~--- one of the desired properties of algorithms for FL.



\begin{figure}[!t]
    \centering
    \begin{tabular}{cc}
        \hspace{-3mm}\includegraphics[width=0.49\linewidth]{Figures/simple_example_logscale_train_loss_synthetic_LogReg_1_10000.pdf} & 
        \hspace{-3mm}\includegraphics[width=0.49\linewidth]{Figures/simple_example_logscale_n_train_loss_synthetic_LogReg_1_10000.pdf}
    \end{tabular}
    
    \caption{{\bf Left:} behavior of stochastic \algname{Clip21-SGD} and \algname{Clip21-SGD2M} without DP noise (see Algorithm~\ref{alg:Clip-SGDM}) initialized at $x^0 = (0, -0.07)^\top$,  with stepsize $\gamma = \nicefrac{1}{\sqrt{T}}$ where $T=10^4$, i.e., close to the solution and small stepsize. We observe that \algname{Clip21-SGD} escapes the good neighborhood of the solution for the problem from Theorem~\ref{th:clip21_non_convergence} with $n=1, L=2, \sigma=5,$ and varying $\tau \in\{1,0.1,0.01\}.$ In contrast, \algname{Clip21-SGD2M} remains stable around the solution. {\bf Right:} convergence of \algname{Clip21-SGD} does not improve with the increase of $n$ for the same problem.}
    \label{fig:nonconvergence}
\end{figure}

\section{\algname{Clip21-SGD2M}: New Method and Theoretical Results}





This section introduces \algname{Clip21-SGD2M} (Algorithm~\ref{alg:Clip-SGDM}), a novel distributed method with clipping that can be viewed as an enhanced version of \algname{Clip21-SGD}, integrating momentum and DP-noise. That is, to control the noise coming from the stochastic gradients, we introduce momentum buffers $\{v_i^{t}\}_{i\in [n]}$ on the clients and clip $\{v_i^{t+1} - g_i^t\}_{i\in[n]}$ in contrast to the stochastic version of \algname{Clip21-SGD} that applies clipping to potentially noisier vectors $\{\nabla f_i(x^{t+1}, \xi_i^{t+1}) - g_i^t\}_{i\in[n]}$. Moreover, similarly to \algname{Clip21-SGD} -- which can be seen as \algname{EF21} \citep{richtarik2021ef21} where the compression operator is replaced with clipping -- \algname{Clip21-SGD2M} with $\hat\beta = 1$ can also be interpreted as \algname{EF21M} \citep{fatkhullin2024momentum} with the same replacement. However, a crucial part of \algname{Clip21-SGD2M} is the second momentum parameterized by $\hat\beta$. This is a key component of the method allowing it to control the DP-noise and preventing the method from the rapid accumulation of DP-noise in the update direction $g^t$. We note that the double-momentum in \algname{Clip21-SGD2M} is noticeably different from the existing algorithmic ideas that are also called double-momentum. That is, in contrast to \algname{EF21-SGD2M} from \citep{fatkhullin2024momentum}, we do not apply explicit momentum on the clients on top of the first one (this is why \algname{Clip21-SGD2M} does not reduce to \algname{EF21-SGD2M} when the clipping operator is formally replaced with the compression operator). Moreover, in contrast to \algname{$\mu^2$-SGD} from \citep{levy2023stable}, \algname{Clip21-SGD2M} does not use iterates averaging and \algname{STORM}-like estimator \citep{cutkosky2019momentum}, and, unlike \algname{AdEMAMix} \citep{pagliardini2024ademamix}, \algname{Clip21-SGD2M} does not use a mixture of two momentum buffers. The reason why we say that \algname{Clip21-SGD2M} has double momentum can be explained as follows: if $\sigma_\omega = 0$ (no DP-noise), $\beta = 1$ (the first momentum is ``turned off''), and $\tau = \infty$ (no clipping), then $g^{t+1} = (1-\hat\beta)g^t + \frac{\hat\beta}{n}\sum_{i=1}^n \nabla f_i(x^{t+1}, \xi_i^{t+1})$, i.e., the method reduces to the standard \algname{SGD} with the heavy-ball momentum \citep{polyak1964some}. 

Next, both \algname{EF21} and \algname{EF21M} rely on the contractiveness property of the compression operator $\cC(x)$, i.e., the (randomized) mapping $\cC:\R^d \to \R^d$ should satisfy
\begin{equation}
    \E{\|\cC(x) - x\|^2} \leq (1 - \nu)\|x\|^2 \text{ for some } \nu \in (0,1], \label{eq:contractive_operator}
\end{equation}
where the expectation is w.r.t.\ the randomness of $\cC$. As shown and discussed by \citet{khirirat2023clip21},  clipping satisfies a condition that resembles \eqref{eq:contractive_operator} namely
\begin{equation}
    \|\clip_\tau(x) - x\|^2 \leq \begin{cases}
        0,& \text{if } \|x\| \leq \tau,\\ \left(1 - \frac{\tau}{\|x\|}\right)^2\|x\|^2,& \text{if } \|x\| > \tau,
    \end{cases} \label{eq:clipping_contraction}
\end{equation}
but there is a significant difference: if $\|x\| > \tau$, the contraction factor depends on $x$ and can be arbitrarily close to $1$. To circumvent this issue, \citet{khirirat2023clip21} prove via induction that for all iterates of \algname{Clip21-GD}, the vectors $\nabla f_i(x^{t+1}) - g_i^t$ have norms bounded by some constant depending on the starting point. We show that a similar statement holds for \algname{Clip21-SGD2M} when the clients compute full-batch gradients and no DP-noise is added, and we start our analysis with this important case. We also present the results in the stochastic case with and without DP noise. 

\begin{algorithm}[t]
\caption{\algname{Clip21-SGD2M} }
\label{alg:Clip-SGDM}
\begin{algorithmic}[1]
\Require $x^0, g^0, v^0 \in \R^d$ (by default $g^0 = v^0 = 0$), momentum parameters $\beta, \hat\beta\in(0,1],$ stepsize $\gamma > 0,$ clipping parameter $\tau > 0$, \textcolor{darkgreen}{DP-variance parameter $\sigma^2_{\omega} \ge 0$} 
    \State Set $g_i^0 = g^0$ and $v_i^0 = v^0$ for all $i\in [n]$
    \For{$t=0, \ldots, T-1$}
        \State $x^{t+1} = x^t - \gamma g^t$
        \For{$i=1,\dots, n$}
            \State $v_i^{t+1} = (1-\beta)v_i^t + \beta\nabla f_i(x^{t+1},\xi^{t+1}_i)$
            \State \textcolor{darkgreen}{$\omega_i^{t+1} \sim \cN(0,\sigma^2_{\omega}\mI)$} \hfill{\small \textcolor{darkgreen}{only for DP version}}
            \State $c_i^{t+1} = \clip_{\tau}(v_i^{t+1} - g_i^t) +$ \textcolor{darkgreen}{$\omega_i^{t+1}$}
            \State %$g_i^{t+1} = g_i^t + \hat\beta c_i^{t+1} - \hat\beta\omega_i^{t+1} = g_i^t + \hat\beta\clip_{\tau}(v_i^{t+1} - g_i^t)$
            $g_i^{t+1} = g_i^t + \hat\beta\clip_{\tau}(v_i^{t+1} - g_i^t)$
        \EndFor
        \State $g^{t+1} = g^t + \frac{\hat\beta}{n}\sum_{i=1}^nc_i^{t+1}$
    \EndFor
\end{algorithmic}	
\end{algorithm}



\subsection{Analysis in the Deterministic Case}

The next result derives a convergence rate for \algname{Clip21-SGD2M} when $\nabla f_i(x^{t+1},\xi^{t+1}_i) \equiv \nabla f_i(x^t)$ almost surely, i.e., Assumption~\ref{asmp:batch_noise} holds with $\sigma = 0$.


\begin{restatable}[Simplified]{theorem}{theoremdeterministic}\label{th:theorem_deterministic}
%\begin{theorem}
    Let Assumptions \ref{asmp:smoothness} and \ref{asmp:batch_noise} with $\sigma = 0$ hold. Let $B\eqdef\max_i\|\nabla f_i(x^0)\| > 3\tau$ and $\Delta \ge f(x^0) - f^*.$ Then, for any constant $\hat{\beta}\in (0,1]$, there exists a stepsize $\gamma\le \min\{\nicefrac{1}{12L}, \nicefrac{\tau}{12BL}\}$ and momentum parameter $\beta=4L\gamma$ such that the iterates of \algname{Clip21-SGD2M} (Algorithm \ref{alg:Clip-SGDM}) converge with the rate
    \begin{align}
    \frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2  \le \cO\left(\frac{L\Delta (1 + \nicefrac{B}{\tau})}{T}\right). \label{eq:deterministic_rate}
    \end{align}
    Moreover, after at most $\frac{2B}{\hat{\beta}\tau}$ iterations, the clipping will eventually be turned off for all workers.
%\end{theorem}
\end{restatable}
\vspace{-3mm}
\begin{proof}[Proof sketch]
    The proof of Theorem~\ref{th:theorem_deterministic} (and all following ones) relies on a similar Lyapunov function that is used by \citet{fatkhullin2024momentum} in the analysis of \algname{EF21M}: 
\begin{align}\label{eq:lyapunov_function}
    \Phi^t \eqdef f(x^t) - f^*
    + \frac{2\gamma}{\hat{\beta}\eta}\frac{1}{n}\sum_{i=1}^n\|g_i^t - v_i^t\|^2
    + \frac{8\gamma\beta}{\hat{\beta}^2\eta^2}\frac{1}{n}\sum_{i=1}^n\|v_i^t - \nabla f_i(x^t)\|^2
    + \frac{2\gamma}{\beta}\|v^t - \nabla f(x^t)\|^2,
\end{align}
where the only (yet crucial) difference is in the division of the first two sums by $\hat\beta$. In the definition of $\Phi^t$, the only parameter that was not introduced earlier in the paper is $\eta,$ and it hides the main technical difficulty of the proof. That is, by induction we prove that $\|v_i^{t+1} - g_i^t\| \leq \nicefrac{\tau}{\eta}$ for some $\eta \sim \tau$ defined in the proof. This bound is essential in deriving a descent of each term in the Lyapunov function.
In view of \eqref{eq:contractive_operator} and \eqref{eq:clipping_contraction}, this allows us to consider clipping as a contractive compression operator for vectors $v_i^{t+1} - g_i^t$ generated by the method, and also this allows us to use the same Lyapunov function as in the analysis of \algname{EF21M}. We defer the detailed proof to Appendix~\ref{appendix:proof_deterministic}.
\end{proof}
The above result establishes a $\cO(\nicefrac{1}{T})$ convergence rate that is optimal for non-convex smooth first-order optimization \citep{carmon2020lower, carmon2021lower}. This result matches the one obtained by \citet{khirirat2023clip21}, and, in particular, similarly to \algname{Clip21-SGD}, \algname{Clip21-SGD2M} turns off clipping on each client after a finite number of steps $t$ satisfying $\|v_i^{t+1} - g_i^t\| \leq \tau$. We also emphasize that Theorem~\ref{th:theorem_deterministic} holds without bounded heterogeneity/gradient assumption. In contrast, even with bounded heterogeneity/gradient assumption, many existing convergence results in the non-convex case \citep{liu2022communication, zhang2022understanding, li2023convergence, allouah2024privacy} do not recover the $\cO(\nicefrac{1}{T})$ rate in the noiseless regime.






\subsection{Analysis in the Stochastic Case without DP-Noise}\label{section:stochastic}

Next, we turn to the stochastic setting where each worker has access to local gradient estimators satisfying \Cref{asmp:batch_noise}. For simplicity, we first consider the case when no DP noise is added. 

\begin{restatable}[Simplified]{theorem}{theoremstochastic}\label{th:theorem_stochastic}
%\begin{theorem}
    Let Assumptions \ref{asmp:smoothness} and \ref{asmp:batch_noise} hold and $\alpha \in (0,1)$.  Let $\wtilde{B} \eqdef \max_i\|\nabla f_i(x^0)\| > 3\tau$ and $\Delta \ge \Phi^0.$ Then, for any constant $\hat{\beta}\in (0,1]$, there exists a stepsize $\gamma$ and momentum parameter $\beta$ such that the iterates of \algname{Clip21-SGD2M} (Algorithm \ref{alg:Clip-SGDM}) with probability at least $1-\alpha$ are such that $\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2$ is bounded by
    \begin{equation}
        \wtilde{\cO}\left(\frac{L\Delta(1 + \nicefrac{\wtilde{B}}{\tau})}{T} 
        + \frac{\sigma(\sqrt{L\Delta} + \wtilde{B}+\sigma)}{\sqrt{Tn}}\right) \label{eq:stochastic_rate}
    \end{equation}
    where $\wtilde{\cO}$ hides constant and logarithmic factors, and higher order terms that decrease in $T$.
%\end{theorem}
\end{restatable}
\begin{proof}[Proof sketch]
    The core of the proof is similar to the one of Theorem~\ref{th:theorem_deterministic}. However, in contrast to the deterministic case, the vectors $v_i^{t+1} - g_i^t$ are stochastic, meaning that under Assumption~\ref{asmp:batch_noise}, they can have arbitrarily large norms. Therefore, we focus on the high-probability analysis and prove by induction that the vectors $v_i^{t+1} - g_i^t$ are bounded \emph{with high probability}, meaning that clipping can be seen as a contractive compressor with high probability for the vectors $v_i^{t+1} - g_i^t$ generated by the method. The proof is also based on a refined estimation of sums of martingale difference sequences; see the details in Appendix~\ref{appendix:stoch_proof_no_DP}. 
\end{proof}
This result demonstrates that \algname{Clip21-SGD2M} achieves an optimal $\cO(\nicefrac{1}{\sqrt{nT}})$ \citep{arjevani2023lower} rate in the stochastic setting. In contrast to the previous works establishing similar rates \citep{liu2022communication, noble2022differentially, allouah2024privacy}, our result does not rely on the boundedness of the gradients or data heterogeneity. 
Moreover, when $\sigma = 0$ (no stochastic noise), the rate from \eqref{eq:stochastic_rate} becomes $\cO(\nicefrac{1}{T})$, recovering the one given by Theorem~\ref{th:theorem_deterministic}. 


\subsection{Analysis in the Stochastic Case with DP-Noise}

Finally, we provide the convergence result for \algname{Clip21-SGD2M} with DP-noise.
\begin{restatable}[]{theorem}{theoremstochasticdp}\label{th:theorem_stochastic_and_dp}
%\begin{theorem}
    Let Assumptions \ref{asmp:smoothness} and \ref{asmp:batch_noise} hold and $\alpha \in (0,1)$. Let $\Delta \geq \Phi^0$.
    Then, there exists a stepsize $\gamma$ and momentum parameters $\beta, \hat{\beta}$ such that the iterates of \algname{Clip21-SGD2M} (Algorithm \ref{alg:Clip-SGDM}) with the DP-noise variance $\sigma_{\omega}^2$  with probability at least $1-\alpha$ are such that $\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2$ is bounded by
    \begin{align}\label{eq:theorem_stochastic_and_dp}
        & 
        \wtilde{\cO}\left(\left(
        \frac{L\Delta\sigma d\sigma_{\omega}^{2}\wtilde{B}^{2}}{(nT)^{3/2}\tau^{2}} \left(\sqrt{L\Delta}
        + \wtilde{B}
        + \sigma\right)\right)^{1/3}  + 
        \frac{\sqrt{L\Delta d}\sigma_{\omega}}{\tau\sqrt{nT}}\left(\sqrt{L\Delta}+\wtilde{B} + \sigma\right)
        \right),
    \end{align}
    where $\wtilde{\cO}$ hides constant and logarithmic factors, and higher order terms decreasing in $T$.
%\end{theorem}
\end{restatable}

In the special case of local Differential Privacy, the noise level has to be chosen in a specific way. In this setting, we obtain the following privacy-utility trade-off.

\begin{restatable}[]{corollary}{theoremdputility}\label{cor:dp_privacy_utility_tradeoff}
    Let Assumptions \ref{asmp:smoothness} and \ref{asmp:batch_noise} hold and $\alpha\in (0,1).$ Let $\Delta \geq \Phi^0$ and $\sigma_{\omega}$ be chosen as $\sigma_{\omega} = \Theta\left(\frac{\tau}{\varepsilon}\sqrt{T\log\left(\frac{T}{\delta}\right) \log\left(\frac{1}{\delta}\right)}\right)$ for some $\varepsilon, \delta \in (0,1)$. Then, there exists a stepsize $\gamma$ and momentum parameters $\beta, \hat{\beta}$ such that the iterates of \algname{Clip21-SGD2M} (Algorithm \ref{alg:Clip-SGDM}) with probability at least $1-\alpha$ satisfy local $(\varepsilon,\delta)$-DP and 
     \begin{equation}
        \hspace{-8pt}\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2 \le 
        \wtilde{\cO}\left(
        \frac{\sqrt{L\Delta d}}{\sqrt{n}\varepsilon }(\sqrt{L\Delta} +  \wtilde{B} + \sigma)
        \right),
    \end{equation}
    where $\wtilde{\cO}$ hides constant and logarithmic factors, and terms decreasing in $T$.
\end{restatable}
 

The proof of the above result is deferred to Appendix~\ref{appendix:corollary_1}. The derived privacy-utility trade-off closely aligns with the known lower bound for locally private algorithms \citep{duchi2018minimax}, differing by at most a factor of $\nicefrac{(\sqrt{L\Delta} +  \wtilde{B} + \sigma)}{\sqrt{L\Delta}}$ and logarithmic factors. However, our experimental results show that \algname{Clip21-SGD2M} achieves a privacy-utility trade-off comparable to, or even better than, \algname{Clip21-SGD}. We leave the question of potential improvement of this factor to future work. Theorems~\ref{th:theorem_deterministic} and \ref{th:theorem_stochastic} and Corollary~\ref{cor:dp_privacy_utility_tradeoff} indicate that \algname{Clip21-SGD2M} achieves optimal convergence rates in both deterministic and stochastic regime, and also has a near-optimal privacy-utility trade-off without boundedness of the gradients/data heterogeneity assumptions. 



\section{Experiments} 

In this section, we provide an empirical evaluation of the proposed algorithm against baselines such as \algname{Clip21-SGD} \citep{khirirat2023clip21} and \algname{Clip-SGD}. The learning rate and momentum (for \algname{Clip21-SGD2M}) are tuned in all experiments. We refer to \Cref{sec:appendix_exp} for further details.

\begin{figure*}[!t]
    \centering
    \begin{tabular}{cccc}
        \hspace{-3pt}\includegraphics[width=0.24\linewidth]{Figures/mini_batch_taus_comparison_logscale_train_loss_tau_0.0001_duke.bz2_logreg_3_None_None_10000.pdf} & 
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/mini-batch_taus_comparison_logscale_train_loss_tau_0.0001_leu.t.bz2_logreg_4_None_None_10000.pdf} &
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/gaussian_taus_comparison_logscale_train_loss_tau_0.0001_duke.bz2_logreg_None_0.05_None_10000.pdf} &
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/gaussian_taus_comparison_logscale_train_loss_tau_0.0001_leu.t.bz2_logreg_None_0.05_None_10000.pdf}\\
        {\rm Duke} &
        {\rm Leukemia} &
        {\rm Duke} &
        {\rm Leukemia}
        
    \end{tabular}
    \vspace{-3mm}
    
    \caption{Comparison of tuned \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} on logistic regression with non-convex regularization for various clipping radii $\tau$ with mini-batch ({\bf two left}) and Gaussian-added ({\bf two right}) stochastic gradients. The final gradient norm is averaged over the last $100$ iterations. The gradient norm dynamics are reported in \Cref{fig:logreg_convergence_plots}.}
    \label{fig:tau_logscale}
\end{figure*}




First, we test the convergence of \algname{Clip-SGD}, \algname{Clip21-SGD}, and the proposed \algname{Clip21-SGD2M} algorithms with stochastic gradients for various clipping radii $\tau$ on several workloads. These results demonstrate the significance of using the momentum technique to achieve better performance. 


% \vspace{-2mm}
\paragraph{Non-convex Logistic Regression.} We demonstrate the performance of all algorithms without adding noise for privacy but with stochastic gradients. We consider two cases: adding Gaussian noise to full local gradient $\nabla f_i(x)$ and mini-batch stochastic gradient. We conduct experiments on logistic regression with non-convex regularization, namely, $f_i(x) = \frac{1}{m}\sum_{j=1}^m\log(1+\exp(-b_{ij}a_{ij}^\top x)) + \lambda\sum_{l=1}^d\frac{x_l^2}{1+x_l^2}$ which is a typical problem considered in previous works \citep{ khirirat2023clip21,li2023convergence}. We use the Duke and Leukemia \citep{chang2011libsvm} datasets. 

We tune the stepsize $\gamma$ for all algorithms, and momentum parameter $\beta$ for \algname{Clip21-SGD2M}. Moreover, we set $\hat{\beta}=1$ since we do not add DP noise in this set of experiments. The detailed tuning details are provided in \Cref{sec:stoch_setting_appendix}. We plot the gradient norm averaged across the last $100$ iterations and  $3$ different runs in \Cref{fig:tau_logscale}. The results demonstrate the resilience of \algname{Clip21-SGD2M} to the choice of the clipping radius $\tau$: it achieves a smaller or similar gradient norm compared to two other algorithms over all values of $\tau.$ This is especially visible when the clipping radius $\tau$ is small. These experimental findings align with the theoretical results presented in this work. Besides, the convergence plots are presented in \Cref{fig:logreg_convergence_plots}. The results demonstrate that \algname{Clip21-SGD2M} converges faster than competitors. 






\paragraph{Training Resnet20 and VGG16.} Next, we conduct experiments in training Resnet20 \citep{he2016deep} and VGG16 \citep{simonyan2014very} models on CIFAR10 dataset \citep{krizhevsky2014cifar10}\footnote{We use the code base from \citep{horvath2020better} with small modifications.}. The results are averaged across $3$ different random seeds and shown in \Cref{fig:tau_logscale_neural_nets} (the clipping operator is applied on all weights simultaneously) and \Cref{fig:tau_layer-wise_logscale_neural_nets} (the clipping operator is applied layer-wise). Similar to the previous section, we tune the stepsize $\gamma$ for all algorithms and the momentum parameter $\beta$ for \algname{Clip21-SGD2M} while setting $\hat{\beta}=1.$ The tuning details are deferred to \Cref{sec:appendix_exp_nn_stoch}. We plot the test accuracy and train loss at the last point of the training. The results show that the performance of \algname{Clip-SGD} consistently deteriorates as the clipping radius $\tau$ decreases, while \algname{Clip21-SGD} and \algname{Clip21-SGD2M} are more stable to the changes of $\tau.$ Moreover, \algname{Clip21-SGD2M} outperforms \algname{Clip21-SGD} for small values of $\tau$ reaching smaller train loss and larger test accuracy that supports the theoretical claims of this paper. We report the training loss and test accuracy dynamics during the training in Figures \ref{fig:vgg16_cifar10}-\ref{fig:vgg16_cifar10_layerwise} for VGG and in Figures \ref{fig:resnet20_cifar10}-\ref{fig:resnet20_cifar10_layerwise} for Resnet20.


\begin{figure*}[!t]
    \centering
    \begin{tabular}{cccc}
        \hspace{-3pt}\includegraphics[width=0.24\linewidth]{Figures/resnet20_taus_comparison_test_acc_cifar10_0.0001_None_0_32_None_450.pdf} & 
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/resnet20_taus_comparison_train_loss_cifar10_0.1_None_0_32_None_450.pdf} &
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/vgg16_taus_comparison_test_acc_cifar10_0.0001_None_0_32_None_450.pdf} &
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/vgg16_taus_comparison_train_loss_cifar10_0.0001_None_0_32_None_450.pdf}\\
        \multicolumn{2}{c}{Resnet20, CIFAR10} &
        \multicolumn{2}{c}{VGG16, CIFAR10} 
    \end{tabular}
    \vspace{-3mm}
    \caption{Comparison of tuned \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} on training Resnet20 ({\bf two left}) and VGG16 ({\bf two right}) models on CIFAR10 dataset where the clipping is applied globally. The train loss and test accuracy dynamics are reported in \Cref{fig:vgg16_cifar10} and \Cref{fig:resnet20_cifar10}.}
    \label{fig:tau_logscale_neural_nets}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \begin{tabular}{cccc}
        \hspace{-3pt}\includegraphics[width=0.24\linewidth]{Figures/resnet20_layer-wise_taus_comparison_test_acc_cifar10_0.0001_None_0_32_None_450.pdf} & 
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/resnet20_layer-wise_taus_comparison_train_loss_cifar10_0.0001_None_0_32_None_450.pdf} &
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/vgg16_layer-wise_taus_comparison_test_acc_cifar10_0.0001_None_0_32_None_450.pdf} &
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/vgg16_layer-wise_taus_comparison_train_loss_cifar10_0.0001_None_0_32_None_450.pdf}\\
        \multicolumn{2}{c}{Resnet20, CIFAR10} &
        \multicolumn{2}{c}{VGG16, CIFAR10} 
    \end{tabular}
    \vspace{-3mm}
    \caption{Comparison of tuned \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} on training Resnet20 ({\bf two left}) and VGG16 ({\bf two right}) models on CIFAR10 dataset where the clipping is applied layer-wise. The training loss and test accuracy dynamics are presented in \Cref{fig:vgg16_cifar10_layerwise} and \Cref{fig:resnet20_cifar10_layerwise}.}
    \label{fig:tau_layer-wise_logscale_neural_nets}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \begin{tabular}{cccc}
        \hspace{-3pt}\includegraphics[width=0.24\linewidth]{Figures/ICML_DP_comparison_final_test_acc_cnn_fine_tune_mnist_True_0_32_None_100.pdf} & 
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/ICML_DP_comparison_final_train_loss_cnn_fine_tune_mnist_True_0_32_None_100.pdf} &
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/ICML_DP_comparison_final_test_acc_mlp_fine_tune_mnist_True_0_32_None_100.pdf} &
        \hspace{-6pt}\includegraphics[width=0.24\linewidth]{Figures/ICML_DP_comparison_final_train_loss_mlp_fine_tune_mnist_True_0_32_None_100.pdf}\\
        \multicolumn{2}{c}{CNN, MNIST} &
        \multicolumn{2}{c}{MLP, MNIST} 
    \end{tabular}
    \vspace{-3mm}
    \caption{Comparison of tuned \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} on training CNN ({\bf two left}) and MLP ({\bf two right}) models on MNIST dataset varying the noise-clipping ration where the clipping is applied globally. The training loss and test accuracy dynamics are presented in \Cref{fig:conv_plots_cnn_dp_test_acc}, \ref{fig:conv_plots_cnn_dp_train_loss}, \ref{fig:conv_plots_mlp_dp_test_acc}, and \ref{fig:conv_plots_mlp_dp_train_loss}.}
    \label{fig:dp_noise_neural_nets}
\end{figure*}



\paragraph{Adding Gaussian Noise for DP.} In the second set of experiments, we test the performance of algorithms with additive Gaussian noise to preserve privacy. Since DP noise variance $\sigma_{\omega}$ typically scales with the clipping radius $\tau$ (e.g., see \Cref{cor:dp_privacy_utility_tradeoff}), we conduct the following set of experiments: we fix a noise-clipping ratio from $\{0.1, 0.3, 1.0, 3.0, 10.0\}$ for neural networks, and find such $\tau$ that gives the lowest train loss or test accuracy depending on the considered workload. The high values of the noise-clipping ratio correspond to stronger DP guarantees, while low values stand for weaker DP guarantees. For each algorithm we tune the stepsize $\gamma$, and additionally the momentum parameters $\beta$ and $\hat{\beta}$ for \algname{Clip21-SGD2M} (see \Cref{sec:appendix_exp_nn_dp}).



We conduct experiments on training CNN and MLP models on MNIST dataset \citep{deng2012mnist} varying the noise-clipping ratio. We highlight that it is a standard experiment setting considered in the literature on differential privacy \citep{papernot2020making,li2023convergence,allouah2024privacy}. The performance results are reported in \Cref{fig:dp_noise_neural_nets}. We observe that no algorithm outperforms others across all values of the noise-clipping ratio in terms of the train loss. However, \algname{Clip-SGD} typically attains smaller train loss than \algname{Clip21-SGD2M} for a large value of the noise-clipping ratio while \algname{Clip21-SGD2M} achieves smaller train loss than \algname{Clip-SGD} when that ratio is small.



\section{Conclusion and Future Work}

In this work, we introduced a new method called \algname{Clip21-SGD2M} and proved that it achieves an optimal convergence rate and near optimal privacy-utility trade-off without assuming boundedness of the gradients or boundedness of the data heterogeneity. Notably, several interesting directions remain unexplored. The first one is related to the generalization of the derived results to the case when stochastic gradients have heavy-tailed noise. Next, it would be interesting to study \algname{AdaGrad}/\algname{Adam}-type \citep{streeter2010less, duchi2011adaptive, kingma2014adam} versions of \algname{Clip21-SGD2M} due to their practical superiority over \algname{SGD} in solving Deep Learning problems. Finally, it is important to extend the current analysis of \algname{Clip21-SGD2M} to the case when generalized smoothness is satisfied \citep{zhang2020why}.



\section*{Acknowledgement}

Rustem Islamov and Aurelien Lucchi acknowledge the financial support of the Swiss National Foundation, SNF grant No 207392. Peter Richt{\'a}rik acknowledges the financial support of King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, iii) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science.


\bibliography{references}
\bibliographystyle{plainnat}


\clearpage
\appendix



\section{Notation}

For brevity, in all proofs, we use the following notation
\begin{align*}
&\delta^t \eqdef f(x^t) - f^*, \quad 
\wtilde V^t \eqdef \frac{1}{n}\sum_{i=1}^n\|g_i^t - v_i^t\|^2,\\
&\wtilde{P}^t \eqdef \frac{1}{n}\sum_{i=1}^n\|v_i^t - \nabla f_i(x^t)\|^2, \quad 
P^t \eqdef \|v^t - \nabla f(x^t)\|^2,\\
&R^t \eqdef \|x^{t+1} - x^t\|^2.
\end{align*}
We additionally denote $\eta^t_i \eqdef \frac{\tau}{\|v_i^t-g_i^{t-1}\|}$ and $\eta\eqdef \frac{\tau}{B}$ where $B$ is defined in each section (it is different in deterministic and stochastic settings). Besides, we define $\cI_t \eqdef \{i\in[n]\mid \|v_i^t - g_i^{t-1}\| > \tau\}.$


We denote $\theta_i^t \eqdef \nabla f_i(x^t,\xi^t_i) - \nabla f_i(x^t).$ From \Cref{asmp:batch_noise}, we have that $\theta_i^t$ is zero-centered $\sigma$-sub-Gaussian random vector conditioned at $x^t,$ namely
\begin{equation}
\E{\theta^t_i\mid x^t} = 0, \quad \E{\exp\left(\frac{\|\theta^t_i\|^2}{\sigma^2}\right) \mid x^t} \leq \exp(1), %\Pr(\|\theta^t_i\| > b) \le 2\exp\left(-\frac{b^2}{2\sigma^2}\right) \quad \forall b > 0.
\end{equation}
which is equivalent to
\begin{equation}
    \Pr(\|\theta^t_i\| > b) \le 2\exp\left(-\frac{b^2}{2\sigma^2}\right) \quad \forall b > 0 \label{eq:sub_Gaussian_alternative}
\end{equation}
up to the numerical factor in $\sigma$ \citep{vershynin2018high}. Moreover, we define an average of $\theta^t_i$ as $\theta^t \eqdef \frac{1}{n}\sum_{i=1}^n \theta^t_i,$ an average of $\omega^t_i$ as $\Omega^t = \frac{1}{n}\sum_{l=1}^t \sum_{i=1}^n\omega_i^l,$ and an average of $g_i^t$ as $\overline{g}^t = \frac{1}{n}\sum_{i=1}^n g_i^t$. Thus, we have the following relation between $g^t$ and $\overline{g}^t:$
\begin{equation}\label{eq:gt_gt_hat}
    g^t = \overline{g}^t + \hat{\beta}\Omega^t.
\end{equation}
Indeed, it is true at iteration $0$ by the initialization. Let us assume that it holds at iteration $t$, then we have
\[
    g^{t+1} = g^t + \frac{\hat{\beta}}{n}\sum_{i=1}^n (\clip_\tau(v_i^{t+1} - g_i^t) +\omega_i^{t+1}) = 
    \overline{g}^t 
    + \hat{\beta}\Omega^t 
    + \frac{\hat{\beta}}{n}\sum_{i=1}^n (\clip_\tau(v_i^{t+1} - g_i^t) +\omega_i^{t+1}) = \overline{g}^{t+1} + \hat{\beta}\Omega^{t+1},
\]
i.e., it holds at iteration $t+1$ as well. 


\section{Useful Lemmas}

\begin{lemma}[Lemma C.3 in \citep{gorbunov2019optimal}]\label{lem:concentration_lemma} Let $\{\xi_k\}_{k=1}^N$ be the sequence of random vectors with values in $\R^n$ such that 
\[
\E{\xi_k \mid \xi_{k-1},\dots, \xi_1} = 0 \text{ almost surely, } \forall k\in\{1,\dots,N\},
\]
and set $S_N \eqdef \sum_{k=1}^N \xi_k$. Assume that the sequence $\{\xi_k\}_{k=1}^N$ are sub-Gaussian, i.e.
\[
\E{\exp\left(\nicefrac{\|\xi_k\|^2}{\sigma_k^2} \mid \xi_{k-1},\dots, \xi_1\right)} \le \exp(1) \text{ almost surely, } \forall k\in\{1,\dots,N\},
\]
where $\sigma_2,\dots,\sigma_N$ are some positive numbers. Then for all $\gamma \ge 0$
\begin{equation}
\Pr\left(\|S_N\| \ge (\sqrt{2}+2\gamma)\sqrt{\sum_{k=1}^N\sigma_k^2}\right) \le \exp(-\nicefrac{\gamma^2}{3}).
\end{equation}
\end{lemma}


\begin{lemma}\label{lem:descent_deltat} Let $f$ be $L$-smooth, $\delta^t = f(x^t) - f^*$, $\{x^t\}$ be generated by \Cref{alg:Clip-SGDM}, and the stepsize $\gamma\le \frac{1}{2L}$. Then
\begin{align}
\begin{aligned}
    \delta^{t+1} &\le \delta^t  
         - \frac{\gamma}{2}\|\nabla f(x^t)\|^2 
         - \frac{1}{4\gamma}\|x^t-x^{t+1}\|^2
         + 2\gamma\|\nabla f(x^t) -v^t\|^2\\
         &\qquad +\; \frac{2\gamma}{n}\sum_{i=1}^n\|g_i^t - v^t_i\|^2
         + \gamma\hat{\beta}^2\|\Omega^t\|^2.
\end{aligned}
\end{align}
\end{lemma}
\begin{proof}
    Using $L$-smoothness of $f$ we have
     \begin{align}
         f(x^{t+1}) &\overset{(i)}{\le}f(x^t) 
         + \<\nabla f(x^t), x^{t+1} - x^t>
         + \frac{L}{2}\|x^{t+1} - x^t\|^2\notag \\
         &\overset{(ii)}{=} f(x^t) 
         -\gamma \<\nabla f(x^t), g^t>
         + \frac{L\gamma^2}{2}\|g^t\|^2\notag \\
         &\overset{(iii)}{=} f(x^t) 
         - \frac{\gamma}{2}\left(\|\nabla f(x^t)\|^2 + \|g^t\|^2 - \|\nabla f(x^t) - g^t\|^2\right)
         + \frac{L\gamma^2}{2}\|g^t\|^2\notag\\
         &= f(x^t) 
         - \frac{\gamma}{2}\|\nabla f(x^t)\|^2 
         - \frac{\gamma}{2}\|g^t\|^2(1-L\gamma)
         + \frac{\gamma}{2}\|\nabla f(x^t) - g^t\|^2\notag\\
         &\overset{(iv)}{\le} f(x^t)  
         - \frac{\gamma}{2}\|\nabla f(x^t)\|^2 
         - \frac{\gamma}{4}\|g^t\|^2
         + \frac{\gamma}{2}\|\nabla f(x^t) - g^t\|^2.
         \label{eq:pjklnqwdllnk}
     \end{align}
     where $(i)$ follows from smoothness; $(ii)$ from the update rule; $(iii)$ from $\|a-b\|^2 = \|a\|^2 + \|b\|^2 - 2\<a,b>$; $(iv)$ from the stepsize restriction $\gamma \le \frac{1}{2L}$. Using \eqref{eq:gt_gt_hat} we continue as follows 
     \begin{align}
         f(x^{t+1}) &\le f(x^t) 
         - \frac{\gamma}{2}\|\nabla f(x^t)\|^2 
         - \frac{\gamma}{4}\|g^t\|^2
         + \gamma\|\nabla f(x^t) - \overline{g}^t\|^2
         + \gamma\hat{\beta}^2\|\Omega^t\|^2\notag\\
         &\overset{(i)}{\le} f(x^t) 
         - \frac{\gamma}{2}\|\nabla f(x^t)\|^2 
         - \frac{\gamma}{4}\|g^t\|^2
         + 2\gamma\|\nabla f(x^t) -v^t\|^2
         + 2\gamma\|\overline{g}^t - v^t\|^2
         + \gamma\hat{\beta}^2\|\Omega^t\|^2\notag\\
         &\overset{(ii)}{\le} f(x^t) 
         - \frac{\gamma}{2}\|\nabla f(x^t)\|^2 
         - \frac{\gamma}{4}\|g^t\|^2
         + 2\gamma\|\nabla f(x^t) -v^t\|^2
         + \frac{2\gamma}{n}\sum_{i=1}^n\|g_i^t - v^t_i\|^2
         + \gamma\hat{\beta}^2\|\Omega^t\|^2,
     \end{align}
     where steps $(i$-$ii)$ follow from Young's inequality. It remains to subtract $f^*$ from both sides and replace $g^t$ with $\frac{1}{\gamma}(x^t - x^{t+1})$.
     
    
\end{proof}




\begin{lemma}[Lemma 4.1 in \citep{khirirat2023clip21}]\label{lem:clipping_property} The clipping operator satisfies for any $x\in\R^d$
\begin{equation}
    \|\clip_{\tau}(x) - x\|\le \max\left\{\|x\| - \tau, 0\right\}.
\end{equation}
    
\end{lemma}


\begin{lemma}[Property of smooth functions]\label{lem:smooth_func} Let $\phi\colon \R^d \to\R$ be $L$-smooth and lower bounded by $\phi^* \in\R,$ i.e. $\phi(x) \ge \phi^*$ for any $x\in\R^d.$ Then we have
\begin{equation}
    \|\nabla \phi(x)\|^2 \le 2L(\phi(x) - \phi^*).
\end{equation}
\begin{proof}
    It is a standard property of smooth functions. We refer to Theorem 4.23 of \citep{orabona2019modern}.
\end{proof}
    
\end{lemma}

\section{Proof of \Cref{th:clip21_non_convergence}}


\begin{proof}

    {\bf The case $n=1$.}  Let us consider the problem $f(x) = \frac{L}{2}\|x\|^2$. Let vectors $\{z_j\}_{j=1}^3$ be defined as
    \[
    z_1 = \begin{pmatrix}
        3 \\ 0
    \end{pmatrix}\sqrt{\frac{3\sigma^2}{100}}, \quad 
    z_2 = \begin{pmatrix}
        0 \\ 4
    \end{pmatrix}\sqrt{\frac{3\sigma^2}{100}}, \quad 
    z_1 = \begin{pmatrix}
        -3 \\ -4
    \end{pmatrix}\sqrt{\frac{3\sigma^2}{100}}.
    \]
    Note that we have 
    \[
    \|z_1\|^2 = \frac{27\sigma^2}{100}, \quad \|z_2\|^2 = \frac{24\sigma^2}{50}, \quad \|z_3\|^2 = \frac{3\sigma^2}{4},
    \]
    meaning that $\tau < \|z_i\|$ for all $i\in[3]$. We define the stochastic gradient  as $\nabla f(x^t,\xi^t) = \nabla f(x^t) + \xi^t =  Lx^t + \xi^t$ where $\xi^t$ is picked uniformly at random from $\{z_1, z_2, z_3\}$. Simple calculations verify that Assumption~\ref{asmp:batch_noise} holds for such noise. Next, the update rule of the method \eqref{eq:clip21_ideal} in the case $n=1$ is
    \[
    x^{t+1} = x^t - \gamma g^t = x^t - \gamma(\nabla f(x^t) + \clip_{\tau}(\nabla f(x^t,\xi^t)-\nabla f(x^t))) = x^t - L\gamma x^t - \gamma\clip_{\tau}(\xi^t).
    \]
    Since $\tau < \|z_i\|$ for any $i\in\{1,2,3\}$ clipping is always active and we have 
    \begin{align*}
        \E{\clip_{\tau}(\xi^t)} &= \frac{1}{3}\clip_{\tau}(z_1) 
        + \frac{1}{3}\clip_{\tau}(z_2)
        + \frac{1}{3}\clip_{\tau}(z_3)\\
        &= \frac{1}{3}\frac{\tau}{\|z_1\|}z_1
        +  \frac{1}{3}\frac{\tau}{\|z_2\|}z_2
        +  \frac{1}{3}\frac{\tau}{\|z_3\|}z_3\\
        &= \frac{1}{3}\frac{\tau}{\frac{3\sqrt{3}\sigma}{10}}\frac{\sigma\sqrt{3}}{10}\begin{pmatrix}
            3 \\ 0
        \end{pmatrix}
        + \frac{1}{3}\frac{\tau}{\frac{4\sqrt{3}\sigma}{10}}\frac{\sigma\sqrt{3}}{10}\begin{pmatrix}
            0 \\ 4
        \end{pmatrix}
        + \frac{1}{3}\frac{\tau}{\frac{5\sqrt{3}\sigma}{10}}\frac{\sigma\sqrt{3}}{10}\begin{pmatrix}
            -3 \\ -4
        \end{pmatrix}\\
        &= \frac{\tau}{9}\begin{pmatrix}
            3 \\ 0
        \end{pmatrix}
        + \frac{\tau}{12}\begin{pmatrix}
            0 \\ 4
        \end{pmatrix}
        + \frac{\tau}{15}\begin{pmatrix}
            -3 \\ -4
        \end{pmatrix}\\
        &= \underbrace{\frac{\tau}{15}\begin{pmatrix}
            2 \\ 1
        \end{pmatrix}}_{\eqdef h}.
    \end{align*}
    Thus, we obtain
    \begin{align*}
        \E{x^T} &= (1-L\gamma)\E{x^{T-1}} - \gamma\E{\clip_{\tau}(\xi^t)}\\
        &= (1-L\gamma)\E{x^{T-1}} - \gamma h\\
        &= (1-L\gamma)^Tx^0 - \gamma h \sum_{t=0}^{T-1}(1-L\gamma)^{T-1-t}\\
        &= (1-L\gamma)^T\begin{pmatrix}
            0 \\ x_{(2)}^0
        \end{pmatrix} - \frac{\tau\gamma}{15}\begin{pmatrix}
            2 \\ 1
        \end{pmatrix}  \frac{1-(1-L\gamma)^T}{1 - (1-L\gamma)}\\
        &= (1-L\gamma)^T\begin{pmatrix}
            0 \\ x_{(2)}^0
        \end{pmatrix} - \frac{\tau}{15L}\begin{pmatrix}
            2 \\ 1
        \end{pmatrix} (1-(1-L\gamma)^T).
    \end{align*}

    
    Therefore, since $x_{(2)}^0 < 0$ we have 
    \begin{align*}
        \E{\|\nabla f(x^T)\|^2} &= \E{\|Lx^T\|^2}\\
        &= \left\|\E{Lx^T}\right\|^2
        + \E{\left\|Lx^T - \E{Lx^T}\right\|^2}\\
        &\ge \left\|\E{Lx^T}\right\|^2\\
        &= \frac{4\tau^2}{165}\left(1-\left(1-L\gamma\right)^T\right)^2 
        + L^2\left((1-L\gamma)^Tx^0_{(2)} - \frac{\tau}{15L}\left(1-\left(1-L\gamma\right)^T\right)\right)^2\\
        &\ge \frac{4\tau^2}{165}\left(1-\left(1-L\gamma\right)^T\right)^2 
        + (1-L\gamma)^{2T}\|Lx^0\|^2 
        + \frac{\tau^2}{165}(1-(1-L\gamma)^T)^2\\
        &= \frac{\tau^2}{45}\left(1-\left(1-L\gamma\right)^T\right)^2 
        + (1-L\gamma)^{2T}\|\nabla f(x^0)\|^2.
    \end{align*}
    Note that the function $a(1-x)^2 + x^2b \ge \frac{ab}{a+b}.$ Applying this result for $a = \frac{\tau^2}{45}, b = \|\nabla f(x^0)\|^2,$ and $x = (1-L\gamma)^T$ we get
    \begin{align*}
        \E{\|\nabla f(x^T)\|^2} \ge 
        \frac{\frac{\tau^2}{45}\|\nabla f(x^0)\|^2}{\frac{\tau^2}{45}+ \|\nabla f(x^0)\|^2} 
        \ge 
        \frac{1}{2}\min\left\{\|\nabla f(x^0)\|^2, \frac{\tau^2}{45}\right\}.
    \end{align*}
    
    {\bf The case $n>1$.} If $n > 1$ then we can consider a similar example where each client is quadratic $\frac{L}{2}\|x\|^2$ and the stochastic gradient is constructed as $\nabla f_i(x^t,\xi^t_i) = \nabla f_i(x^t) + \xi^t_i = Lx^t + \xi^t_i$ where $\xi^t_i$ is sampled uniformly at random from vectors $\{z_1,z_2,z_3\}$ such that
    \[
    z_1 = \begin{pmatrix}
        3 \\ 0
    \end{pmatrix}\sqrt{\frac{3\sigma^2}{100B}}, \quad 
    z_2 = \begin{pmatrix}
        0 \\ 4
    \end{pmatrix}\sqrt{\frac{3\sigma^2}{100B}}, \quad 
    z_1 = \begin{pmatrix}
        -3 \\ -4
    \end{pmatrix}\sqrt{\frac{3\sigma^2}{100B}}.
    \]
    Then, Assumption~\ref{asmp:batch_noise} is satisfied with $\nicefrac{\sigma^2}{B}$. Therefore, if $x_{(2)}^0 = -1$, $\varepsilon < \frac{L}{\sqrt{2}},$ and $\tau \ge \frac{\varepsilon}{3\sqrt{10}}$, this implies that $B \le \frac{243\sigma^2}{5\varepsilon^2} < \frac{27\sigma^2}{50\tau^2}$, and 
    \begin{align*}
        \E{\|\nabla f(x^T)\|^2} \ge
        \frac{1}{2}\min\left\{\|\nabla f(x^0)\|^2, \frac{\tau^2}{45}\right\} \ge \varepsilon^2.
    \end{align*}
\end{proof}



\section{Proof of \Cref{th:theorem_deterministic}}\label{appendix:proof_deterministic}

As we mention in the main part of the paper, the proofs are induction-based: by induction, we show that several quantities remain bounded throughout the work of the method. That is, in Lemmas~\ref{lem:lemma2}-\ref{lem:descent_Vt_tilde}, we establish several useful bounds and recurrences. These lemmas allow us to use the contraction-like property \eqref{eq:clipping_contraction} of the clipping operator and finish the proof of Theorem~\ref{th:theorem_deterministic} applying similar techniques used in the analysis of \algname{EF21}.

\begin{lemma}\label{lem:lemma2} Let each $f_i$ be $L$-smooth. Then, the iterates generated by \algname{Clip21-SGD2M} with\\$\nabla f_i(x^{t+1},\xi^{t+1}_i) = \nabla f_i(x^{t+1})$ (full gradients) and $\sigma_{\omega} = 0$ (no DP-noise) satisfy the following inequality 
\begin{align}
\begin{aligned}
    \|v_i^{t+1} - g_i^t\| &\le (1-\hat{\beta})\|v_i^t - g_i^{t-1}\|
         + \hat{\beta}\max\{0, \|v_i^t - g_i^{t-1}\| - \tau\}
         + L\gamma\beta \|g^t\|\\
         & \qquad + \beta\|\nabla f_i(x^t) - v_i^t\|.
\end{aligned}
\end{align}
\end{lemma}
\begin{proof}
    We have 
    \begin{align*}
        \|v_i^{t+1} - g_i^t\| &\overset{(i)}{=} \|(1-\beta)v_i^t + \beta\nabla f_i(x^{t+1}) - g_i^t\|\\
        &\overset{(ii)}{\le} \|v_i^t-g_i^t\| + \beta\|\nabla f_i(x^{t+1}) - v_i^t\|\\
        &\overset{(iii)}{=} 
        \|v_i^t - g_i^{t-1} - \hat{\beta}\clip_{\tau}(v_i^t - g_i^{t-1})\| 
        + \beta\|\nabla f_i(x^{t+1}) - \nabla f_i(x^t)\|
        + \beta\|\nabla f_i(x^t) - v_i^t\|\\
        &\overset{(iv)}{\le } 
        (1-\hat{\beta})\|v_i^t - g_i^{t-1}\| 
        + \hat{\beta}\|v_i^t - g_i^{t-1} - \clip_{\tau}(v_i^t - g_i^{t-1})\|
        + L\gamma\beta \|g^t\|
        + \beta\|\nabla f_i(x^t) - v_i^t\|\\
        &\overset{(v)}{\le} 
         (1-\hat{\beta})\|v_i^t - g_i^{t-1}\|
         + \hat{\beta}\max\{0, \|v_i^t - g_i^{t-1}\| - \tau\}
         + L\gamma\beta \|g^t\|
        + \beta\|\nabla f_i(x^t) - v_i^t\|.
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$ in deterministic case, $(ii)$ from triangle inequality, $(iii)$ from the update rule of $g_i^t$, $(iv)$ from triangle inequality, update rule of $x^t$, and $L$-smoothness, $(v)$ properties of clipping from \Cref{lem:clipping_property}.
\end{proof}
    
\begin{lemma}\label{lem:bound_gt_norm}
    Let each $f_i$ be $L$-smooth, $\Delta \ge \Phi^0$, and $B > \tau$. Assume that the following inequalities hold for the iterates generated by \algname{Clip21-SGD2M} with $\nabla f_i(x^{t+1},\xi^{t+1}_i) = \nabla f_i(x^{t+1})$ (full gradients) and $\sigma_{\omega} = 0$ (no DP-noise)
    \begin{enumerate}
        \item $\|g^{t-1}\| \le \sqrt{64L\Delta} + 3(B-\tau)$;
        \item $\|\nabla f_i(x^{t-1}) - v_i^{t-1}\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau);$
        \item $\|v_i^t - g_i^{t-1}\| \le B$ $\forall i\in[n]$;
        \item $\gamma \le \frac{1}{12L};$
        \item $\hat{\beta},\beta\in [0,1];$
        \item $\Phi^t \le \Delta$.
    \end{enumerate}
    Then we have 
    \begin{equation}
        \|g^t\| \le \sqrt{64L\Delta} + 3(B-\tau).
    \end{equation}
\end{lemma}
\begin{proof}
    We have 
    \begin{align*}
        &\|g^t\|\\
        \overset{(i)}{=}& \left\|g^{t-1} + \frac{\hat{\beta}}{n}\sum_{i=1}^n \clip_\tau(v_i^t-g_i^{t-1})\right\|\\
        =& \left\|g^{t-1} + \hat{\beta}(v^t - g^{t-1})
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n \left(\clip_\tau(v_i^t-g_i^{t-1}) - (v_i^{t} - g_i^{t-1})\right)\right\|\\
        =& \left\|(1-\hat{\beta})g^{t-1} 
        + \hat{\beta}\nabla f(x^t) + \hat{\beta}(v^t-\nabla f(x^t)) 
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n \left( \clip_\tau(v_i^t-g_i^{t-1}) - (v_i^t-g_i^{t-1})\right)\right\|\\
        \overset{(ii)}{\le}& 
        (1-\hat{\beta})\|g^{t-1}\|
        + \hat{\beta}\|\nabla f(x^t)\| 
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n \|v_i^t-\nabla f_i(x^t)\| + \frac{\hat{\beta}}{n}\sum_{i=1}^n\max\left\{0, \|v_i^t-g_i^{t-1}\| -\tau\right\},
    \end{align*}
    where $(i)$ follows from the update rule $g_i^t$, $(ii)$ from triangle inequality and clipping properties from \Cref{lem:clipping_property}. We continue the derivation of the bound for $\|g^t\|$ as follows
    \begin{align*}
        \|g^t\| 
        &\overset{(i)}{\le} (1-\hat{\beta})\|g^{t-1}\|
        + \hat{\beta}\|\nabla f(x^{t-1})\| 
        + \hat{\beta}\|\nabla f(x^t) - \nabla f(x^{t-1})\|\\ 
        & \;+\; \frac{\hat{\beta}}{n}\sum_{i=1}^n \|(1-\beta)v_i^{t-1}  + \beta\nabla f_i(x^t) -\nabla f_i(x^t)\|
        + \hat{\beta}(B-\tau)\\
        &\overset{(ii)}{\le} (1-\hat{\beta})\|g^{t-1}\|
        + \hat{\beta}\sqrt{2L(f(x^t) - f^*)}
        + L\gamma\hat{\beta}\|g^{t-1}\|
        + \frac{\hat{\beta}}{n}(1-\beta)\sum_{i=1}^n\|\nabla f_i(x^t) - v_i^{t-1}\|\\
        &\;+\; \hat{\beta}(B-\tau)\\
        &\overset{(iii)}{\le} (1-\hat{\beta} + L\gamma\hat{\beta})\|g^{t-1}\|
        + \hat{\beta}\sqrt{2L\Phi^t}
        + \frac{\hat{\beta}}{n}(1-\beta)\sum_{i=1}^n\|\nabla f_i(x^t) - \nabla f_i(x^{t-1})\|\\
        & \;+\; \frac{\hat{\beta}}{n}(1-\beta)\sum_{i=1}^n\|\nabla f_i(x^{t-1}) - v_i^{t-1}\|
        + \hat{\beta}(B-\tau)\\
        &\overset{(iv)}{\le} (1-\hat{\beta} + L\gamma\hat{\beta}(2-\beta)) \|g^{t-1}\|
        + \hat{\beta}\sqrt{2L\Delta}
        + \hat{\beta}(1-\beta)(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau))
        + \hat{\beta}(B-\tau)\\
        &\overset{(v)}{\le}
        (1-\hat{\beta} + L\gamma\hat{\beta}(2-\beta))(\sqrt{64L\Delta} + 3(B-\tau))
        + \hat{\beta}\sqrt{2L\Delta}
        + \hat{\beta}(1-\beta)(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau))\\
        &\;+\; \hat{\beta}(B-\tau),
    \end{align*}
    where $(i)$ follows from triangle inequality and update of $v_i^t$, $(ii)$ from $L$-smoothness and update rule of $x^t$, $(iii)$ from the definition of $\Phi^t$ and triangle inequality, $(iv)$ from the assumptions $2$ and $6$, $(v)$ from the assumption $1$. The above is satisfied if we have simultaneously 
    \begin{align*}
        8(1-\hat{\beta} + 2L\gamma\hat{\beta}) + \sqrt{2}\hat{\beta} + 2\hat{\beta} &\le 8\\
        3(1-\hat{\beta} + 2L\gamma\hat{\beta}) + \frac{3}{2}\hat{\beta} + \hat{\beta} &\le 3.
    \end{align*}
    Both inequalities hold when $L\gamma \le \frac{1}{12}.$
\end{proof}

\begin{lemma}\label{lem:bound_nablafxt_vt}
    Let each $f_i$ be $L$-smooth, $\Delta \ge \Phi^0$, and $B > \tau$. Assume that the following inequalities hold for the iterates generated by \algname{Clip21-SGD2M} with $\nabla f_i(x^{t+1},\xi^{t+1}_i) = \nabla f_i(x^{t+1})$ (full gradients) and $\sigma_{\omega} = 0$ (no DP-noise)
    \begin{enumerate}
        \item $4L\gamma \leq \beta$ and $\gamma \le \frac{1}{4L};$
        \item $\|\nabla f_i(x^{t-1}) - v_i^{t-1}\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau)$;
        \item $\|g^{t-1}\| \le \sqrt{64L\Delta}+3(B-\tau).$
    \end{enumerate}
    Then we have 
    \begin{equation}
        \|\nabla f_i(x^t)-v_i^t\| \le \sqrt{4L\Delta}+\frac{3}{2}(B-\tau) \quad \forall i\in[n].
    \end{equation}
\end{lemma}
\begin{proof}
    We have 
    \begin{align*}
        \|\nabla f_i(x^t)-v_i^t\| &\overset{(i)}{=} \|\nabla f_i(x^t) - (1-\beta)v_i^{t-1} - \beta\nabla f_i(x^t)\|\\
        &= (1-\beta)\|\nabla f_i(x^t) - v_i^{t-1}\|\\
        &\overset{(ii)}{\le} (1-\beta) L\gamma\|g^{t-1}\| 
        + (1-\beta)\|\nabla f_i(x^{t-1})-v_i^{t-1}\|\\
        &\overset{(iii)}{\le} L\gamma\left(\sqrt{64L\Delta} 
        + 3(B-\tau)\right)
        + (1-\beta)\left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)\right)\\
        &= (8L\gamma + 2(1-\beta))\sqrt{L\Delta}
        + \left(3L\gamma + \frac{3(1-\beta)}{2}\right)(B-\tau),
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$, $(ii)$ from triangle inequality, smoothness, and update of $x^t$, $(iii)$ from conditions $2$-$3$ in the statement of the lemma.  We need to satisfy 
    \begin{align*}
        8L\gamma + 2(1-\beta) \le 2 \Leftrightarrow 4L\gamma \le \beta.\\
        3L\gamma + \frac{3}{2}(1-\beta) \le \frac{3}{2} \Leftrightarrow 2L\gamma \le \beta.
    \end{align*}
    Since $4L\gamma \leq \beta$, both inequalities are satisfied.
\end{proof}



\begin{lemma}\label{lem:bound_gt_vt}
    Let each $f_i$ be $L$-smooth, $\Delta \ge \Phi^0$, $B>\tau$, and $i \in \cI_t \eqdef \{i\in[n]\mid \|v_i^t - g_i^{t-1}\| > \tau\}$. Assume that the following inequalities hold for the iterates generated by \algname{Clip21-SGD2M} with $\nabla f_i(x^{t+1},\xi^{t+1}_i) = \nabla f_i(x^{t+1})$ (full gradients) and $\sigma_{\omega} = 0$ (no DP-noise)
    \begin{enumerate}
        \item $4L\gamma\le \beta$;
        \item $L\gamma \le \frac{1}{12};$
        \item $\frac{8}{3}\beta\sqrt{L\Delta} \le \frac{\hat{\beta}\tau}{4};$
        \item $\frac{7}{4}\beta(B-\tau) \le \frac{\hat{\beta}\tau}{4}$;
        \item $\|g^t\| \le \sqrt{64L\Delta} + 3(B-\tau);$
        \item $\|\nabla f_i(x^t) - v_i^t\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau).$
    \end{enumerate}
    Then 
    \begin{align}
        \|v_i^{t+1} - g_i^t\| \le \|v_i^t-g_i^{t-1}\| - \frac{\hat{\beta}\tau}{2}.
    \end{align}
\end{lemma}

\begin{proof}
Since $i\in\cI_t$, then $\|v_i^t-g_i^{t-1}\| > \tau$, thus from \Cref{lem:lemma2} we have 
\begin{align*}
    \|v_i^{t+1} - g_i^t\| &\le 
    (1-\hat{\beta})\|v_i^t  - g_i^{t-1}\|
    +\hat{\beta}(\|v_i^t - g_i^{t-1}\| - \tau)
    + \beta L\gamma\|g^t\| 
    + \beta\|\nabla f_i(x^t) - v_i^t\|\\
    &\overset{(i)}{\le} \|v_i^t  - g_i^{t-1}\| -\hat{\beta} \tau 
    + \beta L\gamma\left(\sqrt{64L\Delta} + 3(B-\tau)\right)
    + \beta\left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)\right)\\
    &= \|v_i^t  - g_i^{t-1}\| - \hat{\beta}\tau
    + (8\beta L\gamma + 2\beta)\sqrt{L\Delta}
    + (3\beta L\gamma + \nicefrac{3\beta}{2})(B-\tau),
\end{align*}
where $(i)$ follows from assumptions $5$-$6$ of the statement of the lemma. Since $L\gamma \le \frac{1}{12},$ we have
\begin{align*}
    \|v_i^{t+1} - g_i^t\| &\le \|v_i^t  - g_i^{t-1}\| 
    - \hat{\beta}\tau
    + \frac{8}{3}\beta\sqrt{L\Delta}
    + \frac{7}{4}\beta(B-\tau).
\end{align*}
Due to assumptions $2$-$3$ of the lemma, we have 
\begin{align*}
    \|v_i^{t+1} - g_i^t\| &\le \|v_i^t  - g_i^{t-1}\| - \frac{\hat{\beta}\tau}{2},
\end{align*}
which concludes the proof.
\end{proof}



\begin{lemma}\label{lem:descent_Pt_tilde}
    Let each $f_i$ be $L$-smooth. Then, for the iterates generated by \algname{Clip21-SGD2M} with $\nabla f_i(x^{t+1},\xi^{t+1}_i) = \nabla f_i(x^{t+1})$ (full gradients) and $\sigma_{\omega} = 0$ (no DP-noise) the quantity\\ $\wtilde{P}^t \eqdef \frac{1}{n}\sum_{i=1}^n\|v_i^t - \nabla f_i(x^t)\|^2$ decreases as
    \begin{equation}
    \wtilde{P}^{t+1} \le (1-\beta)\wtilde{P}^t + \frac{3L^2}{\beta}R^t.
    \end{equation}
\end{lemma}
\begin{proof}
    We have 
    \begin{align*}
        \|v_i^{t+1} - \nabla f_i(x^{t+1})\|^2 
        &\overset{(i)}{=} \|(1-\beta)v_i^t + \beta\nabla f_i(x^{t+1}) - \nabla f_i(x^{t+1})\|^2\\
        &= (1-\beta)^2\|\nabla f_i(x^{t+1}) - v_i^t\|^2\\
        &\overset{(ii)}{\le} (1-\beta)^2(1+\nicefrac{\beta}{2})\|v_i^t - \nabla f_i(x^t)\|^2\\
        &\qquad + (1-\beta)^2(1+\nicefrac{2}{\beta})\|\nabla f_i(x^t) - \nabla f_i(x^{t+1})\|^2\\
        &\overset{(iii)}{\le} (1-\beta)\|v_i^t - \nabla f_i(x^t)\|^2
        + \frac{3L^2}{\beta}\|x^t - x^{t+1}\|^2,
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$, $(ii)$ -- from the inequality $\|a+b\|^2\le(1+\beta/2)\|a\|^2 + (1+2/\beta)\|b\|^2$ that holds for any $a,b \in \R^d$ and $\beta > 0$, and $(iii)$ -- from $(1-\beta)(1+\nicefrac{\beta}{2})\leq 1$, which holds for any $\beta \in [0,1]$, and smoothness. Averaging the inequalities above across $i\in[n],$ we get the statement of the lemma.
\end{proof}

Similarly, we can get the recursion for $P^t\eqdef \|v^t - \nabla f(x^t)\|^2$.
\begin{lemma}\label{lem:descent_Pt}
    Let each $f_i$ be $L$-smooth. Then, for the iterates generated by \algname{Clip21-SGD2M} with $\nabla f_i(x^{t+1},\xi^{t+1}_i) = \nabla f_i(x^{t+1})$ (full gradients) and $\sigma_{\omega} = 0$ (no DP-noise) the quantity \\ $P^t \eqdef \|v^t - \nabla f(x^t)\|^2$ decreases as 
    \begin{equation}
    P^{t+1} \le (1-\beta)P^t + \frac{3L^2}{\beta}R^t.
    \end{equation}
\end{lemma}
Next, we establish the recursion for $\wtilde{V}^t \eqdef \frac{1}{n}\sum_{i=1}^n\|g_i^{t} - v_i^{t}\|^2$.
\begin{lemma}\label{lem:descent_Vt_tilde}
    Let each $f_i$ be $L$-smooth. Consider \algname{Clip21-SGD2M} with $\nabla f_i(x^{t+1},\xi^{t+1}_i) = \nabla f_i(x^{t+1})$ (full gradients) and $\sigma_{\omega} = 0$ (no DP-noise). Let $\|v_i^{t} - g_i^{t-1}\| \le B$, for all $i\in[n]$ and some $B \geq \tau$, and  $\hat{\beta} \le \frac{1}{2\eta}$ . Then
    \[
    \|g_i^{t} - v_i^{t}\|^2 \le (1-\hat{\beta}\eta)\|g_i^{t-1} - v_i^{t-1}\|^2
        + \frac{4\beta^2}{\hat{\beta}\eta}\|v_i^{t-1} - \nabla f_i(x^{t-1})\|^2
        + \frac{4L^2\beta^2}{\hat{\beta}}R^{t-1}.
    \]
    and, in particular,
    \begin{equation*}
         \wtilde{V}^t \le (1-\eta)\wtilde{V}^{t-1} 
    + \frac{4\beta^2}{\hat{\beta}\eta}\wtilde{P}^{t-1} 
    + \frac{4\beta^2L^2}{\hat{\beta}\eta}R^{t-1},
    \end{equation*}
    where $\eta\eqdef \frac{\tau}{B}$, $R^t \eqdef \|x^{t+1} - x^t\|^2$, and $\wtilde{V}^t \eqdef \frac{1}{n}\sum\limits_{i=1}^n\|g_i^{t} - v_i^{t}\|^2$.
\end{lemma}
\begin{proof}
    Since $\|v_i^t-g_i^{t-1}\|\le B$, for $\eta^t_i \eqdef \frac{\tau}{\|v_i^t-g_i^{t-1}\|}$ we have $\eta^t_i \ge \eta$. This implies
    \begin{align*}
        \|g_i^{t} - v_i^{t}\|^2 &\overset{(i)}{=} \|g_i^{t-1} + \hat{\beta}\clip_{\tau}(v_i^t - g_i^{t-1}) - v_i^t\|^2\\
        &= \|\hat{\beta}(g_i^{t-1} - v_i^t + \clip_{\tau}(v_i^t - g_i^{t-1})) + (1-\hat{\beta})(g_i^{t-1} - v_i^t)\|^2\\
        &\overset{(ii)}{\le} (1-\eta)^2\hat{\beta}\|g_i^{t-1}-v_i^t\|^2 
        + (1-\hat{\beta})\|g_i^{t-1}-v_i^t\|^2,
    \end{align*}
    where $(i)$ follows from the update rule of $g_i^t$ and $(ii)$ from the convexity of $\|\cdot\|^2$ and the fact that $\|v_i^t - g_i^{t-1}\| \le B.$ We continue the derivations as follows
    \begin{align*}
        \|g_i^{t} - v_i^{t}\|^2  
        &= (1-\hat{\beta} + \hat{\beta}(1-2\eta + \eta^2))\|g_i^{t-1} - v_i^t\|^2\\
        &=  (1-\hat{\beta}\eta(2-\eta))\|g_i^{t-1} - v_i^t\|^2.
    \end{align*}
    Let $\rho = 2\hat{\beta}\eta$ (note that $\eta \le 1$). Then we have
    \begin{align*}
        \|g_i^{t} - v_i^{t}\|^2  &\le (1-\rho)\|g_i^{t-1} - v_i^t\|^2\\
        &\overset{(i)}{=} (1-\rho)\|g_i^{t-1} - (1-\beta)v_i^{t-1} - \beta\nabla f_i(x^t)\|^2 \\
        &\overset{(ii)}{\le} (1-\rho)(1+\nicefrac{\rho}{2})\|g_i^{t-1} - v_i^{t-1}\|^2
        + (1-\rho)(1+\nicefrac{2}{\rho})\beta^2\|v_i^{t-1} - \nabla f_i(x^t)\|^2\\
        &\overset{(iii)}{\le} (1-\nicefrac{\rho}{2})\|g_i^{t-1} - v_i^{t-1}\|^2
        + \frac{4\beta^2}{\rho}\|v_i^{t-1} - \nabla f_i(x^{t-1})\|^2
        + \frac{4L^2\beta^2}{\rho}R^{t-1},
    \end{align*}
    where $(i)$ follows from the update rule of $g_i^t$, $(ii)$ from the inequality $\|a+b\|^2\le(1+r/2)\|a\|^2 + (1+2/r)\|b\|^2$, which holds for any positive $r$ (i.e., for $r = \rho$ for some $\rho>0$) and $a,b\in \R^d$, $(iii)$ from the fact that $\rho \le 1$ by assumption, the inequality $\|a+b\|^2\le 2\|a\|^2 + 2\|b\|^2$, which holds for any $a,b\in \R^d$, and smoothness. Finally, since $2\hat{\beta}\eta \le 1,$ we ensure that $\rho \le 1,$ and derive the final bound
    \begin{align*}
        \|g_i^{t} - v_i^{t}\|^2 &\le (1-\hat{\beta}\eta)\|g_i^{t-1} - v_i^{t-1}\|^2
        + \frac{4\beta^2}{\hat{\beta}\eta}\|v_i^{t-1} - \nabla f_i(x^{t-1})\|^2
        + \frac{4L^2\beta^2}{\hat{\beta}}R^{t-1}.
    \end{align*}
\end{proof}



\begin{theorem}[Full statement of \Cref{th:theorem_deterministic}]
    Let Assumption \ref{asmp:smoothness} hold. Let\\ $B\eqdef \max\{3\tau, \max_i\|\nabla f_i(x^0)\|\}$ and $\Phi^0$ defined in \eqref{eq:lyapunov_function} satisfies $\Delta \ge \Phi^0$ for some $\Delta >0$. Assume the following inequalities hold
    \begin{enumerate}
        \item {\bf stepsize restrictions:} $\gamma \le \frac{1}{12L}, 4L\gamma = \beta$, and 
        \[
        \frac{5}{8}
        - \frac{32\beta^2L^2}{\hat{\beta}^2\eta^2}\gamma^2 
        - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 \ge 0;
        \]
        \item {\bf momentum restrictions:} $\frac{8}{3}\beta\sqrt{L\Delta} \le \frac{\hat{\beta}\tau}{4}, \frac{7}{4}\beta(B-\tau) \le \frac{\hat{\beta}\tau}{4}, \hat{\beta} \le \frac{1}{2\eta}$\footnote{Note that $\eta = \frac{\tau}{B} \le \frac{1}{3}$ by the choice of $B$, therefore $\hat{\beta} \le \frac{1}{2\eta}$ does not impose any additional assumption on $\hat{\beta}$ and it can be chosen from $[0,1].$}.
    \end{enumerate}
    Then, the Lyapunov function from \eqref{eq:lyapunov_function} for \algname{Clip21-SGD2M} with $\nabla f_i(x^{t+1},\xi^{t+1}_i) = \nabla f_i(x^{t+1})$ (full gradients) and $\sigma_{\omega} = 0$ (no DP-noise) decreases as
    \[
    \Phi^{t+1} \le \Phi^t - \frac{\gamma}{2}\|\nabla f(x^t)\|^2,
    \]
    and we have
    \begin{equation}
    \frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2  \le \frac{2\Delta}{\gamma T} = \cO\left(\frac{1}{T}\right).
    \end{equation}
    Moreover, after at most $\frac{2B}{\hat{\beta}\tau}$ iterations, the clipping operator will be turned off for all workers.
\end{theorem}


\begin{proof}
    For convenience, we define
    $$\nabla f_i(x^{-1}) = v_i^{-1} = g_i^{-1} = 0, \quad \Phi^{-1} = +\infty.$$ 
    Then, we will derive the result by induction, i.e., using the induction w.r.t.\ $t$, we will show that 
    \begin{enumerate}
    \item the Lyapunov function decreases as $\Phi^{t} \le \Phi^{t-1} - \frac{\gamma}{2}\|\nabla f(x^{t-1})\|^2;$
    \item $\|g^t\| \le \sqrt{64L\Delta} + 3(B-\tau)$;
    \item $\|v_i^t - \nabla f_i(x^t)\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau);$
    \item $\|v_i^t - g_i^{t-1}\| \le \max\left\{0, B -\frac{t\hat{\beta}\tau}{2}\right\}.$
    \end{enumerate}
    First, we prove that the base of induction holds.

    \paragraph{Base of induction.}
    \begin{enumerate}
        \item $\|v_i^0 - g_i^{-1}\| = \|v_i^0\| = \beta\|\nabla f_i(x^0)\| \le \frac{1}{2}B \le B$ holds;

        % \item $\|v_i^0 - g_i^{-1}\| = \|\nabla f_i(x^0) - \nabla f_i(x^0)\| = 0 \le B$ holds;

        \item $g^0 = \frac{1}{n}\sum_{i=1}^n (g_i^{-1} + \hat{\beta}\clip_\tau(v_i^0 - g_i^{-1}) = \frac{\hat{\beta}}{n}\sum_{i=1}^n\clip_\tau(\beta\nabla f_i(x^0)).$ Therefore, we have
        \begin{align*}
           \|g^0\| &\le \left\|\frac{\hat{\beta}}{n}\sum_{i=1}^n\beta\nabla f_i(x^0) + (\clip_\tau(\beta\nabla f_i(x^0)) - \beta\nabla f_i(x^0))\right\|\\
           &\le \hat{\beta}\beta\|\nabla f(x^0)\| 
           + \frac{\hat{\beta}}{n}\sum_{i=1}^n\max\left\{0, \beta\|\nabla f_i(x^0)\| - \tau\right\}\\
           &\le \hat{\beta}\beta\sqrt{2L(f(x^0)-f^*)} 
           + \hat{\beta}(B - \tau)\\
           &\le \sqrt{64L\Delta} 
           + 3(B-\tau).
        \end{align*}

        \item We have 
        \begin{align*}
           \|v_i^0 - \nabla f_i(x^0)\| &= \|\beta\nabla f_i(x^0) - \nabla f_i(x^0)\|\\
           &\le (1-\beta)B\\
           &\le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau)
        \end{align*}
        
         \item $\Phi^0 \le \Phi^{-1} - \frac{\gamma}{2}\|\nabla f(x^{-1})\|^2 = \Phi^{-1}$ holds.

    \end{enumerate}

    \paragraph{Transition of induction.}

    Assume that for $K$ we have that for all $t\in \{0,1,\ldots, K\}$
    \begin{enumerate}
        \item $\Phi^{t} \le \Phi^{t-1} - \frac{\gamma}{2}\|\nabla f(x^{t-1})\|^2$ (implying $\Phi^{t} \leq \Delta$);
        \item $\|g^t\| \le \sqrt{64L\Delta} + 3(B-\tau);$
        \item $\|v_i^t - \nabla f_i(x^t)\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau);$
        \item $\|v_i^t - g_i^{t-1}\| \le \max\left\{\hat{\beta}\tau, B -\frac{t\hat{\beta}\tau}{2}\right\}.$
    \end{enumerate}

    We proceed via analyzing two possible situations for $\cI_{K+1} \eqdef \{i\in[n]\mid \|v_i^{K+1} - g_i^{K}\| > \tau\}$: either $|\cI_{K+1}| > 0$ (there are workers with turned on gradient clipping) or $|\cI_{K+1}| = 0$ (for all workers the clipping is turned off).
    
    \subparagraph*{Case $|\cI_{K+1}| > 0.$}  
    Since all requirements of \Cref{lem:bound_gt_vt} are satisfied at iteration $K$ we get for all $i\in \cI_{K+1}$
    \[
    \|v_i^{K+1} - g_i^{K}\| \le \|v_i^{K} - g_i^{K-1}\| - \frac{\hat{\beta}\tau}{2} \overset{(i)}{\le} \max\left\{\tau, B -\frac{K\hat{\beta}\tau}{2}\right\} - \frac{\hat{\beta}\tau}{2} \leq \max\left\{\tau, B -\frac{(K+1)\hat{\beta}\tau}{2}\right\},
    \]
    where (i) follows from the condition 4 of the induction assumption. Similarly due to the assumption of induction, from \Cref{lem:bound_gt_norm} we get that 
    \[
    \|g^{K+1}\| \le \sqrt{64L\Delta} + 3(B-\tau),
    \]
    and from \Cref{lem:bound_nablafxt_vt}
    \[
    \|\nabla f_i(x^{K+1}) - v_i^{K+1}\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau).
    \]
    This means that conditions $2$-$4$ in the assumption of the induction are also verified for step $K+1$. The remaining part is the descent of the Lyapunov function. For estimating\\ $\wtilde{V}^{K+1} \eqdef \frac{1}{n}\sum\limits_{i=1}^n\|g_i^{K+1} - v_i^{K+1}\|^2$ we have \Cref{lem:descent_Vt_tilde} since $\|v_i^{K+1} - g_i^{K}\| \le B-\frac{\tau}{2}$ 
    \[
    \wtilde{V}^{K+1} \le (1-\hat{\beta}\eta)\wtilde{V}^{K} 
    + \frac{4\beta^2}{\hat{\beta}\eta}\wtilde{P}^K
    + \frac{4\beta^2L^2}{\hat{\beta}\eta}R^{K}.
    \]
    Combining this result with the claims of \Cref{lem:descent_deltat,lem:descent_Pt_tilde,lem:descent_Pt} we get
    \begin{align*}
    \Phi^{K+1} &= \delta^{K+1} 
    + \frac{2\gamma}{\hat{\beta}\eta}\wtilde V^{K+1}
    + \frac{8\gamma\beta}{\hat{\beta}^2\eta^2}\wtilde{P}^{K+1}
    + \frac{2\gamma}{\beta} P^{K+1}\\
    &\le \delta^{K} - \frac{\gamma}{2}\|\nabla f(x^{K})\|^2
    - \frac{1}{4\gamma}R^{K}
    + 2\gamma \wtilde{V}^{K}
    + 2\gamma P^{K}\\
    & \;+\; \frac{2\gamma}{\hat{\beta}\eta}\left(
    (1-\hat{\beta}\eta)\wtilde{V}^{K} 
    + \frac{4\beta^2}{\hat{\beta}\eta}\wtilde{P}^{K}
    + \frac{4\beta^2L^2}{\hat{\beta}\eta}R^{K}\right)\\
    & \;+\; \frac{8\gamma\beta}{\hat{\beta}^2\eta^2}\left(
    (1-\beta)\wtilde{P}^{K} 
    + \frac{3L^2}{\beta}R^{K}\right)\\
    & \;+\;\frac{2\gamma}{\beta}\left(
    (1-\beta)P^{K} 
    + \frac{3L^2}{\beta}R^{K}\right)\\
    &=  \delta^{K} 
    - \frac{\gamma}{2}\|\nabla f(x^{K})\|^2
    + \frac{2\gamma}{\hat{\beta}\eta}\wtilde{V}^{K}\left(
    1 - \hat{\beta}\eta 
    + \hat{\beta}\eta\right)
    + \frac{8\gamma\beta}{\hat{\beta}^2\eta^2}\wtilde{P}^{K}\left(
    1-\beta
    + \beta\right)\\
    & \;+\; \frac{2\gamma}{\beta}P^{K}\left(
    1-\beta
    + \beta\right)
    -\frac{1}{4\gamma}\left(1 - \frac{32\beta^2L^2}{\hat{\beta}^2\eta^2}\gamma^2 
    - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 
    - \frac{24L^2}{\beta^2}\gamma^2\right)R^{K}\\
    &= \Phi^{K} 
    - \frac{\gamma}{2}\|\nabla f(x^{K})\|^2
    -\frac{1}{4\gamma}\left(1 - \frac{32\beta^2L^2}{\hat{\beta}^2\eta^2}\gamma^2 
    - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 
    - \frac{24L^2}{\beta^2}\gamma^2\right)R^{K}.
    \end{align*}
    Since we choose $\beta^2 = 64L^2\gamma^2,$ then $-\frac{1}{\beta^2} = -\frac{1}{64L^2\gamma^2}$ and $-\frac{24L^2}{\beta^2}\gamma^2 = -\frac{24L^2}{64^2L^2\gamma^2}\gamma^2 \ge -\frac{3}{8}$ Therefore, 
    \begin{align*}
        1 - \frac{32\beta^2L^2}{\eta^2}\gamma^2 
    - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 
    - \frac{24L^2}{\beta^2}\gamma^2 &\ge 
    \frac{5}{8}
    - \frac{32\beta^2L^2}{\hat{\beta}^2\eta^2}\gamma^2 
    - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 \ge 0,
    \end{align*}
    by the choice of $\gamma.$ Thus, we get
    \[
    \Phi^{K+1} \le \Phi^{K} - \frac{\gamma}{2}\|\nabla f(x^{K})\|^2.
    \]
    In particular, this implies $\Phi^{K+1} \le \Phi^K \le \Delta.$

    \subparagraph*{Case $|\cI_{K+1}| = 0$.} In this case, $\eta^{K+1}_i = 1$ for all $i\in[n]$, i.e., $\clip_{\tau}(v_i^{K+1} - g_i^{K}) = v_i^{K+1} - g_i^{K}$ that leads to $g_i^{K+1} = v_i^{K+1}$. Thus, $\wtilde{V}^{K+1} = 0$. Moreover, $|\cI_{K+1}| = 0$ implies that condition 4 from the induction assumption holds for $t = K+1$ and using this and induction assumption we get $\|g^{K+1}\| \le \sqrt{64L\Delta} + 3(B-\tau)$ from \Cref{lem:bound_gt_norm} and $\|\nabla f_i(x^{K+1}) - v_i^{K+1}\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau)$ from \Cref{lem:bound_nablafxt_vt}. Next, taking into account that $\wtilde{V}^{K+1} = 0$, we can perform similar steps as before for $\Phi^{K+1}$ and get less restrictive inequality
    \begin{align*}
        \Phi^{K+1} &\le \Phi^{K} - \frac{\gamma}{2}\|\nabla f(x^{K})\|^2 
        - \frac{1}{4\gamma}\left(1 - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 
        - \frac{24L^2}{\beta^2}\gamma^2\right)R^{K}.
    \end{align*}
    Again, $1 - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 
        - \frac{24L^2}{\beta^2}\gamma^2 \ge \frac{5}{8} - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 \ge 0$ which is satisfied by the choice of $\gamma.$


    We conclude that in both cases the Lyapunov function decreases as $\Phi^{K+1} \le \Phi^{K} - \frac{\gamma}{2}\|\nabla f(x^{K})\|^2$, and consequently, $\Phi^{K+1} \le \Delta.$ This finalizes the induction step. Therefore, we can guarantee that for all iterations $t\in\{0,1,\ldots, T-1\}$ we have 
    \[
    \Phi^{t+1}\le\Phi^t - \frac{\gamma}{2}\|\nabla f(x^t)\|^2 \Rightarrow \frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2 \le \frac{2\Delta}{\gamma T}.
    \]
    Moreover, the proof shows that the clipping operator will be eventually turned off after at most $\frac{2B}{\hat{\beta}\tau}$ iterations since $\|v_i^t - g_i^{t-1}\| \le \max\left\{\tau, B -\frac{t\hat{\beta}\tau}{2}\right\}$.
\end{proof}





\newpage



\section{Proof of \Cref{th:theorem_stochastic_and_dp}}

The proof of \Cref{th:theorem_stochastic_and_dp} is split into two parts: small and large DP noise.


We define constants $a$, $b$, and $c$, which will be used later in the proofs, as follows:
\begin{align}\label{eq:constants_a_b_c}
    a &\eqdef \left(\sqrt{2}+2\sqrt{3\log\frac{6(T+1)}{\alpha}}\right)\sqrt{d}\sigma_{\omega}\sqrt{\frac{T}{n}},\notag\\
    b^2 &\eqdef  2\sigma^2\log\left(\frac{12(T+1)n}{\alpha}\right),\\
    c^2 &\eqdef \left(\sqrt{2} + 2\sqrt{3\log\frac{6(T+1)}{\alpha}}\right)^2\sigma^2,\notag
\end{align}
where $T$ is the number of iterations, $n$ is the number of workers, $d$ is the dimension of the problem, $\sigma$ is from \Cref{asmp:batch_noise}, $\alpha\in(0,1)$ is a constant, and $\sigma_{\omega}$ is the variance of DP noise.


\begin{lemma}\label{lem:lemma17} Let each $f_i$ be $L$-smooth. Then, for the iterates of \algname{Clip21-SGD2M} we have the following inequality with probability $1$
\begin{align}
\begin{aligned}
    \|v_i^{t+1} - g_i^t\| &\le (1-\hat{\beta})\|v_i^t - g_i^{t-1}\|
        + \hat{\beta}\max\left\{0, \|v_i^t-g_i^{t-1}\| - \tau\right\}
        + \beta L\gamma\|g^t\| \\
        &\qquad + \beta\|\nabla f_i(x^t)-v_i^t\|
        + \beta\|\theta^{t+1}_i\|,
\end{aligned}
\end{align}
where $\theta_i^t \eqdef \nabla f_i(x^t,\xi^t_i) - \nabla f_i(x^t)$.
\end{lemma}
\begin{proof}
    We have 
    \begin{align*}
        \|v_i^{t+1} - g_i^t\| &\overset{(i)}{=} \|(1-\beta)v_i^t + \beta\nabla f_i(x^{t+1}, \xi^{t+1}_i) - g_i^t\|\\
        &\overset{(ii)}{\le} \|v_i^t-g_i^t\| + \beta\|\nabla f_i(x^{t+1},\xi^{t+1}_i) - v_i^t\|\\
        &\overset{(iii)}{=} \|v_i^t - \hat{\beta}\clip_{\tau}(v_i^t - g_i^{t-1}) - g_i^{t-1}\|
        + \beta\|\nabla f_i(x^{t+1},\xi^{t+1}_i) - v_i^t\|\\ 
        &\overset{(iv)}{\le} (1-\hat{\beta})\|v_i^t - g_i^{t-1}\|
        + \hat{\beta}\max\left\{0, \|v_i^t-g_i^{t-1}\| - \tau\right\}
        + \beta\|\nabla f_i(x^{t+1},\xi_i^{t+1}) - \nabla f_i(x^{t+1})\|\\
        & \quad  +  \beta\|\nabla f_i(x^{t+1})-\nabla f_i(x^t)\|
        + \beta\|\nabla f_i(x^{t})-v_i^t\|\\
        &\overset{(v)}{\le} (1-\hat{\beta})\|v_i^t - g_i^{t-1}\|
        + \hat{\beta}\max\left\{0, \|v_i^t-g_i^{t-1}\| - \tau\right\}
        + \beta L\|x^{t+1} - x^t\|\\ 
        &\qquad + \beta\|\nabla f_i(x^t)-v_i^t\|
        + \beta\|\theta^{t+1}_i\|\\
        &\overset{(vi)}{=} (1-\hat{\beta})\|v_i^t - g_i^{t-1}\|
        + \hat{\beta}\max\left\{0, \|v_i^t-g_i^{t-1}\| - \tau\right\}
        + \beta L\gamma\|g^t\| \\
        &\qquad + \beta\|\nabla f_i(x^t)-v_i^t\|
        + \beta\|\theta^{t+1}_i\|,
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$, $(ii)$ from triangle inequality, $(iii)$ from the update rule of $g_i^t$,  $(iv)$ from the properties of the clipping operator from \Cref{lem:clipping_property} and triangle inequality,  $(v)$ from smoothness, $(vi)$ from the update rule of $x^t.$
\end{proof}






\begin{lemma}\label{lem:bound_gt_norm_dp}
    Let each $f_i$ be $L$-smooth, $\Delta \ge \Phi^0$. 
    Assume that the following inequalities hold for the iterates generated by \algname{Clip21-SGD2M}
    \begin{enumerate}
        \item $g^0 = \frac{1}{n}\sum_{i=1}^ng_i^0;$
        \item $\|g^{t-1}\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3 b + 3\hat{\beta}a$;
        \item $\|\overline{g}^{t-1}\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3 b;$
        \item 
        $\|\nabla f_i(x^{t-1}) - v_i^{t-1}\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2} b + \hat{\beta}a$ for all $i\in[n];$
        \item $\|v_i^t - g_i^{t-1}\| \le B$ for all $i\in[n];$
        \item $\gamma \le \frac{1}{12L};$
        \item $\|\theta^t_i\|\le b$ for all $i\in[n];$
        \item $\left\|\frac{1}{n}\sum_{l=1}^{t}\sum_{i=1}^n\omega_i^l\right\| \le a$;
        \item $\beta,\hat{\beta} \in [0,1];$
        \item $\Phi^{t-1} \le 2\Delta.$
    \end{enumerate}
    Then we have 
    \begin{equation}
        \|g^t\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3 b + 3\hat{\beta}a.
    \end{equation}
\end{lemma}
\begin{proof}
    We start as follows
    \begin{align*}
        \|g^t\| &\overset{(i)}{=} \left\|g^{t-1} 
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\clip_\tau(v_i^t-g_i^{t-1}) 
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\omega^t_i\right\|\\
        &= \left\|
        g^{t-1}
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n \left[\nabla f_i(x^{t}) + (v_i^t-\nabla f_i(x^t)) + \clip_\tau(v_i^t-g_i^{t-1}) - (v_i^t-g_i^{t-1})\right]\right.\\
        &\quad -\left.  
        \overline{g}^{t-1} + (1-\hat{\beta})\overline{g}^{t-1}
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\omega^t_i\right\|\\
        &\overset{(ii)}{\le} \left\|g^{t-1} - \overline{g}^{t-1} + \frac{\hat{\beta}}{n}\sum_{i=1}^n \omega_i^t\right\|
        + \hat{\beta}\|\nabla f(x^t)\|
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\|\clip_{\tau}(v_i^t - g_i^{t-1}) - v_i^t + g_i^{t-1}\|\\
        &\quad 
        + (1-\hat{\beta})\|\overline{g}^{t-1}\|
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\|v_i^t- \nabla f_i(x^{t})\|\\
        &\overset{(iii)}{\le} \left\|\overline{g}^{t-1} + \hat{\beta}\Omega^{t-1} - \overline{g}^{t-1} + \frac{\hat{\beta}}{n}\sum_{i=1}^n \omega_i^t\right\|
        + \hat{\beta}\|\nabla f(x^{t-1})\|
        + \hat{\beta}\|\nabla f(x^t) - \nabla f(x^{t-1})\|\\
        &\quad 
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\|\clip_{\tau}(v_i^t - g_i^{t-1}) - v_i^t + g_i^{t-1}\|
        + (1-\hat{\beta})\|\overline{g}^{t-1}\|\\
        &\quad + \frac{\hat{\beta}}{n}\sum_{i=1}^n\|(1-\beta)v_i^{t-1} +\beta\nabla f_i(x^t,\xi^t_i) - \nabla f_i(x^{t})\|,
    \end{align*}
    where $(i)$ follows from the update rule of $g^t$, $(ii)$ -- from the triangle inequality, $(iii)$ -- from the update rule of $v_i^t$, equality \eqref{eq:gt_gt_hat}, and triangle inequality. Using the definition of $\Omega^t$, we continue as follows 
    \begin{align*}
        \|g^t\| &\overset{(iv)}{\le} \hat{\beta}\|\Omega^t\|
        + \hat{\beta}\|\nabla f(x^{t-1})\| 
        + \hat{\beta}L\gamma\|g^{t-1}\|
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\max\{0, \|v_i^{t} - g_i^{t-1}\|-\tau\}
        + (1-\hat{\beta})\|\overline{g}^{t-1}\|\\
        &\quad
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\|(1-\beta)v_i^{t-1} + \beta\nabla f_i(x^t,\xi_i^t) - \nabla f_i(x^t)\|\\
        &\overset{(v)}{\le} 
        \hat{\beta}\sqrt{2L(f(x^{t-1}) - f^*)}
        + \hat{\beta}L\gamma\|g^{t-1}\|
        + (1-\hat{\beta})\|\overline{g}^{t-1}\|
        + \hat{\beta}(B-\tau)
        + \hat{\beta}\|\Omega^t\|\\
        &\quad 
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\left((1-\beta)\|v_i^{t-1} - \nabla f_i(x^t)\| + \beta\|\nabla f_i(x^t,\xi^t_i) - \nabla f_i(x^t)\|\right)\\
        &\overset{(vi)}{\le} \hat{\beta}\sqrt{2L(f(x^{t-1}) - f^*)}
        + \hat{\beta}L\gamma\|g^{t-1}\|
        + (1-\hat{\beta})\|\overline{g}^{t-1}\|
        + \hat{\beta}(B-\tau)
        + \hat{\beta}\|\Omega^t\|\\
        &\quad 
        + \frac{\hat{\beta}\beta}{n}\sum_{i=1}^n\|\theta^t_i\|
        + \frac{\hat{\beta}}{n}(1-\beta)\sum_{i=1}^n\left(\|v_i^{t-1} - \nabla f_i(x^{t-1})\| + \|\nabla f_i(x^t) - \nabla f_i(x^{t-1})\|\right)\\
        &\overset{(vii)}{\le}
        \hat{\beta}\sqrt{2L(f(x^{t-1}) - f^*)}
        + \hat{\beta}L\gamma(2-\beta)\|g^{t-1}\|
        + (1-\hat{\beta})\|\overline{g}^{t-1}\|
        + \hat{\beta}(B-\tau)
        + \hat{\beta}\|\Omega^t\|\\
        &\quad 
        + \frac{\hat{\beta}\beta}{n}\sum_{i=1}^n\|\theta^t_i\|
        + \frac{\hat{\beta}}{n}(1-\beta)\sum_{i=1}^n\|v_i^{t-1} - \nabla f_i(x^{t-1})\|.
    \end{align*}
      $(iv)$ -- from the properties of the clipping operator from \Cref{lem:clipping_property}, $L$-smoothness and update rule of $x^t$, $(v)$ -- from $L$-smoothness and triagnle inequality, $(vi)$ -- from triangle inequality, $(vii)$ -- from $L$-smoothness. Now we use the assumptions $2$-$5$, $7$-$8$, and $10$ to bound the terms
     \begin{align*}
         \|g^t\| &\le \hat{\beta}\sqrt{4L\Delta}
         + 2L\gamma\hat{\beta}\left(\sqrt{64L\Delta} + 3(B-\tau) + 3 b + 3\hat{\beta}a\right)
        + (1-\hat{\beta})\left(\sqrt{64L\Delta} + 3(B-\tau) + 3 b\right)\\
        &\quad 
        + \hat{\beta}(B-\tau)
        + \hat{\beta}a
        + \hat{\beta}\beta b
        + \hat{\beta}(1-\beta)\left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2} b + \hat{\beta}a\right).
     \end{align*}
     Regrouping the terms we obtain
     \begin{align*}
         \|g^t\| &\le \sqrt{L\Delta}[2\hat{\beta} 
         + 16L\gamma\hat{\beta}
         + 8(1-\hat{\beta})
         + 2\hat{\beta}(1-\beta)]
         + b[6L\gamma\hat{\beta}
         + 3(1-\hat{\beta})
         + \hat{\beta}\beta
         + \nicefrac{3}{2}\hat{\beta}(1-\beta)]\\
         &\quad + (B-\tau)[6L\gamma\hat{\beta} 
         + 3(1-\hat{\beta})
         + \hat{\beta}
         + \nicefrac{3}{2}\hat{\beta}(1-\beta)]
         + a[
         6L\gamma\hat{\beta}^2
         + \hat{\beta}
         + \hat{\beta}^2(1-\beta)
         ].
     \end{align*}
     For the first coefficient, we have
     \begin{align*}
         2\hat{\beta}
         + 16L\gamma\hat{\beta}
         + 8(1-\hat{\beta})
         + 2\hat{\beta}(1-\beta) \le 8 
         \Leftarrow 4\hat{\beta} + 16L\gamma\hat{\beta} \le 8\hat{\beta} \Leftarrow 4L\gamma \le 1,
     \end{align*}
     where the last inequality is satisfied by the choice of the stepsize $L\gamma \le \frac{1}{12}.$ For the second coefficient, we have
     \begin{align*}
         &6L\gamma\hat{\beta}
         + 3(1-\hat{\beta})
         + \hat{\beta}\beta
         + \frac{3}{2}\hat{\beta}(1-\beta) \le 3
         \Leftarrow 6L\gamma\hat{\beta} + \hat{\beta}\beta + \frac{3}{2}\hat{\beta}(1-\beta) \le 3\hat{\beta} \\
         \Leftarrow\;&
         6L\gamma + 1 + \frac{3}{2}(1-\beta)\le 3,
     \end{align*}
     where the last inequality is satisfied by the choice of the stepsize $6L\gamma \le \frac{1}{2}$ and momentum parameter $\beta \le 1$. For the third coefficient, we have
     \begin{align*}
         6L\gamma\hat{\beta}
         + 3(1-\hat{\beta})
         + \hat{\beta}
         + \frac{3}{2}\hat{\beta}(1-\beta) \le 3 
         \Leftarrow 6L\gamma\hat{\beta}
         + \hat{\beta}
         + \frac{3}{2}\hat{\beta}(1-\beta) \le 3\hat{\beta}
         \Leftarrow 
         6L\gamma
         + 1
         + \frac{3}{2} \le 3,
     \end{align*}
     where the last inequality is satisfied by the choice of the stepsize $6L\gamma \le \frac{1}{2}$. For the fourth coefficient, we have 
     \begin{align*}
         6L\gamma\hat{\beta}^2 + \hat{\beta} + \hat{\beta}^2(1-\beta) \le 3\hat{\beta} 
         \Leftarrow 6L\gamma\hat{\beta}^2 + \hat{\beta}^2 \le 2\hat{\beta} \Leftarrow 6L\gamma\hat{\beta} + \hat{\beta} \le 2,
     \end{align*}
     where the last inequality is satisfied by the choice of the stepsize $6L\gamma \le \frac{1}{2}$ and momentum parameter $\hat{\beta} \le 1.$ Thus, the statement of the lemma holds.
\end{proof}



\begin{lemma}\label{lem:bound_nablafxt_vt_dp}
    Let each $f_i$ be $L$-smooth, $\Delta \ge \Phi^0$, $B > \tau$. Assume that the following inequalities hold for the iterates generated by \algname{Clip21-SGD2M}
    \begin{enumerate}
        \item $\gamma \le \frac{1}{12L}$;
        \item $6L\gamma \le \beta$;
        \item $\|\nabla f_i(x^{t-1}) - v_i^{t-1}\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2} b + \hat{\beta}a$ for all $i\in[n];$
        \item $\|\theta^t_i\|\le b$ for all $i\in[n];$
        \item $\|g^{t-1}\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3 b + 3\hat{\beta}a;$
        \item $\|\overline{g}^{t-1}\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3 b.$
    \end{enumerate}
    Then we have 
    \begin{equation}
        \|\nabla f_i(x^t)-v_i^t\| \le \sqrt{4L\Delta}+\frac{3}{2}(B-\tau) + \frac{3}{2} b
        + \hat{\beta}a.
    \end{equation}
\end{lemma}
\begin{proof}
    We have 
    \begin{align*}
        \|\nabla f_i(x^t)-v_i^t\| &\overset{(i)}{=} \|\nabla f_i(x^t) - (1-\beta)v_i^{t-1} - \beta\nabla f_i(x^t,\xi^t_i)\|\\
        &\overset{(ii)}{\le} (1-\beta)\|\nabla f_i(x^t) - v_i^{t-1}\|
        + \beta\|\nabla f_i(x^t) - \nabla f_i(x^t,\xi^t_i)\|\\
        &\overset{(iii)}{\le} (1-\beta) L\gamma\|g^{t-1}\| 
        + (1-\beta)\|\nabla f_i(x^{t-1})-v_i^{t-1}\|
        + \beta\|\theta^t_i\|\\
        &\overset{(iv)}{\le} (1-\beta)L\gamma\left(\sqrt{64L\Delta} + 3(B-\tau) + 3 b + 3\hat{\beta}a\right)\\
        & \quad + (1-\beta)\left(\sqrt{4L\Delta}+\frac{3}{2}(B-\tau) + \frac{3}{2} b + \hat{\beta}a\right)
        + \beta b\\
        &= (8L\gamma + 2(1-\beta))\sqrt{L\Delta}
        + (3L\gamma + \nicefrac{3(1-\beta)}{2})(B-\tau)\\
        &\quad + (3L\gamma(1-\beta) + \nicefrac{3}{2}(1-\beta) + \beta)b
        + (3L\gamma\hat{\beta} + (1-\beta)\hat{\beta})a,
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$, $(ii)$ from the triangle inequality, $(iii)$ from triangle inequality, smoothness, and the update rule of $x^t$, $(iv)$ from assumptions $2$-$4$ of the lemma. We notice that
    \begin{align*}
    & 8L\gamma + 2(1-\beta) \le 2 \Leftarrow 4L\gamma \le \beta,\\
    & 3L\gamma + \frac{3}{2}(1-\beta) \le \frac{3}{2} \Leftarrow 2L\gamma \le \beta,\\
    & 3L\gamma + \frac{3}{2}(1-\beta) + \beta \le \frac{3}{2}\beta
    \Leftarrow 6L\gamma \le \beta, \\
    & 3L\gamma\hat{\beta} + (1-\beta)\hat{\beta} \le \hat{\beta} \Leftarrow 3L\gamma \le \beta,
    \end{align*}
    where the last inequalities in each line are satisfied for $\beta$, satisfying the conditions of the lemma.
\end{proof}





\begin{lemma}\label{lem:bound_gt_hat}
    Let each $f_i$ be $L$-smooth, $\Delta \ge \Phi^0, B > \tau.$ Assume that the following inequalities hold for the iterates generated by \algname{Clip21-SGD2M}
    \begin{enumerate}
        \item $\gamma \le \frac{1}{12L};$
        \item $\hat{\beta} \le \min\{\frac{\sqrt{L\Delta}}{a},1\}$;
        \item $\|v_i^t - g_i^{t-1}\|\le B$ for all $i\in[n];$
        \item $\|g^{t-1}\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3b + \hat{\beta}a;$
        \item  $\|\overline{g}^{t-1}\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3b);$
        \item $\|\nabla f_i(x^{t-1}) - v_i^{t-1}\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a$ for all $i\in[n];$
        \item $\Phi^{t-1} \le 2\Delta;$
        \item $\|\theta_i^t\| \le b$ for all $i\in[n].$ 
    \end{enumerate}
    Then we have 
    \begin{align*}
        \|\overline{g}^t\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3b.
    \end{align*}
\end{lemma}
\begin{proof}
    We have 
    \begin{align*}
        \|\overline{g}^{t}\| &\overset{(i)}{=} \left\|\overline{g}^{t-1} + \frac{\hat{\beta}}{n}\sum_{i=1}^n \clip_{\tau}(v_i^t - g_i^{t-1})\right\|\\
        &= \left\|\hat{\beta}\nabla f(x^t) 
        + \hat{\beta}(v^t - \nabla f(x^t))
        + (1-\hat{\beta})\overline{g}^{t-1}
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n [\clip_{\tau}(v_i^t - g_i^{t-1}) - (v_i^t - g_i^{t-1})] \right\|\\
        &\overset{(ii)}{\le} \hat{\beta}\|\nabla f(x^t)\|
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\|v_i^t - \nabla f_i(x^t)\|
        + (1-\hat{\beta})\|\overline{g}^{t-1}\|\\
        &\quad + \frac{\hat{\beta}}{n}\sum_{i=1}^n\|\clip_{\tau}(v_i^t - g_i^{t-1}) - (v_i^t - g_i^{t-1})\|\\
        &\overset{(iii)}{\le} \hat{\beta}\|\nabla f(x^{t-1})\|
        + \hat{\beta}L\gamma\|g^{t-1}\|
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\|(1-\beta)v_i^{t-1} + \beta \nabla f_i(x^t, \xi^{t}_i) - \nabla f_i(x^t)\|\\
        &\quad + (1-\hat{\beta})\|\overline{g}^{t-1}\|
        + \frac{\hat{\beta}}{n}\sum_{i=1}^n\max\{0, \|v_i^t - g_i^{t-1}\| - \tau\}\\
        &\overset{(iv)}{\le} \hat{\beta}\sqrt{2L(f(x^{t-1})  -f^*)}
        + \hat{\beta}L\gamma\|g^{t-1}\|
        + (1-\hat{\beta})\|\overline{g}^{t-1}\|
        + \hat{\beta}(B-\tau)\\
        &\quad + \frac{\hat{\beta}}{n}\sum_{i=1}^n\left((1-\beta) [\|v_i^{t-1} - \nabla f_i(x^{t-1})\|
        + \|\nabla f_i(x^{t-1}) - \nabla f_i(x^t) \|]
        + \beta\|\nabla f_i(x^t) - \nabla f_i(x^t, \xi^t_i)\|\right),
    \end{align*}
    where $(i)$ follows from the update rule of each $g_i^t$, $(ii)$ -- from the triangle inequality, $(iii)$ -- from the update of $v_i^t$ and properties of clipping from \Cref{lem:clipping_property}, $(iv)$ -- from $L$-smoothness, assumption $3$ of the lemma, and triangle inequality. Now we use assumptions $4$-$7$ to derive
    \begin{align*}
        \|g^t\| &\le \hat{\beta}\sqrt{4L\Delta}
        + \hat{\beta}L\gamma(2-\beta)\left(\sqrt{64L\Delta} + 3(B-\tau) + 3b + \hat{\beta}a\right)
        + \hat{\beta}(B-\tau)\\
        &\quad + (1-\hat{\beta})\left(\sqrt{64L\Delta} + 3(B-\tau) + 3b\right)
        + \hat{\beta}(1-\beta)\left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\right)
        + \hat{\beta}\beta b\\
        &= \sqrt{L\Delta}\left(2\hat{\beta}
        + 8L\gamma(2-\beta)\hat{\beta}
        + 8(1-\hat{\beta})
        + 2\hat{\beta}(1-\beta)\right)
        + a(L\gamma\hat{\beta}^2(2-\beta) + \hat{\beta}^2)\\
        &\quad + (B-\tau)\left(3L\gamma\hat{\beta}(2-\beta) + \hat{\beta} + 3(1-\hat{\beta})
        + \frac{3}{2}\hat{\beta}(1-\beta)\right)\\
        &\quad + b(3L\gamma\hat{\beta}(2-\beta) + 3(1-\hat{\beta}) + \nicefrac{3}{2}\hat{\beta}(1-\beta)).
    \end{align*}
    For the second term, we have
    \begin{align*}
        2L\gamma\hat{\beta}^2a + \hat{\beta}^2a \le 2L\gamma\hat{\beta}\sqrt{L\Delta} + \hat{\beta}\sqrt{L\Delta} = (2L\gamma\hat{\beta} + \hat{\beta})\sqrt{L\Delta},
    \end{align*}
    where we use $\hat{\beta} \le \frac{\sqrt{L\Delta}}{a}.$ Therefore, the second term should be added to the first term. Thus, we have for the term with $\sqrt{L\Delta}$
    \begin{align*}
        &2L\gamma\hat{\beta} + \hat{\beta}
        + 2\hat{\beta}
        + 8L\gamma\hat{\beta}(2-\beta) 
        + 8(1-\hat{\beta})
        + 2\hat{\beta}(1-\beta) \le 8\\ 
        \Leftarrow\; & 2L\gamma + 1 + 2 + 8L\gamma(2-\beta) + 2(1-\beta) \le 8 \\
        \Leftarrow\;& 18L\gamma \le 3,
    \end{align*}
    where the last inequality is satisfied by the choice of the stepsize $L\gamma \le \frac{1}{12}.$ For the third coefficient, we have
    \begin{align*}
        3L\gamma\hat{\beta}(2-\beta) 
        + \hat{\beta}
        + 3(1-\hat{\beta})
        + \frac{3}{2}\hat{\beta}(1-\beta) \le 3 
        \Leftarrow 3L\gamma(2-\beta) + 1 
        + \frac{3}{2}(1-\beta) \le 3 
        \Leftarrow 6L\gamma \le \frac{1}{2},
    \end{align*}
     where the last inequality is satisfied by the choice of the stepsize $L\gamma \le \frac{1}{12}.$ For the fourth coefficient, we have the same derivations as for the third one. This implies that 
     \begin{align*}
         \|g^t\| \le 8\sqrt{L\Delta} + 3(B-\tau) + 3b,
     \end{align*}
     which concludes the proof.
     
\end{proof}



\begin{lemma}\label{lem:bound_gt_vt_dp}
    Let each $f_i$ be $L$-smooth, $\Delta \ge \Phi^0$, $B > \tau$, and $i \in \cI_t \eqdef \{i\in[n]\mid \|v_i^t - g_i^{t-1}\| > \tau\}$. Assume that the following inequalities hold for the iterates generated by \algname{Clip21-SGD2M}
    \begin{enumerate}
        \item $12L\gamma \le 1;$
        \item $6L\gamma \le \beta;$
        \item $\beta \le \min\{\frac{3\hat{\beta}\tau}{64\sqrt{L\Delta}},1\}$;
        \item $\beta \le \min\{\frac{\hat{\beta}\tau}{14(B-\tau)},1\};$
        \item $\beta \le \min\{\frac{\hat{\beta}\tau}{22b},1\}$;
        \item $\hat{\beta} \le \min\{\frac{\sqrt{L\Delta}}{a},1\};$
        \item $\|g^t\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3b + 3a;$
        \item $\|\theta^{t+1}_i\| \le b;$
        \item $\|\nabla f_i(x^t) - v_i^t\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b +
        \hat{\beta}a.$
    \end{enumerate}
    Then 
    \begin{align}
        \|v_i^{t+1} - g_i^t\| \le \|v_i^t-g_i^{t-1}\| - \frac{\hat{\beta}\tau}{2}.
    \end{align}
\end{lemma}

\begin{proof}
Since $i\in\cI_t$, then $\|v_i^t-g_i^{t-1}\| > \tau$ and from \Cref{lem:lemma17} we have 
\begin{align*}
    \|v_i^{t+1} - g_i^t\| &\le 
    (1-\hat{\beta})\|v_i^t  - g_i^{t-1}\| 
    + \hat{\beta}\|v_i^t - g_i^{t-1}\| - \hat{\beta}\tau 
    + \beta L\gamma\|g^t\| 
    + \beta\|\nabla f_i(x^t) - v_i^t\|
    + \beta\|\theta^{t+1}_i\|\\
    &\overset{(i)}{\le}  \|v_i^t  - g_i^{t-1}\| - \hat{\beta}\tau 
    + \beta L\gamma\left(\sqrt{64L\Delta} + 3(B-\tau) + 3b + 3\hat{\beta}a\right)\\
    & \;+\; \beta\left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\right)
    + \beta b\\
    &= \|v_i^t  - g_i^{t-1}\| - \hat{\beta}\tau
    + (8\beta L\gamma + 2\beta)\sqrt{L\Delta}
    + (3L\gamma\beta + \nicefrac{3\beta}{2})(B-\tau)\\
    &\;+\; 
    (3L\gamma\beta + \nicefrac{3\beta}{2} + \beta)b
    + (3L\gamma\beta + \beta)\hat{\beta}a,
\end{align*}
where $(i)$ follows from assumptions $6$-$8$ of the lemma. 
Since $12L\gamma \le 1$ we have 
\[
(8\beta L\gamma + 2\beta)\sqrt{L\Delta} \le (\nicefrac{2\beta}{3} + 2\beta)\sqrt{L\Delta} = \frac{8}{3}\beta\sqrt{L\Delta} \le \frac{\hat{\beta}\tau}{8},
\]
where we used $\beta \le \frac{3\hat{\beta}\tau}{64\sqrt{L\Delta}}.$
Since $12L\gamma \le \beta$ we have 
\[
\left(3L\gamma\beta + \frac{3\beta}{2}\right)(B-\tau) \le (\nicefrac{\beta}{4} + \frac{3\beta}{2})(B-\tau) = \frac{7}{4}\beta(B-\tau) \le \frac{\hat{\beta}\tau}{8},
\]
where we used $\beta \le \frac{\hat{\beta}\tau}{14(B-\tau)}$.
Since $12L\gamma\le \beta$ we have
\[
(3L\gamma\beta + \nicefrac{5\beta}{2})b \le \left(\nicefrac{\beta}{4} + \nicefrac{5\beta}{2}\right)b = \frac{11}{4}\beta b \le \frac{\hat{\beta}\tau}{8},
\]
where we used $\beta \le \frac{\hat{\beta}\tau}{22b}.$ Since $12L\gamma \le \beta$ and $\hat{\beta} \le \frac{\sqrt{L\Delta}}{a}$ we have
\[
\left(3L\gamma\beta +
\beta\right)\hat{\beta} a \le (\nicefrac{\beta}{4} + \beta)\sqrt{L\Delta} = \frac{5}{4}\beta\sqrt{L\Delta} \le \frac{\hat{\beta}\tau}{8},
\]
where we used $\beta \le \frac{\hat{\beta}\tau}{22b}.$ Thus we have
\begin{align*}
    \|v_i^{t+1} - g_i^t\| &\le \|v_i^t  - g_i^{t-1}\| 
    - \hat{\beta}\tau
    + 4\cdot \frac{\hat{\beta}\tau}{8} = \|v_i^t  - g_i^{t-1}\| 
    - \frac{\hat{\beta}\tau}{2}, 
\end{align*}
which concludes the proof.
\end{proof}



\begin{lemma}\label{lem:descent_Pt_tilde_dp}
    Let $\|\theta^{t+1}_i\|\le b$ for all $i\in[n]$. Let each $f_i$ be $L$-smooth. Then, for the iterates generated by \algname{Clip21-SGD2M} the quantity $\wtilde{P}^t \eqdef \frac{1}{n}\sum_{i=1}^n\|v_i^t - \nabla f_i(x^t)\|^2$ decreases as 
    \begin{equation}
    \wtilde{P}^{t+1} \le (1-\beta)\wtilde{P}^t + \frac{3L^2}{\beta}R^t + \beta^2b^2 + \frac{2}{n}\beta(1-\beta)\sum_{i=1}^n\<v_i^t - \nabla f_i(x^{t+1}), \theta^{t+1}_i>,
    \end{equation}
    where $R^t \eqdef \|x^{t+1} - x^t\|$ and $\theta^t_i \eqdef \nabla f_i(x^t,\xi^t_i) - \nabla f_i(x^t)$.
\end{lemma}
\begin{proof}
    We have 
    \begin{align*}
        \|v_i^{t+1} - \nabla f_i(x^{t+1})\|^2 
        &\overset{(i)}{=} \|(1-\beta)v_i^t + \beta\nabla f_i(x^{t+1},\xi^{t+1}_i) - \nabla f_i(x^{t+1})\|^2\\
        &= \|(1-\beta)(v_i^t - \nabla f_i(x^{t+1})) + \beta(\nabla f_i(x^{t+1},\xi^{t+1}_i)- \nabla f_i(x^{t+1}))\|^2\\
        &= (1-\beta)^2\|v_i^t - \nabla f_i(x^{t+1})\|^2
        + \beta^2\|\theta^{t+1}_i\|^2\\
        &\quad + 2\beta(1-\beta)\<v_i^t - \nabla f_i(x^{t+1}), \theta^{t+1}_i>\\
        &\overset{(ii)}{\le} (1-\beta)^2(1+\nicefrac{\beta}{2})\|v_i^t - \nabla f_i(x^t)\|^2\\
        & \quad + (1-\beta)^2(1+\nicefrac{2}{\beta})\|\nabla f_i(x^t) - \nabla f_i(x^{t+1})\|^2
        + \beta^2b^2\\
        &\quad + 2\beta(1-\beta)\<v_i^t - \nabla f_i(x^{t+1}), \theta^{t+1}_i>\\
        &\overset{(iii)}{\le} (1-\beta)\|v_i^t - \nabla f_i(x^t)\|^2
        + \frac{3L^2}{\beta}\|x^t - x^{t+1}\|^2
        + \beta^2 b^2\\
        &\quad + 2\beta(1-\beta)\<v_i^t - \nabla f_i(x^{t+1}), \theta^{t+1}_i>,
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$, $(ii)$ from $\|x+y\|^2 \le (1+r)\|x\|^2 + (1+r^{-1})\|y\|^2$ for any $x,y\in\R^d$ and $r > 0$, $(iii)$ from the smoothness and inequalities $(1-\beta)^2(1+\nicefrac{\beta}{2}) \le (1-\beta)$ and $(1-\beta)^2(1+\nicefrac{2}{\beta}) \le \nicefrac{3}{\beta}$. Averaging the inequalities above across all  $i\in[n]$, we get the lemma's statement.
\end{proof}



Similarly, we can get the recursion for $P^t \eqdef \|v^t - \nabla f(x^t)\|^2$.
\begin{lemma}\label{lem:descent_Pt_dp}
    Let $\|\theta^{t+1}\|\le \frac{c}{\sqrt{n}}$ for all $i\in[n]$. Let each $f_i$ be $L$-smooth. Then, for the iterates generated by \algname{Clip21-SGD2M} the quantity $P^t \eqdef \|v^t - \nabla f(x^t)\|^2$ decreases as
    \[
    P^{t+1} \le (1-\beta)P^t 
    + \frac{3L^2}{\beta}R^t 
    + \beta^2\frac{c^2}{n}
    + 2\beta(1-\beta)\<v^t - \nabla f(x^{t+1}), \theta^{t+1}>,
    \]
    where $R^t \eqdef \|x^{t+1} - x^t\|$ and $\theta^t \eqdef \frac{1}{n}\sum_{i=1}^n\theta_i^t = \frac{1}{n}\sum_{i=1}^n (\nabla f_i(x^t,\xi^t) - \nabla f_i(x^t))$.
\end{lemma}
\begin{proof}
    For shortness, we denote $\nabla f(x^t,\xi^t) \eqdef \frac{1}{n}\sum_{i=1}^n \nabla f_i(x^t,\xi^t_i)$ and $\theta^t \eqdef \frac{1}{n}\sum_{i=1}^n(\nabla f_i(x^t,\xi^t) - \nabla f_i(x^t))$. Then, we have 
    \begin{align*}
        \|v^{t+1} - \nabla f(x^{t+1})\|^2
        &\overset{(i)}{=} \|(1-\beta)v^t + \beta\nabla f(x^{t+1},\xi^{t+1}) - \nabla f(x^{t+1})\|^2\\
        &= \|(1-\beta)(v^t - \nabla f(x^{t+1})) + \beta(\nabla f(x^{t+1},\xi^{t+1})- \nabla f(x^{t+1}))\|^2\\
        &= (1-\beta)^2\|v^t - \nabla f(x^{t+1})\|^2
        + \beta^2\|\theta^{t+1}\|^2\\
        & \quad + 2\beta(1-\beta)\<v^t - \nabla f(x^{t+1}), \theta^{t+1}>\\
        &\overset{(ii)}{\le} (1-\beta)^2(1+\nicefrac{\beta}{2})\|v^t - \nabla f(x^t)\|^2\\
        & \quad + (1-\beta)^2(1+\nicefrac{2}{\beta})\|\nabla f(x^t) - \nabla f(x^{t+1})\|^2
        + \beta^2\frac{c^2}{n}\\
        &\quad + 2\beta(1-\beta)\<v^t - \nabla f(x^{t+1}), \theta^{t+1}_i>\\
        &\overset{(iii)}{\le} (1-\beta)\|v^t - \nabla f(x^t)\|^2
        + \frac{3L^2}{\beta}\|x^t - x^{t+1}\|^2
        + \beta^2 \frac{c^2}{n}\\
        &\quad + 2\beta(1-\beta)\<v^t - \nabla f(x^{t+1}), \theta^{t+1}>,
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$, $(ii)$ from $\|x+y\|^2 \le (1+r)\|x\|^2 + (1+r^{-1})\|y\|^2$ for any $x,y\in\R^d$ and $r > 0$, $(iii)$ from the smoothness and inequalities $(1-\beta)^2(1+\nicefrac{\beta}{2}) \le (1-\beta)$ and $(1-\beta)^2(1+\nicefrac{2}{\beta}) \le \nicefrac{3}{\beta}.$
\end{proof}


Next, we establish the recursion for $\wtilde{V}^t \eqdef \frac{1}{n}\sum_{i=1}^n\|g_i^{t} - v_i^{t}\|^2$.
\begin{lemma}\label{lem:descent_Vt_tilde_dp}
    Let $\|\theta^t_i\|\le b$ for all $i\in[n]$, each $f_i$ be $L$-smooth, and $\|v_i^{t} - g_i^{t-1}\| \le B$ for all $i\in[n]$ and some $B > \tau,$ and $\hat{\beta} \le \frac{1}{2\eta}$\footnote{Since $\eta \in (0,1)$, then this restriction is not necessary because the momentum parameter $\hat{\beta} \le 1$ by default.}. Then, for the iterates generated by \algname{Clip21-SGD2M} we have 
    \begin{align}
    \|g_i^{t} - v_i^{t}\|^2 &\le (1-\hat{\beta}\eta)\|g_i^{t-1}- v_i^{t-1}\|^2 
    + \frac{4\beta^2}{\hat{\beta}\eta}\|v_i^{t-1} - \nabla f_i(x^{t-1})\|^2 
    + \frac{4\beta^2L^2}{\hat{\beta}\eta}R^{t-1}
    + \beta^2b^2\\
    &\quad + 2(1-\hat{\beta}\eta)^2\beta\<(g_i^{t-1} - v_i^{t-1}) + \beta(v_i^{t-1} - \nabla f_i(x^{t-1})), \theta^t_i>\notag\\
    &\quad + 2(1-\hat{\beta}\eta)^2\beta\<\beta(\nabla f_i(x^{t-1})-\nabla f_i(x^t)), \theta^t_i>,\notag
    \end{align}
    where $R^t \eqdef \|x^{t+1} - x^t\|$ and $\eta \eqdef \frac{\tau}{B}$. Moreover, averaging the inequalities across all $i\in[n]$, we get
    \begin{align}
    \wtilde{V}^{t} &\le (1-\hat{\beta}\eta)\wtilde{V}^{t-1} 
    + \frac{4\beta^2}{\hat{\beta}\eta}\wtilde{P}^{t-1}
    + \frac{4\beta^2L^2}{\hat{\beta}\eta}R^{t-1}
    + \beta^2b^2\\
    &\quad + \frac{2}{n}(1-\hat{\beta}\eta)^2\beta\sum_{i=1}^n\<(g_i^{t-1} - v_i^{t-1}) + \beta(v_i^{t-1} - \nabla f_i(x^{t-1})) + \beta(\nabla f_i(x^{t-1})-\nabla f_i(x^t)), \theta^t_i>,\notag
    \end{align}
    where $\wtilde{V}^t \eqdef \frac{1}{n}\sum_{i=1}^n\|g_i^{t} - v_i^{t}\|^2$ and $\wtilde{P}^t \eqdef \frac{1}{n}\sum_{i=1}^n\|v_i^t - \nabla f_i(x^t)\|^2$.
\end{lemma}
\begin{proof}
    Since $\|v_i^t-g_i^{t-1}\|\le B$ and $B > \tau$, we have $\eta^t_i \eqdef \frac{\tau}{\|v_i^t-g_i^{t-1}\|} \ge \frac{\tau}{B} =: \eta \in (0,1)$. Thus, we have 
    \begin{align*}
         \|g_i^{t} - v_i^{t}\|^2 &\overset{(i)}{=} \|g_i^{t-1} + \hat{\beta}\clip_{\tau}(v_i^t - g_i^{t-1}) - v_i^t\|^2\\
         &= \|\hat{\beta}(\clip_{\tau}(v_i^t - g_i^{t-1}) - (v_i^t - g_i^{t-1})) + (1-\hat{\beta})(g_i^{t-1} - v_i^t ))\|^2\\
         &\overset{(ii)}{\le} (1-\hat{\beta})\|g_i^{t-1} - v_i^t\|^2
         + \hat{\beta}\|\clip_{\tau}(v_i^t - g_i^{t-1}) - (v_i^t - g_i^{t-1})\|^2\\
         &\overset{(iii)}{\le} 
         (1-\hat{\beta})\|g_i^{t-1} - v_i^t\|^2
         + \hat{\beta}(1-\eta)^2\|g_i^{t-1} - v_i^t\|^2\\
         &= (1-\hat{\beta}\eta(2-\eta))\|g_i^{t-1} - v_i^t\|^2,
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$, $(ii)$ -- from the convexity of $\|\cdot\|^2$, $(iii)$ -- from the properties of the clipping operator in \Cref{lem:clipping_property}. Let $\rho = 2\hat{\beta}\eta \le 1.$ Then we have 
    \begin{align*}
        \|g_i^t - v_i^t\|^2 &\le (1-\rho)\|g_i^{t-1} - v_i^t\|^2\\
        &\overset{(i)}{=} (1-\rho)\|g_i^{t-1} - (1-\beta)v_i^{t-1} - \beta\nabla f_i(x^t,\xi^t_i)\|^2\\
        &= (1-\rho)\|g_i^{t-1} - (1-\beta)v_i^{t-1} - \beta\theta^t_i - \beta\nabla f_i(x^t)\|^2\\
        &= (1-\rho)\|g_i^{t-1} - (1-\beta)v_i^{t-1} - \beta\nabla f_i(x^t)\|^2
        + (1-\rho)\beta^2\|\theta^t_i\|^2\\
        &\quad -\; 2(1-\rho)\beta\<g_i^{t-1} - (1-\beta)v_i^{t-1} - \beta\nabla f_i(x^t), \theta^t_i>\\
        &\overset{(ii)}{\le} (1-\rho)(1+\nicefrac{\rho}{2})\|g_i^{t-1} - v_i^{t-1}\|^2
        + (1-\rho)(1+\nicefrac{2}{\rho})\beta^2\|v_i^{t-1} - \nabla f_i(x^t)\|^2
        + \beta^2b^2\\
        &\quad -\; 2(1-\rho)\beta\<g_i^{t-1} - (1-\beta)v_i^{t-1} - \beta\nabla f_i(x^t), \theta^t_i>\\
        &\overset{(iii)}{\le} (1-\nicefrac{\rho}{2})\|g_i^{t-1} - v_i^{t-1}\|^2
        + \frac{4\beta^2}{\rho}\|v_i^{t-1} - \nabla f_i(x^{t-1})\|^2 
        + \frac{4\beta^2L^2}{\rho}R^{t-1} 
        + \beta^2b^2\\
        &\quad -\; 2(1-\rho)\beta\<g_i^{t-1} - (1-\beta)v_i^{t-1} - \beta\nabla f_i(x^t), \theta^t_i>,
    \end{align*}
    where $(i)$ follows from the update rule of $v_i^t$, $(ii)$ -- from the inequality $\|a+b\|^2 \le (1+r)\|a\|^2 + (1+r^{-1})\|b\|^2$ which holds for any $a,b\in\R^d$ and $r>0,$ and assumption of the lemma, $(iii)$ -- from $L$-smoothness, Young's inequality $\|a+b\|^2 \le 2\|a\|^2 + 2\|b\|^2$.
\end{proof}




\begin{theorem}[Proof of \Cref{th:theorem_stochastic_and_dp}]
    Let $B\eqdef \max\{3\tau, \max_i\{\|\nabla f_i(x^0)\|\}+b\}$, Assumptions \ref{asmp:smoothness} and \ref{asmp:batch_noise} hold, probability confidence level $\alpha\in(0,1)$, constants $a,b,$ and $c$ be defined as in (\ref{eq:constants_a_b_c}), and $\Delta \ge \Phi^0$ for $\Phi^0$ defined in \eqref{eq:lyapunov_function}. Consider the run of \algname{Clip21-SGD2M} (\Cref{alg:Clip-SGDM}) for $T$ iterations with DP noise variance $\sigma_{\omega}$.
    Assume the following inequalities hold
    \begin{enumerate}
        \item {\bf stepsize restrictions:} 
        \begin{enumerate}[$i)$]
        \item $12L\gamma \le 1;$
        \item \begin{equation}\label{eq:quadratic_stepsize_restriction}
        \frac{1}{3}
        - \frac{32\beta^2L^2}{\hat{\beta}^2\eta^2}\gamma^2 
        - \frac{96L^2}{\hat{\beta}^2\eta^2}\gamma^2 \ge 0;
        \end{equation}
        \end{enumerate}
        \item {\bf momentum restrictions:} 
        \begin{enumerate}[$i)$]
        \item  $6L\gamma = \beta;$
        \item $\beta \le \min\{\frac{3\hat{\beta}\tau}{64\sqrt{L\Delta}}, 1\}$;
        \item $\beta \le \min\{\frac{\hat{\beta}\tau}{14(B-\tau)}, 1\};$
        \item $\beta \le \min\{\frac{\hat{\beta}\tau}{22b}, 1\}$;
        \item $\hat{\beta} \le \min\{\frac{\sqrt{L\Delta}}{a}, 1\}$;
        \item $\beta, \hat{\beta} \in (0,1];$
        \item and momentum restrictions defined in (\ref{eq:stepsize_bound_1}), (\ref{eq:stepsize_bound_2}), (\ref{eq:stepsize_bound_3}), (\ref{eq:stepsize_bound_4}), (\ref{eq:stepsize_bound_5}), (\ref{eq:stepsize_bound_6}), (\ref{eq:stepsize_bound_7}), and (\ref{eq:stepsize_bound_8});
        \end{enumerate}
    \end{enumerate}
    Then, with probability $1-\alpha$, we have 
   \begin{align*}
        \frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2 \le 
        %\frac{L\Delta\sqrt{d}}{n\varepsilon }
        \wtilde{\cO}\left(\left(
        \frac{L\Delta\sigma d\sigma_{\omega}^{2}B^{2}}{(nT)^{3/2}\tau^{2}} \left(\sqrt{L\Delta}
        + B
        + \sigma\right)\right)^{1/3}
        +
        \frac{\sqrt{L\Delta d}\sigma_{\omega}}{\tau\sqrt{nT}}\left(\sqrt{L\Delta}+ B + \sigma\right)
        \right),
    \end{align*}
    where $\wtilde{\cO}$ hides constant and logarithmic factors and higher order terms decreasing in $T$.
\end{theorem}

\begin{proof} 
    For convenience, we define $\nabla f_i(x^{-1},\xi^{-1}_i) = v_i^{-1} = g_i^{-1} = 0, \Phi^{-1} = \Phi^0$. Next, let us define an event $E^t$ for each $t\in\{0,\dots,T\}$ such that the following inequalities hold for all $k\in\{0,\dots,t\}$
    \begin{enumerate}
        %\item $\Phi^t \le \Phi^0;$
        \item $\|v_i^k - g_i^{k-1}\| \le B$ for $i\in \cI_{k};$
        \item $\|g^k\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3 b + 3 \hat{\beta}a;$
        \item $\|v_i^k - \nabla f_i(x^k)\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a;$
        \item $\|\theta^k_i\|\le b$ for all $i\in[n]$ and $\|\theta^k\|\le \frac{c}{\sqrt{n}};$
        \item $\left\|\frac{1}{n}\sum_{l=1}^{k+1}\sum_{i=1}^n\omega_i^l\right\| \le a$;
        \item $\Phi^k \le 2\Delta$;
        \item \begin{align*}
        \frac{7}{8}\Delta & \ge \frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\eta)^2\sum_{l=0}^{k-1}\sum_{i=1}^n\<(g_i^{l} - v_i^{l}) + \beta(v_i^{l} - \nabla f_i(x^{l})) + \beta(\nabla f_i(x^{l})-\nabla f_i(x^{l+1})), \theta^t_i>\\
        &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{l=0}^{k-1}\sum_{i=1}^n\<v_i^l - \nabla f_i(x^{l}), \theta^{l+1}_i>
        + 4\gamma(1-\beta)\sum_{l=0}^{k-1}\<v^l - \nabla f(x^{l}), \theta^{l+1}>\\
        &\quad +\frac{15\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{l=0}^{k-1}\sum_{i=1}^n\<\nabla f_i(x^l) - \nabla f_i(x^{l+1}), \theta^{l+1}_i>\\
        & \quad + 4\gamma(1-\beta)\sum_{l=0}^{k-1}\<\nabla f(x^l) - \nabla f(x^{l+1}), \theta^{l+1}>.
        \end{align*}
    \end{enumerate}
    Then, we will derive the result by induction, i.e., using the induction w.r.t.\ $t$, we will show that $\Pr(E^t) \ge 1-\frac{\alpha(t+1)}{T+1}$ for all $t\in\{0,\dots, T-1\}$.
    
    Before we move on to the induction part of the proof, we need to establish several useful bounds. Denote the events $\Theta^t_i, \Theta^t$ and $N^{t+1}$ as 
    \begin{equation}
        \Theta^t_i \eqdef \{\|\theta^t_i\| \ge b \}, \quad \Theta^t \eqdef \left\{ \|\theta^t\|\ge \frac{c}{\sqrt{n}} \right\},\quad \text{and} \quad N^{t+1} \eqdef \left\{ \left\|\frac{1}{n}\sum_{l=1}^t\sum_{i=1}^n\omega_i^l\right\| \ge a \right\}
    \end{equation}
    respectively. From \Cref{asmp:batch_noise} we have (see \eqref{eq:sub_Gaussian_alternative})
    \[\Pr(\Theta^{t}_i) \le 2\exp\left(-\frac{b^2}{2\sigma^2}\right) = \frac{\alpha}{6(T+1)n}\] 
    where the last equality is by definition of $b^2$. Therefore, $\Pr(\overline{\Theta}^t_i) \ge 1 - \frac{\alpha }{6(T+1)n}.$ 
    Besides, notice that the constant $c$ in (\ref{eq:constants_a_b_c}) can be viewed as 
    \[
        c = (\sqrt{2}+2b_3)\sigma \quad\text{where} \quad b_3^2 = 3\log\frac{6(T+1)}{\alpha}.
    \]
    Now, we can use \Cref{lem:concentration_lemma} to bound $\Pr(\Theta^t).$ Since all $\theta^t_i$ are independent $\sigma$-sub-Gaussian random vectors, then we have
    \[
    \Pr\left(\left\|\sum_{i=1}^n\theta^t_i\right\|\ge c\sqrt{n}\right) = \Pr\left(\|\theta^t\| \ge \frac{c}{\sqrt{n}}\right) \le \exp(-b_3^2/3) = \frac{\alpha}{6(T+1)}.
    \]
    We also use \Cref{lem:concentration_lemma} to bound $\Pr(N^t)$. Indeed, since all $\omega_i^l$ are independent Gaussian random vectors, then we have 
    \[\Pr\left(\left\|\sum_{l=1}^t\sum_{i=1}^n\omega_i^l\right\| \ge (\sqrt{2} + 2b_2)\sqrt{\sum_{l=1}^t\sum_{i=1}^n\sigma_{\omega}^2d}\right) \le \exp(-\nicefrac{b_2^2}{3}) = \frac{\alpha}{6(T+1)}.
    \]
    with $b_2^2 = 3\log\left(\frac{6(T+1)}{\alpha}\right).$
    This implies that 
    \[
    \Pr\left(\left\|\frac{1}{n}\sum_{l=1}^t\sum_{i=1}^n\omega_i^l\right\| \ge a\right) \le \frac{\alpha}{6(T+1)}
    \]
    due to the choice of $a$ from (\ref{eq:constants_a_b_c}):
    \[a = (\sqrt{2}+2b_2)\sigma_{\omega}\sqrt{d}\sqrt{\frac{T}{n}}, \quad \text{where} \quad b_2^2 = 3\log\frac{6(T+1)}{\alpha}.
    \]
    Note that with this choice of $a$ we have that the above is true for any $t\in\{1,\ldots, T\}$, i.e., $\Pr(N^t) \ge 1-\frac{\alpha}{6(T+1)}$ for all $t\in\{1,\ldots, T\}.$
    
    Now, we are ready to prove that $\Pr(E^t) \ge 1-\frac{\alpha(t+1)}{T+1}$ for all $t\in\{0,\dots, T-1\}.$ First, we show that the base of induction holds.



    
    \paragraph{Base of induction.}


    
    \begin{enumerate}
        \item $\|v_i^0 - g_i^{-1}\| = \|v_i^0\| = \beta\|\nabla f_i(x^0,\xi^0_i)\| = \beta\|\theta^0_i\| + \beta \|\nabla f_i(x^0)\| \le \frac{1}{2}b + \frac{1}{2}B\le \frac{1}{2}B + \frac{1}{2}B = B$ holds with probability $1-\frac{\alpha}{6(T+1)}$. Indeed, we have 
        \[
        \Pr(\Theta^0_i) \le 2\exp\left(-\frac{b^2}{2\sigma^2}\right) = \frac{\alpha}{6(T+1)n}.
        \]
        Therefore, we have 
        \[
        \Pr\left(\cap_{i=1}^n\overline{\Theta}^0_i\right) = 1 - \Pr\left(\cup_{i=1}^n \Theta^0_i\right) \ge 1 - \sum_{i=1}^n\Pr(\Theta^0_i) = 1-n\frac{\alpha}{6(T+1)n} = 1-\frac{\alpha}{6(T+1)}.
        \]
        Moreover, we have 
        \[
        \Pr(\Theta^0) \le \frac{\alpha}{6(T+1)}.
        \]
        
        This means that the probability of the event that each $\left\|\frac{1}{n}\sum_{l=1}^1\sum_{i=1}^n\omega_i^l\right\|\le a$, $\|\theta^0_i\|\le b$, and  $\|\theta^0\|\le \frac{c}{\sqrt{n}},$ and is at least 
        $$1-\frac{\alpha}{6(T+1)} - n\frac{\alpha}{6n(T+1)} - \frac{\alpha}{6(T+1)} = 1 - \frac{\alpha}{2(T+1)}.$$
        \item We have already shown that
        \[
        \Pr\left(\left\|\frac{1}{n}\sum_{i=1}^n\omega_i^1\right\| \ge a\right) \le \frac{\alpha}{6(T+1)},
        \]
       implying that $\left\|\frac{1}{n}\sum_{i=1}^n\omega_i^1\right\| \le a$ with probability at least $1-\frac{\alpha}{6(T+1)}.$
        

        \item $g^0 = \frac{1}{n}\sum_{i=1}^n (g_i^{-1} + \hat{\beta}\clip_\tau(v_i^0 - g_i^{-1}) = \frac{1}{n}\sum_{i=1}^n\hat{\beta}\clip_\tau(\beta\nabla f_i(x^0,\xi^0_i)).$ Therefore, we have
        \begin{align*}
            \|g^0\| &\le \left\|\frac{1}{n}\sum_{i=1}^n\hat{\beta}\beta\nabla f_i(x^0) + \hat{\beta}\beta\theta^0_i + (\hat{\beta}\clip_\tau(\beta\nabla f_i(x^0,\xi^0_i)) - \hat{\beta}\beta\nabla f_i(x^0,\xi^0_i))\right\|\\
            &\le \hat{\beta}\beta\|\nabla f(x^0)\|
            + \frac{\hat{\beta}\beta}{n}\sum_{i=1}^n\|\theta^0_i\|
            + \frac{1}{n}\sum_{i=1}^n\max\left\{0, \beta\|\nabla f_i(x^0,\xi^0_i)\| - \tau\right\}\\
            &\le \hat{\beta}\beta\sqrt{2L(f(x^0)-f(x^*))}
            + \frac{\hat{\beta}\beta}{n}\sum_{i=1}^n\|\theta^0_i\| 
            + \frac{\hat{\beta}}{n}\sum_{i=1}^n\max\left\{0, \beta\|\nabla f_i(x^0)\| + \beta\|\theta^0_i\|- \tau\right\}\\
            &\le \frac{1}{2}\sqrt{2L\Phi^0} 
            + \frac{2\hat{\beta}\beta}{n}\sum_{i=1}^n\|\theta^0_i\|
            + \frac{\hat{\beta}\beta}{n}\sum_{i=1}^n\|\nabla f_i(x^0)\| - \hat{\beta}\tau\\
            &\le \sqrt{64L\Delta} 
            + 2\hat{\beta}\beta b + \hat{\beta}\beta B - \hat{\beta}\tau\\
            &\le \sqrt{64L\Delta} 
            + \frac{3}{2} B - \tau  + b \le \sqrt{64L\Delta} + 3(B-\tau) + \frac{3}{2}b + \hat{\beta}a.
        \end{align*}

        
        The inequalities above again hold in $\cap_{i=1}^n\overline{\Theta}_i^0$, i.e., with probability at least $1-\frac{\alpha}{6(T+1)}.$

        \item We have 
        \begin{align*}
            \|v_i^0 - \nabla f_i(x^0)\| &= \|\nabla \beta f_i(x^0,\xi_i^0) - \nabla f_i(x^0)\|\\ &\le \beta\|\nabla f_i(x^0, \xi^0_i) - \nabla f_i(x^0)\| + (1-\beta)\|\nabla f_i(x^0)\|\\
            &\le \beta b + (1-\beta)B
        \end{align*}
        This bound holds with probability at least $1-\frac{\alpha}{6(T+1)}$ because it holds in $\cap_{i=1}^n\overline{\Theta}_i^0.$
        
        \item Condition $6$ of the induction assumption also hold, as $\Phi^0 \le 2\Phi^0 \le 2\Delta$ by the choice of $\Delta$.

        \item Finally, condition $7$ of the induction assumption holds since the RHS equals $0$.

    \end{enumerate}
    Therefore, we conclude that the conditions $1$-$7$ hold with a probability of at least 
    \begin{align*}
        \Pr\left(\Theta^0\cap \left(\cap_{i=1}^n\overline{\Theta}_i^0\right)\cap \overline{N}^t\right) &\ge 1 - \Pr(\Theta^0) - \sum_{i=1}^n \Pr(\Theta_i^0) - \Pr(N^0)\\
        &\ge 
        1 
        - \frac{\alpha}{6(T+1)}
        - n\cdot \frac{\alpha}{6n(T+1)} 
        - \frac{\alpha}{6(T+1)}\\
        &= 1-\frac{\alpha}{2(T+1)} > 1- \frac{\alpha}{T+1},
    \end{align*}
    i.e., $\Pr(E^0) \ge 1-\frac{\alpha}{T+1}$ holds. This is the base of the induction.


    \paragraph{Transition step of induction.}

    {\bf Case $|\cI_{K+1}| > 0.$}  Assume that all events $\overline{\Theta}^{K+1}, \overline{\Theta}^{K+1}_i$ and $\overline{N}^{K+1}$ take place, i.e., $\|\theta^{K+1}_i\| \le b, \|\theta^{K+1}\|\le \frac{c}{\sqrt{n}}$ for all $i\in[n]$ and $\left\|\frac{1}{n}\sum_{l=1}^{t+1}\sum_{i=1}^n\omega_i^l\right\|\le a$. That is, we assume that event $\overline{\Theta}^{K+1} \cap \left(\cap_{i=1}^n \overline{\Theta}^{K+1}_i\right) \cap \overline{N}^{K+1} \cap E^K$ holds. Then, by the assumptions of the induction, from \Cref{lem:bound_gt_vt_dp} we get for all $i\in \cI_{K+1}$
    \[
    \|v_i^{K+1} - g_i^{K}\| \le \|v_i^{K} - g_i^{K-1}\| - \frac{\hat{\beta}\tau}{2} \le B - \frac{\hat{\beta}\tau}{2}.
    \]
    Therefore, from \Cref{lem:bound_gt_norm_dp} we get that 
    \[
    \|g^{K+1}\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3b + 3\hat{\beta}a,
    \]
    from \Cref{lem:bound_gt_hat} we get that
    \[
    \|\overline{g}^{K+1}\| \le \sqrt{64L\Delta} + 3(B-\tau) + 3b,
    \]
    and from \Cref{lem:bound_nablafxt_vt_dp}
    \[
    \|\nabla f_i(x^{K+1}) - v_i^{K+1}\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a.
    \]
    This means that conditions 1-5 in the induction assumption are also verified for the step $K+1$. Since for all $t\in\{0, \dots, K+1\}$ inequalities $1$-$5$ are verified, we can write for each $t\in \{0,\ldots,K\}$ by \Cref{lem:descent_deltat,lem:descent_Pt_dp,lem:descent_Vt_tilde_dp,lem:descent_Pt_tilde_dp} the following 

    \begin{align*}
        \Phi^{t+1} &= \delta^{t+1} 
        + \frac{2\gamma}{\hat{\beta}\eta}\wtilde{V}^{t+1} 
        + \frac{8\gamma\beta}{\hat{\beta}^2\eta^2}\wtilde{P}^{t+1} 
        + \frac{2\gamma}{\beta}P^{t+1}\\
        &\le \delta^t - \frac{\gamma}{2}\|\nabla f(x^t)\|^2  {\color{red} - \frac{1}{4\gamma}R^t}
        {\color{blue} + 2\gamma \wtilde{V}^t }
       {\color{orange}+ 2\gamma P^t}\\
        & \;+\; \frac{2\gamma}{\hat{\beta}\eta}\left(
    {\color{blue} (1-\hat{\beta}\eta)\wtilde{V}^{t} }
    {\color{pink} + \frac{4\beta^2}{\hat{\beta}\eta}\wtilde{P}^{t}}
    {\color{red} + \frac{4\beta^2L^2}{\hat{\beta}\eta}R^{t}}
    + \beta^2b^2
    \right.\\
    &\;+\; \left.\frac{2}{n}\beta(1-\hat{\beta}\eta)^2\sum_{i=1}^n\<(g_i^{t} - v_i^{t}) + \beta(v_i^{t} - \nabla f_i(x^{t})) + \beta(\nabla f_i(x^{t})-\nabla f_i(x^{t+1})), \theta^{t+1}_i>\right)\\
    &\;+\; \frac{8\gamma\beta}{\hat{\beta}^2\eta^2}\left({\color{pink}(1-\beta)\wtilde{P}^t} 
    {\color{red} + \frac{3L^2}{\beta}R^t }
    + \beta^2b^2 
    + \frac{2}{n}\beta(1-\beta)\sum_{i=1}^n\<v_i^t - \nabla f_i(x^{t+1}), \theta^{t+1}_i>\right)\\
    &\;+\; \frac{2\gamma}{\beta}\left({\color{orange} (1-\beta)P^t }
    {\color{red} 
    + \frac{3L^2}{\beta}R^t }
    + \beta^2\frac{c^2}{n}
    + 2\beta(1-\beta)\<v^t - \nabla f(x^{t+1}), \theta^{t+1}>\right)
    \end{align*}
    
    Rearranging terms, we get
    \begin{align*}
    \Phi^{t+1} &\le \delta^t 
    - \frac{\gamma}{2}\|\nabla f(x^t)\|^2
    + \frac{2\gamma}{\hat{\beta}\eta}\wtilde{V}^t\left(\hat{\beta}\eta + 1-\hat{\beta}\eta\right)
    + \frac{8\gamma\beta}{\hat{\beta}^2\eta^2}\wtilde{P}^t\left(\beta + 1 - \beta\right)
    + \frac{2\gamma}{\beta}P^t\left(\beta + 1-\beta\right)\\
    & \quad - \frac{1}{4\gamma}R^t\left(1 - \frac{32L^2\beta^2}{\hat\beta^2\eta^2}\gamma^2
    - \frac{96L^2}{\hat\beta^2\eta^2}\gamma^2
    - \frac{24L^2}{\beta^2}\gamma^2\right)
    + b^2\left(\frac{2\beta^2\gamma}{\hat{\beta}\eta} + \frac{8\gamma\beta^3}{\hat{\beta}^2\eta^2}\right)
    + c^2\frac{2\gamma\beta}{n}\\
    & \quad + \frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\sum_{i=1}^n\<(g_i^{t} - v_i^{t}) + \beta(v_i^{t} - \nabla f_i(x^{t})) + \beta(\nabla f_i(x^{t})-\nabla f_i(x^{t+1})), \theta^{t+1}_i>\\
    &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{i=1}^n\<v_i^t - \nabla f_i(x^{t}), \theta^{t+1}_i>
    + 4\gamma(1-\beta)\<v^t - \nabla f(x^{t}), \theta^{t+1}>\\
    &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{i=1}^n\<\nabla f_i(x^t) - \nabla f_i(x^{t+1}), \theta^{t+1}_i>\\
    &\quad + 4\gamma(1-\beta)\<\nabla f(x^t) - \nabla f(x^{t+1}), \theta^{t+1}>.
    \end{align*}
    Using momentum restriction $(i)$ and stepsize restriction $(ii)$, we get rid of the term with $R^t$ and obtain
    \begin{align*}
        \Phi^{t+1} 
    &\le \Phi^t 
    - \frac{\gamma}{2}\|\nabla f(x^t)\|^2
    + b^2\left(\frac{2\beta^2\gamma}{\hat{\beta}\eta} + \frac{8\gamma\beta^3}{\hat{\beta}^2\eta^2}\right)
    + c^2\frac{2\gamma\beta}{n}\\
    & \quad + \frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\sum_{i=1}^n\<(g_i^{t} - v_i^{t}) + \beta(v_i^{t} - \nabla f_i(x^{t})) + \beta(\nabla f_i(x^{t})-\nabla f_i(x^{t+1})), \theta^{t+1}_i>\\
    &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{i=1}^n\<v_i^t - \nabla f_i(x^{t}), \theta^{t+1}_i>
    + 4\gamma(1-\beta)\<v^t - \nabla f(x^{t}), \theta^{t+1}>\\
    &\quad +\frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{i=1}^n\<\nabla f_i(x^t) - \nabla f_i(x^{t+1}), \theta^{t+1}_i>\\
    & \quad + 4\gamma(1-\beta)\<\nabla f(x^t) - \nabla f(x^{t+1}), \theta^{t+1}>.
    \end{align*}
    Now we sum all the inequalities above for $t\in\{0,\dots,K\}$ and get 
    \begin{align}
    \Phi^{K+1} 
    &\le \Phi^0 
    - \frac{\gamma}{2}\sum_{t=0}^{K}\|\nabla f(x^t)\|^2
    + Kb^2\left(\frac{2\beta^2\gamma}{\hat{\beta}\eta} + \frac{8\gamma\beta^3}{\hat{\beta}^2\eta^2} \right)
    + Kc^2\frac{2\gamma\beta}{n} \notag\\
    & \quad + \frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\sum_{t=0}^{K}\sum_{i=1}^n\<(g_i^{t} - v_i^{t}) + \beta(v_i^{t} - \nabla f_i(x^{t})) + \beta(\nabla f_i(x^{t})-\nabla f_i(x^{t+1})), \theta^{t+1}_i>\notag\\
    &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{t=0}^{K}\sum_{i=1}^n\<v_i^t - \nabla f_i(x^{t}), \theta^{t+1}_i>
    + 4\gamma(1-\beta)\sum_{t=0}^{K}\<v^t - \nabla f(x^{t}), \theta^{t+1}>\notag\\
    &\quad + \frac{16\gamma\beta^2}{n\eta^2}(1-\beta)\sum_{t=0}^{K}\sum_{i=1}^n\<\nabla f_i(x^t) - \nabla f_i(x^{t+1}), \theta^{t+1}_i>\notag\\
    & \quad + 4\gamma(1-\beta)\sum_{t=0}^{K}\<\nabla f(x^t) - \nabla f(x^{t+1}), \theta^{t+1}>.\label{eq:njqnsfqknj}
    \end{align}
    Rearranging terms, we get
    \begin{align*}
    &\frac{\gamma}{2}\sum_{t=0}^{K}\|\nabla f(x^t)\|^2
    \le \Phi^0 - \Phi^{K+1}
    + K b^2\left(\frac{2\beta^2\gamma}{\hat{\beta}\eta} + \frac{8\gamma\beta^3}{\hat{\beta}^2\eta^2}\right)
    + Kc^2\frac{2\gamma\beta}{n}\\
    & \quad + \frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\sum_{t=0}^{K}\sum_{i=1}^n\<(g_i^{t} - v_i^{t}) + \beta(v_i^{t} - \nabla f_i(x^{t})) + \beta(\nabla f_i(x^{t})-\nabla f_i(x^{t+1})), \theta^{t+1}_i>\\
    &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{t=0}^{K}\sum_{i=1}^n\<v_i^t - \nabla f_i(x^{t}), \theta^{t+1}_i>
    + 4\gamma(1-\beta)\sum_{t=0}^{K}\<v^t - \nabla f(x^{t}), \theta^{t+1}>\\
    &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{t=0}^{K}\sum_{i=1}^n\<\nabla f_i(x^t) - \nabla f_i(x^{t+1}), \theta^{t+1}_i>\\
    & \quad + 4\gamma(1-\beta)\sum_{t=0}^{K}\<\nabla f(x^t) - \nabla f(x^{t+1}), \theta^{t+1}>.
    \end{align*}
    Taking into account that $\frac{\gamma}{2}\sum_{t=0}^{K}\|\nabla f(x^t)\|^2 \ge 0$, we get that the event $E^K\cap \left(\cap_{i=1}^n\overline{\Theta}^{K+1}_i\right)\cap\overline{N}^t \cap \overline{\Theta}^{K+1}$ implies 
    \begin{align*}
    &\Phi^{K+1}
    \le \Phi^0 
    + K b^2\left(\frac{2\beta^2\gamma}{\hat{\beta}\eta} + \frac{8\gamma\beta^3}{\hat{\beta}^2\eta^2}\right)
    + Kc^2\frac{2\gamma\beta}{n}\\
    & \quad + \frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\sum_{t=0}^{K}\sum_{i=1}^n\<(g_i^{t} - v_i^{t}) + \beta(v_i^{t} - \nabla f_i(x^{t})) + \beta(\nabla f_i(x^{t})-\nabla f_i(x^{t+1})), \theta^{t+1}_i>\\
    &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{t=0}^{K}\sum_{i=1}^n\<v_i^t - \nabla f_i(x^{t}), \theta^{t+1}_i>
    + \frac{4\gamma(1-\beta)}{n}\sum_{t=0}^{K}\sum_{i=1}^n\<v^t - \nabla f(x^{t}), \theta^{t+1}_i>\\
    &\quad + \frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{t=0}^{K}\sum_{i=1}^n\<\nabla f_i(x^t) - \nabla f_i(x^{t+1}), \theta^{t+1}_i>\\
    &\quad + \frac{4\gamma(1-\beta)}{n}\sum_{t=0}^{K}\sum_{i=1}^n\<\nabla f(x^t) - \nabla f(x^{t+1}), \theta^{t+1}_i>.
    \end{align*}
    Next, we define the following random vectors:
    \begin{align*}
    &\zeta_{1,i}^t \eqdef \begin{cases} 
    g_i^t - v_i^t, &\text{ if } \|g_i^t-v_i^t\| \le B\\
    0, &\text{otherwise}
    \end{cases},\\
    &\zeta_{2,i}^t \eqdef \begin{cases} 
    v_i^t - \nabla f_i(x^t), &\text{ if } \|v_i^t - \nabla f_i(x^t)\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\\
	0, &\text{otherwise}
    \end{cases}, \\
    &\zeta_{3,i}^t \eqdef \begin{cases} 
    \nabla f_i(x^t) - \nabla f_i(x^{t+1}), &\text{ if } \|\nabla f_i(x^t) - \nabla f_i(x^{t+1})\| \le L\gamma\left(\sqrt{64L\Delta} + 3(B-\tau) + 3 b + 3\hat{\beta} a\right)\\
	0, &\text{otherwise}
    \end{cases}, \\
    &\zeta_{4}^t \eqdef \begin{cases} 
    v^t - \nabla f(x^{t}), &\text{ if } \|v^t - \nabla f(x^{t})\| \le \sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\\
	0, &\text{otherwise}
    \end{cases}, \\
    &\zeta_{5}^t \eqdef \begin{cases} 
    \nabla f(x^t) - \nabla f(x^{t+1}), &\text{ if } \|\nabla f(x^t) - \nabla f(x^{t+1})\| \le L\gamma\left(\sqrt{64L\Delta} + 3(B-\tau) + 3 b + 3\hat{\beta}a\right)\\
	0, &\text{otherwise}
    \end{cases}.
    \end{align*}
    By definition, all introduced random vectors $\zeta_{l,i}^t, l\in[3], i\in[n], \zeta_{4,5}^t$ are bounded with probability $1$. Moreover, by the definition of  $E^t$ we get that the event $E^K\cap \overline{\Theta}^{K+1}\cap \left(\cap_{i=1}^n\overline{\Theta}^{K+1}_i\right) \cap \overline{N}^{K+1}$ implies 
    \begin{align*}
    &\zeta_{1,i}^t = g_i^t-v_i^t,\quad  \zeta_{2,i}^t = v_i^t-\nabla f_i(x^t), \quad \zeta_{3,i}^t = \nabla f_i(x^t) - \nabla f_i(x^{t+1}),\\
    &\zeta_{4}^t = v^t-\nabla f(x^t),\quad \zeta_{5}^t = \nabla f(x^t) - \nabla f(x^{t+1}).
    \end{align*}
    Therefore, the event $E^K\cap \overline{\Theta}^{K+1} \cap \left(\cap_{i=1}^n\overline{\Theta}^{K+1}_i\right) \cap \overline{N}^{K+1}$ implies 
    \begin{align*}
    &\Phi^{K+1}
    \le \Phi^0 
    + \underbrace{K b^2\left(\frac{2\beta^2\gamma}{\hat{\beta}\eta} + \frac{8\gamma\beta^3}{\hat{\beta}^2\eta^2}\right) + Kc^2\frac{2\gamma\beta}{n}}_{\circledOne}
    + \underbrace{\frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\eta)^2\sum_{t=0}^{K}\sum_{i=1}^n\<\zeta_{1,i}^t, \theta^{t+1}_i>}_{\circledTwo}\\
    &\;+\; \underbrace{\frac{4\gamma\beta^2}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\sum_{t=0}^{K}\sum_{i=1}^n\<\zeta_{2,i}^t, \theta^{t+1}_i>}_{\circledThree}
    +  \underbrace{\frac{4\gamma\beta^2}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\sum_{t=0}^{K}\sum_{i=1}^n\<\zeta_{3,i}^t, \theta^{t+1}_i>}_{\circledFour}\\
    &\;+\; \underbrace{\frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{t=0}^{K}\sum_{i=1}^n\<\zeta_{2,i}^t, \theta^{t+1}_i>}_{\circledFive}
    + \underbrace{\frac{4\gamma(1-\beta)}{n}\sum_{t=0}^{K}\sum_{i=1}^n\<\zeta_{4}^t, \theta^{t+1}_i>}_{\circledSix}\\
    &\;+\; \underbrace{\frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\sum_{t=0}^{K}\sum_{i=1}^n\<\zeta_{3,i}^t, \theta^{t+1}_i>}_{\circledSeven}
    + \underbrace{\frac{4\gamma(1-\beta)}{n}\sum_{t=0}^{K}\sum_{i=1}^n\<\zeta_{5}^t, \theta^{t+1}_i>}_{\circledEight}.
    \end{align*}
    
    %%%%%%%%%%%%%%%%%%
    \subparagraph{Bound of the term $\circledOne$.} Since $6L\gamma \leq \beta$, for the term $\circledOne$ we have 
    \begin{align*}
        K b^2\left(\frac{2\beta^2\gamma}{\hat{\beta}\eta} + \frac{8\gamma\beta^3}{\hat{\beta}\eta^2}\right)
        + Kc^2\frac{2\gamma\beta}{n}&
        \le K b^2\left(\frac{\beta^{3}}{3L\hat{\beta}\eta} + \frac{4\beta^4}{3L\hat{\beta}^2\eta^2}\right)
        + Kc^2\frac{\beta^2}{3Ln}.
    \end{align*}
    By choosing $\beta$ such that 
    \begin{equation}\label{eq:stepsize_bound_1}
    \beta \le \min\left\{
    \left(\frac{L\Delta\hat{\beta}\eta}{8Tb^2}\right)^{1/3},
    \left(\frac{L\Delta\hat{\beta}^2\eta^2}{32Tb^2}\right)^{1/4},
    \left(\frac{L\Delta n}{8Tc^2}\right)^{1/2}
    \right\}
    \end{equation}
    we get that 
    \[
        K b^2\left(\frac{2\beta^2\gamma}{\hat{\beta}\eta} + \frac{8\gamma\beta^3}{\hat{\beta}^2\eta^2}\right)
        + Kc^2\frac{2\gamma\beta}{n} \le 3 \cdot \frac{\Delta}{24} = \frac{\Delta}{8}.
    \]
    This bound holds with probability $1.$ Note that the worst dependency in the restriction on $\beta$ in $T$ is $\cO(\nicefrac{1}{T^{3/4}})$ since $\hat{\beta} \sim \frac{1}{a} \sim \frac{1}{T}$ that comes from the second term in \eqref{eq:stepsize_bound_1}.


    
    %%%%%%%%%%%%%%%%%%
    \subparagraph{Bound of the term $\circledTwo$.} For term $\circledTwo$, let us enumerate random variables as 
    \[
    \<\zeta_{1,1}^0,\theta^1_1>, \dots, \<\zeta_{1,n}^0,\theta^1_n>, \<\zeta_{1,1}^1,\theta^2_1>,\dots, \<\zeta_{1,n}^1,\theta^2_n>, \dots 
    \<\zeta_{1,1}^K,\theta^{K+1}_1>,\dots,
    \<\zeta_{1,n}^K,\theta^{K+1}_n>,
    \]
    i.e., first by index $i$, then by index $t$. Then we have that the event $E^K\cap \left(\cap_{i=1}^n\overline{\Theta}^{K+1}_i\right)$ implies 
    \[
    \E{\frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\eta)^2\<\zeta^l_{1,i},\theta^{l+1}_i>\mid  \<\zeta_{1,i-1}^l,\theta^{l+1}_{i-1}>, \dots, \<\zeta_{1,1}^l,\theta^{l+1}_{1}>, \dots, \<\zeta_{1,1}^{0},\theta^{1}_{1}>} = 0,
    \]
    because $\{\theta^{l+1}_i\}_{i=1}^n$ are independent. Let 
    \[
    \sigma_{2}^2 \eqdef \frac{16\gamma^2\beta^2}{n^2\hat{\beta}^2\eta^2}\cdot B^2 \cdot \sigma^2.
    \]
    Since $\theta^{l+1}_i$ is $\sigma$-sub-Gaussian random vector, for $$\E{\cdot \mid l,i-1} \eqdef \E{ \cdot \mid  \<\zeta_{1,i-1}^l,\theta^{l+1}_{i-1}>, \dots, \<\zeta_{1,1}^l,\theta^{l+1}_{1}>, \dots, \<\zeta_{1,1}^{0},\theta^{1}_{1}>}$$ we have
    \begin{align*}
    &\E{\exp\left(\left|\frac{1}{\sigma_2^2}\frac{16\gamma^2\beta^2}{n^2\hat{\beta}^2\eta^2}(1-\eta)^4\<\zeta^l_{1,i},\theta^{l+1}_i>^2\right| \right) \mid l, i-1}\\
    &\le 
    \E{\exp\left(\frac{1}{\sigma^2_1}\frac{16\gamma^2\beta^2}{n^2\hat{\beta}^2\eta^2}\|\zeta_{1,i}^l\|^2 \cdot \|\theta^{l+1}_i\|^2 \right) \mid l,i-1}\\
    &\le \E{\exp\left(\frac{1}{\sigma_2^2}\frac{16\gamma^2\beta^2}{n^2\hat{\beta}^2\eta^2}\cdot B^2\|\theta^{l+1}_i\|^2\right) \mid l, i-1} \\
    &\le \E{\exp\left(\frac{n^2\hat{\beta}^2\eta^2}{16\gamma^2\beta^2\cdot B^2 \cdot \sigma^2}\frac{16\gamma^2\beta^2}{n^2\hat{\beta}^2\eta^2}\cdot B^2\|\theta^{l+1}_i\|^2\right) \mid l, i-1} \\  
    &= \E{\exp\left(\frac{\|\theta^{l+1}_i\|^2}{\sigma^2}\mid l, i-1\right)} \le \exp(1).
    \end{align*}
    Therefore, we have by \Cref{lem:concentration_lemma} with $\sigma_k^2 \equiv \sigma_2^2$ that 
    \begin{align*}
    &\Pr\left(\frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{1,i}^t,\theta^{t+1}_i>\right\| 
    \ge (\sqrt{2}+\sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n\frac{16B^2\gamma^2\beta^2\sigma^2}{n^2\hat{\beta}^2\eta^2}}\right) \\
    & \quad \le \exp(-\nicefrac{b_1^2}{3}) \\
    &\quad = \frac{\alpha}{14(T+1)}
    \end{align*}
    with $b_1^2 = 3\log\left(\frac{14(T+1)}{\alpha}\right)$.  Note that since $6L\gamma\le \beta$
    \begin{align*}
    (\sqrt{2} + \sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n\frac{16B^2\gamma^2\beta^2\sigma^2}{n^2\hat{\beta}^2\eta^2}} 
    &\le (\sqrt{2} + \sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n\frac{4B^2\beta^{4}\sigma^2}{9L^2n^2\hat{\beta}^2\eta^2}} \\
    &= (\sqrt{2} + \sqrt{2}b_1)\frac{2B\beta^{2} \sigma}{3Ln\hat{\beta}\eta}\sqrt{(K+1)n}\\
    & \le \frac{\Delta}{8},
    \end{align*}
    because we choose $\beta$ such that 
    \begin{equation}\label{eq:stepsize_bound_2}
        \beta \le \left(\frac{3L\Delta\sqrt{n}\hat{\beta}\eta}{16\sqrt{2}(1+b_1)B\sigma\sqrt{T}}\right)^{1/2}, \quad \text{ and } \quad K+1 \le T.
    \end{equation}
    This implies that 
    \[
    \Pr\left(\frac{4\gamma\beta}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{1,i}^t,\theta^{t+1}_i>\right\| \ge \frac{\Delta}{8}\right) \le \frac{\alpha}{14(T+1)}
    \]
    with this choice of momentum parameter. The dependency of \eqref{eq:stepsize_bound_2} on $T$ is $\wtilde\cO(\nicefrac{1}{T^{3/4}})$ since $\hat{\beta} \sim \frac{1}{T}.$

    
    
    %%%%%%%%%%%%%%%%%%
    \subparagraph{Bound of the term $\circledThree$.} The bound in this case is similar to the previous one. Let 
    \[
    \sigma_3^2 \eqdef \frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}\cdot \left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b +  \hat{\beta}a\right)^2\cdot \sigma^2.
    \]
    
    Then,
    \begin{align*}
    &\E{\exp\left(\left|\frac{1}{\sigma_3^2}\frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}(1-\hat{\beta}\eta)^4\<\zeta^l_{2,i},\theta^{l+1}_i>^2\right|\right) \mid l,i-1} \\
    &\le 
    \E{\exp\left(\frac{1}{\sigma_3^2}\frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}\|\zeta_{2,i}^l\|^2 \cdot \|\theta^{l+1}_i\|^2\right)}\\
    &\le \E{\exp\left(\frac{1}{\sigma^3_2}\frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}\cdot \left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\right)^2\cdot \|\theta_i^{l+1}\|^2\right)\mid l,i-1}\\
    &\le \mathbb{E}\left[\exp\left(\left[\frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}\cdot \left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\right)^2\cdot \sigma^2\right]^{-1}\cdot\right.\right.\\
    &\qquad \left.\left.\frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}\cdot \left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\right)^2\cdot \|\theta_i^{l+1}\|^2\right)\mid l,i-1\right]\\
    &= \E{\exp\left(\frac{\|\theta^{l+1}_i\|^2}{\sigma^2}\right)\mid l,i-1} \le \exp(1).
    \end{align*}
    Therefore, we have by \Cref{lem:concentration_lemma} that 
    \begin{align*}
    &\Pr\left[\frac{4\gamma\beta^2}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{2,i}^t,\theta^{t+1}_i>\right\| \right.\\
    &\qquad\ge  \left.(\sqrt{2}+\sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n \frac{16\gamma^2\beta^4\sigma^2}{n^2\hat{\beta}^2\eta^2}\cdot \left(\sqrt{4L\Delta}+ \frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\right)^2}\right]\\ 
    &\le \exp(-\nicefrac{b_1^2}{3}) = \frac{\alpha}{14(T+1)}.
    \end{align*}
    Note that by using the restrictions $\hat{\beta} \le \frac{\sqrt{L\Delta}}{a}$ and $6L\gamma \le \beta$ we get
    \begin{align*}
    & (\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n}\frac{4\gamma\beta^2\sigma}{\hat{\beta}\eta n}\left(\sqrt{4L\Delta}+ \frac{3}{2}(B-\tau) + \frac{3}{2}b+ \hat{\beta}a\right)\\
    \le & (\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n}\frac{2\beta^{3}\sigma}{3L\hat{\beta}\eta n}\left(\sqrt{4L\Delta}+ \frac{3}{2}(B-\tau) + \frac{3}{2}b + \sqrt{L\Delta}\right)\\
    \le& \frac{\Delta}{8}
    \end{align*}
    holds because we choose 
    \begin{align}\label{eq:stepsize_bound_3}
    \beta &\le \left(\frac{3L\Delta\hat{\beta}\eta \sqrt{n}}{16\sqrt{2}(1+b_1)\sigma\sqrt{T}\left(\sqrt{9L\Delta} + \frac{3}{2}(B-\tau) + \frac{3}{2}b\right)}\right)^{1/3},
    &\quad \text{ and } \quad K+1 \le T.
    \end{align}
    This implies 
    \begin{align*}
    &\Pr\left(\frac{4\gamma\beta^2}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{2,i}^t,\theta^{t+1}_i>\right\| \ge \frac{\Delta}{8}\right) \le \frac{\alpha}{14(T+1)}.
    \end{align*}
    Note that the worst dependency in the choice of $\beta$ w.r.t. $T$ is $\wtilde\cO(\nicefrac{1}{T^{1/2}})$ since $\hat{\beta} \sim \frac{1}{T}.$
%%%%%%%%%%%%%%%%%%
    \subparagraph{Bound of the term $\circledFour$.} The bound in this case is similar to the previous one. Let
    \[
    \sigma_4^2 \eqdef \frac{16L^2\gamma^4\beta^4}{n^2\hat{\beta}^2\eta^2}\left(\sqrt{64L\Delta} + 3(B-\tau) + 3b + 3\hat{\beta}a\right)^2\cdot\sigma^2.
    \]
    Then we have
    \begin{align*}
    &\E{\exp\left(\left|\frac{1}{\sigma_4^2}\frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}(1-\hat{\beta}\eta)^4\<\zeta^l_{3,i},\theta^{l+1}_i>^2\right| \right)\mid l,i-1}\\
    &\le 
    \E{\exp\left(\frac{1}{\sigma_4^2}\frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}\|\zeta_{3,i}^l\|^2 \cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1}\\
    &\le \E{\exp\left(\frac{1}{\sigma_4^2}\frac{16\gamma^2\beta^4}{n^2\hat{\beta}^2\eta^2}\cdot L^2\gamma^2\left(\sqrt{64L\Delta} + 3(B-\tau) + 3b + 3a\right)^2\cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1}\\
    &\le \mathbb{E}\left[\exp\left(\left[\frac{16L^2\gamma^4\beta^4}{n^2\hat{\beta}^2\eta^2}\left(\sqrt{64L\Delta} + 3(B-\tau) + 3b + 3\hat{\beta}a\right)^2\cdot\sigma^2\right]^{-1} \right.\right.\\
    &\qquad \left.\left.\frac{16L^2\gamma^4\beta^4}{n^2\hat{\beta}^2\eta^2}\left(\sqrt{64L\Delta} + 3(B-\tau) + 3b + 3\hat{\beta}a\right)^2\cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1\right]\\
    &= \E{\exp\left(\frac{\|\theta^{l+1}_i\|^2}{\sigma^2}\right)} \le \exp(1).
    \end{align*}
    Therefore, we have by \Cref{lem:concentration_lemma} that 
    \begin{align*}
    &\Pr\left(\frac{4\gamma\beta^2}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{3,i}^t,\theta^{t+1}_i>\right\| \right.\\
    &\qquad\ge \left.(\sqrt{2}+\sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n \frac{16L^2\gamma^4\beta^{4}\sigma^2}{n^2\hat{\beta}^2\eta^2}\cdot \left(\sqrt{64L\Delta} + 3(B-\tau + b) + 3\hat{\beta}a\right)^2}\right)\\ 
    &\le \exp(-\nicefrac{b_1^2}{3}) = \frac{\alpha}{14(T+1)}.
    \end{align*}
    Using the restrictions $\hat{\beta} \le \frac{\sqrt{L\Delta}}{a}$ and $6L\gamma\le \beta$ we get
    \begin{align*}
    &(\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n}\frac{4L\gamma^2\beta^2\sigma}{\hat{\beta}\eta n}\left(\sqrt{64L\Delta} + 3(B-\tau + b) + 3\hat{\beta}a\right)\\
    \le & 
    \sqrt{2}(1+b_1)\sqrt{(K+1)n}\frac{\beta^{4}\sigma}{9L\hat{\beta}\eta n}\left(\sqrt{64L\Delta} + 3(B-\tau+b) + 3\sqrt{L\Delta}\right)\\ \le & \frac{\Delta}{8},
    \end{align*}
    because we choose $\beta$ such that
    \begin{align}\label{eq:stepsize_bound_4}
    \beta &\le \left(\frac{9L\Delta\hat{\beta}\eta\sqrt{n}}{8\sqrt{2}(1+b_1)\sigma\sqrt{T}\left(11\sqrt{L\Delta} + 3(B-\tau+b)\right)}\right)^{1/4},
    &\quad \text{and} \quad K+1\le T.
    \end{align}
    This implies 
    \begin{align*}
    &\Pr\left(\frac{4\gamma\beta^2}{n\hat{\beta}\eta}(1-\hat{\beta}\eta)^2\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{2,i}^t,\theta^{t+1}_i>\right\| \ge \frac{\Delta}{8}\right) \le \frac{\alpha}{14(T+1)},
    \end{align*}
    Note that the worst dependency in the choice of $\beta$ w.r.t. $T$ is $\wtilde\cO(\nicefrac{1}{T^{3/8}})$ since $\hat{\beta} \sim \frac{1}{T}.$

   
    
    %%%%%%%%%%%%%%%%%%
    \subparagraph{Bound of the term $\circledFive$.} The bound in this case is similar to the previous one. 
    Let 
    \[
    \sigma_5^2 \eqdef \frac{256\gamma^2\beta^4}{n^2\hat{\beta}^4\eta^4}\cdot \left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)+\frac{3}{2}b + \hat{\beta}a\right)^2\cdot \sigma^2.
    \]
    Then we have 
    \begin{align*}
    &\E{\exp\left(\left|\frac{1}{\sigma_5^2}\frac{256\gamma^2\beta^4}{n^2\hat{\beta}^4\eta^4}(1-\beta)^2\<\zeta^l_{2,i},\theta^{l+1}_i>^2\right|\right)\mid l,i-1} \\
    &\le 
    \E{\exp\left(\frac{1}{\sigma^2_5}\frac{256\gamma^2\beta^4}{n^2\hat{\beta}^4\eta^4}\|\zeta_{2,i}^l\|^2 \cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1}\\
    &\le \E{\exp\left(\frac{1}{\sigma_5^2}\frac{256\gamma^2 \beta^4}{n^2\hat{\beta}^4\eta^4}\cdot \left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)+\frac{3}{2}b+\hat{\beta}a\right)^2\cdot \|\theta^{l+1}_i\|^2\right) \mid l,i-1}\\
    &= \mathbb{E}\left[\exp\left( \left[\frac{256\gamma^2\beta^4}{L^2n^2\hat{\beta}^4\eta^4}\cdot \left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)+\frac{3}{2}b+\hat{\beta}a\right)^2\cdot \sigma^2\right]^{-1}\right.\right.\\
    &\qquad \left.\left. \frac{256\gamma^2\beta^4}{n^2\hat{\beta}^4\eta^4}\cdot \left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)+\frac{3}{2}b+\hat{\beta}a\right)^2\cdot \|\theta^{l+1}_i\|^2\right) \mid l,i-1\right]\\
    &= \E{\exp\left(\frac{\|\theta^{l+1}_i\|^2}{\sigma^2}\right) \mid l,i-1} \le \exp(1).
    \end{align*}
    Therefore, we have by \Cref{lem:concentration_lemma} that 
    \begin{align*}
    &\Pr\left[\frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{2,i}^t,\theta^{t+1}_i>\right\|\right.\\ 
    &\qquad  \ge \left. (\sqrt{2}+\sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n \frac{256\gamma^2\beta^4\sigma^2}{n^2\hat{\beta}^4\eta^4}\left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)+\frac{3}{2}b+\hat{\beta}a\right)^2}\right]\\ 
    &\le \exp(-\nicefrac{b_1^2}{3}) = \frac{\alpha}{14(T+1)}.
    \end{align*}
    Using the restrictions $6L\gamma \le \beta$ and $\hat{\beta} \le\frac{\sqrt{L\Delta}}{a}$ we get 
    \begin{align*}
    &(\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n}\frac{16\gamma\beta^2\sigma}{n\hat{\beta}^2\eta^2}\left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)+\frac{3}{2}b+\hat{\beta}a\right)\\
    \le &(\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n}\frac{8\beta^{3}\sigma}{3Ln\hat{\beta}^2\eta^2}\left(\sqrt{4L\Delta} + \frac{3}{2}(B-\tau)+\frac{3}{2}b + \sqrt{L\Delta}\right)\\
    \le &\frac{\Delta}{8}
    \end{align*}
    because we choose $\beta$ such that 
    \begin{align}\label{eq:stepsize_bound_5}
        \beta &\le \left(\frac{3L\Delta\hat{\beta}^2\eta^2\sqrt{n}}{64\sqrt{2}(1+b_1)\sigma\sqrt{T}\left(3\sqrt{L\Delta} + \frac{3}{2}(B-\tau)+\frac{3}{2}b\right)}\right)^{1/3},
        &\quad \text{and } K+1 \le T.
    \end{align}
    This implies 
    \begin{align*}
    &\Pr\left(\frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\hat{\beta}\beta)\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{2,i}^t,\theta^{t+1}_i>\right\| \ge \frac{\Delta}{8}\right) \le \frac{\alpha}{14(T+1)}.
    \end{align*}
    Note that the worst dependency in the choice of $\beta$ w.r.t. $T$ is $\wtilde{\cO}(\nicefrac{1}{T}^{5/6})$ since $\hat{\beta} \sim \frac{1}{T}$.
    
    %%%%%%%%%%%%%%%%%%
     \subparagraph{Bound of the term $\circledSeven$.} The bound in this case is similar to the previous one. Let
     \[
        \sigma_7^2 \eqdef \frac{256L^2\gamma^4\beta^{4}}{n^2\hat{\beta}^4\eta^4} \left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)^2\cdot \sigma^2.
     \]
     Then we have 
    \begin{align*}
    &\E{\exp\left(\left|\frac{1}{\sigma_7^2}\frac{256L^2\gamma^4\beta^4}{n^2\hat{\beta}^4\eta^4}(1-\beta)^2\<\zeta^l_{3,i},\theta^{l+1}_i>^2\right|\right) \mid l,i-1}\\
    &\le 
    \E{\exp\left(\frac{1}{\sigma_7^2}\frac{256\gamma^2\beta^4}{n^2\hat{\beta}^4\eta^4}\|\zeta_{3,i}^l\|^2 \cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1}\\
    &\le \E{\exp\left(\frac{256\gamma^2\beta^4}{n^2\hat{\beta}^4\eta^4}\cdot L^2\gamma^2\left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)^2\cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1}\\
    &\le \mathbb{E}\left[\exp\left(\left[\frac{256L^2\gamma^4\beta^{4}}{n^2\hat{\beta}^4\eta^4}\left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)^2\cdot \sigma^2\right]^{-1} \right.\right. \\
    & \qquad \left.\left.\frac{256L^2\gamma^4\beta^{4}}{n^2\hat{\beta}^4\eta^4}\left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)^2\cdot \|\theta^{l+1}_i\|^2 \right) \mid l,i-1\right]\\
    &= \E{\exp\left(\frac{\|\theta^{l+1}_i\|^2}{\sigma^2}\right) \mid l,i-1} \le \exp(1).
    \end{align*}
    Therefore, we have by \Cref{lem:concentration_lemma} that 
    \begin{align*}
    &\Pr\left[\frac{16\gamma\beta^2}{n\hat{\beta}^2\eta^2}(1-\beta)\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{3,i}^t,\theta^{t+1}_i>\right\| \ge\right.\\ 
    &\qquad \left.(\sqrt{2}+\sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n \frac{256L^2\gamma^4\beta^{4}\sigma^2}{n^2\hat{\beta}^4\eta^4}\cdot \left(\sqrt{64L\Delta} + 3(B-\tau + b) + 3\hat{\beta}a\right)^2}\right]\\ 
    &\le \exp(-\nicefrac{b_1^2}{3}) = \frac{\alpha}{14(T+1)}.
    \end{align*}
    Using the restrictions $6L\gamma\le\beta$ and $\hat{\beta} \le \frac{\sqrt{L\Delta}}{a}$ we get
    \begin{align*}
    &(\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n}\frac{16L\gamma^2\beta^2\sigma}{\hat{\beta}^2\eta^2n}\left(\sqrt{64L\Delta} + 3(B-\tau + b)+3\hat{\beta}a\right)\\
    \le &(\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n}\frac{4\beta^{4}\sigma}{9L\hat{\beta}^2\eta^2n}\left(8\sqrt{L\Delta} + 3(B-\tau +b)+ 3\sqrt{L\Delta}\right)\\
    \le& \frac{\Delta}{8}   
    \end{align*}
    because we choose
    \begin{align}\label{eq:stepsize_bound_7}
    \beta &\le \left(\frac{9L\Delta\hat{\beta}^2\eta^2\sqrt{n}}{32\sqrt{2}(1+b_1)\sigma\sqrt{T}\left(11\sqrt{L\Delta} + 3(B-\tau+B)\right)}\right)^{1/4},
    &\quad \text{and} \quad K+1 \le T.
    \end{align}
    This implies 
    \begin{align*}
    &\Pr\left(\frac{8\gamma\beta^2}{n\eta^2}(1-\beta)\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{3,i}^t,\theta^{t+1}_i>\right\| \ge \frac{\Delta}{8}\right) \le \frac{\alpha}{14(T+1)}.
    \end{align*}
    Note that the worst dependency in the choice of $\beta$ w.r.t.\ $T$ is $\wtilde{\cO}(\nicefrac{1}{T^{5/8}})$ since $\hat{\beta} \sim \frac{1}{T}.$
    
   
 %%%%%%%%%%%%%%%%%%
     \subparagraph{Bound of the term $\circledSix$.} The bound in this case is similar to the previous one. Let
     \[
        \sigma_6^2 \eqdef \frac{16\gamma^2}{n^2}\left(\sqrt{4L\Delta} +\frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\right)^2\cdot \sigma^2.
     \]
      
     Then we have 
    \begin{align*}
    &\E{\exp\left(\left|\frac{1}{\sigma_6^2}\frac{16\gamma^2}{n^2}(1-\beta)^2\<\zeta^l_{4},\theta^{l+1}_i>^2\right|\right)\mid l,i-1 }\\
    &\le 
    \E{\exp\left(\frac{1}{\sigma_6^2}\frac{16\gamma^2}{n^2}\|\zeta_{4}^l\|^2 \cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1}\\
    &\le \E{\exp\left(\frac{1}{\sigma^2_6}\frac{16\gamma^2}{n^2}\left(\sqrt{4L\Delta} +\frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a)\right)^2\cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1}\\
    &\le \mathbb{E}\left[\exp\left(\left[\frac{16\gamma^2}{n^2}\left(\sqrt{4L\Delta} +\frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a)\right)^2\cdot \sigma^2\right]^{-1} \right.\right.\\
    &\qquad\left.\left. \frac{16\gamma^2}{n^2}\left(\sqrt{4L\Delta} +\frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a)\right)^2\cdot\|\theta^{l+1}_i\|^2\right)\mid l,i-1\right]\\
    &= \E{\exp\left(\frac{\|\theta^{t+1}_i\|^2}{\sigma^2}\right)\mid l,i-1} \le \exp(1).
    \end{align*}
    Therefore, we have by \Cref{lem:concentration_lemma} that 
    \begin{align*}
    &\Pr\left[\frac{\gamma(1-\beta)}{n}\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{4,i}^t,\theta^{t+1}_i>\right\|\right.\\ 
    &\ge \left. (\sqrt{2}+\sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n \frac{16\gamma^2}{n^2}\sigma^2\cdot \left(\sqrt{4L\Delta} +\frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta} a)\right)^2}\right]\\ 
    &\le \exp(-\nicefrac{b_1^2}{3}) = \frac{\alpha}{14(T+1)},
    \end{align*}
    
    Using the restrictions $6L\gamma\le\beta$ and $\hat{\beta} \le \frac{\sqrt{L\Delta}}{a}$ we get
    \begin{align*}
    &(\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n} \cdot \frac{4\gamma}{n}\sigma\left(\sqrt{4L\Delta} +\frac{3}{2}(B-\tau) + \frac{3}{2}b + \hat{\beta}a\right)\\
    \le & (\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n} \cdot \frac{2\beta}{3Ln}\sigma\left(\sqrt{4L\Delta} +\frac{3}{2}(B-\tau) + \frac{3}{2}b + \sqrt{L\Delta}\right)\\
    \le &\frac{\Delta}{8}
    \end{align*}
    because we choose $\beta$ such that
    \begin{align}\label{eq:stepsize_bound_6}
    \beta &\le \left(\frac{3L\Delta\sqrt{n}}{16\sqrt{2}(1+b_1)\sigma\sqrt{T}\left(3\sqrt{L\Delta} +\frac{3}{2}(B-\tau) + \frac{3}{2}b\right)}\right),
    &\quad \text{and} \quad K+1\le T.
    \end{align}
    This implies 
    \begin{align*}
    &\Pr\left(\frac{4\gamma(1-\beta)}{n}\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{4,i}^t,\theta^{t+1}_i>\right\| \ge \frac{\Delta}{8}\right) \le \frac{\alpha}{14(T+1)}.
    \end{align*}
    Note that the worst dependency in the choice of $\beta$ w.r.t. $T$ is $\wtilde{\cO}(\nicefrac{1}{T^{1/2}})$.

%%%%%%%%%%%%%%%%%%

    \subparagraph{Bound of the term $\circledEight$.} The bound in this case is similar to the previous one. Let
    \[
        \sigma^2_8 \eqdef \frac{16L^2\gamma^4}{n^2}\cdot \left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)^2\cdot \sigma^2.
    \]
    Then we have 
    \begin{align*}
    &\E{\exp\left(\left|\frac{1}{\sigma^2_8}\frac{16\gamma^2}{n^2}(1-\beta)^2\<\zeta^l_{5},\theta^{l+1}_i>^2\right|\right)\mid l,i-1}\\
    &\le 
    \E{\exp\left(\frac{1}{\sigma^2_8}\frac{16\gamma^2}{n^2}\|\zeta_{5}^l\|^2 \cdot \|\theta^{l+1}_i\|^2\right)\mid l,i-1}\\
    &\le \E{\exp\left(\frac{1}{\sigma^2_8}\frac{16\gamma^2}{n^2}L^2\gamma^2\left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right) \cdot \|\theta^{l+1}_i\|^2\right)^2\mid l,i-1}.
    \end{align*}
    Since $\theta^{l+1}_i$ is sub-Gaussian with parameter $\sigma^2$, then we can continue the chain of inequalities above using the definition of $\sigma_8^2$
    \begin{align*}
        &\mathbb{E}\left[\exp\left(\left[\frac{16L^2\gamma^4}{n^2}\cdot \left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)^2\cdot \sigma^2\right]^{-1}  \right.\right.\\
        &\qquad \left.\left. \frac{4L^2\gamma^4}{n^2}\cdot \left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)^2\cdot\|\theta^{l+1}_i\|^2\right)\mid l,i-1\right]\\
        &= \E{\exp\left(\frac{\|\theta^{l+1}_i\|^2}{\sigma^2}\right)} \le \exp(1).
    \end{align*}
    Therefore, we have by \Cref{lem:concentration_lemma} that 
    \begin{align*}
    &\Pr\left[\frac{4\gamma(1-\beta)}{n}\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{5,i}^t,\theta^{t+1}>\right\|\right.\\ 
    &\ge \left. (\sqrt{2}+\sqrt{2}b_1)\sqrt{\sum_{t=0}^K\sum_{i=1}^n \frac{16L^2\gamma^4}{n^2}\sigma^2\cdot \left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)^2}\right]\\ 
    &\le \exp(-\nicefrac{b_1^2}{3}) = \frac{\alpha}{14(T+1)}.
    \end{align*}
    Using the restrictions $6L\gamma\le \beta$ and $\hat{\beta}\le \frac{\sqrt{L\Delta}}{a}$ we get
    \begin{align*}
    &(\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n} \cdot \frac{4L\gamma^2}{n}\sigma\left(\sqrt{64L\Delta} +3(B-\tau+b)+3\hat{\beta}a\right)\\
    \le& (\sqrt{2}+\sqrt{2}b_1)\sqrt{(K+1)n} \cdot \frac{\beta^2\sigma}{9Ln}\left(8\sqrt{L\Delta} +3(B-\tau)+3b + 3\sqrt{L\Delta}\right)\\
    \le&\frac{\Delta}{8}
    \end{align*}
    because we choose $\beta$ such that
    \begin{align}\label{eq:stepsize_bound_8}
    \beta &\le \left(\frac{9L\Delta\sqrt{n}}{\sqrt{2}(1+b_1)\sigma\sqrt{T}\left(11\sqrt{L\Delta} +3(B-\tau+b) \right)}\right)^{1/2}
    &\quad \text{and} \quad K+1\le T.
    \end{align}
    This implies 
    \begin{align*}
    &\Pr\left(4\gamma(1-\beta)\left\|\sum_{t=0}^K\sum_{i=1}^n\<\zeta_{5,i}^t,\theta^{t+1}>\right\| \ge \frac{\Delta}{8}\right) \le \frac{\alpha}{14(T+1)}.
    \end{align*}
    Note that the worst dependency w.r.t $T$ is $\wtilde{\cO}(\nicefrac{1}{T^{1/4}}).$

    \paragraph{Final probability.}
    Therefore, the probability event 
    \[
    \Omega \eqdef E^K 
    \cap \overline{\Theta}^{K+1}
    \cap \left(\cap_{i=1}^n\overline{\Theta}^{K+1}_i\right)
    \cap \overline{N}^{K+1}
    \cap E_{\circledOne}
    \cap E_{\circledTwo}
    \cap E_{\circledThree}
    \cap E_{\circledFour}
    \cap E_{\circledFive}
    \cap E_{\circledSix}
    \cap E_{\circledSeven}
    \cap E_{\circledEight},
    \]
    where each $E_{\circledOne}$-$E_{\circledEight}$ denotes that each of $1$-$8$-th terms is smaller than $\frac{\Delta}{8}$, implies that 
    \[
    \circledOne + \circledTwo + \circledThree + \circledFour + \circledFive + \circledSix + \circledSeven + \circledEight \le 8\cdot \frac{\Delta}{8} = \Delta,
    \]
    i.e., condition $7$ in the induction assumption holds. Moreover, this also implies that 
    \[
    \Phi^{K+1} \le \Phi^0 + \Delta \le \Delta + \Delta = 2\Delta,
    \]
    i.e., condition $6$ in the induction assumption holds. The probability $\Pr(E_{K+1})$ can be lower bounded as follows 
    \begin{align*}
    \Pr(E_{K+1}) &\ge \Pr(\Omega)\\
    &= \Pr\left(E_K 
    \cap \overline{\Theta}^{K+1}
    \cap \left(\cap_{i=1}^n\overline{\Theta}^{K+1}_i\right)
    \cap \overline{N}^{K+1}
    \cap E_{\circledOne}
    \cap E_{\circledTwo}
    \cap E_{\circledThree}
    \cap E_{\circledFour}
    \cap E_{\circledFive}
    \cap E_{\circledSix}\right.\\
    &\qquad \left.
    \cap E_{\circledSeven}
    \cap E_{\circledEight}\right)\\
    &= 1 - \Pr\left(\overline{E}_K\cup
    \Theta^{K+1} \cup 
    \left(\cup_{i=1}^n\Theta^{K+1}_i\right)
    \cup N^{K+1}
    \cup \overline{E}_{\circledOne}
    \cup \overline{E}_{\circledTwo}
    \cup \overline{E}_{\circledThree}
    \cup \overline{E}_{\circledFour}
    \cup \overline{E}_{\circledFive}
    \cup \overline{E}_{\circledSix}\right.\\
    &\qquad \left. \cup \overline{E}_{\circledSeven}
    \cup \overline{E}_{\circledEight}\right)\\
    &\ge 1 - \Pr(\overline{E}_K) 
    - \Pr(\Theta^{K+1})
    - \sum_{i=1}^n\Pr(\Theta^{K+1}_i)
    -\Pr(N^{K+1})
    - \Pr(\overline{E}_{\circledOne})
    - \Pr(\overline{E}_{\circledTwo})\\
    & \qquad 
    - \Pr(\overline{E}_{\circledThree})
    - \Pr(\overline{E}_{\circledFour})
    - \Pr(\overline{E}_{\circledFive})
    - \Pr(\overline{E}_{\circledSix})
    - \Pr(\overline{E}_{\circledSeven})
    - \Pr(\overline{E}_{\circledEight})\\
    &\ge 1 - \frac{\alpha(K+1)}{T+1}
    - \frac{\alpha}{6(T+1)}
    - \sum_{i=1}^n \frac{\alpha}{6n(T+1)}
    - \frac{\alpha}{6(T+1)}
    - 0- 7\cdot \frac{\alpha}{14(T+1)}\\
    &= 1-\frac{\alpha(K+2)}{T+1}.
    \end{align*}
    This finalizes the transition step of induction. The result of the theorem follows by setting $K=T-1$. Indeed, from (\ref{eq:njqnsfqknj}) we obtain
    \begin{align}\label{eq:mfweodjnwej}
        \frac{\gamma}{2}\sum_{t=0}^K\|\nabla f(x^t)\|^2 
        \le \Phi^0 
        - \Phi^{K+1} 
        + \Delta 
        \le 2\Delta 
        \Rightarrow \frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2 
        \le \frac{4\Delta}{\gamma T}.
    \end{align}

    \paragraph{Final rate.}

    Translating momentum restrictions (\ref{eq:stepsize_bound_1}), (\ref{eq:stepsize_bound_2}), (\ref{eq:stepsize_bound_3}), (\ref{eq:stepsize_bound_4}), (\ref{eq:stepsize_bound_5}), (\ref{eq:stepsize_bound_6}), (\ref{eq:stepsize_bound_7}), and (\ref{eq:stepsize_bound_8}) to the stepsize restriction using $6L\gamma= \beta$ equality we get that the stepsize should satisfy 
    \begin{align}\label{eq:ninwofejnasdadwe}
        \gamma \le &\frac{1}{L}\wtilde{\cO}\left(\min\left\{
         \underbrace{\left(\frac{L\Delta n}{T\sigma^2}\right)^{1/2}, \left(\frac{L\Delta\hat{\beta}^2\eta^2}{T\sigma^2}\right)^{1/4} }_{\text{from term } 1}, 
         \underbrace{\left(\frac{L\Delta\sqrt{n}\hat{\beta}\eta}{B\sigma \sqrt{T}}\right)^{1/2} }_{\text{from term } 2},
         \underbrace{\left(\frac{L\Delta \sqrt{n}\hat{\beta}\eta}{\sigma(\sqrt{L\Delta} + B + \sigma)\sqrt{T} }\right)^{\frac{1}{3}} }_{\text{from term } 3},\right.\right.\notag\\
         &\left.\left.\underbrace{\left(\frac{L\Delta\hat{\beta}\eta \sqrt{n}}{\sigma(\sqrt{L\Delta} + B + \sigma)\sqrt{T}}\right)^{1/4}}_{\text{from term } 4},
        \underbrace{\left(\frac{L\Delta\hat{\beta}^2\eta^2\sqrt{n}}{\sigma(\sqrt{L\Delta} + B + \sigma) \sqrt{T}}\right)^{1/3}}_{\text{from term } 5},
        \underbrace{\left(\frac{L\Delta\hat{\beta}^2\eta^2\sqrt{n}}{\sigma(\sqrt{L\Delta} + B + \sigma)\sqrt{T} }\right)^{1/4}}_{\text{from term } 7},\right.\right.\notag\\
        &\left.\left.
        \underbrace{\left(\frac{L\Delta \sqrt{n}}{\sigma(\sqrt{L\Delta} + B +\sigma) \sqrt{T}}\right)}_{\text{from term } 6},
        \underbrace{\left(\frac{L\Delta \sqrt{n}}{\sigma (\sqrt{L\Delta} + B + \sigma)\sqrt{T}}\right)^{\frac{1}{2}}}_{\text{from term } 8}\right\}
        \right).
    \end{align} 
    The worst power of $T$ comes from the term $\circledFive$ and equals $\frac{1}{T^{5/6}}.$ The second worst comes from terms $\circledOne$, $\circledTwo$, and $\circledFour$, and equals to $\gamma \le \frac{1}{T^{3/4}}$ in the case $\hat{\beta} \sim \frac{1}{T}$. These terms give the rate of the form
    \begin{align}\label{eq:ojnflqkmqowkdqw}
        & \wtilde{\cO}\left(
        \frac{L\Delta}{T}\left(\frac{T\sigma^2}{L\Delta \hat{\beta}^2\eta^2}\right)^{1/4} 
        + \frac{L\Delta}{T}\left(\frac{\sigma(\sqrt{L\Delta} + B +\sigma)\sqrt{T}}{L\Delta\hat{\beta} \eta\sqrt{n}}\right)^{1/3}
        + \frac{L\Delta}{T}\left(\frac{\sigma(\sqrt{L\Delta} + B+\sigma)\sqrt{T}}{L\Delta\hat{\beta}^2\eta^2\sqrt{n}}\right)^{1/3}
        \right.\notag\\
        &\qquad \left. +\;\frac{L\Delta}{T}\left(\frac{B\sigma\sqrt{T}}{L\Delta\sqrt{n}\hat{\beta}\eta}\right)^{1/2}\right).
    \end{align}
    In the case, when $\hat{\beta}=1$ the worst dependency in \eqref{eq:ninwofejnasdadwe} w.r.t. $T$ comes from the terms $\circledOne$ and $\circledSix$. We also have restriction $\gamma \le \cO(\nicefrac{1}{L}).$ All of those restrictions give the rate of the form
    \begin{align}\label{eq:iqoendfqowjdnwq}
        &\frac{L\Delta}{T}\wtilde{\cO}\left(1
        + \frac{T^{1/2}\sigma}{L^{1/2}\Delta^{1/2}n^{1/2}}
        + \frac{\sigma(\sqrt{L\Delta} + B + \sigma)\sqrt{T}}{L\Delta\sqrt{n}}\right)\notag\\
        =\;& \wtilde{\cO}\left(\frac{L\Delta}{T} 
        + \frac{\sqrt{L\Delta}\sigma}{\sqrt{nT}}
        + \frac{\sigma(\sqrt{L\Delta} + B + \sigma)}{\sqrt{n T}}\right)\notag\\
        =\;& \wtilde{\cO}\left(\frac{L\Delta}{T} 
        + \frac{\sigma(\sqrt{L\Delta} + B + \sigma)}{\sqrt{n T}}\right).
    \end{align}
    Choosing $\hat{\beta} = \sqrt{L\Delta}/a$ in \eqref{eq:ojnflqkmqowkdqw}, where $a$ is defined in \eqref{eq:constants_a_b_c}, and setting $\eta=\frac{\tau}{B}$ we get 
    \begin{align*}
        & \frac{L\Delta}{T}\cdot \wtilde{\cO}\left(
        \left(\frac{T\sigma^2B^2a^2}{L^2\Delta^2\tau^2}\right)^{1/4} 
        + \left(\frac{\sigma aB(\sqrt{L\Delta} + B +\sigma)\sqrt{T}}{L^{3/2}\Delta^{3/2} \tau\sqrt{n}}\right)^{1/3}
        + \left(\frac{\sigma a^2(\sqrt{L\Delta} + B+\sigma)B^2\sqrt{T}}{L^2\Delta^2\tau^2\sqrt{n}}\right)^{1/3}
        \right.\\
        &\qquad \left. +\;\left(\frac{a B^2\sigma\sqrt{T}}{L^{3/2}\Delta^{3/2}\sqrt{n}\tau}\right)^{1/2}\right)\\
        &=  \frac{L\Delta}{T}\cdot \wtilde{\cO}\left(
        \left(\frac{T\sigma^2B^2a^2}{L^2\Delta^2\tau^2}\right)^{1/4} 
        + \left(\frac{\sigma aB\sqrt{T}}{L\Delta \tau\sqrt{n}}\right)^{1/3}
        + \left(\frac{\sigma aB^2\sqrt{T}}{L^{3/2}\Delta^{3/2} \tau\sqrt{n}}\right)^{1/3}
        + \left(\frac{\sigma^2 aB\sqrt{T}}{L^{3/2}\Delta^{3/2} \tau\sqrt{n}}\right)^{1/3}
        \right.\\
        &\qquad  +\;
         \left(\frac{\sigma a^2B^2\sqrt{T}}{L^{3/2}\Delta^{3/2}\tau^2\sqrt{n}}\right)^{1/3}
         +  \left(\frac{\sigma a^2B^3\sqrt{T}}{L^2\Delta^2\tau^2\sqrt{n}}\right)^{1/3}
         +  \left(\frac{\sigma^2 a^2B^2\sqrt{T}}{L^2\Delta^2\tau^2\sqrt{n}}\right)^{1/3}\\
         &\qquad \left.
         +\; \left(\frac{a B^2\sigma\sqrt{T}}{L^{3/2}\Delta^{3/2}\sqrt{n}\tau}\right)^{1/2}\right).
    \end{align*}
    Now we use the exact value for $a$ to derive
    \begin{align}
        &  \wtilde{\cO}\left(
        \left(\frac{L^4\Delta^4T\sigma^2B^2d\sigma_{\omega}^2\frac{T}{n}}{T^4L^2\Delta^2\tau^2}\right)^{1/4} 
        + \left(\frac{L^3\Delta^3\sigma d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}B\sqrt{T}}{T^3L\Delta \tau\sqrt{n}}\right)^{1/3}
        + \left(\frac{L^3\Delta^3\sigma d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}} B^2\sqrt{T}}{T^3L^{3/2}\Delta^{3/2} \tau\sqrt{n}}\right)^{1/3}
        \right.\notag\\
        &\qquad  +\;
        \left(\frac{L^3\Delta^3\sigma^2 d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}B\sqrt{T}}{T^3L^{3/2}\Delta^{3/2} \tau\sqrt{n}}\right)^{1/3}
        + \left(\frac{L^3\Delta^3\sigma d\sigma_{\omega}^2\frac{T}{n} B^2\sqrt{T}}{T^3L^{3/2}\Delta^{3/2}\tau^2\sqrt{n}}\right)^{1/3}
         +  \left(\frac{L^3\Delta^3\sigma d\sigma_{\omega}^2 \frac{T}{n}B^3\sqrt{T}}{T^3L^2\Delta^2\tau^2\sqrt{n}}\right)^{1/3}\notag\\
         &\qquad \left.
         +\; \left(\frac{L^3\Delta^3\sigma^2 d\sigma_{\omega}^2\frac{T}{n} B^2\sqrt{T}}{T^3L^2\Delta^2\tau^2\sqrt{n}}\right)^{1/3}
         + \left(\frac{L^2\Delta^2d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}} B^2\sigma\sqrt{T}}{T^2L^{3/2}\Delta^{3/2}\sqrt{n}\tau}\right)^{1/2}\right)\notag\\
         &= \wtilde{\cO}\left(
        \left(\frac{L^2\Delta^2\sigma^2B^2d\sigma_{\omega}^2}{T^2n\tau^2}\right)^{1/4} 
        + \left(\frac{L^2\Delta^2\sigma d^{1/2}\sigma_{\omega}B}{nT^2 \tau}\right)^{1/3}
        + \left(\frac{L^{3/2}\Delta^{3/2}\sigma d^{1/2}\sigma_{\omega} B^2}{ nT^2\tau}\right)^{1/3}
        \right.\notag\\
        &\qquad  +\;
        \left(\frac{L^{3/2}\Delta^{3/2}\sigma^2 d^{1/2}\sigma_{\omega}B}{nT^2 \tau}\right)^{1/3}
        + \left(\frac{L^{3/2}\Delta^{3/2}\sigma d\sigma_{\omega}^2B^2}{T^{3/2}n^{3/2}\tau^2}\right)^{1/3}
         + \left(\frac{L\Delta\sigma d\sigma_{\omega}^2 B^3}{n^{3/2}T^{3/2}\tau^2}\right)^{1/3}\notag\\
         &\qquad \left.
         +\; \left(\frac{L\Delta\sigma^2 d\sigma_{\omega}^2 B^2}{T^{3/2}n^{3/2}\tau^2}\right)^{1/3}
         + \left(\frac{L^{1/2}\Delta^{1/2}d^{1/2}\sigma_{\omega} B^2\sigma}{Tn\tau}\right)^{1/2}\right).
    \end{align}
    As we can see, the worst dependency on $T$ and $\sigma_{\omega}$ comes from terms $5-7$. Therefore, we omit the rest of the terms. Hence, the worst term w.r.t. $T$ in the presence of DP noise gives the rate 
    \begin{align}\label{eq:igornwjnwe}
        &\wtilde{\cO}\left(
        \left(\frac{L^{3/2}\Delta^{3/2}\sigma d\sigma_{\omega}^2B^2}{T^{3/2}n^{3/2}\tau^2}\right)^{1/3}
         + \left(\frac{L\Delta\sigma d\sigma_{\omega}^2 B^3}{n^{3/2}T^{3/2}\tau^2}\right)^{1/3}
         + \left(\frac{L\Delta\sigma^2 d\sigma_{\omega}^2 B^2}{T^{3/2}n^{3/2}\tau^2}\right)^{1/3}\right)\notag\\
         &= \wtilde{\cO}\left(
        \frac{L^{1/2}\Delta^{1/2}\sigma^{1/3} d^{1/3}\sigma_{\omega}^{2/3}B^{2/3}}{T^{1/2}n^{1/2}\tau^{2/3}}  
         +\frac{L^{1/3}\Delta^{1/3}\sigma^{1/3} d^{1/3}\sigma_{\omega}^{2/3} B}{n^{1/2}T^{1/2}\tau^{2/3}}
         + \frac{L^{1/3}\Delta^{1/3}\sigma^{2/3} d^{1/3}\sigma_{\omega}^{2/3} B^{2/3}}{T^{3/2}n^{3/2}\tau^2}\right)\notag\\
         &= \wtilde{\cO}\left(
        \frac{L^{1/3}\Delta^{1/3}\sigma^{1/3} d^{1/3}\sigma_{\omega}^{2/3}B^{2/3}}{T^{1/2}n^{1/2}\tau^{2/3}} \left((L\Delta)^{1/6}
        + B^{1/3}
        + \sigma^{1/3}\right)
        \right)\notag\\
        &= \wtilde{\cO}\left(\left(
        \frac{L\Delta\sigma d\sigma_{\omega}^{2}B^{2}}{(nT)^{3/2}\tau^{2}} \left(\sqrt{L\Delta}
        + B
        + \sigma\right)\right)^{1/3}
        \right).
    \end{align}    
    Besides, the momentum restrictions $(ii$-$iv)$ and $6L\gamma = \beta$ give us the following restrictions on the stepsize
    \begin{align*}
        \gamma \le \frac{1}{L}\wtilde{\cO}\left(\min\left\{\frac{\tau}{a}, \frac{\tau\sqrt{L\Delta}}{Ba}, \frac{\sqrt{L\Delta}\tau}{\sigma a}\right\}\right)
    \end{align*}
    that translate to the following rate 
    \begin{align}\label{eq:asdvbrw}
        &\frac{L\Delta}{T}\wtilde{\cO}\left(\frac{a}{\tau} + \frac{Ba}{\tau\sqrt{L\Delta}} + \frac{\sigma a}{\tau\sqrt{L\Delta}}\right)\notag\\
       =\; &\wtilde{\cO}\left(
        \frac{L\Delta}{T}\frac{d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}}{\tau}
        + \frac{L\Delta}{T}\frac{Bd^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}}{\tau\sqrt{L\Delta}}
        + \frac{L\Delta}{T}\frac{\sigma d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}}{\tau\sqrt{L\Delta}}
        \right)\notag\\
        =\;& \wtilde{\cO}\left(
        \frac{L\Delta}{T}\frac{d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}}{\tau}
        + \frac{L\Delta}{T}\frac{Bd^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}}{\tau\sqrt{L\Delta}}
        + \frac{L\Delta}{T}\frac{\sigma d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}}{\tau\sqrt{L\Delta}}
        \right)\notag\\
        =\;& \wtilde{\cO}\left(\frac{\sqrt{L\Delta d}\sigma_{\omega}}{\tau\sqrt{nT}}\left(\sqrt{L\Delta}+ B + \sigma\right)
        \right).
    \end{align}

    The restriction in \eqref{eq:quadratic_stepsize_restriction} translates to 
    $$
    \gamma \le \wtilde{\cO}\left(\min\left\{\frac{\hat{\beta}\eta}{L}, \frac{\sqrt{\hat{\beta}\eta}}{L}\right\}\right),$$
    that translates to the following rate of convergence
    \begin{align}\label{eq:nrowlfqewfq}
        &\frac{L\Delta}{T}\wtilde{\cO}\left(
        \frac{B d^{1/2}\sigma_{\omega}\frac{T^{1/2}}{n^{1/2}}}{\tau \sqrt{L\Delta}}
        + \frac{B^{1/2}d^{1/4}\sigma_{\omega}^{1/2}\frac{T^{1/4}}{n^{1/4}}}{\tau^{1/2}}
        \right)\notag\\
        =\;& \wtilde{\cO}\left(
        \frac{\sqrt{L\Delta}B d^{1/2}\sigma_{\omega}}{\sqrt{Tn}\tau }
        + \frac{L^{3/4}\Delta^{3/4}B^{1/2}d^{1/4}\sigma_{\omega}^{1/2}}{T^{3/4}n^{1/4}\tau^{1/2}}
        \right).
    \end{align}
    Combining \eqref{eq:igornwjnwe}, \eqref{eq:asdvbrw}, \eqref{eq:nrowlfqewfq} we derive the final bound
    \begin{align}\label{eq:gn3owjnefoe}
        \wtilde{\cO}\left(\left(
        \frac{L\Delta\sigma d\sigma_{\omega}^{2}B^{2}}{(nT)^{3/2}\tau^{2}} \left(\sqrt{L\Delta}
        + B
        + \sigma\right)\right)^{1/3}
        +
        \frac{\sqrt{L\Delta d}\sigma_{\omega}}{\tau\sqrt{nT}}\left(\sqrt{L\Delta}+ B + \sigma\right)
        \right),
    \end{align}
    where we hide the terms that decrease faster in $T$ than the two in \eqref{eq:gn3owjnefoe}.
   

    \subparagraph{Case $\cI_{K+1} = 0.$} This case is even easier. The only change will be with the term next to $R^t$. We will get 
    \[
    1 - \frac{96L^2}{\hat\beta^2\eta^2}\gamma^2 
        - \frac{24L^2}{\beta^2}\gamma^2 \ge \frac{1}{3} - \frac{96L^2}{\hat\beta^2\eta^2}\gamma^2 \ge 0
    \]
    instead of 
    \[
    1 - \frac{32\beta^2L^2}{\hat\beta^2\eta^2}\gamma^2 
    - \frac{96L^2}{\hat\beta^2\eta^2}\gamma^2 
    - \frac{24L^2}{\beta^2}\gamma^2 \ge 0
    \]
     as in the previous case. This difference comes from \Cref{lem:descent_Vt_tilde_dp} because $\wtilde{V}^{K+1} = 0$. The rest is a repetition of the previous derivations.
    
     \end{proof}


\section{Proof of \Cref{cor:dp_privacy_utility_tradeoff}}\label{appendix:corollary_1}



\theoremdputility*

\begin{proof}
    We need to plug in the value of $\sigma_{\omega}$ inside (\ref{eq:theorem_stochastic_and_dp}). Indeed, we have that 
    \begin{align*}
        & \wtilde{\cO}\left(\frac{\sqrt{L\Delta d} \sqrt{T}\frac{\tau}{\varepsilon}}{\sqrt{n T}\tau}(\sqrt{L\Delta} + B + \sigma)
        + \left(\frac{L\Delta\sigma B^2 \frac{\tau^2}{\varepsilon^2}T}{(nT)^{3/2}\tau^2}(\sqrt{L\Delta} + B + \sigma)\right)^{1/3}
        \right)\\
        =\;& \wtilde{\cO}\left(\frac{\sqrt{L\Delta d} }{\sqrt{n}\varepsilon}(\sqrt{L\Delta} + B + \sigma)
        + \left(\frac{L\Delta\sigma B^2}{n^{3/2}T^{1/2}\varepsilon^2}(\sqrt{L\Delta} + B + \sigma)\right)^{1/3}
        \right)
    \end{align*}
    Leaving only the terms that do not improve with $T$ we get the result, i.e., the utility bound.

    It remains to formally show that for chosen $\sigma_{\omega}$, \algname{Clip21-SGD2M} satisfies local $(\varepsilon, \delta)$-DP. First, we notice that for $\sigma_{\omega} = \frac{8\tau}{\varepsilon}\sqrt{T\log\left(\frac{5T}{4\delta}\right)\log\left(\frac{1}{\delta}\right)}$ each step of \algname{Clip21-SGD2M} satisfies $(\tilde{\varepsilon}, \tilde{\delta})$-DP \citep[Theorem 3.22]{dwork2014algorithmic} with
    \begin{equation*}
        \tilde{\varepsilon} = \frac{\varepsilon}{2\sqrt{2T\log(\frac{1}{\delta})}} \quad \text{and}\quad \tilde{\delta} = \frac{\delta}{T}.
    \end{equation*}
    Then, applying advanced composition theorem \citep[Theorem 3.20 and Corollary 3.21 with $\delta' = \delta$]{dwork2014algorithmic}, we get that $T$ steps of \algname{Clip21-SGD2M} satisfy $(\varepsilon, \delta)$-DP, which concludes the proof.
\end{proof}


\section{Proof of \Cref{th:theorem_stochastic}}\label{appendix:stoch_proof_no_DP}


We highlight that the proof of \Cref{th:theorem_stochastic} mainly follows that of \Cref{th:theorem_stochastic_and_dp}. The main difference comes from the fact that stepsize and momentum restrictions become less demanding as in a purely stochastic setting (without DP noise) $a=0$. This, in particularly, means that the restriction $\hat{\beta} \le \frac{\sqrt{L\Delta}}{a}$ disappears and we can set $\hat{\beta}=1.$


\begin{theorem}[Full statement of \Cref{th:theorem_stochastic}]%\label{th:theorem_dp}
    Let Assumptions \ref{asmp:smoothness} and \ref{asmp:batch_noise} hold, $$B\eqdef\max\{3\tau, \max_i\{\|\nabla f_i(x^0)\|\}+b\} > \tau,$$ probability confidence level $\alpha\in(0,1)$, constants $b$ and $c$ be defined as in \eqref{eq:constants_a_b_c}, and $\Delta \ge \Phi^0$ for $\Phi^0$ defined in \eqref{eq:lyapunov_function}. Let us run \Cref{alg:Clip-SGDM} for $T$ iterations with DP noise variance $\sigma_{\omega} = 0$.
    Assume the following inequalities hold
    
    \begin{enumerate}
        \item {\bf stepsize restrictions:} 
        \begin{enumerate}[$i)$]
        \item $12L\gamma \le 1;$
        \item \[
        \frac{1}{3}
        - \frac{32\beta^2L^2}{\eta^2}\gamma^2 
        - \frac{96L^2}{\eta^2}\gamma^2 \ge 0;
        \]
        \end{enumerate}
        \item {\bf momentum restrictions:} 
        \begin{enumerate}[$i)$]
            \item $6L\gamma=\beta$;
        \item $\beta \le \frac{3\tau}{64\sqrt{L\Delta}}$;
        \item $\beta \le \frac{\tau}{14(B-\tau)};$
        \item $\beta \le \frac{\tau}{22b}$;
        \item and momentum restrictions defined in (\ref{eq:stepsize_bound_1}), (\ref{eq:stepsize_bound_2}), (\ref{eq:stepsize_bound_3}), (\ref{eq:stepsize_bound_4}), (\ref{eq:stepsize_bound_5}), (\ref{eq:stepsize_bound_6}), (\ref{eq:stepsize_bound_7}), and (\ref{eq:stepsize_bound_8}), where $\hat{\beta} =1$.
        \end{enumerate}
    \end{enumerate}
    Then with probability $1-\alpha$ we have 
   \begin{align*}
        \frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(x^t)\|^2 \le \wtilde{\cO}\left(\frac{\sigma(\sqrt{L\Delta} + B+\sigma)}{\sqrt{Tn}}\right),
    \end{align*}
    where $\wtilde{\cO}$ hides constant and logarithmic factors, and higher order terms decrease in $T$.
\end{theorem}

\begin{proof}
    The proof mainly follows that of \Cref{th:theorem_stochastic_and_dp}. Since in this case, we can set $\hat{\beta} =1$ and $a=0$ the worst stepsize restrictions that we have in this case lead to the rate \eqref{eq:iqoendfqowjdnwq} which concludes the proof.
    
\end{proof}


\section{Experiments: Additional Details and Results}\label{sec:appendix_exp}



\subsection{Experiments with Logistic Regression}\label{sec:stoch_setting_appendix}



We conduct experiments on non-convex logistic regression with regularization parameter $\lambda=10^{-3}$ for $10^4$ iterations which is a standard experiment setup considered in the earlier works \citep{gao2024econtrol, islamov2024asgrad, makarenko2022adaptive}. We use Duke and Leukemia datasets from LibSVM library and split the dataset into $n=4$ equal parts. We normalize the row of the feature matrix to demonstrate the differences between algorithms. To simulate the stochastic gradients we either add centered Gaussian noise with variance $\sigma = 0.05$ for the Duke dataset and $\sigma=0.1$ for the Leukemia dataset, or mini-batch gradients with batch-size of $\frac{1}{3}$ of the whole local dataset for Duke dataset and $\frac{1}{4}$ of the whole local dataset for Leukemia dataset. For \algname{Clip21-SGD} and \algname{Clip-SGD} algorithms, we tune the stepsize in $\{2^{-5},\dots,2^5\}$ and choose the one that gives the lowest final gradient norm in average across $3$ random seeds. For \algname{Clip21-SGD2M}, we tune both the stepsize in $\{2^{-5}, \dots, 2^5\}$ and the momentum parameter in $\{0.1, 0.5, 0.9\}$ and choose the best pair of parameters similarly as before. For completeness, we report the convergence curves in \Cref{fig:logreg_convergence_plots}. We observe that \algname{Clip21-SGD2M} is more robust to the choice of the clipping radius $\tau$ while \algname{Clip-SGD} converges well only for large enough $\tau$. Besides, \algname{Clip21-SGD} does not converge in all cases which is also highlighted by our theory in \Cref{th:clip21_non_convergence}.

\begin{figure}[!t]
    \centering
    \begin{tabular}{cccc}
        \includegraphics[width=0.22\linewidth]{Figures/gaussian_comparison_logscale_train_loss_tau_0.1_duke.bz2_logreg_None_0.05_None_10000.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/gaussian_comparison_logscale_train_loss_tau_0.01_duke.bz2_logreg_None_0.05_None_10000.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/gaussian_comparison_logscale_train_loss_tau_0.001_duke.bz2_logreg_None_0.05_None_10000.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/gaussian_comparison_logscale_train_loss_tau_0.0001_duke.bz2_logreg_None_0.05_None_10000.pdf}\\
        {\tiny Gaussian, $\tau=10^{-1}$ }&
        {\tiny Gaussian, $\tau=10^{-2}$ }&
        {\tiny Gaussian, $\tau=10^{-3}$ }&
        {\tiny Gaussian, $\tau=10^{-4}$ } \\
        \includegraphics[width=0.22\linewidth]{Figures/mini-batch_comparison_logscale_train_loss_tau_0.1_duke.bz2_logreg_3_None_None_10000.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/mini-batch_comparison_logscale_train_loss_tau_0.01_duke.bz2_logreg_3_None_None_10000.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/mini-batch_comparison_logscale_train_loss_tau_0.001_duke.bz2_logreg_3_None_None_10000.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/mini-batch_comparison_logscale_train_loss_tau_0.0001_duke.bz2_logreg_3_None_None_10000.pdf}\\
        {\tiny Mini-batch, $\tau=10^{-1}$ }&
        {\tiny Mini-batch, $\tau=10^{-2}$ }&
        {\tiny Mini-batch, $\tau=10^{-3}$ }&
        {\tiny Mini-batch, $\tau=10^{-4}$ } \\
        \includegraphics[width=0.22\linewidth]{Figures/gaussian_comparison_logscale_train_loss_tau_0.1_leu.t.bz2_logreg_None_0.05_None_10000.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/gaussian_comparison_logscale_train_loss_tau_0.01_leu.t.bz2_logreg_None_0.05_None_10000.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/gaussian_comparison_logscale_train_loss_tau_0.001_leu.t.bz2_logreg_None_0.05_None_10000.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/gaussian_comparison_logscale_train_loss_tau_0.0001_leu.t.bz2_logreg_None_0.05_None_10000.pdf}\\
        {\tiny Gaussian, $\tau=10^{-1}$ }&
        {\tiny Gaussian, $\tau=10^{-2}$ }&
        {\tiny Gaussian, $\tau=10^{-3}$ }&
        {\tiny Gaussian, $\tau=10^{-4}$ } \\
        \includegraphics[width=0.22\linewidth]{Figures/mini-batch_comparison_logscale_train_loss_tau_0.1_leu.t.bz2_logreg_4_None_None_10000.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/mini-batch_comparison_logscale_train_loss_tau_0.01_leu.t.bz2_logreg_4_None_None_10000.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/mini-batch_comparison_logscale_train_loss_tau_0.001_leu.t.bz2_logreg_4_None_None_10000.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/mini-batch_comparison_logscale_train_loss_tau_0.0001_leu.t.bz2_logreg_4_None_None_10000.pdf}\\
        {\tiny Mini-batch, $\tau=10^{-1}$ }&
        {\tiny Mini-batch, $\tau=10^{-2}$ }&
        {\tiny Mini-batch, $\tau=10^{-3}$ }&
        {\tiny Mini-batch, $\tau=10^{-4}$ } \\
    \end{tabular}
    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} ($\hat{\beta}=1$) on logistic regression with non-convex regularization for various the clipping radii $\tau$ with mini-batch and Gaussian-added stochastic gradients on Duke ({\bf two first rows}) and Leukemia ({\bf two last rows}).}
    \label{fig:logreg_convergence_plots}
\end{figure}






\subsection{Experiments with Neural Networks}

\subsubsection{Varying Clipping Radius $\tau$}\label{sec:appendix_exp_nn_stoch}

Now we switch to the training of Resnet20 and VGG16 models on CIFAR10 dataset. For all algorithms, we do not use any techniques such as learning rate schedule, warm-up, or weight decay. However, we do tuning of the learning rate for \algname{Clip-SGD} and \algname{Clip21-SGD} from $\{10^{-3},10^{-2},10^{-1},10^0\}$ and choose the one that gives the highest test accuracy. For \algname{Clip21-SGD2M} we tune both the learning rate from $\gamma\in\{10^{-3},10^{-2},10^{-1},10^0\}$ and the momentum parameter from $\beta\in\{0.1, 0.5, 0.9\}$ while setting $\hat{\beta}=1$ and choose the pair of $(\gamma,\beta)$ that reaches the highest test accuracy. The batch size for all algorithms is set to $32.$ We compare the performance of algorithms in two cases: when the clipping is applied globally on the whole model and layer-wise.

We observe in \Cref{fig:resnet20_cifar10,fig:resnet20_cifar10_layerwise,fig:vgg16_cifar10,fig:vgg16_cifar10_layerwise} that the performance of \algname{Clip-SGD} gets worsen once the clipping radius is small enough. For \algname{Clip21-SGD2M} is more robust to the choice of $\tau$ and can achieve smaller train loss and test accuracy even when $\tau$ is small.

\begin{figure}[!t]
    \centering
    \begin{tabular}{cccc}
        \includegraphics[width=0.22\linewidth]{Figures/comparison_train_loss_vgg16_cifar10_0.1_None_0_32_None_150.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/comparison_train_loss_vgg16_cifar10_0.01_None_0_32_None_150.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_train_loss_vgg16_cifar10_0.001_None_0_32_None_300.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_train_loss_vgg16_cifar10_0.0001_None_0_32_None_450.pdf}\\
        \includegraphics[width=0.22\linewidth]{Figures/comparison_test_acc_vgg16_cifar10_0.1_None_0_32_None_150.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/comparison_test_acc_vgg16_cifar10_0.01_None_0_32_None_200.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_test_acc_vgg16_cifar10_0.001_None_0_32_None_300.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_test_acc_vgg16_cifar10_0.0001_None_0_32_None_450.pdf}\\
        $\tau = 10^{-1}$ &
        $\tau = 10^{-2}$ &
        $\tau = 10^{-3}$ &
        $\tau = 10^{-4}$
        
    \end{tabular}
    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} ($\hat{\beta} = 1$) on training VGG16 model on CIFAR10 dataset where the clipping is applied globally.}
    \label{fig:vgg16_cifar10}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{tabular}{cccc}
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_train_loss_vgg16_cifar10_0.1_None_0_32_None_150.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_train_loss_vgg16_cifar10_0.01_None_0_32_None_200.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_train_loss_vgg16_cifar10_0.001_None_0_32_None_450.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_train_loss_vgg16_cifar10_0.0001_None_0_32_None_450.pdf}\\
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_test_acc_resnet20_cifar10_0.1_None_0_32_None_150.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_test_acc_resnet20_cifar10_0.01_None_0_32_None_200.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_test_acc_resnet20_cifar10_0.001_None_0_32_None_450.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_test_acc_resnet20_cifar10_0.0001_None_0_32_None_450.pdf}\\
        $\tau = 10^{-1}$ &
        $\tau = 10^{-2}$ &
        $\tau = 10^{-3}$ &
        $\tau = 10^{-4}$
        
    \end{tabular}
    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} ($\hat{\beta}=1$) on training VGG16 model on CIFAR10 dataset the clipping is applied layer-wise.}
    \label{fig:vgg16_cifar10_layerwise}
\end{figure}


\begin{figure}[!t]
    \centering
    \begin{tabular}{cccc}
        \includegraphics[width=0.22\linewidth]{Figures/comparison_train_loss_resnet20_cifar10_0.1_None_0_32_None_150.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/comparison_train_loss_resnet20_cifar10_0.01_None_0_32_None_150.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_train_loss_resnet20_cifar10_0.001_None_0_32_None_150.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_train_loss_resnet20_cifar10_0.0001_None_0_32_None_150.pdf}\\
        \includegraphics[width=0.22\linewidth]{Figures/comparison_test_acc_resnet20_cifar10_0.1_None_0_32_None_150.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/comparison_test_acc_resnet20_cifar10_0.01_None_0_32_None_200.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_test_acc_resnet20_cifar10_0.001_None_0_32_None_450.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_test_acc_resnet20_cifar10_0.0001_None_0_32_None_450.pdf}\\
        $\tau = 10^{-1}$ &
        $\tau = 10^{-2}$ &
        $\tau = 10^{-3}$ &
        $\tau = 10^{-4}$
        
    \end{tabular}
    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} ($\hat{\beta}=1$) on training Resnet20 model on CIFAR10 dataset where the clipping is applied globally.}
    \label{fig:resnet20_cifar10}
\end{figure}


\begin{figure}[!t]
    \centering
    \begin{tabular}{cccc}
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_train_loss_resnet20_cifar10_0.1_None_0_32_None_150.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_train_loss_resnet20_cifar10_0.01_None_0_32_None_200.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_train_loss_resnet20_cifar10_0.001_None_0_32_None_450.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_train_loss_resnet20_cifar10_0.0001_None_0_32_None_450.pdf}\\
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_test_acc_resnet20_cifar10_0.1_None_0_32_None_150.pdf} & 
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_test_acc_resnet20_cifar10_0.01_None_0_32_None_200.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_test_acc_resnet20_cifar10_0.001_None_0_32_None_450.pdf} &
        \includegraphics[width=0.22\linewidth]{Figures/comparison_layerwise_test_acc_resnet20_cifar10_0.0001_None_0_32_None_450.pdf}\\
        $\tau = 10^{-1}$ &
        $\tau = 10^{-2}$ &
        $\tau = 10^{-3}$ &
        $\tau = 10^{-4}$
        
    \end{tabular}
    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} ($\hat{\beta}=1$) on training Resnet20 model on CIFAR10 dataset where the clipping is applied layer-wise.}
    \label{fig:resnet20_cifar10_layerwise}
\end{figure}


\subsubsection{Results with Additive DP Noise}\label{sec:appendix_exp_nn_dp}

We consider the training of MLP and CNN models on MNIST dataset varying noise-clipping ration.

We use MLP model with $1$ hidden layer of size $256$ and Tanh activation function. CNN model has $2$ convolution layers with $16$ convolutions each and kernel size $5$ with one max-pooling layer and Tanh activation function. We perform a grid search over the learning rate from $\gamma\in\{10^{-3}, \dots, 10^0\}$ and the clipping radius from $\tau\in\{10^{-4}, \dots, 10^{-1}\}.$ The aforementioned tuning is performed for each value of the noise-clipping ratio from $\{0.1, 0.3, 1.0, 3.0, 10.0\}.$ The momentum parameters are tuned over $\beta\in\{0.5, 0.1, 0.01\}$ and $\hat{\beta} \in \{0.01, 0.1, 0.5\}$. We highlight that we do not use the techniques such as a learning rate scheduler although it might improve the performance of algorithms. The batch size for all algorithms is set to $32.$

In \Cref{fig:conv_plots_cnn_dp_train_loss,fig:conv_plots_cnn_dp_test_acc,fig:conv_plots_mlp_dp_train_loss,fig:conv_plots_mlp_dp_test_acc} we demonstrate that \algname{Clip-SGD} and \algname{Clip21-SGD2M} always outperform \algname{Clip21-SGD}. \algname{Clip-SGD} achieves slightly better accuracy for small noise-clipping ratios $\{0.1, 0.3\}$, i.e. weak PD guarantees while \algname{Clip21-SGD2M} is better for high noise-clipping ratios, i.e. strong DP guarantees. 



\begin{figure}[!t]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_0.1_comparison_train_loss_cnn_fine_tune_mnist_True_0_32_None_100.pdf} & 
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_0.3_comparison_train_loss_cnn_fine_tune_mnist_True_0_32_None_100.pdf} &
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_1.0_comparison_train_loss_cnn_fine_tune_mnist_True_0_32_None_100.pdf} \\
        \hspace{5mm}{\small {\rm ratio} $0.1$} &
        \hspace{5mm}{\small {\rm ratio} $0.3$} &
        \hspace{5mm}{\small {\rm ratio} $1.0$} \\
    \end{tabular}
    \begin{tabular}{cc}
         \makecellnew{\includegraphics[width=0.25\linewidth]{Figures/ICML_DP_3.0_comparison_train_loss_cnn_fine_tune_mnist_True_0_32_None_100.pdf}  \\ \hspace{5mm} {\small {\rm ratio} $3.0$}}&  
         \makecellnew{\includegraphics[width=0.25\linewidth]{Figures/ICML_DP_10.0_comparison_train_loss_cnn_fine_tune_mnist_True_0_32_None_100.pdf}\\ \hspace{5mm} {\small {\rm ratio} $10.0$} } %\\
         %{\small {\rm ratio} $3.0$} &
        %{\small {\rm ratio} $10.0$} \\
    \end{tabular}

    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} on training CNN model on MNIST dataset varying the noise-clipping ratio.}
    \label{fig:conv_plots_cnn_dp_train_loss}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_0.1_comparison_test_acc_cnn_fine_tune_mnist_True_0_32_None_100.pdf} & 
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_0.3_comparison_test_acc_cnn_fine_tune_mnist_True_0_32_None_100.pdf} &
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_1.0_comparison_test_acc_cnn_fine_tune_mnist_True_0_32_None_100.pdf} \\
        \hspace{5mm}{\small {\rm ratio} $0.1$} &
        \hspace{5mm}{\small {\rm ratio} $0.3$} &
        \hspace{5mm}{\small {\rm ratio} $1.0$} \\
    \end{tabular}
    \begin{tabular}{cc}
         \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_3.0_comparison_test_acc_cnn_fine_tune_mnist_True_0_32_None_100.pdf} &  
         \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_10.0_comparison_test_acc_cnn_fine_tune_mnist_True_0_32_None_100.pdf} \\
         \hspace{5mm}{\small {\rm ratio} $3.0$} &
       \hspace{5mm} {\small {\rm ratio} $10.0$} \\
    \end{tabular}

    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} on training CNN model on MNIST dataset varying the noise-clipping ratio.}
    \label{fig:conv_plots_cnn_dp_test_acc}
\end{figure}


\begin{figure}[!t]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_0.1_comparison_train_loss_mlp_fine_tune_mnist_True_0_32_None_100.pdf} & 
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_0.3_comparison_train_loss_mlp_fine_tune_mnist_True_0_32_None_100.pdf} &
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_1.0_comparison_train_loss_mlp_fine_tune_mnist_True_0_32_None_100.pdf} \\
        {\small {\rm ratio} $0.1$} &
        {\small {\rm ratio} $0.3$} &
        {\small {\rm ratio} $1.0$} \\
    \end{tabular}
    \begin{tabular}{cc}
         \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_3.0_comparison_train_loss_mlp_fine_tune_mnist_True_0_32_None_100.pdf} &  
         \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_10.0_comparison_train_loss_mlp_fine_tune_mnist_True_0_32_None_100.pdf} \\
         {\small {\rm ratio} $3.0$} &
        {\small {\rm ratio} $10.0$} \\
    \end{tabular}

    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} on training MLP model on MNIST dataset varying the noise-clipping ratio.}
    \label{fig:conv_plots_mlp_dp_train_loss}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{tabular}{ccc}
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_0.1_comparison_test_acc_mlp_fine_tune_mnist_True_0_32_None_100.pdf} & 
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_0.3_comparison_test_acc_mlp_fine_tune_mnist_True_0_32_None_100.pdf} &
        \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_1.0_comparison_test_acc_mlp_fine_tune_mnist_True_0_32_None_100.pdf} \\
        \hspace{5mm}{\small {\rm ratio} $0.1$} &
        \hspace{5mm}{\small {\rm ratio} $0.3$} &
        \hspace{5mm}{\small {\rm ratio} $1.0$} \\
    \end{tabular}
    \begin{tabular}{cc}
         \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_3.0_comparison_test_acc_mlp_fine_tune_mnist_True_0_32_None_100.pdf} &  
         \includegraphics[width=0.25\linewidth]{Figures/ICML_DP_10.0_comparison_test_acc_mlp_fine_tune_mnist_True_0_32_None_100.pdf} \\
         \hspace{5mm}{\small {\rm ratio} $3.0$} &
        \hspace{5mm}{\small {\rm ratio} $10.0$} \\
    \end{tabular}

    
    \caption{Comparison of \algname{Clip-SGD}, \algname{Clip21-SGD}, and \algname{Clip21-SGD2M} on training MLP model on MNIST dataset varying the noise-clipping ratio.}
    \label{fig:conv_plots_mlp_dp_test_acc}
\end{figure}



\end{document}