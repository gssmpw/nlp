\section{Related Work}
\paragraph{Differential Privacy.} The most common approach to obtaining DP guarantees is to clip each client's update, i.e., by bounding their $\ell_2$ norm, and adding a calibrated amount of Gaussian noise to each update or the average. This is typically sufficient to obscure the influence of any single client ____. Commonly, two scenarios of the DP model are considered: \textit{the central model} and \textit{the local model.} In the first setting, central privacy, a trusted server collects updates and adds noise only before updating the server-side model. This ensures that client data remains private from external parties. In the second setting, local privacy, client data is protected even from the server by clipping and adding noise to updates locally before sending them to the server, ensuring privacy from both the server and other clients ____. The local privacy setting offers stronger privacy against untrusted servers but results in poorer learning performance due to the need for more noise to obscure individual updates ____. This can be improved by using a secure shuffler ____, which permutes updates, or a secure aggregator ____, which sums updates before sending them to the server. These methods anonymize updates and enhance privacy while maintaining reasonable learning performance, even without a fully trusted server. Finally, ____ show that when DP is required, one can also achieve compression of updates for free.

In this work, we adopt the local DP model by injecting Gaussian noise into each client's update. However, the average noise can also be viewed as noise added to the average update. Therefore, \algname{Clip21-SGD2M} is compatible with all the aforementioned techniques and can also be applied to the central DP model with a smaller amount of noise.





\paragraph{Distributed methods with clipping.} In  the single-node regime, \algname{Clip-SGD} has been analyzed under various assumptions by many authors ____. Of course, these results can be generalized to the multi-node case if clipping is applied to the aggregated (e.g. averaged) vector, although mini-batching requires a refined analysis when the noise is heavy-tailed____. However, to get DP, clipping has to be applied to the vectors communicated by clients to the server. In this regime, \algname{Clip-SGD} is not guaranteed to converge even without any stochastic noise in the gradients ____. There exist several approaches to bypass this limitation that can be split into two lines of work. The first one relies on explicit or implicit assumptions about bounded heterogeneity. More precisely, ____ analyze a version of \algname{Local-SGD}/\algname{FedAvg} ____ with gradient clipping for homogeneous data case assuming that the stochastic gradients have symmetric distribution around their mean and ____ consider \algname{Local-SGD} with clipping of the models and analyze its convergence under bounded heterogeneity assumption. Moreover, the boundedness of the stochastic gradient is another assumption used in the literature but it implies the boundedness of gradients' heterogeneity of clients as well. This assumption is used in numerous works, including: i) ____ in the analysis of a version of \algname{FedAvg} with clipping of model difference (also empirically studied by ____), ii) ____ who propose and analyze a version of \algname{SCAFFOLD} ____ with gradient clipping (\algname{DP-SCAFFOLD}), iii) ____ who propose and analyze a version of \algname{BEER} ____ with gradient clipping (\algname{PORTER}) under bounded gradient and/or bounded data heterogeneity assumption, and iv) ____ who study a version of \algname{Gossip-SGD} ____ with gradient clipping (\algname{DECOR}). Although most of the mentioned works have rigorous DP guarantees, the corresponding methods are not guaranteed to converge for arbitrary heterogeneous problems. 

The second line of work focuses on the clipping of shifted (stochastic) gradient. In particular, ____ proposed and analyzed \algname{Clip21-GD}, which is based on the application of \algname{EF21} ____ to the clipping operator, and ____ develop and analyze methods that apply clipping to the difference of stochastic gradients and learnable shift -- an idea that was initially proposed by ____ to handle data heterogeneity in the Distributed Learning with unbiased communication compression. However, the analysis from ____ is limited to the noiseless regime, i.e., full-batched gradients are computed on workers, and both of the mentioned works do not provide\footnote{The proof of the DP guarantee by ____ relies on the condition for some $C > 1$ and $\nu, \sigma_\omega \geq 0$ that implies $\min\{\nu^2, \sigma_\omega^2\} \geq C \max\{\nu^2, \sigma_\omega^2\}$. The latter one holds if and only if $\nu = \sigma_\omega = 0$, which means that no noise is added to the method since $\sigma_\omega^2$ is the variance of DP-noise.} DP guarantees. We also note that clipping of gradient differences is helpful in tolerating Byzantine attacks in the partial participation regime ____.



\paragraph{Error Feedback.} Error Feedback (\algname{EF}) ____ is a popular technique for incorporating communication compression into Distributed/Federated Learning. However, for non-convex smooth problems, the existing analysis of \algname{EF} is provided either for the single-node case or relies on restrictive assumptions such as boundedness of the gradient/compression error or boundedness of the data heterogeneity (gradient dissimilarity) ____. Moreover, the convergence bounds for \algname{EF} also depend on the data heterogeneity, which is not an artifact of the analysis as illustrated in the experiments on strongly convex problems ____. ____ address this limitation and propose a new version of Error Feedback called \algname{EF21}. However, the existing analysis of \algname{EF21-SGD} requires the usage of large batch sizes to achieve any predefined accuracy ____. It turns out that the large batch size requirement is unavoidable for \algname{EF21-SGD} to converge, but this issue can be fixed using momentum ____. Momentum is also helpful in the decentralized extensions of Error Feedback ____.