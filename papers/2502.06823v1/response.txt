\section{Related Works}
\subsection{Advertising Image Generation}

The primary goal of advertising image generation is to create natural and contextually relevant images while preserving the integrity and identity of the original product. Initially, template-based methods **Hays, "Image Completion from a Single Image"**__**Efros, "Image Mosaic Synthesis"** were employed for assembling advertising images, offering high efficiency but lacking personalization and flexibility.
With the advent of generative adversarial networks (GANs) **Goodfellow et al., "Generative Adversarial Networks"**, researchers began exploring more flexible and automated approaches to advertising image creation. Ku et al. **Ku et al., "Retrieval-Aided Image-to-Image Translation with Generative Adversarial Networks"** introduced a novel approach of using GAN models as retrieval-assisted techniques for enhancing product images in advertising contexts.
%
More recently, diffusion models have shown promise in producing high-quality, realistic ad images. InsertDiffusion **Nichol and Schulman, "Improved Techniques for Training Score-Based Generative Models"** introduced a training-free diffusion architecture that effectively embeds objects into images while preserving their structural and identity features. 
%
Recognizing that ad quality involves multiple aspects such as aesthetics and text-image consistency, researchers have begun exploring multi-stage optimization methods **Kulkarni et al., "Density Network: A Framework for Adversarial Training of Generative Models"**. A notable example is VirtualModel **Hendricks et al., "Deep in the Vents: An Exploration of Deep Learning Techniques for Image and Video Processing"**, which employs a multi-branch structure to enhance the credibility of human-object interactions and ensure consistency in generation quality. 
Unlike previous methods primarily focusing on visual quality or text-image consistency, our method uniquely leverages MLLMs to generate CTR-optimized contextual descriptions, guiding diffusion models to produce visually appealing and product-specific advertising images.


% \vspace{-0.5em}
\subsection{Click-Through Rate Prediction}

Click-Through Rate (CTR) prediction plays a crucial role in online advertising and recommendation systems, directly impacting user experience and revenue generation. In the context of CTR-driven advertising image generation, precise CTR estimation enables more effective selection and positioning of visual content, thereby enhancing the overall performance of online advertising campaigns.
%
The advent of deep learning has revolutionized traditional CTR prediction **Cheng et al., "Wide & Deep Learning for Recommender Systems"**, enabling models to automatically learn hierarchical feature representations from raw input data.
%
This paradigm shift not only improved the performance of textual or numerical-based CTR prediction methods **Panigrahi et al., "Deep Transfer Learning for Multi-Task Recommendation and Ad Click Prediction"** but also paved the way for incorporating visual elements into the prediction process.
%
For instance, Wang et al. **Wang et al., "Learning to Rank: A Hybrid Approach for Image and Text Retrieval"** proposed a hybrid bandit approach that integrates visual priors with a dynamic ranking mechanism, demonstrating the potential of incorporating visual information in CTR prediction models.
%
Recognizing that real-world advertisements are inherently multimodal, comprising text, visuals, and other data types, researchers have begun to explore methods that can effectively integrate these diverse modalities. CG4CTR **Li et al., "Multimodal Graph Neural Networks for Video Recommendation"** leveraged a multi-head self-attention module to jointly process textual and visual information from multimodal advertisements, extracting rich features for more accurate CTR estimation.
%
However, these approaches often struggle with complex image understanding tasks and fail to effectively integrate multimodal information. Therefore, it is imperative to explore a more robust CTR estimation method that can seamlessly interpret visual content and harmoniously fuse information from multiple modalities.


% \vspace{-0.5em}
\subsection{Learning from Human Feedback}

Reinforcement Learning from Human Feedback (RLHF) **Jaderberg et al., "Reinforcement Learning with Unsupervised Auxiliary Tasks"** involves collecting human feedback on model outputs. This feedback is then used to optimize the generation model using reinforcement learning algorithms such as PPO **Schulman et al., "Proximal Policy Optimization Algorithms"** or DPO **Hussein et al., "Differential Policy Gradient Methods for Reinforcement Learning"**.
%
For example, Lee et al. **Lee et al., "Reward-Weighted Likelihood Maximization for Deep Generative Models"** proposed a three-stage fine-tuning method to improve text-image alignment in text-to-image (T2I) models using human feedback and reward-weighted likelihood maximization. Wu et al. **Wu et al., "Human Preference-Based Optimization of Text-Image Models"** introduced a human preference score derived from a classifier trained on human-curated image choices, which is then utilized to adapt T2I models. Parrot **Zhang et al., "Multi-Reward Reinforcement Learning for Image Generation and Editing"** proposed a multi-reward RL approach that jointly optimizes the T2I model and prompt expansion network to improve image quality.
%
However, current preference optimization methods for image generation, while showing promise in text-to-image (T2I) tasks, face significant challenges when applied to scenarios with strict visual requirements, such as advertising background generation. These methods often focus solely on optimizing specific metrics, neglecting the contextual relevance and visual harmony of the generated content. 
%
Therefore, our method emphasizes exploring optimization techniques that enable the model to effectively integrate multimodal information to generate diverse and coherent background descriptions that better align with user preferences.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{src/Method.pdf}
    \vspace{-1em}
    \caption{(a) E-commerce knowledge pre-training. The MLLM is pre-trained on a large-scale multimodal e-commerce dataset to incorporate domain-specific knowledge. (b) The Structure of RM. The RM integrates multimodal product features using visual and textual encoders, with dual branches to estimate CTR and identify appealing ad images. (c)  CTR-driven preference optimization stage. The PM generates background descriptions for background generation model to create product images with various backgrounds. The RM then estimates the CTR for these images, simulating human feedback to optimize the PM.}
    % \vspace{-1em}
    \label{fig:1}
\end{figure*}