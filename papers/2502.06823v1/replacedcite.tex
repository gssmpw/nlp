\section{Related Works}
\subsection{Advertising Image Generation}

The primary goal of advertising image generation is to create natural and contextually relevant images while preserving the integrity and identity of the original product. Initially, template-based methods____ were employed for assembling advertising images, offering high efficiency but lacking personalization and flexibility.
With the advent of generative adversarial networks (GANs)____, researchers began exploring more flexible and automated approaches to advertising image creation. Ku et al.____ introduced a novel approach of using GAN models as retrieval-assisted techniques for enhancing product images in advertising contexts.
%
More recently, diffusion models have shown promise in producing high-quality, realistic ad images. InsertDiffusion____ introduced a training-free diffusion architecture that effectively embeds objects into images while preserving their structural and identity features. 
%
Recognizing that ad quality involves multiple aspects such as aesthetics and text-image consistency, researchers have begun exploring multi-stage optimization methods____. A notable example is VirtualModel____, which employs a multi-branch structure to enhance the credibility of human-object interactions and ensure consistency in generation quality. 
Unlike previous methods primarily focusing on visual quality or text-image consistency, our method uniquely leverages MLLMs to generate CTR-optimized contextual descriptions, guiding diffusion models to produce visually appealing and product-specific advertising images.


% \vspace{-0.5em}
\subsection{Click-Through Rate Prediction}

Click-Through Rate (CTR) prediction plays a crucial role in online advertising and recommendation systems, directly impacting user experience and revenue generation. In the context of CTR-driven advertising image generation, precise CTR estimation enables more effective selection and positioning of visual content, thereby enhancing the overall performance of online advertising campaigns.
%
The advent of deep learning has revolutionized traditional CTR prediction____, enabling models to automatically learn hierarchical feature representations from raw input data.
%
This paradigm shift not only improved the performance of textual or numerical-based CTR prediction methods____ but also paved the way for incorporating visual elements into the prediction process.
%
For instance, Wang et al.____ proposed a hybrid bandit approach that integrates visual priors with a dynamic ranking mechanism, demonstrating the potential of incorporating visual information in CTR prediction models.
%
Recognizing that real-world advertisements are inherently multimodal, comprising text, visuals, and other data types, researchers have begun to explore methods that can effectively integrate these diverse modalities. CG4CTR____ leveraged a multi-head self-attention module to jointly process textual and visual information from multimodal advertisements, extracting rich features for more accurate CTR estimation.
%
However, these approaches often struggle with complex image understanding tasks and fail to effectively integrate multimodal information. Therefore, it is imperative to explore a more robust CTR estimation method that can seamlessly interpret visual content and harmoniously fuse information from multiple modalities.


% \vspace{-0.5em}
\subsection{Learning from Human Feedback}

Reinforcement Learning from Human Feedback (RLHF)____ involves collecting human feedback on model outputs. This feedback is then used to optimize the generation model using reinforcement learning algorithms such as PPO____ or DPO____.
%
For example, Lee et al.____ proposed a three-stage fine-tuning method to improve text-image alignment in text-to-image (T2I) models using human feedback and reward-weighted likelihood maximization.
Wu et al.____ introduced a human preference score derived from a classifier trained on human-curated image choices, which is then utilized to adapt T2I models. Parrot____ proposed a multi-reward RL approach that jointly optimizes the T2I model and prompt expansion network to improve image quality.
%
However, current preference optimization methods for image generation, while showing promise in text-to-image (T2I) tasks, face significant challenges when applied to scenarios with strict visual requirements, such as advertising background generation. These methods often focus solely on optimizing specific metrics, neglecting the contextual relevance and visual harmony of the generated content. 
%
Therefore, our method emphasizes exploring optimization techniques that enable the model to effectively integrate multimodal information to generate diverse and coherent background descriptions that better align with user preferences.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{src/Method.pdf}
    \vspace{-1em}
    \caption{(a) E-commerce knowledge pre-training. The MLLM is pre-trained on a large-scale multimodal e-commerce dataset to incorporate domain-specific knowledge. (b) The Structure of RM. The RM integrates multimodal product features using visual and textual encoders, with dual branches to estimate CTR and identify appealing ad images. (c)  CTR-driven preference optimization stage. The PM generates background descriptions for background generation model to create product images with various backgrounds. The RM then estimates the CTR for these images, simulating human feedback to optimize the PM.}
    % \vspace{-1em}
    \label{fig:1}
\end{figure*}