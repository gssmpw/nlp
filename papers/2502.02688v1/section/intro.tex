\section{Introduction} 

In Constraint Programming (CP), a problem is defined on variables and constraints. Each variable is provided with a domain defining the set of its possible values. A constraint expresses a property that must be satisfied by a set of variables. CP uses a specific resolution method for each constraint. 

The success of CP relies on the use of high-performance filtering algorithms (also known as propagators). These algorithms remove values from variable domains that are not consistent with the constraint, i.e. that do not belong to a solution of the constraint's underlying sub-problem.
The most well-known propagator is that of the all different (alldiff) constraint, which specifies that a set of variables must all take different values. The efficiency in practice of that propagator strongly depends on its implementation. Thus, algorithms proposing practical improvements on Régin's algorithm~\cite{Regin:AfilteringalgorithmforconstraintsofdifferenceinCSPs} are still appearing~\cite{Zhang:AFastAlgorithmforGeneralizedArcConsistencyoftheAlldifferentConstraint,Zhang:EarlyandEfficientIdentificationofUselessConstraintPropagationforAlldifferentConstraints}.


In this article, we consider another constraint introduced by Régin that is also popular~\cite{Demassey:ACost-RegularBasedHybridColumnGenerationApproach,Nightingale:AutomaticallyimprovingconstraintmodelsinSavileRow,vanHoeve:Onglobalwarming:Flow-basedsoftglobalconstraints,Gualandi:ConstraintProgramming-basedColumnGeneration,Ducomman:AlternativeFilteringfortheWeightedCircuitConstraint:ComparingLowerBoundsfortheTSPandSolvingTSPTW}: the cardinality constraint with costs~\cite{Regin:CostbasedArcConsistencyforGlobalCardinalityConstraints}. 
We propose to try to speed up its filtering algorithm when there is nothing to deduce.
This is often the case at the start of the search, particularly as the optimal value is far from known. In addition, at this stage, the gains can be significant since few values have been removed from the domains, and so the complexity of the algorithms is greater. This approach can be particularly interesting with aggressive restarting methods and could simplify the use of CP: there is less need to worry about the inference strength of constraints versus their cost. We can worry less about the type of filtering to be used and consider the arc consistency right away. 


The global cardinality constraint (gcc)~\cite{Regin:gcc} is a generalization of the alldiff constraint. 
A gcc is specified in terms of a set of variables $X=\{x_1,...,x_p\}$
which take their values in a subset of $V=\{v_1,...,v_d\}$. It constrains the number of times a value $v_i \in V$ is assigned to variables in $X$ to belong to an interval $[l_i,u_i]$.

A gcc with costs (costgcc) is a generalization of a gcc in which a
cost is associated with each value of each variable. Then, each solution of the underlying gcc is associated with a 
global cost equal to the sum of the costs associated with the assigned values of the
solution. In a costgcc constraint the global cost must be less than a given value, H.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.4\columnwidth}
        \begin{tikzpicture}[
        round/.style={regular polygon, regular polygon sides=6, draw=black},
        square/.style={rectangle, draw=black},
        oval/.style={rounded rectangle, draw=black, minimum width=1.5cm, minimum height=0.7cm},
        flecheG/.style={stealth-},
        flecheD/.style={-stealth},
        align=center
        every node/.style={transform shape}
        ]
    %Nodes
    
    \node[oval] (x1) {Peter};
    \node[oval] (x2)            [below of=x1]         {Paul};
    \node[oval] (x3)            [below of=x2]         {Mary};
    \node[oval] (x4)            [below of=x3]         {John};
    \node[oval] (x5)            [below of=x4]         {Bob};
    \node[oval] (x6)            [below of=x5]         {Mike};
    \node[oval] (x7)            [below of=x6]         {Julia};

    \node (w1) [right of=x1, yshift=-0.1cm, xshift= -0.2cm] {1};
    \node (w2) [right of=x1, yshift=-0.55cm, xshift= -0.1cm] {4};
    \node (w3) [right of=x2, yshift=0.2cm, xshift= -0.2cm] {1};
    \node (w4) [right of=x2, yshift=-0.5cm, xshift= -0.2cm] {4};
    \node (w5) [right of=x3, yshift=0.17cm, xshift= -0.1cm] {3};
    \node (w6) [right of=x3, yshift=-0.15cm, xshift= -0.2cm] {1};
    \node (w7) [right of=x4, yshift=0.54cm, xshift= -0.1cm] {3};
    \node (w8) [right of=x4, yshift=0.1cm, xshift= -0.2cm] {1};
    \node (w9) [right of=x5, yshift=0.5cm, xshift= -0.2cm] {1};
    \node (w10) [right of=x6, yshift=0.5cm, xshift= -0.2cm] {1};
    \node (w11) [right of=x7, yshift=0.9cm, xshift= -0.2cm] {1};
    \node (w12) [right of=x7, yshift=-0cm, xshift= -0.2cm] {1};

    \node[square] (x8) [right of=x1, yshift=-1cm, xshift= 1.5cm] {A};
    \node[square] (x9) [below of=x8] {B};
    \node[square] (x10) [below of=x9] {C};
    \node[square] (x11) [below of=x10] {D};
    \node[square] (x12) [below of=x11] {E};

    \node (c1) [right of=x8, xshift= -0.2cm] {[1, 2]};
    \node (c2) [right of=x9, xshift= -0.2cm] {[1, 2]};
    \node (c3) [right of=x10, xshift= -0.2cm] {[1, 1]};
    \node (c4) [right of=x11, xshift= -0.2cm] {[0, 2]};
    \node (c5) [right of=x12, xshift= -0.2cm] {[0, 2]};
    
    %%%

    \draw[-] (x8) -- (x1);
    \draw[-] (x8) -- (x2);
    \draw[-] (x8) -- (x3);
    \draw[-] (x8) -- (x4);

    \draw[-] (x9) -- (x1);
    \draw[-] (x9) -- (x2);
    \draw[-] (x9) -- (x3);
    \draw[-] (x9) -- (x4);

    \draw[-] (x10) -- (x5);

    \draw[-] (x11) -- (x6);
    \draw[-] (x11) -- (x7);

    \draw[-] (x12) -- (x7);

    \end{tikzpicture} 
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\columnwidth}
        \begin{tikzpicture}[
        round/.style={regular polygon, regular polygon sides=6, draw=black},
        square/.style={rectangle, draw=black},
        oval/.style={rounded rectangle, draw=black, minimum width=1.5cm, minimum height=0.7cm},
        flecheG/.style={stealth-},
        flecheD/.style={-stealth},
        align=center
        every node/.style={transform shape}
        ]
            %Nodes
            
            \node[oval] (x1) {Peter};
            \node[oval] (x2) [below of=x1] {Paul};
            \node[oval] (x3) [below of=x2] {Mary};
            \node[oval] (x4) [below of=x3] {John};
            \node[oval] (x5) [below of=x4] {Bob};
            \node[oval] (x6) [below of=x5] {Mike};
            \node[oval] (x7) [below of=x6] {Julia};
    
            \node (w1) [right of=x1, yshift=-0.2cm, xshift= -0.2cm] {1};
            \node (w3) [right of=x2, yshift=0.2cm, xshift= -0.2cm] {1};
            \node (w6) [right of=x3, yshift=0.2cm, xshift= -0.2cm] {1};
            \node (w8) [right of=x4, yshift=0.2cm, xshift= -0.2cm] {1};
            \node (w9) [right of=x5, yshift=0.2cm, xshift= -0.2cm] {1};
            \node (w10) [right of=x6, yshift=0.6cm, xshift= -0.2cm] {1};
            \node (w11) [right of=x7, yshift=0.6cm, xshift= -0.2cm] {1};
            \node (w12) [right of=x7, yshift=0.1cm, xshift= -0.2cm] {1};
        
            \node[square] (x8) [right of=x1, yshift=-1cm, xshift= 1cm] {A};
            \node[square] (x9) [below of=x8] {B};
            \node[square] (x10) [below of=x9] {C};
            \node[square] (x11) [below of=x10] {D};
            \node[square] (x12) [below of=x11] {E};
    
            \node (c1) [right of=x8, xshift= -0.2cm] {[1, 2]};
            \node (c2) [right of=x9, xshift= -0.2cm] {[1, 2]};
            \node (c3) [right of=x10, xshift= -0.2cm] {[1, 1]};
            \node (c4) [right of=x11, xshift= -0.2cm] {[0, 2]};
            \node (c5) [right of=x12, xshift= -0.2cm] {[0, 2]};
            
            %%%
        
            \draw[-] (x8) -- (x1);
            \draw[-] (x8) -- (x2);
        
            \draw[-] (x9) -- (x3);
            \draw[-] (x9) -- (x4);
        
            \draw[-] (x10) -- (x5);
        
            \draw[-] (x11) -- (x6);
            \draw[-] (x11) -- (x7);
        
            \draw[-] (x12) -- (x7);
        
        \end{tikzpicture} 
        \label{fig:introSuppr}
    \end{subfigure}

    \caption{Example of a global cardinality constraint with costs. 
    Source \protect\cite{Regin:CostbasedArcConsistencyforGlobalCardinalityConstraints}. 
    The sum of assignment costs must be less than or equal to 11. On the left, the original problem and on the right, the same problem after deleting all arcs that cannot belong to a solution.
    }
    
    
    \label{fig:intro}
\end{figure}


Cardinality constraints with costs has proved useful in
many real-life applications, such as routing, scheduling, 
rostering, or resource allocation problems.
The total costs are often used for expressing preferences, time or cost.

Figure~\ref{fig:intro} gives an example of a costgcc constraint and the associated filtering algorithm.
There are $7$ workers represented by the variables ${Peter, Paul, Mary, John, Bob, Mike, Julia}$ and $5$ tasks represented by the values ${A, B, C, D, E}$. Each worker has the ability to perform certain tasks and must perform exactly one of them. There is an arc from a worker to a task if the worker can perform the task, its cost corresponding to the time it takes the worker to perform the task. A task has a capacity defining the number of times it must be performed. For example, $A$ must be performed between $1$ and $2$ times. The objective is to find an assignment whose sum of costs is less than $11$. The best possible assignment has a cost of $7$, so it is a solution. On the right-hand side of Figure~\ref{fig:intro}, all arcs that cannot belong to a solution have been removed. For example, the arc $(Peter, B)$ can be deleted. If $B$ is assigned to $Peter$, then the maximum capacity of $B$ will be exceeded, so the arc $(Mary, B)$ or $(John, B)$ cannot be part of the solution. If $(John, B)$ is kept, then a value must be assigned to $Mary$, the only possibility is $(Mary, A)$ with a cost of $3$. The cost of all assignments is now $12$, which is more than $11$, so this is not a solution. Similarly, if $(Mary, B)$ is kept, then the only possibility for $John$ is $(John, A)$ with a cost of $3$ and the total cost is $12$, which is too high.

The filtering algorithm associated with a costgcc constraint~\cite{Regin:CostbasedArcConsistencyforGlobalCardinalityConstraints} can be described as repetitive. First, it computes a maximum flow at minimum cost to determine whether the constraint is consistent (i.e., admits a solution). Then, to find out whether a variable $x$ can be instantiated with a value $a$, it tries to pass a unit of flow through the arc representing the assignment of $a$ to $x$ so that the total cost of the flow is less than $H$. 
This operation involves computing min-cost flow through an arc from a given min-cost flow. This can be done by searching for a shortest path between $x$ and $a$ in the residual graph of the min-cost flow. Furthermore, it has been shown that it is possible to avoid computing a shortest path for each value of each variable and that computing one shortest path per assigned value (which is less than the number of variables) is sufficient~\cite{Regin:CostbasedArcConsistencyforGlobalCardinalityConstraints}. Unfortunately, the algorithm is repeated for each assigned value, which often proves prohibitively expensive.

In this paper, we introduce a new approach avoiding this repetitive aspect as much as possible. Our approach is based on several observations:
\begin{itemize}
    \item Finding a min-cost flow for each assignment is not necessary. Finding that there exists a flow whose cost is less than $H$ is enough. 
\item 
It is not necessary to compute any path exactly because we are only interested in their costs, not the path. Further, the exact value of the cost is not necessary either. An upper bound below a maximum cost is sufficient.
\item The use of landmarks (i.e., particular nodes) have proved their worth in speeding up computations of the shortest paths between large elements (millions of nodes)~\cite{Goldberg:ComputingtheShortestPath:ASearchMeetsGraphTheory}. 
Let $x$ and $y$ be two nodes of a graph and $p$ be another node called landmark, then we have: $d(x,p)+d(p,y) \geq d(x,y)$ where $d(i,j)$ is the shortest path distance from $i$ to $j$. Thus, by selecting one or several good landmarks $p$ we can find a good upper bound of $d(x,y)$ for each pair of nodes $x$, $y$.
\item Calls to the filtering algorithm often do not remove any value. This means that the margin (i.e., slack between $H$ and the min-cost flow value) is often large relative to the data, so using the upper bound should give good results.
\end{itemize}

On the basis of the above, we propose to introduce preprocessing in order to reduce the effective shortest path computations as proposed by Régin's algorithm.
Our approach is to search for landmarks and use them to compute upper bounds on paths to avoid unnecessary explicit shortest path computations. We consider several types of landmarks to integrate the structure of the graph, such as landmarks at the periphery (outline) of the graph or at the center. The advantage of this approach is its low cost because only two shortest paths are required per landmark. We also introduce a way to quickly detect whether a costgcc constraint is arc consistent. 

The paper is organized as follows. Section~\ref{sec:Preliminaries}
recalls some preliminaries on constraint programming, graph and flow theory. Section~\ref{sec:FilteringAlgorithm} describes Régin's algorithm because our method is based on it. 
Section~\ref{sec:UpperBounds} introduces upper bounds on shortest paths based on landmarks and, in Section~\ref{sec:ImprovedFilteringAlgorithm}, the arc consistency algorithm is accordingly adapted. Section~\ref{sec:LandmarkSelection} details some landmark selection methods. Section~\ref{sec:Experimentation} 
gives some experiments on classical problems showing that our approach dramatically reduces the number of computed shortest paths. 