

We provide supplementary information on promising novel technical advancements that can be explored (but are not limited to) to further improve how GenAI models can align with standards and cater to their inherent complexities as discussed in Section~\ref{sec:challenges}. Some of these approaches may have been preliminarily explored by recent works for selected domains with available machine-readable standards data. Collaborations between AI and interdisciplinary areas can foster further advancements and open more novel approaches toward standard alignment.

\textbf{\technicalOpp{Constraint and Knowledge Representations}}
Specifications from standards can be represented as a form of a structured set of constraints either as input or as part of an embedded training regime for aligning GenAI models. The simplest example of this is through in-context learning (ICL), where constraints are framed as prompts in an instruction-like manner with specific informative examples provided to show the target output required from the model
\cite{brown2020language,mishra-etal-2022-reframing}. This has been done on works such as the production of high-quality standard-conforming educational content with CEFR and Bloom's Taxonomy \cite{imperial-etal-2024-standardize,malik-etal-2024-tarzan,elkins2024teachers} and rewriting complex texts to conform to government-mandated plain languages guidelines \cite{da2022redactor,joseph-etal-2024-factpico}. The advantage of ICL is its simplicity, which can easily be explored by non-technical domain users with any arbitrary GenAI-based chat interface (e.g., ChatGPT) and can be attributed to how the paradigm shift started (see Section~\ref{sec:paradigm_shift}). More advanced levels of representation focus on transforming standards into knowledge graphs and ontologies. An example of this is the work by \citet{hernandez2024open}, where they transformed the text content of the EU AI Act into a high-level knowledge graph to show links between defined terms and their associated requirements from the Act's statements for compliance checking.

\textbf{\technicalOpp{Post-Training Improvements}}
Post-training has become one of the major foci in ML research since the release of preference optimization techniques like RLHF \cite{ouyang2022training} and DPO \cite{rafailov2024direct} combined with supervised finetuning (SFT) to improve the response alignment of GenAI models with respect to task requirements. All Baseline models in Table~\ref{tab:model_classification} have undergone post-training through SFT. For standard alignment, post-processing can potentially be emulated by aggregating prompt and response pairs that conform to the specifications of a standard and then finetuning a pretrained model with this data. An example reference for this is OpenAI's Deliberative Alignment method \cite{guan2024deliberative} where an LLM is trained with chain-of-thought (CoT) style prompts \cite{wei2022chain} can identify whether which safety policy specification is applicable when identifying whether to respond to a prompt or not. However, for researchers who want to explore this approach, one caveat is that it will require at least 1,000 instances of very high-quality, expert-level pairwise data to achieve relatively decent performance \cite{zhou2023less}. Nonetheless, combinations of post-training techniques (e.g., SFT + DPO with standard-aligned preference pairs + CoT prompting) are viable approaches for improving a GenAI model's standard compliance capabilities.

\textbf{\technicalOpp{Synthetic Data Generation}}
Synthetic data generation using modern GenAI models that have undergone processes such as larger scaling, instruction-tuning, and preference optimization often outperform other data augmentation techniques for downstream tasks \cite{ye-etal-2022-zerogen,li-etal-2023-synthetic}. In standard alignment of GenAI models, researchers can explore several options related to synthetic data generation to improve compliance capabilities. First, in parallel with post-training enhancements from above, using synthetic data in the form of standard compliant and non-compliant examples can be a practical choice to optimize a model's generation qualities. This approach has been applied by \citet{fan-etal-2024-goldcoin}, where they generated synthetic case scenarios for GDPR and HIPAA Privacy Rules to finetune smaller LLMs for compliance detection. Moreover, recent works have documented higher performance for models that have been finetuned with a combination of high-quality expert data and machine-generated data in alignment tasks which makes the process of compiling standard-aligned feedback data relatively easier \cite{miranda2024hybrid,ivison2024unpacking,lee2024rlaif}.

\textbf{\technicalOpp{Retrieval and Tool Augmentation}}
Augmenting GenAI models, particularly LLMs, with external tools to enhance their problem-solving capabilities has gained increasing research attention in recent years. The use of tools such as calculators, search engines, and API function calls has been shown to improve the zero-shot performance of LLMs across question-answering downstream tasks requiring up-to-date information \cite{hao2023toolkengpt,schick2023toolformer}. In the case of standard alignment, syntactic content-based standards such as the CEFR and CCS standards (see Table~\ref{tab:standards_classification} for reference) requiring specific characteristics of texts such as sentence lengths to measure complexity can greatly benefit from a GenAI model that knows how to call a calculator tool to approximate how long sentences should be generated. Moreover, a search engine tool can also help GenAI models access updated versions of standard specifications from its original web sources as prior version checking. On the other hand, another approach that can improve GenAI models' domain knowledge is to encapsulate it in a retrieval-augmented generation (RAG) ecosystem where auxiliary retrievers that have access to external knowledge bases that can be added to as context to prompts \cite{lewis2020retrieval,ram-etal-2023-context}. 

\textbf{\technicalOpp{Reasoning Capabilities}}
Whether GenAI models, such as LLMs, can reason or not is a highly debated topic in current ML research. Reasoning is an inherent ability that plays a crucial role in how humans solve problems through critical thinking \cite{huang-chang-2023-towards}. Previous research often claims that such capability can be triggered in different ways, such as providing intermediary reasoning steps to prompts \cite{wei2022chain} or using arbitrary models to select and infer reasoning steps from context information \cite{creswell2022faithful}. Assuming GenAI models can actually reason, in standard alignment, such skill may play an important role in deciding which specifications of a standard are required to be followed and which ones can be disregarded safely, given the additional context information of a task. Preliminary work in this direction includes OpenAI's Deliberative Alignment method \cite{guan2024deliberative} where an LLM is optimized to reason over which safety policy specifications should be followed and which can be ignored using their o-series models. As such, GenAI models rated Advanced and Adaptive in \textsc{C3F} should document convincing reasoning capabilities across applicable tasks.