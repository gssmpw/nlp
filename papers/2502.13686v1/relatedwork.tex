\section{Related Work}
\label{sec_related_work}




The reconstruction of partially observed graph signals is a well-studied problem in the literature and many different solution approaches exist. Classical approaches include Tikhonov regularization \cite{shuman2013emerging}, piecewise constant or piecewise planar signal models \cite{JungHMJHE19, ChenCZ24}, kernel-based methods \cite{JianTE24}, non-smooth graph signal interpolation \cite{MAZARGUIL2022108480, ioannidis2019semi}, iterative graph signal reconstruction methods \cite{narang2013localized, wang2015local, christensen2003frames, 8918094}, and techniques based on the bandlimitedness assumption \cite{8918094,  LorenzoBIBL18, YangYYH21, WangCYLF23}. Each of these approaches employs a particular strategy or prior in order to effectively utilize the graph signal information. Traditional Tikhonov regularization methods, optimal iterative reconstruction (O-PGIR) \cite{8918094}, and optimal sampling  \cite{LorenzoBIBL18, YangYYH21} strategies rely on low-pass or band-limited signal models, which may fall short of capturing band-pass or high-pass variations in signal spectra as demonstrated in Fig.~\ref{fig_molene_signal},  especially when signals display isolated and localized behaviors in specific graph regions. TV-regularization ideas \cite{JungHMJHE19} and affine signal models as in \cite{MAZARGUIL2022108480} may better handle non-smooth signal variations; however, they rely on other constraints such as piecewise constancy or low-rank structure of graph signals, which may not always be met in practice.

% the local neighborhood information in the vertex domain, they lack the ability of model extension to new or multiple graphs. 


%inherently assume that the graph signals to be estimated exhibit smooth changes on the graph. Similarly, the Optimal Papoulis-Gerchberg Iterative Reconstruction (O-PGIR) scheme \cite{8918094} or optimal sampling strategies \cite{LorenzoBIBL18, YangYYH21} relying on bandlimited signal models may not adequately capture band-pass or high-pass variations in signal spectra as demonstrated in Fig.~\ref{fig_molene_signal} especially when signals display isolated and localized behaviors in specific graph regions.





Recent trends in the analysis of graph data focus considerably on graph neural network (GNN) models \cite{defferrard2016convolutional, kipf2017semi, HamiltonYL17}, with many variants building on spectral approaches \cite{levie2019cayleynets}, diffusion or message passing models \cite{atwood2016diffusion, Brockschmidt20, ZouCZLZWL24}, and graph attention models \cite{VelickovicCCRLB18, Brody0Y22}.  However, GNNs are subject to two significant limitations: a lack of interpretability and reliance on relatively large training datasets. Additionally, deeper networks do not consistently translate to improved performance within graph settings \cite{ZhaoA20}, in contrast to the notable success of convolutional networks for signals on regular grids. Given the aforementioned limitations of GNNs, deep algorithm unrolling methods introduce a hybrid approach between traditional graph regularization schemes and GNN-based restoration methods, incorporating learnable parameters into iterative algorithms \cite{chen2021graph, nagahama2022graph}. 

% Removed: "Although the formulation in these works permits arbitrary network depth, practical implementation often employs a small number of layers, typically one intermediary layer" -> (nagahama2022graph reports that performance increases with number of layers!)

%\cite{} proposed a hybrid approach that integrates model-based and neural network-based restoration methods. This approach leverages the concept of deep algorithm unrolling (DAU), incorporating learnable parameters into iterative algorithms \cite{chen2021graph, zhang2018dynamically, li2020efficient, bertocchi2020deep, monga2021algorithm}. In \cite{chen2021graph}, a recent study, authors extended the DAU concept to graph signal denoising, presenting unrolled GCNs based on two optimization problems: sparse coding and trend filtering. Although the formulation permits arbitrary network depth, practical implementation often employs a small number of layers, typically one intermediary layer. This choice aligns with the observation that increased network depth does not necessarily yield enhanced performance in this context. Furthermore, the notion of deep algorithm unrolling has been influenced by iterative reconstruction techniques applied to graph signal estimation, where each iteration is unrolled into a layer within a deep network \cite{chen2021timevarying, kojima2023restoration, nagahama2022graph}.





%Inspired by the achievements of convolutional neural networks (CNNs), researchers have sought to extend similar convolutional mechanisms to the realm of graph neural networks, leading to the development of convolutional graph neural networks. These networks can be broadly categorized into two groups: spectral-based approaches \cite{bruna2014spectral, defferrard2016convolutional, kipf2017semi, levie2019cayleynets} and spatial-based approaches \cite{atwood2016diffusion, niepert2016learning, gilmer2017neural}. The acquired node representations through graph neural networks have proven effective in the modeling and analysis of graph signals. Graph convolutional networks (GCNs), as introduced in \cite{kipf2017semi}, serve as counterparts to convolutional neural networks for tasks involving image processing within the graph domain. GCNs possess the ability to autonomously learn network parameters to minimize a designated loss function.

% However, GCNs are subject to two significant limitations: a lack of interpretability and reliance on substantial training datasets. Additionally, findings reported in \cite{kipf2017semi}, \cite{defferrard2016convolutional}, and \cite{chen2021graph} underscore that deeper networks do not consistently translate to improved performance within graph settings, in contrast to the notable successes of convolutional networks for signals on regular grids \cite{ulyanov2020deep},\cite{zhang2018end, wang2019dynamic}. 
 
%Given the aforementioned limitations of GCNs, \cite{nagahama2022graph} proposed a hybrid approach that integrates model-based and neural network-based restoration methods. This approach leverages the concept of deep algorithm unrolling (DAU), incorporating learnable parameters into iterative algorithms \cite{chen2021graph, zhang2018dynamically, li2020efficient, bertocchi2020deep, monga2021algorithm}. In \cite{chen2021graph}, a recent study, authors extended the DAU concept to graph signal denoising, presenting unrolled GCNs based on two optimization problems: sparse coding and trend filtering. Although the formulation permits arbitrary network depth, practical implementation often employs a small number of layers, typically one intermediary layer. This choice aligns with the observation that increased network depth does not necessarily yield enhanced performance in this context. Furthermore, the notion of deep algorithm unrolling has been influenced by iterative reconstruction techniques applied to graph signal estimation, where each iteration is unrolled into a layer within a deep network \cite{chen2021timevarying, kojima2023restoration, nagahama2022graph}.




Our method relies on learning narrowband graph signal prototypes for the reconstruction of graph signals, which bears resemblance to a graph dictionary learning problem. Several previous works have studied graph signal representations over predetermined or learnt graph dictionaries. The  spectral graph wavelet dictionaries (SGWT) proposed in \cite{hammond2011wavelets} extends the wavelet theory to graph domains in view of their spectral characterization. Other efforts include the learning of parametric graph dictionaries on single graphs \cite{thanou2014learning} and multiple graphs \cite{thanou2015multi}, as well as multi-scale graph dictionaries based on Haar wavelets \cite{8642839}. Although our method can be interpreted as a particular type of graph dictionary learning algorithm, it has the following essential differences from the above methods. First, the aforementioned methods are generic dictionary learning algorithms that require fully observed graph signals to train; hence, are not particularly suited to the graph signal reconstruction problem and lack the capability of learning models with only partially observed graph signals. Similarly, they neither  employ any priors on the spectral characteristics of the data, nor present a theoretical understanding of their reconstruction performance. In contrast, our solution actively employs the prior that the signal energy is concentrated in certain bands of the spectrum, thanks to which it involves relatively few model parameters to learn (limited to the center and scale parameters of the kernels fitting to the dominant spectral components), hence, it is particularly tailored to scenarios with severe lack of training data. This feature of our method also makes it favorable against sophisticated but more complex methods such as GNN-based solutions requiring the learning of a large set of model parameters.  Furthermore, we provide an extensive theoretical analysis of the signal reconstruction performance of our method, with a careful justification of when multi-graph learning is advantageous over individual learning.


% Removed: \cite{gavish2010multiscale},


Preliminary versions of our study have been presented in \cite{TurhanV21, KarTV22}. The current paper builds on these studies by significantly extending the experimental results and including a detailed theoretical analysis.