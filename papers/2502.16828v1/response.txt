\section{Related Work}
\subsection{Energy Estimation}

Estimating the energy landscape is a crucial problem across multiple disciplines. The most fundamental approach involves collecting data through modern sequencing techniques and manual experiments. **Zemla, "A 3D model of protein dynamics"** measured tens of thousands of Aequorea victoria (avGFP) derivative genotypes to construct the local fitness landscape of green fluorescent proteins. **Schuster, "Fitness landscapes of protein sequences"** analyzed the fitness of all single mutations in VIM-2 $\beta$-lactamase across a 64-fold range of ampicillin concentrations. Additionally, **Kashtan, "Surprisingly simple modular circuitry in synthetic networks"** conducted high-throughput functional genomics on Salmonella to identify gene networks related to adaptive effects. There have been many similar efforts **Schuster, "Fitness landscapes of protein sequences"; Kaern, "A quantitative blueprint for a novel microbial biofuel producer"**. However, these manual experiments are often associated with high operational costs, making machine learning a promising solution to improve this process in a data-driven manner **Jiang, "Deep potential molecular dynamics"**. **Engel, "Machine learning of molecular energy landscapes"** introduce the deep potential molecular dynamics method, using neural networks to model interatomic forces and potential energy. To mitigate overfitting issues in deep neural networks, **Mousavi, "Spectral regularization for deep neural networks"** apply sparse recovery algorithms from coding theory for spectral regularization. **Rosenberg, "Predicting protein-ligand binding with high-speed AFM"** employ high-speed atomic force microscopy to collect data for training a U-net model to predict the energy landscape of spatial angles on the DHR10-micaN protein. Additionally, **Jiang, "Hierarchical Bayesian modeling of mutation effects"** and **Engel, "Polynomial regression of population functions"**, offer interpretable predictions of mutation effects and population functions through hierarchical Bayesian modeling and polynomial regression, respectively. More recently, **Huang, "Graph neural network for intermolecular interactions"** developed a graph neural network to model intermolecular interactions, predicting Gibbs free energy in solute-solvent interactions. Despite these efforts, these models often depend on true energy values or molecular force fields as supervisory signals. In contrast to these methods, our PESLA does not require supervisory signals for energy; instead, it learns to estimate energy through a self-supervised evolution prediction task. An additional benefit of this approach is that the predicted energy effectively enhances the accuracy of evolution prediction.

\subsection{Evolution Prediction}

Predicting the evolution of stochastic dynamical systems is challenging due to the unknown underlying energy landscape. **Pekalski, "Dimensionality reduction for system identification"** employ dimensionality reduction techniques to construct reduced-order models that capture essential macroscopic information, thereby simplifying the analysis of large-scale systems. To handle the challenge of modeling long-term dynamics, approaches such as learning time-invariant representations have been explored **Turchin, "Learning time-invariant representations"**. Furthermore, **Budiardja, "Koopman operator theory for nonlinear dynamics"** extend Koopman operator theory to map system states into a Hilbert space, facilitating the learning and interpretation of nonlinear dynamics. **Pekalski, "Markov process modeling of stochastic dynamics"**, and **Turchin, "Discrete state transitions for stochastic processes"**, represent stochastic dynamical processes as discrete state transitions within a Markov process framework. In contrast to existing methods, PESLA utilizes energy landscape knowledge to guide system dynamics modeling.