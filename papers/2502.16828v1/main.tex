
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{anyfontsize}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{rotating}

% \setlength{\belowcaptionskip}{-0.1cm}

\newcommand{\fixme}[1]{{\textcolor{red}{#1}}}
\newenvironment{packed_itemize}{
\begin{list}{\labelitemi}{\leftmargin=1.5em}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
  \setlength{\headsep}{0pt}
  \setlength{\topskip}{0pt}
  \setlength{\topmargin}{0pt}
  \setlength{\topsep}{-2pt}
  \setlength{\partopsep}{0pt}
}{\end{list}}

\title{Predicting the Energy Landscape of Stochastic Dynamical System via Physics-informed Self-supervised Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ruikun Li \\
Shenzhen International Graduate School \\
Tsinghua University \\
\texttt{lirk612@gmail.com} \\
\And
Huandong Wang\thanks{Corresponding author} \\
Department of Electronic Engineering \\
BNRist, Tsinghua University \\
\texttt{wanghuandong@tsinghua.edu.cn} \\
\And
Qingmin Liao \\
Shenzhen International Graduate School \\
Tsinghua University \\
\texttt{liaoqm@tsinghua.edu.cn} \\
\And
Yong Li \\
Department of Electronic Engineering \\
BNRist, Tsinghua University \\
\texttt{liyong07@tsinghua.edu.cn} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
Energy landscapes play a crucial role in shaping dynamics of many real-world complex systems. System evolution is often modeled as particles moving on a landscape under the combined effect of energy-driven drift and noise-induced diffusion, where the energy governs the long-term motion of the particles.
Estimating the energy landscape of a system has been a longstanding interdisciplinary challenge, hindered by the high operational costs or the difficulty of obtaining supervisory signals. Therefore, the question of how to infer the energy landscape in the absence of true energy values is critical. In this paper, we propose a physics-informed self-supervised learning method to learn the energy landscape from the evolution trajectories of the system. It first maps the system state from the observation space to a discrete landscape space by an adaptive codebook, and then explicitly integrates energy into the graph neural Fokker-Planck equation, enabling the joint learning of energy estimation and evolution prediction. Experimental results across interdisciplinary systems demonstrate that our estimated energy has a correlation coefficient above 0.9 with the ground truth, and evolution prediction accuracy exceeds the baseline by an average of 17.65\%. The code is available at \href{https://github.com/tsinghua-fib-lab/PESLA}{github.com/tsinghua-fib-lab/PESLA}.
\end{abstract}

\section{Introduction}

Energy landscapes are inherent in many stochastic dynamical systems in nature, 
such as the potential energy surface of protein conformations~\citep{norn2021protein}, the fitness landscape of species evolution~\citep{papkou2023rugged, poelwijk2007empirical}, and the fractal energy landscapes of soft glassy materials.
The evolution of these systems can be modeled as particles moving on the landscape under the combined effect of energy-driven drift and noise-induced diffusion. The structure of the energy landscape governs the long-term motion of particles, forming the deterministic aspect of the dynamics, while inherent random noise disrupts the movement along the energy gradient, driving exploration across energy barriers~\citep{blount2018contingency, kryazhimskiy2014global}. When multiple low-energy regions exist in the landscape, the combined effect of the energy gradient and noise induces high-frequency movement within individual regions and low-frequency transitions between different regions~\citep{lin2024learning}.
In this context, 
energy landscapes have been applied to guide the generation of stable molecular structures~\citep{noe2019boltzmann} and direct the evolution of proteins~\citep{packer2015methods, greenbury2022structure}, and more recently, they have been incorporated as physical knowledge into deep learning for predicting system evolution~\citep{guan2024predicting, wang2024multi, ding2024artificial}.

Due to its fundamental role in governing the system dynamics,  estimating the energy landscape of dynamical systems has become an essential research problem across various disciplines. \citet{couce2024changing} cultivate 50,000 generations of bacteria to measure the fitness effects of mutations, while \citet{sarkisyan2016local} measure tens of thousands of luminescent protein genotypic sequences to construct the functional landscape. These manual experimental approaches are not only costly but also heavily reliant on expert knowledge.
With the success of deep learning in numerous 
disciplines~\citep{jumper2021highly,han2023synergistic,wang2023scientific, chen2024social},
several deep learning models have been proposed to estimate energy or equivalent quantities based on molecular spatial structures~\citep{zhang2018deep}, species genotypes~\citep{tonner2022interpretable}, or population compositions~\citep{skwara2023statistically}. These methods still require high-cost annotations to provide supervisory signals for energy, which limits their practicality. %However, in real-world scenarios, obtaining true energy is often far more costly than acquiring the system's historical evolution trajectories. 
In real-world scenarios, it is typically more accessible to obtain abundant low-cost evolutionary trajectories of the system, which inherently embeds information about energy-driven drift~\citep{weinstein2022non}. Therefore, an important research question arises: can we estimate the energy landscape only based on the system's evolution trajectories in a data-driven manner?

However, estimating the energy landscape from evolutionary trajectories remains a complicated problem with the following challenges.
First, observable evolutionary trajectories typically cover only a limited portion of the vast state space. For instance, there are approximately $10^{11}$ potential triple mutants of a typical protein, while available high-throughput measurement techniques can only handle around $10^4$ to $10^7$ distinct genotypes, covering just a small fraction of the mutational space surrounding the natural sequence~\citep{tonner2022interpretable}. Second, distilling energy information from evolutionary trajectories requires building a model incorporating the energy landscape and the distribution of trajectory data, thereby establishing connections between them. 
Classical Markov state models~\citep{noe2019boltzmann} establish this connection by strictly assuming that sampled data follow a Boltzmann distribution derived from the energy, which unrealistically demands that trajectories are fully sampled from a thermodynamic equilibrium state. In contrast, existing self-supervised learning methods~\citep{kamyshanska2014potential} treat neural networks as black-box models to fit data distributions, completely disregarding the guidance of physical knowledge in terms of energy and system evaluation. 
Currently, there is still no effective model that organically integrates AI techniques and physical knowledge for energy estimation without supervisory signals.

In this paper, we propose a Physics-informed Energy Self-supervised Landscape Analysis (PESLA) method to estimate the energy landscape from historical evolution trajectories in a self-supervised manner. PESLA maps the system state from the observed space to a discretized latent space via vector quantization techniques~\citep{van2017neural}.
Through adaptively learning a codebook to partition the vast state space, our model concentrates on the essential shapes of the energy landscape in discrete domains, thus disregarding the
negligible information of the energy landscape and overcoming the challenge posed by limited observations.
Then, PESLA utilizes the self-supervision signal from the prediction error of the system state to guide energy estimation. In this process, a graph neural ODE inspired by the Fokker-Planck equation is utilized to model the time evolution of probability distributions across different discretized states, and a physics-inspired regularization constraint is employed to integrate the prior knowledge of Boltzmann distribution of long-term dynamics~\citep{sato2014approximation}, without relying on the assumption of thermodynamic equilibrium sampling.
These physics-inspired architectures serve as the bridge to distill information of the energy landscape from the system dynamics, thereby enabling the self-supervised learning of the system's energy landscape.

Our contribution can be summarized as follows:
\begin{packed_itemize}
    \vspace{-0.2cm}
    \item We develop a novel framework to estimate the energy landscape of the system only utilizing the self-supervision signal from predicting the system state, where the physics-information architecture of graph neural Fokker-Planck equation and physics-inspired regularization serves as the bridge to distill information of energy landscape from the system dynamics.
    \item We develop a discrete encoding method of the system state to coarsen the continuous energy landscape using a codebook obtained through vector quantization techniques. It allows our model to concentrate on
    the essential shapes
    of the energy landscape, effectively disregarding its negligible information and enhancing the sample efficiency of limited observational data.
    \item Experimental results across interdisciplinary systems demonstrate that PESLA reliably estimates system energy with absolute correlation coefficients above 0.9 and achieves 17.65\% higher evolution prediction accuracy compared to state-of-the-art baselines.
\end{packed_itemize}


\section{Background and problem setup}


Let us consider a stochastic dynamical process which can be described by the following differential equation:
\begin{equation}\label{equ: sde}
ds_t= f(s_t)dt+\sigma(s_t,t) dW(t),
\end{equation}
\begin{equation}\label{equ: ob}
x_t  = g(s_t).
\end{equation}
\iffalse
\begin{equation}
\begin{aligned}
    ds_t &=-\nabla V(s_t)dt+\sigma(s_t,t) dW(t), \\
    x_t  &= g(s_t).
\end{aligned}
\end{equation}
\fi
Specifically, it represents a system with latent state variable $s_t\in\mathcal{S}$ whose evolution is driven by a deterministic drag force $f(s_t)$ and a random force described by white noise  $\sigma(s_t,t) dW(t)$.
While the state variable $s_t$ is hidden and cannot be observed directly, the observable measurement $x_t\in\mathcal{X}$ of the system is derived \ through a transformation $g:\mathcal{S} \rightarrow \mathcal{X}$, which can be either linear or nonlinear and can even represent a mapping from continuous space to discrete space, thereby describing systems with discrete observable metrics, such as ecological evolution.


More specifically, we focus on systems where the force $f(s_t)$ is conservative. This implies the existence of an energy function $E(s_t)$, also referred to as the energy landscape, such that $f(s_t)=-\nabla E(s_t)$. Then, the dynamic equation~\ref{equ: sde} can be be rewritten as:
\begin{equation}
ds_t= -\nabla E(s_t)dt+\sigma(s_t,t) dW(t),
\end{equation}
The energy landscapes measure the thermodynamic stability of a given state. %link it to boltzmann distribution.
Low-energy regions induce a drift that draws the system state into them with greater probability and duration, manifesting thermodynamically as the Boltzmann distribution, $p \propto e^{-E(s) / kT}$, where $k$ is Boltzmann constant and $T$ represents temperature. For evolution starting from any initial state distribution, the system's long-term dynamics will eventually drift toward the Boltzmann distribution defined by the energy landscape.
Examples of such energy landscapes in different disciplines include
%Typical well-defined energy landscapes in different disciplines include 
fitness landscapes in ecology~\cite{papkou2023rugged}, potential energy in molecular dynamics~\cite{chmiela2017machine}, and free energy in glassy materials~\cite{charbonneau2014fractal}.





\textbf{Learning problem} In this paper, our primary objective is to estimate the energy landscape of a stochastic dynamical system based on its evolution trajectories, without the true energy as a supervisory signal. 
More formally, the input of this learning problem is a set of the $N$-step evolution trajectory $X_N=\{x_{t_i}\}_{i=0}^{N-1}$ of the stochastic dynamical system in the $D$-dimensional observation space $\mathcal{X}$.
Then, for an arbitrary observable state $x$, the objective of this learning problem is twofold: (1) building a transformation $\mathcal{E}$ to map the observable measurement to a latent feature $c=\mathcal{E}(x)$ that determines the energy of the system; (2) estimating the energy $\hat{E}(\mathcal{E}(x))$ as an approximation of the true energy $E(g^{-1}(x))$.
Since the true energy $E(g^{-1}(x))$  is unavailable as a supervisory signal in the learning process, the estimated energy $\hat{E}(\mathcal{E}(x))$ is only required to be a linear transformation of the true energy.

%Since the system temperature is unknown, the energy function only needs to be a linear transformation of the true energy.

\iffalse
Given the $N$-step evolution trajectory $X_N$ of a stochastic dynamical system in a $D$-dimensional state space, we aim to (1) estimate the energy function $E(x)$ and (2) learn the time evolution $f(x_{t_0}, E(*), \Delta t, W_t)$ of the system’s state distribution on the energy landscape. Since the system temperature is unknown, the energy function only needs to be a linear transformation of the true energy.


Given an initial point $x_{t_0}$, the $N$-step evolution trajectory derived from equation~\ref{equ: sde} is represented as $X_N=(x_{t_0}, x_{t_1},...,x_{t_N})$. The prediction for such systems means inferring the series of conditional probability distribution of future states $P(X_N |x_{t_0})=\left(p(x_{t_1}|x_{t_0}),  p(x_{t_2}|x_{t_0}), ...,  p(x_{t_N}|x_{t_0})\right)$ based on the initial state $x_{t_0}$.
\fi


\iffalse
\subsection{System Evolution}

We consider stochastic dynamical systems, which can be either continuous ($x \in \mathbb{R}^N$) or discrete ($x \in \mathbb{Z}^N$), formalized as
\begin{equation}\label{equ: sde}
p(x_{t_0+\Delta t}|x_{t_0})=f(x_{t_0}, \Delta t, \theta_E, W_t),
\end{equation}
where $f(x_{t_0}, \Delta t, \theta_E, *)$ represents the deterministic dynamics under the environmental parameter $\theta_E$, while the Wiener process $W_t$ introduces microscopic uncertainty into the evolutionary process. Given an initial point $x_{t_0}$, the $N$-step evolution trajectory derived from equation~\ref{equ: sde} is represented as $X_N=(x_{t_0}, x_{t_1},...,x_{t_N})$. The prediction for such systems means inferring the series of conditional probability distribution of future states $P(X_N |x_{t_0})=\left(p(x_{t_1}|x_{t_0}),  p(x_{t_2}|x_{t_0}), ...,  p(x_{t_N}|x_{t_0})\right)$ based on the initial state $x_{t_0}$.



We consider stochastic dynamical systems formalized as
\begin{equation}\label{equ: sde}
dx_t=f(x_t, t, \theta_E)dt+\sigma(x_t,t) dW(t) ,
\end{equation}
where $f(x_t, t, \theta_E)$ represents the deterministic dynamics under the environmental parameter $\theta_E$, and $dW(t)$ denotes Wiener process~\citep{lin2024learning}. $f(x_t, t, \theta_E)$ dominates the system's long-term dynamics, forming the predictable part, while $dW(t)$ introduces microscopic uncertainty into the evolutionary process. Given an initial point $x_{t_0}$, the $N$-step evolution trajectory derived from equation~\ref{equ: sde} is represented as $X_N=(x_{t_0}, x_{t_1},...,x_{t_N})$. The prediction for such systems means inferring the series of conditional probability distribution of future states $P(X_N |x_{t_0})=(p(x_{t_1}|x_{t_0}),  p(x_{t_2}|x_{t_0}), ...,  p(x_{t_N}|x_{t_0}))$ based on the initial state $x_{t_0}$, where the key challenge is to model the time evolution of the conditional probability distribution, $p(x_{t_0+\Delta t}|x_{t_0})=g(x_{t_0}, \Delta t)$.


% see paper Fokker–Planck Equations for a Free Energy Functional or Markov Process on a Graph

% If the external potential is conservative and the noise term derives from a reservoir in thermal equilibrium, then the long-time solution to the Langevin equation must reduce to the Boltzmann distribution ---- https://en.wikipedia.org/wiki/Langevin_equation#Recovering_Boltzmann_statistics  Recovering Boltzmann statistics

\subsection{Energy Landscape}

Statistical physics models the evolution of stochastic dynamical systems as the movement of particles on an energy landscape, as seen in processes like the evolutionary succession of mutants in populations~\citep{sella2005application} and protein folding~\citep{yin2020construction}. Energy $E(x)$ is defined as a scalar function over the state space, measuring the thermodynamic stability of a given state. Different disciplines have specific equivalent physical quantities, such as fitness in ecology. For consistency in this paper, we use energy to generically refer to such physical quantities.
In this context, the influence of the environment on system evolution is modeled as the energy function on the landscape, expressed as $f(x_{t_0}, \Delta t, \theta_E, W_t)=f(x_{t_0}, \Delta t, E(*), W_t)$. Therefore, the key to predicting the future evolution of such systems lies in estimating such energy function $E(*)$.

% Stochastic dynamics system is often abstracted as the movement of particles in the state space $\mathcal{X} \subseteq R^D$ so as to introduce powerful analytical tools from statistical physics~\citep{vanchurin2022thermodynamics}. In this context, energy $E(x, t)$ is defined as a scalar function over the state space $\mathcal{X}$, measuring the thermodynamic stability of a given state. During the system's evolution, particles move towards lower elevations along energy valleys within the state space. Different disciplines have specific equivalent physical quantities, such as fitness in ecology. For consistency in this paper, we use energy to generically refer to such physical quantities. The energy quantifies the influence of the environment on the system evolution. Consistent with existing work~\citep{lin2024learning, federicilatent}, we assume that the environment is static or that its change occurs on a timescale that is large enough to be negligible, i.e. $E(x, t)=E(x)$.

\subsection{Fokker-Planck Equation}

The Langevin equation is a fundamental mathematical model for describing stochastic dynamical phenomena in natural sciences, with its long-term evolution converging to the Boltzmann distribution~\citep{sato2014approximation}. The overdamped Langevin equation, $dx=-\nabla E(x)dt+\sqrt{2kT}dW_t$, neglects the inertia and depicts the particle's movement driven by the energy gradient and random noise. Its time evolution of state probability density is modeled by the Fokker-Planck equation~\citep{risken1996fokker},
\begin{equation} \label{equ:fokker-planck}
    \frac{\partial}{\partial t} p(x)= \frac{\partial}{\partial x} [E(x)p(x) + kT \frac{\partial}{\partial x}p(x)] ,
\end{equation}
which describes how the state probability evolves from the initial state to the Boltzmann distribution $p(x) \propto e^{E(x)/kT}$. The evolution process of the system on the energy landscape can be obtained by integrating the Fokker-Planck equation over time.

% Statistical mechanics establishes a bridge between system evolution and the energy landscape. Specifically, when the system evolves over a sufficiently long period, the state distribution converges to the Boltzmann distribution~\citep{sato2014approximation}. At this point, the probability of a state is positively correlated with its energy, following $p(x) \propto e^{E(x)/kT}$, where $k$ is the Boltzmann constant and $T$ is the temperature. The time evolution of the state probability distribution from any initial state is governed by the Fokker-Planck equation~\citep{risken1996fokker}
% \begin{equation} \label{equ:fokker-planck}
%     \frac{\partial}{\partial t} p(x_t)= \frac{\partial}{\partial x} [f(x_t, t, \theta_E)p(x_t)] + \frac{\partial^2}{\partial x^2} [\frac{1}{2}\sigma(x_t, t)^2 p(x_t)]
% \end{equation}
% with drift coefficient $f(x_t, t, \theta_E)=\partial E(x_t)$. The time evolution of the conditional probability distribution can be solved through the integral of equation~\ref{equ:fokker-planck} as $p(x_{t_0+\Delta t}|x_{t_0})=g(x_{t_0}, \Delta t)=p(x_{t_0})+\int_{t_0}^{t_0+\Delta t} p(x_t)dt$. For stochastic dynamical systems with unknown energy, estimating the energy function $E(x)$ is a prerequisite for predicting the Fokker-Planck flow of the state probability, i.e., $g(x_{t_0}, \Delta t) = g(x_{t_0}, E(*), \Delta t)$.

\subsection{Problem Definition}

In this paper, our primary objective is to estimate the energy landscape of a stochastic dynamical system based on its evolution trajectories, without the true energy as a supervisory signal. Given the $N$-step evolution trajectory $X_N$ of a stochastic dynamical system in a $D$-dimensional state space, we aim to (1) estimate the energy function $E(x)$ and (2) learn the time evolution $f(x_{t_0}, E(*), \Delta t, W_t)$ of the system’s state distribution on the energy landscape. Since the system temperature is unknown, the energy function only needs to be a linear transformation of the true energy.
\fi

\section{Method}

% In this section, we introduce a physics-informed energy self-supervised landscape analysis (PESLA) method, which predicts the energy landscape of stochastic dynamical system via physics-informed self-supervised learning task. First, we develop an adaptive codebook learning module to map the observed space to the energy landscape. This approach integrates concepts from reduced-order modeling of complex systems to mitigate uncertainties caused by limited sample coverage. Although there are no direct supervisory signals for the energy landscape, we explicitly incorporate the energy function into a graph neural Fokker-Planck equation. By employing self-supervised learning to minimize the prediction error in the time evolution of state probability distributions, we force accurate energy estimation. Additionally, we introduce physics-inspired regularization constraints into the optimization objective to eliminate the dependence on the assumption that data is sampled from a thermodynamic equilibrium state.
In this section, we introduce a Physics-informed Energy Self-supervised Landscape Analysis (PESLA) method, which learns to predict the energy landscape through a self-supervised evolution prediction task, as shown in Figure~\ref{fig:framework}. First, we develop an adaptive codebook learning module to instantiate the mapping $\mathcal{E}$ from the observed space to the energy landscape. This approach integrates concepts from reduced-order modeling of complex systems to mitigate uncertainties caused by limited sample coverage. Next, we explicitly incorporate the energy function into a graph neural Fokker-Planck equation to model the system's evolution on the energy landscape. Additionally, we introduce physics-inspired regularization constraints into the optimization objective to eliminate the assumption of thermodynamic equilibrium sampling.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/framework.pdf}
    \caption{Framework of PESLA. (a) The energy landscape with evolution trajectories; (b) Partitioning the state space with an adaptive codebook to form the codewords with a graph topology and modeling the time evolution of probability across neighboring regions by graph neural Fokker-Planck equation.}
    \label{fig:framework}
\end{figure}

\subsection{Adaptive Codebook Learning}

% Previous studies have validated that despite the high dimensionality of the state space, the dynamics of complex systems unfold on a very low-dimensional manifold~\citep{vlachas2022multiscale, thibeault2024low}. By aggregating local regions with similar or collective features, coarse-grained models help eliminate uncertainties in the original space while retaining long-term dynamic information~\citep{li2024predicting}. Here, inspired by discrete encoding techniques from the field of computer vision~\citep{van2017neural, razavi2019generating}, we apply this reduced-order modeling concept to estimate the energy landscape.
Constructing the energy landscape involves learning the transformation $\mathcal{E}$ from the observed space $\mathcal{X}$ to the latent space $\mathcal{S}$ where the energy landscape resides. Previous studies have shown that, despite the high dimensionality of the state space, the long-term dynamics of systems unfold on a very low-dimensional manifold in the form of reduced-order model~\citep{vlachas2022multiscale, thibeault2024low, li2024predicting}. This suggests that the energy landscape, which shapes the system's long-term evolution, has inherently low dimensionality. Similar phenomena have been observed in natural language processing and image representation, where a set of discrete codewords is sufficient to capture the essential representation of the original data~\citep{van2017neural, razavi2019generating}. Therefore, modeling the energy landscape as a discrete reduced-order model in the latent space $\mathcal{S}$ offers a promising approach to addressing the challenge of the vast state space~\citep{noe2009constructing}.

To implement such reduced-order approach and identify the energy landscape in the latent space $\mathcal{S}$, we enhance the autoencoder with a learnable codebook $C=\{c_i \in \mathbb{R}^d~|~i=1,2,...,K\}$ to discretize the latent space of the encoded data.
The transformations $\mathcal{E}$ and $g$ between the observed space and the latent space are parameterized by $\Xi$ and $\Omega$, respectively.
Specifically, after a sample $x$ is encoded into a latent vector $s$, it is mapped to the most similar codeword $c_i$, which then serves as the input to the probabilistic decoder $\Omega$ for reconstructing $x$. This k-nearest neighbor (KNN) style discrete aggregation partitions the latent space into multiple local regions (as shown in Figure~\ref{fig:framework}b), each uniquely represented by the energy of a codeword, thereby forming the low-dimensional landscape space. The encoding function $\Xi$ maps the original space to the landscape space, capturing semantic features to ensure similar states fall into the same codeword region, thereby reducing reconstruction error. We emphasize that this design allows for optimal scaling of the state space partitioning from the limited coverage of observed data, rather than simple equidistant grid binning, as shown in Figure~\ref{fig:prinz_data} (center). This adaptive scaling ensures the maximal utilization of codewords, enhancing the robustness to the preset number of codewords. Through the adaptive codebook encoding, the observed trajectories are mapped onto the energy landscape in the form of codewords, i.e., $\{c_{t_i}\}_{i=0}^{N-1}$. 
% Here $c_t$ represents the codeword corresponding to the system state $x_t$. The preset number of codewords theoretically determines the upper limit of the representational capacity of encoding. However, due to the low-dimensional nature of complex systems, not all preset codewords are always utilized, as the robustness analysis in the experiments reveals.


\subsection{Graph Neural Fokker-Planck Equation} \label{sec:GNFPE}

% In the latent space $\mathcal{S}$, the time evolution of the system state is influenced by the combined effect of energy-driven drift and diffusion caused by inherent random noise, theoretically modeled by the Fokker-Planck equation~\citep{risken1996fokker}. On the discretized energy landscape, we extend the traditional Fokker-Planck equation into a graph neural differential equation, enabling joint learning of energy estimation and evolution prediction.

% We construct the codeword topology $A=(a_{ij})_{K \times K}$ based on the adjacency relationships of the codeword regions (as shown in Figure~\ref{fig:framework}) and estimate the energy of each codeword as $E(c_i)$ as the energy landscape $\mathcal{G}=\{A, C, E(*)\}$ of system evolution. The time evolution of conditional state probabilities $\mathbf{p}(x_{t_0+\Delta t}|x_{t_0})$ on the energy landscape $\mathcal{G}$ is modeled as a graph neural diffusion process~\citep{chamberlain2021grand}, formalized as
% \begin{equation}
%     \begin{split}
%         &\mathbf{H}(t_0+\Delta t) = \mathbf{H}(t_0) + \int_{t_0}^{t_0+\Delta t}{\frac{\partial \mathbf{H}(t)}{\partial t}dt}, \\
%         &\mathbf{p}(c_{t_0+\Delta t}|c_{t_0}) = \Psi (\mathbf{H}(t_0+\Delta t)),
%     \end{split}
% \end{equation}
% where $\mathbf{H}(t_0) = \Phi (\mathbf{p}(c_{t_0}))$ encodes the initial condition and $\mathbf{p}(c_{t_0})$ is $K$-dimensional one-hot vector with $c_{t_0}$-th element set to 1. We extend~\citet{chow2012fokker}'s theory by designing a graph neural Fokker-Planck equation to explicitly model state diffusion driven by energy differences between neighboring codewords as
% \begin{equation}
%     \frac{\partial}{\partial t}\mathbf{H}_{c_i} =  \sum_j{\mathbf{W}_{ij}[E_{ji}+\mathbf{\beta_\xi}(\log\mathbf{H}_{c_j}-\log\mathbf{H}_{c_i})] \circ [\sigma(kE_{ji})\mathbf{H}_{c_j} + \left(1-\sigma(kE_{ij})\right)\mathbf{H}_{c_i}]},
% \end{equation}

% where $E_{ij} = E(c_i)-E(c_j)$ and $\mathbf{W}_{ij}$ is calculated by neighborhood attention. The learnable coefficient $\mathbf{\beta_\xi}$ represents the strength of noise acting between neighboring codewords, while $k$ is the scaling factor for the sigmoid activation function. We implement $\Phi$ and $\Psi$ as graph convolutional neural networks and incorporate positional encoding~\citep{chamberlain2021beltrami} to supplement the codeword features, enabling the capture of distance information.

In the latent space $\mathcal{S}$, the time evolution of the system state is influenced by the combined effect of energy-driven drift and diffusion caused by inherent random noise, theoretically modeled by the Fokker-Planck equation~\citep{risken1996fokker}. On the discretized energy landscape, we extend the traditional Fokker-Planck equation into a graph neural differential equation, enabling joint learning of energy estimation and evolution prediction.

We construct the codeword topology $A=(a_{ij})_{K \times K}$ based on the adjacency relationships of the codeword regions (as shown in Figure~\ref{fig:framework}) and estimate the energy of each codeword as $E(c_i)$ as the energy landscape $\mathcal{G}=\{A, C, E(*)\}$ of system evolution. 
At this point, we have projected the original observed trajectory onto a low-dimensional energy landscape, obtaining the transition trajectory of the system state on the codeword topology. Predicting the temporal evolution of the system means modeling the time-dependent evolution of the probability distribution over the codewords. The effects of energy and noise on this evolution are modeled by the Graph Fokker-Planck equation~\citet{chow2012fokker} as:
\begin{equation}
    \frac{dp_i}{dt}=\sum_{j \in N(i), E_{ji}>0}{((E_{ji}+\beta \log{\frac{p_j}{p_i}})p_j}+\sum_{j \in N(i), E_{ji}<0}{((E_{ji}+\beta \log{\frac{p_j}{p_i}})p_i}+\sum_{j \in N(i), E_{ji}=0}{\beta (p_j-p_i)},
\end{equation}
where $p_i$ denotes the probability of node $i$ and $E_{ji} = E_j-E_i$. $\beta$ is a positive constant which governs the noise strength. Denoting $\mathbf{p}(c_{t_0})$ as $K$-dimensional probability distribution at time $t_0$, one can naively obtain the conditional probability distribution $\mathbf{p}(c_{t_0+\Delta t}|c_{t_0})$ after a diffusion time of $\delta t$ by performing a time integration of the Fokker-Planck equation on the initial condition $\mathbf{p}(c_{t_0})$.

% However, using a rigid one-hot vector as the initial probability distribution is not always the optimal approach. The adaptive codebook learning module maps the system state to the most similar codeword feature, discarding similarity information with other codewords, which is particularly noticeable near the boundaries of codeword regions. 
However, considering that the evolution of a node's state often depends on its neighbors, projecting the one-dimensional probability vector into a higher-dimensional space with stronger representational capacity helps capture this rich relational structure.
We employ a graph convolutional neural networks (GCN) based probability encoder, $\mathbf{H}(t_0) = \Phi (\mathbf{p}(c_{t_0}))$, introducing neighborhood information through positional encoding~\citep{chamberlain2021beltrami, yuan2024unist} to lift the one-dimensional probability vector into a high-dimensional representation. Thus, the time evolution of conditional state probabilities $\mathbf{p}(x_{t_0+\Delta t}|x_{t_0})$ on the energy landscape $\mathcal{G}$ is modeled as a graph neural diffusion process~\citep{chamberlain2021grand, yuan2024urbandit}, formalized as
\begin{equation}
    \begin{split}
        &\mathbf{H}(t_0+\Delta t) = \mathbf{H}(t_0) + \int_{t_0}^{t_0+\Delta t}{\frac{\partial \mathbf{H}(t)}{\partial t}dt}, \\
        &\mathbf{p}(c_{t_0+\Delta t}|c_{t_0}) = \Psi (\mathbf{H}(t_0+\Delta t)).
    \end{split}
\end{equation}
We extend~\citet{chow2012fokker}'s theory by designing a graph neural Fokker-Planck equation to explicitly model state diffusion driven by energy differences between neighboring codewords as
\begin{equation}
    \frac{\partial}{\partial t}\mathbf{H}_{c_i} =  \sum_j{\mathbf{W}_{ij}[E_{ji}+\mathbf{\beta_\xi}(\log\mathbf{H}_{c_j}-\log\mathbf{H}_{c_i})] \circ [\sigma(kE_{ji})\mathbf{H}_{c_j} + \left(1-\sigma(kE_{ji})\right)\mathbf{H}_{c_i}]},
\end{equation}
where $\mathbf{W}_{ij}$ is calculated by neighborhood attention. The learnable coefficient $\mathbf{\beta_\xi}$ represents the strength of noise acting between neighboring codewords, while $k$ is the scaling factor for the sigmoid activation function. The ablation study can be found in Appendix~\ref{sec:ablation_study}, where we demonstrate that modeling in the encoded probability space performs significantly better than directly modeling the probability vector.

\subsection{Training} \label{sec:training}

The trainable parameters include the encoder $\Xi$, decoder $\Omega$, codebook $C$, probability encoder $\Phi$, probability decoder $\Psi$, neighborhood attention weights $W$, coefficient vector $\beta_\xi$, and energy function $E(*)$. The detailed model architecture is provided in Appendix~\ref{sec:architecture}. In the following, we introduce the training procedure for the model.

Adaptive codebook learning and evolution prediction form a joint learning task. The optimization objective for the former is to minimize the negative log-likelihood of the reconstructed distribution, i.e., $L_{reconstruct}=-\log{\mathbf{q}_{\Xi,\Omega,C}(x)}$. In our experiments, we use a Gaussian prior distribution decoder with negative log-likelihood loss for continuous systems, and cross-entropy loss for discrete systems. Additionally, the loss function $L_{vq}$ for updating codeword is consistent with the one proposed by~\citet{van2017neural}.
Similarly, we minimize the negative log-likelihood in both the latent space and the landscape space for the evolution prediction task. In the latent space, we minimize the L2 error $L_{latent}=||\Phi(\mathbf{p}(c_{t+\Delta t})) - \Psi(H(t+\Delta t))||$, while in the landscape space, we use cross-entropy $L_{code}=-\mathbf{p}(c_{t+\Delta t})\log{\mathbf{q}(c_{t+\Delta t})}$.

With the mapping of adaptive codebook, we can estimate the distribution $\mathbf{p}(c_i)$ of observed samples within the landscape space and employ the corresponding negative log-probability as reference energies to guide energy estimation. However, this approach fails when evolution trajectories are not sampled from a thermodynamic equilibrium state. 
Proven by statistical mechanics, the state probability distribution evolving in the form of Fokker-Planck equation converges to the Boltzmann distribution.
% $\mathbf{q}(c_i) = \frac{e^{-E(c_i)/kT}}{\sum_j e^{-E(c_j)/kT}}$
Although we cannot expect all sample data to be drawn from a thermodynamic equilibrium state, the long-term evolution of states will eventually converge to the Boltzmann distribution.
This suggests incorporating a regularization term into the long-term prediction task, expressed as the KL divergence between the empirical distribution $\mathbf{p}$ and the Boltzmann distribution $\mathbf{q}$, i.e., $L_{phy} = D_{\text{KL}}(\mathbf{p} \| \mathbf{q}) = \sum_{i=0}^{K} \mathbf{p}(c_i) \log \left(\frac{\mathbf{p}(c_i)}{\mathbf{q}(c_i)}\right)$. Overall, we conduct the training process by optimizing the aforementioned objectives $L=L_{reconstruct}+L_{vq}+L_{latent}+L_{code}+L_{phy}$. Detailed training strategies are provided in Section~\ref{sec:setup} and ablation studies can be found in Appendix~\ref{sec:ablation_study}.


\section{Experiments}

We conduct experiments on three classic dynamical systems from different disciplines to evaluate the accuracy of PESLA in (1) energy estimation and (2) evolution prediction. For fairness, we use the same data preprocessing and apply grid search to fine-tune the learning rates and hyperparameters for all models.
We perform 10 independent training and testing runs for each model to calculate the mean and standard deviation of all evaluation metrics in each experiment.


\subsection{Setup} \label{sec:setup}

\noindent \textbf{Baselines} For the energy estimation task, we employ the Markov state model (MSM)~\citep{majewski2023machine} and autoencoder potential energy (APE)~\citep{kamyshanska2014potential} as baselines. For the evolution prediction task, we compare PESLA with NeuralMJP~\citep{seifner2023neural}, T-IB~\citep{federicilatent}, VAMPNets~\citep{mardt2018vampnets}, and SDE-Net~\citep{kong2020sde}. Details on the implementation and hyperparameter searching of these baseline algorithms can be found in Appendix~\ref{app:grid_search}.

\begin{figure}[!t]
    \centering
    \input{figures/energy_fig_table}
    \caption{
        Visualization of the results on the energy estimation.
        (a): full-space energy correlation $\rho_F$ as a function of data size (top), and comparison across different methods (bottom);
        (b): trajectory energy correlation $\rho_T$ as a function of data size (top), and comparison across different methods (bottom).
    }
\end{figure}


\noindent \textbf{Evaluation Metrics} 
We evaluate the accuracy of energy estimation from two perspectives. The trajectory energy correlation $\rho_T$ represents the Pearson correlation coefficient between the predicted and true energies for all samples along a new trajectory, assessing predictive performance within the regions covered by training data. The full-space energy correlation $\rho_F$ measures the correlation coefficient for the energy of system states uniformly across the entire state space, accounting for unseen areas during training.
For the evolution prediction task, all metrics are measured from $M$ reference trajectories ${X^{\tau}_N}^M$ unfolding from randomly initialized system states, where $\tau$ denotes the lag time of each step. All models are tasked with predicting evolution trajectories starting from these initial states, covering the same time span as the reference trajectories. We evaluate the accuracy of the predicted distributions by calculating the Jensen-Shannon divergence between the marginal ($MJS$) and transition ($TJS@\tau$) probability distributions of the predicted and reference trajectories across all states. For systems with a continuous state space, we discretize it into evenly spaced grid partitions, following previous work \cite{federicilatent, arts2023two}.
Further details can be found in Appendix~\ref{app:metrics_calculation}.

\noindent \textbf{Training strategy}
We first train encoder $\Xi$, decoder $\Omega$ and the feature vectors of the codewords $C$ to construct the landscape topology. Then, we freeze them and train the parameters of the graph neural Fokker-Planck equation and energy function $E(*)$ on the landscape. For all models, we use the Adam optimizer, with the learning rate decaying exponentially by a factor of 0.99 each epoch.


\subsection{2D Prinz Potential}

We first apply PESLA to the 2D particle movement system on an asymmetric potential energy surface~\citep{mardt2018vampnets, federicilatent}. The particle displacement is governed by the stochastic differential equation as $dX_t=\nabla V(X_t)dr+\sigma dW_t$, where the potential energy function $V$, defined by $V(x)=(x^4_1-\frac{x^3_1}{16}-2x^2_1+\frac{3x_1}{16})+(x^4_2-\frac{x^3_1}{8}-2x^2_1+\frac{3x_1}{8})$, consists of four interconnected low-energy regions, as shown in Figure~\ref{fig:prinz_data} (left). A total of 10 trajectories with 100K time steps are generated and details on the generation and preprocessing can be found in Appendix~\ref{app:data_generation}.
The results of energy estimation and evolution prediction are presented in Figure~\ref{fig:4well_energy_data} (bottom) and Figure~\ref{tab:prinz_results}, respectively, where PESLA significantly outperforms the baseline methods in both tasks.

Figure~\ref{fig:prinz_data} (center) visualizes the adaptive codebook learned by PESLA from historical trajectories, with different codewords distinguished by color and shape, representing their mapped regions in the original state space. The varying density of codewords at different locations directly reflects PESLA's adaptive scaling partitioning. We emphasize that the adaptive codebook captures the dynamical knowledge of the energy landscape, which is fundamentally different from the simple equidistant grid-based binning approach. At a macro level, the codebook divides the plane into an approximate 2×2 region corresponding to the four potential wells. The high energy barriers between potential wells serve as the boundaries of the four codeword regions. Low-energy wells are assigned more codewords (e.g., the bottom-left well), suggesting that the model recognizes the importance of low-energy regions as long-term dynamic attractors and allocates more \textit{attention} to them, which aligns with the higher accuracy observed in low-energy regions shown in Figure~\ref{fig:prinz_data} (right). We present the codebooks of multiple independent experiments in Appendix~\ref{app:codebook_4well}, demonstrating that this is not a coincidental phenomenon.    
At a finer level, multiple codewords are assigned to the center of each potential well, while the outer areas are divided into mapped regions approximately perpendicular to the equipotential lines. This indicates that the random walk behavior induced by noise is effectively captured through the differences between codewords.

\begin{figure}[!t]
    \centering
    \input{figures/prinz_fig_table}
    \caption{
        Visualization of the results on the 2D Prinz Potential.
        (a): potential surface and short sample trajectory (left), codebook distinguished by color and shape (center), and comparison between estimated energy and ground truth (right). Blue/red represents low/high values in the heatmap;
        (b): measures of marginal and transition JS divergence for unfolded sequences at the lag time $\tau$ of 100;
        (c): The impact of the preset number of codewords on evolution prediction accuracy (top), energy estimation accuracy (center), and codeword occupancy (bottom).
        % y = 2.7139x + -9.9202
    }
\end{figure}

We examine the sensitivity of PESLA to data size and hyperparameters. Figure~\ref{fig:4well_energy_data} (top) illustrates the impact of training data size on performance when all energy estimation algorithms are required to estimate across full-space samples. Due to the coarse-graining of adaptive codebook, PESLA maintains optimal performance even with reduced data. In contrast, baseline methods are significantly limited by insufficient sample coverage in the state space. As shown in Figure~\ref{fig:4well_energy_data} (top), as the data volume increases, the performance of MSM improves due to the enhanced sample coverage.
Figure~\ref{fig:prinz_representations} reports the robustness of PESLA concerning the preset number of codewords. Although the number of preset codewords can be continuously increased, the actual number of occupied codewords converges automatically, and the accuracy of energy and evolution predictions reaches its peak.

\subsection{Ecological Evolution}

We examine the strong selection weak mutation system within eco-evolutionary dynamics, which is widely studied in ecology to understand the adaptive evolution of populations in specific environments~\citep{kryazhimskiy2009dynamics, bank2016predictability}. The fixation probability of a candidate state $j$ (new mutation) with fitness $f_j$ is governed by the Kimura formula derived from the Wright–Fisher model, given by $p_{i \rightarrow j}=\frac{1-e^{2s_i(j)}}{1-e^{2Ns_i(j)}}$, where $N$ represents the population size and $s_i(j)=\frac{f_j}{f_i}-1$ is the selection coefficient. \citet{sella2005application} have mathematically demonstrated that the logarithmic fitness of such evolutionary systems aligns with the energy of thermodynamic systems. We simulate 1K trajectories, each with 100 time steps, under the two-locus setting where each locus has 64 possible mutation types as our dataset. Figure~\ref{fig:sswm_energy_data} (bottom) and Figure~\ref{tab:sswm_results} respectively report PESLA's superior predictive performance for fitness and system evolution.

\begin{figure}[!t]
    \centering
    \input{figures/sswm_fig_table}
    \caption{
        Visualization of the results on the Ecological Evolution.  
        (a): fitness landscape and short sample trajectory (left), codebook distinguished by color and shape (center), and energy landscape fitted by RANSAC regression from estimated energy (right). Blue/red represents low/high values in the heatmap;
        (b): measures of marginal and transition JS divergence for unfolded sequences at the lag time $\tau$ of 10;
        (c): The impact of the preset number of codewords on evolution prediction accuracy (top), energy estimation accuracy (center), and codeword occupancy (bottom).
        % y = -0.5158x + 4.7611
    }
\end{figure}
\vspace{-0.15cm}

In ecology, fitness measures the relative advantage of a genotype and is negatively correlated with energy~\citep{sella2005application}. PESLA estimates the energy function of genotypes with a correlation coefficient close to -1. The predicted energy is fitted with a RANSAC regression model (see Appendix~\ref{app:metrics_calculation}) and visualized in Figure~\ref{fig:sswm_data} (right). The distribution pattern of codewords within the codebook indicates that PESLA successfully identifies the set of genotypes with high fitness in eco-evolutionary dynamics. Moreover, since the genotype space is characterized by the Hamming distance, states in the same row or column of the codebook are more likely to be mapped to the same codeword (Figure~\ref{fig:sswm_data} (center)). This indicates that the adaptive codebook incorporates knowledge of system dynamics rather than relying on simple equidistant grid binning.

We also examine the impact of data size and the preset number of codewords on PESLA in this system. For the energy estimation within the sample-covered region (measured by $\rho_T$), PESLA shows minimal sensitivity to data size, as shown in Figure~\ref{fig:sswm_energy_data} (top). The influence of the preset number of codewords is similar to that observed in last case, which validates PESLA's robustness.


\subsection{Protein Folding}

We apply PESLA to the folding data of five fast-folding proteins simulated by the Anton supercomputer~\citep{lindorff2011fast}. Each protein has two folding trajectories of equal length, used for model training and testing, respectively. Due to the lack of true energy, we estimate the reference energy using Time-lagged Independent Component Analysis (TICA) and the Markov State Model (MSM) based on the complete dataset (three times larger than the training data), consistent with previous studies~\citep{majewski2023machine, mardt2018vampnets}. For each protein, the lag time used in TICA processing and experiments is based on the mean transition path time reported by~\citet{lindorff2011fast}. The reference energy distribution on the 2D principal component plane identified by TICA is shown in Figure~\ref{fig:protein_data}, with implementation details provided in Appendix~\ref{app:data_generation}. Each protein features a varying number (1 to 4) of low-energy regions with different distributions, posing challenges for energy estimation. Figure~\ref{fig:protein_data2} shows PESLA’s partitioning of the state space for each protein on the TICA principal component plane, demonstrating that PESLA differentiates low-energy, high-energy, and unknown energy regions with varying codeword aggregation rates. This automatic scaling ensures that PESLA's energy predictions remain consistent with reference values (as shown in Figure~\ref{fig:protein_data3}), even in challenging protein folding problems. Additionally, PESLA achieved the best performance in the evolution prediction task (see Appendix~\ref{app:protein_folding}).

\begin{figure}[!t]
    \centering
    \input{figures/protein_fig_table}
    \caption{
        Visualization of the results on the Protein Folding.
        (a): reference energy landscapes of each protein;
        (b): adaptive codebooks of each protein;
        (c): trajectory correlation coefficients $\rho_T$ between predicted and reference energy.
    }
\end{figure}
\vspace{-0.15cm}

% \subsection{\textcolor{red}{Additional Experiments}} \new

% \textcolor{red}{We conduct comprehensive experiments on PESLA's interpretability, consistency, noise robustness, transferability, and scalability, with detailed analyses provided in the Appendix~\ref{sec:comprehensive_evaluation}. First, we verify that evolution prediction is optimal only when the correlation between predicted and true energy exceeds 0.8. We further demonstrate that PESLA's energy landscapes remain consistent across hyperparameter settings and are robust to noise below 50\% of data amplitude. Transferability experiments confirm zero-shot generalization on unseen protein data, and PESLA’s scalability is shown by adjusting codebook size for different problem scales.}


\section{Related Work}

\subsection{Energy Estimation}

Estimating the energy landscape is a crucial problem across multiple disciplines. The most fundamental approach involves collecting data through modern sequencing techniques and manual experiments. \citet{sarkisyan2016local} measured tens of thousands of Aequorea victoria (avGFP) derivative genotypes to construct the local fitness landscape of green fluorescent proteins. \citet{chen2022environmental} analyzed the fitness of all single mutations in VIM-2 $\beta$-lactamase across a 64-fold range of ampicillin concentrations. Additionally, \citet{wang2024high} conducted high-throughput functional genomics on Salmonella to identify gene networks related to adaptive effects. There have been many similar efforts~\citep{starr2018pervasive}. However, these manual experiments are often associated with high operational costs, making machine learning a promising solution to improve this process in a data-driven manner~\citep{rupp2012fast, han2023synergistic}. \citet{zhang2018deep} introduce the deep potential molecular dynamics method, using neural networks to model interatomic forces and potential energy. To mitigate overfitting issues in deep neural networks, \citet{aghazadeh2021epistatic} apply sparse recovery algorithms from coding theory for spectral regularization. \citet{zhang2022rotational} employ high-speed atomic force microscopy to collect data for training a U-net model to predict the energy landscape of spatial angles on the DHR10-micaN protein. Additionally, \citet{tonner2022interpretable} and \citet{skwara2023statistically} offer interpretable predictions of mutation effects and population functions through hierarchical Bayesian modeling and polynomial regression, respectively. More recently, \citet{ijcai2024p642} developed a graph neural network to model intermolecular interactions, predicting Gibbs free energy in solute-solvent interactions. Despite these efforts, these models often depend on true energy values or molecular force fields as supervisory signals. In contrast to these methods, our PESLA does not require supervisory signals for energy; instead, it learns to estimate energy through a self-supervised evolution prediction task. An additional benefit of this approach is that the predicted energy effectively enhances the accuracy of evolution prediction.

\subsection{Evolution Prediction}

Predicting the evolution of stochastic dynamical systems is challenging due to the unknown underlying energy landscape. \citet{vlachas2022multiscale} employ dimensionality reduction techniques to construct reduced-order models that capture essential macroscopic information, thereby simplifying the analysis of large-scale systems. To handle the challenge of modeling long-term dynamics, approaches such as learning time-invariant representations have been explored~\citep{federicilatent, kostic2024learning, li2023learning}. Furthermore, \citet{kostic2022learning, kostic2024sharp} extend Koopman operator theory to map system states into a Hilbert space, facilitating the learning and interpretation of nonlinear dynamics. \citet{wu2018deep} and~\citet{seifner2023neural} represent stochastic dynamical processes as discrete state transitions within a Markov process framework. In contrast to existing methods, PESLA utilizes energy landscape knowledge to guide system dynamics modeling.


\section{Conclusion}

In this paper, we propose the PESLA method to estimate the energy landscape from historical evolution trajectories in a self-supervised manner. By integrating adaptive codebook learning and a graph neural Fokker-Planck equation, PESLA collaboratively models the energy landscape and system dynamics, even with limited observational data. We introduce physics-inspired regularization to help PESLA move beyond the reliance on thermodynamic equilibrium sampling. Experimental results across various systems demonstrate that PESLA outperforms state-of-the-art methods in both energy estimation and evolution prediction. PESLA does not require supervisory signals for energy, making it a powerful data-driven tool for understanding and predicting stochastic dynamical systems. 
% In future work, we plan to explore PESLA's application to more interdisciplinary scenarios, aiming to discover previously undefined energy terms.

\noindent \textbf{Limitations and Future work}
This work focuses on estimating the energy landscape of a class of energy-driven evolutionary systems. 
% However, inferring energy landscapes becomes more challenging when a low-dimensional landscape is ill-defined or varies over time. If a low-dimensional landscape does not exist, meaning the required codewords are as numerous as the state space, prediction performance may suffer with limited data. For time-varying landscapes, where $E(*)$ ideally adapts to $E(*,t)$, further model design exploration is needed to address these dynamics in the future. \new}
However, when a system is driven by non-conservative forces, an energy landscape does not exist, as in the case of motion in viscous fluids. Additionally, inferring energy landscapes becomes more challenging when the landscape is time-varying, such as in cases where climate change alters species fitness. 
Future work will need to explore additional model designs to accommodate the dynamics of time-varying landscapes, where $E(*)$ needs to adapt to $E(*,t)$.
% For time-varying landscapes, where $E(*)$ needs to adapt to $E(*,t)$, further model design exploration will be necessary to accommodate these dynamics in the future.

\newpage
\section*{Acknowledgments}
This work was supported in part by the National Natural Science Foundation of China under U23B2030, 62171260, and 92270114.
% We sincerely thank Dr. Jiliang Hu for his valuable insights and thoughtful suggestions during the project discussions.
We sincerely appreciate the inspiration and valuable insights from discussions with Dr. Jiliang Hu.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\input{appendix}


\end{document}
