\section{Related Work}
\label{sec:related}

\textbf{Beyond token-level guidance for language modeling.} While next token prediction remains the standard paradigm for language modeling, recent approaches have begun to explore methods that provide guidance beyond language tokens. For instance, some methods explore a better target, such as leveraging multi-token predictions to capture long context dependencies \citep{gloeckle2024better,liu2024deepseek} or predicting sequence embedding \citep{lee2024semiparametric}. Additionally, methods explore new types of inputs, e.g., using latents \citep{hao2024training} or self-generated thought as inputs \citep{zelikman2024quiet}, which have shown improving reasoning capabilities. Only recently, concept-level modeling using local encoder-decoder architectures has also been explored to represent a language at a higher abstraction level \citep{the2024large}. Other methods add extra tokens in the input space to increase computation at inference time \cite{nye2021show,wei2022chain,goyal2024think,lanchantin2024learning}. In contrast to other works, we propose a pretraining approach that integrates next token prediction with continuous concepts, connecting high level concepts with fine-grained token guidance. 


\textbf{Sparse Autoencoders (SAEs).} SAEs extend the autoencoder by enforcing sparsity constraints in the latent space \citep{lee2006efficient}. The features learned by SAEs are often interpretable and disentangled, making them useful across various domains, including language modeling \citep{bricken2023monosemanticity}. Additionally, SAEs have gained attention in mechanistic interpretability due to their ability to capture coherent semantic concepts \citep{marks2024sparse}. This property has enabled practical advancements in identifying and manipulating semantic concepts and facilitating steering for controlled model outputs \citep{lieberum2024gemma}. Among SAE variants, TopK SAEs \citep{makhzani2014k} enforce explicit sparsity using a TopK activation function, demonstrating effectiveness even for large models \citep{gao2024scaling}. In this work, we leverage SAE and, to the best of our knowledge, are the first to apply it to LLM pretraining, achieving strong performance while enhancing the interpretability and controllability of the trained model.

\textbf{Knowledge distillation (KD).} 
Our method can also be related to KD, i.e., transfers the expertise of a teacher model to a student model to enhance performance \citep{hinton2015distilling,zagoruyko2017paying}, as \sname extracts high-level semantic features from a pretrained model which is used to train a base model. Recently, KD for LLMs has garnered increasing attention, leveraging knowledge from a teacher to improve the generative and encoding capabilities of a student \citep{sanh2019distilbert,ko2024distillm}. Especially, applying KD to LLM pretraining remains challenging due to the massive token scales (billions to trillions), forcing most current methods to resort to naive token-level probability matching  \citep{team2024gemma,gu2024miniplm}. Additionally, while pretrained models contain a vast amount of learned information and are thus beneficial to use, reusing knowledge from smaller teacher models remains challenging \citep{burns2023weak}. In this work, we show \sname can even leverage the concept extracted from small models to train a large model showing weak-to-strong supervision.