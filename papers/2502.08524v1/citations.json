[
  {
    "index": 0,
    "papers": [
      {
        "key": "gloeckle2024better",
        "author": "Gloeckle, Fabian and Idrissi, Badr Youbi and Rozi{\\`e}re, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel",
        "title": "Better \\& faster large language models via multi-token prediction"
      },
      {
        "key": "liu2024deepseek",
        "author": "DeepSeek-AI",
        "title": "Deepseek-v3 technical report"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lee2024semiparametric",
        "author": "Lee, Hyunji and Kim, Doyoung and Jun, Jihoon and Joo, Sejune and Jang, Joel and On, Kyoung-Woon and Seo, Minjoon",
        "title": "Semiparametric Token-Sequence Co-Supervision"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hao2024training",
        "author": "Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong",
        "title": "Training large language models to reason in a continuous latent space"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zelikman2024quiet",
        "author": "Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D",
        "title": "Quiet-star: Language models can teach themselves to think before speaking"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "the2024large",
        "author": "LCM and Barrault, Lo{\\\"\\i}c and Duquenne, Paul-Ambroise and Elbayad, Maha and Kozhevnikov, Artyom and Alastruey, Belen and Andrews, Pierre and Coria, Mariano and Couairon, Guillaume and Costa-juss{\\`a}, Marta R and others",
        "title": "Large Concept Models: Language Modeling in a Sentence Representation Space"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "nye2021show",
        "author": "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others",
        "title": "Show your work: Scratchpads for intermediate computation with language models"
      },
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      },
      {
        "key": "goyal2024think",
        "author": "Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh",
        "title": "Think before you speak: Training language models with pause tokens"
      },
      {
        "key": "lanchantin2024learning",
        "author": "Lanchantin, Jack and Toshniwal, Shubham and Weston, Jason and Sukhbaatar, Sainbayar and others",
        "title": "Learning to reason and memorize with self-notes"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lee2006efficient",
        "author": "Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew",
        "title": "Efficient sparse coding algorithms"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "bricken2023monosemanticity",
        "author": "Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher",
        "title": "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "marks2024sparse",
        "author": "Marks, Samuel and Rager, Can and Michaud, Eric J and Belinkov, Yonatan and Bau, David and Mueller, Aaron",
        "title": "Sparse feature circuits: Discovering and editing interpretable causal graphs in language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lieberum2024gemma",
        "author": "Lieberum, Tom and Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Sonnerat, Nicolas and Varma, Vikrant and Kram{\\'a}r, J{\\'a}nos and Dragan, Anca and Shah, Rohin and Nanda, Neel",
        "title": "Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "makhzani2014k",
        "author": "Makhzani, Alireza and Frey, Brendan",
        "title": "K-sparse autoencoders"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "gao2024scaling",
        "author": "Gao, Leo and la Tour, Tom Dupr{\\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey",
        "title": "Scaling and evaluating sparse autoencoders"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hinton2015distilling",
        "author": "Geoffrey Hinton and Oriol Vinyals and Jeff Dean",
        "title": "Distilling the Knowledge in a Neural Network"
      },
      {
        "key": "zagoruyko2017paying",
        "author": "Zagoruyko, Sergey and Komodakis, Nikos",
        "title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "sanh2019distilbert",
        "author": "Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf",
        "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
      },
      {
        "key": "ko2024distillm",
        "author": "Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young",
        "title": "Distillm: Towards streamlined distillation for large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "team2024gemma",
        "author": "Gemma Team",
        "title": "Gemma 2: Improving open language models at a practical size"
      },
      {
        "key": "gu2024miniplm",
        "author": "Gu, Yuxian and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie",
        "title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "burns2023weak",
        "author": "Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others",
        "title": "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision"
      }
    ]
  }
]