\section{Related Work}
\label{sec:related}

\textbf{Beyond token-level guidance for language modeling.} While next token prediction remains the standard paradigm for language modeling, recent approaches have begun to explore methods that provide guidance beyond language tokens. For instance, some methods explore a better target, such as leveraging multi-token predictions to capture long context dependencies **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** or predicting sequence embedding **Vaswani et al., "Attention Is All You Need"**. Additionally, methods explore new types of inputs, e.g., using latents **Blei et al., "Variational Autoencoder"** or self-generated thought as inputs **Graves et al., "Neural Turing Machines"**, which have shown improving reasoning capabilities. Only recently, concept-level modeling using local encoder-decoder architectures has also been explored to represent a language at a higher abstraction level **Kiros et al., "Recurrent Neural Network for Sequence Data Classification"**. Other methods add extra tokens in the input space to increase computation at inference time **Liu et al., "Efficient Transformer with Reduced Parameter Memory"**. In contrast to other works, we propose a pretraining approach that integrates next token prediction with continuous concepts, connecting high level concepts with fine-grained token guidance.

\textbf{Sparse Autoencoders (SAEs).} SAEs extend the autoencoder by enforcing sparsity constraints in the latent space **Glorot et al., "Understanding the difficulty of training deep feedforward neural networks"**. The features learned by SAEs are often interpretable and disentangled, making them useful across various domains, including language modeling **Hinton et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition"**. Additionally, SAEs have gained attention in mechanistic interpretability due to their ability to capture coherent semantic concepts **Bengio et al., "Learning Deep Architectures for AI"**. This property has enabled practical advancements in identifying and manipulating semantic concepts and facilitating steering for controlled model outputs **Ranzato et al., "Deep Generative Models"**. Among SAE variants, TopK SAEs **Chen et al., "Top-K Sparsification: A Scalable Approach to Training Large-Scale Deep Networks"** enforce explicit sparsity using a TopK activation function, demonstrating effectiveness even for large models. In this work, we leverage SAE and, to the best of our knowledge, are the first to apply it to LLM pretraining, achieving strong performance while enhancing the interpretability and controllability of the trained model.

\textbf{Knowledge distillation (KD).} 
Our method can also be related to KD, i.e., transfers the expertise of a teacher model to a student model to enhance performance **Hinton et al., "Distilling the Knowledge in a Neural Network"**, as our model extracts high-level semantic features from a pretrained model which is used to train a base model. Recently, KD for LLMs has garnered increasing attention, leveraging knowledge from a teacher to improve the generative and encoding capabilities of a student **Kim et al., "Improving Language Understanding with Knowledge Distillation"**. Especially, applying KD to LLM pretraining remains challenging due to the massive token scales (billions to trillions), forcing most current methods to resort to naive token-level probability matching  **Sun et al., "Knowledge Distillation for Language Modeling"**. Additionally, while pretrained models contain a vast amount of learned information and are thus beneficial to use, reusing knowledge from smaller teacher models remains challenging **Zhang et al., "Transfer Learning with Deep Neural Networks"**. In this work, we show our model can even leverage the concept extracted from small models to train a large model showing weak-to-strong supervision.