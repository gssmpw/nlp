\section{Related Work}
\label{sec:related}

\textbf{Beyond token-level guidance for language modeling.} While next token prediction remains the standard paradigm for language modeling, recent approaches have begun to explore methods that provide guidance beyond language tokens. For instance, some methods explore a better target, such as leveraging multi-token predictions to capture long context dependencies ____ or predicting sequence embedding ____. Additionally, methods explore new types of inputs, e.g., using latents ____ or self-generated thought as inputs ____, which have shown improving reasoning capabilities. Only recently, concept-level modeling using local encoder-decoder architectures has also been explored to represent a language at a higher abstraction level ____. Other methods add extra tokens in the input space to increase computation at inference time ____. In contrast to other works, we propose a pretraining approach that integrates next token prediction with continuous concepts, connecting high level concepts with fine-grained token guidance. 


\textbf{Sparse Autoencoders (SAEs).} SAEs extend the autoencoder by enforcing sparsity constraints in the latent space ____. The features learned by SAEs are often interpretable and disentangled, making them useful across various domains, including language modeling ____. Additionally, SAEs have gained attention in mechanistic interpretability due to their ability to capture coherent semantic concepts ____. This property has enabled practical advancements in identifying and manipulating semantic concepts and facilitating steering for controlled model outputs ____. Among SAE variants, TopK SAEs ____ enforce explicit sparsity using a TopK activation function, demonstrating effectiveness even for large models ____. In this work, we leverage SAE and, to the best of our knowledge, are the first to apply it to LLM pretraining, achieving strong performance while enhancing the interpretability and controllability of the trained model.

\textbf{Knowledge distillation (KD).} 
Our method can also be related to KD, i.e., transfers the expertise of a teacher model to a student model to enhance performance ____, as \sname extracts high-level semantic features from a pretrained model which is used to train a base model. Recently, KD for LLMs has garnered increasing attention, leveraging knowledge from a teacher to improve the generative and encoding capabilities of a student ____. Especially, applying KD to LLM pretraining remains challenging due to the massive token scales (billions to trillions), forcing most current methods to resort to naive token-level probability matching  ____. Additionally, while pretrained models contain a vast amount of learned information and are thus beneficial to use, reusing knowledge from smaller teacher models remains challenging ____. In this work, we show \sname can even leverage the concept extracted from small models to train a large model showing weak-to-strong supervision.