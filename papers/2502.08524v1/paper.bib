@STRING{NeurIPS = "Advances in Neural Information Processing Systems"}
@STRING{ICML = "International Conference on Machine Learning"}
@STRING{ICLR = "International Conference on Learning Representations"}
@STRING{CVPR = "IEEE Conference on Computer Vision and Pattern Recognition"}
@STRING{CVPRW = "IEEE Conference on Computer Vision and Pattern Recognition Workshops"}
@STRING{ICCV = "IEEE International Conference on Computer Vision"}
@STRING{ECCV = "European Conference on Computer Vision"}
@STRING{BMVC = "British Machine Vision Conference"}
@STRING{WACV = "IEEE Winter Conference on Applications of Computer Vision"}
@STRING{ACL = "Annual Conference of the Association for Computational Linguistics"}
@STRING{EMNLP = "Conference on Empirical Methods in Natural Language Processing"}
@STRING{NAACL = "Annual Conference of the North American Chapter of the Association for Computational Linguistics"}
@STRING{AAAI = "AAAI Conference on Artificial Intelligence"}
@STRING{IJCAI = "International Joint Conferences on Artificial Intelligence"}
@STRING{SIGKDD = "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"}
@STRING{WWW = "International World Wide Web Conferences"}
@STRING{CORL = "Conference on Robot Learning"}
@STRING{COLM = "Conference on Language Modeling"}
@STRING{TMLR = "Transactions on Machine Learning Research"}
@STRING{JMLR = "Journal of Machine Learning Research"}
@STRING{ICASSP = "IEEE International Conference on Acoustics, Speech and Signal Processing"}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle=NeurIPS,
  year={2017}
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  booktitle=ACL,
  year={2016}
}

@inproceedings{ko2024distillm,
  title={Distillm: Towards streamlined distillation for large language models},
  author={Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young},
  booktitle=ICML,
  year={2024}
}

@inproceedings{gu2023knowledge,
  title={Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle=ICLR,
  year={2023}
}

@article{cunningham2023sparse,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@article{bricken2023monosemanticity,
    title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
    author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
    year={2023},
    journal={Transformer Circuits Thread},
    note={https://transformer-circuits.pub/2023/monosemantic-features/index.html},
}

@article{templeton2024scaling,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html},
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{the2024large,
  title={Large Concept Models: Language Modeling in a Sentence Representation Space},
  author={LCM and Barrault, Lo{\"\i}c and Duquenne, Paul-Ambroise and Elbayad, Maha and Kozhevnikov, Artyom and Alastruey, Belen and Andrews, Pierre and Coria, Mariano and Couairon, Guillaume and Costa-juss{\`a}, Marta R and others},
  journal={arXiv preprint arXiv:2412.08821},
  year={2024}
}

@article{hao2024training,
  title={Training large language models to reason in a continuous latent space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}

@article{yu2024distilling,
  title={Distilling system 2 into system 1},
  author={Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia},
  journal={arXiv preprint arXiv:2407.06023},
  year={2024}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  booktitle=NeurIPS,
  year={2019},
}

@inproceedings{gu2024knowledge,
  title={MiniLLM: Knowledge Distillation of Large Language Models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle=ICLR,
  year={2024}
}

@article{rawat2024little,
  title={A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs},
  author={Rawat, Ankit Singh and Sadhanala, Veeranjaneyulu and Rostamizadeh, Afshin and Chakrabarti, Ayan and Jitkrittum, Wittawat and Feinberg, Vladimir and Kim, Seungyeon and Harutyunyan, Hrayr and Saunshi, Nikunj and Nado, Zachary and others},
  journal={arXiv preprint arXiv:2410.18779},
  year={2024}
}

@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle=NeurIPS,
  year={2020}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{zhou2024webarena,
  title={Webarena: A realistic web environment for building autonomous agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and others},
  booktitle=ICLR,
  year={2024}
}

@article{deng2023implicit,
  title={Implicit chain of thought reasoning via knowledge distillation},
  author={Deng, Yuntian and Prasad, Kiran and Fernandez, Roland and Smolensky, Paul and Chaudhary, Vishrav and Shieber, Stuart},
  journal={arXiv preprint arXiv:2311.01460},
  year={2023}
}

@inproceedings{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  booktitle=NeurIPS,
  year={2022}
}

@inproceedings{rajamanoharan2024improving,
  title={Improving dictionary learning with gated sparse autoencoders},
  author={Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  booktitle=NeurIPS,
  year={2024}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@inproceedings{zagoruyko2017paying,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle=ICLR,
  year={2017}
}

@inproceedings{lee2006efficient,
  title={Efficient sparse coding algorithms},
  author={Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew},
  booktitle=NeurIPS,
  year={2006}
}

@article{yun2021transformer,
  title={Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
  author={Yun, Zeyu and Chen, Yubei and Olshausen, Bruno A and LeCun, Yann},
  journal={arXiv preprint arXiv:2103.15949},
  year={2021}
}

@inproceedings{shrikumar2016not,
  title={Not just a black box: Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Shcherbina, Anna and Kundaje, Anshul},
  booktitle=ICML,
  year={2016}
}

@inproceedings{sundararajan2017axiomatic,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle=ICML,
  year={2017},
}

@article{baehrens2010explain,
  title={How to explain individual classification decisions},
  author={Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\"u}ller, Klaus-Robert},
  journal={The Journal of Machine Learning Research},
  year={2010},
}

@inproceedings{wu2024reft,
  title={Reft: Representation finetuning for language models},
  author={Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D and Potts, Christopher},
  booktitle=NeurIPS,
  year={2024}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{paster2023openwebmath,
  title={Openwebmath: An open dataset of high-quality mathematical web text},
  author={Paster, Keiran and Santos, Marco Dos and Azerbayev, Zhangir and Ba, Jimmy},
  journal={arXiv preprint arXiv:2310.06786},
  year={2023}
}

@inproceedings{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle=ACL,
  year={2016}
}

@inproceedings{merity2017pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle=ICLR,
  year={2017}
}

@inproceedings{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle=ACL,
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle=AAAI,
  year={2020}
}

@inproceedings{sap2019social,
    title = {Social {IQ}a: Commonsense Reasoning about Social Interactions},
    author = {Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin},
    booktitle = EMNLP,
    year={2019}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{sakaguchi2020winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  booktitle=AAAI,
  year={2020},
}

@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Gemma Team},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{gu2024miniplm,
  title={MiniPLM: Knowledge Distillation for Pre-Training Language Models},
  author={Gu, Yuxian and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  journal={arXiv preprint arXiv:2410.17215},
  year={2024}
}

@inproceedings{goyal2024think,
  title={Think before you speak: Training language models with pause tokens},
  author={Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  booktitle=ICLR,
  year={2024}
}

@article{Olshausen1996EmergenceOS,
  title={Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  author={Bruno A. Olshausen and David J. Field},
  journal={Nature},
  year={1996},
}

@article{mairal2014sparse,
  title={Sparse modeling for image and vision processing},
  author={Mairal, Julien and Bach, Francis and Ponce, Jean and others},
  journal={Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  year={2014},
}

@inproceedings{makhzani2014k,
  title={K-sparse autoencoders},
  author={Makhzani, Alireza and Frey, Brendan},
  booktitle=ICLR,
  year={2014}
}

@article{lieberum2024gemma,
  title={Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2},
  author={Lieberum, Tom and Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Sonnerat, Nicolas and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Dragan, Anca and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2408.05147},
  year={2024}
}

@article{marks2024sparse,
  title={Sparse feature circuits: Discovering and editing interpretable causal graphs in language models},
  author={Marks, Samuel and Rager, Can and Michaud, Eric J and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  journal={arXiv preprint arXiv:2403.19647},
  year={2024}
}

@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
  year={2018},
}
  % url={https://api.semanticscholar.org/CorpusID:49313245}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{xuan2023evaluation,
  title={Evaluation of ChatGPT and Microsoft Bing AI Chat Performances on Physics Exams of Vietnamese National High School Graduation Examination},
  author={Xuan-Quy, Dao and Ngoc-Bich, Le and Xuan-Dung, Phan and Bac-Bien, Ngo and The-Duy, Vo},
  journal={arXiv preprint arXiv:2306.04538},
  year={2023}
}

@article{gao2023assistgpt,
  title={AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn},
  author={Gao, Difei and Ji, Lei and Zhou, Luowei and Lin, Kevin Qinghong and Chen, Joya and Fan, Zihan and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2306.08640},
  year={2023}
}

@inproceedings{bachmann2024pitfalls,
  title={The pitfalls of next-token prediction},
  author={Bachmann, Gregor and Nagarajan, Vaishnavh},
  booktitle=ICML,
  year={2024}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence},
  author={LeCun, Yann},
  journal={Open Review},
  year={2022}
}

@inproceedings{gloeckle2024better,
  title={Better \& faster large language models via multi-token prediction},
  author={Gloeckle, Fabian and Idrissi, Badr Youbi and Rozi{\`e}re, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
  booktitle=ICML,
  year={2024}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@inproceedings{zelikman2024quiet,
  title={Quiet-star: Language models can teach themselves to think before speaking},
  author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
  booktitle=COLM,
  year={2024}
}

@inproceedings{yang2024large,
  title={Do Large Language Models Latently Perform Multi-Hop Reasoning?},
  author={Yang, Sohee and Gribovskaya, Elena and Kassner, Nora and Geva, Mor and Riedel, Sebastian},
  booktitle=ACL,
  year={2024}
}

@inproceedings{lee2024semiparametric,
  title={Semiparametric Token-Sequence Co-Supervision},
  author={Lee, Hyunji and Kim, Doyoung and Jun, Jihoon and Joo, Sejune and Jang, Joel and On, Kyoung-Woon and Seo, Minjoon},
  booktitle=ACL,
  year={2024}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{sutskever2014sequence,
  title={Sequence to Sequence Learning with Neural Networks},
  author={Sutskever, I},
  journal={arXiv preprint arXiv:1409.3215},
  year={2014}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{lanchantin2024learning,
  title={Learning to reason and memorize with self-notes},
  author={Lanchantin, Jack and Toshniwal, Shubham and Weston, Jason and Sukhbaatar, Sainbayar and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}