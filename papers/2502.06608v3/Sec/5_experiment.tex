\section{Experiments}
% \clearpage

\subsection{Implementation Details}\label{sec:Implementation_detail}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{Images/demo_comparison.pdf}
    \vspace{-2.5em}
    \caption{Comparison of 3D generation performance of \method{} and other previous state-of-the-art methods under the same image input.}
    \label{fig:demo_comparison}
    \vspace{-1em}
\end{figure*}


The shape generation experiments are divided into two parts.
% : \method{} experiments and ablation experiments.
In the \method{} experiment, we progressively scaled both resolution and model size.
First, we trained a 1.5B parameter model on a 2M dataset with a latent resolution of 512 tokens, using a learning rate of 1e-4 for 700k steps. 
Next, we switched to a latent resolution of 2048 tokens and continued training for an additional 300k steps with a learning rate of 5e-5. 
Finally, to scale up, we expanded the model parameters to 4B using MoE and increased the latent resolution to 4096 tokens. 
Training resumed on 1M high-quality dataset with a learning rate of 1e-5 for 100k steps. 
The batch size of the three processes is set to 16, 10, and 8 per GPU respectively.
The entire training process took approximately 3 weeks across 160 A100 GPUs.

In the ablation experiments, 
we still use a small dataset (180K) filtered from Objaverse and a 975M parameter model for training. For the non-scaling ablation experiments, as shown in Tab.\ref{tab: diffusion_improvements_abaltion}, we trained the model with a latent resolution of 512 tokens, a learning rate of 1e-4, for about 300k steps, over approximately 3 days on 32 A100 GPUs. For the scaling-up ablation experiments, as shown in rows 2-4 of Tab.\ref{tab: diffusion_scaling_abaltion}, we progressively continued training from the previous experiment with latent resolutions of 2048 tokens, 4096 tokens, and 4096 tokens with the MoE model architecture, respectively, for an additional 100k steps. The learning rates were 5e-5, 1e-5, and 1e-5, respectively. These three scaling-up experiments took around 9 days in total on 32 A100 GPUs. For all ablation experiments, the batch size was set to 16 per-GPU.

It is also worth mentioning that during training, the image foreground is resized to a fixed ratio (90\%) and rotated around the center within a range of $[-10^\circ, 10^\circ]$ with a probability of 0.2. This setting enables the model to generalize well to various input images. During inference, the image is first detected for the foreground and then resized to the same ratio as the training foreground to obtain the best generation effect.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Images/radar_chart.png}
    \caption{Radar chart of the score of different methods in 5 aspects, including 3D plausibility, text-asset alignment, geometry details, texture details, texture-geometry coherency.}
    \label{fig:gpteval}
    \vspace{-1em}
\end{figure}


Following ~\cite{zhao2024michelangelo}, our VAE model adopts a network architecture with an 8-layer encoder and a 16-layer decoder. We use a larger decoder to enhance the ability to decode geometry from the latent space, without increasing the inference cost of the VAE during the flow model training stage.
The weights of surface normal loss $\lambda_{\text{sn}}$, eikonal regularization $\lambda_{\text{eik}}$, and KL-regularization $\lambda_{\text{kl}}$ are set to $10$, $0.1$, and $0.001$, respectively.
For each training data item, our model takes $20,480$ surface points as input and randomly samples $8,192$ near-surface points, $8,192$ volume points, and $8,192$ on-surface points for supervision.

The VAE experiments are divided into two parts as well: \method{} experiments and ablation experiments.
In the \method{} experiment, we train the VAE via SDF supervision with surface normal guidance and eikonal regularization using a learning rate of 5e-5 and a per-GPU batch size of 6 for 2.5M steps. The training process takes approximately 12 days on 32 A100 GPUs, then the VAE is used for the scale-up flow model training.
For the ablation experiment, we evaluate the VAE reconstruction quality from different experiment settings on a small dataset (180K filtered data from Objaverse). We train the VAE with different settings using a learning rate of 1e-4 and a per-GPU batch size of 8 for 286K steps on 8 A100 GPUs.


\subsection{Dataset, Metrics and Baselines}
\subsubsection{Dataset}
We train our model on Objaverse (-XL)~\cite{deitke2023objaverse,deitke2024objaverse}, the largest publicly available 3D dataset, which contains over 10 million unique 3D objects from various sources.
Since most of the data in Objaverse(-XL) cannot be directly used for our model's training, we apply the preprocessing steps introduced in Sec.\ref{sec:data} to prepare the training data, including scoring, filtering, orientation fixing, and training data reproducing.
After preprocessing, we get 2 million high-quality 3D objects. We compute the ground-truth SDF from sampled points based on the reproduced 3D models.
For single-image conditioned flow model training, we render from 8 random viewpoints in front of the 3D models with randomly sampled parameters from the camera's focal length, elevation, and azimuth ranges after orientation fixing but before data reproducing. Specifically, the range of elevation is $[-15^\circ, 30^\circ]$. 
The range of azimuth is $[0^\circ, 180^\circ]$.
The range of focal length is randomly selected from a discrete focal length list. The focal length list is [orthogonal, 50mm, 85mm, 135mm, 2 randomly selected from 35mm-65mm].
 


\subsubsection{Metrics}\label{sec:metric}
For flow model generation quality, we use FID\cite{heusel2017gans} to evaluate our model performance.
The original goal of FID is to evaluate the quality and photorealism of the generated shapes. We aim to introduce FID-related metrics to quantitatively assess the quality of the 3D models generated by \method{}. Typically, the input for generating 3D models is a 2D RGB image, while \method{} primarily generates textureless 3D models, creating an evaluation gap between the two.
To bridge this gap and enable evaluation on a consistent semantic level, we propose an improved evaluation process. Specifically, for 3D ground-truth models, we render paired RGB images $I_{gt}$ and normal maps $N_{gt}$ under the same viewpoints. The RGB images $I_{gt}$ are input into \method{} to generate 3D shapes, from which we render normal maps $N_{gen}$ from the same viewpoints as the input images. We then compute the Normal-FID between the generated normal maps $N_{gen}$ and the ground-truth normal maps $N_{gt}$ to evaluate the overall performance of \method{}.
In addition, the existing metrics, even the Normal-FID, are not flexible enough to adapt to various evaluation criteria and may not accurately reflect human preferences.
Thus, we also incorporate a recently introduced evaluation metric GPTEval3D~\cite{DBLP:conf/cvpr/WuYLZLGLW24} for further comparison.

We adopt some commonly used metrics to evaluate VAE reconstruction quality and flow model generation quality.
For VAE reconstruction quality, we focus on the accuracy of the reconstruction mesh from input points (with corresponding normals). We use the Chamfer distance, F-score with $0.02$ threshold, and the normal consistency as metrics.


\subsection{Quantitative and Qualitative Evaluation\protect\footnote{The original images in Fig.\ref{fig:teaser}, Fig.\ref{fig:demo_comparison}, Fig.\ref{fig:ablation_vis} and Fig.\ref{fig:geo_demo_show}, Fig.\ref{fig:texture_demo_show} are sourced from various 3D generation platforms, benchmarks (such as Rodin, Meshy, 3D Arena), and our own collections.}}


\subsubsection{Comparison with Different Methods in Visualization}
As shown in Fig.\ref{fig:demo_comparison}, we compare \method{} with the most popular image-to-3D generation methods\cite{tochilkin2024triposr, wei2024meshlrm, xu2024instantmesh,wang2024crm, li2024craftsman, DBLP:journals/corr/abs-2408-10198}. It is worth noting that for Craftsman\cite{li2024craftsman}, we used the online demo of Craftsman-1.5 on Huggingface for inference, which is a more advanced version of the Craftsman. 
The first row in the figure shows the original input image, while rows 2-7 present a comparison between the generation 3D models of other methods and \method{}. We compared their geometric quality by rendering normal maps. The results shown in the figure are all 3D normal maps rendered from the same viewpoint.
Notably, we preprocessed the original image by removing the background and fed the processed images to different open-source models via Huggingface demos for online inference and generation. Unlike previous works that typically compare 3D generation results on simple, standard images, we conducted comparisons on complex and widely varying cases.

Specifically, we evaluated the methods across five dimensions from left to right: (1) Semantic Consistency: \method{} generates 3D models with better semantic consistency, as shown in the first and second cases, with greater detail and semantic alignment. (2) Detail: The third and fourth cases demonstrate \method{}'s ability to capture finer details, such as clothing textures and accessories, providing richer visual fidelity. (3) Generalization: The fifth and sixth cases highlight \method{}'s ability to generate high-quality 3D models from both comic-style and cartoon-style images, showcasing its strong generalization capability. (4) Spatial Structure Generation: The seventh and eighth cases show \method{} excels at generating complex spatial structures, demonstrating superior spatial modeling capabilities. (5) Overall Performance: We compared \method{} with the latest and most advanced open-source methods, including both reconstruction and generation approaches, and it is evident that \method{} delivers significantly superior results, leaving a strong impression and outperforming previous approaches by a wide margin.

\subsubsection{Comparison with Different Methods in Metric}

Benefiting from the development of Large Multimodal Models (LMMs), we can easily leverage them to obtain evaluation results more aligned with human preferences. We adapt the evaluation script and test text prompt from \cite{DBLP:conf/cvpr/WuYLZLGLW24} and use the Claude3.5 instead of GPT-4Vision~\cite{DBLP:journals/corr/abs-2303-08774} as the Large Multimodal Models (LMMs).
We compare our results with various types of previous SOTA methods\cite{  
DBLP:conf/iclr/PooleJBM23, 
DBLP:conf/iclr/TangRZ0Z24, 
DBLP:conf/iccv/ChenCJJ23, 
li2023instant3d, 
DBLP:conf/cvpr/MetzerRPGC23, 
DBLP:conf/cvpr/Lin0TTZHKF0L23, 
DBLP:conf/iclr/ShiWYMLY24, 
DBLP:journals/corr/abs-2212-08751, 
DBLP:conf/nips/Wang00BL0023, 
DBLP:journals/corr/abs-2305-02463, 
DBLP:conf/cvpr/WangDLYS23, 
DBLP:conf/iclr/LiuLZLLKW24, 
long2024wonder3d}.
Specifically, we use an off-the-shelf text-to-image model Flux\cite{flux} to generate the input image for our method. 
Fig.\ref{fig:gpteval} demonstrates a Radar chart comparing the evaluation results across five aspects for different methods, as assessed by the LMM model.
The result indicates that our \method{} outperforms the other methods in all aspects.






\subsubsection{SOTA Performance of \method{}}
Fig.\ref{fig:teaser}, Fig.\ref{fig:geo_demo_show} and Fig.\ref{fig:texture_demo_show} showcase various image-to-3D results generated by \method{}. Notably, there are no duplicates among these cases, and the generated models have not undergone any post-processing (such as smoothing or removing floaters). Textured cases were produced through texture map generation, while non-textured cases were rendered from the original mesh. The process of texture generation is detailed in Sec.\ref{sec:texture}. The first image of each case in Fig.\ref{fig:geo_demo_show} and Fig.\ref{fig:texture_demo_show} is the input image, and the following four images are multi-view results rendered from the generated 3D model.

From these results, it is evident that \method{} delivers outstanding 3D model generation. Across the wide range of showcased cases—covering various complex structures, diverse styles, imaginative designs, multi-object compositions, thin surfaces, and richly detailed scenarios—\method{} consistently produces impressive 3D models. Achieving this level of performance is challenging for existing methods. The strong generalization highlights the advantages of large-scale datasets, while the rich detail and interpretive capability underscore the benefits of our high latent resolution and large model size, collectively reflecting \method{}’s state-of-the-art performance.

