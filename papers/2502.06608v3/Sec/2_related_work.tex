\section{Related Work} 
% \subsection{Score Distillation Sampling Based 3D Modeling}
\subsection{Lifting 2D Prior to 3D Modeling}
The diffusion model~\cite{DBLP:conf/nips/HoJA20} has demonstrated strong generative capabilities in image~\cite{rombach2022high,DBLP:conf/icml/RameshPGGVRCS21} or video~\cite{DBLP:conf/iclr/SingerPH00ZHYAG23,DBLP:conf/iccv/WuGWLGSHSQS23} generation. However, due to the limitations of high-quality 3D data, it has long been challenging to directly transfer the techniques from text and image generation to 3D tasks.
DreamFusion~\cite{DBLP:conf/iclr/PooleJBM23} pioneered the use of image diffusion priors for 3D generation by proposing a Score Distillation Sampling method enabling iterative optimization of the 3D representation via differentiable volume rendering~\cite{DBLP:conf/eccv/MildenhallSTBRN20}.
Subsequent work introduced numerous improvements in areas such as 3D represetation~\cite{DBLP:conf/cvpr/Lin0TTZHKF0L23,yi2024gaussiandreamer,DBLP:conf/iclr/TangRZ0Z24}, sampling strategy~\cite{DBLP:conf/nips/Wang00BL0023,DBLP:conf/cvpr/WangDLYS23,liang2024luciddreamer,DBLP:conf/aaai/Zou0CHSZ24}, incorporating additional geometric cues~\cite{DBLP:conf/iccv/TangWZZYM023,long2024wonder3d}, and multi-view image generation consistency~\cite{DBLP:conf/iclr/ShiWYMLY24,DBLP:conf/iclr/LiuLZLLKW24,DBLP:journals/corr/abs-2312-02201}.
Different from text-to-3D generation methods using off-the-shell text-to-image diffusion model, e.g., Stable Diffusion~\cite{rombach2022high}, many works explore to train a viewpoint-aware image diffusion based on input images~\cite{DBLP:conf/iccv/LiuWHTZV23,DBLP:conf/iccv/ChanNCBPLAMKW23,DBLP:journals/corr/abs-2310-15110}.
When image generation across multiple views becomes more consistent or with normal or depth generation, the 3D model can be directly optimized by pixel-level loss instead of time-consuming distillation sampling, resulting in 3D generation in a few minutes~\cite{long2024wonder3d,DBLP:journals/corr/abs-2405-20343,DBLP:journals/corr/abs-2405-11616}.

\subsection{Large 3D Reconstruction Modeling}
Unlike the previously introduced methods, which require a time-consuming optimization process lasting several minutes or even hours, various works propose to learn geometry with diverse representation types (e.g., point cloud~\cite{DBLP:conf/cvpr/FanSG17,DBLP:conf/cvpr/WuZXZC20}, voxel~\cite{DBLP:conf/eccv/GirdharFRG16,DBLP:conf/nips/0001WXSFT17}, mesh~\cite{DBLP:conf/eccv/WangZLFLJ18,DBLP:conf/cvpr/WorchelDHSFE22} or implicit field~\cite{DBLP:conf/cvpr/MeschederONNG19,DBLP:conf/nips/XuWCMN19,DBLP:conf/cvpr/YuYTK21}) from input images in a deterministic process, with encoder-decoder network architecture.
% Due to limitations of available high-quality and diverse 3D model datasets~\cite{DBLP:journals/corr/ChangFGHHLSSSSX15,DBLP:conf/icra/DownsFKKHRMV22,DBLP:conf/iccv/ReizensteinSHSL21,}, these methods could not achieve fine-grained reconstruction results and generalization ability for a long time.
Recently, equipped with a ginormous collection of high-quality 3D models in Objaverse (-XL)~\cite{deitke2023objaverse,deitke2024objaverse} as well as an advanced and scalable Transformer-based architecture~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}, Large Reconstruction Model (LRM)~\cite{hong2023lrm} and its many subsequent variants~\cite{xu2023dmv3d,wang2023pf,li2023instant3d,zhang2024gs,zou2024triplane,wei2024meshlrm,xu2024instantmesh,tochilkin2024triposr} has greatly promoted the development of reconstruction-based methods.
MeshFormer~\cite{DBLP:journals/corr/abs-2408-10198} additionally leverages sparse UNet to downsample the voxel for Transformer layers, leading to impressive reconstruction quality.
% Some other previous works~\cite{} have also shown the advance of the Transformer-based architecture.
One-2-3-45~\cite{DBLP:conf/nips/LiuXJCTXS23} first proposes combining a 2D image diffusion model and a multiview reconstruction model, achieving generation capabilities while maintaining fast reconstruction speed.
Bridged with the text-to-image and image-to-multi-view diffusion models, these multiview reconstruction methods can be easily extended to text-to-3D or image-to-3D generation tasks and achieve impressive results.
However, the inconsistency between input images from different views can lead to a decline in reconstruction quality, and the unobserved regions may yield blurred results.
Thus, these are only `reconstruction' methods rather than `generation' methods, fundamentally limiting the quality ceiling of such methods.

\subsection{3D Diffusion Modeling}
Training a 3D diffusion model for 3D generation is a natural idea that stems from advancements in the field of image~\cite{rombach2022high,DBLP:conf/icml/RameshPGGVRCS21} and video~\cite{DBLP:conf/iclr/SingerPH00ZHYAG23,DBLP:conf/iccv/WuGWLGSHSQS23} generation.
Many previous works train a diffusion model based on various 3D representations, such as voxel~\cite{DBLP:conf/cvpr/MullerSPBKN23}, point cloud~\cite{DBLP:conf/iccv/ZhouD021,DBLP:conf/nips/zengVWGLFK22,DBLP:conf/cvpr/Melas-Kyriazi0V23}, triplane~\cite{DBLP:conf/cvpr/ShueCPA0W23}, or Occupancy/SDF grid~\cite{DBLP:journals/tog/ZhengPWTLS23,DBLP:conf/siggrapha/HuiLHF22}.
Some other works utilize a VAE to transfer the original representation to a compact latent space, and then train a diffusion model on the latent space~\cite{zhang20233dshape2vecset,DBLP:conf/cvpr/ChengLTSG23}.
For a long time, these 3D diffusion methods have struggled to match the performance of the two major categories of approaches mentioned above due to the lack of large and high-quality 3D model datasets.
These methods are mostly trained on simple 3D datasets (e.g., ShapeNet~\cite{chang2015shapenet}), with limited generation ability and effectiveness, which hinders their practical application.
Recently, some researchers have attempted to train a latent 3D diffusion model based on a large amount of high-quality 3D models~\cite{zhang2024clay,wu2024direct3d,li2024craftsman,lan2024ln3diff,DBLP:journals/corr/abs-2403-02234} and have demonstrated impressive 3D generation results.
However, these methods still have limitations on \emph{high-fidelity} generation with \emph{image alignment}.
In this paper, we adopt a 3D representation with better geometry expression ability and improve the diffusion model architecture and training strategy, achieving state-of-the-art performance on 3D shape generation.




