\section{\method{}}
This section outlines the specific framework of the \method{} paradigm, which consists of three main parts: the flow-based generation architecture and sampling schedule ( Sec.\ref{sec:diffusion}); the scaling-up strategy (Sec.\ref{sec:scale_up}); the VAE architecture and supervision (Sec.\ref{sec:vae}).


\subsection{Rectified Flow Transformer}\label{sec:diffusion}




Leveraging a meticulously designed VAE architecture and robust supervision information, \method{}â€™s VAE, described in detail in Sec.\ref{sec:vae}, following extensive training on large-scale datasets, is capable of encoding arbitrary 3D shapes into multi-scale latent representations $X = L\times C$, $L\in \{512, 2048\}$, $C=64$, as well as decoding them back into 3D meshes. Drawing inspiration from models such as LDM~\cite{rombach2022high} and 3DShape2VecSet~\cite{zhang20233dshape2vecset}, we further train a rectified flow model on these latent representations, aiming to generate high-quality, semantically consistent 3D shapes under image-controlled conditions.


\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{Images/shapegen_pipeline.pdf}
\caption{Left: the overall architecture of \method{}. Middle: the detailed internal module of each block. Right: the detailed internal components of the MoE.}
\label{fig:TripoGen_pipeline}
% \vspace{-1em}
\end{figure*}


\subsubsection{Image-to-3D Flow Architecture}
Our flow architecture is inspired by DiT~\cite{peebles2023scalable} and 3DShape2VecSet~\cite{zhang20233dshape2vecset}, utilizing standard transformer blocks to construct the backbone. While this architecture has demonstrated success in class-conditional tasks on ImageNet~\cite{deng2009imagenet} and ShapeNet~\cite{chang2015shapenet}, we found that naively stacking multiple transformer blocks leads to suboptimal modeling capabilities due to insufficient information fusion between shallow and deep feature. Drawing inspiration from U-ViT~\cite{bao2023all} and the UNet structure in Stable Diffusion~\cite{rombach2022high}, we follow Michelangelo~\cite{zhao2024michelangelo} by introducing long skip residual connections between blocks to capture comprehensive feature information, enhancing the network's representational capacity.

As shown on the left side of Fig.\ref{fig:TripoGen_pipeline}, the backbone is divided into three parts: encoder blocks, a middle block, and decoder blocks, with an equal number $N$ of blocks in both the encoder and decoder. Each encoder block is connected to its corresponding decoder block via skip connections. Specifically, the output of the $i$-th encoder block is skip-connected to the output of the $(N-i)$-th decoder block.
Our flow backbone is composed of $2N+1$ transformer blocks with residual connections between them. In this setup, $N$ is 10, the hidden dimension $W$ is 2048, and each transformer block has 16 attention heads.
The entire flow architecture comprises approximately 1.5 billion parameters.
The following equation describes the flow architecture with skip-connections, where $\mathtt{DB}$ denotes the decoder block and $\mathtt{EB}$ denotes the encoder block.
\begin{gather}
\begin{aligned}
\mathbf{Z}_{\mathtt{DB}}^{\left(N-i\right)}=\mathtt{{DB}}^{\left(N-i\right)}&\left(\mathbf{Z}_{\mathtt{DB}}^{\left(N-i-1\right)}\right) + \mathtt{EB}^{\left(i\right)}\left(\mathbf{Z}_{\mathtt{EB}}^{\left(i-1\right)}\right), \\
&i \in \{0,1,...,N\}.
\end{aligned}
\end{gather}
Building on \method{}'s backbone, we designed a method to inject both timestep and image conditioning, enabling controllable 3D generation. For timestep $t$ we first encode it using the $\mathtt{Timesteps}$ layer from the diffusers library\cite{von-platen-etal-2022-diffusers}, followed by an $\mathtt{MLP}$ layer that projects it to the hidden dimension $W$, obtaining a $1 \times W$ feature. 
Similarly, for the input latent $X$, with dimensions $L\times C$ encoded by the VAE, we project it to the hidden dimension $W$ using an $\mathtt{MLP}$, producing a $L\times W$ feature. Following the design of Michelangelo\cite{zhao2024michelangelo} and CLAY\cite{zhang2024clay}, we concatenate the features of timestep $t$ and latent $X$, yielding a $(L+1) \times W$ feature, which is then fed into the flow backbone.

For image conditioning, Michelangelo\cite{zhao2024michelangelo} implements conditioning by concatenating global features extracted by CLIP\cite{radford2021learning} with the input latent $X$. However, using global image features and concatenation-based injection results in a loss of fine control over the generated 3D shapes. In contrast, CLAY\cite{zhang2024clay}, which pioneered explored the large-scale 3D generation, replaces the concatenation method with a cross-attention mechanism for injecting image information. CLAY trains a 1.5 billion parameter text conditioning base model at high computational cost (256 A800 GPUs over 15 days), then freezes the base model and trains an additional 352 million parameters for image conditioning via cross-attention with reduced computational cost (over 8 hours). However, while this approach shortens the training time for image conditioning compared to text conditioning, it limits the ability to fully update model parameters based on image information, resulting in challenges in achieving fine-grained consistency between the generated 3D shapes and the image condition.
Furthermore, captions generated from rendered images can introduce semantic gaps due to lighting, shadows, and textures, which deviate from the actual 3D geometry. Even when using tools like GPT-4V\cite{openai2023gpt4v} to generate captions for 3D models, accuracy issues persist. These added noise to the alignment between captions and shapes, slowing down training convergence and posing challenges to precision.

In contrast, our approach directly leverages CLIP-ViT-L/14\cite{radford2021learning} to extract global image features $I_{\text{global}}$ and DINOv2-Large\cite{oquab2023dinov2} to extract local image features $I_\text{local}$. In each flow block, both global and local features are injected simultaneously using separate cross-attention mechanisms. The outputs are then combined with the original input and passed to the next stage. This method allows the model to attend to both global and local image information in every block, enabling faster training convergence while maintaining strong detail consistency between the generated 3D model and the input image.

The process within each block of the flow architecture can be expressed by the following equation.
\begin{gather}
\mathbf{Z}=\mathtt{Concat}\left(\mathbf{X, t}\right) \\
\mathbf{Z}=\mathbf{Z} + \mathtt{SelfAttn}\left(\mathtt{Norm}\left(\mathbf{Z}\right)\right) \\
\mathbf{Z}=\mathbf{Z} + \mathtt{CrossAttn}\left(\mathtt{Norm}\left(\mathbf{Z}\right), I_\text{local}\right) + \\ 
\mathtt{CrossAttn}\left(\mathtt{Norm}\left(\mathbf{Z}\right), I_{\text{global}}\right) \\
\mathbf{Z}=\mathbf{Z} + \mathtt{FFN}\left(\mathtt{Norm}\left(\mathbf{Z}\right)\right)
\end{gather}





\subsubsection{Rectified Flow Based Generation}
We trained the 3D generation model using our designed flow architecture, exploring sampling strategies including DDPM, EDM, and Rectified Flow, and ultimately selected Rectified Flow for the final generative model.

DDPM leverages a Markov chain to establish a connection between Gaussian noise space and the data distribution, enabling high-quality data generation. Specifically, noise $\epsilon$ is progressively added to the data $x_0$, transforming it into a standard Gaussian distribution. The data sample $x_t$ at any time step $t$ can be expressed by the following equation:
\begin{equation}\label{eq:ddpm}
x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon
\end{equation}
Where 
$\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$, 
$\alpha_t = 1 - \beta_t$
and $\beta_t$ is the predefined noise scheduling parameter.
From the perspective of interpolation, DDPM models a relatively complex curved trajectory from $x_0$ to $x_t$.


EDM redesigns the noise schedule and sampling method, adopting a continuous-time framework to improve both the sampling speed and generation quality of DDPM. The data sample $x_t$ at any time step $t$ is modeled using the original data $x_0$ and noise $\epsilon$ as follows:
\begin{equation}\label{eq:edm}
x_t = x_0 + \sigma(t)\epsilon
\end{equation}
$\sigma(t)$ is a continuous noise standard deviation function, allowing for more flexible noise scheduling strategies, such as the power form $\left[ \left( \sigma_{\text{max}}^{1/\rho} - \sigma_{\text{min}}^{1/\rho} \right) t + \sigma_{\text{min}}^{1/\rho} \right]^\rho$, $\sigma_{\text{min}}$, where and $\sigma_{\text{max}}$ are the minimum and maximum noise standard deviation, and $\rho$ is the hyperparameter that controls the shape of the curve. 
EDM provides a more streamlined approach to modeling $x_t$ compared to DDPM. From an interpolation perspective, EDM also models a curved trajectory from $x_0$ to $x_t$.

Is there a simpler linear trajectory modeling process from $x_0$ to $x_t$? To explore this, we further investigated Rectified Flow, which learns a vector field to map the noise distribution to the data distribution. The data sample $x_t$ at any time step $t$ is modeled using the original data $x_0$ and noise $\epsilon$ as follows:
\begin{equation}\label{eq:rf}
x_t = t x_0 + (1-t)\epsilon
\end{equation}
This represents a simpler linear trajectory, offering a more efficient and streamlined approach compared to DDPM (Eq.\ref{eq:ddpm}) and EDM (Eq.\ref{eq:edm}).


Rectified flow's linear sampling simplifies network training, making it more efficient and stable, which we leverage to train our 3D flow model.
Additionally, drawing inspiration from SD3 logit-normal sampling, we increase the sampling weight for intermediate steps, as predictions for $t$ in the middle of the range $(0,1)$ are more challenging during Rectified Flow training. The sampling weight is adjusted using the following equation, where $m$ is the biasing location parameter and $s$ is the distribution width parameter.
\begin{equation}
\scalebox{0.95}{$
\pi_{\ln}(t; m, s) = \\
\frac{1}{s \sqrt{2\pi} t(1 - t)} \exp\left(-\frac{(\log(t/(1-t)) - m)^2}{2s^2}\right)
$}
\end{equation}

It is well-known that higher resolutions require more noise to sufficiently disrupt the signal. As resolution increases, the uncertainty in the noised latent at the same timestep decreases. Therefore, following SD3, we introduce \textit{Resolution-Dependent Shifting of Timestep} to adjust the timestep during both training and sampling. By remapping to a new timestep, we maintain the same level of uncertainty as with the original resolution.
We define the resolution of the first stage of our progressive training as the base resolution, denoted as 
$n$, with its timestep represented as $t_n$. The subsequent stage's resolution is defined as the fine-tune resolution, denoted as $m$, with its timestep represented as $t_m$. The relationship between $t_m$ and $t_n$ is expressed by the following equation.
\begin{equation}
t_m = \frac{\sqrt{\frac{m}{n}} t_n}{1 + \left(\sqrt{\frac{m}{n}} - 1\right) t_n}
\end{equation}

Leveraging Rectified Flow with logit-normal sampling and resolution-dependent shifting of timestep, we train our 3D flow model.




\subsection{Model and Resolution Scale-up Strategy.}\label{sec:scale_up}
Larger latent resolutions and more extensive models undoubtedly lead to performance improvements. To generate even better results, we aim to scale up both latent resolution and model size while minimizing training and inference costs. Specifically, we increased the latent resolution from 2048 to 4096, and scaled the model parameters from 1.5B to 4B using a Mixture-of-Experts (MoE).

Since the VAE training does not incorporate additional positional encoding in its input, and the varying number of query points used to learn the latent representations are downsampled from a fixed set of surface points, the VAE can generalize to resolutions beyond the training set. And higher number of query points (latent resolution) improves modeling capacity. This extrapolation ability eliminates the need for retraining the VAE, allowing us to directly encode and decode at a 4096 resolution using the VAE trained on $\{512, 2048\}$ resolutions. By leveraging this method to directly increase the latent resolution to 4096, we provide the flow model with finer geometric latent representations for training.

Additionally, to mitigate the risk of unstable training and potential loss divergence during mixed-precision training, \cite{dehghani2023scaling} recommend normalizing $Q$ and $K$ before attention operations. Following this approach, during fine-tuning at higher resolutions in our flow architecture, we apply learnable RMSNorm\cite{zhang2019root} to normalize $Q$ and $K$ within the transformer blocks.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/vae.pdf}
    \caption{\method{}'s transformer-based VAE architecture. The upper is the encoder and the lower is the decoder.}
    \label{fig:vae}
    % \vspace{-1em}
\end{figure}

Directly scaling a dense model is the most straightforward approach for increasing model parameters. While this enhances performance, it significantly increases the computational resource demands and inference latency. 
Rather than this approach, we opted to scale using a Mixture-of-Experts (MoE) architecture. This method not only boosts performance by increasing model parameters but also maintains nearly constant resource usage and inference latency due to the sparse activation of the network during inference.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Images/occ-vs-sdf.pdf}
    \caption{Comparison between model reconstruction based on Occupancy (top) and SDF (bottom).}
    \label{fig:occ-vs-sdf}
    % \vspace{-1em}
\end{figure}

As shown on the right side of Fig.\ref{fig:TripoGen_pipeline}, and following previous work\cite{riquelme2021scaling,fei2024scaling}, we extended the FFN (Feed-Forward Networks) within the transformer blocks using a Mixture-of-Experts (MoE) approach. Instead of a single $\mathtt{FFN}$ module in the dense network, $N$ parallel $\mathtt{FFN}$ expert models are employed, controlled by a gating module to scale up the model parameters. Specifically, the latent $X$ of length $L$ are distributed token-wise to different $\mathtt{FFN}$ experts by the gating module based on top-K probabilities and then reconcatenated to restore the original length $L$.
Inspired by DiT-MoE\cite{fei2024scaling}â€™s approach of sharing certain experts to capture common knowledge and balancing expert loss to reduce redundancy, we apply weighted activation of the top-2 experts in each token, introduce a shared expert branch across all tokens, and use an auxiliary loss to balance expert routing. 
Unlike DiT-MoE, we used the base modelâ€™s $\mathtt{FFN}$ architecture (a two-layer $\mathtt{MLP}$ with one $\mathtt{GELU}$ activation) for constructing $\mathtt{FFN}$ experts. 
Instead of training the MoE model from scratch, 
we initialized it from the base model, where the weights of the multiple $\mathtt{FFN}$ experts in each block were inherited from the corresponding $\mathtt{FFN}$ weights in the base model. Additionally, due to the shallow layers focusing on general features and the deeper layers capturing more object-specific details\cite{zeiler2014visualizing}, we limited MoE application to the final six layers of the decoder, where deep feature modeling is critical. 
This targeted scaling of parameters was applied to the most crucial part of the flow architecture. Under the action of MoE, we modified formula 17 into the following new formula.
\begin{gather}
\mathbf{Z}=\mathtt{Norm}\left(\mathbf{Z}\right), \\
\mathbf{Z}=\mathbf{Z} + \mathtt{Concat}\left(\mathtt{FFN}^{\left(i\right)}\left(\mathbf{Z}\right)\right), 
i \in \{1,2,...,N\}
\end{gather}
In our MoE scaling up, we used 8 expert models, activating the top 2 $\mathtt{FFN}$ experts per MoE block while sharing one $\mathtt{FFN}$. Additionally, MoE expansion was applied to the final 6 layers of the decoder, increasing the overall model parameters from 1.5B to about 4B.



% \vspace{-0.3em}
\subsection{3D Variational Autoencoder (VAE)}\label{sec:vae}
\subsubsection{3D Model Representation}
Most existing 3D shape generation works~\cite{zhang2024clay,li2024craftsman} adopt occupancy field or semi-continuous occupancy~\cite{wu2024direct3d} as the neural implicit representation for 3D model. For each query position $x \in \mathbb{R}^3$, these methods utilize a neural network $\mathcal{D}$ to predict the occupancy value $o$ from the latent features $f$, supervised by the ground-truth occupancy value $\hat{o}$ with Binary Cross Entropy (BEC) loss: 
\begin{gather}
    o=\mathcal{D}\left(x, f\right), \\
    \mathcal{L} = \mathbb{E}_{x\in \mathcal{R}^3}\left[\text{BCE}\left(o, \hat{o}\right)\right].
\end{gather}
Learning geometry through occupancy representation as a classification task is easier to train and converge compared to the signed distance function (SDF) as a regression task.
However, the occupancy representation has limited geometric representation capabilities compared to SDF, which provides more precise and detailed geometry encoding.
Additionally, models reconstructed using occupancy representation often exhibit noticeable aliasing artifacts and typically require further post-processing (e.g., smooth filter or super-sampling) to address these issues.
Without post-processing, these aliasing artifacts sometimes also impact the subsequent texture generation.
Fig.\ref{fig:occ-vs-sdf} shows some examples of geometry reconstruction and texture generation results based on occupancy and SDF, respectively.


Given these considerations, we adopt neural SDF as our 3D model representation. This method, built upon a set of latent tokens, provides a stronger geometric detail than occupancy-based approaches~\cite{zhang2024clay,li2024craftsman,wu2024direct3d}.
Specifically, we predict the SDF value $s$ of each query position as:
\begin{equation}
    s = \mathcal{D}\left(x, f\right).
\end{equation}
For efficiency, we employ the truncated signed distance function (TSDF) in our VAE model. In the following paragraph, we use $s$ to represent TSDF for simplicity.

\subsubsection{Geometry Learning with Surface Normal Guidance}
More importantly, SDF representation theoretically ensures the effectiveness of supervision in the gradient domain of the neural implicit field. 
We think geometric details are relevant to the gradient domain of the neural implicit field, which represents the higher-order information compared to the value domain of the implicit field.
Therefore, we apply surface normal guidance during VAE training to capture finer-grained geometric details, providing a better latent space for model sampling.
Specifically, in addition to the commonly used SDF loss, our approach also includes direct supervision of finer detailed geometry learning using the ground-truth surface normals and an additional eikonal regularization:
\begin{gather}
\mathcal{L}_{\text{vae}}=\mathcal{L}_{\text{sdf}}+\lambda_{\text{sn}}\mathcal{L}_{\text{sn}}+\lambda_{\text{eik}}\mathcal{L}_{\text{eik}}+\lambda_{\text{kl}}\mathcal{L}_{\text{kl}}, \\
\mathcal{L}_{\text{sdf}}=|s - \hat{s}| + \|s - \hat{s}\|_2^2 , \\
\mathcal{L}_{\text{sn}}=\left(1-\left<\frac{\nabla \mathcal{D}\left(x, f\right)}{\|\nabla \mathcal{D}\left(x, f\right)\|}, \hat{n}\right>\right), \\
\mathcal{L}_{\text{eik}}=\|\nabla \mathcal{D}\left(x, f\right)-1\|^2_2,
\end{gather}
where $\hat{n}$ is the ground-truth surface normal, $\left<\cdot,\cdot\right>$ denotes the cosine similarity of two vectors, $\mathcal{L}_\text{eik}$ is the eikonal regularization, and $\mathcal{L}_\text{kl}$ represents the KL-regularization in the latent space.
Unlike SDF loss, which involves sampling points near the surface and randomly throughout space, the surface-normal loss is applied exclusively to surface points, making it a more efficient method for supervising fine-grained geometry learning.


\subsubsection{Network Architecture}
Following the design of 3DShape2Vecset\cite{zhang20233dshape2vecset}, we choose the latent vector set as our latent representation, which encodes a point cloud into latent space and subsequently decodes a geometry function (i.e., SDF) from it.
To facilitate more efficient scaling up, we adopt a state-of-the-art transformer-based encoder-decoder architecture~\cite{zhang20233dshape2vecset,zhang2024clay,zhao2024michelangelo}.
Specifically, we choose the downsampled version in
3DShape2Vecset~\cite{zhang20233dshape2vecset} that subsamples $M$ points $\mathbf{X}'$ from the full set of surface points $\mathbf{X}$, and directly utilizes the point cloud itself as initial latent queries instead of learnable embeddings.
Then, the surface points information, encoded by concatenating positional embedding and surface normal, is integrated into latent queries via cross-attention, resulting in compact latent tokens $\mathbf{Z}$ rich in geometric information, as shown in the following:
\begin{gather}
\mathbf{Z_0}=\mathtt{CrossAttn}\left(\mathtt{PosEmb}\left(\mathbf{X}\right),\mathtt{PosEmb}\left(\mathbf{X}'\right)\right), \\
\mathbf{Z} = \mathtt{Linear}\left(\mathtt{SelfAttn}^{\left(i\right)}\left(\mathbf{Z_0}\right)\right), i \in \{0,1,...,L_{\text{enc}}\},
\end{gather}
where $\mathtt{CrossAttn}$ denotes a cross-attention layer, $\mathtt{SelfAttn}^{\left(i\right)}$ denotes the self-attention layers, and \text{Linear} is a linear layer.


After obtaining the latent representation, we can decode the signed distance value for each query position $x \in \mathbb{R}^3$:
\begin{gather}
\mathbf{\widetilde{Z}}=\mathtt{SelfAttn}^{\left(i\right)}\left(\mathtt{Linear}\left(\mathbf{Z}\right)\right), i \in \{0, 1, ..., L_{\text{dec}}\}, \\
s=\mathtt{CrossAttn}\left(\mathtt{PosEmb}\left(x\right), \mathbf{\widetilde{Z}}\right).
\end{gather}
Finally, the mesh of the 3D model can be extracted by applying Marching Cubes~\cite{DBLP:conf/siggraph/LorensenC87} at a given resolution.

To implement progressive flow model training for faster convergence, we follow \cite{zhang2024clay} to adopt a multi-resolution VAE with $M \in \{512, 2048\}$ tokens, where the VAE weights are shared across different resolutions.
This training strategy, combined with the position-encoding-free feature of the VAE transformer, provides the VAE with strong extrapolation capabilities, allowing it to directly infer the 3D models with higher-resolution (e.g., 4096) tokens without requiring additional fine-tuning.
% \TODO{Is this property due to multi-resolution training or the increased number of input points?}
Unlike previous works, which used only a few surface points (only 2048 or 8192 points)\cite{zhang2024clay} as the VAE input, we opted to use a denser surface point for each 3D model.
We think the purpose of the VAE is to capture as much geometric information of the 3D model as possible, rather than functioning as a sparse point cloud reconstruction task.
The more input points provided, the more geometric information is encoded in the latent space, resulting in higher-quality geometry being decoded.
