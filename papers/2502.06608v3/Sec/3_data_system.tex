\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{Images/data_process_pipeline.pdf}
\caption{Demonstration of the \method{} data-building system. I: Data scoring procedure; II: Data filtering procedure; III: Data fixing and augmentation procedure. IV: Field data producing procedure.}
\vspace{-1em}
\label{fig:data_process_pipeline}
\vspace{-1em}
\end{figure}




\section{Data-Building System.}\label{sec:data}
\method{} is trained on existing open-source datasets such as Objaverse (-XL)\cite{deitke2023objaverse, deitke2024objaverse} and ShapeNet\cite{chang2015shapenet}, which contains approximately $10$ million 3D data. Since most of these data are sourced from the Internet, their quality varies significantly, requiring extensive preprocessing to ensure suitability for training.
To overcome these challenges, \method{} developed a dedicated 3D data processing system that produces high-quality, large-scale datasets for model training.
As illustrated in Fig.\ref{fig:data_process_pipeline}, the system comprises four processing stages (Data Process I$\sim$IV), responsible for data scoring, filtering, fixing and augmentation, and field data producing, respectively.

\subsection{I: Data Scoring}
Each 3D model is scored, with only the high-score models advancing to the subsequent processing stages.
Specifically, we randomly selected approximately $10K$ 3D models and used Blender to render four different views of normal maps for each model. These multi-view normal maps are then manually evaluated by 10 professional 3D modelers, assigning scores on a scale from $1$ (lowest) to $5$ (highest).
Using this annotated data, we trained a linear regression-based scoring model concatenating their CLIP\cite{radford2021learning} and DINOv2\cite{oquab2023dinov2} features as input. This model was subsequently used to infer quality scores from the multi-view normal maps of all 3D models for filtering.

\subsection{II: Data Filtering}
After scoring, further filtering is applied to exclude models with large planar bases, rendering errors in animations, and those containing multiple objects.
Specifically, models with large planar bases are filtered by determining if different surface patches can be classified as a single plane, based on features composed of their centroid positions, normal vectors, and the area of the resulting plane. Blender identifies animated models, sets them to the first frame, and filters out any models that still exhibit rendering errors after being set. And models containing multiple objects are filtered by evaluating the proportion of the largest connected component on the opaque mask, along with the magnitude of the solidity of both the largest connected component and the entire mask.

\subsection{III: Data Fixing and Augmentation}
After data filtering, we perform the orientation fixing of character models to ensure they face forward. Specifically, we select 24 orientations around the x, y, and z axes, and for each, render images from six orthogonal views: front, back, left, right, top, and bottom. The DINOv2\cite{oquab2023dinov2} features from these six views are concatenated to train an orientation estimation model, which is then used to infer and fix the orientation of all character models. Additionally, for all untextured models, we render multi-view normal maps and use ControlNet++\cite{li2024controlnet++} to generate corresponding multi-view RGB data, which serve as conditional inputs during training.

\subsection{IV: Field Data Production}
% \subsubsection{Data process IV: Training Data Reproducing}
Although Objaverse (-XL)~\cite{deitke2023objaverse,deitke2024objaverse} contains a large amount of data, most of the models are unsuitable for direct training, even after processing steps such as scoring, filtering, and fixing. 
Since we adopt the neural implicit field as our 3D model representation, it's necessary to convert the original non-watertight mesh to watertight ones for computing geometry supervision (e.g., occupancy or SDF). 
Rather than using common methods like TSDF-fusion~\cite{DBLP:conf/ismar/NewcombeIHMKDKSHF11} or ManifoldPlus~\cite{DBLP:journals/corr/abs-1802-01698,DBLP:journals/corr/abs-2005-11621}, we are inspired by \cite{DBLP:journals/tog/WangLT22,zhang2024clay} to construct a Unsigned Distance Function (UDF) field with a resolution of $512^3$ grid from the original non-watertight mesh, and then apply Marching Cubes~\cite{DBLP:conf/siggraph/LorensenC87} to extract the iso-surface with a small threshold $\tau=\frac{3}{512}$. 
To remove interior structure for more efficient geometry learning, we follow \cite{zhang2024clay} by resetting the UDF value of the invisible grids to prevent the extraction of interior iso-surface before applying Marching Cubes. 
We then remove some small and invisible interior mesh components by calculating the area and the ambient occlusion ratio of each mesh component.
Finally, we uniformly sample surface points along with their normals, and randomly sample points both within the volume and near the surface.


