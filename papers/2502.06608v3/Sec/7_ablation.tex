\section{Ablation and Analysis}

\begin{table}[t]
\begin{center}
\caption{The ablation for flow model improvements. `Skip-C' is the skip-connection operation and `Sample-S' is the sample schedule.}
\vspace{1em}
\label{tab: diffusion_improvements_abaltion}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{| c | c | c| c|}
\hline
Condition & Skip-C & Sample-S & Normal-FID $\downarrow$\\
% of filter & $e_m$ &   $b_{ij}$ \\
\hline
Dinov2 & \ding{55} & R-Flow  & 10.69\\
\hline
\multirow{4}{*}{CLIP-Dinov2} & \ding{55} & R-Flow  & 10.61\\
\cline{2-4}
 & \ding{51} & DDPM  & 9.63\\
\cline{2-4}
 & \ding{51} & EDM  & 9.50\\
\cline{2-4}
 & \ding{51} & R-Flow  & 9.47 \\
\hline
\end{tabular}
}
\end{sc}
\end{center}
\vspace{-2em}
\end{table}
\subsection{Ablation for Flow Model}


To validate the effectiveness of the proposed flow model improvements and scaling-up strategies, we performed specific ablation experiments and comparative analyses for each improvement.
Using a further filtered $180K$ high-quality Objaverse dataset, we conducted ablation experiments following the training settings in \ref{sec:Implementation_detail}, and evaluated the results using the Normal-FID metric introduced in \ref{sec:metric}.

For the evaluation of Normal-FID, we selected $1K$ data samples from the $180K$ dataset as a dedicated test set for 3D generation performance validation, with the remaining data samples used for training. For the test set, we rendered each 3D ground-truth model's front view paired RGB and normal map using a $50mm$ camera focal length and a $10^\circ$ elevation (the test set rendering settings are included within the training set settings). The RGB images are used to generate 3D shapes, and the normal maps are compared with the normal maps rendered from the generated 3D shapes to calculate the Normal-FID.

Our flow model ablation experiment consists of two parts: flow model improvement training and flow model scaling up. For the flow model improvement experiments, as shown in Tab.\ref{tab: diffusion_improvements_abaltion}, we used a $975M$ parameter model with a latent resolution of $512$ tokens and trained for $300K$ steps for each experiment on the high-quality Objaverse dataset. We conducted comparative analyses on $Condition$, $Skip-connection$, and $Sampling-schedule$ improvements.
From the last three rows in Tab.\ref{tab: diffusion_improvements_abaltion}, we observe that R-Flow sampling yields better generation results compared to EDM and DDPM. Combined with its training efficiency, R-Flow demonstrates clear advantages in 3D generation tasks. Comparing rows 2 and 5 shows that the skip-connection operation significantly affects generation results, with the fusion of deep and shallow features improving flow modeling. Additionally, the comparison between the first two rows indicates that the CLIP condition also slightly improves generation results. 
From the overall quantitative results, the skip-connection operation has the most obvious effect among these ablations.

\begin{table}[t]
\begin{center}
\caption{The ablation for flow model scaling up.}
\vspace{1em}
\label{tab: diffusion_scaling_abaltion}
% \begin{small}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{| c| c|c|c|}
\hline
Dataset  & Token number & MoE &Normal-FID $\downarrow$\\
% of filter & $e_m$ &   $b_{ij}$ \\
\hline
\multirow{4}{*}{Objaverse}  &512 & \ding{55}& 9.47 \\
\cline{2-4}
  &2048 & \ding{55}& 8.38\\
\cline{2-4}
  &4096 & \ding{55}& 8.12\\
\cline{2-4}
  &4096 & \ding{51} & 7.94\\
\hline
\method{}  &4096 & \ding{51} & 3.36\\
\hline
\end{tabular}
}
\end{sc}
% \end{small}
\end{center}
\vspace{-2em}
\end{table}

For the flow model scaling-up experiments, as shown in rows 2-4 of Tab.\ref{tab: diffusion_scaling_abaltion}, we used a $975M$ parameter model with CLIP-DINOv2 dual-conditioning, skip-connection operations, and rectified-flow sampling schedule. These models are trained for a total of $300K$ steps on the high-quality Objaverse data to conduct comparative analyses on latent resolution and MoE. The last row of Tab.\ref{tab: diffusion_scaling_abaltion} represents our largest \method{} model, encompassing the largest data, model size, resolution, and training cost.
From the first three rows of Tab.\ref{tab: diffusion_scaling_abaltion} we observe that as the latent resolution increases, the generated results consistently improve, with the most significant improvement occurring from 512 to 2048 tokens. Comparing rows 3 and 4 shows the gains from increasing model parameters through MoE. 
Comparing rows 4 and 5 demonstrates the performance improvement achieved by increasing high-quality data size. When combined with the results from row 1, we can see that the improvement from the increased high-quality data size surpasses that from higher resolution.
Overall, the large-scale dataset, large model size, and high resolution contribute to significant performance improvements, allowing \method{} to achieve remarkable 3D generation results.

% The comparison between the last row and the others highlights the significant impact of large-scale dataset, large model size, and high resolution. With these improvements, \method{} achieves impressive 3D generation results.

\subsection{Ablation for VAE}

\begin{table}[t]
\begin{center}
\caption{The ablation of different VAE, including 3D representation, training supervision and training dataset. `Repr' refers to the type of 3D representation used, and `$\mathcal{L}_\text{sn}$' and `$\mathcal{L}_\text{eik}$' refer to surface normal loss and eikonal regularization respectively. The `Dataset' indicates whether a large (TripoSG) or a small dataset (Objaverse) is used. }
\vspace{1em}
\label{tab:vae}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{| c | c c c | c c c |}
\hline
Dataset & Repr. & $\mathcal{L}_\text{sn}$ & $\mathcal{L}_{eik}$ & Chamfer $\downarrow$ & F-score $\uparrow$ & N.C. $\uparrow$ \\
\hline
\multirow{4}{*}{Objaverse} & Occ & \ding{55} & \ding{55} & 4.59 & 0.999 & 0.952 \\
\cline{2-7}
 & SDF & \ding{55} & \ding{55} & 4.60 & 0.999 & 0.955 \\ 
\cline{2-7}
 & SDF & \ding{51} & \ding{55} & 4.56 & 0.999 & 0.956 \\
\cline{2-7} 
 & SDF & \ding{51} & \ding{51} & 4.57 & 0.999 & 0.957 \\
\hline 
\method{}& SDF & \ding{51} & \ding{51} & 4.51 & 0.999 & 0.958 \\
\hline 
\end{tabular}
}
\end{sc}
\end{center}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Images/vae-comparison.pdf}
    \caption{Qualitative comparison for the ablation of VAE with different types of 3D representation and training supervision.}
    \label{fig:vae-comparison}
    \vspace{-1em}
\end{figure}

To evaluate the effectiveness of neural SDF implicit representation with surface normal guidance, we experiment with different VAE model settings, including the formulation of neural implicit representation, training supervision, and training dataset.
Tab.\ref{tab:vae} demonstrates the qualitative results of VAE reconstruction quality with different training settings.
We can observe that the SDF representation, combined with surface normal guidance and eikonal regularization, improves the reconstruction quality and geometry details, achieving lower Chamfer distance and higher normal consistency compared to occupancy-based results.
As the amount of training data increases (as demonstrated by \method{}), the reconstruction quality of the VAE further improves.
Fig.\ref{fig:vae-comparison} provides qualitative comparisons between them.
Occupancy-based reconstruction results suffer from aliasing artifacts (highlighted by the blue box), thin structures, and floaters (highlighted by the red boxes).
While SDF representation avoids aliasing artifacts, there remains a gap in achieving high-quality reconstruction, particularly for thin-shell structures where performance may worsen. 
Incorporating surface normal guidance can result in sharper reconstructions with finer details. However, over-emphasizing surface normal guidance during training introduces slight aliasing artifacts (as seen in the first row of Fig.\ref{fig:vae-comparison}), which can be mitigated by introducing eikonal regularization.



\begin{table}[t]
\begin{center}
\caption{The ablation for data quality and quantity.}
\vspace{1em}
\label{tab: data_abaltion}
% \begin{small}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{| c| c|c|c|}
\hline
Dataset  & Size & Data-building System &Normal-FID $\downarrow$\\
% of filter & $e_m$ &   $b_{ij}$ \\
\hline
\multirow{2}{*}{Objaverse}  &$800K$ & \ding{55}& 11.61\\
\cline{2-4}
  &$180K$ & \ding{51}& 9.47 \\
\hline
\method{}  &$2M$ & \ding{51} & 5.81\\
\hline
\end{tabular}
}
\end{sc}
% \end{small}
\end{center}
\vspace{-2em}
\end{table}
\subsection{Ablation for Data-Building System}
To demonstrate the effectiveness of the data-building system proposed by \method{}, we implement ablation experiments on both data quality and quantity. Using the optimal R-Flow training settings (first row of Tab.\ref{tab: diffusion_improvements_abaltion}), we replaced the $180K$ Objaverse dataset produced by \method{} with the original $800K$ Objaverse dataset, which had not undergone scoring, filtering, orientation fixing, untextured model processing, or internal processing of the converted watertight model. This experiment demonstrates the effect of data quality. Similarly, under the same R-Flow settings, we expanded the high-quality dataset from $180K$ Objaverse to $2M$ \method{} to evaluate the effect of data quantity.

As shown in the first two rows of Tab.\ref{tab: data_abaltion}, although our data-building system reduced the $800K$ Objaverse dataset to $180K$, the improved data quality resulted in better generation results, demonstrating that, when training with in-the-wild data, quality outweighs quantity. Furthermore, as seen in last two rows of Tab.\ref{tab: data_abaltion}, increasing the high-quality dataset from $180K$ to $2M$ led to a significant boost in generation performance, showing that with high-quality data, scaling up data size is crucial for achieving better results. 
Additionally, the overall quantitative results in the Tab.\ref{tab: data_abaltion} show that the performance improvement gained from $2M$ high-quality data size is greater than that from improving data quality alone. Furthermore, after enhancing data quality, performance continues to improve with an increase in data size, without encountering a bottleneck at the current training scale.


\subsection{The Visualization for Flow Model Ablation}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Images/ablation_vis.pdf}
    \vspace{-1em}
    \caption{Visualization results of the flow model ablation experiments. `DBS' is the abbreviation for Data-Building System.
    }
    % \caption{Caption: \textcolor{red}{vis corresponding, 180k-4096 only for ablation. quality data scaling better than resolution}}
    \vspace{-1em}
    \label{fig:ablation_vis}
    % \vspace{-1em}
\end{figure*}

In addition to the quantitative results, we also performed a visualization analysis of the core experiments, as shown in Fig.\ref{fig:ablation_vis}. The rows 1, 2, 4 correspond to the three experimental results in Tab.\ref{tab: data_abaltion}, while the row 3 corresponds to the row 4 of results in Tab.\ref{tab: diffusion_scaling_abaltion}. The visualization reveals several insights consistent with the quantitative results: (1) Data quality is more important than the size of raw in-the-wild data (row 1 vs. row 2). (2) The improvement from increasing high-quality data size is more obvious than the improvement from resolution (row 2 vs. row 3 vs. row 4). (3) Increasing the size of high-quality data ($2M$) provides a greater boost to performance than merely improving data quality. After improving data quality, performance continues to improve with increased data size without encountering bottlenecks at the current training scale.






