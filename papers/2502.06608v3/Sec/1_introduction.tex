\textbf{Key words}: 3D Generation, Rectified Flow, Image-to-3D 
\section{Introduction}
Recent advancements in large-scale visual datasets \cite{schuhmann2022laion, bain2021frozen, wang2023internvid} have propelled remarkable progress in generative models. These models effectively compress high-dimensional visual data, such as images and videos, into latent spaces, enabling the generation of high-quality visual content conditioned on various input modalities. State-of-the-art generative AI models, including SD3~\cite{esser2024scaling}, FLUX~\cite{flux}, and Sora~\cite{videoworldsimulators2024}, exemplify this capability, producing strikingly realistic images and videos from diverse conditional inputs. This breakthrough has revolutionized human visual creation, opening new avenues for artistic expression and content generation.

In the domain of 3D content creation, the pursuit of high-quality, production-ready 3D generation remains a primary objective for researchers, artists, and designers. Substantial progress has been made in generating 3D models from single images, with approaches broadly categorized into two paradigms: large-scale reconstruction-based methods~\cite{hong2023lrm,li2023instant3d,wang2023pf,xu2023dmv3d,zhang2024gs,wei2024meshlrm,xu2024instantmesh,wang2024crm,tochilkin2024triposr,zou2024triplane} and diffusion-based methods~\cite{zhang20233dshape2vecset,zhao2024michelangelo,zhang2024clay,wu2024direct3d,li2024craftsman}.
Large-scale reconstruction-based methods primarily utilize a network to regress the 3D model in a deterministic way. While effective, these approaches often struggle with inconsistencies in overlapping regions from multiple input views (which can be generated from a single view by multi-view diffusion models~\cite{shi2023mvdream,wang2023imagedream,liu2023unidream}) and exhibit artifacts in occluded areas.
Conversely, diffusion-based methods train on 3D representations or latent representations compressed by Variational AutoEncoders (VAEs). As generative rather than regression methods, they circumvent some challenges inherent to reconstruction approaches. However, current methods predominantly rely on occupancy representations, often necessitating additional post-processing to mitigate aliasing artifacts and lacking fine-grained geometric details. Moreover, vanilla diffusion architectures and sampling strategies yield suboptimal 3D model quality, resulting in a significant alignment gap between generated models and input images.
A common limitation across both approaches is their heavy reliance on the Objaverse dataset \cite{deitke2023objaverse}. The necessity for rigorous data filtering often reduces the usable data samples by nearly half, presenting a substantial challenge in scaling data compared to the image and video domains.


Given these challenges in 3D generation and the substantial successes observed in image and video synthesis, we posit a critical question: \emph{What is the optimal paradigm for generating high-fidelity 3D models with precise alignment to input conditions?}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Images/pipeline.pdf}
    \caption{The overview of our method consists of two main components: (i) Data-Building System and (ii) \method{} Model. The data-building system processes the 3D models from various datasets (e.g., Objaverse and ShapeNet) through a series of data processing steps to create the training data. Our \method{} model is then trained on this curated dataset for high-fidelity shape generation from a single input image.}
    \label{fig:pipeline}
    % \vspace{-1em}
\end{figure}


In response to this inquiry, we present \emph{\method{}}, a high-fidelity 3D generative model that leverages a rectified flow transformer trained on meticulously curated, large-scale data. Our approach achieves unprecedented quality in image-to-3D generation, characterized by finer details and superior input condition alignment.
Inspired by 3DShape2VecSet~\cite{zhang20233dshape2vecset}, we train our generative model on latent representations efficiently compressed by a VAE model.
We identify that the quality of the generated models is heavily dependent on the capacity of the latent space (i.e., the number of tokens) and the volume of high-quality 3D data available.
Thus, it's crucial to scale up the model with a more efficient neural architecture. To address this challenge, we propose several key advancements:
\begin{enumerate}
\item We pioneer the use of a rectified flow transformer architecture~\cite{DBLP:conf/iclr/LiuG023} in 3D generation, drawing inspiration from recent successes in scaling up image/video models (e.g., SD3~\cite{esser2024scaling}, FLUX
~\cite{flux}, and Meta Movie Gen~\cite{metamoviegen}). This choice is motivated by its simplicity and superior performance in terms of training stability and convergence.
\item Our model builds upon the DiT~\cite{peebles2023scalable} framework, incorporating critical enhancements for improved scalability. These include skip-connections, RMSNorm~\cite{zhang2019root}, and the injection of both global and local features. Furthermore, we adopt the DiT-MoE~\cite{fei2024scaling} approach, replacing standard Feed-Forward modules in each block with a mixture-of-experts (MoE) mechanism. This configuration retains one shared expert while selecting the top two experts for each operation.
\item We have successfully trained a 4 billion (4B) parameter 3D generative model at a latent resolution of $4096$ tokens, leveraging this unprecedented scale to produce highly detailed and geometrically precise structures.
\end{enumerate}

Recognizing the critical role of VAE reconstruction quality in latent diffusion or flow models~\cite{rombach2022high,zhang20233dshape2vecset}, we introduce several improvements to our VAE training process, including \textbf{1)} We employ SDF (Signed Distance Function) representation, which offers superior geometric expressiveness compared to occupancy representations. Occupancy grids often introduce quantization errors, resulting in aliasing artifacts or ``staircasing'' effects when representing continuous surfaces. \textbf{2)} We implement geometry-aware supervision on SDF with surface normal guidance, significantly enhancing 3D model reconstruction. This approach enables sharper and more precise geometry without the quantization artifacts.
These improvements can better connect the latent space and the 3D model space, benefiting the quality of 3D models generated by our flow transformers.

To address the scarcity of high-quality 3D data, we have developed a sophisticated data-building system. This system ensures the generation of standardized, high-quality 3D training data (Image-SDF pairs) from diverse sources through a rigorous process of 1) scoring, 2) filtering, 3) fixing and augmentation, and 4) field data production.
More importantly, we find that \emph{both} data quality and quantity are crucial, demonstrating that improperly processed data can substantially impede the training process.


Building on our proposed solution, we have designed an optimized training configuration that incorporates the proposed improvements, achieving a new state-of-the-art (SOTA) performance in 3D generation with our largest \method{} model.
% \zzx{
Fig.~\ref{fig:pipeline} provides an overview of our method, illustrating both the data-building system and the \method{} model architecture. 
% }
We have validated the effectiveness of each design component through a series of mini-setting experiments.

Our core contributions are as follows:


\begin{itemize}
    \item We introduce a large-scale rectified flow transformer for 3D shape generation, setting a new benchmark in fidelity by leveraging extensive, high-quality data.
    \item We present a novel hybrid supervised training strategy that combines SDF, normal, and eikonal losses for 3D VAE, achieving state-of-the-art 3D reconstruction performance.
    \item We demonstrate the crucial rule of data quality and quantity in training 3D generative models, introducing a robust data processing pipeline capable of producing 2M high-quality 3D data samples.
\end{itemize}