\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
% 引入宏
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{diagbox}
% \usepackage{authblk} % 引入authblk宏包来处理作者信息

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\setlength{\skip\footins}{2pt}
\title{Singing Voice Conversion with Accompaniment Using Self-Supervised Representation-Based Melody Features
% Singing Voice Conversion with Accompaniment: Melody Feature Extracted from Self-Supervised Representation
% Conditioned on Self-Supervised Representation-based Melody Feature Extractor
% RobustSVC: Singing Voice Conversion Conditioned on Self-Supervised Representation-based Melody Feature Extractor 
% with adapter 在考虑要不要加
\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
\IEEEauthorblockN{
Wei Chen\textsuperscript{1},
Binzhu Sha\textsuperscript{1},
Jing Yang\textsuperscript{2},
Zhuo Wang\textsuperscript{2},
Fan Fan\textsuperscript{2},
Zhiyong Wu\textsuperscript{1}$^{\dagger}$\thanks{$^{\dagger}$ Corresponding author.}
}
\IEEEauthorblockA{\textsuperscript{1}Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\\
}
\thanks{This work is supported by National Natural Science Foundation of China (62076144) and Shenzhen Science and Technology Program (WDZC20220816140515001, JCYJ20220818101014030).}
\IEEEauthorblockA{\textsuperscript{2}Huawei Technologies Co., Ltd., Shenzhen, China\\
\{chenw23, sbz22\}@mails.tsinghua.edu.cn, zywu@se.cuhk.edu.hk}
\{yangjing201, wangzhuo21, fanfan1\}@huawei.com,
\vspace{-18pt}}

% \author{

% 	\IEEEauthorblockN{

% 		Wei Chen \IEEEauthorrefmark{1}\textsuperscript{1},

% 		Binzhu Sha \IEEEauthorrefmark{1}\textsuperscript{2},

% 		Jing Yang \IEEEauthorrefmark{2}\textsuperscript{1},
%             Zhuo Wang \IEEEauthorrefmark{2}\textsuperscript{1} ,           
%             Fan Fan \IEEEauthorrefmark{2}\textsuperscript{1},
%             Zhiyong Wu \IEEEauthorrefmark{2}\textsuperscript{1}},

% 	\IEEEauthorblockA{\IEEEauthorrefmark{1}dept. name of organization (of Aff.), name of organization (of Aff.), City, Country\\

% 	\textsuperscript{1}email@email, \textsuperscript{2}email@email}

% 	\IEEEauthorblockA{\IEEEauthorrefmark{2}dept. name of organization (of Aff.), name of organization (of Aff.), City, Country\\

% 	\textsuperscript{1}email@email }

% }
% \author{\IEEEauthorblockN{Wei Chen}
% \IEEEauthorblockA{\textit{Shenzhen International Graduate School} \\
% \textit{Tsinghua University }\\
% Shenzhen, China \\
% chenw23@mails.tsinghua.edu.cn}
% \and
% \IEEEauthorblockN{Binzhu Sha}
% \IEEEauthorblockA{\textit{Shenzhen International Graduate School} \\
% \textit{Tsinghua University }\\
% Shenzhen, China \\
% sbz22@mails.tsinghua.edu.cn}
% \and
% \IEEEauthorblockN{Jing Yang}
% \IEEEauthorblockA{\textit{2012 Labs} \\
% \textit{Huawei Technologies Co., Ltd.}\\
% Shenzhen, China \\
% yangjing201@huawei.com}
% \and
% \IEEEauthorblockN{Zhuo Wang}
% \IEEEauthorblockA{\textit{2012 Labs} \\
% \textit{Huawei Technologies Co., Ltd.}\\
% Shenzhen, China \\
% wangzhuo21@huawei.com}
% \and
% \IEEEauthorblockN{Fan Fan}
% \IEEEauthorblockA{\textit{2012 Labs} \\
% \textit{Huawei Technologies Co., Ltd.}\\
% Shenzhen, China \\
% fanfan1@huawei.com}
% \and
% \IEEEauthorblockN{Zhiyong Wu$^{*}$\thanks{$^{*}$ Corresponding author.}}
% \IEEEauthorblockA{\textit{Shenzhen International Graduate School} \\
% \textit{Tsinghua University }\\
% Shenzhen, China \\
% zywu@se.cuhk.edu.hk}
% }

\maketitle

\begin{abstract}
Melody preservation is crucial in singing voice conversion (SVC).
However, in many scenarios, audio is often accompanied with background music (BGM), which can cause audio distortion and interfere with the extraction of melody and other key features, significantly degrading SVC performance.
Previous methods have attempted to address this by using more robust neural network-based melody extractors, but their performance drops sharply in the presence of complex accompaniment.
Other approaches involve performing source separation before conversion, but this often introduces noticeable artifacts, leading to a significant drop in conversion quality and increasing the user’s operational costs.
To address these issues, we introduce a novel SVC method that uses self-supervised representation-based melody features to improve melody modeling accuracy in the presence of BGM.
In our experiments, we compare the effectiveness of different self-supervised learning (SSL) models for melody extraction and explore for the first time how SSL benefits the task of melody extraction.
The experimental results demonstrate that our proposed SVC model significantly outperforms existing baseline methods in terms of melody accuracy and shows higher similarity and naturalness in both subjective and objective evaluations across noisy and clean audio environments.
% 主客观应该说错了，不是都超越了
% 灾难性遗忘
% 自监督的动机
\end{abstract}
\vspace{-8pt}
\begin{IEEEkeywords}
Singing voice conversion, self-supervised learning model, melody extraction.
\end{IEEEkeywords}


% 歌声转换（SVC）是一种新兴的音频编辑技术，旨在在不改变旋律和歌词等内容的前提下，将歌曲中原唱者的音色转换为目标歌手的音色。尽管语音转换技术在语音领域已有显著进展，SVC 仍然面临诸多挑战，主要体现在以下几个方面：（1） 由于音乐表现风格的多样性，歌唱中的音高、响度和发音变化更大，导致建模难度显著增加；（2） 人类感知对歌声中的音高误差极为敏感，轻微的音高偏差便会被认为是跑调，进而破坏旋律的完整性；（3） 大规模清唱数据集的匮乏限制了 SVC 模型的泛化能力。
% 最初的 SVC 方法通过利用并行数据来精确恢复目标音高，并解决与音高相关的问题。然而，由于配对数据的稀缺，当前的 SVC 模型主要集中于对歌唱信息的解耦任务，例如内容、音色和音高。通过使用 ASR 模型中的语音后验谱（PPG）、瓶颈特征（BNF）和在大量未标记语音数据上训练的自监督学习（SSL）表示，将这些与歌手无关的特征输入声学模型，直接生成目标音频。多种深度生成模型被用于解码，包括自回归模型[10-13]、生成对抗网络（GAN）、变分自编码器（VAE） 和扩散模型。然而，这些模型通常依赖非鲁棒的方法（如pYIN）来提取音高特征，并直接使用输入的 F0 进行旋律建模，因此只能在干净的音频条件下进行歌声转换。
% 在实际应用中，歌声通常伴随有背景音乐，这会导致音频失真，干扰歌声中关键声学特征（如音高、语言内容和音色）的提取，从而显著降低 SVC 的性能。虽然有一些工作使用更为鲁棒的神经网络旋律提取器（如 crepe）来缓解这一问题，但这些方法在面对较高噪声或复杂伴奏时性能急剧下降。另一种常用的解决方法是使用音乐源分离算法从录音中去除伴奏音乐。然而，它们通常会产生不可忽略的伪影，并且使用经过处理的音频作为输入时，仍会导致 SVC 系统的质量大幅下降。
% 在本文中，我们提出了一种全新的any-to-oneSVC 模型，即使输入的歌声伴有背景音乐，它也能稳健地从任意来源的歌唱内容中生成干净歌声。我们通过获取 SSL 模型每一隐藏层输出的加权和嵌入向量，并结合适配器的使用，确保在噪声环境下能够有效提取代表音高信息的旋律特征，同时避免灾难性遗忘，这对于歌声转换至关重要。此外，我们首次探索了自监督学习（SSL）在旋律提取任务中的作用。分析结果显示，SSL 模型在预训练阶段只学习到了浅层的旋律相关知识，而通过微调过程，模型的全部潜力得以发挥。
% 在实验中，我们比较了不同 SSL 模型对旋律提取的效果，并最终选用了基于 HuBERT 的旋律提取器作为我们模型的核心组件。实验结果表明，所提出的 SVC 模型在旋律转换的准确性上显著优于现有的基线方法，并且在噪声和干净音频环境下的主观和客观评估中均表现出更高的相似性和自然度。
\vspace{-10pt}
\section{Introduction}
\vspace{-4pt}
SVC is an emerging audio editing technology that aims to transform the timbre of an original singer into that of a target singer without altering other elements of a song (e.g., melody, lyrics).
Despite recent advancements in the field of speech conversion, SVC remains challenging due to the following reasons:
(1) Different musical styles lead to a greater variability in pitch, loudness, and articulation in singing, which makes modeling more complex;
(2) Human perception is highly sensitive to pitch errors in singing, as they are perceived as off-key and disrupt the melodic content;
(3) The scarcity of large-scale clean singing datasets impedes the generalization of SVC models.

% Initial SVC methods relied on parallel data to accurately recover target pitch and address pitch-related issues in SVC. Due to the limited availability of paired data, current SVC models mainly focus on disentangling singing information, such as content, timbre, and pitch. These models use speaker-independent features from ASR models, such as phonetic posteriorgrams (PPG), bottleneck features (BNFs), and self-supervised learning (SSL) representations trained on large amounts of unlabelled speech data, to directly generate target audio in the acoustic model. Various deep generative models have been employed for decoding, including autoregressive models, generative adversarial networks (GANs), variational autoencoders (VAEs), and diffusion models. However, these models can only handle clean audio for voice conversion due to non-robust pitch feature extraction methods and the direct display of input for modeling melodies.

% In many cases, singing is accompanied by instrumental music, leading to distortions that interfere with the extraction of acoustic features required for SVC, such as pitch, linguistic content, and the singer’s voice characteristics. To mitigate this issue, music source separation algorithms are used to remove music from recordings. However, these algorithms often produce noticeable artifacts, and feeding such processed samples into SVC systems still significantly degrades SVC quality.

Early SVC methods~\cite{kobayashi2014statistical,villavicencio2010applying,kobayashi2015statistical,toda2007one} address pitch-related challenges by leveraging parallel data to accurately recover the target pitch.
However, due to the scarcity of paired data, current SVC models focus on disentangling various aspects of singing, such as content, timbre, and pitch.
These models utilize singer-independent features, such as phonetic posteriorgrams (PPG)~\cite{li2021ppg,sun2016phonetic} and bottleneck features (BNFs)~\cite{ning2023vits} derived from Automatic Speech Recognition (ASR) models, as well as SSL representations~\cite{jayashankar2023self} trained on large amounts of unlabelled speech data, to directly generate target audio within the acoustic model.
Various deep generative models have been employed for decoding, including autoregressive models~\cite{zhang2020durian,takahashi2021hierarchical}, generative adversarial networks (GANs)~\cite{liu2021fastsvc}, variational autoencoders (VAE)~\cite{luo2020singing}, and diffusion models~\cite{liu2021diffsvc}.
However, these models can only perform voice conversion on clean vocal sequences, as they rely on non-robust methods (e.g. PYIN~\cite{mauch2014pyin}) for pitch extraction and directly use the foundamental frequency (F0) in SVC.

While many SVC models only deal with clean vocal data, 
singing is accompanied with BGM in real-world applications, which introduces distortions that interfere with the extraction of key acoustic features (e.g., pitch).
This interference significantly degrades SVC performance.
While some approaches~\cite{zhou2022hifi} employ more robust neural network-based pitch extractors, such as Crepe~\cite{kim2018crepe}, to mitigate this issue, these methods tend to perform poorly in the presence of higher noise levels or complex musical accompaniment.
Another approach to address this issue is to use music source separation algorithms to first remove the BGM from the audio before implementing SVC, which increases the barrier to user adoption.
Furthermore, these algorithms often introduce non-negligible artifacts, and when the processed audio is used as input to the SVC system, the overall conversion quality is still significantly reduced.
% 提高了用户使用的门槛

% (\textcolor{red}{achieving})
In this paper, we propose a novel any-to-one SVC model that can robustly generate clean singing voices directly from any source, even when the input contains complex background accompaniment.
The key to achieving this goal is to accurately extract melody features (including pitch information) from source audio. 
As demonstrated in ~\cite{wang2021towards,10219976}, SSL models show great potential in capturing rich acoustic and musical information.
Additionally,~\cite{hung22_interspeech} highlights the noise robustness of SSL models.
Based on this, we explore two distinct SSL models, HuBERT~\cite{hsu2021hubert} and WavLM~\cite{chen2022wavlm}, into our study.
By leveraging weighted embeddings from the outputs of each hidden layer of an SSL model, we ensure optimal extraction of melody features in noisy conditions.
% 噪声鲁棒性

This work makes the following contributions:
1) We propose a novel SVC method using self-supervised representation-based melody features to enhance the accuracy of melody modeling, even in the presence of BGM.
2) We compare the performance of various SSL models on melody extraction and are the first to investigate how SSL benefits the task of melody extraction.
% Our analysis reveals that during SSL, only the shallow layers of the pretrained model capture melody-related information.
% However, after fine-tuning, the higher Transformer-based encoder layers are able to model melody information using the melody extraction training objective, resulting in a greater contribution to the final prediction and improved melody extraction performance compared to the pretrained model.
3) The experimental results demonstrate that our SVC model significantly outperforms existing baseline methods in terms of melody accuracy, and shows higher similarity and naturalness in both subjective and objective evaluations under noisy and clean audio environments.


% Additionally, we are the first to investigate how SSL benefits the task of pitch extraction.
% Our analysis shows that SSL models only capture shallow melody-related knowledge with shallow layers in the pre-training stage, while the fine-tuning stage could unleash the full capability of the model.
% In our experiments, we compared the performance of various SSL models on melody extraction and ultimately selected a WavLM-based melody extractor for our proposed model.
% The results demonstrate that our SVC model achieves more accurate pitch conversion and outperforms baseline methods in subjective evaluations under both noisy and clean conditions, achieving higher levels of voice similarity and naturalness.


% In this paper, we propose a novel any-to-one SVC model that robustly generates clean vocals from any source, even when the singing is accompanied by background music. By obtaining an embedding vector that represents a weighted sum of the SSL model's hidden layer outputs and utilizing adapters, we ensure optimal melody modeling under noisy conditions while avoiding catastrophic forgetting, which is crucial for effective voice conversion. We also explore the question of how SSL benefits the melody extraction task. Our analysis reveals that while the SSL model learns shallow melody-related knowledge during the pre-training phase, fine-tuning unlocks the model's full potential. We observe that SSL models offer a broader range of fine-tuning optimization, leading to better resistance to small perturbations and stronger generalization abilities. Experimental results demonstrate that our SVC model achieves more accurate melody conversion and outperforms baseline methods in both subjective and objective evaluations under noisy and clean conditions, resulting in higher similarity and naturalness.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{svc_frameworkv4.pdf}
  % \vspace{-10pt}
  \caption{SVC framework. Snowflake represents the parameter that remains unchanged when training the SVC framework.}
  \label{fig:arc}
  \vspace{-10pt}
\end{figure}

% 我们首先训练了旋律提取器和基于CTC Loss的语音识别模型。接着，我们将这两个模型的中间表征提取出来，作为旋律特征和包含内容信息的BNF特征输入到基于编码器-解码器结构的模型中。通过对抗性学习策略的训练，我们得到了高质量的梅尔谱。经过HiFi-GAN声码器处理后，我们得到了转换后的音频。在本节中，我们将简要介绍语音转换框架和旋律提取器的相关内容。
\vspace{-6pt}
\section{Proposed Method}
\vspace{-4pt}
Fig.~\ref{fig:arc} illustrates an overview of our backbone Recognition-synthesis based SVC model~\cite{chen2024svc}.
We first train a melody extractor and an ASR model to extract melody features and BNFs containing content information.
These two features are then input into an encoder-decoder structured model.
We obtain high-quality Mel-spectrograms using three discriminators.
The Mel-spectrograms are further processed by a HiFi-GAN vocoder to obtain the converted audio.
In the following, we elaborate on the SVC framework and the melody extractor.
\vspace{-4pt}
\subsection{Singing Voice Conversion Framework}
\vspace{-4pt}
Referring to Fig.~\ref{fig:arc}, our model comprises five components: an ASR model, a melody extractor, an encoder-decoder architecture, discriminator groups and a vocoder.

The method employs the melody extractor and an ASR model to decouple the audio information and obtain melody features and content features (BNFs).
These features are separately fed into a melody encoder and a content encoder, both constructed with Feed Forward Transformer (FFT)~\cite{renfastspeech} blocks.
Notably, the melody encoder applies conditional instance normalization~\cite{dumoulin2016learned} after the FFT blocks to remove the timbre information of the source singer.
The decoder, based on FFT blocks, concatenates and decodes the input melody embeddings and content embeddings to generate Mel-spectrograms.

Additionally, the model uses adversarial learning strategies, training on both in-set and out-set data.
In-set data comprises songs sung by a single target singer, whereas out-set data includes songs from multiple singers.
Three types of discriminators are utilized to handle in-set and out-set data.
In order to synthesize high-quality Mel-spectrograms, the Real/Fake discriminator distinguishes whether the Mel-spectrograms are from the ground-truth audio or reconstructed from SVC model.
To enhance similarity with the target speaker, the Conversion discriminator judges if the Mel-spectrograms are converted from out-set singers or the target singer.
The Embedding discriminator determines whether melody embeddings originate from out-set songs, aiming to eliminate timbre information from the embeddings.

After the training of SVC framework, we fine-tune the pretrained HiFi-GAN vocoder~\cite{kong2020hifi} using ground-truth-aligned~\cite{shen2018natural} to address potential deficiencies in SVC model predictions.

% 图 1 展示了基于识别合成的主干 SVC 模型 [xx] 的概述。该模型由五个主要组件组成：语音识别模型（如 Wenet）、旋律提取器、编解码器架构、判别器组和声码器。歌唱序列通过旋律提取器和语音识别模型进行处理，以获取旋律特征和内容特征（BNFs）。这些特征分别输入内容编码器和韵律编码器，两者都包含 FFT 结构。特别地，韵律编码器在最后一个 FFT 模块后应用了条件实例标准化，以消除源歌手的音色信息。解码器基于 FFT 块将输入的旋律嵌入和韵律嵌入进行拼接和解码，生成梅尔谱图。
% 此外，模型采用对抗学习策略，通过将歌唱数据划分为集内和集外数据来训练模型。集内数据由单一歌手演唱，而集外数据包含多位歌手的歌曲。三种不同的判别器被用于处理集内和集外数据：真假判别器用于区分集内歌曲是否来自原始 Ground Truth（GT）或是经过 SVC 重建的；转换判别器则判断集外歌手是否成功转换为目标歌手；嵌入判别器则用于区分旋律嵌入是否来自集外歌手，旨在消除嵌入中的音色信息。
% 最后，在训练完 SVC 框架后，通过对预训练的 HiFi-GAN  声码器进行 GTA 微调，以弥补 SVC 模型预测中的潜在缺陷。

% [width=\linewidth]
\begin{figure}[t]
  \centering \hspace{10mm}
  \includegraphics[scale=0.9]{melodyextractorv3.drawio.pdf}
  % \vspace{-15pt}
  \caption{Melody extractor.}
  \label{fig:extract}
  \vspace{-10pt}
\end{figure}
\vspace{-6pt}
\subsection{Melody Extractor}
\vspace{-3pt}
Fig.~\ref{fig:extract} shows an overview of the melody extractor, which consists of three main components: an SSL model, weighted-sum, and FFT blocks.
% As demonstrated in ~\cite{wang2021towards,10219976}, SSL models show great potential in capturing rich acoustic and musical information.
We explore two distinct SSL models, HuBERT~\cite{hsu2021hubert} and WavLM~\cite{chen2022wavlm}, in our study.
% Subsequent experimental results revealed that the melody information in the pretrained SSL models are primarily extracted from the CNN layers and the lower layers of the Transformer.
To better capture melody information from the SSL representations, we apply weighted-sum\cite{chen22g_interspeech} to the hidden states of $L$ layer to generate the output representation $O = \left \{ o_{t}=\sum_{l=0}^{L} w_{l}\cdot h_{t}^{l} \right \} _{t=1}^{T}$, where $w_{l}$ is a learnable weight for the hidden state $h_{t}^{l}$ of the $l$-th layer, $T$ is the number of frames.
Incorporating FFT blocks, we further enhance the extraction of acoustic and musical information by feeding the output representation into these blocks to extract features for predicting pitch energy and voice/unvoice (VUV) flags. 
% Then we use FFT blocks and feed the output representation into the blocks to extract features for predicting pitch energy and voice/unvoice (VUV) flag.
To improve the model robustness against various accompaniments, during training, the melody extractor is fed with singing data that includes BGM.
Additionally, we perform slight fine-tuning on them to fully leverage the potential of the SSL models.
% 上面一句能立得住吗
Finally, the 256-dimensional output from the FFT blocks is chosen as the melody features input for the melody encoder. Note that the extracted feature contains more comprehensive melody information than pitch.  
% Note that the 256 dimensions output which contains more melody information than pitch from FFT blocks is chosen as the melody features input for the melody encoder.


% 说输入输出维数？
% 图2展示了旋律提取器的概览，主要由三个部分组成：SSL模型、加权平均和FFT模块。【xxx】表明，自监督模型在包含丰富的声学和音乐信息方面具有广阔的前景。基于此，我们引入了HuBERT和WavLM两种不同的自监督模型进行研究。后续实验结果表明，预训练的SSL模型中，旋律特征主要来自于CNN层和Transformer低层。为了更好地获取SSL表征中的旋律信息，我们对每一层的隐藏状态进行加权平均，生成最终的输出表示。接着，采用FFT模块对输出进行更深入的特征提取，以预测基频能量和有声/无声标记（VUV）。在训练过程中，旋律提取器输入了带背景音乐（BGM）的歌唱数据，以增强模型的鲁棒性。同时，为了充分发挥SSL模型的潜力，我们对其进行了小幅度的微调。


\vspace{-5pt}
\section{Experimental Setup}
\vspace{-5pt}
\subsection{Dataset}
\vspace{-3pt}
To train the melody extractor, we utilize the MUSDB~\cite{rafii2017musdb18} and OpenSinger~\cite{huang2021multi} datasets.
% The MUSDB dataset contains 10 hours of professional, which is composed of
% 150 songs with full supervision in stereo.
The MUSDB dataset contains 10 hours of professional audio, composed of 150 songs with full supervision in stereo.
This dataset is divided into three subsets: 100, 35, and 15 songs for training, validation, and testing.
The OpenSinger dataset comprises 50 hours of high-quality Chinese vocals recorded in a professional studio, featuring 28 male and 48 female singers.
% The audio is stored in WAV format with a sampling rate of 44.1 kHz.
For the test set, we retain recordings from two male and two female singers, while the remaining recordings are randomly split into training and validation sets in a 9:1 ratio.

To train the SVC framework, we use the Opencpop~\cite{wang2022opencpop} corpus as in-set data, which includes 5.2 hours of recordings by professional female singers.
To enhance the diversity of the melodies, we augment the training data by varying the playback speed from 0.9x to 1.5x.
The OpenSinger dataset is used as out-of-domain data, with the same split methodology as that for training the melody extractor.


% 对于旋律提取器的训练，我们使用了MUSDB和OpenSinger数据集[19]。MUSDB数据集包含150个源混音，涵盖了10小时的专业工作室录制的隔离人声和器乐“干声”。该数据集被分为三个部分：用于训练的100首歌曲、用于验证的35首歌曲以及用于测试的15首歌曲。OpenSinger数据集则由专业录音室录制，包含50小时的高质量中文歌声。这个数据集包括28名男歌手和48名女歌手，音频以wav格式保存，采样率为44.1 kHz。在测试集里，我们保留了两名男歌手和两名女歌手的歌唱录音，其余录音按照9:1的比例随机分为训练集和验证集。
% 为了训练SVC框架，我们使用了Opencpop[21]语料库作为集内数据，该语料库由专业女歌手演唱，总时长为5.2小时。我们通过将语速从0.9倍调整到1.5倍来增强训练数据，从而提高旋律的多样性。OpenSinger数据集则用作集外数据，其数据分割方式与训练旋律提取器时保持一致。

\vspace{-2pt}
\subsection{Training Conditions}\label{condition}
\vspace{-2pt}
During the training of the melody extractor, the ground-truth pitches are extracted from clean audio by taking the median of three methods: PYIN\footnote{https://github.com/librosa/librosa}, REAPER\footnote{https://github.com/google/REAPER}, and Parselmouth\footnote{https://github.com/YannickJadoul/Parselmouth}.
Root Mean Square Energy is computed from the STFT of the waveform with hop\_size$=$160.
% The probability of adding BGM to an input song in the OpenSinger dataset is 50\% with randomly selected from the MUSDB train set.
The MUSDB dataset already involves BGM tracks and BGM-accompanied vocal tracks, but the OpenSinger dataset only involves clean vocal tracks. Therefore, we randomly select BGM from the MUSDB dataset and add BGM to the data in the OpenSinger dataset with a probability of 50\%. 
% The probability of adding BGM to an input song in the OpenSinger dataset is 50\% and the BGM was randomly selected from the MUSDB training set. The song 
% While audio from MUSDB use the original instrumental tracks.
The AdamW optimizer with an initial learning rate of $2\times10^{-5}$ is employed.
% Two pretrained SSL models HuBERT and WavLM are based on pretrained models using the LibriSpeech-960~\cite{panayotov2015librispeech} corpus.
% Both the pretrained SSL models are trained on the LibriSpeech-960~\cite{panayotov2015librispeech} corpus.
we utilize two pretrained SSL models: HuBERT and WavLM, to demonstrate the generalizability of our method.
Both models are trained on the Librispeech~\cite{panayotov2015librispeech} dataset.
To evaluate the effectiveness of fine-tuning the SSL model, weighted-sum, and FFT blocks, we train the melody extractor under the following conditions: \textit{raw single}, \textit{raw weighted-sum}, \textit{single w/o FFT}, \textit{weighted-sum w/o FFT}, \textit{single w/ FFT}, \textit{weighted-sum w/ FFT}, and \textit{proposed}.
\textit{Raw single} means directly using the last-layer output from the pretrained SSL model as the melody features.
\textit{Raw weighted-sum} means using the weighted-sum of each layer output from the pretrained SSL model as the melody features.
\textit{Single w/o FFT} means using the last layer output from the SSL model and training the melody extractor without FFT blocks.
\textit{Weighted-sum w/o FFT} means weighted-sum each layer output from the SSL model and training the melody extractor without FFT blocks.
\textit{Single w/ FFT} means using the last layer output from the pretrained SSL model and training the melody extractor with FFT blocks.
\textit{Weighted-sum w/ FFT} means weighted-sum each layer output from the pretrained SSL model and training the melody extractor with FFT blocks.
Finally, \textit{proposed} means weighted-sum each layer output from the SSL model and training the melody extractor with FFT blocks.
Note that when the SSL model is involved in training, we only train it for 5k steps and then freeze the SSL model's parameters to mitigate the risk of catastrophic forgetting~\cite{french1999catastrophic}.

% single means directly use the last layer output from SSL model and feed into the  
% FFT means with or without FFT，when without FFT the SSL model only train with two linear layer. 
% 组太多了，不知道怎么描述

Referring to Fig.~\ref{fig:arc}, when training the SVC, an ASR model is initially trained using Connectionist Temporal Classification loss, following the configuration described in ~\cite{zhao2022disentangling}, and utilizing the WeNet~\cite{yao2021wenet} implementation\footnote{https://github.com/wenet-e2e/wenet}.
256-dimensional BNFs are extracted from the ASR model.
All audio files are down-sampled to 16 kHz, and Mel-spectrograms are generated using STFT with a 50 ms frame size, 10 ms frame hop, and a Hanning window function.
To compare with state-of-the-art (SoTA) methods, we directly input pitch, content embeddings and energy into the decoder to train the SVC model.
The pitch is obtained using two methods: (1) the method mentioned in~\ref{condition}, referred to as \textit{"Original Pitch\&Energy"}, and (2) a noise-robust neural F0 extractor called Crepe~\cite{kim2018crepe}, referred to as \textit{"Crepe"}.
Additionally, we employ the \textit{"Separated+crepe"} approach, in which we first separated songs into clean vocal tracks and BGMs using Demucs~\cite{rouard2023hybrid}, and then extract pitch and energy from the clean vocal tracks using the Crepe model. 
% the songs are separated before converting the pitch and energy.
% In this case, pitch and energy are extracted from crepe and audio processed by a state-of-the-art music source separation model~\cite{rouard2023hybrid}.
Both the baseline and proposed systems are evaluated under identical conditions unless stated otherwise.
Audio samples are available on our demo page\footnote{https://ccchange2024.github.io/SVC-with-Accompaniment}.

\vspace{-4pt}
\section{Result}
\vspace{-4pt}
\subsection{Melody Features Evaluation}
We first evaluate the performance of melody features extraction under different conditions for SVC.
The source audio used for testing contains unseen BGM from the test set, with signal-to-noise ratios (SNR) of 0, 5, 10, and 15, each accounting for 25\% of the dataset.
To assess the quality of the converted audio, we calculate the F0 Root Mean Square Error (F0RMSE) and the correlation coefficient (F0CORR)~\cite{huang2023singing}.
F0RMSE measures the naturalness of the converted audio, and the F0 sequence is normalized using min-max scaling before processing.
The RMSE between the converted and source waveforms is computed in the absence of BGM.
F0CORR evaluates pitch accuracy by aligning F0 contours of the original and synthesized audio using dynamic time warping and then computing the Pearson correlation coefficient.
This metric highlights the model’s ability to preserve pitch, which is critical for achieving natural and expressive audio.

As shown in Table.~\ref{tab:my_label}, the comparison between the conditions with and without weighted-sum demonstrates the superiority of the weighted-sum approach.
Most fine-tuning methods show performance improvements compared to the pretrained methods.
Notably, the pretrained \textit{weighted-sum w/ FFT} outperforms the \textit{single w/o FFT} during fine-tuning, benefiting from more comprehensive melody information extraction in the SSL model.
% 不知道上面这句话要不要说，因为weightsum相同的情况下finetune是更好的
The proposed method achieves the best performance across two different SSL models, and since WavLM exhibits superior results, we choose WavLM for the proposed method and for further comparisons with other melody extraction models.


\begin{table}[t]
    \caption{Results of different melody input.}
    \label{tab:my_label}
    \centering \hspace{-3mm}
    \scalebox{0.95}{
    \begin{tabular}{|c|c|c|c|c|c|} \hline  
         \multirow{2}{*}{Method}&  \multicolumn{3}{c|}{Condition} & \multirow{2}{*}{F0RMSE$\downarrow$}& \multirow{2}{*}{F0CORR$\uparrow$}\\ \cline{2-4}
         &  FT&  WS&  FFT&  & \\ \hline  
         H-single&  &  &  &  0.360& 0.741\\ \hline  
         H-weighted-sum&  &  $\checkmark $&  &  0.239& 0.745\\ \hline  
         H-single w/o FFT&  $\checkmark $&  &  &  0.246& 0.799
\\ \hline  
         H-weighted-sum w/o FFT&  $\checkmark $&  $\checkmark $&  &  0.228& 0.867
\\ \hline  
         H-single w/ FFT&  &  &  $\checkmark $&  0.287& 0.795
\\ \hline  
         H-weighted-sum w/ FFT&  &  $\checkmark $&  $\checkmark $&  0.234& 0.829
\\ \hline  
         H-proposed&  $\checkmark $&  $\checkmark $&  $\checkmark $&  \textbf{0.202}& \textbf{0.899}
\\ \hline  \hline
         W-single&  &  &  

&  0.320& 0.740
\\ \hline  
 W-weighted-sum& & $\checkmark $& 

& 0.289&0.755
\\ \hline  
 W-single w/o FFT& $\checkmark $& & 

& 0.230&0.887
\\ \hline  
 W-weighted-sum w/o FFT& $\checkmark $& $\checkmark $& 

& 0.194&0.932
\\ \hline  
 W-single w/ FFT& & & $\checkmark $
& 0.260&0.811
\\ \hline  
 W-weighted-sum w/ FFT& & $\checkmark $& $\checkmark $
& 0.201&0.914
\\ \hline  
 W-proposed& $\checkmark $& $\checkmark $& $\checkmark $& \textbf{0.176}&\textbf{0.950}
\\ \hline 
 \multicolumn{6}{l}{H represent using Hubert, W represent using WavLM.}\\
 \multicolumn{6}{l}{FT, WS, and FFT represent fine-tuning, weighted-sum part and FFT blocks.} \\
 \multicolumn{6}{l}{Values in bold show the best scores in each noise condition.}\\
    \end{tabular}
    }
    
    \vspace{-20pt}
\end{table}

\vspace{-4pt}
\subsection{Comparison with State-of-the-art Methods}
\vspace{-4pt}
We conduct both objective and subjective evaluations of various SoTA methods under clean and with BGM conditions.
The objective experiments utilize the F0RMSE and F0CORR metrics.
For the subjective evaluation, we employ a 5-point Mean Opinion Score (MOS) test and invite 15 volunteers with extensive knowledge of music theory to participate.
Notably, the subjective test includes 12 audio samples for both clean and BGM conditions.
% 要强调unseen BGM吗？
The samples with BGM are evenly distributed across SNR of 0, 5, 10, and 15, with three audio samples for each SNR level.

As shown in Table.~\ref{tab:compare}, under clean conditions, the objective metrics show similar performance across different methods.
However, in the subjective tests, our proposed method demonstrates superior performance in terms of both similarity and naturalness, even achieving scores close to those of the source audio.
The \textit{Separated+Crepe} method performs relatively poorly due to the loss of some vocal information in the separated signal, leading to instability in SVC.
In BGM conditions, all methods show a decline in performance as the SNR decreased. Nonetheless, our proposed method consistently achieves the best results, both objectively and subjectively.

\begin{table}[t]
    \caption{Comparison with state-of-the-art methods}
    \label{tab:compare}
    \centering
    \scalebox{0.90}{
    \begin{tabular}{|>{\centering\arraybackslash}p{0.11\linewidth}|>{\centering\arraybackslash}p{0.15\linewidth}|>{\centering\arraybackslash}p{0.12\linewidth}|>{\centering\arraybackslash}p{0.12\linewidth}|c|c|} \hline 
         Method&  SNR Level&  F0RMSE$\downarrow$&  F0CORR$\uparrow$&  NMOS& SMOS\\ \hline
         \multirow{5}{*}{\makecell{Original\\Pitch\&\\Energy}}&  clean&  0.165&  0.965
&  3.38±0.14& 3.58±0.13\\ \cline{2-6}
         &  15dB&  0.261&  0.903
&  \multirow{4}{*}{2.30±0.13}& \multirow{4}{*}{2.88±0.14}\\ 
         &  10dB&  0.306&  0.889
&  & \\ 
         &  5dB&  0.325&  0.818
&  & \\ 
         &  0dB&  0.353&  0.779
&  & \\ \hline
         \multirow{5}{*}{Crepe}&  clean&  \textbf{0.164}&  \textbf{0.967}
&  3.07±0.16& 3.52±0.13\\ \cline{2-6}
         &  15dB&  0.208&  0.915
&  \multirow{4}{*}{2.40±0.14}& \multirow{4}{*}{3.04±0.14}\\ 
         &  10dB&  0.261&  0.895
&  & \\ 
         &  5dB&  0.301&  0.856
&  & 
\\ 
 & 0dB& 0.297& 0.815
& &
\\ \hline
 \multirow{5}{*}{\makecell{Separated\\+\\Crepe}}& clean& 0.171& 0.950
& 3.03±0.16&
3.42±0.12\\ \cline{2-6}
 & 15dB& 0.176& 0.946
& \multirow{4}{*}{2.57±0.16}&
\multirow{4}{*}{3.07±0.14}\\ 
 & 10dB& 0.178& 0.943
& &
\\ 
 & 5dB& \textbf{0.182}& 0.942
& &

\\ 
 & 0dB& 0.202& 0.928
& &\\ \hline
 \multirow{5}{*}{\makecell{proposed\\with\\WavLM}}& clean& 0.169& 0.960
& \textbf{3.54±0.14}&\textbf{3.71±0.14}\\ \cline{2-6}
 & 15dB& \textbf{0.174}& \textbf{0.955}
& \multirow{4}{*}{\textbf{3.49±0.14}}&\multirow{4}{*}{\textbf{3.56±0.13}}\\ 
 & 10dB& \textbf{0.175}& \textbf{0.949}
& &\\ 
 & 5dB& 0.183& \textbf{0.943}
& &\\ 
 & 0dB& \textbf{0.199}& \textbf{0.935}
& &\\\hline
 source& clean &\diagbox[width=42pt,height=10pt]{}{} & \diagbox[width=42pt,height=10pt]{}{}& 4.55±0.11&\diagbox[width=50pt,height=10pt]{}{}\\ \hline 
 \multicolumn{6}{l}{MOS results are reported with 95\% confidence intervals.}\\
    \end{tabular}
    }
    
    \vspace{-10pt}
\end{table}



\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{softmax_all_4_best_checkpoint2.pdf}
  % \vspace{-25pt}
  \caption{Visualized weight that extract representations from SSL models.}
  \label{fig:weight}
  \vspace{-10pt}
\end{figure}
\vspace{-4pt}
\subsection{Contribution Analysis for Weighted-sum}
\vspace{-4pt}
We visualize the weights of weighted-sum part under both pretraining and fine-tuning conditions to illustrate how each layer of the pretrained model contributes to the final melody extraction performance.
From Fig.~\ref{fig:weight}, we observe that in the pretrained model, the main contributions come from the output of the CNN feature extractor and the lower layers of encoder for all the pretrained models.
It indicates that during SSL, only the shallow layers of the pretrained model capture melody-related information.
In contrast, during fine-tuning, we update both the downstream module parameters and the pretrained parameters.
By unleashing the full potential of the pretrained model, the higher Transformer-based encoder layers also learn to model melody information using the melody extraction objective.
This allows the higher layers to make a greater contribution to the final prediction compared to the pretrain stage, ultimately leading to improved melody extraction performance.


\vspace{-6pt}
\section{Conclusion}
\vspace{-6pt}
We have proposed a novel any-to-one SVC model that can generate clean singing voices even when the input includes BGM.
Both objective and subjective evaluations demonstrate that our method exhibits greater robustness to BGM compared to other methods.
Additionally, we explore how self-supervised models learn melody information and found that only the shallow layers of the pretrained model capture melody-related information.
After fine-tuning, the higher Transformer-based encoder layers are also able to model melody information using the melody extraction training objective and have better melody extraction performance.
Future work will focus on applying our proposed method to more complex noisy environments and extending it to any-to-many SVC tasks.

% (\textcolor{red}{Reviewer \#2})
% \vspace{-6pt}
% \section*{Acknowledgment}
% \vspace{-6pt}
% This work is supported by National Natural Science Foundation of China (62076144) and Shenzhen Science and Technology Program (WDZC20220816140515001, JCYJ20220818101014030).


\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \bibitem{b8} D. P. Kingma and M. Welling, ``Auto-encoding variational Bayes,'' 2013, arXiv:1312.6114. [Online]. Available: https://arxiv.org/abs/1312.6114
% \bibitem{b9} S. Liu, ``Wi-Fi Energy Detection Testbed (12MTC),'' 2023, gitHub repository. [Online]. Available: https://github.com/liustone99/Wi-Fi-Energy-Detection-Testbed-12MTC
% \bibitem{b10} ``Treatment episode data set: discharges (TEDS-D): concatenated, 2006 to 2009.'' U.S. Department of Health and Human Services, Substance Abuse and Mental Health Services Administration, Office of Applied Studies, August, 2013, DOI:10.3886/ICPSR30122.v2
% \bibitem{b11} K. Eves and J. Valasek, ``Adaptive control for singularly perturbed systems examples,'' Code Ocean, Aug. 2023. [Online]. Available: https://codeocean.com/capsule/4989235/tree
% \end{thebibliography}

% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
