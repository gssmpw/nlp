\section{Related Work}
Current approaches in unsupervised disentangled representation learning are predominantly based on Variational Autoencoder (VAE) ____ or information-theoretic Generative Adversarial Network (InfoGAN) ____ frameworks. These methods generally share a core principle: introducing additional regularization terms into the model's loss function to reduce statistical dependencies among latent variables, thereby promoting disentangled representations.
\paragraph{VAE-based Methods.} Among VAE-based approaches, $\beta$-VAE ____ stands as a seminal work. By introducing a tunable hyperparameter $\beta$ into the Evidence Lower Bound (ELBO) loss function, this method constrains the posterior distribution of latent variables. While its objective is to encourage alignment between the posterior distribution and a predefined independent prior (typically an isotropic Gaussian), increasing the weight of the KL divergence term ($\beta > 1$) effectively restricts the latent space capacity and induces lower statistical dependencies across dimensions. However, selecting an optimal $\beta$ remains challenging, as higher values may degrade reconstruction fidelity. 

To more directly minimize statistical dependencies, FactorVAE ____ employs an adversarial learning strategy. It introduces a discriminator to distinguish between latent codes sampled from the aggregated posterior distribution and those from an independent prior distribution, thereby explicitly penalizing the Total Correlation (TC) of latent variables. This approach directly optimizes the aggregated posterior to approximate an independent distribution.

Building upon this, $\beta$-TCVAE ____ decomposes the KL divergence term in the VAE objective into three components: index-code mutual information, total correlation (TC), and dimension-wise KL divergence. By assigning distinct weights to these components, $\beta$-TCVAE achieves finer-grained control over latent dependencies. Its central innovation lies in explicitly penalizing TC to reduce interdimensional correlations.
\paragraph{GAN-based Methods.} Within the domain of GAN-based methods, InfoGAN ____ a significant contribution, giving rise to a series of InfoGAN-based variants, including InfoGAN-CR____ and PS-SC GAN____. The objective of InfoGAN is to learn interpretable latent representations by maximizing the mutual information between a subset of the latent variables and the output of the generator. Although its primary objective is not the direct minimization of statistical dependencies across all latent variables, InfoGAN indirectly facilitates a reduction in statistical dependencies between specific latent variables encoding semantic features and the remaining latent variables. This effect contributes to the emergence of disentangled representations. 

InfoGAN-CR, a variant of InfoGAN with a contrastive regularizer, generates multiple images by fixing one dimension, denoted $c_i$, of the latent representation, while randomly sampling the other dimensions, denoted $c_j(i \neq j)$. A classifier is then trained to identify which latent dimension was fixed based on the generated images. The contrastive regularizer promotes differentiation among latent representation dimensions, thereby supporting disentanglement.

\begin{figure*}[htbp] 
\centering 
\includegraphics[width=\textwidth]{pics/framework.pdf} 
\caption{ The framework of the proposed DiD. The blue points denote $\mathbf{c}$ sampled from the uniform distribution, the red points represent samples obtained from two orthogonal axes.} 
\label{fig::model} 
\end{figure*}