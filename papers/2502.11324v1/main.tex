
\documentclass[10pt]{article} 
\usepackage[preprint]{tmlr}
\usepackage{xspace}
\usepackage{wrapfig}
\usepackage{booktabs}






\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath{ {./Figures/} }


\usepackage{amsthm}
\usepackage{longtable}
\usepackage{multirow}

\title{Robust High-Dimensional Mean Estimation With Low Data Size\update{, an Empirical Study}}





\author{\name Cullen Anderson \email cyanderson@umass.edu \\
      \addr University of Massachusetts Amherst
      \AND
      \name Jeff M. Phillips \email jeffp@cs.utah.edu \\
      \addr University of Utah
}




\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{corollary}{Corollary}[theorem]



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\N}{\mathcal{N}}
\renewcommand{\eps}{\varepsilon}

\newcommand{\jeff}[1]{{\color{blue} [\textsf{Jeff: } #1 ]}}
\newcommand{\cullen}[1]{{\color{red} [\textsf{Cullen: } #1 ]}}

\newcommand{\update}[1]{#1}

\newcommand{\estname}[1]{{\text{\sffamily #1}}\xspace}

\newcommand{\sample}{\estname{sample\_mean}}
\newcommand{\gsample}{\estname{good\_sample\_mean}}
\newcommand{\coordmed}{\estname{coord\_median}}
\newcommand{\coordprune}{\estname{coord\_trimmed\_mean}}
\newcommand{\medmean}{\estname{median\_of\_means}}
\newcommand{\ransac}{\estname{RANSAC}}
\newcommand{\geomed}{\estname{geometric\_median}}
\newcommand{\lvog}{\estname{lee\_valiant}}
\newcommand{\lvsim}{\estname{lee\_valiant\_simple}}
\newcommand{\lrv}{\estname{LRV}}
\newcommand{\ev}{\estname{ev\_filtering}}

\newcommand{\evln}{\estname{ev\_filtering\_low\_n}}
\newcommand{\evcov}{\estname{eigenvalue\_filtering\_unknowncov}}
\newcommand{\fite}{\estname{FITE}}
\newcommand{\pgd}{\estname{PGD}}
\newcommand{\que}{\estname{QUE}}
\newcommand{\queln}{\estname{QUE\_low\_n}}
\newcommand{\lmin}
{\estname{$\ell_p$\_min}}
\newcommand{\lminln}{\estname{$\ell_p$\_min\_low\_n}}
\newcommand{\etal}{\emph{et.al. }}
\newcommand{\etals}{\emph{et.al.}}





\def\month{02}  
\def\year{2025} 
\def\openreview{\url{https://openreview.net/forum?id=1QeI99nH9k}} 


\begin{document}


\maketitle


\begin{abstract}

Robust statistics aims to compute quantities to represent data where a fraction of it may be arbitrarily corrupted.   The most essential statistic is the mean, and in recent years, there has been a flurry of theoretical advancement for efficiently estimating the mean in high dimensions on corrupted data.  While several algorithms have been proposed that achieve near-optimal error, they all rely on large data size requirements as a function of dimension. In this paper, we perform an extensive experimentation over various mean estimation techniques where data size might not meet this requirement due to the high-dimensional setting.  

For data with inliers generated from a Gaussian with known covariance, we find experimentally that several robust mean estimation techniques can practically improve upon the sample mean, with the \emph{quantum entropy scaling} approach from Dong \etal (NeurIPS 2019) performing consistently the best.  However, this consistent improvement is conditioned on a couple of simple modifications to how the steps to prune outliers work in the high-dimension low-data setting, and when the inliers deviate significantly from Gaussianity. In fact, with these modifications, they are typically able to achieve roughly the same error as taking the sample mean of the uncorrupted inlier data, even with very low data size. In addition to controlled experiments on synthetic data, we also explore these methods on large language models, deep pretrained image models, and non-contextual word embedding models that do not necessarily have an inherent Gaussian distribution.  Yet, in these settings, a mean point of a set of embedded objects is a desirable quantity to learn, and the data exhibits the high-dimension low-data setting studied in this paper.  We show both the challenges of achieving this goal, and that our updated robust mean estimation methods can provide significant improvement over using just the sample mean. We additionally publish a library of Python implementations of robust mean estimation algorithms, allowing practitioners and researchers to apply these techniques and to perform further experimentation.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Given samples from an unknown distribution, mean estimation is perhaps the most-fundamental and oldest problems in data analysis.  And it is even more relevant in modern analysis for learning and AI tasks where data \update{is very high dimensional}, and there is little else one can reliably compute -- at least not without first grappling with the mean.  

In the past several years, there has been a flurry of theoretical advancement on this topic, including improved asymptotic bounds~\citep{lee2022optimal,gupta2023finite, catoni2011challengingempiricalmeanempirical, gupta2024catoni, lugosi2017subgaussianestimatorsmeanrandom}, 
and the development of more robust methods for dealing with adversarially corrupted data distributions~\citep{lai2016agnostic,diakonikolas2017being,diakonikolas2019robust,cheng2019fast,dong2019quantumentropyscoring,deshmukh2022robustmean}.  

This paper supports this development in two key ways:
\begin{enumerate}
\item We provide a large experimental study of many new methods, which had not been thoroughly compared.  In the non-corrupted case, with moderate data size we do not see substantial improvement over the classic sample mean approach.  However, in the corrupted setting, we find that some \update{methods can significantly improve upon the sample mean.  In some cases consistent improvement on the sample mean requires adjustments that we develop.} \update{As a summary,} the quantum entropy scaling approach of \cite{dong2019quantumentropyscoring} \update{(using an adjustment we describe)} consistently performs the best \update{as long as inliers are reasonable similar to Gaussian}, and often basically matches the mean of the (unknown) inlier data.  

\item We bring to the fore the $d > n$ setting, where there are more dimensions $d$ than data points $n$, or at least we do not have $n$ as substantially larger than $d$.  This setting is becoming more common as dimensionality grows, but has not typically been considered because the theoretical advancements did not provide exciting new bounds here.  In this setting, we revisit some algorithmic derivations and empirically explore what is possible.  
\update{In particular, we revise a common and critical outlier pruning step, and the key adjustment is ultimately simple: a $\sqrt{d/n}$ term, which vanishes when $n \gg d$, needs to be included in a key threshold.  This is detailed in Section \ref{sec:new-algo}. }

\end{enumerate}

Our experimental study considers mean estimation in a variety of settings, focusing on when $n < d$ or $n$ is not much larger than $d$. 
While other experimental studies have been done, many like in \cite{diakonikolas2017being} provided a comparison in the $n \gg d$ case.  And while \cite{deshmukh2022robustmean} has some experiments with $n$ not much larger than $d$, these are not nearly as comprehensive as our study.  
First, we consider standard Gaussian data with known covariance, and no corruption.  
Then we extend this to the setting with various types of adversarial corruption.  
\update{We also consider some limited cases with unknown covariance.  However, because straight-forward adaptations of mean-estimation approaches towards estimating covariance (mapping to a ${d \choose 2}$-dimensional problem) further stresses the need for data size $n$ as a function of $d$, we defer a thorough exploration of this challenge to future work.}  
Finally, we consider real world data scenarios where data is generated via embeddings resulting from large language models, deep pretrained image models, and word embedding models; here we do not have direct enforcement of Gaussianity of the data, but desire a high-dimensional mean nonetheless.  
In all cases, we consider a wide variety of efficient mean estimation approaches, including both classical ones and modern ones with stronger guarantees in the large $n$ setting.  
We provide an anonymous link to our code for easy reproducability here:
\url{https://github.com/cullena20/RobustMeanEstimation}.






\section{Background}
\label{sec:background}

\begin{wrapfigure}{r}{0.255\linewidth}
\vspace{-4mm}
\begin{tabular}{cl}
    \toprule
    & key notation \\ \midrule 
    $n$ & \# data samples \\
    $d$ & \# dimensions \\
    $\eps$ & error bound \\
    $\eta$ & true corruption \\ 
    $\tau$ & expected corruption \\
    \bottomrule
\end{tabular}
\vspace{-4mm}
\end{wrapfigure}

We consider as input a set $X \subset \R^d$ of $n$ samples from an unknown distribution, and the goal is to estimate the mean of that distribution.  Consider first the case where the distribution is the Gaussian $\N_d(0,I)$ where $I$ is the identity matrix representing an \update{isotropic} covariance. 
\update{For $x \sim \N_d(0,I)$ we have $\E[\|x\|^2] = d$.  For the sample mean $\bar x \in \R^d$ from $n$ points drawn iid from $\N_d(0,I)$ we have $\E[\|\bar x\|^2] = d/n$ and more importantly it strongly concentrates as $\Pr[| \|\bar x\|^2 - d/n | > t] \leq 2 \exp(- C t^2)$ for a constant $C$~\citep{vershynin2011randommatrices}. 
This implies for $n = \Omega(d /\eps^2)$ we have $\|\bar x\| < \eps$ with high probability; but for $d > n$ we do not get useful concentration results.  
The Gaussian is the most studied and used distribution for many reasons including that it has Normal marginals for any dimension, is easy to sample from, models an $\ell_2$ loss, and is the limiting distribution of the central limit theorem.  As such, it is our main object of study.  
However, we note that other distributions have distinct behavior for the large $d$ setting.  For instance, for $n$ samples from a distribution with mean $\mu$ and covariance $\Sigma$, the expected squared deviation from the mean in $d$ dimensions can be bounded by $\Tr(\Sigma)/n$ (c.f., \citep{lee2022optimal}).  This implies for instance if $X$ is drawn uniformly from a unit ball (so $\Tr(\Sigma) = 1$) or other distributions with bounded $\Tr(\Sigma)$, then the behavior for $d > n$ can still be well-concentrated. 
  On the other hand, other unbounded and heavy-tailed distributions where, like Gaussians, $\Tr(\Sigma) = \Theta(d)$}
\footnote{\update{We use standard asymptotic notation so for some constants $C_1, C_2, C_3$ and functions $f,g$ then 
$g(x) = O(f(x))$ implies 
$\forall x > C_3$ then $g(x) \leq C_1 f(x) + C_2$; 
$g(x) = \Omega(f(x))$ implies 
$\forall x > C_3$ then $g(x) \geq C_1 f(x) + C_2$, with possibly different constants; and 
$g(x) = \Theta(f(x))$ implies $g(x) = O(f)$ and $g(x) = \Omega(f(x))$.}}\update{, present similar challenges in the $d > n$ setting. } 














\paragraph{Corrupted data models.}
Another setting considers some fraction $\eta \in (0, \frac{1}{2})$ of the data to be adversarially corrupted from $X$~\citep{huber1964robust,diakonikolas2023algorithmic}.  Under the \emph{Huber model}, we draw data $X \sim (1-\eta) P + \eta Q$ where $P$ is the set of inliers with mean $\mu$ (we consider $P = \mathcal{N}_d(\mu, I)$ as identity covariance Gaussian data), and $Q$ is any adversarial outlier distribution.  The stronger \emph{total variation} corruption model first draws $X' \sim P$ (with mean $\mu$), and then creates $X$ by adversarially changing any $\eta$-fraction of $X'$ to a new location. That is, it can also adversarially subtract data from the inlier data in addition to adding outliers.  
How accurately can we recover the mean $\mu$ under these settings?  We mostly focus on the Huber model, and observe that subtractive corruption (a component of the stronger total variation model) can induce a consistent and hard to avoid error, and does not seem to expose significant differences between approaches.  


















As the mean minimizes the sum of squared deviations, the sample mean is very susceptible to outliers.  A single point of corruption can arbitrarily affect the sample mean.  On the other hand, such corruption can be easily detected by filtering out the furthest points from the sample mean, and recomputing the sample mean on the remainder of the data.  A more challenging setting relocates points to roughly $\sqrt{d}$ from the mean, where the inliers are, but all in a tight cluster; then no individual points can be so easily filtered, but the sample mean can be given a non-trivial bias of as much as $\Omega(\eta\sqrt{d})$.  We will empirically consider a variety of challenging $\eta$-corruption situations.  



For many years\update{, when dealing with} high dimensions, practitioners were faced with either potentially large error (e.g., on order of $\eta \sqrt{d}$) in using the sample mean or other generalizations of the median~\citep{small1990survey}, or one could spend time exponential in $d$ and return an estimator that is guaranteed to be close to the true mean~\citep{tukey1975mathematics} \update{(or c.f., }\citep{chen2015robustcovariance,zhu2020does}). Around 2016, two papers broke this barrier \citep{lai2016agnostic} and \citep{diakonikolas2019robust}.  They considered $X \sim \N_d(\mu,I)$, and allowed an $\eta$ fraction of the data to be corrupted and return an estimate of the mean $\hat \mu$ so that $\|\mu - \hat \mu\| \leq O(\eta \sqrt{\log 1/\eta})$ or $\leq O(\eta \sqrt{\log d})$.  These works however assume $n = \Omega(d/\eta^2)$; otherwise one runs into the roadblock that even the sample mean of the inliers (the uncorrupted points) has more than $\eta$ error.  
Since then, much follow-up work has furthered our understanding.  Some work~\citep{dong2019quantumentropyscoring, cheng2019fast, depersin2019nearlylinear} improved the time complexity of robust mean estimation algorithms, and our understanding of the problem's hardness~\citep{diakonikolas2017statisticalquerylowerbounds, hopkins2019hardrobustmeanestimation}.  Others provide formulations where gradient descent can be used despite non-convexity~\citep{cheng2020graddescent, zhu2020generalizedquasigradients}.  There has also been effort to improve other robust statistics tasks such as covariance estimation \citep{chen2015robustcovariance, chen2017robustcovariance, cheng2019fastrobustcovariance}, sparse estimation \citep{balakrishnan2017robustsparse, diakonikolas2019sparseestimation, cheng2022outlierrobustsparseestimationnonconvex, diakonikolas2022sparseestimation, diakonikolas2024robustsparse}, list decodable learning \citep{charikar2017learninguntrusteddata, diakonikolas2017listdecodable}, robustly learning mixtures of Gaussians \citep{bakshi2022robustgausmix}, robust optimization \citep{diakonikolas2019sever, prasad2018robustgradient}, robust regression \citep{diakonikolas2018robustregression, klivans2020robustregression}, or in the context of adversarial machine learning~\citep{tran2018backdoorattacks}.  \update{Importantly, robust statistics are more amenable to differential privacy, in particular to privacy through noise addition, and privacy mechanisms are naturally robust \citep{dwork2009differential, liu2021robustdifferentiallyprivate, hopkins2023robustnessprivacy, asi2023robustness}.}  Recent work has also expanded methods for different corruptions models \citep{liu2021coordcorr, zhu2020resilience}. 
For a more thorough review see the recent textbook by \cite{diakonikolas2023algorithmic}. 









There has also been significant complementary work in mean estimation under heavy-tailed distributions~\citep{lugosi2021robust, lugosi2022mean, gupta2024catoni, catoni2011challengingempiricalmeanempirical, lugosi2017subgaussianestimatorsmeanrandom, devroye2015subgaussianmeanestimators, lee2022optimal}; see the recent survey by \cite{lugosi2019heavytailsurvey}. Recent work has also developed connections between optimality under heavy-tailed distributions, and optimality in the Huber corruption setting \citep{prasad2019unifiedrobustheavy}. 


\section{Mean Estimation Algorithms}
\label{sec:algos}

Here we will document the mean estimation algorithms considered in this paper.  Some are classic, and we also include several ones from the recent literature designed to be potentially practical and algorithmically efficient. 
Some include asymptotic theoretical bounds which use astronomical constants; we make a best effort to replace them with reasonable values so they remain practical.  
Some use \update{an expected corruption} parameter $\tau$, meant to be an upper bound true corruption, $\eta$.
The ones we consider are as follows:  


\textbf{\sample}: The \emph{sample mean} simply returns $\hat \mu = \frac{1}{|X|} \sum_{x \in X} x$.  

\textbf{\coordmed}: The \emph{coordinate-wise median} computes the median of each coordinate individually so $\hat \mu_j = \mathsf{median}(\{x_{i,j} \mid x_i \in X \})$.  

\textbf{\coordprune}: First compute a \emph{trimmed mean estimator} for each coordinate individually, parameterized by a value $\tau \in (0,1)$.  That is, in one dimension, it sorts the data, and removes $\tau |X|$ points which have the smallest values, and also removes $\tau |X|$ with largest values.  Then it computes the mean of the remaining $(1-2\tau)|X|$ points.  The \emph{coordinate-wise trimmed mean} applies this estimator separately for each coordinate; which points are removed in coordinate $j$ have no bearing on which points are removed from coordinate $j'$~\citep{lugosi2021robust}.  



\textbf{\medmean}: Split the data into $k$ chunks, find the mean of each chunk, take the coordinate wise median of these $k$ means~\citep{lugosi2019heavytailsurvey, minsker2023ustatisticsgrowingordersubgaussian, minsker2023efficientmedianmeansestimator}. As a default, we set $k=10$; this hyperparameter is explored in Appendix \ref{app:hp_tuning}.  







\textbf{\geomed}: The \emph{geometric median} is the point which minimizes the sum of distances to all sample points. This is iteratively approximated using the Weiszfeld algorithm~\citep{small1990survey,vardi2001modified}.

\textbf{\lvog}: (\cite{lee2022optimal}) The Lee and Valiant algorithm first estimates the mean $\mu'$ on a $\gamma$ percentage of data points $X_\gamma$ using a mean estimator.  It then centers all points to $X' = \{x' = x - \mu' \mid x \in X\}$.  Let $X_t$ be the $t$ points in $X$ so their corresponding $x'$ have the largest norm.  Let $X'_*$ be the subset consisting of $x' \in X'$ with their corresponding points \emph{not} in $X_\gamma$ or in $X_t$.  
Then return $\mu' + \frac{1}{|X|} \sum_{x' \in X'_*} x'$. 
Rather than the extremely large constants in the original paper, we set $\gamma = 0.5$ and $t=\tau |X|$.  As default, we use $\medmean_k$ estimator with $k=10$ to obtain the initial mean estimator $\mu'$. 


















\textbf{\lrv}: (\cite{lai2016agnostic})
The \lrv method recursively reduces the dimension by half, until $1$ or $2$ dimensions remain.  Following the original author's code\footnote{\url{https://github.com/kevinalai/AgnosticMeanAndCovarianceCode}}, in the $(\leq 2)$-dimensional base case, it returns \coordmed. The recursive step has three components.  
First, it calculates a weight $w_i$ for each point $x_i$ as $w_i = \exp(-\|x_i - a\|^2/(C s^2))$ where $s^2$ is a robust sample estimate of the trace of the true covariance matrix, $a$ is a rough estimator of the mean chosen as \coordmed, and $C$ is a hyperparameter. We use $C=1$; this hyper parameter is explored in Appendix \ref{app:hp_tuning}. 
Second, it computes $\mu_{w} = \frac{1}{|X|}\sum_{x_i \in X} w_i x_i$, which is the weighted mean of the input, and $\Sigma_{w} = \frac{1}{|X|} \sum_{x_i \in X} w_i (x_i - \mu_{w}) (x_i - \mu_{w})^T$, which is the weighted covariance of the input.  Let $V$ by the span of the top $\lfloor d/2 \rfloor$ singular vectors of $\Sigma_w$; let $V_\perp$ be the span of the bottom $\lceil d/2 \rceil$ singular vectors of $\Sigma_w$.  
Third, recurse on data projected onto $V$, and return an estimate $\mu_1$.  We also build an estimator $\mu_2$ of the data projected onto the $\lceil d/2 \rceil$-dimensional remainder space $V_\perp$ using the weighted sample mean projected onto $V_\perp$: that is $\mu_2 = \frac{1}{|X_\perp|}\sum_{x_i^\perp \in X_\perp} w_i x_i^\perp$ where $X_\perp$ is the data projected onto $V_\perp$. Finally return $\mu_1 + \mu_2$.  












\textbf{\ev}: (\cite{diakonikolas2019robust,diakonikolas2019sever}) 
This method observes that when inliers are from a standard Gaussian, then a set of corrupted data which substantially affects the mean estimate must result in a sufficiently large top eigenvalue after centering (i.e., of the sample covariance matrix), and this can be remedied by pruning points which are far along the top eigenvector.  
In this method, if after centering by the sample mean $\hat \mu$, the top eigenvalue exceeds $O(\tau \log {1/\tau})$ (\cite{diakonikolas2017being}\footnote{\url{https://github.com/hoonose/robust-filter}} implements this as $1 + 3 \tau \log(1/\tau)$), then this data is considered additively corrupted along the direction of the top eigenvector. We call this the corruption detection step. Then they consider all points projected onto the associated top eigenvector and sorted $P = \langle p_1, \ldots, p_n\rangle$; and then a set of points furthest from the median $\mathsf{med}(P)$ are pruned.  We call this the pruning step.  The determination of which points to prune is based on those which exceed a Gaussian concentration inequality. Specifically, it finds the smallest index $i$ so $T_i = p_i - \mathsf{med}(P) - 2\tau$ satisfies $\frac{n-i}{n} > \gamma (\mathsf{erfc}(T_i /\sqrt{2})/2 + \tau/(d \log(d \tau / 0.1))$, where $\mathsf{erfc}$ is the complementary error function ($1 -$ the cdf of the Normal) and prunes all points $i$ or larger.  Intuitively, the centered projected data is expected to be a standard Normal distribution, and this bound compares the true percentage of points that exceed a threshold, $T_i$, with the probability that points will exceed that threshold, given by $\mathsf{erfc}$ with some slack terms added. Then the algorithm is recursively called with all points not-yet pruned until the top eigenvalue threshold is not violated. This algorithm critically assumes identity covariance and $n = \Omega(d/\tau^2) \gg d$.  


\textbf{\que}:
(\cite{dong2019quantumentropyscoring})
Quantum Entropy Scoring, \que for short, scores outliers based on quantum entropy regularization, and returns a mean using the same structure as \ev, but with a modified pruning procedure. Rather than pruning points based on their projection onto the top eigenvalue, points are given outlier scores relevant to all directions. First, calculate the normalized matrix exponential $U=\mathsf{exp}(\alpha \Sigma) / \mathsf{tr}(\mathsf{exp}(\alpha \Sigma))$ where $\alpha \geq 0$ is a hyperparamater and $\Sigma$ is the sample covariance. Then, calculate a vector of quantum entropy scores, $w$, with $w_i$ = $(x_i - \mu')^T U (x_i - \mu')$, where $x_i$ is the $i$th data point and $\mu'$ is the sample mean. This is implemented efficiently using a Chebyshev expansion of the matrix exponential and Johnson-Lindenstrauss approximations. Points with the largest scores are pruned, and the algorithm continues recursively with the remaining points until the top eigenvalue threshold is not violated. Following the original author's code \citep{dong2019quantumentropyscoring}\footnote{\url{https://github.com/twistedcubic/que-outlier-detection}}, we prune $\tau/2$ percentage of points during every iteration. Additionally, while the author's provide a theoretical threshold on the top eigenvalue, the constants are not given. Rather than tuning this threshold, we implement it using the same threshold as \ev; that is $1 + 3 \tau \log{1 / \tau}$. Because of this threshold, the algorithm critically assumes identity covariance and $n = \Omega(d/\tau^2) \gg d$.  We set $\alpha=4$ as in the author code; simple experiments show little variation with $\alpha$ between $0.5$ and $200$.  


\textbf{\pgd}:
(\cite{cheng2020graddescent})
Projected Gradient Descent, \pgd for short, frames robust mean estimation as a non-convex optimization problem, and despite non-convexity, directly solves this using gradient descent. \pgd finds a vector, $w$, of outlier scores, which can then be used to return a mean estimate $\mu' = \frac{1}{|X|}\sum_{x_i \in X} w_i x_i$. $w$ is found to minimize the spectral norm of the standard weighted covariance matrix, $\Sigma_w$, subject to the constraint that the weights represent at least a $(1-\tau)$-density fractional subset of the dataset. The vector $w$ is found as an approximate stationary point to this objective by first performing gradient descent on  the spectral norm of the weighted covariance matrix, and then projecting onto the simplex of feasible weight vectors. First, define a function $F(u, w) = u^T \Sigma_w u$. Then, repeat the following for $\gamma$ iterations, where $\gamma$, following the conventions of a code implementation by the same author as the original paper \citep{cheng2021robustlearningfixedstructurebayesian}\footnote{\url{https://github.com/chycharlie/robust-bn-faster}}, is a hyperparameter. Calculate the top eigenvector, $u_t$, of $\Sigma_w$, which corresponds to finding the unit vector $u_t$ such that $F(w, u_t) \geq (1-\tau) \mathsf{max}_{u}F(w, u)$. 
Then, update $w$ as $w = P(w - \alpha \nabla_w F(w, u_t))$ where $P$ projects onto $\Delta_{n, 2\tau} = \{w \in \mathbb{R}^n : \|w\|_1=1$ and $0 \leq w_i \leq \frac{1}{(1-2\tau)n}\}$, and $\nabla_w F(w, u_t)) = X u_t \odot  X u_t - 2 (w^T X u_t) X u_t$ where $\odot$ indicates element-wise multiplication, and $\alpha$ is the learning rate, initialized as $1/n$ and updated dynamically through learning. We set the number of iterations $\gamma=15$; this hyperparameter is explored in Appendix \ref{app:hp_tuning}.




\textbf{\lmin}:
(\cite{deshmukh2022robustmean})
This method frames robust mean estimation as a semi-definite program (SDP). Similar to \pgd, a vector, $w$, of outlier scores is found, and the weighted mean $\mu' = \frac{1}{|X|}\sum_{x_i \in X} w_i x_i$ is returned. The $\ell_p$ norm for hyperparameter $0 \leq p \leq 1$ is maximized with respect to $w$, under the constraint that the top eigenvalue of the weighted covariance matrix is less than a constant. The weight vector $w$ is iteratively updated by solving a SDP until the number of iterations is less than a bound determined by $\tau$, in which case $\hat{\mu}$ defined above is returned. Update $w$ by approximately solving an SDP to maximize $w$ in $\|w\|_1$ over $\Delta_{n, \tau}$. Each step of the optimization problem is convex and can be solved as the following packing SDP:
\[
\mathsf{max}_w \quad \text{s.t.} \quad w_i \geq 0 \; \forall i, \quad \sum_{i=1}^{n} w_i 
\begin{bmatrix}
e_i e_i^T &  \\
 & (x_i - \mu_w)(x_i - \mu_w)^T
\end{bmatrix}
\preceq
\begin{bmatrix}
I_{n \times n} &  \\
 & c_\tau n I_{d \times d}
\end{bmatrix},
\]

where, $c_\tau$ is a function of $\tau$.  
This analysis of this algorithm critically assumes identity covariance and $n = \Omega(d/\tau^2) \gg d$. 




\subsection{New Algorithms and Variants}
\label{sec:new-algo}
We also consider a few new methods, with subtle but important extensions of these existing ones.  

The primary insight needed to adapt methods to the $d \geq n$ case is found by revisiting how we identify outliers with respect to a $d$-dimensional Gaussian distribution.  The bounds used in the $n \gg d$ case have enough data in each direction $d$ to concentrate, whereas in the $d \geq n$ case we need to account for this additional variance.  The key result leverages a theorem of \cite{vershynin2011randommatrices} to understand the concentration of the top eigenvalue of the sample covariance matrix.

\begin{theorem}
\label{thm:Sigma2-bound-main}
Let $X$ be a $n \times d$ matrix whose entries are independently drawn from $\mathcal{N}(\mu, I)$. Let $\Sigma = \frac{1}{n}(X-\bar{\mu})^T(X-\bar{\mu})$ be the sample covariance matrix of $X$, where $\bar{\mu} = \frac{1}{n} \sum_i X_i$ and $X_i$ is the $i$th row of $X$. Then for every $t > 0$, with probability of at least $1 - 3 \exp(-t^2/2)$, one has 
\[
\  \|\Sigma\|_2 \leq \left(1 + \sqrt{d/n} + t/\sqrt{n} + \frac{\sqrt{d + \sqrt{2d}t + t^2}}{n} \right)^2.  
\]
\end{theorem}

The proof is deferred to Appendix \ref{app:ev_theory}.  A more convenient form shows that the fourth term is lower-order and can be absorbed into the probability of failure.  

\begin{corollary}
Under the same setting as Theorem \ref{thm:Sigma2-bound-main}, if one assumes $d/n \leq 16, n \geq 16, t \geq 5$, then with probability of at least $1 - 3 \exp(-t^2/8)$, one has 
\[
\  \|\Sigma\|_2 \leq \left(1 +  \sqrt{d/n} + t/\sqrt{n} \right)^2.  
\]
\label{cor:prune-2t-main}
\end{corollary}




\textbf{\evln}:  The \ev algorithm assumes the sample size is $n = \Omega(d/\tau^2)$.  This assumption is used in several parts of the analysis, and it allows the filtering bound to be simplified to $1 + \tau \log(1/\tau)$; however, when $n = o(d/\tau^2)$, this simplification does not hold, and the filtering bound needs to depend on $d$.  
We instead filter points if the top eigenvalue $\lambda_{\max} > (1 + \sqrt{d/n} + t/\sqrt{n})^2$ using Corollary \ref{cor:prune-2t-main}.  We set $t=10$ to achieve almost $100\%$ ($\approx 0.999$) 
success. All other steps of the algorithm remain the same.

\textbf{\queln}: The \queln algorithm extends the same filtering bound as \ev. Although their paper mentions a $O(\sqrt{d/n})$ factor in the error, the code seems to assume $n = \Omega(d/\eps^2)$ and is implemented very similar to \ev.  As this approach does not work under low data size, in our newly proposed variant, we instead filter points if the top eigenvalue $\lambda_{\max} > (1 + \sqrt{d/n} + t/\sqrt{n})^2$ using Corollary \ref{cor:prune-2t-main}.



\textbf{\lminln}: The \lmin algorithm uses the condition that 
the top eigenvalue of the weighted covariance matrix is bounded by $c_\tau n$ where $c_\tau$ is a hyperparameter suggested to be set at  $1 + \tau \log(1/\tau)$.  As previously observed, this threshold does not hold when $n = o(d/\tau^2)$ and to account for this, in our newly proposed variant we set $c_\tau = (1 + \sqrt{d/n} + t/\sqrt{n})^2$, using Corollary \ref{cor:prune-2t-main}.





\textbf{\lvsim}: We use a simplified version of the Lee and Valiant algorithm~\citep{lee2022optimal}, which aligns with an informal description in their abstract.  It completely removes the $\tau$ percentage of points classified as outliers rather than simply downweighting them. That is, it returns $\hat \mu =  \frac{1}{|X'_*|} \sum_{x' \in X'_*} x$; the average of all points $X'_*$ which were not in the original estimate, nor from the pruned set furthest from $\mu'$. 


























\section{Experiments}
\label{sec:expers}

We generally evaluate the performance of these mean estimation algorithms as data size $n$, dimension $d$, and corruption $\eta$ are varied. Error is measured as the Euclidean distance $\|\mu - \hat \mu\|$ between the true mean $\mu$ and the estimate $\hat \mu$ returned by a mean estimation algorithm. We set the default values as $n=500$, $d=500$, and $\eta=0.1$. We examine the performance as we fix one of these variables and vary the others under various distributions for both the uncorrupted and corrupted data. We first examine uncorrupted standard normal Gaussian data, demonstrating that nothing really improves upon \sample, and observing the robustness of mean estimation techniques when applied to uncorrupted data. We then examine corrupted Gaussian data over various covariances and noise distributions.  The example distributions are chosen among challenging examples in the literature meant to distinguish various models. Experiments were run on a 2022 Macbook Air with Apple M2 Chip, 16GB memory, running MacOS 12. 

At one point in our experiment, the values of $n$, $d$, and $\eta$ are used to generate data according to a supplied data generation function and noise scheme (both of which will vary depending on the experiment). A mean estimate is made on this data using each of the mean estimators being tested. For each mean estimator, error is then stored as the Euclidean distance between the true mean of the data and the returned mean estimate. These errors are accumulated over 5 runs and averaged. We additionally plot the error incurred by the sample mean of the original uncorrupted data, which we call the \gsample error.  This serves as a valuable baseline for comparison. In practice, we can not expect to achieve error better than the sample mean of the inliers. Therefore, a reasonable goal for a robust estimator is to closely match the performance of \gsample, thereby removing the effects of corrupted data points. Could a robust mean estimator somehow improve upon this?  We do not observe this; but we will observe methods that basically match \gsample, even without $n = \Omega(d/\eps^2)$.




\paragraph{Fraction of corrupted data.}
Some algorithms are designed for data where a $\eta$-fraction of the data has been corrupted.  And in some cases, this fraction is taken as a parameter $\tau$ used within the algorithms (\coordprune, \lvsim, \lvog, \ev, \evln, \que, \queln, \pgd, \lmin, \lminln).



In theory, these algorithms work best using their parameter $\tau$ set to the true fraction of corrupted data $\eta$, and may even result in arbitrary error if the parameter $\tau$ is not set to at least an upper bound for the true fraction.  However, increasing the value of $\tau$ in the algorithms also theoretically increases the error incurred by algorithms. 
Recent work by \cite{jain2022robustestimationalgorithmsdont} showed a meta algorithm that allows robust estimation algorithms to perform asymptotically optimal without knowing true corruption $\eta$. We investigate robustness to expected corruption, $\tau$, empirically, in Appendix \ref{app:expected_corruption}, as we fix $\tau$ and vary true corruption $\eta$. We observe that the best algorithms do not show a strong dependence on this relationship, so long as $\tau$ is an upper bound on $\eta$. Hence, for all other experiments, we simply set the parameter $\tau$ according to the true corrupted fraction $\eta$ or to $\tau=0.1$ if the data is not corrupted.



\paragraph{Selecting algorithmic variants.}
There are many algorithms to be considered, and plots can become cluttered.  To reduce this, we perform some comparison among variants. We summarize key findings here, with further details deferred to Section \ref{sec:variants}.  

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
    \hline
    \multirow{2}{*}{\textbf{Algorithm}} & \multicolumn{2}{c}{\textbf{$n=500$, $d=500$}} & \multicolumn{2}{c}{\textbf{$n=200$, $d=500$}} \\
    \cline{2-5}
     & \textbf{Error} & \textbf{Time (s)} & \textbf{Error} & \textbf{Time (s)} \\
    \hline
        \sample  & 2.47 ± 0.04     & 0.00019 ± 0.000002 & 2.74 ± 0.05    & 0.00019 ± 0.000001 \\
        \lrv     & 1.14 ± 0.04     & 0.81 ± 0.12        & 1.76 ± 0.10    & 0.64 ± 0.03 \\
        \pgd     & 1.08 ± 0.02     & 82.4 ± 8.8         & 1.68 ± 0.05    & 72.5 ± 3.2 \\
        \evln    & 1.07 ± 0.02     & 0.20 ± 0.02        & 1.69 ± 0.04    & 0.08 ± 0.03 \\
        \ev      & 13.49 ± 3.56    & 0.48 ± 0.15        & 17.06 ± 5.92   & 0.05 ± 0.02 \\
        \queln   & 1.04 ± 0.03     & 0.71 ± 0.05        & 1.70 ± 0.049    & 0.35 ± 0.03 \\
        \que     & 20.81 ± 0.40    & 2.70 ± 0.08        & 20.88 ± 0.38   & 1.99 ± 0.04 \\
        \lminln  & 1.17 ± 0.04     & 1182.6 ± 35.3      & 1.67 ± 0.03   & 265.9 ± 15.2 \\
        \lmin    & 1.62 ± 0.04    & 1076.8 ± 43.9      &   5.62 ± 0.40  & 250.07 ± 17.2 \\
        \hline
    \end{tabular}
    \caption{Error and Runtime Across Simple Corrupted Identity Covariance Gaussian}
    \label{tab:time}
\end{table}









First, we do not consider \lmin and \lminln in our plots due their exceptionally large runtimes. We report runtimes and errors (defined as the Euclidean distance from the estimated mean to the true mean) of selected algorithms under $n=500$ and $d=500$  and under $n=200$ and $d=500$ over a simple corrupted Gaussian distribution in Table \ref{tab:time}. We report results as the mean $\pm$ the standard deviation, averaged over 5 runs. With the notable exceptions of \lminln, \lmin, and \pgd, most estimators are efficient and took under 3 seconds to run with $n=500$ and $d=500$ for a simple corrupted Gaussian distribution. \lmin and \lminln rely on an SDP solver, which we implement with the cvxpy \citep{diamond2016cvxpy, agrawal2018rewriting} package and the mosek solver. Although this is theoretically efficient, it is slow in practice for the data scale and dimesionality we consider in this paper.  For $n=500$ and $d=500$, both algorithms took about 1100 seconds, or about 18 minutes, to return a mean estimate over a simple corrupted data scheme. 
For that reason, and since we run many trials of each input size and error level, we do not consider these algorithms in our plots. However, we note that employing Corollary \ref{cor:prune-2t-main} for \lminln achieves a noticeable performance increase over \lmin; showing gains from $1.62$ error to $1.17$ error in the $n=500, d=500$ case and from $5.62$ to $1.67$ error in the $n=200, d=500$ case. \pgd is also much slower than other robust estimators, taking approximately 80 seconds to run with $n=500$ and $d=500$. While this significant slow down is relevant when considering a practical algorithm, it is not as prohibitive as \lmin. As a result, we include it in all of our plots. 





Second, we observe that when the data does not satisfy that $n \gg d$, then both \ev and \que can have catastrophic failure.  Our variants \evln and \queln avoid this issue in the $d > n$ and $d \approx n$ settings, while basically matching the effectiveness of their original versions when they do not have catastrophic failure.  This result is highlighted in Table \ref{tab:time}, where we observe that both \que and \ev achieve significantly worse error than \queln and \evln respectively. As a result, we use \evln and \queln in all comparisons.

Thirdly, we find that \lvsim performs slightly better than the original \lvog; however the difference is fairly small.  We also do not notice any meaningful advantages from using \lvsim or \lvog with different choices of initial mean estimators. As such, we only use \lvsim in all comparisons. 



\subsection{Uncorrupted Gaussian Data with Identity Covariance}
\label{sec:uncorr}



We first evaluate the performance of mean estimation algorithms over uncorrupted Gaussian data with identity covariance. In particular, we draw uncorrupted data $X \sim \mathcal{N}_d(\mu, I)$, where $\mu$ is an arbitrary mean and $I$ is identity covariance. For these experiments, we set $\mu$ to be the all-fives vector, but did not find performance to depend on $\mu$. For algorithms that utilize $\tau$, expected corruption, as input, we use the default value of $\tau=0.1$.





\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{UpdatedFigures/uncorrupted.png} 
\caption{Uncorrupted Gaussian Identity Covariance}
\label{fig:uncorr}
\end{figure}

We provide our first experimental plots in Figure \ref{fig:uncorr}; most further experiments will follow this same set-up, consisting of a set of $4$ charts, each measuring the Error $\|\mu - \hat \mu\|$ on the $y$-axis.  The top two charts vary the data size $n$ along the $x$-axis, but on different scales.  The top left shows a large scale from $n = 20$ to $n=5020$, focusing on the $n > d = 500$ paradigm.  The top right shows $n=20$ to $n=520$, focusing on the $n < d$ paradigm.  
The bottom left plot show the effect of varying the dimension from $d=20$ to $d=1020$ while fixing $n=500$.  
The bottom right shows varying the algorithm's parameter, $\tau$, for the expected noise from $0$ to $0.45$ with fixed $n=500$, $d=500$.  
Each algorithm is shown as a curve, with the average error of $5$ independent data generations at regular intervals on the $x$-axis.  A shaded area is shown at a radius of $1$ standard deviation from that average error value. 

The plots are a bit cluttered because most algorithms perform about the same, including \sample.  No algorithm can be seen to noticeably outperform \sample,
\update{which, as the MLE for this data, and by the Gauss-Markov theorem, is not surprising.    
Methods } \lrv, \evln, \queln, \pgd,  \coordprune, \geomed, and \lvsim have about the same error in most cases.  However, \medmean, and \coordmed perform slightly worse, with the gap becoming more apparent in high dimensions.  Moreover, \lvsim and \coordprune do significantly worse with a higher expected corruption parameter $\tau$. This is a result of expected corruption, $\tau$, being a hyperparameter that directly controls the percentage of points to prune.
Finally, as predicted by basic theory, with $n$ fixed as the dimension $d$ increases, the measured error increases at a rate roughly $\sqrt{d}$.  









\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{UpdatedFigures/IdCov/id_gaus_one.png} 
\caption{Corrupted Gaussian Identity Covariance: Additive Variance Shell Noise}
\label{fig:sq-d-corrupt}
\end{figure}

\subsection{Corrupted Gaussian Data with Identity Covariance}
\label{sec:corrid}

We evaluate corrupting noise added to Gaussian data with identity covariance.  In particular, we draw $X \sim (1-\eta) P + \eta Q$ where $P = \mathcal{N}_d(\mu,I)$ and $Q$ describes the corrupted data distribution. This is equivalent to the more general case where any covariance $\Sigma$ is known, as we could simply scale the data to have identity covariance, apply these methods, and scale the mean estimate back. We provide a wrapper in our implementation to perform this operation.

\paragraph{Gaussian noise shifted to variance shell.}
We first consider corrupted data distribution 
$Q = \mathcal{N}_d(\mu', \frac{1}{10} I)$ so $\|\mu - \mu'\| = \sqrt{d}$.  
Since $\E_{x \sim P} [\|x - \mu\|^2] = d$, corrupted data from $Q$ is not easily identified.  The location of this cluster is determined by a random rotation at every generation to ensure that no coordinate-axis specific bias is introduced.
This is shown in Figure \ref{fig:sq-d-corrupt} in the same 4 experiments as with uncorrupted data, except now the bottom right figure varies $\eta$, the fraction of corrupted data from $Q$, along the $x$-axis. We set the expected corruption hyperparameter equal to true corruption, that is $\tau = \eta$.  
\update{In Appendix \ref{app:expected_corruption} we explore the relation between expected corruption $\tau$ versus actual corruption $\eta$; for the most part  as long as $\tau > \eta$.}  


There is now more clear separation between the algorithms designed for adversarial corruption, and those not. 
Here \evln, \queln, and \pgd do the best among all settings, with \lrv  \update{, perhaps doing the best, even appearing better than \gsample for large dimensions, although within 1 standard deviation error margin}.  Due to the high dimensionality, $d$, \gsample, the sample mean of points from the uncorrupted part of the distribution $P$, does not have error approaching $0$ until $n$ is very large. \evln, \queln, and \pgd work so well that they are nearly overlapping this best possible standard.  
Also, perhaps surprisingly, \medmean also does nearly as well, especially under larger $n$, though it degrades much worse with larger $\eta$. 

In contrast, \coordmed, \sample, \coordprune, \geomed, and \lvsim all do considerably worse, even with large data size.  With large corruption levels, these all even seem to do worse than just \sample, indicating that the algorithms prune the wrong data points or face some other similar issue.  























\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{UpdatedFigures/IdCov/id_dkk.png} 
\caption{Corrupted Gaussian Identity Covariance: DKK Noise}
\label{fig:DKK-noise}
\end{figure}



\paragraph{Large + Subtle outliers: DKK Noise.} 

We now recreate the noise distribution from \cite{diakonikolas2017being}, which utilizes a more sophisticated corruption scheme that includes both easier and harder to detect outliers. Half of the noise is drawn from the product distribution over the hypercube where every coordinate is -1 or 0 away from the true mean at that coordinate with equal probability. The other half is drawn from the product distribution where the first coordinate is either 11 or -1 away from the true mean at that coordinate with equal probability, the second coordinate is -3 or -1 away from the corresponding true mean coordinate with equal probability, and all remaining coordinates are -1 away from the true mean.
We call this corruption scheme DKK Noise. 
This is shown in Figure \ref{fig:DKK-noise}, with similar results.  \evln, \queln, \pgd, and \lrv achieve performance nearly matching \gsample, with \medmean also doing almost as well -- at least while the dimension $d$ and rate of corruption $\tau$ are on the smaller side.  
Other than \medmean, all classic methods perform noticeably worse than \gsample and achieve similar error to \sample. The only difference of note here is that \lvsim exhibits far larger error bars, suggesting that its performance may vary significantly depending on random initializations made within the algorithm.  Also, \lrv may even outperform \gsample for very large dimensions.  


\paragraph{Subtractive noise.}
We additionally consider subtractive noise in Figure \ref{fig:subtractive}. Here, an adversary is able to remove a $\eta$ percentage of points from the data distribution. We implement this by removing the $\eta$-percentage of points which are most extreme in some direction.  Unlike in the additive corruption case, there is a strict upper bound on the error under subtractive corruption from a standard Gaussian distribution; the error induced is bounded as $O(\eta)$ even using \sample, and clustering the subtracted points as most extreme in some direction ensures their effect is $\Omega(\eta)$ under \sample. In general, we wish to consider noise distributions that may add outliers and also remove inliers through such subtractive noise.  However, we do not find any surprising capabilities among methods in this scenario.  As a result, for the remainder of this paper, we focus on additive corruption. 



\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{UpdatedFigures/IdCov/id_sub.png} 
\caption{Corrupted Gaussian Identity Covariance: Subtractive Noise}
\label{fig:subtractive}
\end{figure}

Under subtractive corruption, nothing outperforms \sample; and now nothing can match \gsample in error in most settings. However, \evln, \queln, \pgd, and \lrv all nearly match the performance of \sample. Unlike in the previous additive corruption schemes, \medmean performs significantly worse under subtractive corruption, always achieving error notably worse than \sample. 
Among other estimators, \geomed nearly matches \sample error across all settings, \lvsim and \coordprune perform similarly but degrade much more under larger corruption, while \coordmed performs significantly worse.  


We find similar results across several other noise distributions. In addition to the hard-to-detect distributions, we also show that \evln, \queln, \pgd, and \lrv are generally robust to arbitrary outliers. These details are deferred to Appendix \ref{app:idcov_morenoise}.


\subsection{Corrupted Gaussian Data with Unknown Covariance}
\label{sec:corrsph}

We now evaluate corrupted Gaussian data for general unknown covariance. Since \evln and \queln rely on the identity covariance assumption, we employ a simple heuristic to adapt these algorithms to the unknown covariance case. We estimate the trace of the covariance as $\Tr(\hat{\Sigma}) = \frac{1}{n-1} \sum_{i = 1}^n ||x_i - \hat{\mu}||^2$, where $\hat{\mu}$ is the sample mean and $\hat{\Sigma}$ is the sample covariance. We rescale the data to $X’ = \{x’_i = x_i/\sqrt{\frac{\Tr(\hat{\Sigma})}{d}} \mid x_i \in X\}$. We then estimate the mean of $X'$, rescale this estimate by $\sqrt{\frac{\Tr(\hat{\Sigma})}{d}}$, and report the results. This heuristic is used for \evln and \queln across all unknown covariance experiments, and not used for any  other algorithms.  The other standard algorithms are either invariant to this linear rescaling, or themselves account for it; this was supported by our own observations.  


Another possible method is to utilize a robust covariance estimate instead of a sample trace estimate, as discussed in \cite{diakonikolas2023algorithmic}. We do not evaluate such methods, as this would involve a thorough study into robust covariance estimation methods over low data size, which goes beyond the scope of this work. Naively treating robust covariance estimation as robust mean estimation in $d^2$ dimensions further exasperates issues related to low data size. We also choose to use a simple sample trace estimate rather than the robust approach proposed by \cite{lai2016agnostic}. We find that the approach proposed often results in significant underestimates across difficult noise distributions, causing \evln and \queln to fail catastrophically. We note that these underestimates are potentially more harmful than overestimates as through them, even the inlier data may not pass the threshold, causing continuous pruning. While a sample trace estimate approach is more prone to overestimates, this can be remedied by naively pruning large outliers. \cite{diakonikolas2017being} also provides an algorithm for unknown covariance mean estimation similar to \ev, but the corruption detection threshold is not easily adapted to the low data size case. 





\subsubsection{Unknown Spherical Covariance}

We evaluate corrupting noise added to Gaussian data with spherical covariance. We draw $X \sim (1-\eta) P + \eta Q$ where $P = \mathcal{N}_d(\mu,\sigma^2I)$ and $Q$ describes the additive corrupted data distribution. We consider $\mu$ to be the all-fives vector and $\sigma=5$.






\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{UpdatedFigures/LargeSpherical/large_sp_gaus_one.png} 
\caption{Corrupted Gaussian Large Spherical Covariance: Additive Variance Shell Noise}
\label{fig:largesp_varshell}
\end{figure}

\paragraph{Gaussian noise shifted to scaled variance shell}
We adapt the identity covariance noise distribution models by appropriately scaling coordinates by $\sigma$. We first consider the corrupted data distribution 
$Q = \mathcal{N}(\mu', \frac{1}{10} I)$ so $\|\mu - \mu'\| = \sigma \sqrt{d}$.  
With $P$ now having covariance $\sigma^2I$, $\E_{x \sim P} [\|x - \mu\|^2] = \sigma^2 d$, and corrupted data from $Q$ is not easily identified. Results are show in Figure \ref{fig:largesp_varshell}. 

While the overall error here is higher, matching the theory that even for uncorrupted data, the sample mean is expected to have error of $O(\sigma \sqrt{d/n})$, the relative performance of algorithms is nearly identical to the identity covariance case. \evln, \queln, and \pgd nearly match \gsample error throughout, with \lrv performing only slightly worse. \medmean lags behind both estimators but still performs noticeably better than \sample. However, \lvsim performs much better in this scenario, nearly exactly matching \gsample except with large enough corruption \update{-- where with $\eta = 0.45$, it and \medmean probably confuse which points are inliers and have much worse error}. Other methods perform similarly to \sample or worse. 



As in the identity covariance case, we find similar results across noise distributions. The only notable exception is for \evln, which sometimes performs slightly worse and doesn't always converge to \gsample error as $n$ increases, probably due to instabilities in the trace scaling heuristic. We additionally show that relative performance of algorithms is mostly independent of the choice of $\sigma$. The exception to this is \lrv, which notably outperforms all other methods, including \gsample, with large enough $\sigma$ across noise distributions. These details are deferred to Appendix \ref{app:unknownsp_morenoise}.


\paragraph{Unknown Non-Spherical Covariance}
In Appendix \ref{app:unknown_non_sp} we also explore the unknown, non-spherical covariance case.  This is even more sensitive to the covariance estimate, and so is further outside the primary scope of this study. Nonetheless, we continue to observe that the best robust estimators, including \queln, continue to perform well. 

\section{Large Language Model Experiment}
\label{sec:realworld}

To evaluate whether robust mean estimation methods are overly sensitive to distributional assumptions, we evaluate performance over real world data. We first study the problem of estimating the mean of \update{vectors from language models}. \update{Such ``word embeddings'' have had an enormous impact on natural language processing, starting from simple sparse term-frequency vectors~\citep{robertson2009probabilistic}.  Second generation word embeddings (e.g., GloVE~\citep{pennington-etal-2014-glove} and word2vec~\citep{mikolov2013distributed}) made the advancement of creating a ``low" dimensional vector (about 300 dimensions) for each word, where Euclidean (and cosine) distances could be used as a proxy for the similarity between words based on how they are used.  As a serendipitous side-effect, structure emerged where dot-products, means, and linear classifiers made sense in this embedding space~\citep{bolukbasi2016man,dev2019attenuating}.  Third generation embeddings created representations for each word in the context of the nearby words; that is, each use of a word had a different embedding.  These were a first main use of transformer architectures, and implicitly capture more meaning and context with progressive layers of a neural network.  As is most common, we use the last layer of the embedding network as the representation of a word. } \update{Our first study, shown next, uses these third generation word embeddings.}  
Further experiments over deep pretrained image model embeddings and \update{second generation} word embeddings are deferred to Appendix \ref{app:image_experiments} and Appendix \ref{app:word_experiments},  respectively. 

We first examine performance over \update{third generation} embeddings of a \update{homonym word where all instances} correspond to the same \update{meaning}. We then examine performance where embeddings corresponding to one \update{meaning} of a word are corrupted by embeddings corresponding to another \update{meaning} of the same word. Effectively calculating this mean may be important to many downstream tasks (e.g., topic modeling~\citep{griffiths2004finding,blei2009topic}, bias estimation and attenuation~\citep{bolukbasi2016man,dev2019attenuating}).   This models a realistic form of corruption that may arise within LLM embedded data.  


We build a dataset of 400 sentences that use the word "field" corresponding to the following definition: "an area of open land, especially one planted with crops or pasture, typically bounded by hedges or fences". We build another dataset of 400 sentences that use the word "field" corresponding to the following, alternate definition: "a particular branch of study or sphere of activity or interest". From now on, we refer to these as "fields of land" and "fields of study". We generated these sentences using ChatGPT-4o.  For more details on how we generate this dataset, and the exact sentences used, see Appendix \ref{app:llm_dataset}.  We embed these sentences and extract the in-context embeddings for the word "field" using 4 LLMs of varying embedding dimensions: MiniLM~\citep{wang2020minilm}, T5~\citep{raffel2023t5}, BERT~\citep{devlin2019bert}, and ALBERT~\citep{lan2020albert}. MiniLM has an embedding dimension of 384, T5 has one of 512, BERT and ALBERT have embedding dimensions of 768. We choose these 4 LLMs to sample a variety of models across different dimensionalities. 


\subsection{Common Definition Embeddings}





\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/MiniLM_Field_Land.png}
        \caption{MiniLM}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/T5_Field_Land.png}
        \caption{T5}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/Bert_Field_Land.png}
        \caption{BERT}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/Albert_Field_Land.png}
        \caption{ALBERT}
    \end{subfigure}
    \caption{LOOCV Error on "Field of Land" Embeddings}
    \label{fig:loocv_field_land}
\end{figure}

We first consider performance over embeddings corresponding to the same definition. This is analogous to the uncorrupted data case.  As an error metric, we use Leave One Out Cross Validaton (LOOCV).  We only average over the bottom 90\% of errors to account for potential bias introduced by words that less clearly belong to a specific category. 
LOOCV error is defined here as $\frac{1}{n'} \sum_{i=1}^{n'} \|\mathsf{estimator}(X_{-i}) - x_i\|$ where $n' = 0.9 n$ is $90\%$ of the number of data points in the dataset $X$ (those with smallest errors), $x_i$ is the $i$th data point in $X$, and $X_{-i}$ is $X$ excluding $x_i$. LOOCV under the sample mean represents a valuable baseline for comparison as it demonstrates the minimum error to be expected across this data set \update{under the 10\% expected corruption ($\tau = 0.1$) modeled by the algorithms}. We take the dataset of 400 sentences corresponding to the "field of land" definition. We vary data size from $n=10$ to $n=400$, and, as in prior experiments, average results over $5$ runs and report shaded regions to denote 1 standard deviation of error.  For algorithms that utilize $\tau$, expected corruption, as input, we use the default value of $\tau=0.1$.  We employ the sample trace scaling heuristic for \evln and \queln. We additionally halt \queln whenever more than $2\tau$ percentage of the data has been pruned, regardless of whether or not the threshold is passed. We note that the early halting heuristic is necessary for \queln to perform well under this setting and that it does not meaningfully improve \evln; this is further explored in Section \ref{subsec:early_halt}. We show results in Figure \ref{fig:loocv_field_land}.

Our results do not match our synthetic experiments, suggesting that some robust mean estimation algorithms are sensitive to (Gaussian) distributional assumptions, at least under small data size.  Across LLMs, no algorithm significantly beats the error of \sample. Moreover, \evln performs significantly worse than \sample over all embeddings. This is unsurprising due to the sensitivity of \evln to knowledge of the true covariance. Despite having a similar dependency to knowledge of the true covariance, \queln achieves performance nearly matching \sample error throughout. We also find, that \lrv performs meaningfully worse than \sample across all LLMs except MiniLM, though not quite as catostrophically as \evln.  Aside from \coordmed, which performs noticeably worse than \sample over all LLMs besides MiniLM, all other estimators perform similarly to \sample.

\subsection{Corrupted Embeddings}

\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/MiniLM.png}
        \caption{MiniLM}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/t5.png}
        \caption{T5}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/Bert.png}
        \caption{BERT}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/Albert.png}
        \caption{ALBERT}
    \end{subfigure}
    \caption{Error on "Field of Land" Embeddings Corrupted with "Field of Study" Embeddings}
    \label{fig:loocv_corrupted}
\end{figure}

We examine performance over corrupted embeddings of the word "field". We draw corrupted data $X \sim (1 - \eta) P + \eta Q$, where the inlier distribution, $P$, consists of embeddings of the word "field" corresponding to the "field of land" definition, and the outlier distribution, $Q$, consists of embeddings of the word "field" corresponding to the "field of study" definition. As with previous experiments, we measure the Error $\|\mu - \hat{\mu}\|$ on the $y$-axis, taking $\mu$ as the mean of all 400 "field of land" embeddings, and $\hat{\mu}$ as the estimate returned by a mean estimation algorithm. We measure Error vs $\eta$, vary $\eta$ from $0$ to $0.45$, and always have $n = 400$. We average results over $5$ runs and report shaded regions to represent 1 standard deviation of error.  \gsample is plotted to represent the mean of the data before corruption. These results are shown in Figure \ref{fig:loocv_corrupted}. 

We find that mean estimation algorithms can indeed significantly improve performance on this real-world task, but do not observe the same trends as in our synthetic data experiments. \queln and \lvsim are the best estimators, with both significantly outperforming \sample. In fact, \queln performs nearly identical to \gsample across all LLMs, and \lvsim performs similarly, except on MiniLM, where it degrades worse with larger $\eta$ but still outperforms \sample. The performance of \lvsim supports the observation that it is a more effective naive pruning method, which happens to work among the best in these experiments. 
Moreover, \medmean performs very effectively here, always significantly outperforming \sample.  However, neither \evln nor \lrv perform effectively. \lrv outperforms \sample with large enough $\eta$, but these results are not consistent across LLMs suggesting sensitivity to distributional assumptions. Additionally, it almost never outperforms \medmean, \lvsim, or \queln and achieves much worse error with lower $\eta$. This may suggest that \lrv finds irregularities in the uncorrupted data, causing it to return a mean significantly different from \gsample. While this could be beneficial, suggesting that \gsample isn't the best error metric, this is not supported by the "uncorrupted" LOOCV error results, where \lrv also generally results in degraded LOOCV error.
\evln fails catastrophically across LLMs. This matches the "uncorrupted" LOOCV error results, supporting the observation that \evln may fail catostrophically without sufficient knowledge of the true covariance matrix. As in the "uncorrupted" LOOCV error results, there is the somewhat surprising observation that despite seemingly having a similar dependency on knowledge of the true covariance matrix to \evln, \queln performs nearly optimally here.
This suggests the superiority of the quantum entropy based scoring method over naively ranking outliers based on the top eigenvalue of the sample covariance. 

\subsection{Effect Of Early Halting and Ablation}
\label{subsec:early_halt}

\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/NewNoHaltMiniLM_Field_Land.png}
        \caption{LOOCV - MiniLM}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/NEWNoHaltBert_Field_Land.png}
        \caption{LOOCV - BERT}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/NewNoHaltMiniLM.png}
        \caption{Corruption - MiniLM}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/NewNoHaltBert.png}
        \caption{Corruption - BERT}
    \end{subfigure}
    \caption{LLM Comparison - With and without \emph{early halting}}
    \label{fig:llm_no_halt}
\end{figure}



\emph{Early halting} is the following strategy with respect to a given threshold $\tau$:  If more than $2\tau$ points have been pruned by an algorithm, then this halts the pruning process (independent of other criteria) and returns the sample mean of remaining data.  

Here we examine the effect of early halting on \queln and \evln in the context of these real world data where inliers are not generated directly from a prescribed Gaussian distribution.  In other settings explored in this paper, this strategy is almost never invoked, so has no visible effect.  
We compare the performance of both of these algorithm with and without enforcing early halting. We examine LOOCV and Corruption Error over MiniLM and BERT embeddings. Results are shown in Figure \ref{fig:llm_no_halt}. Across all 4 experiments the performance of \queln shows significant degradation without early halting, going from nearly matching \sample in LOOCV error and nearly matching \gsample in corrupted error, to yielding error significantly worse than \sample and \gsample without early halting. Additionally, \queln without halting yields far larger variance results. Meanwhile, \evln performs only slightly better with early halting, and still fails catastrophically across experiments. 



 We perform further ablations on these LLM experiments in Appendix \ref{app:llm_ablations}. We explore the effect of different pruning methods on \evln and different weighting methods on \lrv. In both cases, we find that LOOCV error can be improved using non-Gaussian pruning and weighting methods, whereas corrupted error is not meaningfully improved. The failure of \evln across pruning methods suggests the fundamental sensitivity of the outlier scoring method used in \evln to distributional assumptions, which is not seen in \queln (with early halting). We additionally examine performance over these same experiments with the roles of "field of study" and "field of land" embeddings flipped, finding nearly identical results despite differences in distribution. 

\begin{figure}[h]
\centering
\begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionImage/ResNet2048Corruption.png}
    \caption{ResNet-50 2048 Dimensional Image Embeddings: \\
    Cat Images Corrupted With Dog Images}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/300.png}
    \caption{GloVe 300 Dimensional  Word Embeddings: \\
    Pleasant Words Corrupted With Unpleasant Words}
\end{subfigure}
\caption{Additional Real World Corrupted Experiments}
\label{fig:more_real_world}
\end{figure}


\subsection{Additional Real World Experiments}

We additionally examine the performance of robust mean estimation algorithms on corrupted embeddings from deep pretrained image models and non-contextual word embedding models. For the image embedding experiment, we utilize a set of images of cats and dogs from the CIFAR10 dataset \citep{Krizhevsky2009LearningML} with 2048 dimensional embeddings generated from a pretrained ResNet-50 model \citep{he2015deepresiduallearningimage}. For the word embedding experiment, we utilize a dataset of pleasant and unpleasant words from \citep{aboagye2023interpretable} with 300 dimensional embeddings generated from a pretrained GloVe model \citep{pennington-etal-2014-glove}. Experiments are run analogously to the LLM experiments with identical settings for the mean estimators. For the image embedding experiment, inlier data is defined as embeddings of cat images, outlier data is defined as embeddings of dog images, and data size is fixed at $n=1000$. For the word embedding experiment, inlier data is defined as embeddings of "pleasant" words, outlier data is defined as embeddings of "unpleasant" words, and data size is fixed at $n=100$. Results are shown in Figure \ref{fig:more_real_world}. 


We find similar results to the LLM experiments, with \queln using early halting noticeably outperforming \sample across both settings, and nearly matching \gsample over image embeddings. Other robust estimators tend to perform similarly or worse to \sample, with \evln again demonstrating significant degradation
 without knowledge of distributional assumptions. Additionally, \lvsim, which performs strongly in the LLM experiments, does not perform as well, demonstrating its sensitivity to distributional assumptions. Similarly, \medmean, does not perform as well across word embeddings as it does across LLM and image embeddings, no longer noticeably outperforming other estimators.
 
 We find similar results across varying dimensionalities of image embedding and GloVe models. We examine additional image embeddings models of varying dimensionalities under LOOCV and corrupted error in Appendix \ref{app:image_experiments}. We additionally recreate the corrupted data experiment, but vary data size instead of corruption, finding that even with $n \gg d$, only \queln and \medmean significantly outperform \sample error, with both estimators nearly converging to \gsample error with corruption $\eta = 0.1$.  We also examine GloVe models of varying dimensionalities under LOOCV and corrupted error in Appendix \ref{app:word_experiments}. 

\update{
\section{Non-Gaussian Synthetic Data Experiments}
\label{app:nongauss}
We examine the performance of robust mean estimators across a few non-Gaussian synthetic data experiments. As before, we draw $X \sim (1 - \eta)P + \eta Q$, where $P$ is an inlier data distribution and $Q$ is the corrupted data distribution. Similar to real world experiments, we employ the trace scaling heuristic on \evln and \queln and enforce early halting on \queln if more than a $2\tau$ percentage of the data has been pruned.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/NonGauss/t_far.pdf}
    \caption{Corrupted Multivariate t-distribution}
    \label{fig:multi_t}
\end{figure}

\paragraph{Multivariate t-distribution} 
Define $P$ as the multivariate t-distribution parametrized by $\mu$ as the all-fives vector, $\Sigma$ as the identity matrix, and degrees of freedom $\nu=3$. Observe that the covariance is $\frac{\nu}{\nu - 2}\Sigma$, not $\Sigma$. This is a heavy-tailed distribution with polynomial tail decay. As in other experiments, consider corrupted data distribution $Q = \mathcal{N}_d(\mu', \frac{1}{10}I)$ where $\|\mu - \mu'\| = \sqrt{d}$. Results are shown in Figure \ref{fig:multi_t}. As with the other experiments, \queln, \evln, \pgd, \lrv, and \medmean all notably outperform \sample here. However here, \queln, \evln, \pgd, and \lrv all consistently outperform \gsample. This trend is particularly notable under lower data size and high dimensions. This is explainable given that the sample mean is known to be a sub-optimal estimator for heavy tailed distributions. These results suggest that robust estimators designed for the Huber contamination model have practical application in the heavy-tailed distribution setting. We leave further investigation into this connection to future work, building on \cite{prasad2019unifiedrobustheavy}. We also note that there is a more notable separation between the best performing methods in this settings than in the inlier Gaussian data scenario. In particular, \evln and \lrv consistently outperform \queln, and also \queln exhibits areas of high variance in error.

\paragraph{Laplace Distribution}
Define $P$ as the product distribution of $d$ independent Laplace distributions, each with mean $5$ and scale $1$. Then, the true mean $\mu$ is defined as the all-fives vector. Again, consider corrupted data distribution $Q = \mathcal{N}_d(\mu', \frac{1}{10}I)$ where $\|\mu - \mu'\| = \sqrt{d}$. Results are shown in Figure \ref{fig:laplace}. Results among the best estimators are nearly identical to the Gaussian inlier data scenarios, with \queln, \pgd, \evln all nearly matching \gsample error and with \lrv performing marginally worse. We note that although the Laplacian distribution has a heavier tail than the Gaussian, we do not observe the same trends as in the multivariate t-distribution. This can be explained by the fact that Laplacian tails still decay exponentially, whereas the tails in the multivariate t-distribution decay polynomially.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/NonGauss/laplace.pdf}
    \caption{Corrupted Laplace Distribution}
    \label{fig:laplace}
\end{figure}

\paragraph{Poisson Distribution}

Define $P$ as the product distribution of $d$ independent Poisson distributions, each with mean $5$. Then, the true mean $\mu$ is defined as the all-fives vector. Again consider corrupted data distribution $Q = \mathcal{N}_d(\mu', \frac{1}{10}I)$ where $\|\mu - \mu'\| = \sqrt{d}$. Results are shown in Figure \ref{fig:poisson}. Results among the best estimators are again nearly identical to the Gaussian inlier data scenarios.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/NonGauss/poisson.pdf}
    \caption{Corrupted Poisson Distribution}
    \label{fig:poisson}
\end{figure}

\paragraph{Mixture of Gaussians}

Define $P$ as a mixture of Gaussians with three components, each equally weighted. The components have means $\mu_1 = \vec{1}$, $\mu_2 = \vec{0}$, and $\mu_3 = -\vec{1}$, where $\vec{1}$ and $\vec{0}$ are the $d$-dimensional vectors of all ones and all zeros, respectively. Each component has identity covariance. Define $Q$ as $\mathcal{N}({\vec{2}, \frac{1}{10}I})$ where $\vec{2}$ is the all-twos vector. Results are shown in Figure \ref{fig:mix_gaus}. While robust methods tend to outperform \sample, relative performance here differs from the Gaussian inlier data setting. Firstly, note that \queln does not perform the best and shows irregular areas of high error under lower dimensionality and moderate corruption levels.  The observation that \queln, \evln, and \pgd do not perform as well is likely because they rely significantly on the Gaussianity assumption for the inliers.  Methods such as \medmean, on the other hand which do not rely as directly on this model are not as effected. Furthermore, \evln, \lrv, and \pgd all show significant degradation as corruption levels increase, which is not observed in the Gaussian inlier settings.  This may be occurring if they completely filter one of the three ``inlier" modes as outliers.  
Interestingly, \lrv notably outperforms all other estimators and even \gsample except with corruption $\tau > 0.2$ and especially for $n < d$. The strong performance of \lrv in this setting is interesting, and may be a consequence of the three inlier distributions means lying on a 1-dimensional subspace.  

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/NonGauss/mix_gaus.pdf}
    \caption{Corrupted Mixture Of Gaussians}
    \label{fig:mix_gaus}
\end{figure}
}

\section{Comparing Algorithm Variants and Ablation}
\label{sec:variants}

In this section, we justify and explore our adaptations to \ev, \que, and \lvog. We use these adaptations for the remainder of our experiments.

\paragraph{Eigenvalue-based Threshold}

Here we compare \ev and \evln, along with \que and \queln. We observe that when we do not have $n$ very large compared to $d$, then \ev and \que can \update{dramatically shift from low error to abysmal error rates}. 


We recreate the experiment over corrupted Gaussian data with identity covariance and DKK Noise from Section \ref{sec:corrid}. This is shown in Figure \ref{subfig:evpruning}. We find that \ev and \que fail catastrophically with insufficient data, performing far worse than \sample. However, \evln and \queln never perform worse than \sample and achieve near optimal performance regardless of data size. With sufficient data, \ev and \que abruptly begin to work, and achieve near identical performance to their adjusted threshold counterparts.

\begin{figure}[h]
\centering
\begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/ev_que_catostrophic.png}
    \caption{Corrupted Identity Covariance - DKK Noise}
    \label{subfig:evpruning}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/threshold_vs_eigenvalue.pdf}
    \caption{Uncorrupted Identity Covariance}
    \label{subfig:ev_vs_thresholds}
\end{subfigure}
\caption{Eigenvalue Based Filtering Comparison}
\label{fig:comparison}
\end{figure}

The failure of \ev and \que occurs in the corruption detection step. This corruption detection threshold on the top eigenvalue is initialized as $1+ 3 \tau \log(1/\tau)$ in \ev and \que. This constant threshold uses the fact that with large enough data size, the top eigenvalue of an identity covariance matrix approaches 1 and the $3 \tau \log(1/\tau)$ term can account for tolerable noise. However, this does not account for corruption due to low data size, in which the top eigenvalue of the uncorrupted data will necessarily have a larger expectation as data size decreases. Therefore, \ev and \que can never be expected to work since even without any corruption, the top eigenvalue will exceed the threshold. As a result, we find that \ev and \que keep on pruning until there are only very few data points left, resulting in the catastrophic error exhibited. \evln and \queln remedy this problem by simply incorporating our new result (Corollary \ref{cor:prune-2t-main}) as a threshold on the top eigenvalue of the covariance matrix in the threshold. This is empirically shown in Figure \ref{subfig:ev_vs_thresholds}. The threshold in \ev and \que does not become a true upper bound on the top eigenvalue of the uncorrupted data until the vertical red line, which roughly corresponds to the point that \evln begins to perform better. \que begins to perform better with much less data size than \update{\ev}, but this is unsurprising given the rapid convergence of the inlier top eigenvalue to $1$. Meanwhile, our new threshold used in \evln and \queln is always an upper bound on the top eigenvalue, and is near-optimal in practice. 


 

































\paragraph{Lee and Valiant variants.}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{UpdatedFigures/lee_valiant_comparison_gaus_one.png} 
\caption{Lee Valiant Variants: Identity Covariance - Additive Variance Shell Noise}
\label{fig:lv_variants}
\end{figure}

Here we compare different variants of the Lee and Valiant algorithm. We observe that \lvsim performs a bit better than \lvog, and \medmean is an illustrative choice for initial estimator.  
We recreate the experiment over Gaussian data with identity covariance and additive variance shell noise from Section \ref{sec:corrid} across different variants of the Lee and Valiant algorithm. We test \lvsim and \lvog using \medmean, \lrv, and \evln as initial mean estimators. We additionally plot \lrv and \evln as baselines. This is shown in Figure \ref{fig:lv_variants}. 



\lvsim differs from \lvog in two ways: 
 (1) it removes outliers completely instead of downweighting them and 
 (2) it does not use initial mean estimate in the final result.
We see that \lvsim performs better than \lvog in practice, especially with larger $n$ or $d$. Both \lvsim and \lvog see benefit in the use of improved initial estimators, with this improvement being more significant in \lvog. The difference in the relative improvement in performance between the algorithms is explained by the fact that \lvog additively incorporates the initial estimate directly into its final estimate. However, there is no benefit gained from combining \lvog or \lvsim with an improved initial estimator compared to using the initial estimator alone. As a result of these findings, we only evaluate \lvsim in our experiments.






\section{Conclusion}
\label{sec:conclusion}

We perform the first wide-scale experimental study of robust mean estimation techniques in high dimensions and relatively-low data size. We showed that under Gaussian data with bounded covariances, robust mean estimation techniques can significantly outperform sample mean, nearly matching the optimal error obtainable, regardless of data size, dimensionality, or corruption level. We provide an updated eigenvalue filtering bound that is useful in this high-dimensional setting, and use it to devise a small but novel and meaningful modification to two existing robust mean estimation algorithms; eigenvalue pruning from \cite{diakonikolas2019robust} and quantum entropy scoring from \cite{dong2019quantumentropyscoring}. This enables these methods to almost exactly match optimal error regardless of data size -- that is almost matching the error of the so-called \emph{good sample mean}, which is the mean of the inliers.   \update{It seems \queln works so well because it can identify all ways input distributions deviate from Gaussianity, whereas other methods may require more iterations, which may ultimately prune too many points in the $n \leq d$ setting before it is able to filter outliers in each direction that has them. }

However, all methods perform significantly worse than the mean of all inliers under \emph{subtractive corruption} where an $\eta$-fraction of data points can be removed adversarially.  This suggests that in this Gaussian modeled data regime, practical improvements may be possible in considering the effect of subtractive corruption.  

We also provide a novel evaluation on realistic settings based on the embeddings generated from large language models, deep pretrained image models, and word embedding models.  These are representative of real world settings where the data size $n$ may be smaller or not much larger than the dimension $d$.  
In these settings, quantum entropy scoring with early halting 
tends to perform near optimally, suggesting its potential application to real world data distributions regardless of data size. However, other robust mean estimation algorithms do not work as well as when the inlier data is not Gaussian, as perhaps foreshadowed by theoretical results leveraging this assumption. This suggests that further valuable results may be derived by moving away from the assumption that inliers are precisely Gaussian.  
\update{Our initial explorations for corrupted data with non-Gaussian inliers shows the same techniques mostly perform well, but which method performs the best can vary based on the inlier distribution.  Notably, some methods can even have less error than \gsample when the sample mean is not the MLE.}


Overall, our work demonstrates that there is value in applying robust mean estimation techniques to data, even with insufficient data size for the classic theoretical bounds. We hope that our work inspires researchers to further consider, both experimentally and theoretically, the crucial case of high-dimensional robust statistics under low data size. 












\subsubsection*{Acknowledgments}
The authors thank Meysam Alishahi for early discussions, especially towards the analysis in Theorem 2.  
We also thank the NSF REU project 224492, under which CA was a participant and initiated this work.  JP also thanks funding from NSF 2115677 and 2421782, and Simons Foundation  MPS-AI-00010515.  

\input{main.bbl}
\bibliographystyle{tmlr}

\appendix
\section{Appendix}

\subsection{Updated Eigenvalue Threshold}
\label{app:ev_theory}



Here we leverage a theorem in \cite{vershynin2011randommatrices} to bound the complexity of aggregated high-dimensional Gaussian random variables.  We will use it in a couple ways.  



\begin{theorem}[\citet{vershynin2011randommatrices} Thm. 5.35]
Let $A$ be a $n \times d$ matrix whose entries are independent standard normal random variables. Let $\|A\|_2$ denote the spectral norm of $A$. Then for every $t \geq 0$, with probability of at least $1 - 2 \exp(-t^2/2)$, one has
\[
\sqrt{n} - \sqrt{d} - t \leq s_{\min}(A) \leq \|A\|_2 \leq \sqrt{n} + \sqrt{d} + t.
\]
Where $s_{\min}(A)$ is the smallest singular value of $A$.  The lower bound assumes $n > d$, if not the roles are reversed.  
\label{thm:verGnd}
\end{theorem}











































We prove the following implication.  




\begin{theorem}[restatement of Theorem \ref{thm:Sigma2-bound-main}]
\label{thm:Sigma2-bound}
Let $X$ be a $n \times d$ matrix whose entries are independently drawn from $\mathcal{N}(\mu, I)$. Let $\Sigma = \frac{1}{n}(X-\bar{\mu})^T(X-\bar{\mu})$ be the sample covariance matrix of $X$, where $\bar{\mu} = \frac{1}{n} \sum_i X_i$ and $X_i$ is the $i$th row of $X$. Then for every $t > 0$, with probability of at least $1 - 3 \exp(-t^2/2)$, one has 
\[
\  \|\Sigma\|_2 \leq \left(1 + \sqrt{d/n} + t/\sqrt{n} + \frac{\sqrt{d + \sqrt{2d}t + t^2}}{n} \right)^2.  
\]
\end{theorem}

\begin{proof}
Let $\bar{X} = X - \mu$ be the centered matrix, equivalent to each entry being drawn from $\mathcal{N}(0, I)$. Let $Z = X - \bar{\mu}$ be the matrix centered by the sample mean. Then $\|Z\|_2 = \|\bar{X} + [\mu - \bar{\mu}]\|_2 \leq \|\bar{X}\|_2 + \|\mu - \bar{\mu}\|_2$ by triangle inequality.  




First, by Theorem \ref{thm:verGnd}, we have that $\|\bar X\|_2 \leq \sqrt{n} + \sqrt{d} + t$, with probability at least $1-2\exp(-t^2)$.  

Second, to bound $\|\mu - \bar{\mu}\|_2$ we first decompose by coordinate $\|\mu - \bar{\mu}\|_2^2 = \sum_{j=1}^d (\mu_j - \bar{\mu}_j)^2$.  Now consider $d$ random variables $B_j = \mu_j - \bar{\mu_j}$ for $j = 1 \ldots d$, and further write $B_j = \frac{1}{n} \sum_{i=1}^n F_i$ where $F_i \sim \mathcal{N}(0,1)$.  As a result $B_j \sim \mathcal{N}(0, 1/n) = \frac{1}{\sqrt{n}} \mathcal{N}(0,1)$, since the average of $n$ normals is still normal with variance reduced by factor $n$. 
As a result $B_j$ is a squared normal distribution, and $B = n \|\mu - \bar{\mu}\|^2 = n \sum_{j=1}^d B_j^2$ is a chi-squared distribution $\chi^2(d)$.  

Hence we have~\citep{laurent2000adaptive}
\[
\mathsf{Pr}[B \geq d + 2\sqrt{d} t + 2s^2] 
\leq 
\exp(-s^2).  
\]
Inside the probability expression, using $\sqrt{B/n} = \|\mu -  \bar\mu\|$, and letting $t = \sqrt{2}s$, we can rewrite this as
\[
\mathsf{Pr}\left[\|\mu - \bar{\mu}\| < \sqrt{(d + \sqrt{2} \sqrt{d} t + t^2)/n}\right]  
\geq 
1-\exp(-t^2/2).
\]

So now if both of these events hold, which by union bound occurs with probability at least $1-3\exp(-t^2/2)$, we have that
\[
\|Z\|_2 
  \leq 
\| \bar X\|_2 + \|\mu - \bar \mu\| 
  \leq 
(\sqrt{n} + \sqrt{d} + t) + \sqrt{\frac{d+ \sqrt{2 d} t + t^2}{n}}
\]

Notice that $\|\Sigma\|_2 = \|\frac{1}{n}Z^TZ\|_2 = \frac{\|Z\|_2^2}{n}$.
Thus we have 
\[
\| \Sigma \|_2 = \frac{\|Z\|_2^2}{n} \leq \left( 1 + \sqrt{d}/\sqrt{n} + t/\sqrt{n} + \frac{\sqrt{d + \sqrt{2d}t + t^2}}{n} \right)^2
\]
\end{proof}

Note that the fourth term in this bound, coming from the error in $\|\mu - \bar  \mu\|$, is a lower order effect.  This is captured in the following corollary.  

\begin{corollary}[restatement of Corollary \ref{cor:prune-2t-main}]
Let $X$ be a $n \times d$ matrix whose entries are independently drawn from $\mathcal{N}(\mu, I)$. Let $\Sigma = \frac{1}{n}(X-\bar{\mu})^T(X-\bar{\mu})$ be the sample covariance matrix of $X$, where $\bar{\mu} = \frac{1}{n} \sum_i X_i$ and $X_i$ is the $i$th row of $X$. If one assumes $d/n \leq 16, n \geq 16, t \geq 5$, then with probability of at least $1 - 3 \exp(-t^2/8)$, one has 
\[
\  \|\Sigma\|_2 \leq \left(1 +  \sqrt{d/n} + t/\sqrt{n} \right)^2.  
\]
\label{cor:prune-2t}
\end{corollary}
\begin{proof}
Starting with the bound in Theorem \ref{thm:Sigma2-bound} we have
\begin{align*}
\|\Sigma\|_2 
  &\leq 
\left( 1 + \sqrt{d/n} + t/\sqrt{n} + \frac{\sqrt{d + \sqrt{2d}t + t^2}}{n} \right)^2
 \\ & =
\left( 1 + \sqrt{d/n} + t/\sqrt{n} + \frac{t}{\sqrt{n}} \cdot \frac{\sqrt{d/t^2 + \sqrt{2d}/t + 1}}{\sqrt{n}} \right)^2
 \\ & =
\left( 1 + \sqrt{d/n} + \frac{t}{\sqrt{n}}\left( 1+  \sqrt{(d/n)/t^2 + \sqrt{2}\sqrt{d/n}/t/\sqrt{n} + 1/n} \right) \right)^2
\\ & \leq
\left( 1 + \sqrt{d/n} + \frac{t}{\sqrt{n}}\left( 1+  \sqrt{(16)/t^2 + \sqrt{2}\sqrt{16}/t/\sqrt{n} + 1/n} \right) \right)^2
\\ & \leq
\left( 1 + \sqrt{d/n} + \frac{t}{\sqrt{n}}\left( 1+  \sqrt{16/25 + \sqrt{32}/(5 \sqrt{n}) + 1/n} \right) \right)^2
\\ & \leq
\left( 1 + \sqrt{d/n} + \frac{t}{\sqrt{n}}\left( 1+  \sqrt{16/25 + \sqrt{32}/(5 \cdot 4) + 1/16} \right) \right)^2
\\ & <
\left( 1 + \sqrt{d/n} + 2t/\sqrt{n} \right)^2
\end{align*}
Adjusting $t$ to $2t$ in the probability of failure, so it is $3\exp(-t^2/8)$ instead of $3 \exp(-t^2/2)$, completes the proof.  
\end{proof}




















\subsection{Corrupted Gaussian Data Identity Covariance: Additional Noise Schemes}
\label{app:idcov_morenoise}

We examine the performance of robust mean estimators across additional corruption schemes. We still draw $X \sim (1-\eta) P + \eta Q$ where $P = \mathcal{N}_d(\mu,I)$ and $Q$ describes the corrupted data distribution, where $\mu$ is the all-fives vector. We utilize the following additional corruption schemes:

\paragraph{Two Gaussian clusters shifted to variance shell. }
Consider corrupted data distribution $Q = 0.7\mathcal{N}_d(\mu^0, \frac{1}{10} I) \cup 0.3\mathcal{N}_d(\mu^1, \frac{1}{10} I)$ where $\|\mu - \mu^0\| = \sqrt{d}$, $\|\mu - \mu^1\| = \sqrt{d}$, and $\theta = 75^\circ$ where $\theta$ is the angle between $\mu^0$ and $\mu^1$.  The location of $\mu^0$ is determined by a random rotation matrix to prevent any coordinate-axis specific biases. Results over this noise distribution are shown in Figure \ref{fig:id_cov_gaus_two}.

\paragraph{In Distribution Noise. }
Consider corrupted data distribution, $Q$, where for each corrupted data point $q_i \in Q$, each coordinate $j$ of $q_i$ is drawn from $\mathsf{Uniform}(\mu_j, \mu_j+2)$. Here $\mu_j$ represents the $j$th coordinate of the true mean $\mu$. Results over this noise distribution are shown in Figure \ref{fig:id_cov_unif_top}.

\paragraph{Large Outlier Noise. }
Consider corrupted data distribution $Q = 0.7\mathcal{N}_d(\mu^0, \frac{1}{10} I) \cup 0.3\mathcal{N}_d(\mu^1, \frac{1}{10} I)$ where $\|\mu - \mu^0\| = 10\sqrt{d}$, $\|\mu - \mu^1\| = 20\sqrt{d}$, and $\theta = 75^\circ$ where $\theta$ is the angle between $\mu^0$ and $\mu^1$. The location of $\mu^0$ is determined by a random rotation matrix to prevent any coordinate-axis specific biases. Results over this noise distribution are shown in Figure \ref{fig:id_cov_obvious}.

\paragraph{Large Outlier Noise Mixes. } 
Consider corrupted data distribution $Q = 0.5L \cup 0.5 Q'$ where $L$ is the large outlier corruption scheme previously described and $Q'$ is a subtle corruption scheme. We examine two settings for $Q'$: additive variance shell corruption with one cluster, shown in Figure \ref{fig:id_cov_large_obvious_gaus}, and DKK corruption, shown in Figure \ref{fig:id_cov_large_obvious_dkk}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/IdCov/id_cov-gaus_two.png}
    \caption{Corrupted Gaussian Identity Covariance: Two Variance Shell Clusters}
    \label{fig:id_cov_gaus_two}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/IdCov/id_unif_top.png}
    \caption{Corrupted Gaussian Identity Covariance: In Distribution Noise}
    \label{fig:id_cov_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/IdCov/id_cov-obvious.png}
    \caption{Corrupted Gaussian Identity Covariance: Large Outliers}
    \label{fig:id_cov_obvious}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/IdCov/id_cov-large-obvious_and_gaus_one.png}
    \caption{Corrupted Gaussian Identity Covariance: Large Outliers w/ Additive Variance Shell Noise}
    \label{fig:id_cov_large_obvious_gaus}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/IdCov/id_cov-large-obvious_and_dkk.png}
    \caption{Corrupted Gaussian Identity Covariance: Large Outliers w/ DKK Noise}
    \label{fig:id_cov_large_obvious_dkk}
\end{figure}

Across all of these distributions, including those with large outliers, \evln, \queln, \pgd, and \lrv perform the best, suggesting that their performance is not overly sensitive to the noise distribution. However, we note that across schemes with large outliers, \pgd sees areas of higher variance and slightly worse performance, and \lrv degrades worse as $\eta$ increases with large outliers. This downgrade in performance can be remedied by first preprocessing data by removing large outliers through a naive pruning method, but this step doesn't appear necessary for other methods.  We remark that \lrv requires outlier weights to be clipped to avoid numerical instability issues under large outlier schemes. Otherwise, it will degrade poorly over large outliers and large $\eta$ as predicted outliers will be assigned near-zero weights. We also see again that \medmean significantly outperforms other simple estimators, especially under large data size, although its performance degrades poorly under certain conditions, such as with larger $\eta$. With large outliers, \lvsim nearly matches \gsample error across conditions, achieving much better performance than it does across subtle noise distributions. As it additionally outperforms \coordprune, this suggests that \lvsim may operate as a more effective naive pruning method, as seen in the LLM experiments.

\paragraph{Dependence on true mean}
We additionally verify that performance does not depend on the choice of true mean, $\mu$. We recreate experiments over Additive Variance Shell Noise and DKK Noise over different choices of $\mu$. We replicate the same experimental setup as before, but draw every coordinate of $\mu$ from $\mathcal{N}(0, 50)$ at every iteration in the experiment rather than fixing $\mu$ as the all-fives vector. As a reminder, this occurs for every choice of the independent variable over every run. If performance depended on $\mu$, we would expect to achieve high variance results. Instead, we find nearly identical results to the original experiments across both distributions. These results are shown in Figure \ref{fig:mean_dependence_gaus_one} and Figure \ref{fig:mean_dependence_dkk}. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/IdCov/mean_dependence-gaus_one.png}
    \caption{Dependence On True Mean: Identity Covariance, Additive Variance Shell Noise}
    \label{fig:mean_dependence_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/IdCov/mean_dependence-dkk.png}
    \caption{Dependence On True Mean: Identity Covariance, DKK Noise}
    \label{fig:mean_dependence_dkk}
\end{figure}

\clearpage


\subsection{Corrupted Gaussian Data Unknown Spherical Covariance: Additional Corruption Schemes}
\label{app:unknownsp_morenoise}

We examine the unknown spherical covariance case across additional corruption schemes. We utilize the same uncorrupted distribution as before, $P = \mathcal{N}_d(\mu, \sigma^2 I)$ where $\mu$ is the all-fives vector and $\sigma = 5$. We find similar performance across the distributions we test. 


\paragraph{Adapting noise distributions to spherical covariance}

As in the Gaussian noise shifted to variance shell case, we utilize the well know Gaussian concentration inequality that for data $X \sim \mathcal{N}_d(\mu, \sigma I)$, $\E_{x \sim X} [\|x - \mu\|^2] = \sigma^2 d$. This observation is used to adapt noise distributions in the identity covariance case to this case. For two additive variance shell clusters, each cluster has mean $\mu^i$ for $i \in [0, 1]$ where $\|\mu - \mu^i\| = \sigma \sqrt{d}$, with other conditions remaining the same; results are shown in Figure \ref{fig:large_sp_gaus_two}. For DKK noise, half the noise is drawn over the hypercube where every coordinate is $-\sigma$ or 0 away from the true mean at that coordinate with equal probability. The other half is drawn from the product distribution where the first coordinate is either $11\sigma$ or $-\sigma$ away from the true mean at that coordinate with equal probability, the second coordinate is $-3\sigma$ or $-\sigma$ away from the corresponding true mean coordinate with equal probability, and all remaining
coordinates are $-\sigma$ away from the true mean. Results are shown in Figure \ref{fig:large_sp_dkk}. For in distribution corruption, we draw each coordinate $j$ of a corrupted data point from $\mathsf{Uniform}(\mu_j, \mu_j + 2\sigma)$; results are shown in Figure \ref{fig:large_sp_unif_top}. We also perform subtractive corruption, using the same scheme as in the identity covariance case; results are shown in Figure \ref{fig:large_sp_subtractive_corruption}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/LargeSpherical/large_sp_gaus_two.png}
    \caption{Corrupted Gaussian Large Spherical Covariance: Two Variance Shell Clusters}
    \label{fig:large_sp_gaus_two}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/LargeSpherical/large_sp_dkk.png}
    \caption{Corrupted Gaussian Large Spherical Covariance: DKK Noise}
    \label{fig:large_sp_dkk}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/LargeSpherical/large_sp-unif_top.png}
    \caption{Corrupted Gaussian Large Spherical Covariance: In Distribution Noise}
    \label{fig:large_sp_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/LargeSpherical/large_sp-subtractive_corruption.png}
    \caption{Corrupted Gaussian Large Spherical Covariance: Subtractive Noise}
    \label{fig:large_sp_subtractive_corruption}
\end{figure}


We find similar results to the identity covariance case across the best estimators, again observing the near optimal performance of \queln and \pgd, along with the slightly worse but still near optimal performance of \lrv. As a result of using the scaling data heuristic, \evln degrades slightly. This is especially noticeable with DKK noise, where it does not converge to \gsample error as data size increases. \queln appears to be less sensitive to the trace scaling heuristic, retaining its performance in these experiments. There is some variance among other estimators. Notably, \lvsim performs near optimally across two variance shell corruption and in distribution noise with spherical covariance (its performance in these plots is hidden amongst the best estimators which approximately match \gsample error), whereas it performs comparably  worse across analogous noise distributions for identity covariance data. Still, \lvsim does not generally perform better in the spherical covariance case compared to the (known) identity covariance case; performing consistently worse than \sample compared to outperforming \sample except with large $\eta$ under identity covariance. 








\paragraph{Varying $\sigma$}
We rerun several experiments as we vary $\sigma$ from $\sigma=0.1$ to $\sigma=200$ -- the coordinate wise standard deviation of the true covariance matrix -- and fix other variables as their default values. In particular, we examine Additive Variance Shell Noise, DKK Noise, In Distribution Uniform Noise, and Two Variance Shell Clusters Noise. Results are shown in Figure \ref{fig:vary_var}. As expected, error tends to increase linearly with $\sigma$. Generally, relative performance of the algorithms remains the same, with \evln, \queln, and \pgd nearly identically matching \gsample error throughout. Surprisingly, \lrv error does not grow linearly with $\sigma$, consistently outperforming even \gsample with large enough choices of $\sigma$. A similar trend is also seen for \coordmed, but only across DKK Noise, in which it is noticeably the best estimator with larger values of $\sigma$. 

\begin{figure}[h]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LargeSpherical/sp_std_dependence-gaus_one.png}
        \caption{Additive Variance Shell Noise}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LargeSpherical/sp_std_dependence-dkk.png}
        \caption{DKK Noise}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LargeSpherical/sp_std_dependence-unif_top.png}
        \caption{In Distribution Noise}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LargeSpherical/sp_std_dependence-gaus_two.png}
        \caption{Two Variance Shell Clusters}
    \end{subfigure}
    \caption{Corrupted Gaussian Large Spherical Covariance: Varying Coordinate-Wise Standard Deviation $\sigma$}
    \label{fig:vary_var}
\end{figure}

\clearpage

\subsection{Corrupted Gaussian Data Unknown Non Spherical Covariance}
\label{app:unknown_non_sp}

\subsubsection{Unknown Diagonal Covariance}






Here we consider the performance of mean estimators on corrupted Gaussian data with unknown diagonal non-spherical covariance. We draw uncorrupted data from $\mathcal{N}_d(\mu, \Sigma)$ where $\mu$ is the all fives-vector and $\Sigma$ has large diminishing covariance. In particular, the diagonal elements uniformly decrease from 25 to 0.1.



 \paragraph{Noise Distributions}

 
 
 We adapt the variance shell additive noise distribution to cluster outliers to be a standard deviation away from the true mean along every coordinate axis. That is, consider corrupted data distribution 
 $Q = \mathcal{N}_d(\mu', \frac{1}{10}I)$ with $|\mu'_j - \mu_j| = \Sigma_{j}$, 
  where $\Sigma_j$ is the $j$th diagonal element in $\Sigma$, $\mu'_j$ is the $j$th coordinate of $\mu'$, and $\mu_j$ is the $j$th coordinate of the true mean $\mu$; results are shown in Figure \ref{fig:large_dim_diag_gaus_one}. We adapt in distribution uniform noise to draw each coordinate $j$ of a corrupted data point from 
  $\mathsf{Uniform}(\mu_j, \mu_j + \Sigma_{j})$; 
  results are shown in Figure \ref{fig:large_dim_diag_unif_top}. For large outlier noise we weight the distance of clusters from $\mu$ by $\sqrt{\frac{\Tr(\Sigma)}{d}}$. That is, consider corrupted data distribution $Q = 0.7\mathcal{N}_d(\mu^0, \frac{1}{10} I) \cup 0.3\mathcal{N}_d(\mu^1, \frac{1}{10} I)$ where $\|\mu - \mu^0\| = 10\sqrt{\frac{\Tr(\Sigma)}{d}} \sqrt{d}$, $\|\mu - \mu^1\| = 20\sqrt{\frac{\Tr(\Sigma)}{d}} \sqrt{d}$, and $\theta = 75^\circ$ where $\theta$ is the angle between $\mu^0$ and $\mu^1$.; 
  results are shown in Figure \ref{fig:large_dim_diag_obvious}. We also utilize subtractive noise which already works in this case; results are shown in Figure \ref{fig:large_dim_diag_subtractive_corruption}. 
 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/LargeNonSp/large_non_sp-large_dim_diag_gaus_one.png}
    \caption{Corrupted Gaussian Large Diminishing Diagonal Covariance: Additive Variance Shell Noise}
    \label{fig:large_dim_diag_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/LargeNonSp/large_non_sp-large_dim_diag_unif_top.png}
    \caption{Corrupted Gaussian Large Diminishing Diagonal Covariance: In Distribution Noise}
    \label{fig:large_dim_diag_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/LargeNonSp/large_non_sp-large_dim_diag_obvious.png}
    \caption{Corrupted Gaussian Large Diminishing Diagonal Covariance: Large Outliers}
    \label{fig:large_dim_diag_obvious}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/LargeNonSp/large_non_sp-large_dim_diag_subtractive_corruption.png}
    \caption{Corrupted Gaussian Large Diminishing Diagonal Covariance: Subtractive Noise}
    \label{fig:large_dim_diag_subtractive_corruption}
\end{figure}


Again, we find that \queln and \pgd nearly match \gsample error across distributions, with 
\lrv doing slightly worse but still significantly outperforming \sample. \evln still does among the best here, but, as in the large spherical covariance case, sees slight degradation as a result of the scaling data heuristic. In particular, error does not clearly converge to \gsample error as $n$ increases over Additive Variance Shell Noise and In Distribution Noise. \queln does not encounter this issue despite employing the same heuristic to generalize to non-identity covariance data, suggesting that it is more robust to distributional assumptions. Additionally, \medmean performs best among simpler estimators, outperforming the \sample with $n \approx d$ and performing similarly to \gsample with sufficiently large $n$. 


\paragraph{Varying top Eigenvalue}


We rerun Additive Variance Shell Noise and In Distribution Noise as we vary the squareroot of the top eigenvalue of the true covariance matrix, labeled as $\sigma$, from $\sigma=0.1$ to $\sigma=200$. In particular, the diagonal of the covariance will uniformly decrease from $\sigma^2$ to $0.1$. For every choice of $\sigma$, the noise is scaled as described previously.  These results are shown in Figure \ref{fig:vary_var_non_sp}. Like in the spherical case, we find that the relative performance of algorithms remains nearly identical throughout choices of $\sigma$.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/LargeNonSp/non_sp_std_dependence-gaus_one.png} 
    \caption{Additive Variance Shell Noise}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{UpdatedFigures/LargeNonSp/non_sp_std_dependence-unif_top.png} 
    \caption{In Distribution Noise}
\end{subfigure}
\caption{Corrupted Gaussian large diminishing diagonal covariance: Varying the square root of the top eigenvalue: $\sigma$}
\label{fig:vary_var_non_sp}
\end{figure}

\clearpage

























\subsubsection{Unconstrained Covariance}
\label{sec:corrsph}



So far, we have only examined inlier data with diagonal covariance matrices. However, in line with the intuition that there is nothing inherently special about the standard orthonormal basis, we hope for a robust estimator to work well regardless of the choice of coordinate axis. Since the covariance matrix is always symmetric, it is also diagonalizable by taking the eigenvectors as the orthonormal basis. Then, any possible data distribution over unconstrained covariance can be framed as a data distribution over a diagonal matrix by using these eigenvectors as the orthonormal basis. As a result, any robust estimator that does not leverage the standard orthonormal basis should perform equally well on unconstrained covariance. However, this does not necessarily hold for the estimators that we examine. We employ a trace estimate to adapt \evln and \queln to the unknown covariance case. \coordmed, \coordprune, and \medmean all directly utilize coordinate wise calculations. \lrv utilizes a trace estimate when downweighting points. In this section, we evaluate the performance of robust mean estimators over data with non-diagonal covariance matrices.

\paragraph{Rotated Data Noise}

Because the covariance matrix is always symmetric, it is diagonalizable, and experiments over unconstrained covariance can be framed as an ablation on noise distributions over inliers with diagonal covariances. We reuse data and noise distributions, but randomly rotate everything before estimation, resulting in unconstrained true covariances and appropriately difficult noise distributions. Random rotation is implemented by generating a standard normal matrix and utilizing its QR decomposition. 

We examine the performance on Rotated Identity Covariance with DKK Noise in Figure \ref{fig:rotate_id_dkk}; Rotated Identity Covariance with Subtractive Noise in Figure \ref{fig:rotate_id_sub}; Rotated Large Spherical Covariance with Additive Variance Shell Noise (with coordinate-wise standard deviation $\sigma=5$) in Figure \ref{fig:rotate_sp_gaus_one}; and Rotated Large Diminishing Covariance with Additive Variance Shell Noise (with squareroot of the top eigenvalue $\sigma=5$) in Figure \ref{fig:rotate_nonsp_gaus_one}. As in the original experiments, we set the true mean, $\mu$, to be the all-fives vector. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Rotated/rotate-id_dkk.png}
    \caption{Corrupted Rotated Identity Covariance - DKK Noise}
    \label{fig:rotate_id_dkk}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Rotated/rotate-id_sub.png}
    \caption{Corrupted Rotated Identity Covariance - Subtractive Noise}
    \label{fig:rotate_id_sub}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Rotated/rotate-sp_gaus_one.png}
    \caption{Corrupted Rotated Large Spherical Covariance - Additive Variance Shell Noise}
    \label{fig:rotate_sp_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Rotated/rotate-nonsp_gaus_one.png}
    \caption{Corrupted Rotated Large Diminishing Covariance - Additive Variance Shell Noise}
    \label{fig:rotate_nonsp_gaus_one}
\end{figure}

We find nearly identical results among the best estimators to the corresponding non-rotated data experiment. While many of these algorithms induce a bias to the coordinate axis, they are not enough to significantly skew results in the distributions that we examine. There is some variation between \coordmed and \coordprune with the corresponding non-rotated data experiments, but no major changes in their trends. There is no such variation for \queln, \evln, or \medmean among the settings that we test. 




\clearpage



\subsection{Hyperparameter Tuning}
\label{app:hp_tuning}
In this section we tune the hyperparameters of some of the most interesting algorithms, \medmean, \lrv, \evln, and \pgd. We utilize the best hyperparameters in this section across all other experiments. In general, we find that none of these algorithms are overly sensitive to choices of hyperparamaters, as long as they lay within a reasonable range. 


We evaluate performance over a subset of 4 corrupted data distributions previously discussed: Identity Covariance with DKK Noise; Identity Covariance with In Distribution Noise; Large Spherical Covariance with Variance Shell Additive Noise; Large Diminishing Covariance with Variance Shell Additive Noise. We manually pick the hyperparameters that achieve the best performance across these distributions, or when similar use default ones from the corresponding paper.  

\paragraph{Median of how many means?}

Here we explore the parameter $k$ in \medmean algorithm. This parameter controls the number of chunks that we split the data into; then we take the median of $k$ means determined by these chunks. We vary $k$ in the set $[3, 5, 10, 15, 20, 30]$. For the case where $n < k$, we simply set $k=n$. These results are shown in Figures \ref{fig:med_mean_dkk}, \ref{fig:med_mean_unif_top}, \ref{fig:med_mean_sp_gaus_one}, \ref{fig:med_mean_nonsp_gaus_one}. We find that although there is not always an obvious choice for $k$, that $k=10$ tends to perform well throughout most settings. However, we find that this and larger choices of $k$ are more prone to error as $\eta$ increases than smaller choices of $k$. Approximately when $\eta>0.15$, $k=3$ becomes the best choice of $k$. However, with smaller corruption, such as $\eta=0.10$ which we generally test, $k=3$ performs notably worse, making $k=10$ a better choice. Since we utilize $\eta=0.1$ as a default value, we set $k=10$ throughout our experiments. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/med_mean_id_dkk.png}
    \caption{Median Of Means - Number of Chunks $k$: Identity Covariance, DKK Noise}
    \label{fig:med_mean_dkk}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/med_mean_id_unif_top.png}
    \caption{Median Of Means - Number of Chunks $k$: Identity Covariance, In Distribution Noise}
    \label{fig:med_mean_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/med_mean_sp_gaus_one.png}
    \caption{Median Of Means - Number of Chunks $k$: Large Spherical Covariance, Additive Variance Shell Noise}
    \label{fig:med_mean_sp_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/med_mean_nonsp_gaus_one.png}
    \caption{Median Of Means - Number of Chunks $k$: Large Diminishing Covariance, Additive Variance Shell Noise}
    \label{fig:med_mean_nonsp_gaus_one}
\end{figure}

\clearpage

\paragraph{LRV Weighting Procedure}

 Here we explore the weighting procedure in \lrv. First, we examine the parameter $C$ in the weighting procedure for \lrv. This parameter is used when we calculate weights for each point, $x_i$ as $w_i = \exp(-\|x_i - a\|^2/(C * s^2))$. 

We vary $C$ in the set $[0.1, 0.5, 1, 5, 10, 20, 50]$. Results are shown in Figures \ref{fig:lrvc_id_dkk}, \ref{fig:lrvc_id_unif_top}, \ref{fig:lrvc_sp_gaus_one}, \ref{fig:lrvc_nonsp_gaus_one}. We notice that performance may degrade with choices of $C$ that are too high or too low, such as with $C=0.5$ and $50$. We also notice that smaller choices of $C$ tend to degrade worse with greater corruption. To strike a balance, we select $C=1$ throughout our experiments, which consistently performs among the best throughout the hyperparameter trials that we test, and is the default value used the original author's implementation of \lrv. Although there are cases where larger choices of $C$ noticeably outperform $C=1$, this is not robust as such choices may perform meaningfully worse over different noise distributions. For example, $C=20$ noticeably outperforms $C=1$ over Large Diminishing Covariance with Additive Variance Shell Noise, especially with larger $\eta$, but performs much worse over Identity Covariance with DKK Noise and Large Spherical Covariance with Additive Variance Shell Noise.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/lrv_tuning/id_dkk.png}
    \caption{LRV - Choice Of $C$: Identity Covariance, DKK Noise}
    \label{fig:lrvc_id_dkk}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/lrv_tuning/id_unif_top.png}
    \caption{LRV - Choice Of $C$: Identity Covariance, In Distribution Noise}
    \label{fig:lrvc_id_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/lrv_tuning/sp_gaus_one.png}
    \caption{LRV - Choice Of $C$: Large Spherical Covariance, Additive Variance Shell Noise}
    \label{fig:lrvc_sp_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/lrv_tuning/nonsp_gaus_one.png}
    \caption{LRV - Choice Of $C$: Large Diminishing Covariance, Additive Variance Shell Noise}
    \label{fig:lrvc_nonsp_gaus_one}
\end{figure}

\clearpage

We additionally compare the weighting procedure of \lrv that we consider with one meant for more general distributions discussed by \cite{lai2016agnostic}. Rather than downweighting outliers, this alternate procedure completely prunes outliers by calculating a point, $\mu'$, analogous to the coordinate wise median, finding a ball centered at $\mu'$ that contains $1-\tau$ percentage of data points, and throwing away all points outside of this ball. Results are shown in Figures \ref{fig:lrvg_dkk}, \ref{fig:lrvg_unif_top}, \ref{fig:lrvg_sp_gaus_one}, \ref{fig:lrvg_nonsp_gaus_one}. This general weighting procedure performs meaningfully worse than the Gaussian weighting procedure and degrades significantly worse with larger corruption across all of the distributions considered. However, it achieves similar results as data size increases. Since we focus on the low data size regime and synthetic data with Gaussian inliers, we only evaluate \lrv with Gaussian-based outlier downweighting.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/lrv_tuning_id_dkk.png}
    \caption{LRV - Gaussian Vs General Weighting: Identity Covariance, DKK Noise}
    \label{fig:lrvg_dkk}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/lrv_tuning_id_unif_top.png}
    \caption{LRV - Gaussian Vs General Weighting: Identity Covariance, In Distribution Noise}
    \label{fig:lrvg_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/lrv_tuning_sp_gaus_one.png}
    \caption{LRV - Gaussian Vs General Weighting: Large Spherical Covariance, Additive Variance Shell Noise}
    \label{fig:lrvg_sp_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/lrv_tuning_nonsp_gaus_one.png}
    \caption{LRV - Gaussian Vs General Weighting: Large Diminishing Covariance, Additive Variance Shell Noise}
    \label{fig:lrvg_nonsp_gaus_one}
\end{figure}

\clearpage

\paragraph{Eigenvalue Pruning Tail Threshold}

Here we explore the pruning routine in \evln. First, we examine the parameter $\gamma$ in the pruning threshold for \evln. $\gamma$ weights the expectation that the Gaussian concentration inequality gives for how many points will surpass a certain value; larger values correspond to less aggressive pruning. We vary $\gamma$ in the set $[0.5, 1, 2.5, 5, 10, 20, 50]$. Results are shown in Figures \ref{fig:ev_id_dkk}, \ref{fig:ev_id_unif_top}, \ref{fig:ev_sp_gaus_one}, \ref{fig:ev_nonsp_gaus_one}. We find that using values of $\gamma$ that are too small result in significantly worse error. Setting $\gamma=0.5$ or $\gamma=1$ both achieve performance identical to \sample because the pruning threshold is too sensitive, performing significantly worse than all other choices of $\gamma$, as it determines all data to be outliers. We note that when all data is determined to be outliers, we simply return \sample. However, using reasonably sized $\gamma$ results in mostly similar performance across distributions. Notably, larger values of $\gamma$ tend to perform better over large diminishing covariance with additive variance shell noise, especially with larger $n$. We select $\gamma=5$ throughout our experiments.

\begin{figure}[h]
    \centering    \includegraphics[width=0.85\linewidth]{UpdatedFigures/ev_tuning/id_dkk.png}
    \caption{Eigenvalue Pruning - Choice Of $\gamma$: Identity Covariance, DKK Noise}
    \label{fig:ev_id_dkk}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/ev_tuning/id_unif_top.png}
    \caption{Eigenvalue Pruning - Choice Of $\gamma$: Identity Covariance, In Distribution Noise}
    \label{fig:ev_id_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/ev_tuning/sp_gaus_one.png}
    \caption{Eigenvalue Pruning - Choice Of $\gamma$: Large Spherical Covariance, Additive Variance Shell Noise}
    \label{fig:ev_sp_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/ev_tuning/nonsp_gaus_one.png}
    \caption{Eigenvalue Pruning - Choice Of $\gamma$: Large Diminishing Covariance, Additive Variance Shell Noise}
    \label{fig:ev_nonsp_gaus_one}
\end{figure}

\clearpage

We also explore \evln using two alternate pruning methods not explicitly based on the Gaussian assumption as the current one is: randomized pruning and fixed pruning,. Randomized Pruning removes points based on a random scaling of the largest deviation in the dataset. Define $T$ as the largest deviation of a point projected onto the top eigenvector from the median of the projected points. Draw $Z$ from the distribution on $[0, 1]$ with probability density function $2x$. Then, prune all points whose projected distance onto the top eigenvector is at least $TZ$. This randomized pruning method is derived from the mean estimation algorithm for unknown covariance distributions by \cite{diakonikolas2017being}.
Fixed Pruning simply prunes the $0.5\tau$ percentage of points whose projection onto the top eigenvector is furthest from the median of the projected points at every iteration. This is identical to the pruning method in \queln, with projected deviations being used as "outlier scores", instead of the quantum entropy scores used in \queln. Results are shown in Figures \ref{fig:ev_pruning_id_dkk}, \ref{fig:ev_pruning_id_unif_top}, \ref{fig:ev_pruning_sp_gaus_one}, \ref{fig:ev_pruning_nonsp_gaus_one}. We find that both randomized and fixed pruning are able to match or slightly outperform the standard Gaussian pruning method. However, we note that unlike in \queln, fixed pruning could potentially result in catastrophic error. In particular, if corruption is uniformly distributed across $O(d)$ orthogonal clusters, then \evln may take $O(d)$ runs to return an outlier, since it can only prune in one direction at once. But with fixed pruning, each iteration will prune too many outliers in each direction. We only evaluate Gaussian pruning to follow the conventions of \cite{diakonikolas2017being}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/ev_pruning_id_dkk.png}
    \caption{Eigenvalue Pruning - Pruning Method: Identity Covariance, DKK Noise}
    \label{fig:ev_pruning_id_dkk}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/ev_pruning_id_unif_top.png}
    \caption{Eigenvalue Pruning - Pruning Method: Identity Covariance, In Distribution Noise}
    \label{fig:ev_pruning_id_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/ev_pruning_sp_gaus_one.png}
    \caption{Eigenvalue Pruning - Pruning Method: Large Spherical Covariance, Additive Variance Shell Noise}
    \label{fig:ev_pruning_sp_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/ev_pruning_nonsp_gaus_one.png}
    \caption{Eigenvalue Pruning - Pruning Method: Large Diminishing Covariance, Additive Variance Shell Noise}
    \label{fig:ev_pruning_nonsp_gaus_one}
\end{figure}

\clearpage

\paragraph{Projected Gradient Descent Iterations}

 Here we explore the number of iterations parameter, $\gamma$ for \pgd. We choose values for $\gamma$ in the set $[1, 5, 10, 15, 20]$. Results are shown in Figures \ref{fig:pgd_id_dkk}, \ref{fig:pgd_id_unif_top}, \ref{fig:pgd_sp_gaus_one}, \ref{fig:pgd_nonsp_gaus_one}. We find that low choices of $\gamma$ result in significantly worse performance, while higher choices perform roughly equally. Notably, when $\gamma$ is set equal to $10$, \pgd performs much worse over Identity Covariance with DKK Noise, especially under large $n$, despite this choice of $\gamma$ performing among the best across other distributions. This suggests that larger choices of $\gamma$ may be necessary for \pgd to be robust across different corruption schemes. We note that the runtime of \pgd increases approximately linearly with respect to $\gamma$, so there is a meaningful tradeoff when using larger values of $\gamma$. We set $\gamma=15$ across our experiments because it is the lowest $\gamma$ that performs among the best across the distributions tested.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/pgd_id_dkk.png}
    \caption{Projected Gradient Descent - Number Of Iterations $\gamma$: Identity Covariance, DKK Noise}
    \label{fig:pgd_id_dkk}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/pgd_id_unif_top.png}
    \caption{Projected Gradient Descent - Number Of Iterations $\gamma$: Identity Covariance, In Distribution Noise}
    \label{fig:pgd_id_unif_top}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/pgd_sp_gaus_one.png}
    \caption{Projected Gradient Descent - Number Of Iterations $\gamma$: Large Spherical Covariance, Additive Variance Shell Noise}
    \label{fig:pgd_sp_gaus_one}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{UpdatedFigures/Tuning/pgd_nonsp_gaus_one.png}
    \caption{Projected Gradient Descent - Number Of Iterations $\gamma$: Large Diminishing Covariance, Additive Variance Shell Noise}
    \label{fig:pgd_nonsp_gaus_one}
\end{figure}

\clearpage


\subsection{Robustness To Expected Corruption}
\label{app:expected_corruption}

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/ExpectedCorruption/expected_corruption_robustness-id_dkk.png}
        \caption{Identity Covariance - DKK Noise}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/ExpectedCorruption/expected_corruption_robustness-id_sub.png}
        \caption{Identity Covariance - Subtractive Noise}
    \end{subfigure}
    \bigskip
    
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/ExpectedCorruption/expected_corruption_robustness-sp_gaus_one.png}
        \caption{Large Spherical Covariance - Additive Variance Shell Noise}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/ExpectedCorruption/expected_corruption_robustness-nonsp_gaus_one.png}
        \caption{Large Diminishing Covariance - Additive Variance Shell Noise}
    \end{subfigure}
    \caption{Robustness To Expected Corruption: Error vs Expected Corruption $\tau$}
    \label{fig:corr_robust}
\end{figure}

We examine robustness to expected corruption, $\tau$.~This is a hyperparameter for \evln, \queln, \pgd, \lvsim, and \coordprune. In \evln, $\tau$ only plays a soft role as a slack term in the filtering step. In \queln, $\tau$ controls the number of points that are pruned in every iteration of the algorithm, but the number of iterations is unbounded.  In \lvsim, and \coordprune, $\tau$ explicitly controls the amount of data that is pruned in total.  In \pgd, $\tau$ controls the space of feasible outlier weights. 

We evaluate error as expected corruption, $\tau$, varies from $\tau=0.01$ to $\tau=0.46$ with true corruption fixed as $\eta=0.20$. Otherwise, the experiment setup remains the same as seen previously. As in Appendix \ref{app:hp_tuning}, we replicate experiments over Identity Covariance with DKK Noise and with Subtractive Noise; Large Spherical Covariance with Additive Variance Shell Noise; and Large Diminishing Covariance with Additive Variance Shell Noise. These results are shown in Figure \ref{fig:corr_robust}, with all estimators included for reference. We include \queln with and without early halting.

We find that most estimators perform nearly identically regardless of the choice of $\tau$, except for \pgd, which performs nearly identically when $\tau$ is an upper bound on true corruption $\eta$ but degrades with underestimates of $\eta$.    \queln without early halting performs well throughout choices of $\tau$. With smaller choices of $\tau$, it will prune significantly less points at each iteration, but will run for more iterations until the corruption detection threshold is passed, while for larger choices of $\tau$, it will prune more points at each iteration, but will run for less iterations until the corruption detection threshold is passed. \queln with early halting, which is used throughout the real world experiments, sees degradation with underestimates of $\tau$, but identical performance with overestimates. 

\evln also performs nearly identically regardless of the choice of $\tau$, as expected by the soft dependency of the pruning threshold on $\tau$. \lvsim and \coordprune both degrade noticeably as $\tau$ increases over Identity Covariance data with Subtractive Noise and Large Diminishing Covariance data with Additive Variance Shell Noise; in both cases yielding error worse than \sample the more points they prune. Surprisingly, over Large Spherical Covariance with Additive Variance Shell Noise, \lvsim nearly exactly matches the performance of \pgd, except with slightly worse degradation with large overestimates of $\tau$.  




























































\subsection{Image Embedding Experiments}
\label{app:image_experiments}

We evaluate algorithms on the problem of estimating the mean of embeddings of images generated by deep pretrained image models. As in the LLM experiment, we first examine the problem of mean estimation of image embeddings belonging to the same category, reporting LOOCV error. We then examine a corrupted distribution where images belonging to one category are considered inliers and those belonging to another are considered outliers. We utilize a set of images of cats and dogs from the CIFAR10 dataset \citep{Krizhevsky2009LearningML}. We embed these images using 4 deep pretrained image models of varying embedding dimensions: ResNet-18, ResNet-50 \citep{he2015deepresiduallearningimage}, MobileNet V3 \citep{howard2019searchingmobilenetv3}, and EffecientNet B0 \citep{tan2020efficientnetrethinkingmodelscaling}. ResNet-18 has an embedding dimension of 512, MobileNet V3 has one of 960, EfficientNet B0 has one of 1280, and ResNet-50 has one of 2048. 

\paragraph{Common Category Images}

Here we examine LOOCV error vs data size on embeddings of images of cats. We vary data size from $n=10$ to $n=1000$. Otherwise, experiments are run identically to the LLM experiment, fixing expected corruption $\eta=0.1$, employing the trace scaling heuristic on \evln and \queln, the halting heuristic on \queln, and averaging results over 5 runs. We note that, as in the LLM experiments, employing the halting heuristic on \evln does not improve performance. Results are shown in Figure \ref{fig:loocv_cat_images}.

\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVImage/Resnet512LOOCV.png}
        \caption{ResNet-18 Embeddings}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVImage/MobileNet960LOOCV.png}
        \caption{MobileNet V3 Embeddings}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVImage/EfficientNet1280LOOCV.png}
        \caption{EfficientNet B0 Embeddings}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVImage/ResNet2048LOOCV.png}
        \caption{ResNet-50 Embeddings}
    \end{subfigure}
    \caption{LOOCV Error on Cat Image Embeddings}
    \label{fig:loocv_cat_images}
\end{figure}

As in the LLM experiment, we observe that no algorithm significantly outperforms \sample, despite the nontrivial LOOCV error in each setting. As in the LLM experiment, \evln tends to perform worse than other algorithms, which is unsurprising given its sensitivity to knowledge of the true covariance. Other robust mean estimation algorithms, including \queln, perform near identically to \sample.

\paragraph{Corrupted Images}

For the corrupted case, we draw data $X \sim (1 - \eta) P + \eta Q$, where the inlier distribution, $P$, consists of embeddings of images of cats, and the outlier distribution, $Q$, consists of embeddings of images of dogs. We fix data size $n=1000$ to focus on the $n \approx d$ and $n < d$ regime. Otherwise, the experimental setup is identical to in the LLM experiments. Results are shown in Figure \ref{fig:image_corruption}.

\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionImage/Resnet512Corruption.png}
        \caption{ResNet-18 Embeddings}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionImage/MobileNet960Corruption.png}
        \caption{MobileNet V3 Embeddings}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionImage/EfficentNet1280Corruption.png}
        \caption{EfficientNet B0 Embeddings}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionImage/ResNet2048Corruption.png}
        \caption{ResNet-50 Embeddings}
    \end{subfigure}
    \caption{Error on Cat Image Embeddings Corrupted with Dog Image Embeddings}
    \label{fig:image_corruption}
\end{figure}

These results demonstrate similar trends to the LLM experiment. One key difference is that \coordprune performs much worse than \sample here, compared to the LLM experiment where it tends to slightly outperform \sample. This suggests that naive pruning does not work well in this setting as outliers are not obvious, reinforcing that this is a difficult setting for robust mean estimation. Nonetheless, several robust mean estimators are able to perform well in this case. In particular, \queln is again the strongest performer, noticeably outperforming all other estimators and nearly matching \gsample error across settings. Notably, this strong performance occurs even with $n$ much less than $d$, with relative performance remaining the same even with $n=1000$ and $d=2048$ in the ResNet-50 embedding case. Among the best estimators in the synthetic data case, \pgd tends to perform similarly to \sample, except with large enough corruption across MobileNet V3 embeddings where it outperforms \sample; \lrv tends to perform similarly to \sample except under low corruption, where it noticably degrades; and \evln fails catastrophically throughout. As in the LLM experiments, \medmean outperforms robust estimators that tend to perform better in the synthetic data cases. However, \lvsim no longer performs near optimally, suggesting its sensitivity to distributional assumptions, as expected due to its general poor performance over synthetic data experiments.

\paragraph{Corruption vs Data Size}

We repeat experiments over the same corrupted data scheme but examine error vs data size. We fix true corruption $\eta=0.1$, set expected corruption $\tau=\eta$, and vary data size $n$. We examine the performance of all estimators with data size ranging from $n=100$ to $n=5000$. We additionally provide a zoomed in plot, examining the performance of estimators excluding \evln, \coordmed, and \coordprune -- which all fail catastrophically -- with data size from $n=100$ to $n=1000$. Results are shown in Figure \ref{fig:image_corr_vs_n}.

We find that the relative performance of algorithms remains similar across data sizes. Particularly, even with very large $n$, such as $n=5000$ and $d=512$ under ResNet-18 Embeddings, only \queln and \medmean consistently outperform \sample. \pgd, \lrv, and \lvsim tend to perform slightly worse than \sample. \evln fails catastrophically regardless of data size, though the error stabilizes with larger $n$. These results suggest that the weakness of robust mean estimators over real world data distributions is not just confined to the low data size regime. Yet again, we find that \queln is the best performer, outperforming all other estimators and achieving near optimal performance throughout settings. Additionally, \medmean does not show this same sensitivity to distributional assumptions as other estimators, and as in the synthetic data experiments, tends to perform near optimally with large enough $n$.

\begin{figure}[h]
    \centering

    
    \begin{subfigure}[b]{\textwidth}
        \begin{subfigure}[c]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{UpdatedFigures/CorruptionImage/RNet512-5000.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[c]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{UpdatedFigures/CorruptionImage/RNet512DataSize1000.png}
        \end{subfigure}
        \caption{ResNet-18 Embeddings}
    \end{subfigure}

    \vskip\baselineskip 
\end{figure}

\begin{figure}
\ContinuedFloat
    
    \begin{subfigure}[b]{\textwidth}
        \begin{subfigure}[c]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{UpdatedFigures/CorruptionImage/MNet960-5000.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[c]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{UpdatedFigures/CorruptionImage/MnetDataSize1000.png}
        \end{subfigure}
        \caption{MobileNet V3 Embeddings}
    \end{subfigure}

    
    \begin{subfigure}[b]{\textwidth}
        \begin{subfigure}[c]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{UpdatedFigures/CorruptionImage/ENet1280-5000.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[c]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{UpdatedFigures/CorruptionImage/ENetDataSize1000.png}
        \end{subfigure}
         \caption{EfficientNet B0 Embeddings}
    \end{subfigure}
    \vskip\baselineskip 
   

    
    \begin{subfigure}[b]{\textwidth}
        \begin{subfigure}[c]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{UpdatedFigures/CorruptionImage/RNet2048-5000.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[c]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{UpdatedFigures/CorruptionImage/RNet2048DataSize1000.png}
        \end{subfigure}
        \caption{ResNet-50 Embeddings}
    \end{subfigure}
    
    \caption{Error Vs Data Size on Corrupted Image Data}
    \label{fig:image_corr_vs_n}
\end{figure}

\clearpage

\subsection{Word Embedding Experiments}
\label{app:word_experiments}

We further evaluate algorithms on the problem of estimating the mean of non attention based embeddings of words. As in the LLM experiment, we first examine the problem of mean estimation over words belonging in the same category, reporting LOOCV error. We then examine a corrupted distribution where words belonging to one category are considered inliers and those belonging to another are considered outliers. We examine four different pretrained GloVe \citep{pennington-etal-2014-glove} models from GluonNLP\footnote{\url{https://github.com/dmlc/gluon-nlp/}} generating 50, 100, 200, and 300 dimensional embeddings. We utilize datasets of 100 pleasant words and 100 unpleasant words from \cite{aboagye2023interpretable}. The very limited data size available under this setting provides a valuable real world test for robust estimators under low data size.

\paragraph{Common Category Words}

Here we examine LOOCV error vs data size on embeddings of "pleasant" words. Experiments are run identically to the LLM experiment, employing the trace scaling heuristic on \evln and \queln, the halting heuristic on \queln, and averaging results over 5 runs. Results are shown in Figure \ref{fig:loocv_pleasant}.

\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVGloVe/pleasant50.png}
        \caption{50 Dimensional Embeddings}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVGloVe/pleasant100.png}
        \caption{100 Dimensional Embeddings}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVGloVe/pleasant200.png}
        \caption{200 Dimensional Embeddings}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVGloVe/pleasant300.png}
        \caption{300 Dimensional Embeddings}
    \end{subfigure}
    \caption{LOOCV Error on "Pleasant" GloVe Embeddings}
    \label{fig:loocv_pleasant}
\end{figure}

As in the LLM experiment, we observe that no algorithm significantly outperforms \sample, despite the nontrivial LOOCV error in each setting.  Moreover, we observe that \medmean consistently achieves error slightly worse than \sample, which is not seen in the LLM experiments, suggesting the algorithm's sensitivity to distributional assumptions. However, unlike in the LLM experiment \evln does not fail catastrophically here, instead nearly matching \sample. 
This is not unexpected given \evln, and the trace estimate techniques sensitivity to distributional assumptions will sometime work -- including this case. \lrv also tends to perform worse than other algorithms, though this gap is not as large as in the LLM experiment. 





\paragraph{Corrupted Words}

For the corrupted case, we draw data $X \sim (1 - \eta) P + \eta Q$, where the inlier distribution, $P$, consists of embeddings of "pleasant" words, and the outlier distribution, $Q$, consists of embeddings of "unpleasant" words. This models a more extreme version of the case where ill-defined words may be placed in a category, inducing bias. This is a notable problem for word vectors, which do not take context into account~\citep{hu-etal-2016-different}. The experimental setup is identical to in the LLM experiments. Results are shown in Figure \ref{fig:corrupted_wordvecs}.

\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/50.png}
        \caption{50 Dimensional Embeddings}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/100.png}
        \caption{100 Dimensional Embeddings}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/200.png}
        \caption{200 Dimensional Embeddings}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/300.png}
        \caption{300 Dimensional Embeddings}
    \end{subfigure}
    \caption{Error on "Pleasant" Embeddings Corrupted with "Unpleasant" Embeddings}
    \label{fig:corrupted_wordvecs}
\end{figure}




While these results are different from the LLM experiment, they demonstrate similar trends. In particular, \queln is again the strongest performer, noticeably outperforming all other estimators across the 200 and 300 dimensional cases, and never performing worse than \sample in the 50 and 100 dimensional cases. Notably, this strong performance occurs even with $n$ much less than $d$. Unlike the LLM experiments, here \queln never approaches \gsample, and is beat by other estimators in the 50 and 100 dimensional cases. \lvsim, which tended to perform similarly to \queln and nearly match \gsample in the LLM experiments, does not perform as well in this case. It always beats \sample but does not come close to matching \gsample and performs similarly to other estimators. Likewise, \medmean does not perform as strongly here as in the LLM experiments and even performs worse than \sample over very low corruption. Supported by synthetic data results, this suggests the sensitivity of \medmean and \lvsim to distributional assumptions. As in the LLM experiments, \lrv tends to perform much worse than \sample under low corruption and outperform \sample slightly with higher corruption; \pgd tends to outperform \sample slightly; and \coordmed, \coordprune, and \geomed tend to perform similarly or slightly worse than \sample.  As in the LOOCV experiments, \evln simply matches \sample here. 

\paragraph{Additional Experiments}
We perform additional experiments, swapping the roles of "pleasant" and "unpleasant" embeddings. We report LOOCV error vs data size on embeddings of "unpleasant" words in Figure \ref{fig:loocv_unpleasant}. We report corrupted error vs data size on embeddings of "unpleasant" words corrupted with "pleasant" words in Figure \ref{fig:corrupted_wordvecs_inv}. We observe the same trends as in the previous word embedding experiments. 

\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVGloVe/unpleasant50.png}
        \caption{50 Dimensional Embeddings}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVGloVe/unpleasant100.png}
        \caption{100 Dimensional Embeddings}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVGloVe/unpleasant200.png}
        \caption{200 Dimensional Embeddings}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCVGloVe/unpleasant300.png}
        \caption{300 Dimensional Embeddings}
    \end{subfigure}
    \caption{LOOCV Error on "Unpleasant" GloVe Embeddings}
    \label{fig:loocv_unpleasant}
\end{figure}

\begin{figure}[t]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/50Inv.png}
        \caption{50 Dimensional Embeddings}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/100Inv.png}
        \caption{100 Dimensional Embeddings}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/200Inv.png}
        \caption{200 Dimensional Embeddings}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/CorruptionGloVe/300Inv.png}
        \caption{300 Dimensional Embeddings}
    \end{subfigure}
    \caption{Error on "Unpleasant" Embeddings Corrupted with "Pleasant" Embeddings}
    \label{fig:corrupted_wordvecs_inv}
\end{figure}


\clearpage

\subsection{LLM Experiment Ablations}
\label{app:llm_ablations}



\paragraph{Eigenvalue Pruning Method Comparison}

We compare the performance of different pruning subroutines for \evln over a selection of LLM experiments: LOOCV and Corruption Error over MiniLM and BERT embeddings. We evaluate Gaussian pruning, used throughout this paper, along with randomized pruning and fixed pruning, described in Appendix \ref{app:hp_tuning}. We retain the same conditions as in the original experiments, first scaling data utilizing the sample trace. We also include \sample and \queln in our plots for the sake of comparison, noting that \queln and \evln with fixed pruning only differ in their method of scoring outliers. These results are shown in Figure \ref{fig:llm_ev_pruning}. We notice that both randomized and fixed pruning methods do indeed perform better than the Gaussian pruning method. In particular, fixed pruning has the best LOOCV error over MiniLM and matches the error of \sample over BERT, whereas Gaussian pruning fails dramatically. However, this performance does not translate into the corrupted case, where all three pruning routines lead to significant error compared to even \sample, except with large $\eta$ where it \sample's error approaches that of these methods.  Additionally, as discussed in Appendix \ref{app:hp_tuning}, \evln with fixed pruning is not robust to noise distributions that require several runs of the algorithm to prune i.e. cases where noise lays in multiple orthogonal clusters. Notably, \queln outperforms all variations of \evln in the corrupted data case, reinforcing the observation that the outlier detection method of \queln is more robust to distributional assumptions than that of \evln.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/EVPruningMiniLM_Field_Land.png}
        \caption{LOOCV Error - MiniLM}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/EVPruningBert_Field_Land.png}
        \caption{LOOCV Error - BERT}
    \end{subfigure}
    \bigskip
    
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/EVPruningMiniLM.png}
        \caption{Corrupted Error - MiniLM}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/EVPruningBert.png}
        \caption{Corrupted Error - BERT}
    \end{subfigure}
    \caption{Eigenvalue Pruning - Pruning Method: LLM Comparison}
    \label{fig:llm_ev_pruning}
\end{figure}

\paragraph{LRV Weighting Procedure}

Here we compare the two different weighting procedures for LRV described in Appendix \ref{app:hp_tuning}: Gaussian weighting, based on downweighting outliers, and general (non-Gaussian) weighting, based on completely pruning outliers. We evaluate these two methods over the same subselection of LLM experiments: LOOCV and Corruption Error over MiniLM and BERT embeddings. These results are shown in Figure \ref{fig:llm_lrv_weighting}. We notice that general weighting outperforms Gaussian weighting in LOOCV error, with this difference being especially noticeable across BERT embeddings. However, this performance increase is not seen in either corrupted case, where Gaussian weighting notably outperforms general weighting, except with small $\eta$. This suggests that, at least under low data size, \lrv is not robust to general distributions, even using a general outlier weighting procedure.



\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/LRVMiniLM_Field_Land.png}
        \caption{LOOCV Error - MiniLM}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/LRVBert_Field_Land.png}
        \caption{LOOCV Error - BERT}
    \end{subfigure}
    \bigskip
    
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/LRVMiniLM.png}
        \caption{Corrupted Error - MiniLM}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/LRVBert.png}
        \caption{Corrupted Error - BERT}
    \end{subfigure}
    \caption{LRV - Gaussian Vs General Weighting: LLM Comparison}
    \label{fig:llm_lrv_weighting}
\end{figure}

\clearpage

\paragraph{Additional Experiments}

We recreate the experiments in Section \ref{sec:realworld} over two different settings. First, we examine LOOCV Error over embeddings of the word field that correspond to the "field of study" definition rather than to the "field of land" definition. These results are shown in Figure \ref{fig:loocv_field_study}. Second, we examine corrupted embeddings $X \sim (1 - \eta)P + \eta Q$, where inlier data, $P$, consists of embeddings of the word "field" corresponding to the "field of study" definition and outlier data, $Q$, consists of embeddings of the word "field" corresponding to the "field of land" definition; inverting the inlier and outlier data originally examined. These results are shown in Figure \ref{fig:loocv_corrupted_inv}. While the LOOCV error plots are not identical to the original experiment, corresponding to the expected differences in structure between the distributions of $P$ and $Q$, we find the same overall trends across the 4 plots. We additionally observe the same overall trends for corrupted data compared to the original experiment. However, \lvsim, which was consistently the best algorithm alongside \queln for corrupted data originally, breaks down for MiniLM here; always performing notably worse than \gsample. Supported by the general poor performance of \lvsim over synthetic data experiments, this reinforces the unpredictable sensitivity of \lvsim to distributional assumptions. \queln does not see any such degradation, performing near optimally across all cases, as it does in the original LLM experiment.

\begin{figure}[h!]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/MiniLM_Field_Study.png}
        \caption{MiniLM}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/T5_Field_Study.png}
        \caption{T5}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/Bert_Field_Study.png}
        \caption{BERT}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LOOCV/Albert_Field_Study.png}
        \caption{ALBERT}
    \end{subfigure}
    \caption{LOOCV Error on "Field Of Study" Embeddings}
    \label{fig:loocv_field_study}
\end{figure}

\begin{figure}[h!]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/MiniLMInv.png}
        \caption{MiniLM}
    \end{subfigure}
        \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/T5Inv.png}
        \caption{T5}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/BertInv.png}
        \caption{BERT}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{UpdatedFigures/LLMCorruption/AlbertInv.png}
        \caption{ALBERT}
    \end{subfigure}
    \caption{Error on "Field of Study" Embeddings Corrupted with "Field of Land" Embeddings}
    \label{fig:loocv_corrupted_inv}
\end{figure}

\clearpage

\subsection{Dataset Generation}
\label{app:llm_dataset}


We generate a dataset of 400 sentences for each definition of the word \textit{field} using ChatGPT-4o, accessed in June 2024. Attention based embeddings for the word \textit{field} are extracted from these sentences for use in our LLM experiments. We used the following two prompts to obtain the sentences:

\paragraph{Field of Study}

\begin{quote}
    \textit{I am running an experiment where I examine embeddings of the word "field" with two different contexts. Please generate 400 unique sentences using the word "field" in context with the following definition: "a particular branch of study or sphere of activity or interest." Please return these sentences in the format of a JSON file.}
\end{quote}

\paragraph{Field of Land}

\begin{quote}
    \textit{I am running an experiment where I examine embeddings of the word "field" with two different contexts. Please generate 400 unique sentences using the word "field" in context with the following definition: "an area of open land, especially one planted with crops or pasture, typically bounded by hedges or fences." Please return these sentences in the format of a JSON file.}
\end{quote}

\paragraph{Additional Prompts}

ChatGPT-4o did not produce the full 400 sentences in one go. To address this, we used the following additional prompts until we had generated the required number of sentences, and then manually combined the generated outputs. The prompt for "field of study" sentences is slightly different, as we originally observed that ChatGPT-4o would reuse the same field of study across numerous sentences.

For \textit{Field of Study}:

\begin{quote}
    \textit{Please generate 100 more sentences. Do not repeat similar sentences or use "field" to refer to the same field of study multiple times.}
\end{quote}

For \textit{Field of Land}:

\begin{quote}
    \textit{Please generate 100 more sentences.}
\end{quote}

\paragraph{Tables Of Generated Sentences}

We include the following tables of generated sentences.

\textbf{Field Of Land Sentences}: 

\input{UpdatedFigures/SentenceTables/field_land_table}

\textbf{Field Of Study Sentences}: 

\input{UpdatedFigures/SentenceTables/field_study_table}


\end{document}
