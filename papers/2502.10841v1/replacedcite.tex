\section{Related Works}
\subsection{GAN-based Portrait Animation}
%LivePortrait

Early approaches to portrait animation have utilized GANs for learning motion dynamics in a self-supervised manner. The seminal works ____ typically involve a two-stage process: warping and rendering. These methods use sparse neural keypoints to predict head motion, which is then applied to warp the encoded identity features of the source. The warped features are subsequently passed through a generative decoder to produce animated frames, incorporating in-painted facial details and background elements. In subsequent research, several approaches have sought to improve the warping estimation, with notable contributions including the use of 3D neural keypoints ____, depth information ____, and thin-plate splines ____. Additionally, ReenactArtFace ____ employed a 3D morphable model to control expressions and poses, while ToonTalker ____ applied a transformer-based architecture to enhance the warping process, particularly in the context of cross-domain datasets. Concurrently, efforts to refine the rendering process have resulted in several advancements. For instance, MegaPortraits ____ elevated the resolution of the generated portraits to megapixels by utilizing high-resolution image datasets. FADM ____ proposed a coarse-to-fine animation framework, incorporating a diffusion process to enhance facial details from the initial outputs generated by FOMM ____ and Face Vid2Vid ____. Beyond traditional video reenactment, significant strides have been made in leveraging alternative driving signals, such as 3D facial priors ____ and audio cues ____. Despite these advancements, existing methods primarily rely on explicit feature warping, with an emphasis on facial animation in talking scenarios. 

% This paper introduces a novel approach using video diffusion transformers to generate high-fidelity, expressive animations for portraits.



\subsection{Diffusion-based Portrait Animation}
%Follow-Your-Emoji
%V-Express
%MegActor
%X-Portrait

Recent advancements in diffusion models ____ have led to exceptional performance across a variety of generative tasks, including image ____, video ____, and multi-view renderings ____. Latent diffusion models (LDMs) ____ have further refined this approach by reducing computational costs, performing the diffusion process within a lower-dimensional latent space. In the context of portrait animation, pre-trained diffusion models ____ have been widely adopted for image-to-video tasks. Notably, several studies ____ have demonstrated the effectiveness of integrating reference image features into the self-attention blocks of LDM UNets, enabling improved image editing and video generation while preserving the original appearance context. 
In addition,  ControlNet ____ extends the LDM architecture by incorporating structural control signals, such as landmarks, segmentations, and dense poses, to guide controllable image generation. This has inspired several concurrent works ____ that achieve state-of-the-art full-body portrait animation by seamlessly integrating appearance and motion controls along with temporal attention mechanisms ____ within pre-trained UNet models. Despite these advances, the motion control signals employed in these methods—such as skeletons with or without facial landmarks ____ and dense poses ____—may not fully capture the original motion dynamics. %Moreover, they are often reliant on the accuracy of third-party detectors, which can limit both the expressiveness and stability of the generated animations. 
Furthermore, recent approaches ____ that focus on facial expression and motion control have introduced more expressive landmarks for enhanced facial representation. However, these methods face challenges when applied to diverse body proportions, as they struggle to maintain consistent facial dynamics when transitioning to half-body or full-body compositions. In this work, we investigate the use of an advanced video diffusion transformers architecture to enhance portrait animation capabilities.