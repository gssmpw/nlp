\section{Related Works}
\subsection{GAN-based Portrait Animation}
%LivePortrait

Early approaches to portrait animation have utilized GANs for learning motion dynamics in a self-supervised manner. The seminal works \cite{Siarohin_2019_CVPR,Siarohin_2019_NeurIPS} typically involve a two-stage process: warping and rendering. These methods use sparse neural keypoints to predict head motion, which is then applied to warp the encoded identity features of the source. The warped features are subsequently passed through a generative decoder to produce animated frames, incorporating in-painted facial details and background elements. In subsequent research, several approaches have sought to improve the warping estimation, with notable contributions including the use of 3D neural keypoints \cite{wang2021facevid2vid}, depth information \cite{hong2022depth}, and thin-plate splines \cite{siarohin2021motion}. Additionally, ReenactArtFace \cite{10061279} employed a 3D morphable model to control expressions and poses, while ToonTalker \cite{gong2023toontalker} applied a transformer-based architecture to enhance the warping process, particularly in the context of cross-domain datasets. Concurrently, efforts to refine the rendering process have resulted in several advancements. For instance, MegaPortraits \cite{megaportraits} elevated the resolution of the generated portraits to megapixels by utilizing high-resolution image datasets. FADM \cite{1020894} proposed a coarse-to-fine animation framework, incorporating a diffusion process to enhance facial details from the initial outputs generated by FOMM \cite{Siarohin_2019_NeurIPS} and Face Vid2Vid \cite{wang2021facevid2vid}. Beyond traditional video reenactment, significant strides have been made in leveraging alternative driving signals, such as 3D facial priors \cite{deng2020disentangled,Khakhulin2022ROME,sun2023next3d,DECA:Siggraph2021,10203414} and audio cues \cite{guo2021adnerf,10.1145/3550469.3555393,fei2024flux}. Despite these advancements, existing methods primarily rely on explicit feature warping, with an emphasis on facial animation in talking scenarios. 

% This paper introduces a novel approach using video diffusion transformers to generate high-fidelity, expressive animations for portraits.



\subsection{Diffusion-based Portrait Animation}
%Follow-Your-Emoji
%V-Express
%MegActor
%X-Portrait

Recent advancements in diffusion models \cite{ho2020denoising,song2020denoising,song2020score} have led to exceptional performance across a variety of generative tasks, including image \cite{saharia2022photorealistic}, video \cite{guo2023animatediff,blattmann2023stable,bao2023latentwarp}, and multi-view renderings \cite{liu2023zero1to3, liu2023one2345, gu2023diffportrait3d}. Latent diffusion models (LDMs) \cite{rombach2021highresolution,fei2024dimba} have further refined this approach by reducing computational costs, performing the diffusion process within a lower-dimensional latent space. In the context of portrait animation, pre-trained diffusion models \cite{saharia2022photorealistic, rombach2021highresolution} have been widely adopted for image-to-video tasks. Notably, several studies \cite{cao2023masactrl, lin2023consistent123} have demonstrated the effectiveness of integrating reference image features into the self-attention blocks of LDM UNets, enabling improved image editing and video generation while preserving the original appearance context. 
In addition,  ControlNet \cite{zhang2023adding} extends the LDM architecture by incorporating structural control signals, such as landmarks, segmentations, and dense poses, to guide controllable image generation. This has inspired several concurrent works \cite{hu2023animateanyone, xu2023magicanimate, chang2024magicpose,qiu2024moviecharacter,fang2024motioncharacter} that achieve state-of-the-art full-body portrait animation by seamlessly integrating appearance and motion controls along with temporal attention mechanisms \cite{guo2023animatediff, guo2023sparsectrl} within pre-trained UNet models. Despite these advances, the motion control signals employed in these methods—such as skeletons with or without facial landmarks \cite{hu2023animateanyone, chang2024magicpose} and dense poses \cite{xu2023magicanimate}—may not fully capture the original motion dynamics. %Moreover, they are often reliant on the accuracy of third-party detectors, which can limit both the expressiveness and stability of the generated animations. 
Furthermore, recent approaches \cite{ma2024follow, chen2024echomimic} that focus on facial expression and motion control have introduced more expressive landmarks for enhanced facial representation. However, these methods face challenges when applied to diverse body proportions, as they struggle to maintain consistent facial dynamics when transitioning to half-body or full-body compositions. In this work, we investigate the use of an advanced video diffusion transformers architecture to enhance portrait animation capabilities.