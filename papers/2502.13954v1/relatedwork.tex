\section{Related Work}
\textbf{Multimodal Multi-label Emotion Regression.} It aims to infer human emotions from textual, audio, and visual sequences in video clips, often encompassing multiple affective states. The primary challenges in MMER is integrating multimodal data. Early studies like MISA \citep{b23} address modality heterogeneity by decoupled invariant and modality-specific features for fusion. MMS2S \citep{b9} and HHMPN \citep{b3} focused on modeling label-to-label and label-to-modality dependencies using Transformer and GNNs network. Recent approaches \citep{b5, b6, b7} incorporated advanced training techniques; for example, TAILOR \citep{b7} utilized adversarial learning to differentiate common and private modal features, while AMP \citep{b24} employed masking and parameter perturbation to mitigate modality bias and enhance robustness. However, these works all model from multimodal fusion instead of emotion latent space. \\
\textbf{Uncertainty-aware Learning and Calibration.} Deep models often overconfidently assign high confidence to incorrect predictions, making uncertainty-aware learning essential to ensure confidence accurately reflects prediction uncertainty \citep{b28}. The primary goal is to calibrate model confidence to match the true probability of correctness. There are two main approaches: calibrated uncertainty \citep{b28} and ordinal or ranking-based uncertainty \citep{b29}. Calibration methods, such as histogram binning, temperature scaling, and accuracy versus uncertainty calibration \citep{b30,b28,b31}, align predicted confidence with actual correctness. Meanwhile, ranking-based methods like Confidence Ranking Loss (CRL) \citep{b29} enforce accurate confidence rankings among correctly classified samples based on feature distinctiveness.\\
\textbf{Uncertainty-based Multimodal Fusion.} Uncertainty learning enhances multimodal fusion across tasks. \citet{b27} employed Bayesian deep learning and AvU to guide fusion, while \citet{b26} used temporal-invariant learning to reduce redundancy and noise, improving robustness. But these methods incorporate uncertainty without calibration. In contrast, COLD \citep{b25} leveraged GURs to model feature distributions across modalities, quantifies modality contributions with variance norms, and integrated both calibrated and ranking-based uncertainty to regulate fusion variance. However, there hasn't exploration of uncertainty-aware for MMER.