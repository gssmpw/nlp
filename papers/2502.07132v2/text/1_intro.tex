% \vspace{-.10em}
\section{Introduction}
\label{sec:intro}
% \vspace{-.15em}

\begin{figure}[t]
    \centering
    % \vspace{-0.75em}
    \Description{ % needed for accessibility: helps screen readers.
        The image shows an example of attributes from different data sources (T1, T2, and the GDC standard). Arrows linking the values indicate they are equivalent despite their different terminologies.
    } 
    \includegraphics[width=.85\linewidth]{figures/tumor_grade_domains.pdf}
    % \vspace{-1em}
    \caption{Domain of attributes in different data sources.}
    \label{fig:tumor_grade_domains}
    % \vspace{-1.5em}
\end{figure}


% Extracting insights from data remains arduous and time-consuming, often due to the challenges of integrating data from multiple sources.
Extracting insights from multiple data sources remains an arduous and time-consuming task.
% Combining data from multiple sources remains an arduous and time-consuming task.
% For example, in
In fields such as biomedicine, collecting patient data requires expensive trials that typically involve only a few dozen to hundreds of patients~\cite{li2023proteogenomic}. Since this usually leads to tables with many attributes but few samples, researchers need to combine 
% data from 
different cohorts to obtain larger sample sizes. % to perform re-analysis and increase sample size.
Since data is often collected 
% at different locations and using differing methods,
using differing methods,
combining them into compatible and comparable datasets often becomes a 
% major
challenge~\cite{cheng2024general}. %Consider the following real examples of data harmonization in cancer research.
Consider these examples from cancer research.
% Consider the following examples.

\vspace{-.5em}
% \begin{tcolorbox}[colback=violet!2.5!white,colframe=violet!85!black]
\begin{example}[Clinical Data Harmonization] \label{example1}
% 
% T1 (mmc1.xlsx)
% Histologic_Grade_FIGO: 
% - FIGO grade 1
% - FIGO grade 2
% - FIGO grade 3
% - NA
% 
% T2 (mmc2.xlsx)
% Histologic_grade: 
% - "G1 Well differentiated"
% - "G2 Moderately differentiated"
% - "G3 Poorly differentiated"
% - "Other: High grade"
% - "Other: Not specified"
% - "NA"
% 
% GDC standard:
% G1
% G2
% G3
% G4
% GB
% GX
% High Grade
% Intermediate Grade
% Low Grade
% Unknown
% Not Reported

To obtain a larger dataset for a study on endometrial cancer, researchers aim to combine samples from two patient cohorts collected independently in two studies~\cite{dou2020proteogenomic, dou2023proteogenomic}.
% 
% These studies produced two tables containing clinical data, which we refer to as $T_1$~and~$T_2$.
% Rows in both tables represent an individual patient sample, and $T_1$ contains 179 columns while $T_2$ contains 213 columns.
These studies produced two tables containing clinical data, which we refer to as $T_1$~and~$T_2$. Each row in each table represents an individual patient sample.
Tables $T_1$ and $T_2$, contain 179 and 213 columns and 153 and 190 rows, respectively.
% 
The goal is to combine the data to obtain a table with 392 rows.
However, even though these datasets were produced by the same research consortium, their schemas and naming standards differ significantly.
The first challenge is identifying which columns are semantically equivalent. %: there are 38,127 possible pairs in total. % 179 x 213 = 38,127
Once all pairs of equivalent columns have been identified, their values must be standardized.
Figure~\ref{fig:tumor_grade_domains} shows an example of a pair of equivalent attributes from $T_1$ and $T_2$: \texttt{Histologic\_grade} and \texttt{Histologic\_Grade\_FIGO}.
% $T_1$ contains an attribute named \texttt{Histologic\_grade} with the set of unique values \{\texttt{"FIGO grade 1"}, \texttt{"FIGO grade 2"}, \texttt{"FIGO grade 3"}, \texttt{"NA"}\}
%  and $T_2$ named \texttt{Histologic\_Grade\_FIGO} contains the values 
% \{
% \texttt{"G1 Well differentiated"},
% \texttt{"G2 Moderately differentiated"},
% \texttt{"G3 Poorly differentiated"},
% \texttt{"Other: High grade"},
% \texttt{"Other: Not specified"},
% \texttt{"NA"}\}.
 % \textit{G1, G2, G3, G4}.
 As seen in the figure, the values of these attributes are not represented using the same terminology even though they are semantically equivalent.
 To produce a harmonized table, the researchers must reconcile these values into a single format before merging the rows.
 For instance, they may decide that the final harmonized table $T_{target}$ will contain an attribute named \texttt{histologic\_grade} and that the format of the value used will be from \texttt{Histologic\_grade}, and thus we would need to map the values of $T_1$ to their corresponding values from $T_2$
 (e.g., 
 \mbox{\texttt{"FIGO grade 1"} $\rightarrow$\texttt{"G1 Well differentiated"}},
 \texttt{"FIGO grade 2"} $\rightarrow$ \texttt{"G2 Moderately differentiated"}, and so on).
 Note,~however, 
 that some attributes may be mapped differently.
 % that not all attributes need to be mapped like this. 
 E.g., attributes with unique identifiers can be kept as they appear in the original table while ensuring a uniqueness constraint.
 \qed
\end{example}
% \end{tcolorbox}



% \begin{tcolorbox}[colback=violet!2.5!white,colframe=violet!85!black]
% \vspace{-1em}
\begin{example}[Harmonizing Data to a Standard Vocabulary] \label{example2}
In a subsequent effort to foster data reuse and enable research in pan-cancer analysis~\cite{li2023proteogenomic}, researchers decided to combine 10 tables containing cohorts of a larger variety of cancer types \cite{cao2021proteogenomic, clark2019integrated, dou2020proteogenomic, gillette2020proteogenomic, mcdermott2020proteogenomic, huang2021proteogenomic, krug2020proteogenomic, satpathy2021proteogenomic, vasaikar2019proteogenomic, wang2021proteogenomic}.
They had to map all tables to the Genomic Data Commons (GDC) standard~\cite{gdc}, a standard describing attributes commonly used in cancer research.
Figure~\ref{fig:tumor_grade_domains} shows how the variables \texttt{Histologic\_grade} and \texttt{Histologic\_Grade\_FIGO} map to their equivalent GDC attribute \texttt{tumor\_grade}.
% In this case, both these attributes could be mapped to the GDC variable named \texttt{tumor\_grade} which has acceptable values 
% \{ 
% \texttt{"G1"},
% \texttt{"G2"},
% \texttt{"G3"},
% \texttt{"G4"},
% \texttt{"GB"},
% \texttt{"GX"},
% \texttt{"High Grade"},
% \texttt{"Intermediate Grade"},
% \texttt{"Low Grade"},
% \texttt{"Unknown"},
% \texttt{"Not Reported"}\}.
Once again, the acceptable values in the GDC vocabulary differ from the values used in the tables of Example~\ref{example1}.
\qed
\end{example}
% \end{tcolorbox}



\myparagraph{The Case for Agentic Data Harmonization}
Data harmonization involves several data integration tasks.
Practitioners often rely on spreadsheet software, bespoke scripts, and significant manual work to harmonize data~\cite{cheng2024general}. These custom scripts are often not published with the data and publications, creating barriers to reproducibility and replicability of experiments with new data.~\cite{healthcareInteroperability2025}.

%  LLM opportunities
Large language models (LLMs) open new opportunities to improve data harmonization. They can answer questions about terminology, methodologies and generate code without training data.
Recently, LLMs have shown promising results in data integration tasks, including column type annotation, schema matching, and entity linkage~\cite{narayan-vldb2022, chorus-vldb2024, feuer:vldb2024, tu2023unicorn}. Prompting today's frontier LLMs with a question such as ``\textit{What does FIGO grade mean?}'' reveals that they do learn general knowledge about many topics, including biomedical research (the focus of our examples). This suggests that this information could be leveraged in data harmonization tasks.

% Agentic systems
LLMs are becoming essential components for intelligent agents across various applications due to their capabilities in language understanding, tool utilization, and adapting to new information, resembling human intelligence and reasoning~\cite{yao2022react, xi2023-agents-survey, wang2024-agents-survey}. 
These capabilities have been surfaced by advancements in LLM prompting techniques that elicit reasoning, such as Chain-of-Thought~\cite{wei2022chain}, Tree-of-Thought~\cite{yao2024tree}, and ReAct~\cite{yao2022react}. These techniques make it possible to handle complex tasks such as table understanding tasks through structured data manipulation~\cite{wang2024chainoftable}, and allow generating reasoning traces and task-specific actions that are interleaved to complete tasks.~\cite{yao2022react}.
Moreover, frameworks for building agentic systems, including LangChain~\cite{langchain}, Camel~\cite{li2023camel}, Archytas~\cite{archytas}, and AutoGen~\cite{wu2023autogen}, are becoming increasingly available.

This paper presents our vision of
intelligent agents that can interact with the users and data integration algorithms to synthesize data harmonization pipelines.  
Such agentic systems accept user task requests and prompt the user with questions required to complete a task (e.g., to request additional context or to disambiguate the input).
Similar to AutoML systems, which generate end-to-end machine learning pipelines~\cite{lopez2023alphad3m, shang2019democratizing, berti2019learn2clean}, a data harmonization agent can produce a data processing pipeline that takes as input user data and outputs a harmonized table that satisfies the user requirements. These pipelines could then be published along with research data to document the data generation process for reproducibility.


\vspace{-0.2em}
\myparagraph{Agentic Data Harmonization Challenges}
Building agentic systems presents several challenges.
First, harmonization scenarios are inherently complex, often involving difficult tasks such as schema matching, entity linkage, and data cleaning. These tasks require specialized methods to achieve high-quality results, and the scalability of these methods is critical for harmonizing large datasets.
While general-purpose LLMs offer broad capabilities, they lack transparency~\cite{pmlr-v235-huang24x} and are not optimized for these tasks, leading to inconsistent outputs, high computational costs, and performance bottlenecks, especially when handling large-scale datasets~\cite{llmDisruptVLDB2023,hsieh-etal-2023-distilling}.

Second, integrating algorithms for different tasks and generating cohesive pipelines is non-trivial.
Unlike in AutoML systems, data harmonization pipeline synthesis cannot be driven by search algorithms that optimize well-defined evaluation metrics (e.g., model accuracy) since the quality of data harmonization pipelines cannot be as easily measured using one metric.
Instead, the synthesis may need to be guided by decisions made by the users, since accuracy may depend on external knowledge. For example, in the case of Fig.~\ref{fig:tumor_grade_domains}, deciding if the correct match for `\texttt{NA}' is `\texttt{Not Reported}' or `\texttt{Unknown}' may depend on the data collection methodology.
The agent should automate the laborious tasks without sacrificing accuracy and without becoming a burden: it must learn to ask questions only when necessary to avoid overwhelming the user.
To add to these challenges, LLMs are brittle: (1) they often make mistakes (also known as hallucinations) that must be identified and corrected by users; and (2) they are known to be sensitive to the prompts (i.e., small prompt changes may lead to different results)~\cite{barrie2024prompt, khattab2024dspy}.

\myparagraph{Contributions}
This paper presents \systemname, a prototype of an agentic data harmonization system that implements our vision. 
\systemname leverages LLM capabilities to interact with users, orchestrate specialized data integration primitives, and generate custom code when existing primitives are insufficient. These primitives implement efficient and well-established data integration algorithms that can be combined to make harmonization pipelines guided based on user feedback. Since algorithms can make mistakes, we use LLMs to evaluate the outputs of these primitives. When outputs are incorrect, the agent may take additional steps to automatically correct the errors or seek assistance from the user.
We also describe how \systemname can be applied in a real-world use case, highlighting the potential and limitations of its current design and implementation. To address these limitations, we identify open problems in this field and propose future research directions, building on previous work in machine learning and data management.


In summary, we provide the following contributions:
\vspace{-0.25em}
\begin{itemize}  
    \item We present our vision for agentic systems that help users create data harmonization pipelines by combining LLM-based reasoning, interactive user interfaces, and data integration primitives.
    \item We build \systemname, a prototype data harmonization agent that follows our vision. It integrates \texttt{BDI-kit}, a library of data integration algorithms, to efficiently construct harmonization pipelines. When existing functions are insufficient, \systemname leverages the LLM to dynamically generate custom code.  
    \item We demonstrate a real-world use case where \systemname is applied to map clinical data to the GDC standard~\cite{gdc}, addressing common issues such as differing terminology and schema.
    \item We discuss key challenges in designing data harmonization agent systems and propose a research agenda with immediate steps to address remaining open problems.  
\end{itemize}