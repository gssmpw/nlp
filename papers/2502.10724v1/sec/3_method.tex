\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.98\linewidth]{Figs/overall_framework_v2.pdf}
    \caption{Overall framework: we perform model fine-tuning and 2D pose update in each epoch.
    Model Fine-tuning (Left)-The extracted video segment and its text label will be used to update $f^{\text{HPE}}$ with 2D projection, alignment, and smooth losses.
    2D Pose Update (Right)- 
    After model fine-tuning in each epoch, all images in the current test video are processed through $f^{\text{HPE}}$ to obtain predicted 2D and update the 2D keypoints using EMA. Subsequently, all video segments are checked to fill in any missing 2D keypoints by verifying conditions of motion-text alignment.}
    \label{fig:architecture} 
\end{figure*}

\section{Preliminaries} 
\subsection{Problem Setup}
Given an arbitrary video, for each RGB frame $\mathbf{X}$, we aim to predict the 3D human poses in joint coordinates $\bm{J}^{\text{3D}} \in \mathbb{R}^{K\times3}$ where $K$ is the number of keypoints. Following previous works, we utilize SMPL model~\cite{smpl} which takes as input the joint rotation $\boldsymbol{\theta} \in \mathbb{R}^{72}$ and shape $\boldsymbol{\beta} \in \mathbb{R}^{10}$ parameters along with a translation $\vt\in\mathbb{R}^3$ w.r.t. the camera to produce 3D poses, formally
\begin{equation}
    \bm{J}^{\text{3D}} = \text{SMPL}(\boldsymbol{\theta}, \boldsymbol{\beta}, \vt).
\end{equation}
In our method, the predicted $\boldsymbol{\hat{\theta}}$, $\boldsymbol{\hat{\beta}}$ and $\hat{\vt}$ are given by  
a human mesh recovery network~\cite{hmrnet} which has been trained on the Human3.6M~\cite{human36m} dataset following the setting of \cite{dynaboa,iso,cycleadapt}.  
We denote this network as our 3D human pose estimation model $f^{\text{HPE}}$.

During inference, the model is adapted with the help of 2D pose $\bar{\bm{J}}^{\text{2D}}$ from an off-the-shelf estimator~\cite{openpose} to improve the prediction of 3D poses. 
{The predicted 2D poses $\hat{\bm{J}}^{\text{2D}}$ can be obtained by projection of predicted 3D poses $\hat{\bm{J}}^{\text{3D}}$:
\begin{equation}                
    \hat{\bm{J}}^{\text{2D}}=\Pi(\hat{\bm{J}}^{\text{3D}})=\Pi(\text{SMPL}(\boldsymbol{\hat{\theta}},\boldsymbol{\hat{\beta}},\hat{\vt})),
    ~\label{Eq:motionclip_loss}
\end{equation}
where $\Pi$ is the camera projection function.}
Because our method does not involve direct operations on 3D poses, we will use $\bm{J}$ to denote 2D poses in the following descriptions.

\subsection{Motion-language Models}
A motion-language model 
integrates human motion and natural language modality.
{It utilizes pre-trained language models~\cite{gpt,bert}, to incorporate rich semantic space for improved motion representation. 
This is achieved by training with motion-text data, which enables tasks like text-conditioned motion generation~\cite{zhang2022motiondiffuse,zhang2023generating} and motion captioning~\cite{guo2022tm2t,motiongpt}. 
Our focus is on leveraging motion-language model to 
guide motion semantics for unlabeled test videos in TTA. 
We choose  
MotionCLIP~\cite{motionclip} for its text-aligned motion representation, which is optimally suited to our task.} 

MotionCLIP maps human motion sequences onto the CLIP space~\cite{clip}. It positions semantically similar motions closer together while maintaining clearer separation for distinct motions in the manifold. 
The motion-text affinity is measured using cosine similarity  
between the text CLIP feature from $f^{\text{CLIP}}$ and the motion feature from the motion encoder $f^{\text{MOTION}}$:

\begin{equation} 
    \text{sim}(\rvs,\rvm) = \cos{(f^{\text{CLIP}}(\rvs), f^{\text{MOTION}}(\rvm))}.
    ~\label{Eq:motionclip_loss}
\end{equation}

The motion-text pairs $(\rvs, \rvm)$ are obtained from labeled motion dataset~\cite{BABEL}, where $\rvs$ is text label and $\rvm=\{\bm{p}_1,\cdots, \vp_T\}$ is motion segment of $T=60$ frames. Each pose $\vp_i\in\sR^{24\times6}$ is represented in 6D format~\cite{shi2019two}, including global and local rotations.

\section{Method}


{Fig.~\ref{fig:architecture} shows the overall framework. It consists of 
semantics-aware motion prior (left panel) 
and 2D pose update module (right panel). Using semantics-aware motion prior (Sec.~\ref{sec:alignment}), predicted motion and text label are aligned in a shared space of a motion-language model, supplementing other losses to enhance semantics awareness. In the 2D pose update module (Sec.~\ref{sec:label_update}), 2D poses are updated by EMA (right top panel) while the missing 2D poses are filled with text-aligned motion prediction (right bottom panel). 
This module strengthens the semantics-aware motion prior on occlusion. Details of each module are introduced as follows.}

\label{sec:method}
{\subsection{Semantics-aware Motion Prior}}\label{sec:alignment}

Since the 2D-to-3D space is highly ambiguous, {the model supervised by 2D projection loss still suffers from depth ambiguity}.
However, when video segments are available, contextual semantic information about human motion is evident. We therefore can leverage it to reduce the 2D-to-3D ambiguity by specifying a pose space with clear semantic meaning. 
So aside 2D projection and temporal losses, we also use motion-text-aligned representation space learned by MotionCLIP~\cite{motionclip} to 
ensure the predictions grounded on the video semantics.  
This is achieved by aligning the features of predicted motion and text labels of motion semantics in the shared embedding space. 

\noindent\textbf{Text labeling.} We leverage a Vision-Language Model (VLM)~\cite{gpt4} for {human motion description and matching} with a motion dictionary defined in~\cite{BABEL}.
In general, 
the inputs contain prompts specifying requirements, a video segment, and motion dictionary. 
The outputs are text labels, like running, sitting with feet crossed, or walking upstairs, which can be assigned to each frame within the video segment.
We provide some examples of used text labels in our method as follows:
\begin{tcolorbox}[
    colframe=black,    
    colback=white,     
    sharp corners,     
    width=\linewidth,   
    boxrule=0.2mm,      
]
\scriptsize
\texttt{walking}

\texttt{sitting with feet crossed}

\texttt{standing with knees bent}

\texttt{throw something with the right hand and walking}

\texttt{pose with bent leg and transition and walking}

...
\end{tcolorbox}
\noindent 
More details about the VLM prompts and label verification can be found in Supplementary Materials.


\noindent\textbf{Video segment processing.} 
After the text label for motion in each image is obtained, we aim to align the 
the projected motion and text features within the shared space of MotionCLIP. 
To process video segments for training, we divide the video with {a stride of 1},
and only keep the segments where all images share the same text label. This is to avoid misalignment caused by mixed motions as much as possible.
To further improve alignment, 
we adopt weighted sampling and prioritize those segments whose 
motion predictions deviate from the true semantics. 
Specifically, the sampling weight is defined as 1 minus cosine similarity between predicted motion and its text label in MotionCLIP.


\noindent\textbf{Motion-text feature alignment.} Given a batch of video segments, we pass them through our 3D human pose estimation model $f^{\text{HPE}}$, to obtain pose motion predictions. 
For one single video segment $\mathbf{X}_{i:i+T}$, we first retrieve the exemplar motion segment in the motion dictionary~\cite{BABEL} based on the text label $\rvs_i$. Next, we change pose prediction $\boldsymbol{\hat{\theta}}_{i:i+T}$ outputted from $f^{\text{HPE}}$ to 6D format $\boldsymbol{\hat{\vp}}_{i:i+T}$ and replace the global rotation with that of the retrieved motion segment to obtain $\boldsymbol{\hat{\vp}}'_{i:i+T}$. 
This focuses on updating only the local poses that are more text-relevant while avoiding the optimization towards specific global orientations present in the motion training data~\cite{BABEL}. 
As shown in Fig.~\ref{fig:architecture} (left), the $\boldsymbol{\hat{\vp}}'_{i:i+T}$ is forwarded into the motion encoder $f^{\text{MOTION}}$, while text label $\rvs_i$ is forwarded into the CLIP model $f^{\text{CLIP}}$.
Semantics-aware motion prior is applied by motion-text feature alignment on the shared embedding space:
\begin{equation}
    \gL_{\textnormal{align}}= 1-\text{sim}(\rvs_i,\boldsymbol{\hat{\vp}}'_{i:i+T}),
    ~\label{Eq:2d_loss}
\end{equation}
where $\text{sim}(\cdot,\cdot)$ is introduced in Eq.~\ref{Eq:motionclip_loss}.

The gradient is backpropagated {to update the parameters of the pose estimation model $f^{\text{HPE}}$}.
Both the motion encoder and CLIP model are frozen in our framework so as not to affect the already structured feature space.  
With the motion feature better aligned with the text feature, the motion prediction can be performed based on a smaller semantic space, thus reducing the 2D-to-3D ambiguity.


\subsection{2D Pose Update}\label{sec:label_update}

In our work, we leverage 
a 2D estimator
~\cite{openpose} to provide 2D pose estimates $\bar{\bm{J}}$ and its corresponding visibility $\vv$, typically setting a threshold to exclude uncertain estimates~\cite{dynaboa,cycleadapt}. Therefore, for the keypoint $k$ on the test frame,
the projection loss can be applied with L1 distance as:
\begin{equation}
    \gL_{\textnormal{2D}}= \sum_k \vv_k\cdot||\hat{\bm{J}}_k-\bar{\bm{J}}_k||_1,\ \vv_k=\mathbbm{1}{\{\bar{\bm{J}}_k\neq\emptyset\}}.
    ~\label{Eq:align_loss}
\end{equation}
Note that the projection loss is held for each frame in the video segment and the frame subscript is omitted for clarity.

Clearly, the 2D projection loss mentioned above cannot provide supervision for missing 2D keypoints, specifically $\vv_k=0$.  
However, the semantic meaning of human motions can be highly evident even under occlusion or truncation by inferring from the video context. 
Therefore, we want to utilize 
{the semantics-enhanced motion prediction}
to suggest possible 2D positions for model training.
To this end, we manage to fill in the missing 2D keypoints with text-aligned {motion prediction} if the motion feature has a high cosine similarity with its text feature:
\begin{equation}
\begin{aligned}
\bar{\bm{J}}^{e+1}_k=\hat{\bm{J}}^e_k, \text{~~if }  \bar{\bm{J}}^e_k=\emptyset \text{ and } \text{sim}
    {(\rvs,\boldsymbol{\hat{\vp}}'})> \sigma,   
\end{aligned}
\label{Eq:2d_fill}
\end{equation}
where $e$ subscript represents the epoch number. 
The $k$-th joint of the 2D prediction $\hat{\bm{J}}^e_k$ is assigned to the estimated 2D $\bar{\bm{J}}^{e+1}_k$ in the next epoch if the motion aligns with the text and the estimated keypoint is excluded, \ie $\bar{\bm{J}}^e_k=\emptyset$.

However, the individually predicted 2D poses can still be noisy. 
The text-aligned motion predictions are based on each video segment and require model feedback to update temporally consistent predictions by incorporating all suggested 2D positions.
To this end, we aim to update the estimated 2D alongside the predicted 2D keypoint, using an EMA (Exponential Moving Average) update
with update factor $\alpha$, given by:
\begin{equation}
    \bar{\bm{J}}^{e+1}_k= \alpha*\bar{\bm{J}}^e_k+(1-\alpha)*\hat{\bm{J}}^e_k. 
    ~\label{Eq:2d_ema}
\end{equation}
The EMA update is performed at the end of each epoch, and the updated pose
{$\bar{\bm{J}}^{e+1}_k$ will be used as supervision for prediction $\hat{\bm{J}}^{e+1}_k$ following Eq.~\ref{Eq:align_loss} in the next epoch $e+1$.}
Here, we not only update the fill-in 2D but also the original estimated 2D, as both can boost the performance.


Finally, we can summarize the updates of all cases as:
\begin{equation}
\small
    \bar{\bm{J}}^{e+1}_k= 
    \begin{cases}
        \alpha*\bar{\bm{J}}^{e}_k+(1-\alpha)*\hat{\bm{J}}^e_k, &\text{if } \bar{\bm{J}}^e_k\neq\emptyset, \\
        \hat{\bm{J}}^{e}_k, & \text{if } \bar{\bm{J}}^e_k=\emptyset\,\,\, \& \,\,\,\text{sim} > \sigma,\\
        \emptyset, & \text{if } \bar{\bm{J}}^e_k=\emptyset\,\,\, \& \,\,\,\text{sim} \leq \sigma.
    \end{cases}
    \label{eq:2d_ema_sum}
\end{equation}
Specifically, in epoch 1, we rely on the 2D pose estimates with uncertain values excluded. By the end of epoch 1, EMA update is applied to all estimates with predicted 2D, followed by a fill-in for some missing 2D estimates.
This process is repeated at the end of each subsequent epoch.
We provide an illustration of 2D pose update in Fig.~\ref{fig:architecture} (right).

\subsection{Overall Training}
For each new video, we fine-tune the pre-trained model over several epochs for adaptation. The overall framework is shown in Fig.~\ref{fig:architecture}. The 2D pose updates (Eq.~\ref{eq:2d_ema_sum}) and model fine-tuning are conducted separately, with the 2D poses remaining fixed during the fine-tuning process. And in each epoch, similar with~\cite{cycleadapt}, we have a smoothing loss with the motion denoise module:
\begin{equation}
    \gL_{\textnormal{smooth}}=||\boldsymbol{\hat{\beta}}-\boldsymbol{\bar{\beta}}||_1+ ||\boldsymbol{\hat{\theta}}-\boldsymbol{\bar{\theta}}||_1,
    ~\label{Eq:smooth_loss}
\end{equation}
where the $\boldsymbol{\bar{\beta}}$ comes from the motion denoise module, and $\boldsymbol{\bar{\theta}}$ is the averaged shape estimation with a sliding window operation. Together with motion-text feature alignment loss, 2D projection loss, and loss weight $\lambda$, our training objective in each epoch can be summarized as:
\begin{equation}
    \gL_{\textnormal{overall}}=\lambda_1\gL_{\textnormal{2D}}+\lambda_2\gL_{\textnormal{align}}+\gL_{\textnormal{smooth}}.
    ~\label{Eq:overall_loss}
\end{equation}
