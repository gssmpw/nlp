\section{Experiments}
\label{sec:experiment}

\begin{table*}[!tbp]
\caption{Test-time adaptation comparison results on 3DPW~\cite{3dpw} and 3DHP~\cite{3dhp} based on HMR network pre-trained on Human3.6M~\cite{human36m}. Our method achieves the best performance across all metrics on 3DPW~\cite{3dpw} and 3DHP~\cite{3dhp} datasets, surpassing state-of-the-art methods.}
\label{tab:tta_res} 
\centering
\scalebox{0.95}{
\begin{tabular}{l|ccc|cc}
\toprule
 \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{3DPW}} & \multicolumn{2}{c}{\textbf{3DHP}}  \\
 & \textbf{MPJPE}$\downarrow$ & \textbf{PA-MPJPE}$\downarrow$ & \textbf{MPVPE}$\downarrow$ & \textbf{MPJPE}$\downarrow$ & \textbf{PA-MPJPE}$\downarrow$  \\
 \midrule
 Pre-trained HMR & 230.3 & 123.4 & 253.4 & 218.5 & 119.6 \\
 \midrule
  BOA~\cite{boa} & 137.6 & 76.2 & 171.8 & 135.3  & 88.5 
   \\
  DynaBOA~\cite{dynaboa} & 135.1 & 73.0 & 168.2 &  130.7 & 81.8  
   \\
  DAPA~\cite{DAPA} & 108.0 & 67.5 & 129.8 & - & - \\
  CycleAdapt~\cite{cycleadapt} &  87.7 & 53.8 & 105.7 & 110.3 & 74.4 \\
  Ours & \textbf{76.4} & \textbf{47.2} & \textbf{94.0} & \textbf{101.3} & \textbf{65.1} \\
\bottomrule
\end{tabular}}
\vspace{-0.1in}
\end{table*}



\begin{table}[!tp]
\caption{Test-time adaptation comparison results on EgoBody~\cite{Zhang:ECCV:2022} and 3DPW~\cite{3dpw} based on HMR2.0 network~\cite{hmr2}. Our method achieves better performance than CycleAdapt~\cite{cycleadapt}.}
\label{tab:tta_hmr2} 
\centering
\scalebox{0.8}{
\begin{tabular}{l|cc|cc}
\toprule
 \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{EgoBody}} & \multicolumn{2}{c}{\textbf{3DPW}}  \\
 & \textbf{MPJPE}$\downarrow$ & \textbf{PA-MPJPE}$\downarrow$ & \textbf{MPJPE}$\downarrow$ & \textbf{PA-MPJPE}$\downarrow$  \\
 \midrule
 HMR2.0 & 118.2 & 69.6
 &  80.2 & 53.3
 \\
 \midrule
  CycleAdapt & 108.7 & 60.4 &  73.7	& 46.8
 \\
  Ours & \textbf{104.6} & \textbf{56.1} & \textbf{71.2} & \textbf{44.3} \\
\bottomrule
\end{tabular}} 
\vspace{-0.1in}
\end{table}

\subsection{Setup}
\noindent\textbf{Datasets.}
We follow the adaptation tasks from previous work~\cite{iso, cycleadapt}, using~\textbf{Human3.6M}~\cite{human36m} as the labeled training dataset and~\textbf{3DPW}~\cite{3dpw} and~\textbf{3DHP}~\cite{3dhp} as the unlabeled test datasets.
Human3.6M is a widely used indoor dataset comprising 3.6 million images annotated with 2D and 3D labels. 
Despite its extensive pose diversity, the HMR network~\cite{hmrnet} pre-trained on this dataset often struggle to generalize to in-the-wild settings due to its limited variation in appearance and environmental factors. 
The 3DPW test set comprises 37 videos and presents a significant challenge as it is an outdoor dataset featuring diverse scenarios, including subject occlusion and varied environments. 
The 3DHP test set includes 6 videos with both indoor and outdoor scenarios, and it presents an additional challenge by featuring some poses not seen in Human3.6M.
Furthermore, we validate our method on a egocentric dataset EgoBody~\cite{Zhang:ECCV:2022}, which presents severe body truncation with missing 2D evidence.

\noindent\textbf{Evaluation metrics.}
We report three evaluation metrics: Mean Per Joint Position Error (MPJPE) measuring the Euclidean distance between the predicted joints and the ground truth joints with aligned root joint; Procrustes-Aligned MPJPE (PA-MPJPE) measuring MPJPE after aligning predictions with ground truth in 3D space; Mean Per Vertex Position Error (MPVPE) measuring the average error across all vertices. All results are reported in millimeters. The results on 3DPW contain all evaluation metrics. Since 3DHP does not provide ground truth SMPL parameters, we report only MPJPE and PA-MPJPE for this dataset.

\subsection{Implementation Details}
At the start of test-time adaptation for each test video, the model parameters are initialized with the pre-trained values, following~\cite{cycleadapt}. We employed the Adam optimizer~\cite{adam} with parameters set to $beta1$ = 0.5, $beta2$ = 0.9, and a learning rate of 5.0e-5. A cosine scheduler is used with a minimum learning rate of 1.0e-6. The input images are resized to 224$\times$224, and the frame number of each video segment is 60. We use a batch size of 4 and the total training epoch is 6. The hyperparameters are $\lambda_1=0.1$, $\lambda_2=0.2$, $\sigma=0.75$, $\alpha=0.9$. We use Openpose~\cite{openpose} to provide 2D poses. The comparison methods include state-of-the-art
test-time adaptation methods: BOA~\cite{boa}, DynaBOA~\cite{dynaboa}, DAPA~\cite{DAPA} and CycleAdapt~\cite{cycleadapt}.

\begin{table*}[!tbp]
\caption{Comparison of semantics-incorporated techniques on the 3DPW dataset~\cite{3dpw} based on HMR network. Building on CycleAdapt~\cite{cycleadapt}, we design two methods to enhance pose semantics with generated motion sequences from~\cite{motiongpt}. 
``\textit{downtown\_rampAndStairs}'' and ``\textit{downtown\_bar}'' highlight different challenges. The former involves depth ambiguity when viewing people ascending stairs from behind, while the latter includes obstacles in a low-lighting bar environment.
Our method consistently improves across videos while others do not.}
\label{tab:motion_prior} 
\centering
\scalebox{0.83}{
\begin{tabular}{l|cc|cc|cc}
\toprule
 \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textit{downtown\_rampAndStairs}} & \multicolumn{2}{c|}{\textit{downtown\_bar}} & \multicolumn{2}{c}{\textbf{All videos}} \\
 & \textbf{MPJPE}$\downarrow$ & \textbf{PA-MPJPE}$\downarrow$ & \textbf{MPJPE}$\downarrow$ & \textbf{PA-MPJPE}$\downarrow$ & \textbf{MPJPE}$\downarrow$ & \textbf{PA-MPJPE}$\downarrow$ \\
 \midrule
 CycleAdapt~\cite{cycleadapt} &  94.5 & 60.4
 & 97.3	 & 65.0  &  87.7 & 53.8  \\
 \midrule
        Motion discriminator~\cite{vibe} & 85.7   & 52.9  & 117.9	& 72.2	& 115.0 & 69.1  \\
        Unpaired local poses~\cite{motiongpt}  & 82.3  & 50.6 &  102.6  & 71.7  & 88.2  & 54.5  \\   
        \textbf{Ours} & \textbf{77.8} & \textbf{49.2}  &\textbf{83.8} &\textbf{58.2}  & \textbf{76.4} & \textbf{47.2} \\
    \bottomrule
\end{tabular}}
 \vspace{-0.2in}
\end{table*}


\subsection{Quantitative Results}

 
As shown in Tab.~\ref{tab:tta_res}, the pre-trained HMR network performs poorly on the 3DPW dataset, highlighting the limited generalization ability of pre-training on low-variability scenarios. While previous methods (BOA~\cite{boa}, DynaBOA~\cite{dynaboa}, DAPA~\cite{DAPA}) show improvements, they heavily rely on ground truth 2D labels and are prone to failure with noisy 2D inputs. In contrast, CycleAdapt~\cite{cycleadapt} employs cyclic adaptation between the motion denoise module and the HMR network, effectively mitigating the impact of noisy 2D poses. Our method outperforms CycleAdapt by \textbf{12.9\%} in MPJPE and \textbf{12.3\%} in PA-MPJPE, demonstrating the effectiveness of inducing semantics-aware motion prior to improve 3D predictions. 
Similar promising results are observed on 3DHP. 
Specifically,
our method significantly surpasses others, achieving improvements of \textbf{8.2\%} of MPJPE and \textbf{12.5\%} of PA-MPJPE over CycleAdapt. 

We further validate our method by upgrading the backbone to the state-of-the-art HMR2.0~\cite{hmr2}, which is trained with multiple in-the-wild datasets, on the more challenging Egobody dataset.
As shown in Tab.~\ref{tab:tta_hmr2}, our method continues to outperform CycleAdapt with a stronger backbone.
This indicates that the loss of semantic information still happens in the state-of-the-art backbone.



\subsection{Analysis Experiments}

\textbf{Ablations on the method components.}
As shown in Tab.~\ref{tab:ablation_study}, we conduct an ablation study on 3DPW~\cite{3dpw} to evaluate the impact of each component in our method. Firstly, by adding semantics-aware motion prior, the performance can be improved from MPJPE 87.7mm to 79.3mm, which verifies the effectiveness of integrating semantic information in 3D predictions. 
We then introduce EMA and fill-in incrementally, progressively refining 2D poses and reinforcing the guidance of the motion prior. Each addition can further enhance model performance. 
The above experiments clearly validate the contribution of each component to the overall performance. 

\begin{table}[t]
    \caption{Ablation study of method components on the 3DPW dataset~\cite{3dpw} based on HMR network, demonstrating the incremental improvements achieved by each component.}
    \label{tab:ablation_study} 
    \centering
    \scalebox{0.82}{
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Alignment}   &  \textbf{2D EMA} &\textbf{2D Fill-in} & \textbf{MPJPE$\downarrow$} & \textbf{PA-MPJPE$\downarrow$} \\
        \midrule
        \xmark & \xmark & \xmark & 87.7	& 53.8	\\
        \cmark & \xmark & \xmark & 79.3	& 49.6	\\ 
        \cmark & \cmark & \xmark   & 78.0   & 48.7  \\
        \cmark & \cmark & \cmark  & \textbf{76.4} & \textbf{47.2}   \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.1in}
\end{table}

 


\noindent\textbf{Analysis on semantics-incorporated techniques.} 
To the best of our knowledge, no work has explicitly used semantic information to improve 3D motion.  
Therefore, we design two other methods for semantics enhancement 
based on CycleAdapt~\cite{cycleadapt} to compare the effectiveness with our method. 
Specifically, we use~\cite{motiongpt} to obtain additional motion sequences with the same semantics as the video segment, and enforce the predicted motions to be aligned with the generated motions.

 
We implement the first comparison using motion discriminator~\cite{vibe}.
It guides motion prediction by distinguishing between predicted and generated motions with the same semantic meaning.
As shown in Tab.~\ref{tab:motion_prior}, even though some cases show improvement, the overall performance is worse than CycleAdapt.  
One reason is the difficulty of stabilizing adversarial training; the other is that the discriminator, pre-trained to refine unrealistic motion rather than semantic differentiation.

\begin{figure}[t]
    \centering
        \includegraphics[width=0.97\linewidth]{Figs/distribution.pdf}
    \caption{The improvement distribution on 3DPW dataset based on HMR network. We only show the top 7 semantics for brevity. The improved motions include both common and rare semantics found in action datasets.}  
    \label{fig:improve_distribution}
    \vspace{-0.2in}
\end{figure}

\begin{table}[!thp]
    \caption{Comparison of fill-in threshold influence on model performance (MPJPE) on the 3DPW dataset.  
    Balancing 2D pose quantity and quality can boost model performance.}
    \label{tab:threshold_2d}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{l|c|c|c|c|c|c}
        \toprule
        \textbf{Threshold}  & 0.55 & 0.60 & 0.65 & 0.70 & 0.75 & 0.80 \\
        \midrule
        MPJPE & 78.1 & 76.9 & 77.2 & 77.0 &\textbf{76.4} & 77.2	\\
    
        \bottomrule
    \end{tabular}
    
    } 
    \vspace{-0.2in}
\end{table}

We next implement the second comparison by using the local poses from the generated motion to supervise the predicted motion.
Note that the local poses do not align with the video segment but share the same semantics.
To our surprise, it shows effectiveness in some cases and achieves overall performance comparable to CycleAdapt.  
This may be because strong unpaired local pose supervision pulls predictions into a specific semantic space, while 2D evidence helps prevent misalignment.
However, in the case of ``\textit{downtown\_bar}'', many 2D evidences are unavailable due to occlusion or poor lighting.
Accordingly, the method predicts results identical to the generated motion, ignoring the actual motion in the test video. 

Compared to the above two methods, our method avoids the need for additional motion data generation while simultaneously delivering consistent improvements.  



\begin{figure*}[!t]
    \centering
        \includegraphics[width=0.97\linewidth]{Figs/visualization.pdf} 
    \caption{Qualitative comparison on one video sequence from 3DPW dataset~\cite{3dpw} with text label ``Walking''. We first show the necessity of 2D pose updates to preserve motion semantics. 
    We use \textcolor{green}{green} and \textcolor{red}{red} squares to highlight the transition from walking to standing poses and demonstrate that this issue is alleviated with 2D pose updates (Ours).
    Next, we show that our method predicts motion more consistent with actual walking sequences compared to  
    CycleAdapt~\cite{cycleadapt}. } 
    \label{fig:qualitative} 
     \vspace{-0.1in}
\end{figure*}

\noindent\textbf{Improvement distribution.} 
In Fig.~\ref{fig:improve_distribution}, we illustrate the average MPJPE improvement for the top 7 semantics. Our method enhances both common and less frequently observed motions in human action datasets, such as bending down and standing with bent knees.

\noindent\textbf{Analysis on 2D pose update.}
Tab.~\ref{tab:threshold_2d} illustrates the impact of the threshold on overall performance. 
We find out that using 0.75 threshold adds only 36\% more 2D poses, yet the high quality (PCK=0.58) helps achieve 76.4mm MPJPE.
Even with a moderate quality (PCK=0.53), adequate new poses can still positively impact the model when the threshold is 0.60. 
Overall, the experiment demonstrates that balancing the quality and quantity of 2D poses can enhance model performance.

\subsection{Qualitative Results}
We perform a qualitative comparison to visually assess the effectiveness of our method. 
In Fig.~\ref{fig:qualitative}, 
we first illustrate that without 2D pose updates, the model predictions for the truncated part revert to average poses. Specifically, in Epoch 2, frames highlighted with a \textcolor{green}{green} square show some walking poses. However, by epoch 4, these walking poses are back to standing poses (\textcolor{red}{red} square). With 2D pose updates (``Ours''),  
more frames accurately reflect the intended action. 
This result verifies the necessity of using 2D pose update to preserve motion semantics.

Furthermore, we demonstrate that CycleAdapt~\cite{cycleadapt} often predicts standing poses where 2D pose estimates are missing.  Our method consistently estimates motions closer to the ground truth, verifying the feasibility of using semantic information to refine predicted motions. 
More visualizations can be found in Supplementary Materials.