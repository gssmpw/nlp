\section{Related Work}
\label{sec:formatting}
XAI methods for computer vision can be categorized by two axes: local (heatmap-based) vs. global (concept-based) approaches, and \textit{post-hoc} vs. \textit{ante-hoc} explanations.
In this section, we overview existing methods along these lines.
\\
\noindent \textbf{Heatmap-based explainability.}
This refers to a family of \textit{post-hoc} explainability techniques, often called attribution methods, that visualize the parts of the input image that contribute most to the model's output.
\textit{Gradient-based methods} generate explainable heatmaps by backpropagating gradients with respect to the input of each layer.
Some of these methods, such as FullGrad~\cite{srinivas2019full}, are class-agnostic as they produce roughly identical results regardless of the output class \cite{sundararajan2017axiomatic, smilkov2017smoothgrad}, while others, such as GradCAM~\cite{selvaraju2017grad}, generate class-dependent heatmaps~\cite{simonyan2013deep, chattopadhay2018grad}. This property is essential when the true class is ambiguous.
While widely used, their main drawback is high sensitivity to gradient noise, which may render their outcomes impractical~\cite{adebayo2018sanity}. To address this issue, some Class Activation Maps (CAM) methods~\cite{zhou2016learning}, such as ScoreCAM~\cite{wang2020score}, produce gradient-free explanation maps.

\textit{Attribution propagation methods} decompose the output of a model into the contributions of its layers by propagating “relevance” in a recursive manner, without exclusively relying on gradients. Common attribution propagation methods, such as Layer-wise Relevance Propagation (LRP)~\cite{binder2016layer}, are primarily applicable to Convolutional Neural Networks (CNNs)~\cite{montavon2017explaining, shrikumar2017learning, zhang2018top}. Later approaches have been adapted to accommodate vision transformers (ViTs)~\cite{dosovitskiy2020image}, exploiting their built-in self-attention mechanism~\cite{voita2019analyzing, abnar2020quantifying, chefer2021transformer, radford2021CLIP}.
% We note that, unlike our SALF-CBM, both gradient-based and attribution propagation methods do not provide concept-based explanations. Additionally, since these are \textit{post-hoc} techniques, they do not enable test-time user intervention.
We note that, unlike our SALF-CBM, both gradient-based and attribution propagation methods do not provide concept-based explanations. Additionally, since these are \textit{post-hoc} techniques, they do not enable test-time user intervention.

%-------------------------------------------------------------------------
\noindent \textbf{Concept-based explainability.}
An alternative way of explaining vision models is by using human-interpretable concepts. Various methods provide such explanations in a \textit{post-hoc} manner.
For example, Testing Concept Activation Vectors (TCAV)~\cite{kim2018interpretability} measures the importance of user-defined concepts to the model's prediction by training a linear classifier to distinguish between concepts in their activation space. However, this requires labeling images with their corresponding concepts in advance.
ACE~\cite{ghorbani2019towards} extends this idea by applying multi-resolution segmentation to images from the same class, followed by clustering similar segments into concepts to compute their TCAV scores. Similarly, Invertible Concept Embeddings (ICE)~\cite{zhang2021invertible} and Concept Recursive Activation Factorization (CRAFT)~\cite{fel2023craft} provide concept-based explanations using matrix factorization of feature maps. CRAFT also generates attribution maps that localize concepts in the input image.
These methods, however, are mostly applicable to CNN architectures~\cite{ghorbani2019towards}, which use non-negative activations~\cite{zhang2021invertible, fel2023craft}, and therefore cannot be directly applied to other types of models. Additionally, since they provide \textit{post-hoc} explanations, they do not enable test-time user intervention.

In contrast, \textit{Concept-Bottleneck Models (CBMs)} is a family of \textit{ante-hoc} interpretable models whose explainability mechanism is an integral part of the model itself. CBMs operate by introducing a concept-bottleneck layer into pre-trained models, before the final prediction layer. The goal of this bottleneck is to project features into an interpretable concept space, where each neuron corresponds to a single concept. 
Unlike \textit{post-hoc} methods, the output of CBMs is directly based on interpretable concepts, making them easily explainable and allowing user intervention by modifying concept neurons activations. 
In the original CBM work~\cite{ koh2020concept}, the concept bottleneck layer was trained using manual concept annotations, limiting its ability to scale to large datasets. 
Recently, Post-Hoc CBM (P-CBM)~\cite{yuksekgonul2022post} and Label-Free CBM (LF-CBM)~\cite{oikarinen2023label} addressed this issue by leveraging CLIP to assign concept scores for training images, thus not requiring concept annotations. LF-CBM also presented an automatic process for creating a list of task-relevant concepts using GPT-3. While showing good interpretability results, both P-CBM and LF-CBM present a performance drop on the final classification task compared to the original (non-CBM) model. Additionally, unlike our SALF-CBM, these methods are limited to global concept explanations, and are unable to localize these concepts within the image.