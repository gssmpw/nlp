\section{Conclusions}
In this work, we presented SALF-CBM, a novel framework for transforming any vision neural network into an explainable model that provides both concept-based and spatial explanations for its predictions.
We showed that SALF-CBM enhances model interpretability without compromising performance, outperforming both existing CBMs and the original model across several classification tasks. We demonstrated that it produces high-quality spatial explanations, achieving better zero-shot segmentation results compared to widely used heatmap-based explainability methods.

Additionally, we introduced interactive capabilities for model exploration and debugging, demonstrating their effectiveness in diagnosing and correcting model errors. We believe that such features are particularly valuable for high-stakes applications like medical imaging and autonomous driving. By providing expert practitioners with intuitive tools to understand and adjust model decisions, our approach can boost confidence and support safer deployment in these critical fields. Looking ahead, as new VLMs are developed across various domains, our findings could help inform the design of more powerful interpretability tools for a broad spectrum of AI applications. We plan to explore these directions in future work.
