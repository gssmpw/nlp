\section{Method}
\begin{figure*}[h!]
  \centering
  \begin{subfigure}{0.63\linewidth}
  \includegraphics[height = 5.5cm]{figs/1_method/method_overview_v10.pdf}
  \caption{Training process scheme.}
    \label{fig:method_overview-a}
  \end{subfigure}
  \hspace{-0.9cm}
  \begin{subfigure}{0.37\linewidth}
    \includegraphics[height = 5.5cm]{figs/1_method/visual_prompting_v5.pdf}
    \caption{Local image-concept similarities block.}
    \label{fig:method_overview-b}
  \end{subfigure}
  \caption{\textbf{SALF-CBM training:} \textbf{(a)}~Given a pre-trained backbone model, we: (i)~Generate task-relevant concepts; (ii)~Describe training images using local image-concept similarities; (iii)~Train a spatially-aware concept bottleneck to project features into interpretable concept maps; (iv)~Train a sparse classification layer over these maps. \textbf{(b)} Local image-concept similarities computation using visual prompting.}
  \label{fig:method_overview}
\end{figure*}
Given a pre-trained backbone model, we transform it into an explainable SALF-CBM as illustrated in Figure~\ref{fig:method_overview-a}: \\
\textbf{Step~1:} Automatically generate a list of task-relevant visual concepts; \textbf{Step~2:} Using CLIP, compute a spatial concept similarity matrix that quantifies what concepts appear at different locations in the training images; \textbf{Step~3:} Train a spatially-aware Concept Bottleneck Layer (CBL) that projects the backbone's “black-box” features into interpretable concept maps. \textbf{Step~4:} Train a sparse linear layer over the pooled concept maps to obtain the model's final prediction. We describe each step in the following sections.

%-------------------------------------------------------------------------

\subsection{Concept list generation}
Let $\mathcal{X}$ denote an image classification dataset with $N$ training images $\{x_1, \ldots, x_N\}$ and $L$ possible classes. We aim to generate a list of visual concepts $\mathcal{T}$ that is most relevant to the target classes, without relying on human experts. For this purpose, we follow the automatic procedure described in~\cite{oikarinen2023label}. First, an initial concept list is obtained by prompting GPT as follows: “List the most important features for recognizing something as a \{class\}”; “List the things most commonly seen around a \{class\}”; and “Give superclasses for the word \{class\}”, for each class in the dataset. 
Concepts that are too long, too similar to one of the classes or to another concept, or do not appear in the training data - are then discarded. The resulting filtered list of $M$ concepts is denoted by $\mathcal{T}=\{t_1, \ldots, t_M\}$. See ~\cite{oikarinen2023label} for full details.  

%-------------------------------------------------------------------------

% \subsection{Spatial concept similarity matrix}
\subsection{Local image-concept similarities}
\label{sec:image-concept-similarities}
Vision-language models such as CLIP have been widely used for obtaining \textit{global} image descriptions, as in non-spatial CBMs~\cite{yuksekgonul2022post, oikarinen2023label}.
% In our approach, however, we aim to \textit{locally} describe different image regions in terms of our visual concepts.
Here, we aim to extend this approach to \textit{locally} describe different image regions using visual concepts.
Inspired by~\cite{shtedritski2023RedCircle}, we leverage CLIP's visual prompting property by drawing a red circle around specific image regions, which enables CLIP to focus on these areas while preserving global context. We apply this property to our training set as illustrated in Figure~\ref{fig:method_overview-b}.
Formally, let $x_n \in \mathcal{X}$ denote a training image with spatial dimensions ${H \times W}$. We create a uniform grid of $\Tilde{H}\times \Tilde{W}$ locations in the image with integer strides ${d_H}$ and ${d_W}$, i.e.,
% $\Tilde{H}= \lfloor \frac{H}{d_H} \rfloor$ and $\Tilde{W}= \lfloor \frac{W}{d_W} \rfloor$.
$d_H = \lfloor \frac{H}{\Tilde{H} - 1} \rfloor$ and $d_W = \lfloor \frac{W}{\Tilde{W} - 1} \rfloor$.
For each $x_n$, a set of $\Tilde{H} \cdot \Tilde{W}$ augmented images is then obtained by drawing a red circle with radius $r$ around each location in the grid. We denote by $x_n^{(h,w)}$ the image $x_n$ with a red circle located at ${(h,w)}$. Next, we compute a \textit{local similarity score} between a visual concept $t_m\in\mathcal{T}$ and the image at location ${(h,w)}$ as follows: $P[n,m,h,w]=\frac{I_n^{(h,w)} \cdot T_m}{\lVert I_n^{(h,w)} \rVert \lVert T_m \rVert}$, where $I_n^{(h,w)} = E_I(x_n^{(h,w)})$ and $T_m = E_T(t_m)$ denote the CLIP embeddings of the augmented image and the concept, respectively.
As demonstrated in~\cite{shtedritski2023RedCircle}, this score measures how well the concept $t_m$ describes the image region highlighted by the red circle. By computing this score for every spatial location $(h,w)$ in the grid, we obtain a concept similarity map for the entire image $x_n$, as shown in Figure~\ref{fig:method_overview-b}. 
A complete \textit{spatial concept similarity matrix} $P$ is then constructed by calculating these local similarity scores for all concepts $t_m\in\mathcal{T}$ and all images $x_n \in \mathcal{X}$ in the training set.
This matrix is computed once, prior to training, and is later used for learning the spatially-aware concept bottleneck layer.
We note that the grid resolution $\Tilde{H}\times \Tilde{W}$ and the circle radius $r$ are hyper-parameters, where $\Tilde{H}$ and $\Tilde{W}$ control the coarseness of the concept similarity map, and $r$ defines the receptive field around each location. We optimize these hyper-parameters per-dataset (see supplementary).

%-------------------------------------------------------------------------

\subsection{Training the concept-bottleneck layer}
\label{sec:spatial-bottleneck}
We aim to learn a bottleneck layer $g$ that linearly projects “black-box” feature maps $f(x)$ of a pre-trained backbone model into interpretable concept maps. Rather than spatially pooling the backbone's features as in conventional CBMs, we retain their spatial information, and resize them to fit the grid's dimensions ($\Tilde{H}\times \Tilde{W}$) using a bilinear interpolation. We then use a single $1 \times 1$ convolution layer with $M$ output channels to produce the desired concept maps, i.e., $c(x)=g(f(x)) \in \mathbb{R}^{M \times \Tilde{H} \times \Tilde{W}}$.
We denote the full list of concept maps for all training images $x_n \in \mathcal{X}$ by $C[n,m,h,w]=[c(x_1), \ldots ,c(x_N)]$. In order to obtain concept maps that match the image-concept similarities in $P$, we train our bottleneck layer using an extended version of the cubic cosine similarity loss from~\cite{oikarinen2023label} as follows:
\begin{equation}
\mathcal{L}_{CBL} =-\underset{m=1}{\overset{M}{\sum}} \underset{h,w}{\sum} sim\left(q[m,h,w],p[m,h,w]\right)
\end{equation}
where $q[m,h,w]$ denotes $C[:,m,h,w]$, $p[m,h,w]$ denotes $P[:,m,h,w]$ and $sim(\cdot,\cdot)$ denotes the cubic cosine similarity function $sim\left(q,p\right) =\frac{\bar q \cdot \bar p}{\lVert \bar q \rVert \lVert \bar p \rVert}$. Here, $\bar q$ and $\bar p$ are normalized to have zero-mean and raised elementwise to the power of three to emphasize strong concept-image matches.
We note that our spatial bottleneck layer requires the same number of parameters as the fully-connected bottleneck typically used in non-spatial CBMs~\cite{koh2020concept, yuksekgonul2022post, oikarinen2023label}. 
% , i.e., $D \times M$ where $D$ is the dimensionality of the “black-box” features and $M$ is the number of concepts.
Furthermore, our bottleneck layer accommodates both CNN and vision transformer architectures: For CNN backbones, feature maps are used directly as inputs to the bottleneck, while for ViTs, the patch tokens are reshaped back into their original spatial arrangement. See details in the supplementary.
%-------------------------------------------------------------------------

\subsection{Training the final classification layer}
\label{sec:final-layer}
Once the concept bottleneck layer is trained, we spatially pool its output concept maps $c(x)$ to obtain \textit{global} concept activations $c^{\ast}(x)$, each corresponds to a single visual concept $t_m$. We aim to explain each output class of our model using a small set of interpretable concepts. We therefore train a sparse linear layer on top of $c^{\ast}(x)$ to obtain the final classification scores $z=Wc^{\ast}+b$ and the predicted class $\hat{y}=\arg\max (z)$. Here, $W$ and $b$ denote the classification weights and bias term, respectively. This layer is trained in a fully-supervised manner with the following loss function, using the GLM-SAGA optimizer~\cite{wong2021leveraging}:
\begin{equation}
\underset{n=1}{\overset{N}{\sum}}  \mathcal{L}_{ce}\left(Wc^{\ast}(x_n)+b,y_n\right)+\lambda\mathcal{R}(W)
\end{equation}
% where $\mathcal{L}_{ce}$ is the cross-entropy loss, $y_n$ is the class label of training image $x_n$, $\lambda$ is the regularization strength and $\mathcal{R}(W)=(1-\alpha)\frac{1}{2}\lVert W \rVert_F + \alpha\lVert W \rVert_{1,1}$ is the elastic net regularization term, where $\lVert W \rVert_F$ is the Forbenius norm and $\lVert W \rVert_{1,1}$ is the elementwise matrix norm.
where $\mathcal{L}_{ce}$ is the cross-entropy loss, $y_n$ is the class label of training image $x_n$, $\lambda$ is the regularization strength and $\mathcal{R}$ is the elastic net regularization term:
\begin{equation}
\mathcal{R}(W)=(1-\alpha)\frac{1}{2}\lVert W \rVert_F + \alpha\lVert W \rVert_{1,1}
\end{equation}
where $\lVert W \rVert_F$ is the Forbenius norm and $\lVert W \rVert_{1,1}$ is the elementwise matrix norm.

%-------------------------------------------------------------------------

\subsection{Test-time explainability}
\label{sec:test-time-explainations}
% One of the main contributions of SALF-CBMs is their ability to provide model explanations at different levels of granularity, as described below:
% Once trained, SALF-CBM provides explanations as an integral part of the model at different levels of granularity, without requiring external tools or additional computations.
Once trained, SALF-CBM provides explanations at multiple levels of granularity as an integral part of its forward pass, without any external tools or additional computations.
% as demonstrated in Figure~\ref{fig:test-time-explain}.

\noindent \textbf{Global decision rules.}
As described in Section~\ref{sec:final-layer}, SALF-CBM's final prediction is a linear combination of a sparse set of concept activations. Therefore, one can gain an intuitive understanding of the model's decision rules simply by examining which concepts $t_m \in\mathcal{T}$ are connected to a specific class $l \in \{1, \ldots, L\}$ by non-zero weights.
For instance, in Figure~\ref{fig:test-time-explain}, we show Sankey diagrams visualizing the class weights of SALF-CBM trained on ImageNet, for two different classes: “crate” and “toy store”.
\begin{figure*}[h!]
  \centering
  \includegraphics[ width = 17cm ]{figs/1_method/testime_explain_v4.pdf}
  \caption{\textbf{Test-time explainability.} Global decision rules can be inferred by visualizing the sparse class weights. Individual model decisions are explained \textit{globally} and \textit{locally} by the concept contribution scores and their associated spatial heatmaps.}
  \label{fig:test-time-explain}
\end{figure*}

\noindent \textbf{Concept-based explanations (“tell what”)}.
% We aim to explain individual model decisions by evaluating the contribution of a visual concept $t_m \in\mathcal{T}$ to the the model's output $\hat{y}$ on a given test image $x$. This is achieved by computing a contribution score 
% $S(x, m, \hat{y}=l) = W[m,l]c^{\ast}(x)[m],$
% where $m$ is the concept index, $l \in \{1, \ldots, L\}$ is the index of the predicted class and $c^{\ast}(x)[m]$ is the global concept activation, normalized by its mean and standard deviation on the training data.
% Since $W[:,l]$ is sparse, the majority of contribution scores are zero, so the model's prediction can be explained by a small set of $k$ concepts whose absolute contribution scores are the highest. In Figure~\ref{fig:test-time-explain}, we illustrate the top-3 concepts with the highest contribution scores for the “toy store” and “crate” classes.
We aim to explain individual model decisions by identifying the visual concepts that most contributed to the model's output $\hat{y}$ given a test image $x$. This is achieved by computing a contribution score for each concept $t_m \in\mathcal{T}$ as follows:
\begin{equation}
S(x, m, \hat{y}=l) = W[m,l]c^{\ast}(x)[m]
\end{equation}
where $m$ is the concept index, $l \in \{1, \ldots, L\}$ is the index of the predicted class and $c^{\ast}(x)[m]$ is the global concept activation, normalized by its mean and standard deviation on the training data.
Since $W[:,l]$ is sparse, the majority of contribution scores are zero, so the model's prediction can be explained by a small set of $k$ concepts whose absolute contribution scores are the highest. In Figure~\ref{fig:test-time-explain}, we illustrate the top-3 concepts with the highest contribution scores for the “toy store” and “crate” classes.

\noindent \textbf{Spatial explanations (“show where”)}.
In addition to global concept-based explanations, our method produces heatmaps which highlight the location of each concept in the input image. 
Specifically, given the top-$k$ contributing concepts, we upsample their associated concept maps $c(x)[m]$ to the input image dimensions, using a bilinear interpolation.
Examples of heatmaps associated with the most contributing concepts for two different output classes are presented in Figure~\ref{fig:test-time-explain}.
We note that the resolution of the heatmaps can be controlled by adjusting the density of the visual prompting grid, as discussed in Section~\ref{sec:image-concept-similarities}.

%-------------------------------------------------------------------------

\subsection{Model exploration and debugging}
\label{sec:model-exploration}
We introduce two interactive features that allow users to intuitively explore how their model perceives different image regions and debug failure cases.
\\
\noindent \textbf{Explain Anything}.
Inspired by the Segment Anything Model (SAM)~\cite{ma2024segment}, this feature allows users to actively “prompt” SALF-CBM with inputs such as points, bounding boxes, or free-form masks, to explore what visual concepts were recognized in the specified region-of-interest (ROI).
Specifically, given an image $x$, the computed concept maps $c(x)$ (upsampled to the image dimensions) and a user-provided ROI in the form of a binary mask $I$, we compute the aggregated activation of each concept within the ROI: $a(x,m \mid I)=\sum I \odot c(x)[m]$, where $\odot$ represents elementwise multiplication. By presenting the top-$k$ concepts with the strongest aggregated activation, we provide a concise overview of the model’s perception of the ROI.
We note that in addition to user-provided ROIs, our method supports segmentation masks from tools such as SAM to automatically produce objects descriptions.
\\
\noindent \textbf{Local user intervention}.
We enable users to intervene in the model's final prediction by suggesting counterfactual explanations in specific image regions, i.e., \textit{“how would the model's prediction change if concept $\mathcal{A}$ were more/less activated at location $\mathcal{B}$?”}.
Given an image $x$, the concept map $c(x)[m]$ of a specific concept $t_m$ and the predicted class $\hat{y}$, one can locally edit the concept map according to their judgment and understanding of the task, as follows: $c(x)[m]\leftarrow c(x)[m]+\beta I$, where $I$ is a binary mask of the edited region and $\beta$ is a correction factor that can be either positive or negative. By tuning concept activations up or down in specific regions and re-running the final classification layer, one can observe how the model adjusts its prediction $\hat{y}$ based on the revised concept maps.