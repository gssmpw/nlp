\section{Experiments}

We thoroughly evaluate the different components of our method. In section~\ref{section:class_acc}, we test its classification accuracy compared to several baselines, on different large-scale datasets. In section~\ref{section:zs_seg}, we present qualitative and quantitative evaluations of our SALF-CBM's heatmaps in comparison to several other heatmap-based methods. 
In section~\ref{section:neruons_validation}, we validate the concepts learned by SALF-CBM's bottleneck layer by conducting a user study.
In section~\ref{section:model_exploration_ex}, we demonstrate how the proposed Explain Anything and user intervention features are used to debug model errors.
Additional results are provided in the supplementary.
% \textcolor{red}{Additional results are provided in the supplementary, including experiments with a ViT backbone, validation of concept alignment in the bottleneck layer, and qualitative results on different datasets video sequences.}
% Additional results are provided in the supplementary materials, including experiments with a ViT backbone~\ref{supp:vit_classification}, validation of concept alignment in the concept-bottleneck layer~\ref{supp:concept_validation}, qualitative evaluation of explanations across different datasets~\ref{supp:explanations}, and additional visualizations of concept maps for challenging images and video sequences~\ref{supp:heatmaps}.

%-------------------------------------------------------------------------

\subsection{Classification accuracy}
\label{section:class_acc}
\textbf{Experimental setup.}
We test our method on a diverse set of classification datasets: CUB-200 (fine-grained bird-species classification), Places365 (scene recognition) and ImageNet. 
We train a SALF-CBM on each of the three datasets, using a appropriate backbone model to allow fair comparisons with competing CBM methods~\cite{yuksekgonul2022post, oikarinen2023label}: For CUB-200 we use a ResNet-18 pre-trained on CUB-200, and for both ImageNet and Places365 we use a ResNet-50 pre-trained on ImageNet.
For each dataset, we use the same initial concept list and regularization parameters $\alpha$ and $\lambda$ as in~\cite{oikarinen2023label}, resulting in 370 concepts for CUB-200, 2544 concepts for Places365 and 4741 concepts for ImageNet.
For computing local image-concept similarities, we use CLIP ViT-B/16 and a visual prompting grid of $7\times7$ with $r=32$ for all experiments.
Results with different grid parameters and with a ViT backbone are provided in the supplementary.
\\
\textbf{Results.}
Table~\ref{tab:class_results} presents the classification accuracy of our SLAF-CBM compared to several other methods: (1)~the standard pre-trained backbone model with its original classification layer; (2)~the standard backbone model with a sparse classification layer (reported from ~\cite{oikarinen2023label}); (3) post-hoc CBM (P-CBM)~\cite{yuksekgonul2022post}; and (4) Label-Free CBM (LF-CBM)~\cite{oikarinen2023label}.
We note that in P-CBM~\cite{yuksekgonul2022post}, they do not report their results on ImageNet and Places365, and it is unclear how to scale it to those datasets.
For fair comparisons, results with sparse and non-sparse classification layers are shown separately.
We see that when using a sparse final layer, our SALF-CBM outperforms both P-CBM and LF-CBM on all three datasets. Notably, \textbf{our method is the best performing sparse method on the the two larger-scale datasets (Places365 and ImageNet)}, outperforming even the original backbone with a sparse final layer.
To demonstrate the high-limit potential of our method, we assess its performance with a non-sparse final layer. Remarkably, the non-sparse SALF-CBM achieves better classification results than original (non-sparse) model on both ImageNet and Places365, even though its predictions are based on interpretable concepts.

These results indicate that SALF-CBM facilitates model interpretability without compromising performance; in fact, it can outperform the original backbone model when using a comparable final layer (i.e., sparse or non-sparse).
We also note that the performance gap between the sparse and non-sparse SALF-CBMs is relatively small (less that $1\%$ on ImageNet), indicating that our model effectively captures the full span of possible explanations using a sparse set of concepts.

\begin{table}[t!]
\centering
    \begin{tabular}{@{}l@{}c@{}c@{}c@{}c@{}}
\toprule
                         & \multicolumn{1}{l}{}                   & \multicolumn{3}{c}{Dataset}                                                               \\ \cmidrule(l){3-5} 
Model                    & \multicolumn{1}{l}{Sparse} & \multicolumn{1}{l}{CUB-200} & \multicolumn{1}{l}{Places365} & \multicolumn{1}{l}{ImageNet} \\ \midrule
Standard        & Yes                                    & \textbf{75.96\%}                    & 38.46\%                       & \underline{74.35\%}                      \\
P-CBM~\cite{yuksekgonul2022post}                   & Yes                                    & 59.60\%                    & N/A                           & N/A                          \\
LF-CBM~\cite{oikarinen2023label}                  & Yes                                    & 74.31\%                    & \underline{43.68\%}                       & 71.95\%                      \\
\textbf{SALF-CBM} & Yes                                    & \underline{74.35\%}                        & \textbf{46.73\%}                           & \textbf{75.32\%}
\\ \midrule
Standard                 & No                                     & \textbf{76.70\%}                    & 48.56\%                       & 76.13\%                      \\    
\textbf{SALF-CBM} & No & 76.21\% & \textbf{49.38\%} & \textbf{76.26\%}
\\ \bottomrule
\end{tabular}
    \caption{\textbf{Classification accuracy.} Our method outperforms P-CBM and LF-CBM on all three datasets, and is the highest performing model on ImageNet and Places365. Results are shown separately for sparse and non-sparse final layers. Best results are in bold and 2nd-best are underlined.
    % In Appendix~\ref{supp:vit_classification} we present SALF-CBM's classification results with a ViT backbone model.
    }
    \label{tab:class_results}
\end{table}

%-------------------------------------------------------------------------

\subsection{Beyond classification: zero-shot segmentation}
\label{section:zs_seg}
\begin{figure*}[t!]
  \centering
  \includegraphics[ height = 8.2cm ]{figs/2_experiments/segment_vis_v9.pdf}
  \caption{\textbf{Qualitative heatmaps comparison.} Explanation map of each method with respect to the ground-truth class (from top to bottom): “Cheeseburger”, “Bell-cote”, “Monarch butterfly” and “Goose”.
  % Results with a ViT backbone are shown in Appendix~\ref{supp:vit_classification}.
  }
  \label{fig:heatmaps_comparison}
\end{figure*}

\noindent\textbf{Experimental setup.}
 We conduct a quantitative analysis of the heatmaps generated by our method in a zero-shot segmentation task. We follow a standard protocol for evaluating heatmap-based explainability methods~\cite{chefer2021transformer} on ImageNet-segmentation dataset~\cite{guillaumin2014imagenet}, a subset of the ImageNet validation set containing 4,276 images with ground-truth segmentation masks of the class object.
In order for our concept maps to correspond to ImageNet classes, we train a SLAF-CBM with a ResNet-50 backbone on ImageNet, using a concept list of the form “An image of a \{class\}”, where \{class\} refers to each of the ImageNet classes. According to~\cite{chefer2021transformer}, the resulting heatmaps are binarized to obtain a foreground/background segmentation, and evaluated with respect to the ground-truth masks based on three metrics: mean average precision (mAP) score, mean intersection-over-union (mIOU) and pixel accuracy.
\\
\textbf{Results.}
Table~\ref{tab:seg_results} presents the zero-shot segmentation results of our method, compared to several widely-used explainability methods: LRP~\cite{binder2016layer}, integrated gradients (IG)~\cite{sundararajan2017axiomatic}, GradCAM~\cite{selvaraju2017grad}, GradCAM++~\cite{chattopadhay2018grad}, ScoreCAM~\cite{wang2020score}, and FullGrad~\cite{srinivas2019full}. \textbf{Notably, our SALF-CBM achieves the best pixel accuracy and mIOU segmentation scores, and the second best mAP.} Specifically, our method demonstrates significant improvements in pixel accuracy and mIOU (+3.9\% and +2.52\% over the 2nd-best method, respectively), indicating that our heatmaps are consistently better aligned with the ground-truth masks.
In Figure~\ref{fig:heatmaps_comparison}, we present a qualitative comparison to the baseline methods, for different images from the ImageNet validation set. 
% We observe that our method generates heatmaps that accurately captures the class object.
We observe that LRP~\cite{binder2016layer} and IG~\cite{sundararajan2017axiomatic} typically produce noisy results, and struggle to accurately localize the class object.
GradCAM~\cite{selvaraju2017grad}, GradCAM++~\cite{chattopadhay2018grad}, ScoreCAM~\cite{wang2020score} and FullGrad~\cite{srinivas2019full} manage to highlight the target region, but also include unrelated background areas.
Conversely, our method generates heatmaps that accurately captures the class object, thus providing more precise explanations.

\begin{table}[h!]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
                            Method     & Pixel Acc. ↑   & mIoU ↑         & mAP ↑          \\ \midrule
LRP~\cite{binder2016layer} & 69.52\%          & 36.85\%          & 69.95\%          \\
IG~\cite{sundararajan2017axiomatic}  & 68.49\%          & 46.59\%          & 73.46\%          \\
GradCAM~\cite{selvaraju2017grad}        & 71.34\%          & 53.34\%          & 83.88\%          \\
GradCAM++~\cite{chattopadhay2018grad} & 71.31\%          & 53.56\%          & 83.93\%          \\
ScoreCAM~\cite{wang2020score}             & 69.56\%          & 51.44\%          & 81.78\%          \\
FullGrad~\cite{srinivas2019full}               & \underline{73.04\%} & \underline{55.78\%} & \textbf{88.35\%}    \\ \midrule
\textbf{SALF-CBM}         &  \textbf{76.94\%}   & \textbf{58.30\%}   & {\underline{85.31\%}} \\ \bottomrule
\end{tabular}
\caption{\textbf{Zero-shot segmentation results.} Our SALF-CBM achieves the highest mIoU and pixel accuracy, and the second highest mAP. Best results are in bold, 2nd-best are underlined.}
    \label{tab:seg_results}
\end{table}

%-------------------------------------------------------------------------

\subsection{Bottleneck interpretability validation}
\label{section:neruons_validation}
% \textcolor{red}{We conduct a user study to validate that the concepts learned by SALF-CBM’s bottleneck neurons in-fact correspond to their designated target concepts.
% \\
% \textbf{Experimental setup.} We follow a similar protocol to~\cite{rao2024discover} and evaluate global concept neurons~$c^*$ from SLAF-CBM's bottleneck layer, compared to output neurons of the baseline backbone model (i.e., the same ResNet-50 backbone pre-trained on ImageNet). As in~\cite{rao2024discover}, we first assign the baseline model neurons with concept labels using CLIP-Dissect~\cite{oikarinen2022clip-dissect}. Then, both SALF-CBM's and the baseline's neurons are ranked according to their interpretability scores, using CLIP-Dissect's soft-WPMI metric, and divided into two groups: the top 30\% interpretable neurons, and the remaining 70\%. From each group, we randomly sample 10 neurons, resulting in 20 evaluated neurons per model.
% For each evaluated neuron, we retrieved the five most activated test images and asked 25 users to rate them from 1 (lowest) to 5 (highest) based on two criteria: 
% (a)~\textit{semantic consistency:} “Do these images share a common semantic concept?” and (b)~\textit{concept accuracy:} “Does [neuron label] describe a common concept shared by these images?”.
% \\
% \textbf{Results.} As shown in Table~\ref{tab:user_study_results}, SALF-CBM achieves significantly better user scores in both semantic consistency and concept accuracy, demonstrating its improved interpretability compared to the baseline model.}
We conduct a user study to quantitatively validate that SALF-CBM's bottleneck neurons in-fact correspond to their designated target concepts.
% Additional details and results are provided in the supplementary.
% See additional qualitative results in the supplementary.
\\
\textbf{Experimental setup.} Following~\cite{rao2024discover}, we evaluate global concept neurons~$c^*$
from SLAF-CBM's bottleneck layer compared to output neurons of the baseline ResNet-50 backbone. We assign concept labels to baseline neurons using CLIP-Dissect \cite{oikarinen2022clip-dissect} and rank neurons from both models by interpretability scores using CLIP-Dissect's soft-WPMI metric. We then sample 10 neurons from the top 30\% interpretable neurons and 10 from the remaining 70\% for each model. For each neuron, we show 25 users the five most activated test images and ask them to rate from 1-5: (a) \textit{semantic consistency}: "Do these images share a common semantic concept?" and (b) \textit{concept accuracy}: "Does [neuron label] describe a common concept shared by these images?".
\\
\textbf{Results.} Figure~\ref{fig:user_study_results} shows that SALF-CBM achieves significantly better scores in both semantic consistency and concept accuracy, demonstrating improved interpretability compared to the baseline across all neuron interpretability groups. See additional results in the supplementary.

\begin{figure}[h!]
  \centering
  \includegraphics[ width = 7.8cm ]{figs/2_experiments/UserStudyRes.pdf}
  \vspace{-0.2cm}
  \caption{\textbf{User study results}.}
  \label{fig:user_study_results}
\end{figure}


\twocolumn[{%
  \renewcommand\twocolumn[1][]{#1}%
  \begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=17.5cm]{figs/2_experiments/explain_anything_mainPaper.pdf}
    \captionof{figure}{\textbf{Explain Anything}. For each image, we prompted SALF-CBM with two different ROI masks produced by SAM~\cite{ma2024segment} (\textcolor{red}{red} and \textcolor{blue}{blue} regions). Our method provides accurate concept descriptions for each ROI. \label{fig:explain_anything_results}}

    \vspace{0.3cm} % Optional spacing between figures

    \includegraphics[width=17.5cm]{figs/2_experiments/intervention_v5.pdf}
    \captionof{figure}{\textbf{Model debugging with Explain Anything.} We reveal that the model misclassified the image since it mistakenly identified traffic lights as street signs. Its prediction is corrected by locally editing the relevant concepts maps in the examined ROI. \label{fig:intervention_results}}
  \end{center}%
}]

\subsection{Model exploration and debugging}
\label{section:model_exploration_ex}
We first qualitatively validate our \textit{Explain Anything} feature (Section~\ref{sec:model-exploration}) on different images from the SAM dataset~\cite{ma2024segment}. For each image, we prompt SALF-CBM (with ResNet-50 backbone pre-trained on ImageNet) with two different ROI masks automatically obtained by SAM~\cite{ma2024segment}, highlighted in red and blue. As shown in Figure~\ref{fig:explain_anything_results}, SALF-CBM generates informative region-specific concept descriptions that accurately correspond to the selected ROIs. For instance, in the child's drawing (left image), the dress (blue mask) and grass area (red mask) are correctly identified as fabric-like material and a field or lawn, respectively.

Next, we demonstrate the usefulness of Explain Anything in diagnosing classification errors, and facilitating targeted corrections using local user intervention.
We present a case study from the ImageNet validation set, where our model miscalssified a “traffic light” image as a “parking meter”, as shown in Figure~\ref{fig:intervention_results}.
Applying Explain Anything to the traffic lights region in the image reveals that the model primarily detected sign-related concepts there. However, as indicated by the class weights visualization, these concepts are not associated with the correct “traffic light” class. This misalignment, along with the presence of street-related features in the image, led the model to incorrectly classify the image as a “parking meter.”
To rectify that, we locally edit two concepts maps associated with the true class - “a flashing light” and “the ability to change color” - within the selected ROI. Specifically, we increase their activation there by a correction factor of $\beta=1$. As illustrated in the figure, this mild adjustment promoted these concepts to the top-5 most activated concepts in the ROI, subsequently adjusting the model's output to the correct class.