\section{Introduction}
\label{sec:intro}

Humans often rationalize visually-based assessments or conclusions by describing \textit{what} they have seen and \textit{where} they have seen it, using both semantic concepts and their spatial locations. For example, an image of a \textit{dog} wearing \textit{glasses} and a \textit{hat}, as shown at the top of Figure~\ref{fig:opening_figure}, is likely to be interpreted as playful or funny due to the unexpected spatial composition of concepts. Notably, this mechanism operates independently of a specific task; even when looking for a dog in the bottom image of Figure~\ref{fig:opening_figure}, one may notice the tennis ball in its mouth and the pot next to it.

Similarly, the ability to explain AI models using spatially grounded concepts is crucial for elucidating their decision-making processes. Such an approach enables the introduction of quality control mechanisms, i.e., understanding the underlying causes of a model's behavior and adjusting it when necessary. These capabilities are essential for ensuring the safe and transparent integration of deep neural networks into critical domains such as medical imaging and autonomous driving, as mandated by the AI Act recently passed by the European Parliament~\cite{AIact2023}.

Most current explainable AI (XAI) methods, however, provide either spatial or concept-based explanations.
Spatial approaches, generally referred to as attribution methods, produce heatmaps that highlight the image regions most contributing to the model's output. These heatmaps are generated either by propagating gradients through the model with respect to its input~\cite{simonyan2013deep, cao2015look, shrikumar2017learning, sundararajan2017axiomatic, smilkov2017smoothgrad, srinivas2019full, selvaraju2017grad, bargal2021guided}, or by using attribution-propagation methods~\cite{bach2015pixel, binder2016layer, montavon2017explaining, zhang2018top, voita2019analyzing, abnar2020quantifying, chefer2021transformer} that distribute “relevance” (i.e., the contribution of a neuron to the output) backwards through the network, layer by layer.
While these methods can visualize the model's spatial attention, in the absence of semantic descriptions, their output can be ambiguous~\cite{colin2022cannot, kim2022hive}.

Concept Bottleneck Models (CBMs)~\cite{zhou2018interpretable, losch2019interpretability, koh2020concept, yuksekgonul2022post, oikarinen2023label, wang2023learning}, on the other hand, are an increasingly popular method for obtaining concept-based explanations. Unlike attribution methods, CBMs provide \textit{ante-hoc} explanations—i.e., their explainability mechanism is embedded into the model itself. Current CBMs work by introducing a non-spatial bottleneck layer that maps model features to an interpretable concept space, followed by training a final output layer over these concepts. This design ensures that CBMs are highly interpretable, as their predictions are directly based on the concepts used to explain them.
However, existing CBMs provide global concept-based explanations without localizing them in the image. Moreover, the interpretable bottleneck layer often comes at the expense of the final task accuracy, which limits their applicability.

In this work, we present a \textit{spatially-aware} CBM that combines concept-based explanations with the ability to visually ground them in the input image.
In contrast to traditional CBMs, we preserve the spatial information of features and project them into a spatial concept space. This is achieved in a label-free manner by leveraging the capability of CLIP~\cite{radford2021CLIP} to produce local image embeddings using visual prompts~\cite{shtedritski2023RedCircle}.
Accordingly, we name our method \textit{“spatially-aware and label-free CBM”} (SALF-CBM).
The main contributions of our work are as follows:
\textbf{(1)}~\textbf{Novel unified framework:} we present the first label-free CBM that provides both concept-based (global) and heatmap-based (local) explanations. \textbf{(2)}~\textbf{Classification results:} SALF-CBM outperforms non-spatial CBMs on several classification tasks, and can even achieve better classification results than the original (non-CBM) backbone model. \textbf{(3)}~\textbf{Heatmap quality:} our method produces high-quality heatmaps that can be used for zero-shot segmentation. We demonstrate their advantage over widely-used heatmap-based methods in both qualitative and quantitative evaluations. \textbf{(4)}~\textbf{Explain anything:} SALF-CBM facilitates interactive model exploration and debugging, enabling users to inquire about concepts identified in specific image regions, and to adjust the model's final prediction by locally refining its concept maps. \textbf{(5)}~\textbf{Applicability:} Our method is model-agnostic and can be applied to both CNNs and transformer architectures, while not introducing any additional learnable parameters compared to non-spatial CBMs.