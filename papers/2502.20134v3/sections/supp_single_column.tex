\clearpage
\setcounter{page}{1}
% \maketitlesupplementary

\appendix
\onecolumn

% \appendixpage

% Large "Appendix" title
\begin{center}
    {\Large Appendix}
\end{center}

\section{Implementation details}
\subsection{Local image-concept similarities}
\label{supp:vis_prompt_algo}
The pseudo-algorithm for computing local image-concept similarities using visual prompts is provided below. The operation of drawing a red circle within training image $x_n$ at location $(h,w)$ with radius $r$ is denoted by $\textcolor{red}{\text{Circle}}(x_n; (h, w, r))$. We use circles with a line width of 2 pixels.

% \begin{algorithm}
% 	\caption{Local image-concept similarities}
% 	\begin{algorithmic}
% 		\State \textbf{Input:} (i)~training images $\{x_n\}^{N}_{n=1} \in \mathbb{R}^{3 \times H \times W}$; (ii)~concept list $\{t_m\}^{M}_{m=1}$;
%         (iii)~CLIP's image encoder $E_I$ and text encoder $E_T$;
%         (iv)~circle radius $r$ and grid dimensions $(\Tilde{H},\Tilde{W})$.
%         \State \textbf{Output:} Spatial concept similarity matrix $P$.
%         \State \textbf{Initialize:} $P \leftarrow \mathbf{0}$.
%         \\\hrulefill

%         \State $d_H \leftarrow \lfloor H/(\Tilde{H} + 1) \rfloor$, $d_W \leftarrow \lfloor W/(\Tilde{W} + 1) \rfloor$
%         \For{$n \leftarrow 0$ \textbf{to} $N-1$} \Comment{iterate over images}
%             \For{$h \leftarrow r$ \textbf{to} $\Tilde{H} - r$ \textbf{by} $d_H$}
%                 \For{$w \leftarrow r$ \textbf{to} $\Tilde{W} - r$ \textbf{by} $d_W$} \Comment{iterate over grid locations}
%                     \State $x_n^{(h,w)} \leftarrow \textcolor{red}{\text{Circle}}(x_n; (h, w, r))$
%                     \State $I_n \leftarrow E_I(x_n^{(h,w)})$
%                     \For{$m \leftarrow 0$ \textbf{to} $M-1$} \Comment{iterate over concepts}
%                         \State $T_m \leftarrow E_T(t_m)$
%                         \State $P[n, m, h, w] \leftarrow \frac{I_n \cdot T_m}{\|I_n\| \|T_m\|}$
%                     \EndFor
%                 \EndFor
%             \EndFor
%         \EndFor
%         \State \textbf{return} $P$
% 	\end{algorithmic}
% 	\label{alg:vis_prompt}
% \end{algorithm}

\begin{algorithm}
\caption{Local image-concept similarities}
\begin{algorithmic}
    \State \textbf{Input:} (i)~training images $\{x_n\}^{N}_{n=1} \in \mathbb{R}^{3 \times H \times W}$; (ii)~concept list $\{t_m\}^{M}_{m=1}$;
    (iii)~CLIP's image encoder $E_I$ and text encoder $E_T$;
    (iv)~circle radius $r$, grid dimensions $(\Tilde{H},\Tilde{W})$.
    \State \textbf{Output:} Spatial concept similarity matrix $P$ for the entire training set.
    \State \textbf{Initialize:} $P \leftarrow \mathbf{0}$.
    \\\hrulefill

    \State $d_H \leftarrow \lfloor H/(\Tilde{H} - 1) \rfloor$, $d_W \leftarrow \lfloor W/(\Tilde{W} - 1) \rfloor$
    \State $T \leftarrow E_T(\{t_m\}_{m=1}^{M})$ \Comment{encode all concepts once}
    \For{$n \leftarrow 0$ \textbf{to} $N-1$} \Comment{iterate over training images}
        % \State $X_b \leftarrow \{x_{n}, \dots, x_{n+b-1}\}$
        \State $X_n^{aug} \leftarrow \emptyset$
        \For{$h \leftarrow r$ \textbf{to} $\Tilde{H}-r$ \textbf{by} $d_H$}
        \Comment{iterate over grid locations}
            \For{$w \leftarrow r$ \textbf{to} $\Tilde{W}-r$ \textbf{by} $d_W$}
                \State $X_n^{aug} \leftarrow X_n^{aug} \cup \textcolor{red}{\text{Circle}}(x_n; (h,w,r)) $
            \EndFor
        \EndFor
        \State $I_n \leftarrow E_I(X_n^{aug})$ \Comment{encode entire augmented batch}
        % \For{$i \leftarrow 0$ \textbf{to} $|X_{aug}|-1$}
        %     \State $(n', h', w') \leftarrow \text{idx\_to\_coord}(i, b, \Tilde{H}, \Tilde{W}, d_H, d_W)$
        %     \For{$m \leftarrow 0$ \textbf{to} $M-1$}
        %         \State $P[n+n', m, h', w'] \leftarrow \frac{I_b[i] \cdot T[m]}{\|I_b[i]\| \|T[m]\|}$
        %     \EndFor
        % \EndFor
        \State $P[n, m, h, w] \leftarrow \frac{I_n^{(h,w)} \cdot T_m}{\|I_n^{(h,w)}\| \|T_m\|}$
    \EndFor
    \State \textbf{return} $P$
\end{algorithmic}
\label{alg:batch_vis_prompt}
\end{algorithm}

\subsection{Choosing the grid parameters}
\label{supp:vis_prompt_params}
We experiment with different settings of the visual prompting grid. In Table~\ref{tab:main}, we present the classification accuracy obtained using different values for the circle radius $r$ and the grid size $\Tilde{H} \times \Tilde{W}$, on the ImageNet (left) and CUB-200 (right) datasets. In both cases, the best performance is achieved with $r=32$ and a grid size of $7 \times 7$. We use the same values for Places365.

\begin{table*}[ht]
    \centering
    % First subtable
    \begin{subtable}[t]{0.45\textwidth} % Adjust width as needed
        \centering
        \begin{tabular}{@{}llll@{}}
\toprule
                            Grid size     & $r=27$   & $r=32$         & $r=37$         \\ \midrule
$5 \times 5$ & 74.17\%          & 74.37\%          & 75.01\%          \\
$7 \times 7$  & 74.67\%          & \textbf{75.32\%}          & 75.31\%          \\
$9 \times 9$        & 75.06\%          & 75.22\%          & 75.22\%          \\ \bottomrule
\end{tabular}
        \caption{Results on ImageNet.}
        \label{tab:sub1}
    \end{subtable}
    \hspace{0.05\textwidth} % Add some space between the tables
    % Second subtable
    \begin{subtable}[t]{0.45\textwidth} % Adjust width as needed
        \centering
        \begin{tabular}{@{}llll@{}}
\toprule
                            Grid size     & $r=27$   & $r=32$         & $r=37$          \\ \midrule
$5 \times 5$ & 73.36\%          & 73.59\%          & 73.80\%          \\
$7 \times 7$  & 73.42\%          & \textbf{74.35\%}          & 74.01\%          \\
$9 \times 9$        & 73.83\%          & 74.12\%          & 73.93\%          \\ \bottomrule
\end{tabular}
        \caption{Results on CUB-200.}
        \label{tab:sub2}
    \end{subtable}
    \caption{Classification accuracy for different settings of the visual prompting grid, on the ImageNet (left) and CUB-200 (right) datasets.}
    \label{tab:main}
\end{table*}

\subsection{Spatial concept bottleneck layer}
Our spatial concept bottleneck layer is comprised of a single $1 \times 1$ convolution layer with $M$ output channels and no bias, where $M$ is the number of concepts.
Therefore, it requires the same number of parameters as the fully-connected bottleneck layer typically used in non-spatial CBMs, i.e., $D \times M$ where $D$ is the dimensionality of the “black-box” features.
%-------------------------------------------------------------------------

\clearpage
\section{Results with ViT backbone}
\subsection{Classification accuracy}
\label{supp:vit_classification}
We report the classification results of our SALF-CBM with a ViT-B/16 backbone pre-trained on ImageNet. We experiment with two variations: (1)~Only patch tokens are used, reshaped into their original spatial formation; (2)~Both patch tokens and the \texttt{CLS} token are used, by reshaping the patch tokens into their original spatial formation and concatenating them with the \texttt{CLS} token along the channels dimension.
For each variation, the model is trained with both sparse and non-sparse classification layers. We compare its results to the corresponding standard model—i.e., using the same backbone model without a bottleneck layer and with a comparable classification layer (sparse or non-sparse).
Results are shown in Figure~\ref{fig:vit_results}. When using a sparse final layer, our model significantly outperforms the corresponding standard model for both backbone versions. With a non-sparse final layer, our model's performance is comparable to the standard model when using the \texttt{CLS} token, and is slightly lower when the \texttt{CLS} token is excluded.

\begin{figure}[h!]
  \centering
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \textbf{Sparse final layer}
    \includegraphics[width=7cm]{figs/supp/VitClassification_1.pdf}
    % \caption{\textbf{User study results}.}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.48\textwidth}
    \centering
    \textbf{Non-sparse final layer}
    \includegraphics[width=7cm]{figs/supp/VitClassification_2.pdf}
    % \caption{\textbf{User study results}.}
  \end{minipage}
  \caption{ImageNet classification results with ViT-B/16 backbone, when using a sparse final layer (left) and a dense final layer (right).}
  \label{fig:vit_results}
\end{figure}

% \begin{figure*}[h!]
%   \centering
%   \includegraphics[ width = 12cm ]{figs/supp/vit_classification_results_v2.pdf}
%   \caption{ImageNet classification results with ViT-B/16 backbone, when using a sparse final layer (left) and a dense final layer (right).}
%   \label{fig:vit_results}
% \end{figure*}

% \newpage
\subsection{Spatial heatmaps}
We present qualitative results of the heatmaps generated by our method when using a ViT-B/16 backbone pre-trained on ImageNet. Similar to section~\ref{section:zs_seg}, we train our model on ImageNet using a concept list of the form “An image of a \{class\}”, where \{class\} refers to each of the ImageNet classes.
In Figure~\ref{fig:vit_heatmaps_comparison}, we show the heatmaps produced by our method compared to the raw attention maps of the ViT model, for different images from the ImageNet validation set. 
We observe that our SALF-CBM's heatmaps tend to be more exclusive, while the raw attention maps often include background areas outside the target class object.

\begin{figure*}[h!]
  \centering
  \includegraphics[ width = 13cm ]{figs/supp/vit_heatmaps_compare_v3.pdf}
  \caption{Heatmaps generated by our SALF-CBM with a ViT-B/16 backbone (middle row) for random images from the ImageNet validation set, compared to the raw attention maps of the standard ViT model (bottom row). The ground-truth class of the images (from left to right): “Dalmatian”, “Balloon”, “Castle”, “Zebra”, “Consomme” and “Hamper”.}
  \label{fig:vit_heatmaps_comparison}
\end{figure*}
%-------------------------------------------------------------------------

%-------------------------------------------------------------------------
\clearpage
\section{Bottleneck interpretability validation}
\label{supp:concept_validation}
\subsection{User study questions examples}
We show an example of the \textit{semantic consistency} and \textit{concept accuracy} questions used in our user study, as described in the main paper.
\begin{figure*}[h!]
  \centering
  \includegraphics[ width = 13cm ]{figs/supp/user_study_example.pdf}
  \caption{User study question example.}
  \label{fig:user_study_example}
\end{figure*}

\newpage
\subsection{Concept neurons validation}
In addition to the user study described in the main paper, we qualitatively validate that neurons in our concept bottleneck layer indeed correspond to their designated target concepts. We train a SALF-CBM on each dataset (ImageNet, Places365 and CUB-200) and randomly select 5 neurons from its concept bottleneck layer.
For each neuron, we retrieve the top-3 images with the highest global concept activation $c^{\ast}$ from the corresponding validation set.
As shown in Figure~\ref{fig:retreival}, the target concept of each neuron highly corresponds to the retrieved images.

\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \caption{Results on ImageNet}
        \includegraphics[width=\textwidth]{figs/supp/image_retrieval_imagenet_top3_v2.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \caption{Results on Places365}
        \includegraphics[width=\textwidth]{figs/supp/image_retrieval_places_top3_v2.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \caption{Results on CUB-200}
        \includegraphics[width=\textwidth]{figs/supp/image_retrieval_cub_top3_v2.pdf}
    \end{subfigure}
    
    \caption{\textbf{Qualitative validation of concepts learned by CBL neurons.} Top 3 images with the highest  concept activation $c^{\ast}$, for 5 randomly selected neurons in the CBL. The retrieved images are highly correlated with the neuron's target concept. Results are shown for ImageNet (left), Places365 (middle) and CUB-200 (right) datasets.}
    \label{fig:retreival}
\end{figure*}

%-------------------------------------------------------------------------
\clearpage
\section{Additional experiments}
\label{supp:explanations}
% Add text.
\subsection{Explanations on different datasets}
\label{supp:explanations_datasets}
We present qualitative results of concept-based and spatial explanations across images from different datasets: ImageNet (Figure~\ref{fig:imagenet_explanations}), Places365 (Figure~\ref{fig:places_explanations}) and CUB-200 (Figure~\ref{fig:cub_explanations}).
For each image, we present the most important concepts used by our SALF-CBM to classify the image, along with a heatmap of one of these concepts.
By offering both concept-based explanations and their visualizations on the input image, our model enables a comprehensive understanding of its decision-making process. For example, in the second row of Figure~\ref{fig:places_explanations}, we see that our model correctly classified the image as “athletic field, outdoor” by identifying and accurately localizing the track behind the athlete.

\subsection{Explaning multi-class images}
\label{supp:explanations_multiclass}
We demonstrate our method's ability to produce class-specific explanations in Figure~\ref{fig:imagenet_multiclass}. Given an image $x$ with two possible classes, $\hat{y}=l_1$ and $\hat{y}=l_2$, we compute the concept contribution scores for predicting each class, i.e., $S(x, m, \hat{y}=l_1)$ and $S(x, m, \hat{y}=l_2)$, as described in Section~\ref{sec:test-time-explainations}.
For each image, we present the concepts with the highest contribution scores along with the heatmap of the most contributing concept.

\begin{figure*}[h!]
  \centering
  \includegraphics[ width = 14cm ]{figs/supp/imagenet_explanations_v2.pdf}
  \caption{Concept-based and visual explanations on ImageNet.}
  \label{fig:imagenet_explanations}
\end{figure*}
\begin{figure*}[h!]
  \centering
  \includegraphics[ width = 14cm ]{figs/supp/places_explanations_v2.pdf}
  \caption{Concept-based and visual explanations on Places365.}
  \label{fig:places_explanations}
\end{figure*}
\begin{figure*}[h!]
  \centering
  \includegraphics[ width = 14cm ]{figs/supp/cub_explanations_v2.pdf}
  \caption{Concept-based and visual explanations on CUB-200.}
  \label{fig:cub_explanations}
\end{figure*}

\begin{figure*}[h!]
  \centering
  \includegraphics[ width = 14cm ]{figs/supp/imagenet_multiclass_explanations_v3.pdf}
  \caption{\textbf{Explaining predictions on multi-class images.} For each image, we present the most contributing concepts identified by SALF-CBM for explaining two different output classes that fit the image. We show the heatmap of the top concept for each class.}
  \label{fig:imagenet_multiclass}
\end{figure*}

%-------------------------------------------------------------------------
\clearpage
\section{Additional heatmaps results}
\label{supp:heatmaps}
% Qualitative.
% \subsection{ImageNet segmentation results}
% \label{supp:heatmaps_imagenet_seg}
% Add.
\subsection{Visualizing multiple concepts}
\label{supp:heatmaps_multi_concepts}
We demonstrate our method's ability to localize multiple concepts within a single image. In Figure~\ref{fig:multiple_concepts}, we present qualitative results on several images from the ImageNet validation set. For each image, we show three heatmaps generated by our SALF-CBM, each corresponding to a different visual concept.

\subsection{Visualizing concepts in videos}
\label{supp:heatmaps_video}
By applying SALF-CBM to video sequences in a frame-by-frame manner, we achieve visual tracking of specific concepts. In Figure~\ref{fig:davis_heatmaps}, we demonstrate this capability on several videos from the DAVIS 2017 dataset using a SALF-CBM trained on ImageNet. Despite being trained on a completely different dataset, our model successfully localizes various concepts throughout these videos. For example, in the “soccer ball” video at the top of the figure, the soccer ball is accurately highlighted, even when it is partially occluded in the last frame.

\begin{figure*}[h!]
  \centering
  \includegraphics[ width = 11cm ]{figs/supp/multiple_concepts_heatmaps_v2.pdf}
  \caption{\textbf{Localizing multiple concepts in images.} For each image, we present three heatmaps, each corresponding to a different visual concepts.}
  \label{fig:multiple_concepts}
\end{figure*}

\begin{figure*}[h!]
  \centering
  \hspace{-1.3cm}
  \includegraphics[ width = 12.5cm ]{figs/supp/davis_heatmaps_v3.pdf}
  \caption{\textbf{Visualizing concepts in videos.} By applying SALF-CBM in a frame-by-frame manner, one can visually track concepts over time. Videos are from the DAVIS 2017 dataset (from top to bottom): “soccer ball”, “horsejump-high” and “rollerblade”.}
  \label{fig:davis_heatmaps}
\end{figure*}

%-------------------------------------------------------------------------