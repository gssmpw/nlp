\section{Comparison of Downstream Performance}
We evaluate a range of language models across multiple benchmarks, measuring their performance in both zero-shot and few-shot settings. ARC-Challenge and ARC-Easy \citep{arc}, and PIQA \citep{piqa} were evaluated with 0 and 25 shots, Winogrande \citep{winogrande} with 0 and 5 shots, HellaSwag \citep{hellaswag} and OpenBookQA \cite{OpenBookQA} with 0 and 10 shots, and Lambada \citep{lambada} and MMLU \citep{mmlu} with 0 and 5 shots. 
The results, reported in \Cref{tab:performance_comparison_1,tab:performance_comparison_2}, use normalized logits for ARC-Challenge, ARC-Easy, PIQA, HellaSwag, and OpenBookQA. These benchmarks cover a range of reasoning, commonsense, and language comprehension tasks, providing insight into the models’ ability to process different types of contextual dependencies.

Along with Llamba-1B, Llamba-3B, and Llamba-8B, we evaluate different quadratic and sub-quadratic architectures. Specifically, we include Transformer-based models such as Llama-3.2-1B-Instruct, Llama-3.2-3B-Instruct, and Llama-3.1-8B-Instruct \citep{llama}, as well as Qwen2.5-3B \citep{qwen2}. Models integrating recurrent components include Falcon-Mamba-7B-Instruct and Falcon3-Mamba-7B-Instruct \citep{falcon}, RecurrentGemma-2B-it and RecurrentGemma-9B-it \citep{recurrentgemma}, and Mamba varoatopms \citep{mamba1,mamba2}. 
Additionally, we evaluate Zamba2-7B-Instruct \citep{zamba2}, a hybrid incorporating both attention and SSMs. 
Whenever an instruct-tuned version is available, we use it, as these models generally perform better. These models differ in training strategies, data sources, and architectural choices, enabling a broad comparison of language model capabilities across different methodologies.

\begin{table}[H]
    \small
    \centering
    \setlength{\tabcolsep}{3.2pt}
    \caption{
    Comparison of downstream performance (accuracy \%) across various models in zero-shot and few-shot settings.
    For ARC-Challenge, ARC-Easy, and PIQA, we have used normalized logits' results.
    Along with accuracy, each model is annotated with the number of training or distillation tokens (in trillions) and its architecture—Recurrent (R), Transformer (T), or Hybrid (H). For models with a sliding window, the window size is also specified.
    We use an instruct-tuned version whenever one is available; however, we exclude this label for brevity.
    }
    \label{tab:performance_comparison_1}
    \begin{tabular}{l c c c c c c c c c c}
        \toprule
        \multirow{2}{*}{\textsc{Model}} 
         & \multirow{2}{*}{\textsc{Arch.}}
         & \multirow{2}{*}{\textsc{Tokens (T)}}
         & \multicolumn{2}{c}{\textsc{ARC Challenge}} 
         & \multicolumn{2}{c}{\textsc{ARC Easy}} 
         & \multicolumn{2}{c}{\textsc{PIQA}} 
         & \multicolumn{2}{c}{\textsc{Winogrande}} \\
        \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} \cmidrule(r){10-11}
         & & & 0-shot & 25-shot & 0-shot & 25-shot & 0-shot & 10-shot & 0-shot & 5-shot \\
         
        \midrule

        \textsc{Llama-3.2-1B (Teacher)} & T & $\leq 9$   & 38.1 & 42.0 & 68.5 & 71.8 & 74.4 & 75.4 & 59.7 & 62.0 \\
        \textbf{\textsc{Llamba-1B}}      & R & 0.008      & 37.2 & 41.8 & 69.5 & 71.2 & 74.0 & 74.3 & 60.6 & 58.1 \\
        \textsc{Mamba-1.4B}             & R & 0.3        & 32.9 & 36.0 & 60.9 & 66.6 & 73.7 & 74.4 & 60.6 & 60.1 \\
        \textsc{RecurrentGemma-2B}      & H ($w=2048$) & $\leq 2$  & 35.6 & 48.0 & 51.2 & 73.3 & 67.2 & 75.8 & 55.7 & 64.1 \\
        
        \midrule

        \textsc{Qwen2.5-3B}             & T & $\leq 18$  & 48.1 & 60.8 & 72.9 & 85.1 & 78.3 & 79.8 & 69.8 & 71.3 \\
        \textsc{Llama-3.2-3B (Teacher)} & T & 9          & 45.6 & 52.1 & 74.3 & 79.8 & 75.8 & 77.7 & 67.6 & 68.8 \\
        \textbf{\textsc{Llamba-3B}}      & R & 0.01       & 48.5 & 53.0 & 79.0 & 81.1 & 78.6 & 79.5 & 70.4 & 72.4 \\
        \textsc{Mamba2-2.8B}            & R & 0.3        & 35.9 & 39.5 & 64.3 & 71.8 & 75.6 & 76.4 & 63.4 & 64.6 \\
        
        \midrule

        \textsc{Qwen2.5-7B} & T & 18 & 55.1 & 67.0 & 81.3 & 89.5 & 80.3 & 82.4 & 71.1 & 75.1 \\
        \textsc{Llama-3.1-8B (Teacher)} & T & 15         & 55.1 & 60.0 & 81.7 & 85.8 & 81.1 & 82.4 & 73.9 & 77.3 \\
        \textbf{\textsc{Llamba-8B}}      & R & 0.012      & 54.6 & 60.0 & 82.5 & 85.8 & 80.9 & 81.5 & 73.3 & 76.9 \\
        \textsc{Falcon3-Mamba-7B}       & R & 7.3        & 53.2 & 65.9 & 72.5 & 86.7 & 79.7 & 82.3 & 69.1 & 72.1 \\
        \textsc{Zamba2-7B}             & H & 2.1        & 56.1 & 68.3 & 80.6 & 88.7 & 81.1 & 81.3 & 76.9 & 80.1 \\
        \textsc{RecurrentGemma-9B}      & H ($w=2048$) & 2       & 57.1 & 60.2 & 78.9 & 84.5 & 80.6 & 81.7 & 73.7 & 75.6 \\
        \bottomrule
        \multicolumn{11}{l}{} \\
    \end{tabular}
\end{table}

\begin{table}[H]
    \small
    \setlength{\tabcolsep}{3.5pt}
    \centering
    \caption{
    Comparison of downstream performance (accuracy \%) across various models in zero-shot and few-shot settings. For HellaSwag and OpenBookQA, we have used normalized logits' results.
    Along with accuracy, each model is annotated with the number of training or distillation tokens (in trillions) and its architecture—Recurrent (R), Transformer (T), or Hybrid (H). For models with a sliding window, the window size is also specified.
    We use an instruct-tuned version whenever one is available; however, we exclude this label for brevity.
    }
    \label{tab:performance_comparison_2}
    \begin{tabular}{l c c c c c c c c c c}
        \toprule
        \multirow{2}{*}{\textsc{Model}} 
         & \multirow{2}{*}{\textsc{Arch.}} 
         & \multirow{2}{*}{\textsc{Tokens (T)}}  
         & \multicolumn{2}{c}{\textsc{HellaSwag}} 
         & \multicolumn{2}{c}{\textsc{Lambada}} 
         & \multicolumn{2}{c}{\textsc{MMLU}} 
         & \multicolumn{2}{c}{\textsc{OpenBookQA}} \\
        \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} \cmidrule(r){10-11}
         & & & 0-shot & 10-shot & 0-shot & 10-shot & 0-shot & 5-shot & 0-shot & 10-shot \\
        \midrule
        \textsc{Llama-3.2-1B (Teacher)} & T & $\leq 9$    & 60.8 & 59.4 & 60.1 & 53.1 & 46.0 & 45.5 & 34.6 & 37.6 \\
        \textbf{\textsc{Llamba-1B}}      & R & 0.008       & 61.2 & 60.2 & 48.4 & 39.0 & 38.0 & 31.3 & 37.0 & 38.0 \\
        \textsc{Mamba-1.4B}             & R & 0.3         & 59.1 & 59.6 & 64.4 & 57.0 & 24.7 & 24.8 & 36.8 & 37.4 \\
        \textsc{RecurrentGemma-2B}      & H ($w=2048$) & $\leq 2$  & 60.3 & 69.4 & 52.5 & 53.0 & 40.2 & 42.1 & 30.4 & 43.2 \\
        
        \midrule

        \textsc{Qwen2.5-3B}             & T & $\leq 18$   & 74.9 & 75.2 & 65.8 & 58.1 & 65.5 & 66.4 & 41.8 & 46.2 \\
        \textsc{Llama-3.2-3B (Teacher)} & T & 9           & 70.4 & 73.2 & 65.9 & 61.9 & 60.4 & 59.8 & 35.8 & 39.6 \\
        \textbf{\textsc{Llamba-3B}}      & R & 0.01        & 73.8 & 74.3 & 65.8 & 60.0 & 52.7 & 50.3 & 42.8 & 42.8 \\
        \textsc{Mamba2-2.8B}            & R & 0.3         & 66.2 & 66.6 & 68.1 & 61.2 & 25.7 & 25.1 & 40.4 & 42.0 \\
        
        \midrule

        \textsc{Qwen2.5-7B} & T & 18 & 80.5 & 81.3 & 69.5 & 62.7 & 71.7 & 74.4 & 48.6 & 52.0 \\
        \textsc{Llama-3.1-8B (Teacher)} & T & 15          & 79.3 & 80.0 & 73.0 & 65.6 & 68.0 & 68.4 & 43.0 & 48.2 \\
        \textbf{\textsc{Llamba-8B}}      & R & 0.012       & 77.6 & 78.7 & 69.4 & 65.0 & 61.0 & 60.0 & 43.4 & 45.8 \\
        \textsc{Falcon3-Mamba-7B}       & R & 7.3         & 79.8 & 81.6 & 67.5 & 63.6 & 65.0 & 66.0 & 48.0 & 50.2 \\
        \textsc{Zamba2-7B}             & H & 2.1         & 81.5 & 83.5 & 74.6 & 68.6 & 64.7 & 67.2 & 45.2 & 52.4 \\
        \textsc{RecurrentGemma-9B}      & H ($w=2048$) & 2       & 80.1 & 80.9 & 54.1 & 69.6 & 55.1 & 56.5 & 46.0 & 49.2 \\
        \bottomrule
        \multicolumn{11}{l}{} \\
    \end{tabular}
\end{table}