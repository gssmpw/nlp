\section{Related Work}
\paragraph{Language Models.}
Transformer-based models, such as those in the Llama ____, and Qwen ____ series, have shown strong performance across various language modeling tasks. These models underwent extensive pretraining on large-scale corpora and incorporate techniques like instruction tuning and curated datasets to enhance generalization in few-shot and zero-shot settings on various tasks.

While Transformers remain dominant, recent work has explored alternatives to their purely quadratic attention mechanisms to improve efficiency while maintaining strong performance. Structured state space models (SSMs) ____ have emerged as a promising direction, offering a distinct approach to sequence modeling.
At large scales, Falcon-Mamba ____, a fully SSM-based model stacking Mamba-1 layers, has matched and even outperformed Transformers on key tasks. Falcon3-Mamba extends this by pretraining for an additional 1.5 trillion tokens, incorporating high-quality post-training data, and expanding the context length from 8K to 32K tokens, further enhancing its capabilities.
However, despite these advances, SSM-based models still underperform Transformers on algorithmic tasks ____.

To balance these trade-offs, hybrid models combining attention and SSMs have gained interest for leveraging the strengths of both architectures.
Examples include RecurrentGemma ____, which integrates gated linear recurrences with local attention, and Zyphraâ€™s Zamba ____, which combines Mamba-1 layers with a shared attention mechanism spanning the network.
Zamba-2 ____ builds on this by replacing Mamba-1 blocks with Mamba-2 for improved efficiency, increasing shared attention layers from one to two for enhanced global context modeling, and applying Low-Rank Adaptation (LoRA) ____ to shared MLP blocks for parameter-efficient depth adjustments.
Other hybrid architectures ____ further underscore the interest in models that balance expressiveness and efficiency.

\paragraph{Distillation.}
Various methods have been proposed to distill large Transformer-based models into more efficient architectures while maintaining performance.
SUPRA ____ propose a procedure to linearize softmax attention into a form of linear attention by copying the weight matrices and fine-tuning.
LoLCATs ____ introduces a linearization approach that replaces softmax attention with linear attention through attention transfer, followed by low-rank adaptation, enabling the creation of large-scale linearized models with improved efficiency. 
____ leverages the State-Space Duality (SSD) in ____ to transfer the linear projection weights from the attention layers into Mamba-based models. Their experiments include hybrid models with an increasing proportion of interleaved attention layers, demonstrating that a balanced combination of state-space models (SSMs) and attention preserves performance while improving efficiency.
MOHAWK ____ distills Transformers into SSMs through progressive alignment, allowing subquadratic models to leverage Transformer training resources effectively. These approaches demonstrate the viability of distilling computationally expensive Transformers into efficient models while retaining competitive performance.