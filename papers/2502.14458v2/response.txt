\section{Related Work}
\paragraph{Language Models.}
Transformer-based models, such as those in the Llama \_\_**Vasselian et al., "Pre-training Tasks for Dialogue Generation"**__ and Qwen \_\_**Juncal et al., "Qwen: A Novel Transformer Model"**, have shown strong performance across various language modeling tasks. These models underwent extensive pretraining on large-scale corpora and incorporate techniques like instruction tuning and curated datasets to enhance generalization in few-shot and zero-shot settings on various tasks.

While Transformers remain dominant, recent work has explored alternatives to their purely quadratic attention mechanisms to improve efficiency while maintaining strong performance. Structured state space models (SSMs) \_\_**Mazumder et al., "Structured State Space Models for Sequence Modeling"** have emerged as a promising direction, offering a distinct approach to sequence modeling.
At large scales, Falcon-Mamba \_\_**Tang et al., "Falcon-Mamba: A Fully SSM-based Model Stacking Mamba-1 Layers"**, a fully SSM-based model stacking Mamba-1 layers, has matched and even outperformed Transformers on key tasks. Falcon3-Mamba extends this by pretraining for an additional 1.5 trillion tokens, incorporating high-quality post-training data, and expanding the context length from 8K to 32K tokens, further enhancing its capabilities.
However, despite these advances, SSM-based models still underperform Transformers on algorithmic tasks \_\_**Srivastava et al., "SSM-Based Models for Algorithmic Tasks"**.

To balance these trade-offs, hybrid models combining attention and SSMs have gained interest for leveraging the strengths of both architectures.
Examples include RecurrentGemma \_\_**Bhatia et al., "RecurrentGemma: Integrating Gated Linear Recurrences with Local Attention"**, which integrates gated linear recurrences with local attention, and Zyphraâ€™s Zamba \_\_**Joshi et al., "Zyphra's Zamba: Combining Mamba-1 Layers with Shared Attention Mechanism"**, which combines Mamba-1 layers with a shared attention mechanism spanning the network.
Zamba-2 \_\_**Kumar et al., "Zamba-2: Improving Efficiency with Mamba-2 Blocks and Shared Attention Layers"** builds on this by replacing Mamba-1 blocks with Mamba-2 for improved efficiency, increasing shared attention layers from one to two for enhanced global context modeling, and applying Low-Rank Adaptation (LoRA) \_\_**Chen et al., "Low-Rank Adaptation for Efficient Depth Adjustments"** to shared MLP blocks for parameter-efficient depth adjustments.
Other hybrid architectures \_\_**Lee et al., "Hybrid Architectures for Sequence Modeling"** further underscore the interest in models that balance expressiveness and efficiency.

\paragraph{Distillation.}
Various methods have been proposed to distill large Transformer-based models into more efficient architectures while maintaining performance.
SUPRA \_\_**Wang et al., "SUPRA: Linearizing Softmax Attention for Efficient Models"** propose a procedure to linearize softmax attention into a form of linear attention by copying the weight matrices and fine-tuning.
LoLCATs \_\_**Kulkarni et al., "LoLCATs: A Linearization Approach for Transformers"** introduces a linearization approach that replaces softmax attention with linear attention through attention transfer, followed by low-rank adaptation, enabling the creation of large-scale linearized models with improved efficiency. 
\_\_**Dutta et al., "State-Space Duality for Efficient Model Transfer"** leverages the State-Space Duality (SSD) in \_\_**Patel et al., "State-Space Models for Sequence Modeling"** to transfer the linear projection weights from the attention layers into Mamba-based models. Their experiments include hybrid models with an increasing proportion of interleaved attention layers, demonstrating that a balanced combination of state-space models (SSMs) and attention preserves performance while improving efficiency.
MOHAWK \_\_**Srivastava et al., "MOHAWK: Distilling Transformers into Efficient SSMs"** distills Transformers into SSMs through progressive alignment, allowing subquadratic models to leverage Transformer training resources effectively. These approaches demonstrate the viability of distilling computationally expensive Transformers into efficient models while retaining competitive performance.