\section{Related Work}
\paragraph{Language Models.}
Transformer-based models, such as those in the Llama \citep{llama}, and Qwen \citep{qwen2} series, have shown strong performance across various language modeling tasks. These models underwent extensive pretraining on large-scale corpora and incorporate techniques like instruction tuning and curated datasets to enhance generalization in few-shot and zero-shot settings on various tasks.

While Transformers remain dominant, recent work has explored alternatives to their purely quadratic attention mechanisms to improve efficiency while maintaining strong performance. Structured state space models (SSMs) \citep{mamba1, mamba2} have emerged as a promising direction, offering a distinct approach to sequence modeling.
At large scales, Falcon-Mamba \citep{falcon}, a fully SSM-based model stacking Mamba-1 layers, has matched and even outperformed Transformers on key tasks. Falcon3-Mamba extends this by pretraining for an additional 1.5 trillion tokens, incorporating high-quality post-training data, and expanding the context length from 8K to 32K tokens, further enhancing its capabilities.
However, despite these advances, SSM-based models still underperform Transformers on algorithmic tasks \citep{repeat_after_me,wen2024rnnstransformersyetkey,waleffe2024}.

To balance these trade-offs, hybrid models combining attention and SSMs have gained interest for leveraging the strengths of both architectures.
Examples include RecurrentGemma \citep{recurrentgemma}, which integrates gated linear recurrences with local attention, and Zyphraâ€™s Zamba \citep{zamba}, which combines Mamba-1 layers with a shared attention mechanism spanning the network.
Zamba-2 \citep{zamba2} builds on this by replacing Mamba-1 blocks with Mamba-2 for improved efficiency, increasing shared attention layers from one to two for enhanced global context modeling, and applying Low-Rank Adaptation (LoRA) \citep{lora} to shared MLP blocks for parameter-efficient depth adjustments.
Other hybrid architectures \citep{jamba2024, waleffe2024} further underscore the interest in models that balance expressiveness and efficiency.

\paragraph{Distillation.}
Various methods have been proposed to distill large Transformer-based models into more efficient architectures while maintaining performance.
SUPRA \citep{mercat2024linearizing} propose a procedure to linearize softmax attention into a form of linear attention by copying the weight matrices and fine-tuning.
LoLCATs \citep{lolcats} introduces a linearization approach that replaces softmax attention with linear attention through attention transfer, followed by low-rank adaptation, enabling the creation of large-scale linearized models with improved efficiency. 
\citep{wang2025} leverages the State-Space Duality (SSD) in \citet{mamba2} to transfer the linear projection weights from the attention layers into Mamba-based models. Their experiments include hybrid models with an increasing proportion of interleaved attention layers, demonstrating that a balanced combination of state-space models (SSMs) and attention preserves performance while improving efficiency.
MOHAWK \citep{mohawk} distills Transformers into SSMs through progressive alignment, allowing subquadratic models to leverage Transformer training resources effectively. These approaches demonstrate the viability of distilling computationally expensive Transformers into efficient models while retaining competitive performance.