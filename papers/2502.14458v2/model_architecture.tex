\section{Model Architecture}
\label{model_architecture}

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{assets/discrete_mamba.png}
        \caption{
        The Discrete Mamba-2 block \cite{mohawk} modifies the original Mamba-2 architecture by removing both post-convolution activation and pre-output projection normalization. Additionally, the Discrete Mamba-2 sequence mixer eliminates the $\Delta$ discretization parameter and directly projects the $\mathbf{A}$ matrix from the input.
        }
        \label{fig:discrete_mamba}
    \end{subfigure}
    \hspace{0.1\linewidth} % Adjust the spacing
    \begin{subfigure}[b]{0.4\linewidth}
        \centering
        \includegraphics[width=0.62\linewidth]{assets/llamba_architecture.png}
        \caption{Llamba models—Llamba-1B, Llamba-3B, and Llamba-8B—are based on the architecture of their Llama teacher models. Each block comprises two sub-blocks with residual connections:
        (1) RMS Normalization followed by a Discrete Mamba-2 layer.
        (2) RMS Normalization followed by a feed-forward layer.
        }
        \label{fig:llamba_architecture}
    \end{subfigure}
    \caption{Comparison of the Discrete Mamba-2 block and the Llamba architecture.}
    \label{fig:comparison}
\end{figure}

Unlike the Mamba and Mamba-2 architectures, which were designed for training from scratch, \textit{Llamba is directly motivated by architectural distillation}.
In particular, the Mohawk distillation framework involves aligning sub-networks of the model at various levels of granularity (\Cref{sec:distillation}). 
This constraints Llamba to retain the overall architecture of the teacher model, ideally modifying only the attention matrix mixer by replacing it with a subquadratic alternative.

The Llamba models—Llamba-1B, Llamba-3B, and Llamba-8B—comprise 16, 28, and 32 residual Mamba-2 blocks, respectively, followed by feed-forward layers. These models share the tokenizer and vocabulary of Llama-3.1, with hidden dimensions of 2048 for Llamba-1B, 3072 for Llamba-3B, and 4096 for Llamba-8B. 
In addition, Llamba differs from the original Mamba-2 architecture \citep{mamba2} in the following ways (see \Cref{fig:llamba_architecture}):
\begin{itemize}[leftmargin=*]

\item \textbf{Alternating MLP blocks}:
Llamba interleaves Llama’s Gated MLP components between each Mamba-2 mixing layer, unlike Mamba-1 and Mamba-2, which consist solely of SSM blocks.

\item \textbf{Multi-head structure}: Llama-3.x models use grouped-query attention (GQA) \citep{gqa, mqa}, which employs 32 query heads and 8 key-value heads to boost inference speed and reduce the size of the decoding cache. However, Mamba’s recurrent layers don’t rely on a cache, so these optimizations aren’t needed. Instead, \textit{Llamba blocks feature a Multi-Head variant} of Mamba-2 with 32 heads and dimensions of 64, 96, or 128, along with a state size of 64. While this design differs from Mamba-2’s ``multi-value attention'' (MVA) architecture, it still keeps inference costs low.

\item \textbf{Non-linearities}: We remove the normalization before the output projection and the activation after convolution, as these are non-linear operations that do not exist in the attention block and hurts alignment (See \Cref{subsec:mohawk}). 

\item \textbf{Discretization}: Llamba uses \textit{Discrete-Mamba-2}, a variant that projects the matrix $\mathbf{A}$ directly from the input, eliminating the discretization parameters $\Delta$ to better match the inherently discrete attention mechanisms. 
\end{itemize}

Notably, these changes not only facilitate the distillation process but also improve training efficiency. Alternating with MLPs \textbf{reduces the number of temporal mixing layers}, enabling Llamba to achieve faster computation than other models of comparable size (see \Cref{subsec:throughput}). Furthermore, training becomes simpler and more efficient by eliminating normalization-related all-reduce operations.
