[
  {
    "index": 0,
    "papers": [
      {
        "key": "llama",
        "author": "Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\u00e9e Lacroix and Baptiste Rozi\u00e8re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "qwen2",
        "author": "An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan",
        "title": "Qwen2 Technical Report"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "mamba1",
        "author": "Albert Gu and Tri Dao",
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      },
      {
        "key": "mamba2",
        "author": "Dao, Tri and Gu, Albert",
        "title": "Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "falcon",
        "author": "Jingwei Zuo and Maksim Velikanov and Dhia Eddine Rhaiem and Ilyas Chahed and Younes Belkada and Guillaume Kunsch and Hakim Hacid",
        "title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "repeat_after_me",
        "author": "Samy Jelassi and David Brandfonbrener and Sham M. Kakade and Eran Malach",
        "title": "Repeat After Me: Transformers are Better than State Space Models at Copying"
      },
      {
        "key": "wen2024rnnstransformersyetkey",
        "author": "Kaiyue Wen and Xingyu Dang and Kaifeng Lyu",
        "title": "RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval"
      },
      {
        "key": "waleffe2024",
        "author": "Roger Waleffe and Wonmin Byeon and Duncan Riach and Brandon Norick and Vijay Korthikanti and Tri Dao and Albert Gu and Ali Hatamizadeh and Sudhakar Singh and Deepak Narayanan and Garvit Kulshreshtha and Vartika Singh and Jared Casper and Jan Kautz and Mohammad Shoeybi and Bryan Catanzaro",
        "title": "An Empirical Study of Mamba-based Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "recurrentgemma",
        "author": "Aleksandar Botev and Soham De and Samuel L Smith and Anushan Fernando and George-Cristian Muraru and Ruba Haroun and Leonard Berrada and Razvan Pascanu and Pier Giuseppe Sessa and Robert Dadashi and L\u00e9onard Hussenot and Johan Ferret and Sertan Girgin and Olivier Bachem and Alek Andreev and Kathleen Kenealy and Thomas Mesnard and Cassidy Hardin and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivi\u00e8re and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Armand Joulin and Noah Fiedel and Evan Senter and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and David Budden and Arnaud Doucet and Sharad Vikram and Adam Paszke and Trevor Gale and Sebastian Borgeaud and Charlie Chen and Andy Brock and Antonia Paterson and Jenny Brennan and Meg Risdal and Raj Gundluru and Nesh Devanathan and Paul Mooney and Nilay Chauhan and Phil Culliton and Luiz Gustavo Martins and Elisa Bandy and David Huntsperger and Glenn Cameron and Arthur Zucker and Tris Warkentin and Ludovic Peran and Minh Giang and Zoubin Ghahramani and Cl\u00e9ment Farabet and Koray Kavukcuoglu and Demis Hassabis and Raia Hadsell and Yee Whye Teh and Nando de Frietas",
        "title": "RecurrentGemma: Moving Past Transformers for Efficient Open Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zamba",
        "author": "Paolo Glorioso and Quentin Anthony and Yury Tokpanov and James Whittington and Jonathan Pilault and Adam Ibrahim and Beren Millidge",
        "title": "Zamba: A Compact 7B SSM Hybrid Model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zamba2",
        "author": "Paolo Glorioso and Quentin Anthony and Yury Tokpanov and Anna Golubeva and Vasudev Shyam and James Whittington and Jonathan Pilault and Beren Millidge",
        "title": "The Zamba2 Suite: Technical Report"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lora",
        "author": "Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "jamba2024",
        "author": "Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham",
        "title": "Jamba: A Hybrid Transformer-Mamba Language Model"
      },
      {
        "key": "waleffe2024",
        "author": "Roger Waleffe and Wonmin Byeon and Duncan Riach and Brandon Norick and Vijay Korthikanti and Tri Dao and Albert Gu and Ali Hatamizadeh and Sudhakar Singh and Deepak Narayanan and Garvit Kulshreshtha and Vartika Singh and Jared Casper and Jan Kautz and Mohammad Shoeybi and Bryan Catanzaro",
        "title": "An Empirical Study of Mamba-based Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "mercat2024linearizing",
        "author": "Jean Mercat and Igor Vasiljevic and Sedrick Keh and Kushal Arora and Achal Dave and Adrien Gaidon and Thomas Kollar",
        "title": "Linearizing Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lolcats",
        "author": "Michael Zhang and Simran Arora and Rahul Chalamala and Alan Wu and Benjamin Spector and Aaryan Singhal and Krithik Ramesh and Christopher R\u00e9",
        "title": "LoLCATs: On Low-Rank Linearizing of Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang2025",
        "author": "Junxiong Wang and Daniele Paliotta and Avner May and Alexander M. Rush and Tri Dao",
        "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "mamba2",
        "author": "Dao, Tri and Gu, Albert",
        "title": "Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "mohawk",
        "author": "Aviv Bick and Kevin Y. Li and Eric P. Xing and J. Zico Kolter and Albert Gu",
        "title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models"
      }
    ]
  }
]