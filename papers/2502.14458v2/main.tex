
\documentclass{article} % For LaTeX2e

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% Added:
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{multirow} 
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{float}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{times}

\setlength{\textwidth}{6.8in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-0.5in}
\setlength{\parskip}{6pt}%
\setlength{\parindent}{0pt}%

\title{Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing}

\usepackage{authblk}

\author[$^{1}$]{Aviv Bick\thanks{Work was done while at Cartesia AI.}}
\author[$^2$]{Tobias Katsch}
\author[$^2$]{Nimit Sohoni}
\author[$^2$]{Arjun Desai}
\author[$^{12}$]{Albert Gu}

\affil[$^1$]{Carnegie Mellon University}
\affil[$^2$]{Cartesia.ai}

\affil[ ]{{\texttt{abick@cs.cmu.edu}}}

\date{}


\begin{document}


\maketitle

\begin{abstract}
We introduce Llamba, a family of efficient recurrent language models distilled from Llama-3.x into the Mamba architecture.
The series includes Llamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput and handle significantly larger batch sizes than Transformer-based models, while maintaining comparable benchmark performance.
Furthermore, Llamba demonstrates the effectiveness of cross-architecture distillation using MOHAWK \citep{mohawk}, achieving these results with less than 0.1\% of the training data typically used for models of similar size.
To take full advantage of their efficiency, we provide an optimized implementation of Llamba for resource-constrained devices such as smartphones and edge platforms, offering a practical and memory-efficient alternative to Transformers.
Overall, Llamba improves the tradeoff between speed, memory efficiency, and performance, making high-quality language models more accessible.
\end{abstract}


\input{introduction}
\input{related_work}
\input{model_architecture}
\input{distillation}
\input{on_device}
\input{ablations}
\input{evaluation}
\input{conclusion}

\bibliography{main}
\bibliographystyle{plainnat}

\appendix
\input{appendix}

\end{document}
