\section{Evaluations}

\subsection{Performance}

\paragraph{Comparison Against Pretrained Models.}
\label{subsec:performance}
\Cref{tab:performance_comparison_1} presents a comparative analysis of downstream evaluation results across different models and tasks. The evaluation includes recent advanced models such as hybrids of subquadratic and attention layers (e.g., Zamba2-7B \citep{zamba2}) and purely subquadratic models (e.g., RecurrentGemma-9B \citep{recurrentgemma}, Falcon-Mamba-7B \citep{falcon}). Additionally, we include Llama-3.2-1B, Llama-3.2-3B, and Llama-3.1-8B to benchmark performance against state-of-the-art non-hybrid Transformer models.

We evaluate the models’ performance in both zero-shot and few-shot settings across a range of standard datasets: ARC \citep{arc}, PIQA \citep{piqa}, Winogrande (WG) \citep{winogrande}, HellaSwag (HS) \citep{hellaswag}, Lambada OpenAI (LO) \citep{lambada}, MMLU \citep{mmlu}, and OpenBookQA (OBQA) \citep{OpenBookQA}. 
All evaluations were conducted using \texttt{bfloat16} precision and the \textit{lm-eval-harness v0.4.4} Python library \citep{eval-harness}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{assets/throughput_table.png}
    \caption{
    Tokens processed at different batch sizes across various models. All models were compiled using \texttt{torch.compile(model, fullgraph=True)} with CUDA graph compilation. We evaluated three settings:
    (1) Llamba-8B with \texttt{gen\_len=8192},
    (2) Llama-3.1-8B with \texttt{gen\_len=2048}, and
    (3) Llama-3.1-8B with \texttt{gen\_len=8192}.
    Each was tested with \texttt{prompt\_len=1} and batch sizes ranging from 8 to 2048. The results show that Llamba-8B achieves the highest throughput, particularly at larger batch sizes, where Transformers either slow down or run out of memory (OOM).
    }
    \label{fig:throughput_over_batch_size}
\end{figure}

\paragraph{Comparison Against Distilled Models.}
\Cref{tab:comparison_against_distillation_methods} compares Llamba-8B with other distilled models of similar size.
We specifically focus on MMLU, which is known to be difficult for recurrent models \citep{waleffe2024}, and has the biggest gap for distilled models from prior work. Llamba significantly improves MMLU relative to the teacher model.

We found that MMLU performance takes much longer to improve compared to other benchmarks in our end-to-end distillation. Llamba reached teacher performance on other tasks in a very small number of tokens, while MMLU took longer to improve. 

We also note that some of the baselines are actually hybrid models, which have a 1:1 ratio of attention to recurrent layers. We note that even sliding window attention has a strong effect because the MMLU context size is very small. 
Although Llamba still has a gap to the teacher model, we consider this result a large step forward for the performance of distilled recurrent models.

% - most benchmarks use large window.


\begin{table}[ht]
    \centering
    \begin{tabular}{l l c l c c}
        \toprule
        \textsc{Model} & \textsc{Arch.} & \textsc{Tokens (B)} & \textsc{Teacher} & \textsc{MMLU Score} & \textsc{Relative Score (\%)} \\
        \midrule
        % \textsc{Mistral 7B} & T & - & - & 62.4 & 100 \\
        \textsc{SUPRA} & R & 100 & Mistral 7B & 34.2 & 24.6 \\
        % \textsc{LoLCATs} & H & Mistral 7B & 0.04 & 51.4 & 70.5 \\
        % \midrule
        % \textsc{Llama 3 8B} & T & - & - & 66.6 & 100 \\
        \textsc{Mamba2-Llama 3} & R & 20 & Llama 3 8B & 43.2 & 43.8 \\
        % \textsc{Mamba2-Llama 3 (Hybrid)} & H & Llama 3 8B & 20 & 56.7 & 76.2 \\
        \textsc{Mamba2-Llama 3} & H & 20 & Llama 3 8B & 56.7 & 76.2 \\
        \textsc{LoLCATs} & H & 0.04 & Mistral 7B & 51.4 & 70.5 \\
        \textsc{LoLCATs} & H & 0.04 & Llama 3 8B & 52.8 & 66.8 \\
        % \midrule
        % Llama 3.1 8B & T & - & - & 68.4 & 100 \\
        \textbf{\textsc{Llamba-8B}} & R & 12 & Llama 3.1 8B & \textbf{60.0} & \textbf{80.6} \\
        \bottomrule
    \end{tabular}
    \caption{MMLU performance of different models along with their respective architectures, training tokens, and teachers.
    Along with accuracy, each model is annotated with the number of training or distillation tokens (in trillions) and its architecture—Recurrent (R)
    % Transformer (T),
    or Hybrid (H).
    The relative score indicates the MMLU score relative to the teacher's score where the lower threshold is taken    
    to be 25 (random guessing). 
    }
\label{tab:comparison_against_distillation_methods}
\end{table}
    
\subsection{Throughput}
\label{subsec:throughput}
Llamba achieves higher throughput than its Llama-3.1-8B teacher (see \Cref{fig:throughput_over_batch_size}), even when Llama-3.1-8B generates four times fewer tokens.
This performance gain stems from Llamba’s recurrent Mamba-2 layers, whose state size remains constant regardless of sequence length.
Additionally, Llamba incorporates MLPs with fewer temporal mixing layers than \citet{mamba2}, enabling it to:
(1) scale to batches twice as large as a pure Mamba-2 model, as MLPs are stateless in time, and
(2) reduce kernel launch overhead when CUDA graph optimization is not applied, since Mamba layers typically require more kernel preparation time.

We have evaluated the throughput of Llama-3.1-8B and Llamba-8B models on a single NVIDIA H100 80GB HBM3. To ensure a fair comparison, all models were tested under a reasonable level of optimization, using \texttt{torch.compile(model, fullgraph=True)} and CUDA graph for consistent performance baselines.

Furthermore, on-device evaluation results highlight Llamba’s exceptional performance in decoding scenarios with constrained hardware. Specifically, on Apple Silicon M3 Pro (36GB) using MLX, Llamba maintains consistent high throughput and low memory consumption at 4-bit quantization (see \Cref{fig:on_device_comparison}). In contrast, the inference performance of Llama-3.1-8B deteriorates linearly with increasing context size, emphasizing the superior efficiency of Llamba in handling long sequences.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]
    {assets/mlx-3.png}
    \caption{
    Comparison of on-device decoding throughput and memory consumption between Llamba-8B and Llama-3.1-8B at 4 bit quantization in MLX running on Apple Silicon M3 Pro (36GB). Llamba maintains constant high throughput and low memory consumption while the inference performance of Llama drops linearly with increasing context size.
    }
    \label{fig:on_device_comparison}
\end{figure}