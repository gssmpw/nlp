\section{Introduction}

Transformer-based LLMs dominate language modeling, but their quadratic attention mechanism makes them computationally expensive and difficult to scale efficiently.
This technical paper introduces the \textbf{Llamba model family}, a suite of SSM-based language models—including Llamba-1B, Llamba-3B, and Llamba-8B—that address these efficiency challenges.
Retaining the overall structure of Llama models, Llamba models are distilled from Llama-3, replacing self-attention with Mamba-2 layers to achieve high inference throughput while maintaining strong performance across benchmarks.

Llamba achieves its performance with drastically reduced training data requirements through \emph{architecture distillation}.
Unlike traditional large language models (LLMs) that rely on massive datasets spanning trillions of tokens, Llamba achieves comparable results with significantly fewer resources by using MOHAWK \citep{mohawk} to transfer the knowledge from strong pretrained Transformer-based LLMs 
to a Mamba-based architecture.
For example, \textit{Llamba-3.1-8B was distilled using just 12 billion tokens—less than 0.1\% of the training data required for Llama-3.1-8B}.
This remarkable data efficiency highlights the effectiveness of architecture distillation methods
in transferring knowledge from pretrained models while significantly reducing both data and computational demands. As a result, Llamba presents a scalable and cost-effective solution for high-performance language modeling.
% (in particular, Llama-3) 

Extending the benefits of their efficient design, \textbf{we provide on-device implementations of the Llamba models}
\footnote{https://github.com/cartesia-ai/edge}, optimized for deployment on private devices such as smartphones and edge computing platforms with limited memory and computational resources. 
These implementations highlight the advantages of linear models, such as the Llamba family, by delivering high-quality language modeling on devices where traditional Transformer architectures are often impractical due to their heavy memory and compute demands.

Overall, the Llamba family introduces several key contributions:
\begin{itemize}[leftmargin=*]
% provide recipe for large scale distillation.
    \item \textbf{Distillation efficiency:} Using the MOHAWK framework, Llamba achieves state-of-the-art performance with less than 0.1\% of the training data required by comparable models. This represents a significant advancement in data and compute efficiency for LLMs.
    \item \textbf{On-device deployment:} We provide quantized Llamba models, along with an MLX implementation for edge devices like iPhones and MacBooks, which demonstrate near-constant memory usage, making them ideal for resource-constrained environments.
    \item \textbf{Benchmark performance:} Llamba-1B, Llamba-3B, and Llamba-8B perform on par with traditional models across a wide range of benchmarks, setting a new standard for efficiency and performance in recurrent architectures.
\end{itemize}

These advancements position Llamba as a versatile and scalable solution for efficient language modeling, bridging the gap between performance, resource efficiency, and accessibility.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]
    {assets/budget_plot.png}
    \caption{
    Average accuracy is measured over multiple benchmarks, including ARC Challenge, ARC Easy, PIQA, Winogrande, HELLASWAG, OpenBookQA, and MMLU, providing a comprehensive assessment of a model's Common Sense and Language Understanding.
    }
    \label{fig:tokens_budget}
\end{figure}

\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{3.1pt}
\caption{
Comparison of downstream performance (accuracy \%) in zero-shot settings across various models. 
We use the following abbreviations: 
ARC Challenge (ARC-C), 
ARC Easy (ARC-E), 
Physical Interaction QA (PIQA), 
Winogrande (WG), 
HellaSwag (HS), 
Lambada (LMB), 
Massive Multitask Language Understanding (MMLU), 
and OpenBookQA (OBQA). 
For ARC-C, ARC-E, PIQA, HellaSwag, and OBQA, we use normalized logits' results. 
Averages are computed over all 8 tasks. 
Along with accuracy, each model is annotated with the number of training or distillation tokens (in trillions) and its architecture—Recurrent (R), Transformer (T), or Hybrid (H). For models with a sliding window, the window size is also specified.
This table shows that all Llamba models achieve competitive performance against both Transformer-based and SSM-based models, despite their recurrent layers being trained on significantly fewer tokens.
We use an instruct-tuned version whenever one is available; however, we exclude this label for brevity.
See \Cref{tab:performance_comparison_1,tab:performance_comparison_2} for the full few-shot comparisons.
}
\label{tab:performance_comparison}
\begin{tabular}{l c c c c c c c c c c c}
\toprule
\textsc{Model} & 
% \textsc{Architecture} & 
\textsc{Arch.} & 
\textsc{Tokens (T)} & \textsc{ARC-C} & 
\textsc{ARC-E} & \textsc{PIQA}  & 
\textsc{WG} & \textsc{HS} & 
\textsc{LMB} & \textsc{MMLU} & 
\textsc{OBQA} & \textbf{AVG} \\

\midrule

\textsc{Llama-3.2-1B (Teacher)} & T
& $\le$ 9   & 38.1 & 68.5 & 74.4 & 59.7 & 60.8 & 60.1 & 46.0 & 34.6 & 55.3 \\

\textbf{\textsc{Llamba-1B}} & R
& 0.008     & 37.2 & 69.5 & 74.0 & 60.6 & 61.2 & 48.4 & 38.0 & 37.0 & 53.2 \\

\textsc{RecurrentGemma-2B} & H ($w=2048$)
& $\le$ 2   & 35.6 & 51.2 & 67.2 & 55.7 & 60.3 & 52.5 & 40.2 & 30.4 & 49.1 \\

\midrule

\textsc{Qwen2.5-3B} & T
& $\le$ 18  & 48.1 & 72.9 & 78.3 & 69.8 & 74.9 & 65.8 & 65.5 & 41.8 & 64.6 \\

\textsc{Llama-3.2-3B (Teacher)} & T
& 9         & 45.6 & 74.3 & 75.8 & 67.6 & 70.4 & 65.9 & 60.4 & 35.8 & 61.9 \\

\textbf{\textsc{Llamba-3B}} & R
& 0.01      & 48.5 & 79.0 & 78.6 & 70.4 & 73.8 & 65.8 & 52.7 & 42.8 & 63.9 \\

\textsc{Mamba2-2.8B} & R
& 0.3       & 35.9 & 64.3 & 75.6 & 63.4 & 66.2 & 68.1 & 25.7 & 40.4 & 54.9 \\

\midrule

\textsc{Qwen2.5-7B} & T
& 18        & 55.1 & 81.3 & 80.3 & 71.1 & 80.5 & 69.5 & 71.7 & 48.6 & 69.8 \\

\textsc{Llama-3.1-8B (Teacher)} & T
& 15        & 55.1 & 81.7 & 81.1 & 73.9 & 79.3 & 73.0 & 68.0 & 43.0 & 69.4 \\

\textbf{\textsc{Llamba-8B}} & R
& 0.012     & 54.6 & 82.5 & 80.9 & 73.3 & 77.6 & 69.4 & 61.0 & 43.4 & 68.8 \\

\textsc{Falcon3-Mamba-7B} & R
& 7.3       & 53.2 & 72.5 & 79.7 & 69.1 & 79.8 & 67.5 & 65.0 & 48.0 & 67.0 \\

\textsc{Zamba2-7B} & H
& 2.1       & 56.1 & 80.6 & 81.1 & 76.9 & 81.5 & 74.6 & 64.7 & 45.2 & 70.1 \\

\textsc{RecurrentGemma-9B} & H ($w=2048$)
& 2         & 57.1 & 78.9 & 80.6 & 73.7 & 80.1 & 54.1 & 55.1 & 46.0 & 65.7 \\

\bottomrule
\end{tabular}
\end{table}