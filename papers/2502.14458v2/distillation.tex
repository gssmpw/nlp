\section{Distillation}
\label{sec:distillation}


The Llamba models were distilled using MOHAWK \citep{mohawk} from Meta’s Llama-3.x family \citep{llama}. Specifically, Llamba-3.1-1B was distilled from Llama-3.2-1B-Instruct, Llamba-3B from Llama-3.2-3B-Instruct, and Llamba-8B from Llama-3.1-8B-Instruct.


\subsection{MOHAWK}
\label{subsec:mohawk}

Following the MOHAWK framework \citep{mohawk}, Llamba models were initialized by setting the convolution layer of the Mamba block to an identity kernel (nullifying its effect) and configuring the multiplicative skip connection to directly pass the input unchanged, effectively initializing the block as an identity function.
Subsequently, three key steps were implemented: \textit{Matrix Orientation}, \textit{Hidden-State Alignment}, \textit{Weight Transfer with Knowledge Distillation}.


\paragraph{Matrix Orientation.}
This phase is used to align the student and teacher matrix mixers. Specifically, we minimize the distance between the materialized Llamba matrix mixer and Llama’s self-attention matrix. Notably, Llama uses an MQA architecture, which results in 32 attention heads with shared weights. Since Llamba’s 32 heads are not tied (it uses a Multi-Head architecture), it learns independent weights, unlike the dependent matrices of its teacher.

\paragraph{Hidden-State Alignment.}
For Hidden-State Alignment, each Mamba-2 block of the Llamba model was aligned independently using the L2 distance, guided by the output of the preceding layer.

\paragraph{Weight Transfer \& Knowledge Distillation.}
We begin this stage by transferring the MLP weights, normalization layers, input embedding, and output head from the Llama-3.x models to each Llamba model.
Unlike previous works \citep{wang2024mamballamadistillingaccelerating,mohawk}, we did not freeze the MLP components and optimized them using the same learning rate of the mixing components.
During Knowledge Distillation, each Llamba model was aligned with the respective teacher model using the cross-entropy loss of their logits. After this phase's loss saturation, all models were further distilled from Llama-3.1-70B-Instruct for their remaining tokens.

\subsection{Training Details}
\label{subsec:training_details}


\begin{table}[!t]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
  & \textsc{Stage 1} & \textsc{Stage 2} & \textsc{Stage 3} & \textsc{Overall Tokens} \\
  \midrule
\textsc{Llamba-1B} & 300M & 2.7B & 5B & 8B \\
\textsc{Llamba-3B} & 500M & 4B & 5.5B & 10B \\
\textsc{Llamba-8B} & 500M & 5B & 6.5B & 12B \\
\bottomrule
\end{tabular}
\caption{
Token allocations during the distillation process for different Llamba models and MOHAWK stages (Matrix Orientation, Hidden-State Alignment, and Knowledge distillation).
}
\label{tab:stages_tokens_budget}
\end{table}


The Llamba models were trained using mixed precision and Fully Sharded Data Parallel (FSDP) on a single node with 8 H100 GPUs, with activation checkpointing enabled.
Training used the AdamW optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.95$, and a weight decay of 0.1. The maximum learning rates were $1 \times 10^{-4}$ for the first two MOHAWK stages across all models, $5 \times 10^{-5}$ for the third stage of Llamba-1B and Llamba-3B, and $1 \times 10^{-5}$ for the third stage of Llamba-8B. Batch sizes were set to $64$ in the first MOHAWK stage and $128$ in the second and third stages.
We used the Warm-Stable-Decay (WSD) scheduler \citep{minicpm}, with a minimum learning rate of $1 \times 10^{-8}$ and warm-up and decay phases each spanning 10\% of total training steps.

During the distillation process \Cref{tab:stages_tokens_budget}, a total of $12$ billion tokens were processed for Llamba-8B, $10$ billion tokens for Llamba-3B and Llamba-1B used only $8$ billion tokens, highlighting its significantly smaller allocation of training data used for distillation compared to training without any teacher supervision (see \Cref{fig:tokens_budget}).

\subsection{Data}
\label{mohawk:data}

Data quality is critical for accurately modeling temporal interactions in the MOHAWK distillation setting. MOHAWK transfers only the MLP weights that affect the hidden dimensions, excluding the sequence mixer weights related to the time dimension. This omission limits the ability to capture time-dependent information directly.
For the distillation process, two datasets were used. The first, fineweb-edu-4.0, is derived from fineweb-edu \citep{fineweb}, itself a subset of the broader fineweb dataset. This refined subset includes only highly educational web pages, filtered using a 4.0 classifier score threshold - higher than the 3.0 threshold used for fineweb-edu. Since distillation requires relatively few tokens, this focused approach was practical and effective.


The \textit{Matrix Orientation} and \textit{Hidden-State Alignment} processes were conducted using the fineweb-edu-4.0 dataset with packed sequences of length 2048 (see \Cref{tab:stages_tokens_budget} for more details). In contrast, \textit{Knowledge Distillation} was initially performed using fineweb-edu-4.0, and subsequently, the Open-Hermes-2.5 dataset was employed for an additional 4 epochs, processing 200 million tokens per epoch with sequences of length 4096. The combination of these datasets played a pivotal role in enhancing the MMLU score.

Our results demonstrate that this dataset selection significantly improves the performance of MMLU \citep{mmlu}. As shown in \Cref{fig:data_comparison}, while the C4 \citep{C4} and fineweb datasets achieve similar scores across most benchmarks, fineweb-edu drives a marked improvement in MMLU. Beyond this, our approach highlights an important takeaway: \textit{we achieve strong results using only established open-source datasets, in contrast to many alternative models that rely on highly curated proprietary datasets}. This demonstrates the feasibility of leveraging openly available resources for high-quality model performance.

Furthermore, we emphasize that architecture distillation (e.g. the MOHAWK framework) and data curation are orthogonal and synergistic,
and we hypothesize that our distillation results could be improved further by incorporating even higher-quality datasets.



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\linewidth]
    {assets/mmlu_performance.png}
    \includegraphics[width=0.49\linewidth]
    {assets/benchmark_performance.png}
    \caption{
    An evaluation of Llamba-8B's knowledge distillation step (MOHAWK's stage 3) across three datasets: C4, fineweb, and fineweb-edu. Each model underwent hidden-state alignment (MOHAWK's stage 2) on its respective dataset using 4 billion tokens and subsequently underwent testing with knowledge distillation on 1 billion tokens. It is observed that although all datasets yield similar outcomes across most benchmarks, MMLU shows notable improvement when utilizing fineweb-edu, unlike with fineweb and C4.
    }
    \label{fig:data_comparison}
\end{figure}