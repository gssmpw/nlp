\section{On-Device Implementation}

The advantages of sub-quadratic language models are particularly impactful in compute- and memory-constrained environments, making them ideal for on-device applications. To support efficient inference, we implemented optimized Mamba-2 kernels, including state-space model and Conv1D layers, using Appleâ€™s Metal framework. These kernels are specifically designed for Apple Silicon, leveraging its GPU parallelism and unified memory architecture for efficient execution.

Our implementation integrates seamlessly with MLX \citep{mlx2023}, a machine learning framework optimized for Apple Silicon. MLX enables dynamic graph construction and efficient tensor operations while utilizing unified memory to minimize data transfer overhead. Additionally, we support 4-bit quantization to further reduce memory usage, enabling models to run effectively on devices with limited resources.

These optimizations allow our models to maintain high throughput and low memory consumption, even in long-context scenarios, making them highly suitable for real-time, on-device applications.
The implementation is available in the released repository \url{https://github.com/cartesia-ai/edge}.