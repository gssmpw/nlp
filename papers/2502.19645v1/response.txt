\section{Related Work}
\label{sec:related_work}

Prior works have leveraged language and vision foundation models to enhance robotic capabilities, utilizing them as pretrained visual representations that accelerate robotic policy learning**Dosovitskiy et al., "ViT: Vision Transformers for Computer Vision"**, for object localization in robotics tasks**Devineau et al., "Visual Grounding with Language"**, and for high-level planning and reasoning ____*. More recently, researchers have explored fine-tuning vision-language models (VLMs) to directly predict low-level robotic control actions, producing ``vision-language-action'' models (VLAs)**Kamath et al., "VLAD: Vision-Language-Action Transformers"**, which have demonstrated effective generalization to out-of-distribution test conditions and unseen semantic concepts. These works focus primarily on model development, while we focus on developing a recipe for fine-tuning such models, justifying individual design decisions with insights that we gain from our empirical analysis.

Despite the importance of fine-tuning for real-world VLA deployment, empirical analysis of effective fine-tuning recipes remains limited. While **Sun et al., "LoRA: Low-Rank Adaptation"** study various parameter update strategies and from their findings show that LoRA fine-tuning enables effective adaptation to single-arm robots operating at low control frequencies ($<10$ Hz), their analysis does not extend to bimanual robots with high control frequencies (25-50+ Hz), a more complex control scenario. We address this gap by exploring VLA adaptation design decisions for fast inference and reliable task execution on a real-world bimanual manipulator with a 25 Hz controller.

Recent works by **Li et al., "ACT: Action Tokenization"** and **Shang et al., "DCT-TT: Discrete Cosine Transform-based Tokenization"** improve VLA efficiency through new action tokenization schemes, using vector quantization or discrete cosine transform-based compression to represent action chunks (sequences of actions) with fewer tokens than simple per-dimension binning (as used in RT-2 **Srinivasan et al., "RT-2: Reinforcement Transformer"** and OpenVLA **Mao et al., "OpenVLA: Open-Source Vision-Language Action Model"**). While these approaches achieve 2 to 13$\times$ speedups for autoregressive VLAs, we explore design decisions beyond autoregressive modeling, which remains inherently limited by iterative generation. Our parallel decoding approach, when paired with action chunking, achieves significantly greater speedups: 26$\times$ to 43$\times$ throughput with much lower latency (0.07 ms for single-arm tasks with one input image and 0.321 ms for bimanual tasks with three input images).

Another line of research **Hoffman et al., "DVAE: Diffusion-based Vision-Language Action Model"** demonstrates effective VLA fine-tuning for high-frequency, bimanual manipulation using generative approaches like diffusion or flow matching. While these diffusion-based VLAs achieve higher action throughput than autoregressive VLAs by generating multi-timestep action chunks simultaneously, they introduce computational trade-offs through slower training and multiple denoising or integration steps at inference time. Furthermore, these diffusion VLAs vary considerably in architecture, learning algorithm, vision-language fusion approach, and input-output specifications---and which design elements most significantly impact performance remains unclear. Through controlled experiments, we show that policies fine-tuned with a simpler L1 regression objective can match more complex approaches in task performance while achieving significantly greater inference efficiency.