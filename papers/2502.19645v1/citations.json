[
  {
    "index": 0,
    "papers": [
      {
        "key": "ma2022vip",
        "author": "Ma, Yecheng Jason and Sodhani, Shagun and Jayaraman, Dinesh and Bastani, Osbert and Kumar, Vikash and Zhang, Amy",
        "title": "Vip: Towards universal visual reward and representation via value-implicit pre-training"
      },
      {
        "key": "nair2022r3m",
        "author": "Nair, Suraj and Rajeswaran, Aravind and Kumar, Vikash and Finn, Chelsea and Gupta, Abhinav",
        "title": "R3m: A universal visual representation for robot manipulation"
      },
      {
        "key": "ma2023liv",
        "author": "Ma, Yecheng Jason and Kumar, Vikash and Zhang, Amy and Bastani, Osbert and Jayaraman, Dinesh",
        "title": "Liv: Language-image representations and rewards for robotic control"
      },
      {
        "key": "Karamcheti2023LanguageDrivenRL",
        "author": "Siddharth Karamcheti and Suraj Nair and Annie S. Chen and Thomas Kollar and Chelsea Finn and Dorsa Sadigh and Percy Liang",
        "title": "Language-Driven Representation Learning for Robotics"
      },
      {
        "key": "majumdar2023we",
        "author": "Majumdar, Arjun and Yadav, Karmesh and Arnaud, Sergio and Ma, Jason and Chen, Claire and Silwal, Sneha and Jain, Aryan and Berges, Vincent-Pierre and Wu, Tingfan and Vakil, Jay and others",
        "title": "Where are we in the search for an artificial visual cortex for embodied intelligence?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "gadre2023cows",
        "author": "Gadre, Samir Yitzhak and Wortsman, Mitchell and Ilharco, Gabriel and Schmidt, Ludwig and Song, Shuran",
        "title": "Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation"
      },
      {
        "key": "stone2023open",
        "author": "Stone, Austin and Xiao, Ted and Lu, Yao and Gopalakrishnan, Keerthana and Lee, Kuang-Huei and Vuong, Quan and Wohlhart, Paul and Kirmani, Sean and Zitkovich, Brianna and Xia, Fei and others",
        "title": "Open-world object manipulation using pre-trained vision-language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ahn2022icanisay",
        "author": "Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil J Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng",
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"
      },
      {
        "key": "huang2022innermonologueembodiedreasoning",
        "author": "Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and Pierre Sermanet and Noah Brown and Tomas Jackson and Linda Luu and Sergey Levine and Karol Hausman and Brian Ichter",
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"
      },
      {
        "key": "singh2022progpromptgeneratingsituatedrobot",
        "author": "Ishika Singh and Valts Blukis and Arsalan Mousavian and Ankit Goyal and Danfei Xu and Jonathan Tremblay and Dieter Fox and Jesse Thomason and Animesh Garg",
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"
      },
      {
        "key": "huang2022languagemodelszeroshotplanners",
        "author": "Wenlong Huang and Pieter Abbeel and Deepak Pathak and Igor Mordatch",
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"
      },
      {
        "key": "song2023llmplannerfewshotgroundedplanning",
        "author": "Chan Hee Song and Jiaman Wu and Clayton Washington and Brian M. Sadler and Wei-Lun Chao and Yu Su",
        "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"
      },
      {
        "key": "huang2023voxposer",
        "author": "Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li",
        "title": "Voxposer: Composable 3d value maps for robotic manipulation with language models"
      },
      {
        "key": "duan2024manipulate",
        "author": "Duan, Jiafei and Yuan, Wentao and Pumacay, Wilbert and Wang, Yi Ru and Ehsani, Kiana and Fox, Dieter and Krishna, Ranjay",
        "title": "Manipulate-anything: Automating real-world robots using vision-language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "brohan2023rt",
        "author": "Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others",
        "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control"
      },
      {
        "key": "o2023open",
        "author": "O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and others",
        "title": "Open x-embodiment: Robotic learning datasets and rt-x models"
      },
      {
        "key": "li2023vision",
        "author": "Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and others",
        "title": "Vision-language foundation models as effective robot imitators"
      },
      {
        "key": "kim2024openvla",
        "author": "Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      },
      {
        "key": "durante2024interactive",
        "author": "Durante, Zane and Sarkar, Bidipta and Gong, Ran and Taori, Rohan and Noda, Yusuke and Tang, Paul and Adeli, Ehsan and Lakshmikanth, Shrinidhi Kowshika and Schulman, Kevin and Milstein, Arnold and others",
        "title": "An interactive agent foundation model"
      },
      {
        "key": "huang2023embodied",
        "author": "Huang, Jiangyong and Yong, Silong and Ma, Xiaojian and Linghu, Xiongkun and Li, Puhao and Wang, Yan and Li, Qing and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan",
        "title": "An Embodied Generalist Agent in 3D World"
      },
      {
        "key": "covariant_ai_2024",
        "author": "Andrew Sohn et al.",
        "title": "Introducing RFM-1: Giving robots human-like reasoning capabilities"
      },
      {
        "key": "wayve_ai_2024",
        "author": "Wayve",
        "title": "LINGO-2: Driving with Natural Language"
      },
      {
        "key": "zhen20243dvla",
        "author": "Zhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang",
        "title": "3D-VLA: 3D Vision-Language-Action Generative World Model"
      },
      {
        "key": "wen2024tinyvla",
        "author": "Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and others",
        "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation"
      },
      {
        "key": "black2024pi_0",
        "author": "Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others",
        "title": "pi0: A Vision-Language-Action Flow Model for General Robot Control"
      },
      {
        "key": "belkhale2024minivla",
        "author": "Suneel Belkhale and Dorsa Sadigh",
        "title": "MiniVLA: A Better VLA with a Smaller Footprint"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "kim2024openvla",
        "author": "Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "belkhale2024minivla",
        "author": "Suneel Belkhale and Dorsa Sadigh",
        "title": "MiniVLA: A Better VLA with a Smaller Footprint"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "pertsch2025fastefficientactiontokenization",
        "author": "Karl Pertsch and Kyle Stachowicz and Brian Ichter and Danny Driess and Suraj Nair and Quan Vuong and Oier Mees and Chelsea Finn and Sergey Levine",
        "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "brohan2023rt",
        "author": "Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others",
        "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kim2024openvla",
        "author": "Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wen2024tinyvla",
        "author": "Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and others",
        "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation"
      },
      {
        "key": "liu2024rdt",
        "author": "Liu, Songming and Wu, Lingxuan and Li, Bangguo and Tan, Hengkai and Chen, Huayu and Wang, Zhengyi and Xu, Ke and Su, Hang and Zhu, Jun",
        "title": "RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation"
      },
      {
        "key": "black2024pi_0",
        "author": "Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others",
        "title": "pi0: A Vision-Language-Action Flow Model for General Robot Control"
      }
    ]
  }
]