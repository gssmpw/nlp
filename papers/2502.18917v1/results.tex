\begin{wrapfigure}{r}{0.5\textwidth}  % 'r' means right alignment
    \centering
    \includegraphics[width=0.5\textwidth]{figure/validation.pdf}  % Slightly smaller to allow for margin
    \caption{Evaluation of \tech generated invariants}
    \label{fig:validation}
\end{wrapfigure}

\subsection{Correctness}
\label{subsec:correctness}

\tech produces \textit{filtered invariants}, which we evaluate using an automated pipeline (Figure~\ref{fig:validation}) to assess their quality against the data structures benchmark described in Section~\ref{sec:benchmark}. The evaluation process begins by instrumenting the source code with the candidate invariants, as detailed in Section~\ref{sec:instrumentation}. 
Unlike the generated tests used in \tech for filtering, the evaluation relies on unit tests containing input-output pairs. A \textit{filtered invariant} is considered correct if it does not report any errors for any tests that successfully compile and run. Notably, these unit tests are not available during the \tech generation phase and are used exclusively for evaluation purposes.

For each passing \textit{filtered invariant}, the authors of the paper conducted a manual review. This examination confirmed that all validated \textit{filtered invariants} are indeed \textbf{correct} and effectively capture properties of the data structures. \eg all 3 properties of \CodeIn{AvlTree} described in Section~\ref{sec:avl_intro} are found in \tech invariants.

\parabf{Benefits of Co-Generation.}
The impact of our invariant-test co-generation approach is evident in Tables~\ref{ds_0shot} and~\ref{ds_refine}. When generating invariants in isolation, \tech produces an average of $25$ unique invariants per benchmark from $8$ completions, with a $77\%$ pass rate against unit tests. However, by incorporating test co-generation, \tech successfully eliminates all \emph{incorrect} invariants, achieving perfect accuracy.

\parabf{Refinement Effectiveness.}
The refinement process further strengthens \tech's capabilities. After a single refinement iteration, the number of \textit{filtered invariants} grows from $17$ to $22$ per example, representing a $29\%$ increase, as shown in Table~\ref{ds_refine}. While this growth in invariant count doesn't necessarily indicate greater completeness (as repaired invariants may be semantically equivalent to existing ones), it demonstrates \tech's ability to transform potentially buggy invariants into valid ones through feedback-guided refinement.

\parabf{Summary.}
Our evaluation demonstrates that \tech's invariant-test co-generation approach significantly outperforms invariant-only generation, improving correctness from $77\%$ to $100\%$. The \textit{filtering tests} prove highly effective at identifying valid invariants, while the refinement process successfully expands the set of correct invariants.


\input{ablation_tables}


\subsection{Completeness}
\label{subsec:completeness}

\begin{table}[h!]
    \centering
    \small
    \caption{\tech Performance Over Baseline for Previously Survived Mutants. The table shows additional mutants killed by \tech compared to the baseline and percentage improvement.}
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Data Structure} & \textbf{Unsolved Baseline (\#(\%))} & \textbf{Add. Solved by \tech (\#)} & \textbf{Improv. (\%)} \\
        \midrule
        binary\_search\_tree & 107(23.67) & 7 & 6.54 \\ 
        hash\_table & 258(37.83) & 38 & 14.73 \\ 
        heap & 108(32.24) & 12 & 11.11 \\ 
        linked\_list & 57(13.54) & 2 & 3.51 \\ 
        red\_black\_tree & 184(27.10) & 9 & 4.89 \\ 
        stack & 67(28.39) & 6 & 8.96 \\ 
        vector & 101(29.79) & 33 & 32.67 \\ 
        avl\_tree & 84(17.57) & 0 & 0.00 \\ 
        queue & 91(26.30) & 11 & 12.09 \\
        \midrule
        \textbf{Total} & \textbf{1057} & \textbf{118} & \textbf{11.16} \\
        \bottomrule
    \end{tabular}
    \label{table:specbot_baseline_comparison}
\end{table}

To evaluate the completeness of \tech invariants, we use a variation of \textit{mutation testing} used in prior work to evaluate the strength of correct specifications~\cite{nl2postcond}. Mutation testing creates copies of the code with small changes. Individual copies are called \textit{mutants}. 
If a mutant fails tests, the mutant is said to be \textit{killed}, otherwise it \textit{survives}.
We expect that many mutations will corrupt data structures, so the number of mutants killed by \tech invariants is an indication of the strength of the invariant, or how complete those invariants are.
Note that we only consider invariants that are correct. 

We use the freeware tool mutate\_cpp~\cite{mutatecpp} to perform program mutation, producing between 236 and 682 mutants per program. There are three possible outcomes for each mutant: it fails to compile, it crashes \shuvendu{Do you mean fails an assertion?}\livia{or any runtime crash \eg seg fault} during execution, or it compiles and runs successfully without errors. 
We focus exclusively on mutants that either compile successfully but crash during execution or survive execution without error, as these are distinguishable cases. Mutants that fail to compile remain consistent across all configurations and therefore do not contribute to meaningful differentiation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figure/test_vs_classinvgen.pdf}  % Adjust the width as needed
    \caption{Completeness Experiment Result. The 3 bars from left to right are Tests, \tech, Tests+\tech. Tests+\tech kills the most mutants.}
    \label{fig:mutation}
\end{figure*}


We conducted experiments to evaluate the completeness of three configurations: unit tests only, \tech only, unit tests with \tech. The last configuration has the strongest test oracles.
For \tech only, we removed all assertions checking the expected output in the unit tests, leaving only the sequence of method calls consistent with the original tests. 
\shuvendu{Say that the last option has the strongest test oracles.}
\shuvendu{Should we name them unit test assertions, \tech assertions, union of unit test and \tech assertions?}


As shown in Table~\ref{table:specbot_baseline_comparison}, \tech's invariants demonstrate error detection capabilities. Compared to unit tests alone, \tech's invariants kill an additional $11.2\%$ of mutants on average, with improvements reaching up to $32.67\%$ for specific tasks. These results suggest that \tech's invariants capture important properties that complement traditional unit testing.
It is important to note that the added value of these invariants is more than the increased number of errors they can catch, because of advantages such as debugging, root-causing, and so on, mentioned in the introduction. Moreover, invariants can identify bugs that unit tests alone might miss, as some mutants may produce correct output for given inputs while having corrupted internal data structures.
\shuvendu{Also note that, unlike \tech, the other assertions are written manually after careful understanding of the class. This illustrates that some mutants can produce correct values even though the data structure corruption was not detected.}\livia{addressed}

Figure~\ref{fig:mutation} indicates tests with \tech kill the most mutants, showing that instrumenting unit tests with \tech invariants enhances their ability to identify buggy implementations. Figure~\ref{fig:inv_addition_kill} shows \CodeIn{HashTable} a mutant that survived tests but killed by \tech invariant \CodeIn{assert(this->\_size > 0 || this->table.empty())}. Figure~\ref{fig:inv_addition_kill_const} shows another example killed by \tech invariant \CodeIn{assert(load\_factor > 0)}.



\begin{figure}[htp]
\centering
\begin{lstlisting}[language=c++, escapechar=!]
 void HashTable::clear_table() {
   this->table.clear();
   this->_num_elements = 0;
!\CodeDelete\textbf{-}!  this->_size = 0;
!\CodeAdd\textbf{+}!  this->_size += 0;
 }
}
\end{lstlisting}
    \caption{Mutant that survived unit tests but killed by \tech}
    \label{fig:inv_addition_kill}
\end{figure}

 \begin{figure}[htp]
\centering
\begin{lstlisting}[language=c++, escapechar=!]
   this->hash_function = hash_function;
   this->_num_elements = 0;
   this->_size = size;
!\CodeDelete\textbf{-}!  this->load_factor = 0.75;
!\CodeAdd\textbf{+}!  this->load_factor = -0.75;
   this->table =
       std::vector<std::shared_ptr<std::vector<std::pair<Key, Value>>>>(size);
 }
\end{lstlisting}
    \caption{Another Mutant that survived unit tests but killed by \tech}
    \label{fig:inv_addition_kill_const}
\end{figure}

However, mutation testing has its limitations. For example, a mutant may be equivalent to the original program (\eg\CodeIn{ n = 0} and \CodeIn{n \&= 0}), resulting in no observable behavioral difference. \shuvendu{In prior work, we grouped mutants by the subset of ground truth tests they failed resulting in distinct and buggy mutants that would be different groups from the reference by this definition.}
Additionally, certain mutants may fall outside the scope of the invariants to detect, such as when only observer methods are altered, which do not affect the program's state directly.

\subsection{Comparison of \tech v.s. Daikon}
\label{subsec:daikon_compare}


We compare \tech with Daikon, the most widely adopted dynamic invariant generator~\cite{ernst2007daikon}. 

We replaced the invariant synthesis component in Figure~\ref{fig:framework} with Daikon, using \textit{filtering tests} \dave{My convention is to italicize terms when first introduced.  Not everyone likes this.  You italize this later, which I would not do. Anyway, the most important thing is consistency.  Your co-authors may have a different convention.} to generate program traces. We then ran Daikonâ€™s invariant detector on these traces with a default confidence limit equal to 0.99 (larger values yield stronger filtering). For the most comprehensive set of invariant candidates, we combined all invariants Daikon generated at each functionâ€™s \CodeIn{ENTER} and \CodeIn{EXIT} points and used \tech invariant instrumentation to identify \textit{filtered invariants} using unit tests to filter. The final output is a set of Daikon invariants that successfully pass unit test execution. On average, each benchmark example has around 5 Daikon invariants; however, in some cases, 0 to 2 of these invariants may be incorrect as shown in Table~\ref{tab:daikon_invariants} examined by humans.

It is important to note that Daikon invariants are not always correct, even with respect to the tests used to infer them. Daikon outputs an invariant if its confidence level exceeds a specified configuration threshold, regardless of whether the invariant holds universally. Through the manual review, we identified 7 incorrect class invariants, as summarized in Table~\ref{table:daikon_incorrect_inv}. These invariants pass unit test validation because both the \textit{filtering tests} and the unit tests coincidentally constructed data structures with similar parameters \shuvendu{What parameters?}, producing similar program traces. Consequently, validation was unsuccessful because these Daikon invariants based on \textit{filtering test} traces also happened to hold true on the unit tests.

For instance, when \CodeIn{Vector} objects are constructed in both \textit{filtering tests} and ground truth unit tests, elements are consistently \CodeIn{push\_back}ed in increasing order. However, users of \CodeIn{Vector} are not restricted to this behavior and could, in practice, choose to \CodeIn{push\_back} larger elements first, breaking this assumed invariant. Therefore, \CodeIn{this-\textgreater data[] sorted by \textless} is not a true class invariant.


\begin{table}[h!]
    \centering
    \caption{Incorrect Daikon Invariants that pass unit tests validation. \shuvendu{Do you mean they overfit the unit tests? But Daikon does not see the unit tests during generation.}}
    \small
    \begin{tabular}{|l|p{10cm}|}
        \hline
        \textbf{Data Structure} & \textbf{Invariant Description} \\
        \hline
        HashTable & \CodeInTable{this-\textgreater\_size one of \{5, 10, 40\}} \\
        \hline
        Heap & \CodeInTable{this->data.\_Vector\_base< int, std::allocator<int> >.
        \_M\_impl.\_Vector\_impl\_data.\_M\_start[] elements >= 1}  \\
        \hline
        Vector & \CodeInTable{this-\textgreater data[] sorted by \textless} \\
        \hline
        Queue & \CodeInTable{this-\textgreater maxSize == 10} \\
        \hline
        LinkedList & \CodeInTable{this-\textgreater tail[].data elements \textgreater= 1} \\
        \hline
        Stack & \CodeInTable{this-\textgreater maxSize one of \{10, 160, 1280\}} \\
        & \CodeInTable{this-\textgreater maxSize == 10} \\
        \hline
    \end{tabular}
    \label{table:daikon_incorrect_inv}
\end{table}




\begin{table}[h!]
    \centering
    \small
    \caption{Daikon Incorrect Invariants per Benchmark}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Data Structure} & \textbf{Total \# Invariants} & \textbf{ Incorrect Invariants} \\
        \midrule
        hash\_table           & 8 & 1 \\
        binary\_search\_tree  & 3 & 0 \\
        heap                  & 10 & 1 \\
        red\_black\_tree      & 2 & 0 \\
        avl\_tree             & 4 & 0 \\
        vector                & 3 & 1 \\
        stack                 & 6 & 2 \\
        queue                 & 7 & 1 \\
        linked\_list          & 4 & 1 \\
        \midrule
        \textbf{Average}    & \textbf{5.2} & \textbf{0.78} \\
        \bottomrule
    \end{tabular}
    \label{tab:daikon_invariants}
\end{table}


\label{daikon_compare}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figure/daikon_classinvgen.pdf}  % Adjust the width as needed
    \caption{Daikon vs. \tech Kills}
    \label{fig:daikon_specbot}
\end{figure*}


Among the 40 correct class invariants generated by Daikon and reviewed by us, the most common invariants indicate that the class pointer is not null and remains unchanged throughout the objectâ€™s lifetime (27 invariants). The second most frequent type asserts that the number of elements (typically \CodeIn{this->n}) is non-negative (6 invariants). These two categories provide minimal utility in distinguishing and eliminating mutants (incorrect programs). 

The most valuable invariants are those that capture meaningful constraints, such as \CodeIn{this->n < this->maxSize} in \CodeIn{Stack} and \CodeIn{this->head < this->maxSize \&\& this->tail < this->maxSize} in \CodeIn{Queue}, as these invariants have a noticeable impact on identifying and eliminating mutants as shown in Figure~\ref{fig:daikon_specbot}.

This result suggests a key weakness of Daikon: it cannot differentiate between invariants that are universally true and those that hold only in the context of specific tests. We also postulate that Daikon is less mature for C++ codebases compared to Java, explaining the lack of high-quality class invariants on our benchmark.
Moreover, our findings indicate that LLMs are better at capturing the concept of true "global" or "class" invariants, reflecting properties that are inherent to the data structure rather than incidental to the tests used.




