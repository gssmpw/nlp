
In this section, we discuss how \tech relates to previous works on synthesizing program invariants statically, dynamically and neurally.


\subsection{Static approaches}
Static techniques, such as interpolation~\cite{mcmillan2004interpolants} or abstract interpretation~\cite{cousot1977abstract} perform a symbolic analysis of source code to compute static over-approximations of runtime behavior and represent them as program invariants over suitable domains.
These techniques are often used to prove the safety properties of the code.
They focus on synthesizing loop invariants and method pre/postconditions, and a few around module-level specifications~\cite{lahiri-cav09}. 
Given the undecidability of program verification, these techniques scale poorly for real-world programs, especially in the presence of complex data structures and frameworks. 
In contrast, \tech can be applied to large codebases to synthesize high-quality class invariants but does not guarantee soundness by construction. 


\subsection{Dynamic approaches}
Dynamic synthesis techniques, such as Daikon~\cite{ernst2007daikon}, DIG~\cite{nguyen2012dig}, SLING~\cite{le2019sling}, and specification mining~\cite{ammons2002}, learn invariants by observing the dynamic behaviors of programs over a set of concrete execution traces. 
One advantage of these dynamic techniques is that they can be agnostic to the code and generally applicable to different languages. 
However, these approaches are limited by the templates or patterns over which the invariants can be expressed. 
DySy~\cite{dysy} employs dynamic symbolic execution to alleviate the problem of fixed templates for bounded executions but resorts to ad-hoc abstraction for loops or recursion. 
\citet{hellendoorn2019are} trained models to predict the quality of invariants generated by tools such as Daikon, but do not generate new invariants. 
SpecFuzzer~\cite{facundo2022specfuzzer} generates numerous candidate assertions via fuzzing to construct templates and filters them using Daikon and mutation testing. 
Finally, Geminus~\citep{boockmann2024geminius} aims at synthesizing sound and complete class invariants representing the set of reachable states, guiding their search using random test cases termed Random Walk.

Unlike these approaches, \tech can generate a much larger class of invariants, leveraging multimodal inputs, including source code, test cases, comments, and even the naming convention learned from training data, to enhance invariant synthesis.
Further, unlike prior dynamic approaches, LLM-based test generation (an active area of research~\cite{codamosa-icse23,sch√§fer2023empiricalevaluationusinglarge,yang2024whitefox}) reduces the need to have a high-quality test suite to obtain the invariants.

For the use case of static verification, learning-based approaches have been used to iteratively improve the quality of the synthesized inductive invariants~\cite{garg2014learning, garg2016learning, padhi2016loopinvgen} from dynamic traces. 
However, these approaches have not been evaluated in real-world programs due to the need for symbolic reasoning. 

\subsection{Neural approaches}
LLM-based invariant synthesis is an emerging area of research with some noteworthy recent contributions. \citet{pei2023learning} trained a model for zero-shot invariant synthesis, which incurs high training costs and lacks feedback-driven repair. 
Their approach uses Daikon-generated invariants as both training data and ground truth, which can lead to spurious invariants. 

Prior work on nl2postcond~\cite{nl2postcond} prompts LLMs to generate pre and postcondition of Python and Java benchmarks, illustrating LLMs' ability to generate high-quality specifications. 
However, they do not prune incorrect invariants and do not generate class invariants that \tech does. 
It is an interesting future work to combine this work with \tech to generate complete class-level specifications including pre and postconditions for the public methods of the class.

For static verification, recent works include the use of LLM for intent-formalization from natural language~\cite{lahiri2024evaluating},  and inferring specifications and inductive program invariants~\cite{loopy,ma2024specgen}.
None of these techniques scale to real-world programs due to the need for complex symbolic reasoning. 

