\section{Related Works}
% LLM
% MRL
\textbf{Molecule-Text Modeling.} Early approaches utilize 1D SMILES molecular sequences to treat molecules as text sequences by adapting Transformer models ____ designed for natural language processing____. 
KV-PLM____ specifically employs a masked language modeling loss to pretrain on biomedical texts with 1D SMILES representation. 
MolT5____ specializes T5 model____ and tokenizer for SMILES-to-text and text-to-SMILES translations. 
Further enhancements represent molecules as 2D graphs. 
In particular, MoMu____ and MoleculeSTM____ leverage cross-modal contrastive learning to align the molecule graph representation to text. 
Current approaches to use multi-view representations of molecules primarily rely on contrastive learning, as demonstrated in models like GIT-Mol____ and MolLM____. 
Additionally, aided with the development of vision large language models (VLLMs), molecular large language models with multi-modal learning architectures have been developed. 
Simple projection layers were used in prior works, InstructMol____ and GraphGPT____, to project molecular graph representations to LLM's input text token space.
Recent works have been concentrated on utilizing Q-Former____ suggested in vision domain to bridge the gap between molecule and text modality. 
MolCA____ and 3D-MoLM____ aligns 2D graph and 3D conformer molecular representations to text in purpose to generate effective soft-prompts for large language models.
UniMoT____ employs a vector quantization-driven tokenizer with a Q-Former. 
Current methods for utilizing multi-view representations of molecules are limited to contrastive learning or usage of specialized tokenizers, failing to achieve simultaneous alignment across all views and text, thereby neglecting the core principle of cross-modal alignment.

\textbf{Molecular representation learning}. Recent research in representation learning for molecules has seen significant advancements, particularly in leveraging large-scale unlabeled molecular data. 
SMILES-BERT____, MolBERT____ adapts the BERT architecture on SMILES string for molecular property prediction tasks. 
To better focus on structural information of molecules, various graph-based representation learning models were presented. 
MolCLR____ specifically tailored contrastive learning for molecular graphs using data augmentation while MAT____ reinterpreted the attention mechanism of transformers to consider distance and edges. 
More recent works concentrate on employing 3D geometry, mostly to exploit 3D spatial coordinates. GraphMVP____ proposed a contrastive learning framework that bridges 2D topological and 3D geometric views of molecules. 
GEM____ incorporated 3D geometric information by using bond angles and lengths as additional edge attributes in molecular graphs. 
Uni-Mol is a SE(3)-transformer based model pretrained via 3D position recovery and masked atom prediction. 
Additionally, MolFormer____ integrates SMILES, graph, and 3D conformer information in a unified transformer architecture for molecular property prediction. 
These recent advancements demonstrate a trend towards incorporating more diverse and rich molecular information to improve the quality and applicability of learned representations, validating the approach of our research.



%