\section{Related Works}
% LLM
% MRL
\textbf{Molecule-Text Modeling.} Early approaches utilize 1D SMILES molecular sequences to treat molecules as text sequences by adapting Transformer models \citep{vaswani2017attention} designed for natural language processing~\citep{irwin2022chemformer, wang2019smiles}. 
KV-PLM~\citep{zeng2022deep} specifically employs a masked language modeling loss to pretrain on biomedical texts with 1D SMILES representation. 
MolT5~\citep{edwards2022translation} specializes T5 model~\citep{raffel2020exploring} and tokenizer for SMILES-to-text and text-to-SMILES translations. 
Further enhancements represent molecules as 2D graphs. 
In particular, MoMu~\citep{su2022molecular} and MoleculeSTM~\citep{liu2023multi} leverage cross-modal contrastive learning to align the molecule graph representation to text. 
Current approaches to use multi-view representations of molecules primarily rely on contrastive learning, as demonstrated in models like GIT-Mol~\citep{liu2024git} and MolLM~\citep{tang2024mollm}. 
Additionally, aided with the development of vision large language models (VLLMs), molecular large language models with multi-modal learning architectures have been developed. 
Simple projection layers were used in prior works, InstructMol~\citep{cao2023instructmol} and GraphGPT~\citep{tang2024graphgpt}, to project molecular graph representations to LLM's input text token space.
Recent works have been concentrated on utilizing Q-Former~\citep{li2023blip} suggested in vision domain to bridge the gap between molecule and text modality. 
MolCA~\citep{liu2023molca} and 3D-MoLM~\citep{li2024towards} aligns 2D graph and 3D conformer molecular representations to text in purpose to generate effective soft-prompts for large language models.
UniMoT~\citep{zhang2024unimot} employs a vector quantization-driven tokenizer with a Q-Former. 
Current methods for utilizing multi-view representations of molecules are limited to contrastive learning or usage of specialized tokenizers, failing to achieve simultaneous alignment across all views and text, thereby neglecting the core principle of cross-modal alignment.

\textbf{Molecular representation learning}. Recent research in representation learning for molecules has seen significant advancements, particularly in leveraging large-scale unlabeled molecular data. 
SMILES-BERT~\citep{wang2019smiles}, MolBERT~\citep{li2021mol} adapts the BERT architecture on SMILES string for molecular property prediction tasks. 
To better focus on structural information of molecules, various graph-based representation learning models were presented. 
MolCLR~\citep{wang2022molecular} specifically tailored contrastive learning for molecular graphs using data augmentation while MAT~\citep{maziarka2020molecule} reinterpreted the attention mechanism of transformers to consider distance and edges. 
More recent works concentrate on employing 3D geometry, mostly to exploit 3D spatial coordinates. GraphMVP~\citep{liu2021pre} proposed a contrastive learning framework that bridges 2D topological and 3D geometric views of molecules. 
GEM~\citep{fang2022geometry} incorporated 3D geometric information by using bond angles and lengths as additional edge attributes in molecular graphs. 
Uni-Mol is a SE(3)-transformer based model pretrained via 3D position recovery and masked atom prediction. 
Additionally, MolFormer~\citep{wu2023molformer} integrates SMILES, graph, and 3D conformer information in a unified transformer architecture for molecular property prediction. 
These recent advancements demonstrate a trend towards incorporating more diverse and rich molecular information to improve the quality and applicability of learned representations, validating the approach of our research.



%