\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[preprint]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float} % Add this to the preamble
\usepackage{subcaption}


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{graphicx}

% % If the title and author information does not fit in the area allocated, uncomment the following
% %
% %\setlength\titlebox{<dim>}
% %
% % and set <dim> to something 5cm or larger.

% \documentclass{article} % For LaTeX2e
% \usepackage{iclr2025_conference,times}
% \usepackage{graphicx}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{tabularray}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{wrapfig}

\useunder{\uline}{\ul}{}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{colortbl}
\usepackage{caption}


\title{MV-CLAM: Multi-View Molecular Interpretation with Cross-Modal Projection via Language Model}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{%
  Sumin Ha\textsuperscript{1,*}, \quad Jun Hyeong Kim\textsuperscript{2,}\thanks{These authors contributed equally to the work}, \quad Yinhua Piao\textsuperscript{3}, \quad Sun Kim\textsuperscript{1,3,4,5} \\ 
  \textsuperscript{1}Interdisciplinary Program in Artificial Intelligence, Seoul National University\\
  \textsuperscript{2}Bio-MAX/N-Bio, Seoul National University\\
  \textsuperscript{3}Department of Computer Science and Engineering, Seoul National University\\
  \textsuperscript{4}Interdisciplinary Program in Bioinformatics, Seoul National University\\
  \textsuperscript{5}AIGENDRUG Co., Ltd.,\\
  \texttt{\{suminqw124,tommy0906,2018-27910,bioinfo.sunkim\}@snu.ac.kr}
}


% \author{Sumin Ha \\
%    / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}




\maketitle

\begin{abstract}

Human expertise in chemistry and biomedicine relies on contextual molecular understanding, a capability that large language models (LLMs) can extend through fine-grained alignment between molecular structures and text. 
% While recent multimodal learning advances emphasize detailed cross-modal alignment, existing molecule-text models neglect the complementary information in different molecular views and focus on single-view representations, limiting molecular comprehension. 
Recent multimodal learning advances focus on cross-modal alignment, but existing molecule-text models ignore complementary information in different molecular views and rely on single-view representations, limiting molecular understanding.
Moreover, naïve multi-view alignment strategies face two challenges: (1) \textit{separate aligned spaces} with inconsistent mappings between molecule and text embeddings, and that (2) existing loss objectives \textit{fail to preserve }complementary information for fine-grained alignment. This can limit the LLM's ability to fully understand the molecular properties.
To address these issues, we propose MV-CLAM, a novel framework that aligns multi-view molecular representations into a unified textual space using a multi-query transformer (MQ-Former). Our approach ensures cross-view consistency while a token-level contrastive loss preserves diverse molecular features across textual queries. MV-CLAM enhances molecular reasoning, improving retrieval and captioning accuracy.
% Large language models (LLMs) have shown promise in biomolecular tasks, yet existing methods primarily focus on rough alignment between molecule and text modalities using single-view molecule representations. Naïve approaches to align multiple embeddings to text often suffer from (1) separately aligned molecular embeddings with inconsistent textual representations, (2) inadequate structural information preservation and (3) increased computational overhead.
% To overcome these issues, we propose MV-CLAM, an LLM framework equipped with a novel multi-query transformer, MQ-Former. Our cross-modal projector jointly aligns 2D and 3D molecular representations into a unified textual space with 1D representations, enriching structural context for LLMs. 
% Additionally, we preserve the diversity of embeddings to query tokens by incorporating a novel contrasting loss that allows the cross-modal projector to retain specific information highly specialized for each dimension. Our fine-grained alignment approach enhances molecular understanding, effectively prompting molecules for improved LLM-driven chemical characterization.
The source code of MV-CLAM is available in \url{https://github.com/sumin124/mv-clam.git}. 

\end{abstract}



\section{Introduction}
A profound contextual understanding of both molecular structures and biomedical text is crucial in chemistry and biomedicine.
For large language models to capture these relationships, fine-grained alignment between textual and molecular representations is required to harness their high-context reasoning ability.
% Human expertise in chemistry and biomedicine relies on a deep contextual understanding of molecular structures and biomedical text. Extending this capability to large language models requires fine-grained alignment between textual descriptions and molecular structures, leveraging LLMs’ high-context reasoning ability.
% A deep understanding of molecular structures and biomedical text requires fine-grained alignment with textual descriptions, enabling large language models to leverage their high-context reasoning ability.
In vision-language models, researchers have moved beyond coarse image-text matching toward precise region-word alignment, ensuring detailed semantic correspondence between textual descriptions and visual features~\citep{li2022fine, lavoie2024modeling}.
% In molecular-language learning, however, prior approaches rely on coarse, high-level similarity matching between molecular structures and text, often using (a) a single molecular view (2D or 3D) for alignment and (b)the CLS token as a textual representation for similarity loss calculation, overlooking richer token-wise molecular-text associations.
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figs/2d_3d_mapping_cropped.pdf}
%     \caption{Comparison of molecular representation alignment strategies. (A) Independent alignment of 2D and 3D representations maps 2D properties (e.g., \textit{solubility} and 3D structural information (e.g., \textit{chiral 3-C}) to distinct text spaces. (B) A unified alignment with a Multi-Querying Transformer (MQ-Former) allows all caption tokens share a single text space}
%     \label{fig:2d3d_map}
%     \vspace{-20pt}
% \end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/intro_figure_cropped.pdf}
    \caption{Motivations of MV-CLAM. (A) Complementary molecular information captured by 2D and 3D representations, where 2D graph encodes edge connectivity, and 3D conformers captures spatial coordinate structures. (B) Inconsistent mappings between molecule (2D and 3D) and property tokens (e.g., 2D property token like \textit{solubility} and 3D structural information like \textit{chiral 3-C}) in distinct text spaces. (C) A unified alignment with a Multi-Querying Transformer (MQ-Former) allows all text tokens share a single text space.}
    \label{fig:2d3d_map}
    \vspace{-20pt}
\end{figure}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/methods_figure_cropped.pdf}
    \caption{Methods for molecular language modeling. (A) Contrastive learning aligns two modalities via a contrastive objective, excelling in retrieval but lacking generative capabilities. (B) The Q-Former framework uses learnable query tokens for caption generation but is limited to a single molecular representation. (C) MV-CLAM extends this by integrating multiple representations with modality-specific queries, enabling fine-grained knowledge integration.}
    \label{fig:head_fig}
    % \vspace{-15pt}
\end{figure*}
Recent studies have leveraged large language models (LLMs) for molecular understanding by integrating sequential representations (1D SMILES strings) and structural features (2D molecular graphs and 3D conformers)~\citep{edwards2022translation, liu2023multi}. This approach mitigates the inherent limitations of LLMs which are primarily trained on textual data, that lacks native reasoning over molecular structures. To enable LLMs to further understand molecule information, Q-former based models~\citep{liu2023molca, li2024towards} align molecular structures into text space (Figure~\ref{fig:head_fig}B).

Combining multi-view molecular features simultaneously is essential, as their complementary nature provides a more complete understanding of molecular characteristics.
% LLMs trained on large-scale biochemical literature have demonstrated domain-specific understanding, advancing tasks such as biomedical question answering and chemical knowledge extraction~\citep{edwards2022translation, taylor2022galactica, li2024towards, liu2023multi}. However, their reasoning remains text-dominant, as molecular structures are not inherently understandable to LLMs, limiting their ability to perform structural interpretation akin to human experts. 
%
% To bridge this gap, recent works have incorporated molecular representation learning (MRL) models to extract meaningful structural features from 1D SMILES strings~\citep{irwin2022chemformer}, 2D molecular graphs~\citep{guo2022graph, hu2019strategies, wang2022molecular}, and 3D molecular conformations~\citep{zhou2023unimol, kim2024diffusion, du2023fusing}. 
For example, as shown in Figure~\ref{fig:2d3d_map}A, 2D molecular graphs primarily capture atomic bonding patterns, absent in 3D point clouds. Hence, 2D graphs focus on properties highly affected by atomic bond patterns (eg.,log P, solubility)~\citep{guo2022graph} while 3D molecular conformations encode spatial atomic coordinates that influence molecular interactions and quantum properties such as HOMO and LUMO~\citep{kim2024diffusion, zhou2023unimol, du2023fusing}. 
In the context of molecule understanding, aligning both molecular views into the unified text space of LLMs enables the model to capture all relevant molecular details effectively.

% Given their complementary nature founded on common atomic features, leveraging both representations is essential to ensure all aspects are fully captured.
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figs/2d3d_diagram.pdf}
%     \caption{Complementary molecular information captured by 2D and 3D representations. 2D molecular graphs encode edge connectivity, while 3D conformations provide spatial atomic coordinates essential for capturing quantum properties. Since both dimensions contribute distinct but complementary information, a unified framework is required to fully preserve molecular context for LLMs.}
%     \label{fig:2d3d_chem}
%     \vspace{-15pt}
% \end{figure}

However, existing molecule-text modeling focuses on the alignment of a single molecular view as shown in Figure~\ref{fig:head_fig}A and~\ref{fig:head_fig}B~\citep{cao2023instructmol, li2024towards, liu2023molca, liu2023multi}.
% Overall, prior works focus on aligning one molecular representation per molecule to text description, utillizing a single CLS token for similarity calculation for training. Such methods fails to preserve the abundance of molecular embeddings in a structured manner. 
Naïve approaches to multi-view alignment might be to independently map each molecular view to text using separate alignment modules. However, this leads to several issues.
(1) \textit{Separated aligned spaces}. 
Aligning 2D and 3D molecular representations separately to text results in distinct aligned spaces for the same molecule. 
As shown in Figure~\ref{fig:2d3d_map}B, \textit{``solubility''} and \textit{``chiral 3-C''} correspond to 2D and 3D molecular properties, but each has redundant embeddings in its own space. This inconsistency can prevent the LLM from fully understanding molecular properties, as it lacks a unified representation of 2D and 3D structures.
(2) \textit{Insufficient fine-grained molecule-text alignment}. Existing Q-Former-based approaches~\citep{li2024towards, liu2023molca} for aligning molecule queries into a unified text space select the most similar query-to-single token pairs for contrastive learning (Figure~\ref{fig:g2tloss_modification}). This coarse alignment overlooks structural diversities across molecular views (Appendix Figure~\ref{fig:attn_map_total}B), failing to preserve complementary information necessary for fine-grained alignment and limiting the LLM’s ability to fully understand molecular properties.
% Contrastive learning in Q-Former-based approaches computes molecule-text similarity using only the most similar molecule query token to the CLS token of the text. This coarse alignment neglects structural variations across multiple molecular views, failing to fully leverage the rich molecular-text associations necessary for fine-grained understanding. 
% (3) \textit{High computational cost}. Processing each view independently results in significant computational overhead.

To address this, we propose MV-CLAM, a novel framework that aligns multi-view molecule features using a multi-query transformer, MQ-Former (Figure~\ref{fig:head_fig}C).
Specifically, our approach jointly integrates multi-view molecular representations into a unified textual space, where \textit{``solubility"} and \textit{``chiral 3-C"} have unique unified embedding. Such helps generate universal query tokens with more semantic information. 
% To address this, we propose Multi-Query Transformer (MQ-Former), a novel contrastive learning framework that modifies the contrastive loss to enforce fine-grained molecular-text alignment (Figure~\ref{fig:head_fig}C). Our approach jointly integrates multi-view molecular representations (2D \& 3D) into a unified textual space. 
Additionally, we propose a multi-token contrastive loss to refine alignment by considering all text tokens within the description, rather than a single $\textsc{CLS}$ token. Such multi-token contrasting ensures that molecular structures are contextualized with finer, token-level associations, capturing both atomic and functional relevance. 
% By aligning multi-view molecular representations to a shared, context-rich textual space, 
MV-CLAM enhances molecular reasoning in LLMs, improving both retrieval and captioning accuracy.
%while significantly reducing training time.

Our main contributions are as follows: 

\begin{itemize}
    \item We propose a novel framework, MV-CLAM, that simultaneously aligns multiple molecular views (1D smiles, 2D graphs, and 3D conformers) to a unified textual space to enhance LLM-based molecular reasoning.

    \item We present a novel contrastive learning loss in molecule-language modeling for fine-grained alignment, considering all text tokens with enriched molecular query tokens.

    \item We achieve state-of-the-art performance in molecule-text retrieval and molecule captioning tasks while improving the interpretability of molecular representations.

\end{itemize}


% \section{Introduction}


% Human expertise in chemistry and biomedicine relies on a deep contextual understanding of molecular structures and biomedical text. 
% Extending this capability to large language models (LLMs) demands the integration of rich molecular representations with text, leveraging LLMs' ability of high-context reasoning—a key factor behind their success in complex problem-solving. LLMs excel in high-context reasoning, leveraging structured prompts to enhance multi-step and knowledge-intensive tasks, as seen in Chain-of-Thought (CoT) prompting, which guides models through explicit, intermediate reasoning steps~\citep{wei2022chain}.

% LLMs trained on extensive biochemical literature demonstrate domain-specific language understanding, enabling advancements in biomedical question answering, molecule-text translation, and chemical knowledge extraction~\citep{edwards2022translation, taylor2022galactica, li2024towards, liu2023multi}. Although exploiting domain-specific literature knowledge is beneficial, their reasoning over molecular structures remains limited since their primary mode of processing is text.
% 2D and 3D structures are directly incomprehensible to LLMs yet provide essential chemical context, limiting their ability to reason about molecular structures like human experts.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figs/2d_3d_mapping_cropped.pdf}
%     \caption{Comparison of molecular representation alignment strategies. (A) Independent alignment of 2D and 3D representations maps 2D properties (e.g., \textit{solubility} and 3D structural information (e.g., \textit{chiral 3-C}) to distinct text spaces. (B) A unified alignment with a Multi-Querying Transformer (MQ-Former) allows all caption tokens share a single text space}
%     \label{fig:2d3d_map}
%     \vspace{-15pt}
% \end{figure}

% To bridge the gap between molecular structure and textual understanding, recent works have leveraged self-supervised molecular representation learning (MRL) to extract meaningful molecular features from different structural modalities, including 1D SMILES (Simplified Molecular Input Line Entry System) strings~\citep{irwin2022chemformer}, 2D molecular graphs~\citep{guo2022graph, hu2019strategies, wang2022molecular}, and 3D conformations~\citep{zhou2023unimol, kim2024diffusion, du2023fusing}. As illustrated in Figure~\ref{fig:2d3d_chem}, 2D molecular graphs primarily capture atomic bonding patterns, which are absent in 3D encoders that comprehend molecules as 3D point clouds. Hence, 2D encoders focus on aspects critical for properties such as logP and solubility~\citep{guo2022graph} whereas 3D molecular conformations encode spatial atomic coordinates that influence molecular interactions, binding affinity and quantum properties such as HOMO and LUMO~\citep{kim2024diffusion, zhou2023unimol, du2023fusing}. Given their complementary nature founded on common atomic features, leveraging both representations is essential to ensure all aspects are fully captured.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figs/2d3d_diagram.pdf}
%     \caption{Complementary molecular information captured by 2D and 3D representations. 2D molecular graphs encode edge connectivity and atomic bonding patterns, while 3D conformations provide spatial atomic coordinates, essential for capturing quantum properties. Since both dimensions contribute distinct but complementary information, a unified framework is required to fully preserve molecular context for LLMs.}
%     \label{fig:2d3d_chem}
%     \vspace{-15pt}
% \end{figure}

% Inspired by the success of vision-language modeling~\citep{alayrac2022flamingo, merullo2022linearly}, foundation models are now unifying molecular structures and text, broadly categorized into contrastive learning-based methods (Figure~\ref{fig:head_fig}A) and the Q-Former framework~\citep{li2023blip} (Figure~\ref{fig:head_fig}B). While contrastive learning excels in retrieval but falls short in molecule-to-text generation, Q-Former-based methods~\citep{su2022molecular, liu2023multi, liu2023molca, li2024towards} mitigate this issue yet is strictly refined to aligning a single representation between two modalities. 
% Building on this, we extend the framework by using multiple molecule representations for fine-grained structural and functional knowledge integration.

% A simple approach would be to directly align each view to text using two separate alignment modules (Figure~\ref{fig:2d3d_map}A).
% However, this leads to several issues. 
% (1) \textit{Separated embedding spaces}. As independent pretrained models or encoders are utilized for 2D and 3D structures, the corresponding embeddings exist in a separate space. Without alignment between the respective multiple views, producing a consistent representation that leverages all information is difficult. 
% (2) \textit{Lack of text consistency}. Cross-modal alignment not only aligns molecular information to text, but also vice versa. 
% Independent utilization of Q-formers lead textual representations to lie in different latent space, which conflicts the purpose of utilization. 
% (3) \textit{High computational cost}. Processing each view independently results in significant computational overhead.

% To address these limitations, we propose \textbf{Multi-Querying Transformer (MQ-Former)}. 
% MQ-Former approximates the embedding spaces of 2D and 3D structures using a shared self-attention layer and employs a unified text transformer to generate a single, processed text token for each molecule (Figure~\ref{fig:head_fig}C). 
% Aligning multiple molecular views to a consistent unique text embedding provides a more subtle and robust embedding, allowing models to capture both chemical and spatial semantics in a unified representation (Figure~\ref{fig:2d3d_map}B). 
% In essence, adopting a multi-view approach enables a deeper and more complete molecular understanding. 
% Moreover, by aligning the two views simultaneously, our approach achieves faster training speeds and reduces the training time by more than half compared to handling each view separately.

% Our contributions are as follows:

% \begin{itemize}
%     \item We integrate 1D,2D,3D multi-view molecular representations for molecular contextualization to LLMs. 

%     \item We propose a novel cross-model projector, MQ-Former, that jointly aligns 2D and 3D molecular structures to text, unifying representations and managing inconsistencies from separate embeddings.

%     \item We achieve state-of-the-art performance in molecule-text retrieval and molecule captioning tasks while improving the interpretability of molecular representations.
% \end{itemize}

% related works moved to appendix

\section{MV-CLAM}

%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figs/iclr_figure_2_final.pdf}
%     \caption{Overall architecture of MV-CLAM. MQ-Former provides universal query which acts as a soft prompt to Llama2, optimized by LoRA}
%     \label{fig:llama_fig}
%     \vspace{-15pt}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%

MV-CLAM provides molecule captions given multi-view structural information. 2D and 3D molecular structural information is extracted from specialized encoders and processed through MQ-Former's cross-attention layers to update learnable query tokens for each dimension.
The shared self-attention layer enables information sharing across all modalities.
% These query tokens are aligned to textual space via the shared self attention and multi-objective learning, while also considering the alternative view.
2D and 3D queries are combined to create a universal query, which is trained with our modified multi-objective loss for fine-grained alignment with textual descriptions. The learned universal query is then passed with the prompt and SMILES strings to the language model for caption generation. The overall framework of MV-CLAM shown in Figure~\ref{fig:head_fig}C is comprised of three main components: (1) Molecule structural graph encoders for 2D and 3D molecular structures, (2) MQ-Former as a cross-modal projector, and (3) LLaMA2 as the language model.

% \subsection{Text Encoder}

% SciBERT~\citep{beltagy2019scibert} is a BERT variant tailored for scientific and biomedical domains. SciBERT utilizes SciVocab built from a scientific corpus, and is pretrained on 1.14 million Semantic Scholar papers. MQ-Former's text transformer block adopts SciBERT architecture and initialized with its pretrained weights. Textual descriptions $S$ of length $L$ are tokenized with SciBERT's tokenizer $f_{sci}$ before being input into MQ-Former. 
% \begin{equation}
%   X_{text} = [t_1, t_2,..., t_{L}] = f_{sci}(S)
%   \label{eq: txt}
% \end{equation}
% % Text encoder refers to the text transformer module $T_{\text{text}}$ in MQ-Former.

\subsection{Molecular Graph Encoder}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/iclr_figure_3_final.pdf}
\caption{Training scheme of MQ-Former. The proposed MQ-Former enhances molecular language modeling by incorporating multi-token contrasting and amplified molecule captioning losses to the prior multi-objective loss~\citep{li2023blip, li2024towards, liu2023molca}. (1) The novel multi-token contrasting loss $\ell_{MTC}$ replaces conventional molecule-text contrastive learning, encouraging diverse query-token alignment. (2) The molecule captioning loss $\ell_{MCap}$ is amplified to improve text generation quality. The molecule-text matching loss $\ell_{MTM}$ remains unchanged.}
    \label{fig:overview_fig}
\end{figure*}
To capture structural information from multiple views, we used molecular embeddings from both 3D and 2D structural encoders. 
For the 3D encoder $f_{3d}$, we deployed \textbf{Uni-Mol}~\citep{zhou2023unimol}, a SE(3)-transformer based model pretrained on 209 million 3D molecular conformations using two tasks: 3D position recovery and masked atom prediction.
Input 3D molecule for Uni-Mol is denoted as $m_{3d}=(\mathcal V,\textbf{f},\textbf{P})$, where $\mathcal V$ and $\textbf{f}$ each represents atomic nodes and their features, and $\textbf{P} \in \R^{\mathcal |\mathcal V|\times 3}$ represents 3D coordinates of atoms.
Pair representations are initialized by invariant spatial positional encoding from atom coordinates and interact with atom representations. The output atomic representation $H_{3d} \in \R^{|\mathcal V| \times d_{3d}}$, where $h_i$ corresponds to the $i$-th atom and $d_{3d}$ denotes hidden dimension size of $H_{3d}$, updates learnable 3D query tokens through the cross-attention layers in MQ-Former's 3D molecular transformer block.
% Atomic representation $\mathcal X_{3d}$ for $\mathcal V$ atoms, where $x_i$ corresponds to the $i$-th atom, is utilized to update learnable 3D query tokens through the cross-attention layers in MQ-Former's 3D molecular transformer block $T_{3d}$.
\begin{equation}
  H_{3d} = [h_1, h_2, ..., h_{|\mathcal V|}] = f_{3d}(m_{3d})
  \label{eq: 3d}
\end{equation}

For the 2D molecular encoder $f_{2d}$, we adopted \textbf{Molecule Attention Transformer (MAT)}~\citep{maziarka2020molecule}, pretrained on two million molecule samples from ZINC15 dataset~\citep{irwin2012zinc}. Given 2D molecule $m_{2d}=(\mathcal V,\textbf{f},\textbf{A})$ where $A$ represents edges within the molecule as adjacency matrix, MAT generates atomic representations $H_{2d} \in \R^{|\mathcal V| \times d_{2d}}$ using a specialized molecule-specific attention mechanism that considers edges, atomic distances and atomic features. The atomic representations interact with the learnable 2D query tokens via cross-attention layers in 2D molecular transformer block.
% Likewise, atomic representations are processed through the cross-attention layers within MQ-Formers' 2D-molecular transformer block $ T_{2d}$.
% Likewise, atomic representations are processed through the cross-attention layers within MQ-Formers' 2D-molecular transformer block.
\vspace{-10pt}
\begin{equation}
  H_{2d} = [h_1, h_2, ..., h_{|\mathcal V |}] = f_{2d}(m_{2d})
  \label{eq: 2d}
  % \vspace{-5pt}
\end{equation}

\subsection{MQ-Former: Multi-Querying Transformer}
Previous studies applying Q-Former to the molecular domain projects single-dimensional structural embeddings into the textual space~\citep{li2024towards, zhang2024unimot}.
These models consist of a single molecule transformer and a text transformer.
However, this approach is inherently limited in preserving molecular information when aligning with text embeddings for two main reasons: (1) separate aligned spaces with inconsistent mappings between molecule and text embeddings, and (2) information loss caused by single-token contrastive learning.
% However, this approach is inherently limited in its capacity for detailed preservation of molecule and text embeddings upon modality alignment for the following two reasons: (1) separately aligned spaces if expanded to multiple dimensions within a single modality, and (2) single-token contrastive loss to calculate pairwise similarity. 
MQ-Former addresses this limitation by introducing a novel architecture capable of aligning multiple modalities to a unified aligned space using a refined multi-objective loss for better information preservation (Figure~\ref{fig:overview_fig}).


Our approach combines structural representations of two dimensions, but the architecture can be extended using multiple molecule transformers and a single text transformer.
Each molecule transformer, based on the BERT architecture with additional cross-attention layer, processes $K$ learnable query tokens specific to their respective views. Following previous studies~\citep{li2024towards,liu2023molca}, we adopt the SciBERT~\citep{beltagy2019scibert} architecture for the text transformer and initialize all blocks with SciBERT's pretrained weights. Hence, textual descriptions $S$ of length $L$ are tokenized with SciBERT's tokenizer $f_{sci}$ to $X_{\text{text}} = \{x_1, x_2, ... , x_T\}$ (T: number of tokens in text) before being processed through MQ-Former's text transformer. 
The cross-attention mechanism extracts relevant information from embeddings into the query tokens, and shared self-attention layers enable information exchange across all embeddings, over-passing the limitation of separated aligned spaces.

% Previous studies applying Q-Former to the molecular domain projects single-dimensional structural embeddings into the textual space~\citep{li2024towards, zhang2024unimot}.
% These models consist of a single molecule transformer and a text transformer.
% However, this approach is inherently limited in its capacity to handle more than two modalities. 
% % Original Q-Former consist of two transformers: a molecule transformer and a text transformer. 
% % The architecture operates by updating a set of $K$ learnable query tokens that interact with graph representation derived from graph encoders via cross-attention mechanisms.
% % However, this approach is inherently limited in its capacity to handle more than two modalities. 
% MQ-Former addresses the limitation by introducing a novel architecture capable of aligning multiple modalities to the text space (Figure~\ref{fig:overview_fig}). 
% % While our current application to the molecular space is refined to combining structural representation of two dimensions, the architecture can be expanded to multiple modalities. 
% Our approach combines structural representations of two dimensions, but the architecture can be extended using multiple molecule transformers and a single text transformer.
% % ($T_{\text{mol}_1},...,T_{\text{mol}_N}$) and a single text transformer $T_{\text{text}}$. 
% % Our novel structure consists of $N+1$ transformer modules: $N$ molecule transformers ($f_{Pro1},...,f_{ProN})$ with single text transformer. 
% Each molecule transformer, based on the BERT architecture with additional cross-attention layer, processes $K$ learnable query tokens specific to their respective views. All blocks are initialized with SciBERT's pretrained weights. The cross-attention mechanism extracts relevant information from embeddings into the query tokens, and shared self-attention layers enable information exchange across text and multi-view data.
% These query tokens share information with both text and other structural embeddings through shared self-attention layers and interact with graph embeddings from dedicated encoders via cross-attention mechanisms.
% % These query tokens share information with both text and other structural embeddings through shared self-attention layers, while being specialized to their modality by interacting with graph embeddings from dedicated graph encoders via cross-attention mechanisms.
% In contrast, the text transformer retains the standard architecture of BERT without additional cross-attention modules.
% All transformers, initialized from the SciBERT checkpoint, allows to exchange information through shared self-attention layers.

% Figure~\ref{fig:overview_fig} illustrates MQ-Former generating query tokens specified for $N$ different views of a given molecule.
Figure~\ref{fig:overview_fig} illustrates MQ-Former generating a universal query tokens for a molecule given two different views. Two molecule transformer modules each updates distinct $K$ query tokens $Q_{2d} \in \R^{K\times768}$ and $Q_{3d} \in \R^{K\times768}$, which are randomly initialized. The learned query tokens, $\hat{Q}_{2d}$ and $\hat{Q}_{3d}$ of same size, are updated representations of these initial tokens, refined through the alignment of multiple molecule views and textual descriptions $X_{\text{text}} \in \R^{L\times768}$. Updated query tokens are concatenated to create a single universal query $\hat{Q} \in \R^{2K\times768}$, containing complementary structural information aligned to textual space. The resulting universal query tokens are then used as inputs for the language model, along with 1D SMILES string and task prompt as depicted in Figure~\ref{fig:head_fig}C.
% To create a universal query as the final input for the language model, we concatenate the query tokens, each implicit containing structural information and aligned with the textual space for better interpretability by the language model: 

\vspace{-5pt}
\begin{equation}
\begin{aligned}
    \hat{Q} &= f_{\text{concat}}({\hat Q_{2d}}, {\hat Q_{3d}}) \\
    &= f_{\text{MQformer}}(H_{2d}, H_{3d}, X_{\text{text}}, Q_{2d}, Q_{3d})
\end{aligned}
\label{eq:concat}
\end{equation}

% \begin{equation}
%     \begin{aligned}
%       \mathcal M &= f_{\text{concat}}(f_{\text{pro}_{2d}}, f_{\text{pro}_{3d}}) = \text{concat}([m_1, m_2, \ldots, m_K], [m_{K+1}, m_{K+2}, \ldots, m_{2K}])
%     \end{aligned}
%     \label{eq:concat}
% \end{equation}
% The resulting $2K$ universal query tokens that are generated given multi-view molecular embeddings and textual descriptions are then used as inputs for the language model, along with 1D SMILES string and task prompt as depicted in Figure~\ref{fig:llama_fig}.

\subsection{LLaMA2 \& LoRA}
The pretraining corpus of LLaMA2~\citep{touvron2023llama} includes a vast amount of biomedical literature and thereby exerts powerful text generation capability with internal chemistry knowledge. 
This allows LLaMA2 to effectively interpret 1D molecular sequences and address tasks related to molecular comprehension.
% Based on these strengths, we selected LLaMA2 as our base language model $f_{\text{LM}}$. 
% Based on these strengths, we selected LLaMA2 as our base language model $f_{\text{lm}}$. 
% Formally, a mixed token sequence of molecular and textual tokens are given to generate responses. 
% The language model adopts a causal mask to generate textual responses $\hat{Z}$ with length $n$, where the prediction of the $i$-th token $\hat{z}_i$ depends on the preceding tokens.
% The language model adopts a causal mask to generate textual responses, where the prediction of each token depends on the preceding tokens.
% For the final prediction, each token is mapped to the most probable word in vocabulary using a softmax function after a linear layer.
% % For the final prediction, each token $\tilde{z}$ is mapped to the most probable word $w$ in the vocabulary via a linear layer followed by a Softmax function.
% \begin{equation}
%   \Tilde{z_i} = \arg\max\limits_{w {\in} vocab} f_{\text{LM}} (\hat{z_i})[w] 
%   \label{eq: lm}
% \end{equation}
% % \begin{equation}
% %   \Tilde{z_i} = \arg\max\limits_{w {\in} vocab} f_{vocab} (\hat{z_i})[w] 
% %   \label{eq: lm}
% % \end{equation}
Despite its inherent capabilities, the language model necessitates fine-tuning to effectively address the universal queries posed by MQ-Former, particularly due to the modifications in the tokenizer resulting from changes in module processing of textual descriptions. To facilitate efficient fine-tuning, we implemented low-rank adaptation (LoRA, \cite{hu2021lora}).



\section{Training MV-CLAM}

The training of MV-CLAM consists of two stages. (1) Guiding MQ-Former to align both multi-view molecular representations to a consistent textual space, and (2) Refining query tokens to be effectively soft-prompted by LLaMA2. Molecular encoders are frozen during the entire pipeline.
% The MV-CLAM pipeline is structured into two primary stages, with each designed to guide MQ-Former in generating query tokens from different perspective during training. 
% The first stage focuses on training MQ-Former to align 3D and 2D molecular representations with the corresponding textual space where molecule description reside.
% In the following stage, the training ensures that the molecular embeddings, now aligned with the textual space as query tokens, can be effectively integrated and utilized by a specific language model - in our case, LLaMA2~\citep{touvron2023llama}.

\subsection{Stage 1: Training MQ-Former}

Two sets of $K$ learnable query tokens are updated by each molecule transformer block in Stage 1. Molecule transformer blocks hold self-attention, cross-attention and feed-forward layers. Specifically, the self attention layers in all blocks of MQ-Former are shared to exchange information between modalities and view. The 2D and 3D query tokens $Q_{2d}(i)$, $Q_{3d}(i)$ for $i$-th molecule are processed through their respective molecule transformers. Our $2K$ universal query token $\hat{Q}(i)$ is formed by concatenating the learned query sets. The objective is to train MQ-Former to learn a unified latent space for all molecular embeddings and obtain highly informed molecular soft-prompt $\hat{Q}(i)$ without any inconsistencies.
% In Stage 1, 2D and 3D query tokens are processed through self-attention and cross-attention layers in the molecule transformers to incorporate structural information, while text data is passed through the text transformer with shared shared attention mechanisms.
% After passing through the encoder blocks, the objective is to train MQ-Former to better align molecular representations with their respective textual descriptions.
% The training employs a multi-objective training loss constituted of molecule-text contrasting $\ell_{MTC}$, molecule-text matching $\ell_{MTM}$ and molecule captioning $\ell_{MCap}$ inspired by the BLIP-2 framework~\citep{li2023blip, li2024towards}.
% Each loss term represents molecule-text contrasting (MTC), molecule-text matching (MTM), and molecule captioning (MCap).

For training, we introduce the following key modifications to the multi-objective loss in previous works inspired by the BLIP-2 framework~\citep{li2023blip, li2024towards}, designed to maximize the diversity of queries. In order to preserve complementary chemical aspects embedded in each dimension, we introduce the following key modifications: (1) a novel multi-token contrasting loss $\ell_{MTC}$ in replacement to single-token (molecule-text) contrasting, and (2) amplification of the molecule captioning loss $\ell_{MCap}$. Molecule-text matching is used without further modifications $\ell_{MTM}$. This allows our model to capture and preserve both fine-grained atomic interactions and high-level chemical semantics, enhancing interpretability and expressiveness in molecular language modeling. Overall, the total loss for training MQ-Former $\ell_{MQ}$ in Stage 1 is as follows: 
% $\ell_{MQ} = \ell_{MTC} + \ell_{MTM} + \alpha \cdot \ell_{MCap}$
\begin{equation}
    \ell_{MQ} = \ell_{MTC} + \ell_{MTM} + \alpha * \ell_{MCap}
    \label{eq: lm}
\end{equation}
% }


\begin{figure}[t]
    \centering    \includegraphics[width=0.45\textwidth]{figs/g2t_loss_modification.pdf}
    \caption{Molecule-text similarity for query-token contrasting. (A) Previous approach compute coarse-level similarity between molecule queries and CLS text token. (B) We propose a new approach to compute token-level similarity between molecule queries and all text tokens, which preserves molecule query diverse information. }
    \label{fig:g2tloss_modification}
\end{figure}

\textbf{Multi-Token Contrasting}. Unlike the previous approach that retrieved only the maximum similarity between a query token and CLS text token(Figure~\ref{fig:g2tloss_modification}A), we introduce a refined similarity computation where each molecule token is matched against all text tokens, retrieving the maximum similarity for each token against all \textit{T} text tokens (Figure~\ref{fig:g2tloss_modification}B). The average loss over all \textit{k} tokens represents a fine-grained similarity calculation between molecule-text pairs, preventing query collapse, where a single query token with high similarity dominates the training process by aligning only with easily capturable text concepts. By distributing alignment across multiple queries and text tokens, we achieve richer molecule-text representations, improving cross-modal association.
% Molecule-text contrasting (MTC) loss, $\ell_{MTC}$, enables MQ-Former to align molecular features with their corresponding text descriptions.

%%%%attention 내용 우선 삭제
% During $\ell_{MTC}$ computation, uni-modal self-attention mask ensure each transformer processes query tokens independently, preventing information exchange and promoting distinct representations for matching and non-matching molecule-text pairs.

% These are then concatenated to form a unified 2K universal query tokens $\{q_{i,k}\}$, which combineds multi-view molecular information.
$\ell_{MTC}$ is measured as the batch mean of the sum of molecule-to-text loss $\ell_{g2t}$ and text-to-molecule loss $\ell_{t2g}$. For each query in the universal query token, we calculate the maximum cosine similarity it has against all text tokens $x(i) \in X_{\text{text}}(i)$ with temperature scaling for precision. The average of the calculated similarity for $2K$ queries represents pairwise similarity in a more precise manner.
Similarly, $\ell_{t2g}$ aligns the text representation with its matching molecular query while contrasting it against all other queries within the batch. The similarity calculation can be formulated as the following:  

% molecular universal query $\hat{Q}(i)$ and text representation $X_{\text{text}}(i)$ with temperature scaling for precision. $\ell_{MTC}$ is computed as the batch mean of the sum of the  $\ell_{g2t}$ encourages the universal query representation which encodes both 2D and 3D molecular structures, to match its corresponding text representation while contrasting it against all other text representations within the batch. Similarly, $\ell_{t2g}$ aligns the text representation with its matching molecular query. Together $\ell_{MTC}$ form a bidirectional alignment between molecular features and textual descriptions, enhancing the ability of MQ-Former to jointly represent and contrast molecules and their associated textual descriptions. 
\vspace{-15pt}
\begin{equation}
\begin{aligned}
    S(i,j) &= \frac{1}{2K} \sum_{2K} \max\limits_{t} \cos({\hat Q}_k(i), x_t(j)) \\
    S'(i,j) &= \frac{1}{T} \sum_{T} \max\limits_{k} \cos(x_t(i), {\hat Q}_k(j))
\end{aligned}
\end{equation}


Together $\ell_{MTC}$ form a bidirectional alignment between molecular features and textual descriptions in a detailed token-wise manner.
$\ell_{g2t}$ and $\ell_{t2g}$ is as written below, where $M$ is the size of the batch and $\tau$ is the temperature parameter.

\vspace{-15pt}
\begin{equation}
\begin{aligned}
    % S(i,j) = \frac{1}{2K} \sum_{2K} \max\limits_{t} cos({\hat Q}_k(i), x_t(j)) \\
    % S'(i,j) = \frac{1}{T} \sum_{T} \max\limits_{k} cos(x_t(i), {\hat Q}_k(j)) \\
    \ell_{g2t} = -\sum_{i=1}^{M} \log \frac{\exp(S(i, i) / \tau)}{\sum_{j=1}^{M} \exp(S(i, j) / \tau)} \\ 
    \ell_{t2g} = -\sum_{i=1}^{M} \log \frac{\exp(S'(i, i) / \tau)}{\sum_{j=1}^{M} \exp(S'(i, j) / \tau)}
    % \ell_{g2t} &= \sum_{i=1}^{M} \log \frac{\exp({1\over 2K}\sum_{2K} max_t cos({\hat Q}_k(i), x_t(i))/\tau)}{\sum_{j=1}^{M} \exp({1\over 2K}\sum_{2K} max_t cos({\hat Q}_k(i), x_t(j))/\tau)} \\
    % \ell_{t2g} &= \sum_{i=1}^{M} \log \frac{\exp({1\over T}\sum_{T} max_k cos(x_t(i) , {\hat Q}_k(i))/\tau)}{\sum_{j=1}^{M} exp({1\over T}\sum_{T} max_k cos(x_t(i) , {\hat Q}_k(j))/\tau)}
\end{aligned}
\end{equation}

% \begin{figure}[t]
%     \centering    \includegraphics[width=0.5\textwidth]{figs/g2t_loss_modification.pdf}
%     \caption{Molecule-text similarity for query-token contrasting. (A) Previous approach compute coarse-level similarity between molecule queries and CLS text token. (B) We propose a new approach to compute token-level similarity between molecule queries and all text tokens, which preserves molecule query diverse information. }
%     \label{fig:g2tloss_modification}
% \end{figure}
% \begin{equation}
%     \ell_{g2t} = \sum_{i=1}^{M} \log \frac{\exp\left(\max_k \cos({\hat Q}(i), X_{\text{text}}(i))/\tau\right)}{\sum_{j=1}^{M} \exp\left(\max_k \cos({\hat Q}(i), X_{\text{text}}(j))/\tau\right)}
% \end{equation}

% \begin{equation}
%     \ell_{t2g} = \sum_{i=1}^{M} \log \frac{\exp(\max_k \cos(X_{\text{text}}(i), {\hat Q}(i))/\tau)}{\sum_{j=1}^{M} \exp(\max_k \cos(X_{\text{text}}(i), {\hat Q}(j))/\tau)}
% \end{equation}
% $M$ is the size of the batch, and $\tau$ is the temperature parameter. 
% Conversely, text-to-molecule (t2g) $\ell_{t2g}$ loss computes the similarity between the text and universal query while comparing it against all other molecules in the batch:


% By optimizing combined loss of $\ell_{gtc}$ and $\ell_{t2g}$, MTC maximizes the alignment between matching molecule-text pairs and reduces similarity for non-matching pairs, thereby improving accurate mapping between molecular structures to their descriptions.
% \textbf{Molecule-text Contrasting}. The molecule-text contrasting (MTC) loss $\ell_{MTC}$ trains MQ-Former to extract molecular features that align with the related text descriptions. 
% During the optimization of the MTC loss, the self-attention layers of the molecule transformer and text transformer operate independently without sharing attention mechanisms. 
% This approach allows focusing on learning distinct representations for matching and non-matching pairs, ensuring the model can clearly differentiate between correct and incorrect molecule-text pairs.
% The 2D and 3D query tokens $\{q2_{i,k}\}$ and $\{q3_{i,k}\}$, each representing set of $K$-length query tokens of $i$-th molecule, are processed separately through the encoder blocks of their respective molecule transformers. 
% The outputs of the 2D and 3D queries are concatenated to form a $2K$-length universal query $\{q_{i,k}\}$, incorporating multi-view molecular information for the given molecule. 
% The similarity between the universal query $\{q_{i,k}\}$ and the text representation $txt_{i}$ is measured using cosine similarity, with temperature scaling applied for more precise comparisons.
% Molecule-to-text (g2t) loss computes consine similarity between universal query tokens and its corresponding text representation, comparing similarity against all other text representations in the batch. The loss computed as follows:
% \begin{equation}
%     \ell_{g2t} = \sum_{i=1}^{M} \log \frac{\exp(\max_k \cos(q_{i,k}, \text{txt}_i)/\tau)}{\sum_{j=1}^{M} \exp(\max_k \cos(q_{i,k}, \text{txt}_j)/\tau)}
% \end{equation}
% where $M$ is the size of the batch, and $\tau$ is the temperature parameter. 
% Meanwhile, text-to-molecule (t2g) loss does the opposite, computing the cosine similarity between text representation and the universal query tokens of the corresponding molecule, and comparing it to all other molecules in the batch.
% \begin{equation}
%     \ell_{t2g} = \sum_{i=1}^{M} \log \frac{\exp(\max_k \cos(\text{txt}_i, q_{i,k})/\tau)}{\sum_{j=1}^{M} \exp(\max_k \cos(\text{txt}_i, q_{j,k})/\tau)}
% \end{equation}
% By optimizing the model to maximize alignment between molecular and textual features, minimizing the combined losses in the MTC loss ensures that correct molecule-text pairs exhibit higher similarity than incorrect pairs.
% This dual directional approach enhances the model's ability to accurately map molecular structures to their corresponding textual descriptions.

\textbf{Molecule-text Matching}. $\ell_{MTM}$ is for a binary classification task to predict matching molecule-text pairs.
%%%%attention 내용 우선 삭제
% Bi-directional self-attention masks lead all text and molecular embeddings from different dimensions to share their information, guiding MQ-Former to capture fine-grained similarities between the domains.
Universal query tokens are obtained then processed through a linear classifier after mean pooling. Let $\rho({\hat Q}(i), X_{\text{text}}(i))$ denote the predicted probability that universal query $\hat Q(i)$ matches its corresponding text description $X_{\text{text}}(i)$. $\ell_{MTM}$ is calculated as follows:
{\small
\begin{equation}
\begin{aligned}
    \ell_{\text{MTM}} &= \frac{1}{M} \sum_{i=1}^{M} \left( 
    - \log \rho(\hat{Q}(i), X_{\text{text}}(i)) \right. \\
    &\quad \left. + \log \rho(\hat{Q}(i), X_{\text{text}}(j)) + \log \rho(\hat{Q}(r), X_{\text{text}}(i)) 
    \right)
\end{aligned}
\end{equation}
}
where $X_{\text{text}}(j)$, ${\hat Q}(r)$ are randomly selected negative samples from the batch. Overall, $\ell_{MTM}$ aids MQ-Former to maximize the likelihood of matched pairs and minimize mismatches, enhancing its ability to differentiate between true and false pairs.

% \textbf{Molecule-text Matching}. The MTM loss $\ell_{MTM}$ functions as a binary classification task where the model predicts whether a molecule-text pair is correctly matched.
% In this task, the molecule transformer and text transformer share self-attention layers, enabling the text embeddings to share information with the 2D and 3D query tokens.
% This setup allows the model to capture fine-grained similarities between a molecule and its corresponding text, helping the model comprehend the deeper relationships between the modalities.
% Same as the case in MTC loss computation, universal query is obtained by concatenating output from the 2D and 3D molecule transformers.
% The model processes these query tokens with a linear classifier after mean pooling of the query representations.
% Let $\rho(d2_i,d3_i,cap_i)$ denote the predicted probability that the universal query $q_i$, obtained by concatenating the $d2_i$ and $d3_i$, matches the text description $cap_i$ match, and $l_{MTM}$ is defined as follows:
% \begin{equation}
%     \ell_{MTM} = \frac{1}{M} \sum_{i=1}^{M} \left( - \log \rho(d2_i, d3_i, cap_i) + \log \rho(d2_i, d3_i, cap_j) + \log \rho(d2_r, d3_r, cap_i) \right)
% \end{equation}
% where $cap_j$, $d2_r$ and $d3_r$ are randomly selected negative samples from the batch, sampled from a uniform distribution $U(1,M)$.
% % The model processes these query tokens using mean pooling, followed by a linear classifier to make classification predictions.
% The MTM loss trains the model to capture relationships between molecular structures and their associated text by maximizing the likelihood of correct matches and discouraging incorrect ones, improving its ability to differentiate between true and false matches.

\textbf{Molecule Captioning}. $\ell_{MCap}$ is designed to generate accurate text descriptions based on multi-view query tokens.
% A multi-modal causal self-attention masking strategy ensures that molecule query tokens rely on cross-attention with molecular embeddings for text generation, preventing direct access to text tokens.
Text is generated auto-regressively, where each token is predicted sequentially based on the corresponding molecular queries. 
Instead of harnessing universal queries, $\ell_{MCap}$ sums up separate losses for 2D and 3D query tokens, ensuring that each query token retains its unique dimensional information for high captioning ability.
The $\ell_{MCap}$ is defined as follows:
\begin{equation}
\begin{aligned}
    \ell_{MCap} = - \frac{1}{M} \sum_{i=1}^{M} \log p(X_{\text{text}}(i)|\hat{Q}_{2d}(i)) \\
    - \frac{1}{M} \sum_{i=1}^{M} \log p(X_{\text{text}}(i)|\hat{Q}_{3d}(i))
\end{aligned}
\end{equation}
where $p(X_{\text{text}}|\hat Q_{2d})$ and $p(X_{\text{text}}|\hat Q_{3d})$ represents the probability of generating the text description based independently on 2D or 3D molecular queries, respectively.
% \begin{equation}
%     \ell_{MCap_2d} = - \frac{1}{M} \sum_{i=1}^{M} \log p(\text{txt}_i | q2_i)
% \end{equation}
% \begin{equation}
%     \ell_{MCap_3d} = - \frac{1}{M} \sum_{i=1}^{M} \log p(\text{txt}_i | q3_i)
% \end{equation}
While the other two losses focus on aligning or matching molecule-text pairs, the $\ell_{MCap}$ directly impacts the ability to generate new text based on molecular representations, encouraging further diverse feature learning in correspondence to our modified multi-token contrasting loss.
Given its critical role, we assigned a greater weight $\alpha$, guiding MQ-Former to generate quality tokens for text-generation tasks. 


% \textbf{Molecule Captioning}. The MCap loss $\ell_{MCap}$ is designed to generate accurate textual descriptions of molecules based on their structural representations. 
% To achieve this, the model employs a bidirectional masking strategy that prevents molecule query tokens from directly accessing text tokens, forcing the text generation to rely on cross-attention to extract relevant molecular features.
% Text is generated auto-regressive manner, where each token is predicted sequentially based on the molecular query tokens. 
% Unlike the other two losses, which calculate a single loss for the universal query, the MCap loss optimizes the captioning ability for each dimension's query tokens separately by computing two separate losses:
% \begin{equation}
%     \ell_{MCap_2d} = - \frac{1}{M} \sum_{i=1}^{M} \log p(\text{txt}_i | q2_i)
% \end{equation}
% \begin{equation}
%     \ell_{MCap_3d} = - \frac{1}{M} \sum_{i=1}^{M} \log p(\text{txt}_i | q3_i)
% \end{equation}
% where $p(\text{txt}|q)$ is the probability of MQ-Former generating the text description based independently on either 2D or 3D molecular queries.
% This ensures that each query token retains its unique dimensional information while improving the captioning capability.
% The MCap loss, sum of two distinct loss terms, optimizes the model’s ability to align molecular structures with their textual outputs, leading to more accurate and precise molecule descriptions.
% While the other two losses focus on matching or distinguishing between existing molecule-text pairs, the MCap loss directly impacts the model's ability to generate new text based on molecular representations.
% Given the critical role, we assigned a weight $\alpha$ to the MCap loss during multi-objective training, guiding the model to prioritize its text-generation capabilities. 


\subsection{Stage 2: Specializing LLaMA2 for Molecule Captioning}
In Stage 2, MQ-Former is further trained alongside LLaMA2 to generate molecular descriptions. The goal is to enhance MQ-Former's ability to produce universal queries that are not only aligned with the textual space but better interpretable by LLaMA2. In this stage, textual descriptions are tokenized and decoded using LLaMA tokenizer. Universal query tokens, 1D SMILES are given as input with prompt.
Autoregressive generation loss of LLaMA2 is used for training the framework with LoRA~\citep{hu2021lora}. Detailed LoRA setting are in Appendix A3. 

% In Stage 2, MQ-Former associated with the pretrained 3D and 2D graph encoders, Uni-mol and MAT, is further trained in connection with the LLaMA2 language model to generate molecular description. 
% The objective of this stage is to further train MQ-Former's ability to produce universal queries that are easier for the LLaMA2 to interpret as well as well-aligned with the textual space and enriched with multi-view molecular information.
% During this phase, the graph encoders remain frozen as they were Stage 1, while MQ-Former is fine-tuned using the MTC and MTM losses. 
% Instead of optimizing the MCap loss, the captioning loss is derived from the language model's captioning result, utilizing the universal query.
% To be specific, the universal queries are concatenated with the molecule's 1D SMILES representation and a prompt, which are then fed into LLaMA2 as an input. 
% To fine-tune LLaMA2 effectively, we adopt the LoRA (Low-Rank Adaptation) strategy \citep{hu2021lora}, focusing on a small subset of parameters rather than the entire model. 
% The detailed parameter settings for LoRA are provided in Appendix A.3.

% The LoRA parameters are set to $r$=$8$, $\alpha$=$32$, and a dropout rate of 0.1, and are integrated into the [\(k_{proj}, v_{proj}, q_{proj}, o_{proj}, gate_{proj}, up_{proj}, down_{proj}\)] modules. 
% This setup introduces 19 million trainable parameters, accounting for only 0.29\% of the total parameters in LLaMA2-7B, allowing efficient fine-tuning. 


\section{Experiments}

% \subsection{Datasets}
% \subsection{Experimental Setup}

\subsection{Datasets}
\textbf{PubChem324K}. For molecule-text alignment and molecule captioning, we collected 324k molecular SMILES-text pairs from PubChem~\citep{kim2021pubchem}. 
% 2D graph features are constructed using~\cite{maziarka2020molecule} codes and 3D molecular conformations are calculated using MMFF algorithm in RDKit~\citep{landrum2013rdkit}. 
2D graph features were constructed using~\cite{maziarka2020molecule}, and 3D conformers were generated with ETKDG and optimized using the MMFF algorithm in RDKit~\citep{landrum2013rdkit}.
We follow dataset construction as provided in 3D-MoLM~\citep{li2024towards} which also requires 3D molecular conformations. 
High-quality subset of 15k pairs with text longer than 19 words are sampled for train, valid, test datasets. Shorter pairs are used for pretraining. 
The statistics for the final PubChem324k dataset used in this study are presented in Appendix Table ~\ref{tab:pubchem_stat}. 

\subsection{Benchmark models}
Baseline models include (1) pretrained language models for science: Sci-BERT~\citep{beltagy2019scibert}, (2) models with molecule-text contrastive learning: KV-PLM~\citep{zeng2022deep}, MoMu~\citep{su2022molecular}, MoleculeSTM~\citep{liu2023multi} and (3) models with Q-Former modules: MolCA~\citep{liu2023molca}, 3D-MoLM~\citep{li2024towards}, UniMoT~\citep{zhang2024unimot}. 
For molecule captioning, we also benchmark Llama2-7B and 2D-MoLM, each as a variant of 3D-MoLM using 1D and 2D information along with MolT5~\citep{edwards2022translation} and InstructMol~\citep{cao2023instructmol}.


% \begin{wrapfigure}{r}{0.5\textwidth} 
%     \centering
%     \resizebox{0.5\textwidth}{!}{ % Ensure the table fits within the wrap figure
%     \begin{tabular}{cccc}
%     \hline
%     \textbf{Subset}   & \textbf{\#Molecule-Text Pairs} & \textbf{\#Min Words} & \textbf{\#Avg Words} \\ \hline
%     Pretrain          & 290,507                       & 1                    & 17.84               \\
%     Train             & 11,753                        & 20                   & 57.24               \\
%     Valid             & 977                           & 20                   & 58.31               \\
%     Test              & 1,955                         & 20                   & 55.21               \\ \hline
%     \end{tabular}
%     }
%     \caption{PubChem324k dataset statistics}
%     \label{tab:pubchem_stat}
% \end{wrapfigure}



\section{Results}

\subsection{Molecule-Text Retrieval}
We evaluate MV-CLAM for molecule-text retrieval on the PubChem324k dataset. 
%이거 필요한가???
% After pretraining for 35 epochs, the model is fine-tuned on the training subset with longer captions for 10 epochs. 
% Baselines include 1) pretrained language models for science: Sci-BERT~\citep{beltagy2019scibert}, 2) models utilizing molecule-text contrastive learning objectives: KV-PLM~\citep{zeng2022deep}, MoMu~\citep{su2022molecular} and 3) models trained via Q-Former's multi-objective approach for molecule-text alignment: 3D-MoLM~\citep{li2024towards}, UniMoT~\citep{zhang2024unimot}. 
We perform two rounds of evaluation on molecule-to-text and text-to-molecule retrieval tasks, using Accuracy and Recall@20 metrics: within batch size of 64 and is across the entire test set. We report baseline performances as written in literature~\citep{li2024towards, zhang2024unimot}. 
% We only compare results without GPT-3.5 enrichment for simplification as it does not significantly affect performance. 

% MV-CLAM outperforms baseline models that represent molecules as either 1D SMILES strings (Sci-BERT, KV-PLM), 2D graphs (MoMu, UniMoT) and 3D conformers (3D-MoLM) as shown in Table~\ref{tab:retrieval_results}.
As shown in Table~\ref{tab:retrieval_results}, MV-CLAM outperforms baseline approaches that represent molecules as 1D SMILES strings, 2D graphs, or 3D conformers.
% Additionally, results are achieved within a total of 45 epochs, comparative to 3D-MoLM that trains for 60 epochs. 
% We attribute this strong performance to our modification of the Q-Former's multi-objective loss within the multi-view molecule representation framework. 
% Not only do we incorporate a unified query combining both 2D and 3D queries for diversified contrasting with text tokens, we amplify the molecule captioning loss. 
We attribute our superior performance to (1) our use of a universal query that aligns both 2D and 3D molecular representations to a consistent text, and (2) a modified multi-objective loss, designed to maximize query diversity and prevent over-reliance on dominant alignment patterns.
% Additionally, the amplification on molecule captioning loss ensures that the text transformer is better equipped to decode molecular descriptions under 2D and 3D conditions, maximizing information extraction. This allows our model to capture both fine-grained atomic interactions and high-level chemical semantics, enhancing interpretability and expressiveness in molecular language modeling.

% While good retrieval performance is often indicative of strong cross-modal understanding that benefit captioning tasks as demonstrated in previous studies~\citep{li2024towards, li2023blip}, the relationship is not absolute. Hence we proceed to evaluate the performance of molecule captioning. 

\begin{table*}[t]
\centering
\caption{Molecule-Text retrieval performance in batch and test set for different models. The highest value in each category is indicated in bold, and the second highest value is underlined. For MoleculeSTM* and MolCA*, we report results from UniMoT \citep{zhang2024unimot}.
}
\resizebox{\textwidth}{!}{
\scriptsize
\tiny
\begin{tabular}{lcccccccc}
\hline
\multicolumn{1}{c}{}                        & \multicolumn{4}{c}{Retrieval in batch}                                                 & \multicolumn{4}{c}{Retrieval in test set}                         \\ \cline{2-9} 
\multicolumn{1}{c}{}                        & \multicolumn{2}{c}{M2T}                   & \multicolumn{2}{c}{T2M}                   & \multicolumn{2}{c}{M2T}        & \multicolumn{2}{c}{T2M}         \\ \cline{2-9} 
\multicolumn{1}{c}{\multirow{-3}{*}{Model}} & ACC            & R@20            & ACC            & R@20            & ACC            & R@20           & ACC            & R@20           \\ \hline
\multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{1D SMILES}}                                                                                                                      \\ \hline
Sci-BERT\citep{beltagy2019scibert}                                   & 85.32          & 98.74           & 84.20           & 98.43           & 41.67          & 87.31          & 40.18          & 86.77          \\
KV-PLM\citep{zeng2022deep}                                     & 86.05          & 98.63           & 85.21          & 98.47           & 42.80           & 88.46          & 41.67          & 87.80           \\ \hline
\multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph}}                                                                                                                                              \\ \hline
MoMu-S\citep{su2022molecular}                                     & 87.58          & 99.24           & 86.44          & 99.38           & 47.29          & 90.77          & 48.13          & 89.92          \\
MoMu-K\citep{su2022molecular}                                    & 88.23          & 99.41           & 87.29          & 99.42           & 48.47          & 91.64          & 49.46          & 90.73          \\ 
MoleculeSTM* \citep{liu2023multi} & 90.50 & 99.60 & 88.60 & 99.50 & 52.70 & 92.90 & 53.20 & 92.50 \\
MolCA* \citep{liu2023molca}  & 92.60 & 99.80 & 91.30 & 99.50 & 67.90 & 94.40 & 68.60 & 93.30 \\
\hline
\multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph + Tokenizer}}                                                                                                                                          \\ \hline
UniMoT\citep{zhang2024unimot}                                   & 93.60    & {\textbf{100.0}}  & 92.70    & 99.40     & 69.50  & 96.30    & 69.80     & 94.40    \\ \hline
\multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{3D Conformer}}                                                                                                                                          \\ \hline
3D-MoLM\citep{li2024towards}                                    & 93.50     & {\textbf{100.0}}  & 92.89    & 99.59     & 69.05    & 95.91    & 70.13     & 94.88    \\ \hline

\multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph + 3D Conformer}}                                                                                                                                                 \\ \hline
MV-CLAM w/ \textsc{single-token contrasting }                                      & {\ul 96.57} & {\ul 99.95}     & {\ul 97.03} & \textbf{99.95}  & {\ul 76.32} & {\ul 96.57} & {\ul 77.03} & {\ul 96.42} \\ 
 
\multicolumn{1}{l}{MV-CLAM w/ \textsc{multi-token contrasting}}   & \textbf{97.34} & {\ul 99.95}     & \textbf{97.19} & {\ul 99.90}  & \textbf{78.67} & \textbf{96.98} & \textbf{79.34} & \textbf{96.93} \\ 
\hline 
\end{tabular}

}
\label{tab:retrieval_results}
\end{table*}


% \begin{table*}[t]
% \centering
% \caption{Molecule-Text retrieval performance in batch and test set for different models. The highest value in each category is indicated in bold, and the second highest value is underlined. For MoleculeSTM* and MolCA*, we report results from UniMoT \citep{zhang2024unimot}.
% }
% \resizebox{\textwidth}{!}{
% \scriptsize
% \small
% \begin{tabular}{lcccccccc}
% \hline
% \multicolumn{1}{c}{}                        & \multicolumn{4}{c}{Retrieval in batch}                                                 & \multicolumn{4}{c}{Retrieval in test set}                         \\ \cline{2-9} 
% \multicolumn{1}{c}{}                        & \multicolumn{2}{c}{M2T}                   & \multicolumn{2}{c}{T2M}                   & \multicolumn{2}{c}{M2T}        & \multicolumn{2}{c}{T2M}         \\ \cline{2-9} 
% \multicolumn{1}{c}{\multirow{-3}{*}{Model}} & ACC            & R@20            & ACC            & R@20            & ACC            & R@20           & ACC            & R@20           \\ \hline
% \multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{1D SMILES}}                                                                                                                      \\ \hline
% Sci-BERT                                  & 85.32          & 98.74           & 84.20           & 98.43           & 41.67          & 87.31          & 40.18          & 86.77          \\
% KV-PLM                                    & 86.05          & 98.63           & 85.21          & 98.47           & 42.80           & 88.46          & 41.67          & 87.80           \\ \hline
% \multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph}}                                                                                                                                              \\ \hline
% MoMu-S                                     & 87.58          & 99.24           & 86.44          & 99.38           & 47.29          & 90.77          & 48.13          & 89.92          \\
% MoMu-K                                 & 88.23          & 99.41           & 87.29          & 99.42           & 48.47          & 91.64          & 49.46          & 90.73          \\ 
% MoleculeSTM* & 90.50 & 99.60 & 88.60 & 99.50 & 52.70 & 92.90 & 53.20 & 92.50 \\
% MolCA*  & 92.60 & 99.80 & 91.30 & 99.50 & 67.90 & 94.40 & 68.60 & 93.30 \\
% \hline
% \multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph + Tokenizer}}                                                                                                                                          \\ \hline
% UniMoT                                   & {\ul 93.60}     & {\textbf{100.0}}  & 92.70    & 99.40     & {\ul 69.50}    & {\ul 96.30}    & 69.80     & 94.40    \\ \hline
% \multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{3D Conformer}}                                                                                                                                          \\ \hline
% 3D-MoLM                                  & 93.50     & {\textbf{100.0}}  & {\ul 92.89}    & {\ul 99.59}     & 69.05    & 95.91    & {\ul 70.13}     & {\ul 94.88}    \\ \hline

% \multicolumn{9}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph + 3D Conformer}}                                                                                                                                                 \\ \hline
% MV-CLAM                                       & \textbf{96.57} & {\ul 99.95}     & \textbf{97.03} & \textbf{99.95}  & \textbf{76.32} & \textbf{96.57} & \textbf{77.03} & \textbf{96.42} \\ \hline
% \end{tabular}
% }
% \label{tab:retrieval_results}
% \end{table*}


\subsection{Molecule Captioning} \label{sec:caption}
Following previous studies\citep{li2024towards}, we use BLEU, ROUGE, METEOR metrics to evaluate molecule captioning on the PubChem324k dataset. 
% As outlined in Section 4.2, we apply LoRA to fine-tune LLaMA2 for the molecular domain, training 10 epochs on the pretraining subset and an additional 10 epochs on the training subset. 
Table \ref{tab:captioning_results} shows MV-CLAM consistently outperforms all baselines with notable performance gain from our modified multi-objective loss.
% LLaMA2-7B and 2D-MoLM results are reported as generalists, those that are trained with combination of 3D-MolT dataset~\citep{li2024towards}. 
PubChem324k dataset includes molecular nomenclature, which our model accurately generates in addition to information on clinical usage and chemical properties.
% Table~\ref{tab:comparison} highlights the model’s ability to identify correct IUPAC(International Union of Pure and Applied Chemistry) nomenclature and generic drug names.
Appendix Table~\ref{tab:comparison} highlights the model’s ability to correctly identify International Union of Pure and Applied Chemistry (IUPAC) nomenclature and generic drug names that differ significantly in language model processing. 
IUPAC names follow systematic chemical rules, making them complex and highly structured, while generic drug names are more standardized and commonly used in clinical contexts. 
Despite these differences, MV-CLAM successfully identifies both types of names, showcasing its ability to handle a range of linguistic and chemical complexities.
% Moreover, in some cases (Table~\ref{tab:comparison}. Rifapentine), MV-CLAM generates literature-matching captions that are not present in the ground truth, demonstrating its capacity to produce highly informed and contextually relevant outputs. 
% This highlights the model’s potential to enhance molecular understanding using enriched and accurate text generation.
Moreover, MV-CLAM demonstrates its capacity to generate literature-matching captions absent in ground truth, as seen in the case of \textit{Rifapentine} (Appendix Table~\ref{tab:comparison}), highlighting the ability to produce highly informed outputs.

\begin{table*}[t]
\centering
\caption{Molecule captioning performance across models. The highest value in each category is bolded, and the second highest is underlined. Models marked with \textdagger were pretrained on larger datasets, as noted in their original papers. Results for InstructMol and MolCA are from UniMoT \citep{zhang2024unimot}, with MolCA evaluated in two variations using OPT-125M (small) and OPT-1.3B (large) as language models. 
% MV-CLAM$^{\ddagger}$ denotes performance achieved using training with the original multi-contrastive loss suggested in Q-Former~\citep{li2023blip}.
}
\resizebox{\textwidth}{!}{
\scriptsize
\small
\begin{tabular}{lccccccc}
\hline
\multicolumn{1}{c}{}                        & BLEU-2        & BLEU-4        & ROUGE-1       & ROUGE-2       & ROUGE-L       & METEOR        \\ \hline
\multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{1D SMILES}}                                                                                              \\ \hline
\multicolumn{1}{l}{MolT5-Small\citep{edwards2022translation}}             & 22.53         & 15.23         & 30.44         & 13.45         & 20.30         & 23.98         \\
\multicolumn{1}{l}{MolT5-Base\citep{edwards2022translation}}              & 24.51         & 16.61         & 32.19         & 14.04         & 21.35         & 26.10         \\
\multicolumn{1}{l}{MolT5-Large\citep{edwards2022translation}}             & 25.87         & 17.28         & 34.07         & 16.42         & 23.41         & 28.04         \\
\multicolumn{1}{l}{Llama2-7B\textdagger \citep{li2024towards}}              & 27.01         & 20.94         & 35.76         & 20.68         & 28.88         & 32.11         \\ \hline
\multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph}}                                                                                               \\ \hline
\multicolumn{1}{l}{MoMu-Small\citep{su2022molecular}}              & 22.86         & 16.01         & 30.98         & 13.65         & 20.75         & 24.35         \\
\multicolumn{1}{l}{MoMu-Base\citep{su2022molecular}}               & 24.74         & 16.77         & 32.45         & 14.62         & 22.09         & 27.16         \\
\multicolumn{1}{l}{MoMu-Large\citep{su2022molecular}}              & 26.34         & 18.01         & 34.75         & 16.86         & 24.76         & 28.73         \\ 
\multicolumn{1}{l}{2D-MoLM\textdagger\citep{li2024towards}}              & 27.15         & 21.19         & 36.02         & 20.76         & 29.12         & 32.28         \\
\multicolumn{1}{l}{InstructMol*\citep{cao2023instructmol}}    & 18.90 & 11.70 & 27.30 & 11.80 & 17.80 & 21.30  \\
\multicolumn{1}{l}{MolCA-Small*\citep{liu2023molca}}    & 25.90 & 17.50 & 34.40 & 16.60 & 23.90 & 28.50 \\ 
\multicolumn{1}{l}{MolCA-Large*\citep{liu2023molca}}    & 28.60 & 21.30 & 36.20 & 21.40 & 29.70 & 32.60 \\ 
\hline
\multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph + Tokenizer}}                                                                                           \\ \hline

\multicolumn{1}{l}{UniMoT\citep{zhang2024unimot}}                 & 31.30   & 23.80  & 37.50   & 23.70  & 33.60   & 34.80   \\ \hline

\multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{3D Conformer}}                                                                                           \\ \hline

\multicolumn{1}{l}{3D-MoLM\citep{li2024towards}}                 & 30.32   & 22.52   & 36.84   & 22.32   & 31.23   & 33.06   \\ \hline
\multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph + 3D Conformer}}                                                                                                  \\ \hline

\multicolumn{1}{l}{MV-CLAM w/ \textsc{single-token contrasting}}                    & {\ul 31.75}& {\ul 24.48}& {\ul 40.43}& {\ul 25.72}& {\ul 33.79}& {\ul 36.54} \\ 

\multicolumn{1}{l}{MV-CLAM w/ \textsc{multi-token contrasting}}                    & \textbf{32.32}& \textbf{25.11}& \textbf{40.87}& \textbf{26.48}& \textbf{34.79}& \textbf{36.87} \\ \hline 
\end{tabular}
}
\label{tab:captioning_results}
% \vspace{-15pt}
\end{table*}



% \begin{table*}[t]
% \centering
% \caption{Molecule captioning performance across models. The highest value in each category is bolded, and the second highest is underlined. Models marked with \textdagger were pretrained on larger datasets, as noted in their original papers. Results for InstructMol and MolCA are from UniMoT \citep{zhang2024unimot}, with MolCA evaluated in two variations using OPT-125M (small) and OPT-1.3B (large) as language models.}
% \resizebox{\textwidth}{!}{
% \scriptsize
% \small
% \begin{tabular}{lcccccc2c}
% \hline
% \multicolumn{1}{c}{}                        & BLEU-2        & BLEU-4        & ROUGE-1       & ROUGE-2       & ROUGE-L       & METEOR        \\ \hline
% \multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{1D SMILES}}                                                                                              \\ \hline
% \multicolumn{1}{l}{MolT5-Small}             & 22.53         & 15.23         & 30.44         & 13.45         & 20.30         & 23.98         \\
% \multicolumn{1}{l}{MolT5-Base}              & 24.51         & 16.61         & 32.19         & 14.04         & 21.35         & 26.10         \\
% \multicolumn{1}{l}{MolT5-Large}             & 25.87         & 17.28         & 34.07         & 16.42         & 23.41         & 28.04         \\
% \multicolumn{1}{l}{Llama2-7B\textdagger}              & 27.01         & 20.94         & 35.76         & 20.68         & 28.88         & 32.11         \\ \hline
% \multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph}}                                                                                               \\ \hline
% \multicolumn{1}{l}{MoMu-Small}              & 22.86         & 16.01         & 30.98         & 13.65         & 20.75         & 24.35         \\
% \multicolumn{1}{l}{MoMu-Base}               & 24.74         & 16.77         & 32.45         & 14.62         & 22.09         & 27.16         \\
% \multicolumn{1}{l}{MoMu-Large}              & 26.34         & 18.01         & 34.75         & 16.86         & 24.76         & 28.73         \\ 
% \multicolumn{1}{l}{2D-MoLM\textdagger}              & 27.15         & 21.19         & 36.02         & 20.76         & 29.12         & 32.28         \\
% \multicolumn{1}{l}{InstructMol*}    & 18.90 & 11.70 & 27.30 & 11.80 & 17.80 & 21.30  \\
% \multicolumn{1}{l}{MolCA-Small*}    & 25.90 & 17.50 & 34.40 & 16.60 & 23.90 & 28.50 \\ 
% \multicolumn{1}{l}{MolCA-Large*}    & 28.60 & 21.30 & 36.20 & 21.40 & 29.70 & 32.60 \\ 
% \hline
% \multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph + Tokenizer}}                                                                                           \\ \hline
% \multicolumn{1}{l}{UniMoT}                 & {\ul 31.30}   & {\ul 23.80}   & {\ul 37.50}   & {\ul 23.70}   & {\ul 33.60}   & {\ul 34.80}   \\ \hline

% \multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{3D Conformer}}                                                                                           \\ \hline
% \multicolumn{1}{l}{3D-MoLM}                 & 30.32   & 22.52   & 36.84   & 22.32   & 31.23   & 33.06   \\ \hline
% \multicolumn{7}{l}{\cellcolor[HTML]{EFEFEF}\textbf{2D Graph + 3D Conformer}}                                                                                                  \\ \hline
% \multicolumn{1}{l}{MV-CLAM}                    & \textbf{31.75}& \textbf{24.48}& \textbf{40.43}& \textbf{25.72}& \textbf{33.79}& \textbf{36.54} \\ \hline
% \end{tabular}
% }
% \label{tab:captioning_results}
% \vspace{-15pt}
% \end{table*}


\subsection{Effectiveness of MQ-Former} \label{sec: mqformer}
In this section, we substantiate the effectiveness of incorporating multi-view chemical information within the MQ-Former architecture. 
We conduct both quantitative and qualitative analysis to compare our superiority to the prior single-view alignment using Q-Former. Molecular encoders are identically set for the ablation studies.
% Hyperparameters are optimized for each graph encoder setting. 
% Table~\ref{tab:captioning_results_ablation} shows that the combination of both modalities leads to a notable synergistic effect, improving the model's overall performance. 
\begin{table}[t]
\centering
\caption{Captioning Performance Comparison. We compare the captioning performance using the original Q-Former module for each single-view and multi-view(pre-combined) molecular embeddings. MV-CLAM$^{\ddagger}$ denotes performance achieved using multi-token contrasting while the other, single-token contrasting.}
% MV-CLAM$^{\ddagger}$ denotes performance achieved using training with the original multi-contrastive loss suggested in Q-Former~\citep{li2023blip}}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\hline
Model      & B-2 & B-4 & R-1 & R-2 & R-L & M \\ \hline
2D only             & 29.72         & 22.26         & 38.22         & 23.45         & 31.61         & 34.22         \\
3D only              & 29.45         & 22.03         & 37.86         & 23.11         & 31.83         & 33.79         \\

Multi-view             & 29.80        & 22.70    & 39.07 & 24.92 & 33.09 & 35.49 \\        
MV-CLAM            & {\ul 31.75}& {\ul 24.48}& {\ul 40.43}& {\ul 25.72}& {\ul 33.79}& {\ul 36.54}        \\
\multicolumn{1}{l}{MV-CLAM$^{\ddagger}$}                    & \textbf{32.32}& \textbf{25.11}& \textbf{40.87}& \textbf{26.48}& \textbf{34.79}& \textbf{36.87} \\ \hline 
\end{tabular}
}
\label{tab:caption_multiview}
\end{table}
% \vspace{-15pt}

As a quantitative analysis, we compared our approach to prior works that independently align 2D embeddings or 3D embeddings with Q-Former. We also evaluated an alternative setup where multi-view molecular embeddings were pre-combined and aligned to text with Q-Former. 
We show that the combination of both modalities leads to a notable synergistic effect, improving the model's overall performance (Table~\ref{tab:caption_multiview}). 
Coupled with our modified contrastive loss, the simultaneous alignment of both modalities using MQ-Former ensures that critical information is utilized, leading to more robust and detailed description predictions.
% Coupled with our modified contrastive loss, our model gains a richer understanding of molecular properties which in turn improves accuracy and expressiveness of molecule captioning. 
% The alignment of both modalities ensures that critical information from each is utilized, resulting in more robust predictions that better capture details of chemical structures.
% This synergy supports the hypothesis that multi-modal fusion, when carefully orchestrated, can surpass the limitations of single-modal approaches in understanding complex molecular characteristics. 
% Furthermore, the simultaneous alignment of both modalities using MQ-Former allows each query token set to adaptively attend to separate words within the text for further fine-grained alignment. 
Our framework outperforms the setting where multi-view embeddings are pre-combined and aligned to text using a single Q-Former module. Overall, the results supports the hypothesis that well-orchestrated multi-view fusion can surpass the limitations of single-view approaches to capture diverse complementary characteristics within molecules. 

% \begin{table*}[b]
% \centering
% \caption{Molecule Captioning Ablation Study}
% \resizebox{0.9\textwidth}{!}{
% \scriptsize
% \tiny
% \begin{tabular}{lccccccc}
% \hline
% \multicolumn{1}{c}{}                        & BLEU-2        & BLEU-4        & ROUGE-1       & ROUGE-2       & ROUGE-L       & METEOR        \\ \hline
% \multicolumn{1}{l}{2D-Qformer}             & 29.72         & 22.26         & 38.22         & 23.45         & 31.61         & 34.22         \\
% \multicolumn{1}{l}{3D-Qformer}              & 29.45         & 22.03         & 37.86         & 23.11         & 31.83         & 33.79         \\
% \multicolumn{1}{l}{Ours}             & \textbf{31.75}& \textbf{24.48}& \textbf{40.43}& \textbf{25.72}& \textbf{33.79}& \textbf{36.54}         \\\hline
% \end{tabular}
% }
% \end{table*}



We exemplify two case studies to interpret how each transformer module and modality focus on distinct aspects of the molecule and its corresponding text. 
These qualitative studies provide insight into the alignment process by analyzing how different views contribute to the comprehensive understanding of molecular structures and their textual descriptions. 

\textbf{Case Study 1: Visualizing Attention Maps for 2D and 3D Query Tokens.} 
Embedding grounded on different latent spaces and dimensions differently align molecular information to text.
Visualization of the distinct alignment is performed by extracting and comparing the attention maps of the shared self-attention layers when processing 2D and 3D query tokens respectively with text tokens. 


With multi-token contrasting loss, each query token attends distinctly to individual tokens in the captioning sentence, exhibiting diverse attention scores (Appendix Figure~\ref{fig:attn_map_total}). 
While query maintaining diversity, 2D query tokens effectively capture 2D-related terms - such as \textit{boiling point} - focusing on chemical and material properties that may be overlooked in 3D settings.
Conversely, 3D query tokens capture 3D-specific structural information, such as \textit{bis (2-dimethylamino)ethyl)}, informed by 3D spatial coordinates. 
In contrast, when MQ-Former is trained with the original contrastive loss, it not only lacks diversity among query tokens but also struggles to properly align with 2D- and 3D-related terms.

% In the first example, only 2D queries assign exceptionally high attention weights to the word '\textit{water}' (Appendix Figure~\ref{fig:attnmap_2d}). 
% The discrepancy between two attention maps implies that 2D query tokens efficiently focus on chemical and material properties that may be neglected in 3D settings.
% In contrast, for the sentences containing of structural equation information, 3D attention map shows strong attention to positions inherent in molecular formula (Appendix Figure~\ref{fig:attnmap_3d}).
% Significant attention is assigned on the number '\textit{3}' in 3D attention map, less pronounced in the 2D attention map.
% This suggests that the 3D query tokens, informed by 3D spatial coordinates, are more attuned to the structural aspects of the molecule.
% In summary, 2D and 3D query tokens each focus on different aspects within the same sentence, complementing each other to prevent critical information from being missed and thereby leading to more informative and accurate molecule descriptions.

\textbf{Case Study 2: Comparing molecule captions with 2D-Qformer and 3D-Qformer.}
% Figure \ref{fig:caption_ablation} illustrates the difference between the uni-modal Q-Former ablation models and ours. 
We illustrates the difference in captioning results between the uni-modal Q-Former ablation models and ours demonstrating the effects of utilizing multi-view molecular understanding in text generation (Appendix Figure~\ref{fig:caption_ablation}).
% The 2D and 3D uni-modal ablations struggle to fully capture structures complex and large, such as '\textit{(R)-3-hydroxytriacontanoyl-CoA}'. 
% The ablation models fail to retain sufficient structural information to differentiate long carbon chains, focusing only on preserving characteristic functional groups such as carboxylic acid and sulfonate groups. 
% Sulfonate and phosphonate groups are often considered as bioisosteric replacements for carboxylic acids in medicinal chemistry~\citep{macchiarulo2007exploring} due to their structural similarity (tetrahedral geometry around the central atom) and anionic nature. 
% In contrast, our model succeeds in preserving both the detailed functional group information and the long carbon chains, enabling the generation of correct nomenclature and descriptive information.
The 2D and 3D uni-modal ablations struggle to fully capture complex and large structures like `\textit{(R)-3-hydroxytriacontanoyl-CoA}'. The ablation models fail to retain sufficient structural information required to differentiate long carbon chains with their functional groups.
However, our model captures not only carboxylic acid but also phosphonate groups, which are often considered bioisosteric replacements for sulfonate acids in medicinal chemistry due to their structural similarity~\citep{macchiarulo2007exploring}.
In comparison, the ablation models only managed to capture one of these groups, indicating that multi-view approach enables the generation of accurate nomenclature and richer descriptive information.


\section{Conclusion}

In this paper, we introduce MV-CLAM equipped with MQ-Former, a novel cross-modal projector. 
The essence of cross-modal projection lies in aligning the enriched molecular representation spaces with the text space of language models. 
Our architecture successfully retains complementary information from multiple dimension into a single universal token easily interpreted by large language models for molecule description tasks. 
Extensive experiments demonstrate that MV-CLAM has successfully fine-tunes large language models for molecule understanding, including molecule-text retrieval and molecule captioning tasks, with potential for broader applications.

\section{Limitations}
For future work, we aim to extend this framework to incorporate additional molecular representations, including other chemical structures, proteomics, and multiomics data. By aligning more views within MV-CLAM's architecture, we anticipate improved navigation of the drug space and a deeper understanding of molecular interactions across biological contexts. Additionally, curating larger molecule-text datasets is expected to enhance the model's performance and its ability to generalize to subtle molecular variations.

% certain limitations remain. First, while our current framework focuses on 2D and 3D structural views, more views can be aligned simultaneously using our architecture. Extending it to fully incorporate other molecular representations like protein structures or multi-omics data. 

% We demonstrated the scalability of MQ-Former across multiple views, including 2D and 3D molecular structures. 
% 
% By integrating these views, MQ-Former can more comprehensively navigate the drug space and enhance our understanding of molecular interactions across diverse biological contexts. 
% Moreover, we anticipate verifying MQ-Former’s model-agnostic capabilities, ensuring that it can be effectively integrated into various pretrained language models. 
% This could open opportunities for the model to serve as a universal tool for molecular understanding across disciplines, ranging from chemistry to genomics. 
% Future research could also explore cross-modal transfer learning for related tasks such as molecular property prediction, bioactivity forecasting, and drug discovery optimization.

% By integrating multi-view information, our approach allows for the consideration of features that would be overlooked by relying on a single view. 
% Rather than utilizing multiple Q-Formers for each case, we designed cross-modal projector that is capable of processing 2D and 3D information together, sharing self-attention weights between them to lessen the computational burden while retaining the ability to comprehensively analyze both representations. 
% As a results, we could not only improved molecule-text retrieval and molecule captioning performance when fine-tuned with LLaMA2, but it also demonstrated strong potential in zero-shot molecule editing experiments.
% Through case studies, we could demonstrate that using 2D and 3D query tokens led to more accurate and enriched molecule descriptions, as each set of tokens from different views contributed distinct semantic influences to the components within the sentences.

% However, there are still limitations to the approach. 
% Since the training scheme of MV-CLAM requires multi-stage approach along with the multiple phases within each stage, it inevitably introduces additional complexity compared to training everything in one go. 
% A more streamlined approach, such as combining the language model training directly into Stage 2 without need of prior stage, could potentially simplify the process and make it more efficient. Additionally, there is ambiguity regarding how much weight to assign to the captioning loss to achieve the better performance. While prioritizing the captioning loss improved performance, setting the weight more than enough caused a significant drop in MQ-Former's effectiveness. 
% Further exploration of MV-CLAM is needed to discover diverse ways of integrating 2D and 3D query tokens. 
% Additionally, future studies could investigate the performance of the model when combined with language models pretrained on other scientific texts, which may provide insights into broader applications of the model in molecular and scientific domains.
% ICLR requires electronic submissions, processed by
% \url{https://openreview.net/}. See ICLR's website for more instructions.

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully.

% \subsection{Style}

% Papers to be submitted to ICLR 2025 must be prepared according to the
% instructions presented here.

% %% Please note that we have introduced automatic line number generation
% %% into the style file for \LaTeXe. This is to help reviewers
% %% refer to specific lines of the paper when they make their comments. Please do
% %% NOT refer to these line numbers in your paper as they will be removed from the
% %% style file for the final version of accepted papers.

% Authors are required to use the ICLR \LaTeX{} style files obtainable at the
% ICLR website. Please make sure you use the current files and
% not previous versions. Tweaking the style files may be grounds for rejection.

% \subsection{Retrieval of style files}

% The style files for ICLR and other conference information are available online at:
% \begin{center}
%    \url{http://www.iclr.cc/}
% \end{center}
% The file \verb+iclr2025_conference.pdf+ contains these
% instructions and illustrates the
% various formatting requirements your ICLR paper must satisfy.
% Submissions must be made using \LaTeX{} and the style files
% \verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
% \verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
% have to do is replace the author, title, abstract, and text of the paper with
% your own.

% The formatting instructions contained in these style files are summarized in
% sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.

% Paper title is 17~point, in small caps and left-aligned.
% All pages should start at 1~inch (6~picas) from the top of the page.

% Authors' names are
% set in boldface, and each name is placed above its corresponding
% address. The lead author's name is to be listed first, and
% the co-authors' names are set to follow. Authors sharing the
% same address can be on the same line.

% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations. 

% \section{Headings: first level}
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone, regardless of the formatter being used.

% \subsection{Citations within the text}

% Citations within the text should be based on the \texttt{natbib} package
% and include the authors' last names and year (with the ``et~al.'' construct
% for more than two authors). When the authors or the publication are
% included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
% in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
% should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
% towards AI~\citep{Bengio+chapter2007}.'').

% The corresponding references are to be listed in alphabetical order of
% authors, in the \textsc{References} section. As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.

% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.

% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.

% You may use color figures.
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{acl2025_conference}
% \bibliographystyle{acl_natbib}


\newpage
\appendix
\section{Appendix}
\subsection{Related Works}
% LLM
% MRL
\textbf{Molecule-Text Modeling.} Early approaches utilize 1D SMILES molecular sequences to treat molecules as text sequences by adapting Transformer models \citep{vaswani2017attention} designed for natural language processing~\citep{irwin2022chemformer, wang2019smiles}. 
KV-PLM~\citep{zeng2022deep} specifically employs a masked language modeling loss to pretrain on biomedical texts with 1D SMILES representation. 
MolT5~\citep{edwards2022translation} specializes T5 model~\citep{raffel2020exploring} and tokenizer for SMILES-to-text and text-to-SMILES translations. 
Further enhancements represent molecules as 2D graphs. 
In particular, MoMu~\citep{su2022molecular} and MoleculeSTM~\citep{liu2023multi} leverage cross-modal contrastive learning to align the molecule graph representation to text. 
Current approaches to use multi-view representations of molecules primarily rely on contrastive learning, as demonstrated in models like GIT-Mol~\citep{liu2024git} and MolLM~\citep{tang2024mollm}. 
Additionally, aided with the development of vision large language models (VLLMs), molecular large language models with multi-modal learning architectures have been developed. 
Simple projection layers were used in prior works, InstructMol~\citep{cao2023instructmol} and GraphGPT~\citep{tang2024graphgpt}, to project molecular graph representations to LLM's input text token space.
Recent works have been concentrated on utilizing Q-Former~\citep{li2023blip} suggested in vision domain to bridge the gap between molecule and text modality. 
MolCA~\citep{liu2023molca} and 3D-MoLM~\citep{li2024towards} aligns 2D graph and 3D conformer molecular representations to text in purpose to generate effective soft-prompts for large language models.
UniMoT~\citep{zhang2024unimot} employs a vector quantization-driven tokenizer with a Q-Former. 
Current methods for utilizing multi-view representations of molecules are limited to contrastive learning or usage of specialized tokenizers, failing to achieve simultaneous alignment across all views and text, thereby neglecting the core principle of cross-modal alignment.

\textbf{Molecular representation learning}. Recent research in representation learning for molecules has seen significant advancements, particularly in leveraging large-scale unlabeled molecular data. 
SMILES-BERT~\citep{wang2019smiles}, MolBERT~\citep{li2021mol} adapts the BERT architecture on SMILES string for molecular property prediction tasks. 
To better focus on structural information of molecules, various graph-based representation learning models were presented. 
MolCLR~\citep{wang2022molecular} specifically tailored contrastive learning for molecular graphs using data augmentation while MAT~\citep{maziarka2020molecule} reinterpreted the attention mechanism of transformers to consider distance and edges. 
More recent works concentrate on employing 3D geometry, mostly to exploit 3D spatial coordinates. GraphMVP~\citep{liu2021pre} proposed a contrastive learning framework that bridges 2D topological and 3D geometric views of molecules. 
GEM~\citep{fang2022geometry} incorporated 3D geometric information by using bond angles and lengths as additional edge attributes in molecular graphs. 
Uni-Mol is a SE(3)-transformer based model pretrained via 3D position recovery and masked atom prediction. 
Additionally, MolFormer~\citep{wu2023molformer} integrates SMILES, graph, and 3D conformer information in a unified transformer architecture for molecular property prediction. 
These recent advancements demonstrate a trend towards incorporating more diverse and rich molecular information to improve the quality and applicability of learned representations, validating the approach of our research.



% \subsection{Datasets of additional experiments}
\subsection{Datasets Statistics}

\textbf{PubChem}. 
We gathered 324k SMILES-text pairs from PubChem, generating 2D graphs and 3D conformations using existing methods~\citep{maziarka2020molecule,landrum2013rdkit}.
Molecules with valid structures were used, with 15k longer-text pairs for training, and shorter ones for pretraining.
\begin{table}[H]
    \centering
    \caption{PubChem324k dataset statistics}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccc}
    \hline
    \textbf{Subset}   & \textbf{\#Molecule-Text Pairs} & \textbf{\#Min Words} & \textbf{\#Avg Words} \\ \hline
    Pretrain          & 290,507                       & 1                    & 17.84               \\
    Train             & 11,753                        & 20                   & 57.24               \\
    Valid             & 977                           & 20                   & 58.31               \\
    Test              & 1,955                         & 20                   & 55.21               \\ \hline
    \end{tabular}
    }
    \label{tab:pubchem_stat}
\end{table}
% \vspace{-10pt}

For the molecule captioning task, we chose not to use ChEBI-20 dataset~\citep{degtyarenko2007chebi} due to two main considerations~\citep{li2024towards}.
First, ChEBI-20 is a curated subset of PubChem, which introduces potential issues of data redundancy and leakage given the overlap between the two datasets.
Second, ChEBI-20 replaces molecular names with generic terms like `the molecule', limiting the evaluation of the model's ability to associate structural features with accurate molecular names.
Therefore, we utilized the PubChem dataset, which retains molecular names and offers a broader variety of structures, ensuring a more comprehensive evaluation of our framework in molecule captioning task.


\subsection{Experimental Settings}

\textbf{Stage 1 Molecule-Text Retrieval Pretraining}. Stage 1 serves to effectively transform molecular representations into query tokens interpretable in textual space. 
Using the PubChem324k pretraining subset with shorter textual descriptions, that is less informative but easier to align, MQ-former is trained for 35 epochs. 
A total of 301,658 molecules generated valid 2D graphs and 3D conformers, and thereby was used for pretraining.
The goal of this stage was to optimize MQ-Former's universal query generation by multi-objective training (molecule-text contrasting, molecule-text contrasting, and molecule captioning). 
Pretraining was conducted for 35 epochs using 3 NVIDIA A6000 GPUs with a batch size of 99. 
Learnable query tokens of each view was set to 12 tokens and were randomly initialized. 
Both the Uni-Mol and MAT graph encoders were frozen throughout the pipeline to prevent the model from focusing too much on modifying the graph encoders, ensuring the training prioritized aligning representations with the textual space. 
To put emphasis on the decoding ability given the molecule tokens, we assigned a weight of 2 to the captioning loss. 
Maximum text length was configured to 256. 
We used an optimizer with a warmup step of 200 and a learning rate scheduler with a decay rate of 0.9. 
Gradient accumulation was set to 1 batch per step.

\textbf{Stage 1 Molecule-Text Retrieval Finetuning}. After 35 epochs of pretraining, we loaded the checkpoint and fine-tuned MQ-Former for an additional 10 epochs on PubChem's train, validation and test datasets, consisting of 12,000, 1,000, and 2,000 molecules respectively. Training is conducted using our modified multi-token contrastive loss. 
This serves to raise alignment capability given longer and more complex textual descriptions. 
The optimizer, learning rate scheduler, batch size and text length settings are identical to the previous phase.

\textbf{Stage 2 Molecule Captioning Pretraining}. Stage 2 serves to further refine the universal tokens in a manner suited to a specific language model, LLaMA2~\citep{touvron2023llama} available at \url{https://huggingface.co/baffo32/decapoda-research-llama-7B-hf}. Using the trained model checkpoint from Stage 1 training stage, we conducted 4 epochs of pretraining on the PubChem dataset.
% During the phase, we optimized two tasks: molecule-text contrasting and molecule-text matching for MQ-Former, while using LLaMA2 for the molecule captioning task.
The universal query generated by MQ-Former, along with the 1D SMILES string and an instruction prompt were given as input to the language model to generate textual descriptions for the molecules.

% For efficient fine-tuning of LLaMA2, we applied LoRA (Low-Rank Adaptaion)~\citep{hu2021lora} with the following configuration: $r$=$8$, $\alpha$=$32$, and a dropout rate of 0.1. 
To fine-tune LLaMA2 efficiently, we employed LoRA ~\citep{hu2021lora} with a configuration of $r$=$8$, $\alpha$=$32$, and a 0.1 dropout rate.
% These parameters were integrated into the [\(k_{proj}, v_{proj}, q_{proj}, o_{proj}, gate_{proj}, up_{proj}, down_{proj}\)] modules, resulting in an additional 19 million trainable parameters, which accounted for 0.29\% of the total parameter count in the LLaMA2-7B model.
These settings were applied to the [\(k_{proj}, v_{proj}, q_{proj}, o_{proj}, gate_{proj}, \\ up_{proj}, down_{proj}\)] modules, adding 19 million trainable parameters, which constituted 0.29\% of the total parameters in the LLaMA2-7B model.
% These settings were applied to the [\(k_{\mspace{-2mu}proj}, v_{\mspace{-2mu}proj}, q_{\mspace{-2mu}proj}, o_{\mspace{-2mu}proj}, gate_{\mspace{-2mu}proj}, up_{\mspace{-2mu}proj}, down_{\mspace{-2mu}proj}\)] modules, adding 19 million trainable parameters, which constituted 0.29\% of the total parameters in the LLaMA2-7B model.
% These settings were applied to the [\(k_{proj}, v_{proj}, q_{proj}, o_{proj}, gate_{proj}, up_{proj}, \\ down_{proj}\)] modules, adding 19 million trainable parameters, which constituted 0.29\% of the total parameters in the LLaMA2-7B model.
Unlike Stage 1, we used batch size of 30 with a maximum text length of 320 considering the prompt size. 
Token length for generation was set to range between 128 and 320. Gradient accumulation was set to 2. The training was carried out using 3 NVIDIA A6000 GPUs.

\textbf{Stage 2 Molecule Captioning Fine-tuning}. Stage 2 pre-training checkpoint was further fine-tuned on the train dataset for additional 10 epochs. Experimental settings are same as stage 2 pre-training phase, and validated using valid, test datasets.

\subsection{Effectiveness of MQ-Former}
In this section, we provide the detailed explanations and figures of Section \ref{sec: mqformer}. We illustrate the underlying mechanism for MQ-Former, which aligns two representations by providing (1) generated captions with ground truth, (2) caption comparison with Q-former based single-view alignment, and (3) attention map visualization.
% In this appendix section, we demonstrate the efficiency of our proposed MQ-Former, which aligns two modalities (2D and 3D molecular representations) simultaneously to text. We begin by comparing the captions generated by our model to ground truth PubChem label descriptions, showing that our model successfully outputs IUPAC names, generic names, and clinical usages that align with literature references. Next, we visualize the attention maps for our query tokens alongside their distances in latent embeddings, revealing that 2D queries attend to 2D-related textual descriptions and 3D queries to 3D-related ones, with our MQ-Former effectively capturing both modalities. We also compare the captions generated using 2D-only or 3D-only embeddings aligned by the former Q-Former module to highlight the limitations of single-modal approaches. Finally, we examine multi-view embeddings aligned to text by the former Q-Former, demonstrating that simply representing the molecule via multi-view embeddings and aligning them simultaneously does not fully extract diverse perspectives. This analysis emphasizes the need for the novel MQ-Former architecture, which efficiently aligns multi-modal molecular representations to text for richer and more accurate understanding.

\subsubsection{Comparison of MV-CLAM Captions with Ground Truth}
Appendix Table \ref{tab:comparison} provides caption examples within the test dataset as specified in Section \ref{sec:caption}. 
MV-CLAM not only correctly generates IUPAC and generic names but also additional information unavailable in ground truth labels. 

\begin{table*}[htbp]
\caption{Comparison of ground truth and MV-CLAM descriptions. Matching keywords are highlighted in bold, while additional details provided by MV-CLAM are marked in red.}
\label{tab:comparison}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|p{6.5cm}|p{6.5cm}}
    \hline
    \textbf{Molecule} & \textbf{Ground Truth} & \textbf{MV-CLAM} \\
    \hline
    \raisebox{-0.95\totalheight}{\includegraphics[width=4.5cm]{rifapentine_rdkit.png}} 
    & 
    \textcolor{blue}{\textbf{Rifapentine}} is a \textbf{rifamycin antibiotic} that is similar in structure and activity to rifampin and rifabutin and that is used in combination with other agents as \textbf{therapy of tuberculosis}, particularly in once or twice weekly regimens. Rifapentine is associated with transient and asymptomatic elevations in serum aminotransferase and is a likely cause of \textbf{clinically apparent acute liver injury}.
    & 
    \textcolor{blue}{\textbf{Rifapentine}} is a \textbf{rifamycin antibiotic} that is similar in structure and activity to rifampin and rifabutin and that is used in combination with other agents as \textbf{therapy of tuberculosis}, particularly in once or twice weekly regimens. Rifapentine is associated with transient and asymptomatic elevations in serum aminotransferase and is a likely cause of \textbf{clinically apparent acute liver injury}. Rifapentine is a \textcolor{red}{\textbf{long-acting, cyclopentyl-substituted derivative of rifamycin.}}\\
    \hline
    \raisebox{-1.0\totalheight}{\includegraphics[width=4.5cm]{case_2_rdkit.jpg}} 
    & 
    \textcolor{blue}{\textbf{N-(2-hydroxytricosanoyl)-15-methylhexadecasphing-4-enine-1-phosphocholine}} is an N-acyl-15-methylhexadecasphing-4-enine-1-phosphocholine in which the\textbf{ acyl group has 23 carbons and 0 double bonds and is 2-hydroxylated}. It is functionally related to a 15-methylhexadecasphing-4-enine.
    & 
    \textcolor{blue}{\textbf{N-(2-hydroxytricosanoyl)-15-methylhexadecasphing-4-enine-1-phosphocholine}} is an N-acyl-15-methylhexadecasphing-4-enine-1-phosphocholine in which the\textbf{ acyl group has 23 carbons and 0 double bonds and is 2-hydroxylated}. It is functionally related to a 15-methylhexadecasphing-4-enine.\\
    \hline
\end{tabular}
}
\end{table*}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figs/case_study_comparison.pdf}
    \vspace{0.5em}
    \begin{minipage}{0.8\linewidth}
        \centering
        \caption{Comparison of Uni-modal Q-Former Ablation and Ours}
        \label{fig:caption_ablation}
    \end{minipage}
\end{figure*}


% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.8\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/attnmap_newloss.pdf}
%         \caption{Attention map visualization with multi-token contrasting loss. Each query exhibit different attention weights across the textual descriptions. Additionally, 2D query tokens focus on chemical and material properties (e.g., boiling point, toxic, eye contact), while 3D query tokens capture structural information (e.g., bis(2-(dimethylamino)ethyl)).}
%         \label{fig:attnmap_newloss}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.8\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/attnmap_orgloss.pdf}
%         \caption{Attention map visualization with original contrasting loss. Comparative to our modified loss setting, all query tokens have consistent attention distributions for all text tokens. The tokens lack word specificity for each dimension.}
%         \label{fig:attnmap_orgloss}
%     \end{subfigure}
%     \caption{Comparison of attention map visualizations using different contrasting losses. Sentence: \textit{[DEC] bis ( 2 - ( dimethylamino ) ethyl ) ether appears as a clear or yellow liquid . bp : 188 °c . toxic by inhalation , by skin absorption , ingestion , and eye contact . [SEP]}}.
%     \label{fig:attnmap_comparison}
% \end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figs/attnmap_fig_total_cropped.pdf}
\caption{Comparison of attention map visualizations using different contrasting losses. The x-axis represents the word tokens in the sentence: \textit{[DEC] bis ( 2 - ( dimethylamino ) ethyl ) ether appears as a clear or yellow liquid . bp : 188 °c . toxic by inhalation , by skin absorption , ingestion , and eye contact . [SEP].}, while the y-axis corresponds to the query tokens representing the molecule. (A) shows each query exhibiting different attention weights across the textual descriptions. Additionally, 2D query tokens focus on chemical and material properties (e.g., boiling point, toxic, eye contact), while 3D query tokens capture structural information (e.g., bis(2-(dimethylamino)ethyl)). Comparatively in (B), all query tokens have consistent attention distributions for all text tokens and lack word specificity for each dimension.}
    \label{fig:attn_map_total}
\end{figure*}

\subsubsection{Single-View Alignment Captions}
Appendix Figure~\ref{fig:caption_ablation} highlights the differences in captioning results between the uni-modal Q-Former ablation models and ours. This demonstrates that the multi-view approach generates richer and more precise molecular descriptions.



\subsubsection{Attention Map Visualization}
We provide images of the attention maps explained in Section \ref{sec: mqformer} (Appendix Figure~\ref{fig:attn_map_total}).
The attention maps of the shared self-attention layers are visualized to compare the processing of 2D and 3D query tokens with and without the multi-token contrasting loss.
With the proposed loss, query tokens exhibit diverse attention scores for each word in the captioning sentences while effectively distinguishing 2D- and 3D-related terms.
Specifically, 2D query tokens focus on chemical and material properties (e.g., \textit{boiling point, toxic, eye contact}), while 3D query tokens capture structural information (e.g., \textit{bis(2-(dimethylamino)ethyl)}).
In contrast, the original contrastive loss reduces query token diversity and weakens MQ-Former's ability to align with 2D- and 3D-specific terms.
This demonstrates that MQ-Former with the revised contrastive loss not only effectively preserves modality-specific information from 2D and 3D while aligning seamlessly with textual semantics but also guarantees query token diversity.



% Attention map of the shared self-attention layers is visualized to compare the processing of 2D and 3D query tokens. 
% \textcolor{blue}{As shown in the figures, the query tokens for each dimension exhibit distinct attention patterns across the sentence. 
% To further analyze the embeddings of 2D, 3D queries, and our universal query tokens, we visualized them in the latent space alongside the word embeddings of "water" - a chemical property with high attention to 2D - and "3"-a positional coordinate with high attention to 3D (Appendix Figure~\ref{fig:pca_result}). 
% The results reveal that the universal query token maintains moderate distances to both word embeddings, reflecting the interplay between 2D and 3D molecular views. 
% This demonstrates that MQ-Former effectively preserves modality-specific information from 2D and 3D while aligning seamlessly with textual semantics.}


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/attentionmap_2d.pdf}
%     \caption{Attention map highlighting 2D query token focusing on chemical properties in the text.}
%     \label{fig:attnmap_2d}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/attentionmap_3d.pdf}
%     \caption{Attention map highlighting 3D query token focusing on structural information in the text.}
%     \label{fig:attnmap_3d}
% \end{figure*}


% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figs/attnmap_2d_ex1.pdf}
%     \vspace{0.5em}
%     \begin{minipage}{0.8\linewidth}
%         \centering
%         \caption{Attention map visualization. 2D query tokens focus on chemical properties like water solubility present in text descriptions.}
%         \label{fig:attnmap_2d}
%     \end{minipage}
% \end{figure*}



% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figs/attnmap_newloss.pdf}
%     \vspace{0.5em}
%     \begin{minipage}{0.8\linewidth}
%         \centering
%         \caption{Attention map visualization with multi-token contrasting loss.}
%         \label{fig:attnmap_newloss}
%     \end{minipage}
% \end{figure*}



% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figs/attnmap_orgloss.pdf}
%     \vspace{0.5em}
%     \begin{minipage}{0.8\linewidth}
%         \centering
%         \caption{Attention map visualization with original contrasting loss.}
%         \label{fig:attnmap_orgloss}
%     \end{minipage}
% \end{figure*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.45\linewidth]{figs/pca_result.png}
%     \vspace{0.5em}
%     \begin{minipage}{0.8\linewidth}
%         \centering
%         \caption{Latent space representation of query tokens and word embeddings, illustrating the alignment of 2D, 3D, and universal queries with textual semantics.}
%         \label{fig:pca_result}
%     \end{minipage}
% \end{figure*}

% \subsection{Molecular Question-Answering}

% For the molecular question-answering task, we utilized the 3D-MolT~\citep{li2024towards} dataset, which includes question-prompt and text-answer pairs derived from the same PubChem data we used in prior. 
% Dataset statistics are in Appendix Table~\ref{tab:appendix_3dmolt}
% % We investigate to see whether utilizing both 3D and 2D molecular representations could enhance performance in a Q\&A setting, comparing it against models using either 2D or 3D queries alone. 
% The dataset consists of three distinct subsets: 1) Question-answering about non-3D properties, 2) Question-answering about 3D properties, and 3) Descriptive molecular properties. 
% For robust guidance into instruction tuning, the three datasets were used in combination for training a single epoch. 
% Checkpoints were retrieved from Stage 2 (molecule captioning). 
% Given the dataset size, the model was further fine-tuned for 5 epochs on non-3D, descriptive property tasks and 1 epoch on 3D property tasks.

% Computed property prediction is evaluated with mean absolute error and the metrics for descriptive property prediction are BLEU, ROUGE, METEOR. 
% As a method to validate our approach of exploiting multiple views that can specialize and capture different aspects of the molecule, we compare the results between single modal alignment with Q-Former under the same experiment setting. 
% Table~\ref{tab:descriptive_QA} and Table~\ref{tab:property_3d2d} indicate MV-CLAM consistently outperform the single-modal versions. 

% \begin{table}[h]
% \centering
% \caption{Comparison of Descriptive Property Generation Performance: We evaluate the performance of single-view approaches using MAT or Uni-Mol embeddings exclusively, which are projected through Q-Former, in contrast to the multi-view alignment provided by MV-CLAM.}
% \resizebox{0.8\textwidth}{!}{
% \begin{tabular}{ccccccc}
% \hline
% Model      & BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR \\ \hline
% 2D-Qformer & 31.24  & 25.13  & 39.30   & 25.16   & 34.11   & 49.88  \\
% 3D-Qformer & 29.22  & 22.82  & 37.38   & 22.54   & 31.47   & 27.29  \\
% Ours       & \textbf{31.70}  & \textbf{25.60}  & \textbf{39.61}   & \textbf{25.46}   & \textbf{34.51}   & \textbf{50.61}  \\ \hline
% \end{tabular}
% }
% \label{tab:descriptive_QA}
% \end{table}

% \begin{table}[h]
% \centering
% \caption{Comparison of Q\&A performance on 3D and non-3D properties: We reproduced and evaluated the performance of single-view approaches using MAT or Uni-Mol embeddings exclusively, which are projected through Q-Former, in contrast to the multi-view alignment provided by MV-CLAM.}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{ccccccccc}
% \hline
% Model & Molecular Weight & LogP           & Complexity    & \begin{tabular}[c]{@{}c@{}}Topological Polar\\ Surface Area\end{tabular} & HOMO        & LUMO        & HOMO-LUMO    & SCF Energy   \\ \hline
% 2D-Qformer     & {\ul 47.51 (0.98)}       & \textbf{0.8982 (0.96)} & {\ul 111.61 (0.96)}   & {\ul 16.68 (0.96)}                                                               & 0.78 (0.99)          & \textbf{0.47 (0.99)} & \textbf{0.41 (0.90)} & {\ul 1.33 (1.00)}    \\
% 3D-Qformer     & 49.48 (0.89)             & 1.4772 (0.88)          & 129.69 (0.75)         & 23.24 (0.87)                                                                     & {\ul 0.47 (0.87)}    & 0.44 (0.98)          & 1.28 (0.99)          & 1.42 (0.90)          \\
% Ours           & \textbf{37.15 (0.95)}    & {\ul 0.9675 (0.97)}    & \textbf{71.30 (0.92)} & \textbf{14.22 (0.94)}                                                            & \textbf{0.43 (0.93)} & {\ul 0.45 (0.90)}    & {\ul 0.45 (0.98)}    & \textbf{0.62 (0.97)} \\ \hline
% \end{tabular}
% }
% \label{tab:property_3d2d}
% \end{table}



% \subsubsection{Multi-View Representation Analysis} \label{sec:multi_view}
% \textcolor{blue}{To highlight the necessity of MQ-Former, we conducted an ablation study comparing our architecture with a variant that aligns multi-view molecular representations using a single Q-Former module. The multi-view molecular embedding was constructed by concatenating the 2D embeddings from MAT and the 3D embeddings from Uni-Mol, then projected to textual space using the Q-Former. Unlike the concatenation-based approach, MQ-Former preserves the rich, distinct representations of molecular views. This design facilitates more fine-grained alignment with text, maintaining diversified information, which results in higher-quality captions across all evaluated metrics (Table \ref{tab:caption_multiview}). Overall, MQ-Former enables the preservation of detailed and diverse molecular representations, facilitating precise alignment with textual descriptions and delivering superior performance in the captioning task.}
% Unlike the concatenation-based approach, MQ-Former preserves the rich, distinct representations of molecular views. This design facilitates more fine-grained alignment with text, maintaining diversified information, which results in higher-quality captions and improved performance across tasks, as shown in Appendix Table \ref{tab:retrieval_multiview} and \ref{tab:caption_multiview}. While the ablation model demonstrated higher accuracy on the full test set for Stage 1, its lower Recall@20 indicates difficulty in distinguishing between molecular structures at a finer granularity. This suggests that, although the ablation model effectively captures overall summaries, its capacity to differentiate specific molecular details is limited. Consequently, this shortcoming negatively impacts captioning performance, yielding lower scores across all evaluated metrics. Overall, MQ-Former enables the preservation of detailed and diverse molecular representations, facilitating precise alignment with textual descriptions and delivering superior performance across tasks.}

% \begin{table*}[h]
% \centering
% \caption{Retrieval Performance Comparison: Multi-View Representation with Single Q-Former}
% \resizebox{\textwidth}{!}{
% \scriptsize
% \tiny
% \begin{tabular}{ccccccccc}
% \hline
% \multicolumn{1}{c}{}                        & \multicolumn{4}{c}{Retrieval in batch}                                                 & \multicolumn{4}{c}{Retrieval in test set}                         \\ \cline{2-9} 
% \multicolumn{1}{c}{}                        & \multicolumn{2}{c}{M2T}                   & \multicolumn{2}{c}{T2M}                   & \multicolumn{2}{c}{M2T}        & \multicolumn{2}{c}{T2M}         \\ \cline{2-9} 
% \multicolumn{1}{c}{\multirow{-3}{*}{Model}} & ACC            & R@20            & ACC            & R@20            & ACC            & R@20           & ACC            & R@20           \\ \hline
% Multi-view + Q-Former &  95.6 & 99.39 & 96.06 & 99.34 & 80.26 & 96.83 & 80.56 & 96.83
% \\ 
% MV-CLAM & 96.73 & 99.90 & 96.01 & 99.85 & 70.90 & 96.98 & 71.15 & 95.96
% \\ \hline
% \end{tabular}
% }
% \label{tab:retrieval_multiview}
% \end{table*}

% \begin{table}[h]
% \centering
% \caption{Captioning Performance Comparison: Multi-View Representation with Single Q-Former}
% \resizebox{0.9\textwidth}{!}{
% \begin{tabular}{ccccccc}
% \hline
% Model      & BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR \\ \hline
% 2D + Qformer             & 29.72         & 22.26         & 38.22         & 23.45         & 31.61         & 34.22         \\
% 3D + Qformer              & 29.45         & 22.03         & 37.86         & 23.11         & 31.83         & 33.79         \\

% Multi-view + Q-Former             & {\ul 29.80}         & {\ul 22.70}     & {\ul 39.07} & {\ul 24.92} & {\ul 33.09} & {\ul 35.49} \\        
% MV-CLAM             & \textbf{31.75}& \textbf{24.48}& \textbf{40.43}& \textbf{25.72}& \textbf{33.79}& \textbf{36.54}        \\ \hline
% \end{tabular}
% }
% \label{tab:caption_multiview}
% \end{table}

\subsection{Downstream Task 1. Question Answering}
\subsubsection{Dataset: 3D-MolT}
A total of 18439K molecule-instruction text pairs are employed using the dataset split as given in the original paper~\citep{li2024towards}. 
The dataset consists of two types of molecular property prediction tasks: (1) Computed property prediction including 3D-dependent properties (e.g. HOMO) and (2) descriptive property prediction.

\begin{table}[H]
\centering
\caption{Statistics of the PubChemQC and PubChem datasets across different subsets.}
\resizebox{\columnwidth}{!}{ 
% \scriptsize
\begin{tabular}{lcccccc}
\hline
\multirow{2}{*}{\textbf{Subset}} & \multicolumn{2}{c}{\textbf{PubChemQC}}      & \multicolumn{3}{c}{\textbf{PubChem}}        \\ \cline{2-6} 
                                 & \#Mol       & \#Comp. QA    & \#Mol       & \#Comp. QA   & \#Desc. QA   \\ \hline
Pretrain                         & 3,119,717   & 12,478,868    & 301,658     & 1,199,066    & 1,508,290    \\
Train                            & 623,944     & 2,495,776     & 12,000      & 46,680       & 60,000       \\
Valid                            & 77,993      & 311,972       & 1,000       & 3,898        & 5,000        \\
Test                             & 77,993      & 311,972       & 2,000       & 7,785        & 10,000       \\ \hline
\end{tabular}
\scriptsize
\tiny
\label{tab:appendix_3dmolt}
}
\end{table}

\subsubsection{Experimental Settings}
For the molecular question-answering task, we utilized the 3D-MolT~\citep{li2024towards} dataset, which includes question-prompt and text-answer pairs derived from the same PubChem data we used in prior. 
Dataset statistics are in Appendix Table~\ref{tab:appendix_3dmolt}
The dataset consists of three distinct subsets: (1) Question-answering about non-3D properties, (2) Question-answering about 3D properties, and (3) Descriptive molecular properties.

For robust guidance into instruction tuning, the three sub-datasets of 3D-MolT \cite{li2024towards} were used in combination for training a single epoch. 
% The pretrained MV-CLAM checkpoints from the molecule captioning stage were used for initialization to the instruction-tuning process.
To ensure a fair comparison with single-view methods, we initialized the instruction-tuning process using the pretrained MV-CLAM checkpoints from the molecule captioning stage, employing the original loss function rather than the multi-token contrasting loss.
Given the dataset size, the model was further fine-tuned for 5 epochs on non-3D, descriptive property tasks and 1 epoch on 3D property tasks.
For computed property prediction, we evaluated performance using mean absolute error (MAE). 
For descriptive property prediction, we measured BLEU, ROUGE, and METEOR scores.


\subsubsection{Results}
For baselines, we reproduced results for 3D-MoLM and 2D-MoLM (with MAT~\citep{maziarka2020molecule} graph encoder). These baselines represent single-modal alignment using Q-Former, and provides a fair point of comparison to demonstrate the efficacy of our multi-view cross-modal alignment. Appendix Tables~\ref{tab:descriptive_QA}, \ref{tab:property_3d} and \ref{tab:property_2d} show that MV-CLAM consistently outperformed the single-modal models.

\begin{table}[H]
\centering
\caption{Comparison of Descriptive Property Generation Performance}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\hline
Model      & B-2 & B-4 & R-1 & R-2 & R-L & M \\ \hline
2D-MoLM & 31.24  & 25.13  & 39.30   & 25.16   & 34.11   & 49.88  \\
3D-MoLM & 29.22  & 22.82  & 37.38   & 22.54   & 31.47   & 27.29  \\
MV-CLAM$^{\ddagger}$       & \textbf{31.70}  & \textbf{25.60}  & \textbf{39.61}   & \textbf{25.46}   & \textbf{34.51}   & \textbf{50.61}  \\ \hline
\end{tabular}
}
\label{tab:descriptive_QA}
% \vspace{-15pt}
\end{table}


\begin{table}[H]
\centering
\caption{Q\&A performance on 3D properties}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\hline
Model & HOMO        & LUMO        & HOMO-LUMO    & SCF Energy   \\ \hline
2D-MoLM     & 0.78 (0.99)          & 0.47 (0.99) & {\ul{0.39 (0.90)}} & {\ul 0.98 (1.00)}    \\
3D-MoLM     & {\ul 0.42 (0.99)}    & {\ul{0.44 (0.98)}}          & 1.26 (0.99)          & 1.22 (0.98)          \\
MV-CLAM$^{\ddagger}$        & \textbf{0.35 (0.98)} & \textbf{0.42 (0.93)}    & \textbf{0.35 (0.99)}    & \textbf{0.32 (0.99)} \\ \hline
\end{tabular}
}
\label{tab:property_3d}
\vspace{-15pt}
\end{table}

\begin{table}[H]
\centering
\caption{Q\&A performance on non-3D properties. MW, TPSA denotes molecular weight and topological surface area.}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\hline
Model & MW & LogP           & Complexity    & TPSA\\ \hline
2D-MoLM     & 47.51 (0.98)       & {\ul{0.89 (0.99)}} & 110.78 (0.99)   & {\ul 16.65 (0.99)}    \\
3D-MoLM     & {\ul{42.76 (0.96)}}             & 1.25 (0.96)          & {\ul{105.03 (0.96)}}         & 20.97 (0.92)   \\
MV-CLAM$^{\ddagger}$           & \textbf{21.35 (0.92)}    & \textbf{0.69 (0.94)}    & \textbf{55.14 (0.91)} & \textbf{9.65 (0.91)} \\ \hline
\end{tabular}
}
\label{tab:property_2d}
\vspace{-15pt}
\end{table}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/editing_case_.pdf}
    \caption{Zero-shot editing with chemical properties}
    \label{fig:edit}
    \vspace{-10pt}
\end{figure*}
\subsection{Downstream Task 2: Zero-shot Molecule Editing}
Unlike conventional natural languages, SMILES encode molecular topology and properties demanding a specialized understanding of its notation system. Thereby, previous efforts in text-based de-novo molecule generation with large language models typically involves training or developing tokenizers that account for the unique grammar of SMILES~\citep{edwards2022translation}.
By fine-tuning MV-CLAM, we enabled the model to output SMILES strings without additional tokenizer training. 

\subsubsection{Dataset: ZINC20}
Following the experiment settings of~\cite{liu2023multi}, 200 molecules randomly selected from the ZINC20 dataset are given 6 single-objective molecule editing instructions. 
The 200 molecules follow the property distribution of the entire dataset, and do not overlap with the PubChem324k training dataset in previous stages. 
The six instructions are the following. (1) The molecule is soluble in water. (2) The molecule is insoluble in water. (3) The molecule has high permeability. (4) The molecule has low permeability. (5) The molecule is like a drug. (6) The molecule is not like a drug. (7) The molecule has more hydrogen bond donors. (8) The molecule has more hydrogen bond acceptors.

\subsubsection{Experimental Settings}
Zero-shot molecule editing was conducted on the curated dataset presented in \cite{liu2023multi} which consists of 200 randomly sampled molecules from the ZINC dataset. Each molecule was paired with molecule editing prompts (chemical instructions such as "\textit{The molecule is more soluble in water"}) and their corresponding SMILES. The dataset included molecular structures that were unseen during training. Starting with the original SMILES, the universal molecular token generated by the trained MQ-Former, and the editing prompt, we generated SMILES of the edited molecule. Using the pretrained MV-CLAM checkpoints from the molecule captioning stage, we conducted zero-shot molecule editing, utilizing the model's pre-existing multi-view molecular understanding from prior stages. The model was further fine-tuned for 4 epochs on the PubChem 324k pretraining and training datasets. This fine-tuning enabled MV-CLAM to directly generate SMILES from molecular universal tokens and was crucial to produce valid SMILES, considering the nature of LLaMA's general-purpose tokenizer which was not explicitly trained for SMILES generation.
We evaluate the edited results by computing desired chemical properties using RDKit \citep{landrum2013rdkit}, and classify whether the modification was valid shot.
% In our approach, we attempt to take a more streamlined approach by fine-tuning MV-CLAM to directly output SMILES strings. After training the model initialized by Stage 2 checkpoint to output the target SMILES sequence given the universal molecular query generated by MQ-Former, we perform zero-shot molecule editing 
% This takes advantage of model's pre-existing multi-view molecular understanding from prior training and efficiently grasp the intricacies of SMILES notation to generate edited molecules with instruction prompts.

\subsubsection{Results}
In this section we show successful case studies of the language model generating valid SMILES strings with adequate property modifications. Compared to previous works which mostly generate mere modifications of a single functional group, MV-CLAM generates diversified chemical structure modifications that may not be immediately obvious. This ability to generate more complex modifications is particularly advantageous for domain experts, as simple functional group changes are typically easy to perform manually. We attribute this diversity to the model's robust understanding of molecules within the textual space. The alignment between molecules and text is achieved by focusing on distinct substructures and molecular properties through the multi-view approach. 

(Appendix Figure~\ref{fig:edit}, \ref{fig:editing_sol},\ref{fig:editing_per},\ref{fig:editing_drug},\ref{fig:editing_hyd}). The values presented indicate the predicted LogP (octanol-water partition coefficient), topological surface area (TPSA), quantitative estimate of drug-likeness (QED) and number of hydrogen bond and acceptors. Each figure showcases original molecules alongside their modified counterparts with numerical indicators representing the chemical properties before and after the zero-shot editing. LogP values reflect solubility in water, while topological surface area relates to molecular permeability. QED reflects drug likeliness. The modifications are aligned with targeted property-based editing prompt, demonstrating the flexibility and chemical expertise of MV-CLAM.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/editing_solubility.pdf}
    \caption{Editing Solubility (LogP Adjustments): Smaller LogP indicates higher solubility in water. Molecules were successfully modified given the prompt \textit{"The molecule is soluble/insoluble in water"}.}
    \label{fig:editing_sol}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.90\linewidth]{figs/editing_permeability.pdf}
    \caption{Editing Permeability (Topological Surface Area, TPSA Adjustments): A higher TPSA implies lower permeability, while a lower TPSA suggests higher permeability. Molecules were successfully modified given the prompt \textit{"The molecule has high/low permeability"}.}
    \label{fig:editing_per}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/editing_drug.pdf}
    \caption{Editing Drug Likeliness (Quantitative Estimate of Drug-likeness, QED): A higher QED suggests a compound is more likely to possess favorable pharmacokinetic and ADMET (absorption, distribution, metabolism, excretion, and toxicity) properties, being more drug-likely. Molecules were successfully modified given the prompt \textit{"The molecule is/is not like a drug"}.}
    \label{fig:editing_drug}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/editing_hydrogen.pdf}
    \caption{Editing Hydrogen Bond Acceptor/Donors: The number of hydrogen bond acceptors and donors in the molecule were given for evaluation. Molecules were successfully modified given the prompt \textit{"The molecule has more hydrogen bond donors/acceptors"}.}
    \label{fig:editing_hyd}
\end{figure*}


% \begin{figure*}[]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figs/editing_case1.jpeg}
%     \caption{Molecule Editing Case Study}
%     \label{fig:editing1}
% \end{figure*}

% \begin{figure*}[]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figs/editing_case2.jpeg}
%     \caption{Molecule Editing Case Study}
%     \label{fig:editing2}
% \end{figure*}

% \subsection{Ablation Studies for Stage 1. Training MQ-Former}
% To better understand the contributions of individual components in our model, we conducted a series of ablation studies focusing on three factors: the graph encoder architecture, the training loss design, number of query tokens used in the model. We report the preliminary results retrieval metrics for the first stage of pretraining MQ-Former. Although early molecule-text retrieval results do not directly translate to improved molecule captioning outcomes, they have a tendency to exhibit positive correlation in previous studies.

% \paragraph{Graph Encoder Ablation}

% We examine three variations of 2D graph encoders, all of which remain frozen during MQ-Former training (Appendix Table~\ref{tab:retrieval_results_encoder_ablation}).  Under a consistent 3D encoder configuration, we report retrieval metrics for GIN initialized randomly, MAT embeddings adjusted via an additional linear layer for size reduction, and preserved MAT embeddings. 
% The results illustrate that the quality of graph encoders significantly influenced the initial performance during the first stage of pretraining MQ-Former. 
% This observation was a key motivation behind MQ-Former; maintaining high-quality embeddings from pretrained graph encoders appears to be effective for textual alignment.

% \begin{table*}[h]
% \centering
% \caption{Retrieval performance comparison in batch and test set for different 2D graph encoders.}
% \resizebox{\linewidth}{!}{
% \scriptsize
% \tiny
% \begin{tabular}{ccccccccc}
% \hline
% \multicolumn{1}{c}{}                        & \multicolumn{4}{c}{Retrieval in batch}                                                 & \multicolumn{4}{c}{Retrieval in test set}                         \\ \cline{2-9} 
% \multicolumn{1}{c}{}                        & \multicolumn{2}{c}{M2T}                   & \multicolumn{2}{c}{T2M}                   & \multicolumn{2}{c}{M2T}        & \multicolumn{2}{c}{T2M}         \\ \cline{2-9} 
% \multicolumn{1}{c}{\multirow{-3}{*}{Model}} & ACC            & R@20            & ACC            & R@20            & ACC            & R@20           & ACC            & R@20           \\ \hline
% Random                                   & 87.42          & 99.54           & 87.31          & 99.54           & 38.87          & 88.59          & 37.54          & 88.03          \\ 
% MAT\_linear                                     & 90.38          & 99.64           & 89.26          & 99.64           & 55.96          & 90.84          & 54.37          & 90.69          \\ 
% Ours                                    & \textbf{96.16} & \textbf{99.85}  & \textbf{96.06} & \textbf{99.85}   & \textbf{67.72} & \textbf{96.62} & \textbf{68.69} & \textbf{95.86} \\ \hline
% \end{tabular}
% }
% \label{tab:retrieval_results_encoder_ablation}
% \end{table*}

% \paragraph{\textcolor{blue}{Number of Query Tokens}}
% \textcolor{blue}{We conducted a preliminary ablation study comparing the use of a single query token versus multiple query tokens (Appendix Table~\ref{tab:retrieval_results_numquery_ablation}). We also showcase an attention map (Appendix Figure \ref{fig:query_att}) to show multiple query tokens allow the model to capture distinct attention patterns in textual descriptions. This decision aligns with the design philosophy of BLIP-2 \citep{li2023blip} and ensures that MQ-Former is capable of leveraging the unique information provided by each modality for more comprehensive molecule captioning.
% }

% \begin{table*}[h]
% \centering
% \caption{Retrieval performance comparison in batch and test set for different number of query tokens.}
% \resizebox{0.9\textwidth}{!}{
% \scriptsize
% \tiny
% \begin{tabular}{ccccccccc}
% \hline
% \multicolumn{1}{c}{}                        & \multicolumn{4}{c}{Retrieval in batch}                                                 & \multicolumn{4}{c}{Retrieval in test set}                         \\ \cline{2-9} 
% \multicolumn{1}{c}{}                        & \multicolumn{2}{c}{M2T}                   & \multicolumn{2}{c}{T2M}                   & \multicolumn{2}{c}{M2T}        & \multicolumn{2}{c}{T2M}         \\ \cline{2-9} 
% \multicolumn{1}{c}{\multirow{-3}{*}{Model}} & ACC            & R@20            & ACC            & R@20            & ACC            & R@20           & ACC            & R@20           \\ \hline
% 1 Query Token &  96.16 & 99.85 & 95.40& 99.85 & 70.08 & 96.42 & 70.97 & 95.5
% \\ 
% 12 Query Tokens & 96.73 & 99.90 & 96.01 & 99.85 & 70.90 & 96.98 & 71.15 & 95.96
% \\ \hline
% \end{tabular}
% }
% \label{tab:retrieval_results_numquery_ablation}
% \end{table*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figs/attention_multiple_query.pdf}
%     \caption{Attention map of length 12 molecular query token. Different queries attend to different words within the textual descriptions, allowing comprehensive alignment between molecules and text.}
%     \label{fig:query_att}
% \end{figure*}


% \paragraph{\textcolor{blue}{Training Loss Ablation}}
% \textcolor{blue}{We also evaluated the effect of loss weighting in the multi-objective training framework, along with the evaluation of symmetric components in molecule-text contrasting loss (Appendix Table~\ref{tab:retrieval_loss}). 
% % These findings demonstrate that each components sufficiently contribute to training MQ-Former, and 
% These findings demonstrate that amplifying the LM loss weight better aligns molecular and textual representations, justifying its use in subsequent training stages. Due to different batches within experiments, we only report the metrics for the entire test set.}




% \begin{table*}[h]
% \centering
% \caption{Retrieval performance comparison in test set for training loss weight and components.}
% \resizebox{0.5\textwidth}{!}{
% \scriptsize
% \tiny
% \begin{tabular}{ccccc}
% \hline
% \multicolumn{1}{c}{}                        & \multicolumn{2}{c}{M2T}                   & \multicolumn{2}{c}{T2M}           \\ \cline{2-5} 
% \multicolumn{1}{c}{\multirow{-3}{*}{Model}} & ACC            & R@20            & ACC            & R@20           \\ \hline
% lm loss * 1         & 69.87 & 97.75 & 69.26 & 95.55 \\ 
% % -g2t loss                & 90.38          & 99.64           & 89.26          & 99.64         \\ 
% % -t2g loss       & 96.16 &   & &   \\ 
% Ours &70.90 & 96.98 & 71.15 & 95.96 \\
% \hline
% \end{tabular}
% }
% \label{tab:retrieval_loss}
% \end{table*}

\subsection{Ablation Studies for Stage 2. Specializing LLaMA2 for Molecule Captioning}
\paragraph{1D Molecular Representations}
We conducted an ablation study to compare the use of SELFIES \citep{krenn2020self} with SMILES as input representations (Appendix Table~\ref{tab:ablation_selfies}). Using the pretrained Stage 2 checkpoint, the model was further trained for captioning under identical settings. After 10 stages of training with SELFIES, SMILES consistently demonstrated superior performance across metrics such as BLEU, METEOR, and ROUGE, validating the effectiveness of our selection.

\begin{table}[ht]
\centering
\caption{Captioning performance comparison for 1D molecular representations}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccc}
\hline
Model      & B-2 & B-4 & R-1 & R-2 & R-L & M \\ \hline
SELFIES & 28.39 & 20.89 & 33.25 & 37.58 & 22.49 & 31.37 \\        
SMILES             & 31.75& 24.48& 40.43& 25.72& 33.79& 36.54        \\ \hline
\end{tabular}
}
\label{tab:ablation_selfies}
\end{table}

\subsection{Failure Case Study}

\begin{table*}[b!]
    \caption{Failure Case Study: Comparison of Ground Truth and MV-CLAM Descriptions}
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c|p{6.5cm}|p{6.5cm}}
        \hline
        \textbf{Molecule} & \textbf{Ground Truth} & \textbf{MV-CLAM} \\
        \hline
        \raisebox{-.8\totalheight}{\includegraphics[width=5cm]{figs/oleoyl-coa.png}} 
        &
        \textbf{Lactoyl-CoA(4-)} is an acyl-CoA(4-) that is the tetraanion of lactoyl-CoA arising from deprotonation of phosphate and diphosphate functions. It is a conjugate base of a lactoyl-CoA.
        
        & 
        \textcolor{red}{\textbf{oleoyl-CoA(4-)}} is an acyl-CoA(4-) that is the tetraanion of oleoyl-CoA, arising from deprotonation of phosphate and diphosphate functions. It has a role as a human metabolite and a Saccharomyces cerevisiae metabolite. It is a saturated fatty acyl-CoA(4-) and a 3-substituted propionyl-CoA(4-). It is a conjugate base of an oleoyl-CoA\\
        \hline
        \raisebox{-1.0\totalheight}{\includegraphics[width=5.5cm]{figs/fail_case2.png}} 
        &
        \textbf{Ajugaciliatin B} is a diterpene lactone isolated from the whole plants of Ajuga ciliata and has been shown to exhibit neuroprotective activity. It has a role as a plant metabolite and a neuroprotective agent. It is a diterpene lactone, an acetate ester, a butenolide, a carbobicyclic compound, an organochlorine compound and a tertiary alcohol. Ajugaciliatin B is a natural product found in Ajuga ciliata with data available.

        &
        \textcolor{red}{\textbf{ajugaciliatin C}} is a natural product found in Ajuga ciliata with data available.
        \textcolor{red}{\textbf{Ajugaciliatin E}} is a diterpene lactone isolated from the whole plants of Ajuga ciliata. It has a role as a plant metabolite. It is a butenolide, an acetate ester, a diterpene lactone and a organochlorine compound. It is functionally related to a tiglic acid.
        \textcolor{red}{\textbf{Ajugaciliatin E}} is a natural product found in Ajuga ciliata\\
        \hline
    \end{tabular}%
    }
    \label{tab:comparison_failure}
\end{table*}


Appendix Table~\ref{tab:comparison_failure} showcases two instances where MV-CLAM fails to differentiate structurally similar molecules. 
First, the model misclassifies lactoyl-CoA as oleoyl-CoA despite the key difference being the length of the carbon chain. 
This indicates a limitation in the model’s capacity to capture subtle variations in carbon chain lengths. 
Second, the model misidentifies Ajugaciliatin B as subtypes E and C, demonstrating that while it successfully recognizes the molecule’s primary backbone, it struggles to distinguish the small functional groups that define each subtype. 
This suggests that the model is not sufficiently sensitive to minor structural modifications. 
Both errors appear to stem from the model’s difficulty in perceiving refine differences in chemical properties and spatial structure between the ground truth and its predictions. 
This underscores a broader challenge in molecular captioning: capturing subtle yet critical molecular features that may not greatly impact the primary structure but are crucial contributors for property.

To overcome these limitations, we propose several future studies. 
First, expanding our MQ-Former to align additional views or modalities, along with finer-grained molecular or related biological embeddings, could offer complementary insights to enhance the model’s ability to differentiate between similar molecules. 
This multi-view alignment could offer a more holistic understanding of the molecule’s structure and properties. 
In addition, curating larger molecule datasets would enhance the model's capacity to generalize, ensuring it has sufficient exposure to a wide range of molecular variations during training. 
These developments will address the current shortcomings and pave the way for more accurate molecular identification in future iterations of the model.

 


% \subsection{Molecule Captioning Loss}

\end{document}

