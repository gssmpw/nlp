[
  {
    "index": 0,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "irwin2022chemformer",
        "author": "Irwin, Ross and Dimitriadis, Spyridon and He, Jiazhen and Bjerrum, Esben Jannik",
        "title": "Chemformer: a pre-trained transformer for computational chemistry"
      },
      {
        "key": "wang2019smiles",
        "author": "Wang, Sheng and Guo, Yuzhi and Wang, Yuhong and Sun, Hongmao and Huang, Junzhou",
        "title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zeng2022deep",
        "author": "Zeng, Zheni and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong",
        "title": "A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "edwards2022translation",
        "author": "Edwards, Carl and Lai, Tuan and Ros, Kevin and Honke, Garrett and Cho, Kyunghyun and Ji, Heng",
        "title": "Translation between molecules and natural language"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "raffel2020exploring",
        "author": "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J",
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "su2022molecular",
        "author": "Su, Bing and Du, Dazhao and Yang, Zhao and Zhou, Yujie and Li, Jiangmeng and Rao, Anyi and Sun, Hao and Lu, Zhiwu and Wen, Ji-Rong",
        "title": "A molecular multimodal foundation model associating molecule graphs with natural language"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2023multi",
        "author": "Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Animashree",
        "title": "Multi-modal molecule structure--text model for text-based retrieval and editing"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024git",
        "author": "Liu, Pengfei and Ren, Yiming and Tao, Jun and Ren, Zhixiang",
        "title": "Git-mol: A multi-modal large language model for molecular science with graph, image, and text"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "tang2024mollm",
        "author": "Tang, Xiangru and Tran, Andrew and Tan, Jeffrey and Gerstein, Mark B",
        "title": "MolLM: a unified language model for integrating biomedical text with 2D and 3D molecular representations"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "cao2023instructmol",
        "author": "Cao, He and Liu, Zijing and Lu, Xingyu and Yao, Yuan and Li, Yu",
        "title": "Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tang2024graphgpt",
        "author": "Tang, Jiabin and Yang, Yuhao and Wei, Wei and Shi, Lei and Su, Lixin and Cheng, Suqi and Yin, Dawei and Huang, Chao",
        "title": "Graphgpt: Graph instruction tuning for large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2023molca",
        "author": "Liu, Zhiyuan and Li, Sihang and Luo, Yanchen and Fei, Hao and Cao, Yixin and Kawaguchi, Kenji and Wang, Xiang and Chua, Tat-Seng",
        "title": "MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "li2024towards",
        "author": "Li, Sihang and Liu, Zhiyuan and Luo, Yanchen and Wang, Xiang and He, Xiangnan and Kawaguchi, Kenji and Chua, Tat-Seng and Tian, Qi",
        "title": "Towards 3d molecule-text interpretation in language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhang2024unimot",
        "author": "Zhang, Juzheng and Bian, Yatao and Chen, Yongqiang and Yao, Quanming",
        "title": "UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "wang2019smiles",
        "author": "Wang, Sheng and Guo, Yuzhi and Wang, Yuhong and Sun, Hongmao and Huang, Junzhou",
        "title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2021mol",
        "author": "Li, Juncai and Jiang, Xiaofei",
        "title": "Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wang2022molecular",
        "author": "Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Barati Farimani, Amir",
        "title": "Molecular contrastive learning of representations via graph neural networks"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "maziarka2020molecule",
        "author": "Maziarka, {\\L}ukasz and Danel, Tomasz and Mucha, S{\\l}awomir and Rataj, Krzysztof and Tabor, Jacek and Jastrz{\\k{e}}bski, Stanis{\\l}aw",
        "title": "Molecule attention transformer"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liu2021pre",
        "author": "Liu, Shengchao and Wang, Hanchen and Liu, Weiyang and Lasenby, Joan and Guo, Hongyu and Tang, Jian",
        "title": "Pre-training molecular graph representation with 3d geometry"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "fang2022geometry",
        "author": "Fang, Xiaomin and Liu, Lihang and Lei, Jieqiong and He, Donglong and Zhang, Shanzhuo and Zhou, Jingbo and Wang, Fan and Wu, Hua and Wang, Haifeng",
        "title": "Geometry-enhanced molecular representation learning for property prediction"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "wu2023molformer",
        "author": "Wu, Fang and Radev, Dragomir and Li, Stan Z",
        "title": "Molformer: Motif-based transformer on 3d heterogeneous molecular graphs"
      }
    ]
  }
]