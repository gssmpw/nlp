@article{cao2023instructmol,
  title={Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery},
  author={Cao, He and Liu, Zijing and Lu, Xingyu and Yao, Yuan and Li, Yu},
  journal={arXiv preprint arXiv:2311.16208},
  year={2023}
}

@article{edwards2022translation,
  title={Translation between molecules and natural language},
  author={Edwards, Carl and Lai, Tuan and Ros, Kevin and Honke, Garrett and Cho, Kyunghyun and Ji, Heng},
  journal={arXiv preprint arXiv:2204.11817},
  year={2022}
}

@article{fang2022geometry,
  title={Geometry-enhanced molecular representation learning for property prediction},
  author={Fang, Xiaomin and Liu, Lihang and Lei, Jieqiong and He, Donglong and Zhang, Shanzhuo and Zhou, Jingbo and Wang, Fan and Wu, Hua and Wang, Haifeng},
  journal={Nature Machine Intelligence},
  volume={4},
  number={2},
  pages={127--134},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{irwin2022chemformer,
  title={Chemformer: a pre-trained transformer for computational chemistry},
  author={Irwin, Ross and Dimitriadis, Spyridon and He, Jiazhen and Bjerrum, Esben Jannik},
  journal={Machine Learning: Science and Technology},
  volume={3},
  number={1},
  pages={015022},
  year={2022},
  publisher={IOP Publishing}
}

@article{li2021mol,
  title={Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction},
  author={Li, Juncai and Jiang, Xiaofei},
  journal={Wireless Communications and Mobile Computing},
  volume={2021},
  number={1},
  pages={7181815},
  year={2021},
  publisher={Wiley Online Library}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{li2024towards,
  title={Towards 3d molecule-text interpretation in language models},
  author={Li, Sihang and Liu, Zhiyuan and Luo, Yanchen and Wang, Xiang and He, Xiangnan and Kawaguchi, Kenji and Chua, Tat-Seng and Tian, Qi},
  journal={arXiv preprint arXiv:2401.13923},
  year={2024}
}

@article{liu2021pre,
  title={Pre-training molecular graph representation with 3d geometry},
  author={Liu, Shengchao and Wang, Hanchen and Liu, Weiyang and Lasenby, Joan and Guo, Hongyu and Tang, Jian},
  journal={arXiv preprint arXiv:2110.07728},
  year={2021}
}

@article{liu2023molca,
  title={MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter},
  author={Liu, Zhiyuan and Li, Sihang and Luo, Yanchen and Fei, Hao and Cao, Yixin and Kawaguchi, Kenji and Wang, Xiang and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2310.12798},
  year={2023}
}

@article{liu2023multi,
  title={Multi-modal molecule structure--text model for text-based retrieval and editing},
  author={Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Animashree},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1447--1457},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{liu2024git,
  title={Git-mol: A multi-modal large language model for molecular science with graph, image, and text},
  author={Liu, Pengfei and Ren, Yiming and Tao, Jun and Ren, Zhixiang},
  journal={Computers in biology and medicine},
  volume={171},
  pages={108073},
  year={2024},
  publisher={Elsevier}
}

@article{maziarka2020molecule,
  title={Molecule attention transformer},
  author={Maziarka, {\L}ukasz and Danel, Tomasz and Mucha, S{\l}awomir and Rataj, Krzysztof and Tabor, Jacek and Jastrz{\k{e}}bski, Stanis{\l}aw},
  journal={arXiv preprint arXiv:2002.08264},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{su2022molecular,
  title={A molecular multimodal foundation model associating molecule graphs with natural language},
  author={Su, Bing and Du, Dazhao and Yang, Zhao and Zhou, Yujie and Li, Jiangmeng and Rao, Anyi and Sun, Hao and Lu, Zhiwu and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2209.05481},
  year={2022}
}

@inproceedings{tang2024graphgpt,
  title={Graphgpt: Graph instruction tuning for large language models},
  author={Tang, Jiabin and Yang, Yuhao and Wei, Wei and Shi, Lei and Su, Lixin and Cheng, Suqi and Yin, Dawei and Huang, Chao},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={491--500},
  year={2024}
}

@article{tang2024mollm,
  title={MolLM: a unified language model for integrating biomedical text with 2D and 3D molecular representations},
  author={Tang, Xiangru and Tran, Andrew and Tan, Jeffrey and Gerstein, Mark B},
  journal={Bioinformatics},
  volume={40},
  number={Supplement\_1},
  pages={i357--i368},
  year={2024},
  publisher={Oxford University Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{wang2019smiles,
  title={Smiles-bert: large scale unsupervised pre-training for molecular property prediction},
  author={Wang, Sheng and Guo, Yuzhi and Wang, Yuhong and Sun, Hongmao and Huang, Junzhou},
  booktitle={Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics},
  pages={429--436},
  year={2019}
}

@article{wang2022molecular,
  title={Molecular contrastive learning of representations via graph neural networks},
  author={Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Barati Farimani, Amir},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={279--287},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{wu2023molformer,
  title={Molformer: Motif-based transformer on 3d heterogeneous molecular graphs},
  author={Wu, Fang and Radev, Dragomir and Li, Stan Z},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={5312--5320},
  year={2023}
}

@article{zeng2022deep,
  title={A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals},
  author={Zeng, Zheni and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={862},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{zhang2024unimot,
  title={UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation},
  author={Zhang, Juzheng and Bian, Yatao and Chen, Yongqiang and Yao, Quanming},
  journal={arXiv preprint arXiv:2408.00863},
  year={2024}
}

