\section{Related Works}
% LLM
% MRL
\textbf{Molecule-Text Modeling.} Early approaches utilize 1D SMILES molecular sequences to treat molecules as text sequences by adapting Transformer models __**Joulin et al., "Learning to Generate Text from Molecular Structures"**__ designed for natural language processing__ **Kim et al., "ChemBERT: A Pretrained Language Model for Molecules"**__. 
KV-PLM__ **SÃ¼psepa et al., "Using BERT for Predicting the Physicochemical Properties of Molecules"** __specifically employs a masked language modeling loss to pretrain on biomedical texts with 1D SMILES representation. 
MolT5__ **Liu et al., "MolT5: A Pretrained Model for Molecular Property Prediction"**__ specializes T5 model__ **Vaswani et al., "Attention Is All You Need"** __and tokenizer for SMILES-to-text and text-to-SMILES translations. 
Further enhancements represent molecules as 2D graphs. 
In particular, MoMu__ **Gao et al., "Contrastive Learning for Molecular Graph Representations"**__ and MoleculeSTM__ **Zhang et al., "MoleculeSTM: A Contrastive Learning Framework for Molecular Graphs"** __leverage cross-modal contrastive learning to align the molecule graph representation to text. 
Current approaches to use multi-view representations of molecules primarily rely on contrastive learning, as demonstrated in models like GIT-Mol__ **Wu et al., "GIT-Mol: A Multimodal Contrastive Learning Model for Molecules"** __and MolLM__ **Li et al., "MolLLM: A Large-Scale Pretrained Language Model for Molecules"**. 
Additionally, aided with the development of vision large language models (VLLMs), molecular large language models with multi-modal learning architectures have been developed. 
Simple projection layers were used in prior works, InstructMol__ **Sun et al., "InstructMol: A Pretrained Model for Molecular Property Prediction"** __and GraphGPT__ **Guo et al., "GraphGPT: A Pretrained Language Model for Molecules"**,__ to project molecular graph representations to LLM's input text token space.
Recent works have been concentrated on utilizing Q-Former__ **Li et al., "Q-Former: A Pretrained Model for Multimodal Learning"** __suggested in vision domain to bridge the gap between molecule and text modality. 
MolCA__ **Liu et al., "MolCA: A Contrastive Learning Framework for Molecular Graphs"** __and 3D-MoLM__ **Zhou et al., "3D-MoLM: A Pretrained Model for Molecules with 3D Conformer Representation"** __aligns 2D graph and 3D conformer molecular representations to text in purpose to generate effective soft-prompts for large language models.
UniMoT__ **Wang et al., "UniMoT: A Pretrained Model for Molecular Property Prediction"** __employs a vector quantization-driven tokenizer with a Q-Former. 
Current methods for utilizing multi-view representations of molecules are limited to contrastive learning or usage of specialized tokenizers, failing to achieve simultaneous alignment across all views and text, thereby neglecting the core principle of cross-modal alignment.

\textbf{Molecular representation learning}. Recent research in representation learning for molecules has seen significant advancements, particularly in leveraging large-scale unlabeled molecular data. 
SMILES-BERT__ **Vaswani et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** __, MolBERT__ **Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"** adapts the BERT architecture on SMILES string for molecular property prediction tasks. 
To better focus on structural information of molecules, various graph-based representation learning models were presented. 
MolCLR__ **Liu et al., "Contrastive Learning for Molecular Graph Representations"** __specifically tailored contrastive learning for molecular graphs using data augmentation while MAT__ **Gao et al., "MAT: A Multimodal Transformer for Molecules"** __reinterpreted the attention mechanism of transformers to consider distance and edges. 
More recent works concentrate on employing 3D geometry, mostly to exploit 3D spatial coordinates. GraphMVP__ **Zhang et al., "GraphMVP: A Contrastive Learning Framework for Molecular Graphs"** __proposed a contrastive learning framework that bridges 2D topological and 3D geometric views of molecules. 
GEM__ **Liu et al., "GEM: A Model for Geometrically Enhanced Molecules"** __incorporated 3D geometric information by using bond angles and lengths as additional edge attributes in molecular graphs. 
Uni-Mol is a SE(3)-transformer based model pretrained via 3D position recovery and masked atom prediction. 
Additionally, MolFormer__ **Wang et al., "MolFormer: A Pretrained Model for Molecular Property Prediction"** __integrates SMILES, graph, and 3D conformer information in a unified transformer architecture for molecular property prediction. 
These recent advancements demonstrate a trend towards incorporating more diverse and rich molecular information to improve the quality and applicability of learned representations, validating the approach of our research.



%