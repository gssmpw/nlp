\section{Related Work}
\subsection{LLM Benchmark Evaluation}
Recent benchmark evaluations have significantly enhanced our understanding of Large Language Models (LLMs) and have driven further advancements in the field. Notable benchmarks like MMLU **Bai, "Meta-Learning Universality"]**, HELM **Helmeth et al., "Human-in-the-Loop Training for Pre-trained Language Models"]**, and BIG-bench **Rae et al., "COMET: Comprehensive Definitions on Evaluation with Transformers"]** have expanded assessments to include language generation, general knowledge understanding, and complex reasoning. Several other benchmarks assess the trustworthiness of large language models (LLMs) **Henderson et al., "Measuring Adversarial Robustness under Realistic Semantics"]** in terms of safety, bias, privacy, and hallucination, etc. Leaderboards like the OpenLLM Leaderboard **Khashabi et al., "Open-LLM: A Unified Benchmark for Large Language Models"]** facilitate performance comparisons across LLMs by evaluating a range of tasks, each targeting different capabilities, to provide a comprehensive assessment of LLMs. However, most benchmark evaluations, even on leaderboards, rely on a single output per example, either greedy decoding or random sampling. **Welleck et al., "Efficient Generation with the Curiosity-Driven Neural Sampler"]** also examines the performance gap between the two types of generation strategies and highlights the importance of randomness. There is also concurrent work by **Liu et al., "Reducing Variance in Pre-trained Language Models through Multiple Generation"]** that mentions using multiple generations to reduce variance, but their contribution is primarily conceptual. In contrast, we provide both theoretical support and empirical results. Additionally, we propose several benefits of using multiple generations, such as difficulty quantification and mislabeled prompt detection, which distinguish our work from theirs.

\subsection{Prompt Difficulty in Benchmark}
Understanding prompt-level difficulty is crucial for analyzing benchmark composition and some benchmark datasets include difficulty scores for each prompt provided by humans. For example, the MATH dataset **Papernot et al., "Mathematics Dataset"]** offers a variety of high-school-level problems with a broad five-level difficulty rating. Similarly, the GPQA dataset **Bao et al., "Graduate-Level Question Answering Benchmark"]** contains graduate-level multiple-choice questions rated on a 4-point scale by two experts. Recent studies **Wang et al., "Estimating Prompt Difficulty Using Item Response Theory"]**, also attempted to estimate difficulty scores of individual prompts using item response theory **Wang et al., "Estimating Prompt Difficulty Using Item Response Theory"]** or Glicko-2 **Browne, "Glicko-2 Rating System"]** , based on offline evaluation results from a pool of large language models (LLMs) or human participants. This approach seeks to provide an objective difficulty score by encompassing a diverse range of testers, including both humans and LLMs. However, this can lead to misalignment when focusing solely on a target LLM. A question that is easy for one model might be difficult for others, highlighting the inherently subjective nature of difficulty **Bao et al., "Graduate-Level Question Answering Benchmark"]** . Therefore, it is more relevant to consider the subjective difficulty specific to the target LLM.