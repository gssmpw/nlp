[
  {
    "index": 0,
    "papers": [
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring massive multitask language understanding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liang2022holistic",
        "author": "Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others",
        "title": "Holistic evaluation of language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "srivastava2022beyond",
        "author": "Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\\`a} and others",
        "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2023decodingtrust",
        "author": "Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others",
        "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models."
      },
      {
        "key": "huang2024trustllm",
        "author": "Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and others",
        "title": "Trustllm: Trustworthiness in large language models"
      },
      {
        "key": "zhang2024defining",
        "author": "Zhang, Wenbo and Xu, Zihang and Cai, Hengrui",
        "title": "Defining Boundaries: A Spectrum of Task Feasibility for Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "open-llm-leaderboard-v1",
        "author": "Edward Beeching and Cl\u00e9mentine Fourrier and Nathan Habib and Sheon Han and Nathan Lambert and Nazneen Rajani and Omar Sanseviero and Lewis Tunstall and Thomas Wolf",
        "title": "Open LLM Leaderboard (2023-2024)"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "song2024good",
        "author": "Song, Yifan and Wang, Guoyin and Li, Sujian and Lin, Bill Yuchen",
        "title": "The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "miller2024adding",
        "author": "Miller, Evan",
        "title": "Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hendrycks2measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "rein2023gpqa",
        "author": "Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R",
        "title": "Gpqa: A graduate-level google-proof q\\&a benchmark"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ding2024easy2hard",
        "author": "Ding, Mucong and Deng, Chenghao and Choo, Jocelyn and Wu, Zichu and Agrawal, Aakriti and Schwarzschild, Avi and Zhou, Tianyi and Goldstein, Tom and Langford, John and Anandkumar, Anima and others",
        "title": "Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization"
      },
      {
        "key": "polotinybenchmarks",
        "author": "Polo, Felipe Maia and Weber, Lucas and Choshen, Leshem and Sun, Yuekai and Xu, Gongjun and Yurochkin, Mikhail",
        "title": "tinyBenchmarks: evaluating LLMs with fewer examples"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "cai2016item",
        "author": "Cai, Li and Choi, Kilchan and Hansen, Mark and Harrell, Lauren",
        "title": "Item response theory"
      },
      {
        "key": "natesan2016bayesian",
        "author": "Natesan, Prathiba and Nandakumar, Ratna and Minka, Tom and Rubright, Jonathan D",
        "title": "Bayesian prior choice in IRT estimation using MCMC and variational Bayes"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "glickman2012example",
        "author": "Glickman, Mark E",
        "title": "Example of the Glicko-2 system"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "desender2017subjective",
        "author": "Desender, Kobe and Van Opstal, Filip and Van den Bussche, Eva",
        "title": "Subjective experience of difficulty depends on multiple cues"
      }
    ]
  }
]