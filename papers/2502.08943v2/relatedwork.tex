\section{Related Work}
\subsection{LLM Benchmark Evaluation}
Recent benchmark evaluations have significantly enhanced our understanding of Large Language Models (LLMs) and have driven further advancements in the field. Notable benchmarks like MMLU \citep{hendrycks2020measuring}, HELM \citep{liang2022holistic}, and BIG-bench \citep{srivastava2022beyond} have expanded assessments to include language generation, general knowledge understanding, and complex reasoning. Several other benchmarks assess the trustworthiness of large language models (LLMs) \citep{wang2023decodingtrust, huang2024trustllm, zhang2024defining} in terms of safety, bias, privacy, and hallucination, etc. Leaderboards like the OpenLLM Leaderboard \citep{open-llm-leaderboard-v1} facilitate performance comparisons across LLMs by evaluating a range of tasks, each targeting different capabilities, to provide a comprehensive assessment of LLMs. However, most benchmark evaluations, even on leaderboards, rely on a single output per example, either greedy decoding or random sampling. \citet{song2024good} also examines the performance gap between the two types of generation strategies and highlights the importance of randomness. There is also concurrent work by \citet{miller2024adding} that mentions using multiple generations to reduce variance, but their contribution is primarily conceptual. In contrast, we provide both theoretical support and empirical results. Additionally, we propose several benefits of using multiple generations, such as difficulty quantification and mislabeled prompt detection, which distinguish our work from theirs.

\subsection{Prompt Difficulty in Benchmark}
Understanding prompt-level difficulty is crucial for analyzing benchmark composition and some benchmark datasets include difficulty scores for each prompt provided by humans. For example, the MATH dataset \citep{hendrycks2measuring} offers a variety of high-school-level problems with a broad five-level difficulty rating. Similarly, the GPQA dataset \citep{rein2023gpqa} contains graduate-level multiple-choice questions rated on a 4-point scale by two experts. Recent studies \citep{ding2024easy2hard,polotinybenchmarks} also attempted to estimate difficulty scores of individual prompts using item response theory \citep{cai2016item,natesan2016bayesian} or Glicko-2 \citep{glickman2012example}, based on offline evaluation results from a pool of large language models (LLMs) or human participants. This approach seeks to provide an objective difficulty score by encompassing a diverse range of testers, including both humans and LLMs. However, this can lead to misalignment when focusing solely on a target LLM. A question that is easy for one model might be difficult for others, highlighting the inherently subjective nature of difficulty \citep{desender2017subjective}. Therefore, it is more relevant to consider the subjective difficulty specific to the target LLM.