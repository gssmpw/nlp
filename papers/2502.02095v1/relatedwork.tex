\section{Related Work}
\textbf{Long Context LLMs} Some studies explore training-free methods to extend the input context window, such as modifications to the model architecture~\citep{DBLP:conf/iclr/PengQFS24,DBLP:conf/iclr/XiaoTCHL24}, position encoding~\citep{DBLP:conf/icml/DingZZXSX0Y24,xiao2024duoattentionefficientlongcontextllm}, and others. Another approach focuses on training-based techniques~\citep{bai-etal-2024-longalign,munkhdalai2024leavecontextbehindefficient,DBLP:conf/icml/FuPNYHK024}. Many LLMs can support input context windows of 32K or even over 128K words. However, far fewer are capable of generating outputs exceeding 2K words in length. Recent studies~\citep{DBLP:conf/emnlp/PhamSI24,bai2024longwriterunleashing10000word} have employed outcome supervision to extend the output window. Most recently, ~\citet{zhang2024longrewardimprovinglongcontextlarge} proposed LongReward, which is orthogonal to our work. However, in addition to the instruction and response, it requires an additional reference long document as input, which limits its applicability in both outcome and process supervision.

% Despite the significant contributions of previous work, exploration into long-form text generation remains insufficient. We aim to conduct a more in-depth investigation into this area. 


\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.99\linewidth]{Figs/fig2.pdf}
  \caption{
The pipeline of LongDPO. LongDPO incorporates process supervision and MCTS to collect stepwise preference data, where the preference data share the same prefix and only one pair is collected at each layer. During the selection phase, LongDPO uses the global memory pool to filter out candidates that may result in inconsistency, then selects the highest-scoring one as the chosen candidate, with another randomly selected as the rejected candidate. During tree expansion, some chosen candidates may have low rewards, LongDPO uses external knowledge to provide critiques for refinement. Then the collected preference pairs are used for step-level DPO training. 
  }
  \label{fig2}
\end{figure*}


\textbf{Process Supervision in Preference Learning} % scaling inference-time compute
% 介绍inference-time scaling比较火，然后 列举运用MCTS的文章，最后讲出与previous work的区别
Recently, scaling inference-time compute has become increasingly popular~\citep{chen2024optimaoptimizingeffectivenessefficiency,setlur2024rewardingprogressscalingautomated,snell2024scalingllmtesttimecompute}. Process supervision with MCTS can further enhance models' reasoning abilities~\citep{tian2024selfimprovementllmsimaginationsearching,zhang2024chainpreferenceoptimizationimproving,zhang2024accessinggpt4levelmathematical}. Recent studies~\citep{wang2024selfimprovementllmsmctsleveraging,xu2024sramctsselfdrivenreasoningaugmentation} use MCTS in both math and code tasks. In addition to MCTS, ~\citet{zhao2024marcoo1openreasoningmodels} also incorporate self-reflection. ~\citet{cheng2024sparselfplaytreesearchrefinement} employ tree search and train a refiner for iterative optimization. In this work, we primarily focus on exploring the potential of process supervision with MCTS in long-form generation.

\textbf{Use LLM to Critic} The LLM-generated critiques are able to provide additional information and have been widely applied~\citep{madaan2023selfrefineiterativerefinementselffeedback,yuan-etal-2024-llmcrit}. CriticGPT~\citep{mcaleese2024llm}, trained using reinforcement learning, can generate critiques that surpass those produced by humans. Recent studies~\citep{ankner2024critiqueoutloudrewardmodels, ye2024improvingrewardmodelssynthetic} use self-generated critiques for each piece of preference data, which are used to train reward models.~\citet{yu2024selfgeneratedcritiquesboostreward} further uses an instance-level critiques filter to reduce conflicts.