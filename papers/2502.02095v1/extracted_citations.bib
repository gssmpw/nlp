@inproceedings{DBLP:conf/emnlp/PhamSI24,
  author       = {Chau Pham and
                  Simeng Sun and
                  Mohit Iyyer},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {Suri: Multi-constraint Instruction Following in Long-form Text Generation},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2024, Miami, Florida, USA, November 12-16, 2024},
  pages        = {1722--1753},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.findings-emnlp.94},
  timestamp    = {Mon, 18 Nov 2024 09:05:59 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/PhamSI24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/PengQFS24,
  author       = {Bowen Peng and
                  Jeffrey Quesnelle and
                  Honglu Fan and
                  Enrico Shippole},
  title        = {YaRN: Efficient Context Window Extension of Large Language Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=wHBfxhZu1u},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/PengQFS24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/XiaoTCHL24,
  author       = {Guangxuan Xiao and
                  Yuandong Tian and
                  Beidi Chen and
                  Song Han and
                  Mike Lewis},
  title        = {Efficient Streaming Language Models with Attention Sinks},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=NG7sS51zVF},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/XiaoTCHL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/DingZZXSX0Y24,
  author       = {Yiran Ding and
                  Li Lyna Zhang and
                  Chengruidong Zhang and
                  Yuanyuan Xu and
                  Ning Shang and
                  Jiahang Xu and
                  Fan Yang and
                  Mao Yang},
  title        = {LongRoPE: Extending {LLM} Context Window Beyond 2 Million Tokens},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=ONOtpXLqqw},
  timestamp    = {Mon, 21 Oct 2024 07:58:41 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/DingZZXSX0Y24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/FuPNYHK024,
  author       = {Yao Fu and
                  Rameswar Panda and
                  Xinyao Niu and
                  Xiang Yue and
                  Hannaneh Hajishirzi and
                  Yoon Kim and
                  Hao Peng},
  title        = {Data Engineering for Scaling Language Models to 128K Context},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=TaAqeo7lUh},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/FuPNYHK024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ankner2024critiqueoutloudrewardmodels,
      title={Critique-out-Loud Reward Models}, 
      author={Zachary Ankner and Mansheej Paul and Brandon Cui and Jonathan D. Chang and Prithviraj Ammanabrolu},
      year={2024},
      eprint={2408.11791},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.11791}, 
}

@inproceedings{bai-etal-2024-longalign,
    title = "{L}ong{A}lign: A Recipe for Long Context Alignment of Large Language Models",
    author = "Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      He, Yuze  and
      Qi, Ji  and
      Hou, Lei  and
      Tang, Jie  and
      Dong, Yuxiao  and
      Li, Juanzi",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.74",
    pages = "1376--1395",
    abstract = "Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign{---}a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30{\%}, while also maintaining their proficiency in handling short, generic tasks.",
}

@misc{bai2024longwriterunleashing10000word,
      title={LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs}, 
      author={Yushi Bai and Jiajie Zhang and Xin Lv and Linzhi Zheng and Siqi Zhu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
      year={2024},
      eprint={2408.07055},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.07055}, 
}

@misc{chen2024optimaoptimizingeffectivenessefficiency,
      title={Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System}, 
      author={Weize Chen and Jiarui Yuan and Chen Qian and Cheng Yang and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2410.08115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.08115}, 
}

@misc{cheng2024sparselfplaytreesearchrefinement,
      title={SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models}, 
      author={Jiale Cheng and Xiao Liu and Cunxiang Wang and Xiaotao Gu and Yida Lu and Dan Zhang and Yuxiao Dong and Jie Tang and Hongning Wang and Minlie Huang},
      year={2024},
      eprint={2412.11605},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.11605}, 
}

@misc{madaan2023selfrefineiterativerefinementselffeedback,
      title={Self-Refine: Iterative Refinement with Self-Feedback}, 
      author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
      year={2023},
      eprint={2303.17651},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.17651}, 
}

@article{mcaleese2024llm,
  title={Llm critics help catch llm bugs},
  author={McAleese, Nat and Pokorny, Rai Michael and Uribe, Juan Felipe Ceron and Nitishinskaya, Evgenia and Trebacz, Maja and Leike, Jan},
  journal={arXiv preprint arXiv:2407.00215},
  year={2024}
}

@misc{munkhdalai2024leavecontextbehindefficient,
      title={Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention}, 
      author={Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal},
      year={2024},
      eprint={2404.07143},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.07143}, 
}

@misc{setlur2024rewardingprogressscalingautomated,
      title={Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning}, 
      author={Amrith Setlur and Chirag Nagpal and Adam Fisch and Xinyang Geng and Jacob Eisenstein and Rishabh Agarwal and Alekh Agarwal and Jonathan Berant and Aviral Kumar},
      year={2024},
      eprint={2410.08146},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08146}, 
}

@misc{snell2024scalingllmtesttimecompute,
      title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      eprint={2408.03314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.03314}, 
}

@misc{tian2024selfimprovementllmsimaginationsearching,
      title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing}, 
      author={Ye Tian and Baolin Peng and Linfeng Song and Lifeng Jin and Dian Yu and Haitao Mi and Dong Yu},
      year={2024},
      eprint={2404.12253},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.12253}, 
}

@misc{wang2024selfimprovementllmsmctsleveraging,
      title={Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning}, 
      author={Xiyao Wang and Linfeng Song and Ye Tian and Dian Yu and Baolin Peng and Haitao Mi and Furong Huang and Dong Yu},
      year={2024},
      eprint={2410.06508},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.06508}, 
}

@misc{xiao2024duoattentionefficientlongcontextllm,
      title={DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads}, 
      author={Guangxuan Xiao and Jiaming Tang and Jingwei Zuo and Junxian Guo and Shang Yang and Haotian Tang and Yao Fu and Song Han},
      year={2024},
      eprint={2410.10819},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10819}, 
}

@misc{xu2024sramctsselfdrivenreasoningaugmentation,
      title={SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation}, 
      author={Bin Xu and Yiguan Lin and Yinghao Li and Yang Gao},
      year={2024},
      eprint={2411.11053},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.11053}, 
}

@misc{ye2024improvingrewardmodelssynthetic,
      title={Improving Reward Models with Synthetic Critiques}, 
      author={Zihuiwen Ye and Fraser Greenlee-Scott and Max Bartolo and Phil Blunsom and Jon Ander Campos and Matthias Gall√©},
      year={2024},
      eprint={2405.20850},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20850}, 
}

@misc{yu2024selfgeneratedcritiquesboostreward,
      title={Self-Generated Critiques Boost Reward Modeling for Language Models}, 
      author={Yue Yu and Zhengxing Chen and Aston Zhang and Liang Tan and Chenguang Zhu and Richard Yuanzhe Pang and Yundi Qian and Xuewei Wang and Suchin Gururangan and Chao Zhang and Melanie Kambadur and Dhruv Mahajan and Rui Hou},
      year={2024},
      eprint={2411.16646},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.16646}, 
}

@inproceedings{yuan-etal-2024-llmcrit,
    title = "{LLMC}rit: Teaching Large Language Models to Use Criteria",
    author = "Yuan, Weizhe  and
      Liu, Pengfei  and
      Gall{\'e}, Matthias",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.472/",
    doi = "10.18653/v1/2024.findings-acl.472",
    pages = "7929--7960",
    abstract = "Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, current research in this area tends to consider only a limited number of criteria, or only a limited number of quality assessment aspects. To fill this gap, we propose a general framework that enables large language models (LLMs) to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs in-context demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback generation framework using different LLMs. The results reveal the fine-grained effects of adding criteria and demonstrations and provide valuable guidance on how to teach LLMs to use criteria more effectively."
}

@misc{zhang2024accessinggpt4levelmathematical,
      title={Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B}, 
      author={Di Zhang and Xiaoshui Huang and Dongzhan Zhou and Yuqiang Li and Wanli Ouyang},
      year={2024},
      eprint={2406.07394},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.07394}, 
}

@misc{zhang2024chainpreferenceoptimizationimproving,
      title={Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs}, 
      author={Xuan Zhang and Chao Du and Tianyu Pang and Qian Liu and Wei Gao and Min Lin},
      year={2024},
      eprint={2406.09136},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.09136}, 
}

@misc{zhang2024longrewardimprovinglongcontextlarge,
      title={LongReward: Improving Long-context Large Language Models with AI Feedback}, 
      author={Jiajie Zhang and Zhongni Hou and Xin Lv and Shulin Cao and Zhenyu Hou and Yilin Niu and Lei Hou and Yuxiao Dong and Ling Feng and Juanzi Li},
      year={2024},
      eprint={2410.21252},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21252}, 
}

@misc{zhao2024marcoo1openreasoningmodels,
      title={Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions}, 
      author={Yu Zhao and Huifeng Yin and Bo Zeng and Hao Wang and Tianqi Shi and Chenyang Lyu and Longyue Wang and Weihua Luo and Kaifu Zhang},
      year={2024},
      eprint={2411.14405},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.14405}, 
}

