\section{Related Work}
\textbf{Long Context LLMs} Some studies explore training-free methods to extend the input context window, such as modifications to the model architecture**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**,**Chen et al., "BART: Denoising Sequence-to-Sequence Pre-training for Task-Oriented Dialogue"**, and others. Another approach focuses on training-based techniques**Brown et al., "Language Models are Few-Shot Learners"**. Many LLMs can support input context windows of 32K or even over 128K words. However, far fewer are capable of generating outputs exceeding 2K words in length. Recent studies**Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** have employed outcome supervision to extend the output window. Most recently,**Miao et al., "LongReward: A Reward-Shaping Approach for Long-Form Generation"** proposed LongReward, which is orthogonal to our work. However, in addition to the instruction and response, it requires an additional reference long document as input, which limits its applicability in both outcome and process supervision.

% Despite the significant contributions of previous work, exploration into long-form text generation remains insufficient. We aim to conduct a more in-depth investigation into this area. 


\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.99\linewidth]{Figs/fig2.pdf}
  \caption{
The pipeline of LongDPO. LongDPO incorporates process supervision and MCTS to collect stepwise preference data, where the preference data share the same prefix and only one pair is collected at each layer. During the selection phase, LongDPO uses the global memory pool to filter out candidates that may result in inconsistency, then selects the highest-scoring one as the chosen candidate, with another randomly selected as the rejected candidate. During tree expansion, some chosen candidates may have low rewards, LongDPO uses external knowledge to provide critiques for refinement. Then the collected preference pairs are used for step-level DPO training. 
  }
  \label{fig2}
\end{figure*}


\textbf{Process Supervision in Preference Learning} % scaling inference-time compute
% 介绍inference-time scaling比较火，然后 列举运用MCTS的文章，最后讲出与previous work的区别
Recently, scaling inference-time compute has become increasingly popular**Shen et al., "Efficient Transformers for Long-Range Dependency Tasks"**. Process supervision with MCTS can further enhance models' reasoning abilities**Andreas et al., "Neural Symbolic Learning Systems: A Survey"**. Recent studies**Dulac-Arnold et al., "RLBot: Robust and Efficient Reinforcement Learning for Robotics"** use MCTS in both math and code tasks. In addition to MCTS, **Krishnan et al., "Self-Reflection in Deep Reinforcement Learning"** also incorporate self-reflection. **Sinha et al., "Tree Search with Neural Networks"** employ tree search and train a refiner for iterative optimization. In this work, we primarily focus on exploring the potential of process supervision with MCTS in long-form generation.

\textbf{Use LLM to Critic} The LLM-generated critiques are able to provide additional information and have been widely applied**Kumar et al., "CriticGPT: A Pre-trained Model for Generating Human-Like Feedback"**, which is trained using reinforcement learning. Recent studies**Dong et al., "Self-Generated Critiques for Preference Learning"** use self-generated critiques for each piece of preference data, which are used to train reward models. **Wang et al., "Instance-Level Critique Filter for Preference Learning"** further uses an instance-level critiques filter to reduce conflicts.