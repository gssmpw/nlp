%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
% \DeclareUnicodeCharacter{FF0C}{,}
\usepackage[T1]{fontenc}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
% \usepackage{todonotes}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% \newcommand{\lemon}[1]{\textcolor{red}{#1}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml_to_arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information}
\usepackage{tcolorbox}

\begin{document}

\twocolumn[
\icmltitle{LongDPO: Unlock Better Long-form Generation Abilities for LLMs \\ via Critique-augmented Stepwise Information}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Bowen Ping}{sch1}
\icmlauthor{Jiali Zeng}{comp}
\icmlauthor{Fandong Meng}{comp}
\icmlauthor{Shuo Wang}{sch2}
\icmlauthor{Jie Zhou}{comp}
\icmlauthor{Shanghang Zhang}{sch1}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sch1}{Peking University, China}
\icmlaffiliation{comp}{Pattern Recognition Center, WeChat AI, Tencent Inc, China}
\icmlaffiliation{sch2}{Tsinghua University, China}

\icmlcorrespondingauthor{Shanghang Zhang}{shanghang@pku.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones.

% \lemon{Long-form generation is crucial for tasks such as academic writing and repo-level code generation. Despite recent advancements, current models, including GPT-4, still fall short in performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, diminished quality, or inconsistency.
% In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to ensure consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs.
% Our experimental results indicate that this approach significantly enhances both the length and quality of long-form generation benchmarks, while maintaining nearly lossless performance on general benchmarks across various model backbones.}
\end{abstract}


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.99\linewidth]{Figs/fig1.pdf}
  \caption{
The above is outcome supervision in long-form generation tasks. Below is LongDPO uses process supervision with a global memory to maintain factual consistency, and external critiques to refine low-reward chosen candidates.
  }
  \label{fig1}
\end{figure}

\section{Introduction}

Recent advancements in large language models (LLMs)~\citep{zhou2024llmtimesmapreducesimplifiedlongsequenceprocessing,xiao2024duoattentionefficientlongcontextllm,xiao2024infllmtrainingfreelongcontextextrapolation,ping2024deltacometrainingfreedeltacompressionmixedprecision} have significantly enhanced their ability to process long text sequences leading models like GPT-4o~\citep{openai2024gpt4ocard} can handle context of up to 128K. 
Despite these advancements, there has been less emphasis on the models' ability to generate high-quality long-form text outputs.
The capability to produce long-form content is essential for various real-world applications, including writing academic papers, novels, and scripts in literature, generating legal contracts in law, and producing repository-level code in technology~\citep{bai2024longwriterunleashing10000word,wang2024autosurveylargelanguagemodels}. 
However, many LLMs still struggle to generate content exceeding 2,000 words, highlighting the need for further advancements in this area.

Previous research has investigated methods to extend the output window by constructing long-form training data and utilizing preference learning. For example, Suri~\citep{DBLP:conf/emnlp/PhamSI24} creates various instructions for the same response and introduces instructional ORPO. LongWriter~\citep{bai2024longwriterunleashing10000word} employs an agent-based pipeline that decomposes ultra-long generation tasks into subtasks to build a long-form dataset, followed by supervised fine-tuning and DPO. These approaches primarily rely on outcome supervision~\citep{DBLP:conf/iclr/LightmanKBEBLLS24} during DPO, which provides feedback on the final result, for long-form generation tasks.

Nevertheless, outcome supervision is not well-suited for long-form generation due to several key challenges. Specifically, outcome supervision means that intermediate steps are not adequately guided, potentially leading to a loss of coherence and inconsistency in the final output~\cite{zhang2024longrewardimprovinglongcontextlarge}. As a result, this coarse supervision can produce content that does not fully meet query requirements, leading to problems like length deviations or poor quality~\cite{zhang2024longrewardimprovinglongcontextlarge,bai2024longwriterunleashing10000word}. 
The absence of fine-grained feedback prevents the model from learning to improve particular aspects of the output.

In this paper, we introduce a novel approach called LongDPO for long-form generation using step-level supervision. 
Our method is divided into two main components: constructing preference data with stepwise supervision signals and implementing stepwise DPO.
Specifically, 
we employ Monte Carlo Tree Search (MCTS)~\cite{6145622} with a large language model (LLM) acting as a judge during the evaluation phase to collect stepwise preference pairs. 
To maintain consistency in the generated text, we incorporate a global memory pool to store factual content from selected nodes during the MCTS search.
However, not all chosen candidates in the collected preference pairs are of high quality. 
To address this issue, we utilize the judge model from the MCTS evaluation phase to provide critiques, followed by critique-augmented generation to refine the chosen candidates. Compared to external critiques, searching for additional solutions is inefficient and may yield only limited performance improvements~\citep{qi2024mutualreasoningmakessmaller}. Additionally, relying solely on self-critique, which depends on the model's inherent capabilities, can result in unstable performance gains~\citep{qi2024mutualreasoningmakessmaller, zhang2024understandingdarkllmsintrinsic}.
Given the high-quality stepwise preference pair data, we propose using a stepwise DPO for training to enhance the learning process.
As shown in Figure~\ref{fig1}, vanilla DPO applies sample-wise supervision directly. Previous work has shown that this approach may result in a less distinct reward margin, making learning more difficult~\citep{lai2024stepdpostepwisepreferenceoptimization}. In contrast, LongDPO uses fine-grained learning at each step, which may yield better results.


% We evaluate our method on long-form benchmarks  LongBench-Write-en~\citep{bai2024longwriterunleashing10000word}, ~\citep{wu2024spinning} and TruthfulQA~\citep{DBLP:conf/acl/LinHE22}, respectively.
We evaluate long-form generation capabilities with specific length requirements and the ability to follow complex long-form instructions using LongBench-Write-en and LongGenBench~\citep{bai2024longwriterunleashing10000word, wu2024spinning}, as well as several general benchmarks such as TruthfulQA~\citep{DBLP:conf/acl/LinHE22} to assess general task performance. Our method, built on Llama- and Qwen-based backbones, outperforms their DPO versions in long-form generation tasks while maintaining near-lossless performance on general tasks.
% We evaluate our method on long-form benchmarks, such as LongBench-Write-en and LongGenBench~\citep{bai2024longwriterunleashing10000word, wu2024spinning}, as well as several general benchmarks such as TruthfulQA~\citep{DBLP:conf/acl/LinHE22}. Our method improves performance on long-form generation benchmarks. 

% For example, for tasks with output lengths exceeding 4,000 words, the absolute performance increases are 8\% for Llama-based models and 11\% for Qwen-based models, with quality improvements of 5\% and 7\%, respectively. In addition, the performance on general benchmarks remains nearly lossless.

% Finally, we conduct an in-depth analysis to compare the diversity and consistency to show that our model's responses align more closely with human preferences.


Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose LongDPO, which differs from traditional outcome supervision by adopting process supervision. LongDPO enables step-wise, more fine-grained learning for long-form text generation. 
    \item To implement process supervision, we introduce MCTS to construct step-level preference data. Specifically, we utilize a global memory pool to maintain factual consistency and incorporate external critiques to collect better preference pairs.
    \item We validate that our method enhances performance in long-form generation tasks while maintaining near-lossless results for general tasks. Meanwhile, we conduct an in-depth analysis to show that the generated responses align more closely with human preferences.
\end{itemize}

\section{Related Work}
\textbf{Long Context LLMs} Some studies explore training-free methods to extend the input context window, such as modifications to the model architecture~\citep{DBLP:conf/iclr/PengQFS24,DBLP:conf/iclr/XiaoTCHL24}, position encoding~\citep{DBLP:conf/icml/DingZZXSX0Y24,xiao2024duoattentionefficientlongcontextllm}, and others. Another approach focuses on training-based techniques~\citep{bai-etal-2024-longalign,munkhdalai2024leavecontextbehindefficient,DBLP:conf/icml/FuPNYHK024}. Many LLMs can support input context windows of 32K or even over 128K words. However, far fewer are capable of generating outputs exceeding 2K words in length. Recent studies~\citep{DBLP:conf/emnlp/PhamSI24,bai2024longwriterunleashing10000word} have employed outcome supervision to extend the output window. Most recently, ~\citet{zhang2024longrewardimprovinglongcontextlarge} proposed LongReward, which is orthogonal to our work. However, in addition to the instruction and response, it requires an additional reference long document as input, which limits its applicability in both outcome and process supervision.

% Despite the significant contributions of previous work, exploration into long-form text generation remains insufficient. We aim to conduct a more in-depth investigation into this area. 


\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.99\linewidth]{Figs/fig2.pdf}
  \caption{
The pipeline of LongDPO. LongDPO incorporates process supervision and MCTS to collect stepwise preference data, where the preference data share the same prefix and only one pair is collected at each layer. During the selection phase, LongDPO uses the global memory pool to filter out candidates that may result in inconsistency, then selects the highest-scoring one as the chosen candidate, with another randomly selected as the rejected candidate. During tree expansion, some chosen candidates may have low rewards, LongDPO uses external knowledge to provide critiques for refinement. Then the collected preference pairs are used for step-level DPO training. 
  }
  \label{fig2}
\end{figure*}


\textbf{Process Supervision in Preference Learning} % scaling inference-time compute
% 介绍inference-time scaling比较火，然后 列举运用MCTS的文章，最后讲出与previous work的区别
Recently, scaling inference-time compute has become increasingly popular~\citep{chen2024optimaoptimizingeffectivenessefficiency,setlur2024rewardingprogressscalingautomated,snell2024scalingllmtesttimecompute}. Process supervision with MCTS can further enhance models' reasoning abilities~\citep{tian2024selfimprovementllmsimaginationsearching,zhang2024chainpreferenceoptimizationimproving,zhang2024accessinggpt4levelmathematical}. Recent studies~\citep{wang2024selfimprovementllmsmctsleveraging,xu2024sramctsselfdrivenreasoningaugmentation} use MCTS in both math and code tasks. In addition to MCTS, ~\citet{zhao2024marcoo1openreasoningmodels} also incorporate self-reflection. ~\citet{cheng2024sparselfplaytreesearchrefinement} employ tree search and train a refiner for iterative optimization. In this work, we primarily focus on exploring the potential of process supervision with MCTS in long-form generation.

\textbf{Use LLM to Critic} The LLM-generated critiques are able to provide additional information and have been widely applied~\citep{madaan2023selfrefineiterativerefinementselffeedback,yuan-etal-2024-llmcrit}. CriticGPT~\citep{mcaleese2024llm}, trained using reinforcement learning, can generate critiques that surpass those produced by humans. Recent studies~\citep{ankner2024critiqueoutloudrewardmodels, ye2024improvingrewardmodelssynthetic} use self-generated critiques for each piece of preference data, which are used to train reward models.~\citet{yu2024selfgeneratedcritiquesboostreward} further uses an instance-level critiques filter to reduce conflicts.

\section{LongDPO} Our method consists of two main parts: 1) collecting stepwise preference data, and 2) using the collected preference data for DPO training.

\subsection{Stepwise Preference Data Construction}
Currently, MCTS has demonstrated its potential in reasoning tasks which employs an additional reward model to better preference data at each reasoning step~\citep{chen2024alphamathzeroprocesssupervision,xie2024montecarlotreesearch}, enabling 7B models to achieve performance comparable to GPT-o1~\citep{guan2025rstarmathsmallllmsmaster}. Intuitively, long-form generation may also be learned by collecting stepwise preference data. We will elaborate on collecting preference data in the following. 

\begin{comment}
 MCTS executes four procedures: selection, expansion, evaluation, and backpropagation.
During selection, MCTS usually uses the UCB score~\citep{auer2002finite}.
\begin{equation}
    % UCB(i) =  \times  
    UCB_{i} = \alpha \times \sqrt{2 \times \ln \left( \frac{N_i}{n_i} \right)} + v_{i},
    \label{ucb}
\end{equation}
where \( n_{i} \) and \( N_{i} \) represent the visit count and the parent visit count of the node, respectively. \( \alpha \) is a scalar that balances exploration and exploitation. \( v_{i} \) denotes the value of the node, and we use the average reward provided by a reward model. During the expansion phase, MCTS generates multiple candidate nodes for evaluation. In the evaluation phase, the newly generated nodes are assessed. In the backpropagation phase, the value and visit count of the node and its parent nodes are updated.
\end{comment}

% \subsection{Stepwise Preference Pair Collection}


\subsubsection{Overview} MCTS executes four procedures: selection, expansion, evaluation, and back-propagation. To be specific, our tree is executed according to the following:
    \begin{itemize}
        \item \textbf{Selection}: We select the node to be expanded using Equation~\ref{ucb} with a global memory pool to filter out inconsistent nodes.
        \begin{equation}
        % UCB(i) =  \times  
        UCB_{i} = \alpha \times \sqrt{2 \times \ln \left( \frac{N_i}{n_i} \right)} + v_{i},
        \label{ucb}
    \end{equation}
    where \( n_{i} \) and \( N_{i} \) represent the visit count and the parent visit count of the node, respectively. \( \alpha \) is a scalar that balances exploration and exploitation. \( v_{i} \) denotes the value of the node, and we use the average reward provided by a reward model.
    \item \textbf{Expansion}: For each node to be expanded, we generate several child nodes using a sampling-based algorithm \citep{holtzmancurious}. 
    \item \textbf{Evaluation}: In terms of evaluating each node, we assess each node using the value provided by a reward model, as previous work has demonstrated its effectiveness~\citep{ wang2024selfimprovementllmsmctsleveraging,wang2024litesearchefficacioustreesearch}. We consider seven principles to evaluate each node. Each principle is rated between 1 and 5, as detailed in Appendix~\ref{text_reward}.
    \item \textbf{Back-propagation}: We update the parent node using the value of the leaf nodes and also update the parent node's visit count.
\end{itemize}

Specifically, given a query \( q \), during the expansion phase, the node in layer \( t \) is represented as \( s_t \). The newly node $s_{t+1}$ is generated using the Equation~\ref{step}: 
\begin{equation}
    \label{step}
    s_{t+1} = \pi_{\theta}(q \oplus s_{1} \oplus s_{2}\oplus \dots \oplus s_{t}),
\end{equation}
where $\pi_{\theta}$ is the generator, and $\oplus$ represents the concatenation operation. In each evaluation phase, its corresponding value is evaluated as:
\begin{equation}
    \label{reward}
    r_{s_{t+1}} = \Theta(q \oplus s_1 \oplus s_2 \oplus \dots \oplus s_t, s_{t+1}),
\end{equation}
where \( r_{s_{t+1}} \) is the average reward of the seven principles, \( \Theta \) is the reward model used to evaluate the reward of \( s_{t+1} \) as the suffix. When reaching each leaf node, the back-propagation phase is executed. At each selection phase, we use Equation~\ref{ucb} along with a global memory pool to make selections, as detailed in the next subsection.

\subsubsection{Preference Pair Extraction}
To maintain factual consistency as much as possible, we use a global memory pool $M$ to store relevant factual context. After selecting each node \( s\), the memory pool \( M\) is also updated accordingly. To be specific, after selecting the node $s_t$, we extract the factual content of $s_t$ using the model $\Theta$. The extracted factual content is denoted as $\{m_1,m_2,\dots,m_{k'} \}$. After extracting, we first employ $\Theta$ to check the extracted factual contents to ensure they are factually correct as much as possible. We retain only the factual content $\{m_1,m_2,\dots,m_k \}$ that does not conflict with the internal knowledge of \( \Theta \), using the template provided in Appendix~\ref{find_fact}. Then, we update the memory correspondingly $M_t = M_{t-1} \cup \{m_1,m_2,\dots,m_k \}$. Next, we will use $M_t$ to select the node $s_{t+1}$.

% After extracting, we first employ $\Theta$ to check the extracted factual contents to ensure they are factually correct as much as possible using templates in Appendix~\ref{find_fact}. 

Specifically, after the expansion phase, we visit the nodes in descending order of their average rewards derived using Equation~\ref{reward}. We break the currently visited node $s_{cur}$ into chunks of 128 words, resulting in a sequence of chunks \(\{ chunk_{1}, chunk_{2}, \dots, chunk_{j} \} \) and calculate the similarity score using each $m_k$ in $M_t$ as a query. 
\begin{equation}
    \label{similarity_score}
    sim_{kj} = E(m_{k}) \times E(chunk_{j})^T,
\end{equation}
where $sim_{kj}$ is the similarity score, $E(x)$ represents get the embedding of $x$, we use gte-Qwen2-1.5B-instruct\footnote{\url{https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct}} as embedding model.
Then, we use the similarity score to filter irrelevant factual content for each $m_k$.
\begin{equation}
    \label{support_fact}
    A_{k} = \{ chunk_j \mid sim_{kj} \geq \delta \},
\end{equation}
where $\delta$ the similarity threshold is set to 0.8. Finally, we use each \( m_k \) and its corresponding supported factual content $A_{k}$ to check for any inconsistencies using model $\Theta$ using templates in Appendix~\ref{judge_fact}. Finally, if no inconsistencies are found, we select $s_{cur}$ as $s_{t+1}$. Otherwise, we will visit the next candidate node without expanding the current one further. For each layer of the tree, we select one pair for preference learning: the node with the highest average reward and no consistency errors is selected as the chosen candidate $s_{win}$, while another node is randomly selected as the rejected candidate $s_{lose}$.

Different from~\citet{zhang2024longrewardimprovinglongcontextlarge}, we focus on global information by using the previously selected node as a query, rather than solely focusing on the current local text. On the other hand, we use a more powerful model for a double-check to ensure that the extracted facts are not factually incorrect as much as possible. 

% \begin{equation}
%     \text{Similarity}(m_i, \text{chunk}_j) = \text{score}_j \quad \forall \, i \in \{1, 2, \dots, k\}, \quad j \in \{1, 2, \dots, n\}
% \end{equation}

% At each level, we select the $Res_{t}$ with the largest \( r \) as the positive sample and randomly choose another from the remaining ones as the negative sample as a pair. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.99\linewidth]{Figs/reward_distribution_percentage_new.pdf}
  \caption{
  Reward analysis of the selected candidates, we focus solely on the chosen candidate in each preference pair. On the x-axis, '0-3.0' represents the proportion of candidates with an average reward  $ < 3.0$, while '3.0-3.5' represents the proportion of candidates with an average reward $\geq 3.0$ but $< 3.5$. Detailed reward distribution can be found in Appendix~\ref{reward_distribution_full}. 
  }
  \label{reward_distribution}
\end{figure}

\subsection{Chosen Candidates Refinement using Critiques}
After collecting preference pairs for long-form generation, we then randomly select 1,000 pairs and only analyze the average reward of the chosen candidate in each pair, as shown in Figure~\ref{reward_distribution}. On the one hand, many of the chosen candidates in each preference pair have low rewards which may lead to suboptimal performance. On the other hand, the large reward discrepancies between different samples could result in unstable training~\citep{wu2024betadpodirectpreferenceoptimization}.

One way to improve performance is by expanding the search space. On the one hand, this is inefficient, especially in the context of long-form generation. On the other hand, recent studies~\citep{brown2024largelanguagemonkeysscaling,qi2024mutualreasoningmakessmaller} have shown that the gains from this approach are limited. Therefore, we propose leveraging external critiques to guide the generator in text generation, as self-critique relies on the model's inherent capabilities. Recent studies have highlighted its instability in driving improvement~\citep{qi2024mutualreasoningmakessmaller,zhang2024understandingdarkllmsintrinsic}.

To be specific, we collect the chosen candidates in each preference pair with average rewards below the threshold \( \eta \) for refinement, as shown in Equation~\ref{thresh_hold}.

\begin{equation}
    \label{thresh_hold}
    S_{\text{refinement}} = \{ s_{win} \mid r_{s_{win}} \leq \eta \},
\end{equation}
where $s_{win}$ and $r_{s_{win}}$ represent the chosen candidate of the collected preference pair and the corresponding average reward. We only refine the chosen candidates, set \( \eta = 2.5 \), and have conducted an ablation study. 

\textbf{Collect Data for Critiques Generation} $S_{\text{refinement}}$ contains the chosen candidates that need to be refined. Next, we prepare the data for the generation of critiques. Specifically, each data is a triplet \( (\text{principle}_u, s_{sib}, s_{win}) \), where \( \text{principle}_u \) is used in the evaluation phase in MCTS to assess the reward of each node, \( s_{win} \) is the chosen candidate to be refined, and \( s_{sib} \) is the sibling node of \( s_{win} \), which serves as an example of refinement as illustrated in Figure~\ref{fig2}. Detailed principles are given in Appendix~\ref{text_reward}.

We construct each pair as the following: for each \( \text{principle}_u \) and \( s_{win} \), if there exists a \( s_{sib} \) whose reward is greater than \( s_{win} \) under \( \text{principle}_u \), the tuple \( (\text{principle}_u, s_{sib}, s_{win}) \) forms a pair to generate critiques.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.62\linewidth]{Figs/fig_refine.pdf}
  \caption{Main body of generated critiques which have detailed in Appedix~\ref{refine_template}
  }
  \label{refine_main_body}
\end{figure}

\textbf{Generate critiques} Next, we use the reward model $\Theta$ to generate critiques for each triplet using template in Appendix~\ref{refine_template}. Figure~\ref{refine_main_body} has shown the main body of the critiques. ``Analysis," ``Justification," and ``Relevant Text" are used to enhance the accuracy of the analysis, while the ``Confidence Score" helps assess the model's confidence in the accuracy of its analysis. ``Writing Suggestion" provides recommendations for improvement.

\textbf{Critique-augmented Generation} For each \( s_{win} \), we utilize its corresponding critiques \( \{z_1, z_2, \dots, z_\lambda\} \), sorted in descending order by ``Confidence Score," to perform critique-augmented generation. Specifically, if \( s_{win} \) is selected in layer \( t+1 \), we rewrite Equation~\ref{step} as follows:
\begin{equation}
    \label{critic-augmented}
        s_{win\_new} = \pi_{\theta}(q \oplus s_{1} \oplus s_{2}\oplus \dots \oplus s_{t} \oplus z_1 \oplus \dots \oplus z_\lambda\ ),
\end{equation}
where we use each ``Writing Suggestion" from \( z_\lambda\ \), with a maximum of three. Then, we use the refined data for DPO training.

\subsection{LongDPO Training Objective}
Previous work on outcome supervision in long-form generation directly utilizes the complete chosen and rejected responses for training~\citep{DBLP:conf/emnlp/PhamSI24,bai2024longwriterunleashing10000word}.
\begin{multline} \label{eq:dpo}
    \mathcal{L}_{DPO} = -\mathbb{E}_{(q, y_w, y_l) \sim D} \Big[ \log \sigma \big( \\ \beta \log\frac{\pi_\theta(y_w|q)}{\pi_{ref}(y_w|q)}
    - \beta \log\frac{\pi_\theta(y_l|q)}{\pi_{ref}(y_l|q)} \big) \Big],
\end{multline}
where $y_{w}$ and $y_{l}$ is the chosen and rejected response, respectively and $\pi_{ref}$ is the reference model. $D$ is the pair-wise preference dataset, $\sigma$ is the sigmoid function, and $\beta$ controls the degree of deviation from the reference model.

In LongDPO, the response \( y \) is decomposed into \( y = s_{1} \oplus s_{2} \oplus \dots \oplus s_{t} \), where \( s_{i} \) represents the \( i \)-th intermediate result. LongDPO conducts learning at each step. Specifically, for the \((i+1)\)-th step, \( s_{win} \) is the chosen step, \( s_{lose} \) is the rejected step, and \(s_{1 \sim i} = s_1\oplus \dots \oplus s_i\) has already been learned. LongDPO aims to maximize the probability of $s_{win}$ and minimize the probability of $s_{lose}$.
\begin{multline} \label{eq:LongDPO}
    \mathcal{L}_{LongDPO} = -\mathbb{E}_{(q\oplus s_{1 \sim i}, s_{win}, s_{lose}) \sim D} \Big[ \log \sigma \big( \\ \beta \log\frac{\pi_\theta(s_{win}|q\oplus s_{1 \sim i})}{\pi_{ref}(s_{win}|q\oplus s_{1 \sim i})}
    - \beta \log\frac{\pi_\theta(s_{lose}|q\oplus s_{1 \sim i})}{\pi_{ref}(s_{lose}|q\oplus s_{1 \sim i})} \big) \Big].
\end{multline}

\begin{table*}[ht]
    \centering
    \caption{Evaluation results on LongBench-Write-en. LongWriter-Llama and LongWriter-Qwen represent LongWriter-llama-8B and LongWriter-Qwen2.5-7B, respectively. Our method improves performance across four output length ranges, particularly for outputs exceeding 4K. We have set a random seed to ensure reproducibility.}
    \begin{tabular}{l|cc|cc|cc|cc|cc cc}
    \toprule
      \multirow{2}{*}{Models} & \multicolumn{2}{c|}{\textbf{[0, 500)}} & \multicolumn{2}{c|}{\textbf{[500, 2k)}} & \multicolumn{2}{c|}{\textbf{[2k, 4k)}} & \multicolumn{2}{c|}{\textbf{[4k, 20k)}} & \multicolumn{2}{c}{\textbf{Average}} \\
     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
    & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ \\
    \midrule
    \textbf{LongWriter-Llama} & 88.10 & 86.00 & 74.50 & 86.90 & 89.10 & 88.30 & 80.80 & 79.20 & 83.12 & 85.10 \\
    \qquad\emph{\textbf{w/ DPO}} & \textbf{90.93} & 85.78 & 76.67 & 85.46 & 90.01 & 90.53 & 81.07 & 80.90 & 85.55 & 85.66 \\
    \qquad\emph{\textbf{w/ LongDPO}} & 90.68 & \textbf{86.27} & \textbf{77.23} & \textbf{91.25} & \textbf{93.35} & \textbf{90.53} & \textbf{88.25} & \textbf{85.06} & \textbf{87.38} & \textbf{88.28}  \\
   \cmidrule(lr){1-1}  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
   \textbf{LongWriter-Qwen} & \textbf{90.80} & 87.99 & 84.37 & 89.37 & 84.21 & 84.84 & 58.69 & 78.13 & 79.51 & 85.08 \\
    \qquad\emph{\textbf{w/ DPO}} & 86.32 & 88.23 & \textbf{88.71} & 89.16 & \textbf{89.28} & 84.09 & 60.89 & 78.82 & 81.30 & 85.07 \\
    \qquad\emph{\textbf{w/ LongDPO}} & 88.93 & \textbf{91.91} & 85.47 & \textbf{91.25} & 88.63 & \textbf{85.60} & \textbf{71.14} & \textbf{85.41} & \textbf{83.54} & \textbf{88.54}  \\    
    \bottomrule
    \end{tabular}
    \label{main_result}
\end{table*}

\section{Experimental Results}
\subsection{Setting Up}
\label{exp:setup}
% 方法上的超参(MCTS tree depth, max_tokens等等)，训练上 (lr,), data
\paragraph{Setting on Collecting Stepwise Pair}
We conduct our experiments using LongWriter-llama3.1-8b~\footnote{\url{https://huggingface.co/THUDM/LongWriter-llama3.1-8b}} and LongWriter-Qwen2.5-7B-Instruct~\footnote{\url{https://www.modelscope.cn/models/swift/MS-LongWriter-Qwen2.5-7B-Instruct}}. To evaluate text rewards and generate critiques for Eq~\ref{critic-augmented}, we utilize Llama-3.1-70B-Instruct \footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}}.
For the MCTS tree configuration, we set the maximum depth to 4, with each node generating 4 child nodes during expansion. Each node can contain up to 2048 tokens, and we use a decoding temperature of 0.7, along with a fixed random seed for reproducibility.

\paragraph{Training Setting}
We randomly sample 2.5k instructions from WildChat \citep{zhaowildchat} to collect stepwise preference pairs, which we then combine with Ultrafeedback \citep{DBLP:conf/icml/CuiY0YH0NXXL0024} for DPO training. 
The learning rate is set to 1e-6, and we employ a cosine learning rate scheduler. 
The maximum sequence length is configured to 32,768 through packing, with a random seed set to 42, and training is conducted for 250 steps.


\paragraph{Evaluation}
We evaluate long-form generation capabilities using the following benchmark:
\begin{itemize}
    \item \textbf{LongBench-Write} employs two metrics: the length score \( S_{l} \), which assesses how closely the model's generated length matches the required length, and the quality score \( S_{q} \), which evaluates the quality of the model's output using GPT-4o~\citep{bai2024longwriterunleashing10000word}. Our evaluation is performed using the English version.
    \item \textbf{LongGenBench}~\citep{wu2024spinning} evaluates whether models can maintain writing coherence and follow instructions which proposes three metrics to evaluate. 
    Completion Rate (CR) assesses the degree to which all designated subtasks are successfully completed. 
    STIC-1 evaluates the model’s adherence to
specific task instructions. 
STIC-2 provides more granular evaluations, measuring the overall completion of specific task instructions.
\end{itemize}
We use the official scripts for evaluation \footnote{\url{https://github.com/THUDM/LongWriter}} \footnote{\url{https://github.com/mozhu621/LongGenBench}}. 
Additionally, we assess the model's general abilities using the following: 
\begin{itemize}
  \item \textbf{TruthfulQA}~\citep{DBLP:conf/acl/LinHE22} to evaluate the helpfulness of the model's response.
  \item \textbf{MMLU}~\citep{hendryckstest2021} to evaluate the model's multitask processing. We use a 5-shot evaluation in our assessment following~\citep{grattafiori2024llama3herdmodels} setting.
  \item \textbf{GSM8K}~\citep{cobbe2021trainingverifierssolvemath} to evaluate the reasoning ability of LLM. We use an 8-shot evaluation following~\citep{grattafiori2024llama3herdmodels} setting.
\end{itemize}
We utilize UltraEval~\citep{he-etal-2024-ultraeval} and lm-evaluation-harness~\citep{eval-harness} for evaluation. 

\paragraph{Baselines}
The \textbf{LongWriter-(.) w/ DPO} baseline models are versions of \textbf{LongWriter-(.)} that have been trained using DPO. 
For each instruction from WildChat~\citep{zhaowildchat}, we generate four responses. The response with the highest reward is selected as the chosen candidate, while one of the remaining responses is randomly selected as the rejected candidate. Then combine Ultrafeedback for training. % Other settings are the same. % Then, the collected preference pairs are combined with Ultrafeedback for DPO training. % Other settings like learning rate remain consistent across the comparison.

% We compare the official SFT models. 
% We further apply DPO for the official SFT models. 
% For the DPO baseline, we generate four responses for each instruction. Among them, the response with the highest reward is selected as the chosen candidate, while one of the remaining responses is randomly selected as the rejected candidate. All 

% \paragraph{Baselines}
% We compare the official SFT models. 
% We further apply DPO for the official SFT models. 
% For the DPO baseline, we generate four responses for each instruction. Among them, the response with the highest reward is selected as the chosen candidate, while one of the remaining responses is randomly selected as the rejected candidate. All other settings remain consistent across the comparison.

\subsection{Main Results}
% 要说明dpo结果convincing (1. 和LongWriter观察相符 2. 从model独立采样并不好,缺少细粒度的supervision)，说明一下结果好，(简单分析一下)
The main results are presented in Table~\ref{main_result}. Our method significantly outperforms baselines across both the Llama and Qwen series models. 
Consistent with the results of~\citet{bai2024longwriterunleashing10000word}, the use of DPO alone did not lead to a substantial performance improvement. 
This could be due to the challenge of maintaining response quality when directly sampling long responses generated by DPO~\citep{cheng2024sparselfplaytreesearchrefinement}. 
In contrast, our method demonstrates performance gains, likely because fine-grained supervision facilitates the acquisition of high-quality data.

%In terms of length score, LongWriter-Llama + LongDPO achieves consistent improvements across various lengths, producing text that better aligns with length requirements. Notably, for output lengths exceeding 4k, the performance improved by approximately 8\%. As for the quality score, we have detailed the results in Table~\ref{tb:quality_detail}. 
% Comparing the SFT and DPO backbones, the primary reasons for the improved scores of our generated texts are the enhancements in ``Clarity," ``Breadth and Depth," and ``Reading Experience."

To be specific, regarding the length score, LongWriter-Llama w/ LongDPO consistently shows improvements across various lengths, generating text that more accurately meets the length requirements. Notably, for outputs exceeding 4,000 words, performance improved by approximately 8\%. 
The quality score results are detailed in Table~\ref{tb:quality_detail}. 
When comparing LongWriter-Llama and LongWriter-Llama w/ DPO, % 跟表格对应的模型名称
the primary factors contributing to the improved scores of our generated texts are enhancements in ``Clarity," ``Breadth and Depth," and ``Reading Experience."

\begin{table*}[ht]
\centering
\caption{Performance comparison across more long-form and general benchmarks. LongGenBench can be used to evaluate output lengths up to 32k. The other benchmarks assess the model's performance on more general tasks like helpfulness and reasoning. For TruthfulQA, we report partition ``MC1" and ``MC2". For each task, all three methods use the same decoding settings, and we have set a random seed to ensure reproducibility.}
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{Models} & \multicolumn{3}{c}{\textbf{LongGenBench (16k)}} & \multicolumn{3}{c}{\textbf{LongGenBench (32k)}} & \multicolumn{2}{c}{\textbf{TruthfulQA}} & \textbf{MMLU} & \textbf{GSM8k} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){7-7} \cmidrule(lr){8-9}  \cmidrule(lr){10-10}  \cmidrule(lr){11-11} 
 & CR & STC1 & STC2 & CR & STC1 & STC2 & ACC & ACC & ACC & ACC \\
\cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){7-7} \cmidrule(lr){8-9}   \cmidrule(lr){10-10}  \cmidrule(lr){11-11} 
\textbf{LongWriter-Llama} & 46.00 & 22.60 & 9.80  & 34.50 & \textbf{33.60} & 10.00 & 38.43 & 56.07 & 63.24 & 57.70 \\
\qquad\emph{\textbf{w/ DPO}}  & 64.99 & 25.99 & 16.29 & 65.24 & 32.47 & 20.39 & 38.17 & 55.68 & 63.30 & 59.20  \\
% \qquad\emph{\textbf{+LongDPO}} 
\qquad\emph{\textbf{w/ LongDPO}} & \textbf{69.38} & \textbf{27.59} & \textbf{18.45} & \textbf{66.96} & 32.63 & \textbf{20.83} & \textbf{40.76} & \textbf{58.78} & \textbf{63.67} & \textbf{61.30} \\
\cmidrule(lr){1-1}  \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){7-7} \cmidrule(lr){8-9}   \cmidrule(lr){10-10}  \cmidrule(lr){11-11} 
\textbf{LongWriter-Qwen}  & \textbf{98.94} & 31.39 & 31.02 & 58.67 & \textbf{33.58} & 18.93 & \textbf{45.29} & 61.78 & 74.16 & 83.78 \\
\qquad\emph{\textbf{w/ DPO}}  & 95.95  & 31.18  & 29.83   & 82.23 & 29.02 & 22.33  & 39.29  & 57.67 & 63.67 & 83.85    \\
\qquad\emph{\textbf{w/ LongDPO}} & 98.51 & \textbf{33.07} & \textbf{32.52} & \textbf{84.95} & 29.86 & \textbf{24.32} & 44.92 & \textbf{62.75} & \textbf{74.25} & \textbf{84.08} \\
\bottomrule
\end{tabular}
\label{tab:long_general}
\end{table*}

\begin{table*}[ht]
    \centering
    \caption{Ablation on refinement methods and ``w/o critique" stands for without critiques meaning MCTS is applied alone. ``Self-critique" refers to critiques generated by the model itself. To verify generalization, we set different values of \(\eta\) and report the average result.}
    \begin{tabular}{l|cc|cc|cc|cc|cc cc}
    \toprule
      \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{\textbf{[0, 500)}} & \multicolumn{2}{c|}{\textbf{[500, 2k)}} & \multicolumn{2}{c|}{\textbf{[2k, 4k)}} & \multicolumn{2}{c|}{\textbf{[4k, 20k)}} & \multicolumn{2}{c}{\textbf{Average}} \\
     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
    & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ \\
    \midrule
    \textbf{LongWriter-Llama} & 88.10 & 86.00 & 75.40 & 86.90 & 89.10 & 88.30 & 80.80 & 79.20 & 83.12 & 85.30 \\
    \qquad\emph{\textbf{w/o critique}} & 89.69 & 87.00 & 75.46 & 89.58 & 92.72 & 89.01 & 83.93 & 79.51 & 85.45 & 86.27 \\
    \qquad\emph{\textbf{w/ self-critique}} & 92.51 & 88.15 & 74.40 & 89.81 & 90.15 & 88.48 & 83.62 & 81.38 & 85.17 & 86.96 \\
    \qquad\emph{\textbf{w/ LongDPO}} & 90.74 & 89.14 & 76.61 & 90.70 & 93.46 & 91.10 & 87.77 & 81.94 & \textbf{87.14} & \textbf{88.22} \\
   \cmidrule(lr){1-1}  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
    \textbf{LongWriter-Qwen} & 90.80 & 87.99 & 84.37 & 89.37 & 84.21 & 84.84 & 58.69 & 78.13 & 79.51 & 85.08 \\
   \qquad\emph{\textbf{w/o critique}} & 89.59 & 86.99 & 85.35 & 89.01 & 88.14 & 84.31 & 63.98 & 80.20 & 81.77 & 85.12 \\
    \qquad\emph{\textbf{w/ self-critique}} & 90.67 & 90.68 & 83.60 & 93.26 & 87.46 & 86.61 & 65.20 & 78.24 & 81.73 & 87.20 \\
    \qquad\emph{\textbf{w/ LongDPO}} & 89.36 & 91.18 & 85.48 & 92.10 & 89.60 & 87.16 & 67.66 & 83.17 & \textbf{83.03} & \textbf{88.40} \\    
    \bottomrule
    \end{tabular}
    \label{self-refine}
\end{table*}

\subsection{Generalization on more long-form and general benchmarks}
% 讲一下各个benchmark，分析对比dpo优点，分析不同backbone的结果
Table~\ref{tab:long_general} displays the results of various methods on LongGenBench. For both the Llama and Qwen series models, their performance on LongGenBench shows significant improvement. Notably, in terms of CR, this suggests that the model can better follow instructions after being trained with LongDPO. Additionally, using LongDPO results in better performance than DPO.

For other tasks, a similar trend can be observed: directly applying DPO fails to deliver significant performance improvements and, in some cases, even leads to notable declines. This is particularly evident in the MMLU task, where the performance of LongWriter-Qwen significantly deteriorates after applying DPO. In contrast, our method results in virtually no degradation of the model's other capabilities and even leads to slight improvements. This illustrates the generalizability of our approach to tasks beyond long-form generation.
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.99\linewidth]{Figs/case_study.pdf}
  \caption{
  A case is randomly sampled from LongGenBench. The instruction primarily requires visiting the farmers' market starting from week 10 and then every 5 weeks thereafter. On the left, LongWriter-Llama fulfills the requirement in week 10 but fails in week 15. On the right, after applying LongDPO, LongWriter-Llama is able to consistently meet the demands.
  }
  \label{case_study}
\end{figure*}


\subsection{Comparision with Different Critic Methods}
% 先说refine的必要性，之后说self-refine对比，最后分析结果
% 可以补充为什么一定要做refine
% Although using MCTS to search in a larger space increases the likelihood of finding high-quality solutions, as shown in Table~\ref{self-refine}, the gains from MCTS are limited, especially in terms of the quality score. A detailed observation of Figure~\ref{reward_distribution} reveals that there is a significant gap in the candidates' rewards, with some low-quality nodes still present. Additionally, the large fluctuations in rewards between training pairs can make the training unstable \citep{wu2024betadpodirectpreferenceoptimization}. Thus, improving the quality of the chosen nodes is essential.

Self-critique is widely used~\citep{ankner2024critiqueoutloudrewardmodels, ye2024improvingrewardmodelssynthetic} to leverage models' internal knowledge to provide feedback to provide a better solution. 
However, recent studies have emphasized that relying solely on a model's internal knowledge can result in unstable performance gains~\citep{qi2024mutualreasoningmakessmaller, zhang2024understandingdarkllmsintrinsic}. 
To further verify whether self-generated critiques can effectively collect better preference pairs, we compare self-generated critiques with external critiques in Table~\ref{self-refine}. We have ensured that the only difference lies in the critic model used between self-critique and LongDPO.

To enable a more thorough comparison, we set multiple values for \(\eta\) in Equation~\ref{thresh_hold}. Specifically, we set \(\eta\) to \{2.0, 2.5, 3.0\} and report the average performance in Table~\ref{self-refine}. We detailed the results in Table~\ref{sigma} and~\ref{sigma_qwen}. Self-critique exhibits performance fluctuations which may be because the generator's internal knowledge is insufficient, making it difficult to distinguish high-quality steps in long-form generation.

\section{Analysis}
\subsection{Reliability of Evaluation}
\textbf{Reliability on Quality Score} 
We evaluate the consistency of GPT-4o in LongBench-Write based on three evaluation runs and report the variance following \citep{longwriter_openreview}. Table \ref{tab:models_sq} presents the results of the average quality score, which may indicate that GPT-4o demonstrates good consistency.
\begin{table}[ht]
\centering
\caption{Human evaluation with win rates under three criteria: Diversity, Consistency, and Informativeness}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Rate} & \textbf{Diversity} & \textbf{Consistency} & \textbf{Informative} \\
\midrule
Win & 65.0 & 61.7 & 61.7 \\
Tie & 8.30 & 16.7 & 6.70 \\
Lose & 26.7 & 21.6 & 31.6 \\
\bottomrule
\end{tabular}
}
\label{tab:human_result}
\end{table}

\textbf{Human Evaluation} 
In addition to utilizing GPT-4o, we conduct a human evaluation to assess the generated text in terms of diversity, consistency, and informative detailed guidelines can be seen in~\ref{human_annotation}.
We compare the responses generated by LongWriter-Llama and LongWriter-Qwen with those produced by the same models trained using LongDPO.
Three independent annotators, who are undergraduate and graduate students, are tasked with comparing the response pairs and evaluating them as win, tie, or lose. 
The results, present in Table~\ref{tab:human_result}, indicate that our responses are rated as superior by the human judges. 
Additionally, Table~\ref{tab:human_agreement} shows the agreement among the three judges, demonstrating a high level of consistency in their evaluations.

\subsection{Case Study} 
Figure~\ref{case_study} presents a case sampled from LongGenBench. The instruction primarily requires visiting the farmers' market starting from week 10 and then every 5 weeks thereafter. LongWriter-Llama fulfills the requirement in week 10 but fails in week 15. However, after applying LongDPO, it is able to consistently meet the demands.

We analyze the attention distribution across models and observe that, in week 15, LongWriter-Llama fails to attend to ``farmers market." However, after applying LongDPO, it successfully does so. We find that a small number of attention heads have attended to ``farmers market," with over 1\% of attention heads scoring above 0.5. However, the LongWriter model does not exhibit a similar pattern. This behavior may be linked to retrieval heads~\citep{wu2024retrievalheadmechanisticallyexplains}. We also provide examples in Figure~\ref{fact1} and~\ref{fact2} to show factual correctness after applying LongDPO. 

\begin{table}[ht]
\centering
\caption{Human agreement between different annotators. Judge-1, Judge-2, and Judge-3 are three human judges.}
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
 \textbf{Judge} & \textbf{Judge-1} & \textbf{Judge-2} & \textbf{Judge-3} \\
\midrule
Judge-1 & - & 61.7 & 63.4 \\
Judge-2 & 61.7 & - & 61.7 \\
Judge-3 & 65.0 & 58.4 & - \\
\bottomrule
\end{tabular}
}
\label{tab:human_agreement}
\end{table}
\section{Conclusion}
In this paper, we propose LongDPO to enhance long-form generation abilities for LLMs. First, we construct stepwise preference pairs by leveraging process supervision with MCTS, incorporating a global memory pool to maintain factual consistency, and utilizing external critiques to refine the chosen candidates which are of low-rewards in the collected preference pairs. Then, LongDPO utilizes step-level preference data for DPO training. This approach enhances performance in long-form generation tasks while maintaining near-lossless performance on several general tasks. Furthermore, we conduct an in-depth analysis to show models trained by LongDPO can generate responses that align more closely with humans.


% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}
This work proposes methods for generating high-quality long-form text, which may help make human content creation easier. We have introduced approaches aimed at improving the factual accuracy of the generated content. We believe that long-form generation is a promising direction worth exploring which may have an impact on legal, code content generation. 


% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Reward Evaluation Templates}
\begin{tcolorbox}
[colframe=blue!75!black,colback=blue!5!white,title=Reward Evaluation Template]
% [
%     colback=white, % background color
%     colframe=gray, % frame color
%     % coltitle=black, % title color
%     title=\textbf{Template for Evaluating Text Reward}, % box title
%     fonttitle=\bfseries\large, % title font
%     arc=4mm, % corner radius
% ]
You are an expert at evaluating the quality of text.

As an impartial evaluator, please assess the assistant’s response to a user’s requirements. Now, you will receive specific principles that provide the criteria for evaluating the response. Principles begin,

\textbf{Principle1}: The response is accurate and free of factual errors.

\textbf{Principle2}: The response meets the user’s purpose and needs.

\textbf{Principle3}: The response is non-toxic and safe.

\textbf{Principle4}: The response meets the user’s formatting requirements and maintains logical consistency.

\textbf{Principle5}: The response contains diverse and comprehensive information with minimal repetition.

\textbf{Principle6}: The response provides an excellent reading experience.

\textbf{Principle7}: The response is insightful and provides the user with additional avenues for thought. Principles end.

In the next, you will receive detailed guidelines to help you rate the response according to each principle. Now, guidelines begin

\textbf{5}: A perfect response with no improvement needed. The content is comprehensive, accurate, clear, and well-structured. The response fully addresses all aspects of the question or need without any omissions or errors.

\textbf{4}: A very good response with minor issues. It is almost perfect but may have slight areas that could be improved, such as minor details that are unclear or a small omission. Overall, it still meets the need effectively.

\textbf{3}: An acceptable response that generally meets the question or need but has noticeable shortcomings. The content might be incomplete or unclear, or there may be minor grammar or logical errors. It needs improvement but is still functional.

\textbf{2}: A response with significant issues that requires substantial improvement. The content is incomplete, unclear, or contains major errors, omissions, or misunderstandings. It does not fully satisfy the request.

\textbf{1}: A completely inadequate response that fails to meet the question or need. It contains serious errors or misunderstandings and cannot provide useful help. 

Guidelines end. 

Now, you will receive the user request and the assistant's response to evaluate. 

\textbf{\textless User Request\textgreater}

\$INST\$

\textbf{\textless/User Request\textgreater}

\textbf{\textless Response\textgreater}

\$RESPONSE\$

\textbf{\textless/Response\textgreater}

Your task is to evaluate the quality of the response and assign a rating with distinguishable differentiation for each principle. When rating, please carefully read the guidelines and ensure your ratings fully adhere to them. You must first provide a brief analysis of its quality, then determine the weights for each \textbf{Principle}, for example \{"Principle1": [0.2,0.2,0.2,0.2,0.2]\} represents the final score is 0.2 * 1 + 0.2 * 2 + 0.2 * 3 + 0.2 * 4 + 0.2 * 5 = 3. The output must strictly follow the JSON format: \texttt{\{"Analysis":..., "Principle1":[..,..,..,..,..], "Principle2":[..,..,..,..,..], "Principle3":[..,..,..,..,..], "Principle4":[..,..,..,..,..], "Principle5":[..,..,..,..,..], "Principle6":[..,..,..,..,..], "Principle7":[..,..,..,..,..]\}}. You do not need to consider whether the response meets the user's length requirements in your evaluation. Ensure that only one integer or float is output for each principle.
\label{text_reward}
\end{tcolorbox}

\subsection{Templates for Generate Critiques}
\begin{tcolorbox}[colframe=blue!75!black, colback=blue!5!white, coltitle=white, title=Templates for Generate Critiques]
You are an expert at evaluating the quality of text. In the following, you will revice a user request, one principle and two candidates:

\textbf{\textless User Request\textgreater}

\$INST\$

\textbf{\textless/User Request\textgreater}

\textbf{\textless Principle\textgreater}

\$PRINCIPLE\$

\textbf{\textless/Principle\textgreater}

\textbf{\textless Candidate1\textgreater}

\$CANDIDATE1\$

\textbf{\textless/Candidate1\textgreater}

\textbf{\textless Candidate2\textgreater}

\$CANDIDATE2\$

\textbf{\textless/Candidate2\textgreater}

Now, your task is 
1. Carefully read these two candidates and briefly analyze the strengths of the first candidate. 
2. Provide a "Justification" explaining why it scores higher. 
3. Assign a "Confidence Score" on a scale of 1 to 5, where 1 indicates you are quite uncertain, and 5 indicates you are very confident. 
4. Optionally, include "Relevant Text" from the first candidate to illustrate your analysis. 
5. Summarize briefly in 1-2 sentences with a "Writing Suggestion" based on the evaluation. The output must strictly follow the JSON format: \texttt{\{"Analysis":..., "Justification":..., "Writing Suggestion":..., "Confidence Score":...,"Relevant Text":...\}}. Ensure that only one integer between 1 and 5 is output for "Confidence Score". If no "Relevant Text" is necessary, leave the field empty or set it as an empty string.
% Insert any relevant text or leave empty.
\label{refine_template}
\end{tcolorbox}

\subsection{Templates for Check Consistency}
\begin{tcolorbox}[colframe=blue!75!black, colback=blue!5!white, coltitle=white, title=Template for Finding Fact]
You're an expert in natural language processing and information retrieval. You will receive a response. Your task is to extract factual statements from the response provided. 

Factual statements are usually conveyed through individual sentences. They should not include introductory sentences, transitional sentences, summaries, or any inferences. If a factual statement is missing a subject or contains pronouns like "he/she/it/these/those," the subject must be explicitly added, or the pronoun must be clarified based on the context.

Now, please process the following AI assistant’s response:

\textbf{\textless Response\textgreater}

\$RESPONSE\$

\textbf{\textless/Response\textgreater}

Please carefully read and analyze the given content. Then, breaking the factual content. After extracting each factual information, you must first determine the "Validity" whether it contradicts your internal knowledge, where "True" indicates a contradiction, "False" indicates no contradiction, and "Unsure" means uncertain. Provide the relevant "Evidence" accordingly. Then, output the result in the following format: \texttt{\{"Analysis":..., "Fact1":\{"Content":...,"Validity":...,"Evidence":...\}, "Fact2":\{"Content":...,"Validity":...,"Evidence":...\},...\}}. Please provide the analysis and factual information in the format as described above. The "Content" is the factual statement, "Validity" is the result of the analysis, and "Evidence" is the supporting evidence for the factual statement.
\label{find_fact}
\end{tcolorbox}

\begin{tcolorbox}[colframe=blue!75!black, colback=blue!5!white, coltitle=white, title=Template for Judge Inconsistency]
You are an expert at evaluating text. You will receive factual statements along with a related response. Your task is to carefully evaluate whether the response contradicts the factual statement. Please use the following principles to generate your assessment:

\textbf{Contradict}: You can find strong evidence indicating factual inaccuracies in the response that are inconsistent with the given factual statement.

\textbf{Not Contradict}: You are unable to find evidence indicating factual inaccuracies in the provided response that contradicts the given factual statement.
Ensure that you do not use any information or knowledge beyond the response provided, and only check whether the statement is supported by the response.

Now, please refer to the principles to give your judgement:

\textbf{\textless Statement\textgreater}

\$STATEMENT\$

\textbf{\textless/Statement\textgreater}

\textbf{\textless Response\textgreater}

\$RESPONSE\$

\textbf{\textless/Response\textgreater}

You must provide an analysis first, followed by the judgement. The output must strictly follow the JSON format: \texttt{\{"Analysis":..., "Judgement":...,"Evidence":...\}.}

\label{judge_fact}
\end{tcolorbox}


\subsection{Guidelines for Human Annotation}

\begin{tcolorbox}[colframe=blue!75!black, colback=blue!5!white, coltitle=white, title=Guidelines for Human Annotation]
    \textbf{1. Diversity:} Which text is more diverse in content? This can be evaluated holistically, considering factors such as the lexical variety, the richness of semantics, the complexity of writing style, and the diversity in article structure. \vspace{1em}

    \textbf{2. Consistency:} Which text demonstrates a higher degree of consistency? This can be assessed holistically, considering factors such as thematic coherence, ensuring the central theme remains clear; logical coherence, reflected in the natural flow of ideas; and factual consistency, verified through accurate and reliable information. \vspace{1em}

    \textbf{3. Informative:} Which text is more informative in content? This can be evaluated holistically, considering factors such as the accuracy of the information presented, the comprehensiveness in covering all relevant aspects, the clarity of explanations, and the ease of readability and understanding.
    \label{human_annotation}
\end{tcolorbox}


\subsection{More Evaluation Results}

\begin{table}[htbp]
    \centering
    \caption{Detailed quality score for length exceeding 4000 in LongBench-Write-en.}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{l|ccccccc}
    \toprule
    & $S_q$ & Relevance & Accuracy & Coherence & Clarity & Breadth and Depth & Reading Experience \\
    \midrule
    LongWriter-Llama & 79.20 & 90.90 & 87.50 & 84.48 & 81.89 & 59.48 & 71.55 \\
    \quad\emph{+DPO} & 80.90 & 93.75 & 83.33 & 77.08 & 77.08 & 83.33 & 70.83 \\
    \quad\emph{+LongDPO} & 85.06 & 93.75 & 85.42 & 85.42 & 81.25 & 87.50 & 77.08 \\
    \midrule
    LongWriter-Qwen & 78.13 & 83.33 & 81.25 & 83.33 & 77.08 & 68.75 & 75.00 \\
    \quad\emph{+DPO} & 78.81 & 85.41 & 81.25 & 83.33 & 81.25 & 85.41 & 70.83 \\
    \quad\emph{+LongDPO} & 85.41 & 91.67 & 91.67 & 83.33 & 83.33 & 83.33 & 79.16 \\
    \bottomrule
    \end{tabular}
    }
    \label{tb:quality_detail}
\end{table}


\begin{table*}[ht]
    \centering
    \caption{Results on changing \(\eta\) using llama-based backbones}
    \begin{tabular}{l|cc|cc|cc|cc|cc}
    \toprule
      \multirow{2}{*}{LongWriter-Llama} & \multicolumn{2}{c|}{\textbf{[0, 500)}} & \multicolumn{2}{c|}{\textbf{[500, 2k)}} & \multicolumn{2}{c|}{\textbf{[2k, 4k)}} & \multicolumn{2}{c|
      }{\textbf{[4k, 20k)}} & \multicolumn{2}{c
      }{\textbf{Average}} \\
     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
    & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$  \\
    \midrule
    Self-critique \emph{+$ \eta \le 2.0$}  & 94.07 & 88.97 & 72.39 & 87.99 & 86.86 & 89.39 & 82.72 & 80.55 & 84.01 & 86.72 \\
     \qquad \emph{+$\eta \le 2.5$} & 93.08 & 88.48 & 76.43 & 91.04 & 91.66 & 88.54 & 84.63 & 82.35 & \textbf{86.45} & \textbf{87.60} \\
     \qquad \emph{+$\eta \le 3.0$} & 90.38 & 87.01 & 74.37 & 90.41 & 91.94 & 87.50 & 83.50 & 81.25 & 85.04 & 86.54 \\
   \cmidrule(lr){1-1}  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
    LongDPO \emph{+$\eta \le 2.0$} & 92.01 & 92.91 & 72.55 & 91.45 & 93.35 & 93.75 & 88.86 & 80.20 & 86.69 & \textbf{89.57} \\
    \qquad \emph{+$\eta \le 2.5$} & 90.68 & 86.27 & 77.23 & 91.25 & 93.35 & 90.53 & 88.25 & 85.06 & \textbf{87.38} & 88.19 \\
    \qquad \emph{+$\eta \le 3.0$}  & 89.51 & 88.23 & 80.04 & 89.39 & 93.68 & 89.01 & 86.19 & 80.55 & 86.47 & 86.80 \\
    \bottomrule
    \end{tabular}
    \label{sigma}
\end{table*}

\begin{table*}[ht]
    \centering
    \caption{Results on changing $\eta$ using Qwen-based backbones}
    \begin{tabular}{l|cc|cc|cc|cc|cc}
    \toprule
      \multirow{2}{*}{LongWriter-Qwen} & \multicolumn{2}{c|}{\textbf{[0, 500)}} & \multicolumn{2}{c|}{\textbf{[500, 2k)}} & \multicolumn{2}{c|}{\textbf{[2k, 4k)}} & \multicolumn{2}{c|}{\textbf{[4k, 20k)}} & \multicolumn{2}{c}{\textbf{Average}} \\
     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} 
    & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ \\
    \midrule
    Self-critique \emph{+$\eta \le 2.0$} & 88.71 & 88.23 & 84.45 & 93.54 & 86.37 & 84.46 & 64.88 & 78.47 & 81.10 & 86.17 \\
   \qquad \emph{+$\eta \le 2.5$} & 91.96 & 91.66 & 83.16 & 92.91 & 88.94 & 86.36 & 67.69 & 79.16 & \textbf{82.93} & 87.52 \\
   \qquad \emph{+$\eta \le 3.0$} & 91.33 & 92.15 & 83.20 & 93.33 & 87.06 & 89.01 & 63.04 & 77.08 & 81.16 & \textbf{87.89} \\
   \cmidrule(lr){1-1}  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
    LongDPO \emph{+$\eta \le 2.0$} & 87.84 & 91.45 & 86.21 & 92.15 & 91.35 & 86.86 & 66.85 & 82.59 & 83.06 & 88.26 \\
    \qquad \emph{+$\eta \le 2.5$} & 88.93 & 91.91 & 85.47 & 91.25 & 88.63 & 85.60 & 71.14 & 85.41 & \textbf{83.54} & \textbf{88.54} \\
    \qquad \emph{+$\eta \le 3.0$} & 91.32 & 90.19 & 84.75 & 92.91 & 88.82 & 89.01 & 64.99 & 81.51 & 82.47 & 88.51 \\
    \bottomrule
    \end{tabular}
    \label{sigma_qwen}
\end{table*}


\begin{table}[ht]
\centering
\caption{Evaluated Models and the average $S_q$ Scores. We evaluate LongWriter-Llama + LongDPO and LongWriter-Qwen + LongDPO, while \citet{longwriter_openreview} report the remaining results.}
\begin{tabular}{lcc}
    \toprule
    \textbf{Evaluated Models} & \boldmath$S_q$ \\
    \midrule
    Claude 3.5 Sonnet & $87.7 \pm 0.5$ \\
    GPT-4 Turbo       & $86.6 \pm 0.4$ \\
    GPT-4o mini       & $90.3 \pm 0.3$ \\
    GPT-4o            & $91.8 \pm 0.5$ \\
    GLM-4-9B-chat     & $85.5 \pm 0.4$ \\
    Llama-3.1-8B-Instruct & $70.6 \pm 0.3$ \\
    Llama-3.1-70B-Instruct & $80.3 \pm 0.3$ \\
    Mistral-Large-Instruct & $88.3 \pm 0.4$ \\
    Suri-I-ORPO       & $53.5 \pm 0.5$ \\
    LongWriter-Llama     & $82.2 \pm 0.4$ \\
    LongWriter-Llama + LongDPO   & $88.2 \pm 0.5$ \\
    LongWriter-Qwen + LongDPO   & $88.6 \pm 0.5$ \\
    \bottomrule
    \end{tabular}
\label{tab:models_sq}
\end{table}


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.50\linewidth]{Figs/reward_distribution_percentage_full_new.pdf}
  \caption{
  Detailed reward analysis of the chosen candidates.
  }
  \label{reward_distribution_full}
\end{figure}


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.50\linewidth]{Figs/factual_1.pdf}
  \caption{
  The part highlighted in red is the correct answer to the question. LongWriter-Llama fails to provide the correct answer, but after applying LongDPO, it is able to answer correctly.
  }
  \label{fact1}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.50\linewidth]{Figs/factual_2.pdf}
  \caption{
  The part highlighted in red is the correct answer to the question. LongWriter-Llama fails to provide the correct answer, but after applying LongDPO, it is able to answer correctly.
  }
  \label{fact2}
\end{figure}

% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=0.99\linewidth]{Figs/training_data.pdf}
%   \caption{
% The left side shows the training data collected using MCTS, which believes "dehydration" is a good slimming method. The right side provides a more reasonable response, indicating that "dehydration" is unhealthy. Using memory can help collect better training data.
%   }
%   \label{training_data}
% \end{figure}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
