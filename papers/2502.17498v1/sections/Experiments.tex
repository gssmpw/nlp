\section{Experiments}
\subsection{Experimental Settings}
\paragraph{Datasets and metrics.} Following previous research \citep{DBLP:conf/acl/WangLSXDLCWS24,DBLP:conf/iclr/LightmanKBEBLLS24}, we evaluate value-based process verifiers based on their verification ability through best-of-$N$ sampling. Specifically, given a problem $p$, we sample $N$ candidate solutions from a generator. Then, the candidates are re-ranked according to the score generated by the given verifier. The candidate with the highest score will be treated as the best candidate and selected as the final solution. Finally, we compare the consistency of the final solution and ground-truth answer to determine whether the solution is correct. The statistical success rate will be reported. Following \citet{DBLP:journals/corr/abs-2408-03314}, we also include the beam-search metric to evaluate the performance of value-based process verifiers at test time. Specifically, the number of beams $N$ and beam size $M$ will be set. Given a problem $p$, the verifier is required to score every step during the generation process. For each iteration, $N$ beams will generate $M$ next-step candidates individually, the verifier will select the top $N$ candidates from the $N*M$ beams as the beams for the next iteration, and then continue the iteration until finished. Like Best-of-N, we compare the consistency of the final solution and ground-truth answer. The statistical success rate will be reported.

We conduct our experiments on the challenging MATH dataset\citet{DBLP:conf/nips/HendrycksBKABTS21} for verification. We use the test split following \citet{DBLP:conf/iclr/LightmanKBEBLLS24} as our test set, which consists of 500 randomly selected questions from MATH. As described by \citet{DBLP:conf/acl/WangLSXDLCWS24}, the subset evaluation produces similar results to the full-set evaluation. We uses MetaMATH\citep{DBLP:conf/iclr/YuJSYLZKLWL24} as the fine-tuning dataset as used in \citet{DBLP:conf/acl/WangLSXDLCWS24}.

\paragraph{Baselines and implementation details.}
The generator in our experiments is based on LLemma-7b. We train it on MetaMATH for 3 epochs to get the generator. Based on the train split of MATH dataset, we construct the training dataset of the process verifier. To clarify, We use the generator to sample 15 solutions per problem. Following previous works\citep{DBLP:conf/iclr/LightmanKBEBLLS24, DBLP:conf/acl/WangLSXDLCWS24}, we split each solution into steps by the pre-defined rule-based strategies (e.g. newline as the delimiters). For each step, we combine it with its previous steps to form an incomplete solution, then sample 8 rollouts to perform Monte Carlo estimation and annotate the state value. In general, we sample $15*8=120$ samples for each problem and the training dataset has 180k samples in total.

We use the Qwen2.5-Math-7B-Instruct\citep{yang2024qwen2} and deepseek-math-7b-instruct\citep{DBLP:journals/corr/abs-2402-03300} as the value-based process verifier base model. For the scalar-regression methods, similar to \citet{DBLP:conf/acl/WangLSXDLCWS24}, we train the verifier in the language modeling way, adding special tokens to the model's vocabulary and use the probability of generating the positive token as the predicted state value. For the expectation-based methods, we add a linear layer then use the softmax function to map the categorical output into a probability distribution. We compare the following methods:
\begin{itemize}
    \item Scalar Regression (outcome). Scalar-regression model that is trained on the final state value.
    \item Scalar Regression. Scalar-regression model that is trained on every intermediate state value, including the final state value.
    \item Expectation Regression (MSE). Expectation-based model that trained with mean-square error objective function on every intermediate state value, including the final state value.
    \item Expectation Classification (HL). Expectation-based model that trained with Histogram Loss objective function on a pre-defined distribution on every intermediate state value, including the final state value.
\end{itemize}

For the Expectation Classification method, we use the truncated Gaussian distribution where $\mu$ is the sampled state value, $\sigma^2$ is $\frac{\mu(1-\mu)}{k}$, as the Gaussian distribution can be used as an approximation of the binomial distribution, which can result in a more precise posterior distribution and thus reduce distribution mismatch. A further analysis of distribution selection can be found in section\ref{analysis_sampling}.

\subsection{Results}
\input{tables/result_main}
For the BoN experiments, we report the Best-of-N results and $N\in\{8,16,32,64,128\}$. For the beam search experiments, we report the result of both the number of beams and beam size are the same value, $M=N\in\{4,8\}$. The results are shown in Table \ref{tab:experiment-results}. Under the categorical distribution definition, the expectation-based models that directly optimized the categorical distribution by its expectation, or incorporating the pre-defined Gaussian distribution then directly optimized the shape of the categorical distribution using Histogram loss, perform consistently better than the scalar regression baseline, showing that the structural prior injection method can effectively improve the performance of process verifier trained via different optimization objective functions. The results also indicate that under reasonable structural prior, the expectation-based method can be an elegant replacement for the scalar-regression verifier at little-to-no cost.

We also see a superior performance of the outcome-supervised verifier compared with its process-supervised version on the Best-of-N task, when using Qwen2.5-Math-7B-Instruct as the base model of the verifier. One possible reason is that the value annotations are accurate for the final states. For the intermediate states, the labels contain error and noise due to Monte Carlo estimation and thus result in inferior performance. We provide more information about the Monte Carlo error analysis in Appendix \ref{app:noise}. However, though the outcome-supervised verifier has satisfied performance for the Best-of-N task, it doesn't perform well in Beam Search, which shows the importance of process supervision and the broad advantages of our method.