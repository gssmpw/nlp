\section{Related Work}
\subsection{Test-time Scaling for LLM Reasoning.}
The test-time scaling technique requires models to generate long Chain-of-Thought(CoT) explicitly or implicitly as its thinking steps or reasoning steps, which can effectively improve the reasoning capabilities of large language models. Some works provide relative reasoning demonstrations or thinking paradigms on the input, using few-shot prompting techniques like CoT prompting\citep{DBLP:conf/nips/Wei0SBIXCLZ22} or in-context learning\citep{DBLP:journals/corr/abs-2211-09066} to achieve test-time scaling. These methods can improve the reasoning capabilities of large language models to some extent, but the performance is highly sensitive to the quality and quantity of few-shot demonstrations\citep{qin-etal-2024-context}. Some researchers focus on incorporating searching algorithm into the LLM reasoning scenario by explicitly performing searching algorithm during the reasoning process\citep{DBLP:conf/nips/YaoYZS00N23,DBLP:conf/emnlp/LiuG0HZQZ23,DBLP:conf/iclr/0002WSLCNCZ23,DBLP:conf/emnlp/ZhaoXKHX23}. The methods are shown to be effective at the cost of large token consumptions, and the human-crafted searching algorithm can only fit specific tasks\citep{DBLP:journals/corr/abs-2409-12183}, which makes the methods restricted. Another line of implementation of the test-time scaling technique is reinforcement learning. By providing suitable signals or feedback, the LLMs can learn to reason from scratch\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} or the instruct-tuned checkpoint\cite{DBLP:conf/nips/ZelikmanWMG22}.

\subsection{Process-supervised verifier.}
Researchers have found that the process-supervised verifiers that are trained on fine-grained signals are effective for LLM reasoning and reinforcement learning, compared with outcome-supervised verifiers\cite{DBLP:conf/iclr/LightmanKBEBLLS24}. However, The definition of fine-grained signals is vague. \citet{DBLP:conf/iclr/LightmanKBEBLLS24} define the fine-grained signal as the stepwise calculate correctness and the signal is -1 if the reasoning step is incorrect, 1 if the reasoning step is correct, 0 if the reasoning step is neural. \citet{DBLP:conf/acl/WangLSXDLCWS24} define the fine-grained signal as the binary signal. Specifically, they map the Monte Carlo return to binary labels. If all outcomes are wrong, the signal of the current step is labeled to be 0. If any outcome is correct, the current step's signal is labeled 1. Some other works define the signal in a rule-based version, consisting of calculating error, formatting error, or else\citep{DBLP:conf/icml/XiCHJZHDLGWGSFZ24, DBLP:journals/corr/abs-2412-06559}. In the LLM reasoning scenario, as only the final outcome has specific and accurate labels, inductive bias is inevitably introduced when defining the fine-grained signal, which may result in reward hacking due to the definition flaw\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. Following previous works\citep{DBLP:journals/corr/abs-2406-06592,DBLP:journals/corr/abs-2408-03314,lu2024autopsv}, we define the fine-grained signal as the state value of the current step, which is calculated using the labels of final outcome only to reduce the potential human-crafted inductive bias.

\subsection{Regression as Classification.}
Several works have replaced regression with classification to improve performance in different domains\citep{DBLP:journals/jair/WeissI95,DBLP:conf/iclr/ZhangYMZY23}. Notably, \citet{DBLP:conf/icml/ImaniW18} proposed the HL-Gauss cross-entropy loss as the drop-in replacement of MSE loss, showing that by incorporating prior distribution information, the classification loss can be effective in regression tasks. The authors then performed experiments in several different distributions and found that the HL-Gaussian distribution can greatly improve the performance, while other distribution-learning approaches can result in even worse performance. \citet{DBLP:conf/icml/FarebrotherOVTC24} further extended the classification optimization method to several more complex domains, showing the general benefits of performing cross-entropy on the categorical distribution for regression tasks. However, how to perform better categorical distribution modeling is still unclear. In the LLM reasoning scenario, we endow the categorical distribution with fixed bins the specific definition, explaining the difference between regression and classification from the perspective of prior structural information injection. We show that the regression loss can also benefit from suitable categorical distribution modeling, indicating that the categorical distribution modeling can achieve the general benefits compared with scalar regression methods.

\section{More Implementation Details}
We provide the hyper-parameters when training the generator and value-based process verifiers in Table \ref{tab:hyperparams}.
\begin{table}[h!]
    \centering
    \begin{tabular}{c|c}
    \toprule
      Parameter   &  Value \\
    \midrule
      Epochs & 3(generator)/1(verifier) \\
      Learning Rate   &  2e-6 \\
      Batch size(per device) & 4(generator)/2(verifier) \\
      Gradient Accumulation Steps & 8 \\
      Max Sequence Length & 1024 \\
      Float Point Precision & torch.bfloat16 \\
      GPUs & 4 \\
    \bottomrule
    \end{tabular}
    \caption{The hyper-parameter when performing all the experiments.}
    \label{tab:hyperparams}
\end{table}

\section{Reasoning process transit from non-deterministic to deterministic.}\label{app:noise}
\input{figs/entropy}
Finally, we further analyze the error and noise when estimating state value via Monte Carlo Sampling. First of all, we argue that the reasoning process transits from non-deterministic to deterministic. We measure the process as deterministic or non-deterministic by the entropy of the estimated state values as follows:
\begin{equation}
    E(k)=-\frac{1}{N}\sum_i^N H(\hat{V}^\pi(s_i^k))+H(1-\hat{V}^\pi(s_i^k)), \nonumber
\end{equation}
and
\begin{equation}
    H(\hat{V}^\pi(s_i^k))=\hat{V}^\pi(s_i^k)\log \hat{V}^\pi(s_i^k), \nonumber
\end{equation}
where $s_i^k$ is the $k$-th state of the $i$-th data. The entropy will be close to 0 when the estimated value at the $k$-th step is close to marginal values(i.e. 0 or 1), and will achieve its maximum when the estimated value at the $k$-th step is close to the middle(i.e. 0.5). We estimate the entropy by our verifier's training dataset, created by the fine-tuned LLemma-7b. The average steps of our training dataset is 7.8, so we tallied the results of the first 4 steps and the last 3 steps, labeled as \{1, 2, 3, 4, -3, -2, -1\}. The results are shown in Figure\ref{fig:entropy}. As shown in the figure, the entropy of the estimated state value keeps decreasing as the reasoning process goes, which means that the estimated state value as well as the ground-truth probability $p$ is getting close to marginal, assuming that the estimated state value can be the proxy to the ground-truth probability $p$. By estimating the ground-truth categorical distribution with Gaussian distribution $X\sim N(p,\frac{p(1-p)}{k})$ and let $k=8$, the standard deviation of $X$ when $p$ is 0.5 is about 0.177. As the bin width of the categorical distribution is $\frac{1}{k}=0.125$, the three-sigma region almost covers the whole categorical distribution, which shows that the one-time sampling from the categorical distribution to estimate the state value induces non-negligible errors, especially in early steps of the whole reasoning process.
