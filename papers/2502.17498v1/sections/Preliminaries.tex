\section{Preliminaries}
We first formally review the concepts and formulations of the Markov decision process(MDP) and the Bellman Equation in the Large Language Model(LLM) reasoning scenario. Then, we review the Monte Carlo estimation method, which is used to estimate the state action value. Finally, we introduce the value-based process verifier.

\subsection{Markov decision process and Bellman Equation}\label{sec:mdp}
The Markov decision process can be represented as a 4-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})$, where $\mathcal{S}=\{s\}$ is the state space, $\mathcal{A}=\{a\}$ is the action space, $\mathcal{P}=\mathcal{P}(s_t,a_t,s_{t+1})$ is the transition probability measuring the probability that action $a_t$ in state $s_t$ at time t will lead to state $s_{t+1}$ at time t+1. In the deterministic scenario, the transition probability will be restricted to 1, showing that given $s_t$ and $a_{t}$, the next state will be $s_{t+1}$. $\mathcal{R}=\mathcal{R}(s_t,a_t,s_{t+1})$ is the reward function, measuring the reward received after transitioning from state $s_t$ to $s_{t+1}$ due to action $a_t$. The policy $\pi$ is a mapping from the state space $\mathcal{S}$ to the action space $\mathcal{A}$. A completed MDP consists of multiple states $\{s_1,s_2,...,s_t\}$, where each state $s_i$ first samples the next action $a_i$ using the policy $\pi$, then samples the next state $s_{i+1}$ according to the transition probability distribution given the current state $s_i$ and action $a_i$.

The Bellman Equation defines the relationship between neighboring values in an MDP. Given the state value $V$ defined as 
\begin{equation}\label{pre:value_def}
    V^{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]
\end{equation}
where $\pi$ being the current policy and $s$ being the current state, and return $G_t$ defined as
\begin{equation}
    G_t=\sum_{k=0}^{\infty}\gamma \mathcal{R}_{t+k}
\end{equation}
where $\mathcal{R}_{t+k}$ is the abbreviation of $\mathcal{R}(s_{t+k},a_{t+k},s_{t+k+1})$ and $\gamma$ being the discount factor measuring the rate of reward decay, the expectation version of Bellman Equation is defined as
\begin{equation}\label{pre:bellman_raw}
\begin{aligned}
    V^{\pi}(s)&=\mathbb{E}_{\pi}[R_t|S_t=s]+\gamma\sum_{s'\in \mathcal{S}}P(s'|s)V^\pi(s') \\
    &=r(s)+\gamma\sum_{s'\in \mathcal{S}}P(s'|s)V^\pi(s')
\end{aligned}
\end{equation}
where $r(s)$ is defined as the expectation of immediate reward at state $s$.

In the LLM reasoning scenario, a completed reasoning path is split by a rule-based splitter like "$\backslash$n", forming a few non-overlapping reasoning steps. The action is defined as a single reasoning step, and the state is defined as the cumulative reasoning steps. Given current state $s_t$ and action $a_t$, the next state $s_{t+1}$ is uniquely determined, which shows that the MDP is deterministic. Another popular setting is to let the discount factor $\gamma$ equal one and the immediate reward equal zero, except for the last outcome reward being binary. Given a trajectory that has N steps, the MDP settings can be described as follows:
\begin{equation}\label{pre:reward_outcome}
    r(s_t)=\left\{
    \begin{aligned}
        0 & & {t<N} \\
        r & & {t=N}
    \end{aligned}
    \right.
\end{equation}
where $r$ is the outcome reward, $r=1$ if the outcome is correct, $r=0$ if the outcome is incorrect. Given the settings above, the Bellman Expectation Equation (i.e. Eqn.\ref{pre:bellman_raw}) can be simplified as follows:
\begin{equation}\label{pre:value_neigh}
    V^\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)V^\pi(s')
\end{equation}
where $s'=s\bigcup\{a\}$ is the next state of $s$ given current state $s$ and action $a$.

\subsection{Monte Carlo Estimation}
Monte Carlo estimation is the method that estimates state value function given policy $\pi$ using Monte Carlo sampling in MDP. Rewriting Eqn.\ref{pre:value_def}, we have:
\begin{equation}
    V^\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]\approx\frac{1}{N}\sum_{i=1}^N [G_t^{(i)}|S_t=s]
\end{equation}
where each $G_t^{(i)}$ is the return of a completed trajectory sampled from trajectories that $S_t=s$ using policy $\pi$. Monte Carlo estimation is an unbiased and effective estimation method. 

In the LLM reasoning scenario, the return $G_t$ can be simplified as:
\begin{equation}
    G_t=\sum_{k=0}^{\infty}\gamma \mathcal{R}_{t+k}=r
\end{equation}
where $r$ is defined after Eqn.\ref{pre:reward_outcome}. Then, the Monte Carlo estimation of state value $V$ can be simplified as:
\begin{equation}
    V^\pi(s)\approx\frac{1}{N}\sum_{i=1}^N [G_t^{(i)}|S_t=s]=\frac{1}{N}\sum_{i=1}^N [r^{(i)}|S_t=s]
\end{equation}
which shows that we can estimate the intermediate state value using the outcome reward.

\subsection{Value-based Process Verifier}
The value-based process verifier aims at estimating the state value during the reasoning process. Given the initial question $q$ and previous reasoning steps $\{a_1,a_2,...,a_t\}$, the value-based process verifier is asked to generate a scalar value that represents the state value of the current state $s_t=\{a_1,a_2,...,a_t\}$. Under the environment settings shown in \S\ref{sec:mdp}, given the ground truth answer, it's able to perform an automatic annotation approach to obtain process state value signals using the Monte Carlo method, and the state value is interpreted as the success rate of the outcome starting from the current state. We can thus formulate the value-base process verifier as a mapping from binary function to probabilities using parameter $\theta$:
\begin{equation}
    f_\theta(q, s_t)\rightarrow[0,1]
\end{equation}
