\section{Analysis and Discussion}
In this section, we will discuss the influence of the Statistics-based Distance metric and the structural prior. Though related to each other, we discuss the distance metric and structural prior in different sections for the sake of clarity. The former is based on the cross-entropy loss that can be directly supervised by specific types of categorical distribution definitions. The latter is based on the mean-square error objective function where the training objective is only supervised on the expectation of the categorical distribution, which allows us to define the distribution more freely.
\subsection{Posterior Distribution Selection.}\label{analysis_sampling}
\input{figs/distributions}
In this section, we analyze the difference in distribution selection and their corresponding performance. To clarify, we use the following distributions:
\begin{itemize}
    \item One-hot distribution. A one-hot distribution that the category which sampled state value points to is set to be 1, other categories are set to 0.
    \item Guassian distribution(dynamic). Truncated Gaussian distribution with dynamic variance. $\mu$ is the sampled state value, $\sigma^2$ is $\frac{\mu(1-\mu)}{k}$.
    \item Gaussian distribution(static). Truncated Gaussian distribution with fixed variance as described in \citet{DBLP:conf/icml/FarebrotherOVTC24}. We set three standard deviations to be two bin widths in our experiments.
\end{itemize}
We report the statistics-based distance for different posterior distributions in Figure \ref{fig:distance}. Specifically, we calculate the statistics-based distance varying on different ground-truth probability $p$ from 0 to 1. We use the Wasserstein distance to measure the distance between categorical distributions. The distance between the $i$-th category and $j$-th category is $\frac{|i-j|}{k}$ following the definition of our categorical distribution. As shown in Figure \ref{fig:distance}, compared with Gaussian distributions, one-hot distribution has a much greater distance varying on the ground-truth probability $p$. For the Gaussian(dynamic) distribution, the statistics-based distance is smaller than Gaussian(static) distribution in most cases varying on the ground-truth probability $p$.

We report the performance of value-based process verifiers incorporating each posterior distribution based on Deepseek-math-7b-instruct in Figure \ref{fig:bon}. Compared with the baseline scalar regression method, incorporating different distributions can effectively improve the performance. As shown in the figure, the improvement is relatively more impressive when incorporating Gaussian distributions, especially the Gaussian(dynamic) distribution compared with the one-hot distribution. The results show that the statistics-based distance can be a good metric for designing the posterior distribution and avoiding distribution mismatch. 

Finally, we report the global estimated value distribution in Figure \ref{fig:distribution}, as a proxy to the ground-truth probability distribution which is actually intractable. As shown in the figure, the estimated value distribution is not an uniform distribution but clustered at marginal values, which may indicate that the ground-truth probability distribution is not flat but relatively extreme. Combining Figure \ref{fig:distance} and Figure \ref{fig:distribution}, the reason why the performances are slightly different between verifiers incorporating different distributions may be because of their similar statistics-based distance for $p$ closed to marginal values.

\subsection{Prior Categorical Distribution Selection.}
\input{figs/mse_comparison}
In this section, we analyze the difference in the categorical definition and their corresponding performance, thus discussing the influence of prior structural information to be injected into the verifier. To clarify, we compare the Dirac delta function definitions and the category quantities to construct the categorical distribution, then optimize the distribution under the mean-square error objective function. Following the definition of $\delta_{z_i}$ in sec \ref{sec:regression}, let $\delta_{z_i}$ be the Dirac delta function at location $z_i$, where $i\in[1,2,...,k+1]$.  We compare the following Dirac delta functions:
\begin{itemize}
    \item Equidistant. $\delta_{z_i}=\frac{i-1}{k}$.
    \item Cosine. $\delta_{z_i}=1-cos(\frac{i-1}{k}\pi)$.
    \item Symmetrical Equidistant. $\delta_{z_i}=-1+\frac{2(i-1)}{k}$
\end{itemize}
We fixed the category quantity to be 9, then the categorical distribution using the equidistant Dirac delta function can be treated as formulating a Binomial distribution with a sample size of $8$. The other Dirac delta functions can be treated as the varieties of the equidistant Dirac delta function, though we preserve the monotonicity, upper and lower bound of the equidistant Dirac delta function. Apart from the different Dirac delta function definitions, we also explore the difference in category quantities. We test the category quantity to be {2,4,9,16} and for all categorical distributions that varying on category quantities, we use the equidistant Dirac delta function. For both experiments, we report the Best-of-N result in Figure \ref{fig:mse}.

Our results show that the structural prior can effectively influence the performance of value-based process verifiers. Though preserving similar features of the equidistant Dirac delta function, the other Dirac delta functions are more difficult to provide meaningful prior information and thus the optimization process will be more difficult. Similar results can be found in the experiment of varieties of category quantity. It shows that setting the category quantity to match the Binomial distribution is useful for better performance.

