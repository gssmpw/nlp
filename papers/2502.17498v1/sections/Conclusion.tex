\section{Conclusion}
In this paper, we introduce the \textbf{structural prior}, transforming the scalar value into the expectation of a pre-defined categorical distribution, which allows us to improve the value representation and address the Monte Carlo estimation error from the perspective of distribution estimation. Under suitable structural prior, we show that the error is derived from the mismatch between the estimated posterior distribution and the ground-truth distribution, which is intractable to estimate as we have only limited samples. We then provide the Statistics-based Distance as the metric to measure the distance between ground truth distribution and posterior distribution. Under the vanilla mean-square error objective function and Histogram Loss objective function, our experiments show that the transformation can yield consistent improvements in performance on different tasks in the LLM reasoning scenario, showing that the structural prior can be of great benefit to the optimization process of the value-based process verifier. Finally, we compare the effectiveness varyin on structural priors. By performing experiments on each objective function, we show that prior selection can greatly influence the performance of value-based process verifiers.
