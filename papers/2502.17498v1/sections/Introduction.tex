\section{Introduction}
The process verifier plays a crucial role in Large Language Model(LLM) reasoning scenario, which requires numerous intermediate steps and the quality of the intermediate reasoning step is pivotal\citep{DBLP:conf/iclr/LightmanKBEBLLS24}. Compared with outcome verifiers solely focusing on outcome correctness\citep{DBLP:journals/corr/abs-2110-14168}, process verifiers provide more detailed signals throughout the multi-step decision-making process including the final outcome, which can be an elegant replacement to outcome verifiers at little-to-no cost\citep{DBLP:conf/iclr/LightmanKBEBLLS24}. In the LLM reasoning scenario, only the outcome binary reward is accurate and has no inductive bias. We thus focus on the value-based process verifier, where the process signal is the value of the current state\citep{lu2024autopsv, DBLP:journals/corr/abs-2406-06592,DBLP:journals/corr/abs-2408-03314}. The value-based process verifier incorporates limited inductive bias: the Markov Decision Environment has only the outcome binary reward, and the discount factor is equal to 1. In such an environment, the value signal can be effectively annotated through Monte Carlo Sampling, as introduced in \citet{DBLP:conf/acl/WangLSXDLCWS24}.

Though Monte Carlo estimation is an unbiased estimation of the ground-truth value(i.e. the success rate of the current state to outcome), it's still inaccurate especially when the sampling quantity is limited. The first problem of the limited sampling quantity is that it can only represent limited different values. As the Monte Carlo estimation method tries to estimate the state value in a discrete sampling space, the differences in representation granularity between the estimated value and the ground-truth value will be unacceptable. The second problem of the limited sampling quantity is that the variance of Monte Carlo estimation is related to the ground-truth value $p$. When $p$ is close to a marginal value, the variance of the Monte Carlo estimation result is small and the estimation can be accurate. When $p$ is close to the middle value, the estimation will be inaccurate except for increasing the sampling quantity.

To address the first problem, people often use the mean-square error objective function and map the discrete estimated value into the contiguous space\citep{lu2024autopsv, DBLP:journals/corr/abs-2406-06592,DBLP:journals/corr/abs-2408-03314}. The mean-square error objective function introduces the distance prior and shows that the distance between two different values can be measured by the square of the difference between the two values, which extends the discrete estimated values into the contiguous space. To address the second problem, people often use the temporal difference method to update the value of the current state using values of subsequent states iteratively, greatly reducing the sampling variance at the cost of a larger sampling quantity\citep{DBLP:journals/tac/TsitsiklisR97,DBLP:journals/corr/abs-2407-04811}. However, in the LLM reasoning scenario, it's expensive to extend the sampling quantity as each sample requires a completed rollout to the final state, which limits the application of Monte Carlo estimation as the annotation of the state value.

In this paper, we propose the structural prior injection method. To clarify, motivated by the distance prior introduced by the mean-square error objective function that is used to bridge the gap between discrete labels and contiguous target space, we inject the structural prior into the na\"ive scalar Monte Carlo state value expression, representing the scalar value as the expectation of some pre-defined categorical distribution. Under a suitable prior definition, the Monte Carlo estimation result can be treated as a one-step sampling of the ground-truth target distribution that is derived from the ground-truth state value $p$. The transformation allows us to interpret the Monte Carlo sampling error and the coarse-grained signal as the mismatch between the ground-truth target distribution and the posterior distribution conditioned on a sample taken from the ground-truth target distribution, and the error is derived from the difficulty of recovering the ground-truth distribution given limited sampling data.

To handle the problem, we propose the distance metric called Statistics-based Distance, which is used to measure the distance between two categorical distributions conditioned on the probability of sampling the corresponding category from the ground-truth target distribution. The distance metric can be used to judge the reasonableness of the prior structural information to be injected, which can therefore guide the optimization approach. We optimize the categorical distribution via different objective functions, including the mean-square error and the cross-entropy(HL) loss\citep{DBLP:conf/icml/ImaniW18}, showing that the reasonable structural prior can improve the performance of value-based process verifiers on different objective functions and different tasks for about 1$\sim$2 points at little-to-no cost. Through ablation studies, we also show that different structural prior can result in very different performances.

Finally, we summarize our contributions in this paper:
\begin{itemize}
    \item We propose the structural prior injection method, transferring the Monte Carlo sampling error into the distribution mismatch problem, showing that the error is due to the difficulty of recovering the ground-truth distribution using limited sampling data.
    \item We propose the Statictics-based Distance metric, which can guide the posterior distribution selection and handle the mismatch problem and is supported by the experiments.
    \item We show that the improvements derive from the reasonable structural prior injection, which is a promising and meaningful direction for future work.
\end{itemize}