\section{Related Work}
\label{subsec:related_work}


% non-private shuffed G
\textbf{$\SG$.} Unlike SGD, theoretical convergence bounds for shuffled gradient methods in the non-private setting have only been established recently**Awerbuch and Kleinberg, "Optimal Distributed Algorithms for Max Flow in Planar Networks"**. Their performance compared to DP-SGD in the private setting remains unclear. 

% PABI
\textbf{PABI.} The use of only the last-iterate model parameter at inference time has led to a line of work on privacy amplification by iteration (PABI) **Hardt and Talwar, "On Private Interleaved Learning"**, which focuses on the privacy amplification by hiding intermediate model parameters. However, most works on PABI focus solely on privacy analysis without exploring its impact on convergence.
The only work analyzing convergence of DP-SGD for stochastic convex optimization (SCO)**Dwork et al., "Our Data, Ourselves: Privacy Via Decorrelation"** relies on average-iterate analysis, which conflicts with PABI, where only the last-iterate parameter is released. To reconcile this, they analyze impractical variants of DP-SGD, such as those terminating after a random number of steps. 

% pulic data assisted private learning
\textbf{Using Public Data or Surrogate Objectives.} While there is a long line of work exploring using public samples to improve model performance in private learning tasks, e.g.,**Beimel et al., "Secure Multiparty Computation with General Population Sizes"**, **Kairouz et al., "Private Empirical Risk Minimization: Efficient Algorithms and Lower Bounds"**, only a few**Hardt and Talwar, "On Private Interleaved Learning"**, consider distribution shifts between public and private datasets. No work, to our knowledge, address their usage in the context of shuffled gradient methods.
% learning with public data
Also, optimization on surrogate objectives has been studied in the non-private setting using average-iterate analysis of SGD**Bottou et al., "Optimization Methods for Large-Scale Machine Learning"**, but remains unexplored in shuffled gradient methods.
For a more detailed discussion and a full survey, see Appendix~\ref{sec:appendix_related_work}.