%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\usepackage{subfigure}
\usepackage{float}
\usepackage{booktabs} % for professional tables
\usepackage[inline]{enumitem}
\setlist[itemize]{noitemsep, topsep=0pt}
\setlist[enumerate]{noitemsep, topsep=0pt}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{threeparttable}
\usepackage{diagbox}
\usepackage{adjustbox}
\usepackage{multirow}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\SG}{\textit{ShuffleG}}
\newcommand{\DPSG}{\textit{DP-ShuffleG}}
\newcommand{\interleaved}{\textit{Interleaved-ShuffleG}}
\newcommand{\privpub}{\textit{Priv-Pub-ShuffleG}}
\newcommand{\pubpriv}{\textit{Pub-Priv-ShuffleG}}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{math_commands}
\allowdisplaybreaks

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{
% Private Shuffled Gradient Methods with Public Data
The Cost of Shuffling in Private Gradient Based Optimization
}

\begin{document}

\twocolumn[
\icmltitle{
% Private Shuffled Gradient Methods with Public Data
The Cost of Shuffling in Private Gradient Based Optimization
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shuli Jiang}{aff}
\icmlauthor{Pranay Sharma}{aff}
\icmlauthor{Zhiwei Steven Wu}{aff}
\icmlauthor{Gauri Joshi}{aff}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{aff}{Carnegie Mellon University, Pittsburgh, PA, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Shuli Jiang}{shulij@andrew.cmu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} 
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
We consider the problem of differentially private (DP) convex empirical risk minimization (ERM). While the standard DP-SGD algorithm is theoretically well-established, practical implementations often rely on shuffled gradient methods that traverse the training data sequentially rather than sampling with replacement in each iteration. Despite their widespread use, the theoretical privacy-accuracy trade-offs of private shuffled gradient methods (\textit{DP-ShuffleG}) remain poorly understood, leading to a gap between theory and practice. In this work, we leverage privacy amplification by iteration (PABI) and a novel application of Stein's lemma to provide the first empirical excess risk bound of \textit{DP-ShuffleG}. Our result shows that data shuffling results in worse empirical excess risk for \textit{DP-ShuffleG} compared to DP-SGD.
To address this limitation, we propose \textit{Interleaved-ShuffleG}, a hybrid approach that integrates public data samples in private optimization. By alternating optimization steps that use private and public samples, \textit{Interleaved-ShuffleG} effectively reduces empirical excess risk. 
Our analysis introduces a new optimization framework with surrogate objectives, adaptive noise injection, and a dissimilarity metric, which can be of independent interest.
Our experiments on diverse datasets and tasks demonstrate the superiority of \textit{Interleaved-ShuffleG} over several baselines.
\end{abstract}



\section{Introduction}
\label{sec:introduction}

Differential privacy (DP)~\cite{dwork2014algorithmic} has become a cornerstone of privacy-preserving machine learning, providing robust guarantees against the leakage of sensitive information in training datasets. 
In this work, we revisit the classical problem of $(\eps, \delta)$-differentially private convex empirical risk minimization (ERM)~\cite{bassily2014private_erm}, a framework that underpins many privacy-preserving machine learning tasks. Given the training dataset $\gD = \{\rvd_1, \dots, \rvd_n\}$, the private ERM problem can be formulated as follows:
\begin{align}
\label{eq:main_problem}
    &\min_{\rvx \in \R^d} \Big\{G(\rvx; \gD) = F(\rvx; \gD) + \psi(\rvx)\Big\}, \\
    \nonumber
    &\text{ where } F(\rvx; \gD) = \frac{1}{n}\sum_{i=1}^{n} \Big\{ f_i(\rvx) := f(\rvx;\rvd_i) \Big\},
\end{align}
while ensuring 
$(\eps, \delta)$-differential privacy.
Here, $\rvx$ represents the model parameters, $\psi$ is a convex regularization and
$f_i$'s, for all $i$, are assumed to be convex, smooth, and Lipschitz-continuous\footnote{Convexity and smoothness are standard assumptions in the optimization literature. Lipschitzness is used only for privacy analysis \cite{Feldman2018privacy_amp_iter, ye2022singapore_paper}, and is not required for convergence analysis. Indeed, the Lipschitzness assumption can be removed by using gradient clipping ~\cite{Abadi2016dpsgd} in practice. For simplicity, we retain the Lipschitz assumption. }. 
For clarity of presentation, we consider twice differentiable $\psi$ (e.g., $\psi(\rvx) = \|\rvx\|^2$) in the main paper\footnote{
In our experiments, we consider $\psi$ as $\ell_1$ regularizer, $\ell_2$ regularizer and the projection operator. 
Detailed proofs for $\psi$ as the $\ell_1$ regularizer and as the projection operator onto a convex set are provided in Appendix~\ref{sec:appendix_other_reg}.
}. 



A well-known approach to address the above problem is DP-SGD~\cite{Abadi2016dpsgd, bassily2014private_erm}, the private variant of stochastic gradient descent. In DP-SGD, at each iteration, a gradient is computed using a randomly picked sample from the training data,
followed by the addition of Gaussian noise to ensure differential privacy. 
However, the reliance on i.i.d. sampling introduces practical challenges. 
Consequently, DP-SGD in its original form is seldom implemented in practice. Instead, as noted in~\cite{Ponomareva2023dpfyml, chua2024how_private_dp_sgd, chua2024scalable_dp}, 
\textit
{shuffled gradient methods}, which traverse samples from the training dataset sequentially in a certain order, are often used in private optimization codebases and libraries, such as \textit{Tensorflow Privacy}~\cite{tensorflow_privacy} and \textit{PyTorch Opacus}~\cite{pytorch_opacus}, but their privacy parameters are often incorrectly set based on the analysis of DP-SGD.

In the non-private setting, shuffled gradient methods (a class of methods, which we abbreviate with $\SG$) converge provably faster than SGD~\cite{liu2024last_iterate_shuffled_gradient}.
However, the convergence of \textit{private} shuffled gradient methods (which we denote by $\DPSG$), and how it compares to DP-SGD is poorly understood. 
This gap motivates our first key question: 
\fbox{%
% \centering
\parbox{0.48\textwidth}{%
\textit{What is the privacy-convergence trade-off of private shuffled gradient methods?}
}%
}

To evaluate the privacy-convergence trade-off for a private optimization algorithm, we fix the privacy loss and measure the empirical excess risk, a standard metric in ERM. This metric captures the trade-off by accounting for both the error from noise injection for privacy preservation and the optimization error.



The privacy analysis of private shuffled gradient methods presents unique challenges.  
The optimal privacy-convergence trade-offs for DP-SGD is achieved using a technique called privacy amplification by subsampling (PABS)~\cite{bassily2014private_erm}. However, PABS requires independent sampling of data points, hence is not applicable to shuffled gradient methods.
Instead, privacy amplification by iteration (PABI) emerges as a viable alternative in the convex setting, where privacy is amplified by hiding intermediate parameters and releasing only the final output. 
While prior work~\cite{Feldman2018privacy_amp_iter, altschuler2022apple_paper, ye2022singapore_paper} has focused on the privacy guarantees of PABI (see related work in section \ref{subsec:related_work} and Appendix~\ref{sec:appendix_related_work}), its impact on convergence rates in private optimization remains underexplored.
Moreover, $\SG$ differs from SGD by using biased gradients within each epoch. 
Consequently, adding noise introduces additional error terms absent in DP-SGD. 
We bound this term through a novel application of Stein's lemma.


\textbf{Excess Risk for Private Shuffled Gradient Methods.}
Addressing these challenges, we establish for the first time that the empirical excess risk of private shuffled gradient methods ($\DPSG$)
is $\widetilde{\gO} \Big( \frac{1}{n^{2/3}} \big( \frac{\sqrt{d}}{\eps} \big)^{4/3} \Big)$, given that the algorithm satisfies $(\eps, \delta)$-differential privacy.
This rate is worse than the empirical excess risk of DP-SGD, $\widetilde{\gO} \big( \frac{\sqrt{d}}{n \eps} \big)$, with matching lower bound~\cite{bassily2014private_erm}. The worse excess risk for $\DPSG$ matches similar empirical observations in \cite{chua2024scalable_dp}.
This disparity can perhaps be intuitively understood: shuffled gradient methods outperform SGD in non-private settings due to the reduced variance of the gradient estimator. However, this also implies reduced inherent randomness, resulting in a worse privacy guarantee.




A promising direction to improve the empirical excess risk of private shuffled gradient methods is to leverage public data. The use of public samples, which can be accessed cheaply in many real-world scenarios, has been shown to improve utility in private learning problems, both theoretically~\cite{bassily20priv_query_release_pub_data, ullah2024public_data_priv_sco, block2024oracleefficient_dp_learning} and empirically~\cite{yu2022dp_fine_tune_lm, bu2023dp_fine_tune_fm}.
However, no prior work has explored the use of public samples in the context of private shuffled gradient methods. We consider the practical setting where the public and private datasets may come from different distributions.
While using public samples enhances the privacy guarantee, leading to less noise being added, it also risks greater divergence from the target objective. 
This trade-off motivates our second key question: 
\fbox{%
\parbox{0.48\textwidth}{%
\textit{
Can public samples help improve the privacy-convergence trade-offs of private shuffled gradient methods?
}
}%
}



To answer this question, we propose the novel \textbf{generalized shuffled gradient framework} (see Algorithm~\ref{alg:generalized_shuffled_gradient_fm}), which introduces flexibility along several dimensions. First, instead of using a fixed objective function across all epochs, this framework allows optimizing a potentially different surrogate objective $G(\rvx; \gD^{(s)} \cup \gP^{(s)})$ in each epoch $s$,
where the dataset $\gD^{(s)} \cup \gP^{(s)}$ contains both private ($\gD^{(s)}$) and public ($\gP^{(s)}$) samples. 
Second, it enables adaptive noise injection, where different amounts of noise 
are added in different epochs 
to ensure the desired privacy guarantee.
For example, when optimizing with only public samples, no noise needs to be added, while noise is necessary when using private samples.
Further, to analyze this setup, we introduce a novel metric to measure the dissimilarity between the true and the surrogate objective
(see Assumption~\ref{ass:dissim_partial_lipschitzness}), specifically designed for shuffled gradient methods.




Using the generalized shuffled gradient framework, we study three algorithms (see \Cref{subsec:pub_data_algos}): 1) $\privpub$, where the initial few epochs involve optimizing only using private samples, followed by some epochs on public samples; 2) in $\pubpriv$, the order of using public and private samples is reversed compared to $\privpub$; and 3) $\interleaved$, which involves using both private and public samples within each epoch. 
We show that $\interleaved$ achieves a smaller empirical excess risk compared to $\privpub$ and $\pubpriv$ (\Cref{tab:emp_risk_comp_main}), as well as $\DPSG$.


%% contributions
\subsection{Our Contributions}
\label{subsec:contributions}


\begin{enumerate}[itemsep=0mm, leftmargin=4mm]
    \item \textbf{Generalized Shuffled Gradient Framework.}
    In \Cref{sec:generalized_shufflg}, we present a generalized shuffled gradient framework (Algorithm~\ref{alg:generalized_shuffled_gradient_fm}) that allows surrogate objectives (based on public samples) and adaptive noise addition across epochs. We state the general convergence result, based on a novel dissimilarity metric, in \Cref{thm:convergence_generalized_shufflg_fm}.
    \item \textbf{Understanding $\DPSG$.} In \Cref{sec:private_shuffled_g}, we show the empirical excess risk of $\DPSG$, which follows as a special case of \Cref{thm:convergence_generalized_shufflg_fm}. 
    \item \textbf{Effective Public Sample Usage.}
    Based on the general framework, in \Cref{sec:public_data_dp_shuffleg}, we propose $\interleaved$, an algorithm that uses both private and public samples within each epoch and achieves a smaller empirical excess risk, compared to $\DPSG$ as well as some other approaches that use public samples.
    \item \textbf{Experiments.}
    In \Cref{sec:exp}, we empirically demonstrate the superior performance of $\interleaved$ compared to the baselines
    in three tasks on diverse datasets.
\end{enumerate}


\subsection{Related Work}
\label{subsec:related_work}


% non-private shuffed G
\textbf{$\SG$.} Unlike SGD, theoretical convergence bounds for shuffled gradient methods in the non-private setting have only been established recently~\cite{mishchenko2021rr, mishchenko2021prox_fed_rr, liu2024last_iterate_shuffled_gradient}. Their performance compared to DP-SGD in the private setting remains unclear. 

% PABI
\textbf{PABI.} The use of only the last-iterate model parameter at inference time has led to a line of work on privacy amplification by iteration (PABI)~\cite{Feldman2018privacy_amp_iter, altschuler2022apple_paper, ye2022singapore_paper}, which focuses on the privacy amplification by hiding intermediate model parameters. However, most works on PABI focus solely on privacy analysis without exploring its impact on convergence.
The only work analyzing convergence of DP-SGD for stochastic convex optimization (SCO)~\cite{Feldman2018privacy_amp_iter} relies on average-iterate analysis, which conflicts with PABI, where only the last-iterate parameter is released. To reconcile this, they analyze impractical variants of DP-SGD, such as those terminating after a random number of steps. 

% pulic data assisted private learning
\textbf{Using Public Data or Surrogate Objectives.} While there is a long line of work exploring using public samples to improve model performance in private learning tasks, e.g.,~\cite{bassily20priv_query_release_pub_data, ullah2024public_data_priv_sco}, only a few~\cite{bie2022private_est_pub_data_shift, Bassily2023priv_adp_from_pub_source} consider distribution shifts between public and private datasets. No work, to our knowledge, address their usage in the context of shuffled gradient methods.
% learning with public data
Also, optimization on surrogate objectives has been studied in the non-private setting using average-iterate analysis of SGD~\cite{Woodworth2023two_losses} but remains unexplored in shuffled gradient methods.
For a more detailed discussion and a full survey, see Appendix~\ref{sec:appendix_related_work}.




\section{Problem Formulation}
\label{sec:prelim}

\textbf{Notation.} 
Given a positive integer $m$, we define $[m] = \{1,2,\dots, m\}$.
The symbol $\pi$ is used to denote a permutation, $\Pi_n$ denotes the set of all permutations of $[n]$, and $\|\cdot \|$ refers to the $\ell_2$ norm. 
$\sI_d$ denotes the identity matrix of dimension $d$.
$\rvx^* = \argmin_{\rvx \in \R^{d}} G(\rvx; \gD)$ denotes the optimum of the true objective.


We solve the optimization problem given by (\ref{eq:main_problem}) under differential privacy, formally defined as follows:
\begin{definition}[Differential Privacy (DP)~\cite{dwork2014algorithmic}]
\label{def:DP}
    A randomized mechanism $\gM: \gW \rightarrow \gR$ 
    % with a domain $\gW$ and range $\gR$ 
    satisfies $(\eps, \delta)$-differential privacy, for $\eps \geq 0, \delta \in (0, 1) $, if for any two {\bf adjacent datasets} $\gD, \gD'$ 
    and for any subset of outputs $S \subseteq \gR$, it holds that 
    \begin{align*}
        \Pr[\gM(\gD) \in S] \leq e^{\eps} \Pr[\gM(\gD') \in S] + \delta
    \end{align*}
\end{definition}
$\eps$ and $\delta$ are called the privacy loss of the algorithm $\gM$. 

In this work, we study private shuffled gradient methods (\DPSG), which optimize the objective in (\ref{eq:main_problem}) over $K$ epochs. During epoch $s\in [K]$, the $i$-th update is given by 
$\rvx_{i+1}^{(s)} = \rvx_{i}^{(s)} - \eta (\nabla f_{j}(\rvx_{i}^{(s)}) + \rho_i^{(s)} )$, $\forall i\in [n]$, where $\eta$ is the learning rate, $\rho_i^{(s)} \sim \gN(0, \sigma^2\sI_d)$ is the Gaussian noise vector, and $j\in [n]$ is the index of the sample selected for gradient computation. Each sample is used exactly once per epoch, with regularization $\psi$ being applied only at the end of every epoch, to ensure convergence~\cite{mishchenko2021prox_fed_rr}. 


Next, we discuss the three most commonly studied variants of shuffled gradient methods, which differ in how samples are selected in each epoch. \textbf{Incremental Gradient (IG)} method processes samples in the same \textit{pre-determined} order across epochs. \textbf{Shuffle Once (SO)} also follows the same order across epochs, but the order is a random permutation $\pi$ of $[n]$. Finally, \textbf{Random Reshuffling (RR)} picks a new random permutation $\pi^{(s)}$ at the beginning of each epoch $s$, which determines the order for that epoch.



To understand the privacy-convergence trade-offs of \DPSG, 
first we define the \textit{empirical excess risk} as
\begin{align}
\label{eq:empirical_excess_risk}
    \E\left[G(\rvx; \gD) - G(\rvx^*; \gD)\right]
\end{align}
where $\rvx^* = \argmin_{\rvx\in \R^{d}}G(\rvx; \gD)$, $\gD = \{\rvd_1,\dots,\rvd_n\}$ is a 
private training dataset, and $G(\rvx;\gD)$ is the \textit{target objective}.
The empirical excess risk captures the trade-off between privacy and convergence, reflecting the optimization error incurred to ensure some fixed privacy guarantee.


Our second goal is to effectively use public samples to improve the empirical excess risk of $\DPSG$. 
Alongside $\gD$, we have access to some public dataset $\gP$, with a potentially different distribution.
To allow the flexibility of using varying proportions of samples from both datasets, we formulate the optimization objective as a sequence of surrogate objectives that capture the proportion of public and private data used in each epoch.
In each epoch $s$, we use $n_d^{(s)} (\leq n)$ private samples from $\gD$ and $n-n_d^{(s)}$ public samples from $\gP$.
The private dataset used in epoch $s$, denoted $\gD^{(s)}$
is formed by generating a random permutation $\pi^{(s)}$ of $[n]$ and selecting the first $n_d^{(s)}$ samples in $\pi^{(s)} (\gD)$, namely, $\gD^{(s)} := \{ \rvd_{\pi_{i}^{(s)}} \}_{i=1}^{n_d^{(s)}}$. 
The public data used in epoch $s$ is $\gP^{(s)} := \{ \rvp_j^{(s)} \}_{j=1}^{n-n_d^{(s)}} \subseteq \gP$ with $|\gP^{(s)}| = n-n_d^{(s)}$.
The \textit{surrogate objective} function used in epoch $s\in [K]$ is


\begin{align}
\label{eq:surrogate_objective}
    &G(\rvx; \gD^{(s)} \cup \gP^{(s)}) = F(\rvx; \gD^{(s)} \cup \gP^{(s)}) + \psi(\rvx),\\
    \nonumber
    &
    F(\rvx; \gD^{(s)} \cup \gP^{(s)}) 
    = \frac{1}{n} \Big(\sum_{\rvd \in \gD^{(s)}} f(\rvx; \rvd)
    + \sum_{\rvp \in \gP^{(s)}} f(\rvx; \rvp) \Big).
\end{align}



The above objective generalizes the target objective $ G(\rvx; \gD)$ in (\ref{eq:main_problem}) and recovers the objective of $\DPSG$,
when $n_d^{(s)} = n$  
and $\gP^{(s)} = \emptyset, \forall s\in [K]$. 
It also allows flexible use of private and public samples, supporting schemes like public pre-training followed by private fine-tuning or mixed usage of private and public samples within an epoch. See \Cref{subsec:pub_data_algos} for the discussion on some such approaches.



To quantify the difference between the target objective (\ref{eq:main_problem}) and the surrogate objective used in epoch $s$ (\ref{eq:surrogate_objective}), we define the objective difference as follows:
\begin{align}
\label{eq:def_H}
    H^{(s)}(\rvx) 
    % &= \frac{1}{n}\sum_{i=1}^{n} h_i^{(s)}(\rvx) 
    &= G(\rvx; \gD) - G(\rvx; \gD^{(s)} \cup \gP^{(s)})
\end{align}




\section{Generalized Shuffled Gradient Framework}
\label{sec:generalized_shufflg}

\begin{algorithm}[h]
\caption{Generalized Shuffled Gradient Framework}
\label{alg:generalized_shuffled_gradient_fm} 
    \begin{algorithmic}[1]
    \STATE Input: 
    Initial point $\rvx_1^{(1)}$, learning rate $\eta$, number of epochs $K$.
    Private dataset $\gD$. Number of private samples to use $\{n_d^{(s)}\}_{s=1}^{K}$, $0\leq n_d^{(s)} \leq n$. Public datasets $\{\gP^{(s)}\}_{s=1}^{K}$ with $|\gP^{(s)}| = n_d^{(s)}$.
    Noise standard deviation $\{\sigma^{(s)}\}_{s=1}^{K}$.
    \STATE \textit{IG}: Fix an order $\pi$, set $\pi^{(s)} = \pi, \forall s$
    \STATE \textit{SO}: Generate permutation $\pi$ of $[n]$, set $\pi^{(s)} = \pi, \forall s$
    \FOR{$s = 1,2,\dots,K$}
        \STATE \textit{RR}: Generate permutation $\pi^{(s)}$ of $[n]$
        \FOR{$i = 1,2,\dots, n_d^{(s)}$}
            \STATE Sample noise $\rho_i^{(s)} \sim \gN(0, (\sigma^{(s)})^2 \sI_d)$
            \STATE $\rvx_{i+1}^{(s)} \leftarrow \rvx_i^{(s)} - \eta \Big(\nabla f(\rvx_i^{(s)};  \rvd_{\pi_i^{(s)}} ) + \rho_i^{(s)} \Big)$
        \ENDFOR
        \FOR{$i = n_d+1, n_d + 2, \dots, n$}
            \STATE Sample noise $\rho_i^{(s)} \sim \gN(0, (\sigma^{(s)})^2\sI_d)$
            \STATE $\rvx_{i+1}^{(s)} \leftarrow \rvx_i^{(s)} - \eta \Big( \nabla f(\rvx_i^{(s)}; \rvp_{i-n_d}^{(s)}) + \rho_i^{(s)}\Big)$
        \ENDFOR
        \STATE $\rvx_1^{(s+1)} \leftarrow \argmin_{\rvx\in \R^{d}} n  \psi(\rvx) + \frac{\|\rvx - \rvx_{n+1}^{(s)}\|^2}{2\eta}$
    \ENDFOR
    \STATE \textbf{return} $\rvx_1^{(K+1)}$
    \end{algorithmic}
\end{algorithm}


In this section, we introduce the generalized shuffled gradient framework (Algorithm~\ref{alg:generalized_shuffled_gradient_fm}), which incorporates surrogate objectives, and noise injection for privacy preservation. This framework unifies private shuffled gradient methods ($\DPSG$) and their non-private variants as special cases. Specifically, $\DPSG$ corresponds to using only the true private dataset across all epochs ($n_d^{(s)}=n$ and $\gP^{(s)} = \emptyset$, $\forall s\in [K]$), while in the non-private version, no noise is added to the gradients ($\sigma^{(s)} = 0$). We provide the convergence analysis of the general framework here, with the convergence of $\DPSG$ as a corollary and its empirical excess risk derived in \Cref{sec:private_shuffled_g}.


We first introduce the assumptions and notation in \Cref{subsec:ass}.
To analyze the impact of surrogate objectives on convergence, we introduce a novel dissimilarity measure in \Cref{subsec:dissim_measure}
to measure the difference between the target objective, i.e., $G(\rvx;\gD)$, and surrogate objectives, i.e., $G(\rvx; \gD^{(s)} \cup \gP^{(s)})$.
Using this measure, we present the convergence results in \Cref{subsec:convergence}.


\subsection{Assumptions and Notation}
\label{subsec:ass}
We emphasize that only Assumptions~\ref{ass:convexity},~\ref{ass:smoothness}, and~\ref{ass:reg} are required for convergence analysis. Assumption~\ref{ass:lipschitzness} is standard for privacy analysis and can be removed by using gradient clipping in practice. 
Recall that $\gD$ is the training dataset in the target objective (\ref{eq:main_problem}), and $\gP$ is the public dataset. 

\begin{assumption}[Convexity]
\label{ass:convexity}
    $f(\rvx; \rvd)$ is convex, for all $\rvd \in \gD \cup \gP$.
\end{assumption}



\begin{assumption}[Smoothness]
\label{ass:smoothness}
    A function $f: \R^{d} \rightarrow \R$ is $L$-smooth if $\|\nabla f(\rvx) - \nabla f(\rvy)\| \leq \Lsm \|\rvx - \rvy\|$, for some $\Lsm \geq 0$, for all $\rvx, \rvy$. 
    $f(\rvx; \rvd)$ is $L$-smooth, $\forall \rvd \in \gD$;
    $f(\rvx; \rvp)$ is $\widetilde{L}$-smooth, $\forall \rvp \in \gP$.
\end{assumption}




\begin{assumption}[Lipschitz Continuity]
    \label{ass:lipschitzness}
    A convex function $f: \R^d \rightarrow \R$ is $\G$-Lipschitz if $\|\nabla f(\rvx)\| \leq \G$.
    $f(\rvx, \rvd)$ is $\G$-Lipschitz, $\forall \rvd\in\gD$;
    $f(\rvx; \rvp)$ is $\widetilde{G}$-Lipschitz, for all $\rvp \in \gP$.
\end{assumption}



\begin{assumption}
\label{ass:reg}
    The regularization function $\psi$ is twice differentiable and $\mu_{\psi}$-strongly convex, for $\mu_{\psi} \geq 0$.
\end{assumption}


We denote the maximum smoothness and Lipschitz constants by $L^* = \max\{L, \widetilde{L}\}$ and $G^{*}=\max\{G, \widetilde{G}\}$.





\subsection{Dissimilarity Measure}
\label{subsec:dissim_measure}

Next, we measure the dissimilarity between the target objective function $G(\rvx;\gD)$ (\ref{eq:main_problem}) and the surrogate objective function $G(\rvx; \gD^{(s)} \cup \gP^{(s)})$ in epoch $s\in [K]$ (\ref{eq:surrogate_objective}), based on the smoothness and the Lipschitzness of the objective difference $H^{(s)}(\rvx)$ defined in (\ref{eq:def_H}). 


It follows from Assumptions \ref{ass:smoothness} and \ref{ass:lipschitzness} that $H^{(s)}$ is $(L + L^*)$-smooth, 
and $(G + G^*)$-Lipschitz continuous.
However, these constants can be too large, leading to loose convergence bounds. For example, when the dataset used in every epoch is exactly the same as the training dataset $\gD$, i.e., $n_d^{(s)} = n, \gP^{(s)} = \emptyset$, $\forall s\in [K]$,
then $H^{(s)} \equiv 0$. In this case, the smoothness and the Lipschitzness parameters of $H^{(s)}$ are both $0$. Therefore, for sharper analysis, we explicitly model the smoothness and Lipschitzness of $H^{(s)}$. 


\begin{assumption}
\label{ass:H_smoothness}
    $H^{(s)}$ is $L_H^{(s)}$-smooth, for all $s\in[K]$.
\end{assumption}
As discussed above, $L_H^{(s)} \leq L + L^*$.
\begin{assumption}
\label{ass:dissim_partial_lipschitzness}
    % 
    For epoch $s\in [K]$, there exists constants $\{C_i^{(s)}\}_{i=1}^{n}$ such that for $1\leq i \leq n_d^{(s)}$,
    \\
    $$\max_{\pi\in\Pi_n} \E_{\widehat{\pi}}\Big[ \Big\| \sum_{j=1}^{i} \left(\nabla f(\rvx; \rvd_{\pi_j}) - \nabla f(\rvx; \rvd_{\widehat{\pi}_j}) \right) \Big\| \Big]
        \leq C_i^{(s)}, \text{ and} $$
    \vspace{-3pt}
    \begin{align*}
        & \max_{\pi\in\Pi_n}\E_{\widehat{\pi}}\Big[\Big\|
            \sum_{j=1}^{n_d}\Big( \nabla f(\rvx; \rvd_{\pi_j}) - \nabla f(\rvx; \rvd_{\widehat{\pi}_j})
            \Big)\\
        & + \sum_{j=n_d+1}^{n} \Big(
            \nabla f(\rvx; \rvd_{\pi_j})
            - \nabla f(\rvx; \rvp_{j-n_d}^{(s)})
        \Big)
        \Big\|\Big] \leq C_i^{(s)},
    \end{align*}
    for $n_d^{(s)} < i \leq n$. 
\end{assumption}


This measure of dissimilarity 
is inspired by prior work on distributed SGD-based optimization in non-private settings, 
e.g.,~\cite{wang2021fednova}
and optimization with surrogate objectives \cite{Woodworth2023two_losses}.
These works define dissimilarity by directly comparing the gradients 
evaluated at individual samples. 
For example, 
$\|\nabla f(\rvx; \rvd_{i}) - \nabla f(\rvx; \rvd_{j})\| \leq C$.


However, this crude notion of dissimilarity is less suitable for analyzing shuffled gradient methods and can lead to overly loose bounds.
To illustrate this, consider the case of using $\widehat{\gD}$ in optimization, where $\widehat{\gD}$ is a permuted version of the true dataset $\gD$, and $n_d^{(s)} = n$, for all $s \in [K]$. It follows that $\gP^{(s)} = \emptyset$ for all $s$. 
The typical dissimilarity measure based on the gradients of individual samples would imply $\| \nabla H^{(s)} (\rvx)\| \leq C$. On the other hand, from our proposed \Cref{ass:dissim_partial_lipschitzness}, it follows that $C_n^{(s)}=0$. Therefore, $\| \nabla H^{(s)} (\rvx) \| \equiv 0, \forall s\in[K]$. 
This also makes intuitive sense, since the true and surrogate datasets are identical.




\subsection{Convergence}
\label{subsec:convergence}

Based on the above dissimilarity measure, we present the convergence results of \textit{generalized shuffled gradient framework} in Algorithm~\ref{alg:generalized_shuffled_gradient_fm}.
In the convergence bound, we use $\sigma_{any}^2 = \frac{1}{n} \sum_{i=1}^{n} \|\nabla f_i(\rvx^{*})\|^2$ to
measure the optimization uncertainty in shuffled gradient methods, following~\cite{liu2024last_iterate_shuffled_gradient}.
See Appendix~\ref{sec:appendix_proof_main_thm} for the full proof.


\begin{theorem}[Convergence of Generalized Shuffled Gradient Framework]
\label{thm:convergence_generalized_shufflg_fm}
    Under Assumptions~\ref{ass:convexity},~\ref{ass:smoothness},~\ref{ass:reg},~\ref{ass:H_smoothness},~\ref{ass:dissim_partial_lipschitzness}, for $\beta > 0$, 
    if $\mu_{\psi} \geq L_H^{(s)} + \beta$, $\forall s\in [K]$, and 
     $\eta \lesssim \frac{1}{n \Lsm^* \sqrt{1+\log K}}$,   
    Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees
    % \vspace{-5pt}
    \begin{align}
    \label{eq:convergence_generalized_shufflg_fm}
        &\E\left[ G(\rvx_1^{(K+1)};\gD) \right] - G(\rvx^*;\gD)
        \lesssim \underbrace{\eta^2 n^2 \sigma_{any}^2 (1+\log K) \Lsm^*}_{\text{Optimization Uncertainty}} \nn \\
        & \quad + \underbrace{ \frac{\|\rvx_1^{(1)} - \rvx^{*}\|^2}{\eta n K}}_{\text{Due to Initialization}} + \max_{k\in [K]} \Bigg(
        \underbrace{ \frac{1}{n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s} }_{\text{Non-vanishing Dissimilarity}} \\
        & + \underbrace{ \eta^2 \Lsm^* \sum_{s=1}^{k} \frac{\frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s} }_{\text{Vanishing Dissimilarity}}
        + \underbrace{ \eta^2 \Lsm^* nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2}{k+1-s} }_{\text{Due to Noise Injection}} \Bigg), \nn
    \end{align}
    and the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}$ 
    and the order of samples $\pi^{(s)}$, $\forall i\in [n], s\in [K]$.
\end{theorem}




% remark on special cases
\textbf{Convergence Bound for Non-Private Shuffled Gradient Methods.} In the special case where we only use private data, i.e., $n_d^{(s)}= n$, and no noise is injected, $\sigma^{(s)}=0$, $\forall s\in [K]$, then the non-vanishing dissimilarity is $C_n^{(s)} = 0$\footnote{The vanishing dissimilarity term is at most the same order as the optimization uncertainty term.}.
Consequently, the bound in (\ref{eq:convergence_generalized_shufflg_fm}) recovers the convergence rate of non-private shuffled gradient methods in~\cite{liu2024last_iterate_shuffled_gradient}.

% vanishing vs. non-vanishing dissimilarity terms
\textbf{Impact of Dissimilarity.} Additionally, we observe two dissimilarity terms in (\ref{eq:convergence_generalized_shufflg_fm}), one \textit{vanishing} and the other \textit{non-vanishing}. 
If the dataset used in optimization $\gD^{(s)}\cup \gP^{(s)}$ is different from the original dataset $\gD$, we cannot expect \Cref{alg:generalized_shuffled_gradient_fm} to converge exactly to the actual solution $\rvx^*$. The \textit{Non-vanishing Dissimilarity} term in (\ref{eq:convergence_generalized_shufflg_fm}) captures this, since $C_n^{(s)} \neq 0$ in this case (see \Cref{ass:dissim_partial_lipschitzness}). On the other hand, if the dataset $\gD^{(s)}\cup \gP^{(s)}$ used across all the epochs are permutations of the original dataset $\gD$, as discussed in \Cref{subsec:dissim_measure}, $C_n^{(s)} = 0$, for all $s$. Therefore, the \textit{Non-vanishing Dissimilarity} term in (\ref{eq:convergence_generalized_shufflg_fm}) disappears. However, even with identical datasets, the different ordering of samples in $\gD^{(s)}\cup \gP^{(s)}$ compared to $\gD$ would result in different optimization trajectories. This effect is captured by $\{ C_i^{(s)} > 0\}$ in the \textit{Vanishing Dissimilarity} term in (\ref{eq:convergence_generalized_shufflg_fm}). The $\eta^2$ scaling ensures that this can be made vanishingly small.





\section{Private Shuffled Gradient Methods}
\label{sec:private_shuffled_g}

In this section, we derive the convergence rate and the empirical excess risk of $\DPSG$. As discussed earlier, $\DPSG$ is a special case of the \textit{generalized shuffled gradient framework} (Algorithm~\ref{alg:generalized_shuffled_gradient_fm}) where the surrogate objectives
are identical to the target objective,
i.e., $n_d^{(s)} = n, \gP^{(s)} = \emptyset$, and $\sigma^{(s)} = \sigma$, $\forall s\in [K]$. We first present the convergence bound of $\DPSG$ as a corollary of Theorem~\ref{thm:convergence_generalized_shufflg_fm} in Corollary~\ref{corollary:conv_dpsg}. In Lemma~\ref{lemma:privacy_private_shuffled_g}, we state the differential privacy guarantee of $\DPSG$, in terms of the noise variance $\sigma$.
Finally, we discuss the choice of the learning rate $\eta$ and the number of epochs $K$ to 
achieve the minimal empirical excess risk while ensuring $(\eps, \delta)$-DP.




\begin{corollary}[Convergence of $\DPSG$\footnote{One can set $\beta = 0$ when $L_H^{(s)} = 0$, which is the case here since no surrogate datasets is used. This implies $\mu_{\psi} \geq 0$, as indicated in Assumption~\ref{ass:reg}, suffices to ensure convergence.}]
\label{corollary:conv_dpsg}
    If we set $n_d^{(s)} = n$, $\gP^{(s)} = \emptyset$, and constant noise variance $(\sigma^{(s)})^2 = \sigma^2$ for all $s\in [K]$,
    then under the conditions in \Cref{thm:convergence_generalized_shufflg_fm}, Algorithm~\ref{alg:generalized_shuffled_gradient_fm} ($\DPSG$) guarantees
    \begin{align*}
        & \E [G(\rvx_1^{(K+1)};\gD)] - G(\rvx^*;\gD)
        \lesssim \eta^2 n^2(1+\log K) L^* \footnotemark \\
        \nonumber
        & \quad + \frac{\|\rvx_1^{(1)} - \rvx^{*}\|^2}{\eta n K} + \eta^2 n d \sigma^2 L^{*} (1 + \log K)
    \end{align*}
\end{corollary}
\footnotetext{This term subsumes both the optimization uncertainty and the vanishing dissimilarity terms.}


\textbf{Privacy of $\DPSG$.}
We use privacy amplification by iteration (PABI, see Appendix~\ref{subsec:appendix_pabi_related} for details) to bound the privacy loss within an epoch.
PABI allows us to add a smaller amount of noise to achieve the same privacy guarantee compared to directly applying composition results (Proposition~\ref{prop:rdp_composition}).
This privacy amplification arises because the intermediate iterates within an epoch $\{\rvx_i^{(s)} \}_{i=1}^n$ are hidden. In practical applications, these intermediate parameters are never directly used in downstream tasks. 
However, PABI requires the update steps to be ``contractive" (see Definition~\ref{def:contraction}). While each gradient step within an epoch (line 8 of Algorithm~\ref{alg:generalized_shuffled_gradient_fm}) satisfies this property, the regularization step at the end of each epoch (line 14 of Algorithm~\ref{alg:generalized_shuffled_gradient_fm}) is not necessarily contractive. This prevents us from using PABI across multiple epochs.
Hence, we use composition (Proposition~\ref{prop:rdp_composition}) to bound the total privacy loss across the $K$ epochs, as presented next. 
See Appendix~\ref{subsec:appendix_private_shuffled_g_privacy} for the proof.


\begin{lemma}[Privacy of $\DPSG$]
\label{lemma:privacy_private_shuffled_g}
    Under Assumptions~\ref{ass:smoothness} and~\ref{ass:lipschitzness}, if the learning rate is $\eta \leq 1/L$, $\DPSG$ is $(\frac{2\alpha G^2 K}{\sigma^2} + \frac{\log 1/\delta}{\alpha - 1}, \delta)$-DP, for $\alpha > 1, \delta \in (0, 1)$.
\end{lemma}


\textbf{Empirical Excess Risk.} 
To ensure $\DPSG$ satisfies $(\eps, \delta)$-DP, we set
$\sigma = \widetilde{\gO}(\frac{G\sqrt{K}}{\eps})$, $\eta = \widetilde{\gO}(\frac{1}{nL^{*} K^{1/3}})$ and $K = \gO(\frac{n\eps^2}{d})$ to minimize the bound in \Cref{corollary:conv_dpsg}.
These choices yield the empirical excess risk of $\DPSG$
in $n,d,\eps$ as
\begin{align}
    \E [G(\rvx_1^{(K+1)};\gD)] - G(\rvx^*;\gD) = \widetilde{O}\Big( \frac{1}{n^{2/3}} ( \frac{\sqrt{d}}{\eps} )^{4/3} \Big). \label{eq:empirical_excess_rrisk_DPShuffleG}
\end{align}
Here, $\widetilde{\gO}$ hides logarithmic factors in $(n, d, 1/\delta)$.
See Appendix~\ref{subsec:appendix_private_shuffle_g_empirical_excess_risk} for a full derivation.


\textbf{Comparison with DP-(S)GD.} The lower bound for empirical excess risk when minimizing convex, smooth objectives is $\Omega\left(\frac{\sqrt{d}}{n \eps}\right)$~\cite{bassily2014private_erm}. DP-GD and DP-SGD, both classically used to solve private ERM, achieve matching upper bounds. However, the bound in (\ref{eq:empirical_excess_rrisk_DPShuffleG}) suggests a worse empirical excess risk for $\DPSG$.

This aligns partially with the empirical findings of~\cite{chua2024scalable_dp}, which demonstrated that even after adding the most optimistic amount of noise to two shuffled gradient methods, SO and RR, they
underperform DP-SGD in private binary classification tasks with the same privacy guarantees. Their setting, however, allows intermediate model parameter releases and does not require convex objectives.

The worse privacy guarantee of $\DPSG$ 
can be intuitively explained: the provably faster convergence of shuffled gradient methods~\cite{liu2024last_iterate_shuffled_gradient}, compared to SGD, is due to the reduced variance of their gradient estimators. However, this implies that to achieve the same privacy guarantee, these methods need a larger noise variance. 





\section{Leveraging Public Data}
\label{sec:public_data_dp_shuffleg}


Given the pessimistic empirical excess risk of $\DPSG$ discussed above, how can it be improved? In this section, we explore leveraging public data samples $\gP$ in the context of private shuffled gradient methods. We propose a novel approach that interleaves the usage of public and private samples during training, demonstrating its effectiveness in reducing empirical excess risk.


\subsection{Algorithms}
\label{subsec:pub_data_algos}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{algo_pub_data.pdf}
    \caption{Illustration of algorithms that use public data. }
    \label{fig:alg_pub_data}
    % \vspace{-7pt}
\end{figure}

The following algorithms, which leverage public samples, are specific instantiations of Algorithm~\ref{alg:generalized_shuffled_gradient_fm}. An illustration of these algorithms is provided in Figure~\ref{fig:alg_pub_data}.
We begin with two common baselines:

\textbf{1) $\privpub$}: Train only on the private dataset $\gD$ for the first $S$ epochs, where $S \in [K-1]$. For the remaining $K-S$ epochs, train only on the public dataset $\gP$. Specifically, Algorithm~\ref{alg:generalized_shuffled_gradient_fm} is instantiated as follows:
\begin{itemize}[leftmargin=4mm]
    \item For the first $S$ epochs ($s \leq S$), $n_d^{(s)} = n$ and $\gP^{(s)} = \emptyset$,
    \item For the remaining $K-S$ epochs ($s \geq S+1$), $n_d^{(s)} = 0$ and $\gP^{(s)} = \gP$.
\end{itemize}
Consequently, during the first $S$ epochs, there is no non-vanishing dissimilarity. 
However, noise with variance $\noisePrivPub$ is added during the first $S$ epochs to preserve privacy.  During the last $K-S$ epochs, using the public data $\gP$ rather than the private data $\gD$ results in the non-vanishing dissimilarity as $C_n^{(s)} = C_n^{\text{full}}$, $\forall s\in \{S+1,\dots,K\}$.
No noise is needed during the last $K-S$ epochs.



\textbf{2) $\pubpriv$}: Train only on the public dataset $\gP$ for the first $S$ epochs, then switch to the private dataset $\gD$ for the remaining $K - S$ epochs. In Algorithm~\ref{alg:generalized_shuffled_gradient_fm}
\begin{itemize}[itemsep=0mm, leftmargin=4mm]
    \item For the first $S$ epochs ($s \leq S$), $n_d^{(s)} = 0$ and $\gP^{(s)} = \gP$,
    \item For the remaining $K-S$ epochs ($s \geq S+1$), $n_d^{(s)} = n$ and $\gP^{(s)} = \emptyset$.
\end{itemize}
Consequently, during the first $S$ epochs, the non-vanishing dissimilarity is $C_n^{(s)} = C_n^{\text{full}}$, $\forall s\in [S]$. 
However, no additive noise is required. During the last $K-S$ epochs, there is no non-vanishing dissimilarity, and noise with variance $\noisePubPriv$ is added.



In addition, we propose
\textbf{3) $\interleaved$}: In each epoch $s\in [K]$, we fix $n_d^{(s)} = n_d$, where the first $n_d \in [n]$ steps use samples from the private dataset $\gD$ for gradient computation, followed by $n - n_d$ steps using samples from the public dataset $\gP$. 
As a result, during every epoch, 
the first $n_d$ steps involve no non-vanishing dissimilarity, while the remaining $n-n_d$ steps introduce dissimilarity arising from the use of samples from the public dataset.
Specifically, we denote the non-vanishing dissimilarity as $C_n^{(s)} = C_n^{\text{part}}$, $\forall s\in [K]$.
Moreover, noise with variance $\noiseInter$ is applied at every step across all epochs.


\textbf{Convergence and Privacy.} We summarize the key parameters and convergence bounds for each algorithm, all of which follow as corollaries of Theorem~\ref{thm:convergence_generalized_shufflg_fm} in Appendix~\ref{subsec:appendix_algo_pub_data_param_convergence}.
Similar to the privacy analysis of $\DPSG$ based on PABI, we present the privacy guarantees for algorithms that use public samples in Appendix~\ref{subsec:appendix_algo_pub_data_privacy}. 




\subsection{Empirical Excess Risk Comparison}
\label{subsec:comparison}

We fix the number of gradient steps in all three algorithms: $K$ epochs, each with $n$ gradient steps. 
Let $p$ denote the fraction of gradient steps computed using private samples. 
Therefore, in $\privpub$, $S = pK$; in $\pubpriv$, $K-S = pK$; and in \interleaved, $n_d = p n$.
For simplicity, we assume both $pK$ and $pn$ are integers, and restrict $p$ to $[\frac{1}{K}, 1]$. 
\begin{table}[!htp]
    \centering
\begin{adjustbox}{width=0.5\textwidth}
    \begin{tabular}{|c|c|}
    \hline
        Algorithm & Empirical Excess Risk \\
    \hline
        {\small \privpub} & $\widetilde{\gO}\left( 
         \left(\frac{p}{n}\right)^{2/3} \left(\frac{\sqrt{d}}{\eps} \right)^{4/3}
         + \frac{(C_n^{\text{full}})^2}{n^2 \beta}
         \right)$ \\
    \hline
        {\small \pubpriv} & 
        $\widetilde{\gO}\left( 
         \left( \frac{p}{n} \right)^{2/3} \left(\frac{\sqrt{d}}{\eps} \right)^{4/3}
         + \frac{(C_n^{\text{full}})^2}{n^2 \beta}
         \right)$ \\
    \hline
        {\small \interleaved} & $\widetilde{\gO}\left( 
        \left(\frac{1}{n[(1-p)n + 1]}\right)^{2/3}
        \left( \frac{\sqrt{d}}{\eps} \right)^{4/3}
        + \frac{(C_n^{\text{part}})^2}{n^2\beta}
        \right)$\\
    \hline
    \end{tabular}
\end{adjustbox}
    \caption{
    Empirical excess risk
    in terms of dataset size $n$, model dimension $d$, privacy parameter $\eps$, the fraction of gradient steps that use private samples $p\in [\frac{1}{K},1]$, and the 
    dissimilarity measures $C_n^{\text{full}}$ and $C_n^{\text{part}}$, defined in \Cref{subsec:pub_data_algos}.
    The notation $\widetilde{\gO}$ suppresses logarithmic factors.
    }
    % \vspace{-6pt}
    \label{tab:emp_risk_comp_main}
\end{table}


We ensure all the algorithms satisfy the same $(\eps, \delta)$-DP guarantee, and compare their empirical excess risk bounds in Table~\ref{tab:emp_risk_comp_main}.
Detailed derivations along with the choice of the noise variance, learning rate $\eta$, and number of epochs $K$, are provided in Appendix~\ref{subsec:appendix_algo_pub_data_risk}. The bounds in Table~\ref{tab:emp_risk_comp_main} illustrate a trade-off when using public samples. The first term, which reflects the cost of privacy, is reduced compared to $\DPSG$ (since $p \leq 1$). However, due to the dissimilarity between the public and private datasets, we get an additional non-vanishing term.




First, $\interleaved$ reduces the privacy-related (first) term more aggressively than the other two schemes when 
$$\Big(\frac{1}{ n[(1-p)n + 1]}\Big)^{2/3} \leq \left(\frac{p}{n}\right)^{2/3},$$
which holds for $p \geq 1/n$.
This improvement, shown in Appendix~\ref{subsec:appendix_algo_pub_data_privacy}, results from the more effective use of PABI within each epoch, which causes a reduction in privacy loss by a factor of $\frac{1}{n+1-n_d}$. On the other hand, the privacy loss bounds for $\privpub$ and $\pubpriv$ remain independent of $n$.



To compare the second terms in \Cref{tab:emp_risk_comp_main}, recall that $C_n^{\text{full}}$ and $C_n^{\text{part}}$ measure the dissimilarity when, respectively, all or a part of the $n$ gradient steps in an epoch are computed using samples from the public dataset $\gP$.
Clearly $C_n^{\text{part}} \leq C_n^{\text{full}}$, hence, the dissimilarity term for $\interleaved$ is lower compared to the other two schemes.


% \textbf{Conclusion.} 
To summarize, $\interleaved$ achieves a lower empirical excess risk than the other two baselines, $\privpub$ and $\pubpriv$. 
It also reduces the privacy-related term compared to $\DPSG$, at the cost of an additional error term due to dissimilarity.




\section{Experiments}
\label{sec:exp}

\textbf{Tasks
\footnote{Code available at: \url{https://anonymous.4open.science/r/private_shuffled_G-A86F}}.
}
We consider three tasks, each associated with a distinct objective function. For every task, we describe the component function $f(\rvx; \rvq)$ on a given sample $\rvq \in \gD \cup \gP$, and the regularization function $\psi(\rvx)$. The true and the surrogate objective functions are constructed based on $f$ and $\psi$ accordingly. 
\begin{enumerate}[itemsep=0mm, leftmargin=4mm]
    \item \textbf{Mean Estimation}: $f(\rvx; \rvq) = \frac{1}{2}\| \rvx - \rvq\|^2$. $\psi(\rvx) = \gI\{\rvx \in \gB_{C}\}$, where $\gB_{C}$ is a ball of radius $C$ at the origin.
    \item \textbf{Ridge Regression}: 
    Let $\rvq = (\rva, y)$, where $\rva$ and $y$ represent the feature vector and the response, respectively.
    $f(\rvx; \rvq) = (\langle \rvx, \rva\rangle - y)^2$, $\psi(\rvx) = \frac{\lambda_{r}}{2}\|\rvx\|^2$ for $\lambda_{r} > 0$.
    \item \textbf{Lasso Logistic Regression}: 
    Let $\rvq = (\rva, y)$, for $y \in \{\pm 1\}$, represent the feature vector and label, respectively.
    $f(\rvx; \rvq) = -y \log (h(\rvx; \rva)) - (1-y) \log (h(\rvx; \rva))$, where $h(\rvx; \rva) = \frac{1}{1+\exp(-\langle \rvx, \rva \rangle)}$. $\psi(\rvx) = \lambda_{l} \|\rvx\|_1$ for $\lambda_{l} > 0$.
\end{enumerate}
\textbf{Datasets.} 
We construct a private and a public set of samples from each dataset. Each set, private or public, contains $n$ samples of dimension $d$.
The construction of private and public sets simulates real-worlds scenarios, e.g., data corruption, demographic biases. 
A summary of the datasets is presented in Table~\ref{tab:dataset_summary}. 
See Appendix~\ref{subsec:appendix_datasets} for more details.


\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
        Task & Dataset & $n$ & $d$ \\
    \hline
        Mean Estimation & \texttt{MNIST-69} & 1000 & 784\\
    \hline
        \multirow{2}{*}{Ridge Regression} & \texttt{CIFAR-10} & 1000 & 3072\\
        & \texttt{Crime} & 159 & 124\\
    \hline
        \multirow{2}{*}{\shortstack{Lasso \\Logistic Regression}} & \texttt{COMPAS} & 2013 & 11 \\
         & \texttt{CreditCard} & 200 & 21 \\
    \hline
    \end{tabular}
    \caption{A summary of datasets.}
    \label{tab:dataset_summary}
\end{table}

\vspace{-10pt}

\textbf{Baselines.} 
In our experiments, all optimization algorithms apply Random Reshuffling (RR) to private samples.
Thus, we replace ``$\SG$'' in their names with RR, resulting in \textit{Interleaved-RR}, \textit{Priv-Pub-RR}, \textit{Pub-Priv-RR} and \textit{DP-RR}.
And we include one additional baseline: \textit{Public Only}, which uses only the public dataset without noise injection.



\textbf{Hyperparameters.} 
In algorithms that use public samples, we set percentage of private sample usage as $p=0.5$. 
We set regularization parameters as $C=10, \lambda_{r}=0.1$, $\lambda_{l}=0.1$.
The number of epochs is $K = 50$.
To ensure the Lipschitz continuity of the objectives, we apply gradient clipping with a norm of 10. The privacy parameters are $\delta = 10^{-6}$, with $\eps \in \{5, 10\}$ in mean estimation and lasso logistic regression, and $\eps \in \{1, 5\}$ in ridge regression. 
We perform a grid search on the learning rate $\eta \in \{0.1, 0.5, 0.01, \dots, 5e-9, 1e-9\}$.
Each experiment is repeated for 10 runs.


\textbf{Results.} 
All results are presented in Figure~\ref{fig:exp_res_main}. Each color represents one specific optimization algorithm. The solid lines indicate the mean performance across 10 runs, while the shaded regions denote one standard deviation.
Additional results of using other variants of $\SG$ to private samples and varying $p$ can be found in Appendix~\ref{subsec:appendix_more_results}.




\textbf{Discussion.}
Optimizing solely on the public dataset often leads to suboptimal solutions when the private and public datasets have slight distributional differences, as shown by the green curves. Conversely, relying only on the private dataset (i.e., $\DPSG$) is also suboptimal in high-privacy regimes, as shown by the blue curve, where excessive noise addition slows convergence. 
This is evident across all plots, except for \texttt{CIFAR-10} at $\epsilon = 5$, where the exception arises because larger $\eps$ values require less noise and hence, and the benefits of incorporating public data are reduced.
Moreover, in regimes with a smaller $\eps$, $\interleaved$
consistently outperforms the baselines, as shown by the red curves. 
This is consistent with our theoretical findings.




\begin{figure}[H]
    \includegraphics[width=0.48\linewidth]{res_plots/mean_est/mean_est_mnist69_K_50_eps_5_nexp_10_m_RR_p_0.5.pdf}
    \includegraphics[width=0.48\linewidth]{res_plots/mean_est/mean_est_mnist69_K_50_eps_10_nexp_10_m_RR_p_0.5.pdf}\\
    \includegraphics[width=0.48\linewidth]{res_plots/lin_reg/lin_reg_cifar10_K_50_eps_1_nexp_10_m_RR_p_0.5.pdf}
    \includegraphics[width=0.48\linewidth]{res_plots/lin_reg/lin_reg_cifar10_K_50_eps_5_nexp_10_m_RR_p_0.5.pdf}\\
    \includegraphics[width=0.48\linewidth]{res_plots/lin_reg/lin_reg_crime_corrupted2_K_50_eps_1_nexp_10_m_RR_p_0.5.pdf}
    \includegraphics[width=0.48\linewidth]{res_plots/lin_reg/lin_reg_crime_corrupted2_K_50_eps_5_nexp_10_m_RR_p_0.5.pdf}\\
    \includegraphics[width=0.48\linewidth]{res_plots/log_reg/log_reg_compas_K_50_eps_5_nexp_10_m_RR_p_0.5.pdf}
    \includegraphics[width=0.48\linewidth]{res_plots/log_reg/log_reg_compas_K_50_eps_10_nexp_10_m_RR_p_0.5.pdf}\\
    \includegraphics[width=0.48\linewidth]{res_plots/log_reg/log_reg_default_K_50_eps_5_nexp_10_m_RR_p_0.5.pdf}
    \includegraphics[width=0.48\linewidth]{res_plots/log_reg/log_reg_default_K_50_eps_10_nexp_10_m_RR_p_0.5.pdf}\\
    \includegraphics[width=0.9\linewidth]{res_plots/RR_legend.pdf}
    \caption{Results on each dataset across different tasks. Each algorithm runs for $K=50$ epochs, with privacy loss $\eps \in \{1, 5, 10\}$ and $\delta=10^{-6}$.  
   The solid lines represent the mean performance, while the shaded regions denote one std. across 10 random runs.
   }
    \label{fig:exp_res_main}
    % \vspace{-5pt}
\end{figure}





\section{Conclusion}
\label{sec:conclusion}


We study private convex ERM problems solved via shuffled gradient methods ($\DPSG$) and provide the first empirical excess risk bound, which is larger than the lower bound.
To reduce this risk, we incorporate public samples, and propose $\interleaved$, which interleaves the usage of private and public samples during training.
We demonstrate its superior performance compared to $\DPSG$ and other baselines, theoretically and empirically.







\clearpage
\section*{Impact Statement}




This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


\nocite{langley00}

\bibliography{mybib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix}
\input{appendix_2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


