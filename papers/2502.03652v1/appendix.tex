\section{More about Related Work}
\label{sec:appendix_related_work}


\textbf{Private Optimization. }
The privacy loss and convergence of DP-SGD is well understood~\cite{Abadi2016dpsgd}, including tight upper and lower bounds for solving private empirical risk minimization problems in convex settings \cite{bassily2014private_erm}.
However, recent work has observed the gap between theory and practice: shuffled gradient methods are widely implemented in codebases, while the amount of noise applied to the gradients to ensure privacy guarantees is computed based on the analysis of DP-SGD \cite{chua2024how_private_dp_sgd, chua2024scalable_dp}.
This line of work, however, focuses on the privacy analysis only, and there is no unified analyses that consider both optimization and privacy.

\textbf{Shuffled Gradient Methods in the Non-Private Setting.}
While the convergence rate of SGD in non-private settings is well understood~\cite{Shamir2013sgd}, understanding the convergence of shuffled gradient methods, particularly Random Reshuffling (RR), has been a more recent development. Significant advances include characterizing the convergence rate of RR~\cite{mishchenko2021rr, mishchenko2021prox_fed_rr} and establishing last-iterate convergence results for shuffled gradient methods in general~\cite{liu2024last_iterate_shuffled_gradient}.
It is known that the best convergence rate by SGD in the non-private setting is $\gO\left(\frac{1}{\sqrt{T}}\right)$ for $T$ gradient steps, while~\cite{liu2024last_iterate_shuffled_gradient} shows that the convergence of shuffled gradient methods is $\gO\left(\frac{1}{K^{2/3}}\right)$, where $K = T / n$ is the number of epochs, each consisting of $n$ gradient steps based on $n$ samples. The results suggest that shuffled gradient methods converge faster than SGD in the non-private setting. However, it is unclear how their performances compare in the private setting, and we address this gap in this work.


\textbf{Privacy Amplification by Iteration (PABI). }
In many applications, only the last iterate model parameter is used during inference, while intermediate model parameters generated during training are discarded. However, the common privacy analyses based on composition of privacy loss per gradient step implicitly assumes that all intermediate model parameters are released. This discrepancy has motivated a line of research investigating the privacy loss of releasing only the last iterate model parameter while hiding all intermediate model parameters~\cite{Feldman2018privacy_amp_iter, altschuler2022apple_paper, ye2022singapore_paper} and the privacy amplification that arises by hiding intermediate model parameters is referred to as privacy amplification by iteration (PABI). 

Most existing works on PABI, however, focus exclusively on privacy guarantees without exploring their implications for convergence. One exception is the seminal work~\cite{Feldman2018privacy_amp_iter}, which applies PABI to the convergence bound of solving private stochastic convex optimization (SCO) problems.
Their convergence analysis relies on existing average-iterate bounds (see Theorem 28 of~\cite{Feldman2018privacy_amp_iter}), where the output of the optimization algorithm is averaged across all intermediate model parameters. This violate the assumption in PABI, where only the last iterate model parameter is released. 
To bridge this gap, \cite{Feldman2018privacy_amp_iter} analyzes impractical variants of DP-SGD, such that the optimization algorithm skips a random number of gradient steps on the first few samples from the training dataset (Skip-PNSGD) or that the algorithm terminates after a randomly chosen number of gradient steps (Stop-PNSGD). 
In addition, while they briefly mention the use of public data in private optimization, they do not address more realistic scenarios where public and private datasets follow different distributions.

In the follow-up work,~\cite{altschuler2022apple_paper} shows that DP-SGD applied to convex, smooth, and Lipschitz objectives with a bounded domain $\gW$ incurs a finite privacy loss, rather an infinite privacy loss as privacy composition indicates. However, their analysis critically depends on privacy amplification by subsampling, specific to DP-SGD, and and the assumption that all model parameters remain within $\gW$ at every gradient step. 
Specifically, the update sequence analyzed is $\rvx_{t+1} = \text{Proj}_{\gW}\left( \rvx_{t} - \eta (\nabla f_i(\rvx_t) + \rho) \right)$, $\forall t\in [T]$, where $\rvx_t$ is the model parameter at $t$-th gradient step, $i \in [n]$ is the index of the sample used for gradient computation, $\rho$ is the Gaussian noise vector and $\text{Proj}$ is the projection operator, or a special case of regularization. 
This update ensures $\rvx_t \in \gW$, $\forall t\in [T]$, a key condition for applying their privacy bound.
However, in shuffled gradient methods, 
if regularization (e.g., projection) is applied after each gradient step, 
one would no longer approximate the full gradient after an epoch to ensure convergence to the target objective and hence, it is crucial to apply the regularization only at the end of every epoch~\cite{mishchenko2021prox_fed_rr}.
This implies that in shuffled gradient methods, we cannot guarantee $\rvx_t\in \gW$ for every gradient step.
These differences make the results of~\cite{altschuler2022apple_paper} inapplicable to private shuffled gradient methods.
Another work~\cite{ye2022singapore_paper} shows that in a more restricted setting where the objective function is strongly convex, even without a bounded domain, hiding intermediate model parameters leads to a finite privacy loss. 



\textbf{Public Data Assisted Private Learning. }
There is a long line of work on using public samples to improve statistical learning tasks, e.g.,~\cite{bassily20priv_query_release_pub_data, block2024oracleefficient_dp_learning, ullah2024public_data_priv_sco}.
In machine learning, public data is commonly used to improve model performance by either identifying gradient subspaces~\cite{zhou2021bypassing, kairouz2021priv_erm_pub_subspace} or through public pre-training~\cite{yu2022dp_fine_tune_lm, bu2023dp_fine_tune_fm}.
Empirical studies have also explored the use of public samples in DP-SGD to solve ERM problems~\cite{Wang2020dp_small_pub_data}.
Limited attention has been given to addressing distribution shifts between private and public datasets in statistical learning tasks~\cite{bie2022private_est_pub_data_shift, Bassily2023priv_adp_from_pub_source}.
None of these works investigate the use of public samples in the context of private shuffled gradient methods for solving ERM or tackle distributional differences between public and private datasets in this specific setting.

\textbf{Optimization on a Surrogate Objective.}
The use of surrogate objectives in optimization is studied in the non-private setting in \cite{Woodworth2023two_losses},
which analyzes SGD using average-iterate methods.
However, no prior work has investigated the use of surrogate objectives in the context of shuffled gradient methods.






\section{Preliminaries}
\label{sec:appendix_prelim}


\subsection{Differential Privacy}
\label{subsec:appendix_dp_related}

We begin by defining standard $(\eps,\delta)$-differential privacy (DP) and RÃ©nyi Differential Privacy (RDP), the conversion between these two definitions and the composition theorem. 

\begin{definition}[Differential Privacy (DP)~\cite{dwork2014algorithmic}]
\label{def:appendix_DP}
    A randomized mechanism $\gM: \gW \rightarrow \gR$ with a domain $\gW$ and range $\gR$ satisfies $(\eps, \delta)$-differential privacy for $\eps \geq 0, \delta \in (0, 1) $, if for any two {\bf adjacent datasets} $\gD, \gD'$ 
    and for any subset of outputs $S \subseteq \gR$ it holds that 
    \begin{align*}
        \Pr[\gM(\gD) \in S] \leq e^{\eps} \Pr[\gM(\gD') \in S] + \delta
    \end{align*}
\end{definition}
Here, $\eps$ and $\delta$ are often referred to as the privacy loss of the algorithm $\gM$.

\begin{definition}[Renyi Divergence]
\label{def:renyi_divergence}
    For two probability distributions $P$ and $Q$ defined over $\gR$, the Renyi divergence of order $\alpha > 1$ is
    $\infdivalpha{P}{Q} := \frac{1}{\alpha-1}\log \E_{x \sim Q} \Big(\frac{P(x)}{Q(x)}\Big)^{\alpha}$.
\end{definition}


\begin{definition}[$(\alpha, \eps)$-Renyi Differential Privacy (RDP)~\cite{Mironov2017rdp}]
\label{def:RDP}
    A randomized mechanism $f: \gD\rightarrow \gR$ is said to have $\eps$-Renyi differential privacy of order $\alpha$, or $(\alpha, \eps)$-RDP for short, if for any adjacent $D, D' \in \gD$, it holds that $\infdivalpha{f(D)}{f(D')} \leq \eps$.
\end{definition}

% RDP => DP
\begin{proposition}[From RDP to DP (Proposition 3 of~\cite{Mironov2017rdp})]
\label{prop:rdp_to_dp}
    If $f$ is an $(\alpha, \eps)$-RDP mechanism, it also satisfies $(\eps + \frac{\log 1/\delta}{\alpha - 1}, \delta)$-DP for any $0 < \delta < 1$. 
\end{proposition}

% RDP composition
\begin{proposition}[RDP Composition (Proposition 1 of~\cite{Mironov2017rdp})]
\label{prop:rdp_composition}
    Let $f: \gD \rightarrow \gR_1$ be $(\alpha, \eps_1)$-RDP and $g: \gR_1 \times \gD \rightarrow \gR_2$ be $(\alpha, \eps_2)$-RDP, then the mechanism defined as $(X, Y)$, where $X \sim f(D)$ and $Y \sim g(X, D)$, satisfies $(\alpha, \eps_1 + \eps_2)$-RDP. 
\end{proposition}


\subsection{Privacy Amplification By Iteration (PABI)}
\label{subsec:appendix_pabi_related}

We use PABI in the privacy analysis for improved privacy-convergence trade-offs.
At a high level, the privacy amplification arises due to hiding intermediate parameters and only release the last-iterate parameter in an optimization procedure.
Our analysis builds on the results of PABI in~\cite{Feldman2018privacy_amp_iter}. We begin by introducing the concept of contractive noisy iterations, the key setting where PABI applies, and how the optimization steps in private shuffled gradient methods fall under this setting.

% Contraction
\begin{definition}[Contraction (Definition 16 of~\cite{Feldman2018privacy_amp_iter})]
\label{def:contraction}
    For a Banach space $(\gZ, \|\cdot \|)$ A function $g: \gZ \rightarrow \gZ$ is said to be contractive if it is 1-Lipschitz, i.e., $\forall \rvx, \rvy \in \gZ$, $\|g(\rvx) - g(\rvy)\| \leq \| \rvx - \rvy \|$.
\end{definition}

\begin{remark}
\label{remark:contractive_operators}
    As shown in~\cite{Feldman2018privacy_amp_iter}, taking one gradient step of a convex and $L$-smooth objective $f$, i.e., $g(\rvx) = \rvx - \eta \nabla_{\rvx} f(\rvx)$, where the learning rate $\eta \leq 2/L$, is contractive. 
\end{remark}

% Contractive noisy iteration
\begin{definition}[Contractive Noisy Iteration (Definition 19 of~\cite{Feldman2018privacy_amp_iter})]
\label{def:cni}
    Given a random initial state $X_0 \in \gZ$, a sequence of contractive functions $g_t: \gZ \rightarrow \gZ$, and a sequence of noise distribution $\{\rho_t\}_{t=1}^{T}$, the contractive noisy iteration (CNI) is defined by the update rule: $X_{t+1} = g_{t+1}(X_t) + Z_{t+1}$,
    where $Z_{t+1}$, $\forall t\in [T]$, is drawn independently from $\rho_{t+1}$. The random variable output by this process after $T$ steps is denoted as $CNI(X_0, \{g_t\}_{t=1}^{T}, \{\rho_t\}_{t=1}^{T})$.  
\end{definition}

% PABI bound
\begin{theorem}[Privacy Amplification by Iteration (Theorem 22 of~\cite{Feldman2018privacy_amp_iter} with Gaussian Noise)]
\label{thm:pabi}
    Let $X_T$ and $X_{T}'$ denote the output of $CNI_T(X_0, \{g_t\}_{t=1}^{T}, \{\rho_t\}_{t=1}^{T})$ and $CNI_T(X_0, \{g_t'\}_{t=1}^{T}, \{\rho_t\}_{t=1}^{T})$.
    Let $s_t := \sup_{\rvx} \|g_t(\rvx) - g_t'(\rvx)\|$, where $\rho_t \sim \gN(0, \sigma^2 \sI_d)$ for all $t$. Let $a_1, \dots, a_T$ be a sequence of reals and let $z_t := \sum_{i \leq t} s_i - \sum_{i\leq t}a_i$. If $z_t \geq 0$ for all $t$ and $z_T = 0$, then
    \begin{align*}
        \infdivalpha{X_T}{X_T'} \leq \sum_{t=1}^{T} \frac{\alpha a_t^2}{2 \sigma^2}
    \end{align*}
\end{theorem}


\section{Proof of Theorem~\ref{thm:convergence_generalized_shufflg_fm}}
\label{sec:appendix_proof_main_thm}




\textbf{Notation.}
In the proof, we denote the Bregman divergence induced by a real-valued convex function $g(\rvx): \R^d \rightarrow \R \cup \{+\infty\}$ as $B_g(\rvx, \rvy) = g(\rvx) - g(\rvy) - \langle \nabla g(\rvy), \rvx - \rvy\rangle, \forall \rvx, \rvy \in \R^d$, and $\text{dom}(g)$ denotes the domain of $g(\rvx)$. 


We show convergence with more general assumptions on the smoothness and Lipschitzness constants.
We summarize important notations used in the proof and introduce the generalized assumptions as follows:
\begin{enumerate}[itemsep=0mm]
    \item Number of epochs: $K \geq 2$

    \item $\E_{A}\left[\cdot\right]$ denotes taking the expectation w.r.t. variable $A$. When the context is clear, $A$ is omitted.

    \item The target objective function: 
    \begin{align}
    \label{eq:appendix_true_objective_def}
        &G(\rvx) = G(\rvx; \gD) = F(\rvx; \gD) + \psi(\rvx)\\
        \nonumber
        &\text{where } \gD = \{\rvd_1,\dots,\rvd_n\}, \quad 
        F(\rvx) := F(\rvx; \gD) = \frac{1}{n}\sum_{i=1}^{n} f(\rvx; \rvd_i) = \frac{1}{n}\sum_{i=1}^{n} f_i(\rvx)
    \end{align}

    \item The optimum: $\rvx^* = \argmin_{\rvx\in\R^d} G(\rvx)$.

    Note that we always care about the convergence of the target objective function, i.e., $\E\left[G(\rvx; \gD)\right] - \E\left[G(\rvx^*; \gD)\right]$
    
    \item Optimization uncertainty: $\sigma_{any}^2 = \frac{1}{n}\sum_{i=1}^{n} \|\nabla f_i(\rvx^*)\|^2$.

    

    \item 
    Objective function used in the $s$-th epoch, under permutation $\pi^{(s)} \in \Pi_n$, for $s\in [K]$:
    \begin{align}
    \label{eq:appendix_surrogate_objective_def}
        &G^{(s)}(\rvx) = G(\rvx; \gD^{(s)} \cup \gP^{(s)}) = F(\rvx; \gD^{(s)} \cup \gP^{(s)}) + \psi(\rvx)
    \end{align}
    where
    \begin{itemize}[itemsep=0mm]
        \item  $\gD^{(s)} \in \{\emptyset\} \cup \{\{\rvd_{\pi_1^{(s)}}^{(s)}, \dots, \rvd_{\pi_{n_d}^{(s)}}^{(s)}\}: 1\leq n_d^{(s)} \leq n\}$ is the private dataset used in epoch $s$, generated by first permuting $\gD$ and then taking the first $n_d^{(s)}$ samples
        \item $\gP^{(s)} \in  \{\emptyset\} \cup \{\{\rvp_1^{(s)}, \dots, \rvp_{n - n_d^{(s)}}^{(s)}\}\}$, $\gP^{(s)} \subseteq \gP$,
        is the public dataset used in epoch $s$
    \end{itemize}
    and
    \begin{align*}
        F^{(s)}(\rvx) = F(\rvx; \gD^{(s)} \cup \gP^{(s)}) 
        &
        = \frac{1}{n} \Big(
            \sum_{i=1}^{n_d^{(s)}} f(\rvx; \rvd_{\pi_i^{(s)}})
            + \sum_{i=n_d^{(s)} + 1}^{n} f(\rvx; \rvp_{i-n_d^{(s)}}^{(s)})
        \Big)\\
        &\quad = \frac{1}{n} \Big(
            \sum_{i=1}^{n_d^{(s)}} f_{\pi_i^{(s)}}^{(s, priv)}(\rvx)
            + \sum_{i=n_d^{(s)} + 1}^{n} f_{j-n_d^{(s)}}^{(s, pub)}(\rvx)
        \Big)
    \end{align*}
    


    \item The objective difference for epoch $s\in [K]$: 
    \begin{align}
        H^{(s)}(\rvx) 
        &= G(\rvx;\gD) - G(\rvx;\gD^{(s)} \cup \gP^{(s)})
    \end{align}


    \item Smoothness:
    \begin{assumption}[Smoothness (Generalized Version of Assumption~\ref{ass:smoothness})]
    \label{ass:appendix_refined_smoothness}
    $f(\rvx; \rvd_i)$ is $L_i$-smooth, $\forall i\in [n]$ and $\rvd_i \in \gD$.
    $f(\rvx; \rvd_{\pi_i^{(s)}})$ is $\widehat{L}_{\pi_i^{(s)}}^{(s)}$-smooth, $\forall i \in [n_d^{(s)}]$. 
    $f(\rvx; \rvp_{j}^{(s)})$ is $\widetilde{L}_{j}^{(s)}$-smooth, $\forall j\in [n-n_d^{(s)}]$.
    \end{assumption}

    
    \item The average smoothness constant 
    \begin{enumerate}[itemsep=0mm]
        \item of the target objective: $L = \frac{1}{n}\sum_{i=1}^{n} L_i$.
        \item of the objective used in the $s$-th epoch: 
        $\widehat{L}^{(s)} = \frac{1}{n}\Big( \sum_{i=1}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)} + \sum_{j=1}^{n-n_d^{(s)}} \widetilde{L}_{j}^{(s)} \Big)$.
    \end{enumerate}

    \item The maximum smoothness constant
    \begin{enumerate}[itemsep=0mm]
        \item of the target objective: $L^* = \max_{i\in[n]}\{L_i\}$.
        \item of the objective used in the $s$-th epoch: $\widehat{L}^{(s)*} = \max\{
        \{\widehat{L}_{\pi_i^{(s)}}^{(s)}\}_{i=1}^{n_d^{(s)}} \cup
        \{\widetilde{L}_{i}\}_{i=1}^{n-n_d^{(s)}} \}$
    \end{enumerate}

    \item The maximum average smoothness constant: $\bar{L}^{*} = \max\{L, \max_{s\in [K]}\widehat{L}^{(s)}\}$.

    \item Lipschitzness (only needed for privacy analysis):
    \begin{assumption}[Lipschitz Continuous (Generalized Version of Assumption~\ref{ass:lipschitzness}]
    \label{ass:appendix_refined_lipschitzness}
        $f(\rvx, \rvd_i)$ is $G_i$-Lipschitz continuous, $\forall i\in [n]$ and $\rvd_i\in \gD$. $f(\rvx; \rvd_{\pi_i^{(s)}})$ is $\widehat{G}_{\pi_i^{(s)}}^{(s)}$-Lipschitz continuous, $\forall i\in [n_d^{(s)}]$. $f(\rvx; \rvp_j^{(s)})$ is $\widetilde{G}_j^{(s)}$-Lipschitz continuous, $\forall j\in [n-n_d^{(s)}]$. 
    \end{assumption}


    \item The maximum Lipschitz parameter:
        \begin{enumerate}[itemsep=0mm]
            \item of the target objective: $G^{*} = \max_{i\in [n]}\{G_i\}$
            \item of the objective used in the $s$-th epoch: $G^{(s)*} = \max\{ \{ \widehat{G}_{\pi_i^{(s)}}^{(s)}\}_{i=1}^{n_d^{(s)}} \cup \{\widetilde{G}_{i}^{(s)}\}_{i=1}^{n-n_d^{(s)}} \}$
        \end{enumerate}

    \item Gaussian noise applied to the gradient at the $i$-th step in epoch $s$, for $i \in [n], s\in [K]$: $\rho_i^{(s)} \sim \gN(0, (\sigma^{(s)})^2\sI_d)$

\end{enumerate}

\textbf{Roadmap.} We begin by presenting useful lemmas used in the convergence proof in section~\ref{subsec:appendix_useful_lemmas}.
After that, we show the one epoch convergence section~\ref{subsec:appendix_one_epoch_convergence} and the expected one epoch convergence, taking into account the randomness due to data shuffling and noise injection, in section~\ref{subsec:appendix_expected_one_epoch_convergence}.
Finally, we give show the convergence bound across $K$ epochs in section~\ref{subsec:appendix_k_epoch_convergence}.

\subsection{Useful Lemmas}
\label{subsec:appendix_useful_lemmas}

\begin{lemma}[Stein's Lemma]
\label{lemma:steins_lemma}
    For a zero-mean isotropic Gaussian random variable $\rho \sim \gN(0, \sigma^2\sI_d)$, and a differentiable function $h: \R^{d} \rightarrow \R^d$, the following holds:
    \begin{align*}
        \E\left[ \langle \rho, h(\rho) \rangle \right]
        = \sigma^2 \E\left[\text{tr}(\nabla_{\rho} h(\rho))\right]
    \end{align*}
    where $\nabla h(\rho)$ is the Jacobian matrix of $h(\rho)$ and $\text{tr}(\cdot)$ denotes the trace operator.
\end{lemma}


\begin{lemma}[Lemma 3.6 of~\cite{liu2024last_iterate_shuffled_gradient}]
\label{lemma:breg_div_ub_lb}
    Given a convex and differentiable function $g(\rvx): \R^d \rightarrow \R$ satisfying 
    $\| \nabla g(\rvx) - \nabla g(\rvy) \| \leq L \|\rvx - \rvy\|$, $\forall \rvx, \rvy \in \R^d$ for some $L > 0$, then $\forall \rvx, \rvy \in \R^d$,
    \begin{align*}
        \frac{\| \nabla g(\rvx) - \nabla g(\rvy) \|^2}{2 L} \leq B_g(\rvx, \rvy) \leq \frac{L}{2}\|\rvx - \rvy\|^2
    \end{align*}
\end{lemma}

% optimization noise / uncertainty
\begin{lemma}[Lemma E.1 of~\cite{liu2024last_iterate_shuffled_gradient}]
\label{lemma:opt_noise_bound}
    Under Assumption~\ref{ass:appendix_refined_smoothness}, for any permutation $\pi$ of $[n]$,
    \begin{align*}
        \frac{1}{n}\sum_{i=2}^{n} L_i \left\| \sum_{j=1}^{i-1} \nabla f_j(\rvx^*) \right\|^2 \leq n^2 L \sigma_{any}^2,
    \end{align*}
    where $L = \frac{1}{n}\sum_{i=1}^{n} L_i$. 
\end{lemma}


\begin{lemma}[Extension of Lemma 6.2 of~\cite{liu2024last_iterate_shuffled_gradient}]
\label{lemma:bregmandiv_relationship}
    Given two sequences of reals: $d^{(1)}, d^{(2)}, \dots, d^{(K)}, d^{(K+1)}$ and $e^{(1)}, e^{(2)}, \dots, e^{(K)}$, suppose there exist positive constants $a, b, c$ satisfying
    \begin{align}
    \label{eq:breg_div_ub}
        d^{(k+1)} \leq \frac{a}{k} + b(1+\log k) + c\sum_{l=2}^{k} \frac{d^{(l)}}{k-l+2} + \sum_{l=1}^{k} \frac{e^{(l)}}{k-l+1}, \quad \forall k\in [K]
    \end{align}
    then the following inequality holds
    \begin{align}
    \label{eq:breg_div_rel}
        d^{(k+1)} \leq \Big(\frac{a}{k} + b(1+\log k) + M \Big)\sum_{i=0}^{k-1} (2c(1+\log k))^{i}
    \end{align}
    where $M := \max_{k\in [K]}\sum_{l=1}^{k} \frac{e^{(l)}}{k-l+1}$.
\end{lemma}

\begin{proof}
    We use induction to show Eq.~\ref{eq:breg_div_rel}. 
    
    Base Case: for $k = 1$, 
    by Eq.~\ref{eq:breg_div_ub} and the definition of $M$,
    $d^{(2)}\leq a+b + e^{(1)} \leq a + b + M$, which also satisfies Eq.~\ref{eq:breg_div_rel}.

    Induction Hypothesis: suppose Eq.~\ref{eq:breg_div_rel} holds for 1 to $k-1$ (where $2 \leq k \leq K$), i.e., 
    \begin{align}
        d^{(l)} \leq \Big(\frac{a}{l-1} + b(1+\log (l-1)) + M \Big)\sum_{i=0}^{l-2} (2c(1+\log (l-1)))^{i}
    \end{align}
    which implies
    \begin{align}
        d^{(l)} \leq \Big(\frac{a}{l-1} + b(1+\log k) + M \Big)\sum_{i=0}^{l-2} (2c(1+\log k))^{i}
    \end{align}

    Now for $d^{(k+1)}$, by Eq.~\ref{eq:breg_div_ub},
    \begin{align}
        d^{(k+1)} &\leq \frac{a}{k} + b(1+\log k) + c\sum_{l=2}^{k} \frac{d^{(l)}}{k-l+2} + \sum_{l=1}^{k} \frac{e^{(l)}}{k-l+1}\\
        &\leq \frac{a}{k} + b(1+\log k) + c\sum_{l=2}^{k} \frac{d^{(l)}}{k-l+2} + M\\
    \label{eq:breg_div_rel_interm1}
        &\leq \frac{a}{k} + ac \sum_{l=2}^{k} \sum_{i=0}^{i-2} \frac{(2c(1+\log k))^i}{(k-l+2) (l-1)} \\
        \nonumber
        &\quad + \Big( b(1+\log k) + M \Big) \Big(1 + c \sum_{l=2}^{k} \sum_{i=0}^{l-2} \frac{(2c(1+\log k))^i}{k-l+2} \Big)
    \end{align}

    Note that
    \begin{align}
        c \sum_{l=2}^{k} \sum_{i=0}^{i-2} \frac{(2c(1+\log k))^i}{(k-l+2) (l-1)}
        &= c \sum_{i=0}^{k-2} (2c(1+\log k))^i \Big(\sum_{l=2+i}^{k} \frac{1}{(k-l+2)(l-1)} \Big)\\
        &= \frac{c}{k+1}\sum_{i=0}^{k-2} (2c(1+\log k))^{i} \Big(
        \sum_{l=2+i}^{k} \frac{1}{k-l+2} + \frac{1}{l-1}
        \Big)\\
        &\leq \frac{c}{k+1}\sum_{i=0}^{k-2} (2c(1+\log k))^{i}
        \sum_{l=1}^{k} \frac{2}{l}\\
        &\leq \frac{\sum_{i=0}^{k-2} (2c(1+\log k))^{i+1}}{k+1}\\
        &\leq \frac{\sum_{i=0}^{k-1} (2c(1+\log k))^i}{k+1}\\
    \label{eq:breg_div_rel_interm2}
        &\leq \frac{\sum_{i=1}^{k-1} (2c (1+\log k))^i}{k}
    \end{align}
    and
    \begin{align}
        c \sum_{l=2}^{k}\sum_{i=0}^{l-2} \frac{(2c(1+\log k))^i}{k-l+2}
        &= c \sum_{i=0}^{k-2} (2c(1+\log k))^i
        \sum_{l=2+i}^{k} \frac{1}{k-l+2}
        \leq c \sum_{i=0}^{k-2} (2c(1+\log k))^i
        \sum_{l=1}^{k} \frac{1}{l}\\
        &\leq c(1+\log k)\sum_{i=0}^{k-2} (2c(1+\log k))^{i} 
        \leq \sum_{i=0}^{k-2} (2c(1+\log k))^{i+1}\\
    \label{eq:breg_div_rel_interm3}
        &\leq \sum_{i=1}^{k-1} (2c(1+\log k))^{i}
    \end{align}

    Combining Eq.~\ref{eq:breg_div_rel_interm1}, Eq.~\ref{eq:breg_div_rel_interm2} and Eq.~\ref{eq:breg_div_rel_interm3}, 
    \begin{align}
        d^{(k+1)}
        &\leq \frac{a}{k} + \frac{a}{k} \sum_{i=1}^{k-1}(2c(1+\log k))^{i}
        + \Big(b (1+\log k) + M \Big)\Big( 1 + \sum_{i=1}^{k-1}(2c(1+\log k))^{i} \Big)\\
        &= \Big( \frac{a}{k} + b(1+\log k) + M \Big) \sum_{i=0}^{k-1} (2c(1+\log k))^{i}
    \end{align}
    which finishes the induction.
    
\end{proof}



\subsection{One Epoch Convergence}
\label{subsec:appendix_one_epoch_convergence}
The following lemma  is a generalization of Lemma D.1 of~\cite{liu2024last_iterate_shuffled_gradient} from two dimensions: allowing the usage of surrogate objectives and adding additional noise for privacy preservation. 

% one epoch convergnece: bregman div + 3 point identity
\begin{lemma}
\label{lemma:one_epoch_bg_3pid}
    Under Assumptions~\ref{ass:convexity} and~\ref{ass:reg},
    for any epoch $s\in [K]$, permutation $\pi^{(s)}$ and $\rvz\in \R^d$, Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees
    \begin{align}
        &G(\rvx_1^{(s+1)}) - G(\rvz)\\
        \nonumber
        &\leq H^{(s)}(\rvx_1^{(s+1)}) - H^{(s)}(\rvz)
        + \frac{\|\rvz - \rvx_1^{(s)}\|^2}{2n \eta} - (\frac{1}{2n \eta} + \frac{\mu_\psi}{2}) \|\rvz - \rvx_1^{(s+1)}\|^2
        - \frac{1}{2n \eta}\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2\\
        \nonumber
        &\quad + \frac{1}{n} \Big(\sum_{i=1}^{n_d^{(s)}}\Big(
            B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)}) - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)})
        \Big)
        + \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)\Big)\\
        \nonumber
        &\quad + \frac{1}{n}\sum_{i=1}^{n}\langle -\rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle. 
    \end{align}
\end{lemma}



\begin{proof}[Proof of Lemma~\ref{lemma:one_epoch_bg_3pid}]
    It suffices to only consider $\rvx \in \text{dom}(\psi)$. 

    Let $\rvg^{(s)} = \sum_{i=1}^{n_d^{(s)}}\Big( \nabla f_{\pi_i^{(s)}}^{(s, priv)} (\rvx_i^{(s)}) + \rho_i^{(s)}\Big)
    + \sum_{i=n_d^{(s)}+1}^{n} \Big( \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_{i}^{(s)}) + \rho_i^{(s)}\Big)$.

    According to the update rule in \Cref{alg:generalized_shuffled_gradient_fm}, $\rvx_{n+1}^{(s)} = \rvx_1^{(s)} - \eta \cdot \rvg^{(s)}$. Observe that
    \begin{align*}
        \rvx_1^{(s+1)} 
        &= \argmin_{\rvx \in \R^d} \Big\{ n \psi(\rvx) + \frac{\| \rvx - \rvx_{n+1}^{(s)}\|^2}{2\eta} \Big\}
        = \argmin_{\rvx \in \R^d} \Big\{ n\psi(\rvx) + \frac{\| \rvx - \rvx_1^{(s)} + \eta \cdot \rvg^{(s)}\|^2}{2 \eta} \Big\}\\
        &= \argmin_{\rvx\in\R^d}\Big\{ n \psi(\rvx) + \frac{\|\rvx - \rvx_1^{(s)}\|^2 + \eta^2 \|\rvg^{(s)}\|^2 + 2 \langle \rvx - \rvx_1^{(s)}, \eta \rvg^{(s)} \rangle}{2 \eta}
        \Big\}\\
        &= \argmin_{\rvx \in \R^d}\Big\{ n \psi(\rvx) + \frac{\|\rvx - \rvx_1^{(s)}\|^2}{2\eta} + \langle \rvx - \rvx_1^{(s)}, \rvg^{(s)}\rangle
        \Big\}
    \end{align*}

    By the first-order optimality condition, there exists some vector $\nabla \psi(\rvx_1^{(s+1)})$ 
    in the subgradient of $\psi(\rvx_1^{(s+1)})$ such that
    \begin{align*}
        n \nabla \psi (\rvx_1^{(s+1)}) + \rvg^{(s)} + \frac{\rvx_1^{(s+1)} - \rvx_1^{(s)}}{\eta} = \mathbf{0}
        \iff \rvg^{(s)} = -n \nabla \psi(\rvx_1^{(s+1)}) + \frac{\rvx_1^{(s)} - \rvx_1^{(s+1)}}{\eta}
    \end{align*}

    Therefore, for $\rvz \in \text{dom}(\psi)$,
    \begin{align}
        &\langle \rvg^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle\\
        \nonumber
        &= n \langle \nabla \psi(\rvx_1^{(s+1)}), \rvz - \rvx_1^{(s+1)}\rangle
        + \frac{1}{\eta}\langle \rvx_1^{(s)} - \rvx_1^{(s+1)}, \rvx_1^{(s+1)} - \rvz\rangle\\
        &\stackrel{\text{(a)}}{\leq} n\Big( \psi(\rvz) - \psi(\rvx_1^{(s+1)}) - \frac{\mu_\psi}{2}\|\rvz - \rvx_1^{(s+1)}\|^2\Big)
        + \frac{1}{\eta}\langle \rvx_1^{(s)} - \rvx_1^{(s+1)}, \rvx_1^{(s+1)} - \rvz\rangle\\
        &= n\Big( \psi(\rvz) - \psi(\rvx_1^{(s+1)}) - \frac{\mu_\psi}{2}\|\rvz - \rvx_1^{(s+1)}\|^2\Big)
        + \frac{1}{2\eta}\Big(\|\rvz - \rvx_1^{(s)}\|^2 - \|\rvz - \rvx_1^{(s+1)}\|^2 - \|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2 \Big)\\
    \label{eq:dot_form_1}
        &= n\Big( \psi(\rvz) - \psi(\rvx_1^{(s+1)})\Big)
        + \frac{\|\rvz - \rvx_1^{(s)}\|^2}{2\eta} - (\frac{1}{2\eta} + \frac{n\mu_\psi}{2}) \|\rvz - \rvx_1^{(s+1)}\|^2
        - \frac{1}{2\eta}\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2
    \end{align}
    where (a) is by Assumption~\ref{ass:reg} on the $\mu_\psi$-strong convexity of $\psi$.

     By the definition of $\rvg^{(s)}$,
    \begin{align}
        \langle \rvg^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle
        &= \langle \sum_{i=1}^{n_d^{(s)}}\Big( \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}) 
        + \rho_i^{(s)}\Big)
        + \sum_{i=n_d^{(s)}+1}^{n} \Big(
            \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)})
            + \rho_i^{(s)}
        \Big)
        , \rvx_1^{(s+1)} - \rvz\rangle\\
    \label{eq:dot_interm}
        &= \sum_{i=1}^{n_d^{(s)}} \langle \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}), \rvx_1^{(s+1)} - \rvz\rangle 
        + \sum_{i=n_d^{(s)}+1}^{n} \langle \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)}),
        \rvx_1^{(s+1)} - \rvz
        \rangle
        + \sum_{i=1}^{n}\langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle
    \end{align}

    Since for $i \leq n_d^{(s)}$,
    \begin{align*}
        &B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)}) 
        = f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_1^{(s+1)}) - f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}) - \langle \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}), \rvx_1^{(s+1)} - \rvx_i^{(s)}\rangle\\
        &B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)}) 
        = f_{\pi_i^{(s)}}^{(s, priv)}(\rvz) - f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}) - \langle \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}), \rvz - \rvx_i^{(s)}\rangle
    \end{align*}
    and for $n_d^{(s)} < i \leq n$,
    \begin{align*}
        &B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
        = f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_1^{(s+1)}) 
        - f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)})
        - \langle \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)}), \rvx_1^{(s+1)} - \rvx_i^{(s)}\rangle\\
        &B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)}) = f_{i-n_d^{(s)}}^{(s, pub)}(\rvz) - f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)})
        - \langle \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)}),
        \rvz - \rvx_i^{(s)} \rangle
    \end{align*}
    there is for $i \leq n_d^{(s)}$,
    \begin{align}
    \label{eq:dot_to_bregmandiv_priv}
        \sum_{i=1}^{n_d^{(s)}}\langle \g f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}), \rvx_1^{(s+1)} - \rvz\rangle 
        &= \sum_{i=1}^{n_d^{(s)}}\Big(f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_1^{(s+1)}) - f_{\pi_i^{(s)}}^{(s, priv)}(\rvz) - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)}) + B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)}) \Big)
    \end{align}
    and for $n_d^{(s)} < i \leq n$,
    \begin{align}
    \label{eq:dot_to_bregmandiv_pub}
        &\sum_{i=n_d^{(s)}+1}^{n}\langle \g f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)}), 
        \rvx_1^{(s+1)} - \rvz\rangle\\
    \nonumber
        &= \sum_{i=n_d^{(s)}+1}^{n}\Big(
            f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_1^{(s+1)})
            - f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            + B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)
    \end{align}
    Therefore, summing up Eq.~\ref{eq:dot_to_bregmandiv_priv} and Eq.~\ref{eq:dot_to_bregmandiv_pub}, we have
    \begin{align}
        &\sum_{i=1}^{n_d^{(s)}} \langle \g f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}), \rvx_1^{(s+1)} - \rvz\rangle
        + \sum_{i=n_d^{(s)}+1}^{n} \langle \g
        f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)}),
        \rvx_1^{(s+1)} - \rvz\rangle\\
        \nonumber
        &= \sum_{i=1}^{n_d^{(s)}}\Big(f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_1^{(s+1)}) - f_{\pi_i^{(s)}}^{(s, priv)}(\rvz)\Big)
        + \sum_{i=n_d^{(s)}+1}^{n}\Big(
            f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_1^{(s+1)})
            - f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
        \Big)\\
        \nonumber
        &\quad - \sum_{i=1}^{n_d^{(s)}}\Big(
            B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)}) - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)})
        \Big)
        - \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)\\
    \label{eq:dot_to_bregmandiv}
        &= n F^{(s)}(\rvx_1^{(s+1)}) - n F^{(s)}(\rvz)\\
        \nonumber
        &\quad - \sum_{i=1}^{n_d^{(s)}}\Big(
            B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)}) - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)})
        \Big)
        - \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)
    \end{align}

    Hence, plugging Eq.~\ref{eq:dot_to_bregmandiv} back to Eq.~\ref{eq:dot_interm}, there is
    \begin{align}
    \label{eq:dot_form_2}
        &\langle \rvg^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle\\
        \nonumber
        &= n F^{(s)}(\rvx_1^{(s+1)}) - n F^{(s)}(\rvz)
        + \sum_{i=1}^{n}\langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle\\
        \nonumber
        &\quad - \sum_{i=1}^{n_d^{(s)}}\Big(
            B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)}) - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)})
        \Big)
        - \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)
    \end{align}

    Recall that $G(\rvx) = F(\rvx; \gD) + \psi(\rvx)$ is the target objective (see Eq.~\ref{eq:appendix_true_objective_def}) and $G^{(s)}(\rvx) = F^{(s)}(\rvx; \gD^{(s)}, \gP^{(s)}) + \psi(\rvx)$ (see Eq.~\ref{eq:appendix_surrogate_objective_def}) is the objective used in the $s$-th epoch during optimization for $s\in [K]$. 
        
    Now, by Eq.~\ref{eq:dot_form_1} and Eq.~\ref{eq:dot_form_2}, after rearranging
    \begin{align}
        G^{(s)}(\rvx_1^{(s+1)}) - G^{(s)}(\rvz)
        &\leq \frac{\|\rvz - \rvx_1^{(s)}\|^2}{2n \eta} - (\frac{1}{2n \eta} + \frac{\mu_\psi}{2}) \|\rvz - \rvx_1^{(s+1)}\|^2
        - \frac{1}{2n \eta}\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2\\
        \nonumber
        &\quad + \frac{1}{n} \sum_{i=1}^{n_d^{(s)}}\Big(
            B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)}) - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)})
        \Big)\\
        \nonumber
        &\quad + \frac{1}{n} \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big) \\
        \nonumber
        &\quad + \frac{1}{n}\sum_{i=1}^{n}\langle -\rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle
    \end{align}


    
    And following the above, for any $\rvz\in \R^d$ and $s\in [K]$, 
    \begin{align}
        G(\rvx_1^{(s+1)}) - G(\rvz)
        &=\left( G^{(s)}(\rvx_1^{(s+1)}) - G^{(s)}(\rvz) \right)
        + \left( G(\rvx_1^{(s+1)}) - G^{(s)}(\rvx_1^{(s+1)}) \right)
        -  \left( G(\rvz) - G^{(s)}(\rvz) \right)\\
        &\leq H^{(s)}(\rvx_1^{(s+1)}) - H^{(s)}(\rvz)
        + \frac{\|\rvz - \rvx_1^{(s)}\|^2}{2n \eta} - (\frac{1}{2n \eta} + \frac{\mu_\psi}{2}) \|\rvz - \rvx_1^{(s+1)}\|^2
        - \frac{1}{2n \eta}\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2\\
        \nonumber
        &\quad + \frac{1}{n} \sum_{i=1}^{n_d^{(s)}}\Big(
            B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)}) - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)})
        \Big)\\
        \nonumber
        &\quad + \frac{1}{n} \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big) \\
        \nonumber
        &\quad + \frac{1}{n}\sum_{i=1}^{n}\langle -\rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle
    \end{align}

\end{proof}


The following lemma is a generalization of Lemma D.2 of~\cite{liu2024last_iterate_shuffled_gradient} from two dimensions: allowing the usage of surrogate objectives and adding additional noise for privacy preservation.


\begin{lemma}
\label{lemma:one_epoch_bound_bregmandiv}
    Under Assumptions~\ref{ass:convexity},~\ref{ass:dissim_partial_lipschitzness} and~\ref{ass:appendix_refined_smoothness}, for any epoch $s\in [K]$, permutation $\pi^{(s)}$ and $\rvz\in\R^d$, if the learning rate $\eta \leq \frac{1}{n \sqrt{10 \widehat{L}^{(s)} \widehat{L}^{(s)*}}}$, Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees
        \begin{align}
        &\frac{1}{n} \Big(\sum_{i=1}^{n_d^{(s)}}
        \Big( B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_{1}^{(s+1)}, \rvx_i^{(s)})
        - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)}) \Big)
        + \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s,pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)
        \Big) \\
        \nonumber
        &\leq \widehat{L}^{(s)} \|\rvx_{1}^{(s+1)} - \rvx_1^{(s)}\|^2
        + 10\eta^2 n^2 \widehat{L}^{(s)} L B_F(\rvz, \rvx^*)
        \\
        \nonumber
        &
        + 5 \eta^2 \frac{1}{n} \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1}\nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2 \Big)\\
        \nonumber
        &\quad 
        + 5\eta^2 \frac{1}{n} \Big( \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2\Big)\\
        \nonumber
        &\quad + 5\eta^2 L^{(s)*} \frac{1}{n}\Big(\sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
        \\
        \nonumber
        &\quad \quad + \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2
        \Big)
    \end{align}
    
\end{lemma}



\begin{proof}[Proof of Lemma~\ref{lemma:one_epoch_bound_bregmandiv}]

    By Lemma~\ref{lemma:breg_div_ub_lb}, for $i \leq n_d^{(s)}$, and permutation $\pi_i^{(s)} \in \Pi_n$,
    \begin{align}
        &B_{f_{\pi_i^{(s)}}^{(s, priv)}} (\rvx_{1}^{(s+1)}, \rvx_i^{(s)}) \leq \frac{\widehat{L}_{\pi_i^{(s)}}^{(s)}}{2} \|\rvx_{1}^{(s+1)} - \rvx_{i}^{(s)}\|^2
        \leq \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big( \|\rvx_{1}^{(s+1)} - \rvx_1^{(s)}\|^2 + \|\rvx_i^{(s)} - \rvx_1^{(s)} \|^2 \Big)\\
        &B_{f_{\pi_i^{(s)}}^{(s, priv)}} (\rvz, \rvx_i^{(s)})
        \geq \frac{\left\|\nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}) - \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvz) \right\|^2}{2 \widehat{L}_{\pi_i^{(s)}}^{(s)}}
    \end{align}
    and for $n_d^{(s)} < i \leq n$,
    \begin{align}
        &B_{f_{i}^{(s, pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
        \leq \frac{\widetilde{L}_{i-n_d^{(s)}}^{(s)}}{2}
        \|\rvx_1^{(s+1)} - \rvx_i^{(s)}\|^2
        \leq \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big(
            \|\rvx_i^{(s+1)} - \rvx_1^{(s)}\|^2
            + \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2
        \Big)\\
        &B_{f_{i}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \geq \frac{\left\| \nabla f_{i}^{(s, pub)}(\rvx_i^{(s)}) - \nabla f_{i}^{(s, pub)}(\rvz) \right\|^2}{2\widetilde{L}_{i-n_d^{(s)}}^{(s)}}
    \end{align}
    Therefore,
    \begin{align}
        &\frac{1}{n} \Big(\sum_{i=1}^{n_d^{(s)}}
        \Big( B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_{1}^{(s+1)}, \rvx_i^{(s)})
        - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)}) \Big)
        + \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s,pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)
        \Big) \\
        \nonumber
        &\leq \frac{1}{n}\sum_{i=1}^{n_d^{(s)}}
        \Big( \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big(\|\rvx_{1}^{(s+1)} - \rvx_1^{(s)}\|^2 + \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2 \Big)
        - \frac{ \left\|\nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}) - \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvz) \right\|^2}{2 \widehat{L}_{\pi_i^{(s)}}^{(s)}}
        \Big)\\
        \nonumber
        &\quad + \frac{1}{n}\sum_{i=n_d^{(s)}+1}^{n}\Big(
            \widetilde{L}_{i-n_d^{(s)}}^{(s)} \Big( \|\rvx_1^{(s+1)} - \rvx_1^{(s)} \|^2 + \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2\Big)
        - \frac{\left\| \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)}) - \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz) \right\|^2}{2\widetilde{L}_{i-n_d^{(s)}}^{(s)}}
        \Big)\\
    \label{eq:breg_diff_interm_2}
        &= \widehat{L}^{(s)} \|\rvx_{1}^{(s+1)} - \rvx_1^{(s)}\|^2 
        + \frac{1}{n} \Big( \underbrace{\sum_{i=1}^{n_d^{(s)}}
        \widehat{L}_{\pi_i^{(s)}}^{(s)}\|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2}_{I_1}
        + \underbrace{\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2}_{I_2}
        \Big)\\
        \nonumber
        &\quad - \frac{1}{n} \Big( 
        \sum_{i=1}^{n_d^{(s)}} \frac{ \left\|\nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i) - \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvz) \right\|^2}{2 \widehat{L}_{\pi_{i}^{(s)}}^{(s)}}
        + \sum_{i=n_d^{(s)}+1}^{n} \frac{\left\| \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)}) - \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz) \right\|^2}{2\widetilde{L}_{i-n_d^{(s)}}^{(s)}}
        \Big)
    \end{align}
    where recall that $\widehat{L}^{(s)} = \frac{1}{n}\sum_{i=1}^{n} \left( \sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} + \sum_{j=1}^{n-n_d^{(s)}} \widetilde{L}_j^{(s)}  \right)$.
    Now we bound $I_1 \triangleq \sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2$ (in Part I) and $I_2 \triangleq \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2$ (in Part II) as follows:
    
    \textbf{Part I: } For $i \leq n_d^{(s)}$,
    \begin{align}
        I_1 \triangleq &\sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2
        = \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2
        = \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \eta^2 \Big\| \sum_{j=1}^{i-1} (\nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \rho_j^{(s)})  \Big\|^2 \tag{From the update in \Cref{alg:generalized_shuffled_gradient_fm}}\\
        &= \eta^2 \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \Big(\nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) 
        - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)
        - \nabla f_{\pi_j^{(s)}}(\rvz) \\
        \nonumber
        &\qquad \qquad \qquad \qquad + \nabla f_{\pi_j^{(s)}}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvx^*)
        + \nabla f_{\pi_j^{(s)}}(\rvx^*) + \rho_j^{(s)}
        \Big)\Big\|^2 \\
    \label{eq:breg_diff_interm_1}
        &\leq \eta^2 \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}\Big(
            5\Big\| \sum_{j=1}^{i-1} \Big(\nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) \Big) \Big\|^2
            + 5\Big\| \sum_{j=1}^{i-1}\Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz) \Big) \Big\|^2\\
        \nonumber
            &\quad + 5 \Big\|\sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvx^*) \Big) \Big\|^2
            + 5\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*) \Big\|^2
            + 5\Big\| \sum_{j=1}^{i-1}\rho_j^{(s)} \Big\|^2
        \Big)
    \end{align}
    We proceed by bounding each term in Eq.~\ref{eq:breg_diff_interm_1} separately. First, note that
    \begin{align}
        &\sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\|\sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) \Big)\Big\|^2\\
        \nonumber
        &\leq \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} (i-1)\sum_{j=1}^{i-1} \Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) \Big\|^2\\
        &= \sum_{j=1}^{n_d^{(s)}-1} \Big( \sum_{i=j+1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} (i-1) \Big)\Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2
        \Big) \\
        &\leq \sum_{j=1}^{n_d^{(s)}-1} n (\sum_{i=1}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)})
        \Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2 \\
        & \label{eq:breg_diff_term_1}
        \leq n (\sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}) \sum_{j=1}^{n_d^{(s)}}
        \Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_i^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2
    \end{align}
    Next,  

    \begin{align}
    \label{eq:breg_diff_term_dissim}
        \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
        \leq \widehat{L}^{(s)*} \sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
    \end{align}
    Moreover,
    \begin{align}
        &\sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}
        \left\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_{j}^{(s)}}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvx^*) \Big) \right\|^2\\
        \nonumber
        &\stackrel{\text{(a)}}{\leq} \sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)} 2 \Big(\sum_{j=1}^{i-1} L_{\pi_j^{(s)}} \Big) \Big( \sum_{l=1}^{i-1} B_{f_{\pi_l^{(s)}}}(\rvz, \rvx^*)
        \Big)
        \leq 2 n L \sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big(\sum_{l=1}^{i-1} B_{f_{\pi_l^{(s)}}}(\rvz, \rvx^*) \Big)\\
        &\stackrel{\text{(b)}}{\leq} 2 n L \sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big(\sum_{l=1}^{n} B_{f_{\pi_l}^{(s)}}(\rvz, \rvx^*) \Big)
    \label{eq:breg_diff_term_2}
        = 2 n^2 L B_F(\rvz, \rvx^*) \cdot \sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}
    \end{align}
    where $(a)$ is by Lemma~\ref{lemma:breg_div_ub_lb} and $(b)$ is due to $B_{f_i^{(s)}}(\rvz, \rvx^*) \geq 0, \forall \rvz\in\R^{d}, i\in [n]$.


    Plugging Eq.~\ref{eq:breg_diff_term_1}, Eq.~\ref{eq:breg_diff_term_dissim} and Eq.~\ref{eq:breg_diff_term_2} back to Eq.~\ref{eq:breg_diff_interm_1}, there is
    
    \begin{align}
    \label{eq:breg_div_interm_ub}
        I_1 \triangleq & \sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2 \\
        \nonumber
        &\leq 5 \eta^2 n (\sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}) \sum_{j=1}^{n_d^{(s)}}
        \Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_i^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2
        \\
        \nonumber
        &\quad + \widehat{L}^{(s)*} \sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2\\
        \nonumber
        &\quad + 10 \eta^2 n^2 L B_F(\rvz, \rvx^*) \cdot \sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}
        + 5 \eta^2 \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}
        \Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}} (\rvx^*) \Big\|^2
        + 5 \eta^2 \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)}
        \Big\| \sum_{j=1}^{i-1}\rho_j^{(s)}\Big\|^2
    \end{align}

    \textbf{Part II: } Similarly, for $n_d^{(s)} < i \leq n$,
    \begin{align}
        I_2 \triangleq &\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2
        = \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \eta^2\left\| \sum_{j=1}^{n_d^{(s)}}(\nabla f_{\pi_{j}^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \rho_j^{(s)}) + \sum_{j=n_d^{(s)}+1}^{i-1} (\nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) + \rho_j^{(s)})  \right\|^2
        \tag{From the update of Algorithm~\ref{alg:generalized_shuffled_gradient_fm}}\\
        &=\eta^2 \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) 
        - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvz)\\
        \nonumber
        &\quad + \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvz)
        - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)\\
        \nonumber
        &\quad + \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        - \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)
        + \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*) 
        + \sum_{j=1}^{i-1} \rho_j^{(s)}
        \Big\|^2\\
    \label{eq:breg_diff_interm_1_pub}
        &\leq \eta^2 \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big(
            5 \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) 
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvz) \Big\|^2\\
        \nonumber
            &\quad + 5\Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2\\
        \nonumber
            &\quad + 5\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
            - \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*) \Big\|^2
            + 5\Big\|\sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*) \Big\|^2
            + 5\Big\|\sum_{j=1}^{i-1}\rho_j^{(s)} \Big\|^2
        \Big)
    \end{align}
    We proceed by bounding each term in Eq.~\ref{eq:breg_diff_interm_1_pub} separately. First, note that
    \begin{align}
        &\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}
        \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) 
        - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvz) \Big\|^2\\
        &\leq \sum_{i=n_d^{(s)}+1}^{n}
        \widetilde{L}_{i-n_d^{(s)}}^{(s)}
        (i-1) \Big(\sum_{j=1}^{n_d^{(s)}} \Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2
        + \sum_{j=n_d^{(s)}+1}^{i-1}\Big\| \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) - \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvz)\Big\|^2
        \Big)\\\
    \label{eq:breg_diff_term_1_pub}
        &\leq \sum_{j=1}^{n_d^{(s)}} \Big(\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}(i-1))\Big)\Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) \Big\|^2\\
        \nonumber
        &\quad + \sum_{j=n_d^{(s)}+1}^{n-1} \Big(\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}(i-1)\Big)\Big\| \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)})
        - \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvz)\Big\|^2\\
        &\leq \sum_{j=1}^{n_d^{(s)}} n \Big(\sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big) \Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2 \\
        \nonumber
        &\quad 
        + \sum_{j=n_d^{(s)}+1}^{n} n \Big(\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big)
        \Big\| \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) - \nabla f_{j-n_d^{(s)}}^{(s,pub)}(\rvz)\Big\|^2 
    \end{align}
    Next, 
    \begin{align}
    \label{eq:breg_diff_term_dissim_pub}
        &\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2\\
        \nonumber
        &\leq L^{(s)*} \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2
    \end{align}
    Moreover,
    \begin{align}
        \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}
        \Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
            - \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*) \Big\|^2
        &\stackrel{\text{(a)}}{\leq} \sum_{i= n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \cdot 2\Big( \sum_{j=1}^{i-1} L_{\pi_j^{(s)}} \Big) \Big( \sum_{j=1}^{i-1} B_{\pi_j^{(s)}}(\rvz, \rvx^*)
        \Big)\\
        &\stackrel{\text{(b)}}{\leq} \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \cdot 2\Big( \sum_{j=1}^{i-1} L_{\pi_j^{(s)}} \Big)
        \Big( \sum_{j=1}^{n} B_{\pi_j^{(s)}}(\rvz, \rvx^*) \Big)\\
        &\leq 2 \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \cdot n L \cdot \Big( \sum_{j=1}^{n} B_{j}(\rvz, \rvx^*) \Big)\\
    \label{eq:breg_diff_term_2_pub}
        &\leq 2 n L B_F(\rvz, \rvx^*) \cdot \sum_{i=n_d^{(s)}+1}^{n} \widehat{L}_{i-n_d^{(s)}}^{(s)}
    \end{align}
    where $(a)$ is by Lemma~\ref{lemma:breg_div_ub_lb} and $(b)$ is due to $B_{f_i}(\rvz, \rvx^*) \geq 0, \forall \rvz\in\R^{d}, i\in [n]$.
    
    Plugging Eq.~\ref{eq:breg_diff_term_1_pub}, Eq.~\ref{eq:breg_diff_term_dissim_pub} and Eq.~\ref{eq:breg_diff_term_2_pub} back to Eq.~\ref{eq:breg_diff_interm_1_pub}, there is
    \begin{align}
    \label{eq:breg_div_interm_ub_pub}
        &\sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2\\
        \nonumber
        &\leq 5 \eta^2 n (\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}) \sum_{j=1}^{n_d^{(s)}} \Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2\\
        \nonumber
        &\quad + 5\eta^2 n (\sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}) \sum_{j=n_d^{(s)}+1}^{n}\Big\| \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) - \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvz)\Big\|^2\\
    \nonumber
        &\quad + 5\eta^2 L^{(s)*} \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2\\
        \nonumber
        &\quad 
        % + 5\eta^2 \sum_{i=n_d^{(s)}+1}^{n} \widehat{L}^{(s)*}(C_i^{(s)})^2
        + 10 \eta^2 n L B_F(\rvz, \rvx^*) \sum_{i=n_d^{(s)}+1}^{n} \widehat{L}_{i-n_d^{(s)}}^{(s)}
        % + 10\eta^2 n^3 \widehat{L}^{(s)} L B_F(\rvz, \rvx^*)
        + 5\eta^2 \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + 5\eta^2 \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
    \end{align}


    Combining Eq.~\ref{eq:breg_div_interm_ub} and Eq.~\ref{eq:breg_div_interm_ub_pub}, there is
    \begin{align}
    \label{eq:breg_div_interm_ub_sum}
        &\frac{1}{n}\Big( \sum_{i=1}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \|\rvx_i^{(s)} - \rvx_1^{(s)} \|^2 
        + \sum_{i=n_d^{(s)}+1}^{n} \widehat{L}_i^{(s)} \|\rvx_i^{(s)} - \rvx_1^{(s)}\|^2\Big) \\
        \nonumber
        &\leq 5\eta^2 n \widehat{L}^{(s)} \sum_{j=1}^{n_d^{(s)}}\Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_{j}^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2
        + 5\eta^2 n \widehat{L}^{(s)} \sum_{j=n_d^{(s)}+1}^{n}\Big\| \nabla f_{j-n_d^{(s)}}^{(s, pub)} - \nabla f_{j-n_d^{(s)}}^{(s,pub)}(\rvz)\Big\|^2 \\
        \nonumber
        &\quad + 10 \eta^2 n^2 \widehat{L}^{(s)} L B_F(\rvz, \rvx^*)
        + 5 \eta^2 \frac{1}{n} \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1}\nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2 \Big)\\
        \nonumber
        &\quad 
        + 5\eta^2 \frac{1}{n} \Big( \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2\Big)\\
        \nonumber
        &\quad + 5\eta^2 L^{(s)*} \frac{1}{n}\Big(\sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
        \\
        \nonumber
        &\quad \quad + \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2
        \Big)
    \end{align}

    Hence, plugging Eq.~\ref{eq:breg_div_interm_ub_sum} back to Eq.~\ref{eq:breg_diff_interm_2}, there is
    \begin{align}
        &\frac{1}{n} \Big(\sum_{i=1}^{n_d^{(s)}}
        \Big( B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_{1}^{(s+1)}, \rvx_i^{(s)})
        - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)}) \Big)
        + \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s,pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)
        \Big) \\
        \nonumber
        &\leq \widehat{L}^{(s)} \|\rvx_{1}^{(s+1)} - \rvx_1^{(s)}\|^2 
        \\
        \nonumber
        &\quad + 5\eta^2 n \widehat{L}^{(s)} \sum_{j=1}^{n_d^{(s)}}\Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_{j}^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2
        + 5\eta^2 n \widehat{L}^{(s)} \sum_{j=n_d^{(s)}+1}^{n}\Big\| \nabla f_{j-n_d^{(s)}}^{(s, pub)} - \nabla f_{j-n_d^{(s)}}^{(s,pub)}(\rvz)\Big\|^2\\
        \nonumber
        &\quad + 10 \eta^2 n^2 \widehat{L}^{(s)} L B_F(\rvz, \rvx^*)
        + 5 \eta^2 \frac{1}{n} \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1}\nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2 \Big)\\
        \nonumber
        &\quad 
        % + 5 \eta^2 \frac{1}{n} \sum_{i=1}^{n-1} \widehat{L}^{(s)*} (C_i^{(s)})^2
        + 5\eta^2 \frac{1}{n} \Big( \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2\Big)\\
        \nonumber
        &\quad - \frac{1}{n} \Big( 
        \sum_{i=1}^{n_d^{(s)}} \frac{ \left\|\nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i) - \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvz) \right\|^2}{2 \widehat{L}_{\pi_i^{(s)}}^{(s)}}
        + \sum_{i=n_d^{(s)}+1}^{n} \frac{\left\| \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvx_i^{(s)}) - \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz) \right\|^2}{2\widetilde{L}_{i-n_d^{(s)}}^{(s)}}
        \Big)\\
        \nonumber
        &\quad + 5\eta^2 L^{(s)*} \frac{1}{n}\Big(\sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
        \\
        \nonumber
        &\quad \quad + \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2
        \Big)
    \end{align}


    If one sets the learning rate $\eta$ such that
    \begin{align}
        &5 \eta^2 n \widehat{L}^{(s)} \leq \frac{1}{n} \cdot \frac{1}{2 \widehat{L}^{(s)*}}, \quad
        \Rightarrow \eta \leq \frac{1}{n\sqrt{10 \widehat{L}^{(s)} \widehat{L}^{(s)*}}}
    \end{align}
    then there is
    \begin{align}
        &\frac{1}{n} \Big(\sum_{i=1}^{n_d^{(s)}}
        \Big( B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvx_{1}^{(s+1)}, \rvx_i^{(s)})
        - B_{f_{\pi_i^{(s)}}^{(s, priv)}}(\rvz, \rvx_i^{(s)}) \Big)
        + \sum_{i=n_d^{(s)}+1}^{n}\Big(
            B_{f_{i-n_d^{(s)}}^{(s,pub)}}(\rvx_1^{(s+1)}, \rvx_i^{(s)})
            - B_{f_{i-n_d^{(s)}}^{(s, pub)}}(\rvz, \rvx_i^{(s)})
        \Big)
        \Big) \\
        \nonumber
        &\leq \widehat{L}^{(s)} \|\rvx_{1}^{(s+1)} - \rvx_1^{(s)}\|^2
        + 5 \eta^2 \frac{1}{n} \sum_{i=1}^{n-1} \widehat{L}^{(s)*} (C_i^{(s)})^2
        + 10\eta^2 n^2 \widehat{L}^{(s)} L B_F(\rvz, \rvx^*)
        \\
        \nonumber
        &
        + 5 \eta^2 \frac{1}{n} \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1}\nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2 \Big)\\
        \nonumber
        &\quad 
        + 5\eta^2 \frac{1}{n} \Big( \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2\Big)\\
        \nonumber
        \nonumber
        &\quad + 5\eta^2 L^{(s)*} \frac{1}{n}\Big(\sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
        \\
        \nonumber
        &\quad \quad + \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2
        \Big)
    \end{align}
    
\end{proof}


% gathering everything together + dissimilarity
\begin{lemma}[One Epoch Convergence]
\label{lemma:one_epoch_convergence}
    Under Assumptions~\ref{ass:convexity},~\ref{ass:reg},~\ref{ass:dissim_partial_lipschitzness},~\ref{ass:appendix_refined_smoothness} and Lemma~\ref{ass:H_smoothness}, for any epoch $s\in [K]$, $\beta > 0$, and $\forall \rvz \in \R^d$, 
    if $\eta \leq \frac{1}{n\sqrt{10 \widehat{L}^{(s)} \widehat{L}^{(s)*}}}$, 
    Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees
        \begin{align}
        G(\rvx_1^{(s+1)}) - G(\rvz)
        &\leq \frac{1}{2 n \eta}\Big( \|\rvz - \rvx_1^{(s)}\|^2  - \|\rvz - \rvx_1^{(s+1)}\|^2 \Big)
        + \Big( \frac{L_H^{(s)} + \beta}{2} - \frac{\mu_{\psi}}{2} \Big)\| \rvz - \rvx_1^{(s+1)}\|^2
        \\
        \nonumber
        &\quad + 10 \eta^2 n \widehat{L}^{((s)} L B_F(\rvz, \rvx^*) 
        + \underbrace{ \frac{1}{2n^2 \beta}(C_n^{(s)})^2 }_{\text{Non-vanishing Dissimilarity}}
        % + \underbrace{ 5\eta^2 \widehat{L}^{(s)*} \frac{1}{n} 
 % \sum_{i=1}^{n-1}(C_i^{(s)})^2 }_{\text{Vanishing Dissimilarity}}
 \\
        \nonumber
        &\quad
        + \underbrace{ 5 \eta^2 \frac{1}{n} \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1}\nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2 \Big)
        }_{\text{Optimization Uncertainty}}\\
        \nonumber
        &\quad + \underbrace{ \frac{1}{n}\sum_{i=1}^{n}\langle -\rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle
        + 5\eta^2 \frac{1}{n} \Big( \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2\Big)
        }_{\text{Injected Noise}}\\
    \nonumber
        &\quad + \underbrace{5\eta^2 L^{(s)*} \frac{1}{n}\Big(\sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
        }_{\text{Vanishing Dissimilarity}}
        \\
        \nonumber
        &\quad \quad + \underbrace{ \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2
        \Big)
        }_{\text{Vanishing Dissimilarity}}
    \end{align}

\end{lemma}


% gathering things and add dissimilarity bound here
\begin{proof}[Proof of Lemma~\ref{lemma:one_epoch_convergence}]
    By Lemma~\ref{lemma:one_epoch_bg_3pid} and Lemma~\ref{lemma:one_epoch_bound_bregmandiv}, for any $s \in [K]$ and $\forall \rvz\in \R^d$, if $\eta \leq \frac{1}{n\sqrt{10 \widehat{L}^{(s)} \widehat{L}^{(s)*}}}$,
    \begin{align}
    \label{eq:one_epoch_convergence_interm}
        &G(\rvx_1^{(s+1)}) - G(\rvz)\\
        \nonumber
        &\leq H^{(s)}(\rvx_1^{(s+1)}) - H^{(s)}(\rvz)
        + \frac{\|\rvz - \rvx_1^{(s)}\|^2}{2n \eta} - (\frac{1}{2n \eta} + \frac{\mu_\psi}{2}) \|\rvz - \rvx_1^{(s+1)}\|^2
        - \frac{1}{2n \eta}\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2\\
        \nonumber
        &\quad + \frac{1}{n}\sum_{i=1}^{n}\langle -\rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle
        + \widehat{L}^{(s)} \|\rvx_{1}^{(s+1)} - \rvx_1^{(s)}\|^2 
        % + 5 \eta^2 \frac{1}{n} \sum_{i=1}^{n-1} L^{(s)*} (C_i^{(s)})^2
        + 10\eta^2 n^2 \widehat{L}^{(s)} L B_F(\rvz, \rvx^*)\\
        \nonumber
        &\quad 
        + 5 \eta^2 \frac{1}{n} \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1}\nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2 \Big)\\
        \nonumber
        &\quad 
        + 5\eta^2 \frac{1}{n} \Big( \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2\Big)\\
    \nonumber
        &\quad + 5\eta^2 L^{(s)*} \frac{1}{n}\Big(\sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
        \\
        \nonumber
        &\quad \quad + \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2
        \Big)
    \end{align}

    Since $\eta \leq \frac{1}{n \sqrt{10 \widehat{L}^{(s)} \widehat{L}^{(s)*}}}$, there is $\widehat{L}^{(s)} \leq \sqrt{\widehat{L}^{(s)} \widehat{L}^{(s)*}} \leq \frac{1}{\sqrt{10} n \eta} \leq \frac{1}{2 n \eta}$, 
    and so $(\widehat{L} - \frac{1}{2n \eta})\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2 \leq 0$.

    For any $\beta > 0$ and any $\rvz\in\R^{d}$, 
    \begin{align}
        H^{(s)}(\rvx_{1}^{(s+1)}) - H^{(s)}(\rvz)
        &= H^{(s)}(\rvx_{1}^{(s+1)}) -  H^{(s)}(\rvz)
        - \langle \nabla H^{(s)}(\rvz), \rvx_{1}^{(s+1)} - \rvz\rangle + \langle \nabla H^{(s)}(\rvz), \rvx_{1}^{(s+1)} - \rvz\rangle\\
        &\stackrel{\text{(a)}}{\leq} \frac{L_H^{(s)}}{2}  \|\rvx_{1}^{(s+1)} - \rvz\|^2 + \frac{1}{2n^2 \beta}(C_n^{(s)})^2 + \frac{\beta}{2} \|\rvx_{1}^{(s+1)} - \rvz\|^2 \\
    \label{eq:dissim_ub}
        &= \frac{L_H^{(s)} + \beta}{2} \|\rvx_{1}^{(s+1)} - \rvz\|^2 + \frac{1}{2n^2 \beta}(C_n^{(s)})^2
    \end{align}
    where (a) is by Assumption~\ref{ass:H_smoothness},~\ref{ass:dissim_partial_lipschitzness}, Lemma~\ref{lemma:breg_div_ub_lb} and Young's inequality. 

    We comment 
    that if $H^{(s)} = 0$ for epoch $s$, a tighter bound $H^{(s)}(\rvx_1^{(s+1)}) - H^{(s)}(\rvz) = 0$ holds, as Young's inequality is not tight in this case. Consequently, one can set $\beta = 0$. 

    Hence, plugging Eq.~\ref{eq:dissim_ub} back to Eq.~\ref{eq:one_epoch_convergence_interm}, there is
    \begin{align}
        G(\rvx_1^{(s+1)}) - G(\rvz)
        &\leq \frac{1}{2 n \eta}\Big( \|\rvz - \rvx_1^{(s)}\|^2  - \|\rvz - \rvx_1^{(s+1)}\|^2 \Big)
        + \Big( \frac{L_H^{(s)} + \beta}{2} - \frac{\mu_{\psi}}{2} \Big)\| \rvz - \rvx_1^{(s+1)}\|^2
        \\
        \nonumber
        &\quad + 10 \eta^2 n \widehat{L}^{((s)} L B_F(\rvz, \rvx^*) + \frac{1}{2n^2 \beta}(C_n^{(s)})^2 
        % + 5\eta^2 \widehat{L}^{(s)*} \frac{1}{n} 
 % \sum_{i=1}^{n-1}(C_i^{(s)})^2
 \\
        \nonumber
        &\quad
        + 5 \eta^2 \frac{1}{n} \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1}\nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2 \Big)\\
        \nonumber
        &\quad + \frac{1}{n}\sum_{i=1}^{n}\langle -\rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle
        + 5\eta^2 \frac{1}{n} \Big( \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2\Big)\\
    \nonumber
        &\quad + 5\eta^2 L^{(s)*} \frac{1}{n}\Big(\sum_{i=2}^{n_d^{(s)}}
        \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2
        \\
        \nonumber
        &\quad \quad + \sum_{i=n_d^{(s)} + 1}^{n} \Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2
        \Big)
    \end{align}

\end{proof}



\subsection{Expected One Epoch Convergence}
\label{subsec:appendix_expected_one_epoch_convergence}

There are two sources of randomness involved in each epoch: 1) the shuffling operator in optimization, and 2) injected Gaussian noise to perturb the gradient for privacy preservation. 
1) can be bounded using Lemma~\ref{lemma:opt_noise_bound}. To bound 2),
in this section, we show upper bounds on the expectation of the additional error term due to noise injection and the noise variance in Lemma~\ref{lemma:noise_bias} and Lemma~\ref{lemma:noise_variance}. We then give an expected one epoch convergence bound, where the expectation is taken over the two sources of randomness, in Lemma~\ref{lemma:expected_one_epoch_convergence}. 




\begin{lemma}[Additional Error]
\label{lemma:noise_bias}
    For any epoch $s\in [K]$ and $\forall \rvz \in \R^{d}$, consider the injected noise $\rho_{i}^{(s)} \sim \gN(0, (\sigma^{(s)})^2 \sI_d)$, $\forall i\in [n]$, if the regularization function $\psi$ is twice differentiable and $\rvz$ is independent of $\rho_i^{(s)}, \forall i\in [n]$, then the error caused by noise injection in epoch $s$ is
    \begin{align}
        \E\left[\frac{1}{n}\sum_{i=1}^{n} \langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right] 
        \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
    \end{align}
    where the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}_{i=1}^{n}$.
\end{lemma}



\begin{proof}[Proof of Lemma~\ref{lemma:noise_bias}]
    First, note that if $\rvz$ is independent of $\rho_i^{(s)}$, $\forall i\in [n]$, 
    there is $\E\left[\langle \frac{1}{n}\sum_{i=1}^{n} \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right] 
    = \frac{1}{n}\sum_{i=1}^{n}\E\left[ \langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right] = \frac{1}{n}\sum_{i=1}^{n}\E\left[ \langle \rho_i^{(s)}, \rvx_1^{(s+1)}  \rangle \right]$.

    Recall that the update rule in Algorithm~\ref{alg:generalized_shuffled_gradient_fm} in epoch $s\in [K]$ is
    \begin{align}
        &\rvx_{i+1}^{(s)} = \rvx_{i}^{(s)} - \eta \Big(\nabla f_{\pi_i^{(s)}}^{(s, priv)} + \rho_i^{(s)}\Big),\quad \forall i \in[n_d^{(s)}]\\
        &\rvx_{i+1}^{(s)} = \rvx_i^{(s)} - \eta \Big(\nabla f_{i-n_d^{(s)}}^{(s, pub)} + \rho_i^{(s)}\Big), \quad \forall n_d^{(s)} < i \leq n\\
    \label{eq:prox_step}
        &\rvx_{1}^{(s+1)} = \argmin_{\rvx\in\R^d} n \psi(\rvx) + \frac{\| \rvx - \rvx_{n+1}^{(s)}\|^2}{2\eta}
    \end{align}

    Since $\psi$ is twice differentiable, by Stein's Lemma (Lemma~\ref{lemma:steins_lemma}), for any $i\in [n]$, conditional on $\rho_j^{(s)}, \forall j\neq i$,
    \begin{align}
    \label{eq:application_steins_lemma}
        \E_{\rho_i^{(s)}}\left[ \langle \rho_i^{(s)}, \rvx_{1}^{(s+1)} \rangle \mid \{\rho_j^{(s)}\}_{j\neq i}\right]
        = (\sigma^{(s)})^2\cdot \E_{\rho_i^{(s)}}\left[\text{tr}(\frac{\partial \rvx_1^{(s+1)}}{\partial \rho_i^{(s)}}) \mid \{\rho_j^{(s)}\}_{j\neq i} \right]
    \end{align}
    We proceed by computing $\frac{\partial \rvx_1^{(s+1)}}{\partial \rho_i^{(s)}}$.
    By the optimality condition of $\rvx_1^{(s+1)}$ as in Eq.~\ref{eq:prox_step},

    \begin{align}
        &n \nabla \psi(\rvx_1^{(s+1)}) + \frac{1}{\eta}\cdot (\rvx_{1}^{(s+1)} - \rvx_{n+1}^{(s)})  = \mathbf{0}\\
        &n\eta \nabla \psi(\rvx_1^{(s+1)}) + \rvx_1^{(s+1)} - \rvx_{n+1}^{(s)} = \mathbf{0}
    \end{align}

    And using implicit differentiation of the above optimality condition, 
    \begin{align}
        &n \eta \frac{\partial \nabla \psi(\rvx_1^{(s+1)})}{\partial \rho_i^{(s)}}
        + \frac{\partial \rvx_1^{(s+1)}}{\partial \rho_i^{(s)}}
        - \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}} = \mathbf{0}\\
        &n \eta \nabla^2\psi(\rvx_1^{(s+1)})\frac{\partial \rvx_1^{(s+1)}}{\partial \rho_i^{(s)}} + \frac{\partial \rvx_1^{(s+1)}}{\partial \rho_i^{(s)}} - \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}} = \mathbf{0}\\
        &\Big( n \eta \nabla^2 \psi(\rvx_1^{(s+1)}) + \sI_d \Big) \frac{\partial \rvx_1^{(s+1)}}{\partial \rho_i^{(s)}}
        = \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}}\\
    \label{eq:diff_implicit}
        &\frac{\partial \rvx_{1}^{(s+1)}}{\partial \rho_i^{(s)}} = \Big(\eta n \nabla^2 \psi(\rvx_1^{(s+1)}) + \sI_d \Big)^{-1} \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}}
    \end{align}
    where $\nabla^2 \psi(\rvx_1^{(s+1)})$ is the Hessian of $\psi$ evaluated at $\rvx_1^{(s+1)}$.

    We proceed by computing $\frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}}$.
    Note that $\rho_i^{(s)}$ directly affects the update of $\rvx_{i+1}^{(s)}$ and indirectly affects the subsequent updates of $\rvx_{j}^{(s)}$ for all  $j > i + 1$. Hence, we decompose $\rvx_{n+1}^{(s)}$ as follows: for $i \leq n_d^{(s)}$,
    \begin{align}
        \rvx_{n+1}^{(s)} &= \underbrace{
            \rvx_1 - \eta \sum_{j=1}^{i-1}\Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \rho_j^{(s)} \Big)
            - \nabla f_{\pi_i^{(s)}}(\rvx_i^{(s)})
        }_{\text{Independent of $\rho_i^{(s)}$}}\\
        \nonumber
        &\quad - \underbrace{
            \rho_i^{(s)}
        }_{\text{Direct dependency}}
        - \underbrace{
            \eta \sum_{j=i+1}^{n_d^{(s)}}\Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \rho_j^{(s)}\Big) - \eta \sum_{j=n_d^{(s)}+1}^{n}\Big(\nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) + \rho_j^{(s)}
        \Big)
        }_{\text{Implicit dependency on $\rho_i^{(s)}$ through $\rvx_j^{(s)}$'s}}
    \end{align}
    and for $n_d^{(s)} < i \leq n$,
    \begin{align}
        \rvx_{n+1}^{(s)} 
        &= \underbrace{
            \rvx_1 - \eta \sum_{j=1}^{n_d^{(s)}} \Big(\nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \rho_j^{(s)}\Big) - \eta \sum_{j=n_d^{(s)}+1}^{i-1}\Big(\nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) + \rho_j^{(s)}\Big)
            - \nabla f_i^{(s, pub)}(\rvx_i^{(s)})
        }_{\text{Independent of $\rho_i^{(s)}$ through $\rvx_j^{(s)}$'s}}\\
        \nonumber
        &\quad - \underbrace{\rho_i^{(s)}}_{\text{Direct dependency}}
        - \underbrace{
            \eta \sum_{j=i+1}^{n}\Big(\nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) + \rho_j^{(s)}\Big)
        }_{\text{Implicit dependency on $\rho_i^{(s)}$}}
    \end{align}
    And so for $i\leq n_d^{(s)}$,
    \begin{align}
    \label{eq:partial_x_n1_small_i}
        \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}} 
        &= -\eta \sI_d - \eta \sum_{j=i+1}^{n_d^{(s)}} \frac{\partial \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)})}{\partial \rho_i^{(s)}}
        - \eta \sum_{j=n_d^{(s)}+1}^{n} \frac{\partial \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)})}{\partial \rho_i^{(s)}}\\
        &= -\eta \sI_d - \eta \sum_{j=i+1}^{n_d^{(s)}} \nabla^2 f_{\pi_j^{(s)}}^{(s,priv)}(\rvx_j^{(s)})\frac{\partial \rvx_j^{(s)}}{\partial \rho_i^{(s)}}
        - \eta \sum_{j=n_d^{(s)}+1}^{n} \nabla^2 f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) \frac{\partial \rvx_j^{(s)}}{\partial \rho_i^{(s)}}
    \end{align}
    and for $n_d^{(s)} < i \leq n$,
    \begin{align}
    \label{eq:partial_x_n1_large_i}
    \frac{\rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}}
        = -\eta \sI_d - \eta \sum_{j=i+1}^{n}\frac{\partial \nabla f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)})}{\partial \rho_i^{(s)}}
        = -\eta \sI_d - \eta \sum_{j=i+1}^{n} \nabla^2 f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) \frac{\partial \rvx_j^{(s)}}{\partial \rho_i^{(s)}}
    \end{align}

    We now compute $\frac{\partial \rvx_j^{(s)}}{\partial \rho_i^{(s)}}$ for $j > i$. First, note that by the update rule, 
    \begin{align}
    \label{eq:partial_x_i1}
        \frac{\partial \rvx_{i+1}^{(s)}}{\partial \rho_i^{(s)}}
    = -\eta \sI_d
    \end{align}
    
    and for any $i < j \leq n$,
    \begin{align}
        &\frac{\partial \rvx_{j+1}^{(s)}}{\partial \rho_i^{(s)}}
        = \begin{cases}
            \frac{\partial}{\partial \rho_i^{(s)}}\Big(\rvx_{j}^{(s)} - \eta \Big(\nabla f_{\pi_{j}^{(s)}}^{(s, priv)}(\rvx_{j}^{(s)}) + \rho_{j}^{(s)} \Big)
        \Big) & \text{if $j\leq n_d^{(s)}$}\\
            \frac{\partial}{\partial \rho_i^{(s)}}\Big(\rvx_{j-n_d^{(s)}}^{(s, pub)}(\rvx_{j}^{(s)}) + \rho_{j}^{(s)} \Big) & \text{Otherwise} 
        \end{cases}\\
        &\Rightarrow \frac{\partial \rvx_{j+1}^{(s)}}{\partial \rho_i^{(s)}}
        = \begin{cases}
            \Big(\sI_d - \eta \nabla^2 f_{\pi_{j}^{(s)}}^{(s, priv)}(\rvx_{j}^{(s)})\Big) \frac{\partial \rvx_{j}^{(s)}}{\partial \rho_i^{(s)}} & \text{if $j \leq n_d^{(s)}$}\\
            \Big( \sI_d - \eta \nabla^2 f_{j-n_d^{(s)}}^{(s,pub)}(\rvx_{j}^{(s)})\Big)\frac{\partial \rvx_{j}^{(s)}}{\partial \rho_i^{(s)}} & \text{Otherwise}
        \end{cases}
    \end{align}
    Therefore, by the above recursion, for any $i < j \leq n_d^{(s)}$,
    \begin{align}
    \label{eq:partial_x_j_small_i}
        \frac{\partial \rvx_{j+1}^{(s)}}{\partial \rho_i^{(s)}}
        = -\eta \cdot \prod_{k=i+1}^{j}\Big( \sI_d - \eta \nabla^2 f_{\pi_{k}^{(s)}}^{(s,priv)}(\rvx_{k}^{(s)})\Big) 
    \end{align}
    and similarity, for any $n_d^{(s)} < j \leq n$,
    \begin{align}
    \label{eq:partial_x_j_large_i}
        \frac{\partial \rvx_{j+1}^{(s)}}{\partial \rho_i^{(s)}}
        = -\eta \cdot \Big(\prod_{k=i+1}^{n_d^{(s)}}\Big(\sI_d - \eta \nabla^2 f_{\pi_{k}^{(s)}}^{(s,priv)}(\rvx_{k}^{(s)})\Big)
        \Big)\cdot \Big(\prod_{k=n_d^{(s)}+1}^{j} \Big(\sI_d - \eta \nabla^2 f_{k-n_d^{(s)}}^{(s,pub)}(\rvx_{k}^{(s)}) \Big) \Big)
    \end{align}
    
    
    Therefore, plugging Eq.~\ref{eq:partial_x_i1}, Eq.~\ref{eq:partial_x_j_small_i} and Eq.~\ref{eq:partial_x_j_large_i} back to Eq.~\ref{eq:partial_x_n1_small_i} or Eq.~\ref{eq:partial_x_n1_large_i}, there is, for $i \leq n_d^{(s)}$,

    \begin{align}
        \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}}
        &= -\eta \sI_d
        + \eta^2 \nabla^2 f_{\pi_{i+1}^{(s)}}^{(s, priv)}(\rvx_{i+1}^{(s)})
        + \eta^2 \sum_{j=i+2}^{n_d^{(s)}} \nabla^2 f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)})
        \prod_{k=i+1}^{j-1} \Big( \sI_d - \eta \nabla^2 f_{\pi_k^{(s)}}^{(s, priv)}(\rvx_{k}^{(s)})\Big)\\
        \nonumber
        &\quad + \eta^2 \nabla^2 f_{1}^{(s, pub)}(\rvx_{n_d^{(s)}+1}^{(s)}) \prod_{k=i+1}^{n_d^{(s)}}\Big( \sI_d - \eta \nabla^2 f_{\pi_k^{(s)}}^{(s, priv)}(\rvx_k^{(s)})\Big)\\
        \nonumber
        &\quad + \eta^2 \sum_{j=n_d^{(s)}+2}^{n} \nabla^2 f_{j-n_d^{(s)}}^{(s, pub)}(\rvx_j^{(s)}) \Big(\prod_{k=i+1}^{n_d^{(s)}}\Big(\sI_d - \eta\nabla^2 f_{\pi_k^{(s)}}^{(s, priv)}(\rvx_k^{(s)})\Big)\Big) \cdot\Big( \prod_{k=n_d^{(s)}+1}^{j-1}\Big(\sI_d - \eta \nabla f_{k-n_d^{(s)}}^{(s, pub)}(\rvx_k^{(s)}) \Big) \Big)
    \end{align}
    and for $n_d^{(s)} < i \leq n$,
    \begin{align}
        \frac{\rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}}
        &= -\eta \sI_d + \eta^2 \nabla^2 f_{i+1-n_d^{(s)}}^{(s,pub)}(\rvx_{i+1}^{(s)})\\
        \nonumber
        &\quad + \eta^2 \sum_{j=i+2}^{n} \nabla^2 f_{j-n_d^{(s)}}^{(s,pub)}(\rvx_j^{(s)})
        \cdot \Big(\prod_{i+1}^{n_d^{(s)}}\Big(\sI_d  - \eta \nabla^2 f_{\pi_k^{(s)}}^{(s,priv)}(\rvx_k^{(s)}) \Big)\Big)
        \cdot \Big(\prod_{k=n_d^{(s)}+1}^{j-1} \Big(\sI_d - \eta \nabla f_{k-n_d^{(s)}}^{(s, pub)}(\rvx_k^{(s)}) \Big)
    \end{align}
    By Assumption~\ref{ass:convexity} and~\ref{ass:smoothness}, 
    $\|\nabla^2 f_i^{(s, priv)}(\rvx)\|_{op} \leq \widehat{L}^{(s)*}$ and $\|\nabla^2 f_{j}^{(s,pub)}(\rvx)\|_{op} \leq \widehat{L}^{(s)*}$, $\forall i\in [n_d^{(s)}], j\in[n-n_d^{(s)}]$ and $\forall \rvx\in\R^d$, where $\|\cdot\|_{op}$ denotes the matrix operator norm and recall that $\widehat{L}^{(s)*} = \argmax_{i\in[n]}\widehat{L}_i^{(s)}$.
    
    And so if $\eta \leq \frac{1}{\widehat{L}^{(s)*}}$, $\forall i \in [n]$,
    \begin{align}
    \label{eq:op_ub}
        \left\| \frac{\rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}} \right\|_{op}
        \leq n \eta^2 \widehat{L}^{(s)*}
    \end{align}

    Moreover, by Assumption~\ref{ass:reg}, $\lambda_{min}(\nabla^2 \psi(\rvx)) \geq 0$, $\forall \rvx \in \R^d$, 
    where $\lambda_{min}$ denotes the minimum eigenvalue. And so
    \begin{align}
        \left\| \Big(\sI_d + \eta n \nabla^2 \psi(\rvx_1^{(s+1)})\Big)^{-1} \right\|_{op} 
        \leq 1
    \end{align}
    Hence, by Eq.~\ref{eq:diff_implicit}, 
    \begin{align}
        \left\| \frac{\partial \rvx_{1}^{(s)}}{\partial \rho_i^{(s)}} \right\|_{op} 
        = \left\| \Big(\eta n \nabla^2 \psi(\rvx_1^{(s+1)}) + \sI_d \Big)^{-1} \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}} \right\|_{op}
        \leq n\eta^2 \widehat{L}^{(s)*}
    \end{align}
    Since for some symmetric real matrix $\mA$, $\text{tr}(\mA) \leq d \|\mA\|_{op}$, 
    \begin{align}
        \text{tr}(\frac{\partial \rvx_{1}^{(s+1)}}{\partial \rho_i^{(s)}})
        \leq nd \eta^2 \widehat{L}^{(s)*}
    \end{align}
    Hence, by Eq.~\ref{eq:application_steins_lemma}, for any $i\in [n]$, conditional on $\rho_j^{(s)}, \forall j\neq i$,
    \begin{align}
        \E_{\rho_i^{(s)}}\left[ \langle \rho_i^{(s)}, \rvx_{1}^{(s+1)} \rangle \mid\{\rho_j^{(s)}\}_{j\neq i} \right]
        = (\sigma^{(s)})^2\cdot \E_{\rho_i^{(s)}}\left[\text{tr}(\frac{\partial \rvx_1^{(s+1)}}{\partial \rho_i^{(s)}}) \mid\{\rho_j^{(s)}\}_{j\neq i} \right]
        \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
    \end{align}
    and by law of total expectation,
    \begin{align}
        \E\left[\langle \rho_i^{(s)}, \rvx_1^{(s+1)}\rangle \right]
        = \E\left[ \E_{\rho_i^{(s)}}\left[ \langle \rho_i^{(s)}, \rvx_1^{(s+1)}\rangle \mid \{\rho_j\}_{j\neq i} \right] \right] 
        \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
    \end{align}
    and so
    \begin{align}
        \E\left[\frac{1}{n}\sum_{i=1}^{n} \langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right]
        \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
    \end{align}
    
    
\end{proof}


\begin{lemma}[Noise Variance]
\label{lemma:noise_variance}
    For any epoch $s\in [K]$ and $\forall \rvz \in \R^d$, consider the injected noise $\rho_i^{(s)} \sim \gN(0, (\sigma^{(s)})^2 \sI_d)$, $\forall i\in [n]$, the variance caused by noise injection in epoch $s$ is, $\forall i\in [n]$,
    \begin{align}
        \E\left[\Big\| \sum_{j=1}^{i} \rho_j^{(s)}\Big\|^2\right]
        \leq id (\sigma^{(s)})^2
    \end{align}
    where the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}_{i=1}^{n}$.
\end{lemma}


\begin{proof}[Proof of Lemma~\ref{lemma:noise_variance}]
    First, note that $\rho_i^{(s)}$ and $\rho_j^{(s)}$, i.e., the noise injected at step $i$ and step $j$ in epoch $s$, are independent, for any $i \neq j$. Thus,
    \begin{align}
        \E\left[ \Big\|\sum_{j=1}^{i} \rho_j^{(s)} \Big\|^2 \right]
        &= \sum_{j=1}^{i}\E\left[\Big\| \rho_j^{(s)}\Big\|^2\right]
        + 2 \sum_{j=1}^{i} \sum_{k=j+1}^{i}
        \E\left[\langle \rho_i^{(s)}, \rho_j^{(s)}\rangle \right]\\
        &= \sum_{j=1}^{i}\E\left[\Big\| \rho_j^{(s)}\Big\|^2\right]\\
        &\leq i d (\sigma^{(s)})^2
    \end{align}
\end{proof}




% upper bound expected cumulative bias by noise injection
\begin{lemma}[Expected One Epoch Convergence]
\label{lemma:expected_one_epoch_convergence}
    Under Assumptions~\ref{ass:convexity},~\ref{ass:reg},~\ref{ass:dissim_partial_lipschitzness},~\ref{ass:appendix_refined_smoothness} and Lemma~\ref{ass:H_smoothness}, for any epoch $s\in [K]$, $\beta > 0$ and $\forall \rvz \in \R^d$, 
    if $\eta \leq \frac{1}{n\sqrt{10 \widehat{L}^{(s)} \widehat{L}^{(s)*}}}$ and $\rvz$ is independent of $\rho_i^{(s)}$, $\forall i\in [n]$,
    Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees
    \begin{align}
        &\E\left[G(\rvx_1^{(s+1)})\right] - \E\left[G(\rvz)\right]\\
        \nonumber
        &\leq  \frac{1}{2 n \eta}\Big( \E\left[\|\rvz - \rvx_1^{(s)}\|^2\right]  - \E\left[ \|\rvz - \rvx_1^{(s+1)}\|^2 \right] \Big)
        + \Big( \frac{L_H^{(s)} + \beta}{2} - \frac{\mu_{\psi}}{2} \Big) \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right]
        + 10\eta^2 n^2 \widehat{L}^{(s)} L \E\left[  B_F(\rvz, \rvx^*)\right]\\
        \nonumber
        &\quad + \underbrace{
            5 \eta^2 n^2 \widehat{L}^{(s)}\sigma_{any}^2
        }_{\text{Optimization Uncertainty}}
        + \underbrace{
            \frac{1}{2n^2 \beta}(C_n^{(s)})^2 
        }_{\text{Non-vanishing Dissimilarity}}
         + \underbrace{
            5\eta^2 \widehat{L}^{(s)*} \frac{1}{n} \sum_{i=1}^{n-1}(C_i^{(s)})^2
        }_{\text{Vanishing Dissimilarity}}
         + \underbrace{
            6 \eta^2 nd (\sigma^{(s)})^2 \widehat{L}^{(s)*}
        }_{\text{Injected Noise}}
    \end{align}
    where the expectation is taken w.r.t. both the injected noise within epoch $s$, i.e., $\{\rho_i^{(s)}\}_{i=1}^{n}$, and the shuffling operator $\pi^{(s)}$.
\end{lemma}


\begin{proof}[Proof of~\ref{lemma:expected_one_epoch_convergence}]
    By Assumption~\ref{ass:dissim_partial_lipschitzness},
    \begin{align}
        & \frac{1}{n}\Big( \sum_{i=2}^{n_d^{(s)}}
        \E_{\pi^{(s)}}\left[ \Big\| \sum_{j=1}^{i-1} \Big( \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) - \nabla f_{\pi_j^{(s)}}(\rvz)\Big) \Big\|^2\right]
        \\
        \nonumber
        &\quad \quad + \sum_{i=n_d^{(s)} + 1}^{n} \E_{\pi^{(s)}}\left[\Big\| \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz) + \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{i-n_d^{(s)}}^{(s, pub)}(\rvz)
            - \sum_{j=1}^{n_d^{(s)}} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=n_d^{(s)}+1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvz)
        \Big\|^2\right]
        \Big)\\
    \nonumber
    &\leq \frac{1}{n}\sum_{i=1}^{n-1} (C_i^{(s)})^2
    \end{align}
    By Lemma~\ref{lemma:opt_noise_bound}, for any permutation $\pi^{(s)}\in \Pi_{n}$, there is
    \begin{align}
        \frac{1}{n} \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2 \Big)
        \leq n^2 \widehat{L}^{(s)} \sigma_{any}^2
    \end{align}

    and so
    \begin{align}
        \E_{\pi^{(s)}}\left[ \frac{1}{n} \Big( \sum_{i=2}^{n_d^{(s)}}\widehat{L}_{\pi_i^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_{\pi_j^{(s)}}(\rvx^*)\Big\|^2
        + \sum_{i=n_d^{(s)}+1}^{n}\widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \nabla f_j(\rvx^*)\Big\|^2 \Big)
        \right]
        \leq n^2 \widehat{L}^{(s)} \sigma_{any}^2
    \end{align}
    where the expectation is taken w.r.t. the shuffling operator $\pi^{(s)}$.

    By Lemma~\ref{lemma:noise_variance},
    \begin{align}
        &\E_{\{\rho_j^{(s)}\}_{j=1}^{n}}\left[
            \frac{1}{n}\Big(\sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} \Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2
            + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}\Big\| \sum_{j=1}^{i-1} \rho_j^{(s)}\Big\|^2\Big)
        \right]\\
        \nonumber
        &\leq \frac{1}{n}\Big( \sum_{i=2}^{n_d^{(s)}} \widehat{L}_{\pi_i^{(s)}}^{(s)} (i-1) d (\sigma^{(s)})^2 + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)}(i-1) d (\sigma^{(s)})^2 \Big) \\
        &\leq \frac{1}{n} n d (\sigma^{(s)})^2 \Big(\sum_{i=2}^{n_d^{(s)}}\widehat{L}_i^{(s)} + \sum_{i=n_d^{(s)}+1}^{n} \widetilde{L}_{i-n_d^{(s)}}^{(s)} \Big)
        \leq n d (\sigma^{(s)})^2 \widehat{L}^{(s)}
    \end{align}

    where the expectation is taken w.r.t. the injected noise $\{\rho_j^{(s)}\}_{j=1}^{n}$.

    And by Lemma~\ref{lemma:one_epoch_convergence},  Lemma~\ref{lemma:noise_bias} and Lemma~\ref{lemma:noise_variance}, for any $\rvz\in \R^d$,
    \begin{align}
        &\E\left[ G(\rvx_1^{(s+1)})\right] - \E\left[ G(\rvz) \right]\\
        \nonumber
        &\leq \frac{1}{2 n \eta}\Big( \E\left[\|\rvz - \rvx_1^{(s)}\|^2\right]  - \E\left[ \|\rvz - \rvx_1^{(s+1)}\|^2 \right] \Big)
        + \Big( \frac{L_H^{(s)} + \beta}{2} - \frac{\mu_{\psi}}{2} \Big) \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right]\\
        \nonumber
        &\quad  + 10\eta^2 n^2 \widehat{L}^{(s)} L \E\left[  B_F(\rvz, \rvx^*)\right]
        + \frac{1}{2n^2 \beta}(C_n^{(s)})^2 
         + 5\eta^2 \widehat{L}^{(s)*} \frac{1}{n} \sum_{i=1}^{n-1}(C_i^{(s)})^2
            + 5 \eta^2 n^2 \widehat{L}^{(s)}\sigma_{any}^2 \\
        \nonumber
        &\quad + 
        \eta^2 (\sigma^{(s)})^2 nd \widehat{L}^{(s)*}
        + 5\eta^2 nd (\sigma^{(s)})^2 \widehat{L}^{(s)}\\
        &\leq  \frac{1}{2 n \eta}\Big( \E\left[\|\rvz - \rvx_1^{(s)}\|^2\right]  - \E\left[ \|\rvz - \rvx_1^{(s+1)}\|^2 \right] \Big)
        + \Big( \frac{L_H^{(s)} + \beta}{2} - \frac{\mu_{\psi}}{2} \Big) \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right]
        + 10\eta^2 n^2 \widehat{L}^{(s)} L \E\left[  B_F(\rvz, \rvx^*)\right]\\
        \nonumber
        &\quad + \underbrace{
            5 \eta^2 n^2 \widehat{L}^{(s)}\sigma_{any}^2
        }_{\text{Optimization Uncertainty}}
        + \underbrace{
            \frac{1}{2n^2 \beta}(C_n^{(s)})^2 
        }_{\text{Vanishing Dissimilarity}}
         + \underbrace{
            5\eta^2 \widehat{L}^{(s)*} \frac{1}{n} \sum_{i=1}^{n-1}(C_i^{(s)})^2
        }_{\text{Non-vanishing Dissimilarity}}
         + \underbrace{
            6 \eta^2 nd (\sigma^{(s)})^2 \widehat{L}^{(s)*}
        }_{\text{Injected Noise}}
    \end{align}
    where the expectation is taken w.r.t. both the injected noise within epoch $s$, i.e., $\{\rho_i^{(s)}\}_{i=1}^{n}$, and the shuffling operator $\pi^{(s)}$.

    
\end{proof}




\subsection{Convergence Across $K$ Epochs}
\label{subsec:appendix_k_epoch_convergence}

Now that we have the expected convergence rate for one epoch, we follow a similar approach as in~\cite{liu2024last_iterate_shuffled_gradient}, first showing the convergence for any arbitrary number of epochs $k\in [K]$ by picking proper $\rvz$ points as the virtual sequence and using a weighted telescoping sum in Lemma~\ref{lemma:convergence_across_arbitrary_epochs} and then showing the convergence for $K$ epochs in Theorem~\ref{thm:convergence_generalized_shufflg_fm_appendix}.


\begin{lemma}[Convergence Across Arbitrary Epochs]
\label{lemma:convergence_across_arbitrary_epochs}
    Under Assumptions~\ref{ass:convexity},~\ref{ass:reg},~\ref{ass:dissim_partial_lipschitzness},~\ref{ass:appendix_refined_smoothness} and Lemma~\ref{ass:H_smoothness} , for any number of epochs $k\in [K]$ and $\beta > 0$, 
    if $\mu_{\psi} \geq L_H^{(s)} + \beta$, $\forall s\in [k]$, and $\eta \leq \frac{1}{n\sqrt{10 \max_{s\in[k]} (\widehat{L}^{(s)} \widehat{L}^{(s)*}}) }$
    Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees
    \begin{align}
        &\E\left[G(\rvx_1^{(k+1)})\right] - G(\rvx^*)\\
        \nonumber
        &\leq \frac{1}{2\eta n k} \| \rvx^* - \rvx_1^{(1)}\|^2 
        + 10 \eta^2 n^2 L\max_{s\in[k]}\widehat{L}^{(s)} \sum_{s=2}^{k} \frac{1}{k+2-s} \E\left[B_F(\rvx_1^{(s)}, \rvx^*)\right]\\
        \nonumber
        &\quad + 5\eta^2 n^2 \sigma_{any}^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)} }{k+1-s}
        + \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        + 5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s}
        + 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s}
    \end{align}

\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:convergence_across_arbitrary_epochs}]
    Fix an arbitrary number of epochs $k\in [K]$.
    
    If $\mu_\psi \geq L_H^{(s)} + \beta$, 
    $\forall s\in [k]$, $\eta \leq \frac{1}{n\sqrt{10 \max_{s\in [k]} (\widehat{L}^{(s)} \widehat{L}^{(s)*}})}$, and $\rvz$ independent of $\{\rho_i^{(s)}\}_{i=1}^{n}$,
    then by Lemma~\ref{lemma:expected_one_epoch_convergence}, for any $s\in [k]$,
    \begin{align}
    \label{eq:expected_one_epoch_convergence_strong_reg}
    &\E\left[G(\rvx_1^{(s+1)})\right] - \E\left[G(\rvz)\right]\\
        \nonumber
        &\leq  \frac{1}{2 n \eta}\Big( \E\left[\|\rvz - \rvx_1^{(s)}\|^2\right]  - \E\left[ \|\rvz - \rvx_1^{(s+1)}\|^2 \right] \Big)
        + 10\eta^2 n^2 \widehat{L}^{(s)} L \E\left[  B_F(\rvz, \rvx^*)\right]\\
        \nonumber
        &\quad + 5 \eta^2 n^2 \widehat{L}^{(s)}\sigma_{any}^2
        + \frac{1}{2n^2 \beta}(C_n^{(s)})^2 
         +  5\eta^2 \widehat{L}^{(s)*} \frac{1}{n} \sum_{i=1}^{n-1}(C_i^{(s)})^2
         + 6 \eta^2 nd (\sigma^{(s)})^2 \widehat{L}^{(s)*}
    \end{align}

    Define the non-decreasing sequence
    \begin{align*}
        v_s = \frac{1}{k+1-s}, \forall s\in [k], \quad v_0 = v_1 = \frac{1}{k}
    \end{align*}
    and the auxiliary points
    \begin{align}
    \label{eq:z_def}
        \rvz^{(0)} = \rvx^*, \quad \rvz^{(s)} = \Big( 1- \frac{v_{s-1}}{v_s}\Big) \rvx_1^{(s)} + \frac{v_{s-1}}{v_s} \rvz^{(s-1)}, \forall s\in [k]
    \end{align}
    Equivalently, $\rvz^{(s)}$ can be re-written as
    \begin{align}
    \label{eq:z_eq_form}
        \rvz^{(s)} = \frac{v_0}{v_s}\rvx^* + \sum_{l=1}^{s} \frac{v_l - v_{l-1}}{v_s} \rvx_1^{(l)}, \forall s\in [0]\cup [k]
    \end{align}
    
    Note that for an epoch $s$, $\rvz^{(s)}$ only depends on $\rvx^*$ and $\rvx_1^{(l)}$, for $l \leq s$, and hence by the update rule, $\rvz^{(s)}$ is independent of $\rho_i^{(s)}$, $\forall i\in [n]$.
    And so for any $s\in [k]$, we can choose $\rvz = \rvz^{(s)}$ in Eq.~\ref{eq:expected_one_epoch_convergence_strong_reg} and this leads to

    \begin{align}
    \label{eq:expected_one_epoch_convergence_strong_reg_z}
    &\E\left[G(\rvx_1^{(s+1)})\right] - \E\left[G(\rvz)\right]\\
        \nonumber
        &\leq  \frac{1}{2 n \eta}\Big( \E\left[\|\rvz^{(s)} - \rvx_1^{(s)}\|^2\right]  - \E\left[ \|\rvz^{(s)} - \rvx_1^{(s+1)}\|^2 \right] \Big)
        + 10\eta^2 n^2 \widehat{L}^{(s)} L \E\left[  B_F(\rvz^{(s)}, \rvx^*)\right]\\
        \nonumber
        &\quad + 5 \eta^2 n^2 \widehat{L}^{(s)}\sigma_{any}^2
        + \frac{1}{2n^2 \beta}(C_n^{(s)})^2 
         +  5\eta^2 \widehat{L}^{(s)*} \frac{1}{n} \sum_{i=1}^{n-1}(C_i^{(s)})^2
         + 6 \eta^2 nd (\sigma^{(s)})^2 \widehat{L}^{(s)*}
    \end{align}

    Note that by Eq.~\ref{eq:z_def},
    \begin{align}
        \|\rvz^{(s)} - \rvx_1^{(s)}\|^2 = \frac{v_{s-1}^2}{v_s^2} \| \rvz^{(s-1)} - \rvx_1^{(s)}\|^2
        \leq \frac{v_{s-1}}{v_s}\|\rvz^{(s-1)} - \rvx_1^{(s)}\|^2
    \end{align}
    where the last inequality is due to $v_{s-1} \leq v_s$. Hence, following Eq.~\ref{eq:expected_one_epoch_convergence_strong_reg_z},

    \begin{align}
    \label{eq:convergence_across_epochs_interm}
        &v_s \cdot \left(\E\left[G(\rvx_1^{(s+1)})\right] - \E\left[G(\rvz)\right] \right)\\
        \nonumber
        &\leq  \frac{1}{2 n \eta}\Big( \E\left[ v_{s-1} \|\rvz^{(s-1)} - \rvx_1^{(s)}\|^2\right]  - v_s \E\left[ \|\rvz^{(s)} - \rvx_1^{(s+1)}\|^2 \right] \Big)
        + 10 v_s \eta^2 n^2 \widehat{L}^{(s)} L \E\left[  B_F(\rvz^{(s)}, \rvx^*)\right]\\
        \nonumber
        &\quad + 5 v_s \eta^2 n^2 \widehat{L}^{(s)}\sigma_{any}^2
        + v_s \frac{1}{2n^2 \beta}(C_n^{(s)})^2 
         +  5v_s \eta^2 \widehat{L}^{(s)*} \frac{1}{n} \sum_{i=1}^{n-1}(C_i^{(s)})^2
         + 6 v_s \eta^2 nd (\sigma^{(s)})^2 \widehat{L}^{(s)*}
    \end{align}
    Summing Eq.~\ref{eq:convergence_across_epochs_interm} from $s = 1$ to $k$ to obtain
    \begin{align}
    \label{eq:telescoping_sum_result}
        &\sum_{s=1}^{k} v_s \cdot \left(\E\left[G(\rvx_1^{(s+1)})\right] - \E\left[G(\rvz)\right] \right)\\
        \nonumber
        &\leq \frac{1}{2 n \eta} \Big(\E\left[ v_0 \| \rvz^{(0)} - \rvz_1^{(1)}\|^2 \right]
        - v_{k}\E\left[\| \rvz^{(k)} - \rvx_1^{(k+1)}\|^2\right]\Big)
        + 10 \eta^2 n^2 L \sum_{s=1}^{k} \widehat{L}^{(s)} v_s \E\left[B_F(\rvz^{(s)}, \rvx^*)\right]\\
        \nonumber
        &\quad + 5\eta^2 n^2 \sigma_{any}^2 \sum_{s=1}^{k} v_s \widehat{L}^{(s)}
        + \frac{1}{2n^2 \beta}\sum_{s=1}^{k} v_s (C_n^{(s)})^2
        + 5\eta^2 \sum_{s=1}^{k} v_s \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2
        + 6\eta^2 nd \sum_{s=1}^{k} v_s (\sigma^{(s)})^2 \widehat{L}^{(s)*}
    \end{align}

     Note since $\|\rvz^{(k)} - \rvx_1^{(k+1)}\|^2 \geq 0$ and $\rvz^{(0)} = \rvx^*, v_0 = \frac{1}{k}$,
    \begin{align}
    \label{eq:convergence_across_epochs_tele_sum}
        \frac{1}{2\eta n}\Big( v_0 \E\left[ \| \rvz^{(0)} - \rvx_1^{(1)}\|^2 \right] - v_k \E\left[ \| \rvz^{(k)} - \rvx_1^{(k+1)} \|^2 \right] \Big)
        \leq \frac{1}{2\eta n k} \| \rvx^* - \rvx_1^{(1)}\|^2 
    \end{align}

    We first bound the L.H.S. of Eq.~\ref{eq:telescoping_sum_result}.
    By Assumption~\ref{ass:convexity} and~\ref{ass:reg}, the objective function $G(\rvx) = F(\rvx) + \psi(\rvx)$ is convex, and hence, by Eq.~\ref{eq:z_eq_form}, there is
    \begin{align}
        G(\rvz^{(s)}) \leq \frac{v_0}{v_s} G(\rvx^*) + \sum_{l=1}^{s}\frac{v_l - v_{l-1}}{v_s} G(\rvx_1^{(l)})
        = G(\rvx^*) + \sum_{l=1}^{s}\frac{v_l - v_{l-1}}{v_s} \Big( G(\rvx_1^{(l)}) - G(\rvx^*)\Big)
    \end{align}
    which implies
    \begin{align}
        &\sum_{s=1}^{k} v_s \Big( G(\rvx_1^{(s+1)}) - G(\rvz^{(s)})\Big)\\
        \nonumber
        &\geq \sum_{s=1}^{k} \Big( v_s \Big(G(\rvx_1^{(s+1)}) - G(\rvx^*)\Big) - \sum_{l=1}^{s} (v_l - v_{l-1}) \Big(G(\rvx_1^{(l)}) - G(\rvx^*)\Big) \Big)\\
        &\geq \sum_{s=1}^{k} v_s \Big(G(\rvx_1^{(s+1)}) - G(\rvx^*)\Big)
        - \sum_{s=1}^{k}\sum_{l=1}^{s} (v_l - v_{l-1}) \Big(G(\rvx_1^{(l)}) - G(\rvx^*)\Big)\\
        &= \sum_{s=1}^{k} v_s \Big(G(\rvx_1^{(s+1)}) - G(\rvx^*)\Big)
        - \sum_{l=1}^{k}(k+1-l) (v_l - v_{l-1}) \Big(G(\rvx_1^{(l)}) - G(\rvx^*)\Big)\\
        &= v_k \Big( G(\rvx_1^{(k+1)}) - G(\rvx^*) \Big)
        + \sum_{s=1}^{k-1} \frac{1}{k+1-s} \Big(G(\rvx_1^{(s+1)}) - G(\rvx^*)\Big)\\
        \nonumber
        &\quad - k(v_1 - v_0)\Big( G(\rvx_1^{(1)}) - G(\rvx^*) \Big)
        - \sum_{l=2}^{k}(k+1-l)(\frac{1}{k+1-l} - \frac{1}{k+2-l})\Big(G(\rvx_1^{(l)}) - G(\rvx^*)\Big)\\
        &= v_k \Big( G(\rvx_1^{(k+1)}) - G(\rvx^*) \Big)
        + \sum_{s=2}^{k} \frac{1}{k+2-s} \Big( G(\rvx_1^{(s)}) - G(\rvx^*) \Big)\\
        \nonumber
        &\quad - k(v_1 - v_0) \Big( G(\rvx_1^{(1)}) - G(\rvx^*)\Big)
        - \sum_{l=2}^{k}\frac{1}{k+2-l}\Big(G(\rvx_1^{(l)}) - G(\rvx^*)\Big)\\
        &= v_k \Big( G(\rvx_1^{(k+1)}) - G(\rvx^*) \Big) - k(v_1 - v_0)\Big( G(\rvx_1^{(1)}) - G(\rvx^*)\Big)
    \end{align}
    Note that $v_1 = v_0$ and $v_k = 1$ by definition, and so taking expectation of both sides,
    \begin{align}
    \label{eq:convergence_across_arbitrary_epochs_lhs}
        \sum_{s=1}^{k} v_s\Big( \E\left[G(\rvx_1^{(s+1)}) \right] - \E\left[G(\rvz^{(s)})\right]\Big)
        \geq \E\left[G(\rvx_1^{(k+1)})\right] - G(\rvx^*)
    \end{align}

    We now bound the term involving $\E\left[B_F(\rvz^{(s)}, \rvx^*)\right]$ in the R.H.S. of Eq.~\ref{eq:telescoping_sum_result}. 
    By the convexity of $B_F(\cdot, \rvx^*)$ fixing the second argument (due to $F$ being convex), and Eq.~\ref{eq:z_eq_form}, 
    \begin{align}
        B_F(\rvz^{(s)}, \rvx^*) \leq \frac{v_0}{v_s}B_F(\rvx^*, \rvx^*) + \sum_{l=1}^{s}\frac{v_l - v_{l-1}}{v_s}B_F(\rvx_1^{(l)}, \rvx^*)
        = \sum_{l=1}^{s}\frac{v_l - v_{l-1}}{v_s}B_F(\rvx_1^{(l)}, \rvx^*)
    \end{align}
    which implies
    \begin{align}
        10 \eta^2n^2 L \sum_{s=1}^{k} v_s \widehat{L}^{(s)} \E\left[B_F(\rvz^{(s)}, \rvx^*)\right]
        &\leq 10 \eta^2 n^2
        L \max_{s\in[k]}\widehat{L}^{(s)}
        \sum_{s=1}^{k} \sum_{l=1}^{s} (v_l - v_{l-1})\E\left[B_F(\rvx_1^{(l)}, \rvx^*)\right]\\
        &= 10 \eta^2 n^2 L \max_{s\in[k]}\widehat{L}^{(s)}
        \sum_{l=1}^{k} (k+1-l)(v_l - v_{l-1}) \E\left[B_F(\rvx_1^{(l)}, \rvx^*)\right] \\
        &= 10 \eta^2 n^2 L \max_{s\in[k]}\widehat{L}^{(s)}
        \sum_{l=2}^{k} (k+1-l)(\frac{1}{k+1-l} - \frac{1}{k+2-l}) \E\left[B_F(\rvx_1^{(l)}, \rvx^*)\right] \\
        &= 10 \eta^2 n^2 L \max_{s\in[k]}\widehat{L}^{(s)} \sum_{l=2}^{k} \frac{1}{k+2-l} \E\left[B_F(\rvx_1^{(l)}, \rvx^*)\right]\\
    \label{eq:convergence_across_epochs_bregmandiv}
        &= 10 \eta^2 n^2 L \max_{s\in[k]}\widehat{L}^{(s)}   \sum_{l=2}^{k} v_{l-1} \E\left[B_F(\rvx_1^{(l)}, \rvx^*)\right]
    \end{align}

    Therefore, plugging Eq.~\ref{eq:convergence_across_epochs_tele_sum}, Eq.~\ref{eq:convergence_across_arbitrary_epochs_lhs} and Eq.~\ref{eq:convergence_across_epochs_bregmandiv} back to Eq.~\ref{eq:telescoping_sum_result}, there is, for any fixed epoch $k\in [K]$
    \begin{align}
        &\E\left[G(\rvx_1^{(k+1)})\right] - G(\rvx^*)\\
        \nonumber
        &\leq \frac{1}{2\eta n k} \| \rvx^* - \rvx_1^{(1)}\|^2
        + 10 \eta^2 n^2 L\max_{s\in[k]}\widehat{L}^{(s)} \sum_{s=2}^{k} \frac{1}{k+2-s} \E\left[B_F(\rvx_1^{(s)}, \rvx^*)\right]\\
        \nonumber
        &\quad + 5\eta^2 n^2 \sigma_{any}^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)} }{k+1-s}
        + \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        + 5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s}
        + 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s}
    \end{align}



\end{proof}


\begin{theorem}[Convergence of Generalized Shuffled Gradient Framework (Re-statement of Theorem~\ref{thm:convergence_generalized_shufflg_fm})]
\label{thm:convergence_generalized_shufflg_fm_appendix}

    Under Assumptions~\ref{ass:convexity},~\ref{ass:reg},~\ref{ass:dissim_partial_lipschitzness},~\ref{ass:appendix_refined_smoothness} and Lemma~\ref{ass:H_smoothness}, for $\beta > 0$, 
    if $\mu_{\psi} \geq L_H^{(s)} + \beta$, $\forall s\in [K]$, and 
     $\eta \leq \frac{1}{2n \sqrt{10 \bar{L}^* \max_{s\in [K]}\widehat{L}^{(s)*} (1+\log K)}}$, 
    where $\bar{L}^* = \max\{L, \max_{s\in[K]} \widehat{L}^{(s)}\}$,
    Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees

    \begin{align}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - G(\rvx^*)\\
        \nonumber
        &\leq \underbrace{ \frac{1}{\eta n K} \|\rvx^* - \rvx_1^{(1)}\|^2 
        }_{\text{Initialization}}
        + \underbrace{ 10 \eta^2 n^2 \sigma_{any}^2 (1+\log K)\max_{s\in[K]} \widehat{L}^{(s)} 
        }_{\text{Optimization Uncertainty}} + 2 M
    \end{align}
    where
    \begin{align*}
        M = \max_{k\in [K]} \Big(
        \underbrace{ \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        }_{\text{Non-vanishing Dissimilarity}}
        + \underbrace{5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s} }_{\text{Vanishing Dissimilarity}}
        + \underbrace{ 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s} }_{\text{Injected Noise}} \Big)
    \end{align*}
    and the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}$ and the order of samples $\pi^{(s)}$, $\forall i\in [n], s\in [K]$.
\end{theorem}


\begin{proof}[Proof of Theorem~\ref{thm:convergence_generalized_shufflg_fm_appendix}]
    Taking the learning rate $\eta \leq \frac{1}{2n \sqrt{20 \bar{L}^* \max_{s\in [K]}\widehat{L}^{(s)*} (1+\log K)}}$, 
    where $\bar{L}^* = \max\{L, \max_{s\in[K]} \widehat{L}^{(s)}\}$, i.e., the max average smoothness parameters, satisfies the condition of Lemma~\ref{lemma:convergence_across_arbitrary_epochs}, and so for any number of epochs $k\in [K]$,
    \begin{align}
        &\E\left[G(\rvx_1^{(k+1)})\right] - G(\rvx^*)\\
        \nonumber
        &\leq \frac{1}{2\eta n k} \| \rvx^* - \rvx_1^{(1)}\|^2
        + 10 \eta^2 n^2 L\max_{s\in[k]}\widehat{L}^{(s)} \sum_{s=2}^{k} \frac{1}{k+2-s} \E\left[B_F(\rvx_1^{(s)}, \rvx^*)\right]\\
        \nonumber
        &\quad + 5\eta^2 n^2 \sigma_{any}^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)} }{k+1-s}
        + \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        + 5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s}
        + 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s}
    \end{align}

    Note that $\sum_{s=1}^{k} v_s = \sum_{s=1}^{k} \frac{1}{k+1-s} = \sum_{s=1}^{k} \frac{1}{s}\leq 1+\log k$, and so
    \begin{align*}
        5 \eta^2 n^2 \sigma_{any}^2 \sum_{s=1}^{k} \frac{\widehat{L}^{(s)}}{k+1-s}
        \leq \eta^2 n^2 \sigma_{any}^2  (1+\log k) \max_{s\in [k]} \widehat{L}^{(s)}
    \end{align*}
    Hence,
    \begin{align}
        &\E\left[G(\rvx_1^{(k+1)})\right] - G(\rvx^*) \\
        \nonumber
        &\leq \frac{1}{2\eta n k}  \| \rvx^* - \rvx_1^{(1)}\|^2 
        + 10 \eta^2 n^2 L\max_{s\in[k]}\widehat{L}^{(s)} \sum_{s=2}^{k} \frac{1}{k+2-s} \E\left[B_F(\rvx_1^{(s)}, \rvx^*)\right]\\
        \nonumber
        &\quad + 5\eta^2 n^2 \sigma_{any}^2 (1+\log k) \max_{s\in [k]}\widehat{L}^{(s)}
        + \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        + 5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s}
        + 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s}
    \end{align}

    By the optimality condition, $\nabla G(\rvx^*) = \nabla F(\rvx^*) + \nabla \psi(\rvx^*) = \mathbf{0}$. Thus, for any $k\in [K]$,
    \begin{align}
        \E\left[G(\rvx_1^{(k+1)})\right] - G(\rvx^*) 
        &\geq \E\left[ G(\rvx_1^{(k+1)})\right] - \E\left[ G(\rvx^*)\right] - \E\left[\langle \nabla F(\rvx^*) + \nabla \psi(\rvx^*), \rvx_1^{(k+1)} - \rvx^* \rangle \right] \\
        &= \E\left[ B_F(\rvx_1^{(k+1)},\rvx^*)\right] + \E\left[ B_{\psi}(\rvx_1^{(k+1)}, \rvx^*)\right]
        \geq \E\left[ B_F(\rvx_1^{(k+1)}, \rvx^*)\right]
    \end{align}
    which implies that for any $k\in [K]$,
    \begin{align}
        &\E\left[B_F(\rvx_1^{(k+1)}), \rvx^*)\right]\\
        \nonumber
        &\leq \frac{1}{2\eta n k} \| \rvx^* - \rvx_1^{(1)}\|^2
        + 10 \eta^2 n^2 L\max_{s\in[k]}\widehat{L}^{(s)} \sum_{s=2}^{k} \frac{1}{k+2-s} \E\left[B_F(\rvx_1^{(s)}, \rvx^*)\right]\\
        \nonumber
        &\quad + 5\eta^2 n^2 \sigma_{any}^2 (1+\log k) \max_{s\in [k]}\widehat{L}^{(s)}
        + \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        + 5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s}
        + 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s}
    \end{align}
    Note that $\max_{s\in [k]}\widehat{L}^{(s)} \leq \max_{s\in [K]} \widehat{L}^{(s)}$.
    
    Now we apply Lemma~\ref{lemma:bregmandiv_relationship} with
    \begin{itemize}[itemsep=0mm]
        \item $d^{(k+1)}=\begin{cases}
                \E\left[ B_F(\rvx_1^{(k+1)}, \rvx^*)\right] &k \in [K-1]\\
                \E\left[ F(\rvx_1^{(K+1)})  \right] - \E\left[ F(\rvx^*) \right]
                &k = K
            \end{cases}$
        \item $e^{(s)} = \frac{1}{2n^2 \beta}(C_n^{s})^2 + 5\eta^2 \widehat{L}^{(s)*}\frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2 + 6\eta^2 nd (\sigma^{(s)})^2 \widehat{L}^{(s)*}, \forall s\in [K]$
        \item $a = \frac{1}{2\eta n}\| \rvx_1^{(1)} - \rvx^*\|^2$
        \item $b = 5\eta^2 n^2 \sigma_{any}^2(1+\log k) \max_{s\in [K]} \widehat{L}^{(s)}$
        \item $c = 10\eta^2 n^2 L \max_{s\in[K]} \widehat{L}^{(s)}$
    \end{itemize}
    to obtain
    \begin{align}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - G(\rvx^*)\\
        \nonumber
        &\leq \Big( \frac{1}{2\eta n K} \|\rvx^* - \rvx_1^{(1)}\|^2
        + 5\eta^2 n^2 \sigma_{any}^2 (1+\log K)\max_{s\in[K]} \widehat{L}^{(s)} + M\Big)
        \sum_{i=0}^{K-1} \Big(20 \eta^2 n^2 L \max_{s\in [K]}\widehat{L}^{(s)}(1+\log K) \Big)^{i}
    \end{align}
    where 
    \begin{align*}
        M = \max_{k\in [K]} \Big(
        \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        + 5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s}
        + 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s} \Big)
    \end{align*}

    By setting $\eta \leq \frac{1}{2n \sqrt{10 \bar{L}^{*} \max_{s\in [K]}\widehat{L}^{(s)*} (1+\log K)}}$, 
    where $\bar{L}^{*} = \max\{L, \max_{s\in[K]} \widehat{L}^{(s)}\}$,
    there is
    \begin{align}
        \sum_{i=0}^{K-1} \Big(20 \eta^2 n^2 L \max_{s\in [K]}\widehat{L}^{(s)}(1+\log K) \Big)^{i}
        \leq \sum_{i=0}^{K-1} \Big(
            \frac{n^2 L \max_{s\in[K]}\widehat{L}^{(s)} (1+\log K)}{2 n^2 \bar{L}^* \max_{s\in[K]} \widehat{L}^{(s)*} (1+\log K)}
        \Big)^{i}
        \leq \sum_{i=0}^{\infty} \frac{1}{2^{i}} = 2
    \end{align}

    Therefore,
    \begin{align}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - G(\rvx^*) \\
        \nonumber
        &\leq \frac{1}{\eta n K} \|\rvx^* - \rvx_1^{(1)}\|^2 
        + 10 \eta^2 n^2 \sigma_{any}^2 (1+\log K)\max_{s\in[K]} \widehat{L}^{(s)} + 2 M
    \end{align}
    
\end{proof}


