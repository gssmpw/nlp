\section{Private Shuffled Gradient Methods}
\label{sec:appendix_private_shuffled_g}

\subsection{Convergence Analysis}


We give the full convergence bound of $\DPSG$ as follows.
Since the full $\gD$ is used across all epochs, i.e., $n_d^{(s)} = n$, let the dissimilarity measure (see Assumption~\ref{ass:dissim_partial_lipschitzness}) be $C_i^{(s)} = C_i$, $\forall i < n$ and recall that $C_n^{(s)} = 0$, $\forall s\in [K]$. 

And by Theorem~\ref{thm:convergence_generalized_shufflg_fm}, the convergence of $\DPSG$ is
\begin{align}
    &\E\left[ G(\rvx_1^{(K+1)}) \right] - G(\rvx^*) \leq \underbrace{
    \frac{1}{\eta n K}\|\rvx^* - \rvx_1^{(1)}\|^2
    }_{\text{Initialization}}
    + \underbrace{ 10 \eta^2 n^2 \sigma_{any}^2 (1+\log K)\max_{s\in[K]} \widehat{L}^{(s)}
    }_{\text{Optimization Uncertainty}}\\
    \nonumber
    &\quad + \underbrace{ 10 \eta^2 \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n} (C_i)^2 (1+\log K)
    }_{\text{Vanishing Dissimilarity}}
    + \underbrace{ 12 \eta^2 nd \sigma^2 \widehat{L}^{(s)*} }_{\text{Noise Injection}}
\end{align}

Specifically, we note that the vanishing dissimilarity term is at most the order of the optimization uncertainty term. 
And hence, we merge these two terms in the simplified version of the bound given in Corollary~\ref{corollary:conv_dpsg}.



\subsection{Privacy Analysis}
\label{subsec:appendix_private_shuffled_g_privacy}




\begin{lemma}[Privacy of $\DPSG$ (Restatement of Lemma~\ref{lemma:privacy_private_shuffled_g})]
\label{lemma:appendix_privacy_private_shuffled_g}
    Under Assumptions~\ref{ass:smoothness} and~\ref{ass:lipschitzness}, if the learning rate is $\eta \leq 1/L$, $\DPSG$ is $(\frac{2\alpha G^2 K}{\sigma^2} + \frac{\log 1/\delta}{\alpha - 1}, \delta)$-DP, for $\alpha > 1, \delta \in (0, 1)$.
\end{lemma}


\begin{proof}[Proof of Lemma~\ref{lemma:appendix_privacy_private_shuffled_g}]
    We first show the privacy loss per epoch by using privacy amplification by iteration (PABI) in Renyi Differential Privacy (RDP, see Definition~\ref{def:RDP}), and then use the composition theorem of RDP (see Proposition~\ref{prop:rdp_composition}) to derive the total privacy loss across $K$ epochs. Finally, we convert the privacy loss in RDP to DP based on Proposition~\ref{prop:rdp_to_dp}. 

    By Remark~\ref{remark:contractive_operators}, if one sets the learning rate in $\DPSG$ as $\eta \leq 1/L^{*}$, each gradient update step in epoch $s\in [K]$, i.e., $\rvx_{i+1}^{(s0} = \rvx_{i}^{(s)} - \eta \left(\nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i^{(s)}) + \rho_i^{(s)} \right)$, $\forall i\in [n]$, (line 8 of Algorithm~\ref{alg:generalized_shuffled_gradient_fm}) satisfies a ``noisy contractive sequence'' (CNI, see Definition~\ref{def:cni}) and hence, we can apply Theorem~\ref{thm:pabi} to reason about the privacy loss of epoch $s$, $\forall s\in[K]$.

    Let $\gD = \{\rvd_1, \dots, \rvd_n\}$ and $\gD' = \{\rvd_1, \dots, \rvd_{t}', \dots, \rvd_n\}$ be two neighboring datasets that differ at index $t$. 
    On dataset $\gD$, the CNI is defined by the initial point $\rvx_1^{(s)}$, sequence of functions $g_i(\rvx) = \rvx_i^{(s)} - \eta \nabla f(\rvx; \rvd_{\pi_i^{(s)}})$, for all $\rvx$, and sequence of noise distributions $\gN(0, (\eta \sigma)^2 \sI_d)$. Similarly, on dataset $\gD'$, the CNI is defined 
    in the same way with the exception of $g_j'(\rvx) = \rvx - \eta \nabla f(\rvx; \rvd_{\pi_j^{(s)}}')$ for the index $j$ such that $\pi_j^{(s)} = t$. 
    Let $\rvx_{n+1}^{(s)}$, $(\rvx_{n+1}^{(s)})'$ be the output of the CNI on dataset $\gD$ and $\gD'$, respectively.
    
    By Assumption~\ref{ass:lipschitzness}, $f(\rvx; \rvd_i)$ is $G_i$-Lipschitz, and hence,
    \begin{align*}
        \sup_{\rvw}\| g_j(\rvx) - g_j'(\rvx) \| = \sup_{\rvw}\| \eta \nabla f(\rvx; \rvd_{t}) - \eta \nabla f(\rvx; \rvd_{t}') \| \leq 2 \eta G^{*}    
    \end{align*}
    where recall that $G^{*} = \max_{i\in [n]} \{G_i\}$.

    We now apply Theorem~\ref{thm:pabi} with $a_1, \dots, a_{j-1} = 0$ and $a_{j}, \dots, a_n = \frac{2\eta G^{*}}{n - j + 1}$. Note that $s_{\pi_j^{(s)}} = 2 \eta G^{*}$ and $s_{i} = 0$, $\forall i \neq \pi_j^{(s)}$. In addition, $z_i \geq 0$ for all $i \leq n$ and $z_n = 0$. Hence,
    \begin{align*}
        \infdivalpha{\rvx_{n+1}^{(s)}}{(\rvx_{n+1}^{(s)})'} \leq \sum_{i=j}^{n} \frac{\alpha}{2 \eta^2 \sigma^2} \cdot \frac{4 \eta^2 (G^{*})^2}{(n - j + 1)^2}
        = \frac{2 \alpha (G^{*})^2}{\sigma^2 (n-j + 1)}
    \end{align*}
    The maximum privacy loss happens when $j = n$, that is, when the sample $\rvd_{t}$ is the last one being processed in epoch $s$. 
    And it is not hard to see that $\max_{j\in [n]} \infdivalpha{\rvx_{n+1}^{(s)}}{(\rvx_{n+1}^{(s)})'} \leq \frac{2\alpha (G^*)^2}{\sigma^2}$,
    which implies $\rvx_{n+1}^{(s)}$ in Algorithm~\ref{alg:generalized_shuffled_gradient_fm} is $(\alpha, \frac{2\alpha (G^*)^2}{\sigma^2})$-RDP, for $\alpha > 1$.
    The output of epoch $s$, $\rvx_1^{(s+1)}$ can be seen as a post-processing step of $\rvx_{n+1}^{(s)}$, and hence, $\rvx_1^{(s+1)}$ is also $(\alpha, \frac{2\alpha (G^*)^2}{\sigma^2})$-RDP.
    
    By Proposition~\ref{prop:rdp_composition}, the output $\rvx_1^{(K+1)}$ is $(\alpha, \frac{2\alpha (G^{*})^2 K}{\sigma^2})$-RDP. And by Proposition~\ref{prop:rdp_to_dp}, $\rvx_1^{(K+1)}$ is $(\frac{2\alpha (G^{*})^2 K}{\sigma^2} + \frac{\log 1/\delta}{\alpha - 1}, \delta)$-DP, for $\alpha > 1$ and $\delta \in (0, 1)$.
    
\end{proof}


\textbf{Remark on the Privacy Loss of Random Reshuffling (RR).}
For Incremental Gradient Methods (IG) and Shuffle Once (SO), it is not hard to see that the worst-case privacy loss bound presented above is tight. One might argue that, in the case of Random Reshuffling (RR), where the permutation $\pi_i^{(s)}$ is re-generated at the beginning of each epoch, leading to a reshuffling of the sample order in $\gD$, this additional randomness could amplify privacy, thereby reducing the privacy loss. However, we argue that this potential improvement is limited to a constant level. 
Deriving a significantly smaller privacy loss bound in the RR setting -- such as one that scales proportionally to $1/n$ -- is unlikely without additional assumptions.

We use the following lemma to derive a tighter bound on the privacy loss of RR per epoch, taking into account the randomness introduced by shuffling:

\begin{lemma}[Joint convexity of scaled exponentiation of RÃ©nyi divergence, Lemma 4.1 of~\cite{ye2022singapore_paper}]
\label{lemma:joint_convexity_renyi_div}
    Let $\mu_1, \dots, \mu_m$ and $\nu_1, \dots, \nu_m$ be distributions over $\R^d$. Then, for any RDP order $\alpha \geq 1$, and any coefficients $p_1, \dots, p_m \geq 0$ that satisfy $p_1 + \dots + p_m = 1$, the following inequality holds
    \begin{align}
        e^{(\alpha-1) \infdivalpha{\sum_{j=1}^{m} p_j \mu_j}{\sum_{j=1}^{m} p_j \nu_j} }
        \leq \sum_{j=1}^{m} p_j \cdot e^{(\alpha-1) \infdivalpha{\mu_j}{\nu_j} }
    \end{align}
\end{lemma}

From the previous proof and the PABI bound (Theorem~\ref{thm:pabi}), we observe that the privacy loss for a single epoch is primarily determined by the index $j$ such that $\pi_j^{(s)} = t$, where $t$ is the index of the sample at which the two neighboring datasets
$\gD$ and $\gD'$ differ ($\rvd_t \in \gD$ and $\rvd_{t}' \in \gD'$).
Since shuffling ensures that $j$ can take any value in $\{1,2,\dots, n\}$ with equal probability, $j$ is a random variable uniformly distributed over $[n]$.

We apply Lemma~\ref{lemma:joint_convexity_renyi_div} by instantiating the distributions $\mu_i$ as the CNI's on $\gD$ with $j = i$ for value $i\in [n]$ and similarly, the distributions $\nu_i$ as the CNI's on $\gD'$ with $j = i$ for value $i\in [n]$. It easy to see that $p_i = \frac{1}{n}$. Hence, privacy loss of RR, $\eps_{\text{per-epoch}}^{(s)}$ for epoch $s$, is given by
\begin{align}
    \eps_{\text{per-epoch}} &= \frac{1}{\alpha-1} \log e^{(\alpha-1) \cdot \infdivalpha{\rvx_{n+1}^{(s)}}{\rvx_{n+1}^{(s)}}}\\
    &\leq \frac{1}{\alpha-1} \log \left(
        \frac{1}{n}\sum_{j=1}^{n} e^{(\alpha-1)\cdot \frac{2\alpha (G^{*})^2}{\sigma^2 (n-j+1)}}
    \right)
    = \frac{1}{\alpha -1}\log \left(
        \underbrace{ \frac{1}{n}\sum_{j=1}^{n} e^{(\alpha-1) \cdot \frac{2\alpha(G^*)^2}{\sigma^2\cdot j}}
        }_{:=S_n}
    \right)
\end{align}

In shuffled gradient methods, the dataset size $n$ is finite and usually small to allow for $K \geq 2$ epochs over the dataset to ensure convergence. Therefore, we cannot asymptotically approximate the bound by treating $n\rightarrow \infty$. 
When $n$ is small, the term $S_n$ in bound is dominated by $e^{(\alpha-1) \frac{2\alpha(G^*)^2}{\sigma^2}}$ and $\frac{1}{n}$,
leading to the approximation
$S_n \approx \frac{1}{n}e^{(\alpha-1) \frac{2\alpha (G^*)^2}{\sigma^2}}$.
Consequently, the upper bound on $\eps_p$ becomes $\frac{1}{\alpha - 1}\log S_n \approx \frac{2\alpha (G^*)^2}{\sigma^2} - \log n$.
This indicates the privacy loss bound for random reshuffling (RR) is nearly identical to that of IG and SO. As a result, the shuffling operation provides only a marginal improvement in privacy loss in this case.

Similar privacy loss bounds occur in the PABI-based privacy analysis of (impractical) variants of SGD and one can of course apply strong assumptions on $\infdivalpha{\rvx_{n+1}^{(s)}}{(\rvx_{n+1}^{(s)})'}$ to reduce the above upper bound. See, for example, Lemma 25 of the seminal work on PABI~\cite{Feldman2018privacy_amp_iter}.


\subsection{Computing the Empirical Excess Risk}
\label{subsec:appendix_private_shuffle_g_empirical_excess_risk}


To ensure the output $\rvx_1^{(K+1)}$ is $(\eps, \delta)$-DP, we set $\alpha = \frac{\sigma \sqrt{\log(1/\delta)}}{G^{*} \sqrt{2K}}$ based on Lemma~\ref{lemma:privacy_private_shuffled_g} to minimize the overall privacy loss. This choice implies $\sigma = \gO\left(\frac{G^{*} \sqrt{K \log(1/\delta)}}{\eps}\right)$.
By Corollary~\ref{corollary:convergence_dpsg}, the learning rate is set to be $\eta = \gO\left(\frac{\|\rvx_1^{(1)} - \rvx^{*} \|^{2/3}}{n L^{*} \left(K (1 + \log K)\right)^{1/3}}\right)$ to optimize the convergence bound, while satisfying the conditions of both Corollary~\ref{corollary:convergence_dpsg} (convergence) and Lemma~\ref{lemma:privacy_private_shuffled_g} (privacy). After choosing the learning rate, the convergence bound of $\DPSG$ is now given by
\begin{align*}
    \E\left[G(\rvx_1^{(K+1)})\right] - \E\left[G(\rvx^*)\right]
    &\leq \gO\Big(\|\rvx_1^{(1)} - \rvx^*\|^{4/3} (1+\log K)^{1/3} 
    \Big( \frac{L^*}{K^{2/3}} + \frac{1}{L^* K^{2/3}} + \frac{d (G^*)^2 K^{1/3}\log 1/\delta}{n L^* \eps^2} \Big) \Big)
\end{align*}
Finally, setting the number of epochs as
$K = \gO\left(\frac{n\eps^2}{d} \right)$ to minimize the above bound, resulting in
the following empirical excess risk:
\begin{align*}
    \E\left[G(\rvx_1^{(K+1)})\right] - \E\left[G(\rvx^*)\right]
    = \widetilde{O}\left( \frac{1}{n^{2/3}}(\frac{\sqrt{d}}{\eps})^{4/3} \right)
\end{align*}
with respect to $n$, $d$, and $\eps$, while ignoring other terms. 
Here, $\widetilde{\gO}$ hides logarithmic factors in $(n, d, 1/\delta)$.






\section{Private Shuffled Gradient Methods with Public Data}
\label{sec:appendix_algo_pub_data}

In this section, we give more details on algorithms that leverage public data samples to improve the privacy-convergence trade-offs.

\textbf{Setting.} 
In addition to the private dataset $\gD$, which defines the target objective (Eq.~\ref{eq:main_problem}), suppose we have access to a single public dataset $\gP = \{\rvp_1, \dots, \rvp_n\}$. Furthermore, let $f(\rvx; \rvp_i)$ be $\widetilde{L}_i$-smooth and $\widetilde{G}_i$-Lipschitz, $\forall i\in [n]$. The maximum and average smoothness parameters of the objective functions on the public dataset are then defined as $\widetilde{L}^{*} = \max_{i\in[n]}\widetilde{L}_i$ and $\widetilde{L} = \frac{1}{n}\sum_{i=1}^{n} \widetilde{L}_i$, respectively. 
The maximum Lipschitz parameter of the objective functions on the public dataset is defined as $\widetilde{G}^{*} = \max_{i\in [n]} \widetilde{G}_i$.

\subsection{Parameters and Convergence Analysis}
\label{subsec:appendix_algo_pub_data_param_convergence}

Each algorithm that leverages public data samples --- $\privpub$, $\pubpriv$ and $\interleaved$ --- is an instantiation of the generalized shuffled gradient framework (Algorithm~\ref{alg:generalized_shuffled_gradient_fm}) with specific parameters.
We summarize the key parameters of each algorithm in Table~\ref{tab:algo_pub_samples}. 

The specific parameter choices for each algorithm result in different dissimilarity measures and the maximum smoothness parameter, both of which are critical factors in the convergence bound. Their values are summarized in Table~\ref{tab:dissim_max_smoothness}.

\textbf{Convergence.} We now present the convergence of each algorithm, as corollaries of Theorem~\ref{thm:convergence_generalized_shufflg_fm}, in Corollary~\ref{corollary:convergence_pub_data_opt}.
Note that to ensure the following bounds are tight, we enforce that the number of pre-determined epochs to be $S \in \{1,2,\dots,K-1\}$.


\begin{table}[]
    \centering
\begin{threeparttable}
    \begin{tabular}{|c|l|l|l|}
    \hline
         \diagbox{Parameters}{Algorithm} & $\privpub$ & $\pubpriv$ & $\interleaved$\\
    \hline
    %      Datasets: $(\gD^{(s)}, \gP^{(s)})$ 
    %      & $ = \begin{cases}
    %         (\gD, \emptyset) & \text{if $s \leq S$\tnote{\S}}\\
    %         (\emptyset, \gP) & \text{if $S < s \leq K$}
    %      \end{cases}$
    %      & $= \begin{cases}
    %          (\emptyset, \gP) & \text{if $s \leq S$}\\
    %          (\gD, \emptyset) & \text{if $S < s \leq K$}
    %      \end{cases}$
    %      & $(\gD, \gP)$ \tnote{\dag} \\
    % \hline
        \# private samples used: $n_d^{(s)}$
        & $=\begin{cases}
            n & \text{if $s\leq S$\tnote{\S}}\\
            0 & \text{if $S < s \leq K$}
        \end{cases}$
        & $=\begin{cases}
            0 & \text{if $s \leq S$}\\
            n & \text{if $S < s \leq K$}
        \end{cases}$
        & $n_d$\tnote{\dag}, $\forall s\in [K]$\\
    \hline
        Noise variance: $(\sigma^{(s)})^2$ 
         & $= \begin{cases}
             (\sigma_{\text{prp}})^2 & \text{if $s \leq S$}\\
             0 & \text{if $S < s \leq K$}
         \end{cases}$
         & $= \begin{cases}
             0 & \text{if $s \leq S$}\\
             (\sigma_{\text{pup}})^2 & \text{if $S < s \leq K$}
         \end{cases}$
         & $= (\sigma_{\text{int}})^2, \forall s\in [K]$ \\
    \hline
    \end{tabular}
    \begin{tablenotes}
        \item[\S] $S \in \{1,2,\dots, K-1\}$ is a pre-determined number of epochs.
        \item[\dag] $n_d\in [n]$ is a pre-determined number of private samples to use in every epoch.
    \end{tablenotes}
\end{threeparttable}
    \caption{Parameters of different algorithms that leverage public data samples.}
    \label{tab:algo_pub_samples}
\end{table}


\begin{table*}[]
    \centering
\begin{threeparttable}
\begin{adjustbox}{width=1\textwidth}
    \begin{tabular}{|c|l|l|l|}
    \hline
        \diagbox{Terms}{Algorithm} & $\privpub$ & $\pubpriv$ & $\interleaved$\\
    \hline
         Dissimilarity (Non-vanishing): $C_n^{(s)}$
         & $=\begin{cases}
             0 & \text{if $s \leq S$}\\
             C_n^{\text{full}} & \text{if $S < s \leq K$}
         \end{cases}$ 
         & $=\begin{cases}
             C_n^{\text{full}} & \text{if $s \leq S$}\\
             0 & \text{if $s < S \leq K$}
         \end{cases}$ 
         & $= C_n^{\text{part}}$, $\forall s \in [K]$ \\
    \hline
         Dissimilarity: $\frac{1}{n}\sum_{i=1}^{n-1} (C_i^{(s)})^2 $
         & $= \begin{cases}
             0 & \text{if $s \leq S$}\\
             \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{\text{full}})^2  & \text{if $S < s \leq K$}
         \end{cases}$
         & $= \begin{cases}
            \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{\text{full}})^2 & \text{if $s \leq S$}\\
             0 & \text{if $S < s \leq K$}
         \end{cases}$
         & $= \frac{1}{n}\sum_{i= 1}^{n-1} (C_{i}^{\text{part}})^2$, $\forall s\in [K]$ \\
    \hline
         Max smoothness parameter $\widehat{L}^{(s)*}$
         & $= \begin{cases}
            L^* & \text{if $s \leq S$}\\
            \widetilde{L}^{*} & \text{if $S < s \leq K$}
         \end{cases}$
         & $= \begin{cases}
             \widetilde{L}^{*} & \text{if $s \leq S$}\\
             L^* & \text{if $S < s \leq K$}
         \end{cases}$
         & $= \max\{L^*, \widetilde{L}^*\}$ \\
    \hline
    \end{tabular}
\end{adjustbox}
    % \begin{tablenotes}
    %     \item[\dag] $C_n^{\text{full}} = \max_{\pi \in \Pi_n} \left\|\sum_{j=1}^{n} \Big( \nabla f(\rvx; \rvd_{\pi_j}) - \nabla f(\rvx; \widehat{\rvp}_{\pi_j}) \Big) \right\|$
    %     \item[\ddag] $C_n^{\text{part}} = \max_{\pi \in \Pi_n} \left\|\sum_{j=n_d+1}^{n} \Big( \nabla f(\rvx; \rvd_{\pi_j}) - \nabla f(\rvx; \widehat{\rvp}_{\pi_j}) \Big)\right\|$
    %     \item[\S] $C_i = \max_{\pi \in \Pi_n} \left\| \sum_{j=1}^{i} \Big( \nabla f(\rvx; \rvd_{\pi_j} - \nabla f(\rvx; \widehat{\rvp}_{\pi_j})\Big) \right\|$, $\forall i < n$. 
    % \end{tablenotes}
\end{threeparttable}
    \caption{The resulting dissimilarity measures and the maximum smoothness parameters of different algorithms. Here, $C_n^{\text{full}}$ measures the dissimilarity between $\gD$ and $\gP$ over the full datasets. $C_n^{\text{part}}$ measures the dissimilarity between $\gD$ and using the first $n-n_d$ samples from $\gP$. 
    This notion similarly extends to $C_i^{\text{part}}$ and $C_i^{\text{full}}$, for $i< n$.
    }
    \label{tab:dissim_max_smoothness}
\end{table*}




\begin{corollary}[Convergence of Public Data Assisted Optimization]
\label{corollary:convergence_pub_data_opt}
    If one instantiates Algorithm~\ref{alg:generalized_shuffled_gradient_fm} with parameters indicated in Table~\ref{tab:algo_pub_samples}, then
    under Assumptions~\ref{ass:convexity},~\ref{ass:smoothness},~\ref{ass:reg},~\ref{ass:H_smoothness} and~\ref{ass:dissim_partial_lipschitzness}, 
    for $\beta > 0$, 
    if $\mu_{\psi} \geq L_H^{(s)} + \beta$, $\forall s\in [K]$, and 
     $\eta \leq \frac{1}{2n \max\{L^*, \widetilde{L}^{*}\} \sqrt{10 (1+\log K)}}$,     Algorithm~\ref{alg:generalized_shuffled_gradient_fm}
    guarantees convergence 
    \begin{align}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - \E\left[G(\rvx^*)\right]
        \leq \underbrace{ \frac{1}{\eta n K}\E\left[\|\rvx^* - \rvx_1^{(1)}\|^2 \right] }_{\text{Initialization}}
        + \underbrace{ 10 \eta^2 n^2 \sigma_{any}^2 (1+\log K)\max\{L, \widetilde{L}\} }_{\text{Optimization Uncertainty}} + 2 M
    \end{align}
    where
    \begin{itemize}[itemsep=0mm]
        \item For \textbf{$\privpub$},
            \begin{align*}
                M &= \underbrace{
                    \frac{1 + \log (K-S)}{2 n^2 \beta} (C_n^{\text{full}})^2
                }_{\text{Non-vanishing Dissimilarity}}
                + \underbrace{
                    5\eta^2 (1+ \log (K - S)) \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{\text{full}})^2\widetilde{L}^{*}
                }_{\text{Vanishing Dissimilarity}}
                + \underbrace{
                    6 \eta^2 nd (1 + \log S) (\sigma_{\text{prp}})^2 L^{*}\}
                }_{\text{Injected Noise}}
            \end{align*}

        \item For \textbf{$\pubpriv$},
            \begin{align*}
                M &= \underbrace{
                    \frac{1 + \log S}{2n^2 \beta} (C_n^{\text{full}})^2
                }_{\text{Non-vanishing Dissimilarity}}
                + \underbrace{
                    5 \eta^2 (1+\log S) \frac{1}{n}\sum_{i=1}^{n-1} (C_i^{\text{full}})^2 \widetilde{L}^{*}
                }_{\text{Vanishing Dissimilarity}}
                + \underbrace{
                    6 \eta^2 nd (1 + \log (K-S)) (\sigma_{\text{pup}})^2 L^{*}\}
                }_{\text{Injected Noise}}
            \end{align*}

        \item For \textbf{$\interleaved$},
            \begin{align*}
                M &= \underbrace{
                    \frac{1 + \log K}{2n^2 \beta} (C_n^{\text{part}})^2
                }_{\text{Non-vanishing Dissimilarity}}
                + \underbrace{
                    5 \eta^2 (1 + \log K) \frac{1}{n}\sum_{i=1}^{n-1} (C_{i}^{\text{part}})^2 \max\{L^*, \widehat{L}^*\}
                }_{\text{Vanishing Dissimilarity}}
                + \underbrace{
                    6 \eta^2 nd (1+\log K) (\sigma_{\text{int}})^2 \max\{ L^{*}, \widehat{L}^{*}\}
                }_{\text{Injected Noise}}
            \end{align*}
    \end{itemize}

\end{corollary}






\subsection{Privacy Analysis}
\label{subsec:appendix_algo_pub_data_privacy}

In this section, we derive the privacy guarantees of the three algorithms that leverage public samples: $\privpub$, $\pubpriv$ and $\interleaved$.

\begin{lemma}[Privacy of Public Data Assisted Optimization]
\label{lemma:appendix_privacy_pub_data_opt}
    Let $\alpha > 1$ and $\delta \in (0, 1)$. If the learning rate is $\eta \leq 1/L^{*}$,
    \begin{itemize}[itemsep=0mm]
        \item The output $\rvx_1^{(K+1)}$ of \textit{$\privpub$} is $(\frac{2 \alpha (G^*)^2 S}{(\sigma_{\text{prp}})^2} + \frac{\log 1/\delta}{\alpha-1}, \delta)$-DP.
        \item The output $\rvx_1^{(K+1)}$ of \textit{$\pubpriv$} is $(\frac{2\alpha (G^*)^2 (K-S)}{(\sigma_{\text{pup}})^2} + \frac{\log 1/\delta}{\alpha-1}, \delta)$-DP.
    \end{itemize}
    If the learning rate is $\eta \leq 1/\max\{L^{*}, \widetilde{L}^{*}\}$,
    \begin{itemize}[itemsep=0mm]
        \item The output $\rvx_1^{(K+1)}$ of \textit{$\interleaved$} is $(\frac{2\alpha (\max\{G^{*}, \widetilde{G}^{*}\})^2 K}{(n+1 - n_d) (\sigma_{\text{int}})^2} + \frac{\log 1/\delta}{\alpha-1}, \delta)$-DP.
    \end{itemize}
\end{lemma}


\begin{proof}[Proof of Lemma~\ref{lemma:appendix_privacy_pub_data_opt}]
    The proof is similar to the proof of Lemma~\ref{lemma:appendix_privacy_private_shuffled_g} showing the privacy loss of $\DPSG$. If the learning rate is set as $\eta \leq 1/L^*$ for $\privpub$ and $\pubpriv$ and $\eta \leq 1/\max\{L, L^{*}\}$ for $\interleaved$, it is then guaranteed that each gradient step in one epoch is ``contractive'', which enables us to apply PABI (Theorem~\ref{thm:pabi}) to reason about the per-epoch privacy loss. 

    The per-epoch privacy loss of $\privpub$ and $\pubpriv$ is the same as the per-epoch privacy loss in $\DPSG$ whenever the private dataset $\gD$ is used during the epoch. Specifically, for $\alpha > 1$,
    \begin{itemize}[itemsep=0mm]
        \item For $\privpub$, $\rvx_{1}^{(s+1)}$ is $(\alpha, \frac{2\alpha (G^*)^2}{(\sigma_{\text{prp}})^2})$-RDP, if $s \leq S$ and there is no privacy loss 0 otherwise.
        \item For $\pubpriv$, $\rvx_{1}^{(s+1)}$ is $(\alpha, \frac{2\alpha (G^*)^2}{(\sigma_{\text{pup}})^2})$-RDP, if $S+1 \leq s \leq K$, and there is no privacy loss 0 otherwise.
    \end{itemize}

    By applying composition (Proposition~\ref{prop:rdp_composition}) across $K$ and $K-S$ epochs for $\privpub$ and $\pubpriv$, respectively, and subsequently converting the RDP bound to a DP bound using Proposition~\ref{prop:rdp_to_dp}, we obtain the overall privacy loss as stated in the lemma statement.

    For $\interleaved$, we consider two neighboring datasets $\gD = \{\rvd_1,\dots, \rvd_n\}$ and $\gD' = \{\rvd_1, \dots, \rvd_t', \dots, \rvd_n\}$ that differ at index $t$. 
    In every epoch, only the first $n_d$ steps use samples from the private dataset $\gD$, and the remaining steps all use public samples. Hence, $t$ can only occur at step $j \leq n_d$ in the sequence of updates in every epoch. 
    
    We apply Theorem~\ref{thm:pabi} with $a_1, \dots, a_{j-1} = 0$ and $a_j,\dots, a_n = \frac{2\eta \max\{G^{*}, \widetilde{G}^{*}\}}{n-j+1}$, where $j \leq n_d$. Note that $s_{\pi_j^{(s)}} = 2\eta \max\{G^{*}, \widetilde{G}^{*}\}$ and $s_i = 0$, $\forall i\neq \pi_j^{(s)}$.  In addition, $z_i \geq 0$ for all $i\leq n$ and $z_n = 0$. Hence,
    \begin{align*}
        \infdivalpha{\rvx_{n+1}^{(s)}}{(\rvx_{n+1}^{(s)})'} \leq \sum_{i=j}^{n} \frac{\alpha}{2 \eta^2 (\sigma_{\text{int}})^2} \cdot \frac{4\eta^2 (\max\{G^{*}, \widetilde{G}^{*}\})^2}{(n-j+1)^2}
        = \frac{2\alpha (\max\{G^{*}, \widetilde{G}^{*}\})^2}{(\sigma_{\text{int}})^2(n-j+1)}
    \end{align*}
    where $(\rvx_{n+1}^{(s)})'$ is the point obtained by optimizing on the neighboring dataset $\gD'$.

    Since $j \leq n_d$, the maximum privacy loss happens at $j = n_d$, that is, when the sample $\rvd_t$ is used at step $n_d$ in one epoch. And so
    \begin{align*}
        \max \infdivalpha{\rvx_{n+1}^{(s)}}{(\rvx_{n+1}^{(s)})'}
        \leq \frac{2\alpha (\max\{G^{*}, \widetilde{G}^{*}\})^2}{(\sigma_{\text{int}})^2 (n-n_d+1)}
    \end{align*}
    which implies $\rvx_{n+1}^{(s)}$ and hence, $\rvx_{1}^{(s+1)}$, is $(\alpha, \frac{2\alpha (\max\{G^{*}, \widetilde{G}^{*}\})^2}{(\sigma_{\text{int}})^2 (n-n_d+1)} )$-RDP.

    Applying composition (Proposition~\ref{prop:rdp_composition}) across $K$ epochs and converting the RDP bound to a DP bound using Proposition~\ref{prop:rdp_to_dp} leads to the overall privacy loss as stated in the lemma statement.

\end{proof}



\subsection{Computing the Empirical Excess Risk}
\label{subsec:appendix_algo_pub_data_risk}

In this section, we derive the empirical excess risk of the three algorithms that leverage public samples: $\privpub$, $\pubpriv$ and $\interleaved$.

We begin by determining the optimal order $\alpha$ in in the RDP bound that minimizes the privacy loss, and the resulting amount of noise required for each algorithm to ensure the algorithm satisfies $(\eps, \delta)$-DP, as summarized in Table~\ref{tab:algo_pub_data_order_noise}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
        \diagbox{Algorithm}{Term} & Renyi Order $\alpha$ & Noise Variance \\
    \hline
         \privpub & $\alpha = \frac{\sigma_{\text{prp}}\sqrt{\log 1/\delta}}{G^* \sqrt{2S}}$ 
         & $(\sigma_{\text{prp}})^2 = \gO\left( \frac{(G^*)^2 S \log 1/\delta}{\eps^2} \right)$ \\
    \hline
         \pubpriv & $\alpha = \frac{\sigma_{\text{pup}} \sqrt{\log 1/\delta}}{G^* \sqrt{2(K-S)}}$ 
         & $(\sigma_{\text{pup}})^2 = \gO\left( \frac{(G^*)^2 (K-S) \log 1/\delta}{\eps^2} \right)$ \\
    \hline
         \interleaved & 
         $\alpha = \frac{\sigma_{\text{int}} \sqrt{(n+1-n_d) \log 1/\delta}}{G^* \sqrt{2K}}$ 
         & $(\sigma_{\text{int}})^2 = \gO\left( \frac{(\max\{G^*, \widetilde{G}^{*}\})^2 K \log 1/\delta}{\eps^2 (n+1-n_d) } \right)$ \\
    \hline
    \end{tabular}
    \caption{Choices of the order $\alpha$ in the RDP bound (Lemma~\ref{lemma:appendix_privacy_pub_data_opt}) and the resulting amount of noise required for each algorithm to ensure the output $\rvx_1^{(K+1)}$ satisfies $(\eps,\delta)$-DP.}
    \label{tab:algo_pub_data_order_noise}
\end{table}

% \begin{table}[H]
%     \centering
% \begin{adjustbox}{width=1\textwidth}
%     \begin{tabular}{|c|l|l|l|}
%     \hline
%         \diagbox{Terms}{Algorithm} & $\privpub$ & $\pubpriv$ & $\interleaved$\\
%     \hline
%         Renyi order $\alpha$
%         % priv-pub
%         & $= \frac{\sigma_{\text{prp}}\sqrt{\log 1/\delta}}{G^* \sqrt{2S}}$
%         % pub-priv
%         & $= \frac{\sigma_{\text{pup}} \sqrt{\log 1/\delta}}{G^* \sqrt{2(K-S)}}$
%         % interleaved
%         & $= \frac{\sigma_{\text{int}} \sqrt{(n+1-n_d) \log 1/\delta}}{G^* \sqrt{2K}}$ \\
%         % renyi order alpha cont'd
%         % priv-pub
%         % & $= \frac{\sigma_{\text{prp}}\sqrt{\log 1/\delta}}{G^* \sqrt{2 pK}}$
%         % % pub-priv
%         % & $= \frac{\sigma_{\text{pup}} \sqrt{\log 1/\delta}}{G^* \sqrt{2 p K}}$
%         % % interleaved
%         % & $= \frac{\sigma_{\text{int}}\sqrt{((1-p)n + 1) \log 1/\delta}}{G^* \sqrt{2K}}$  \\
%         Noise variance
%         % priv-pub
%         & $(\sigma_{\text{prp}})^2 = \gO\left( \frac{(G^*)^2 S \log 1/\delta}{\eps^2} \right)$
%         % pub-priv
%         & $(\sigma_{\text{pup}})^2 = \gO\left( \frac{(G^*)^2 (K-S) \log 1/\delta}{\eps^2} \right)$
%         % interleaved
%         & $(\sigma_{\text{int}})^2 = \gO\left( \frac{(\max\{G^*, \widetilde{G}^{*}\})^2 K \log 1/\delta}{\eps^2 \sqrt{(1-p)n + 1}} \right)$ \\
%     \hline
%     \end{tabular}
% \end{adjustbox}
%     \caption{
%     Choices of the order $\alpha$ in the RDP bound (Lemma~\ref{lemma:appendix_privacy_pub_data_opt}) and the resulting amount of noise required for each algorithm to ensure the output $\rvx_1^{(K+1)}$ satisfies $(\eps,\delta)$-DP.}
%     \label{tab:algo_pub_data_order_noise}
% \end{table}


Based on Corollary~\ref{corollary:convergence_pub_data_opt},
we set the learning rate as $\eta = \gO\left( \frac{\|\rvx_1^{(1)} - \rvx^*\|^{2/3}}{n \max\{L^{*}, \widetilde{L}^{*}\} (K(1+\log K))^{1/3}} \right)$ in $\privpub$, $\pubpriv$ and $\interleaved$
to minimize the convergence bound, while satisfying the conditions of both Corollary~\ref{corollary:convergence_pub_data_opt} (convergence) and Lemma~\ref{lemma:appendix_privacy_pub_data_opt} (privacy). 
After choosing the learning rate, the convergence bounds of the algorithms are now given by
\begin{itemize}
    \item $\privpub$:
    \begin{align*}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - \E\left[G(\rvx^*)\right]\\
        &\leq \underbrace{ \gO \left( 
                \frac{\|\rvx_1^{(1)} - \rvx^*\|^{4/3} \max\{L^{*}, \widetilde{L}^{*}\} (1+\log K)^{1/3}}{K^{2/3}} \right)
            }_{\text{Initialization}}
        + \underbrace{ \gO \left( 
            \frac{\|\rvx_1^{(1)} - \rvx^*\|^{4/3} \sigma_{any}^2 (1+\log K)^{1/3}}
            {K^{2/3} \max\{L^{*}, \widetilde{L}^{*} \} } 
            \right)
        }_{\text{Optimization Uncertainty}}\\
        &\quad + \underbrace{
            \gO\left( 
            \frac{1 + \log (K-S) }{n^2 \beta} (C_n^{\text{full}})^2 \right)
        }_{\text{Non-vanishing Dissimilarity}}
            + \underbrace{
                \gO \left( \frac{\|\rvx_1^{(1)} - \rvx^*\|^{4/3} \frac{1}{n}\sum_{i=1}^{n}(C_i^{\text{full}})^2}
                {n^2 \max\{L^{*}, \widetilde{L}^{*}\} K^{2/3}}
                \cdot \frac{(1+ \log (K- S))}{(1+ \log K)^{1/3}} \right)
        }_{\text{Vanishing Dissimilarity}} \\
        &\quad + \underbrace{ \gO\left( 
            \frac{\|\rvx_1^{(1)} - \rvx^{*}\|^{4/3}d (G^{*})^2 \log (1/\delta) S }
            {n \max\{L^{*}, \widetilde{L}^{*} \} \eps^2 K^{2/3}} \cdot \frac{(1+ \log S )}{(1 + \log K)^{2/3}}
        \right)
        }_{\text{Noise Injection}}
    \end{align*}

    \item $\pubpriv$:
    \begin{align*}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - \E\left[G(\rvx^*)\right]\\
        &\leq \underbrace{ \gO \left( 
                \frac{\|\rvx_1^{(1)} - \rvx^*\|^{4/3} \max\{L^{*}, \widetilde{L}^{*}\} (1+\log K)^{1/3}}{K^{2/3}} \right)
            }_{\text{Initialization}}
        + \underbrace{ \gO \left( 
            \frac{\|\rvx_1^{(1)} - \rvx^*\|^{4/3} \sigma_{any}^2 (1+\log K)^{1/3}}
            {K^{2/3} \max\{L^{*}, \widetilde{L}^{*} \} } 
            \right)
        }_{\text{Optimization Uncertainty}}\\
        &\quad + \underbrace{ \gO\left(
            \frac{1 + \log S}{n^2 \beta}(C_n^{\text{full}})^2 \right)
        }_{\text{Non-vanishing Dissimilarity}}
        + \underbrace{ 
            \gO\left( \frac{\|\rvx_1^{(1)} - \rvx^{*}\|^{4/3} \frac{1}{n}\sum_{i=1}^{n}(C_i^{\text{part}})^2}
            {n^2 \max\{L^{*}, \widetilde{L}^{*}\} K^{2/3} } \cdot \frac{1 + \log S}{(1+\log K)^{2/3}} \right)
        }_{\text{Vanishing Dissimilarity}} \\
        &\quad + \underbrace{ \gO\left( \frac{\|\rvx_1^{(1)} - \rvx^{*}\|^{4/3} d (G^{*})^2  \log (1/\delta) (K-S)}{n \max\{L^{*}, \widetilde{L}^{*}\} \eps^2 K^{2/3}} \cdot \frac{1 + \log (K - S)}{(1+\log K)^{2/3}} \right)
        }_{\text{Noise Injection}}
    \end{align*}


    \item $\interleaved$:
    \begin{align*}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - \E\left[G(\rvx^*)\right]\\
        &\leq \underbrace{ \gO \left( 
                \frac{\|\rvx_1^{(1)} - \rvx^*\|^{4/3} \max\{L^{*}, \widetilde{L}^{*}\} (1+\log K)^{1/3}}{K^{2/3}} \right)
            }_{\text{Initialization}}
        + \underbrace{ \gO \left( 
            \frac{\|\rvx_1^{(1)} - \rvx^*\|^{4/3} \sigma_{any}^2 (1+\log K)^{1/3}}
            {K^{2/3} \max\{L^{*}, \widetilde{L}^{*} \} } 
            \right)
        }_{\text{Optimization Uncertainty}}\\
        &\quad + \underbrace{ \gO\left( \frac{1 + \log K}{n^2 \beta} (C_n^{\text{part}})^{2} \right) 
        }_{\text{Non-vanishing Dissimilarity}}
        + \underbrace{ \gO\left( \frac{\|\rvx_1^{(1)} -\rvx^{*}\|^{4/3} \frac{1}{n}\sum_{i=1}^{n-1}(C_{i}^{\text{part}})^2}{n^2 \max\{L^{*}, \widetilde{L}^{*} \} K^{2/3}  } (1+\log K)^{1/3} \right)
        }_{\text{Vanishing Dissimilarity}}\\
        &\quad + \underbrace{ \gO\left( \frac{\|\rvx_1^{(1)} - \rvx^{*}\|^{4/3} d (\max\{G^{*}, \widetilde{G}^{*}\})^2 \log (1/\delta) K^{1/3}}
        {n \max\{L^{*}, \widetilde{L}^{*} \} \eps^2 (n-1+n_d)} (1+\log K)^{1/3} \right)
        }_{\text{Noise Injection}}
    \end{align*}
\end{itemize}



\textbf{Comparison.} Recall that to ensure a fair comparison, we fix the total number of gradient steps using private samples from $\gD$ and public samples from $\gP$ to be identical across $K$ epochs. We use $p\in (0,1]$ to denote the fraction of gradient steps computed using private samples. Specifically,
\begin{enumerate}[itemsep=0mm]
    \item For \pubpriv, we set the number of epochs using the private dataset $\gD$ as $S = p K$. Since $S \in [K]$, here, $p \in [\frac{1}{K}, 1]$.
    \item For \privpub, we set the number of epochs using private dataset $\gD$ as $K - S = p K$. Since $K - S \in [K]$, again, $p\in [\frac{1}{K}, 1]$.
    \item For \interleaved, we set the number of steps using samples from the private dataset $\gD$ within every epoch as $n_d = p n$.
\end{enumerate}

Based on the above, we consider the fraction of steps of using private samples as $p\in [\frac{1}{K}, 1]$.
For simplicity, we assume that both the number of epochs using the private dataset ($pK$) and the number of steps using private samples within a single epoch ($pn$) are integers.
We summarize the optimal number of epochs $K$ for each algorithm to minimize the convergence bound, along with the resulting empirical excess risk in Table~\ref{tab:empirical_excess_risk_algo_pub_data}. 
To keep the comparison clean, the empirical excess risk bounds are represented in $n,d,\eps, p$ and the non-vanishing dissimilarity term,
while treating other terms as constants.


\begin{table}[H]
    \centering
\begin{threeparttable}
    \begin{tabular}{|c|c|c|}
    \hline
         \diagbox{Algorithm}{Term} & \# Epochs $K$ & Empirical Excess Risk \\
    \hline
         \privpub & $\gO\left(\frac{n\eps^2}{d p} \right)$ 
         & $\widetilde{\gO}\left( 
         \left(\frac{p}{n}\right)^{2/3} \left(\frac{\sqrt{d}}{\eps} \right)^{4/3}
         + \frac{(C_n^{\text{full}})^2}{n^2 \beta}
         \right)$ \\
    \hline
        \pubpriv & $\gO\left( \frac{n\eps^2}{d p} \right)$
        & $\widetilde{\gO}\left( 
         \left( \frac{p}{n} \right)^{2/3} \left(\frac{\sqrt{d}}{\eps} \right)^{4/3}
         + \frac{(C_n^{\text{full}})^2}{n^2 \beta}
         \right)$ \\
    \hline
        \interleaved & $\gO\left( \frac{n\eps^2 [ (1-p)n+1 ]}{d} \right)$
        & $\widetilde{\gO}\left( 
        \left(\frac{1}{n[(1-p)n + 1]}\right)^{2/3}
        \left( \frac{\sqrt{d}}{\eps} \right)^{4/3}
        + \frac{(C_n^{\text{part}})^2}{n^2\beta}
        \right)$\\
    \hline
    \end{tabular}
\end{threeparttable}
    \caption{
    Empirical excess risk for each algorithm using public samples, represented in terms of $n,d,\eps,p$ and the non-vanishing dissimilarity term.
    Here, $p\in [\frac{1}{K},1]$ is the fraction of gradient steps that use private samples. 
    The dissimilarity measures $C_n^{\text{full}}$ and $C_n^{\text{part}}$ reflects the difference between the private dataset $\gD$ and the public dataset $\gP$, when using full and partial samples from $\gP$ in a single epoch, respectively. 
    The notation $\widetilde{\gO}$ suppresses logarithmic factors in $n, d, 1/\eps^2$ and $1/\delta$.}
    \label{tab:empirical_excess_risk_algo_pub_data}
\end{table}




% \begin{table}[H]
%     \centering
% \begin{threeparttable}
% % \begin{adjustbox}{width=1.1\textwidth}
%     \begin{tabular}{|c|l|l|l|}
%     \hline
%         \diagbox{Terms}{Algorithm} & $\privpub$ & $\pubpriv$ & $\interleaved$\\
%     \hline
    %     \multirow{2}{*}{Renyi order $\alpha$} 
    %     % priv-pub
    %     & $= \frac{\sigma_{\text{prp}}\sqrt{\log 1/\delta}}{G^* \sqrt{2S}}$
    %     % pub-priv
    %     & $= \frac{\sigma_{\text{pup}} \sqrt{\log 1/\delta}}{G^* \sqrt{2(K-S)}}$
    %     % interleaved
    %     & $= \frac{\sigma_{\text{int}} \sqrt{(n+1-n_d) \log 1/\delta}}{G^* \sqrt{2K}}$ \\
    %     % renyi order alpha cont'd
    %     % priv-pub
    %     & $= \frac{\sigma_{\text{prp}}\sqrt{\log 1/\delta}}{G^* \sqrt{2 pK}}$
    %     % pub-priv
    %     & $= \frac{\sigma_{\text{pup}} \sqrt{\log 1/\delta}}{G^* \sqrt{2 p K}}$
    %     % interleaved
    %     & $= \frac{\sigma_{\text{int}}\sqrt{((1-p)n + 1) \log 1/\delta}}{G^* \sqrt{2K}}$  \\
    %     Corresponding noise std. $\sigma$ 
    %     % priv-pub
    %     & $\gO\left( \frac{G^* \sqrt{2 p K \log 1/\delta}}{\eps} \right)$
    %     % pub-priv
    %     & $\gO\left( \frac{G^* \sqrt{2 p K \log 1/\delta}}{\eps} \right)$
    %     % interleaved
    %     & $\gO\left( \frac{G^* \sqrt{2 K \log 1/\delta}}{\eps \sqrt{(1-p)n + 1}} \right)$ \\
    % \hline
    %     Learning rate $\eta$ 
    %     & \multicolumn{3}{|c|}{$= \gO\left(
    %     \frac{\|\rvx_1^{(1)} - \rvx^*\|^{2/3}}{n \max\{L^{*}, \widehat{L}^{*}\} (K(1+\log K))^{1/3}} 
    %     \right)$}\\
    % \hline
%         \# Epochs $K$ 
%         % priv-pub
%         & $= \gO\left( \frac{n\eps^2}{d p} \right)$
%         % pub-priv
%         & $= \gO\left( \frac{n \eps^2}{ d p} \right)$
%         % interleaved
%         & $= \gO\left( \frac{n^2 \eps^2 (1-p)}{d} \right)$ \\
%     \hline
%         \multirow{2}{*}{Empirical Excess Risk (w.r.t. $n,d,\eps$)
%         } \tnote{\dag} 
%         % priv-pub
%         & $\widetilde{\gO}\tnote{\ddag} \left( (\frac{p}{n})^{2/3}(\frac{\sqrt{d}}{\eps})^{4/3} \right)$
%         % pub-priv
%         & $\widetilde{\gO}\left( (\frac{p}{n})^{2/3}(\frac{\sqrt{d}}{\eps})^{4/3} \right)$
%         % interleaved
%         & $\widetilde{\gO}\left( (\frac{\sqrt{d}}{n \eps})^{4/3} \frac{1}{(1-p)^{2/3}}\right)$\\
%         % priv-pub non-vanishing dissimilarity
%         & $+ \widetilde{\gO}\left( \frac{(C_n^{\text{full}})^2}{n^2 \beta} \right)$
%         % pub-priv non-vanishing dissimilarity
%         & $+ \widetilde{\gO}\left( \frac{(C_n^{\text{full}})^2}{n^2 \beta} \right)$
%         % interleaved non-vanishing dissimilarity
%         & $+ \widetilde{\gO}\left( \frac{(C_n^{\text{part}})^2}{n^2 \beta} \right)$ \\
%     \hline
%     \end{tabular}
% % \end{adjustbox}
%     \begin{tablenotes}
%         \item[\dag] To keep the presentation clean, we only list terms w.r.t. $n,d,\eps$ and the non-vanishing dissimilarity term. See Appendix~\ref{sec:appendix_algo_pub_data} for the full convergence bounds of $\privpub$, $\pubpriv$ and $\interleaved$.
%         \item[\ddag] $\widetilde{\gO}$ here suppresses logarithmic terms in $n, d, 1/\eps^2, 1/\delta$.
%     \end{tablenotes}
% \end{threeparttable}
%     \caption{
%     \textcolor{red}{Double check the extreme cases of $p=0$ and $p=1$. } 
%     Choices of $\alpha$, corresponding $\sigma$, selected $\eta$ and $K$, and the empirical excess risk for different algorithms.}
% \end{table}


\section{Using Other Regularization Functions $\psi$}
\label{sec:appendix_other_reg}

In this section, we consider using regularization functions $\psi$ that are not twice differentiable.
Specifically, we consider $\psi$ being the $\ell_1$ regularizer or the projection operator onto a convex set $\gB$.

The convergence proof for these cases remains the same as when $\psi$ is twice differentiable up to Section~\ref{subsec:appendix_one_epoch_convergence}, which analyzes one-epoch convergence prior to taking the expectation with respect to the injected noise.
We need to re-compute the expected additional error term introduced by noise injection (Lemma~\ref{lemma:noise_bias}), which was bounded directly through Stein's lemma when $\psi$ is twice differentiable. However, stein's lemma does not apply when the function of the noise, which involves $\psi$, exists points at which it is not twice differentiable.



\subsection{The $\ell_1$ Regularizer}
\label{subsec:appendix_l1_reg}

Instead of using Stein's lemma out of the shelf, we follow a similar idea and directly apply integration by parts. An additional offset term appears in this case, due to the non-differentiable points in the soft thresholding function after applying the $\ell_1$ regularization. To simplify the bound on the additional offset term, we make an assumption here:
\begin{assumption}
\label{ass:l1_offset}
    Let the regularization function in the objective (Eq.(\ref{eq:main_problem})) be $\psi(\rvx) = \lambda \|\rvx\|_1$ for some $\lambda > 0$.
    For all $s\in [K]$, there exists a constant $B > 0$ such that the model parameter $\rvx_{n+1}^{(s)}$ satisfies
    \begin{align}
        \E\left[ \left\| \rvx_{n+1}^{(s)} - \eta \lambda n \mathbf{1} \right\|_{\infty}\right] \leq B
    \end{align}
    where the expectation is taken w.r.t. the injected noise vectors, $\mathbf{1} \in \R^d$ is the all one vector, and recall that $\rvx_{n+1}^{(s)}$ is a function of the noise vectors. 
\end{assumption}

\begin{lemma}[Additional Error ($\ell_1$ Regularization)]
\label{lemma:noise_bias_l1_reg}
    For any epoch $s\in [K]$ and $\forall \rvz \in \R^{d}$, consider the injected noise $\rho_{i}^{(s)} \sim \gN(0, (\sigma^{(s)})^2 \sI_d)$, $\forall i\in [n]$, if the regularization function is $\psi(\rvx) = \lambda \|\rvx\|_1$ for some parameter $\lambda > 0$ and if $\rvz$ is independent of $\rho_i^{(s)}, \forall i\in [n]$, then the error caused by noise injection in epoch $s$ is
    \begin{align}
        \E\left[\frac{1}{n}\sum_{i=1}^{n} \langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right] 
        \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
        + \frac{2 d(\sigma^{(s)})^2}{\sqrt{2\pi}} B
    \end{align}
    where the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}_{i=1}^{n}$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:noise_bias_l1_reg}]

For epoch $s \in [K]$,
\begin{align}
    \rvx_{1}^{(s+1)} = \argmin_{\rvx\in\R^{d}} n \lambda \|\rvx\|_1 + \frac{\|\rvx - \rvx_{n+1}^{(s)}\|^2}{2\eta}   
\end{align}
and for $j\in [d]$, 
\begin{align}
    \rvx_{1, j}^{(s+1)} = \begin{cases}
        \rvx_{n+1,j}^{(s)} - \eta \lambda n & \text{if $\rvx_{n+1, j}^{(s)} > \eta \lambda n$}\\
        \rvx_{n+1, j}^{(s)} + \eta \lambda n & \text{if $\rvx_{n+1, j}^{(s)} < -\eta \lambda n$}\\
        0 & \text{if $|\rvx_{n+1, j}^{(s)}| \leq \eta \lambda n$}
    \end{cases}
    := g(\rvx_{n+1,j}^{(s)})
\end{align}
where $\rvx_{n+1,j}^{(s)}$ and $\rvx_{1,j}^{(s+1)}$ denote the $j$-th coordinate of $\rvx_{n+1}^{(s)}$ and $\rvx_{1}^{(s+1)}$, respectively.

For $i\in [n]$, conditional on $\rho_k^{(s)}, \forall k\neq i$,
\begin{align}
\label{eq:l1_reg_cond_exp}
    \E_{\rho_i^{(s)}}\left[\left\langle \rvx_{1}^{(s+1)}, \rho_i^{(s)} \right\rangle \mid \{\rho_k^{(s)}\}_{k\neq i}\right]
    = \sum_{j=1}^{d} \E_{\rho_{i,j}^{(s)}}\left[ \rvx_{1, j}^{(s+1)} \rho_{i, j}^{(s)} \mid \{\rho_{k, j}^{(s)}\}_{k \neq i}\right]
\end{align}
where $\rho_{i, j}^{(s)} \in \gN(0, (\sigma^{(s)})^2)$ denotes the $j$-th coordinate of the noise vector $\rho_i^{(s)}$, $\forall i\in [n], j\in[d]$.

For $i\in [n]$ and $j\in [d]$, let $m_{i,j}^{(s)}$ denote the value of the random variable $\rho_{i, j}^{(s)}$. 
\begin{align}
    &\E_{\rho_{i,j}^{(s)}}\left[\rvx_{1,j}^{(s+1)} \rho_{i, j}^{(s)} \mid \{\rho_{k, j}^{(s)}\}_{k \neq i} \right]
    = \E_{\rho_{i,j}^{(s)}}\left[ g(\rvx_{n+1,j}^{(s)}) \rho_{i, j}^{(s)} \mid \{\rho_{k, j}\}_{k \neq i}^{(s)} \right]\\
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}}
    \left(
    \int_{\rvx_{n+1,j}^{(s)} < -\eta \lambda n} \underbrace{(\rvx_{n+1,j}^{(s)} + \eta \lambda n)}_{:=u_1}
    \underbrace{
    m_{i, j}^{(s)} e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}
    }_{:=d v_1} dm_{i, j}^{(s)}
    + \int_{\rvx_{n+1,j}^{(s)} > \eta \lambda n} \underbrace{(\rvx_{n+1,j}^{(s)} - \eta \lambda n)}_{:= u_2} 
    \underbrace{m_{i, j}^{(s)} e^{-\frac{(m_{i, j}^{(s)})^2}{2 (\sigma^{(s)})^2}}}_{:=d v_2} dm_{i, j}^{(s)}
    \right)
\end{align}

Let $u_1 = \rvx_{n+1,j}^{(s)} + \eta \lambda n$ and $d v_1 = m_{i,j}^{(s)} e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} dm_{i, j}^{(s)}$. 
Then, $d u_1 = \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}} d m_{i,j}^{(s)}$ and $v_1 = -(\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}$.

Similarly, let $u_2 = \rvx_{n+1,j}^{(s)} - \eta \lambda n$ and $d v_2 = m_{i,j}^{(s)} e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}$. Then, $d u_2 = \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}}$ and $v_2 = -(\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}$.



Using integration by parts,
\begin{align}
    &\frac{1}{\sigma^{(s)} \sqrt{2\pi}}
    \int_{\rvx_{n+1, j}^{(s)} < -\eta \lambda n} 
        (\rvx_{n+1,j}^{(s)} + \eta \lambda n)
        m_{i,j}^{(s)} e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} d m_{i,j}^{(s)}\\
    \nonumber
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}}
    \left[ - (\rvx_{n+1,j}^{(s)} + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2(\sigma^{(s)})^2}} \right]_{\rvx_{n+1,j}^{(s)} < -\eta \lambda n}
    - \frac{1}{\sigma^{(s)} \sqrt{2\pi}}
    \int_{\rvx_{n+1, j}^{(s)} < -\eta \lambda n}
    - (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)} )^2}{2 (\sigma^{(s)})^2}} \frac{\partial \rvx_{n+1, j}^{(s)}}{\partial \rho_{i, j}} dm_{i,j}^{(s)}\\
\label{eq:appendix_l1_1}
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}}
    \left[ - (\rvx_{n+1,j}^{(s)} + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2(\sigma^{(s)})^2}} \right]_{\rvx_{n+1,j}^{(s)} < -\eta \lambda n}
    + \frac{ (\sigma^{(s)})^2}{\sigma^{(s)} \sqrt{2\pi}} \int_{\rvx_{n+1, j}^{(s)} < - \eta \lambda n}
    \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}} e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} dm_{i,j}^{(s)}
\end{align}
and
\begin{align}
    &\frac{1}{\sigma^{(s)} \sqrt{2\pi}} 
        \int_{\rvx_{n+1,j}^{(s)} > \eta \lambda n} (\rvx_{n+1,j}^{(s)} - \eta \lambda n) 
        m_{i,j}^{(s)} e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} d m_{i,j}^{(s)}\\
    \nonumber
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}}\left[
    - (\rvx_{n+1,j}^{(s)} - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}
    \right]_{\rvx_{n+1,j}^{(s)} > \eta \lambda n}
    - \frac{1}{\sigma^{(s)} \sqrt{2\pi}}
    \int_{\rvx_{n+1,j}^{(s)} > \eta \lambda n}
    - (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}} dm_{i,j}^{(s)}\\
\label{eq:appendix_l1_2}
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}}\left[
    - (\rvx_{n+1,j}^{(s)} - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}
    \right]_{\rvx_{n+1,j}^{(s)} > \eta \lambda n}
    + \frac{(\sigma^{(s)})^2}{\sigma^{(s)} \sqrt{2\pi}}
    \int_{\rvx_{n+1,j}^{(s)} > \eta \lambda n} \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}}
    e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} d m_{i,j}^{(s)}
\end{align}

Summing up Eq.~\ref{eq:appendix_l1_1} and Eq.~\ref{eq:appendix_l1_2},
\begin{align}
\label{eq:l1_reg_exp_single_coord}
    &\E_{\rho_{i,j}^{(s)}}\left[ \rvx_{1,j}^{(s+1)} \rho_{i, j}^{(s)} \mid \{\rho_{k,j}^{(s)}\}_{k\neq i} \right]\\
    \nonumber
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}} \left( \left[ -(\rvx_{n+1,j}^{(s)} + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}  \right]_{\rvx_{n+1,j}^{(s)} < - \eta \lambda n} + \left[ -(\rvx_{n+1,j}^{(s)} - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} \right]_{\rvx_{n+1,j}^{(s)} > \eta \lambda n}\right) \\
    \nonumber
    &\quad + \frac{ (\sigma^{(s)})^2}{ (\sigma^{(s)}) \sqrt{2\pi}}\left(
    \int_{\rvx_{n+1,j}^{(s)} < - \eta \lambda n} \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)} } e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}dm_{i,j}^{(s)}
    + \int_{\rvx_{n+1,j}^{(s)} > \eta \lambda n} \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)} } e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} dm_{i,j}^{(s)}
    \right)
\end{align}


We use $\rvx_{n+1,j}^{(s)}(m_{i,j}^{(s)})$ to denote the value of $\rvx_{n+1,j}^{(s)}$ as a function of the noise value $m_{i,j}^{(s)}$.

By Eq.~\ref{eq:op_ub}, $\left\| \frac{\rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}} \right\|_{op} \leq n \eta^2 \widehat{L}^{(s)*}$ is bounded, $\forall i\in [n], s\in [K]$, and hence, $|\frac{\partial \rvx_{n+1,j}^{(s)}(m_{i,j}^{(s)})}{\partial m_{i,j}^{(s)}}|$ is also bounded.
This implies that $\frac{1}{\sqrt{2\pi}} \cdot -(\rvx_{n+1,j}^{(s)}(m_{i,j}^{(s)}) + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}\rightarrow 0$ 
and $\frac{1}{\sqrt{2\pi}}\cdot -(\rvx_{n+1,j}^{(s)}(m_{i,j}^{(s)}) - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2(\sigma^{(s)})^2}} \rightarrow 0$
as $m_{i,j}^{(s)} \rightarrow \infty$ or $m_{i,j}^{(s)} \rightarrow -\infty$. 

We now compute the terms in the above Eq.~\ref{eq:l1_reg_exp_single_coord} separately.
\begin{align}
    &\frac{1}{\sigma^{(s)} \sqrt{2\pi}}\left[ -(\rvx_{n+1,j}^{(s)} + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}  \right]_{\rvx_{n+1,j}^{(s)} < - \eta \lambda n}\\
    \nonumber
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}}\left[ -(\rvx_{n+1,j}^{(s)}(m_{i,j}^{(s)}) + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}  \right]_{\inf\{m: \rvx_{n+1,j}^{(s)}(m) < - \eta \lambda n\}}^{\sup\{m: \rvx_{n+1,j}^{(s)}(m) < - \eta \lambda n\}}
\end{align}

Note that $\sup\{m: \rvx_{n+1,j}^{(s)}(m) < - \eta \lambda n\}$ is finite and $\inf\{m: \rvx_{n+1,j}^{(s)}(m) < - \eta \lambda n\} \rightarrow -\infty$. Let $m_{+} = \sup\{m: \rvx_{n+1,j}^{(s)}(m) < - \eta \lambda n\}$, and the above is then
\begin{align}
    &\frac{1}{\sigma^{(s)} \sqrt{2\pi}}\left[ -(\rvx_{n+1,j}^{(s)} + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}  \right]_{\rvx_{n+1,j}^{(s)} < - \eta \lambda n}\\
    \nonumber
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}}\cdot -(\rvx_{n+1,j}^{(s)}(m_{+}) + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{m_{+}^2}{2 (\sigma^{(s)})^2}} - 0\\
\label{eq:l1_reg_t1}
    &= -\frac{1}{\sigma^{(s)} \sqrt{2\pi}}(\rvx_{n+1,j}^{(s)}(m_{+}) + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{m_{+}^2}{2 (\sigma^{(s)})^2}}
\end{align}

Similarly, note that $\sup\{m: \rvx_{n+1,j}^{(s)}(m) > \eta \lambda n\}\rightarrow \infty$ and $\inf\{m: \rvx_{n+1,j}^{(s)}(m) > \eta \lambda n\}$ is finite. Let $m_{-} = \inf\{m: \rvx_{n+1,j}^{(s)}(m) > \eta \lambda n\}$.
\begin{align}
    &\frac{1}{(\sigma^{(s)}) \sqrt{2\pi}}
    \left[ -(\rvx_{n+1,j}^{(s)} - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} \right]_{\rvx_{n+1,j}^{(s)} > \eta \lambda n}\\
    \nonumber
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}}
    \left[ -(\rvx_{n+1,j}^{(s)}(m_{i,j}^{(s)}) - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}} \right]_{\inf\{m: \rvx_{n+1,j}^{(s)}(m) > \eta \lambda n\}}^{\sup\{m: \rvx_{n+1,j}^{(s)}(m) > \eta \lambda n\} }\\
    &= 0 - \frac{1}{\sigma^{(s)} \sqrt{2\pi}}\cdot -(\rvx_{n+1,j}^{(s)}(m_{-}) - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{-})^2}{2 (\sigma^{(s)})^2}}\\
\label{eq:l1_reg_t2}
    &= \frac{1}{\sigma^{(s)} \sqrt{2\pi}} (\rvx_{n+1,j}^{(s)}(m_{-}) - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{-})^{2}}{2 (\sigma^{(s)})^2}}
\end{align}

Furthermore,
\begin{align}
    &\frac{(\sigma^{(s)})^2}{\sigma^{(s)} \sqrt{2\pi}}\left(
    \int_{\rvx_{n+1,j}^{(s)} < - \eta \lambda n} \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)} } e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}dm_{i,j}^{(s)}
    + \int_{\rvx_{n+1,j}^{(s)} > \eta \lambda n} \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)} } e^{-\frac{(m_{i,j}^{(s)})^2}{2(\sigma^{(s)})^2}} dm_{i,j}^{(s)}
    \right)\\
    \nonumber
    &\leq \frac{(\sigma^{(s)})^2}{\sigma^{(s)} \sqrt{2\pi}}\left(
    \int_{\rvx_{n+1,j}^{(s)} < - \eta \lambda n} \left|\frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)} }\right| e^{-\frac{(m_{i,j}^{(s)})^2}{2(\sigma^{(s)})^2}}dm_{i,j}^{(s)}
    + \int_{\rvx_{n+1,j}^{(s)} > \eta \lambda n} \left|\frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}} \right| e^{-\frac{(m_{i,j}^{(s)})^2}{2(\sigma^{(s)})^2}} dm_{i,j}^{(s)}
    \right)\\
    &\leq \frac{(\sigma^{(s)})^2}{\sigma^{(s)} \sqrt{2\pi}}
    \int_{\R} \left| \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i,j}^{(s)}} \right| e^{-\frac{(m_{i,j}^{(s)})^2}{2 (\sigma^{(s)})^2}}
    d m_{i,j}^{(s)}\\
    &\leq (\sigma^{(s)})^2 \max\left| \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}} \right|\cdot \frac{1}{\sigma^{(s)}\sqrt{2\pi}} \int_{\R} e^{-\frac{(m_{i,j}^{(s)})^2}{2(\sigma^{(s)})^2}} d m_{i,j}^{(s)}\\
\label{eq:l1_reg_t3}
    &= (\sigma^{(s)})^2 \max\left| \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}} \right|
\end{align}

Plugging Eq.~\ref{eq:l1_reg_t1}, Eq.~\ref{eq:l1_reg_t2} and Eq.~\ref{eq:l1_reg_t3} back to Eq.~\ref{eq:l1_reg_exp_single_coord},
\begin{align}
    &\E_{\rho_{i,j}^{(s)}}\left[\rvx_{1,j}^{(s+1)} \rho_{i, j}^{(s)} \mid \{\rho_{k, j}^{(s)}\}_{k \neq i} \right]
    \leq (\sigma^{(s)})^2 \max\left| \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)} } \right|\\
    \nonumber
    &\quad \underbrace{-\frac{1}{\sigma^{(s)}\sqrt{2\pi}}(\rvx_{n+1,j}^{(s)}(m_{+}) + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{m_{+}^2}{2(\sigma^{(s)})^2}}
    + \frac{1}{\sigma^{(s)} \sqrt{2\pi}} (\rvx_{n+1,j}^{(s)}(m_{-}) - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{-})^{2}}{2 (\sigma^{(s0})^2}}
    }_{:= \Delta_{i,j}^{(s)}}
\end{align}

where $m_{+} = \sup\{m: \rvx_{n+1,j}^{(s)}(m) < -\eta \lambda n\}$ and $m_{-} = \inf\{m: \rvx_{n+1,j}^{(s)}(m) > \eta \lambda n\}$.
We note that $-(\rvx_{n+1,j}^{(s)}(m_{+}) + \eta \lambda n) \geq 0$ and $(\rvx_{n+1,j^{(s)}}(m_{-}) - \eta \lambda n) \geq 0$. 

Let $\Delta_{i, j}^{(s)} = -\frac{1}{\sigma^{(s)} \sqrt{2\pi}}(\rvx_{n+1,j}^{(s)}(m_{+}) + \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{m_{+}^2}{2 (\sigma^{(s)})^2}}
+ \frac{1}{\sigma^{(s)} \sqrt{2\pi}} (\rvx_{n+1,j}^{(s)}(m_{-}) - \eta \lambda n) (\sigma^{(s)})^2 e^{-\frac{(m_{-})^{2}}{2 (\sigma^{(s)})^2}}$.
Note that the subscript $i$ here indicates in $\Delta_{i, j}^{(s)}$, the value $\rvx_{n+1,j}^{(s)}(m)$ is a deterministic function of the noise $\rho_{i}^{(s)}$'s value $m$, by fixing the values of $\{\rho_{k}^{(s)}\}$, $\forall i\neq k$.
Also, when $\eta \lambda n = 0$, we can integrate over $\R$ and in this case $\Delta_j^{(s)} = 0$. This case is essentially what Stein's lemma addresses. 


Following Eq.~\ref{eq:l1_reg_cond_exp}, for $i\in [n]$, conditional on $\rho_k^{(s)}, \forall k\neq i$,
\begin{align}
    \E_{\rho_i^{(s)}}\left[\left\langle \rvx_{1}^{(s+1)}, \rho_i^{(s)} \right\rangle \mid \{\rho_k^{(s)}\}_{k\neq i}\right]
    &= \sum_{j=1}^{d} \E_{\rho_{i,j}^{(s)}}\left[ \rvx_{1, j}^{(s+1)} \rho_{i, j}^{(s)} \mid \{\rho_{k, j}^{(s)}\}_{k \neq i}\right]\\
    &\leq (\sigma^{(s)})^2 \sum_{j=1}^{d} \max\left| \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}} \right|
    + \sum_{j=1}^{d} \E\left[\Delta_{i,j}^{(s)} \mid \{\rho_k^{(s)}\}_{k\neq i} \right]
\end{align}


Note that $\sum_{j=1}^{d} \max \left| \frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}} \right| = \sum_{j=1}^{d} \max \left| \left[\frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_{i}^{(s)}}\right]_{j,j} \right|$, i.e., sum of the absolute value of the $j$-th diagonal entry of the Jacobian matrix $\frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}}$. 
Again, by Eq.~\ref{eq:op_ub}, $\left\| \frac{\rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}} \right\|_{op} \leq n \eta^2 \widehat{L}^{(s)*}$ is bounded, $\forall i\in [n], s\in [K]$.
Therefore, for $i\in [n]$,
\begin{align}
    \sum_{j=1}^{d}\max\left|\frac{\partial \rvx_{n+1,j}^{(s)}}{\partial \rho_{i, j}^{(s)}}\right|
    \leq \max \left\| \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_{i}^{(s)}} \right\|_{tr}
    \leq d \cdot \max \left\| \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_{i}^{(s)}} \right\|_{op}
    \leq d n \eta^2 \widehat{L}^{(s)*}
\end{align}
where $\|\cdot\|$ denotes the trace norm of a matrix. Hence, by law of total expectation,
\begin{align}
    \E_{\rho_{i}^{(s)}}\left[ \left\langle \rvx_1^{(s+1)}, \rho_i^{(s)} \right\rangle \right]
    &= \E\left[\E_{\rho_i^{(s)}}\left[\left\langle \rvx_{1}^{(s+1)}, \rho_i^{(s)} \right\rangle \mid \{\rho_k^{(s)}\}_{k\neq i}\right]\right]\\
    &\leq (\sigma^{(s)})^2 d n \eta^2 \widehat{L}^{(s)*}
    + \sum_{j=1}^{d}\E\left[\E_{\rho_i^{(s)}}\left[ \Delta_{i,j}^{(s)} \mid  \{\rho_{k}^{(s)}\}_{k\neq i}\right]\right]\\
    &\leq (\sigma^{(s)})^2 d n \eta^2 \widehat{L}^{(s)*}
    + \sum_{j=1}^{d} \frac{2 (\sigma^{(s)})^2}{\sqrt{2\pi}} B
    = (\sigma^{(s)})^2 d n \eta^2 \widehat{L}^{(s)*}
    + \frac{2 d (\sigma^{(s)})^2 }{\sqrt{2\pi}} B
\end{align}
where the last inequality is by Assumption~\ref{ass:l1_offset} and the fact that $\frac{1}{(\sigma^{(s)})\sqrt{2\pi}} e^{-\frac{m^2}{2(\sigma^{(s)})^2}}\leq 1$. Therefore,
\begin{align}
    \E\left[\frac{1}{n}\sum_{i=1}^{n} \langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right] 
    \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
    + \frac{2 d (\sigma^{(s)})^2}{\sqrt{2\pi}} B
\end{align}

\end{proof}


The rest of the proof follows section~\ref{subsec:appendix_expected_one_epoch_convergence} and section~\ref{subsec:appendix_k_epoch_convergence}. The final bound we get in the case where $\psi(\rvx) = \frac{\lambda}{2}\|\rvx_1\|$, for $\lambda > 0$, is


\begin{theorem}[Convergence under $\ell_1$ regularization]
\label{thm:convergence_l1}
    Under Assumption~\ref{ass:convexity},~\ref{ass:smoothness},~\ref{ass:l1_offset},~\ref{ass:H_smoothness},~\ref{ass:dissim_partial_lipschitzness}, for $\beta > 0$, 
    if $\mu_{\psi} \geq L_H^{(s)} + \beta$, $\forall s\in [K]$, and 
     $\eta \leq \frac{1}{2n \sqrt{10 \bar{L}^* \max_{s\in [K]}\widehat{L}^{(s)*} (1+\log K)}}$, 
    where $\bar{L}^* = \max\{L, \max_{s\in[K]} \widehat{L}^{(s)}\}$,
    Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees

    \begin{align}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - G(\rvx^*)\\
        \nonumber
        &\leq \frac{1}{\eta n K}\|\rvx^* - \rvx_1^{(1)}\|^2
        + 10 \eta^2 n^2 \sigma_{any}^2 (1+\log K)\max_{s\in[K]} \widehat{L}^{(s)} + 2 M
    \end{align}
    where
    \begin{align*}
        M = \max_{k\in [K]} \Big(
        \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        + 5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2} {k+1-s}
        + 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s} 
        + 2\sum_{s=1}^{k} \frac{d (\sigma^{(s)})^2 B}{\sqrt{2\pi}(k+1-s)}\Big)
    \end{align*}
    and the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}$ and the order of samples $\pi^{(s)}$, $\forall i\in [n], s\in [K]$.
\end{theorem}

The convergence of $\DPSG$ in this case is:
\begin{corollary}[Convergence of $\DPSG$ under $\ell_1$ regularization]
\label{corollary:convergence_dpsg}
    If we set 
    $\gD^{(s)} = \gD$, $\gP^{(s)} = \emptyset$, and constant noise variance $(\sigma^{(s)})^2 = \sigma^2$ for all epochs $s\in [K]$,
    then under the conditions in \Cref{thm:convergence_l1}, Algorithm~\ref{alg:generalized_shuffled_gradient_fm} ($\DPSG$) guarantees
    \begin{align*}
        & \E [G(\rvx_1^{(K+1)})] - G(\rvx^*)
        \lesssim \eta^2 n^2 \sigma_{any}^2 (1+\log K) L^*  + \frac{\|\rvx_1^{(1)} - \rvx^{*}\|^2}{\eta n K} + \eta^2 n d \sigma^2 L^{*} (1 + \log K)
        + d \sigma^2 B (1+\log K)
    \end{align*}
    and the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}$ and the order of samples $\pi^{(s)}$, $\forall i\in [n], s\in [K]$.
\end{corollary}

Since the additional term is non-vanishing, the choice of $\sigma, \eta$ and $K$ is the same as in the case where $\psi$ is twice differentiable, with $\sigma = \widetilde{\gO}(\frac{G^{*}\sqrt{K}}{\eps}), \eta = \widetilde{\gO}(\frac{1}{n L^{*}K^{1/3}}), K= \gO(\frac{n\eps^2}{d})$.
The empirical excess risk in this case is then $\E\left[G(\rvx_1^{(K+1)})\right] - G(\rvx^*) = \widetilde{O}\left(\frac{1}{n^{2/3}} \Big( \frac{\sqrt{d}}{\eps} \Big)^{4/3} + nB \right)$.
We leave it as an open question whether this additional $nB$ term can be  eliminated under $\ell_1$ regularization.




\subsection{The Projection Operator}
\label{subsec:appendix_proj_op}

In this section, we consider the regularization function being the projection operator, i.e., $\psi(\rvx) = \gI\{\rvx \in \gB\}$, where $\gI$ is the indicator function and $\gB$ is a convex set.
We again need to re-compute the expectation of the additional error term introduced by noise injection (Lemma~\ref{lemma:noise_bias}).
We cannot apply Stein's lemma in this case or directly use integration by parts in this case. Instead, we use Young's inequality to break the correlation between $\rho_i^{(s)}$ and $\rvx_1^{(s+1)}$, where recall that $\rvx_1^{(s+1)}$ is a function of $\rho_i^{(s)}$. This leads to a non-vanishing variance term (one that does not scale with the learning rate $\eta$) due to the variance of $\rho_i^{(s)}$. We leave as an open question whether this term can further be reduced when $\psi$ is the projection operator.

Unlike in the $\ell_1$ regularization case, the additional error term due to noise injection here introduces other terms that can be subsumed into the convergence bound. 
After deriving the additional error term in Lemma~\ref{lemma:noise_bias_proj_op}, we derive the convergence bound of one epoch in expectation in Lemma~\ref{lemma:expected_one_epoch_convergence_proj}
and finally the full convergence bound across $K$ epochs in Lemma~\ref{thm:convergence_proj}.


\begin{lemma}[Additional Error (Projection Operator)]
\label{lemma:noise_bias_proj_op}
    For any epoch $s\in [K]$ and $\forall \rvz \in \R^{d}$, consider the injected noise $\rho_{i}^{(s)} \sim \gN(0, (\sigma^{(s)})^2 \sI_d)$, $\forall i\in [n]$, if the regularization function is $\psi(\rvx) = \gI\{\rvx \in \gB\}$ for a convex set $\gB$ and if $\rvz$ is independent of $\rho_i^{(s)}, \forall i\in [n]$, then the error caused by noise injection in epoch $s$ is
    \begin{align}
    &\E\left[\frac{1}{n}\sum_{i=1}^{n} \langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right] 
    \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
    + \frac{1}{2}d (\sigma^{(s)})^2\\
    \nonumber
    &\quad + \frac{5\eta^2}{2} \Bigg(
        n\sum_{j=1}^{n_d} \E\left[\Big\| \nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvz)\Big\|^2\right]
        + n \sum_{j=n_d+1}^{n} \E\left[\Big\| \nabla f_{j-n_d}^{(s,pub)}(\rvx_j^{(s)})
        - \nabla f_{j-n_d}^{(s,pub)}(\rvz)
        \Big\|^2 \right]
        + (C_n^{(s)})^2\\
    \nonumber
    &\quad + n L B_F(\rvz, \rvx^*) + n^2 \sigma_{any}^2 + n d (\sigma^{(s)})^2
    \Bigg)
\end{align}
    where the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}_{i=1}^{n}$.
\end{lemma}


\begin{proof}[Proof of Lemma~\ref{lemma:noise_bias_proj_op}]

For epoch $s\in [K]$,
\begin{align}
    \rvx_1^{(s+1)} = \argmin_{\rvx\in \R^{d}} n \gI\{\rvx \in \gB\} + \frac{\|\rvx - \rvx_{n+1}^{(s)}\|^2}{2 \eta}
    = \argmin_{\rvx \in \gB} \|\rvx - \rvx_{n+1}^{(s)}\|
\end{align}
$\rvx_{1}^{(s+1)}$ is essentially the projection of $\rvx_{n+1}^{(s)}$ onto $\gB$.

For $i\in [n]$, 
\begin{align}
    \E\left[\left\langle \rvx_{1}^{(s+1)}, \rho_i^{(s)} \right\rangle \right]
    &= \E\left[\left\langle \rvx_{n+1}^{(s)}, \rho_i^{(s)} \right\rangle\right] + \E\left[\left\langle \rvx_{1}^{(s+1)} - \rvx_{n+1}^{(s)}, \rho_i^{(s)} \right\rangle\right]\\
    &\leq \E\left[\left\langle \rvx_{n+1}^{(s)}, \rho_i^{(s)} \right\rangle\right]
    + \frac{1}{2}\E\left[\left\| \rho_i^{(s)}\right\|^2\right]
    + \frac{1}{2}\E\left[\left\| \rvx_{n+1}^{(s)} - \rvx_{1}^{(s)}\right\|^2 \right]
\end{align}
where the last step is by Young's inequality.


We apply Stein's lemma to bound $\E\left[\left\langle \rvx_{n+1}^{(s)}, \rho_i^{(s)} \right\rangle\right]$ as follows: Conditional on $\rho_{j}^{(s)}, \forall j\neq i$,
\begin{align}
    \E_{\rho_i^{(s)}}\left[\left\langle \rvx_{n+1}^{(s)}, \rho_i^{(s)} \right\rangle \mid \{\rho_j^{(s)}\}_{j\neq i}\right]
    = (\sigma^{(s)})^2\cdot \E_{\rho_i^{(s)}}\left[\text{tr}\left( \frac{\partial \rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}}\right) \mid \{\rho_j^{(s)}\}_{j\neq i}\right]
\end{align}

By Eq.~\ref{eq:op_ub}, if $\eta \leq \frac{1}{\widehat{L}^{(s)*}}$, $\forall i \in [n]$ and $s\in [K]$, $\left\| \frac{\rvx_{n+1}^{(s)}}{\partial \rho_i^{(s)}} \right\|_{op}
\leq n \eta^2 \widehat{L}^{(s)*}$. And so for $i\in [n]$,
\begin{align}
\label{eq:noise_bias_reg_proj_t1}
    \E\left[\left\langle \rvx_{n+1}^{(s)}, \rho_i^{(s)}\right\rangle\right]
    = \E\left[\E_{\rho_i^{(s)}}\left[ \left\langle \rvx_{n+1}^{(s)}, \rho_i^{(s)} \right\rangle \mid \{\rho_j^{(s)}\}_{j\neq i}\right]\right]
    \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
\end{align}

By Lemma~\ref{lemma:noise_variance}, 
\begin{align}
\label{eq:noise_bias_reg_proj_t2}
    \E\left[\left\|\rho_i^{(s)} \right\|^2\right] \leq d (\sigma^{(s)})^2
\end{align}

We use similar techniques to bound $\E\left[\left\| \rvx_{n+1}^{(s)} - \rvx_1^{(s)} \right\|^2\right]$ as in the convergence proof in section~\ref{subsec:appendix_one_epoch_convergence} and in Lemma~\ref{lemma:expected_one_epoch_convergence}. Specifically, based on the update in Algorithm~\ref{alg:generalized_shuffled_gradient_fm},

\begin{align}
    &\E\left[ \left\| \rvx_{n+1}^{(s)} - \rvx_1^{(s)}\right\|^2 \right]\\
    \nonumber
    &= \eta^2 \E\Big[\Big\| \sum_{j=1}^{n_d} \Big(\nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_j^{(s)}) + \rho_j^{(s)} \Big)
    + \sum_{j=n_d+1}^{n} (\nabla f_{j-n_d}^{(s, pub)}\Big(\rvx_{j}^{(s)}) + \rho_j^{(s)} \Big) \Big\|^2
    \Big]\\
    &\leq 5\eta^2 \Bigg(
        \E\left[\Big\|\sum_{j=1}^{n_d} \nabla f_{\pi_j^{(s, priv)}}^{(s, priv)}(\rvx_j^{(s)}) + \sum_{j=n_d+1}^{n} \nabla f_{j-n_d}^{(s, pub)}(\rvx_j^{(s)})
        - \sum_{j=1}^{n_d}\nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvz)
        - \sum_{j=n_d+1}^{n} \nabla f_{j-n_d}^{(s, pub)}(\rvz)
        \Big\|^2\right]\\
    \nonumber
    &\quad + \E\left[ \Big\| \sum_{j=1}^{n_d}\nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvz)
    + \sum_{j=n_d+1}^{n} \nabla f_{j-n_d}^{(s,pub)}(\rvz)
    - \sum_{j=1}^{n_d} \nabla f_{\pi_j^{(s)}}(\rvz)
    - \sum_{j=n_d+1}^{n}\nabla f_{\pi_j^{(s)}}(\rvz)
    \Big\|^2 \right]\\
    \nonumber
    &\quad + \E\left[\Big\| \sum_{j=1}^{n} \nabla f_{\pi_j^{(s)}}(\rvz) - \sum_{j=1}^{n} \nabla f_{\pi_j^{(s)}}(\rvx^*)
    \Big\|^2\right]
    + \E\left[\Big\| \sum_{j=1}^{n} \nabla f_{\pi_j^{(s)}}(\rvx^*) \Big\|^2\right]
    + \E\left[\Big\| \sum_{j=1}^{n} \rho_j^{(s)} \Big\|^2 \right]
    \Bigg)\\
\label{eq:noise_bias_reg_proj_t3}
    &\leq 5\eta^2 \Bigg(
        n\sum_{j=1}^{n_d} \E\left[\Big\| \nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvz)\Big\|^2\right]
        + n \sum_{j=n_d+1}^{n} \E\left[\Big\| \nabla f_{j-n_d}^{(s,pub)}(\rvx_j^{(s)})
        - \nabla f_{j-n_d}^{(s,pub)}(\rvz)
        \Big\|^2 \right]\\
    \nonumber
    &\quad + (C_n^{(s)})^2
    + n L B_F(\rvz, \rvx^*) + n^2 \sigma_{any}^2 + n d (\sigma^{(s)})^2
    \Bigg)
\end{align}
where the last inequality is due to Jensen's inequality, Assumption~\ref{ass:dissim_partial_lipschitzness}, 
Lemma~\ref{lemma:breg_div_ub_lb}, the definition of $\sigma_{any}^2$ and Lemma~\ref{lemma:noise_variance}.

Combining Eq.~\ref{eq:noise_bias_reg_proj_t2}, Eq.~\ref{eq:noise_bias_reg_proj_t2} and Eq.~\ref{eq:noise_bias_reg_proj_t2}, since $\rvz$ is independent of $\rho_{i}^{(s)}, \forall i\in [n]$, there is
\begin{align}
    &\E\left[\frac{1}{n}\sum_{i=1}^{n} \langle \rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right] 
    \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
    + \frac{1}{2}d (\sigma^{(s)})^2\\
    \nonumber
    &\quad + \frac{5\eta^2}{2} \Bigg(
        n\sum_{j=1}^{n_d} \E\left[\Big\| \nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvz)\Big\|^2\right]
        + n \sum_{j=n_d+1}^{n} \E\left[\Big\| \nabla f_{j-n_d}^{(s,pub)}(\rvx_j^{(s)})
        - \nabla f_{j-n_d}^{(s,pub)}(\rvz)
        \Big\|^2 \right]
        + (C_n^{(s)})^2\\
    \nonumber
    &\quad + n L B_F(\rvz, \rvx^*) + n^2 \sigma_{any} + n d (\sigma^{(s)})^2
    \Bigg)
\end{align}
    
\end{proof}


The additional error term due to noise injection when $\psi$ is the projection operator stated above slightly changes the constants in the convergence bound. We give the expected one-epoch convergence bound in this case in Lemma~\ref{lemma:expected_one_epoch_convergence_proj} as follows.



\begin{lemma}[Expected One Epoch Convergence (Projection)]
\label{lemma:expected_one_epoch_convergence_proj}
    Under Assumptions~\ref{ass:convexity},~\ref{ass:dissim_partial_lipschitzness},~\ref{ass:appendix_refined_smoothness} and Lemma~\ref{ass:H_smoothness}, for any epoch $s\in [K]$, $\beta > 0$ and $\forall \rvz \in \R^d$, 
    if $\psi(\rvx) = \gI\{\rvx\in \gB\}$ for convex set $\gB$, $\eta \leq \frac{1}{n\sqrt{10 (\widehat{L}^{(s)} +1) \widehat{L}^{(s)*}}}$ and $\rvz$ is independent of $\rho_i^{(s)}$, $\forall i\in [n]$,
    Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees
    \begin{align}
        &\E\left[G(\rvx_1^{(s+1)}) - G(\rvz)\right]\\
        \nonumber
        &\leq \frac{1}{2n\eta}\Big(\E\left[\|\rvz - \rvx_1^{(s)}\|^2 \right] - \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right] \Big)
        + \Big( \frac{L_H^{(s)} + \beta}{2} - \frac{\mu_{\psi}}{2} \Big) \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right] 
        + 10 \eta^2 n^2 L (\widehat{L}^{(s)} +1) \E\left[B_F(\rvz, \rvx^*)\right]\\
        \nonumber
        &\quad
        + \underbrace{ 5\eta^2 n^2 (\widehat{L}^{(s)} +1) \sigma_{any}^2 }_{\text{Opt. Uncertainty}}
        + \underbrace{ 5\eta^2 \frac{1}{n}\sum_{i=1}^{n-1} \widehat{L}^{(s)*} (C_i^{(s)})^2
        + \frac{5}{2}\eta^2 (C_n^{(s)})^2
        }_{\text{Vanishing Dissimilarity}}
        + \underbrace{ \frac{1}{2n^2 \beta}(C_n^{(s)})^2 }_{\text{Non-vanishing Dissimilarity}}
        + \underbrace{ 6 \eta^2 nd (\sigma^{(s)})^2 (\widehat{L}^{(s)}+1) }_{\text{Injected Noise}}
        + \underbrace{ \frac{1}{2}d (\sigma^{(s)})^2}_{\text{Add. Error}}
    \end{align}
    and the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}$ and the order of samples $\pi^{(s)}$, $\forall i\in [n], s\in [K]$.
\end{lemma}


\begin{proof}[Proof of Lemma~\ref{lemma:expected_one_epoch_convergence_proj}]
Following Lemma~\ref{lemma:one_epoch_bg_3pid},~\ref{lemma:one_epoch_bound_bregmandiv} and~\ref{lemma:one_epoch_convergence}, after taking expectation, there is for any $\rvz\in \R^{d}$,
\begin{align}
\label{eq:proj_interm}
    &\E\left[G(\rvx_1^{(s+1)}) - G(\rvz)\right]\\
    \nonumber
    &\leq \E\left[ H^{(s)}(\rvx_1^{(s+1)}) - H^{(s)}(\rvz)\right]
        + \frac{\E\left[\|\rvz - \rvx_1^{(s)}\|^2 \right]}{2n \eta}
    - (\frac{1}{2n \eta} + \frac{\mu_\psi}{2}) \E\left[\|\rvz - \rvx_1^{(s+1)}\|^2\right]
        - \frac{1}{2n \eta}\E\left[\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2 \right]\\
    \nonumber
    &\quad + \E\left[\frac{1}{n}\sum_{i=1}^{n}\langle -\rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz\rangle\right]
    + \widehat{L}^{(s)}\E\left[\| \rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2\right]
    + 5\eta^2 \frac{1}{n}\sum_{i=1}^{n-1} \widehat{L}^{(s)*} (C_i^{(s)})^2
    + 10 \eta^2 n^2 \widehat{L}^{(s)} L \E\left[B_F(\rvz, \rvx^*)\right]\\
    \nonumber
    &\quad + 5\eta^2 n^2 \widehat{L}^{(s)} \sigma_{any}^2 + 5\eta^2 nd (\sigma^{(s)})^2 \widehat{L}^{(s)}\\
    \nonumber
    &\quad + 5\eta^2 n \widehat{L}^{(s)} \sum_{j=1}^{n_d} \E\left[\Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_{j}^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2\right]
        + 5\eta^2 n \widehat{L}^{(s)} \sum_{j=n_d+1}^{n} \E\left[\Big\| \nabla f_{j-n_d}^{(s, pub)} - \nabla f_{j-n_d}^{(s,pub)}(\rvz)\Big\|^2\right]\\
    \nonumber
        &\quad - \frac{1}{n} \Big( 
        \sum_{i=1}^{n_d} \frac{ \E\left[\left\|\nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i) - \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvz) \right\|^2 \right] }{2 \widehat{L}_{\pi_i^{(s)}}^{(s)}}
        + \sum_{i=n_d+1}^{n} \frac{ \E\left[ \left\| \nabla f_{i-n_d}^{(s, pub)}(\rvx_i^{(s)}) - \nabla f_{i-n_d}^{(s, pub)}(\rvz) \right\|^2\right] }{2\widetilde{L}_{i-n_d}^{(s)}}
        \Big) 
\end{align}


By Lemma~\ref{lemma:noise_bias_proj_op},
\begin{align}
\label{eq:noise_bias_proj_op}
    &\E\left[\frac{1}{n}\sum_{i=1}^{n} \langle -\rho_i^{(s)}, \rvx_1^{(s+1)} - \rvz \rangle \right] 
    \leq (\sigma^{(s)})^2 nd \eta^2 \widehat{L}^{(s)*}
    + \frac{1}{2}d (\sigma^{(s)})^2\\
    \nonumber
    &\quad + \frac{5\eta^2}{2} \Bigg(
        n\sum_{j=1}^{n_d} \E\left[\Big\| \nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvx_j^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s,priv)}(\rvz)\Big\|^2\right]
        + n \sum_{j=n_d+1}^{n} \E\left[\Big\| \nabla f_{j-n_d}^{(s,pub)}(\rvx_j^{(s)})
        - \nabla f_{j-n_d}^{(s,pub)}(\rvz)
        \Big\|^2 \right]
        + (C_n^{(s)})^2\\
    \nonumber
    &\quad + n L B_F(\rvz, \rvx^*) + n^2 \sigma_{any}^2 + n d (\sigma^{(s)})^2
    \Bigg)
\end{align}

Plugging Eq.~\ref{eq:noise_bias_proj_op} back to Eq.~\ref{eq:proj_interm},
\begin{align}
    &\E\left[G(\rvx_1^{(s+1)}) - G(\rvz)\right]\\
    \nonumber
    &\leq \E\left[ H^{(s)}(\rvx_1^{(s+1)}) - H^{(s)}(\rvz)\right]
        + \frac{\E\left[\|\rvz - \rvx_1^{(s)}\|^2 \right]}{2n \eta}
    - (\frac{1}{2n \eta} + \frac{\mu_\psi}{2}) \E\left[\|\rvz - \rvx_1^{(s+1)}\|^2\right]
        - \frac{1}{2n \eta}\E\left[\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2 \right]\\
    \nonumber
    &\quad
    + \widehat{L}^{(s)}\E\left[\| \rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2\right]
    + 5\eta^2 \frac{1}{n}\sum_{i=1}^{n-1} \widehat{L}^{(s)*} (C_i^{(s)})^2
    + \frac{5}{2}\eta^2 (C_n^{(s)})^2
    + 10 \eta^2 n^2 L (\widehat{L}^{(s)}+1) \E\left[B_F(\rvz, \rvx^*)\right]\\
    \nonumber
    &\quad + 5\eta^2 n^2 (\widehat{L}^{(s)} +1) \sigma_{any}^2 + 6 \eta^2 nd (\sigma^{(s)})^2 (\widehat{L}^{(s)}+1)
    + \frac{1}{2}d (\sigma^{(s)})^2 \\
    \nonumber
    &\quad + 5\eta^2 n (\widehat{L}^{(s)} +1 ) \sum_{j=1}^{n_d} \E\left[\Big\| \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvx_{j}^{(s)}) - \nabla f_{\pi_j^{(s)}}^{(s, priv)}(\rvz)\Big\|^2\right]
        + 5\eta^2 n (\widehat{L}^{(s)} + 1 )\sum_{j=n_d+1}^{n} \E\left[\Big\| \nabla f_{j-n_d}^{(s, pub)} - \nabla f_{j-n_d}^{(s,pub)}(\rvz)\Big\|^2\right]\\
    \nonumber
        &\quad - \frac{1}{n} \Big( 
        \sum_{i=1}^{n_d} \frac{ \E\left[\left\|\nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvx_i) - \nabla f_{\pi_i^{(s)}}^{(s, priv)}(\rvz) \right\|^2 \right]}{2 \widehat{L}_{\pi_i^{(s)}}^{(s)}}
        + \sum_{i=n_d+1}^{n} \frac{ \E\left[\left\| \nabla f_{i-n_d}^{(s, pub)}(\rvx_i^{(s)}) - \nabla f_{i-n_d}^{(s, pub)}(\rvz) \right\|^2 \right]}{2\widetilde{L}_{i-n_d}^{(s)}}
        \Big) 
\end{align}

If one sets the learning rate $\eta$ such that
\begin{align}
    5\eta^2 n(\widehat{L}^{(s)} + 1) \leq \frac{1}{n}\cdot \frac{1}{2\widehat{L}^{(s)*}}, \quad
    \Rightarrow \eta \leq \frac{1}{n \sqrt{10 (\widehat{L}^{(s)} + 1) \widehat{L}^{(s)*}}}
\end{align}
then it follows that
\begin{align}
    &\E\left[G(\rvx_1^{(s+1)}) - G(\rvz)\right]\\
    \nonumber
    &\leq \E\left[ H^{(s)}(\rvx_1^{(s+1)}) - H^{(s)}(\rvz)\right]
        + \frac{\E\left[\|\rvz - \rvx_1^{(s)}\|^2 \right]}{2n \eta}
    - (\frac{1}{2n \eta} + \frac{\mu_\psi}{2}) \E\left[\|\rvz - \rvx_1^{(s+1)}\|^2\right]
        - \frac{1}{2n \eta}\E\left[\|\rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2 \right]\\
    \nonumber
    &\quad
    + \widehat{L}^{(s)}\E\left[\| \rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2\right]
    + 5\eta^2 \frac{1}{n}\sum_{i=1}^{n-1} \widehat{L}^{(s)*} (C_i^{(s)})^2
    + \frac{5}{2}\eta^2 (C_n^{(s)})^2
    + 10 \eta^2 n^2 L (\widehat{L}^{(s)}+1) \E\left[B_F(\rvz, \rvx^*)\right]\\
    \nonumber
    &\quad + 5\eta^2 n^2 (\widehat{L}^{(s)} +1) \sigma_{any}^2 + 6 \eta^2 nd (\sigma^{(s)})^2 (\widehat{L}^{(s)}+1)
    + \frac{1}{2}d (\sigma^{(s)})^2
\end{align}

Finally, by Eq.~\ref{eq:dissim_ub},

\begin{align}
    \E\left[H^{(s)}(\rvx_{1}^{(s+1)})\right] - \E\left[H^{(s)}(\rvz)\right]
    &\leq \frac{L_H^{(s)} + \beta}{2} \E\left[\|\rvx_{1}^{(s+1)} - \rvz\|^2\right] + \frac{1}{2n^2 \beta}(C_n^{(s)})^2
\end{align}
and hence,
\begin{align}
    &\E\left[G(\rvx_1^{(s+1)}) - G(\rvz)\right]\\
    \nonumber
    &\leq \frac{1}{2n\eta}\Big(\E\left[\|\rvz - \rvx_1^{(s)}\|^2 \right] - \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right] \Big)
    + \Big( \frac{L_H^{(s)} + \beta}{2} - \frac{\mu_{\psi}}{2} \Big) \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right] \\
    &\quad + \Big(\widehat{L}^{(s)} - \frac{1}{2n \eta}\Big) \E\left[\| \rvx_1^{(s+1)} - \rvx_1^{(s)}\|^2\right]\\
    \nonumber
    &\quad
    + \frac{1}{2n^2 \beta}(C_n^{(s)})^2
    + 5\eta^2 \frac{1}{n}\sum_{i=1}^{n-1} \widehat{L}^{(s)*} (C_i^{(s)})^2
    + \frac{5}{2}\eta^2 (C_n^{(s)})^2
    + 10 \eta^2 n^2 L (\widehat{L}^{(s)} +1) \E\left[B_F(\rvz, \rvx^*)\right]\\
    \nonumber
    &\quad + 5\eta^2 n^2 (\widehat{L}^{(s)} +1) \sigma_{any}^2 + 6 \eta^2 nd (\sigma^{(s)})^2 (\widehat{L}^{(s)}+1)
    + \frac{1}{2}d (\sigma^{(s)})^2
\end{align}

Since $\eta \leq \frac{1}{n\sqrt{10 (\widehat{L}^{(s)} + 1)\widehat{L}^{(s)*}}}$, $\widehat{L}^{(s)}\leq \sqrt{(\widehat{L}^{(s)} + 1) \widehat{L}^{(s)*}} \leq \frac{1}{n \eta \sqrt{10}} \leq \frac{1}{2n\eta}$, and so
\begin{align}
    &\E\left[G(\rvx_1^{(s+1)}) - G(\rvz)\right]\\
    \nonumber
    &\leq \frac{1}{2n\eta}\Big(\E\left[\|\rvz - \rvx_1^{(s)}\|^2 \right] - \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right] \Big)
    + \Big( \frac{L_H^{(s)} + \beta}{2} - \frac{\mu_{\psi}}{2} \Big) \E\left[\| \rvz - \rvx_1^{(s+1)}\|^2 \right] \\
    \nonumber
    &\quad
    + \frac{1}{2n^2 \beta}(C_n^{(s)})^2
    + 5\eta^2 \frac{1}{n}\sum_{i=1}^{n-1} \widehat{L}^{(s)*} (C_i^{(s)})^2
    + \frac{5}{2}\eta^2 (C_n^{(s)})^2
    + 10 \eta^2 n^2 L (\widehat{L}^{(s)} + 1) \E\left[B_F(\rvz, \rvx^*)\right]\\
    \nonumber
    &\quad + 5\eta^2 n^2 (\widehat{L}^{(s)} +1) \sigma_{any}^2 + 6 \eta^2 nd (\sigma^{(s)})^2 (\widehat{L}^{(s)}+1)
    + \frac{1}{2}d (\sigma^{(s)})^2
\end{align}




\end{proof}



The rest of the proof for the convergence across $K$ epochs directly follows the argument in section~\ref{subsec:appendix_k_epoch_convergence}. We provide the final convergence bound when $\psi(\rvx) = \gI\{\rvx \in \gB\}$ for a convex set $\gB$ in Theorem~\ref{thm:convergence_proj} as follows.


\begin{theorem}[Convergence under projection]
\label{thm:convergence_proj}
    Under Assumptions~\ref{ass:convexity},~\ref{ass:reg},~\ref{ass:dissim_partial_lipschitzness},~\ref{ass:appendix_refined_smoothness} and Lemma~\ref{ass:H_smoothness}, for $\beta > 0$, 
    if $\mu_{\psi} \geq L_H^{(s)} + \beta$, $\forall s\in [K]$, and 
     $\eta \leq \frac{1}{2n \sqrt{10 \bar{L}^* \max_{s\in [K]}\widehat{L}^{(s)*} (1+\log K)}}$, 
    where $\bar{L}^* = \max\{L, \max_{s\in[K]} \widehat{L}^{(s)}\}+1$,
    Algorithm~\ref{alg:generalized_shuffled_gradient_fm} guarantees

    \begin{align}
        &\E\left[ G(\rvx_1^{(K+1)}) \right] - G(\rvx^*)\\
        \nonumber
        &\leq \underbrace{ \frac{1}{\eta n K}\E\left[\|\rvx^* - \rvx_1^{(1)}\|^2 \right] 
        }_{\text{Initialization}}
        + \underbrace{ 10 \eta^2 n^2 \sigma_{any}^2 (1+\log K)\max_{s\in[K]} \widehat{L}^{(s)} 
        }_{\text{Optimization Uncertainty}} + 2 M
    \end{align}
    where
    \begin{align*}
        M = \max_{k\in [K]} \Big(
        \underbrace{ \frac{1}{2n^2 \beta}\sum_{s=1}^{k} \frac{ (C_n^{(s)})^2 }{k+1-s}
        }_{\text{Non-vanishing Dissimilarity}}
        + \underbrace{5\eta^2 \sum_{s=1}^{k} \frac{ \widehat{L}^{(s)*} \frac{1}{n}\sum_{i=1}^{n-1}(C_i^{(s)})^2 + (C_n^{(s)})^2 } {k+1-s} }_{\text{Vanishing Dissimilarity}}
        + \underbrace{ 6\eta^2 nd \sum_{s=1}^{k} \frac{ (\sigma^{(s)})^2 \widehat{L}^{(s)*} }{k+1-s} }_{\text{Injected Noise}} 
        + \underbrace{ \frac{1}{2}\sum_{s=1}^{k}\frac{d (\sigma^{(s)})^2}{k+1-s}
        }_{\text{Add. Error}}\Big)
    \end{align*}
    and the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}$ and the order of samples $\pi^{(s)}$, $\forall i\in [n], s\in [K]$.
\end{theorem}

The convergence of $\DPSG$ in this case is:
\begin{corollary}[Convergence of $\DPSG$ under projection]
\label{corollary:convergence_dpsg_proj}
    If we set 
    $\gD^{(s)} = \gD$, $\gP^{(s)} = \emptyset$, and constant noise variance $(\sigma^{(s)})^2 = \sigma^2$ for all epochs $s\in [K]$,
    then under the conditions in \Cref{thm:convergence_l1}, Algorithm~\ref{alg:generalized_shuffled_gradient_fm} ($\DPSG$) guarantees
    \begin{align*}
        & \E [G(\rvx_1^{(K+1)})] - G(\rvx^*)
        \lesssim \eta^2 n^2 \sigma_{any}^2 (1+\log K) L^*  + \frac{\|\rvx_1^{(1)} - \rvx^{*}\|^2}{\eta n K} + \eta^2 n d \sigma^2 L^{*} (1 + \log K)
        + d \sigma^2 (1 + \log K)
    \end{align*}
    and the expectation is taken w.r.t. the injected noise $\{\rho_i^{(s)}\}$ and the order of samples $\pi^{(s)}$, $\forall i\in [n], s\in [K]$.
\end{corollary}

Again, it is unclear whether the additional error term $d\sigma^2(1+\log K)$ can be reduced when $\psi$ is the projection operator.
We leave this as an open question.






\section{Experiments}
\label{sec:appendix_experiments}

% The code is available at: \url{https://anonymous.4open.science/r/private_shuffled_G-A86F/}


\subsection{More about Datasets}
\label{subsec:appendix_datasets}

We construct the private ($\gD$) and public ($\gP$) sets of samples from each dataset for each task as follows:
\begin{enumerate}[itemsep=0mm]
        \item \textbf{Mean Estimation.}
        \begin{itemize}
            % \item \texttt{synthetic}: $n=200, d = 400$.
            % $\rvd \sim \gN(0, \sI_d)$, $\forall \rvd\in \gD$ and $\widehat{\rvp} \sim \gN(0.01, \sI_d)$, $\forall \widehat{\rvp} \in \gP$.
            \item \texttt{MNIST-69}. 
            $n = 1000, d = 784$. We want to estimate the average pixel intensity of a given digit.
            $\gD$ consists of the first 1000 training samples of digit 6. $\gP$ consists of the first 1000 training samples of digit 9, with each sample rotated $180^{\circ}$ to mimic digit 6.
        \end{itemize}

    \item \textbf{Ridge Regression}:
    \begin{itemize}[itemsep=0mm]
        \item \texttt{CIFAR-10}. $n=1000$, $d = 3072$. The task is to predict the class of a given image. 
        $\gD$ contains 200 samples per class across 10 classes.
        $\gP$ simulates a real-world scenario where collecting data from certain classes is difficult, containing samples from only the first 4 classes (250 samples per class).
        \item \texttt{Crime}\footnote{\href{https://archive.ics.uci.edu/dataset/183/communities+and+crime}{Communities and Crime}}.
        % \footnote{\url{https://archive.ics.uci.edu/dataset/183/communities+and+crime}}. 
        $n = 159$, $d = 124$. The task is to predict per capita violent crimes in a region. Data with missing entries is removed and split into two halves. 
        $\gD$ consists of one half, while $\gP$ simulates corrupted data with a small random rotation: $\gP = \mX_0 \mR$, where $\mR = \sI_d + \gN(0, \sI_d)$ and $\mX_{0}$ represents the other half of the original dataset.
    \end{itemize}
    \item \textbf{Lasso Logistic Regression}:
    \begin{itemize}
        \item \texttt{COMPAS}
        \footnote{\href{https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv}{ProPublica Recidivism Dataset}}.
        % \footnote{\url{https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv}}. 
        $n= 2103, d = 11$. 
        The task is to predict whether a criminal defendant will reoffend within two years. The dataset, known for biases in predictions across ethnic groups, is split into African-American ($\gP$) and Caucasian ($\gD$) groups. This split reflects real-world disparities in data distributions.
         \item \texttt{CreditCard}
        \footnote{\href{https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients}{Default of Credit Card Clients}}.
        % \footnote{\url{https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients}}. 
        $n = 200, d = 21$. 
         The task is to predict whether a client defaults on their credit card payment. The dataset is split by education level: university-level ($\gP$) and below high school ($\gD$). 
         The private dataset ($\gD$) has a higher default rate, creating a balanced class distribution, while the public dataset ($\gP$) exhibits an extremely low default rate.
    \end{itemize}
\end{enumerate}


\subsection{Additional Results}
\label{subsec:appendix_more_results}


\subsubsection{Variants of $\DPSG$}
\label{subsec:appendix_var_dpsg}

In the main paper, we present results using Random Reshuffling (RR). Here, we show more results using the other two variants of shuffled gradient methods, Incremental Gradient (IG) and Shuffle Once (SO), on datasets \texttt{CreditCard} and \texttt{MNIST-69}.

Again, we replace ``ShuffleG'' in each algortihm's name with ``IG'' or ``SO''. This results in the following algorithms for comparison:
\begin{enumerate}
    \item IG-based: \textit{Interleaved-IG}, \textit{Priv-Pub-IG}, \textit{Pub-Priv-IG} and \textit{DP-IG}
    \item SO-based: \textit{Interleaved-SO}, \textit{Priv-Pub-SO}, \textit{Pub-Priv-SO} and \textit{DP-SO}
\end{enumerate}
We also include the baseline \textit{Public Only} which uses public samples ($\gP$) only. 

Here, we fix $p=0.5$ and the privacy parameters are $\eps\in \{5, 10\}$ and $\delta = 10^{-6}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/log_reg_default_K_50_eps_5_nexp_10_m_IG_p_0.5.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/log_reg_default_K_50_eps_10_nexp_10_m_IG_p_0.5.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/mean_est_mnist69_K_50_eps_5_nexp_10_m_IG_p_0.5.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/mean_est_mnist69_K_50_eps_10_nexp_10_m_IG_p_0.5.pdf}\\
    \includegraphics[width=0.48\linewidth]{exp_plots_appendix/IG_legend.pdf}
    \caption{Results of comparing IG-based algorithms on two datasets.}
    \label{fig:appendix_IG}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/log_reg_default_K_50_eps_5_nexp_10_m_SO_p_0.5.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/log_reg_default_K_50_eps_10_nexp_10_m_SO_p_0.5.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/mean_est_mnist69_K_50_eps_5_nexp_10_m_SO_p_0.5.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/mean_est_mnist69_K_50_eps_10_nexp_10_m_SO_p_0.5.pdf}\\
    \includegraphics[width=0.48\linewidth]{exp_plots_appendix/SO_legend.pdf}
    \caption{Results of comparing SO-based algorithms on two datasets.}
    \label{fig:appendix_SO}
\end{figure}




\subsubsection{Varying $p$}

In this setting, we vary the fraction of private samples $p$ used in algorithms that leverage public data. Here, present results with $p\in \{0.25, 0.75\}$ on datasets \texttt{CreditCard} and \texttt{MNIST-69}.

We use RR in each algorithm. The privacy parameters are $\eps\in \{5, 10\}$ and $\delta=10^{-6}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/log_reg_default_K_50_eps_5_nexp_10_m_RR_p_0.25.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/log_reg_default_K_50_eps_5_nexp_10_m_RR_p_0.75.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/log_reg_default_K_50_eps_10_nexp_10_m_RR_p_0.25.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/log_reg_default_K_50_eps_10_nexp_10_m_RR_p_0.75.pdf}\\
    \includegraphics[width=0.48\linewidth]{res_plots/RR_legend.pdf}
    \caption{Results of using different fractions of private samples for $p\in \{0.25, 0.75\}$ on dataset \texttt{CreditCard}.}
    \label{fig:appendix_varying_p_credit_card}
\end{figure}


\begin{figure}[H]
    \centering
    %%
    \centering
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/mean_est_mnist69_K_50_eps_5_nexp_10_m_RR_p_0.25.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/mean_est_mnist69_K_50_eps_5_nexp_10_m_RR_p_0.75.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/mean_est_mnist69_K_50_eps_10_nexp_10_m_RR_p_0.25.pdf}
    \includegraphics[width=0.24\linewidth]{exp_plots_appendix/mean_est_mnist69_K_50_eps_10_nexp_10_m_RR_p_0.75.pdf}\\
    \includegraphics[width=0.48\linewidth]{res_plots/RR_legend.pdf}
    \caption{Results of using different fractions of private samples for $p\in \{0.25, 0.75\}$ on dataset \texttt{MNIST-69}.}
    \label{fig:appendix_varying_p_mnist69}
\end{figure}

