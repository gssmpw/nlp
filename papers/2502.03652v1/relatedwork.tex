\section{Related Work}
\label{subsec:related_work}


% non-private shuffed G
\textbf{$\SG$.} Unlike SGD, theoretical convergence bounds for shuffled gradient methods in the non-private setting have only been established recently~\cite{mishchenko2021rr, mishchenko2021prox_fed_rr, liu2024last_iterate_shuffled_gradient}. Their performance compared to DP-SGD in the private setting remains unclear. 

% PABI
\textbf{PABI.} The use of only the last-iterate model parameter at inference time has led to a line of work on privacy amplification by iteration (PABI)~\cite{Feldman2018privacy_amp_iter, altschuler2022apple_paper, ye2022singapore_paper}, which focuses on the privacy amplification by hiding intermediate model parameters. However, most works on PABI focus solely on privacy analysis without exploring its impact on convergence.
The only work analyzing convergence of DP-SGD for stochastic convex optimization (SCO)~\cite{Feldman2018privacy_amp_iter} relies on average-iterate analysis, which conflicts with PABI, where only the last-iterate parameter is released. To reconcile this, they analyze impractical variants of DP-SGD, such as those terminating after a random number of steps. 

% pulic data assisted private learning
\textbf{Using Public Data or Surrogate Objectives.} While there is a long line of work exploring using public samples to improve model performance in private learning tasks, e.g.,~\cite{bassily20priv_query_release_pub_data, ullah2024public_data_priv_sco}, only a few~\cite{bie2022private_est_pub_data_shift, Bassily2023priv_adp_from_pub_source} consider distribution shifts between public and private datasets. No work, to our knowledge, address their usage in the context of shuffled gradient methods.
% learning with public data
Also, optimization on surrogate objectives has been studied in the non-private setting using average-iterate analysis of SGD~\cite{Woodworth2023two_losses} but remains unexplored in shuffled gradient methods.
For a more detailed discussion and a full survey, see Appendix~\ref{sec:appendix_related_work}.