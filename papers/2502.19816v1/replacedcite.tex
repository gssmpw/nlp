\section{Related Work}
\label{sec:related work}

\subsection{Coarse-to-Fine Learning}

Coarse-to-Fine learning____ has become a significant focus in computer vision and machine learning, aiming to leverage coarse-grained labeled data to enhance fine-grained recognition.
ANCOR____ introduced a fine-grained angular contrastive learning method that uses coarse labels to guide the angular loss function and generate sample pairs. Sun et al.____ proposed a dynamic metric learning approach that adapts the metric space to different semantic scales.
Yang____ address the cross-granularity gap by clustering coarse classes into pseudo-fine classes and introducing a meta-embedder that jointly optimizes both visual and semantic discrimination for effective pseudo-labeling.
Grafit____ implements a joint learning scheme that integrates instance and coarse label supervision losses to learn fine-grained image representations. 
MaskCon____ proposes a contrastive learning method that uses coarse labels to generate masked soft labels, leveraging both inter-sample relations and coarse label information. 
HCM____ embeds visual representations into a hyperbolic space and enhances their discriminative power using hierarchical cosine margins. 
FALCON____ enables fine-grained class discovery from coarsely labeled data without requiring fine-grained level supervision.
Some studies____ have also explored incremental few-shot learning with mixing of coarse and fine labels. 

\subsection{Self-supervised Learning}

As a branch of unsupervised learning, self-supervised learning (SSL) focuses on extracting discriminative features from large-scale unlabeled data, bypassing the need for human annotations____. 
Context-based SSL methods exploit inherent contextual relationships, such as spatial structures and texture consistency within intact samples, using domain-specific pretext tasks____ like image rotation prediction____ and jigsaw puzzles____.
Contrastive learning has progressed from explicitly using negative examples____ to ideas like self-distillation and feature decorrelation, all of which adhere to the principle of promoting positive example invariance while pushing apart different examples____. Masked image modeling____ reconstructs pixels____ or local features____, leveraging co-occurrence relationships among image patches as supervision signals.

\subsection{Few-shot Learning}

Few-shot learning (FSL) is designed to mitigate the model's heavy reliance on large-scale training data, enabling models to quickly generalize to new tasks with limited annotated training data____. This recognition paradigm is ideally suited for fine-grained recognition, as collecting fine-grained samples is often costly. The methodologies for FSL can be taxonomically divided into three primary categories: data augmentation-based methods____, optimization-based methods____, and metric-based methods____. 
Recent advances in metric-based methods have incorporated attention mechanisms____ to model dependencies between elements in input sequences, utilized dense feature vectors to extract richer and finer image-to-image correlations____. While the aforementioned works have shown success in traditional FSL tasks, they still require a significant number of labels with the same granularity as the test data during training.