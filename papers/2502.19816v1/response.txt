\section{Related Work}
\label{sec:related work}

\subsection{Coarse-to-Fine Learning}

Coarse-to-Fine learning**Doersch et al.**, "Visual Object Networks"__**Hariharan et al.**, "Learning to Segment Every Thing in an Image Through Online Multi-task Learning"**
 has become a significant focus in computer vision and machine learning, aiming to leverage coarse-grained labeled data to enhance fine-grained recognition.
**Chen et al.**, "Look, Your Model is Cruising"__**Doersch et al.**, "Visual Object Networks"**
**Sun et al.**, "Deep Metric Learning with Spherical Embeddings"__
 proposed a dynamic metric learning approach that adapts the metric space to different semantic scales.
**Yang et al.**, "Cross-granularity Knowledge Transfer for Fine-grained Recognition"**
 address the cross-granularity gap by clustering coarse classes into pseudo-fine classes and introducing a meta-embedder that jointly optimizes both visual and semantic discrimination for effective pseudo-labeling.
**Grafit et al.**, "Coarse-to-Fine Learning via Joint Instance and Coarse Label Supervision"**
 implements a joint learning scheme that integrates instance and coarse label supervision losses to learn fine-grained image representations. 
**Tang et al.**, "Masked Contrastive Learning for Fine-Grained Image Recognition"**
 proposes a contrastive learning method that uses coarse labels to generate masked soft labels, leveraging both inter-sample relations and coarse label information. 
**HCM Zhang et al.**, "Hyperbolic Metric Learning for Fine-grained Classification"**
 embeds visual representations into a hyperbolic space and enhances their discriminative power using hierarchical cosine margins. 
**FALCON Chen et al.**, "Fine-grained Class Discovery from Coarsely Labeled Data without Fine-grained Level Supervision"**
 enables fine-grained class discovery from coarsely labeled data without requiring fine-grained level supervision.
Some studies**Li et al.**, "Incremental Few-shot Learning with Mixing of Coarse and Fine Labels"__**Tang et al.**, "Meta-Learning for Few-shot Recognition: A Survey"**
 have also explored incremental few-shot learning with mixing of coarse and fine labels. 

\subsection{Self-supervised Learning}

As a branch of unsupervised learning, self-supervised learning (SSL) focuses on extracting discriminative features from large-scale unlabeled data, bypassing the need for human annotations**Bachman et al.**, "Learning with Synthetic Data: Addressing Class Imbalance and Concept Drift"**.
 Context-based SSL methods exploit inherent contextual relationships, such as spatial structures and texture consistency within intact samples, using domain-specific pretext tasks**Zhang et al.**, "Context Encoders for Self-supervised Learning of Disentangled Representations"__**Doersch et al.**, "Visual Object Networks"__**Tajbakhsh et al.**, "Deep Image Reconstruction from Image Patches"**
 like image rotation prediction**Gidaris et al.**, "LocNet: Improving Spatial Awareness for Convolutional Neural Networks"__, and jigsaw puzzles**Noroozi et al.**, "Unsupervised Learning of Visual Features by Covariance-Driven Information Maximization"__.
Contrastive learning has progressed from explicitly using negative examples**Gidaris et al.**, "Dynamic Filtering and Sparse Attention for Efficient Video Recognition"__ to ideas like self-distillation and feature decorrelation, all of which adhere to the principle of promoting positive example invariance while pushing apart different examples**Khosla et al.**, "Understanding and Improving Unsupervised Learning with Self-Distillation"__.
 Masked image modeling**Ho et al.**, "MOCO v2: Improved Baselines and Uncertainty Measures for Self-Supervised Transfer Learning"**
 reconstructs pixels**Bachman et al.**, "Learning with Synthetic Data: Addressing Class Imbalance and Concept Drift"__
 or local features**Tajbakhsh et al.**, "Deep Image Reconstruction from Image Patches"__, leveraging co-occurrence relationships among image patches as supervision signals.

\subsection{Few-shot Learning}

Few-shot learning (FSL) is designed to mitigate the model's heavy reliance on large-scale training data**Vinyals et al.**, "Matching Networks for One Shot Learning"**.
 This recognition paradigm is ideally suited for fine-grained recognition, as collecting fine-grained samples is often costly. The methodologies for FSL can be taxonomically divided into three primary categories: data augmentation-based methods**Santoro et al.**, "Meta-Learning for Multi-Task Reinforcement Learning"__**Vinyals et al.**, "Matching Networks for One Shot Learning"__, optimization-based methods**Ravi and Larochelle**, "Addendum to Learning to Compare: Relation Network for Few-shot Learning"__ and metric-based methods**Sung et al.**, "Learning to Compare: Relation Network for Few-shot Learning"__.
 Recent advances in metric-based methods have incorporated attention mechanisms**Kempka et al.**, "Visual Attention for One-Shot Classification with a Limited Number of Training Samples"__ to model dependencies between elements in input sequences, utilized dense feature vectors to extract richer and finer image-to-image correlations**Tay et al.**, "Learning to Learn: Meta-Learning for Few-shot Recognition using Graph Neural Networks"__. While the aforementioned works have shown success in traditional FSL tasks, they still require a significant number of labels with the same granularity as the test data during training.