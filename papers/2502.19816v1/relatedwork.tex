\section{Related Work}
\label{sec:related work}

\subsection{Coarse-to-Fine Learning}

Coarse-to-Fine learning~\cite{ristin2015categories, taherkhani2019weakly, wei2023cat} has become a significant focus in computer vision and machine learning, aiming to leverage coarse-grained labeled data to enhance fine-grained recognition.
ANCOR~\cite{bukchin2021fine} introduced a fine-grained angular contrastive learning method that uses coarse labels to guide the angular loss function and generate sample pairs. Sun et al.~\cite{sun2021dynamic} proposed a dynamic metric learning approach that adapts the metric space to different semantic scales.
Yang~\cite{yang2021towards} address the cross-granularity gap by clustering coarse classes into pseudo-fine classes and introducing a meta-embedder that jointly optimizes both visual and semantic discrimination for effective pseudo-labeling.
Grafit~\cite{touvron2021grafit} implements a joint learning scheme that integrates instance and coarse label supervision losses to learn fine-grained image representations. 
MaskCon~\cite{feng2023maskcon} proposes a contrastive learning method that uses coarse labels to generate masked soft labels, leveraging both inter-sample relations and coarse label information. 
HCM~\cite{xu2023hyperbolic} embeds visual representations into a hyperbolic space and enhances their discriminative power using hierarchical cosine margins. 
FALCON~\cite{grcicfine} enables fine-grained class discovery from coarsely labeled data without requiring fine-grained level supervision.
Some studies~\cite{zhao2021mgsvf, xiang2022coarse} have also explored incremental few-shot learning with mixing of coarse and fine labels. 

\subsection{Self-supervised Learning}

As a branch of unsupervised learning, self-supervised learning (SSL) focuses on extracting discriminative features from large-scale unlabeled data, bypassing the need for human annotations~\cite{gui2024survey}. 
Context-based SSL methods exploit inherent contextual relationships, such as spatial structures and texture consistency within intact samples, using domain-specific pretext tasks~\cite{hu2024asymmetric} like image rotation prediction~\cite{gidaris2018unsupervised} and jigsaw puzzles~\cite{noroozi2016unsupervised}.
Contrastive learning has progressed from explicitly using negative examples~\cite{he2020momentum, chen2020improved} to ideas like self-distillation and feature decorrelation, all of which adhere to the principle of promoting positive example invariance while pushing apart different examples~\cite{wang2020understanding}. Masked image modeling~\cite{assran2022masked} reconstructs pixels~\cite{he2022masked} or local features~\cite{baevski2022data2vec}, leveraging co-occurrence relationships among image patches as supervision signals.

\subsection{Few-shot Learning}

Few-shot learning (FSL) is designed to mitigate the model's heavy reliance on large-scale training data, enabling models to quickly generalize to new tasks with limited annotated training data~\cite{10319790}. This recognition paradigm is ideally suited for fine-grained recognition, as collecting fine-grained samples is often costly. The methodologies for FSL can be taxonomically divided into three primary categories: data augmentation-based methods~\cite{ref26}, optimization-based methods~\cite{ref31}, and metric-based methods~\cite{jin2024few, wei2022embarrassingly}. 
Recent advances in metric-based methods have incorporated attention mechanisms~\cite{ref50} to model dependencies between elements in input sequences, utilized dense feature vectors to extract richer and finer image-to-image correlations~\cite{ref54}. While the aforementioned works have shown success in traditional FSL tasks, they still require a significant number of labels with the same granularity as the test data during training.