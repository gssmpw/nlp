@article{andriushchenko2024agentharm,
  title={Agentharm: A benchmark for measuring harmfulness of llm agents},
  author={Andriushchenko, Maksym and Souly, Alexandra and Dziemian, Mateusz and Duenas, Derek and Lin, Maxwell and Wang, Justin and Hendrycks, Dan and Zou, Andy and Kolter, Zico and Fredrikson, Matt and others},
  journal={arXiv preprint arXiv:2410.09024},
  year={2024}
}

@article{andriushchenko2024jailbreaking,
  title={Jailbreaking leading safety-aligned llms with simple adaptive attacks},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2404.02151},
  year={2024}
}

@misc{chen2024agentpoisonredteamingllmagents,
      title={AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases}, 
      author={Zhaorun Chen and Zhen Xiang and Chaowei Xiao and Dawn Song and Bo Li},
      year={2024},
      eprint={2407.12784},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.12784}, 
}

@misc{chowdhury2024breakingdefensescomparativesurvey,
      title={Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models}, 
      author={Arijit Ghosh Chowdhury and Md Mofijul Islam and Vaibhav Kumar and Faysal Hossain Shezan and Vaibhav Kumar and Vinija Jain and Aman Chadha},
      year={2024},
      eprint={2403.04786},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.04786}, 
}

@article{cui2024risk,
  title={Risk taxonomy, mitigation, and assessment benchmarks of large language model systems},
  author={Cui, Tianyu and Wang, Yanling and Fu, Chuanpu and Xiao, Yong and Li, Sijia and Deng, Xinhao and Liu, Yunpeng and Zhang, Qinglin and Qiu, Ziyi and Li, Peiyang and others},
  journal={arXiv preprint arXiv:2401.05778},
  year={2024}
}

@misc{das2024securityprivacychallengeslarge,
      title={Security and Privacy Challenges of Large Language Models: A Survey}, 
      author={Badhan Chandra Das and M. Hadi Amini and Yanzhao Wu},
      year={2024},
      eprint={2402.00888},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00888}, 
}

@misc{debenedetti2024agentdojodynamicenvironmentevaluate,
      title={AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents}, 
      author={Edoardo Debenedetti and Jie Zhang and Mislav Balunović and Luca Beurer-Kellner and Marc Fischer and Florian Tramèr},
      year={2024},
      eprint={2406.13352},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.13352}, 
}

@article{he2024emerged,
  title={The emerged security and privacy of llm agent: A survey with case studies},
  author={He, Feng and Zhu, Tianqing and Ye, Dayong and Liu, Bo and Zhou, Wanlei and Yu, Philip S},
  journal={arXiv preprint arXiv:2407.19354},
  year={2024}
}

@article{huang2023catastrophic,
  title={Catastrophic jailbreak of open-source llms via exploiting generation},
  author={Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06987},
  year={2023}
}

@inproceedings{liuautodan,
  title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{nasr2023scalable,
  title={Scalable extraction of training data from (production) language models},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023}
}

@misc{shayegani2023surveyvulnerabilitieslargelanguage,
      title={Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks}, 
      author={Erfan Shayegani and Md Abdullah Al Mamun and Yu Fu and Pedram Zaree and Yue Dong and Nael Abu-Ghazaleh},
      year={2023},
      eprint={2310.10844},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.10844}, 
}

@inproceedings{shen2024anything,
  title={`` do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={1671--1685},
  year={2024}
}

@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yang2024watch,
  title={Watch out for your agents! investigating backdoor threats to llm-based agents},
  author={Yang, Wenkai and Bi, Xiaohan and Lin, Yankai and Chen, Sishuo and Zhou, Jie and Sun, Xu},
  journal={arXiv preprint arXiv:2402.11208},
  year={2024}
}

@article{zeng2024good,
  title={The good and the bad: Exploring privacy issues in retrieval-augmented generation (rag)},
  author={Zeng, Shenglai and Zhang, Jiankun and He, Pengfei and Xing, Yue and Liu, Yiding and Xu, Han and Ren, Jie and Wang, Shuaiqiang and Yin, Dawei and Chang, Yi and others},
  journal={arXiv preprint arXiv:2402.16893},
  year={2024}
}

@article{zhang2024breaking,
  title={Breaking agents: Compromising autonomous llm agents through malfunction amplification},
  author={Zhang, Boyang and Tan, Yicong and Shen, Yun and Salem, Ahmed and Backes, Michael and Zannettou, Savvas and Zhang, Yang},
  journal={arXiv preprint arXiv:2407.20859},
  year={2024}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{zou2024poisonedrag,
  title={Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models},
  author={Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan},
  journal={arXiv preprint arXiv:2402.07867},
  year={2024}
}

