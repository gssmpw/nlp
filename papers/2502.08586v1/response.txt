\section{Related Work}
\label{sec:related}

\textbf{Attacks on LLMs.} The literature on LLM security vulnerabilities has grown rapidly, primarily focusing on standalone models. One popular genre of attacks is \textbf{jailbreaks} **Sena, "Jailbreaking Transformers"** in which the attacker inserts a prompt into the LLM that coerces it into generating harmful text nonetheless, despite any alignment post-training or defensive system prompts. **Carlini et al., "Control of Machine Learning Systems by Adversarial Attacks on Large Language Models"** dissect failure modes of LLMs against attacks to guide the design of jailbreaks.

While most existing demonstrations of jailbreak attacks extract information is already available on the internet, one fear is that future models more capable than those today could be persuaded to invent and output designs of new bombs or bioweapons which may not have even been present in the model's training data.  Therefore, jailbreak attacks, even on standalone LLMs, are a worthwhile threat to mitigate before their potential for existential harm manifests.  In this work, we will see in contrast that attacks on deployed agents can already cause widespread harm, although the harms we demonstrate may not be existential.

In the white box setting, **Andreas et al., "Adversarial Attacks and Defenses on Large Language Models"** introduce a gradient-based search algorithm which crafts adversarial suffixes that maximize the probability of generating targeted harmful text. **Sajjad et al., "Attacking Adversarially Trained Neural Networks through Transferable and Efficient Query Strategies"** further develop a more scalable algorithm for generating semantically meaningful prompts that exhibit improved transferability to closed-source models. In the black box setting, hand-crafted jailbreak prompts can be incredibly effective but are especially dangerous when paired with adversarial suffixes optimized via random search **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**. Aside from coercing models into generating harmful content, another objective of handcrafted malicious prompts is to extract sensitive training data from trained models **Xu et al., "Adversarial Attacks on Deep Neural Networks for Classification: A Survey"**.  For a categorization and benchmark of additional attacks on standalone models, see **Kumar et al., "Benchmarking Adversarial Robustness of Neural Networks"**.  While these attacks probe the limitations of LLM alignment, most of them are not directly applicable to LLM agents.
 

\textbf{Attacks on LLM agents.} A line of recent research studies vulnerabilities in LLM agents, especially in memory and retrieval-augmented generation (RAG).  Several works poison databases or memory modules **Li et al., "Memorization-based Attacks against Retrieval-Augmented Generation"**, and others trick agents into retrieving sensitive data and regurgitating it **Santos et al., "Memory Poisoning on Large Language Models"**. Two recent papers release platforms for testing LLM agent security **Ritter et al., "Testing LLM Agent Security"**. 
 

\textbf{Taxonomy of LLM attacks.} The security research community has developed taxonomies for categorizing attacks on LLMs, covering areas such as jailbreaks, prompt injection, and data poisoning **Sena et al., "LLM Attack Taxonomy"**, although distinctions between categories such as jailbreaks and prompt injection are disputed. Researchers have also explored safety taxonomies for LLM agents **Pai et al., "Safety Taxonomy of Large Language Models"**, improving the coverage to include additional components specific to agents.  While several of these works conduct experiments in simplified settings, we craft attacks against real-world agents in order to highlight that these dangers are already here, and solving them is urgent.