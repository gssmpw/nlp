\section{Related Work}
\label{sec:related}

\textbf{Attacks on LLMs.} The literature on LLM security vulnerabilities has grown rapidly, primarily focusing on standalone models. One popular genre of attacks is \textbf{jailbreaks} ____, in which the attacker inserts a prompt into the LLM that coerces it into generating harmful text nonetheless, despite any alignment post-training or defensive system prompts. ____ dissect failure modes of LLMs against attacks to guide the design of jailbreaks.

While most existing demonstrations of jailbreak attacks extract information is already available on the internet, one fear is that future models more capable than those today could be persuaded to invent and output designs of new bombs or bioweapons which may not have even been present in the model's training data.  Therefore, jailbreak attacks, even on standalone LLMs, are a worthwhile threat to mitigate before their potential for existential harm manifests.  In this work, we will see in contrast that attacks on deployed agents can already cause widespread harm, although the harms we demonstrate may not be existential.

In the white box setting, ____ introduce a gradient-based search algorithm which crafts adversarial suffixes that maximize the probability of generating targeted harmful text. ____ further develop a more scalable algorithm for generating semantically meaningful prompts that exhibit improved transferability to closed-source models. In the black box setting, hand-crafted jailbreak prompts can be incredibly effective but are especially dangerous when paired with adversarial suffixes optimized via random search ____. Aside from coercing models into generating harmful content, another objective of handcrafted malicious prompts is to extract sensitive training data from trained models ____.  For a categorization and benchmark of additional attacks on standalone models, see ____.  While these attacks probe the limitations of LLM alignment, most of them are not directly applicable to LLM agents.
 

\textbf{Attacks on LLM agents.} A line of recent research studies vulnerabilities in LLM agents, especially in memory and retrieval-augmented generation (RAG).  Several works poison databases or memory modules ____, and others trick agents into retrieving sensitive data and regurgitating it ____. Two recent papers release platforms for testing LLM agent security ____. 

\textbf{Taxonomy of LLM attacks.} The security research community has developed taxonomies for categorizing attacks on LLMs, covering areas such as jailbreaks, prompt injection, and data poisoning ____, although distinctions between categories such as jailbreaks and prompt injection are disputed. Researchers have also explored safety taxonomies for LLM agents ____, improving the coverage to include additional components specific to agents.  While several of these works conduct experiments in simplified settings, we craft attacks against real-world agents in order to highlight that these dangers are already here, and solving them is urgent.