\section{Related Works}
\label{Related Works}
	\subsection{ Evolution and trends in edge‑AI research }
	The domain of EI has experienced rapid expansion, which has been distinguished
	by a series of methodological and applied breakthroughs from 2018 to the present.
	A chronological overview, illustrated in Fig. 2, serves to articulate this progression:
	
	\textbf{2018-Foundational Integrations of Deep Learning and IoT:} The year marked the inception of integrating deep learning with IoT, as seminal works by Li et al. \cite{c25} and Zhao et al. \cite{c26} established the initial frameworks for embedding ML models within IoT infrastructures, laying the groundwork for subsequent EI innovations.
	
	\textbf{2019: The Advent of Deep Learning in Edge Computing:} In this phase, the focus transitioned to deploying deep learning models directly onto the Edge layer, with studies by Manogaran et al. \cite{c27} and Azar et al. \cite{c28} exploring architectural strategies to enhance computational offloading.
	
	\textbf{2020: Data Inference Convergence with the Edge:} This year witnessed an emphasis on data analytics at the edge, highlighted by contributions from Hu et al. \cite{c29}, and Li et al. \cite{c14}, advanced the practical implementation of ML models for real-time data inference within edge environments.
	
	\textbf{2021: Empowering Things with EI:} Researchers such as Kristiani et al. \cite{c31}, Ghosh et al. \cite{c9}, and Raj et al. \cite{c32} shifted focus towards empowering IoT devices ('Things') with edge intelligence, proposing architectures that leveraged the computational proximity of Edge layers.
	
	\textbf{2022: Setting up ML Pipelines on EI:} The setup of ML pipelines on EI became a focal point, with studies like Arunachalam et al. \cite{c33} exploring the benefits of distributed ML processes to optimize the entire data lifecycle on edge devices.
	
	\textbf{2023: Utilizing ML Pipelines for EI Challenges:} Investigations by Achar et al. \cite{c16} and Wazwaz et al. \cite{c17} probed deeper into the application of ML pipelines to solve complex EI problems, underscoring the transition from theory to application-centric solutions.
	
	\textbf{2024: Empowering Edge-AI by Inference at Things:} As of 2024, our research takes a
	step further by advancing inference at the ‘Things’ level, optimizing latency and
	reliability for EI applications, and pushing the evolution of EI research beyond
	prior works for real-world implementation.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width= 0.8\textwidth]{Fig2.png}
		\caption{Edge-AI Studies Timeline}
	\end{figure}
	
	\subsection{ Overview of edge‑AI architectural methodologies}
	Various methods have been proposed for deploying ML models on the Edge layer, each
	tailored to specific contexts. In Fig. 3, each of these methods is generally categorized
	into sub-methods. This figure also represents the research domain addressed explicitly
	in this study. The research begins by searching in the field of EI studies. This field consists of two broad sub-areas known as training and inference, and our research focuses
	on designing hybrid architectures for EI systems by deploying an ML pipeline across
	the Things, Edge, and Cloud layers, optimizing latency and network traffic by leveraging the processing capabilities of these layers. Understanding EI architectural methodologies is crucial for appreciating the context and effectiveness of ML pipeline deployment in EI systems. Tables 1 and 2 provide a concise overview of critical studies in
	the EI domain from 2018 to 2024, highlighting architectural choices, ML models, use
	cases, datasets, methodologies, techniques, and performance metrics, including details
	on the InTec model. These comparisons cover cloud-edge to device-edge integrations
	using public and private datasets, primarily in healthcare and industry. Techniques such
	as model partitioning, compression, and optimization are detailed alongside parameters like accuracy, efficiency, latency, and energy consumption, showcasing each approach’s
	operational effectiveness and resource impact. The purpose of Table 2 is to present various performance metrics reported by each work, even if they differ across studies. For
	instance, some works, like \cite{c23}, reported Recall and FPR, while others, such as \cite{c27}, focused on Latency. It’s important to note that our study did not focus on optimizing
	or comparing ML model accuracy. Instead, our work emphasizes distributing the ML
	pipeline while maintaining better ML model accuracy. The diversity in metrics reflects
	how different studies approach performance evaluation.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width= 0.7\textwidth]{Fig3.png}
		\caption{Pipeline Distribution in Edge-AI Mind Map}
	\end{figure}
	
	\subsubsection{End-Based and Edge-Based Edge-AI Architectures}
	End-based and edge-based architectures endeavor to optimize computational efficiency at the network's periphery. While end devices facilitate data processing at their origin to potentially reduce latency, their limited computational capacity restricts the complexity of deployable machine learning models. Conversely, edge-based architecture, by situating the inference process on edge servers, offers improved computational resources, enabling the deployment of more sophisticated, albeit lightweight, models closer to data sources. This approach, however, must navigate the constraints of edge resources. A notable example is a study \cite{c27} that introduced an architecture utilizing smart wristbands for transmitting medical data directly to an edge server. The Proposed InTec framework differentiates itself by
	distributing the ML pipeline across Things, Edge, and Cloud layers, enabling the
	deployment of more complex models while optimizing computational resources
	across the entire architecture.
	
	\subsubsection{Hybrid Edge-AI Architecture}
	Hybrid EI architectures represent a sophisticated approach to deploying ML pipelines by integrating multiple processing layers and optimizing computational workload distribution. This method excels in leveraging the combined processing power
	of cloud, edge, and end devices to facilitate efficient ML inference. Such architectures are categorized into distinct subgroups, each addressing specific operational
	needs and challenges:
	
	\textit{1. Cloud-Edge Architectures:} This subgroup melds the capabilities of cloud and
	edge computing, strategically allocating tasks to either layer based on their computational demands and the critical nature of the functions. The design challenge
	here revolves around the dynamic allocation of resources and tasks, aiming to
	minimize latency while ensuring efficient data processing. Cloud servers primarily manage the heavy lifting of data inference training and updating ML
	models, whereas edge servers focus on real-time data analysis. This distribution
	helps significantly reduce system response times. Notable studies, such as those
	cited in references \cite{c25} and \cite{c32}, explore innovative methods like Early-Exit and
	model partitioning to enhance responsiveness while minimizing latency despite
	the overhead introduced by data compression and the need for continuous model
	deployment and updating across the cloud-edge continuum. Our presented framework, InTec, differentiates itself by extending the distribution of the ML pipeline
	beyond just the cloud and edge layers, incorporating the Things layer as well. This
	approach enables more efficient real-time processing and reduces latency further
	by localizing more computational tasks and optimizing resource use across the
	entire IoT architecture.
	
		\textit{2. Edge-Device Architectures:} Tailored for time-sensitive applications, such as health monitoring and autonomous vehicles, this architecture emphasizes rapid inference by deploying pre-trained models directly onto edge servers and devices. The challenge lies in managing the resource limitations inherent to both layers, necessitating the use of models that can provide accurate inferences with minimal computational demand. Research \cite{c31} Demonstrates the application of techniques like model layer division and Early-Exit strategies to address these constraints, ensuring
		that data can be processed efficiently across the spectrum of available resources.
		The InTec framework, the main contribution of this research, further extends this
		concept by distributing the ML pipeline across edge devices and integrating the
		Things and Cloud layers.
	
	\begin{landscape}
		\begin{longtable}{@{}p{1.5cm}p{3.5cm}p{2.5cm}p{2.5cm}p{2.5cm}p{3cm}p{5cm}p{2.2cm}@{}}
			\caption{The comparison of existing studies on the Edge-AI research area}\\\\
			\toprule
			References & Architecture & ML Model & Use Case & Dataset & Method & Technique & Parameters \\
			\midrule
			\endfirsthead
			\caption[]{The comparison of existing studies on the Edge-AI research area (continued)}\\\\
			\toprule
			References & Architecture & ML Model & Use Case & Dataset & Method & Technique & Parameters \\
			\midrule
			\endhead
			\bottomrule
			\endfoot
			\cite{c25} & Cloud/Edge & CNN & Industry & Private Dataset & ML-Model & Partitioning + Early-Exit & Accuracy, Efficiency \\
			\cite{c26} & Edge & CNN & NR & Coco & ML-Model & Partitioning + Dynamic Clusters Offloading & Latency, Network Traffic, Throughput \\
			\cite{c28} & Cloud/Edge & FFNN & Healthcare & Stress Recognition in Automobile Drivers & ML-Model & Compression & Accuracy, Energy, Latency, Network Traffic \\
			\cite{c36} & Cloud/Edge & CNN & Healthcare & MHEALTH & Hybrid Architecture & Optimizing & Accuracy \\
			\cite{c27} & Edge & EC-BDLN & Healthcare & Private Dataset & Edge-based Architecture & Optimizing & Accuracy \\
			
			\cite{c14} & Device/Edge & AlexNet & NR & Cifar-10 & ML-Model & Partitioning + Early-Exit + DNN Offloading & Accuracy, Latency, Traffic, Throughput \\
			\cite{c29} & Cloud/Edge & AlexNet/VGGNet-19 & NR & NR & ML-Model & DNN Offloading & Latency \\
			\cite{c37} & Cloud/Edge & VGG-16 & Industry & T-Less & ML-Model & DNN Offloading & Accuracy, Latency \\
			\cite{c32} & Cloud/Edge & SVR & Air Quality & Private Dataset & Hybrid Architecture & ML Pipeline Offloading & Accuracy \\
			\cite{c31} & Cloud/Device/Edge & CNN & Object Detection & ImageNet & Hybrid Architecture & ML Pipeline Offloading & Energy \\
			\cite{c15}  (baseline article) & Cloud/Edge & Autoencoder + FFNN & Healthcare & MHEALTH & Hybrid Architecture & ML Pipeline Offloading & Accuracy, Latency, Traffic \\
			\cite{c33} & Edge & ResNet & Industry & ImageNet & Edge-based Architecture & ML Pipeline Offloading + Optimizing & NR \\
			\cite{c16} & Cloud & GoogleNet-BiLSTM & Security & HMDB51 & Architecture & ML Pipeline Offloading + Optimizing & Accuracy \\
			\cite{c17} & Cloud/Edge & Light GBM & Healthcare & HAR (DWS) & Architecture & ML Pipeline Offloading & Accuracy, Latency \\
			This research (InTec) & Cloud/Edge/Things & CNN-LSTM & Healthcare & MHEALTH & Hybrid Architecture & ML Pipeline Offloading & Accuracy, Latency, Traffic, Throughput, Energy \\
		\end{longtable}
		
		
		\begin{table}[ht]
			\centering
			\caption{Performance Metrics}
			\resizebox{\linewidth}{!}{
				\begin{tabular}{ccccccccccc}
					\toprule
					Paper & Architecture & ML Model & Accuracy \% & Recall \% & FPR \% & Precision \% & Latency (ms) & Traffic Reduction \% \\
					\midrule
					\cite{c25} & Cloud/Edge & CNN & NR & 100 & 30 & NR & NR & NR \\
					\cite{c26} & Edge & CNN & NR & NR & NR & NR & 25000 & 52 \\
					\cite{c28} & Cloud/Edge & FFNN & 98 & NR & NR & NR & 860 + Tl & 103 \\
					\cite{c36} & Cloud/Device & CNN & 92.5 & 92 & NR & 91.6 & NR & NR \\
					\cite{c27} & Edge & EC-BDLN & 96.5 & NR & NR & NR & NR & NR \\
					\cite{c14} & Device/Edge & AlexNet & 77 & NR & NR & NR & 400 & NR \\
					\cite{c29} & Cloud/Edge & AlexNet / VGGNet-19 & NR & NR & NR & NR & 90 + Tl & NR \\
					\cite{c37} & Cloud/Edge & VGG-16 & 95 & NR & NR & NR & 35 + Tl & NR \\
					\cite{c32} & Cloud/Edge & SVR & NR & NR & NR & NR & NR & NR \\
					\cite{c31} & Device/Edge & CNN & NR & NR & NR & NR & NR & NR \\
					\cite{c15} & Cloud/Edge & Autoencoder + FFNN & 99.89 & NR & NR & NR & NR & 48.84 \\
					\cite{c33} & Edge & ResNet & NR & NR & NR & NR & NR & NR \\
					\cite{c16} & Cloud & GoogleNet–BiLSTM & 74.79 & 68.70 & NR & 73.01 & NR & NR \\
					\cite{c17} & Cloud/Edge & Light GBM & 99.6 & 99.6 & NR & 99.6 & 38 & NR \\
					 \\
					\bottomrule
				\end{tabular}
			}
			\vspace{10pt} % Add some space between tables and legends
			
			\begin{tabular}{l l}
				\textbf{Legend:} & NR = Not Reported \\
			\end{tabular}
		\end{table}
		\vfill
	\end{landscape}
	

	
	\textit{3. Cloud-Edge-Device Architectures:} This architectural framework aims to fully utilize processing capabilities from the cloud to edge devices, targeting unparalleled efficiency and rapid data inference. By allocating machine learning tasks according to the unique computational strengths and requirements of each layer, it strives for a cohesive operation. A notable application, referenced in \cite{c31} demonstrates this approach by integrating IoT technologies like LoRaWAN and MQTT for real-time environmental
	monitoring and object detection, with critical processing performed at the edge
	to boost responsiveness. Despite its ambitious goal to enhance processing power
	and reduce latency, this strategy encounters inherent challenges: coordinating
	complex multi-layer operations, managing resources across diverse environments,
	addressing latency to ensure timely data processing, ensuring scalability amidst
	fluctuating IoT network demands, and optimizing energy consumption for sustainability. These hurdles underscore the complexity of achieving seamless integration and operational efficiency within such a distributed architecture. The proposed InTec framework simplifies and optimizes
	the coordination of these multi-layer operations by strategically distributing the
	ML pipeline across the Things, Edge, and Cloud layers. This approach enhances
	processing power, reduces latency, and improves scalability and energy efficiency,
	making it more adaptable to fluctuating IoT network demands and diverse operational environments.
	
	\subsection{Development of Machine Learning Models for Edge-AI} 
	The development of EI machine learning models focuses on optimizing deployment across diverse devices, ensuring accuracy and adaptability to resource constraints. This involves creating models for EI with an emphasis on resource limitations, as seen in Lyu et al. \cite{c38}, who designed a privacy-preserving deep learning model for fog computing. Model compression techniques, including partitioning, pruning, and quantization, are essential for fitting models into resource-constrained environments, enhancing deployability and efficiency, as detailed by Zhao et al. \cite{c26}. Advancements in EI hardware, such as multi-core CPUs, GPUs, and enhanced microcontrollers, further support the execution of complex machine learning tasks on edge devices, enabling advanced applications like wearable IoT devices for human activity recognition, as demonstrated by Bianchi et al. \cite{c36}.