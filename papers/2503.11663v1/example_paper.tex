%%%%%%%% mlsys 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{xspace}

\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{amssymb}



\newcommand{\ourwork}[0]{MEADOW\xspace}
\newcommand{\x}[0]{$\times$\xspace}
\newcommand{\ourdataflow}[0]{TPHS\xspace}

\newcommand{\tsup}[1]{\textsuperscript{\texttt{#1}}}
\newcommand{\tsub}[1]{\textsubscript{\texttt{#1}}}

\newcommand{\ttsub}[2]{\large\texttt{#1}\tsub{#2}\normalsize\xspace}
\newcommand{\ttsup}[2]{\large\texttt{#1}\tsup{#2}\normalsize\xspace}
\newcommand{\ttt}[1]{\large\texttt{#1}\normalsize\xspace}


\newcommand{\smqktv}[0]{\ttt{Q+SM(QK\tsup{T})xV}}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{mlsys2024} with \usepackage[nohyperref]{mlsys2024} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\isaccepted}{}

% Use the following line for the initial blind version submitted for review:
\usepackage{mlsys2025}
% \usepackage{isaccepted}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{mlsys2024}

% The \mlsystitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\mlsystitlerunning{MEADOW: Memory-efficient Dataflow and Data Packing for Low Power Edge LLMs}

\begin{document}

\twocolumn[
\mlsystitle{MEADOW: Memory-efficient Dataflow and Data Packing for Low Power Edge LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the mlsys2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\mlsyssetsymbol{equal}{*}
% \isaccepted
\begin{mlsysauthorlist}
\mlsysauthor{Abhishek Moitra}{yale}
\mlsysauthor{Arkapravo Ghosh}{yale}
\mlsysauthor{Shrey Agarwal}{iitr}
\mlsysauthor{Aporva Amarnath}{ibm}
\mlsysauthor{Karthik Swaminathan}{ibm}
\mlsysauthor{Priyadarshini Panda}{yale}
\end{mlsysauthorlist}

\mlsysaffiliation{yale}{Department of Electrical and Computer Engineering, Yale University, CT, USA}
\mlsysaffiliation{iitr}{IIT Roorkie, Roorkie, India}
\mlsysaffiliation{ibm}{IBM Research - Yorktown Heights Yorktown Heights, NY USA}

\mlsyscorrespondingauthor{Abhishek Moitra}{abhishek.moitra@yale.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\mlsyskeywords{Machine Learning, MLSys}

\vskip 0.3in

\begin{abstract}
 % In prior works, self-attention layers in LLMs execute SM(QKT)xV operations in GEMM mode, which involves repeated off-chip data transfers that increase latency, especially in the prefill stage where larger inputs intensify memory demands. During the decode stage, the smaller input size reduces computational overhead, but weight fetches still dominate latency, making memory optimization critical to improve overall efficiency. 

% Large language models (LLMs) have achieved remarkable performance across different real world application scenarios. However, deploying these models on low-power edge devices is crucial to broaden their accessibility and impact. Despite their benefits, LLMs impose significant computation and memory overhead in both the prefill and decode stages, presenting a major challenge for efficient edge deployment.
The computational and memory challenges of large language models (LLMs) have sparked several optimization approaches towards their efficient implementation. 
While prior LLM-targeted quantization, and prior works on sparse acceleration have significantly mitigated the memory and computation bottleneck, they do so assuming high power platforms such as GPUs and server-class FPGAs with large off-chip memory bandwidths and employ a generalized matrix multiplication (GEMM) execution of all the layers in the decoder. In such a GEMM-based execution, data is fetched from an off-chip memory, computed and stored back. However, at reduced off-chip memory capacities, as is the case with low-power edge devices, this implementation strategy significantly increases the attention computation latency owing to the repeated storage and fetch of large intermediate tokens to and from the off-chip memory. Moreover, fetching the weight matrices from a bandwidth constrained memory further aggravates the memory bottleneck problem. To this end, we introduce \ourwork, a framework that significantly reduces the off-chip memory access for LLMs with a novel token-parallel head-sequential (TPHS) dataflow. Additionally, \ourwork applies weight packing, that performs loss-less decomposition of large weight matrices to their unique elements thereby, reducing the enormous weight fetch latency. \ourwork demonstrates 1.5$\times$ and 2.5 $\times$ lower decode and prefill latency, respectively, compared to a GEMM-based LLM implementation on the low power Xilinx ZCU102 FPGA platform that consumes less than 10W. Additionally, \ourwork achieves an end-to-end latency improvement of over 40\%, compared to prior LLM optimization works.


% \ourwork demonstrates 1.5$\times$ and 2.5$\times$ lower decode and prefill latency, respectively, compared to a GEMM-based LLM implementation on the low power Xilinx ZCU102 FPGA platform that consumes less than 10W. Additionally, compared to prior LLM optimization works, \ourwork achieves 1.6$\times$ and 2$\times$ lower decode and prefill latency at memory bandwidths as low as 1~Gbps.
\end{abstract}
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \mlsysEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\mlsysEqualContribution} % otherwise use the standard text.

% \printAffiliations{}

\section{Introduction}
\label{sec:introduction}

% \textbf{Memory bottleneck of Transformers}
% \begin{enumerate}
%     \item Huge growth of LLMs
%     \item Low power edge AI is essential to democratize them
%     \item given the huge design space of LLMs, it is essential to have a low-power reconfigurable hardware platform
%     \item FPGAs provide a very good low-power reconfgurable hardware platform
%     \item However, FPGAs have limited PS-PL bandwidth support which leads to highly memory bound implementations for today's large scale LLMs. 
%     \item Fig. shows a distribution of input, output, and computation latency for the ZCU102 FPGA with a 128-bit PS-PL bus width for different layers of a OPT-125M decoder. The SM(QKT)V operation has a huge memory fetch and storage overhead of inputs and outputs. Additionally in both decode and prefill, the weight fetching is the major overhead.  
% \end{enumerate}

The explosive growth of large language models (LLMs) necessitates efficient, low-power hardware solutions to make them accessible across diverse AI applications \cite{zhang2024llmcompass, minaee2024large, chang2024survey}. 
In particular, there have been several efforts to deploy LLMs across a swath of applications at the edge, ranging from autonomous driving systems~\cite{marcu2023lingoqa} to mobile device assistants~\cite{mobileAIBench}. Even though there have been a few custom ASIC solutions targeting fixed transformer models \cite{tambe202322, park2024lpddr}, their significant design/verification complexity and the consequent impact on the time-to-market makes it difficult for them to cater to the rapidly changing nature of the models and their underlying applications. On the other hand, more general-purpose CPU/GPU/TPU solutions deployed on the cloud cannot be replicated on edge devices due to their inherent Size, Weight and Power (SWaP) limitations.

%However, in order to effectively deploy these models on the target edge device, one must take into account its Size, Weight and Power (SWaP) limitations.
Data-center scale hardware solutions, like the AMD Alveo series ~\cite{alveo}, leverage high bandwidth memory (HBM) to handle the intense demands of LLMs, but they also consume over 200 Watts of power. In contrast, platforms like the Xilinx ZCU102 \cite{zcu102} and Xilinx ZCU104 \cite{zcu104} offer a reconfigurable, low-power alternative with a sub-10 Watt power budget, making them well-suited for exploring the extensive design space of LLMs, while, balancing power and performance. However, without HBM, these platforms face limitations in available memory bandwidth. This constraint presents a challenge, as the attention computations that drive modern LLMs are highly memory-bound. To mitigate the memory bottleneck in LLMs, techniques like weight quantization \cite{xiao2023smoothquant, lin2024awq, xu2024llamaf} and sparse computation \cite{huang2024elsa, zhang2024llmcompass} have been proposed to reduce data transfer and computational complexity \cite{wang2023cta, ma2023llm}. However, these solutions are largely tailored for larger GPUs and/or TPUs. Achieving efficient LLM acceleration on low power-budget devices with restricted memory, calls for a cohesive approach that combines architecture optimization, dataflow restructuring, and parameter compression.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/intro_fig2.pdf}\vspace{-5mm}
    \caption{Figure showing the (a) Decoder architecture used in LLMs (b) the prefill latency distribution across data fetch, store and computation across different layers in the decoder (c) the decode latency distributions. During decode, compute and storage latency is negligible compared to the weight and input fetch latency. All latency results are based on OPT-125M LLM implementation on the Xilinx ZCU102 FPGA with off-chip DRAM bandwidth = 12Gbps.}
    \label{fig:intro_fig}
\end{figure*}

%Unlike data-center scale offerings like the AMD Alveo series ~\cite{alveo}, that are equipped with high bandwidth memory (HBM) and consume over 200 W of power, platforms such as Xilinx ZCU102 \cite{zcu102} and Xilinx ZCU104 \cite{zcu104}, with a sub-10 Watts power budget, present a reconfigurable, yet efficient hardware platform that could cover the vast design space of LLMs, while co-optimizing their power/performance. However, in the absence of HBM, they are limited in terms of their available memory bandwidth. Given that attention computation operations that power modern LLMs are largely memory-bound, this presents a significant challenge. To address the memory bottleneck in LLMs, strategies like weight quantization \cite{xiao2023smoothquant, lin2024awq, xu2024llamaf} and sparse computation \cite{huang2024elsa, zhang2024llmcompass} have been proposed that reduce data transfer and computational complexity \cite{wang2023cta, ma2023llm}. However, to accelerate LLM computation on devices with limited memory, an integrated architecture, dataflow and parameter compression solution is required.

A typical LLM, especially for generative language processing, comprises of multiple layers of decoder hierarchy. A decoder architecture (as shown in Fig.~\ref{fig:intro_fig}a) has self-attention and matrix multiplication operations. During inference, the LLM operates in two stages: \emph{Prefill} and \emph{Decode}. In the prefill stage, a user-provided prompt is decomposed into multiple tokens. These tokens simultaneously undergo matrix multiplications with multi-dimensional weights to yield \ttt{Q}, \ttt{K} and \ttt{V} outputs. Subsequently, the \ttt{Q}, \ttt{K} and \ttt{V} values undergo fine-grained spatial correlations by means of \ttt{SM}(\ttsup{QK}{T})\ttt{xV} operations in multiple self-attention heads, where \ttt{SM} denotes a Softmax operation. The attention outputs are finally projected to higher dimension space by the projection (\texttt{Proj}) and \ttt{MLP} layers. Post prefill, the LLM enters the decode stage, where it predicts subsequent output tokens one-by-one. %Evidently, the LLM self-attention and matrix multiplication operations, under limited memory bandwidth, heavily bottleneck both the prefill and decode stages.





%In addition to low-power implementations, achieving high accuracy is crucial, which is why modern LLMs employ decoders (as shown in Fig.~\ref{fig:intro_fig}a) with self-attention and matrix operations at both the prefill and decode stages.


% standard architecture shown in Fig. \ref{fig:intro_fig}a As evident from the typical LLM decoder architecture shown in Fig.~\ref{fig:intro_fig}a, 


% LLM self-attention and matrix multiplication operations, under limited memory bandwidth, heavily bottleneck both the prefill and decode stages.

% Additionally, at both prefill and To overcome the memory bandwidth limitation 



% involve memory-intensive matrix multiplications and self-attention computations during the prefill and decode stages that face heavy .


%Given the vast design space of LLMs, reconfigurable platforms are essential for optimizing their power-performance trade-offs \cite{}. LLMs typically have achieved high accuracy on complex tasks. They involve attention computation that are largely memory bound. For ubiquitous AI, it is important to deploy LLMs on low power edge devices. However, the limited bandwidth of low power edge devices poses challenges for large-scale LLMs. 

% \textbf{Prior Works on Transformers}
% \begin{enumerate}
%     \item To overcome the memory bottleneck problem, works have largely focused on weight quantization. Fake quantization quantizes the weights/inputs to perform faster off-chip data transfers and dequantizes them during processing to maintain optimal accuracy. \cite{LlamaF} uses group wise quantization. \cite{llmcompass} quantization and pruning framework that optimizes the comptuational graph of LLMs. 
%     \item \textbf{problem with quantization:} they all transfer all the weights of a particular layer. For example, if the weight matrix dimensions is NxM, then all NxM elements are transferred from the off-chip data. 
%     \item Other works have focused on computation reduction. 
%     FACT reduces computations by using early correlation \cite{fact}. FlightLLM employes sparse comptuation \cite{flightllm}. CTA \cite{cta} uses token compression methods that fuse multiple tokens based on importance to minimize computation and memory bottleneck. 
%     \item \textbf{problem with computation optimization:} As FPGAs have surplus LUTs, computation is not a problem as a large number of PEs can be implemented. In fact, in such scenarios, the memory is the major bottleneck, which is not tackled by these works. The computation only consumes N\%. 
%     \item Some works like FlightLLM have used high power HBM equipped FPGAs to overcome the memory wall bottleneck. However, HBMs incur high cost/computations and power/computations which are not suitable for democratized and low power edge AI solutions. 
% \end{enumerate}



% Fig.~\ref{fig:intro_fig}a shows the architecture of an LLM decoder. LLM inference consists of prefill and decode stages. 




% prefill and decode: LLMs have two stages: 1) Prefill stage wherein an input prompt is decomposed into multiple tokens that are simultaneously processed to understand the context of the prompt. Since, multiple tokens are processed at the prefill stage, the input size is TxD. 2) After inferring the context of the input prompt in the prefill stage, the LLM starts predicting one token at a time in the decode stage. As the prediction occurs one token at a time, the input size in the decode stage is 1xD. 

% In all prior works, the \ttt{SM(}\ttsup{QK}{T}\ttt{)xV} layer is executed in the generalized matrix multiplication (GEMM) mode. Here, the input matrices for each self-attention head are fetched from the off-chip memory, computed in the GEMM array and the output is stored back to the off-chip memory. As shown in Fig. \ref{fig:intro_fig}b, This repeated data transfer consumes significant latency in the prefill stage. Moreover, as during the decode stage, the input size is small, the compute and output memory storage overheads are absent. As shown in Fig. \ref{fig:intro_fig}c, the weight fetches dominate the overall latency. 

In most prior works, the \ttt{SM}(\ttsup{QK}{T}) \ttt{xV} layers are executed in the form of a generalized matrix multiplication (GEMM) operation \cite{zeng2024flightllm, wang2023cta, huang2024elsa}. Here, the input matrices for each self-attention head are fetched from the off-chip memory, processed in the GEMM array, and the output is stored back to the off-chip memory. As shown in Fig.~\ref{fig:intro_fig}b, under limited off-chip memory bandwidth (12~Gbps), this repeated data transfer significantly increases the latency in the prefill stage, where larger input sizes exacerbate memory access demands. During the decode stage, however, the input size is much smaller, reducing compute and data storage overheads to a negligible fraction, while the weight fetches dominate latency (Fig. \ref{fig:intro_fig}c). Thus, optimizing on memory accesses through efficient compute dataflow in both the prefill and decode stages is essential to reduce the overall latency.


% Furthermore during attention computation in all prior works, the Q and softmax(QK$^T$)V (\ttt{Q+SM(QKT)V}) layers of the LLM are executed in the generalized matrix multiplication (GEMM) mode. Here, the input matrices for each self attention head are fetched from PS-PL, computed in the FPGA and then the output is sent from the PL-PS. This repeated data transfer consumes significant latency. Therefore, a dataflow suited for the \ttt{Q+SM(QKT)V} layer is required. Moreover, as seen in Fig. \ref{}, during the decode stage it is the weight fetches that require the major latency. Although quantization can alleviate the amount of data transferred, it alone requires all the weight values to be transferred.

% As seen in Fig. \ref{}b, during the prefill stage, the \ttt{Q+SM(QKT)V} layers consume significant latencies for input and output data fetch and storage, respectively. For example, an analysis of a ZCU102 FPGA with a 128-bit PS-PL bus width running an OPT-125M decoder reveals significant memory constraints: the SM(QKT)V operations exhibit high memory fetch and storage demands, and weight-fetching dominates latency in both decode and prefill stages. 

% For transformers, as shown in Fig. \ref{fig:gemm_arch}b, GEMM-based implementations exhibit a notable distribution of time spent on weight/input fetches, computation, and output storage. A key challenge arises during attention computation, where significant output storage bottlenecks occur. This is particularly problematic in the QKT and SMV layers, where the Q, K, and V matrices are fetched from off-chip DRAM. Since computations are performed head-by-head, the TxT outputs for each head must be stored off-chip, incurring considerable overhead, especially as T increases.

% Previous works generally utilize a GEMM-based execution mode for transformer models across all layers. As illustrated in Fig. \ref{fig
% }b, GEMM-based approaches distribute latency across weight and input fetching, computation, and output storage. However, GEMM encounters a significant bottleneck in output storage, particularly within the attention mechanism. This issue is especially evident in the QKT and SMV layers, where the Q, K, and V matrices are fetched from off-chip DRAM. Since these matrices are processed on a head-by-head basis, the 
% T×T outputs for each head must be stored off-chip, leading to substantial overhead as the sequence length T grows.

% \textbf{Problems with Prior Approaches}
% \begin{enumerate}
%     \item \textbf{show a distribution of input, output, and weight sizes for different layers of a decoder. With pipelined and indexing our goal is to overcome the memory bound implementations. The SM(QKT)V operation has a huge memory fetch and storage overhead of inputs and outputs. Additionally in both decode and prefill, the weight fetching is the major overhead.}
%     \item With pipelined and indexing our goal is to overcome the memory bound implementations.
%     \item Low power FPGAs are equipped with DDR DRAM memories which offer limited bandwidth aggrevating the memory bottleneck problem. 
%     \item Sparse accelerator works only overcome the computational bottleneck and do not mitigate the memory bottleneck. No CSR or CSC
%     \item Quant works mitigate the memory wall problem but transfer all of the weight matrix from DRAM-FPGA.
%     \item Additionally, all prior follow gemm arch which incurs back and forth data movements between DRAM-FPGA to store and fetch activations and weights. SEquential Head
%     \item Furthermore, the GEMM dataflow is employed irrespective of the transformer (transformer architecture and quantization) and FPGA (memory bandwidth \& No. of PEs).
% \end{enumerate}

% \textbf{What we do in this work}
% \begin{enumerate}
%     \item To this end, we target memory bottleneck reduction in a two pronged approach. 
%     \item Applying indexing and bit packing to compress the weight matrices reducing the latency to fetch weight matrices
%     \item We use a pipelined data flow that prevents data movements to store and fetch outputs of intermediate layer. Pipelining inherently uses head parallel.
%     \item Finally, to cater to different FPGA platforms and transformer workloads, we develop “tool” to choose network and FPGA-specific data flows
%     \item “Tool” uses the FPGA (No. of PEs \& Memory Bandwidth) and Transformer architecture (Weight precision \& number of attention heads, channel dimensions and so on) characteristics decide appropriate data flows for efficient implementation. 
% \end{enumerate}
% \textbf{Table showing what are our contributions}

%To address these challenges, we introduce the \ourwork framework, leveraging a novel Token Parallel Head Sequential (TPHS) dataflow that efficiently pipelines the \ttt{Q}, \ttsup{QK}{T}, \ttt{SM}, and \ttt{SMxV} layers to significantly reduce off-chip data fetches and storage during the prefill stage. To further mitigate the latency and bandwidth overhead of weight fetches, \ourwork implements Weight Packing, which compacts the weight matrix by transferring only its unique elements, significantly minimizing weight transfer volume. Additionally, \ourwork applies bit-packing techniques on the weights to maximize DRAM bandwidth utilization, enhancing memory efficiency.

To address the above challenges, we introduce the \ourwork framework. During the prefill and decode stage, \ourwork executes the \ttt{KV}, \ttt{Proj} and \ttt{MLP} layers in the GEMM mode while, the \ttt{Q}, \ttsup{QK}{T}, \ttt{SM}, and \ttt{SMxV} layers are executed with a novel Token-Parallel Head-Sequential (TPHS) dataflow which performs effective layer pipelining and significantly reduces the off-chip data fetches and storage latency. To further mitigate the latency and bandwidth overhead of weight fetches, \ourwork implements Weight Packing, which compacts the weight matrix by transferring only its unique elements, significantly minimizing weight transfer volume. Additionally, \ourwork applies bit-packing techniques on the weights to maximize memory bandwidth utilization, enhancing memory efficiency. %Weight packing does not apply any additional approximations on the weight values which results in iso-accuracy performance.

% To address these challenges, we introduce the \ourwork framework, leveraging a novel Token Parallel Head Sequential (TPHS) dataflow that efficiently pipelines the \ttt{Q}, \ttsup{QK}{T}, \ttt{SM}, and \ttt{SMxV} layers to significantly reduce off-chip data fetches and storage during the prefill stage. To further mitigate the latency and bandwidth overhead of weight fetches, \ourwork implements Weight Packing, which compacts the weight matrix by transferring only its unique elements, significantly minimizing weight transfer volume. Additionally, \ourwork applies bit-packing techniques on the weights to maximize DRAM bandwidth utilization, enhancing memory efficiency. As weight and bit-packing only apply matrix decomposition to unique values, and do not apply any approximation technique, \ourwork entails no accuracy degradation. 

The key contributions of our work are as follows:
\begin{enumerate}
    \item We propose \ourwork that uses a novel Token Parallel Head Sequential (TPHS) dataflow to compute the \ttt{SM}(\ttsup{QK}{T})\ttt{xV} layers in pipeline, significantly reducing the volume of data transfers to and from off-chip memory.
    \item We introduce Weight Packing, a technique that decomposes LLM weight matrices into unique elements to minimize weight fetch latency at prefill and decode stages. Additionally, to further accelerate weight fetches and maximize DRAM bandwidth efficiency, we implement bit-packing to compactly store and transfer weight data. Weight packing is an approximation-less technique that yields loss-less accuracy performance.
    \item We evaluate \ourwork on the ZCU102 FPGA with a peak power budget of 10W across varying off-chip DRAM bandwidths and input token lengths on state-of-the-art OPT-125M and OPT-1.1B LLM models. \ourwork achieves 2.5$\times$ and 1.5$\times$ lower prefill and decode latency compared to GEMM-based implementations for 1-6 Gb/s data bandwidth ranges. \ourwork also achieves over 40\% end-to-end latency improvement compared to prior LLM optimization works. 
    \item We demonstrate the generalizability of \ourwork across vision transformer (ViT) benchmarks, achieving 1.6$\times$ lower inference latency compared to GEMM-based ViT implementations. We also demonstrate how \ourwork can be applied to multiple FPGA configurations with varying PE sizes and memory bandwidth. 
\end{enumerate}

% To address memory bottlenecks, we propose a two-pronged approach that targets both weight fetch and output storage. First, we apply indexing and bit-packing techniques to compress weight matrices, thereby reducing the latency of weight retrieval from PS-PL. As weight fetching is common to both prefill and decode, indexing and bit-packing reduces the latency at both stages. Second, we implement the token parallel head sequential (TPHS) dataflow that avoids frequent input and output data fetch and store, respectively between PS and PL thereby reducing the latency. The weight indexing and bit-packing combined with TPFS dataflow leads to N$\times$ and M$\times$ reduction in prefill and decode latency, respectively. \ourwork also leads to K$\times$ higher throughput under multi-batch execution compared to GEMM implementations. To support various FPGA platforms and transformer workloads, we introduce an automated tool that customizes dataflows based on platform specifications—such as the number of processing elements (PEs) and memory bandwidth—as well as transformer characteristics like weight precision, attention heads, and channel dimensions. This tool optimizes dataflow configurations to ensure efficient deployment on FPGA hardware.

% \section{Related Works and Motivation}
% \textbf{Quantization}: Does not leverage the redundancy in the weight patterns

% \subsection{Problems with GEMM-based Accelerators}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/GEMM_Architecture.pdf}
%     \caption{Only show for T = 256 and T = 512 and T=1024}. Remove 4
%     \label{fig:gemm_arch}
% \end{figure}
% Fig. \ref{fig:gemm_arch}a illustrates the computation flow in GEMM-based accelerators, which follows a standard sequence: 1) Input and weight data are fetched from memory, 2) MAC (Multiply-Accumulate) operations are performed, 3) Outputs are stored back to DRAM, and 4) These outputs are later fetched as inputs for subsequent layers.

% For transformers, as shown in Fig. \ref{fig:gemm_arch}b, GEMM-based implementations exhibit a notable distribution of time spent on weight/input fetches, computation, and output storage. A key challenge arises during attention computation, where significant output storage bottlenecks occur. This is particularly problematic in the QKT and SMV layers, where the Q, K, and V matrices are fetched from off-chip DRAM. Since computations are performed head-by-head, the TxT outputs for each head must be stored off-chip, incurring considerable overhead, especially as T increases.

% Although some works \cite{flightllm, etc} mitigate this by utilizing on-chip BRAM to store QKT and SM outputs, this approach is unfeasible for smaller FPGAs due to insufficient BRAM resources as seen in Fig. \ref{fig:gemm_arch}c. This limitation underscores the need for a more efficient dataflow that avoids storing intermediate values in external memory. To address this, we propose token-level pipelining, which processes data immediately as it is generated, eliminating the need for excessive off-chip storage.

\section{Related Work}

\textbf{Data compression techniques:} Weight and input quantization is a widely adopted approach for data compression in LLMs. Works such as SmoothQuant \cite{xiao2023smoothquant}, AWQ \cite{lin2024awq}, and LlamaF \cite{xu2024llamaf} apply fake quantization methods to lower off-chip data transfers, and dequantize the compressed inputs and weights during computation to maintain good accuracy. A recent work MECLA \cite{qin2024mecla} applies a sub-matrix partitioning technique wherein, different sub-matrices within a larger matrix is approximated as a function of a base sub-matrix. 

\textbf{Sparse Computations:} 
Sparse computation techniques leverage the inherent dynamic sparsity of LLMs to reduce computation. Unstructured sparsity, as implemented in methods like ELSA \cite{huang2024elsa, fang2022algorithm, chen2023dynamic} with N:M sparsity, selectively prunes non-essential connections, effectively reducing computational load. FlightLLM \cite{zeng2024flightllm} implements N:M sparse computation using FPGA-based accelerators with HBM to address memory bottlenecks.

Structured pruning addresses the limitations of unstructured pruning by removing entire blocks or groups of computations. For example, token compression in CTA \cite{wang2023cta} reduces memory and compute demands by compressing less critical tokens. Gradient-based pruning, as used in LLM Pruner \cite{ma2023llm}, selectively prunes attention heads based on gradient information, focusing computational resources on essential parts of the model. ALISA \cite{zhao2024alisa} focuses on retaining tokens that are crucial towards generating new tokens via a sparse window attention technique. FACT \cite{qin2023fact} focuses on performing eager computation of attention tokens at minimal computation overhead and performing sparse computations for subsequent layers. 

\textbf{Parallel research directions} towards designing more hardware efficient transformer architectures are also being developed. EdgeBERT \cite{tambe2021edgebert}, PIVOT \cite{moitra2024pivot} and TReX \cite{moitra2024trex} use entropy of inputs to perform dynamic voltage scaling, attention skipping and reuse, respectively to achieve hardware efficiency. FlexLLM \cite{miao2024flexllm} introduce a unique inference and parameter-efficient finetuning to achieve efficient yet, highly accurate LLMs.

\ourwork is an orthogonal solution to prior techniques, introducing architectural and dataflow innovations along with weight packing to optimize weight fetch latency. By restructuring the dataflow and enhancing memory access patterns, \ourwork minimizes latency in retrieving weights, addressing memory bottlenecks in low memory bandwidth hardware without sacrificing model accuracy.

\section{\ourwork Architecture}
\label{sec:meadow_arch}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/arch_overview.pdf}\vspace{-5mm}
    \caption{(a) Tiled architecture of \ourwork containing parallel and broadcasting processing elements (PEs), pipelined softmax (SM) module, modules for layer normalization (LN) and non-linear activation functions like ReLU/GeLU (NL). (b) The hybrid PE architecture capable of operating in GEMM and pipelined modes. (c) Architecture and execution flow of a parallel and broadcasting MAC PE. (d) The pipelined softmax (SM) module. }
    \label{fig:arch_overview}
\end{figure*}

\ourwork follows a tiled architecture as shown in Fig. \ref{fig:arch_overview}a containing multiple processing elements (PEs), modules for layer normalization (LN), softmax operations (SM) and non-linear (NL) activation functions, such as, ReLU/GeLU. Each PE contains several multipliers and accumulators to carry out the multiply-accumulate operations. For computation, the input data is fetched from the off-chip DRAM to the Input block RAM (BRAM). The raw input values are directly transferred to the respective input register files (RF) of the PEs. Since, \ourwork applies an additional weight packing to reduce off-chip weight fetches, the Weight BRAM stores the packed and encoded weight values which first needs to be processed by the Weight Unpacking and Index Look-up (WILU) Module. The WILU module reads data from the Weight BRAM and sends the data to the respective weight RFs of the PEs. The outputs from each PE are stored back to the output BRAM. All communications between BRAM and PE, SM, LN and NL modules are enabled by the network on chip (NoC) interconnect. The NoC additionally handles data communication between PEs and SM modules to facilitate the TPHS dataflow, defined in Section~\ref{sec:tphs}. 


% A network-on-chip (NoC) interconnect performs data transfer between the input and weight block RAMs (BRAMs) and from PE to the output BRAM. The NoC is also responsible for communicating data between PEs. 
% Since, \ourwork applies an additional weight packing to reduce the off-chip weight fetches, they have to undergo unpacking in the Weight unpacking and Index Look-up (WILU) Module. The WILU module reads data from the weight BRAM and sends the data to the respective weight RFs of the PEs. Since no data packing is applied to the inputs, the raw input values stored in the Input BRAM are directly transferred to the PEs. 



% Fig. \ref{fig:arch_overview}a shows the architecture overview of \ourwork. For computing any transformer layer, the indexed and packed weights and inputs are stored in the Weight and Input BRAMs, respectively. The indexed and packed weights are unpacked by the BUM module based on the mode as explained in Section \ref{sec:unpacking_hw}. Following this, the unpacked weights and inputs are directed to the respective PEs using the NoC. Each PE contains the contains a MAC array, weight and input register file (RF), a pipeline register and output RF. 


\textbf{Hybrid PE for GEMM and Pipelined Execution:} \ourwork employs a dual execution strategy: GEMM mode for the \ttt{KV}, \ttt{Proj}, and \ttt{MLP} layers, and the TPHS dataflow for the \ttt{Q}, \ttsup{QK}{T}, \ttt{SM}, and \ttt{SMxV} layers, enabling pipelined execution that minimizes data fetch and store latency. To support both GEMM and pipelined modes seamlessly, \ourwork utilizes a hybrid PE architecture, designed for flexible execution across modes. The PE shown in Fig. \ref{fig:arch_overview}b, integrates a multiply-accumulate (MAC) unit, input, weight, and output register files (RF), along with a pipeline register (PREG). All RFs and the pipeline registers are double-buffered to minimize data fetch and store latency \cite{moitra2024pivot}.

For the GEMM mode, data from the input and weight BRAMs are loaded into the input and weight RF, respectively. These data values are fetched and processed in the MAC array and the outputs are stored in the output RF. Once the output RF reaches capacity, the data is transferred to the output BRAM via the NoC. In contrast, for the pipelined mode, weights are loaded from the BRAM into the weight RF while the inputs are fetched directly from the pipeline register. The Input BRAM remains inactive during the pipelined mode of operation. After the MAC operation, the outputs are transferred directly through the NoC to the pipeline register of a target module (such as the softmax unit or another PE) in the subsequent pipeline stage.



% In \ourwork, the PE supports two different modes of operation: 1) Pipelined mode and 2) GEMM mode of Execution. \textbf{Show these in the diagram} Under the GEMM mode, the MAC array fetches its inputs from the Input RF and the outputs are stored in the Output RF. Under token pipelined execution mode, the input is derived from the pipeline register and the outputs are sent to the pipeline register of a desired PE. The PE NoC ensures reliable data movement between the different PEs in the PE array.

\textbf{Parallel and Broadcasting PE:} \ourwork's tiled architecture contains a mix of Parallel MAC and Broadcasting MAC PEs (for example \ttsub{PE}{1-8} = Parallel and \ttsub{PE}{9-10} are Broadcasting MAC PEs as shown in Fig. \ref{fig:arch_overview}a). As shown in Fig. \ref{fig:arch_overview}c, both parallel and broadcasting MAC PEs use an array of multipliers but use different accumulation strategies. The Parallel MAC PE incorporates an adder tree, allowing it to multiply all elements along the multiplication dimension (\ttsub{d}{mult}) in a single cycle. In contrast, the Broadcasting MAC PE features accumulators (registers coupled with adders), enabling it to broadcast each input element along \ttsub{d}{mult} across all corresponding output channels and perform multiplication and accumulation sequentially over \ttsub{d}{mult} cycles. The Parallel MAC and Broadcasting MAC PEs are essential for facilitating the TPHS dataflow, described in Section~\ref{sec:tphs}.

\textbf{Pipelined Softmax Module (SM Module):} The numerically stable softmax computation of a given token is shown in Equation \ref{eq:softmax}. 
\begin{equation}
    SM = \frac{e^{x_i-max}}{\Sigma_i e^{x_i-max}}
    \label{eq:softmax}
\end{equation}
The computation requires three sequential stages: 1) finding the maximum across all the features in the token, 2) computing the exponent and the summation of all exponents and, 3) f
inally, dividing each exponent value with the exponent summation. Due to the sequential nature of the softmax stages, it is latency intensive. To this end, \ourwork pipelines the three stages across tokens to improve the softmax computation throughput. As shown in Fig. \ref{fig:arch_overview}d, the SM Module consists of three pipelined stages \ttt{MAX}, \ttt{EXP} and \ttt{DIV}. Each stage processes a token feature-by-feature over \ttt{F} cycles, where \ttt{F} is the number of features in the token. The \ttt{MAX} stage compares the feature values and returns the maximum value at the end of \ttt{F} cycles. Subsequently, the values are written to the \ttt{EXP} stage buffer. In the \ttt{EXP} stage, the maximum value output from the \ttt{MAX} stage is subtracted from each feature and the exponent values are computed. For hardware efficiency, the exponent is computed using the \ttt{EXP} \ttt{LUT} lookup table. Simultaneously, the exponent values are summed up and are stored in the DIV stage buffer. Finally, in the \ttt{DIV} stage the exponent values are fetched from the \ttt{DIV} stage buffer and divided by the exponent summation value.

% \textbf{Execution of a Decoder in \ourwork:} Fig. \ref{fig:arch_overview}c shows the execution flow of a transformer decoder. The layer normalized inputs undergo multiplication with the \ttsub{W}{K} and \ttsub{W}{V} matrices in the GEMM execution mode to yield \ttt{K} and \ttt{V} values, respectively. Following this, the \ttt{Q}-\ttt{SM(}\ttsup{QK}{T}\ttt{)V} layer is computed using the token parallel head sequential (TPHS) dataflow which computes the \ttt{SMV} outputs and stores them in the off-chip DRAM. Next, these values are fetched from the DRAM memory to compute the projection output, followed by MLP computation. The projection, and MLP layers are executed in the GEMM mode. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline_2.pdf}\vspace{-5mm}
    \caption{Figure showing an example of (a) token parallel head sequential (TPHS) dataflow with two input tokens being processed parallely (b) The pipelined execution of a transformer with 3 heads (\texttt{H1-H3}) and 4 input tokens (\ttsub{IP}{1-4}).}
    \label{fig:pipeline}
\end{figure*}

 

\section{TPHS Dataflow}\label{sec:tphs}
% The attention for each decoder is computed in three stages:
% \textbf{1. Pre-computation of K and V in GEMM mode:} The K and V values are first computed in the GEMM mode of execution for all the $T$ input tokens. For this the decoder inputs, post-layer normalization operation and weights $W_K$ and $W_V$ are fetched from the DRAM and the output $K$ and $V$ matrices are stored back to the DRAM.   

To overcome the memory bound implementation of \ttt{Q}, \ttsup{QK}{T}, \ttt{SM(QKT)} and \ttt{SMxV} operation, \ourwork uses the token parallel head sequential (TPHS) dataflow. The TPHS dataflow shown in Fig. \ref{fig:pipeline}a, pipelines all the computations for each attention head in parallel across multiple tokens. In the example in Fig. \ref{fig:pipeline}a, we show how attention head 1 (\ttt{H1}) is computed for input tokens \ttsub{IP}{1} and \ttsub{IP}{2}. The TPHS dataflow requires the following data from the off-chip DRAM to be stored before computation- the input tokens \ttsub{IP}{1} and \ttsub{IP}{2} of size \ttt{1xD} each, the \ttsub{K}{H1}, \ttsub{V}{H1} pre-computed values for head H1 of size \ttt{TxHD}, where \ttt{T} and \ttt{HD} are the total number of input tokens and head dimension, respectively. Additionally, for the \ttsub{Q}{H1} computation, the \ttsub{W}{Q,H1} matrix of dimensions \ttt{DxHD} are required.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/indexing2.pdf}\vspace{-5mm}
    \caption{Figure showing (a) process of generating the unique matrix and the trends in the reduction ratios for OPT-125M and OPT-1.3B LLM models across different layers in the decoder. Reduction ratios are averaged across all the decoder layers. (b) packet-specific encoding precision and (c) frequency-aware reindexing to further optimize the DRAM bandwidth.}
    \label{fig:indexing_data_packing}
\end{figure*}
\ttsub{IP}{1} and \ttsub{IP}{2} are multiplied by \ttsub{W}{Q,H1} parallelly in \ttsub{PE}{1-3} and \ttsub{PE}{4-6}, respectively. This results in \ttsub{Q}{1,H1} and \ttsub{Q}{2,H1} for the two input tokens. \ttsub{Q}{1,H1} and \ttsub{Q}{2,H1} data is sent to the pipeline registers of \ttsub{PE}{7} and \ttsub{PE}{8}, respectively where they are multiplied with \ttt{T} tokens of \ttsub{K}{H1} resulting in \ttsub{\ttsup{QK}{T}}{1,H1} and \ttsub{\ttsup{QK}{T}}{2,H1} values over \ttt{T} cycles. 
At each cycle, the \ttsup{QK}{T} outputs are sent to the \ttt{MAX} stage of softmax module which returns the maximum across all the \ttsup{QK}{T} values at the end of \ttt{T} cycles. Subsequently, these values are forwarded to the \ttt{EXP} stage and the \ttt{DIV} stage which finally yield the \ttt{SM} values over \ttt{T} cycles. In Fig. \ref{fig:pipeline}a, the \ttt{MAX}, \ttt{EXP} and \ttt{DIV} stages are combined into \ttt{SM} stage for simple visualization. The respective softmax outputs are sent to the pipeline registers of the broadcasting PEs \ttsub{PE}{9} and \ttsub{PE}{10} to compute the \ttt{SMxV} output. Here, the \ttt{SM} outputs are multiplied with \ttsub{V}{H1} tokens over \ttt{T} cycles to yield \ttsub{SMV}{1,H1} and \ttsub{SMV}{2,H1} outputs for both tokens. The \ttsub{SMV}{1,H1} and \ttsub{SMV}{2,H1} outputs are stored to the off-chip DRAM. As shown in Fig. \ref{fig:pipeline}a, each stage requires \ttt{T} clock cycles. 

Fig. \ref{fig:pipeline}b shows an example of the pipelined execution of TPHS dataflow. Here, we consider a transformer having 3 self-attention heads with 4 tokens and two tokens being simulatenously processed. Additionally, we show the expanded stages inside the SM module for better visualization. In the TPHS dataflow, first all H1 self-attention heads are computed for every input token before proceeding to the computation of H2. This minimizes the amount of back-and-forth data transfers of the \ttsub{W}{Q}, \ttt{K} and \ttt{V} matrices thereby minimizing additional latency overhead. 

\section{Weight Packing}



% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/process_overview.png}
%     \caption{}
%     \label{fig:process_overview}
% \end{figure}


\subsection{Creating the Unique Matrix}

Let \ttt{W} be a matrix of trained weight values with dimensions \ttt{NxM}, where \ttt{M} represents the inner product dimension. As shown in Fig. \ref{fig:indexing_data_packing}a, the inner dimension \ttt{M} is divided into chunks of size \ttt{C}, where each element in \ttt{C} is a Q-bit value based on the quantization of the weight matrix.
Next, as illustrated in Fig. \ref{fig:indexing_data_packing}a, a \texttt{Unique Matrix} is generated, containing the unique chunks, each assigned a unique ID. These chunk IDs are used to encode the weight matrix, resulting in the creation of the \texttt{Encoded W} matrix. To intuitively understand the amount of redundancy in the LLM weight matrices, we define the reduction ratio as the ratio between the total number of chunks in the encoded W matrix ($N\times M/C$) and the number of unique chunks. Higher reduction ratio signifies more redundancy and vice-versa. 
As seen in Fig. \ref{fig:indexing_data_packing}a, for the decoder weights of OPT-125M and OPT-1.3B the reduction ratio varies in the order of $10^2$ to $10^3$ suggesting high redundancy in the weight matrices.
% Fig. \ref{fig:indexing_data_packing}a displays a histogram showing the frequency of each unique chunk, revealing significant data redundancy. Fig. \ref{fig:d} further highlights the ratio between the total number of chunks in matrix \( W \) and the number of unique chunks in the index matrix.

\subsection{Packet-specific Encoding Precision}
To improve the DRAM bandwidth efficiency, multiple elements of the encoded W matrix are grouped together to form a packet and transferred from the DRAM for processing. 
As shown in Fig. \ref{fig:indexing_data_packing}b with naive data packing, all the packets use the same data precision to represent the encoded weights. The precision here is determined by the maximum number of unique chunks in the unique matrix (5 as in the Fig. \ref{fig:indexing_data_packing}a). However, using homogeneous bit-precision across packets lead to inefficiencies, as cycles are wasted transmitting low-precision encoded values that could otherwise be represented with fewer bits. For example, packets \ttt{E} and \ttt{F} use 3-bit precision to represent 2-bit numbers. 

To this end, we employ packet-specific bit-precision to represent the encoded values, where each packet is assigned an optimal precision to maximize packing efficiency. As depicted in Fig. \ref{fig:indexing_data_packing}b, employing packet-specific bit-precision allows low-bit encoded values to be packed together more effectively, thereby reducing the number of cycles required for transmission. 
The encoding precision for each packet is determined by the maximum encoded value in the respective packet. Additional mode bits are now used to determine the bit-precision of each packet (for example 3-bits for packet \ttt{A'} and 2-bits for packets \ttt{E'}, \ttt{G'}). Packets with mode = 0 and mode = 1 use 3-bits and 2-bits to represent the encoded values, respectively. The mode bits will be used by the WILU module to unpack the grouped encoded values. Packet-specific encoding allows packing more data per packet thereby improving the DRAM bandwidth efficiency.

\subsection{Frequency-aware Re-indexing}

As illustrated in Fig. \ref{fig:indexing_data_packing}c, frequently occurring chunk IDs in the encoded W matrix (e.g., chunk ID = 3) may necessitate higher precision, which can limit the efficiency of bit packing.
In frequency-aware re-indexing, the chunk IDs are re-assigned to each unique chunk based on their frequency of occurrence \textit{i.e.,} chunk IDs appearing more frequently are assigned lower chunk ID. For instance, in the example presented in Fig. \ref{fig:indexing_data_packing}c, chunk IDs [0, 1, 2, 3, 4] with frequencies [2, 2, 1, 6, 5] are re-assigned new chunk IDs [2, 3, 4, 0, 1]. This approach increases the proportion of low-precision chunk IDs in the encoded W matrix, resulting in efficient bit packing and thereby reducing transfer cycles. The modified encoded W and the reindexed unique matrix are transferred from the DRAM for processing.

\subsection{Weight unpacking and Index Look-up Module}
\label{sec:unpacking_hw}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/wilu_module.pdf}\vspace{-5mm}
    \caption{(a) The WILU Module (b) The mode-aware unpacking (MAU) module.}
    \label{fig:wilu_module}
\end{figure}

Fig. \ref{fig:wilu_module}a shows the execution of the WILU module. The WILU module reads the encoded and packed weight values from the weight BRAM as discussed in Section \ref{sec:meadow_arch}. A packet read from the weight BRAM contains mode bits and packed encoded weight values. The mode aware unpacking (MAU) module unpacks the packed encodings based on the mode as shown in Fig. \ref{fig:wilu_module}b. For example, for an 8-bit packed encoding, d0 to d7 is unpacked in 1, 2 and 4-bit values for modes 0, 1 and 2, respectively. The unpacked encodings are used to look up the reindexed unique matrix to get the actual weight values that are sent to the weight RF of the respective PE through the NoC.


% The input word to the PL contains multiple packets of mode-wise packed weight encodings, i.e., the indices of the look-up-table (LUT) storing the unique weight patterns. The first three bits within a packet denote the mode (Fig. \ref{fig:bit_unpacking_hw}). The weight encodings are stored contiguously in the packet, with a precision of $2^{mode}$ bits, as illustrated in Fig. \ref{fig:bit_unpacking_hw}c. The bit-unpacking (BU) module (Fig. \ref{fig:bit_unpacking_hw}b) unpacks a packet according to the mode bits as shown in Fig. \ref{fig:bit_unpacking_hw}c. The BU module contains individual mode has its respective Mode module, which ensures proper grouping of the packet bits. Within this module, the L-bits packet is sliced into $\left\lceil L/B \right\rceil$ values of B-bits precision each. As shown in the table in Fig. \ref{fig:bit_unpacking_hw}b, the precision (B) varies with the mode. These unpacked indices are shifted into a D-depth FIFO buffer of width B-bits over D clock cycles. Every entry in the FIFO buffers of each module represent an index value. To ensure correct operation, the index values from the appropriate Mode module must be used to look up the actual weights in the LUT. Thus, we use a MUX with select line same as the mode bits to select the correct index value. Although it takes D clock cycles initially to fill the FIFO buffers, the next packet is fetched and pushed into the pipeline in the steady state. Also, a valid signal is generated within the bit-unpacking module to indicate that the MUX output is a valid index value. Note that the unpacked indices generated by each Mode module are zero-padded to ensure bit-width consistency of the MUX inputs.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/prefill_GEMM_vs_ours.pdf}\vspace{-6mm}
    \caption{Time to first token (TTFT) Comparison of \ourwork with GEMM-based decoder implementation of the (a) OPT-125M and (b) OPT-1.3B LLM models on the ZCU102 FPGA with varying off-chip DRAM bandwidths. The evaluations are performed with 64 and 512 tokens during the prefill stage. }
    \label{fig:results_gemm_ours}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/decode_gemm_ours.pdf} \vspace{-6mm}
    \caption{Time between tokens (TBT) comparison of \ourwork with GEMM-based decoder implementations of the (a) OPT-125M and (b) OPT-1.3B LLM models on the ZCU102 FPGA with varying off-chip DRAM bandwidths. For all cases the number of prefill tokens set to 512. The TBT is then measured for the 64th and 512th predicted token in the decode stage.}
    \label{fig:decode_gemm_ours}
\end{figure*}


% \textbf{Projection Computation:} During projection computation, the \ttt{SMV} token outputs are fetched from the off-chip DRAM memory and multiplied with the projection layer weights in the GEMM mode to yield the final attention outputs. 

% tokens of size 1xHD is multiplied with K tokens each of size HDx1 resulting in 1x1 QKT values over T cycles. 
% At each cycle the QKT outputs are sent to the Max stage of softmax module which returns the maximum across all the QKT values at the end of T cycles. Subsequently, these values are forwarded to the Exp stage and the division stage which finally yield the 1x1 softmax values over T cycles. 
% In the SMV stage, the 1x1 softmax outputs are multiplied with V tokens of size 1xHD over T cycles in the Broadcasting PE to yield SMV output of size 1xHD after T cycles. 
% Next, the SMV output tokens of size 1xHD from different heads are concatenated into 1 1xHD sized token and sent to the Proj Stage PEs. Each Proj PE holds the partitioned DxD W\_Proj weights of size PxHD.


% \subsection{MLP Pipelining}

\section{Results and Analyses}
\subsection{Experiment Setup}

\textbf{LLM Models and Datasets:} For benchmarking \ourwork, we use the OPT-125M and OPT-1.3B LLM models \cite{zhang2022opt} finetuned on the LAMBADA dataset using zero-shot adaptation with Smoothquant \cite{xiao2023smoothquant} post-training quantization. The weights and inputs are quantized to 8-bit precision. The 8-bit weight and input quantized OPT-125M and OPT-1.1B models achieve \textbf{60.7\% and 69.7\% accuracy} on the LAMBADA dataset.

\textbf{Xilinx ZCU102 FPGA Implementation:} For hardware evaluation, we implement the hybrid GEMM-Pipelined architecture of \ourwork on the Xilinx ZCU102 FPGA using the hardware parameters shown in Table \ref{tab:parameter_table}. The implementation uses 150K LUT, 845 BRAM and 2034 DSP resources. To maximize the number of PEs, we utilize both LUTs and the DSP blocks. Additionally, register files and pipeline registers are implemented using the LUT-based registers. 

\begin{table}[]
    \centering
    \resizebox{0.8\linewidth}{!}{\begin{tabular}{|c|c|}\hline
        \textbf{Parameter} & \textbf{Value} \\ \hline
         \#Parallel \& \#Broadcasting PEs & 84, 12 \\ \hline
         \#Multipliers per PE & 64 \\ \hline
         \#SM, \#LN \& \#ReLU Modules & 84, 8, 8 \\ \hline
         Weight, Input \& Output BRAM Size & 1MB, 1MB \& 1MB \\ \hline
         Weight, Input \& Output RF Size & 4KB, 4KB \& 4KB \\ \hline
         Clock Frequency & 100 MHz \\ \hline
    \end{tabular}}
    \caption{Hardware Parameter Table for ZCU102 FPGA Evaluation}\vspace{-4mm}
    \label{tab:parameter_table}
\end{table}

\textbf{GEMM Baseline:} To benchmark prefill and decode latency, we use the GEMM baseline. The GEMM baseline is realized by operating the \ourwork architecture in fully GEMM mode. Here, all the layers in the decoder \ttt{Q}, \ttt{K}, \ttt{V}, \ttsup{QK}{T}, \ttt{SMxV}, \ttt{Proj} and \ttt{MLP} are executed in the GEMM mode. This captures the standard execution pattern that is followed in all prior LLM optimization works. 

\textbf{Prefill and Decode Latency Measurement:} We use time to first token (TTFT) and time between tokens (TBT) to measure the prefill and decode latency, respectively. TTFT measures the time from when a prompt is submitted to the LLM until the first generated token is produced. It reflects the initial processing delay to infer the context of a given prompt by the LLM. TBT measures the latency of generating the $N^{th}$ token after the LLM has produced $N-1$ tokens post the prefill stage \cite{zhang2024llmcompass}. 
%\textcolor{blue}{MENTION what you do with MEADOW during prefill and decode. TPHS and GEMM and Weight packing. Its confusing because in intro you mentioned you do TPHS during prefill. No where in methodology have you talked about what is executed for prefill and what for decode}

\textbf{\ourwork Operation Modes:} During the prefill and decode stage, we execute the TPHS dataflow for the \smqktv layers and GEMM is used for the remaining \ttt{K}, \ttt{V}, \ttt{Proj} and \ttt{MLP} layers. Weight Packing is applied in both stages. Note, during Decode, there is a marginal latency speedup with TPHS compared to GEMM operation for \smqktv since the input token size is 1. As we will see later, the decode stage latency gains are primarily stemming from weight packing.
% mult per PE, bram memory, RF size, 
% parallel mac PEs, broadcasting pes, sm units, gelu units, layer norm units

% brams = 845, lut distribution for different modules

% baseline: GEMM implementations with softmax on-chip having same number of PEs. 
% The off-chip data fetch and storage happens using the AXI HP port that can support 16-512 bits per cycle. To analyze the performance, we vary the number of bits transferred per cycle in the AXI interconnect bus of the MPSoC. softmax, GeLU and layer normalization units are realized using DSP cores as they support high precision multiply and accumulate operations. For PE implementations, we use LUTs and the remaining DSP cores.  

\subsection{Prefill and Decode Latency Improvements}


\textbf{Prefill}: Fig. \ref{fig:results_gemm_ours}a and Fig. \ref{fig:results_gemm_ours}b compares the TTFT achieved by \ourwork and GEMM-based OPT-125M and OPT-1.3B LLM models for varying DRAM bandwidths. At DRAM bandwidth of 12 Gbps, \ourwork achieves 1.5$\times$-1.7$\times$ and 1.5-1.6$\times$ for OPT-125M and OPT-1.3B LLMs, respectively across different number of prefill tokens. At a low DRAM bandwidth of 1 Gbps, \ourwork achieves 1.57-2.5$\times$ and 1.55-2$\times$ lower TTFT compared to GEMM implementations for OPT-125M and OPT-1.3B LLM models, respectively. 

\textbf{Decode}: Fig. \ref{fig:decode_gemm_ours}a and Fig. \ref{fig:decode_gemm_ours}b compare the TBT achieved by \ourwork and GEMM-based approaches on the OPT-125M and OPT-1.3B LLM models, across varying DRAM bandwidths. For predicting the 64th and 512th token at 12 Gbps DRAM bandwidth, \ourwork reduces TBT by 1.4-1.46$\times$ and 1.4-1.52$\times$ for the OPT-125M and OPT-1.3B models, respectively. When operating at a constrained DRAM bandwidth of 1 Gbps, \ourwork achieves a 1.4$\times$-1.47$\times$ reduction in TBT for the OPT-125M model and a 1.5$\times$-1.53$\times$ reduction for the OPT-1.3B model.

% The TTFT reductions in \ourwork increase with decrease in DRAM bandwidth as less data is transferred between off-chip DRAM. 

% % As the DRAM bandwidth is lowered to 1Gbps, the TTFT reductions in \ourwork increase to The TTFT reductions increase to N$\times$ as the bandwdith is further lowered. 
% Fig. \ref{fig:decode_gemm_ours}a and Fig. \ref{fig:decode_gemm_ours}b shows the decode latency improvements achieved by \ourwork compared to GEMM-based implementations. For decode, \ourwork achieves N$\times$ and M$\times$ lower latency compared to GEMM-implementations. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/prefill_latency_distribution_2.pdf}
    \caption{Prefill latency distribution for data fetch, compute and storage with 512 tokens at (a) 12 Gbps and (b) 1 Gbps off-chip DRAM bandwidth. The latency distribution is shown for one decoder layer of the OPT-125M LLM.}\vspace{-5mm}
    \label{fig:prefill_decode_distributions}
\end{figure}

The latency reduction observed in \ourwork for both prefill and decode stages stems from the targeted optimizations in data fetch and storage cycles. In GEMM-based implementations, executing the \ttt{Q+SM(QK\tsup{T})xV} layers during the prefill stage requires fetching weights and intermediate values from off-chip DRAM, performing matrix multiplications, and storing outputs back to DRAM. These data transfers impose substantial latency, especially as the size of the intermediate outputs scales directly with the number of attention heads and prefill stage tokens. This latency is exacerbated when the DRAM bandwidth is constrained (as illustrated in Fig. \ref{fig:prefill_decode_distributions}a and Fig. \ref{fig:prefill_decode_distributions}b).% Evidently, at lower DRAM bandwidth, MEADOW yields higher speedup due to the integrated optimizations.
 \ourwork's TPHS dataflow with pipelined operations within the \ttt{Q+SM(QK\tsup{T})xV} layers minimizes the number of off-chip memory accesses yielding a significant reduction in latency. For the \ttt{KV+Proj} and \ttt{MLP} layers, where data fetches are dominated by weight matrix transfers, the introduction of weight packing further reduces the latency by decreasing the volume of weight data fetched from the off-chip DRAM. 


% GEMM implementations consume significant data fetch latency as along with weights, they also have to fetch intermediate inputs from the DRAM to the FPGA. Latency worsens at lower bandwidth. With pipelining, the data fetch and storage latency signficantly lowers leading to a large drop in overall latency. Further, applying indexing and bit-packing further lowers the prefill latency by N$\times$ due to weight data fetch reduction. 
% It must be noted that since, GEMM allocates all the PEs towards matrix multiplication for each layer, the compute time is lower. However, computing the whole layer at once requires off-chip storage of the data which leads to higher data storage latency. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/decode_latency_distribution_2.pdf}\vspace{-3mm}
    \caption{Decode latency distribution for data fetch, compute and storage at (a) 12 Gbps and (b) 1 Gbps off-chip DRAM bandwidth for one decoder layer of the OPT-125M LLM. The latency is shown for predicting the 64th token with 512 tokens at the prefill stage. The compute and store latencies are negligibly small compared to data fetch latency.}
    \label{fig:decode_distributions}
\end{figure}
During the decode stage, only a single token is processed at a time, significantly reducing input fetch and output storage demands compared to the prefill stage with its large pool of tokens. This limited data transfer, shown in Fig. \ref{fig:decode_distributions}a and Fig. \ref{fig:decode_distributions}b, makes weight data fetching the primary bottleneck. \ourwork is able to reach lower decode latency due to the weight packing strategy that reduces weight fetch latency.








% \subsection{\ourwork Overcomes Memory Bottleneck}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/RLM_plots.pdf}
%     \caption{Caption}
%     \label{fig:rlm plots}
% \end{figure}

% From Fig. \ref{fig:rlm plots}, it is evident that compared to GEMM implementations \ourwork overcomes the memory bound region of implementations. The operational intensity in \ourwork increases due to pipelining which reduces the input fetches, output data storage. Additionally, applying weight indexing further lowers the number of data fetches. This improves the operational intensity Q, QKT, SMV and MLP, by A\x, B\x, C\x and D\x, respectively. This overcomes the memory bound implementations. 

\subsection{Efficacy of the Weight Packing Strategy}
% Fig. \ref{fig:indexing_ablation}. 1) In indexing, the weight matrix is partitioned and encoded according to the unique chunks in the matrix. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/indexing_ablation.pdf}
    \caption{(a) Latency comparison of weight matrix transfer for 3 different weight packing optimizations. 1) indexing + naive data packing (Naive), 2) Indexing + packet specific encoding precision (Packet specific) and 3) frequency aware re-indexing + packet-specific encoding precision. (b) Histogram of the unique chunk IDs (shown for Chunk IDs between 200 and 1000) (c) histogram of the chunk IDs after performing frequency-aware re-indexing.}\vspace{-5mm}
    \label{fig:indexing_ablation}
\end{figure}
Indexing reduces a large weight matrix to unique chunk values and represents the weight matrix in terms of the unique chunk IDs. Fig. \ref{fig:indexing_ablation}a, analyses the latency improvements over different packing optimizations for the first MLP layer weights of decoder 1 of the OPT-125M LLM. The MLP1 weight is decomposed into 1272 unique chunks leading to a 11-bit encoded W precision. These 11-bit encoded W values are now grouped together into packets to improve the DRAM bandwidth efficiency.

% For a realistic DRAM data transfer, a 13-bit data type is not typically supported as the bus width is optimized for standard data widths (e.g., 16, 32, 64 bits). Therefore, 13-bit encoded W values are upscaled to the nearest supported width, represented in this case as 16-bit values. These 16-bit encoded W values are now grouped together into packets to improve the DRAM bandwidth efficiency. 

As seen in Fig. \ref{fig:indexing_ablation}a, with naive packing, a latency improvement of merely 1.4$\times$ is achieved as several low-bit precision encoded W values are represented with 11-bit values. Upon using packet specific encoding precision, a 1.54$\times$ lower latency is observed as multiple low bit encoded W values are grouped per packet which reduces the number of data fetch cycles. 

The limited improvements in memory fetch latency with naive and packet-specific grouping arises due to the frequent occurrence of high-value chunk IDs, which hinders effective grouping of the encoded W values, as illustrated in Fig. \ref{fig:indexing_ablation}b. To this end, frequency-aware reindexing increases the number of low bit chunk IDs (Fig. \ref{fig:indexing_ablation}c) and thereby improves the packing efficiency leading to 2.63$\times$ lower weight fetch latency. 

% to eading to merely 1.4x lower weight fetch latency. Additionally, many smaller bit values are encoded with 1in Fig. \ref{} this leads to 16-bit encoded values. Upon packet specific encoding precision, the However, alone indexing leads to 1.4x latency reduction as some even the low bit-precision encoded W values are packed with high precision


\subsection{Comparison with Prior Works}
\begin{table}[h!]
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{|c|c|c|c|}\hline 
         & CTA & FlightLLM  & \textbf{\ourwork}\\ 
         & \cite{wang2023cta} & \cite{zeng2024flightllm} & \textbf{(Ours)} \\ \hline
       KV, Proj, MLP  & GEMM & GEMM & \textbf{GEMM} \\ \hline
       Q, SM(QKT)V & GEMM & GEMM & \textbf{TPHS} \\ \hline
       Quantization & W8A8 & W8A8 & \textbf{W8A8} \\ \hline
       Weight Packing & \ding{55} & \ding{55} & \ding{52} \\ \hline
    \end{tabular}}
    \caption{Evaluation settings for prior work comparison. }
    \label{tab:prior_work}
\end{table}


We implement prior state-of-the-art LLM optimization approaches- CTA~\cite{wang2023cta} and FlightLLM \cite{zeng2024flightllm} on the \ourwork architecture with implementation parameters shown in 
Table \ref{tab:parameter_table}. As seen in Table \ref{tab:prior_work}, CTA \cite{wang2023cta} and FlightLLM \cite{zeng2024flightllm} execute all layers in the decoder in the GEMM mode. For fairness, the activations and weights in all works are maintained at 8-bit precision. \ourwork is implemented with weight packing and the TPHS dataflow for the \smqktv layers for both prefill and decode stages. The \ttt{K}, \ttt{V}, \ttt{Proj} and \ttt{MLP} layers are executed in the GEMM mode. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/prior_work_comp_2.pdf}\vspace{-5mm}
    \caption{Figure comparing the (a) TTFT and (b) TBT latency of prior state-of-the-art LLM optimization works with \ourwork at different off-chip DRAM bandwidths.}%\vspace{-3mm}
    \label{fig:prior_work_comp}
\end{figure}
Fig. \ref{fig:prior_work_comp}a and Fig. \ref{fig:prior_work_comp}b compares the TTFT and TBT latency of prior works with \ourwork. CTA \cite{wang2023cta} employs token compression to mitigate data redundancy, aiming to reduce memory and computational load by processing essential tokens only. While this approach decreases compute cycles, output storage, and input fetch latency in the \smqktv layers, the intermediate values for the remaining significant tokens still require fetching and storage in off-chip DRAM. Under constrained memory bandwidth, the latency involved in weight and token fetch/storage creates a substantial bottleneck, which limits CTA’s overall benefits during both prefill and decode stages.

FlightLLM \cite{zeng2024flightllm}, on the other hand, leverages unstructured N:M sparse acceleration architecture to cut down computations. While unstructured sparsity can lower compute requirements, it leaves input fetch latency largely unoptimized, and like CTA, FlightLLM does not apply any weight packing technique. To mitigate intermediate storage requirements during \smqktv operations, FlightLLM utilizes on-chip storage at decode time. However, since output storage latency during decode is negligible, as illustrated in Fig. \ref{fig:prior_work_comp}b, weights remain the dominant bottleneck, restricting overall performance gains.

% As prior approaches  limited effectiveness under low memory bandwidth conditions, highlighting the need for an optimized dataflow and weight compression for achieving latency benefits during both prefill and decode stages at constrained memory bandwidths. Addressing these gaps, \ourwork leverages a TPHS dataflow and a weight-packing strategy, enabling substantial reductions in token fetch and throughput latency across a range of DRAM bandwidth constraints.

Evidently, prior methods perform prefill and decode with unoptimized weight matrix sizes and only partially eliminate intermediate data fetch and storage cycles during the \smqktv operations. This partial approach limits their effectiveness, particularly under low-memory bandwidth constraints, where repeated fetches of intermediate values and weights cause latency bottlenecks. \ourwork offers architectural support and the TPHS dataflow innovation to completely eliminate the data fetch and storage latency of the \smqktv layers. Additionally, weight packing further reduces the latency of fetching the weight matrix. Overall, this translates to a 40\% improvement in the end-to-end latency with \ourwork compared to FlightLLM and CTA on ZCU102 FPGA-based OPT-125M implementation. 


% As prior techniques only partially alleviate the input data fetch and storage problems


\subsection{Choosing between GEMM \& TPHS Dataflow}
From Fig. \ref{fig:gemm_pipeline_suitability}a, it is observed that the choice of GEMM and TPHS dataflow for the \smqktv layers is dependent on the number of PEs and the off-chip DRAM bandwidth. For high memory bandwidth scenarios, (BW:51, PE:14) and (BW:51, PE:96) GEMM is the dataflow choice. In contrast, TPHS is suitable for low memory bandwidth configurations (Fig. \ref{fig:gemm_pipeline_suitability}b). {This study justifies our framework as a suitable choice for deployment on a range of low memory capability edge devices.}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/gemm_tphs_choice.pdf}\vspace{-6mm}
    \caption{(a) Table showing optimal dataflow chosen for executing the \smqktv layers and the corresponding optimal prefill latency obtained for the OPT-125M LLM model. (b) The roofline plots for different (Bandwidth (BW), PE) configurations (1,14), (1,96), (51,14) and (51,96).}
    \label{fig:gemm_pipeline_suitability}
\end{figure}
% optimal  shows the dataflow design choices that \ourwork chooses for computing the Q+SM(QKT)V layers at different number of PEs and off-chip DRAM and MLP layers based on the transformer architecture and FPGA bandwidth and the number of PEs available. Fig. \ref{fig:gemm_pipeline_dist}a shows the latency distribution of data fetch, computation and output storage for GEMM and pipeline dataflow for the attention module for 4 corner cases, A= (BW: 1, PE: 30), B= (BW: 1, PE: 96), C= (BW: 12, PE: 30), D= (BW: 12, PE: 96). As seen in Fig. \ref{fig:gemm_pipeline_dist}a, case A and B, the data fetch and output storage latencies dominate in GEMM implementations leading to higher latency compared to pipelined execution mode. For case D, the output storage leads to higher latency in GEMM implementations which is absent in TPHS dataflow. For case C when the bandwidth, the comptuation becomes the bottleneck, where \ourwork-Tool chooses GEMM execution of attention layers to attain lower latency. \textbf{have a better name for pipelined mode. Token parallel HS dataflow.}. When the number of PEs reduce, the token parallelism reduces and therefore leads to higher latency. 

% pipelining attention layers leads to lower latencies compared to the GEMM dataflow at low memory bandwidths irrespective of the number of PEs. This is because the 

% Unlike prior works that follow a homogeneous GEMM dataflow agnostic of the transformer architecture and FPGA resources, we interestingly found that the GEMM and pipelined dataflow are suitable depending upon the transformer architecture and FPGA resources. 
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/gemm_pipeline_distributions.pdf}
%     \caption{Caption}
%     \label{fig:gemm_pipeline_dist}
% \end{figure}

\subsection{ViT Latency Improvements with \ourwork}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/vit_latency.pdf}\vspace{-3mm}
    \caption{DeiT-S and DeiT-B ViT inference latency improvements with \ourwork compared to GEMM-based implementations on the ZCU102 FPGA. }\vspace{-2mm}
    \label{fig:enter-label}
\end{figure}

We also show the generality of \ourwork for ViT models. Vision transformers (ViTs) process multiple tokens together like the prefill stage of an LLM. With combined TPHS/GEMM dataflow and weight packing, \ourwork achieves 1.5-1.6$\times$ lower inference latency on the DeiT-S and DeiT-B \cite{touvron2021training} models trained on the ImageNet dataset \cite{deng2009imagenet} across different off-chip DRAM bandwidths. 

\section{Conclusion}
This work proposes \ourwork- targeting the latency intensive data fetch/store cycles of intermediate outputs and weights through the TPHS dataflow and Weight Packing to achieve 1.5$\times$ and 2.5$\times$ lower decode and prefill latency compared to GEMM-based implementations. \ourwork is crafted to achieve low latency LLM execution at highly constrained off-chip DRAM bandwidths achieving over 40\% end-to-end latency improvement compared to prior LLM optimization works. Additionally, we demonstrate the versatility of \ourwork by applying it towards ViT implementations. This typically makes \ourwork suitable for low power edge applications such as autonomous driving and mobile chatbots for both vision and NLP tasks.

\section{Acknowledgment}
This work was supported in part by CoCoSys, a JUMP2.0 center sponsored by DARPA and SRC, the National Science Foundation (CAREER Award, Grant \#2312366, Grant \#2318152), the DARPA Young Faculty Award and the DoE MMICC center SEA-CROGS (Award \#DE-SC0023198).

% the intermediate data fetch/store cycles during the \smqktv layers in the prefill and decode stage. An additional optimization of Weight Packing further optimizes the overhead of fetching large weight matrices during prefill and decode stages.

% low memory bandwidth scenarios the memory bottleneck of LLM execution in the prefill and decode stages under highly constrained off-chip DRAM bandwidths. is crafted for LLM execution on hardware platforms with low off-chip DRAM bandwidth memory.  

\bibliography{example_paper}
\bibliographystyle{mlsys2025}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
