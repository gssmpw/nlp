\section{Related Works and Motivation}
% \textbf{Quantization}: Does not leverage the redundancy in the weight patterns

% \subsection{Problems with GEMM-based Accelerators}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/GEMM_Architecture.pdf}
%     \caption{Only show for T = 256 and T = 512 and T=1024}. Remove 4
%     \label{fig:gemm_arch}
% \end{figure}
% Fig. \ref{fig:gemm_arch}a illustrates the computation flow in GEMM-based accelerators, which follows a standard sequence: 1) Input and weight data are fetched from memory, 2) MAC (Multiply-Accumulate) operations are performed, 3) Outputs are stored back to DRAM, and 4) These outputs are later fetched as inputs for subsequent layers.

% For transformers, as shown in Fig. \ref{fig:gemm_arch}b, GEMM-based implementations exhibit a notable distribution of time spent on weight/input fetches, computation, and output storage. A key challenge arises during attention computation, where significant output storage bottlenecks occur. This is particularly problematic in the QKT and SMV layers, where the Q, K, and V matrices are fetched from off-chip DRAM. Since computations are performed head-by-head, the TxT outputs for each head must be stored off-chip, incurring considerable overhead, especially as T increases.

% Although some works \cite{flightllm, etc} mitigate this by utilizing on-chip BRAM to store QKT and SM outputs, this approach is unfeasible for smaller FPGAs due to insufficient BRAM resources as seen in Fig. \ref{fig:gemm_arch}c. This limitation underscores the need for a more efficient dataflow that avoids storing intermediate values in external memory. To address this, we propose token-level pipelining, which processes data immediately as it is generated, eliminating the need for excessive off-chip storage.