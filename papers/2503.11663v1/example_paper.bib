@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@article{moitra2024pivot,
  title={PIVOT-Input-aware Path Selection for Energy-efficient ViT Inference},
  author={Moitra, Abhishek and Bhattacharjee, Abhiroop and Panda, Priyadarshini},
  journal={arXiv preprint arXiv:2404.15185},
  year={2024}
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2018}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@inproceedings{wang2023cta,
  title={Cta: Hardware-software co-design for compressed token attention mechanism},
  author={Wang, Haoran and Xu, Haobo and Wang, Ying and Han, Yinhe},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={429--441},
  year={2023},
  organization={IEEE}
}


@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}


@inproceedings{park2024lpddr,
  title={An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models},
  author={Park, Sang-Soo and Kim, KyungSoo and So, Jinin and Jung, Jin and Lee, Jonggeon and Woo, Kyoungwan and Kim, Nayeon and Lee, Younghyun and Kim, Hyungyo and Kwon, Yongsuk and others},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={970--982},
  year={2024},
  organization={IEEE}
}

@inproceedings{tambe202322,
  title={22.9 A 12nm 18.1 TFLOPs/W sparse transformer processor with entropy-based early exit, mixed-precision predication and fine-grained power management},
  author={Tambe, Thierry and Zhang, Jeff and Hooper, Coleman and Jia, Tianyu and Whatmough, Paul N and Zuckerman, Joseph and Dos Santos, Maico Cassel and Loscalzo, Erik Jens and Giri, Davide and Shepard, Kenneth and others},
  booktitle={2023 IEEE International Solid-State Circuits Conference (ISSCC)},
  pages={342--344},
  year={2023},
  organization={IEEE}
}

@article{minaee2024large,
  title={Large language models: A survey},
  author={Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.06196},
  year={2024}
}

@inproceedings{zeng2024flightllm,
  title={Flightllm: Efficient large language model inference with a complete mapping flow on fpgas},
  author={Zeng, Shulin and Liu, Jun and Dai, Guohao and Yang, Xinhao and Fu, Tianyu and Wang, Hongyi and Ma, Wenheng and Sun, Hanbo and Li, Shiyao and Huang, Zixiao and others},
  booktitle={Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
  pages={223--234},
  year={2024}
}

@article{marcu2023lingoqa,
  title={LingoQA: Visual Question Answering for Autonomous Driving}, 
  author={Ana-Maria Marcu and Long Chen and Jan Hünermann and Alice Karnsund and Benoit Hanotte and Prajwal Chidananda and Saurabh Nair and Vijay Badrinarayanan and Alex Kendall and Jamie Shotton and Oleg Sinavski},
  journal={arXiv preprint arXiv:2312.14115},
  year={2023},
}

@misc{mobileAIBench,
      title={MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases}, 
      author={Rithesh Murthy and Liangwei Yang and Juntao Tan and Tulika Manoj Awalgaonkar and Yilun Zhou and Shelby Heinecke and Sachin Desai and Jason Wu and Ran Xu and Sarah Tan and Jianguo Zhang and Zhiwei Liu and Shirley Kokane and Zuxin Liu and Ming Zhu and Huan Wang and Caiming Xiong and Silvio Savarese},
      year={2024},
      eprint={2406.10290},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10290}, 
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@inproceedings{qin2023fact,
  title={Fact: Ffn-attention co-optimized transformer architecture with eager correlation prediction},
  author={Qin, Yubin and Wang, Yang and Deng, Dazheng and Zhao, Zhiren and Yang, Xiaolong and Liu, Leibo and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--14},
  year={2023}
}

@article{zhao2024alisa,
  title={ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching},
  author={Zhao, Youpeng and Wu, Di and Wang, Jun},
  journal={arXiv preprint arXiv:2403.17312},
  year={2024}
}

@inproceedings{qin2024mecla,
  title={MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition},
  author={Qin, Yubin and Wang, Yang and Zhao, Zhiren and Yang, Xiaolong and Zhou, Yang and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={1032--1047},
  year={2024},
  organization={IEEE}
}

@article{wang2019structured,
  title={Structured pruning of large language models},
  author={Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao},
  journal={arXiv preprint arXiv:1910.04732},
  year={2019}
}

@misc{alveo,
  title= {{AMD Alveo™ Adaptable Accelerator Cards}},
url={{https://www.amd.com/en/products/accelerators/alveo.html}}

}

@misc{zcu102,
  title= {{Zynq UltraScale+ MPSoC ZCU102 Evaluation Kit}},
url={{https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html}}

}

@misc{zcu104,
  title= {{Zynq UltraScale+ MPSoC ZCU104 Evaluation Kit}},
url={{https://www.xilinx.com/products/boards-and-kits/zcu104.html}}

}

@article{lin2024awq,
  title={{AWQ}: Activation-aware Weight Quantization for On-Device {LLM} Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@article{xu2024llamaf,
  title={{Llamaf: An efficient LLAMA2 architecture accelerator on embedded FPGAs}},
  author={Xu, Han and Li, Yutong and Ji, Shihao},
  journal={arXiv preprint arXiv:2409.11424},
  year={2024}
}

@inproceedings{huang2024elsa,
  title={{ELSA: Exploiting Layer-wise N: M Sparsity for Vision Transformer Acceleration}},
  author={Huang, Ning-Chi and Chang, Chi-Chih and Lin, Wei-Cheng and Taka, Endri and Marculescu, Diana and Wu, Kai-Chiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8006--8015},
  year={2024}
}

@inproceedings{zhang2024llmcompass,
  title={{LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference}},
  author={Zhang, Hengrui and Ning, August and Prabhakar, Rohan Baskar and Wentzlaff, David},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={1080--1096},
  year={2024},
  organization={IEEE}
}

@article{fang2022algorithm,
  title={An algorithm--hardware co-optimized framework for accelerating n: M sparse transformers},
  author={Fang, Chao and Zhou, Aojun and Wang, Zhongfeng},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={30},
  number={11},
  pages={1573--1586},
  year={2022},
  publisher={IEEE}
}

@article{miao2024flexllm,
  title={FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning},
  author={Miao, Xupeng and Oliaro, Gabriele and Cheng, Xinhao and Wu, Mengdi and Unger, Colin and Jia, Zhihao},
  journal={arXiv preprint arXiv:2402.18789},
  year={2024}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{moitra2024trex,
  title={TReX-Reusing Vision Transformer's Attention for Efficient Xbar-Based Computing},
  author={Moitra, Abhishek and Bhattacharjee, Abhiroop and Kim, Youngeun and Panda, Priyadarshini},
  journal={arXiv preprint arXiv:2408.12742},
  year={2024}
}

@inproceedings{chen2023dynamic,
  title={Dynamic n: M fine-grained structured sparse attention mechanism},
  author={Chen, Zhaodong and Qu, Zheng and Quan, Yuying and Liu, Liu and Ding, Yufei and Xie, Yuan},
  booktitle={Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
  pages={369--379},
  year={2023}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{herrera2024flex,
  title={FLEX: FLEXible Federated Learning Framework},
  author={Herrera, Francisco and Jim{\'e}nez-L{\'o}pez, Daniel and Argente-Garrido, Alberto and Rodr{\'\i}guez-Barroso, Nuria and Zuheros, Cristina and Aguilera-Martos, Ignacio and Bello, Beatriz and Garc{\'\i}a-M{\'a}rquez, Mario and Luz{\'o}n, M},
  journal={arXiv preprint arXiv:2404.06127},
  year={2024}
}

@inproceedings{tambe2021edgebert,
  title={Edgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference},
  author={Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M and Brooks, David and others},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={830--844},
  year={2021}
}