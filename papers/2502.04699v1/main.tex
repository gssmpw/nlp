%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[bb=dsserif]{mathalpha}

\usepackage{color-edits}
\addauthor{vs}{red}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Meta-learner for Heterogeneous Effects in Difference-in-Differences}



\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\E}{\mathbb{E}}
%\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argsup}{arg\,sup}
%\DeclareMathOperator*{\arginf}{arg\,inf}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\st}{\sum{t=1}^{T}}
%\newcommand{\eps}{\epsilon}
\newcommand{\G}{\mathcal{G}^\eps}
\newcommand{\kk}{\textbf{k}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\D}{\mathcal D}
\newcommand{\mbf}{\mathbf}
\newcommand{\p}{\mathbf{p}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\g}{\textbf{g}}
\newcommand{\softmax}{\textbf{softmax}}
\newcommand{\w}{\mbf{w}}
\newcommand{\winexp}{\textsc{WIN-EXP}}
\newcommand{\winexpG}{\textsc{WIN-EXP-G}}
\renewcommand{\Pr}{\ensuremath{\mathrm{Pr}}}
\newcommand{\opt}{\ensuremath{\textsc{OPT}}}
\newcommand{\ci}{\perp \!\!\! \perp}
\DeclareMathOperator*{\argmin}{arg\,min}
%% HERE arXiv


\newcommand{\tto}{,\ldots,}
\newcommand{\eg}{e.g., \xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\etc}{etc.\@\xspace}
\newcommand{\mcR}{{\mathcal R}}
\newcommand{\mcD}{{\mathcal D}}
\newcommand{\mcA}{{\mathcal A}}
\newcommand{\mcH}{{\mathcal H}}
\newcommand{\Hnorm}[1]{\norm{#1}_{\mcH}}
\newcommand{\mcM}{{\mathcal M}}
\newcommand{\mcN}{{\mathcal N}}
\newcommand{\mcP}{{\mathcal P}}
\newcommand{\mbP}{{\mathbb P}}
\newcommand{\mbG}{{\mathbb G}}
\newcommand{\mcQ}{{\mathcal Q}}
\newcommand{\mcX}{{\mathcal X}}
\newcommand{\iid}{\emph{i.i.d.}\ }
\newcommand{\mcB}{{\mathcal B}}
\newcommand{\mcG}{{\mathcal G}}
\newcommand{\mcF}{{\mathcal F}}
\newcommand{\Fnorm}[1]{\norm{#1}_{\mcF}}
\newcommand{\mcT}{{\mathcal T}}
\newcommand{\mcY}{{\mathcal Y}}
\newcommand{\mcV}{{\mathcal V}}
\newcommand{\mcW}{{\mathcal W}}
\newcommand{\etal}{et.\ al.\ }
\newcommand{\natur}{{\mathbb N}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bs}{\begin{align}\begin{split}\nonumber}
\newcommand{\bsnumber}{\begin{align}\begin{split}}
\newcommand{\es}{\end{split}\end{align}}
%\newcommand{\singlespace}{\linespread{1}\small\normalsize}
%\newcommand{\doublespace}{\linespread{1.2}\small\normalsize}
\newcommand{\fns}{\singlespace\small}
\newcommand{\expect}{\mathcal{E}}
\newcommand{\cooldate}{\begin{flushright}\footnotesize\today\normalsize\end{flushright}}
\renewcommand{\(}{\left(}
\newcommand{\real}{{\mathbb R}}
\newcommand{\mcZ}{{\mathcal Z}}
\newcommand{\mcL}{{\mathcal L}}
\newcommand{\M}{{\bf M}}
\newcommand{\bps}{{\bf \psi}}
\newcommand{\brho}{{\bf \rho}}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}
\newcommand{\x}{\mathbf{x}}
\newcommand{\xstar}{\mathbf{x}^*}
\newcommand{\bff}{\mathbf}
\newcommand{\aaa}{{\bf a}}

\newcommand{\spanF}{\ensuremath{\text{span}}}

\newcommand{\breg}{{\mcD}}
\newcommand{\Lp}{L_{1,p}^{\epsilon_n}}

\newcommand{\ldot}[2]{\langle #1, #2 \rangle}

\newcommand{\Var}{\ensuremath{\text{Var}}}
\newcommand{\MSE}{\ensuremath{\text{MSE}}}
\newcommand{\sign}{\ensuremath{\mathtt{sign}}}

\newcommand{\textfrac}[2]{{\textstyle\frac{#1}{#2}}}
\def\half{\frac{1}{2}}
\def\texthalf{{\textstyle\frac{1}{2}}}
%\newcommand{\norm}[1]{\|{#1}\|} %
\newcommand{\Knorm}[1]{\norm{#1}_K} %
\newcommand{\KH}{K_\mcH}
\newcommand{\KHn}{K_{\mcH,n}}
\newcommand{\KF}{K_{\mcF}}
\newcommand{\KFn}{K_{\mcH,n}}
\newcommand{\KHnorm}[1]{\norm{#1}_{\KH}} %
\newcommand{\KFnorm}[1]{\norm{#1}_{\KF}} %
\newcommand{\onenorm}[1]{\norm{#1}_1} %
\newcommand{\twonorm}[1]{\norm{#1}_2} %
\newcommand{\infnorm}[1]{\norm{#1}_{\infty}} %
\newcommand{\opnorm}[1]{\norm{#1}_{op}} %
\newcommand{\fronorm}[1]{\norm{#1}_{F}} %
\newcommand{\inner}[2]{\langle{#1},{#2}\rangle} %
\newcommand{\KHinner}[2]{\inner{#1}{#2}_{\KH}} %
\newcommand{\KFinner}[2]{\inner{#1}{#2}_{\KF}} %
\newcommand{\binner}[2]{\left\langle{#1},{#2}\right\rangle} %
\def\supp#1{\mathrm{supp}({#1})}
\def\defeq{\triangleq} %
\newcommand{\qtext}[1]{\quad\text{#1}\quad} 
\def\balign#1\ealign{\begin{align}#1\end{align}}
\def\balignat#1\ealign{\begin{alignat}#1\end{alignat}}
\def\bitemize#1\eitemize{\begin{itemize}#1\end{itemize}}
\def\benumerate#1\eenumerate{\begin{enumerate}#1\end{enumerate}}
\newenvironment{talign}
 {\let\displaystyle\textstyle\csname align\endcsname}
 {\endalign}
\def\balignt#1\ealignt{\begin{talign}#1\end{talign}}%
\def\Cov{\mrm{Cov}} %
\newcommand{\cone}{\mathbb{C}}
\newcommand{\deltarow}[1]{g^{(#1)}} % j-th row of the matrix difference En[zx^t] - E[zx^t]
\DeclareMathOperator{\trace}{tr} %

\newcommand{\Rad}{\ensuremath{\mathcal{R}}}

\newcommand{\conv}{\ensuremath{\mathtt{conv}}}
\newcommand{\splin}{\ensuremath{\text{splin}}}
\newcommand{\nnet}{\ensuremath{\text{nnet}}}
\newcommand{\rkhs}{\ensuremath{\text{rkhs}}}

\newcommand{\noise}{\ensuremath{\text{noise}}}
\newcommand{\adj}{\ensuremath{\text{adj}}}
\newcommand{\new}{\ensuremath{\text{new}}}
\newcommand{\res}{\ensuremath{\text{res}}}
\newcommand{\agg}{\ensuremath{\text{agg}}}
\newcommand{\mcS}{\ensuremath{{\cal S}}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\fdim}{d}
\newcommand{\mean}{\ensuremath{\text{mean}}}

\newcommand{\Z}{\ensuremath{{\cal Z}}}

\begin{document}

\twocolumn[
\icmltitle{A Meta-learner for Heterogeneous Effects in Difference-in-Differences}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{icme}{*}

\begin{icmlauthorlist}
\icmlauthor{Hui Lan}{icme}
\icmlauthor{Haoge Chang}{columbia}
\icmlauthor{Eleanor Dillon}{msr}
\icmlauthor{Vasilis Syrgkanis}{mse}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{intern}{Part of this work is done during an internship at Microsoft Research.}
\icmlaffiliation{icme}{Institute of Computational and Mathematical Engineering, Stanford University, Stanford, USA}
\icmlaffiliation{mse}{Department of Management Science and Engineering, Stanford University, Stanford, USA}
\icmlaffiliation{msr}{Microsoft Research, New England}
\icmlaffiliation{columbia}{Department of Economics, Columbia University}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Hui Lan}{huilan@stanford.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{*Part of this work is done during an internship at Microsoft Research. **Vasilis Syrgkanis is Supported by NSF Award IIS-2337916.}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines.

\end{abstract}
\section{Introduction}
Difference-in-Differences estimators have become a foundational tool for causal inference in economics \cite{roth2023s}, social sciences \cite{reanalysis} and healthcare \cite{wang2024advances} for evaluating causal effects of policy interventions or treatments when both pre- and post-treatment outcomes are observed. In contrast to cross-sectional data, having panel data enables researchers to work with different assumptions that are often considered more plausible in application. Due to the non-random assignment of treatments, estimating the causal effect of a treatment or intervention in observational studies often requires strong assumptions, such as conditional exogeneity, which rules out unobserved confounding. Panel data consist of repeated observations of the same units over time, which allows researchers to control for certain types of unobserved, time-invariant characteristics. Due to its flexibility and robustness in handling non-experimental data, the DiD approach has gained significant traction in empirical research, especially in the evaluation of policy interventions \citep[e.g.][etc.]{thome2024understanding}, labor market changes \citep[e.g.][etc.]{min_wage, rossin2013effects, pierce2016surprisingly}, environmental regulations \citep[e.g.][etc.]{gao2020evaluation}, and public health \citep[e.g.][etc.]{finkelstein2012oregon, dimick2014methods}.

Despite several recent methodological advances in the DiD literature \cite{roth2023s, reanalysis}, most state-of-the-art approaches are still only able to generate average causal effects, or at best group average causal effects for predefined subpopulations. On the contrary, in many empirical applications, especially on large-scale datasets that stem from digital platforms, practitioners are interested in treatment effect heterogeneity for personalized decision making. The estimation of heterogeneous treatment effects has gained considerable attention in recent years due to its potential to uncover variation in how different subpopulations respond to an intervention. Motivated by the success of machine learning techniques in learning complex tasks, many studies have employed them in learning heterogeneous treatment effects, see for instance \citealp{shalit2017estimating, shi2019adapting,x-learner,R-learner, oprescu2019orthogonal,DR-learner}, etc. However, estimating heterogeneous treatment effects for panel data remains relatively unexplored in literature. 

In this paper, we explore the estimation of heterogeneous treatment effects of a binary treatment using panel data under the canonical parallel trends condition used in DiD setups \citep[e.g.][etc.]{job_training,min_wage}. The parallel trends assumption posits that, in the absence of treatment, the treated and control units would have followed similar trends over time. Recent research has explored different approaches in addressing limitations of traditional methods \citep[e.g.][]{roth2023s}. One line of work focuses on relaxing the unconditional parallel trends assumption by taking into account systematic differences in the time trends due to other (observed) characteristics through the conditional parallel trends condition \citep[e.g.][etc.]{heckman1997matching, DR-DID}. Another line of research tackles the challenges of estimating average treatment effects under treatment effect heterogeneity over time for multi-period settings \citep[e.g.][etc.]{IW-estimator, CSDID}. This paper synthesizes the insights from these two lines of works, and extends the framework to incorporate heterogeneous treatment effects across any dimension, in a flexible manner.

We propose a doubly robust estimation framework for the conditional average treatment effect on the treated (CATT), and show that the mean squared error (MSE) of the learned model is robust to the estimation error of auxiliary models that need to be estimated. While there are doubly robust estimators proposed for unconditional ATT with panel data \citep[e.g.][]{DR-DID, CSDID}, there does not exist one for the heterogeneous effect. In contrast to the conditonal average treatment effect (CATE), the asymmetry of the CATT allows our proposed method to avoid estimating a conditional outcome model under treatment, which can be hard to learn given a unbalanced dataset with a small number of treated units. We also draw the connection to the literature on debiasing under covariate shift \cite{cov_shift}, and provide an extension of our main result to a unifying framework for general conditional functionals, encompassing many widely encountered empirical problems such as conditional prediction powered inference under co-variate shift, heterogeneous long-term effects via surrogates based on historical data and heterogeneous treatment effects tailored to target sub-populations. Moreover, we extend our main result to the case of a binary instrument (or exposure to treatment) with two-sided non-compliance, and provide a doubly robust estimator for the conditional local average treatment effect of the exposed. 


Similar to \citealp{DR-IV-Param}, \citealp{semenova2021debiased} and \citealp{oprescu2019orthogonal}, we consider a framework that allows for the conditional parallel trends assumption to condition on a high dimensional set of observed covariates, denoted as $W$. This conditioning strengthens the plausibility of the assumptions and improves the robustness of the resulting estimators. Our focus is on the estimation of the average treatment effect on the treated (ATT) while conditioning on any subset, $X$, of the covariates $W$. Estimating the projection of heterogeneous treatment effects onto a subset of covariates is particularly advantageous for interpretation, when the goal is to uncover heterogeneity with respect to a set of key features that are of most interest. For instance, in medical applications, we might have high-dimensional imaging data that can be used to predict the outcome, while we are only interested in understanding how the treatment effect is modified by other features such as age, bone density, etc. Furthermore, this framework can be helpful for decision making when trying to leverage the findings to deploy a personalized policy on a larger population for which only a subset of covariates is available. 


We demonstrate using synthetic and semi-synthetic experiments that the proposed meta-learner outperforms prior baselines. Finally, we applied our method on a real-world case study on the effects of raising minimum wage on teen employment. Our flexible doubly robust meta-learner automatically identified dimensions and patterns of heterogeneity that had not been highlighted in prior literature. In particular, our method uncovered that the county population plays a significant role on the magnitude of the treatment effect of raising the minimum wage on teen employment and even though this effect can be quite large and negative for small counties, it becomes negligible and close to zero on large counties. We developed an out-of-sample validation pipeline and showcased that the patterns of heterogeneity identified by our methodology are statistically significant. 


% The contributions in this paper are summarized below:
% \begin{itemize}
%     \item Proposed a novel, doubly robust meta-learner for heterogeneous CATT with panel data, and also extended to the setup with binary instruments.  
%     \item Proposed a unifying framework for the debiasing of general conditional functionals under covariate shift.
%     \item Demonstrated using experiments that the proposed meta-learner outperforms other baselines, and performed a case study on the effects of raising minimum wage on teen employment. 
% \end{itemize}

\section{Problem Statement}
We consider the standard setup in the DiD framework. We observe a balanced panel with $n$ units and $T$ periods. We denote time by $t=0,...,T-1$.  The units are assumed to be an i.i.d sample from a superpopulation. For each unit $i$, we observe a time series of outcomes $\{Y_{it}\}_{t=1}^T$, a time series of binary treatment status $\{D_{it}\}_{t=1}^T$, and time-invariant covariates $W_i$. For simplicity, we restrict our discussion to $T=2$ periods in this section and Section \ref{sec:catt}. We discuss extensions to the multi time period setting in Section \ref{sec:multi-period}. 

We adopt the potential outcomes framework and assume for unit $i$ at time $t$, the outcome is generated as:$$Y_{i,t} = D_{i,t} Y_{i,t}(1) + (1-D_{i,t}) Y_{i,t}(0)$$ where $Y_{i,t}(d)$ denotes the potential outcome at time $t$ under treatment $d$. For brevity of notation, we may drop the unit subscript $i$. We assume that both the treated and untreated groups are untreated at $t=0$, and the treated group becomes treated at $t=1$, while the control group remains untreated. Our target estimand is the conditional average treatment effect on the treated (CATT), conditioning on any subset $X$ of the covariates $W$:
\begin{align*}
\theta_0(X) = \E[Y_1(1) - Y_1(0)|D=1, X].
\end{align*}


\subsection{Assumptions and Identification} \label{sec:assum_iden}

Panel data allows us to disentangle unobserved confounding to some degree by leveraging both cross-sectional and time-series variations. In this section, we focus on the conditional parallel trends assumption that is commonly employed in the empirical literature to identify treatment effects for panel data. This assumption posits that the untreated outcome will evolve in parallel for both the treated and untreated group, for units with the same observed characteristics $W$.
\begin{assumption}[Conditional Parallel Trends]\label{assum:cpta}
    \begin{align*}
    ~&\E[Y_{1}(0) - Y_{0}(0) |D_{1}=1, W] \\
    ~&= \E[Y_{1}(0) - Y_{0}(0) |D_{1}=0, W] 
    \end{align*}
\end{assumption}
Conditioning on covariates makes the assumption more plausible, as it allows the treatment assignment to depend on any baseline trends that are predictable from the observed covariates. A practical motivation comes from the abundance of pre-treatment outcome data (for time periods before $t=0$). It could be reasonable to condition on the full outcome history to try to account for cases where the magnitude of the growth (or decline) through time might depend on the base outcome level. For instance, employees with a higher salary usually receive higher pay raises through time. 
\begin{assumption}[No-anticipation Assumption]\label{assum:no-an}
    $$ \E[Y_{0}(0) - Y_{0}(1) | D_{1}=1, W]=0$$
\end{assumption}
In practical applications, Assumption \ref{assum:cpta} is imposed with the full set of covariates $W$ for plausibility, as we expect more covariates to able to capture more confounding. However, we might only be interested in the heterogeneity of the treatment effect in a smaller and interpretable subset of the covariates $X \subset W$.
\begin{proposition}\label{prop:did}
    Under Assumptions \ref{assum:cpta} and \ref{assum:no-an}, the CATT, $\theta_0(X)$, can be identified as:
    \begin{align*}
        \theta_0(X)
        =~& \E[Y_1 - Y_0 - g_0(X) | D=1, X],
    \end{align*}
    where $g_0(x) := \E[Y_1(0) - Y_0(0)|D=0,W=w]$.
\end{proposition}

\section{DR-Learner for CATT}\label{sec:catt}
In the special case when $W=X$, the statistical problem that results from Proposition~\ref{prop:did} is identical to the estimation of the conditional average treatment effect under conditional ignorability with outcomes $Y_1 - Y_0$ (even though, the resulting statistical model can only be interpreted as a CATT, due to the one-sided nature of the parallel trends assumption). For discussion, see Appendix \ref{appen:full_x}. 

However, when $X\subset W$, this equivalence no longer holds and prior approaches for CATE estimation under conditional exogeneity is no longer applicable and can lead to biased results even in the limit of infinite samples. For instance, the simplest identification formula for the CATE and its accompanying estimation estimation strategy, the $T$-Learner, would estimate the statistical model:
\begin{align*}
    \tau_0(X) = \E[g_1(W) - g_0(W) \mid X],
\end{align*}
where $g_d(W) = \E[Y_1 - Y_0\mid D=d, W]$. However, under the conditional parallel trends assumption it is no longer the case that $\E[Y_1 - Y_0 \mid D=1, W] = \E[Y_1(1) - Y_0(1)\mid W]$, since the parallel trends assumption crucially does not make any restriction that the trends under treatment are conditionally parallel between treated and control units. Therefore $\E[g_1(W)\mid X] \neq \E[Y_1(1) - Y_0(1) \mid X]$, which subsequently implies that $\tau_0(X) \neq \theta_0(X)$. This difference will be more pronounced for datasets where there is a big difference in the covariate distribution between the treated and un-treated groups. 

Thus, when $X\subset W$, the statistical problem that we need to solve based on the identification formula in Proposition~\ref{prop:did} is inherently different than the statistical problem of estimate a CATE. Hence, we need to develop novel meta-learners, specifically for the CATT, that enjoy local robustness properties analogous to the robustness properties of methods that have been developed for the CATE in prior work \cite{R-learner,osl,oprescu2019orthogonal,DR-learner}. Our main result will be a doubly-robust meta learner for the CATT. 

The simplest plug-in meta-learning approach for the CATT is to construct an estimate $\hat{g}_0$ of the baseline growth model $g_0$ using generic ML techniques (since it corresponds to the regression problem of predicting the difference $Y_1 - Y_0$ from covariate $W$, using samples only from the control population, i.e., $D=0$) and then estimate a CATT model by learning a second-stage regression model that predicts the label $Y_1 - Y_0 - \hat{g}_0(W)$ from covariates $X$, using samples only from the treated population, i.e., $D=1$. 

It is well-known \cite{chernozhukov2018double} that using ML estimators in a plug-in manner may cause large estimation bias due to, for example, regularization and model mis-speification. A doubly-robust estimator alleviates this concern as it is less sensitive to errors in the baseline growth model $\hat{g}_0$, and allows for consistent estimation under weaker statistical conditions. 

To present our main result, we need to present a set of preliminary definitions and assumptions. To avoid ill-posed extrapolations between the treated and untreated groups, we need the following overlap condition:
\begin{assumption}[Sufficient Overlap] \label{assum:propensity_overlap}
    For all $W$, there exist $c>0$ such that $ c\leq \mathbb{P}(D=1|W) \leq 1-c$.
\end{assumption}
A key concept related to robustness is that of Neyman orthogonality:
\begin{definition}[Conditional Neyman Othogonality]
    Let $m(Z;\theta,\eta)$ be a moment for the target estimand $\theta(\cdot)$ with nuisance functions $\eta = (\eta_1, \eta_2, \dots)$. Such moment is Neyman orthogonal if the directional derivatives with respect to all nuisance functions $\eta$ is zero when evaluated at the true nuisances, i.e.
    \begin{align*}
        \partial_{\eta}\E[m(Z; \theta_0,\eta)|W]\Big|_{\eta = \eta_0} = 0
    \end{align*}
\end{definition}
\begin{lemma}[Doubly Robust CATT on Subspace of Covariates] \label{lemma:ortho_moment}
    Under Assumptions \ref{assum:cpta}, \ref{assum:no-an} and \ref{assum:propensity_overlap}, the true CATT $\theta_0$ is a solution to the following conditional moment equation:
    \begin{align*}
    \E\left[\left(\frac{D-\pi_0(W)}{(1-\pi_0(W))}\right)(\Delta Y - g_0(W)) - D\theta(X) \,\middle\vert\, X\right] =0
    \end{align*}
    where $\Delta Y = Y_1 - Y_0$, $g_0(W) = \E[\Delta Y|D=0,W]$, $\pi_0(W) = \mathbb{P}(D=1|W)$. Moreover, this moment is conditionally Neyman orthogonal with respect to all nuisance functions (i.e. $\pi(W)$ and $g(W)$).
\end{lemma}
\begin{remark}\label{rmk:dr_cate}
    Comparing with the DR-learner \cite{DR-learner, chernozhukov2017double} for conditional average treatment effect (CATE), we note that, by refocusing on the CATT, our proposed moment condition no longer requires the estimation of the conditional expectation of the outcome $\Delta Y$ for the treated group w.r.t the high dimensional $W$. This can be especially advantageous in practical settings where there are only a small number of treated units in the panel, making the estimation of the conditional expectation of the treated units difficult. Moreover, we show that simply regressing the CATE pseudo-outcome as in the DR-learner for CATE will give a biased estimate when the treated and control groups have very different distributions. For more details, please refer to Appendix \ref{appen:full_x}. 
\end{remark}

The next key insight of our paper is that the Neyman orthogonal moment restriction from Lemma~\ref{lemma:ortho_moment} can be turned into a loss minimization problem and models that satisfy the conditional moment restrictions can be equivalently viewed as minimizers of a strongly convex loss function. This insight is crucial in order to turn the statistical problem into a statistical learning theory problem and subsequently into meta-learning estimation strategy, which will allow for the use of generic ML methods for the estimation of $\theta_0$.

\begin{proposition} \label{prop:loss_catt}
    Consider the incomplete squared loss:
\begin{align*}
    \mathcal{L}(\theta; \pi_0, g_0) = \E\left[D\theta(X)^2 - 2 \widehat{Y}\theta(X)\right]\label{loss}
\end{align*}
where $\widehat{Y}(\pi_0, g_0) = \left(\frac{D-\pi_0(W)}{ 1-\pi_0(W)}\right)(\Delta Y- g_0(W))$. Under the same assumptions as in Lemma \ref{lemma:ortho_moment}, the minimizer of $\mathcal{L}(\theta; \pi_0, g_0)$ over any hypothesis space $\Theta$ is equivalent to the solution to the best-projection problem of the CATT among the treated:
\begin{align*}
    \min_{\theta \in \Theta} \E[(\theta(X) - \theta_0(X))^2\mid D=1]
\end{align*}
\end{proposition}

Note that this is a convex loss function, which suggests computational tractability and fast statistical learning rates and allows it to be efficiently solved using any standard optimization solver. Another advantage of the loss minimization approach is that the out-of-sample loss can be used as a metric for model selection over different function classes \cite{aggregation}. Moreover, as we show next in our main estimation theorem, this loss-based estimator enjoys double robustness properties, in that it leads to fast rates for the CATT if the product of the estimation rates for $\hat{\pi}$ and $\hat{g}$ decays fast enough.

In the theorem below, we use $\hat{\theta}$ to denote a generic estimator that achieves small excess risk with respect to the plug-in loss ${\cal L}(\theta;\hat{\pi}, \hat{g})$, where $\hat{\pi}$, $\hat{g}$ are nuisance estimates, constructed from an auxiliary dataset (sample-splitting). Note that the problem of achieving a small excess risk with respect to a given loss is a standard statistical learning theory problem and hence many ML techniques can be invoked to provide such a guarantee. Hence, our theorem accommodates estimators resulting from a variety of CATT ML estimators, such as empirical risk minimization on the empirical loss, gradient boosted forests or neural networks. 
\begin{theorem}[CATT Rates]\label{thm:catt_rates}
    Let $\hat{\pi}, \hat{g}$ be estimates of the nuisance functions, constructed using an auxiliary dataset. Let $\|\theta\|_{D=1}=\sqrt{\E[\theta(X)^2|D=1]}$ denote the $L_2$ norm over the treated population. Let $\hat{\theta}$ be the result of any estimation process using $n$ samples, satisfying w.p. $1-\delta$
    \begin{align*}
        \mathcal{L}(\hat{\theta}; \hat{\pi}, \hat{g}) - \inf_{\theta\in\Theta}\mathcal{L}(\theta; \hat{\pi}, \hat{g}) \leq R_{n,\delta}^2
    \end{align*} Suppose Assumptions ~\ref{assum:cpta}, ~\ref{assum:no-an}, and ~\ref{assum:propensity_overlap}. If the hypothesis space $\Theta$ is convex or is well specified (i.e. $\theta_0\in \Theta$), then $\hat{\theta}$ satisfies w.p. $1-\delta$:
    \begin{multline*}
    \smash 
        \|\hat{\theta}(X) - \theta_*(X)\|^2_{D=1}  \leq\\
        \textstyle{\frac{4}{\rho} R_{n,\delta}^2 + \beta\,\E\left[\E\left[(\hat{g}(W) - g_0(W))\left(\frac{\pi_0(W) - \hat{\pi}(W)}{1-\hat{\pi}(W)}\right)\,\middle\vert\,X\right]^2\right]}
    \end{multline*} 
    where $\rho=\mathbb{P}(D=1)$ and $\beta= \frac{2}{\rho^2c^2}$ and
    \begin{align*}
        \theta_* \in \arg\min_{\theta \in \Theta} \|\theta(X) - \theta_0(X)\|_{D=1}^2
    \end{align*}
\end{theorem}

\textbf{Lagged Dependent Outcome Alternate Assumption:} In Appendix \ref{appen:lagged}, we also provide an extension of our approach under the lagged dependent outcome assumption, which posits that the past outcomes capture sufficient information to disentangle future outcomes and treatment assignment. This assumption is commonly used to model time series data and is also used in estimating treatment effects \citep[e.g.][etc.]{angrist2009mostly,autoregressive}. 

\textbf{DiD with Instruments: } As a further extension, we consider the setting of estimating heterogeneous effects from panel data with a binary instrument $Z$. This has applications in policy evaluation where the exposure to the policy does not perfectly determine treatment receipt due to non-compliance \cite{gerber2012field}, and we are only willing to assume parallel trends on the exposure and not the chosen treatment. Thus, policy exposure can be interpreted as the instrument. 
% Since the instrument is often more exogenous than the actual treatment uptake, it might be more plausible to believe the identifying assumptions (e.g. conditional parallel trends) with respect to the instrument.
In Appendix \ref{app:iv-cate}, we present a meta-learner for this IV-DID setup.

\begin{table*}[ht]
\centering
\caption{MSE (mean ± standard deviation) Over 100 Simulations. Each row represent a different meta-learner, and columns represent the different nuisance function classes. }
\label{table:mse_cpta}
\vskip 0.15in
\footnotesize
\begin{tabular}{|l|l|l|l|l|l|}
\hline
& \begin{tabular}{@{}c@{}}Linear \\ Regression\end{tabular} & Lasso (CV) & Ridge (CV) & Random Forest & Best \\
\hline
Neural Net (OR)  & 0.12 ± 0.02 & 0.12 ± 0.02 & 0.12 ± 0.02 & 0.38 ± 0.18 & 0.12 ± 0.02 \\
Neural Net (DR) & \textbf{0.1 ± 0.02} & \textbf{0.1 ± 0.03} & \textbf{0.1 ± 0.02} & \textbf{0.14 ± 0.04} & \textbf{0.1 ± 0.02} \\
% nn_OR_AR & 17.73 ± 12.92 & 0.12 ± 0.02 & 0.14 ± 0.04 & 0.12 ± 0.02 & 1.27 ± 0.65 & 0.12 ± 0.02 \\
% nn_DR_AR & 9.32 ± 8.89 & 0.1 ± 0.02 & 0.1 ± 0.03 & 0.1 ± 0.02 & 0.63 ± 0.4 & 0.1 ± 0.02 \\
\hline
XGBoost (OR) & 0.09 ± 0.02 & 0.09 ± 0.02 & 0.09 ± 0.02 & 0.31 ± 0.16 & 0.09 ± 0.02 \\
XGBoost (DR)  & \textbf{0.04 ± 0.01} & \textbf{0.04 ± 0.01} & \textbf{0.04 ± 0.02} & \textbf{0.06 ± 0.03} & \textbf{0.04 ± 0.01} \\
% xgb_OR_AR & 16.23 ± 11.86 & 0.09 ± 0.02 & 0.11 ± 0.04 & 0.09 ± 0.02 & 1.15 ± 0.69 & 0.09 ± 0.02 \\
% xgb_DR_AR & 6.64 ± 6.98 & 0.04 ± 0.01 & 0.05 ± 0.03 & 0.04 ± 0.02 & 0.54 ± 0.45 & 0.04 ± 0.01 \\
% \hline
% Linear (OR) & 1.62 ± 0.86 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.51 ± 0.18 & 0.26 ± 0.07 \\
% Linear (DR) & 0.33 ± 0.11 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 \\
% linear_OR_AR & 17.45 ± 12.68 & 0.26 ± 0.07 & 0.28 ± 0.08 & 0.26 ± 0.07 & 1.18 ± 0.56 & 0.26 ± 0.07 \\
% linear_DR_AR & 1.71 ± 1.84 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.42 ± 0.19 & 0.26 ± 0.07 \\
\hline
\end{tabular}
\vskip -0.1in
\end{table*}
\section{General Conditional Functionals Under Covariate Shift}
In this section, we show that the CATT estimation problem under conditional parallel trends can be viewed as a special case of a much more broad statistical estimation problem which can capture many other empirical problems beyond heterogeneous effects in DiD analysis. 

In particular, we consider the following estimation problem. Consider data consisting of $Z$, which contains covariates $W$ drawn from a target distribution $\mathcal{D}_{t}$. Let $\E_t[\cdot]$ denote the expectation with respect to the distribution $\mathcal{D}_t$. The goal is to estimate a conditional linear functional $\E_{t}[m(Z;g_0)|X]$ of the regression function $g_0(W)=\E[Y|W]$, where $X$ is a subset of $W$, $m$ is a linear moment functional of $g_0$ and the expectation is taken with respect to the target distribution. On the other hand, labels for the target variable $Y$ of the regression function are available only on data where the covariates are drawn from a different source distribution, i.e., $(Y, W) \sim \mathcal{D}_s$. Let $\E_{s}[\cdot]$ denote the expectation with respect to $\mathcal{D}_s$. We assume that there is only covariate drift and no concept drift, i.e. 
\begin{assumption}[No concept drift]\label{ass:no-drift}
$g_0(W) = \E_{s}[Y|W] = \E_{t}[Y|W]=\E[Y|W]$.
\end{assumption} 
Let $E$ denote the indicator variable of whether the sample stems from the target distribution environment. We can then rewrite the statistical estimand as:
\begin{align*}
    \theta(X) = \E[m(Z;g)|E=1, X]
\end{align*}

For instance, in the case of the CATT problem in the DiD setting, the moment is $m(Z;g)=Y_1(1) - Y_0(1) - g(W)$ and the outcome regression $g(W)=\E[Y_1(0) - Y_0(0)| W, D=0]$ is learned based on the covariate distribution of the untreated units, while the estimand is the conditional functional $\E[m(Z;g)| X, D=1] = \E[Y_1 - Y_0 - g(W)| X, D=1]$, which is a conditional expectation taken over the covariate distribution of the treated units, conditioning on a subset $X$ of $W$. Since the label for the regression function $g(W)$ is $Y_1(0) - Y_0(0)$, it is only available for the untreated group. 

Debiasing techniques for unconditional functionals under covariate shift were analyzed in the prior work of \citealp{cov_shift}. In this paper, we substantially extend their analysis to the case of conditional functionals and provide a doubly robust meta-learning strategy for any such conditional linear functional problem under covariate shift.


We further motivate this setup with several other empirically prevalent examples from the machine learning and causal inference literature. 
\begin{example}[Conditional prediction powered inference] \label{ex:distillation}
In settings where prediction is a central task, it is often desirable to leverage predictive models to improve the efficiency and accuracy of statistical inference. Consider some high-dimensional features or covariates $W$, some labels $Y$, and a simulation model $g(W)$ for the predictive task $\E[Y\mid W]$. An example of the prediction powered inference framework of \cite{angelopoulos2023prediction}, asks to estimate $\E_{t}[Y]$. However, we might only have labeled data on a smaller or slightly different sub-population $D_s$. In this case, we can use the simulation model and instead target the statistical estimand $\E_t[g(W)]$, using the labeled data only for debiasing the simulation model. Our work extends this setting to allow for the estimation of conditional means with respect to a subset of the covariates $X$, in the target distribution, i.e. $\theta(X)=\E_t[g(W)\mid X]$ and in a setting where the covariate shift density ratio is unknown (prior work considers only the case of a known covariate shift). In many applications, labels might be expensive to obtain and are only available for a small subpopulation which can be a different covariate distribution from the whole population. This setting fits into the framework with $m(Z;g)=g(W)$.
\end{example}

\begin{example}[Heterogeneous long-term effects from short-term experiments using historical data] \label{ex:surrogates}
    Here we consider settings where we have run a short-term experiment, where a treatment $D$ was randomized over a population of users drawn from $D_t$ and our goal is to estimate the effect of $D$ on a long-term outcome $Y$. However, we want to estimate that effect without the need to wait for the long-term effect to materialize, but solely based on short-term data. A typical technique used in this setting is the surrogate approach, where we assume that the long-term outcome $Y$, is not directly affected by the treatment $D$, but is affected indirectly through some short-term or "surrogate" post-treatment outcomes $S$, i.e. $Y(d) = Y(S(d))$. Under this assumption, it can be shown that the long-term effect can be identified by measuring the effect of the treatment on the predicted long term outcome, based on the surrogates and other potentially pre-treatment covariates $X$. For any set of pre-treatment co-variates $X$, we can identify the CATE as $\theta(X) = \E[Y(1)-Y(0)\mid X] = \E[g(W)\mid D=1, X] - \E[g(W)\mid D=0, X]$, where $W=(S, X)$ and $g(W) = \E[Y\mid W]$. The function $g(W)$ can be learned using historical data where we have access to short-term signals $S$, characteristics $X$ and long term outcomes $Y$. However, the historical covariate distribution $D_s$ can potentially be different from the distribution $D_t$. Since the treatment is randomized, we can write the target estimand as:
    \begin{align*}
        \theta(X) = \E_s\left[g(W) \left(\frac{D}{\pi} - \frac{1-D}{1-\pi}\right)\,\middle\vert\, X\right]
    \end{align*}
    where $\pi=\mathbb{P}(D=1) = \mathbb{P}(D=1\mid W)$. This setting falls in the framework with $m(Z;g) = g(X) \left(\frac{D}{\pi} - \frac{1-D}{1-\pi}\right)$.
\end{example}

\begin{example}[CATE with covariate shift]\label{ex:cate}
Consider the case of estimating the CATE $\tau_0(X)$ under conditional exogeneity. Many times we want to understand the projection of the CATE on a subset of variables $W$ and over some target population $D_t$ over which we will deploy our personalized policy. However, we might want to use a bigger population $D_s$ to train our CATE model, so as to increase accuracy. In this setting, the target statistical estimand can be written as $\E_t[g(1, W) - g(0, W)\mid X]$, where $g(D, W) = \E[Y\mid D, W]$. This lies in the framework with $m(Z;g)=g(1, W) - g(0, W)$.
\end{example}

We provide a debiasing framework for this problem. Before presenting the main results, we state the necessary definitions and assumptions.
\begin{definition}[Conditional Riesz Representer]
    The Conditional Riesz Representer of a continuous linear functional $m(Z;g)$ on $X$, with respect to some function $g(W)$, is the square-integrable random variable $\alpha(X)$ such that:
    \begin{align*}
        \E_s[m(Z;g)|X] = \E_s[{\alpha(W)}g(W)|X]\\ \quad \forall g(W) \quad s.t. \quad \E[g(W)^2]<\infty
    \end{align*}
\end{definition}

\begin{assumption}[Sufficient Overlap Under Covariate Shift] \label{assum:overlap_cov_shift}
    For all $W$, there exist $c>0$ such that $ c\leq \mathbb{P}(E=1|W) \leq 1-c$.
\end{assumption}

\begin{theorem}[Neyman Orthogonal Moments for General Conditional Funcitonals under Covariate Shift]\label{thm:gen_cov_shift}
Suppose that {Assumptions \ref{ass:no-drift} and \ref{assum:overlap_cov_shift} hold}. Consider a nuisance regression function $g_0(W) = \E[Y|W]$, and target estimand $\theta(X) = \E_{t}[m(Z;g_0)|X]=\E[m(Z;g_0)|E=1,X]$, where $m(Z;g)$ is a continuous linear functional of $g$. The true solution $\theta_0(X)$ satisfies the following conditional moment restriction that is Neyman orthogonal with respect to all the nuisance functions {$\pi(W)$, $\alpha(W)$} and $g(W)$:
% \begin{align*}
%     \E\left[S(m(Z;g) - \theta(w)) + \frac{(1-S)\pi(X)}{1-\pi(X)}\alpha(X)(Y-g(X))|W\right]
% \end{align*}
\begin{align*}
    \E\Big[ & E\cdot (m(Z;g) - \theta(X)) + \\
    &  (1-E)\cdot {\frac{\pi(W)}{1-\pi(W)}\alpha(W)}(Y-g(W)) \Big| X \Big] = 0
\end{align*}
where {$\pi(W) = \mathbb{P}(E=1|W)$ and $\alpha(W)$ is the conditional Riesz representer of $\E_{s}[m(Z;g)\mid X]$.}
\end{theorem}
In the CATT application the Riesz Representer is $-1$. In Example~\ref{ex:distillation}, the Riesz representer is $1$. In Example~\ref{ex:surrogates} the Riesz representer is $\frac{q(W)}{\pi} + \frac{1-q(W)}{1-\pi}$, where $q(W)=\mathbb{P}(D=1\mid W, E=1)$. In Example~\ref{ex:cate} the Riesz representer $\alpha(D,W)$ is $\frac{D}{\mathbb{P}(D=1|E=1,W)} - \frac{1-D}{\mathbb{P}(D=0|E=1,W)}$. 

As in Proposition \ref{prop:loss_catt}, this conditional moment can be turned into a convex doubly robust loss minimization problem. 
\begin{align*}
    \mathcal{L}(\theta; \pi, g) ~&= \E\left[E\theta(X)^2 - 2 \widehat{Y}\theta(X)\right]
\end{align*} where $\widehat{Y} = E m(Z;g) + \frac{(1-E)\pi(W)}{1-\pi(W)}\alpha(W)(Y-g(W))$. Note that in this loss function the variable $Y$ is always multiplied by $1-E$ and therefore it respects the constraint that outcomes $Y$ are only available in the source environment. The double robustness property of the loss will make the resulting estimand robust to estimation errors in the nuisance functions. Analogous to Theorem~\ref{thm:catt_rates}, we can prove fast statistical learning rates, for the resulting estimator based on this doubly robust loss. It is easy to verify that for the CATT setting this loss coincides with the loss in Section~\ref{sec:catt}. 

\begin{table*}[ht]
\centering
\caption{MSE (mean ± standard deviation) Over 100 Simulations of Imbalanced Dataset. Each row represent a different meta-learner, and columns represent the different nuisance function classes. }
\label{table:mse_Imbalanced}
\vskip 0.15in
\footnotesize
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
& \begin{tabular}{@{}c@{}}Linear \\ Regression\end{tabular} & Lasso (CV) & Ridge (CV) & Random Forest & Best \\
\hline
Neural Net (OR)  & 0.22 ± 0.06 & 0.21 ± 0.06 & 0.21 ± 0.06 & 0.4 ± 0.15 & 0.21 ± 0.05 \\
Neural Net (DR) & \textbf{0.18 ± 0.07} & \textbf{0.18 ± 0.05} & \textbf{0.18 ± 0.05} & \textbf{0.24 ± 0.07} & \textbf{0.18 ± 0.05} \\
Neural Net (CATE OR)  & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.51 ± 0.16 & 0.27 ± 0.08 \\
Neural Net (CATE DR) & 0.22 ± 0.07 & 0.22 ± 0.07 & 0.21 ± 0.07 & 0.33 ± 0.11 & 0.21 ± 0.07 \\
% \hline
% Linear (OR) & 1.55 ± 0.75 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.47 ± 0.16 & 0.27 ± 0.08 \\
% Linear (DR) & 0.35 ± 0.1 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.28 ± 0.08 & 0.27 ± 0.08 \\
% Linear (CATE OR) & 0.66 ± 0.27 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.52 ± 0.17 & 0.27 ± 0.08 \\
% Linear (CATE DR) & 0.36 ± 0.11 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.31 ± 0.09 & 0.27 ± 0.08 \\
\hline
XGBoost (OR)  & 0.21 ± 0.06 & 0.21 ± 0.06 & 0.21 ± 0.06 & 0.34 ± 0.11 & 0.21 ± 0.06 \\
XGBoost (DR)   & \textbf{0.12 ± 0.03} & \textbf{0.12 ± 0.03} & \textbf{0.12 ± 0.03} & \textbf{0.18 ± 0.06} & \textbf{0.12 ± 0.03} \\
XGBoost (CATE OR)  & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.51 ± 0.16 & 0.27 ± 0.08 \\
XGBoost (CATE DR)   & 0.15 ± 0.05 & 0.15 ± 0.04 & 0.15 ± 0.05 & 0.34 ± 0.13 & 0.15 ± 0.04 \\
\hline
\end{tabular}
\vskip -0.1in
\end{table*}

\begin{table*}[ht]
\centering
\caption{MSE (mean ± standard deviation) over 100 semi-synthetic datasets generated from the Minimum Wage dataset.}
\label{table:mse_min_wage}
\vskip 0.15in
\footnotesize
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
 & \begin{tabular}{@{}c@{}}Linear \\ Regression\end{tabular} & Lasso (CV) & Ridge (CV) & Random Forest & Best \\
\hline
XGBoost (OR)  & 1.97 ± 0.04 & 2.02 ± 0.05 & 1.96 ± 0.04 & 2.08 ± 0.09 & 2.07 ± 0.09 \\
XGBoost (DR) & \textbf{1.91 ± 0.04} & \textbf{1.88 ± 0.04} & \textbf{1.91 ± 0.04} & \textbf{1.8 ± 0.09} & \textbf{1.8 ± 0.09} \\
XGBoost (CATE OR)  & 2.72 ± 0.06 & 2.71 ± 0.07 & 2.73 ± 0.06 & 3.4 ± 0.36 & 2.69 ± 0.07 \\
XGBoost (CATE DR)  & 2.73 ± 0.06 & 2.7 ± 0.06 & 2.73 ± 0.07 & 3.47 ± 0.3 & 2.66 ± 0.07 \\
\hline
Linear (OR)  & 1.96 ± 0.04 & 2.01 ± 0.04 & 1.96 ± 0.04 & 2.07 ± 0.08 & 2.06 ± 0.08 \\
Linear (DR)  & \textbf{1.92 ± 0.04} & \textbf{1.89 ± 0.04} & \textbf{1.92 ± 0.04} & \textbf{1.83 ± 0.07} & \textbf{1.83 ± 0.07} \\
Linear (CATE OR)  & 2.78 ± 0.05 & 2.76 ± 0.05 & 2.78 ± 0.05 & 3.14 ± 0.38 & 2.76 ± 0.05 \\
Linear (CATE DR)  & 2.8 ± 0.05 & 2.75 ± 0.05 & 2.8 ± 0.05 & 3.13 ± 0.36 & 2.71 ± 0.05 \\
\hline
\end{tabular}
\vskip -0.1in
\end{table*}
\section{Extension to Multi-Period Setting} \label{sec:multi-period}
In the multiple time period setting, we observe the outcomes for each unit for time periods $t = 0, 1, \dots, T$. Moreover, assume that no unit is treated at period $0$. Consider first the case where all treated units are treated at period $G=1$ and we assume the conditional parallel trends assumption that for all $t\geq 1$, $\E[Y_{t}(0)-Y_0(0)\mid D=1, W]=\E[Y_{t}(0)-Y_0(0)\mid D=0, W]$. Note that in this case, we can treat the distance $\Delta \in \{0, \dots, T-1\}$ of a target period $t$ from the initial treatment time period $1$ as a random variable. We can also denote with $Y_{post}(0)$ as the random variable corresponding to the post-treatment period outcome we are looking at. Then we can equivalently write:
\begin{align*}
    \E[Y_{post}(1) - Y_{post}(0)\mid X, \Delta=t] = \E[Y_t(1) - Y_t(0)\mid X]
\end{align*}
Thus we can treat the distance from treatment $\Delta$, as yet another covariate in our framework and make it part of $X$. This way, we can flexibly estimate treatment effect heterogeneity as a function of the distance from the initial treatment period and let ML methods select the best model on how distance from initial treatment changes the effect.

Next consider the more general setting of a staggered roll-out, i.e. each treated unit ($D=1$) is treated at some period $G=[1, T]$ and remains treated after that period. We denote the never-treated group as $G=\infty$.
In this setting, we can make the parallel trends assumption that for all $g\in [1,T]$ and for all $t\geq g$ $\E[Y_{t}(0) - Y_0(0)\mid D=1, W, G=g]=\E[Y_{t}(0) - Y_0(0)\mid D=0, W,G=\infty]$.
Similarly, we can incorporate heterogeneity as a function of the initial treatment period $G$ and the distance $\Delta$ to the treatment period, by making these variables as part of our heterogeneity set $X$: for all $g\in [1, T]$ and $t\in [g, T]$:
\begin{multline*}
    \E[Y_{post}(1) - Y_{post}(0)\mid D=1, X, \Delta=\ell, G=g]\\
    = \E[Y_{g+\ell}(1) - Y_{g+\ell}(0)\mid D=1, X, G=g]
\end{multline*}
Prior work of \citealp{CSDID} considers heterogeneity with respect to $G$ and $\Delta$, albeit in a fully non-parametric manner and does not provide a method for model selection, so as to uncover in a more data-driven manner the functional form of this heterogeneity. We note that the prior work of \citealp{CSDID} also considers doubly robust estimation and inference on weighted averages of these heterogeneous effect models across different values of $G$ and $\Delta$, which we do not discuss in this work.

\section{Experiments and Results}\label{sec:exp}
\subsection{Fully Synthetic Data}
First, to compare the results of our proposed method with other baselines, we conducted fully simulated experiments where the datasets are generated from known data generating processes that satisfy the identifying assumptions described in Section~\ref{sec:assum_iden}. The data has 20 covariates, and the CATT learners look at the projection onto 5 covariates. We report the mean MSE (mean square error) between the predicted CATT and the true CATT on covariates of the treated units of a held out test set. We compare our results with the following baseline models, here $g_d(W) = \E[Y_1 - Y_0|W, D=d]$, $g(W, D) = g_1(W)D+g_0(W)(1-D)$, and $\pi(W) = \mathbb{P}(D=1|W)$:
\begin{itemize}
    \item Outcome regression (OR) learner: $\theta(X) = \E[Y_1 - Y_0 - g_0(W)|D=1, X]$
    \item CATE outcome regression learner: $\theta(X) = \E[g_1(W) - g_0(W)|D=1, X]$
    \item CATE DR-learner: $\theta(X) = \E[g(W,1) - g(W,0) + \left(\frac{D}{\pi(W)} - \frac{1-D}{1-\pi(W)}\right)(Y - g(W,D))|D=1, X]$
\end{itemize}
We considered three different final-stage models for the CATT: neural net, XGBoost, and linear models, to fit the meta-learners. Simulation results are presented in Table~\ref{table:mse_cpta}. The results of linear models can be found in the Appendix~\ref{sec:exp}. The columns represent different ML methods that are used to learn the outcome regression. The propensity function, i.e. $\mathbb{P}(D=1|W)$, is always fitted using logistic regression. The "Best" column, represents using the ML method that achieved the lowest out-of-sample MSE for the outcome regression. In the Appendix, we also provide results that investigate the performance of our DiD CATT method and the baselines even when the parallel trends assumption is violated. The results in Appendix~\ref{app:add_exp} suggest that the doubly robust estimator reduces the MSE, as compared to the baselines, even under the violation of parallel trends.

Moreover, we also consider unbalanced datasets, where the size of the control group is much larger than that of the treated group. In particular, the propensities of each unit was lowered by a factor of 10. The results are presented in Table \ref{table:mse_Imbalanced}. We see that while all models suffered in performance, our proposed doubly robust model still outperforms the other meta-learners as it leverages the asymmetry of the CATT definition to be more robust to unbalanced settings. Notably, the proposed learner out-performs the doubly robust CATE learner as discussed in Remark \ref{rmk:dr_cate}.

\subsection{Minimum Wage Case Study}
\begin{figure}[ht]
\vskip 0in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/min_wage_pop.png}}
\caption{Predicted CATT with respect to log county population.}
\label{fig:min_wage_pop}
\end{center}
\vskip -0.1in
\end{figure}
We applied our proposed approach to the minimum wage dataset that is also studied in \citealp{CSDID} and \citealp{callaway2023policy}. This dataset studies the effect of minimum wage changes on teen employment during the period 2001–2007. The outcome variable of interest is the log of county-level teen employment, while the treatment variable is defined as a binary indicator representing whether a county's minimum wage exceeds the federal minimum wage. The dataset includes covariates such as county population and average annual pay, which serve as controls to account for differences in local economic conditions. For the ease of interpretation, we focus only on the raise in minium wage at year 2004 as the treatment. In our analysis, we treat years after treatment assignment time (2004) as an additional covariate to control for, as discussed in Section \ref{sec:multi-period}. Employment rates as well as other covariates from before 2003 are also used as the covariates. 
\begin{figure}[ht]
\vskip 0in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/cal_pop_xgb.png}}
\caption{Calibration plot for CATT of minimum wage with respect to log county population.}
\label{fig:cal_pop_xgb}
\end{center}
\vskip -0.3in
\end{figure}

As a preliminary evaluation, we tested the performance of our methods on semi-synthetic data generated from this dataset. The semi-synthetic data was generated by bootstrapping the samples in the dataset and applying a chosen function for treatment assignment and treatment effect to compute the post-treatment outcomes. Since this dataset is rather low dimensional, we did not experiment with neural networks. The mean MSE with respect to the true CATT function is reported in Table \ref{table:mse_min_wage}. The results show that the proposed method out-performs outcome regression learners as well as the CATE learners. 

We then applied our method to the original real dataset. Figure \ref{fig:min_wage_pop} shows the CATT prediction over different values of log county population. The three models used (linear regression, XGBoost, and kernel ridge regression) all showed some extend of positive trends. This suggests that raising minimum wage might have a smaller negative effect on teen employment for counties with a larger population.

\textbf{Validating CATT: }Since we do not have access to ground truth treatment effects for real datasets, we need a way to validate the heterogeneity that is picked up by the model is not due to noise. One approach is through calibration. The first step of the procedure is quantile binning the CATT predictions on a held out validation set. Next, the CATT predictions on a held out test set will be put into the bins according to the tresholds. The group average treatment effect on the treated (GATT) for each bin is calculated as the mean of the heterogeneous model predictions, as well as calculating the unconditional ATT for each group (i.e. conditioning on the empty set, we get $\theta = \E[\widehat{Y}]/\E[D]$). If the heterogeneity is indeed significant, we expect the calibration plots line up in the $45^{\circ}$ line with non-overlapping confidence intervals. 


Figure \ref{fig:cal_pop_xgb} presents the calibration plot for the doubly robust CATT learner realized using XGBoost. While we see that the GATT for the lowest quantile has a larger confidence intervals, the highest most two quantiles have non-overlapping confidence intervals. This suggests that there is significant heterogeneity between high and low populations. Together with Figure \ref{fig:min_wage_pop}, the results seem to suggest that the treatment effect for counties with large populations is close to zero.   


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Causal Inference and Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{DiD with Instruments}\label{app:iv-cate}
As an extension, we consider a widely encountered setting of estimating heterogeneous treatment effects from panel data with a binary instrument $Z$, with two sided non-compliance. In this setting, the target estimand is the conditional local average treatment effect among the exposed (CLATT) in the second (post-treatment) time period:
$$\theta_0(X) =  \E[Y_1(1)-Y_1(0)\mid D_1(1)>D_1(0), Z=1, X]$$
First, we consider the natural conditional extensions, which allows for more heterogeneity and flexibility, of the parallel trends assumptions stated in \citealp{IV_DID}. 
\begin{assumption}[No carryover assumption]
Let $\mathbf{d} = (d_0, d_1)$ denote the treatment path, then $Y_{0}(\mathbf{d},z) = Y_{0}(d_0,z)$ and $Y_{1}(\mathbf{d},z) = Y_{1}(d_1,z)$.   
\end{assumption}
This assumption requires that the outcome is only affected by the current treatment, in other words, there is no carry over effects from previous treatments. 

\begin{assumption} [Exclusion restriction for potential outcomes]\label{assum:exo-iv}
    For all $t$, $Y_{t}(\mathbf{d},z) = Y_{t}(\mathbf{d})$
\end{assumption}
This is the standard exclusion restriction assumption for instrumental variables that the instrument only affects the outcome through the treatment.

\begin{assumption}[Monotonicity Assumption] \label{Assum:no-defier}
    $\mathbb{P}(D_{1}(1)\geq D_{1}(0)) = 1$ or  $\mathbb{P}(D_{1}(1)\leq D_{1}(0)) = 1$
\end{assumption}
This assumption requires that the effect of the instrument is monotone - that it either increases treatment adoption or decreases treatment adoption, but not both. This assumption is needed for identification under two-sided non-compliance. 

\begin{assumption}[No anticipation in treatment]\label{assum:no-an-iv}
    $D_{0}(1) = D_{0}(0)$ for all units with $Z = 1$
\end{assumption}
Similar to the standard no-anticipation assumption that the treatment assignment in the second period should not have an affect on the outcome in the first period, here we assume that the exposure event that happens at the second period should not have an anticipatory effect on the treatment adoption in the first period. 

\begin{assumption}[CPTA in Treatment]\label{assum:PTA_Z_D}
    \begin{align*}
        ~&\E[D_{1}(0) - D_{0}(0)|Z=0, W] \\
        ~&= \E[D_{1}(0) - D_{0}(0)|Z=1, W]
    \end{align*}
\end{assumption}
Here we no longer require the instrument to be independent with the potential outcome of the treatments, but instead require that the trend under no exposure is (mean) independent to the exposure. 

\begin{assumption}[CPTA in Outcome]\label{assum:PTA_Z_Y}
    \begin{align*}
    ~&\E[Y_{1}(D_{1}(0)) - Y_{0}(D_{0}(0))|Z=0, W]\\
    ~&= \E[Y_{1}(D_{1}(0)) - Y_{0}(D_{0}(0))|Z=1, W]
    \end{align*}
\end{assumption}

\begin{assumption}[Sufficient Overlap in Instrument] \label{assum:propensity_overlap_Z}
    For all $W$, there exist $c>0$ such that $ c\leq \mathbb{P}(Z=1|W) \leq 1-c$.
\end{assumption}
Moreover if the instrument does not have any effects on the treatment, the local average treatment effect will also not be identified. Hence, we need the following assumption to low-bound the effects of the instrument on the treatment.
\begin{assumption}[Strong Instrument under PTA]\label{assum:strong_iv}
    There exist $c_z>0$ such that $$\Big|\E[D_1 - D_0 - \E[D_1 - D_0\mid Z=0, W]\mid Z=1,X]\Big| \geq c_z$$
\end{assumption}

%For a detailed discussion on the CPTA for instrumented panel data and comparison with the standard CPTA, please refer to the Appendix E. 

\begin{proposition}\label{prop:IV}
    Under Assumptions \ref{assum:exo-iv}, \ref{Assum:no-defier}, \ref{assum:no-an-iv}, \ref{assum:PTA_Z_D}, \ref{assum:PTA_Z_Y}, and \ref{assum:strong_iv}, the CLATE, $\theta_0(W)$, can be identified as:
    \begin{align*}
        \theta_0(W) = \frac{\E[Y_1 - Y_0 - \E[Y_1 - Y_0\mid Z=0, W]\mid Z=1,X]}{\E[D_1 - D_0 - \E[D_1 - D_0\mid Z=0, W]\mid Z=1,X]}
    \end{align*}
    % Alternatively, under Assumptions \ref{assum:exo-iv}, \ref{Assum:no-defier}, \ref{assum:no-an-iv}, \ref{assum:lagged_Z_D}, and \ref{assum:lagged_Z_Y}, the CLATE, $\tau(W)$, can be identified as:
    % \begin{align}
    %     \tau(W) = \frac{\E[Y_1 - \E[Y_1\mid Z=0, X, Y_0]\mid Z=1,W]}{\E[D_1 - \E[D_1\mid Z=0, X, D_0]\mid Z=1,W]}
    % \end{align}
\end{proposition}
 In the special case that $W=X$ under the parallel trends assumption, the problem is again equivalent to the standard IV problem, and \citealp{DR-IV} proposed a doubly robust algorithm for estimating the heterogeneous LATE. For more discussion, see Appendix \ref{appen:full_x}. 
% \textcolor{red}{when using projection on W directly, clip negatives; estimate cov(Z-pi/1-pi, D|X) instead}


\begin{lemma}[Doubly Robust Conditional Moment Restriction for CLATE]\label{lemma:ortho_moment_IV}
     Under Assumptions \ref{assum:exo-iv}, \ref{Assum:no-defier}, \ref{assum:no-an-iv}, \ref{assum:PTA_Z_D}, and \ref{assum:PTA_Z_Y}, \ref{assum:propensity_overlap_Z}, the true CLATE is a solution to the following conditional moment equation:
    \begin{align*}
    \E\left[ \widehat{Z}\left\{(\Delta Y- g_Y(W)) - (\Delta Y- g_D(W))\theta(X) \right\}\,\middle\vert\, X \right] = 0
\end{align*}
    where $\Delta S = S_1 - S_0$ for $S = Y$ or $D$, $g_S(W) = \E[S_1 - S_0|Z=0,W]$, and $\widehat{Z} = \frac{Z-\mathbb{P}(Z=1|W)}{1-\mathbb{P}(Z=1|W)}$. Moreover, this moment is Neyman orthogonal with respect to all nuisance functions.
\end{lemma}

\begin{proposition}\label{prop:loss_clate}
Consider the incomplete squared loss:
\begin{align*}
    ~&\mathcal{L}_{IV}(\theta; \pi_0, g_{0,Y}, g_{0,D})\\   
    ~&= \E\left[ \widehat{Z}\left\{(\Delta D- g_{0,D}(W))\theta(X)^2 - 2(\Delta Y- g_{0,Y}(W)) \theta(X)\right\}\right] 
\end{align*}
where$\Delta S = S_1 - S_0$ for $S = Y$ or $D$, $g_{0,S}(W) = \E[S_1 - S_0|D=0,W]$, and $\widehat{Z} = \frac{Z-\pi_0(W)}{1-\pi_0(W)}$ for $\pi_0(W) = \mathbb{P}(Z=1|W)$.  Under the same assumptions as in Lemma \ref{lemma:ortho_moment_IV}, the minimizer of $\mathcal{L}(\theta; \pi_0, g_{0,Y}, g_{0,D})$ over any hypothesis space $\Theta$ is equivalent to the solution to the best-projection problem of the CATT among the treated:
\begin{align*}
    \min_{\theta \in \Theta} \E[(\theta(X) - \theta_0(X))^2\mid Z=1, D(1)>D(0)]
\end{align*}
\end{proposition}

\begin{theorem}[CLATE Rates]\label{thm:clate_rates}
    Let $\hat{\pi}, \hat{g}_D, \hat{g}_Y$ be estimates of the nuisance functions, constructed using an auxiliary dataset. Let $\|\theta\|_{D=1,CM}=\sqrt{\E[\theta(X)^2|D=1, D(1)>D(0)]}$ denote the $L_2$ norm over the compliers among the treated population. Let $\hat{\theta}$ be the result of any estimation process using $n$ samples, satisfying w.p. $1-\delta$:
    \begin{align*}
        \E\left[\mathcal{L}_{IV}(\hat{\theta}; \hat{\eta}) - \inf_{\theta\in\Theta}\mathcal{L}_{IV}(\theta; \hat{\eta})\right] \leq R_{n,\delta}^2
    \end{align*}
    Define the nuisance errors to be:
    \begin{align*}
        \text{Error}(\pi, g_D) :=~& \E\left[\E\left[\left(\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}\right)(g_{0,D}(W) - g_{D}(W))\,\middle\vert\, X \right]^2\right]^{\frac{1}{2}}\\
        \text{Error}(\pi, g_Y) :=~& \E\left[\E\left[\left(\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}\right)(g_{0,Y}(W) - g_{Y}(W))\,\middle\vert\, X \right]^2\right]^{\frac{1}{2}}\\
    \end{align*}
    Suppose Assumptions \ref{assum:exo-iv}, \ref{Assum:no-defier}, \ref{assum:no-an-iv}, \ref{assum:PTA_Z_D}, and \ref{assum:PTA_Z_Y}, \ref{assum:propensity_overlap_Z} are satisfied. Moreover, assume that there exist finite constant $B$ such that $|\theta(X)|\leq B$ for all $X$ with positive measure and all $\theta\in\Theta$. If the hypothesis space $\Theta$ is convex or is well specified (i.e. $\theta_0\in \Theta$), and $\text{Error}(\pi, g_D)$ is sufficiently small ($\text{Error}(\pi, g_D)\leq\frac{chk}{8B^2}$), then $\theta$ satisfies, w.p. $1-\delta$:
    \begin{align*}
    \|\theta(X) - \theta_*(X)\|^2_{D=1, CM} 
    &\leq \frac{4}{hk - \frac{8B^2}{c}\text{Error}(\pi, g_D)}R_{n}^2 + \left(\frac{\max(4B^2,2)}{c\left(hk - \frac{8B^2}{c}\text{Error}(\pi, g_D)\right)}\right)\left(\text{Error}(\pi, \hat{g}_Y) +\text{Error}(\pi, \hat{g}_D)\right) 
\end{align*}
    where $h = \mathbb{P}(Z=1)$, $k = \mathbb{P}(D(1)>D(0)|Z=1)$, and
    \begin{align*}
        \theta_* \in \arg\min_{\theta \in \Theta} \|\theta(X) - \theta_0(X)\|_{\Theta}^2
    \end{align*}
\end{theorem}

\section{Alternate Assumptions: Lagged Dependent Outcome}\label{appen:lagged}
The parallel trends assumption guards against linear, time invariant, additive confounding. However, this may be unrealistic in practice. For instance, it might be sensible for the increment with respect to time to depend on the initial level of the outcome, i.e. $Y_1(0) - Y_0(0) \propto Y_0(0)$. One may also be interested in the natural extension to the parallel trends assumptions that also accounts for more complicating confounding pattern. An popular alternative to model panel data is through the lagged dependent variable assumption.

\begin{definition}[Outcome Support]
    Let $\mathbb{Y}_{dt}$ denote the support of the outcome at time $t$ for the cohort with treatment assignment $d$.
\end{definition}

\begin{assumption}[Lagged Dependent Outcome with Covariates] \label{assum:lag-dep-outcome}
$\E[Y_1(0)|Y_0(0)=y,D=1, W] = \E[Y_1(0)|Y_0(0)=y,D=0, W] $ for all $y \in \mathbb{Y}_{00}$, and $x \in \mathbb{X}$.
\end{assumption}
Note that this assumption may be seen as a special case of Assumption \ref{assum:cpta}, where the conditioning variable also includes the pre-treatment outcome as well as the observed covariates. This assumption might be more convincing in some practical applications. For instance, in wage studies, current income is generally believed to he highly dependent on past income. However, in cases where the distribution of outcome is significantly different between the treated and untreated groups, this assumption might lead an increase in bias due to matching the pre-treatment outcomes, as shown in \cite{daw2018matching}.

\begin{assumption}[Overlap in Pre-treatment Outcome]\label{assum:outcome-overlap}
    $\mathbb{Y}_{10} \subseteq \mathbb{Y}_{00}$
\end{assumption}
Note this is a testable assumption, and one can also perform a diagnostic test to ensure that the treated and control outcome distributions have sufficient overlap. 

\begin{proposition}\label{prop:lagged}
    Under Assumptions \ref{assum:lag-dep-outcome} and \ref{assum:outcome-overlap}, the CATT, $\theta_0(W)$, can be identified as:
    \begin{align*}
        \theta_0(X) = \E[Y_{1}(1) - Y_{1}(0)|D=1, X] = \E[Y_1|D=1,X] - \E[g(Y_0,W)|D=1,X]
    \end{align*}
    where $g(y,W) = \E[Y_1|D=0, Y_0=y, W]$
\end{proposition}
\begin{proof}
    \begin{align*}
    \E[Y_1(0)|D=1,X] =~& \int_{\mathbb{Y}_{10}}\E[Y_1(0)|Y_0=y,D=1,W]p(Y_0=y, W|D=1,X)dy\\
    =~& \int_{\mathbb{Y}_{10}}\E[Y_1(0)|Y_0=y,D=0, W]p(Y_0=y, W|D=1,X)dy\\
    =~&\int_{\mathbb{Y}_{10}}\E[Y_1|Y_0=y,D=0, W]p(Y_0=y, W|D=1,X)dy\\
    =~& \E[g(Y_0,X)|D=1,X]
\end{align*}
\end{proof}

Similarly, we may also replace the parallel assumptions for IV-CATT by the conditional lagged dependent variable assumptions for both the outcome and treatment. 

\begin{assumption}[Lagged Treatment]\label{assum:lagged_Z_D}
    $$
    \E[D_{1}(0)|Z=0, W, D_{0}] = \E[D_{1}(0)|Z=1, W, D_{0}]
    $$
\end{assumption}
\begin{assumption}[Lagged Outcome]\label{assum:lagged_Z_Y}
    $$
    \E[Y_{1}(D_{1}(0)) |Z=0, W, Y_{0}] = \E[Y_{1}(D_{1}(0)) |Z=1, W, Y_{0}]
    $$
\end{assumption}
Under the lagged outcome framework, we need a slightly different notion of strong instruments as in the PTA framework.
\begin{assumption}[Strong Instrument under Lagged Outcome]\label{assum:strong_iv_lagged}
    There exist $c_z>0$ such that $$\left|\E[D_1 - \E[D_1|Z=1, W, D_0]|Z=1, X]\right| \geq c_z$$
\end{assumption}

\begin{proposition}\label{prop:IV-lagged}
    Under Assumptions \ref{assum:exo-iv}, \ref{Assum:no-defier}, \ref{assum:no-an-iv}, \ref{assum:lagged_Z_D}, and \ref{assum:lagged_Z_Y}, the CLATE, $\theta_0(W)$, can be identified as:
    \begin{align*}
        \theta_0(X) = \frac{\E[Y_1 - \E[Y_1\mid Z=0, Y_0, W]\mid Z=1,X]}{\E[D_1 - \E[D_1 \mid Z=0, D_0, W]\mid Z=1,X]}
    \end{align*}
    % Alternatively, under Assumptions \ref{assum:exo-iv}, \ref{Assum:no-defier}, \ref{assum:no-an-iv}, \ref{assum:lagged_Z_D}, and \ref{assum:lagged_Z_Y}, the CLATE, $\tau(W)$, can be identified as:
    % \begin{align}
    %     \tau(W) = \frac{\E[Y_1 - \E[Y_1\mid Z=0, X, Y_0]\mid Z=1,W]}{\E[D_1 - \E[D_1\mid Z=0, X, D_0]\mid Z=1,W]}
    % \end{align}
\end{proposition}

\textbf{Unifying the two assumptions:} First we observe that both Proposition \ref{prop:did} and \ref{prop:lagged} shares the same general form:
\begin{align*}
    \theta_0(X) = \E[S - g(V)|D=1, X]
\end{align*} where $S$ is an observed outcome random variable and $g(V)$ is a nuisance function that is the conditional expectation $\E[S|D=0, V]$ on some covariates $V$, which is a superset of $X$.
Under the parallel trends assumption, $S = Y_1-Y_0$ and $V = W$, and under the lagged outcome assumption, $S = Y_1$ and $V = [W, Y_0]$. Thus, we see that the results in Section \ref{sec:catt} can be generalized to the lagged dependent variable assumption. For IV-DID, similarly to the standard DiD case, when considering $X \subset W$, we can rewrite the identification in Proposition \ref{prop:IV} so that the CLATE is the solution as:
\begin{align*}
    \theta_0(X) = \frac{ \E[S_Y - g_{Y}(V_Y)\mid Z=1, X]}{ \E[S_D - g_{D}(V_D)\mid Z=1, X]}
\end{align*} where $S_r$ is an observed outcome random variable for $r=Y,D$, and $g_r(V)$ is a nuisance function that is the conditional expectation $\E[S_r|Z=0, V]$ on some covariates $V$, which is a superset of $X$.


\section{Conditioning on the full set of $W$}\label{appen:full_x}
\subsection{Standard DID}
Here, we consider the case where $X=W$, and are interested in estimating:

$$
\theta_0(W) = \E[Y_{1}(1) - Y_{1}(0)| D_1=1, W]
$$

Leveraging the no-anticipation and (conditional) parallel trends assumption, we can identify this as:
$$
\theta_0(W)  = \E[Y_{1} - Y_{0}| D_{1}=1, W] - \E[Y_{1}-Y_{0}| D_{1}=0, W]
$$
Note that this shares the same form of identification with the conditional average treatment effect (CATE) under conditional ignorability, but with the differences as the outcome:
$$
CATE = \E[Y(1)-Y(0)|W] = \E[Y|W, D=1] - \E[Y|W,D=0]
$$
In other words, the meta-learners of CATT can be constructed the same way that was constructed for CATE using the difference in outcome. For instance, the doubly-robust pseudo-outcome can be constructed as:
$$
Y^{DR} = g(1,W) - g(0,W) + \left(\frac{D}{\pi(W)} - \frac{1-D}{1-\pi(W)}\right)(Y_1 - Y_0 - g(D,W))
$$ 
where $g(D,W)$ is an estimator for the conditional expectation $\E[Y_{1}-Y_{0}|W,D]$, and $\pi(W)$ is an estimator of the propensity $\mathbb{P}(D=1|W)$. The nuisance functions $g(D,W)$ and $\pi(W)$ may be estimated using any ML methods. 

As in \cite{DR-learner}, the doubly robust learner (DR-learner) can be constructed as $ \theta(W) = \E[Y^{DR}|W]$. Note that due to the asymmetry of the parallel trends assumption, this estimator gives the conditional average treatment effect of the treated. If we further assume that the treatment effects are also mean independent of the treatment conditional on $W$ (i.e. $\E[Y_1(1) - Y_1(0)|D=1,W]=\E[Y_1(1) - Y_1(0)|D=0,W]$), then the CATE estimator on the difference in the outcomes identifies the conditional average treatment effects.

However, this CATE pseudo-outcome will give the biased estimate of the CATT when projecting on a subset of covariates, i.e. $\theta^{CATE}(X) = \E[Y^{DR}|X]$ where $X\subset W$. Here we see that $\theta^{CATE}(X) = \E[\E[Y^{DR}|W]|X] = \E[\E[Y_{1}(1) - Y_{1}(0)| D=1, W]|X] \neq \E[\E[Y_{1}(1) - Y_{1}(0)| D=1, W]| D=1, X] = \E[Y_{1}(1) - Y_{1}(0)| D_1=1, X]$. This difference will be more pronounced for datasets where there is a big difference in the covariate distribution between the treated and un-treated groups. 

\subsection{IV-DID}
In this section, we show that when conditioning on the full set of variables $W$, the CLATE can be estimated by the DR-IV learner in \cite{DR-IV}. By the standard LATE identification argument we can write:
\begin{align*}
    \E[Y_1(1)-Y_1(0)\mid D_1(1)>D_1(0), Z=1, W]
    =~& \frac{\E[(Y_1(1)-Y_1(0))\, 1\{D_1(1)>D_1(0)\}\mid Z=1, W]}{\mathbb{P}(D_1(1)>D_1(0)\mid Z=1, W)}\\
    =~& \frac{\E[(Y_1(D_1(1))-Y_1(D_1(0)))\, 1\{D_1(1)>D_1(0)\}\mid Z=1, W]}{\E[D_1(1) - D_1(0)\mid Z=1, W]}\\
    =~& \frac{\E[Y_1(D_1(1))-Y_1(D_1(0))\mid Z=1, W]}{\E[D_1(1)-D_1(0)\mid Z=1, W]}
\end{align*}
Under the parallel trends assumption in the outcome, the numerator is identified as:
\begin{align*}
    \E[Y_1(D_1(1))-Y_1(D_1(0))\mid Z=1, W] 
    =~& \E[Y_1 - Y_0\mid Z=1, W] - \E[Y_1(D(0)) - Y_0\mid Z=1, W]\\
    =~& \E[Y_1 - Y_0\mid Z=1, W] - \E[Y_1 - Y_0\mid Z=0, W]\\
\end{align*}
Moreover, under the parallel trends assumption in the treatment, the denominator is identified as:
\begin{align*}
    \E[D_1(1)-D_1(0)\mid Z=1, W] 
    =~& \E[D_1 - D_0\mid Z=1, W] - \E[D_1(0) - D_0\mid Z=1, W]\\
    =~& \E[D_1 - D_0\mid Z=1, W] - \E[D_1 - D_0\mid Z=1, W]\\
\end{align*}

For brevity of notation, let $Y$ denote $Y_1-Y_0$ and let $D$ denote $D_1-D_0$Thus, under the PTA assumptions the effect is identified as:
\begin{align*}
    \theta_0(W) 
    ~&= \frac{\E[Y\mid Z=1, W] - \E[Y\mid Z=0, W]}{\E[D\mid Z=1, W] -  \E[D\mid Z=0, W]}
\end{align*}
Moreover, note that we can also write these quantities as conditional covariances. 

In particular, let $\alpha(W) = \E[Y\mid Z=1, W] - \E[Y\mid Z=0, W]$ and $\gamma(W) = \E[Y\mid Z=0, W]$. Without loss of generality we can write:
\begin{align*}
    \E[Y\mid Z, W] = Z (\E[Y\mid Z=1, W] - \E[Y\mid Z=0, W]) + \E[Y\mid Z=0, W]  = Z \alpha(W) + \gamma(W)
\end{align*}
Thus we can write:
\begin{align*}
    Y =~& Z \alpha(W) + \gamma(W) + \epsilon, & \E[\epsilon \mid Z, W] = 0
\end{align*}
Then we have:
\begin{align*}
    \text{Cov}(Y, Z\mid W) = \E[\tilde{Y} \tilde{Z}\mid W] = \E[Y \tilde{Z} \mid W]
\end{align*}
where $\tilde{Y} = Y - \E[Y\mid W]$ and $\tilde{Z}=Z-\pi_0(W)=Z-\E[Z\mid W]$.

Moreover, note that:
\begin{align*}
    \E[Y \tilde{Z} \mid W] =~& \E[\alpha(W) Z \tilde{Z}] + \E[\gamma(W) \tilde{Z}\mid W] + \E[\epsilon \tilde{Z}\mid W]\\
    =~& \alpha(W) \text{Var}(Z\mid W) + \gamma(W) \E[\tilde{Z}\mid W] + \E[\E[\epsilon\mid Z, W] \tilde{Z}\mid W] \\
    =~& \alpha(W) \text{Var}(Z\mid W)
\end{align*}
Thus we have:
\begin{align*}
    \text{Cov}(Y, Z\mid W) = \E[Y\tilde{Z}\mid W] = (\E[Y\mid Z=1, W] - \E[Y\mid Z=0, W])\, \Var(Z\mid W)
\end{align*}

Similarly, we can derive:
\begin{align*}
    \text{Cov}(D, Z\mid W) = \E[D\tilde{Z}\mid W] = (\E[D\mid Z=1, W] - \E[D\mid Z=0, W])\,\Var(Z\mid W)
\end{align*}

Thus we have deduced that we can equivalently identify the conditional LATE among the exposed as:
\begin{align*}
    \theta_0(W) ~&= \frac{\E[Y \tilde{Z}\mid W]}{\E[D \tilde{Z}\mid W]} = \frac{\text{Cov}(Y,Z\mid W)}{\text{Cov}(D, Z\mid W)} =\frac{(\E[Y\mid Z=1, W] - \E[Y\mid Z=0, W])\,\Var(Z\mid W)}{(\E[D\mid Z=1, W] - \E[D\mid Z=0, W])\, \Var(Z\mid W)}\\
    ~&= \frac{\E[Y\mid Z=1, W] - \E[Y\mid Z=0, W]}{\E[D\mid Z=1, W] - \E[D\mid Z=0, W]}
\end{align*}

Let $\hat{\alpha}$ be an estimate of:
\begin{align*}
    a_0(W) ~&:= \E[Y \tilde{Z}\mid W] 
    = (\E[Y\mid Z=1, W] - \E[Y\mid Z=0, W])\, \Var(Z\mid W)
\end{align*}
and $\hat{\beta}$ an estimate of:
\begin{align*}
    \beta_0(W) ~&:= \E[D \tilde{Z}\mid W] 
    = (\E[D\mid Z=1, W] - \E[D\mid Z=0, W])\, \Var(Z\mid W)
\end{align*}
and let $\hat{\theta}=\hat{\alpha}/\hat{\beta}$. Then we can construct the random variable
\begin{align*}
    \hat{Y}(\hat{g}) = \hat{\theta}(W) + \frac{(Y - \hat{\theta}(W) D) \tilde{Z}}{\hat{\beta}(W)}
\end{align*} 
and the moment equation for the conditional LATT is:
\begin{align*}
    \phi = \E[\hat{Y}(\hat{g}) - \theta(W)|W]
\end{align*}

Similar to the standard DiD case, projecting onto a lower dimensional subset of covariates will give a biased estimated of the CLATE.

\section{Proofs}
\subsection{Identification}
\begin{proof}[Proof of Proposition \ref{prop:did}]
\begin{align*}
    ~&\E[Y_{1}(1) - Y_{1}(0)|D=1, X]\\
    =~& \E[Y_1(1) - Y_0(1)|D=1,X] - \E[Y_1(0) - Y_0(1)|D=1,X]\\
    =~& \E[Y_1 - Y_0|D=1, X] - \E[Y_1(0) - Y_0(0)|D=1,X]\tag{\text{By Assumption~\ref{assum:no-an}}} \\
    =~& \E[Y_1 - Y_0|D=1, X] - \E\{\E[Y_1(0) - Y_0(0)|D=1,W]|D=1, X\}\\
    =~& \E[Y_1 - Y_0|D=1, X] - \E\{\E[Y_1(0) - Y_0(0)|D=0,W]|D=1, X\} \tag{\text{By Assumption~\ref{assum:cpta}}}\\
    =~& \E[Y_1 - Y_0 - \E[Y_1(0) - Y_0(0)|D=0,W] | D=1, X] \label{eq:iden_sub}
\end{align*}
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:IV}]
    Here we want to show that the CLATE, $\theta_0(X)$, can be identified as:
    \begin{align*}
        \theta_0(X) = \frac{\E[Y_1 - Y_0 - \E[Y_1 - Y_0\mid Z=0, W]\mid Z=1,X]}{\E[D_1 - D_0 - \E[D_1 - D_0\mid Z=0, W]\mid Z=1,X]}
    \end{align*}

    We first analyze the denominator:
    \begin{align*}
        ~&\E[D_1 - D_0 - \E[D_1 - D_0\mid Z=0, X]\mid Z=1,X]\\
        =~& \E[D_1(1) - D_0(1) - \E[D_1(0) - D_0(0)\mid Z=0, W]\mid Z=1,X] \\
        =~& \E[D_1(1) -D_1(0) + D_1(0)- D_0(1) - \E[D_1(0) - D_0(0)\mid Z=0, W]\mid Z=1,X] \\
        =~& \E[D_1(1) -D_1(0) + D_1(0)- D_0(0) - \E[D_1(0) - D_0(0)\mid Z=0, W]\mid Z=1,X] \tag{By Assumption \ref{assum:no-an-iv}}\\
        =~& \E[D_1(1) -D_1(0) + \E[D_1(0)- D_0(0)|Z=1,W] - \E[D_1(0) - D_0(0)\mid Z=0, W]\mid Z=1,X] \\
        =~& \E[D_1(1) -D_1(0) \mid Z=1,X] \tag{By Assumption \ref{assum:PTA_Z_D}}\\
        =~& \mathbb{P}(D_1(1) >D_1(0)|Z=1, X) \tag{By Assumption \ref{Assum:no-defier}}
    \end{align*}

    Now we analyze the numerator:
    \begin{align*}
        ~&\E[Y_1 - Y_0 - \E[Y_1 - Y_0\mid Z=0, W]\mid Z=1,X]\\
        =~& \E[Y_1(D(1)) - Y_0(D(1)) - \E[Y_1(D(0)) - Y_0(D(0))\mid Z=0, W]\mid Z=1,X]\\
        =~& \E[Y_1(D(1)) -Y_1(D(0)) + Y_1(D(0)) - Y_0(D(0)) - \E[Y_1(D(0)) - Y_0(D(0))\mid Z=0, W]\mid Z=1,X] \tag{By Assumption \ref{assum:no-an-iv}}\\
        =~& \E[Y_1(D(1)) -Y_1(D(0)) + \E[Y_1(D(0)) - Y_0(D(0))|Z=1, W] - \E[Y_1(D(0)) - Y_0(D(0))\mid Z=0, W]\mid Z=1,X] \\
        =~& \E[Y_1(D(1)) -Y_1(D(0))\mid Z=1,X] \tag{By Assumption \ref{assum:PTA_Z_Y}}\\
        =~& \E[(D(1) - D(0))(Y_1(1) -Y_1(0))\mid Z=1,X] \tag{By Assumption \ref{Assum:no-defier}}\\
        =~& \E[Y_1(1) -Y_1(0)\mid Z=1,D(1)>D(0),X]\mathbb{P}(D(1)>D(0)|Z=1,X)
    \end{align*}
    Thus, combining them, we get:
    \begin{align*}
        \frac{\E[Y_1 - Y_0 - \E[Y_1 - Y_0\mid Z=0, W]\mid Z=1,X]}{\E[D_1 - D_0 - \E[D_1 - D_0\mid Z=0, W]\mid Z=1,X]} = \E[Y_1(1) -Y_1(0)\mid Z=1,D(1)>D(0),X]
    \end{align*}
\end{proof}

\subsection{Orthogonal Moments}
\begin{proof}[Proof of Lemma \ref{lemma:ortho_moment}]
First, we show that the true CATT function $\theta_0(X) = \E[Y_1(1) - Y_1(0)|D=1, X]$ is the solution to the moment:
\begin{align*}
    \E\left[m(Z; \theta_0, g_0, \pi_0)|X\right] = \E\left[\left(\frac{D-\pi_0(W)}{(1-\pi_0(W))}\right)(\Delta Y - g_0(W)) - D\theta_0(X) \,\middle\vert\, X\right] =0
\end{align*}
where $Z = (W,D,Y)$, $\Delta Y = Y_1 - Y_0$, $g_0(W) = \E[\Delta Y|D=0,W]$, $\pi_0(W) = \mathbb{P}(D=1|W)$.
First, since this moment is conditioned on $X$, we can multiply by any functions of $X$. Thus, we can divide by the propensity with $X$, i.e. $\gamma_0(X) = \mathbb{P}(D=1|X)$, which is bounded away from zero:
\begin{gather*}
    \E\left[\left(\frac{D-\pi_0(W)}{(1-\pi_0(W))}\right)(\Delta Y - g_0(W)) - D\theta_0(X) \,\middle\vert\, X\right] = 0\\
    \Updownarrow\\
     \E\left[\left(\frac{D-\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}\right)(\Delta Y - g_0(W)) - \frac{D}{\gamma_0(X)}\theta_0(X) \,\middle\vert\, X\right] = 0
\end{gather*}
The latter term is $\E\left[\frac{D}{\gamma_0(X)}\theta_0(X) \,\middle\vert\, X\right] = \E[\theta_0(X)|D=1, X] = \theta_0(X)$. Now we consider the first term:
\begin{align*}
    ~&\E\left[\left(\frac{D-\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}\right)(\Delta Y - g_0(W)) \,\middle\vert\, X\right]\\
    =~& \E\left[\left(\frac{D}{\gamma_0(X)} - \frac{(1-D)\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}\right)(\Delta Y - g_0(W)) \,\middle\vert\, X\right]\\
    =~&\E\left[\frac{D}{\gamma_0(X)}(\Delta Y - g_0(W)) \,\middle\vert\, X\right] - \E\left[\frac{(1-D)\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}(\Delta Y - g_0(W)) \,\middle\vert\, X\right]\\
    =~&\E\left[\Delta Y - g_0(W)\,\middle\vert\, D=1, X\right] - \E\left[\E\left[\frac{(1-D)\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}(\Delta Y - g_0(W)) \,\middle\vert\, W\right]\,\middle\vert\, X\right]\\
    =~& \theta_0(X) - \E\left[\E\left[\frac{\pi_0(W)}{\gamma_0(X)}(\Delta Y - g_0(W)) \,\middle\vert\, D=0, W\right]\,\middle\vert\, X\right]\\
    =~& \theta_0(X) - \E\left[\frac{\pi_0(W)}{\gamma_0(X)}\E\left[\Delta Y - g_0(W)\,\middle\vert\, D=0, W\right]\,\middle\vert\, X\right]\\
    =~& \theta_0(X)
\end{align*}
Thus, the moment condition is satisfied for the true CATT $\theta_0(X)$.
Now, we show that the moment is Neyman orthogonal with respect to all nuisance functions. It suffices to show that the directional derivative with respect to all the nuisance functions are $0$ when evaluated at the true nuisance and target functions. Recall that the directional derivative of a functional $m(Z;f)$ with respect to the function $f(W)$ in the direction of $\Delta f(W)$ is defined as: $\partial_{f}\E[m(z;f)][\Delta f] = \frac{d}{dt}\E[m(z;f+t\cdot \Delta f)]\Big|_{t=0}$.

First, we look the directional derivative with respect to the outcome regression $g(W)$:
\begin{align*}
    \partial_{g}\E[m(Z;\theta, g,\pi)|X][\Delta g]|_{\theta_0, g_0, \pi_0} =~& \E\left[\frac{D-\pi_0(W)}{1-\pi_0(W)}\Delta g(W)\,\middle\vert\, X\right]\\
    =~&\E\left[\E\left[\frac{D-\pi_0(W)}{1-\pi_0(W)}\,\middle\vert\, W\right]\Delta g(W)\,\middle\vert\, X\right]\\
    =~&\E\left[\frac{\pi_0(W)-\pi_0(W)}{1-\pi_0(W)}\Delta g(W)\,\middle\vert\, X\right]=0\\
\end{align*}
Now, we look at the the directional derivative with respect to the outcome regression $\pi(W)$:
\begin{align*}
    \partial_{\pi}\E[m(Z;\theta, g,\pi)|X][\Delta \pi]|_{\theta_0, g_0, \pi_0} =~& \E\left[\left(\frac{-(1-\pi_0(W))\Delta \pi(W) + (D - \pi_0(W))\Delta \pi(W)}{(1-\pi_0(W))^2}\right)(\Delta Y - g_0(W))\,\middle\vert\, X\right]\\
    =~& \E\left[\left(\frac{\Delta \pi(W) (D-1)}{(1-\pi_0(W))^2}\right)(\Delta Y - g_0(W))\,\middle\vert\, X\right]\\
    =~& \E\left[\E\left[\left(\frac{\Delta \pi(W) (D-1)}{(1-\pi_0(W))^2}\right)(\Delta Y - g_0(W))\,\middle\vert\,W\right]\,\middle\vert\, X\right]\\
    =~&\E\left[-\left(\frac{\Delta \pi(W)}{1-\pi_0(W)}\right)\E\left[\Delta Y - g_0(W))\,\middle\vert\,D=0,W\right]\,\middle\vert\, X\right] = 0\\
\end{align*}
Thus, we have shown that this moment is Neyman orthogonal with respect to all nuisances.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:gen_cov_shift}]
 First, we show that the true estimand $\theta_0(X) = \E_{source}[m(Z;g_0)|X]$ satisfies the following conditional moment restriction \begin{align*}
    \E\Big[ m^{DR}(Z; \theta_0, g_0, \pi_0, \alpha_0)\Big|X\Big] = \E\left[E(m(Z;g_0)) - \theta_0(X)) + \frac{(1-E)\pi_0(W)}{1-\pi_0(W)}\alpha_0(W)(Y-g_0(W)) \,\middle\vert\, X \right] = 0
\end{align*}
where $\pi_0(W) = \mathbb{P}(E=1|W)$ and $\alpha(W)$ is the Riesz representer of $\E_{s}[m(Z;g)|X]$.
Similar to the earlier the proof of Lemma \ref{lemma:ortho_moment}, we can divide both sides of the moment equation by $\gamma_0(X) = \mathbb{P}(E=1|X)$ since it is bounded away from $0$. So it is equivalent to show:
\begin{align*}
    \E\left[\frac{E}{\gamma_0(X)}(m(Z;g_0)) - \theta_0(X)) + \frac{(1-E)\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}\alpha_0(W)(Y-g_0(W)) \,\middle\vert\, X \right] = 0
\end{align*}
First, let's look at the first term:
\begin{align*}
    \E\left[\frac{E}{\gamma_0(X)}(m(Z;g_0)) - \theta_0(X))\,\middle\vert\, X \right] = \E\left[(m(Z;g_0)) - \theta_0(X))\,\middle\vert\, E=1, X \right] = 0
\end{align*}
Thus, it remains to show that the second term also has conditional expectation of $0$.
\begin{align*}
    ~&\E\left[ \frac{(1-E)\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}\alpha_0(W)(Y-g_0(W)) \,\middle\vert\, X \right]\\
    =~& \E\left[ \E\left[\frac{(1-E)\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}\alpha_0(W)(Y-g_0(W)) \,\middle\vert\, W \right]\,\middle\vert\, X \right]\\
    =~& \E\left[ \E\left[\frac{\pi_0(W)}{\gamma_0(X)}\alpha_0(W)(Y-g_0(W)) \,\middle\vert\,E=0, W\right]\,\middle\vert\, X \right]\\
    =~& \E\left[ \frac{\pi_0(W)}{\gamma_0(X)}\alpha_0(W)\E\left[(Y-g_0(W)) \,\middle\vert\,E=0, W\right]\,\middle\vert\, X \right]=0\\
\end{align*}
Now, we proceed to show that the moment $ m^{DR}(Z;\theta, g, \pi, \alpha)$ is Neyman orthogonal. First, we look at the directional derivative with respect to the nuisance $g(W)$.
\begin{align*}
    ~&\partial_{g}\E[m^{DR}(Z;\theta, g,\pi, \alpha)|X][\Delta g]|_{\theta_0, g_0, \pi_0, \alpha_0}\\
    =~& \partial_{g}\E[Em(Z;g)|X][\Delta g]|_{ g_0} - \E\left[ \frac{(1-E)\pi_0(W)}{1-\pi_0(W)}\alpha_0(W)\Delta g(W) \,\middle\vert\, X \right]\\
\end{align*}
We first look at the first term:
\begin{align*}
    \partial_{g}\E[Em(Z;g)|X][\Delta g]\Big|_{g_0} = ~& \partial_{g}\E[\gamma_0(W)\E[m(Z;g)|E=1, X]|X][\Delta g]\Big|_{g_0}\\
    = ~& \partial_{g}\E[\gamma_0(X)\E[\alpha_0(W)g(W)|E=1, X]|X][\Delta g]\Big|_{g_0} \tag{By the Definition of $\alpha_0(W)$}\\
    = ~& \E[\gamma_0(X)\E[\alpha_0(W)\Delta g(W)|E=1, X]|X]\\
    = ~& \E[E\alpha_0(W)\Delta g(W)|W]\\
\end{align*}
Putting this back, we get:
\begin{align*}
    ~&\partial_{g}\E[m^{DR}(Z;\theta, g,\pi)|X][\Delta g]|_{\theta_0, g_0, \pi_0, \alpha_0}\\
    =~& \E\left[E\alpha_0(W)\Delta g(W) - \frac{(1-E)\pi_0(W)}{1-\pi_0(W)}\alpha_0(W)\Delta g(W) \,\middle\vert\, X \right]\\
    =~& \E\left[\alpha_0(W)\Delta g(W)\left(E - \frac{(1-E)\pi_0(W)}{1-\pi_0(W)}\right)\,\middle\vert\, X \right]\\
    =~& \E\left[\alpha_0(W)\Delta g(W)\E\left[E - \frac{(1-E)\pi_0(W)}{1-\pi_0(W)}\,\middle\vert\,W\right]\,\middle\vert\, X\right] = 0\\
\end{align*}
Next, we look at the derivative with respect to $\pi(W)$:
\begin{align*}
    ~&\partial_{\pi}\E[m^{DR}(Z;\theta, g,\pi, \alpha)|X][\Delta \pi]|_{\theta_0, g_0, \pi_0, \alpha_0}\\
    =~& \E\left[ \left(\frac{(1-E)(1-\pi(W))\Delta \pi(W) + (1-E)\pi(W)\Delta \pi(W)}{(1-\pi_0(W))^2}\right)\alpha_0(W)( Y -g_0(W) )\,\middle\vert\, X \right]\\
    =~& \E\left[ \frac{(1-E)\Delta \pi(W)}{(1-\pi_0(W))^2}\alpha_0(W)( Y -g_0(W) )\,\middle\vert\, X\right]\\
    =~& \E\left[ \E\left[ \frac{(1-E)\Delta \pi(W)}{(1-\pi_0(W))^2}\alpha_0(W)( Y -g_0(W)) \,\middle\vert\, W \right]\,\middle\vert\, X \right]\\
    =~& \E\left[ \E\left[ \frac{\Delta \pi(W)}{1-\pi_0(W)}\alpha_0(W)( Y -g_0(W) )\,\middle\vert\, E=0, W \right]\,\middle\vert\, X \right]\\
    =~& \E\left[ \frac{\Delta \pi(W)}{1-\pi_0(W)}\alpha_0(W)\E\left[ ( Y -g_0(W) \,\middle\vert\, E=0, W \right]\,\middle\vert\, X \right] = 0
\end{align*}

Lastly, we show that the directional derivative with respect to $\alpha(W)$ is equal to 0.
\begin{align*}
    ~&\partial_{\alpha}\E[m^{DR}(Z;\theta, g,\pi,\alpha)|X][\Delta \alpha]|_{\theta_0, g_0, \pi_0, \alpha_0}\\
    =~& \E\left[ \frac{(1-E)\pi_0(W)}{1-\pi_0(W)}\Delta\alpha(W)( Y - g_0(W)) \,\middle\vert\, X \right]\\
    =~& \E\left[ \E\left[\frac{(1-E)\pi_0(W)}{1-\pi_0(W)}\Delta\alpha(W)( Y - g_0(W)) \,\middle\vert\, W \right]\,\middle\vert\, X \right]\\
    =~& \E\left[ \E\left[\pi_0(W)\Delta\alpha(W)( Y - g_0(W)) \,\middle\vert\,E=0, W \right]\,\middle\vert\, X \right]\\
    =~& \E\left[ \pi_0(W)\Delta\alpha(W)\E\left[ Y - g_0(W)\,\middle\vert\,E=0, W \right]\,\middle\vert\, X \right]=0
\end{align*}

\end{proof}

\begin{proof}[Proof of Lemma \ref{lemma:ortho_moment_IV}]
    First we show that the true CLATE, $\theta_0(X) = \E[Y_1(1) -Y_1(0)\mid Z=1,D(1)>D(0),X]$ , is the solution to the following moment equation:
    \begin{align*}
    \E\left[m^{DR}(Z; \theta_0, g_{0,Y},g_{0,D}, \pi_0)|X\right] = \E\left[ \widehat{Z}\left\{(\Delta Y- g_{0,Y}(W)) - (\Delta D- g_{0,D}(W))\theta(X) \right\}\,\middle\vert\, X \right] = 0
\end{align*}
    where $\Delta S = S_1 - S_0$ for $S = Y$ or $D$, $g_{0,S}(W) = \E[S_1 - S_0|Z=0,W]$, and $\widehat{Z} = \frac{Z-\pi_0(W)}{1-\pi_0(W)}$ with $\pi_0(W) = \mathbb{P}(Z=1|W)$.
    We can apply same trick as in the other orthogonality proofs to divide by $\gamma_0(X) = \mathbb{P}(Z=1|X)$.
    We first consider the first term:
    \begin{align*}
        ~&\E\left[ \frac{\widehat{Z}}{\gamma_0(X)}(\Delta Y- g_{0,Y}(W))\,\middle\vert\, X \right]\\
        =~& \E\left[ \frac{Z-\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}(\Delta Y- g_{0,Y}(W))\,\middle\vert\, X \right]\\
        =~& \E\left[ \left(\frac{Z}{\gamma_0(X)} - \frac{(1-Z)\pi_0(W)}{(1-\pi_0(W))\gamma_0(X)}\right)(\Delta Y- g_{0,Y}(W))\,\middle\vert\, X \right]\\
        =~& \E\left[\left(\frac{Z}{\gamma_0(X)}\right)(\Delta Y- g_{0,Y}(W))\,\middle\vert\, X \right]- \E\left[ \frac{\pi_0(W)}{\gamma_0(X)}\E\left[\left(\frac{1-Z}{(1-\pi_0(W)))}\right)(\Delta Y- g_{0,Y}(W))\,\middle\vert\, W \right]\,\middle\vert\, X\right]\\
        =~& \E\left[\Delta Y- g_{0,Y}(W)\,\middle\vert\,Z=1, X \right] - \E\left[ \frac{\pi_0(W)}{\gamma_0(X)} \E\left[(\Delta Y- g_{0,Y}(W))\,\middle\vert\, Z=0, W \right]\,\middle\vert\, X \right]\\
        =~& \E\left[\Delta Y- g_{0,Y}(W)\,\middle\vert\,Z=1, X \right]
    \end{align*}
    Similarly, for the second term:
    \begin{align*}
        \E\left[ \frac{\widehat{Z}}{\gamma_0(X)}(\Delta D- g_{0,D}(W))\theta(X) \,\middle\vert\, X \right]=~& \theta_0(X)\E\left[ \frac{\widehat{Z}}{\gamma_0(X)}(\Delta D- g_{0,D}(W)) \,\middle\vert\, X \right]\\
        =~& \theta_0(X)\E\left[\Delta D- g_{0,D}(W)\,\middle\vert\,Z=1, X \right]
    \end{align*}
    By the definition of $\theta_0(X)$, this shows that it is a solution to the doubly robust moment equation.
    Now, we proceed to show that the moment $m^{DR}(Z; \theta, g_{Y},g_{D}, \pi)$ is Neyman orthogonal. First, we look at the directional derivative with respect to the nuisance $g_Y(W)$.
    \begin{align*}
        \partial_{g_Y}\E[m^{DR}(Z;\theta, g_{Y},g_{D}, \pi)|X][\Delta g_Y]\Big|_{\theta_0, g_{0,Y},g_{0,D}, \pi_0}
        =~&- E\left[ \widehat{Z}\Delta g_Y(W)\,\middle\vert\, X \right]\\
        =~& - \E\left[ \frac{Z-\pi_0(W)}{1-\pi_0(W)}\Delta g_{Y}(W)\,\middle\vert\, X \right]\\
        =~& - \E\left[ \E\left[\frac{Z-\pi_0(W)}{1-\pi_0(W)}\,\middle\vert\, W\right]\Delta g_{Y}(W)\,\middle\vert\, X \right]= 0
    \end{align*}
    Similarly, 
    \begin{align*}
        \partial_{g_D}\E[m^{DR}(Z;\theta, g_{Y},g_{D}, \pi)|X][\Delta g_D]\Big|_{\theta_0, g_{0,Y},g_{0,D}, \pi_0}
        =~& E\left[ \widehat{Z}\Delta g_D(W)\theta_0(X)\,\middle\vert\, X\right]\\
        =~& \theta_0(X)\E\left[ \E\left[\frac{Z-\pi_0(W)}{1-\pi_0(W)}\,\middle\vert\, W \right]\Delta g_{D}(W)\,\middle\vert\, X\right]= 0
    \end{align*}
    Lastly, we check the directional derivative with respect to $\pi(W)$:
    \begin{align*}
        ~&\partial_{\pi}\E[m^{DR}(Z;\theta, g_{Y},g_{D}, \pi)|X][\Delta \pi]\Big|_{\theta_0, g_{0,Y},g_{0,D}, \pi_0}\\
        =~& \E\left[ \frac{-(1-\pi_0(W)\Delta \pi(W) + (Z-\pi_0(W)\Delta \pi(W)}{(1-\pi_0(W))^2}\left\{(\Delta Y- g_{0,Y}(W)) - (\Delta D- g_{0,D}(W))\theta(X) \right\}\,\middle\vert\, X \right]\\
        =~& \E\left[ \frac{(Z-1)\Delta \pi(W)}{(1-\pi_0(W))^2}\left\{(\Delta Y- g_{0,Y}(W)) - (\Delta D- g_{0,D}(W))\theta(X) \right\}\,\middle\vert\, X \right]\\
        =~& \E\left[  \E\left[\frac{(Z-1)\Delta \pi(W)}{(1-\pi_0(W))^2}\left\{(\Delta Y- g_{0,Y}(W)) - (\Delta D- g_{0,D}(W))\theta(X) \right\}\,\middle\vert\, W\right]\,\middle\vert\, X \right]\\
        =~& \E\left[  \E\left[\frac{(\Delta \pi(W)}{1-\pi_0(W)}\left\{(\Delta Y- g_{0,Y}(W)) - (\Delta D- g_{0,D}(W))\theta(X) \right\}\,\middle\vert\, Z=0, W\right]\,\middle\vert\, X \right]\\
        =~& \E\left[ \frac{(\Delta \pi(W)}{1-\pi_0(W)}\left\{E\left[\Delta Y- g_{0,Y}(W)| Z=0, W\right]- \E\left[\Delta D- g_{0,D}(W)|Z=0, W\right]\theta(X) \right\}\,\middle\vert\, X \right] = 0\\
    \end{align*}
\end{proof}

\subsection{Losses}
\begin{proof}[Proof of Proposition \ref{prop:loss_catt}]
Note that the true CATT $\theta_0$ satisfies the conditional moment restrictions in Lemma~\ref{lemma:ortho_moment}, which imply that:
\begin{align*}
    \E[D\theta_0(X)\mid X] = \E[\hat{Y}\mid X]
\end{align*}
Hence, the loss $\mathcal{L}(\theta; \pi_0, g_0)$ at any function $\theta$ can be simplified as:
\begin{align*}
    \mathcal{L}(\theta; \pi_0, g_0) =~& \E\left[D\theta(X)^2 - 2 \widehat{Y}\theta(X)\right]\\
    =~& \E\left[D\theta(X)^2 - 2 \E[\widehat{Y}\mid X]\theta(X)\right]\\
    =~& \E\left[D \theta(X)^2 - 2 \E[D\theta_0(X)\mid X]\theta(X)\right]\\
    =~& \E\left[D\theta(X)^2 - 2 D \theta_0(X)\theta(X)\right]
\end{align*}
Note that when the loss is evaluated at $\theta_0$, then it takes the value $\E[-D\theta_0(X)^2]$.
Moreover, note that minimizing $\mathcal{L}(\theta; \pi_0, g_0)$ is equivalent to minimizing the difference $\mathcal{L}(\theta; \pi_0, g_0) - \mathcal{L}(\theta_0; \pi_0, g_0)$, which in turn simplifies to:
\begin{align*}
    \E\left[D\theta(X)^2 - 2 D\theta_0(X)\theta(X) + D\theta_0(X)^2\right]
    =\E\left[D(\theta(X) - \theta_0(X))^2\right]
\end{align*}
Hence, minimizing $\mathcal{L}(\theta; \pi_0, g_0)$ over any space $\Theta$ is equivalent to minimizing over $\Theta$ the loss function:
\begin{align*}
    \E\left[(\theta(X) - \theta_0(X))^2\mid D=1\right]
\end{align*}

\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:loss_clate}]
Note that the true CATT $\theta_0$ satisfies the conditional moment restrictions in Lemma~\ref{lemma:ortho_moment_IV}, which imply that:
\begin{align*}
    \E[\widehat{Z}(\Delta D - g_D(W))\theta_0(X)\mid X] = \E[\widehat{Z}(\Delta Y - g_Y(W))\mid X]
\end{align*}
Let $\eta_0$ denote the set of nuisance functions. The loss $\mathcal{L}_{IV}(\theta; \eta_0)$ at any function $\theta$ can be simplified as:
\begin{align*}
    \mathcal{L}_{IV}(\theta; \eta_0) =~& \E\left[\widehat{Z}(\Delta D - g_D(W))\theta(X)^2 - 2\widehat{Z}(\Delta D - g_D(W))\theta_0(X)\theta(X)\right]\\
    =~& \E\left[\widehat{Z}(\Delta D - g_D(W))\theta(X)^2 - 2 \E[\widehat{Z}(\Delta D - g_D(W))\theta_0(X)\mid X]\theta(X)\right]\\
    =~& \E\left[\widehat{Z}(\Delta D - g_D(W))\left(\theta(X)^2 - 2 \theta_0(X)\theta(X)\right)\right]
\end{align*}
Note that when the loss is evaluated at $\theta_0$, then it takes the value $\E[-\widehat{Z}(\Delta D - g_D(W))\theta_0(X)^2]$.
Moreover, note that minimizing $\mathcal{L}_{IV}(\theta; \eta_0)$ is equivalent to minimizing the difference $\mathcal{L}_{IV}(\theta; \eta_0) - \mathcal{L}_{IV}(\theta_0; \eta_0)$, which in turn simplifies to:
\begin{align*}
    ~&\E\left[\widehat{Z}(\Delta D - g_D(W))\left(\theta(X)^2 - 2\theta_0(X)\theta(X) + \theta_0(X)^2\right)\right]\\
    =~&\E\left[\widehat{Z}(\Delta D - g_D(W))(\theta(X) - \theta_0(X))^2\right]\\
    =~&\E\left[\left(Z - \frac{(1-Z)\pi_0(W)}{1-\pi_0(W)}\right)(\Delta D - g_D(W))(\theta(X) - \theta_0(X))^2\right]\\
    =~&\E\left[Z(\Delta D - g_D(W))(\theta(X) - \theta_0(X))^2\right] - \E\left[\pi_0(W)(\theta(X) - \theta_0(X))^2\E\left[(\Delta D - g_D(W))\,\middle\vert\,Z=0,W\right]\right]\\
    =~&\E\left[Z(\Delta D - g_D(W))(\theta(X) - \theta_0(X))^2\right]\\
    =~&\E\left[(\Delta D - g_D(W))(\theta(X) - \theta_0(X))^2|Z=1\right]\mathbb{P}(Z=1)\\
    =~&\E\left[\E\left[(\Delta D - g_D(W))|Z=1,X\right](\theta(X) - \theta_0(X))^2|Z=1\right]\mathbb{P}(Z=1)\\
    =~&\E\left[\mathbb{P}(D_1(1) >D_1(0)|Z=1, X) (\theta(X) - \theta_0(X))^2|Z=1\right]\mathbb{P}(Z=1) \tag{See the proof of Proposition \ref{prop:IV}}\\
    =~&\E\left[(D_1(1) >D_1(0)) (\theta(X) - \theta_0(X))^2|Z=1\right]\mathbb{P}(Z=1)\\
    =~&\E\left[(\theta(X) - \theta_0(X))^2|Z=1, D(1)>D(0)\right]\mathbb{P}(Z=1)\mathbb{P}(D(1)>D(0)|Z=1)\\
\end{align*}

Hence, minimizing $\mathcal{L}_{IV}(\theta; \eta_0)$ over any space $\Theta$ is equivalent to minimizing over $\Theta$ the loss function:
\begin{align*}
    \E\left[(\theta(X) - \theta_0(X))^2\mid Z=1, D(1)>D(0)\right]
\end{align*}
\end{proof}

\subsection{Rates}
Before proving Theorem \ref{thm:catt_rates}, we first present some auxiliary Lemmas.
\begin{lemma}\label{lemma:loss_difference}
    Let $\eta = (\pi,g)$ denote the set of nuisance functions, and let $\eta_0$ be the true nuisance functions. Consider the loss defined in Proposition~\ref{prop:loss_catt}. Then, we have that for all $\theta_1$, $\theta_2$, $\eta_1$ and $\eta_2$, $$|\mathcal{L}(\theta_1; \eta_1) - \mathcal{L}(\theta_2; \eta_1) - \mathcal{L}(\theta_2; \eta_1) + \mathcal{L}(\theta_2; \eta_2)| \leq 2\sqrt{\E\left[\E\left[\widehat{Y}(\eta_1) - \widehat{Y}(\eta_2)\Big|X\right]^2\right]} \|\theta_1-\theta_2\|$$.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:loss_difference}]
\begin{align*}
    ~&|\mathcal{L}(\theta_1; \eta_1) - \mathcal{L}(\theta_2; \eta_1) - \mathcal{L}(\theta_2; \eta_1) + \mathcal{L}(\theta_2; \eta_2)|\\
    =~& \left|\E\left[D(\theta_1^2(X) -\theta_2^2(X) ) + 2\widehat{Y}(\eta_1)(\theta_2(X) -\theta_1(X) \right] - \E\left[D(\theta_1^2(X) -\theta_2^2(X) ) + 2\widehat{Y}(\eta_2)(\theta_2(X)-\theta_1(X)) \right]\right|\\
    =~& \left|\E\left[ 2\left(\widehat{Y}(\eta_1) -\widehat{Y}(\eta_2)\right)(\theta_1(X) -\theta_2(X)) \right] \right|\\
    =~& 2\left|\E\left[\E\left[\left(\widehat{Y}(\eta_1) -\widehat{Y}(\eta_2)\right)(\theta_1(X) -\theta_2(X)) \,\middle\vert\,X\right]\right] \right|\\
    \leq ~& 2\sqrt{\E\left[\E\left[\widehat{Y}(\eta_1) - \widehat{Y}(\eta_2)\Big|X\right]^2\right]} \|\theta_1(X)-\theta_2(X)\|
\end{align*}
\end{proof}

% \vscomment{$c<\Pr(D=1\mid W) < 1-c$, $\rho=\Pr(D=1)$
% $$\beta = \rho + (1-\rho) \frac{1-c}{c} = \frac{\rho c + (1-\rho)(1-c)}{c} \leq \frac{1}{c}$$
% $$\frac{2}{\rho} R_n + \frac{1}{\rho} \frac{1}{c^2} \E[\E[(g-g_0)\frac{\pi-\pi_0}{1-\pi}\mid W]^2]$$
% }

We then show that the bias in the pseudo-outcome $\widehat{Y}$ is equal to the product of the biases in the nuisance functions.
\begin{lemma}\label{lemma:mixed_bias}
    Let $\eta = (\pi,g)$ denote the set of nuisance functions, and let $\eta_0$ be the true nuisance functions. Consider the pseudo-outcome defined in Proposition~\ref{prop:loss_catt}. Then we have:
    $$ \E[\widehat{Y}(\eta_0) - \widehat{Y}(\hat{\eta})|X] = \E\left[(\hat{g}(W) - g_0(W))\frac{\pi_0(W) - \hat{\pi}(W)}{1-\hat{\pi}(W)}\,\middle\vert\,X\right]$$
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:mixed_bias}]
    \begin{align*}
        ~&\E[\widehat{Y}(\eta_0) - \widehat{Y}(\hat{\eta})|X]\\
        =~&\E\left[\frac{D-\pi_0(W)}{1-\pi_0(W)}\left(\Delta Y - g_0(W)\right) - \frac{D-\hat{\pi}(W)}{1-\hat{\pi}(W)}\left(\Delta Y - \hat{g}(W)\right) \,\middle\vert\,X\right] \\
        =~&\E\left[\E\left[\left(D - \frac{(1-D)\pi_0(W)}{1-\pi_0(W)}\right)\left(\Delta Y - g_0(W)\right) - \left(D - \frac{(1-D)\hat{\pi}(W)}{1-\hat{\pi}(W)}\right)\left(\Delta Y - \hat{g}(W)\right) \,\middle\vert\,W\right]\,\middle\vert\,X\right]  \\
        =~&\E\left[\E\left[D(\hat{g}(W) - g_0(W))-  \frac{(1-D)\pi_0(W)}{1-\pi_0(W)}\left(\Delta Y - g_0(W)\right) +\frac{(1-D)\hat{\pi}(W)}{1-\hat{\pi}(W)}\left(\Delta Y - \hat{g}(W)\right) \,\middle\vert\,W\right] \,\middle\vert\,X\right]\\
        =~&\E\left[\E\left[\pi_0(W)(\hat{g}(W) - g_0(W))\,\middle\vert\,W\right] -\E\left[\Delta Y - g_0(W)\,\middle\vert\,D=0,W\right] +\E\left[\frac{(1-D)\hat{\pi}(W)}{1-\hat{\pi}(W)}\left(\Delta Y - \hat{g}(W)\right) \,\middle\vert\,W\right]\,\middle\vert\,X\right] \\
        =~& \E\left[\pi_0(W)(\hat{g}(W) - g_0(W))+\E\left[\frac{1-D}{1-\pi_0(W)}\frac{(1-\pi_0(W))\hat{\pi}(W)}{1-\hat{\pi}(W)}\left(\Delta Y - \hat{g}(W)\right) \,\middle\vert\,W\right] \,\middle\vert\,X\right]\\
        =~& \E\left[\pi_0(W)(\hat{g}(W) - g_0(W)) +\E\left[\frac{(1-\pi_0(W))\hat{\pi}(W)}{1-\hat{\pi}(W)}\left(\Delta Y - \hat{g}(W)\right) \,\middle\vert\,D=0, W\right]\,\middle\vert\,X\right] \\
        =~& \E\left[\pi_0(W)(\hat{g}(W) - g_0(W)) +\E\left[\frac{(1-\pi_0(W))\hat{\pi}(W)}{1-\hat{\pi}(W)}\left(g_0(W) - \hat{g}(W)\right) \,\middle\vert\,D=0, W\right] \,\middle\vert\,X\right]\\
        =~&\E\left[(\hat{g}(W) - g_0(W))\frac{\pi_0(W) - \hat{\pi}(W)}{1-\hat{\pi}(W)}\,\middle\vert\,X\right]
    \end{align*}
\end{proof}

The rates in Theorem~\ref{thm:catt_rates} is an application of Theorem 1 in \citealp{osl}. We reproduce the theorem in our notation for completeness. Let $d(\hat{\eta}, \eta_0)$ denote a distance metric for the function space of the nuisance functions $\mathcal{F}$, and $\|(\cdot)\|_{\Theta}$ denote a norm for $\Theta$. We denote $\text{Star}(\Theta,\theta)$ to be the star hull, i.e.$\text{Star}(\Theta,\theta) = \{t\theta + (1-t)\theta'| \,\forall \theta' \in \Theta, \, t\in [0,1]\}$. Moreover, let $\theta'$ be an arbitrary element in $\Theta$.
% \begin{assumption}[Orthogonal Loss] The population loss is Neyman Orthogonal, i.e. for all $\theta \in \Theta$ and $\eta \in \mathcal{F}$:
% $$
% \partial_{\theta}\partial_{\eta}\mathcal{L}(\theta;\eta)[\eta - \eta_0, \theta - \theta_0]
% $$
% \end{assumption}
\begin{assumption}[First Order Optimality]\label{assum:osl_1}
    $\theta'$ satisfies the first-order optimality condition for $\mathcal{L}(\theta;\eta_0)$:
    $$\partial_{\theta}\mathcal{L}(\theta;\eta_0)[\theta - \theta']\geq0 \quad \forall \quad \theta \in \text{Star}(\Theta, \theta')$$
\end{assumption}
\begin{assumption}[Higher Order Smoothness]\label{assum:osl_2} 
There exist constant $\beta_1$ such that:
$$
\partial_{\theta}^2 \mathcal{L}(\bar{\theta}, \eta_0)[\theta - \theta', \theta - \theta']\leq \beta_1\|\theta - \theta'\|^2_{\Theta} 
$$for all $\theta\in\Theta$ and all $\bar{\theta}\in\text{Star}(\Theta, \theta')$.


\end{assumption}
\begin{assumption}[Strong Convexity]\label{assum:osl_3}
    The population loss is strongly convex with respect to $\theta$, i.e. there exist constants $\lambda$, $\kappa>$0 and $r\geq0$, such that for all $\theta \in \Theta$, ${\theta'}\in \text{Star}(\Theta, \theta')$, and $\eta \in\mathcal{F}$:
    $$
    \partial_{\theta}^2\mathcal{L}(\bar{\theta}, \eta)[\theta-\theta', \theta-\theta']\geq \lambda \|\theta - \theta'\|^2 - \kappa d(\eta, \eta_0)^{\frac{4}{1+r}}
    $$
\end{assumption}
\begin{assumption}\label{assum:osl_4}
    There exist $r\in[0,1)$ and constant $\beta_2$ such that for all $\theta, {\theta'}\in \text{Star}(\Theta, \theta')$ and all $\eta_1$, $\eta_2$ in $\mathcal{F}$:
    $$
    \|\mathcal{L}(\theta; \eta_1) - \mathcal{L}(\theta'; \eta_1) - \mathcal{L}(\theta; \eta_2) + \mathcal{L}(\theta'; \eta_2)|\leq \beta_2 \|\theta - \theta'\|^{1-r}_{\Theta}d(\eta_1, \eta_2)^2
    $$
\end{assumption}
\begin{theorem}[Theorem 1 from \cite{osl}]\label{thm:osl}
    Suppose Assumptions~\ref{assum:osl_1}, \ref{assum:osl_2}, \ref{assum:osl_3}, and \ref{assum:osl_4} are satisfied for some $\theta'\in\Theta$. Then for any $\theta\in\Theta$, the following holds:
    $$
    \|\theta - \theta'\|_{\Theta}^2\leq \frac{4}{\lambda}\left(\mathcal{L}(\theta, \hat{\eta}) - \mathcal{L}(\theta', \hat{\eta})\right) + \left(\left(\frac{\beta_2}{\lambda}\right)^{\frac{2}{1+r}} + \frac{\kappa}{\lambda}\right)d(\eta_0, \hat{\eta})^{\frac{4}{1+r}}
    $$
\end{theorem}
We are finally ready to prove Theorem~\ref{thm:catt_rates}.
\begin{proof}[Proof of Theorem~\ref{thm:catt_rates}]
    Since results follow from Theorem \ref{thm:osl}, we first show that the minimizer of the loss in the function class $\Theta$, i.e. $\theta_*$, satisfies Assumptions~\ref{assum:osl_1}, \ref{assum:osl_2}, \ref{assum:osl_3}, and \ref{assum:osl_4} for the proposed loss $\mathcal{L}(\theta;\eta)$ with $\|(\cdot)\|_{\Theta} = \|(\cdot)\|_{D=1}$. 
    Assumption~\ref{assum:osl_1} is satisfied when $\Theta$ is convex or when $\theta_0\in\Theta$. 
    Assumptions ~\ref{assum:osl_2} and \ref{assum:osl_3} requires us to bound:
    \begin{align*}
        \partial_{\theta}^2 \mathcal{L}(\bar{\theta}, \hat{\eta})[\theta - \theta_*, \theta - \theta_*] = \E[D(\theta(X) - \theta_*(X))^2] = \rho \|(\theta(X) - \theta_*(X))\|_{D=1}^2
    \end{align*}
    Thus  Assumptions ~\ref{assum:osl_2} and \ref{assum:osl_3} are satisfied with $\beta_1 = \lambda = \rho$ and $\kappa = 0$. 
    To show Assumption \ref{assum:osl_4}, we need to convert the $\|(\cdot)\|_2$ in \ref{lemma:mixed_bias} into $\|(\cdot)\|_{D=1}$:
    \begin{align*}
        \|(\theta(X) - \theta_*(X))\|^2 
        =~& \int (\theta(X) - \theta_*(X))^2 \mathbb{P}p(X)dX\\
        =~& \int (\theta(X) - \theta_*(X))^2 \mathbb{P}(D=1|X)\frac{1}{\mathbb{P}(D=1|X)}p(X)dX\\
        \leq~& \frac{1}{c}\int (\theta(X) - \theta_*(X))^2 \mathbb{P}(D=1|X)p(X)dX\\
        =~& \frac{1}{c}\|\theta(X) - \theta_*(X)\|_{\Theta}^2\\
    \end{align*}
    Thus, Lemmas \ref{lemma:loss_difference} and \ref{lemma:mixed_bias} imply Assumption \ref{assum:osl_4} with $r=0$, $\beta_2 = \frac{3}{c}$, and $$d(\eta, \eta_0)^2 = \E\left[\E\left[(\hat{g}(W) - g_0(W))\left(\frac{\pi_0(W) - \hat{\pi}(W)}{1-\hat{\pi}(W)}\right)\,\middle\vert\,X\right]^2\right]^{1/2}$$

    Thus invoking Theorem \ref{thm:osl}, we get that:
    $$
    \|\theta - \theta'\|_{\Theta}^2\leq \frac{4}{\rho}R(n,\delta)+ \frac{2}{\rho^2 c^2}\E\left[\E\left[(\hat{g}(W) - g_0(W))\left(\frac{\pi_0(W) - \hat{\pi}(W)}{1-\hat{\pi}(W)}\right)\,\middle\vert\,X\right]^2\right]
    $$
\end{proof}

Analogously, we can prove the rates in the case with instrument. Consider $\mathcal{L}_{IV}(\theta;\eta)$ from Proposition ~\ref{prop:loss_clate}, where we let $\eta$ denote the set of nuisances $\pi(W)$, $g_D(W)$ and $g_Y(W)$. We first present an auxiliary lemma to bound $|\mathcal{L}_{IV}(\theta_1; \eta_1) - \mathcal{L}_{IV}(\theta_2; \eta_1) - (\mathcal{L}_{IV}(\theta_2; \eta_1) - \mathcal{L}_{IV}(\theta_2; \eta_2))| $.
\begin{lemma}\label{lemma:loss_difference_iv}
    Let $\eta = (\pi,g_Y, g_D)$ denote the set of nuisance functions, and let $\eta_0 = (\pi_0,g_{0,Y}, g_{0,D})$ be the true nuisance functions. Consider the loss defined in Proposition~\ref{prop:loss_clate}. Assume there exist finite constant $B$ such that $|\theta(X)|\leq B$ for all $X$ with positive measure, and all $\theta\in\Theta$. Then, we have that for all $\theta_1$, $\theta_2$, $\eta$, 
    \begin{align*}
    ~&|\mathcal{L}_{IV}(\theta_1; \eta) - \mathcal{L}_{IV}(\theta_2; \eta) - (\mathcal{L}_{IV}(\theta_2; \eta_0) - \mathcal{L}_{IV}(\theta_2; \eta_0))| \\
        \leq~& 4B^2 \E\left[\E\left[\left(\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}\right)(g_{0,D}(W) - g_{D}(W))\,\middle\vert\,X\right]^2\right]^{\frac{1}{2}}\|\theta(X) - \theta(X)\|\\
        ~&+ 2\E\left[\E\left[\left(\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}\right)(g_{0,Y}(W) - g_{Y}(W))\,\middle\vert\,X\right]^2\right]^{\frac{1}{2}}\|\theta(X) - \theta(X)\|
    \end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:loss_difference_iv}]
    \begin{align*}
        \mathcal{L}_{IV}(\theta_1; \eta) - \mathcal{L}_{IV}(\theta_2; \eta)= \E\left[\widehat{Z}(\eta)\left\{(\Delta D - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X)) - 2(\Delta Y - g_{Y}(W))(\theta_1(X) - \theta_2(X))\right\}\right]
    \end{align*}
    \begin{align*}
        \mathcal{L}_{IV}(\theta_1; \eta_0) - \mathcal{L}_{IV}(\theta_2; \eta_0)= \E\left[\widehat{Z}(\eta_0)\left\{(\Delta D - g_{0,D}(W))(\theta_1^2(X) - \theta_2^2(X)) - 2(\Delta Y - g_{0,Y}(W))(\theta_1(X) - \theta_2(X))\right\}\right]
    \end{align*}
    Let's first consider the $\E\left[\widehat{Z}(\eta_0)(D - g_{0,D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]$ term:
    \begin{align*}
        ~&\E\left[\widehat{Z}(\eta_0)(\Delta D - g_{0,D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\ 
        =~&  E\left[\left(Z-\frac{(1-Z)\pi_0(W)}{1-\pi_0(W)}\right)(\Delta D - g_{0,D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\
        =~& \E[Z\Delta D(\theta_1^2(X) - \theta_2^2(X))] - \E[Zg_{0,D}(W)(\theta_1^2(X) - \theta_2^2(X))]\\
        ~&- E\left[E\left[(\Delta D - g_{0,D}(W))|Z=0,W\right]\pi_0(W)(\theta_1^2(X) - \theta_2^2(X))\right]\\
        =~& \E[\Delta DZ(\theta_1^2(X) - \theta_2^2(X))] - \E[Zg_{0,D}(W)(\theta_1^2(X) - \theta_2^2(X))]
    \end{align*}
    Now, for the $\E\left[\widehat{Z}(\eta)(D - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]$ term:
    \begin{align*}
        ~&\E\left[\widehat{Z}(\eta)(\Delta D - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\ 
        =~&  \E\left[\left(Z-\frac{(1-Z)\pi(W)}{1-\pi(W)}\right)(\Delta D - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\
        =~& \E[\Delta DZ(\theta_1^2(X) - \theta_2^2(X))] - \E[Zg_{D}(W)(\theta_1^2(X) - \theta_2^2(X))] \\
        ~&- \E\left[\E\left[\frac{1-Z}{1-\pi_0(W)}(\Delta D - g_{D}(W))|W\right]\frac{(1-\pi_0(W))\pi_0(W)}{1-\pi(W)}(\theta_1^2(X) - \theta_2^2(X))\right]\\
        =~& \E[\Delta DZ(\theta_1^2(X) - \theta_2^2(X))] - \E[Zg_{D}(W)(\theta_1^2(X) - \theta_2^2(X))]
        \\
        ~&- \E\left[\E\left[(\Delta D - g_{D}(W))|Z=0,W\right]\frac{(1-\pi_0(W))\pi_0(W)}{1-\pi(W)}(\theta_1^2(X) - \theta_2^2(X))\right]\\
        =~& \E[\Delta DZ(\theta_1^2(X) - \theta_2^2(X))] - \E[Zg_{D}(W)(\theta_1^2(X)- \theta_2^2(X))]\\
        ~&- \E\left[\frac{(1-\pi_0(W))\pi_0(W)}{1-\pi(W)}(g_{0,D}(W) - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\
    \end{align*}
    Putting them together, we get:
    \begin{align*}
        ~&\E\left[\widehat{Z}(\eta)(\Delta D - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X)) - \widehat{Z}(\eta_0)(\Delta D - g_{0,D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\
        =~&\E[Z(g_{0,D}(W) - g_{D}(W))(\theta_1^2(X)- \theta_2^2(X))] - \E\left[\frac{(1-\pi_0(W))\pi_0(W)}{1-\pi(W)}(g_{0,D}(W) - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\
        =~&\E\left[\left(Z-\frac{(1-\pi_0(W))\pi_0(W)}{1-\pi(W)}\right)(g_{0,D}(W) - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\
        =~&\E\left[\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}(g_{0,D}(W) - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X))\right]\\
    \end{align*}
    Similarly,
    \begin{align*}
        ~&\E\left[\widehat{Z}(\eta)(\Delta Y - g_{Y}(W))(\theta_1(X) - \theta_2(X)) - \widehat{Z}(\eta_0)(\Delta Y - g_{0,Y}(W))(\theta_1(X) - \theta_2(X))\right]\\
        =~&\E\left[\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}(g_{0,Y}(W) - g_{Y}(W))(\theta_1(X) - \theta_2(X))\right]
    \end{align*}
    Thus, we have shown that:
    \begin{align*}
        ~&|\mathcal{L}_{IV}(\theta_1; \eta) - \mathcal{L}_{IV}(\theta_2; \eta) - (\mathcal{L}_{IV}(\theta_2; \eta_0) - \mathcal{L}_{IV}(\theta_2; \eta_0))|\\
        =~& \left|\E\left[\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}(g_{0,D}(W) - g_{D}(W))(\theta_1^2(X) - \theta_2^2(X)) - 2\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}(g_{0,Y}(W) - g_{Y}(W))(\theta_1(X) - \theta_2(X))\right]\right|\\
        \leq~& \E\left[\E\left[\left(\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}\right)(g_{0,D}(W) - g_{D}(W))\,\middle\vert\,X\right]^2\right]^{\frac{1}{2}}\E\left[(\theta_1^2(X) - \theta_2^2(X))^2\right]^{\frac{1}{2}} \\
        ~&+ 2\E\left[\E\left[\left(\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}\right)(g_{0,Y}(W) - g_{Y}(W))\,\middle\vert\,X\right]^2\right]^{\frac{1}{2}}\E\left[(\theta_1(X) - \theta_2(X))^2\right]^{\frac{1}{2}}\\
        \leq~& 4B^2 \E\left[\E\left[\left(\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}\right)(g_{0,D}(W) - g_{D}(W))\,\middle\vert\,X\right]^2\right]^{\frac{1}{2}}\|\theta(X) - \theta(X)\|\\
        ~&+ 2\E\left[\E\left[\left(\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}\right)(g_{0,Y}(W) - g_{Y}(W))\,\middle\vert\,X\right]^2\right]^{\frac{1}{2}}\|\theta(X) - \theta(X)\|\\
    \end{align*}
\end{proof}
We can now prove Theorem~\ref{thm:clate_rates}.
\begin{proof}[Proof of Theorem~\ref{thm:clate_rates}]
    Since results follows from Theorem \ref{thm:osl}, we first show that the minimizer of the loss in the function class $\Theta$, i.e. $\theta_*$, satisfies Assumptions~\ref{assum:osl_1}, \ref{assum:osl_2}, \ref{assum:osl_3}, and \ref{assum:osl_4} for the proposed loss $\mathcal{L}_{IV}(\theta;\eta)$ with $\|(\cdot)\|_{\Theta} = \|(\cdot)\|_{Z=1,CM}$
    First, Assumption~\ref{assum:osl_1} is satisfied when $\Theta$ is convex or when $\theta_0\in\Theta$. Now, we look at the second order directional derivative with respect to $\theta$. Following the same steps as in the proof of Porposition~\ref{prop:loss_clate}, we get:
    \begin{align*}
        ~&\partial_{\theta}^2 \mathcal{L}_{IV}(\bar{\theta}, \eta_0)[\theta - \theta_*, \theta - \theta_*]\\
        =~& \E[\widehat{Z}(\eta_0)(\Delta D-g_{0,D}(W))(\theta(X) - \theta_*(X))^2]\\
        =~&\E\left[(\theta(X) - \theta_*(X))^2|Z=1, D(1)>D(0)\right]\mathbb{P}(Z=1)\mathbb{P}(D(1)>D(0)|Z=1)\\
        =~& hk\|\theta(X) - \theta_*(W)\|_{Z=1,CM}\\ 
    \end{align*}
    Thus Assumption~\ref{assum:osl_2} is statisfied with $\beta_1 = hk$

    However, for Assumption~\ref{assum:osl_3}, we need to bound the second directional derivative for any $\eta$. Therefore, we consider the distance between $\partial_{\theta}^2 \mathcal{L}_{IV}(\bar{\theta}, \eta)[\theta - \theta_*, \theta - \theta_*] - \partial_{\theta}^2 \mathcal{L}_{IV}(\bar{\theta}, \eta_0)[\theta - \theta_*, \theta - \theta_*]$:
    \begin{align*}
        ~&\partial_{\theta}^2 \mathcal{L}_{IV}(\bar{\theta}, \eta)[\theta - \theta_*, \theta - \theta_*] - \partial_{\theta}^2 \mathcal{L}_{IV}(\bar{\theta}, \eta_0)[\theta - \theta_*, \theta - \theta_*]\\
        =~& 2\E[\widehat{Z}(\eta)(\Delta D-g_D(W))(\theta(X) - \theta_*(X))^2] - \E[\widehat{Z}(\eta_0)(\Delta D-g_{0,D}(W))(\theta(X) - \theta_*(X))^2]\\
        =~& 2\E\left[\frac{\pi_0(W) - \pi(W)}{1-\pi(W)}(g_{0,D}(W) - g_{D}(W))(\theta(X) - \theta_*(X))^2\right] \tag{By the same reasoning in the proof of Lemma ~\ref{lemma:loss_difference_iv}}\\
        \leq~& 2\E\left[\E\left[(\hat{g}_D(W) - g_{0,D}(W))\left(\frac{\pi_0(W) - \hat{\pi}(W)}{1-\hat{\pi}(W)}\right)\,\middle\vert\,X\right]^2\right]^{1/2}\|(\theta(X) - \theta_*(X))\|_4^{2} \tag{By Cauchy-Schwarz}\\
        \leq~& 8B^2\E\left[\E\left[(\hat{g}_D(W) - g_{0,D}(W))\left(\frac{\pi_0(W) - \hat{\pi}(W)}{1-\hat{\pi}(W)}\right)\,\middle\vert\,X\right]^2\right]^{1/2}\|(\theta(X) - \theta_*(X))\|^2_2\\
        \leq~& \frac{8B^2}{c}\E\left[\E\left[(\hat{g}_D(W) - g_{0,D}(W))\left(\frac{\pi_0(W) - \hat{\pi}(W)}{1-\hat{\pi}(W)}\right)\,\middle\vert\,X\right]^2\right]^{1/2}\|(\theta(X) - \theta_*(X))\|^2_{\Theta}
    \end{align*}
    Thus, for sufficiently small nuisance error, Assumption~\ref{assum:osl_3} is satisfied with $\kappa=0$, and $$\lambda = hk - \frac{8B^2}{c}\text{Error}(\pi, g_D) $$
    Lemma \ref{lemma:loss_difference_iv} implies Assumption \ref{assum:osl_4} with $r=0$, $\beta_2 = \frac{1}{c}\max\{4B^2, 2\} $, and $d(\eta, \eta_0)^2 =\text{Error}(\pi, \hat{g}_D) + \text{Error}(\pi, \hat{g}_Y)$.
    Thus invoking Theorem \ref{thm:osl}, we get that:
    \begin{align*}
    \|\theta(X) - \theta_*(X)\|^2_{\Theta} 
    &\leq \frac{4}{hk - \frac{8B^2}{c}\text{Error}(\pi, g_D)}R_{n}^2 + \left(\frac{\max(4B^2,2)}{c\left(hk - \frac{8B^2}{c}\text{Error}(\pi, g_D)\right)}\right)\left(\text{Error}(\pi, \hat{g}_Y) +\text{Error}(\pi, \hat{g}_D)\right) 
\end{align*}
\end{proof}

\section{Additional Experiment Details and Results} \label{app:add_exp}
\subsection{Experiment Setup}
Here we describe the data generating processes (DGP) for the fully synthetic experimens. We consider soome observed covariates $W$ with dimension $d_W$, and some unobserved confounding $U$, of dimension $d_U$. Let $\mu_W$, $\mu_U$ be the mean of $W$ and $U$, where each entry is sampled from a uniform ditsribution ranging from 0 to 1. Let $I_d$ denote the identity matrix with dimension $d$.
\begin{align*}
    W \sim~& \mathcal{N}(\mu_W, I_{d_X})\\
    W_{masked} \sim~& \text{Half of the dimensions of $W$ are randomly set to $0$}\\
    U \sim~& \mathcal{N}(\mu_U, I_{d_U})\\
    p =~& \frac{1}{1+exp(-\frac{1}{2} \beta_D^T (W-\mu_W)*(\alpha_U^T (U-\mu_U))^2)} \tag{$p$ is clipped s.t. $p\in[0.9,0.1]$}\\
    D \sim~& Binomial(p)\\
    \theta_0 = ~& \frac{1}{2}W_1*\mathbb{1}(W_2>0)
\end{align*}
For experiments with DGP that satisfies the conditional parallel trends assumptions:
\begin{align*}
    Y_0 =~& 5(\alpha_U^T(U-\mu_U))^2W_6 + W_2 + \epsilon_0, \quad \epsilon_0 \sim \mathcal{N}(0,0.5)\\
    Y_1 =~& 5(\alpha_U^T(U-\mu_U))^2W_6 + \mathbb{1}(W_1>0)W_1 + \beta_Y^T W_{masked} + W_3 + D*\theta_0 +\epsilon_1, \quad \epsilon_1 \sim \mathcal{N}(0,0.5)\\
\end{align*}
The results in Table ~\ref{table:mse_cpta} and \ref{table:mse_cpta_full} are generated using this process with $d_W = 20$ and $d_U=5$. We also ran experiments with higher dimensional covariates ($d_W=100$), and the results are presented in Table \ref{table:mse_cpta_hd}. The results in Table ~\ref{table:mse_Imbalanced} is generated using the same setup, but with $0.1*p$ as the treatment probabilities. These results all showcase that our proposed doubly robust CATT learner out performs the baseline methods. In addition to this DGP, we also experimented with a DGP that does not satisfy the conditional parallel trends assumptions. 
\begin{align*}
    \gamma \sim~& Uniform([-1,1])\\
    Y_0 =~& (\alpha_U^T(U-\mu_U))^2X_6 + X_2 + \epsilon_0, \quad \epsilon_0 \sim \mathcal{N}(0,0.5)\\
    m =~& |Y_0|\\
    Y_1 =~& (\alpha_U^T(U-\mu_U))^2X_6 + m\gamma^TX\odot X + \mathbb{1}(X_1>0)X_2 + D*\theta_0 + \epsilon_1, \quad \epsilon_1 \sim \mathcal{N}(0,0.5)
\end{align*}
Experiment results for this DGP is presented in Table \ref{table:mse_lagged_full}. We see that in this case, the conditional parallel trends is violated so the learner that assumes conditional parallel trends has a higher MSE than the those that assume lagged dependent outcome (as this DGP has a lagged outcome component). Moreover, we see that even when the assumptions are violated, the proposed learner is still more robust than the baseline outcome regression learner. 

For the semi-sythetic experiments on the minimum wage dataset, each dataset is constructed by first sampling 10000 units with replacement from the original dataset. We keep the covariate and pre-treatment outcome information, and generate the treatment assignment and the outcome in the post-treatment time period. The probability of receiving treatment is generated from the logitistic transformation of a linear transformation of a linear function of 2 "region" variables that are binary, and the log average payment information for year 2001 (i.e. $2*\text{(region 3)} - 2*\text{(region 4)} + (\text{(log average pay)}-10)$). The time trends, i.e.$Y_{post}(0) - Y_{pre}(0)$, is generated by $0.1*\text{(log average pay)} + 0.1*\text{(region 3)} + 0.1*\text{(years after treatment)}+ \text{(region 4)}*\text{(years after treatment)}^2 + \text{(log average pay)}^{\frac{1}{2}}*\text{(log average population)}$. The treatment effect is defined as $0.1*\text{(log average population)} + 0.1*\text{(log average population)}^{\frac{1}{2}}$.
% Below is the pseudo code for the validation procedure described in Section \ref{sec:exp}.
% \begin{algorithm}
% \caption{Calibration of heterogeneous treatment effects}
% \label{algo:calibration}
% \begin{algorithmic}[1]
% \STATE \textbf{Input:} Predicted CATT values for validation and calibration set, pseudo-outcomes ($\widehat{Y}$) for the calibration set
% \STATE Compute quantiles of the predicted CATT values to define groups
% \FOR{each quantile group}
%     \STATE Identify the samples in the calibration set whose predicted CATT falls into this quantile.
%     \STATE Compute the average pseudo-outcome (GATT) for the group, and calculate the standard error of the GATT estimate.
%     \STATE Calculate the average predicted CATE for the group
% \ENDFOR

% \STATE \textbf{Output:} GATE estimates, standard errors, group probabilities, predicted CATEs
% \end{algorithmic}
% \end{algorithm}


\subsection{Additional Results}


\begin{table*}[ht]
\centering
\caption{MSE (mean ± standard deviation) over 100 simulations following the conditional parallel trends condition. Each row represent a different meta-learner, and columns represent the different nuisance function classes.}
\label{table:mse_cpta_full}
\vskip 0.15in
\begin{tabular}{|l|l|l|l|l|l|}
\hline
  & Basic & Lasso (CV) & Ridge (CV) & Random Forest & Best \\
\hline
Neural Net (CPTA OR)& 0.12 ± 0.02 & 0.12 ± 0.02 & 0.12 ± 0.02 & 0.38 ± 0.18 & 0.12 ± 0.02 \\
Neural Net (CPTA DR) & 0.1 ± 0.02 & 0.1 ± 0.03 & 0.1 ± 0.02 & 0.14 ± 0.04 & 0.1 ± 0.02 \\
Neural Net (Lagged OR) & 0.12 ± 0.02 & 0.14 ± 0.04 & 0.12 ± 0.02 & 1.27 ± 0.65 & 0.12 ± 0.02 \\
Neural Net (Lagged DR) & 0.1 ± 0.02 & 0.1 ± 0.03 & 0.1 ± 0.02 & 0.63 ± 0.4 & 0.1 ± 0.02 \\
\hline
XGBoost (OR) & 0.09 ± 0.02 & 0.09 ± 0.02 & 0.09 ± 0.02 & 0.31 ± 0.16 & 0.09 ± 0.02 \\
XGBoost (DR)  & 0.04 ± 0.01 & 0.04 ± 0.01 & 0.04 ± 0.02 & 0.06 ± 0.03 & 0.04 ± 0.01 \\
XGBoost (Lagged OR)   & 0.09 ± 0.02 & 0.11 ± 0.04 & 0.09 ± 0.02 & 1.15 ± 0.69 & 0.09 ± 0.02 \\
XGBoost (Lagged DR)  & 0.04 ± 0.01 & 0.05 ± 0.03 & 0.04 ± 0.02 & 0.54 ± 0.45 & 0.04 ± 0.01 \\
\hline
Linear (OR)   & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.51 ± 0.18 & 0.26 ± 0.07 \\
Linear (DR) & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 \\
Linear (Lagged OR)   & 0.26 ± 0.07 & 0.28 ± 0.08 & 0.26 ± 0.07 & 1.18 ± 0.56 & 0.26 ± 0.07 \\
Linear (Lagged DR)   & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.26 ± 0.07 & 0.42 ± 0.19 & 0.26 ± 0.07 \\
\hline
\end{tabular}
\end{table*}

\begin{table*}[ht]
\centering
\caption{MSE (mean ± standard deviation) Over 100 Simulations of Imbalanced Dataset. Each row represent a different meta-learner, and columns represent the different nuisance function classes. }
\label{table:mse_Imbalanced_full}
\vskip 0.15in
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
 & No Controls & \begin{tabular}{@{}c@{}}Linear \\ Regression\end{tabular} & Lasso (CV) & Ridge (CV) & Random Forest & Best \\
\hline
Neural Net (OR) & 1.53 ± 0.74 & 0.22 ± 0.06 & 0.21 ± 0.06 & 0.21 ± 0.06 & 0.4 ± 0.15 & 0.21 ± 0.05 \\
Neural Net (DR) & 0.52 ± 0.31 & 0.18 ± 0.07 & 0.18 ± 0.05 & 0.18 ± 0.05 & 0.24 ± 0.07 & 0.18 ± 0.05 \\
Neural Net (CATE OR) & 0.66 ± 0.27 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.51 ± 0.16 & 0.27 ± 0.08 \\
Neural Net (CATE DR) & 0.53 ± 0.22 & 0.22 ± 0.07 & 0.22 ± 0.07 & 0.21 ± 0.07 & 0.33 ± 0.11 & 0.21 ± 0.07 \\
% \hline
% Linear (OR) & 1.55 ± 0.75 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.47 ± 0.16 & 0.27 ± 0.08 \\
% Linear (DR) & 0.35 ± 0.1 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.28 ± 0.08 & 0.27 ± 0.08 \\
% Linear (CATE OR) & 0.66 ± 0.27 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.52 ± 0.17 & 0.27 ± 0.08 \\
% Linear (CATE DR) & 0.36 ± 0.11 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.31 ± 0.09 & 0.27 ± 0.08 \\
\hline
XGBoost (OR) & 1.22 ± 0.58 & 0.21 ± 0.06 & 0.21 ± 0.06 & 0.21 ± 0.06 & 0.34 ± 0.11 & 0.21 ± 0.06 \\
XGBoost (DR) & 0.4 ± 0.14 & 0.12 ± 0.03 & 0.12 ± 0.03 & 0.12 ± 0.03 & 0.18 ± 0.06 & 0.12 ± 0.03 \\
XGBoost (CATE OR) & 0.66 ± 0.27 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.51 ± 0.16 & 0.27 ± 0.08 \\
XGBoost (CATE DR) & 0.49 ± 0.22 & 0.15 ± 0.05 & 0.15 ± 0.04 & 0.15 ± 0.05 & 0.34 ± 0.13 & 0.15 ± 0.04 \\
\hline
\end{tabular}
\vskip -0.1in
\end{table*}

\begin{table*}[ht]
\centering
\caption{MSE (mean ± standard deviation) over 100 simulations following the conditional parallel trends condition, with 100 covariates.}
\label{table:mse_cpta_hd}
\vskip 0.15in
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 & \begin{tabular}{@{}c@{}} Linear \\ Regression\end{tabular} & Lasso (CV) & Ridge (CV) & Random Forest & Best \\
\hline
Neural Net OR  & 0.21 ± 0.05 & 0.21 ± 0.05 & 0.2 ± 0.06 & 1.27 ± 0.69 & 0.21 ± 0.06 \\
Neural Net DR  & 0.18 ± 0.05 & 0.18 ± 0.06 & 0.18 ± 0.06 & 0.64 ± 0.38 & 0.18 ± 0.06 \\
Neural Net CATE OR  & 0.28 ± 0.08 & 0.28 ± 0.08 & 0.28 ± 0.08 & 0.65 ± 0.25 & 0.28 ± 0.08 \\
Neural Net CATE DR  & 0.3 ± 0.1 & 0.29 ± 0.09 & 0.29 ± 0.1 & 1.08 ± 0.71 & 0.2 ± 0.06 \\
Linear OR  & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 1.3 ± 0.63 & 0.27 ± 0.08 \\
Linear DR  & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.27 ± 0.08 & 0.44 ± 0.13 & 0.27 ± 0.08 \\
Linear CATE OR  & 0.28 ± 0.08 & 0.27 ± 0.08 & 0.28 ± 0.08 & 0.65 ± 0.25 & 0.27 ± 0.08 \\
Linear CATE DR  & 0.29 ± 0.08 & 0.29 ± 0.08 & 0.29 ± 0.08 & 0.82 ± 0.33 & 0.28 ± 0.08 \\
XGBoost OR & 0.21 ± 0.06 & 0.2 ± 0.05 & 0.21 ± 0.05 & 0.96 ± 0.45 & 0.21 ± 0.05 \\
XGBoost DR  & 0.13 ± 0.04 & 0.12 ± 0.03 & 0.12 ± 0.03 & 0.61 ± 0.25 & 0.12 ± 0.03 \\
XGBoost CATE OR  & 0.28 ± 0.08 & 0.27 ± 0.08 & 0.28 ± 0.08 & 0.65 ± 0.25 & 0.27 ± 0.08 \\
XGBoost CATE DR  & 0.26 ± 0.09 & 0.24 ± 0.08 & 0.25 ± 0.09 & 1.18 ± 0.85 & 0.15 ± 0.05 \\
\hline
\end{tabular}
\end{table*}

\begin{table*}[ht]
\centering
\caption{MSE (mean ± standard deviation) over 100 simulations that does not satisfy the conditional parallel trends assumption. Each row represent a different meta-learner, and columns represent the different nuisance function classes.}
\label{table:mse_lagged_full}
\vskip 0.15in
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 & \begin{tabular}{@{}c@{}} Linear \\ Regression\end{tabular} & Lasso (CV) & Ridge (CV) & Random Forest & Best \\
\hline
\begin{tabular}{@{}c@{}}Neural Net \\ (CPTA OR)\end{tabular}& 76.93 ± 135.25 & 76.34 ± 129.71 & 74.87 ± 127.94 & 35.49 ± 91.98 & 36.85 ± 87.85 \\
\begin{tabular}{@{}c@{}}Neural Net \\ (CPTA DR)\end{tabular}  & 17.07 ± 74.16 & 15.54 ± 54.25 & 20.41 ± 86.35 & 17.24 ± 63.37 & 18.38 ± 65.13 \\
\begin{tabular}{@{}c@{}}Neural Net \\ (Lagged OR)\end{tabular} & 70.31 ± 98.19 & 70.07 ± 93.93 & 69.98 ± 100.44 & 26.21 ± 39.93 & 24.81 ± 32.88 \\
\begin{tabular}{@{}c@{}}Neural Net \\ (Lagged DR)\end{tabular} & 4.37 ± 4.66 & 4.93 ± 5.65 & 4.94 ± 5.89 & 4.99 ± 14.46 & 5.09 ± 10.25 \\
\hline
\begin{tabular}{@{}c@{}}XGBoost \\ (CPTA OR)\end{tabular}  & 65.67 ± 122.65 & 63.5 ± 127.89 & 63.82 ± 113.59 & 29.39 ± 69.84 & 30.15 ± 85.27 \\
\begin{tabular}{@{}c@{}}XGBoost \\ (CPTA DR)\end{tabular}  & 20.49 ± 58.55 & 22.99 ± 81.52 & 23.31 ± 74.74 & 26.9 ± 128.52 & 31.11 ± 149.05 \\
\begin{tabular}{@{}c@{}}XGBoost \\ (Lagged OR)\end{tabular} & 55.62 ± 82.27 & 56.87 ± 83.95 & 53.95 ± 77.19 & 21.29 ± 32.85 & 22.39 ± 38.38 \\
\begin{tabular}{@{}c@{}}XGBoost \\ (Lagged DR)\end{tabular}  & 9.88 ± 14.34 & 9.34 ± 13.13 & 10.33 ± 21.81 & 10.76 ± 40.39 & 8.14 ± 13.38 \\
\hline
\begin{tabular}{@{}c@{}}Linear \\ (CPTA OR)\end{tabular}  & 18.41 ± 62.54 & 18.0 ± 61.84 & 18.41 ± 62.68 & 17.61 ± 65.51 & 17.61 ± 65.51 \\
\begin{tabular}{@{}c@{}}Linear \\ (CPTA DR)\end{tabular} & 14.56 ± 57.69 & 14.7 ± 58.43 & 14.56 ± 57.7 & 15.84 ± 64.12 & 15.84 ± 64.12 \\
\begin{tabular}{@{}c@{}}Linear \\ (Lagged OR)\end{tabular} & 12.08 ± 28.93 & 11.64 ± 27.55 & 12.07 ± 28.9 & 9.78 ± 21.18 & 9.78 ± 21.18 \\
\begin{tabular}{@{}c@{}}Linear \\ (Lagged DR)\end{tabular} & 4.78 ± 5.81 & 4.85 ± 5.97 & 4.78 ± 5.81 & 3.99 ± 7.18 & 3.99 ± 7.18 \\
\hline
\end{tabular}
\end{table*}

\begin{figure}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/cal_pop_kernel.png}}
\caption{Calibration plot for CATT w.r.t log county population for the XGBoost doubly robust learner.}
\label{fig:cal_pop_kernel}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/cal_pop_linear.png}}
\caption{Calibration plot for CATT w.r.t log county population for the linear doubly robust learner.}
\label{fig:cal_pop_linear}
\end{center}
\vskip -0.2in
\end{figure}
\end{document}


\end{document}




% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
