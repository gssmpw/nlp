\section{Related Works}
The DR problem for industrial processes falls within the broader class of enterprise-wide optimization (EWO) problems described by ____. In the context of DR, this often involves solving complex mixed-integer dynamic optimization (MIDO) problems, which combine discrete decisions (e.g., product scheduling) with continuous dynamic process models. These problems are typically transformed into mixed-integer nonlinear programming (MINLP) formulations for solution. While MIDO and MINLP approaches offer a rigorous framework for integrating scheduling and control decisions____, they often face computational challenges due to problem size and complexity.


____ introduced scale-bridging models (SBMs) for DR applications, using data-driven low-order dynamic models to ensure feasible schedules with reduced computational burden. The approach demonstrated significant cost savings when applied to an air separation unit under time-varying electricity prices. Building on this work, ____, ____, and ____ compared different paradigms for integrating scheduling and control in DR problems. ____ developed a simulation-based optimization framework that combines SBMs with model predictive control (MPC), while ____ contrasted this ``top-down'' approach with a ``bottom-up'' economic MPC formulation. ____ developed a Koopman-based approach achieving real-time NMPC with 98\% reduced computational cost. Applied to an air separation unit, this method demonstrated ~8\% cost savings over steady-state operation, though highlighting tradeoffs between computational efficiency and economic optimality.

Reinforcement learning (RL) presents a compelling approach to demand response in industrial processes, offering advantages over traditional optimization methods. RL learns optimal control policies through environment interaction without requiring explicit mathematical models of system dynamics or constraints. Its effectiveness has been demonstrated in industrial applications, particularly in fed-batch bioreactors ____, which present significant run-to-run variabilities ____. Recent advances include integrating RL with PID controllers for improved interpretability and sample efficiency ____, with ____ showing enhanced performance through combined neural network-PID architectures. The data-driven nature of RL complements scheduling-based modeling approaches by leveraging historical data to improve decision-making. While challenges such as sample efficiency and stability remain, modern approaches such as scalable algorithms through action-space dimensionality reduction ____ and hierarchical RL frameworks have shown promise in industrial control applications, as demonstrated by ____ in optimal continuous control of refrigeration systems under Time-of-Use pricing. While these previous works have demonstrated the potential of both traditional optimization and reinforcement learning approaches, the integration of RL with MPC for demand response optimization remains largely unexplored. Our work addresses this gap by presenting a hierarchical framework that combines reinforcement learning with LMPC, demonstrating enhanced sample efficiency and constraint satisfaction while maintaining economic performance in industrial demand response applications.