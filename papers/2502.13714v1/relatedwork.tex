\section{Related Works}
The DR problem for industrial processes falls within the broader class of enterprise-wide optimization (EWO) problems described by \citet{flores-tlacuahuac_simultaneous_2006}. In the context of DR, this often involves solving complex mixed-integer dynamic optimization (MIDO) problems, which combine discrete decisions (e.g., product scheduling) with continuous dynamic process models. These problems are typically transformed into mixed-integer nonlinear programming (MINLP) formulations for solution. While MIDO and MINLP approaches offer a rigorous framework for integrating scheduling and control decisions~\citep{zhang2015air}, they often face computational challenges due to problem size and complexity.


\citet{pattison_optimal_2016} introduced scale-bridging models (SBMs) for DR applications, using data-driven low-order dynamic models to ensure feasible schedules with reduced computational burden. The approach demonstrated significant cost savings when applied to an air separation unit under time-varying electricity prices. Building on this work, \citet{dias_simulation-based_2018}, \citet{caspari_integration_2020}, and \citet{schulze2023datadrivenmodelreductionnonlinear} compared different paradigms for integrating scheduling and control in DR problems. \citeauthor{dias_simulation-based_2018} developed a simulation-based optimization framework that combines SBMs with model predictive control (MPC), while \citeauthor{caspari_integration_2020} contrasted this ``top-down'' approach with a ``bottom-up'' economic MPC formulation. \citeauthor{schulze2023datadrivenmodelreductionnonlinear} developed a Koopman-based approach achieving real-time NMPC with 98\% reduced computational cost. Applied to an air separation unit, this method demonstrated ~8\% cost savings over steady-state operation, though highlighting tradeoffs between computational efficiency and economic optimality.

Reinforcement learning (RL) presents a compelling approach to demand response in industrial processes, offering advantages over traditional optimization methods. RL learns optimal control policies through environment interaction without requiring explicit mathematical models of system dynamics or constraints. Its effectiveness has been demonstrated in industrial applications, particularly in fed-batch bioreactors \citep{kaisare2003simulation, peroni2005optimal}, which present significant run-to-run variabilities \citep{YOO2021108, PETSAGKOURAKIS202235}. Recent advances include integrating RL with PID controllers for improved interpretability and sample efficiency \citep{lawrence2022deep}, with \cite{CIRL} showing enhanced performance through combined neural network-PID architectures. The data-driven nature of RL complements scheduling-based modeling approaches by leveraging historical data to improve decision-making. While challenges such as sample efficiency and stability remain, modern approaches such as scalable algorithms through action-space dimensionality reduction \citep{zhu2020scalable} and hierarchical RL frameworks have shown promise in industrial control applications, as demonstrated by \cite{kim_optimal_2023} in optimal continuous control of refrigeration systems under Time-of-Use pricing. While these previous works have demonstrated the potential of both traditional optimization and reinforcement learning approaches, the integration of RL with MPC for demand response optimization remains largely unexplored. Our work addresses this gap by presenting a hierarchical framework that combines reinforcement learning with LMPC, demonstrating enhanced sample efficiency and constraint satisfaction while maintaining economic performance in industrial demand response applications.