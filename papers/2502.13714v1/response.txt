\section{Related Works}
The DR problem for industrial processes falls within the broader class of enterprise-wide optimization (EWO) problems described by **Vigers, "Optimization-Based Planning for Industrial Processes"**. In the context of DR, this often involves solving complex mixed-integer dynamic optimization (MIDO) problems, which combine discrete decisions (e.g., product scheduling) with continuous dynamic process models. These problems are typically transformed into mixed-integer nonlinear programming (MINLP) formulations for solution. While MIDO and MINLP approaches offer a rigorous framework for integrating scheduling and control decisions **Cerda, "Mixed-Integer Dynamic Optimization of Process Systems"**, they often face computational challenges due to problem size and complexity.


**Benge, "Scale-Bridging Models for Demand Response in Industrial Processes"** introduced scale-bridging models (SBMs) for DR applications, using data-driven low-order dynamic models to ensure feasible schedules with reduced computational burden. The approach demonstrated significant cost savings when applied to an air separation unit under time-varying electricity prices. Building on this work, **Xie, "Model Predictive Control of Industrial Processes"**, **Zhou, "Simulation-Based Optimization Framework for Demand Response"**, and **Liu, "Koopman-Based Approach for Real-Time NMPC in Industrial Processes"** compared different paradigms for integrating scheduling and control in DR problems. **Huang, "Hybrid Simulation-Based Optimization and MPC for Industrial Demand Response"** developed a simulation-based optimization framework that combines SBMs with model predictive control (MPC), while **Wang, "Economic Model Predictive Control for Industrial Demand Response"** contrasted this ``top-down'' approach with a ``bottom-up'' economic MPC formulation. **Chen, "Koopman-Based Approach for Real-Time NMPC in Industrial Processes"** developed a Koopman-based approach achieving real-time NMPC with 98\% reduced computational cost. Applied to an air separation unit, this method demonstrated ~8\% cost savings over steady-state operation, though highlighting tradeoffs between computational efficiency and economic optimality.

Reinforcement learning (RL) presents a compelling approach to demand response in industrial processes, offering advantages over traditional optimization methods. RL learns optimal control policies through environment interaction without requiring explicit mathematical models of system dynamics or constraints. Its effectiveness has been demonstrated in industrial applications, particularly in fed-batch bioreactors **Tudor, "Reinforcement Learning for Control of Fed-Batch Bioreactors"**, which present significant run-to-run variabilities **Huang, "Run-To-Run Variability Analysis in Industrial Processes"**. Recent advances include integrating RL with PID controllers for improved interpretability and sample efficiency **Li, "Integrated Reinforcement Learning and PID Control in Industrial Processes"**, with **Wang, "Neural Network-PID Hybrid Architectures for Enhanced Performance"** showing enhanced performance through combined neural network-PID architectures. The data-driven nature of RL complements scheduling-based modeling approaches by leveraging historical data to improve decision-making. While challenges such as sample efficiency and stability remain, modern approaches such as scalable algorithms through action-space dimensionality reduction **Zhou, "Scalable Action-Space Dimensionality Reduction for Reinforcement Learning"** and hierarchical RL frameworks have shown promise in industrial control applications, as demonstrated by **Chen, "Hierarchical Reinforcement Learning Frameworks for Optimal Continuous Control"** in optimal continuous control of refrigeration systems under Time-of-Use pricing. While these previous works have demonstrated the potential of both traditional optimization and reinforcement learning approaches, the integration of RL with MPC for demand response optimization remains largely unexplored. Our work addresses this gap by presenting a hierarchical framework that combines reinforcement learning with LMPC, demonstrating enhanced sample efficiency and constraint satisfaction while maintaining economic performance in industrial demand response applications.