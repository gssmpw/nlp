\section{Related Works}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{Figure_1.png}
    \caption{Overview of FwNet-ECA Architecture}
    \label{fig:1}
\end{figure}

CNNs have been instrumental in advancing computer vision, offering a robust framework for image recognition\cite{lecun2015deep}. Their hierarchical feature extraction capabilities, which are designed to mimic the human visual cortex, have proven effective across a wide range of applications\cite{serre2007feedforward}. Key developments in CNN architectures\cite{krizhevsky2012imagenet}\cite{simonyan2014very}\cite{szegedy2015going}\cite{he-2016}, have progressively improved performance in image classification, detection, and segmentation tasks. However, CNNs' reliance on convolutional layers for feature extraction inherently limits their capacity to model long-range dependencies efficiently, a critical aspect for tasks requiring global context understanding\cite{long2015fully}. 

The advent of Transformers in natural language processing has catalyzed a paradigm shift in computer vision, leading to the exploration of ViT\cite{vaswani2017attention}. Pioneered by Dosovitskiy et al. \cite{dosovitskiy2020image}, ViT treat images as sequences of patches, enabling more flexible modeling of global dependencies without the need for convolutional operations. This approach achieved impressive results in image classification but was inherently  constrained by high computational requirements and large model sizes\cite{touvron2021training}. Subsequent improvements aimed to tackle these issues through architectural refinements, often reintroducing some level of inductive bias. A notable example is the Swin Transformer\cite{liu2021swin}, which curtails computational complexity via localized attention mechanisms. To facilitate interaction between different windows, it introduces shifted window attention, which is a very clever method but is complex in operation. In this work, we aim to propose a simpler and more intuitive method.

In the visual domain, the Fourier transform, particularly the Discrete Fourier Transform (DFT), appears in many convolutional neural networks. With CNNs demonstrating substantial advancements in visual recognition and analysis, various strategies have emerged to optimize and innovate for specific visual tasks. One such method involves exploiting frequency domain representations through DFT, thereby enhancing task performance by leveraging frequency features\cite{lee2018single}\cite{yang2020fda}. Another concurrent approach leverages the convolution theorem in conjunction with FFT to expedite CNN computations\cite{li2020falcon}. Within Transformer architectures, networks like FNet\cite{lee2021fnet} and GFNet\cite{rao2021global} have replaced Self Attention mechanisms with filtering operations for both CV and NLP tasks.

Regarding channel attention, SENet\cite{hu2018squeeze} initiated the concept of channel-wise attention, enabling networks to dynamically adjust feature channel weights through its SE(Squeeze-and-Excitation) module, significantly boosting model performance. Subsequent innovations, such as ECA-Net\cite{wang2020eca} and CBAM\cite{woo2018cbam}, not only improved computational efficiency but also fortified feature processing capabilities by integrating spatial attention. In this work, we added an ECA(Efficient Channel Attention) module to the main structure of the model to construct inter channel attention.

Notably, in contrast to similar efforts like FNet and GFNet, FwNet-ECA distinguishes itself in several key aspects:
\begin{itemize}
\item \textbf{Retention of Self-Attention:} Unlike FNet and GFNet, which abandon Self Attention entirely, FwNet-ECA integrates partial Self Attention mechanisms, leveraging their proven effectiveness in both NLP and CV.
\item 	\textbf{Channel Interaction:} FwNet-ECA addresses the shortfall of FNet and GFNet by explicitly considering inter-channel relationships through mechanisms like local channel attention.
\item 	\textbf{Domain Adaptation:} Unlike FNet, which is tailored for NLP tasks, FwNet-ECA is purpose-built for CV applications, demonstrating versatility and domain-specific optimization.
\item 	\textbf{Different purposes:} Although the method is similar to GFNet, the purpose of constructing FwNet-ECA is to find a simple and efficient method to replace the more complex moving window attention.
\end{itemize}