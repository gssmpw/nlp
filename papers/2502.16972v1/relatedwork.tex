\section{Related Works}
\label{sec:related}

% Pre-trained diffusion distillation methods can significantly reduce the computational cost associated with a high number of function evaluations, making them a promising direction for accelerating diffusion models. We categorize these distillation methods into three groups:

% \textbf{Output Reconstruction Based:} These methods focus on reconstructing image outputs. Specifically, \cite{salimans2022progressive, berthelot2023tract} focus on output values; \cite{poole2022dreamfusion, wang2024prolificdreamer, yin2024one, yin2024improved} concentrate on output distributions; \cite{lukoianov2024score, karras2022elucidating} operate in a one-step denoising image space; and \cite{zhou2024score, zhou2024adversarial} employ Fisher divergence.

% \textbf{Trajectory Distillation Based:} These methods distill the trajectory generation process. In particular, consistency distillation approaches \cite{song2023consistency, kim2023consistency, lu2024simplifying} focus on self-consistency, while \cite{liu2023instaflow, zhu2025slimflow} concentrate on rectified flow distillation. Additionally, \cite{frans2024one} integrates consistency modeling into rectified flows.

% \textbf{Adversarial Distillation Based:} These methods integrate an adversarial loss to better approximate the target distribution. For instance, \cite{sauer2024fast, sauer2025adversarial} employ a pre-trained classifier during training.

Pre-trained diffusion distillation methods have emerged as a powerful strategy to alleviate the significant computational burden inherent in diffusion models, which traditionally require a large number of function evaluations to produce high-quality samples. By compressing the iterative denoising process into far fewer steps, these techniques not only expedite sample generation but also make it feasible to deploy such models in low-resource environments. In this work, we systematically categorize these distillation approaches into three major groups based on their underlying objectives and operational paradigms.

\textbf{Output Reconstruction Based:} These methods aim to minimize the discrepancy between the outputs of the teacher (i.e., the pre-trained diffusion model) and the student model by directly reconstructing the image outputs. Some approaches, such as Progressive Distillation~\cite{salimans2022progressive, berthelot2023tract}, focus on aligning the output values by enforcing a close correspondence between the denoising steps of the teacher and student. Other methods, for instance SDS and its variants~\cite{poole2022dreamfusion, wang2024prolificdreamer, yin2024one, yin2024improved}, concentrate on matching the output distributions to preserve the statistical characteristics of the generated images. Additionally, certain works operate in a one-step denoising image space~\cite{lukoianov2024score, karras2022elucidating}, allowing for the direct generation of high-quality images from a single function evaluation, while others employ Fisher divergence objectives~\cite{zhou2024score, zhou2024adversarial} to more rigorously align the gradients of the score functions. Together, these techniques effectively reduce the number of sampling steps required while maintaining the fidelity of the generated outputs.

\textbf{Trajectory Distillation Based:} Instead of concentrating solely on the final output, trajectory-based methods focus on the entire denoising path—from the initial random noise to the eventual clean image. By distilling the full trajectory, these approaches ensure that the student model replicates not only the end result but also the dynamic behavior of the teacher model throughout the diffusion process. Consistency distillation techniques~\cite{song2023consistency, kim2023consistency, lu2024simplifying} emphasize the self-consistency of the denoising trajectory, ensuring stable and accurate progression across different time steps. In contrast, rectified flow distillation methods such as InstaFlow and SlimFlow~\cite{liu2023instaflow, zhu2025slimflow} focus on producing a straighter, more direct trajectory, thereby mitigating the accumulation of approximation errors that typically arise from curved paths. Moreover, recent studies have demonstrated that integrating consistency modeling directly into rectified flows~\cite{frans2024one} can further enhance the fidelity of the generated trajectories, effectively combining the strengths of both approaches.

\textbf{Adversarial Distillation Based:} A third category of distillation methods leverages adversarial learning to refine the student model's output distribution. By incorporating an adversarial loss—often implemented via a pre-trained classifier or discriminator—these methods drive the student model to more closely approximate the target distribution provided by the teacher. Notably, works such as~\cite{sauer2024fast, sauer2025adversarial} have successfully employed this strategy to achieve competitive performance with significantly fewer sampling steps. The adversarial framework not only enhances the perceptual quality of the generated images but also provides a flexible plug-and-play mechanism that can complement other distillation strategies.