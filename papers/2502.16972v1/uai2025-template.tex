% \documentclass{uai2025} % for initial submission
\documentclass[accepted]{uai2025}

% also before submission to see how the non-anonymous paper would look like 
                        
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2025} % ptmx math instead of Computer
                                         % Modern (has noticeable issues)
% \documentclass[mathfont=newtx]{uai2025} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}
\usepackage{svg}
\usepackage{graphicx}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{color}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\myauthornote}[3]{{\color{#2} {\sc #1}: #3}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand\xuhui[1]{\myauthornote{Xuhui}{orange}{#1}}
\newcommand\wzk[1]{\myauthornote{wzk}{blue}{#1}}
\newcommand{\modelname}{TraFlow}

% Redefine \REQUIRE and \ENSURE to Input and Output
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\input{math}
%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

\title{TraFlow: Trajectory Distillation on Pre-Trained Rectified Flow}

% The standard author block has changed for UAI 2025 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
% \author[1]{\href{mailto:<jj@example.edu>?Subject=Your UAI 2025 paper}{Jane~J.~von~O'L\'opez}{}}
% \author[1]{Harry~Q.~Bovik}
% \author[1,2]{Further~Coauthor}
% \author[3]{Further~Coauthor}
% \author[1]{Further~Coauthor}
% \author[3]{Further~Coauthor}
% \author[3,1]{Further~Coauthor}
% % Add affiliations after the authors
% \affil[1]{%
%     Computer Science Dept.\\
%     Cranberry University\\
%     Pittsburgh, Pennsylvania, USA
% }
% \affil[2]{%
%     Second Affiliation\\
%     Address\\
%     …
% }
% \affil[3]{%
%     Another Affiliation\\
%     Address\\
%     …
%   }
\author[1]{\href{mailto:zhangkai.wu@mq.edu.au?Subject=Your UAI 2025 paper}{Zhangkai~Wu}}
\author[1]{\href{mailto:xuhui.fan@mq.edu.au?Subject=Your UAI 2025 paper}{Xuhui~Fan}}
\author[2]{\href{mailto:hongyu.wu@students.mq.edu.au?Subject=Your UAI 2025 paper}{Hongyu~Wu}}
\author[1]{\href{mailto:longbing.cao@mq.edu.au?Subject=Your UAI 2025 paper}{Longbing~Cao}}

\affil[1,2]{%
    Macquarie University, Australia
}
\affil[1]{%
    \{zhangkai.wu, xuhui.fan, longbing.cao\}@mq.edu.au
}
\affil[2]{%
    hongyu.wu@students.mq.edu.au
}

  
  \begin{document}
\maketitle
 
\begin{abstract}
Majorities of distillation methods on pre-trained diffusion models or on pre-trained rectified flow, focus on either the distillation outputs or the trajectories between random noises and clean images to speed up sample generations from pre-trained models. In those trajectory-based distillation methods, consistency distillation requires the self-consistent trajectory projection to regulate the trajectory, which might avoid the common ODE approximation error {while still be concerning about sampling efficiencies}. At the same time, rectified flow distillations enforce straight trajectory for fast sampling, although an ODE solver is still required. In this work, we propose a trajectory distillation method, \modelname, that enjoys the benefits of both and enables few-step generations. TraFlow adopts the settings of consistency trajectory models, and further enforces the properties of self-consistency and straightness throughout the entire trajectory. These two properties are pursued by reaching a balance with following three targets: (1) reconstruct the output from pre-trained models; (2) learn the amount of changes by pre-trained models; (3) satisfy the self-consistency over its trajectory. Extensive experimental results have shown the effectiveness of our proposed method.
\end{abstract}


\section{Introduction}
\label{sec:intro}
Diffusion models and rectified flows~\cite{ddpm2020neurips,song2020score,rombach2022high,poole2022dreamfusion,esser2024scaling} have become the dominant method in the synthesis and editing of high-quality images. However, its iterative design requires us to perform a large number of neural network evaluations to denoise from noise to high-quality data. To this end, distilling pre-trained diffusion models or pre-trained rectified flows~\cite{salimans2022progressive, wang2024prolificdreamer, yin2024one, luo2024diff, nguyen2024swiftbrush, yin2024improved, zhu2025slimflow, sauer2025adversarial, sauer2024fast, wang2022diffusion, xu2024ufogen, frans2024one, xie2024distillation} which trained models to match their teacher model's sample quality in fewer steps is one of the mainstream approaches. 

Trajectory-based distillation methods constitute a representative class of distillation techniques applied to pre-trained diffusion models. These methods can be broadly categorized into consistency distillation\cite{song2023consistency,kim2023consistency,lu2024simplifying} and rectified flow distillation\cite{liu2023instaflow,zhu2025slimflow}.

In consistency distillation, a trajectory projection function is introduced to model the incremental changes between two time steps. A fundamental requirement of this approach is the self-consistency property, which stipulates that the cumulative change over a large time step must be equivalent to the sum of changes over two smaller steps. By directly modeling these incremental changes, consistency distillation eliminates the need to solve an ordinary differential equation (ODE) during sample generation. However, despite avoiding ODE integration, multi-step generation remains necessary, as accurately modeling self-consistent trajectories presents a significant challenge.

Conversely, rectified flow distillation enforces a straight-line trajectory between random noise and ground-truth samples, thereby accelerating sample generation. By constraining the velocity field to approximate a linear trajectory, this approach mitigates approximation errors, allowing for simpler numerical integration methods such as Euler’s method. Consequently, fewer steps are required to generate high-quality samples. Nevertheless, since the learned velocity field is not strictly constant in practice, numerical integration using Euler’s method still introduces approximation errors.



% Given this advanced mechanism, there have been a few approaches~\cite{liu2023instaflow,zhu2025slimflow,li2024flowdreamer} tried to distill pre-trained rectified flows. However, these approaches concentrate on the outputs only. Distilling the straight trajectory, which is the main character of rectified flows, is missing. Without sufficient utilizing the straightness of flow trajectory, Distilling pre-trained rectified flows might loose the unique insights into it. \xuhui{will elaborate more}  
% are accused of  by directly minimizing the output difference between the distilled rectified flow and its pre-trained teacher model. However, comparing the output differences only might produce unstable distilled samples, as shown in Figure~\ref{}.

In this paper, we propose \modelname, a trajectory distillation method that may enjoy the benefits of both. The proposed \modelname adopts the setting of consistency trajectory model~\cite{kim2023consistency}. In details, \modelname\ considers three (self-)consistency distilling targets: (1) the self-consistency of the  trajectory, which means two different previous points will be transited to the same value at a later time point;  (2) the consistency of velocities, which refers to that the gradient of the distilling trajectory will be close to the amount of changes in the pre-trained model; (3) the consistency of trajectory outputs, which simply compares the trajectory outputs (at time step $0$) between the distilled model and the pre-trained teacher model. These three targets regulate the distilled model from aspects of outputs, velocities, and trajectory itself. 
% The generated samples would be more stable than that from InstaFlow, as shown in Figure~\ref{}. 

% In addition, we introduce an adversarial mechanism to further improve the capability of generator. By incorporating real samples in the discriminator, our learned student model may generate better samples than the teacher model.

The contributions of this paper can be summarised as: (1), we distill a pre-trained rectified flow into a time-step integrated model, which is easier than integrating the velocities; (2), in addition to the output difference loss, we introduce velocity approximation loss and output consistency loss; (4), we obtain state-of-the-art performance which uses smaller models to generate high-quality data with one-step.

\section{Preliminaries}
\label{sec:preliminaries}
\textbf{Diffusion models} Diffusion models~(DMs)~\cite{ddpm2020neurips} generate data by learning the score function of noise-corrupted images at multiple levels of noise. At each time step $t$, the \emph{clean} image $\mbx_0$ is first diffused into a noisy image $\mbx_t$ through a forward process defined as $\mbx_t:=\alpha_t \mbx_0 + \sigma_t \mbepsilon, \mbepsilon\sim \cN(\mbepsilon;\mbzero, \mbI)$, where $\alpha_t$ and $\sigma_t^2$ are the diffusion coefficient and variance, respectively. DMs then learn a neural network $\mbepsilon_{\mbtheta}(\mbx_t, t)$ that matches the score of corrupted data $\mbs_{\text{real}}(\mbx_t)=\nabla_{\mbx_t}\log p_{t, \text{real}}(\mbx_t)=-\sigma_t^{-1}(\mbx_t-\alpha_t\mbx_0)$ by minimizing loss $\E_{t, \mbx_t}\left[\omega(t)\|\mbepsilon_{\mbtheta}(\mbx_t, t)-\mbs_{\text{real}}(\mbx_t)\|^2\right]$, where $\omega(t)$ is a weighting function. Given a trained $\mbepsilon_{\mbtheta}(\mbx_t, t)$, the data are generated through an iterative denoising process with $T$ decreasing to $0$. 

% Distilling a pre-trained diffusion model into a small-size compact model that uses fewer steps for data generation has garnered significant attention. Distribution Matching Distillation~(DMD)~\cite{poole2022dreamfusion,wang2024prolificdreamer, yin2024one,yin2024improved} targets at training a single-step distilled student that matches the generated distribution from a pre-trained teacher model, through minimizing the following expectation over $t$ of a \emph{reverse} KL-divergence between the distributions from the student and teacher models, respectively:
% \begin{align} \label{eq:DMD-objective-function}
%     \E_{t}\left[\KL{p_{t,\text{fake}}}{p_{t,\text{real}}}\right]=\E_{\mbx_t}\left[\log\frac{p_{t,\text{fake}}(\mbx_t)}{p_{t,\text{real}}(\mbx_t)}\right]
% \end{align}
% With $\mbx_t$ parameterized as $\mbx_t:=\alpha_t g_{\mbphi}(\mbz)+\sigma_t\mbepsilon$, \cref{eq:DMD-objective-function}'s gradient with respect to $\mbphi$ can be written as:
% \begin{multline}    
%     \nabla_{\mbphi}\E_{t}\left[\KL{p_{t,\text{fake}}}{p_{t,\text{real}}}\right]\\
% =\E_{t,\mbz,\mbx_t}\left[\omega_t\alpha_t(\mbs_{\text{fake}}(\mbx_t,t)-\mbs_{\text{real}}(\mbx_t,t))\nabla_{\mbphi}g_{\mbphi}(\mbz)\right]
% \end{multline}
% where $\mbz\sim\cN(\mbzero,\mbI), t\sim\text{Unif}[0,1] $. The ``fake'' denoising model is obtained by minimizing a score-matching loss on fake data.

\noindent\textbf{Consistency Distillation} As one of the trajectory distillation methods, consistency distillation~(CM)~\cite{song2023consistency} studies a consistency function $f_{\mbphi}(\mbx_t, t)$ mapping an noisy image $\mbx_t$ back to the clean image $\mbx_0$ as $f_{\mbphi}(\mbx_t, t) = \mbx_0$. $f_{\mbphi}(\mbx_t, t)$ is usually parameterized as:
\begin{align}
    f_{\mbphi}(\mbx_t, t) = c_{\text{skip}}(t)\mbx_t+c_{\text{out}}(t)F_{\mbphi}(\mbx_t, t),
\end{align}
to satisfy the boundary condition at $t=0$, where $F_{\mbphi}$ is the actual neural network to train, and $c_{\text{skip}}(t)$ and $c_{\text{out}}(t)$ are time-dependent factors such that $c_{\text{skip}}(0) = 1, c_{\text{out}}(0) = 0$. 

While CM can be trained from scratch, a more popular choice for CM is to do distillation on pre-trained sampling trajectory, such as pre-trained DMs. In particular, the distillation objective function can be written as a distance metric between adjacent points  as:
\begin{align}
    \cL_{\text{CM}} = \E_{i}\left[\omega(t_i)d(\mbf_{\mbphi}(\mbx_{t_{i}+\Delta t}, t_{i}+\Delta t), \mbf_{\mbphi^-}(\widehat{\mbx}^{\mbtheta}_{t_{i}}, t_{i}))\right],
\end{align}
where $d(\cdot, \cdot)$ is a metric function, $\mbphi^-$ is the exponential moving average (EMA) of the past values $\mbphi$, and $\widehat{\mbx}^{\mbtheta}_{t_{i}}$ is obtained from pre-trained model as
$\widehat{\mbx}^{\mbtheta}_{t_{i}}=\mbx_{t_i}-(t_i-t_{i+1})t_{i+1}\nabla_{\mbx_{t_{i+1}}}\log p_{t_{i+1}}(\mbx_{t_{i+1}})$

Consistency Trajectory Model~(CTM)~\cite{kim2023consistency} is introduced to minimize the accumulated estimation errors and discretization inaccuracies in multi-step consistency model sampling. While CM projects a noisy image $\mbx_t$ to its clean image $\mbx_0$, CTM extends it by designing a projection function which starts from time step $t$ to its later step $s (s<t)$ as:
\begin{align} \label{eq:ctm-parameterization}
    f_{\mbphi}(\mbx_t, t, s) = s/t\cdot\mbx_t+(1-s)/t\cdot F_{\mbphi}(\mbx_t, t, s).
\end{align}
where $F_{\mbphi}()$ is the actual neural networks to be optimized. Equation~(\ref{eq:ctm-parameterization}) ensures $f_{\mbphi}(\mbx_t, t, s)$ satisfies the boundary condition when $f_{\mbphi}(\mbx_t, t, 1) = \mbx_1$. 

\textbf{Rectified flow methods} Rectified flow methods~\cite{lipman2022flow,liu2023flow,albergo2023stochastic} use ordinary differential equations~(ODEs) to transit between two distributions $p_0(\cdot)$ and $p_1(\cdot)$. Letting $\mbx_0\sim p_0(\mbx_0), \mbx_1\sim p_1(\mbx_1)$, rectified flows define a linear interpolation as $\mbx_t=t\mbx_1+(1-t)\mbx_0, 0\le t\le 1$. The model training may be achieved by:
\begin{align}
    \mbtheta=\argmin_{\mbtheta}\int \E\left[\|(\mbx_1-\mbx_0)-\mbv_{\mbtheta}(\mbx_t, t)\|^2\right]dt
\end{align}
$\widehat{\mbx}_0 = \mbx_1 +\int_1^0 \mbv_{\mbtheta}(\mbx_t,t)dt$. 

% $\mbmu_{\mbphi}(\mbx_1, t)=\mbx_1 +\int_1^t \mbv_{\mbtheta}(\mbx_t,t)dt$. 

% \begin{align}
%     \mbphi=\argmin_{\mbphi}\int \E\left[\|(\mbx_1-\mbx_0)-\frac{\mbmu_{\mbphi}(\mbx_1, t)}{\partial t}\|^2\right]dt
% \end{align}
% $\widehat{\mbx}_0=\mbmu_{\mbphi}(\mbx_1, 0)$.

Generating new image $\mbx_0$ from a white noise $\mbepsilon$ may follow the Ordinary Differential Equations~(ODEs) as:
\begin{align}
    d\mbx_t=\mbv(\mbx_t, t)dt, t\in [0, 1]
\end{align}
With the ground-truth \emph{velocity} setting as $d\mbx_t/dt = \mbx_1-\mbx_0$, the trained $\mbv(\mbx_t, t)$ is expected to approximate the straight direction from $\mbx_1$ to $\mbx_0=\mbepsilon$. As a result, rectified flows may use fewer steps than diffusion models to generate high-quality data.

There have been a few explorations on distilling a pre-trained rectified flow. InstaFlow~\cite{liu2023instaflow,zhu2025slimflow} proposes a direct distillation method which use one neural network to approximate the learned velocity's integration over the whole time. FlowDreamer~\cite{li2024flowdreamer} adopts score distillation sampling method to distill learned velocities. 

\begin{table*}[ht]
    \caption{Different objective functions in distilling pre-trained rectified flows}
    \label{tab:my_label}
    \centering
    \begin{tabular}{c|c|c}
    \bottomrule
    Methods     &  Reflow & InstaFlow \\ \hline
    Objective function     & $\displaystyle \mathbb{E}_{\mbx_1\sim \pi_1}\!\Biggl[\int_0^1 \Biggl\| \mbv_{\phi}(\mbx_t,t) - \int_{0}^1 \mbv(\mbx_{r},r)\,dr \Biggr\|^2\,dt\Biggr]$ & $\displaystyle \mathbb{E}_{\mbx_1\sim \pi_1}\!\Biggl[\Biggl\| \mbv_{\phi}(\mbx_1,1) - \int_{0}^1 \mbv(\mbx_{r},r)\,dr \Biggr\|^2\Biggr]$ \\
    \hline
    Methods     &   FlowDreamer & \modelname\ \\ \hline
    Objective function     & $\displaystyle \mathbb{E}_{\substack{\mbx_1\sim \pi_1 \\ t}}\!\Bigl[\Bigl\| \mbv_{\phi}(\mbx_t,t) - \mbv(\mbx_{t},t) \Bigr\|^2\Bigr]$ & $\displaystyle \mathbb{E}_{\mbx_1\sim\pi_1}\!\Biggl[\Biggl\| \frac{\partial G_{\phi}(\mbx_1, 1, t)}{\partial t} - \int_0^1 \mbv(\mbx_r, r)\,dr \Biggr\|^2\Biggr]$ \\
    \bottomrule
    \end{tabular}
\end{table*}

\section{Methodologies}
\label{sec:methodologies}
The proposed Trajectory Distillation Flows~(\modelname\ ) aim to distill a pre-trained rectified flow $\mbv(\mbx_t,t)$~\cite{esser2024scaling} into a ``one-step'' data generator $G_{\mbphi}(\mbx_1, 1, 0), \mbx_1\sim\cN(\mbzero,\mbI)$. This generator $G_{\mbphi}(\mbx_1, 1, 0)$ is expected to produce high-quality data without time-consuming iterative sampling procedures.

Given the pre-trained rectified flow $\mbv(\mbx_t, t)$, the clean image $\mbx_0$ can be obtained by solving the ODEs as $\widehat{\mbx}_0 = \text{ODE}[\mbv](\mbx_1)= \mbx_1+\int_1^0 \mbv(\mbx_s,s)ds$. Integration $\int_1^0 \mbv(\mbx_s,s)ds$ can be approximated using methods such as Euler forward method $\int_1^0 \mbv(\mbx_s,s)ds= \sum_{k=1}^K \mbv(\mbx_{t_k},{t_k})\Delta t_k$, where $t_1=0<t_2<\ldots<t_K<t_{K+1}=1$ represents the time steps taken to estimate the image. It is noted that such approximations may result in numerical errors. 


Similarly to $\mbx_0$, the intermediate variable $\mbx_t$ may be estimated as ${\mbx}_t = (1-t)\widehat{\mbx}_0 + t{\mbx}_1=\mbx_1 - (1-t)\int_1^0 v(\mbx_r, r)dr$, or ${\mbx}_t = \mbx_1 + \int_1^t v(\mbx_r, r)dr$. For example, we may approximate $\int_t^1 v(\mbx_r, r)dr$ as $\int_t^1 v(\mbx_r, r)dr\approx \mbv(\mbx_{1},1)(1-t)/2+\mbv(\mbx_{\frac{1+t}{2}},\frac{1+t}{2})(1-t)/2$.

\subsection{Formulating \modelname\ }
We use $G_{\mbphi}(\mbx_t, t, s)$ to denote \modelname, which takes current time step $t$, its state $\mbx_t$, and the terminating time step $s$ as inputs. $G_{\mbphi}(\mbx_t, t, s)$ is assumed to produce its state at time step $s$ as $\widehat{\mbx}_s:=G_{\mbphi}(\mbx_t, t, s)$. Since it needs to satisfy the boundary condition of $G_{\mbphi}(\mbx_1, 1, 1)=\mbx_1$, we may formulate it as:
\begin{align}
    G_{\mbphi}(\mbx_t, t, s) = \frac{s}{t}\mbx_t + (1-\frac{s}{t})g_{\mbphi}(\mbx_t, t, s)
\end{align}
In this way, $g_{\mbphi}(\mbx_t, t, s)$ would be unconstrained, and the neural network structure can be used to describe $g_{\mbphi}()$.

In fact, assume \modelname\ ' state $\mbx_s$ follows a velocity of $\mbmu(\mbx_r, r)$, $G_{\mbphi}(\mbx_t, t, s)$ can be understood as an integration of $\mbmu(\mbx_t, t)$, which is:
\begin{align}
    \mbx_s = \mbx_t + \int_t^s \mbmu(\mbx_r, r)dr
\end{align}

In the following parts, we describe the three regulators that refine the trajectory of our \modelname\ . These regulators are from the perspectives of output, velocity and trajectory. 


\subsection{Main components of \modelname\ }
\noindent\textbf{Output reconstruction}
We first regulate \modelname\ to produce outputs similar to the teacher pre-trained model. Following~\cite{liu2023instaflow,zhu2025slimflow}, we may formulate the output reconstruction loss as a root mean square error~(RMSE) between the two outputs at time step $0$:
\begin{align} \label{eq:loss-output-reconstruction}
 \cL_{\text{output}} =  \E_{\mbx_1\sim\mbpi_1}\left[\|G_{\mbphi}(\mbx_1, 1, 0)- \widehat{\mbx}_0\|^2\right]   
 \end{align}
where $G_{\mbphi}(\mbx_1, 1, 0)$ directly transits random noise $\mbx_1$ at time step $1$ to outputs at time step $0$. 
By minimizing $\cL_{\text{output}}$, \modelname\ learns to compress the mapping from a random noise $\mbx_1$ to the pre-trained rectified flow $\mbv(\mbx_t, t)$'s estimated sample $\widehat{\mbx}_0$ in one step. 
% Alternatively, we may formulate the disti in one stepllation loss as
% \begin{align}
%      \cL_{\text{Dis}} =  \E_{\mbx_1\sim\mbpi_1}\left[\int_0^1\omega(t)\|g_{\mbphi}(\mbx_1, t)- \widehat{\mbx}_t\|^2dt\right]
% \end{align}


\noindent\textbf{Velocity approximation}
In addition to reconstructing output, we consider the differences of velocities between \modelname\ and the pre-trained rectified flow, which is:
% \begin{align} 
%      \cL_{\text{vel}} =  \E_{\mbx_1, t}\left[\left\|\frac{\partial G_{\mbphi}(\mbx_1, 1, t)}{\partial t}- (\widehat{\mbx}_0-\mbx_1)\right\|^2\right]
% \end{align}
% or 

\begin{multline} \label{eq:loss-velocities}
\cL_{\text{vel}} =  \E_{\mbx_1, t}\Biggl[\Biggl\|\lim_{\Delta t\to 0}\frac{G_{\mbphi}(\mbx_t, t, t+\Delta t)-G_{\mbphi}(\mbx_t, t, t)}{\Delta t} - \\ 
 (\widehat{\mbx}_0-\mbx_1)\Biggr\|^2\Biggr]    
\end{multline}


Minimizing \cref{eq:loss-velocities} helps to regulate the trajectory's velocity such that the velocity at any time step $t$ can approximate the amount of changes from $t=1$ to $t=0$. 

It is noted that this velocity approximation is different from the reflow mechanism. These two approximations are based on different inputs. More specifically, reflow takes the time step $t$ and its current state $\mbx_t$ as inputs, whereas our proposed velocity approximation is fed with the time step $t$ and state at previous time step.  

Minimizing \cref{eq:loss-velocities} approximates the velocities between the distilled and pre-trained models. In particular, we may use auto-differentiation of $G_{\mbphi}(\mbx_1, 1, t)$ to compare the velocity estimated in the pre-trained model. This is much faster and computational convenient than comparing the integration. 

\noindent\textbf{Output consistency}
Inspired from Consistency model~\cite{song2023consistency} and Shotcut model~\cite{frans2024one}, we attach a self-consistency property of the proposed \modelname, which means that the projecting terminal of trajectory are the same given we have evaluated in the middle way as $G_{\mbphi}(\mbx_t, t, s) = G_{\mbphi}(\widehat{\mbx}_{t+d}, t+d, s), d = (s-t)/{2}$. We have two options as $\widehat{\mbx}_{t+d} = G_{\mbphi}(\mbx_t, t, t+d)$, or $ \widehat{\mbx}_{t+d} = \mbx_t + \int_t^{t+d}\mbv(\mbx_r, r)dr$. 

As a result, the loss regarding the consistency of output can be defined as:
\begin{align} \label{eq:loss-output-consistency}
    \cL_{\text{con}} = \E_{\mbx_1\sim\mbpi_1, s,t}\left[\|G_{\mbphi}(\mbx_t, t, s) - G_{\mbphi}(\widehat{\mbx}_{t+d}, t+d, s)\|^2\right]
\end{align}

Alternatively, we may adopt the soft consistency matching loss~\cite{kim2023consistency} as:
\begin{align}
    G_{\mbphi}(\mbx_t, t, s)\approx G_{\text{sg}(\mbphi)}(\mbx_t+\int_t^u \mbv(\mbx_r, r)dr, u, s)
\end{align}

\begin{figure*}
    \centering
    %\includesvg[width=1\linewidth]{figs/fram.svg}
    \includegraphics[width=1\linewidth]{figs/fram.png}
    \caption{Visualisations of our \modelname\ . Grey arrows represent the trajectories of pre-trained models, whereas yellow arrows correspond to the trajectories of the distillation models and the pink arrows correspond to the one step generation. We can see that in the pannel, we randomly choose $N= \{0,\frac{1}{4},\frac{1}{2},1\} $. Each yellow dot can show the velocity in the trajtorcy model and the shadow part is the approximated integration approximated in our \modelname\. We show three stages in approximation the one-way trajectory for one-step generation (a); the velocity approximation for N-way generation (b); the output consistency for N-way generation (c). }
    \label{fig:visualisation-on-trajectory-distillation-methods}
\end{figure*}
% \textbf{Example} When $g_{\mbphi}(\mbx_1, t): =\mbx_1 - (1-t)\mbv_{\mbphi}(\mbx_1, 1)$, we have:
% \begin{align}
%      \cL_{\text{Dis}} =   \E_{\mbx_1\sim\mbpi_1}\left[\int_0^1\omega(t)\|(1-t)\mbv_{\mbphi}(\mbx_1, 1)- \int_t^1 v(\mbx_r, r)dr\|^2dt\right]
% \end{align}

% \textbf{Example} When $g_{\mbphi}(\mbx_1, t): =\mbx_1 - (1-t)\mbv_{\mbphi}(\mbx_1, t)$, we have:
% \begin{align}
%      \cL_{\text{Dis}} =   \E_{\mbx_1\sim\mbpi_1}\left[\int_0^1\omega(t)\|(1-t)\mbv_{\mbphi}(\mbx_1, t)- \int_t^1 v(\mbx_r, r)dr\|^2dt\right]
% \end{align}



% \textbf{Incorporating adversarial loss}
% We may introduce an adversarial loss as:
% \begin{align}
%     \cL_{\text{adv}} = \log D_{\mbpsi}(\widehat{\mbx}_t,t) + \log (1-D_{\mbpsi}(\widehat{\mbx}^{\mbphi}_t, t))
% \end{align}
% where $D_{\mbpsi}(,)$ is the discriminator with $\mbx_t$ and $t$ as input, and where $\widehat{\mbx}^{\mbphi}_t=\mbx_t - (1-t)\mbv_{\mbphi}(\mbx_1, 1)$. 

\textbf{Objective function} The objective function may be written as:
\begin{align}
    \cL = \cL_{\text{output}}+\lambda_{\text{vel}}\cL_{\text{vel}}+\lambda_{\text{con}}\cL_{\text{con}}.
\end{align}
where $\lambda_{\text{vel}}$ and $\lambda_{\text{con}}$ are the weights for $\cL_{\text{vel}}$ and $\cL_{\text{vel}}$, respectively. 
% We usually set them as $\lambda_{\text{vel}} = XX, \lambda_{\text{con}}=XX$. 

% \begin{algorithm}
% \caption{Velocity Distillation Algorithm}
% \label{alg:velocity-distillation}
% \begin{algorithmic}[1]
% \REQUIRE Pre-trained rectified flow $\mbv(\mbx_t, t)$, 
% \ENSURE Optimized generator parameter $\mbtheta_*$
% \WHILE{not converge}
% \STATE Update $\mbphi$
% $$\E_{\mbx_0,t,\mbepsilon}\left[\omega(t)\left(\mbs_{\mbphi}(\mbx_t, t)\right)\frac{\partial \mbx_t}{\partial \mbtheta}\right]
% $$
% \STATE Update $\mbtheta$
% $$\E_{\mbx_0,t,\mbepsilon}\left[\omega(t)\left(\mbs_{\mbphi}(\mbx_t, t)-\mbepsilon^{\top}\nabla_{\mbx_t}\log \mbv(\mbx_t, t)\mbepsilon\right)\frac{\partial \mbx_t}{\partial \mbtheta}\right]
% $$
% \ENDWHILE
% \end{algorithmic}
% \end{algorithm}


% Several points:
% \begin{itemize}
%     % \item \textbf{done} use a new model $\mbmu_{\mbphi}$ to replace $\nabla_{\mbx_0}\log q_{\mbtheta}(\mbx_0)$
%     % \item \textbf{done} Consider integrated KL-divergence
%     % \item \textbf{done} Consider the adversarial loss case
%     \item Consider classifier-free technique.
%     % \item Consider other divergences, such as JS-divergence. May similar to the implicit distillatin paper. 
%  \end{itemize}


\subsection{Sampling}
\begin{algorithm}
\caption{Sampling procedure of \modelname\ }
\label{alg:sampling-algorithm}
\begin{algorithmic}[1]
\REQUIRE Trained \modelname\ $G_{\mbphi}(\mbx_t, t, s)$, pre-defined sampling steps $t_N=1>t_{N-1}>\ldots>t_0=0$
\ENSURE $\widehat{\mbx}_{0, 1}$
\STATE Start from $\mbx_{t_N}\sim \mbpi_1$
\FOR{$n=N, N-1, \ldots, 1$}
\STATE $\widehat{\mbx}_{t_{n-1}}= G_{\mbphi}(\widehat{\mbx}_{t_n}, t_n, t_{n-1})$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Connections to other distillation methods}
Our proposed \modelname\ has deep connections to existing distillation methods. 

\modelname\ uses the same model structure as the consistency trajectory model~(CTM)~\cite{kim2023consistency}. The CTM mainly optimise the self-consistency loss to optimize the distilled model. However, the CTM does not consider the output reconstruction loss or the velocity, whereas the velocity may lead to straight trajectory which can help reduce the number of steps to generate samples. 

Boot~\cite{gu2023boot} also considers the velocity alignment with the pre-trained model, by comparing the gradient of function. 

InstaFlow~\cite{liu2023instaflow} and SlimFlow~\cite{zhu2025slimflow} focus on output reconstruction. As velocity and self-consistency are not regularized, the trajectory is not well defined, which may result in unstable generation.  

Shortcut model~\cite{frans2024one} regulates the velocity behavior and self-consistent trajectories. As it estimates velocity, an approximated ODE solver is still needed. 




\begin{figure*}[!ht]
    \centering
    \includegraphics[width=2\columnwidth]{figs/ab.pdf}
    \caption{Training curves of training stages (a) and initialization(b).}
    \label{fig:tr}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=2\columnwidth]{figs/nsteps.pdf}
    \caption{$N$-step Generation by our 3M \modelname\ in $1,2,4$ evaluation times, on CIFAR-10 32$\times$32 image scale, using pre-trained Annealing 1-Rectified Flow as the teacher model.}
    \label{fig:NFE}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=2\columnwidth]{figs/params.pdf}
    \caption{One step generation by our \modelname\ in distilled model parameter varing from \{3M, 7M, 15M, 27M \}  on CIFAR-10 32$\times$32 image scale, using pre-trained Annealing 1-Rectified Flow as the teacher model.}
    \label{fig:params}
\end{figure*}

\begin{table}[ht]
    \centering
    \caption{Symbols and their descriptions in training \modelname.}
    \label{tab:notation}
    \begin{tabular}{@{}cc@{}}
        \toprule
        \textbf{Notion} & \textbf{Description} \\
        \midrule
        $\phi$  & Generator parameters \\
        $\psi$  & Discriminator parameters \\
        $\mbt$     & Current time step ($\mbt\in[0,1]$) \\
        $\mbs$     & Target time step for state projection \\
        $N$     & Number of steps/evaluations \\
        $\pi_{0}$ & Data distribution at $\mbt=0$ \\
        $\pi_{1}$ & Noise distribution at $\mbt=1$ \\
        $g$     & Residual function within $G_{\phi}$ \\
        $G$     & Distilled generator function \\
        $\mbz$     & variable from $\mathcal{N}(\mathbf{0},\mathbf{I})$ \\
        \bottomrule
    \end{tabular}
\end{table}
\section{Related Works} \label{sec:related}

% Pre-trained diffusion distillation methods can significantly reduce the computational cost associated with a high number of function evaluations, making them a promising direction for accelerating diffusion models. We categorize these distillation methods into three groups:

% \textbf{Output Reconstruction Based:} These methods focus on reconstructing image outputs. Specifically, \cite{salimans2022progressive, berthelot2023tract} focus on output values; \cite{poole2022dreamfusion, wang2024prolificdreamer, yin2024one, yin2024improved} concentrate on output distributions; \cite{lukoianov2024score, karras2022elucidating} operate in a one-step denoising image space; and \cite{zhou2024score, zhou2024adversarial} employ Fisher divergence.

% \textbf{Trajectory Distillation Based:} These methods distill the trajectory generation process. In particular, consistency distillation approaches \cite{song2023consistency, kim2023consistency, lu2024simplifying} focus on self-consistency, while \cite{liu2023instaflow, zhu2025slimflow} concentrate on rectified flow distillation. Additionally, \cite{frans2024one} integrates consistency modeling into rectified flows.

% \textbf{Adversarial Distillation Based:} These methods integrate an adversarial loss to better approximate the target distribution. For instance, \cite{sauer2024fast, sauer2025adversarial} employ a pre-trained classifier during training.

Pre-trained diffusion distillation methods have emerged as a powerful strategy to alleviate the significant computational burden inherent in diffusion models, which traditionally require a large number of function evaluations to produce high-quality samples. By compressing the iterative denoising process into far fewer steps, these techniques not only expedite sample generation but also make it feasible to deploy such models in low-resource environments. In this work, we systematically categorize these distillation approaches into three major groups based on their underlying objectives and operational paradigms.

\textbf{Output Reconstruction Based:} These methods aim to minimize the discrepancy between the outputs of the teacher (i.e., the pre-trained diffusion model) and the student model by directly reconstructing the image outputs. Some approaches, such as Progressive Distillation~\cite{salimans2022progressive, berthelot2023tract}, focus on aligning the output values by enforcing a close correspondence between the denoising steps of the teacher and student. Other methods, for instance SDS and its variants~\cite{poole2022dreamfusion, wang2024prolificdreamer, yin2024one, yin2024improved}, concentrate on matching the output distributions to preserve the statistical characteristics of the generated images. Additionally, certain works operate in a one-step denoising image space~\cite{lukoianov2024score, karras2022elucidating}, allowing for the direct generation of high-quality images from a single function evaluation, while others employ Fisher divergence objectives~\cite{zhou2024score, zhou2024adversarial} to more rigorously align the gradients of the score functions. Together, these techniques effectively reduce the number of sampling steps required while maintaining the fidelity of the generated outputs.

\textbf{Trajectory Distillation Based:} Instead of concentrating solely on the final output, trajectory-based methods focus on the entire denoising path—from the initial random noise to the eventual clean image. By distilling the full trajectory, these approaches ensure that the student model replicates not only the end result but also the dynamic behavior of the teacher model throughout the diffusion process. Consistency distillation techniques~\cite{song2023consistency, kim2023consistency, lu2024simplifying} emphasize the self-consistency of the denoising trajectory, ensuring stable and accurate progression across different time steps. In contrast, rectified flow distillation methods such as InstaFlow and SlimFlow~\cite{liu2023instaflow, zhu2025slimflow} focus on producing a straighter, more direct trajectory, thereby mitigating the accumulation of approximation errors that typically arise from curved paths. Moreover, recent studies have demonstrated that integrating consistency modeling directly into rectified flows~\cite{frans2024one} can further enhance the fidelity of the generated trajectories, effectively combining the strengths of both approaches.

\textbf{Adversarial Distillation Based:} A third category of distillation methods leverages adversarial learning to refine the student model's output distribution. By incorporating an adversarial loss—often implemented via a pre-trained classifier or discriminator—these methods drive the student model to more closely approximate the target distribution provided by the teacher. Notably, works such as~\cite{sauer2024fast, sauer2025adversarial} have successfully employed this strategy to achieve competitive performance with significantly fewer sampling steps. The adversarial framework not only enhances the perceptual quality of the generated images but also provides a flexible plug-and-play mechanism that can complement other distillation strategies.



\section{Experiment} \label{sec:exp}

\subsection{Experimental Setup} \label{subsec:setup}
\textbf{Notations} We list the notation to train our \modelname, specify the training parameters and data in teacher, student models. Refer to Table~\ref{tab:notation} for more information. \\
\textbf{Datasets} For fair comparision, We evaluate \modelname\ on traditional real-world datasets, CIFAR-10, and extend our model on large scale datasets, FFQH-64 and ImageNet. Further details regarding these datasets can be found in Table~\ref{tab:data}.\\
\textbf{Pre-trained Models} We initialized our trajectory model $ G_{\phi}(\mathbf{x}_{t}, t, s) $ with open-source pre-trained velocity checkpoints $ V_{\theta}(\mathbf{x}_{t}, t) $, and subsequently distilled it into an $ N $-step model. Specifically, for the CIFAR-10 dataset, we distilled the model from the EDM and 1-Rectfiled Flow, follow by~\cite{zhu2025slimflow}. We choose EDM in~\cite{kim2023consistency} and ADM~\cite{albergo2023stochastic} as the teacher model respectively for ImageNet and FFHQ-64 dataset.  \\
\textbf{Model Architecture}  To ensure fair comparisons across datasets, we adopt different model architectures that follow the design choices in \cite{kim2023consistency,song2023consistency,zhu2025slimflow}. Specifically, for the CIFAR-10 dataset we use the Noise Conditioned Score Networks~(\texttt{NCSN++}) architecture in~\cite{karras2022elucidating}, for ImageNet we employ the Ablated Diffusion Model~(\texttt{ADM}) architecture as detailed in~\cite{dhariwal2021diffusion}. We compare in Table~\ref{tab:unet} w.r.t. ResNetBlock, AttentionBlocks and SDE function corresponding to their specific U-Net design.\\
\textbf{Training Hyperparameters}  We follow the optimization argument from SlimFlow~\cite{zhu2025slimflow}. Table~\ref{tab:hyper_cifar} shows the optimization hyperparameters. \\
\textbf{Evaluation Metrics.} We assess \modelname\ for unconditioned image generation using Precision and Recall~\cite{kynkaanniemi2019improved}, the Inception Score (IS)~\cite{salimans2016improved}, and the Fréchet Inception Distance (FID)~\cite{heusel2017gans}. We also report parameter counts, FLOPs and MACs to illustrate the distilled model’s efficiency. Additional details can be found in Appendix~\ref{app:metrics}.\\
\textbf{Time Efficiency} We assess the time efficiency of \modelname\ across various loss functions by measuring both the evaluation time of $G$ and the performance of the ODE solver. Table~\ref{tab:time} summarizes the throughput results (imgs/sec./GPU). 

% \begin{table*}[!htbp]
% \centering
% \caption{Training configuration of~\modelname~ varying different model size on CIFAI-10.}
% \label{tab:hyper_cifar}
% \begin{tabular}{lccccc}
% \toprule
% & \textbf{Hyperparameters} & \textbf{CIFAI-10 3M} & \textbf{CIFAI-10 7M} & \textbf{CIFAI-10 15M} & \textbf{CIFAI-10 28M} \\
% \midrule
% \multirow{4}{*}{\textbf{U-Net}}       
% & Base channels                 &                     &                     &                      &                      \\
% & Channel multipliers           &                     &                     &                      &                      \\
% & Attention resolution          &                     &                     &                      &                      \\
% & Blocks                        &                     &                     &                      &                      \\
% \midrule
% \multirow{8}{*}{\textbf{Optimization}}
% & Learning rate $\times 10^{4}$ &                     &                     &                      &                      \\
% & Warm up                       &                     &                     &                      &                      \\
% & Batch                         &                     &                     &                      &                      \\
% & Images trained                &                     &                     &                      &                      \\
% & Optimizer                     &                     &                     &                      &                      \\
% & Dataset flips                 &                     &                     &                      &                      \\
% & Optimizer                     &                     &                     &                      &                      \\
% & Dropout probability           &                     &                     &                      &                      \\
% & Mixed-Precision (FP16)        &                     &                     &                      &                      \\
% \midrule
% \multirow{4}{*}{\textbf{Score}}
% & ODE Solver                    &                     &                     &                      &                      \\
% & EMA decay rate                &                     &                     &                      &                      \\
% & LR ramp-up (Mimg)            &                     &                     &                      &                      \\
% & EMA half-life (Mimg)         &                     &                     &                      &                      \\
% \bottomrule
% \end{tabular}
% \end{table*}





\begin{table}[!htbp]
\centering
\caption{Time Efficiency (imgs/sec./H100) of \modelname\ with different loss functions per datasets. (All values are approximate estimates)}
\label{tab:time}
\resizebox{0.35\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
       & E.q.~\ref{eq:loss-output-reconstruction} & E.q.~\ref{eq:loss-velocities} & E.q.~\ref{eq:loss-output-consistency} \\
\midrule
\textbf{CIFAR-10} & 400 & 400 & 100 \\
\textbf{ImageNet}  & 400 & 400 & 100 \\
\textbf{FFHQ-64}   & 160 & 160 & 20  \\
\bottomrule
\end{tabular}
}
\end{table}




\begin{table*}[htbp]
\centering
\caption{Comparison of different DMs by $N$-step generation performance on CIFAR-10 at corresponding model scales. Bold red numbers indicate distilled models' parameters. We report sample quality (FID and IS) for model sizes ($4\texttt{MB}, 7\texttt{MB}, 15\texttt{MB}, 28\texttt{MB}$) across N-step generation ($N = 1, 2, and 4$) with a shadow background.}
\label{tab:cifar}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{ccccccl}
\toprule
\multirow{2}{*}{\textbf{NFE}$\downarrow$} & \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Model Size}} & \multicolumn{2}{c}{\textbf{Generation Quality}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-7}
 &  & \textbf{FLOPs (G)} & \textbf{MACs (G)} & \textbf{Params (M)} & \textbf{FID}$\downarrow$ & \textbf{IS}$\uparrow$ \\
\midrule
50 & DDIM~\cite{song2020denoising} & 12.2 & 6.1  & 35.7 & 4.67 & --- \\
20 & DDIM~\cite{song2020denoising} & 12.2 & 6.1  & 35.7 & 6.84 & --- \\
10 & DDIM~\cite{song2020denoising} & 12.2 & 6.1  & 35.7 & 8.23 & --- \\
10 & DPM-solver-2~\cite{lu2022dpm}   & 12.2 & 6.1  & 35.7 & 5.94 & --- \\
10 & DPM-solver-fast~\cite{lu2022dpm}  & 12.2 & 6.1  & 35.7 & 4.70 & --- \\
10 & 3-DEIS~\cite{zhang2022fast}        & 20.6 & 10.3 & 61.8 & 4.17 & --- \\
\midrule 
2  & PD~\cite{salimans2022progressive}   & 41.2 & 20.6 & 55.7 & 5.58 & 9.05 \\
2  & CD~\cite{song2023consistency}        & 41.2 & 20.6 & 55.7 & 2.93 & 9.75 \\
2  & CT~\cite{song2023consistency}        & 41.2 & 20.6 & 55.7 & 5.83 & 8.85 \\
  4 & \modelname \textit{-16M}      & 7.4  & 3.7  & \red{\textbf{15.7}} & 5  & --- \\
  4 & \modelname \textit{-28M}     & 13.2 & 6.6  & \red{\textbf{27.9}} & 4  & --- \\
  2 & \modelname \textit{-16M}      & 7.4  & 3.7  & \red{\textbf{15.7}} & 5  & --- \\
  2 & \modelname  \textit{-28M}    & 13.2 & 6.6  & \red{\textbf{27.9}} & 4  & --- \\
\midrule
1  & 1-Rectified Flow (+Distill)~\cite{liu2022flow} & 20.6 & 10.3 & 61.8 & 378 (6.18) & 1.13 (9.08) \\
1  & 2-Rectified Flow (+Distill)~\cite{liu2022flow} & 20.6 & 10.3 & 61.8 & 12.21 (4.85) & 8.08 (9.01) \\
1  & 3-Rectified Flow (+Distill)~\cite{liu2022flow} & 20.6 & 10.3 & 61.8 & 8.15 (5.21) & 8.47 (8.79) \\
  1 & \modelname\textit{-16M}      & 7.4  & 3.7  & \red{\textbf{15.7}} & 5.8  & --- \\
  1 & \modelname\textit{-28M}      & 13.2 & 6.6  & \red{\textbf{27.9}} & 4.5  & --- \\
\bottomrule
\end{tabular}%
}
\end{table*}



\begin{table}[!htbp]
\centering
\normalsize
\caption{Comparison of different DMs by $N$-step generation performance on FFHQ-64 at corresponding model scales. Bold red numbers indicate distilled models' parameters. We report sample quality (FID and IS) for model sizes ($16\texttt{MB}$) across N-step generation ($N = 1, 2, 4$) with a shadow background.}
\label{tab:ffhq}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c l c c c c c}
\toprule
\multirow{2}{*}{\textbf{NFE}$\downarrow$} & \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Model Size}} & \multicolumn{2}{c}{\textbf{Generation Quality}} \\
\cmidrule(lr){3-5}\cmidrule(lr){6-7}
 &  & \textbf{FLOPs} & \textbf{MACs} & \textbf{Params} & \textbf{FID}$\downarrow$ & \textbf{Prec.}$\uparrow$ \\
\midrule
79 & EDM~\cite{karras2022elucidating}   & 167.9  & 82.7  & 55.7 & 2.47  &  \\
10 & DDIM~\cite{song2020denoising}         & 167.9  & 82.7  & 55.7 & 18.30 &  \\
\midrule
5  & AMED-Solver~\cite{zhou2024fast}       & 167.9  & 82.7  & 55.7 & 12.54 &  \\
 
2  & \modelname\textit{-16M}                        & 30.4   & 14.8  & \red{\textbf{15.7}} & 7   & --- \\
\midrule
1  & Boot~\cite{gu2023boot}               & 52.1   & 25.3  & 66.9 & 9.00  &  \\
1  & SlimFlow~\cite{zhu2025slimflow}        & 53.8   & 26.3  & 27.9 & 7.21  &  \\
1  & SlimFlow~\cite{zhu2025slimflow}        & 30.4   & 14.8  & 15.7 & 7.70  &  \\
 
1  & \modelname\textit{-16M}                        & 30.4   & 14.8  & \red{\textbf{15.7}} & 7   & --- \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table}[!htbp]
\centering
\normalsize
\caption{Comparison of different DMs by $N$-step generation performance on ImageNet at corresponding model scales. Bold red numbers indicate distilled models' parameters. We report sample quality (FID and IS) for smaller model sizes ($81\texttt{MB}$) across N-step generation ($N = 1, 2, 4$) with a shadow background.}
\label{tab:img}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{c l c c c c c}
\toprule
\multirow{2}{*}{\textbf{NFE}$\downarrow$} & \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Model Size}} & \multicolumn{2}{c}{\textbf{Generation Quality}} \\
\cmidrule(lr){3-5}\cmidrule(lr){6-7}
 &  & \textbf{FLOPs} & \textbf{MACs} & \textbf{Params} & \textbf{FID}$\downarrow$ & \textbf{Prec.}$\uparrow$ \\
\midrule
250 & ADM~\cite{karras2022elucidating}    &  &  &  & 2.07 & 0.74 \\
79  & EDM~\cite{dhariwal2021diffusion}      & 219.4 & 103.4 & 295.9 & 2.44 & 0.71 \\
\midrule
2   & PD~\cite{salimans2022progressive}       & 219.4 & 103.4 & 295.9 & 15.39 & 0.59 \\
2   & CD~\cite{song2023consistency}            & 219.4 & 103.4 & 295.9 & 4.70 & 0.69 \\
2   & CT~\cite{song2023consistency}            & 219.4 & 103.4 & 295.9 & 11.10 & 0.69 \\
 
2   & \modelname\textit{-81M}                           & 67.8  & 31.0  & \red{\textbf{80.7}}  & 12 &  \\
\midrule
1   & CD~\cite{song2023consistency}            & 219.4 & 103.4 & 295.9 & 6.20 & 0.68 \\
1   & CT~\cite{song2023consistency}            & 219.4 & 103.4 & 295.9 & 13.00 & 0.71 \\
1   & SlimFlow~\cite{zhu2025slimflow}           & 67.8  & 31.0  & 80.7  & 12.34 &  \\
 
1   & \modelname\textit{-81M}                           & 67.8  & 31.0  & \red{\textbf{80.7}}  & 12 &  \\
\bottomrule
\end{tabular}
}
\end{table}


\begin{table*}[!htbp]
\centering
\caption{Ablation study w.r.t initialization from, choices of teacher model, divergence and loss function, and approximation time in Euler Solver of 3MB \modelname\ on CIFAR-10.}
\label{tab:ab}
\begin{tabular}{c c c c c c c}
\toprule
\textbf{Initialize from $\theta$} & \textbf{Teacher Model} & \textbf{Divergence $\mathcal{D}$} & $E.q.~\ref{eq:loss-output-reconstruction}$ & $E.q.~\ref{eq:loss-velocities}$&
$E.q.~\ref{eq:loss-output-consistency}$& \textbf{FID}$\downarrow$ \\
\midrule
+ & EDM & L2 & + & - & - & 16 \\
- & EDM & L2 & + & - & - & 15\\
- & 2-Rectified Flow & L2 & + & - & - & 14\\
- & 2-Rectified Flow & LPIPS & + & - & - & 13\\
- & 2-Rectified Flow & LPIPS & + & + & - & 13\\
- & 2-Rectified Flow & LPIPS & + & + & + & 13\\
- & 2-Rectified Flow & LPIPS & + & + & 2 & 13\\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Convergence  of Training \modelname\ Model} \label{subsec:training}
Since we employ a velocity model $v_{\theta}(\mbx_{t},t)$ as the teacher to distill a generator model $g_{\phi}(\mbx_{t},t,s)$ as the student, 
the training process encounters two challenges. First, the network must learn to transition from velocity estimation to generator output; second, the teacher model provides limited support for learning the intermediate time step $s$. 

To address the first knowledge transmission challenge, we adopt a two-stage training strategy. Initially, we train a generator-based consistency model $g_{\phi}(.|t)$  to guide the network in transitioning from learning the velocity at time $t$ to learning the generator at time $t$ (i.e., training Eq.~\ref{eq:loss-output-consistency}. Subsequently, we perform sample reconstruction in the image space while simultaneously conducting imitation learning for the velocity (i.e., training Eqs.~\ref{eq:loss-output-reconstruction} and ~\ref{eq:loss-velocities}. In Figure~\ref{fig:tr} (a), we compare two training schemes—one where Eqs.~\ref{eq:loss-output-reconstruction}, ~\ref{eq:loss-velocities}, and ~\ref{eq:loss-output-consistency} are trained jointly, and another where Eq.~\ref{eq:loss-output-consistency} is trained first followed by Eqs.~\ref{eq:loss-output-reconstruction} and ~\ref{eq:loss-velocities}—and report the evolution of FID over iterations.

For the second challenge, note that different teacher models support different input formats. Unlike the ADM model, the EDM model does not support conditional generation (it accepts only the sample at time $t$ and the time $t$ itself). Therefore, we override the EDM U-Net module by encoding $s$ using the time encoding for $t$, which is then inserted into the ResNet and attention modules. Moreover, we reuse the initialization of the $t$ module for training our model. Figure~\ref{fig:tr}(b) compares the FID evolution over iterations when training \modelname\ with and without this initialization.




\subsection{$N$-Step Generation for Distilled Models} 
\label{subsec:small}

In this section, we show the effectiveness of our \modelname\ in $N$-step generation, varying various parameter scales. We compare the generation quality among models of different sizes under different Numbers of Function Evaluations (NFE)~\cite{lu2022dpm}, which is the number of calls $V_{\phi}(.)/ g_{\theta}(.)$ to the velocity/trajortory models.

By specifying the depth and kernal size in ResNet and Attetion Moudle from U-Net frramework in student Models $g_{\phi}()$, We distill several smaller \modelname\ from teacher models. Simultaneously, training \modelname\ in Eq.~\ref{eq:loss-output-consistency} can contain the consistency trajectory in generation $N$-step samples, we can specify the ODE solver to report the $N$-step generation.  

\textbf{32$\times$32 Datassets}~We evaluated the generation performance of our distilled models with varying sizes across different datasets. By leveraging the differences in ResNet and Attention modules within the U-Net architecture, we can distill student models with fewer parameters tailored to the teacher model, as detailed in Table~\ref{tab:hyper_cifar}.

Table~\ref{tab:cifar} reports the performance of our models on the CIFAR-10 dataset. For the teacher models, we selected EDM and 2-Rectiflow to distill models of various sizes, and we also present generation performance under different step counts.


\textbf{64$\times$64 Datasets} Furthermore, Tables~\ref{tab:ffhq}, ~\ref{tab:img} reports the performance of our models on 64×64 datasets. Following the approach outlined in~\cite{zhu2025slimflow}, we employed the 2-Rectified model as the teacher model and distilled $7,15,28,32MB$ models varying the neural network units in U-Net specified in Table~\ref{tab:hyper_cifar}.

Figure~\ref{fig:NFE} demonstrates the $N$-step generation quality achieved by our \modelname, showing that our trajectory model can generate high-quality samples with only a few function evaluations—even achieving effective results in a single step. The distilled model significantly accelerates sampling while maintaining excellent sample quality. Furthermore, Figure~\ref{fig:params} illustrates that our distilled \modelname, across varying parameter sizes \{3M, 7M, 15M, 27M \}, is capable of 1-step generation without compromising sample quality.


\subsection{Ablation Studies} \label{subsec:abla}

In this chapter, we analyze the component selection in training our \modelname. Specifically, we investigate whether the quality of generated samples from our trained distilled model is influenced by (i) whether our student network, conditioned on $(\mbx_{t}, t, s)$, is initialized from the teacher network conditioned on $(\mbx_{t}, t)$; (ii) the choice of the pre-trained teacher model; (iii) the divergence function adopted in Eq.~\ref{eq:loss-output-reconstruction}; (iv) the velocity correction loss in Eq.~\ref{eq:loss-velocities}; (v) the consistency loss in Eq.~\ref{eq:loss-output-consistency}; and (vi) the number of evaluations in the Euler solver for the consistency loss in Eq.~\ref{eq:loss-output-consistency}. We report the corresponding ablation results via FID in Table~\ref{tab:ab}.

\section{Conclusion} \label{sec:conc}
In this paper, we presented \modelname, a trajectory distillation framework that effectively compresses pre-trained rectified flows into a compact generator. Our approach simultaneously aligns the output, velocity, and self-consistency of the distilled model with the teacher model.
%and the incorporation of an adversarial loss further improves sample fidelity and diversity
Extensive experiments on CIFAR-10, CelebA, FFHQ-64, and ImageNet demonstrate that \modelname\ achieves competitive generation quality with significantly fewer sampling steps and reduced computational cost. This work highlights the potential of trajectory distillation for accelerating generative models and paves the way for future research on efficient and high-quality image synthesis.

% \section*{Impact Statement}


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\newpage

\bibliography{uai2025-template}

\newpage

\onecolumn

\title{Title in Title Case\\(Supplementary Material)}
\maketitle



\appendix

\begin{table*}[!htbp]
\centering
\caption{U-Net Comparison employed in distilled models varing datasets.}
\label{tab:unet}
\begin{tabular}{lc|c|c}
\bottomrule
\multirow{2}{*}{\textbf{Components}} & \multirow{2}{*}{\textbf{Functions}} & \textbf{NCSN++} & \textbf{ADM} \\
 & &  (CIFAR-10,FFHQ-64)  & (ImageNet)  \\
\hline
\multirow{4}{*}{ResNetBlock}
 & Resampling filter            & Bilinear   & Box         \\
 & Noise embedding              & Fourier    & Positional  \\
 & Skip connections  & Residual   & -          \\
 & Residual blocks per resolution & 4        & 3           \\
 
\hline
\multirow{4}{*}{AttentionBlock}
 & Resolutions          & ${16}$     & ${32, 16, 8}$ \\
 & Heads                & 1          & 6--9--12      \\
 & Blocks in DownSampling    & 4          & 9             \\
 & Blocks in UpSampling    & 2          & 13            \\
 \hline
 \multirow{2}{*}{ScoreFunction}& Vanillia forward SDE    & VP          &      VE       \\
 & Denoise Condition & -        & label           \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!htbp]
\centering
\caption{Training configuration of~\modelname~varying different model sizes on CIFAI-10.}
\label{tab:hyper_cifar}
\begin{tabular}{lccccc}
\toprule
& \textbf{Hyperparameters} & \textbf{CIFAI-10 3M} & \textbf{CIFAI-10 7M} & \textbf{CIFAI-10 15M} & \textbf{CIFAI-10 28M} \\
\midrule
\multirow{4}{*}{\textbf{U-Net}}       
& Base channels                 & 128                 & 128                 & 192                  & 256                  \\
& Channel multipliers           & 1, 2, 2, 4          & 1, 2, 2, 4          & 1, 2, 4, 4           & 1, 2, 4, 4           \\
& Attention resolution          & 16                  & 16                  & 8                    & 8                    \\
& Blocks                        & 2, 2, 2, 2          & 2, 2, 2, 2          & 2, 2, 2, 2           & 2, 2, 2, 2           \\
\midrule
\multirow{9}{*}{\textbf{Optimization}}
& Learning rate $\times 10^{4}$ & 1.5                 & 1.0                 & 0.75                 & 0.5                  \\
& Warm up                       & 5k iterations       & 5k iterations       & 5k iterations        & 5k iterations        \\
& Batch                         & 64                  & 64                  & 64                   & 64                   \\
& Images trained                & 50M                 & 50M                 & 50M                  & 50M                  \\
& Optimizer                     & AdamW               & AdamW               & AdamW                & AdamW                \\
& Dataset flips                 & Enabled             & Enabled             & Enabled              & Enabled              \\
& Weight decay                  & 0.01                & 0.01                & 0.01                 & 0.01                 \\
& Dropout probability           & 0.1                 & 0.1                 & 0.1                  & 0.1                  \\
& Mixed-Precision (FP16)        & Enabled             & Enabled             & Enabled              & Enabled              \\
\midrule
\multirow{4}{*}{\textbf{Score}}
& ODE Solver                    & RK45                & RK45                & RK45                 & RK45                 \\
& EMA decay rate                & 0.999               & 0.999               & 0.999                & 0.999                \\
& LR ramp-up (Mimg)             & 1                   & 1                   & 1                    & 1                    \\
& EMA half-life (Mimg)          & 10                  & 10                  & 10                   & 10                   \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table}[!htbp]
    \caption{Experimental Details of Datasets.}
    \label{tab:data}
    \begin{tabular}{ccccc}
    \toprule
        \textbf{Data Shape}  & \textbf{Dataset}  & \textbf{Samples} &\textbf{Data Size}\\
    \midrule
         \multirow{2}{*}{$(3\times32\times32$)}& CIFAR-10 &  60K &  160M \\
         & CelebA &  60K &  160M \\
         
    \midrule
         \multirow{2}{*}{$(3\times64\times64$)}
         & FFHQ-64 &  70K & 2 GB\\
         & ImageNet & 1.2M & 6 GB \\

    \bottomrule
    \end{tabular}
\end{table}

\section{Technical Details}
\label{app:Technical}
\subsection{Datasets}
\label{app:dataset}
\textbf{CIFAR-10}~\footnote{\url{https://www.cs.toronto.edu/~kriz/cifar.html}}: This dataset contains 60,000 32×32 color images evenly distributed over 10 distinct classes (6,000 images per class). It is split into a training set of 50,000 images and a test set of 10,000 images, making it a standard benchmark in machine learning and computer vision.


\textbf{FFHQ-64}~\footnote{\url{https://github.com/NVlabs/ffhq-dataset}}: The original FFHQ dataset consists of 70,000 high-quality images at 1024×1024 resolution. It is not categorized into specific classes; rather, it is used for tasks like generative modeling of faces. For our experiments, we downscale the images to 64×64, maintaining the dataset's focus on high-quality facial imagery without predefined class labels. The dataset is provided in LMDB format for efficient access and storage. Packaged files and usage instructions are available in an anonymous repository.


\textbf{ImageNet}~\footnote{\url{http://www.image-net.org/}}: ImageNet is one of the most influential benchmarks in computer vision. It comprises over 1.2 million training images and around 50,000 validation images, categorized into 1,000 diverse classes. The dataset’s vast scale and rich annotations have made it an essential resource for developing and evaluating deep learning models.

    
\subsection{Metrics}
\label{app:metrics}
\textbf{Precision and Recall.} 
Following the approach in~\cite{dhariwal2021diffusion}, we obtain these two metrics by evaluating 10k real images and 10k synthesized images. In a pre-trained feature space, we use nearest neighbors to define the manifolds for both real and generated data. Precision indicates the fraction of generated samples that lie within the manifold of real data, whereas Recall measures the fraction of real data that falls within the manifold of generated samples. Since Precision is related to fidelity (how realistic the samples appear) and Recall is associated with diversity (how varied the samples are), both metrics complement each other. 

\medskip\noindent
\textbf{Inception Score (IS).} IS is influenced by both fidelity and diversity. We compute IS from the logits of a pre-trained Inception Network~\cite{szegedy2016rethinking} trained on ImageNet~\cite{russakovsky2015imagenet}. A high IS arises when the model classifies each sample with strong confidence and, collectively, covers multiple classes. However, when dealing with face datasets such as FFHQ or CelebA, the limited possibility of assigning samples to distinct categories (e.g., 26 classes) makes it hard to capture diversity, causing the IS to predominantly reflect fidelity. We measure IS with 10k generated images.
For our implementation, we employ implement of Precision, Recall and and IS from ADM repository~\footnote{\url{https://github.com/openai/guided-diffusion}}.

\medskip\noindent
\textbf{FID.} 
To evaluate Fréchet Inception Distance (FID), we approximate real and generated data as Gaussians in the Inception feature space and measure their Wasserstein distance, thus taking into account both fidelity and diversity. For our implementation, we adopt clean-FID~\footnote{\url{https://github.com/GaParmar/clean-fid}} to compute FID.

\medskip\noindent
\textbf{Parameter counts (B).} We report the total number of parameters in the model in billions (B), counting of a neural network is determined by summing the learnable parameters. . The count is determined by summing all learnable weights (e.g., convolutional kernels, dense layer matrices) and biases. A higher parameter count may enable richer representation learning, while a lower count suggests a compact architecture suitable for resource-constrained environments. For transformer-based models, we include all attention weights and feed-forward network parameters.

\medskip\noindent
\textbf{FLOPs.} FLOPs quantify the total floating-point operations (additions and multiplications) required for a single forward pass of the model. For a convolutional layer with kernel size 
H×W, FLOPs are calculated as:
\begin{equation*}
\text{FLOPs} = 2 \times H \times W \times k^2 \times C_{\text{in}} \times C_{\text{out}}.
\end{equation*} 
This metric captures the overall computational complexity and serves as an indicator of efficiency. Lower FLOP counts generally imply a more efficient model, which is crucial for real-time applications and deployment on resource-constrained devices.

\medskip\noindent
\textbf{MACs.} Multiply–accumulate operations (MACs) provide a practical estimate of the model’s computational workload during inference. MACs measure fused multiply-add operations, which are commonly optimized as a single instruction in hardware (e.g., GPUs and TPUs). For the same convolutional layer, MACs are defined as:
\begin{equation*}
\text{MACs} = H \times W \times k^2 \times C_{\text{in}} \times C_{\text{out}}.
\end{equation*}
MACs directly correlate with hardware efficiency, as modern accelerators often optimize for MAC throughput. We calculate MACs using the \texttt{calflops} library\footnote{\url{https://github.com/MrYxJ/calculate-flops.pytorch}} and emphasize their relevance for deployment on edge devices.
\end{document}
