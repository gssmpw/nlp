







\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}%


\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{todonotes}
\usepackage{tabularx} %
\usepackage{eso-pic}

\usepackage[english, capitalise]{cleveref}
\usepackage[acronym,toc,shortcuts]{glossaries}

\newacronym{llm}{LLM}{Large Language Model}
\newacronym{ehr}{EHR}{Electronic Health Record}
\newacronym{loinc}{LOINC}{Logical Observation Identifiers Names and Codes}
\newacronym{gbm}{GBM}{Gradient Boosted Machine}
\newacronym{auroc}{AUROC}{Area Under the Receiver Operating Characteristic Curve}
\newacronym{femr}{FEMR}{Framework for Electronic Medical Records}

\glsdisablehyper

\urldef\cpturl\url{https://gist.github.com/lieldulev/439793dc3c5a6613b661c33d71fdd185}
\urldef\icdurl\url{https://hcup-us.ahrq.gov/toolssoftware/procedureicd10/procedure_icd10_archive.jsp}
\urldef\cvxurl\url{https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=cvx}


\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%
\newtheorem{proposition}[theorem]{Proposition}%

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom


\begin{document}

\AddToShipoutPictureBG*{%
  \begin{tikzpicture}[remember picture, overlay]
    \node[anchor=north, yshift=-1cm] at (current page.north) {%
      \parbox{0.95\textwidth}{\centering
        \textbf{\textcolor{red}{
          Warning: We have been informed that our experiments contain label leakage due to the inclusion of the current stay length in the EHR serialization. Removing this artifact is expected to impact performance results. We are in the process of rerunning our experiments and will update our findings accordingly.
        }}
      }
    };
  \end{tikzpicture}%
}

\title[Large Language Models are Powerful EHR Encoders]{Large Language Models are Powerful EHR Encoders}


\author[1,2]{\fnm{Stefan} \sur{Hegselmann}}

\author[1]{\fnm{Georg} \spfx{von} \sur{Arnim}} %

\author[1]{\fnm{Tillmann} \sur{Rheude}} %

\author[1]{\fnm{Noel} \sur{Kronenberg}} %

\author[3,4]{\fnm{David} \sur{Sontag}} %

\author[2]{\fnm{Gerhard} \sur{Hindricks}} %

\author[1]{\fnm{Roland} \sur{Eils}} %

\author[1]{\fnm{Benjamin} \sur{Wild}} %


\affil[1]{\orgdiv{Center for Digital Health}, \orgname{Berlin Institute of Health (BIH), Charité - University Medicine Berlin}, \city{Berlin}, \country{Germany}}

\affil[2]{\orgname{German Heart Center of the Charité}, \city{Berlin}, \country{Germany}}

\affil[3]{\orgdiv{Computer Science and Artificial Intelligence Laboratory (CSAIL)}, \orgname{Massachusetts Institute of Technology (MIT)}, \city{Cambridge}, \country{MA, USA}}

\affil[4]{\orgdiv{Layer Health, Inc.}, \country{MA, USA}}

\vspace{-4cm} 
\abstract{
\acp{ehr} offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific \ac{ehr} foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose \acp{llm} based embedding methods as \ac{ehr} encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of \acp{llm} pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art \ac{llm}-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHR-specific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that \ac{llm}-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying \ac{llm} and the available context window. Overall, our findings demonstrate that repurposing \acp{llm} for \ac{ehr} encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional \ac{ehr} modeling and facilitating more interoperable and generalizable healthcare applications.
}





\maketitle


\section{Introduction}
\label{sec:introduction}
\acp{ehr} have become a widespread technology in modern healthcare, providing a comprehensive, longitudinal view of a patient’s health status \cite{dash_big_2019}. Machine learning methods can leverage this rich data to perform risk stratification and support clinical decision making \cite{ahsan_retrieving_2024, rajkomar_machine_2019, xu_layoutxlm_2021}. In recent years, researchers have explored a variety of prediction tasks based on \acp{ehr}, including hospital readmission \cite{golas_machine_2018, rajkomar_scalable_2018}, length of hospital stay \cite{rajkomar_scalable_2018}, sepsis onset detection \cite{lauritsen_early_2020, moor_predicting_2023}, mortality prediction \cite{rajkomar_scalable_2018, thorsen-meyer_dynamic_2020}, discharge diagnoses \cite{rajkomar_scalable_2018}, and heart failure outcomes \cite{desai_comparison_2020}. The overarching objective is to harness existing \ac{ehr} data through machine learning to enhance clinical outcomes and reduce healthcare costs.

However, machine learning on \ac{ehr} data poses significant challenges due to its inherent complexity. \ac{ehr} data is characterized by variable-length sequences of patient visits, irregular sampling intervals, missing entries, heterogeneous and noisy information, and a wide range of hierarchical medical concepts \cite{kim_evolving_2019}. As a result, deep learning models often achieve only modest improvements over traditional methods such as logistic regression or tree-based methods \cite{rajkomar_scalable_2018, rasmy_med-bert_2021, steinberg_language_2021}. To mitigate these issues, recent approaches have employed large-scale foundation models that are pre-trained on unlabeled \ac{ehr} data using unsupervised learning \cite{bommasani_opportunities_2022}. Many of these models adopt strategies from natural language processing, such as masked-word prediction as in BERT \cite{devlin_bert_2019} or autoregressive next-word prediction as in GPT \cite{radford_language_2019}. Treating EHR data as sequences of medical codes, enables analogous methods such as masked code prediction \cite{odgaard_core-behrt_2024, pang_cehr-bert_2021, rasmy_med-bert_2021, li_behrt_2020} or next code prediction \cite{steinberg_language_2021}. However, these techniques also face significant limitations: coding standards and healthcare practices differ strongly across sites, and interoperable \ac{ehr} foundation models would likely need to be trained on a wide variety of \ac{ehr} datasets, which is difficult to achieve due to the sensitivity of healthcare data. Therefore, the development of \ac{ehr}-specific foundation models remains constrained by the limited size and restricted availability of \ac{ehr} data.

In contrast, \acp{llm} benefit from training on vast general-purpose text corpora and a broad range of natural-language tasks \cite{raffel_exploring_2020}. This extensive pre-training enables their language comprehension and allows them to capture domain-agnostic patterns that can be adapted for healthcare applications. Consequently, \acp{llm} have demonstrated strong performance in extracting medical concepts \cite{agrawal_large_2022}, summarizing medical texts \cite{van_veen_adapted_2024}, and predicting medical outcomes \cite{hegselmann_tabllm_2023} even in low-resource settings. However, most modern \acp{llm}, such as GPT \cite{brown_language_2020} or Llama \cite{touvron_llama_2023}, utilize a decoder-only transformer architecture, which complicates the generation of robust text representations. To overcome this limitation, recent work has introduced methods to convert decoder-only \acp{llm} into effective embedding models for downstream prediction tasks \cite{behnamghader_llm2vec_2024, lee_nv-embed_2024, muennighoff_generative_2024, li_towards_2023}. Additionally, these state-of-the-art models offer an increased context window, making them well-suited for handling long inputs such as serialized \ac{ehr} data.

In this study, we systematically evaluate whether general-purpose \ac{llm}-embedding models can effectively encode \ac{ehr} records for 15 distinct clinical prediction tasks \cite{wornow_ehrshot_2023} (see \cref{fig:overview}). To this end, we first transform the \ac{ehr} data into a concise text representation capturing the most relevant patient information at prediction time. We then use two \ac{llm}-embedding models, GTE-Qwen2-7B-Instruct \cite{yang_qwen2_2024, li_towards_2023} and LLM2Vec-Llama3.1-8B-Instruct \cite{behnamghader_llm2vec_2024, grattafiori_llama_2024}, to generate \ac{ehr} representations. The resulting embeddings serve as inputs to a logistic regression classifier, which is trained separately for each prediction task. We focus on evaluating the performance in few-shot settings to evaluate the generalization ability of this approach and conduct extensive ablation studies to identify the specific components that drive the \ac{llm}’s effectiveness.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.02\textwidth]{figures/Fig1.pdf}
    \caption{\textbf{Study Overview.} (a) We use the EHRSHOT database for our experiments. Medical events of each patient are converted into numerical embeddings using an \ac{ehr} foundation model or an \ac{llm}-embedding model. A logistic regression classification head is trained, validated, and tested on 15 clinical tasks from 4 task groups as proposed by \cite{wornow_ehrshot_2023}. (b) \ac{ehr} foundation models are pre-trained on unlabeled \ac{ehr} data. Common unsupervised learning tasks are masked code or next code prediction. To obtain a representation for an \ac{ehr}, the hidden states of the pre-trained models are used.  (c) \acp{llm} are pre-trained on vast amount of text data. To obtain an \ac{llm}-embedding model architectural changes are applied, and contrastive learning is used to improve the representational performance. To obtain an \ac{ehr} embedding, the data is first serialized as text and then processed by the \ac{llm}-embedding model. Again, hidden states are used for the embedding. (Icons from flaticon.com)}
    \label{fig:overview}
\end{figure}



\section{Results}
\label{sec:results}

\subsection{EHR Database and Prediction Tasks}
We used the EHRSHOT database containing adult patients from the Stanford Health Care and Lucile Packard Children’s Hospital from 1990 to February 8th, 2023 \cite{wornow_ehrshot_2023}. This dataset includes comprehensive \ac{ehr} timelines for 6.739 patients, covering 921.499 visits, and 41.661.637 clinical events (\cref{tab:cohort_overview}). The database is part of a rigorous \ac{ehr} benchmark containing 15 clinical prediction tasks grouped into four task groups. It also provides canonical dataset splits and provides publicly available code. \Cref{tab:prediction_tasks_overview} presents a detailed breakdown of the task groups, individual tasks, and the corresponding number of labels used in our experiments. Notably, a single patient may contribute multiple labels for a given prediction task, as relevant clinical events can recur over time. 

\begin{table}[]
    \caption{\textbf{Cohort Overview.} The EHRSHOT database contains 6,739 patients with 921,499 visits, and 41,661,637 clinical events. There are canonical dataset splits into train, validation, and test sets for reproducible experiments \cite{wornow_ehrshot_2023}. Percentages of the total numbers in parentheses.}
    \label{tab:cohort_overview}
    \centering
    \begin{tabular}{p{3cm} p{2cm} p{2cm} p{2cm} p{2cm}} \toprule
    \textbf{Attribute}                  & \textbf{Train}    & \textbf{Validation} & \textbf{Test}     & \textbf{Total}     \\ \midrule
    \textbf{Num Events}   & 15,511,472 (37.2) & 13,005,205 (31.2)   & 13,144,960 (31.6) & 41,661,637 (100.0) \\
    \textbf{Num Visits}   & 339,504 (36.8)    & 300,325 (32.6)      & 281,670 (30.6)    & 921,499 (100.0)    \\
    \textbf{Num Patients} & 2,295 (34.1)      & 2,232 (33.1)        & 2,212 (32.8)      & 6,739 (100.0)      \\
    \textbf{Num Female}   & 1,173 (34.1)      & 1,142 (33.2)        & 1,126 (32.7)      & 3,441 (100.0)      \\
    \textbf{Num Male}     & 1,122 (34.0)      & 1,090 (33.1)        & 1,086 (32.9)      & 3,298 (100.0)      \\
    \textbf{Age, mean $\pm$ SD}      & 59.6 $\pm$ 17.8       & 58.8 $\pm$ 18.0         & 59.5 $\pm$ 18.0       & 59.3 $\pm$ 17.9        \\
    \textbf{American Indian}    & 14 (56.0)         & 7 (28.0)            & 4 (16.0)          & 25 (100.0)         \\
    \textbf{Asian}              & 356 (34.1)        & 347 (33.3)          & 340 (32.6)        & 1,043 (100.0)      \\
    \textbf{Black}              & 98 (32.9)         & 105 (35.2)          & 95 (31.9)         & 298 (100.0)        \\
    \textbf{Pacific Islander}   & 23 (31.1)         & 21 (28.4)           & 30 (40.5)         & 74 (100.0)         \\
    \textbf{Unknown}            & 518 (33.1)        & 530 (33.9)          & 515 (32.9)        & 1,563 (100.0)      \\
    \textbf{White}              & 1,286 (34.4)      & 1,222 (32.7)        & 1,228 (32.9)      & 3,736 (100.0)      \\
    \textbf{Hispanic}           & 374 (36.0)        & 342 (32.9)          & 322 (31.0)        & 1,038 (100.0)      \\
    \textbf{Non-Hispanic}       & 1,921 (33.7)      & 1,890 (33.2)        & 1,890 (33.2)      & 5,701 (100.0)      \\ \bottomrule
    \end{tabular}
\end{table}

\begin{table}[]
    \caption{\textbf{Prediction Tasks Overview.} The EHRSHOT benchmark defines 15 clinical prediction tasks spanning four different task groups. The number of examples per task differs based on the prevalence and frequency of clinical events. Again, canonical splits for training, validation, and testing are defined to ensure reproducible experiments \cite{wornow_ehrshot_2023}.}
    \label{tab:prediction_tasks_overview}
    \centering
    \begin{tabular}{p{2.85cm} p{2cm} p{2cm} p{2cm} p{2.15cm}} \toprule  %
    \textbf{Attribute} & \textbf{\begin{tabular}[c]{@{}l@{}}Train Labels   \\ (Positive)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Valid Labels   \\ (Positive)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Test Labels   \\ (Positive)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Total Labels   \\ (Positive)\end{tabular}} \\ \midrule
    \multicolumn{5}{l}{\textbf{Operation Outcomes}} \\ \midrule
    Long Length of Stay    & 2,569 (681)               & 2,231 (534)               & 2,195 (552)              & 6,995 (1,767)              \\
    30-day Readmission     & 2,609 (370)               & 2,207 (281)               & 2,189 (260)              & 7,005 (911)               \\
    ICU Transfer           & 2,402 (113)               & 2,052 (92)                & 2,037 (85)               & 6,491 (290)               \\ \midrule
    \multicolumn{5}{l}{\textbf{Anticipating Lab Test Results}} \\ \midrule
    Thrombocytopenia       & 68,776 (9,774)             & 54,504 (6,962)             & 56,338 (7,960)            & 179,618 (24,696)          \\
    Hyperkalemia           & 76,349 (1,215)             & 60,168 (886)              & 63,653 (948)             & 200,170 (3,049)            \\
    Hypoglycemia           & 122,108 (1,065)            & 95,488 (858)              & 100,568 (783)            & 318,164 (2,706)            \\
    Hyponatremia           & 81,336 (20,181)            & 64,473 (14,674)            & 67,028 (16,003)           & 212,837 (50,858)          \\
    Anemia                 & 70,501 (9,544)             & 56,224 (7,445)             & 58,155 (7,636)            & 184,880 (24,625)          \\ \midrule
    \multicolumn{5}{l}{\textbf{Assignment of New Diagnoses}}  \\ \midrule
    Hypertension           & 1,260 (184)               & 1,250 (177)               & 1,261 (160)              & 3,771 (521)               \\
    Hyperlipidemia         & 1,684 (205)               & 1,441 (189)               & 1,317 (172)              & 4,442 (566)               \\
    Pancreatic Cancer      & 2,576 (155)               & 2,215 (53)                & 2,220 (56)               & 7,011 (264)               \\
    Celiac                 & 2,623 (62)                & 2,284 (11)                & 2,222 (21)               & 7,129 (94)                \\
    Lupus                  & 2,570 (104)               & 2,226 (33)                & 2,243 (20)               & 7,039 (157)               \\
    Acute MI               & 2,534 (175)               & 2,177 (146)               & 2,127 (144)              & 6,838 (465)               \\ \midrule
    \multicolumn{5}{l}{\textbf{Anticipating Chest X-ray Findings}} \\ \midrule   
    Chest X-Ray   Findings & 7,481 (4,771)              & 9,366 (6,032)              & 9,428 (6,400)             & 26,275 (17,203)  \\      
    \bottomrule
    \end{tabular}
\end{table}


\subsection{EHR Text Serialization}
To utilize an \ac{llm}-embedding model for \ac{ehr} representation, the patient records were encoded into a structured text based on the widely used Markdown format. Due to runtime constraints, the maximum serialization length was limited to 4.096 tokens (approximately 16.000 characters). This constraint informed the serialization strategy, prioritizing the inclusion of recent data to ensure critical medical information was preserved. The serialized record is divided into clearly labeled sections (see example in \cref{fig:example_ehr_text_serialization}). All dates were normalized relative to a reference prediction date of January 1st, 2024, explicitly stated at the beginning. This is followed by the patient’s basic demographic information. Approximately 65\% of the recorded values were time-series data of \ac{loinc} concepts. To reduce the volume of this data, the serialization focuses on 24 frequently recorded concepts, grouped into three primary categories: Body Metrics, Vital Signs, and Lab Results. For each selected concept, the last three recorded values were included, along with the corresponding units and a classification as low, normal, or high, where applicable. The serialization then summarizes all patient visits, including visit type, time and duration, followed by any event not associated with specific visits. Detailed visit records are then provided in descending chronological order, beginning with the most recent visit. Each visit entry is further categorized into conditions, medications, and procedures for improved clarity and utility. As a result, the text serialization ensures a structured and concise representation of EHR data, facilitating efficient processing by LLM-based models.


\subsection{LLM-Embedding Models and Baselines}
We focus on two LLM-embedding models as part of our experiments to explore their effectiveness in representing EHR data. The first model, GTE-Qwen2-7B-instruct (GTE-Qwen2-7B), is based on the Qwen2-7B LLM \cite{yang_qwen2_2024} and incorporates bidirectional attention and contrastive learning to enhance embedding tasks \cite{li_towards_2023}. The second model, LLM2Vec-Llama-3.1-8B-Instruct (LLM2Vec-Llama-3.1-8B), is built on the Llama-3.1-8B Instruct architecture  \cite{grattafiori_llama_2024} and employs similar optimization techniques to improve embeddings \cite{behnamghader_llm2vec_2024}. Both models were trained using instructions for the embeddings task. Hence, we added a simple prompt for each task, e.g., “Given a patient's electronic healthcare record (EHR) in Markdown format, retrieve relevant passages that answer the query: has the patient anemia?” (see \cref{tab:instructions_for_llm_embedding_models}). Our primary goal was to assess how these LLM-based models, trained on publicly available text data, perform in representing EHRs compared to an EHR-specific foundation model. For this, we included CLMBR-T-Base, a 141-million-parameter autoregressive foundation model trained on 2.57 million de-identified EHRs from Stanford Medicine \cite{steinberg_language_2021, wornow_ehrshot_2023}. For each model, we used a logistic regression classification head trained on the training split, with hyperparameters tuned on the validation split. Additionally, we included a counts-based baseline using a \ac{gbm} model, which has demonstrated superior performance over logistic regression for EHR tasks \cite{wornow_ehrshot_2023}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.77\textwidth]{figures/Fig2.pdf}
    \caption{\textbf{Example EHR Text Serialization.} The EHR data is serialized into text to apply LLM-embedding models. We use Markdown formatting and prioritize relevant medical information. All dates were normalized relative to a reference date of January 1, 2024. Next, the patient’s demographics are listed. Time-series data coded via Logical Observation Identifiers Names and Codes (LOINC) was aggregated into 24 key concepts listed with the last three values, units, and classifications into low, normal, and high. Then, a list of all visits and all concepts not associated with a visit are given. Lastly, detailed visit entries beginning with the most recent are listed. Unique concepts are categorized into conditions, medications, and procedures. The last three values of a concept are given when present.}
    \label{fig:example_ehr_text_serialization}
\end{figure}

\newcommand{\smalldash}{\scalebox{0.95}{-}}
\newcommand{\ci}[3]{#1\,_{#2\smalldash#3}}


\begin{table}[]
    \caption{\textbf{Performance on All Examples.} Macro averaged area under receiver operating characteristic curve (AUROC) performance and bootstrapped 95\% confidence intervals of included models for four prediction task groups. The macro averaged performance across all task groups is given in the right-most column. The LLM-embedding model GTE-Qwen2-7B with a logistic regression (LR) classification head outperforms the EHR foundation model CLMBR-T-Base and the Counts-based baseline using a gradient boosted machine (GBM) head. LLM2Vec-Llama-3.1-8B only outperforms CLMBR-T-Base on the task group for assignment of new diagnosis. Combining the embeddings of the LLM-embedding models and CLMBR-T-Base by concatenation leads to a further increase in performance. Additional model variants with fewer parameters or using an encoder-only architecture show a lower overall performance.}
    \label{tab:performance_on_all_examples}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.6pt} 
    \begin{tabular}{>{\raggedright\arraybackslash}p{3.05cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm}}
    \toprule
\textbf{Model}                           & \textbf{Operational Outcomes} & \textbf{Anticipating Lab Test Results} & \textbf{Assignment of New Diagnosis} & \textbf{Anticipating Chest X-ray Findings} & \textbf{Macro Avg. Across Task Groups} \\ \midrule
\multicolumn{6}{l}{\textbf{LLM-Embedding Models}} \\ \midrule
GTE-Qwen2-7B                        & $\ci{0.844}{.821}{.867}$         & $\ci{0.867}{.860}{.874}$                                                                     & $\ci{0.715}{.674}{.755}$                & $\ci{0.670}{.656}{.683}$                      & $\ci{0.774}{.749}{.798}$                       \\
GTE-Qwen2-1.5B                      & $\ci{0.800}{.776}{.824}$         & $\ci{0.865}{.859}{.872}$                                                                     & $\ci{0.699}{.656}{.741}$                & $\ci{0.665}{.652}{.678}$                      & $\ci{0.757}{.732}{.783}$                       \\
LLM2Vec-Llama-3.1-8B                & $\ci{0.787}{.762}{.812}$         & $\ci{0.777}{.766}{.787}$                                                                     & $\ci{0.727}{.680}{.773}$                & $\ci{0.676}{.663}{.690}$                      & $\ci{0.742}{.714}{.769}$                       \\
LLM2Vec-Llama 2 1.3B                & $\ci{0.705}{.679}{.731}$         & $\ci{0.652}{.641}{.663}$                                                                     & $\ci{0.633}{.595}{.670}$                & $\ci{0.617}{.605}{.629}$                      & $\ci{0.652}{.627}{.676}$                       \\ \midrule
\multicolumn{6}{l}{\textbf{LLM-Embedding Model + EHR Foundation Model}} \\ \midrule
GTE-Qwen2-7B \hspace{1cm}+ CLMBR-T-Base         & $\ci{0.882}{.863}{.900}$         & $\ci{0.887}{.881}{.893}$                                                                     & $\ci{0.725}{.682}{.768}$                & $\ci{0.711}{.699}{.723}$                      & $\ci{0.801}{.777}{.826}$                       \\
LLM2Vec-Llama-3.1-8B + CLMBR-T-Base & $\ci{0.830}{.809}{.851}$         & $\ci{0.841}{.832}{.849}$                                                                     & $\ci{0.731}{.686}{.777}$                & $\ci{0.714}{.702}{.725}$                      & $\ci{0.779}{.753}{.805}$ \\ \midrule
\multicolumn{6}{l}{\textbf{Baselines \cite{wornow_ehrshot_2023}}} \\ \midrule
CLMBR-T-Base                        & $\ci{0.824}{.803}{.845}$         & $\ci{0.832}{.824}{.840}$                                                                     & $\ci{0.707}{.667}{.746}$                & $\ci{0.713}{.702}{.724}$                      & $\ci{0.769}{.746}{.792}$                       \\
Counts-based + GBM                       & $\ci{0.774}{.752}{.797}$         & $\ci{0.728}{.716}{.741}$                                                                     & $\ci{0.719}{.669}{.768}$                & $\ci{0.656}{.641}{.671}$                      & $\ci{0.719}{.691}{.748}$ \\ \midrule
\multicolumn{6}{l}{\textbf{Encoder Language Models}} \\ \midrule
DeBERTaV3 large             & $\ci{0.725}{.699}{.752}$         & $\ci{0.712}{.700}{.724}$                                                                     & $\ci{0.671}{.625}{.716}$                & $\ci{0.625}{.612}{.639}$                      & $\ci{0.683}{.656}{.711}$                       \\
DeBERTa V3 base              & $\ci{0.754}{.732}{.777}$         & $\ci{0.707}{.694}{.720}$                                                                     & $\ci{0.661}{.611}{.712}$                & $\ci{0.623}{.609}{.638}$                      & $\ci{0.686}{.657}{.716}$                       \\
BERT large                          & $\ci{0.735}{.711}{.759}$         & $\ci{0.720}{.708}{.732}$                                                                     & $\ci{0.666}{.619}{.714}$                & $\ci{0.640}{.626}{.653}$                      & $\ci{0.690}{.662}{.718}$                       \\
BERT base                           & $\ci{0.741}{.716}{.766}$         & $\ci{0.718}{.706}{.730}$                                                                     & $\ci{0.677}{.629}{.724}$                & $\ci{0.638}{.625}{.651}$                      & $\ci{0.694}{.665}{.722}$                       \\
ClinicalBERT                 & $\ci{0.745}{.721}{.770}$         & $\ci{0.737}{.726}{.748}$                                                                     & $\ci{0.702}{.657}{.746}$                & $\ci{0.650}{.637}{.664}$                      & $\ci{0.709}{.682}{.736}$     \\
\bottomrule
\end{tabular}
\end{table}
    

\subsection{Performance Results on 15 Prediction Tasks}
The performance results measured by the \ac{auroc} for all training and validation examples are presented in \cref{tab:performance_on_all_examples}. The GTE-Qwen2-7B model demonstrated superior performance over CLIMBR-T-Base in three of the four task categories: operation outcomes, lab test result prediction, and assignment of new diagnoses. However, in predicting chest X-ray findings, the EHR foundation model (CLIMBR-T-Base) outperformed GTE-Qwen2-7B. The LLM2Vec-Llama-3.1-8B embeddings showed slightly lower performance compared to GTE-Qwen2-7B, surpassing CLIMBR-T-Base only in the assignment of new diagnoses task. Both LLM-based embedding models clearly outperformed the counts-based baseline with a gradient-boosted machine head, which is traditionally regarded as a strong baseline for EHR tasks \cite{rasmy_med-bert_2021, steinberg_language_2021}. The only exception was in the assignment of new diagnoses, where the counts baseline marginally outperformed GTE-Qwen2-7B.
We also combined the CLMBR-T-Base embedding with the LLM representations by simple concatenation to test whether they encode orthogonal information. The combined embeddings led to a considerable performance boost for both models with an average \ac{auroc} performance of 0.801 (0.777 - 0.826) instead of 0.774 (0.749 - 0.798) for GTE-Qwen2-7B and 0.779 (0.753 - 0.805) instead of 0.742 (0.714 - 0.769) for LLM2Vec-Llama-3.1-8B.
The smaller GTE-Qwen2-1.5B model only performed slightly worse than GTE-Qwen2-7B with an average performance of 0.757 (0.732 - 0.783) compared to 0.774 (0.749 - 0.798) making it a performant model choice for EHR embeddings. LLM2Vec-Llama-2-1.3B on the other hand performed significantly worse than the larger model which could be due to the older Llama 2 architecture. \cref{fig:scaling_behavior_of_models} illustrates the relationship between model size and average performance across all tasks, suggesting the trend that larger models tend to perform better. Among the evaluated models, CLIMBR-T-Base proved to be the most efficient in terms of performance relative to model size. It is noteworthy, however, that the LLM-based embedding models were not specifically optimized for this EHR data modality, which may partially explain their performance gap.


\begin{figure}[]
    \centering
    \includegraphics[width=.85\textwidth]{figures/Fig4.pdf}
    \caption{\textbf{Scaling Behavior of Models.} Number of model parameters (x-axis) and macro averaged area under receiver operating characteristic curve (AUROC) performance and 95\% confidence intervals across all four task groups (y-axis). LLMs with more parameters show an increased performance. The specialized EHR foundation model, CLMBR-T-Base, is the most efficient prediction model.}
    \label{fig:scaling_behavior_of_models}
\end{figure}


\subsection{Performance Results in Few-Shot Setting}
To evaluate model performance with limited training data, we conducted experiments in a few-shot setting using small numbers of training examples. The experiments followed the EHRSHOT task definitions \cite{wornow_ehrshot_2023}. \cref{fig:performance_in_few_shot_setting} illustrates the aggregated AUROC across all subtasks within the four task categories for varying numbers of training examples (see \cref{fig:auprc_performance_in_few_shot_settings} for AUPRC results). Both LLM-embedding models demonstrated strong results in the few-shot setting, indicating that their pretraining on general text can be successfully transferred to serialized EHR data. Notably, GTE-Qwen2-7B consistently outperformed CLIMBR-T-Base across all training example sizes for predicting lab test results and assigning new diagnoses. For operational outcomes, a minimum of 32 training examples was required for GTE-Qwen2-7B to surpass the performance of the EHR-specific foundation model. In contrast, LLM2Vec-Llama-3.1-8B showed weaker performance in the few-shot setting. It only outperformed CLIMBR-T-Base in a limited number of scenarios, primarily for the assignment of new diagnoses. Both LLM-embedding models consistently achieved improvements over the counts-based baseline across most configurations, underscoring the impact of their pretraining on general text. Consistent with previous findings \cite{wornow_ehrshot_2023}, the largest performance gains over the counts-based baseline were observed at intermediate training example sizes. As the number of training examples increased, the advantage of pretrained LLM-based models decreased, highlighting the diminishing returns of foundation models as more labeled data becomes available.

\begin{figure}[]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/Fig3.pdf}
    \caption{\textbf{Performance in Few-Shot Settings.} Macro averaged area under receiver operating characteristic curve (AUROC) performance across subtasks for four task groups across (bold). Blurred lines are averaged AUROC values across five bootstrapped runs using different seeds \cite{wornow_ehrshot_2023}. Similar to the EHR foundation model, CLMBR-T-Base, the LLM-embedding models show the largest performance gains over the counts-based model at intermediate numbers of training example. For an increased amount of training examples, the advantage of pretrained LLM-based models decreases.}
    \label{fig:performance_in_few_shot_setting}
\end{figure}


\subsection{Effect of Different Contexts Sizes}
We evaluated the impact of varying context sizes on the performance of the LLM-embedding models by testing GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B with input lengths of 512, 1.024, 2.048, 4.096 (the default), and 8.192 tokens. The objective was to determine whether longer input sequences enhance performance and to assess if the models effectively leverage additional historical data. As summarized in \cref{tab:performance_of_llm_embedding_models_across_context_sizes} and \cref{fig:performance_of_llm_embedding_models_across_context_sizes}, the two models exhibited distinct behaviors.

\begin{table}[]
    \caption{\textbf{Performance of LLM-Embedding Models Across Context Sizes.} Macro averaged area under receiver operating characteristic curve (AUROC) performance and 95\% confidence intervals for task groups and macro averaged performance across all task groups for different context sizes of the LLM-embedding models. GTE-Qwen2-7B shows the best performance for 4,096-token context. LLM2Vec-Llama-3.1-8B shows the best performance for 2,048 tokens. Using a context size of 8,192 tokens does not show an improvement.}
    \label{tab:performance_of_llm_embedding_models_across_context_sizes}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.6pt} 
    \begin{tabular}{>{\raggedright\arraybackslash}p{3.05cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm}}
    \toprule
\textbf{Model}                           & \textbf{Operational Outcomes} & \textbf{Anticipating Lab Test Results} & \textbf{Assignment of New Diagnosis} & \textbf{Anticipating Chest X-ray Findings} & \textbf{Macro Avg.   Across Task Groups} \\ \midrule
    GTE-Qwen2-7B         & $\ci{0.844}{.821}{.867}$           & $\ci{0.867}{.860}{.874}$                    & $\ci{0.715}{.674}{.755}$                  & $\ci{0.670}{.656}{.683}$                        & $\ci{0.774}{.749}{.798}$                       \\
    8.192 context size               & $\ci{0.826}{.804}{.847}$             & $\ci{0.783}{.773}{.793}$                    & $\ci{0.684}{.637}{.730}$                  & $\ci{0.678}{.666}{.691}$                        & $\ci{0.743}{.716}{.770}$                       \\
    2.048 context size               & $\ci{0.818}{.794}{.842}$             & $\ci{0.877}{.870}{.883}$                    & $\ci{0.723}{.681}{.766}$                  & $\ci{0.652}{.638}{.666}$                        & $\ci{0.767}{.742}{.793}$                       \\
    1.024 context size               & $\ci{0.827}{.805}{.850}$             & $\ci{0.885}{.879}{.891}$                    & $\ci{0.679}{.641}{.716}$                  & $\ci{0.645}{.631}{.658}$                        & $\ci{0.759}{.736}{.782}$                       \\
    512 context size                 & $\ci{0.692}{.666}{.718}$             & $\ci{0.741}{.730}{.752}$                    & $\ci{0.641}{.605}{.677}$                  & $\ci{0.613}{.597}{.628}$                        & $\ci{0.672}{.648}{.696}$                       \\ \midrule
    LLM2Vec-Llama-3.1-8B & $\ci{0.787}{.762}{.812}$           & $\ci{0.777}{.766}{.787}$                    & $\ci{0.727}{.680}{.773}$                  & $\ci{0.676}{.663}{.690}$                        & $\ci{0.742}{.714}{.769}$                       \\
    8.192 context size               & $\ci{0.793}{.769}{.816}$             & $\ci{0.759}{.748}{.770}$                    & $\ci{0.713}{.659}{.767}$                  & $\ci{0.689}{.676}{.701}$                        & $\ci{0.738}{.708}{.769}$                     \\
    2.048 context size               & $\ci{0.812}{.787}{.837}$             & $\ci{0.879}{.873}{.885}$                    & $\ci{0.721}{.688}{.754}$                  & $\ci{0.652}{.636}{.667}$                        & $\ci{0.766}{.744}{.788}$                     \\
    1.024 context size               & $\ci{0.797}{.773}{.820}$             & $\ci{0.889}{.884}{.895}$                    & $\ci{0.662}{.626}{.699}$                  & $\ci{0.630}{.615}{.645}$                        & $\ci{0.744}{.721}{.767}$                     \\
    512 context size                 & $\ci{0.663}{.634}{.691}$             & $\ci{0.820}{.810}{.830}$                    & $\ci{0.631}{.588}{.673}$                  & $\ci{0.609}{.593}{.625}$                        & $\ci{0.680}{.653}{.707}$ \\
    \bottomrule
    \end{tabular}
\end{table}
\begin{figure}[]
    \centering
    \includegraphics[width=.75\textwidth]{figures/Fig5.pdf}
    \caption{\textbf{Performance of LLM-Embedding Models Across Context Sizes.} Different context sizes (x-axis) and the respective macro averaged area under receiver operating characteristic curve (AUROC) performance across all task groups (y-axis) for the LLM-embedding models. Full results are given in \cref{tab:performance_of_llm_embedding_models_across_context_sizes}.}
    \label{fig:performance_of_llm_embedding_models_across_context_sizes}
\end{figure}


For GTE-Qwen2-7B, the best overall performance was achieved with a context of 4.096 tokens, with only a slight decline observed when using 2.048 or 1.024 tokens. This suggests that the most relevant information is contained within the first 1.024 tokens and that the model can accurately extract these details even when presented within a longer context. In contrast, LLM2Vec-Llama-3.1-8B showed a marked improvement with a context size of 2.048 tokens and a moderate gain at 1.024 tokens, indicating that the model struggles to effectively handle longer sequences up to 4.096 tokens. Both models experienced a significant drop in performance when limited to only 512 tokens, highlighting that such a short context omits crucial information; a noteworthy point given that many existing language models are constrained to 512 input tokens \cite{jiang_health_2023}. Furthermore, extending the context to 8.192 tokens resulted in decreased performance for both models, implying limitations in extracting relevant information from very long inputs. This decline may be partially attributed to the mean pooling of the last hidden layers used for generating embeddings, which can dilute the impact of important token-level information; an alternative strategy, such as incorporating an additional attention mechanism, might help mitigate this issue.



\subsection{Effect of Chunked Contexts}
To investigate whether the models can process the full 4.096-token context cohesively, we conducted an experiment in which the serialized EHR input was divided into chunks of 512, 1.024, and 2.048 tokens. For each chunk, separate embeddings were generated and then averaged to create a final representation (see \cref{tab:performance_of_llm_embedding_models_for_chunked_context}). For GTE-Qwen2-7B, the performance decrease with smaller chunks was relatively modest, indicating that the information contained within the full 4.096-token input remains effectively used even when segmented. Notably, using 512-token chunks yielded an overall AUROC performance of 0.735, compared to 0.672 when processing a contiguous 512-token context, which suggests that chunking can mitigate input constraints. In contrast, LLM2Vec-Llama-3.1-8B demonstrated improved performance with chunked inputs, consistent with its behavior on shorter context sizes. This enhancement was primarily driven by better lab value prediction, implying that LLM2Vec-Llama-3.1-8B is particularly effective when processing inputs of up to 2.048 tokens.

\begin{table}[]
    \caption{\textbf{Performance of LLM-Embedding Models for Chunked Context.} Macro averaged area under receiver operating characteristic curve (AUROC) performance and 95\% confidence intervals for task groups and macro averaged performance across all task groups for chunked inputs and averaged embeddings of the LLM-embedding models. The performance trends are similar to the context size experiments with less decrease in performance as all 4.096 input tokens are still incorporated.}
    \label{tab:performance_of_llm_embedding_models_for_chunked_context}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.6pt} 
    \begin{tabular}{>{\raggedright\arraybackslash}p{3.05cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm}}
    \toprule
\textbf{Model}                           & \textbf{Operational Outcomes} & \textbf{Anticipating Lab Test Results} & \textbf{Assignment of New Diagnosis} & \textbf{Anticipating Chest X-ray Findings} & \textbf{Macro Avg.   Across Task Groups} \\ \midrule
GTE-Qwen2-7B         & $\ci{0.844}{.821}{.867}$           & $\ci{0.867}{.860}{.874}$                    & $\ci{0.715}{.674}{.755}$                  & $\ci{0.670}{.656}{.683}$                        & $\ci{0.774}{.749}{.798}$                       \\
2 x 2.048 tokens chunks            & $\ci{0.827}{.803}{.852}$           & $\ci{0.860}{.853}{.868}$                    & $\ci{0.713}{.673}{.753}$                  & $\ci{0.670}{.658}{.682}$                        & $\ci{0.768}{.743}{.792}$                       \\
4 x 1.024 tokens chunks            & $\ci{0.836}{.814}{.859}$           & $\ci{0.854}{.846}{.861}$                    & $\ci{0.709}{.665}{.753}$                  & $\ci{0.671}{.659}{.683}$                        & $\ci{0.768}{.742}{.793}$                       \\
8 x 512 tokens chunks              & $\ci{0.768}{.743}{.793}$           & $\ci{0.784}{.774}{.794}$                    & $\ci{0.722}{.682}{.762}$                  & $\ci{0.667}{.655}{.680}$                        & $\ci{0.735}{.710}{.760}$                       \\ \midrule
LLM2Vec-Llama-3.1-8B & $\ci{0.787}{.762}{.812}$           & $\ci{0.777}{.766}{.787}$                    & $\ci{0.727}{.680}{.773}$                  & $\ci{0.676}{.663}{.690}$                        & $\ci{0.742}{.714}{.769}$                       \\
2 x 2.048   tokens chunks          & $\ci{0.812}{.787}{.837}$           & $\ci{0.879}{.873}{.885}$                    & $\ci{0.721}{.688}{.754}$                  & $\ci{0.652}{.636}{.667}$                        & $\ci{0.766}{.744}{.788}$                       \\
4 x 1.024   tokens chunks          & $\ci{0.797}{.773}{.820}$           & $\ci{0.889}{.884}{.895}$                    & $\ci{0.662}{.626}{.699}$                  & $\ci{0.630}{.615}{.645}$                        & $\ci{0.744}{.721}{.767}$                       \\
8 x 512 tokens chunks              & $\ci{0.774}{.750}{.797}$           & $\ci{0.774}{.750}{.797}$                    & $\ci{0.774}{.750}{.797}$                  & $\ci{0.774}{.750}{.797}$                        & $\ci{0.774}{.750}{.797}$ \\
    \bottomrule
    \end{tabular}
\end{table}


\begin{table}[]
    \caption{\textbf{EHR Serialization Ablation Experiments for LLM-embedding Models.} Macro averaged area under receiver operating characteristic curve (AUROC) performance and 95\% confidence intervals for task groups and macro averaged performance across all task groups for different EHR serialization ablations studies. Removing the task-specific instructions and aggregated information lead to the largest drop in performance for both LLM-embedding models. For LLM2Vec-Llama-3.1-8B, some ablations even show an increased overall performance.}
    \label{tab:ehr_serialization_ablation_experiments_for_llm_embeddings_models}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.6pt} 
    \begin{tabular}{>{\raggedright\arraybackslash}p{3.05cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm} 
                >{\raggedright\arraybackslash}p{1.8cm}}
    \toprule
\textbf{Model}                           & \textbf{Operational Outcomes} & \textbf{Anticipating Lab Test Results} & \textbf{Assignment of New Diagnosis} & \textbf{Anticipating Chest X-ray Findings} & \textbf{Macro Avg.   Across Task Groups} \\ \midrule
GTE-Qwen2-7B & $\ci{0.844}{.821}{.867}$           & $\ci{0.867}{.860}{.874}$                    & $\ci{0.715}{.674}{.755}$                  & $\ci{0.670}{.656}{.683}$                        & $\ci{0.774}{.749}{.798}$                       \\
no instructions            & $\ci{0.762}{.737}{.788}$           & $\ci{0.768}{.757}{.780}$                    & $\ci{0.703}{.663}{.743}$                  & $\ci{0.666}{.653}{.680}$                        & $\ci{0.725}{.700}{.750}$                       \\
no demographics            & $\ci{0.844}{.822}{.866}$           & $\ci{0.864}{.857}{.871}$                    & $\ci{0.700}{.654}{.746}$                  & $\ci{0.673}{.661}{.685}$                        & $\ci{0.770}{.744}{.797}$                       \\
no aggregated              & $\ci{0.855}{.833}{.877}$           & $\ci{0.713}{.701}{.726}$                    & $\ci{0.740}{.697}{.783}$                  & $\ci{0.668}{.654}{.682}$                        & $\ci{0.744}{.718}{.770}$                       \\
no visits (both)           & $\ci{0.752}{.728}{.776}$           & $\ci{0.878}{.872}{.884}$                    & $\ci{0.735}{.690}{.779}$                  & $\ci{0.663}{.649}{.677}$                        & $\ci{0.757}{.731}{.783}$                       \\
no conditions              & $\ci{0.839}{.816}{.862}$           & $\ci{0.866}{.859}{.873}$                    & $\ci{0.731}{.694}{.767}$                  & $\ci{0.649}{.635}{.663}$                        & $\ci{0.771}{.748}{.794}$                       \\
no medications             & $\ci{0.840}{.817}{.862}$           & $\ci{0.866}{.859}{.872}$                    & $\ci{0.706}{.664}{.748}$                  & $\ci{0.672}{.659}{.685}$                        & $\ci{0.771}{.746}{.796}$                       \\
no procedures              & $\ci{0.842}{.819}{.865}$           & $\ci{0.866}{.859}{.873}$                    & $\ci{0.718}{.677}{.758}$                  & $\ci{0.668}{.654}{.682}$                        & $\ci{0.773}{.749}{.798}$ \\ \midrule
LLM2Vec-Llama-3.1-8B & $\ci{0.787}{.762}{.812}$           & $\ci{0.777}{.766}{.787}$                    & $\ci{0.727}{.680}{.773}$                  & $\ci{0.676}{.663}{.690}$                        & $\ci{0.742}{.714}{.769}$                       \\
no instructions                    & $\ci{0.759}{.734}{.784}$           & $\ci{0.751}{.740}{.763}$                    & $\ci{0.723}{.677}{.770}$                  & $\ci{0.676}{.663}{.690}$                        & $\ci{0.727}{.700}{.755}$                       \\
no demographics                    & $\ci{0.793}{.769}{.817}$           & $\ci{0.780}{.769}{.790}$                    & $\ci{0.719}{.672}{.765}$                  & $\ci{0.679}{.667}{.692}$                        & $\ci{0.743}{.715}{.770}$                       \\
no aggregated                      & $\ci{0.812}{.789}{.836}$           & $\ci{0.705}{.692}{.718}$                    & $\ci{0.715}{.665}{.766}$                  & $\ci{0.676}{.662}{.690}$                        & $\ci{0.727}{.698}{.757}$                       \\
no visits (both)                   & $\ci{0.734}{.709}{.759}$           & $\ci{0.862}{.855}{.869}$                    & $\ci{0.721}{.667}{.775}$                  & $\ci{0.677}{.663}{.690}$                        & $\ci{0.749}{.718}{.779}$                       \\
no conditions                      & $\ci{0.795}{.771}{.819}$           & $\ci{0.784}{.774}{.795}$                    & $\ci{0.704}{.654}{.754}$                  & $\ci{0.662}{.649}{.675}$                        & $\ci{0.736}{.708}{.765}$                       \\
no medications                     & $\ci{0.786}{.762}{.810}$           & $\ci{0.786}{.776}{.797}$                    & $\ci{0.714}{.663}{.765}$                  & $\ci{0.677}{.664}{.691}$                        & $\ci{0.741}{.711}{.770}$                       \\
no procedures                      & $\ci{0.791}{.767}{.816}$           & $\ci{0.779}{.768}{.789}$                    & $\ci{0.733}{.690}{.776}$                  & $\ci{0.675}{.661}{.688}$                        & $\ci{0.744}{.718}{.770}$                      
\\
    \bottomrule
    \end{tabular}
\end{table}


\subsection{Ablations of EHR Serialization}
To assess the impact of different components of the EHR serialization, we conducted a series of ablation studies for both LLM-embedding models, as summarized in \cref{tab:ehr_serialization_ablation_experiments_for_llm_embeddings_models}. The removal of instructions had a notable effect on performance for both GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B, leading to a decrease from 0.774 (0.749 - 0.798) to 0.725 (0.700 - 0.750) and from 0.742 (0.714 - 0.769) to 0.727 (0.700 - 0.755) in averaged AUROC, respectively. The most significant drop was observed in tasks related to operational outcomes and lab result prediction, suggesting that the instructions played a critical role in guiding the model to focus on relevant clinical information.
The inclusion of aggregated information, comprising four body metrics, six vital signs, and 14 lab values, also had a major impact on predictive performance. Removing this information substantially reduced the accuracy of lab test predictions, with performance dropping from 0.867 (0.860 - 0.874) to 0.713 (0.701 - 0.726) for GTE-Qwen2-7B and from 0.777 (0.766 - 0.787) to 0.705 (0.692 - 0.718) for LLM2Vec-Llama-3.1-8B. This highlights the importance of the most recent lab values in predicting future lab results. The effect was particularly pronounced for GTE-Qwen2-7B in predicting conditions such as thrombocytopenia, hyperkalemia, and hyponatremia. However, for hypoglycemia and anemia, GTE-Qwen2-7B performed on par with CLIMBR-T-Base, even though the lab values were still part of the serialized representation (see \cref{fig:task_specific_auroc_performance} and \cref{fig:task_specific_auprc_performance} for task specific performance). Interestingly, the removal of aggregated information led to slight performance improvements in predicting operational outcomes and the assignment of new diagnoses, suggesting that in some cases, a more focused representation may be beneficial.
Visit-related information proved to be particularly critical for operational outcome prediction, as removing it led to a performance drop from 0.844 (0.821 - 0.867) to 0.752 (0.728 - 0.776) for GTE-Qwen2-7B and from 0.787 (0.762 - 0.812) to 0.734 (0.709 - 0.759) for LLM2Vec-Llama-3.1-8B. Notably, the removal of visit information resulted in a substantial performance increase for LLM2Vec-Llama-3.1-8B predicting lab values, from 0.777 (0.766 - 0.787) to 0.862 (0.855 - 0.869). This suggests that the model struggles to extract relevant information when presented with a large volume of contextual data, a challenge that GTE-Qwen2-7B handles more effectively.
On the other hand, the removal of demographic information, conditions, medications, and procedures had only a minor effect on performance, indicating that these features were either redundant or less relevant for the prediction tasks. While the influence of aggregated information and visit details was clear for predicting operational outcomes and lab results, the ablation studies showed less impact on tasks related to new diagnoses and chest X-ray prediction. This suggests that these latter task groups rely more on a broader set of contextual factors rather than any single component of the serialization.



\section{Discussion}
\label{sec:discussion}

Our study demonstrates that general-purpose LLM-embedding models, originally pre-trained on extensive natural language corpora, can be repurposed as robust foundation models for EHR prediction tasks. Specifically, models like GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B not only surpassed a strong counts-based baseline but, in several clinical domains, matched or even exceeded the performance of a dedicated EHR foundation model (CLIMBR-T-Base) \cite{steinberg_language_2021, wornow_ehrshot_2023}. Notably, this is despite CLIMBR-T-Base being trained on data from the same hospital system as the EHRSHOT database, underscoring the strong generalization capabilities of LLM-based embeddings. This is especially significant given the inherent challenges of limited and heterogeneous EHR data. Across 15 diverse clinical tasks, including few-shot scenarios, our findings indicate that the transferable knowledge acquired during large-scale text pretraining enables these models to effectively capture and encode complex clinical information. Furthermore, our analysis shows that performance scales with both model size and architectural improvements, suggesting that future advancements in general-purpose LLMs may further enhance their applicability in healthcare settings. In contrast, traditional EHR-specific models face scaling limitations due to restricted data availability and the necessity for specialized architectures \cite{si_deep_2021}. Collectively, these results highlight that repurposing LLMs as embedding generators offers a powerful, flexible, and scalable alternative for clinical prediction tasks.

Our experiments suggest that several key factors contribute to the success of LLM-embedding models for encoding EHR data. First, providing clear, task-specific instructions within the serialized EHR text was crucial, as it guided the models to focus on the most clinically relevant sections, particularly benefiting tasks that require attention to specific input segments. This design leverages the general-purpose pretraining and further instruction-based fine-tuning of LLM-embedding models \cite{behnamghader_llm2vec_2024, li_towards_2023}, enabling them to identify and extract meaningful patterns from heterogeneous and complex EHR inputs. Second, the inclusion of aggregated clinical data, especially semantic LOINC-coded events and detailed visit-related information, was strongly associated with improved performance on operational outcomes and lab test predictions. In contrast, other components of the EHR did not show as clear a relationship with predictive accuracy. The effective handling of context size also plays a significant role. While models like GTE-Qwen2-7B can fully utilize contexts up to 4,096 tokens, our findings show that performance degrades with overly short or excessively long inputs. Short contexts risk omitting recent, crucial clinical encounters, whereas overly long contexts can dilute important token-level details. In cases where models face challenges with longer inputs such as LLM2Vec-Llama-3.1-8B our chunking experiments indicate that segmenting the input and averaging the resulting embeddings can mitigate this issue. Finally, combining LLM embeddings with those from an EHR-specific foundation model (CLMBR-T-Base) led to a considerable boost in performance. This improvement suggests that LLMs capture orthogonal and complementary clinical information. One explanation for this effect could be the limited context size of CLMBR, restricted to 496 medical codes, which prevents it from fully leveraging all information present in our EHR serialization \cite{steinberg_language_2021}. Alternatively, the boost may stem from the general-purpose pretraining on vast text corpora, which provides LLMs with domain-agnostic knowledge that remains untapped in models trained exclusively on EHR data.

Unlike count-based models and specialized EHR foundation models, LLM-based models are agnostic towards specific coding systems and data formats. Traditional EHR models depend on predefined vocabularies such as SNOMED, LOINC, and ICD, which restricts their applicability across diverse healthcare settings. In contrast, LLMs operate on raw textual representations, enabling them to interpret any code that can be mapped to a human-readable form. This flexibility is particularly valuable given the persistent challenges in EHR data interoperability, stemming from variable coding practices, privacy constraints, and regulatory limitations, which often hinder the aggregation of large, standardized datasets for model pretraining \cite{lehne_why_2019}. Moreover, since LLMs are pre-trained on broad and diverse text corpora, including medical literature and case reports, they are well-equipped to capture the semantics of rare or underrepresented clinical concepts \cite{steinfeldt_medical_2025}. This capacity mitigates the limitations of count-based models that typically filter out infrequent events, ensuring that even rare but clinically significant phenomena are effectively encoded \cite{kirchler_large_2025}. Additionally, the ability of LLMs to process both structured and unstructured data paves the way for integrated, multimodal representations that could further enhance clinical decision support systems \cite{moor_foundation_2023}.


Our findings highlight that LLM-embedding methods merge the strengths of count-based models, specialized EHR models, and the expansive generalization capabilities of large language models. While count-based and specialized EHR foundation models can be tuned to output well-calibrated probabilities, thereby enhancing trust in clinical applications, they are inherently limited by their reliance on predefined vocabularies and constrained training data. In contrast, LLMs can process any textual representation, enabling them to effectively handle rare or unseen codes, although they typically produce unstructured outputs that challenge clinical grounding and calibration \cite{hegselmann_tabllm_2023}. LLM-embedding methods bridge this gap by leveraging the representational power of LLMs while maintaining compatibility with traditional predictive modeling frameworks. By transforming EHR data into structured text and embedding it using general-purpose LLMs, we enable the application of logistic regression or other conventional machine learning classifiers to make clinically meaningful predictions. This approach preserves the calibration advantages of traditional models while benefiting from the rich contextual understanding of LLMs.






\subsection{Limitations}
This study has several limitations. First, our approach relies on a subjectively designed EHR serialization that we deemed to capture the most medically relevant information. This design choice may introduce bias when comparing LLM-based embedding models with dedicated EHR foundation models that operate on raw data. In addition, the performance of the LLM-embedding models is sensitive to the specific content and instructions provided in the serialized text, which may limit reproducibility and generalization ability. Although these models achieve competitive predictive performance across diverse clinical tasks, including few-shot scenarios, their substantially larger parameter counts lead to longer computation times and higher resource usage. Moreover, by relying on LLM embedding methods, we must train a downstream classifier from scratch, thereby forgoing the inherent zero-shot or few-shot prompting capabilities of LLMs. Finally, our serialization was limited to 4,096 tokens to manage runtime constraints, potentially omitting valuable long-range historical information, and our evaluation on a single institutional dataset may limit broader applicability across diverse healthcare systems.






\subsection{Future work}
Future research should explore serialization‐free approaches that allow LLMs to process raw EHR data directly, thereby reducing potential biases introduced by manual text transformation. Integrating zero-shot and few-shot prompting into the LLM-based embedding framework could further enhance model flexibility and reduce dependency on downstream training. It will also be important to develop strategies to extend the effective context window beyond 4.096 tokens to capture more comprehensive patient histories. Moreover, investigating techniques for distilling large LLMs into smaller, more efficient models may enhance their practical applicability in clinical settings. Finally, expanding evaluations to multi-institutional datasets and examining how complementary insights from both domain-specific EHR models and general-purpose LLMs can be synergistically combined will be critical for advancing the development of robust, scalable EHR foundation models.







\section{Methods}
\label{sec:methods}

\subsection{EHR Database and Prediction Task}
The EHR data utilized in our experiments is from the EHRSHOT benchmark for few-shot evaluation of EHR foundation models \cite{wornow_ehrshot_2023}. We obtained version 2.1 of the dataset, which is accessible via gated access under a research data use agreement. This dataset comprises longitudinal records for 6,739 patients, 921,499 visits, and 41,661,637 clinical events collected between 1990 and February 8th, 2023.
Each clinical event is linked to a specific patient and includes information such as start time, end time, a semantic code, a value, unit, visit ID, and the corresponding OMOP source table. We used the official ehrshot-benchmark repository\footnote{Github repository: \url{https://github.com/som-shahlab/ehrshot-benchmark}} as a starting point to design our experiments enabling to us to build on existing functionalities and to facilitate comparisons with prior methods. The benchmark uses the \ac{femr}\footnote{Github repository: \url{https://github.com/som-shahlab/femr}}, which provides Python classes for efficient loading and processing of EHR data. All extensions and experiments conducted for this paper have been made publicly available via our GitHub repository: \url{https://github.com/stefanhgm/ehrshot-benchmark}.
The EHRSHOT benchmark defines a rigorous evaluation including 15 clinical predictions tasks categorized into four groups: operational outcomes, anticipating lab test results, assignment of new diagnoses, and anticipating chest X-ray findings \cite{wornow_ehrshot_2023}. Task labels are derived from clinical events, resulting in significant variations in task-specific sample sizes. For instance, frequent events like lab tests provide considerably more examples compared to rarer events such as new diagnoses. The benchmark focuses on analyzing model performance in a few-shot setting, which is particularly relevant for large pre-trained foundation models \cite{bommasani_opportunities_2022} due to their ability to generalize from limited training data. To this end, the benchmark defines evaluation settings with a constrained number of training and validation examples. Specifically, for k in {1, 2, 4, 8, 12, 16, 24, 32, 48, 64, 128}, the benchmark uses k positive and k negative training examples, along with k positive and k negative validation examples, to train and tune supervised classifiers. Testing is always performed on the full set of examples. The classifiers evaluated within the EHRSHOT framework include logistic regression, random forests, and gradient boosting machines \cite{ke_lightgbm_2017}. Performance is reported using the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). For few-shot settings, we average the results over five runs with different seeds and compute bootstrapped 95\% confidence intervals \cite{wornow_ehrshot_2023}. Macro averages are reported for each task group, and an overall macro average is provided across all groups.







\subsection{EHR Text Serialization}
To leverage LLM-embedding models for representing EHR records, we serialized the records into textual formats. We had to limit the length of the serializations to 4.096 tokens (approximately 16.000 characters) to carry out all experiments on the available computing infrastructure (see \cref{subsec:computational_setup_and_running_times}). Two experimental runs were conducted with serializations extending up to 8.192 tokens. The primary goal was to create a detailed and informative serialization requiring minimal preprocessing while ensuring that medically relevant information appeared early in the text. This approach mitigated truncation risks in lengthy records, preserving critical details even when older entries were omitted.
To convert the visits and clinical events in the EHRSHOT dataset into text, we leveraged the semantic information embedded in the dataset. Each clinical event was labeled using the format “ontology/code”. EHRSHOT provided a set of prepared ontologies for resolving concept codes into their descriptions, which we incorporated. An analysis was performed to evaluate the utilization of these ontologies for all events of a subset of 200 patients across the task groups for operational outcomes and new diagnoses, covering 2.968 labels. We identified the following ontologies: \textit{\acl{loinc}, SNOMED, RxNorm, CPT4, Domain, CARE\textunderscore{}SITE, RxNorm Extension, Medicare Specialty, ICD10PCS, CMS Place of Service, Cancer Modifier, ICD9Proc, CVX, ICDO3, HCPCS, OMOP Extension, Condition Type}. We excluded ontologies containing only a single value (\textit{Domain, Medicare Specialty, CMS Place of Service, OMOP Extension, and Condition Type}). Codes of the ontologies \textit{CPT4, CARE\textunderscore{}SITE, ICD10PCS, Cancer Modifier, CVX,} and \textit{ICDO3} were not resolved with the provided ontology. Cancer Modifier code contained UICC cancer stages that we parsed manually. For \textit{CPT4, ICD10PCS,} and \textit{CVX} we used custom mapping files that we manually added.\footnote{We downloaded \textit{CPT4} from \cpturl, \textit{ICD10PCS} from \icdurl, and \textit{CVX} from \cvxurl.} We excluded \textit{CARE\textunderscore{}SITE} and \textit{ICDO3} since we found no way to resolve these to useful description. \\
Various approaches exist for serializing structured data, including row-by-row serialization \cite{liu_tapex_2021}, template-based methods \cite{li_table-gpt_2024}, or structured data formats like JSON, HTML, and Markdown \cite{dong_encoding_2024}. We used Markdown due to its minimal overhead and overall benefits of using a structured data input format for LLMs \cite{sui_table_2024}. To harmonize dates, all timestamps were normalized relative to January 1, 2024, designated as the prediction reference time. Serialization began with patient demographics, typically the first event for each patient, where birthdates were converted into ages (in years) for simplicity. Since 65\% of the dataset comprised time-series data encoded via LOINC, which we found imbalanced, we aggregated LOINC-coded events. Using the same patient subset as the ontology analysis, we identified the most frequent codes and categorized them into vital signs, body metrics, and lab values, selecting 24 key medical concepts. To avoid duplicates, we merged synonymous codes (see \cref{tab:semantic_codes_for_aggregated_concepts_in_ehr_serialization}). The last three values of each concept are given, and we filtered implausible values. To further enrich the text representation, we manually added default units and assessments (low, normal, high) based on standard ranges (see \cref{tab:semantic_codes_for_aggregated_concepts_in_ehr_serialization}). Following the aggregated data, a summary of all visits was included to address the potential truncation of older visits. Events not associated with visits were then presented, using the same aggregation logic to display the last three values where applicable. Finally, a detailed chronological presentation of all visits was included, with events categorized into conditions (SNOMED, Visit, Cancer Modifier, CVX, HCPCS), medications (RxNorm, RxNorm Extension), and procedures (CPT4, ICD10PCS, ICD9Proc). 







\subsection{Potential Bias of Manually Defining an EHR Text Serialization}
Defining an EHR serialization involved subjective decisions, which may have introduced bias. For instance, awareness of the prediction tasks could influence the prioritization of certain data elements, potentially favoring task-relevant information. To minimize this risk, we aimed to create an objectively defined serialization that encapsulates key aspects of the EHR records. Also, due to computational constraints, we evaluated only three variants of our final serialization for 4,096 tokens: (1) appending a list of all unique conditions at the beginning, (2) omitting the three values for the listed comments (e.g., for “Cigarette consumption” and “pH measurement, venous” in \cref{fig:example_ehr_text_serialization}), and (3) combining both approaches. For GTE-Qwen2-7B, the chosen serialization performed best, but all variants outperformed CLMBR-T-Base. However, for LLM2Vec-Llama-3.1-8B, only the serialization omitting all values (variant 2) performed slightly better. We selected the current serialization for its simplicity and completeness of medical information. This approach avoided introducing additional entries of all unique conditions and preserved the most recent values for visit-level concepts.






\subsection{LLM-Embedding Models and Baselines}
In this study, we evaluated two LLM-embedding models, GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B, based on state-of-the-art decoder-only LLMs. These models were selected for their ability to handle the 4.096-token EHR serializations used in our experiments. For comparison, we also tested a smaller variant of both models. As additional baselines we included commonly used encoder-only embedding models with smaller input sizes (512 tokens). To use them with 4,096 token inputs, the EHR serializations were split into up to eight 512-token chunks, and the resulting embeddings were averaged to generate a single representation. For all LLM-embedding models and smaller language models used in this study, we used the mean pooling of the last layer as the final embedding \cite{behnamghader_llm2vec_2024, reimers_sentence-bert_2019}. The LLM2Vec models used a slight variation that only incorporates the tokens that do not belong to the instruction. Below is an overview of all models:

\paragraph{GTE-Qwen2-7B} This LLM-embedding model is based on the Qwen2-7B-Instruct LLM \cite{yang_qwen2_2024} using a decoder-only Transformer architecture with 28 layers, 28 attention heads, and a hidden size of 3.584\footnote{Hugging Face identifier: Alibaba-NLP/gte-Qwen2-7B-instruct}. It was trained with autoregressive next token prediction and converted into an embedding model using the General Text Embedding (GTE) method \cite{li_towards_2023}. This conversion replaces causal attention with bidirectional attention, enabling the model to attend to both left and right contexts for token embedding, akin to BERT. Contrastive learning was applied using a mixture of private datasets to enhance embedding performance. The model also incorporates instructions tailored for embedding tasks, supporting a context size of up to 32.000 tokens.

\paragraph{GTE-Qwen2-1.5B} A smaller variant of GTE-Qwen2-7B, this model is based on Qwen2-1.5B-Instruct, with 28 layers, 12 attention heads, and a hidden size of 1,536. It was also trained using the GTE method \cite{li_towards_2023} and supports a context size of up to 32.000 tokens.

\paragraph{LLM2Vec-Llama-3.1-8B} This model is built upon the Llama-3.1-8B-Instruct LLM \cite{grattafiori_llama_2024} with a decoder-only Transformer architecture with 32 layers, 32 attention heads, and a hidden size of 4.096\footnote{Hugging Face identifier: McGill-NLP/LLM2Vec-Meta-Llama-31-8B-Instruct-mntp-supervised}. Initially trained for next-token prediction, it was converted to an embedding model using the LLM2Vec method \cite{behnamghader_llm2vec_2024}. This method adds bidirectional attention and fine-tunes the model with supervised contrastive learning on embedding tasks. The finetuning used curated data from the public E5 dataset \cite{springer_repetition_2024, wang_fine-tuning_2024}, containing approximately 1.5 million entries. The model supports task-specific instructions and accommodates a context size of up to 128.000 tokens.

\paragraph{LLM2Vec-Llama-2-1.3B} A smaller LLM2Vec variant, this model is derived from Sheared-LLaMA-1.3B \cite{xia_sheared_2024}, a pruned version of the Llama-2-7B-hf model \cite{touvron_llama_2023}. It includes 24 layers, 16 attention heads, and a hidden size of 2.048. The model follows the same LLM2Vec training methodology as the larger LLM2Vec-Llama-3.1-8B \cite{behnamghader_llm2vec_2024}.

\paragraph{DeBERTa v3 base/large} DeBERTa v3 is an encoder-only Transformer model designed for token embeddings \cite{he_debertav3_2022}. It improves upon its predecessor by replacing the masked language modeling objective with replaced token detection and utilizing Gradient-Disentangled Embedding Sharing. We evaluated the base variant (12 layers, 12 attention heads, 768 hidden size) and the large variant (24 layers, 12 attention heads, 1,024 hidden size), with parameter counts of 183M and 434M, respectively\footnote{Hugging Face identifiers: microsoft/deberta-v3-{base,large}}.

\paragraph{BERT base/large} BERT is a well-established text embedding model using an encoder-only transformer trained with the masked language modelling objective \cite{devlin_bert_2019}. We included both the base (12 layers, 12 attention heads, 768 hidden size, 110M parameters) and large (24 layers, 16 attention heads, 1,024 hidden size, 340M parameters) variants as benchmarks\footnote{Hugging Face identifiers: google-bert/bert-{base,large}-uncased}. While not state-of-the-art, BERT models remain widely used in embedding tasks.

\paragraph{Bio$\_$ClinicalBERT} This model builds on BERT-Base, further fine-tuned on biomedical \cite{lee_biobert_2020} and clinical data \cite{alsentzer_publicly_2019}. It is a widely adopted embedding model for medical text and was included as a baseline for comparison\footnote{Hugging Face identifier: emilyalsentzer/Bio$\_$ClinicalBERT}.

\paragraph{CLIMBR-T-Base} CLMBR-T-Base is a specialized EHR foundation model trained on 2.57 million deidentified EHRs from Stanford Medicine with autoregressive next code prediction \cite{steinberg_language_2021, wornow_ehrshot_2023}. It uses gated recurrent units and has 12 layers and a hidden dimension of 768\footnote{Hugging Face identifier: StanfordShahLab/clmbr-t-base}. The model has 141M parameters and allows for a context window of 496 codes. CLIMBR-T-Base has demonstrated consistent improvements over count-based baselines for a variety of clinical prediction tasks \cite{wornow_ehrshot_2023}. It serves as a main baseline for our experiments to test specialized EHR models against general purpose text embeddings models for representing EHR records.

\paragraph{LLM-Embedding Model and CLIMBR-T-Base} To test whether the LLM-embedding models and the EHR foundation model learn orthogonal information, we combined both models for the prediction. To this end we simply appended both embeddings. The resulting embeddings have dimensions 4.352 (GTE-Qwen2-7B) and 4.864 (LLM2Vec-Llama-3.1-8B).

\paragraph{Counts-based Model} Counts models have proven to be strong baselines for EHR prediction tasks \cite{rajkomar_scalable_2018, rasmy_med-bert_2021, steinberg_language_2021}. The basic idea is to encode all EHR events of a patient in a single vector where each entry represents the number of occurrences of a medical concept. We used the counts baseline introduced in \cite{wornow_ehrshot_2023} that further extends this approach with ontology expansion, enriching the vectors with parent and child concepts.
\newline \newline Based on the embeddings or the counts vectors given by the methods described above a classification head was trained and validated for each prediction task. For the embedding models we used a logistic regression head. For the counts-based model, we used a gradient boosting machine \cite{ke_lightgbm_2017}, which proved superior \cite{wornow_ehrshot_2023}. We adopted the parameter tuning of the classification heads from the EHRSHOT benchmark to ensure comparability of results.







\subsection{Instructions for LLM-Embedding Models}
The GTE and LLM2Vec models used instruction-tuned embeddings, requiring task-specific prompts. Hence, we added simple instructions for each prediction task based on their respective instruction templates. For instance, for prediction of anemia we added “Given a patient's electronic healthcare record (EHR) in Markdown format, retrieve relevant passages that answer the query: has the patient anemia”. The existing EHRSHOT benchmark encoded the EHRs of the same patient and the identical prediction times only once for efficiency reasons. However, to support task-specific instructions, we had to change this default behavior leading to 1.161.412 instead of 406.379 EHRs to encode resulting in longer processing times. The difference between 1.161.412 labels used in our experiments and the total number of labels of 1.178.665 (\cref{tab:prediction_tasks_overview}) is since some labels even have the same tasks and prediction time and are merged.  We list all instructions in \cref{tab:instructions_for_llm_embedding_models} and perform ablations testing the effect of the instructions.






\subsection{Computational Setup and Running Times}
\label{subsec:computational_setup_and_running_times}
All experiments were conducted on the Charité High-Performance Cluster using Nvidia A100 GPUs with 80 GB memory, configured with one, four, or eight GPUs (DGX systems). Running the GTE-Qwen2-7B model with our serialization truncated at 4,096-tokens on an 8-GPU DGX system required approximately 20 hours. For the LLM2Vec-Llama-3.1-8B model, runtime errors occurred during multi-GPU experiments with the full dataset. These issues were resolved by splitting the data into smaller batches, which introduced additional overhead. Additionally, we optimized the LLM2Vec code by removing an initial text pruning step that took approximately eight hours when using the full data. Using this modified setup, the LLM2Vec-Llama-3.1-8B model took approximately 30 hours to complete on eight A100 GPUs.





\subsection{Performance Results on 15 Prediction Tasks and Few-Shot Setting}
Following the EHRSHOT benchmark, we evaluated all models across 15 prediction tasks under various few-shot settings. The benchmark includes a modular pipeline designed to execute key tasks, with the flexibility to optionally utilize a Slurm cluster for distributed execution. Running all steps within this pipeline ensures full reproducibility of results. Step four of the pipeline, which generates EHR representations with CLIMBR-T-Base and the counts-based model, was extended to incorporate our method for creating language model-based EHR representations. This adaptation allowed us to reuse significant portions of the existing code, including the task evaluation framework. Additionally, we implemented new functionality for EHR serialization and slightly modified other steps of the benchmark to accommodate our experimental setup. For instance, the label creation process was adjusted (step three) to enable task-specific instructions for the LLM-embedding models. All modifications have been documented and can be tracked in our public GitHub repository.









\subsection{Effect of Different Contexts Sizes}
We investigated the impact of varying context sizes in the LLM-embedding models. We wanted to determine whether encoding information from older visits enhances prediction performance and whether longer inputs might dilute critical details, such as laboratory values, in the final embeddings. Specifically, we evaluated GTE-Qwen2-7B and LLM2Vec-Llama-3.1-8B models with input token limits of 512, 1.024, 2.048, and 8.192 tokens. Input tokens exceeding these thresholds were discarded. Due to the design of our EHR serialization process, additional input tokens primarily consisted of medical concepts from past visits. By testing these varying context sizes, we aimed to assess the balance between capturing historical medical data and preserving the clarity of high-priority information within the embeddings.









\subsection{Effect of Chunked Contexts}
To further explore whether LLM-embedding models effectively interpret the entirety of their input, we compared the performance of models processing complete inputs versus segmented (chunked) inputs. For this, the 4.096-token inputs were divided into smaller chunks of sizes 512, 1.024, and 2.048 tokens\footnote{In our implementation we used these chunks sizes decreased by eight tokens to cater for special tokens that might be added by the implementations of the language models.}. Separate embeddings were generated for each chunk, and a final embedding was obtained by averaging the embeddings of all chunks. This approach aligns with the mean pooling applied to the last layer that we used to compute embeddings. In this setup, task-specific instructions, placed at the beginning of the inputs, were included only in the first chunk. This design allowed us to evaluate the models’ ability to contextualize task instructions and whether chunking affects the overall performance.







\subsection{Ablations of EHR Serialization}
To better understand the contribution of various components in the EHR serialization process to the performance of the LLM-embedding models, we conducted a series of ablation experiments. First, we assessed the impact of task-specific instructions by removing them entirely to determine their influence on the final embeddings. Subsequently, we performed additional ablations by systematically excluding specific components of the serialization. These included demographic data, aggregated LOINC codes, and both visit summaries and detailed entries. Furthermore, we examined the effect of removing specific fields from the detailed visit entries, such as conditions, medications, and procedures. Throughout these experiments, the rest of the pipeline was kept consistent to isolate the effects of the removed components. This approach allowed us to identify which parts of the serialization process were most critical for generating effective EHR representations, providing insights into how these models leverage structured medical data for prediction tasks.



\bibliography{references}


\newpage 
\appendix
\section{Appendix}

\begin{table}[]
    \caption{\textbf{Semantic Codes for Aggregated Concepts in EHR Serialization.} We aggregated time-series data encoded via \ac{loinc} concepts and identified the most frequent concepts from which we selected 24 key medical concepts. To reduce duplicate information, we merged synonymous semantic codes. The primary \ac{loinc} codes is presented first in the column Semantic Codes followed by identified duplicates.  We also defined a unit, minimum and maximum allowed  values for filtering, a normal range to classify values in low, normal, and high, and a formatting strategy to create our EHR serialization.}
    \label{tab:semantic_codes_for_aggregated_concepts_in_ehr_serialization}
    \centering
    \setlength{\tabcolsep}{3pt} 
    \renewcommand{\arraystretch}{0.92}
    \footnotesize
    \begin{tabularx}{\textwidth}{X p{3cm} p{1.6cm} p{1.3cm} p{1.2cm} p{1.7cm}} \toprule
    \textbf{Medical Concept}  & \textbf{Semantic Codes}                                                        & \textbf{Unit} & \textbf{Min-Max Range} & \textbf{Normal Range} & \textbf{Formatting} \\ \midrule
    \multicolumn{6}{l}{\textbf{Recent Body Metrics}} \\ \midrule
    Body weight               & LOINC/29463-7                                                                  & oz            & 350-10000              &                       & One decimal         \\
    Body height               & LOINC/8302-2                                                                   & inch          & 5-100                  &                       & One decimal         \\
    Body mass index /   BMI   & LOINC/39156-5                                                                  & kg/m2         & 10-100                 & 18.5-24.9             & One decimal         \\
    Body surface area         & LOINC/8277-6, SNOMED/301898006                                                 & m2            & 0.1-10                 &                       & Two decimals        \\ \midrule
    \multicolumn{6}{l}{\textbf{Recent Vital Signs}} \\ \midrule
    Heart rate                & LOINC/8867-4,   SNOMED/364075005, SNOMED/78564009                              & bpm           & 5-300                  & 60-100                & Integer             \\
    Systolic blood   pressure & LOINC/8480-6,   SNOMED/271649006                                               & mmHg          & 20-300                 & 90-140                & Integer             \\
    Diastolic blood pressure  & LOINC/8462-4,   SNOMED/271650006                                               & mmHg          & 20-300                 & 60-90                 & Integer             \\
    Body temperature          & LOINC/8310-5                                                                   & °F            & 80-120                 & 95-100.4              & One decimal         \\
    Respiratory rate          & LOINC/9279-1                                                                   & breaths/min   & 1-100                  & 12-18                 & Integer             \\
    Oxygen saturation         & LOINC/LP21258-6                                                                & \%            & 1-100                  & 95-100                & Integer             \\ \midrule
    \multicolumn{6}{l}{\textbf{Recent Lab Results}} \\ \midrule
    Hemoglobin                & LOINC/718-7,   SNOMED/271026005, SNOMED/441689006                              & g/dL          & 1-20                   & 12-17                 & One decimal         \\
    Hematocrit                & LOINC/4544-3,   LOINC/20570-8, LOINC/48703-3, SNOMED/28317006                  & \%            & 10-100                 & 36-51                 & Integer             \\
    Erythrocytes              & LOINC/789-8,   LOINC/26453-1                                                   & 106/uL        & 1-10                   & 4.2-5.9               & Two decimals        \\
    Leukocytes                & LOINC/20584-9,   LOINC/6690-2                                                  & 103/uL        & 1-100                  & 4-10                  & One decimal         \\
    Platelets                 & LOINC/777-3,   SNOMED/61928009                                                 & 103/uL        & 10-1000                & 150-350               & Integer             \\
    Sodium                    & LOINC/2951-2,   LOINC/2947-0, SNOMED/25197003                                  & mmol/L        & 100-200                & 136-145               & Integer             \\
    Potassium                 & LOINC/2823-3,   SNOMED/312468003, LOINC/6298-4, SNOMED/59573005                & mmol/L        & 0.1-10                 & 3.5-5.0               & One decimal         \\
    Chloride                  & LOINC/2075-0,   SNOMED/104589004, LOINC/2069-3                                 & mmol/L        & 50-200                 & 98-106                & Integer             \\
    Carbon dioxide,   total   & LOINC/2028-9                                                                   & mmol/L        & 10-100                 & 23-28                 & Integer             \\
    Calcium                   & LOINC/17861-6,   SNOMED/271240001                                              & mg/dL         & 1-20                   & 9-10.5                & One decimal         \\
    Glucose                   & LOINC/2345-7,   SNOMED/166900001, LOINC/2339-0, SNOMED/33747003, LOINC/14749-6 & mg/dL         & 10-1000                & 70-100                & Integer             \\
    Urea nitrogen             & LOINC/3094-0,   SNOMED/105011006                                               & mg/dL         & 1-200                  & 8-20                  & Integer             \\
    Creatinine                & LOINC/2160-0,   SNOMED/113075003                                               & mg/dL         & 0.1-10                 & 0.7-1.3               & One decimal         \\
    Anion gap                 & LOINC/33037-3,   LOINC/41276-7, SNOMED/25469001                                & mmol/L        & -20-50                 & 3-11                  & Integer \\ \bottomrule
\end{tabularx}
\end{table}



\begin{table}[]
    \caption{\textbf{Instructions for LLM-Embedding Models.} The LLM-embedding models were trained using instructions; hence, we also defined simple task-specific prompts for each of the 15 clinical prediction tasks. Each prompt is prepended by the prefix given below, containing a general task description.}
    \label{tab:instructions_for_llm_embedding_models}
    \centering
    \begin{tabular}{p{4cm} p{8cm}} \toprule
    \textbf{Task}        & \textbf{Prompt} \\ \midrule
    Prefix               & Given a patient's \ac{ehr} in Markdown format, retrieve relevant passages that answer the query:  \\
    Long Length of Stay  & Will the patient stay in the hospital for more than 7 days \\
    30-day Readmission   & Will the patient be readmitted to the hospital within 30 days \\
    ICU Transfer         & Will the patient be transferred to the intensive care unit \\
    Thrombocytopenia     & Has the patient thrombocytopenia \\
    Hyperkalemia         & Has the patient hyperkalemia \\
    Hypoglycemia         & Has the patient hypoglycemia \\
    Hyponatremia         & Has the patient hyponatremia \\
    Anemia               & Has the patient anemia \\
    Hypertension         & Has the patient hypertension \\
    Hyperlipidemia       & Has the patient hyperlipidemia \\
    Pancreatic Cancer    & Has the patient pancreatic cancer \\
    Celiac               & Has the patient celiac disease \\
    Lupus                & Has the patient lupus \\
    Acute MI             & Has the patient had an acute myocardial infarction \\
    Chest X-Ray Findings & What are the chest x-ray findings of the patient \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.\textwidth]{figures/Fig6.pdf}
    \caption{\textbf{AUPRC Performance in Few-Shot Settings.} Macro averaged area under the precision-recall curve (AUPRC) performance across subtasks for four task groups across (bold). Blurred lines are averaged AUPRC values across five bootstrapped runs using different seeds \cite{wornow_ehrshot_2023}.}
    \label{fig:auprc_performance_in_few_shot_settings}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/Fig7.pdf}
    \caption{\textbf{Task-specific AUROC performance.} Area under receiver operating characteristic curve (AUROC) performance with 95\% confidence intervals across all 15 prediction tasks \cite{wornow_ehrshot_2023}.}
    \label{fig:task_specific_auroc_performance}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/Fig8.pdf}
    \caption{\textbf{Task-specific AUPRC performance.} Area under the precision-recall curve (AUPRC) performance with 95\% confidence intervals across all 15 prediction tasks \cite{wornow_ehrshot_2023}.}
    \label{fig:task_specific_auprc_performance}
\end{figure}


\end{document}
