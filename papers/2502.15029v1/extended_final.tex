%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
%\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
\documentclass[sigconf,nonacm]{aamas} 


%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}


% \usepackage{todonotes}
\newcommand{\gauy}[1]{{\color{blue}Gauy: #1}}
\usepackage{pmat} %%%%%% To draw matrices with dashed divisions. Info at: http://tug.ctan.org/macros/plain/pmat/pmat.pdf




\usepackage{tikz}

\usepackage{subcaption}



\renewcommand{\inf}{\mu}
\newcommand{\pnomem}{P}
\newcommand{\pmem}{P}

\newcommand{\fnote}[1]{{\color{teal}F: #1}}
\usepackage{breqn}



\newcommand{\well}{well-behaved}
\newcommand{\Well}{Well-behaved}


\newcommand{\STATE}{S}
\renewcommand{\state}{s}
\newcommand{\memState}{R}
\newcommand{\MState}{\bar{\memState}}
\newcommand{\mstate}{\bar{s}}
\newcommand{\msdis}{\bar{\mu}}
\newcommand{\stochastic}{\{\STATE_{\round}\}_{\round \geq 0}}
\newcommand{\memstochastic}{\{\memState_{\round}\}_{\round \geq m + 1}}
\newcommand{\mstochastic}{\{\MState_{\round}\}_{\round \geq m}}
% \newcommand{\state}{s}
\newcommand{\memstate}{s}
\newcommand{\Round}{T}
\newcommand{\round}{t}
\newcommand{\memround}{t}
\newcommand{\roundend}{\tau}
\newcommand{\Colour}{X}
\newcommand{\colour}{x}
\newcommand{\maxmemory}{m}
\newcommand{\mG}{\bar{G}}
\newcommand{\mV}{\bar{V}}
\newcommand{\mE}{\bar{E}}
\newcommand{\fix}{}%{\vspace{-\baselineskip}}

\newcommand{\average}{\tau}
\newcommand{\weight}{H}

\newcommand{\blue}{\text{blue}}
\newcommand{\red}{\text{red}}
\newcommand{\green}{\text{green}}
\newcommand{\torus}{torus}

\newcommand{\rev}[1]{{\color{blue}{#1}}}


% \newcommand{\textdef}[1]{\emph{#1}}
\newcommand{\textdef}[1]{\textbf{#1}}

\newcommand{\citenp}[1]{\citeauthor{#1} \citeyear{#1}}
\newcommand{\citenptext}[1]{\citeauthor{#1} (\citeyear{#1})}
\newcounter{questionscounter}



\newtheorem*{remark}{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\makeatletter
\gdef\@copyrightpermission{
  \begin{minipage}{0.2\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{by}}
  \end{minipage}\hfill
  \begin{minipage}{0.8\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
  \end{minipage}
  \vspace{5pt}
}
\makeatother

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{Y.~Vorobeychik, S.~Das, A.~Nowé  (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your OpenReview submission number.
%%% In anonymous mode, it will be printed on the first page.

\title[AAMAS-2025 Formatting Instructions]{Voter Model Meets Rumour Spreading: A Study of Consensus Protocols on Graphs with Agnostic Nodes [Extended Version]}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Marcelo Matheus Gauy}
\affiliation{
  \institution{São Paulo State University}
  \city{Itapeva}
  \country{Brazil}}
\email{marcelo.gauy@unesp.br}

\author{Anna Abramishvili}
\affiliation{
  \institution{King's College London}
  \city{London}
  \country{United Kingdom}}
\email{anna.abramishvili@kcl.ac.uk}

\author{Eduardo Colli}
\affiliation{
  \institution{University of São Paulo}
  \city{São Paulo}
  \country{Brazil}}
\email{colli@ime.usp.br}

\author{Tiago Madeira}
\affiliation{
  \institution{University of São Paulo}
  \city{São Paulo}
  \country{Brazil}}
\email{tmadeira@alumni.usp.br}

\author{Frederik Mallmann-Trenn}
\affiliation{
  \institution{King's College London}
  \city{London}
  \country{United Kingdom}}
\email{frederik.mallmann-trenn@kcl.ac.uk}

\author{Vinícius Franco Vasconcelos}
\affiliation{
  \institution{University of São Paulo}
  \city{São Paulo}
  \country{Brazil}}
\email{vfvmat@ime.usp.br}

\author{David Kohan Marzagão}
\affiliation{
  \institution{King's College London}
  \city{London}
  \country{United Kingdom}}
\email{david.kohan@kcl.ac.uk}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}

Problems of consensus in multi-agent systems are often viewed as a series of independent, simultaneous local decisions made between a limited set of options, all aimed at reaching a global agreement. Key challenges in these protocols include estimating the likelihood of various outcomes and finding bounds for how long it may take to achieve consensus, if it occurs at all. 

To date, little attention has been given to the case where some agents have no initial opinion. In this paper, we introduce a variant of the consensus problem which includes what we call `agnostic' nodes and frame it as a combination of two known and well-studied processes: voter model and rumour spreading. We show (1) a martingale that describes the probability of consensus for a given colour, (2) bounds on the number of steps for the process to end using results from rumour spreading and voter models, (3) closed formulas for the probability of consensus in a few special cases, and (4) that the computational complexity of estimating the probability with a Markov chain Monte Carlo process is $O(n^2 \log n)$ for general graphs and $O(n\log n)$ for Erd\H{o}s-Rényi graphs, which makes it an efficient method for estimating probabilities of consensus. Furthermore, we present experimental results suggesting that the number of runs needed for a given standard error decreases when the number of nodes increases.




\end{abstract}


\keywords{Consensus processes; voter model; rumour spreading; multi-agent consensus}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\section{Introduction}


In multi-agent consensus problems, agents make a sequence of independent and autonomous choices from a finite set based on their local information. Agents have a shared goal of reaching a consensus state, in which they all represent the same choice. The process is often abstracted as a graph in which nodes represent agents, their colour represent their current choice (or opinion, or state), and edges of the graph represent visibility or influence between agents. 



Multi-agent processes on graphs have been shown to have several applications, 
including autonomous robots or drones~\cite{yan2013survey, ismail2018survey},  electrical flow estimation \cite{becchetti2018pooling, cruciani2021phase},
mutation fixation in biology \cite{moran1958random, lieberman2005evolutionary}, among others. 
In the simplest of such processes, the \textit{voter model}, at each point in time (called `round') nodes may change their colour based on the opinion of their neighbours until consensus is reached (e.g. the case where all nodes share the same colour). More formally, this process can be either synchronous or asynchronous. In the asynchronous case, a node is chosen uniformly at random and selects a neighbour proportional to the weight of the edge between then. It then adopts the colour (or opinion) of the chosen neighbour. In the synchronous case, all nodes act simultaneously and independently (i.e., the choice of one does not affect the choice of the other in the same round). Consensus processes in multi-agent systems have been extensively studied (e.g. \cite{martinez2005synchronous,lynch1996distributed, cao2015event,olfati2007consensus, marzagao2017multi}).

Given an initial colour configuration, the probability of consensus and time-bounds for the number of rounds before such consensus is achieved for a given colour are some of the core problems studied in this domain. Extensive results have been found for both synchronous \cite{hassin2001distributed} and asynchronous \cite{cooper2016linear} process, as well as for processes with undecided states (\cite{angluin,perron, clementi_et_al:LIPIcs.MFCS.2018.28, petra1}). In these, nodes do not change directly from one colour to another but transition via an `undecided' state in between them.  

One of the features of the classical voter model is that it assumes all nodes start off with a colour/opinion. In some scenarios, we may want to model a process in which, at the start, some nodes do not have an opinion at all, which may be different from being `undecided' after been given a set of option, as they have not been in contact with any of the opinions in this process. One can think of examples related to election scenarios in which voters do not yet know the candidates and may be therefore influenced by the first contact with a candidate. Or in a process on a blockchain, in which new blocks are mined and the information of new mined blocks traverse the network, possibly competing with other new blocks mined at a similar time. 

In this paper, we introduce and study a variant of the voter model in which nodes can have an extra state, which we call `agnostic' (represented by the colour, say, white). We call it \textit{voter model with agnostic nodes}. Agnostic nodes become gnostic if they choose a gnostic neighbour to copy its colour. Once gnostic, they can never become agnostic again, i.e., if a gnostic node chooses an agnostic one, the gnostic node keeps its current colour and nothing happens. For a precise definition of the problem, see Section~\ref{subsec:agnostic}. The main challenge posed when studying this variant is that there is an asymmetry between states, in that agnostic states can become gnostic but not vice versa. We provide an efficient Monte Carlo algorithm for estimating the probability of consensus in this voter model with agnostic states for any graph and any initial configuration. Furthermore, we provide time-bounds for consensus to be achieved.

The variant we study can also be seen as a generalisation of the rumour spreading model~\cite{feige1990randomized, fountoulakis2010reliable, acan2015push, panagiotou2017asynchronous}. In it, there are nodes that are `informed' and nodes that are `uninformed' and the process studies the time bounds until all nodes become informed. Like in our variant, an 'informed' node cannot become 'uninformed'. Our voter model with agnostic nodes is analogous to two or more rumours that compete not only to gather more agnostic nodes but also to flip the opinion of other gnostic nodes. Alternatively, one can also see the voter model with agnostic nodes as a combination of two models happening simultaneously: the classical voter model with a rumour spreading process. 

The following motivates the problem with a toy example. 


\tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=1}}
\tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=1}}
\tikzset{white/.style={draw=black, circle, fill=white, inner sep=1}}
\tikzset{green/.style={draw=green, circle, fill=green!30, inner sep=1}}
\tikzset{orange/.style={draw=orange, circle, fill=orange!30, inner sep=1}}
\tikzset{yellow/.style={draw=yellow, circle, fill=yellow!30, inner sep=1}}

\begin{example}\label{exm:motivation}
    Consider the graph and initial configuration depicted in Figure \ref{fig:motivational_example}. In this example, each node chooses a neighbour with uniform probability. For example, $v_2$ has $50\%$ chance of choosing $v_1$ and thus becoming blue at round $S_1$ and $50\%$ chance of choosing $v_3$ and thus becoming red at round $S_1$. If a node chooses an agnostic node, their colour does not change. For example, if $v_1$ chooses $v_2$, $v_1$ will stay blue. 

    With that in mind, what is the probability that there will be, say, a red consensus? What can we say about the expected number of rounds until that happens?
    
    \begin{figure}[b]
    \centering
    \begin{tikzpicture}[scale = 1.5]
    
        % Nodes
        \node[blue] (A) at (-0.52, 0.3) {$v_1$};
        \node[white] (B) at (-0.52, -0.3) {$v_2$};
        \node[red] (C) at (0, 0) {$v_3$};
        \node[white] (D) at (0.6, 0) {$v_4$};

        % Edges
        \draw (A) -- (B);
        \draw (A) -- (C);
        \draw (B) -- (C);
        \draw (C) -- (D);
    \end{tikzpicture}
    \caption{A motivational example of an undirected graph with an initial configuration $S_0 = s_0$ consisting of one blue node ($v_1$), one red node ($v_3$), and two agnostic nodes ($v_2$ and $v_4$). Transition probabilities are uniform, i.e., $v_3$ has $\frac{1}{3}$ chance of choosing a given neighbour, whereas $v_4$ chooses $v_3$ and becomes red with probability $1$. What are the probabilities of consensus in this case?}
    \label{fig:motivational_example}
\end{figure}
\end{example}




We will return to Example \ref{exm:motivation} later in this paper. Our main contributions of this paper are as follows:

\begin{enumerate}
    \item We obtain a martingale for the voter model with agnostic states for the case in which the underlying (weighted) graph represents a reversible Markov chain (Theorem~\ref{thm:martingale}).
    \item Although the martingale obtained may not be efficiently computed in the general case, we prove a closed formula for the consensus probability in the case of a complete graph with an asynchronous process (Corollary~\ref{corollary:complete_graph}).
    \item We show that the existence of agnostic nodes does not affect the complexity bounds for the expected time of achieving consensus, as the agnostic nodes typically disappear faster than consensus is achieved (Lemma~\ref{lemma:time_bounds} and Propositions~\ref{prop:rumour_general},~\ref{prop:rumour_random}). This comes from standard known results for rumour spreading processes.
    \item We present a Markov chain Monte Carlo (MCMC) algorithm (Section~\ref{subsec:MCMC}) to efficiently compute the consensus probability based on the fact that agnostic nodes disappear quickly (Section~\ref{sec:time_bounds}). This is an efficient algorithm that provides an unbiased estimate.
    \item We present an experimental analysis to support results using our MCMC algorithm, showing that few runs are necessary to obtain good probability estimates (Section~\ref{sec:experiments}). It also suggests estimates get better as graph sizes increase.
\end{enumerate}




\section{Background and Main Definitions}


   In this section, we present concepts and results from the literature that will be used in subsequent sections. We first introduce the classical version of consensus protocol used in this paper, also known as voter model~\cite{donnelly1983finite, nakata1999probabilistic, hassin2001distributed, cooper2016linear,DBLP:journals/talg/KanadeMS23,DBLP:conf/analco/OliveiraP19}, in which all vertices have an initial opinion. We then propose a variant of the voter model in which some vertices do not have an initial opinion, which we label agnostic vertices. The voter model has been widely studied in the context of multi-agent systems. The winning probabilities of each colour and bounds on the convergence time were obtained for undirected graphs by Hassin \& Peleg~\cite{hassin2001distributed}. Cooper \& Rivera extended this work to the linear voting model, which captures digraphs as well as several similar consensus processes~\cite{cooper2016linear}. 
    
    \subsection{Classical Voter Model} \label{subsec:voter}
    The (pull) \textdef{voter model} defines a round-based consensus process on a strongly connected directed graph $G = (V,E)$.\footnote{Henceforth, we assume all graphs are strongly connected unless stated otherwise.} In such processes agents are represented by nodes in this graph. At each round, each node has a colour associated to it, representing the respective agent's current state (or opinion). Their goal is to reach consensus, i.e., a situation where every agent is in the same state. To that end, at each round, all agents update their state synchronously based on the colour of their out-neighbours.\footnote{For precision, we consider that agents change their state at the end of each round, after all nodes have made their decisions.}
    The probability that $v$ copies colour of node $u$ in a given round is represented by the weight of edge $(v,u)$. The weights of edges starting at a given node are assumed to be positive and to sum to $1$. We collate all these probabilities in an out-matrix $H$, which can be also seen as the adjacency matrix of $G$ where entry $H(v, u)$ represents the weight of edge $(v, u)$. We adopt the notation $H(v,u) = 0$ if $(v,u) \notin E$, and note that self loops are allowed and thus $v$ may adopt its own colour. Once reached, a consensus is stable. 

    Let $\Colour = \{c_1, \dots, c_k\}$ be the set of all possible colours on a consensus process. A \textdef{configuration} on a graph $G=(V,E)$ is a function $\state \in \Colour^V$ that associates each node $v \in V$ with a colour $c \in \Colour$, i.e., $\state(v)$ represents $v$'s colour in configuration $\state$. More formally, a process is a sequence of random variables $\stochastic$, with  $\STATE_{t+1}\in \Colour^V$ being a configuration generated based on $\STATE_t$. We say colour $i$ \textdef{wins} the process if a configuration $\STATE_t =\state$, such that $\state(v) = i$ for all $v$, is reached.
% 
    Here, we assume processes converge with probability $1$. For discussion of fringe cases and work related to graphs in which processes may not converge, see, e.g., \cite{marzag2021influence}. 
    
    
    Observe that the out-matrix $H$ of the graph $G$ can be seen as the transition matrix of a time homogeneous Markov chain (e.g., see Chapter $6$, \citenptext{grimmett2001probability})  representing the probabilities of one round in the consensus process~\cite{cooper2016linear}. If $G$ is strongly connected, this Markov chain is irreducible and finite, so there exists a unique stationary distribution $\mu$ of $H$, that is, there is a row vector $\mu$ such that $\mu H=\mu$. We call the values $\mu(v)$ the influence of the vertex $v$ in the consensus protocol. In this context, previous work~\cite{cooper2016linear}, show that the winning probabilities of each colour can be determined by the initial configuration only and are given by the following proposition.

    
    
    \begin{proposition}[\citenp{cooper2016linear}]\label{prop:nicola's-result}
        Consider a consensus process on a strongly connected graph $G$ (further, we assume $G$ is such that consensus is always achieved for all initial configurations), with associated adjacency matrix $H$ and $\mu$ its unique stationary distribution. Assume the initial configuration is given by $\state\in\{c_1,\dots,c_k\}^{V}$. 
        Then, we have that the winning probability of colour $c_i$ is: 
        \[\mathbb{P}(\textrm{colour }c_i\textrm{ wins}\mid \STATE_0 = \state) = \sum_{v \in V, \STATE(v)=c_i}\mu(v)\]
    \end{proposition}

    We can see as a corollary, that for (non-bipartite, connected) undirected graphs, the probability of a given colour winning is simply the number of incident edges in nodes of that colour divided by the $2E$, where $E$ is the number of edges in the graph \cite[Corollary 2.2 and Section 2.3]{hassin2001distributed}. Example \ref{exm:other_example} discusses the idea applied to our motivating example.

    \begin{example}\label{exm:other_example}
    Consider the modified version of Example \ref{exm:motivation} with a different initial configuration: agnostic nodes are instead gnostic and coloured, say, orange. In other words, assume we are under the assumptions of the classical voter model with 3 colours. We have $\mu = \frac{1}{8}(2, 2, 3, 1)$. Then, from the we have the probability of red winning being $\frac{3}{8}$, of blue winning being $\frac{2}{8}$ and of orange winning being $\frac{3}{8}$. 
\end{example}

    \subsection{Rumour Spreading Process}

    A rumour spreading process on a graph represents the process of information `travelling' across the edges to eventually reach all nodes on a graph. More formally, and using the notation for the voter model, we would have two colours, one being `red' and the other representing a node being `uninformed'. Many strategies of information transmission were designed, such as push, pull and push-and-pull. In this work, we will concentrate, as mentioned in Section~\ref{subsec:voter}, on the pull protocol. In it, if node $v$ selects $u$, then $v$ becomes informed if $u$ is informed, otherwise $v$ is unchanged (informed or uninformed). The process can be synchronous or asynchronous.
    
    For the push protocol, node $v$ selects $u$ and pushes its state towards $u$. If $v$ is agnostic, $u$ retains its state, otherwise $u$ adopts the state of $v$. Observe that the push protocol cannot be done synchronously for consensus problems as it is not clear how to resolve the possible ambiguity (nodes $v_1$ and $v_2$ with different gnostic states, both select the same node $u$). Asynchronous push-and-pull would be defined as is standard in rumour spreading theory. A random agnostic node is chosen and performs a pull, and a random gnostic node is chosen and performs a push. Observe that the asynchronous push-and-pull would eventually become equivalent to asynchronous push as agnostic vertices disappear. Lastly, observe that, just like the push protocol, push-and-pull cannot be done synchronously.

    We will use standard techniques and proof ideas of the rumour spreading literature to show that the rumour spreading process is fast in general for the pull version. These are done in Propositions~\ref{prop:rumour_general} and~\ref{prop:rumour_random} where we use the results from the literature to obtain an $O(n\log(n))$ bound for general graphs and a $O(\log(n))$ bound for random graphs (in the synchronous case, the asynchronous case adds an extra factor $n$ multiplying both). For other known bounds from the literature, see Sections~\ref{sec:discussion} and~\ref{sec:related_work}.
    
    \subsection{Voter Model with agnostic Nodes}\label{subsec:agnostic}
    
    We now introduce the main concept to be explored in this work. The main difference of the process with agnostic nodes is that there is an asymmetry between gnostic and agnostic nodes: an agnostic node can become gnostic but gnostic nodes cannot become agnostic. A more precise definition is given as follows.
    

    \begin{definition}[Voter Model with agnostic Nodes]\label{def:agnostic}
        A (pull) voter model with agnostic nodes generalises the notion of (pull) voter model by changing the rule with which nodes update their colour. As before, at each round $t$, each node $v$ chooses a one of its out-neighbours $u$ proportionally to the weight of the edge in $G$. However,
        \begin{enumerate}
            \item If $u$ is agnostic, $v$ does not change its colour. 
            \item If $u$ is gnostic, $v$ copies colour of $u$.
        \end{enumerate}
    \end{definition}
    At the same time that Definition \ref{def:agnostic} can be seen as a generalisation of the voter model, it can also be seen as a generalisation of the rumour spreading process. A node that is `uninformed' behaves equivalently to an `agnostic' node. The difference being, of course, that we consider more than one rumour spreading in the network, and competing with each other at the same time it influences agnostic (or uninformed) nodes. 

    We now go back to Example \ref{exm:motivation} and solve it by simply accounting for all possible states and their probabilities. 
    
    \begin{example}[Example \ref{exm:motivation} continued]\label{exm:motivation2}
        Recall Example \ref{exm:motivation}. Here we solve it `by hand' to motivate the introduction of a martingale property for this model.  Figure \ref{fig:example1solution} shows all possible configurations for $S_1$, the probabilities of reaching them ($a_i$) and the probability that red wins from each of them ($b_i$) calculated by applying Proposition \ref{prop:nicola's-result}. Note that $\mu = \frac{1}{8}(2, 2, 3, 1)$. We have that $\mathbb{P}(\textrm{red wins}\mid \STATE_0 = \state) = \frac{5}{8}$. 
\tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=0.5}}
\tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=0.5}}

\begin{figure}[b]
\centering
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
    % Nodes
    \node[red] (A) at (-0.52, 0.3) {\tiny $v_1$};
    \node[red] (B) at (-0.52, -0.3) {\tiny $v_2$};
    \node[red] (C) at (0, 0) {\tiny $v_3$};
    \node[red] (D) at (0.6, 0) {\tiny $v_4$};
    % Edges
    \draw (A) -- (B);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (C) -- (D);
\end{tikzpicture}
\caption*{$a_1=\frac{1}{6},b_2=1$}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
    % Nodes
    \node[blue] (A) at (-0.52, 0.3) {\tiny $v_1$};
    \node[red] (B) at (-0.52, -0.3) {\tiny $v_2$};
    \node[red] (C) at (0, 0) {\tiny $v_3$};
    \node[red] (D) at (0.6, 0) {\tiny $v_4$};
    % Edges
    \draw (A) -- (B);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (C) -- (D);
\end{tikzpicture}
\caption*{$a_2=\frac{1}{6},b_2=\frac{3}{4}$}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
    % Nodes
    \node[red] (A) at (-0.52, 0.3) {\tiny $v_1$};
    \node[blue] (B) at (-0.52, -0.3) {\tiny $v_2$};
    \node[red] (C) at (0, 0) {\tiny $v_3$};
    \node[red] (D) at (0.6, 0) {\tiny $v_4$};
    % Edges
    \draw (A) -- (B);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (C) -- (D);
\end{tikzpicture}
\caption*{$a_3=\frac{1}{6},b_3=\frac{3}{4}$}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
    % Nodes
    \node[blue] (A) at (-0.52, 0.3) {\tiny $v_1$};
    \node[blue] (B) at (-0.52, -0.3) {\tiny $v_2$};
    \node[red] (C) at (0, 0) {\tiny $v_3$};
    \node[red] (D) at (0.6, 0) {\tiny $v_4$};
    % Edges
    \draw (A) -- (B);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (C) -- (D);
\end{tikzpicture}
\caption*{$a_4=\frac{1}{6},b_4=\frac{1}{2}$}
\end{subfigure}

\vspace{0.2cm}

\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
    % Nodes
    \node[red] (A) at (-0.52, 0.3) {\tiny $v_1$};
    \node[red] (B) at (-0.52, -0.3) {\tiny $v_2$};
    \node[blue] (C) at (0, 0) {\tiny $v_3$};
    \node[red] (D) at (0.6, 0) {\tiny $v_4$};
    % Edges
    \draw (A) -- (B);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (C) -- (D);
\end{tikzpicture}
\caption*{$a_5=\frac{1}{12},b_5=\frac{5}{8}$}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
    % Nodes
    \node[blue] (A) at (-0.52, 0.3) {\tiny $v_1$};
    \node[red] (B) at (-0.52, -0.3) {\tiny $v_2$};
    \node[blue] (C) at (0, 0) {\tiny $v_3$};
    \node[red] (D) at (0.6, 0) {\tiny $v_4$};
    % Edges
    \draw (A) -- (B);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (C) -- (D);
\end{tikzpicture}
\caption*{$a_6=\frac{1}{12},b_6=\frac{3}{8}$}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
    % Nodes
    \node[red] (A) at (-0.52, 0.3) {\tiny $v_1$};
    \node[blue] (B) at (-0.52, -0.3) {\tiny $v_2$};
    \node[blue] (C) at (0, 0) {\tiny $v_3$};
    \node[red] (D) at (0.6, 0) {\tiny $v_4$};
    % Edges
    \draw (A) -- (B);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (C) -- (D);
\end{tikzpicture}
\caption*{$a_7=\frac{1}{12},b_7=\frac{3}{8}$}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
    % Nodes
    \node[blue] (A) at (-0.52, 0.3) {\tiny $v_1$};
    \node[blue] (B) at (-0.52, -0.3) {\tiny $v_2$};
    \node[blue] (C) at (0, 0) {\tiny $v_3$};
    \node[red] (D) at (0.6, 0) {\tiny $v_4$};
    % Edges
    \draw (A) -- (B);
    \draw (A) -- (C);
    \draw (B) -- (C);
    \draw (C) -- (D);
\end{tikzpicture}
\caption*{$a_8=\frac{1}{12},b_8=\frac{1}{8}$}
\end{subfigure}
\caption{Every configuration $s_{1i}$ which can be reached from $S_0$ in Example~\ref{exm:motivation} after one round, i.e. $a_i:=\mathbb{P}(S_1=s_{1i}|S_0)>0$. The probability of a red consensus in each case is denoted by $b_i$ and can be calculated by applying Proposition~\ref{prop:nicola's-result}. Therefore, the probability of red to win in Example~\ref{exm:motivation} is $\sum_{i=1}^8a_ib_i=\frac{5}{8}$.}
\label{fig:example1solution}
\end{figure}
    \end{example}

\begin{remark}\label{rmk:one_round_probabilities}
    Example \ref{exm:motivation2} highlights the following: if we know the probabilities that agnostic nodes become, say, red, once they become gnostic, can we amend formula in Proposition \ref{prop:nicola's-result} so that it is valid for the voter model with agnostic nodes? In Figure \ref{fig:motivational_example}, such probabilities for nodes $v_2$ and $v_4$ are, respectively, $\frac{1}{2}$ and $1$. Multiplying these values with the importance of each node and summing them up gives us, $0\mu(v_1) + \frac{1}{2}\mu(v_2) + 1\mu(v_3) + 1\mu(v_4) = \frac{5}{8}$, which coincides with the probability of red winning. We will show that this is not a coincidence in Theorem \ref{thm:martingale}.
\end{remark}

















\section{Probability of Consensus}

In this section, we will provide a general study on the probability of consensus for the (pull) voter model with agnostic nodes.
Our theorems deal with the case with only two colours (red and blue) for gnostic nodes. It is easy to see that the generalisation for more colours is immediate.

\subsection{Martingale Property}

The following theorem provides a martingale (see, e.g., \cite[Chapter 12]{grimmett2001probability}) associated with the voter model with agnostic nodes, which will then immediately give a formula such as in Proposition \ref{prop:nicola's-result}. 



\begin{theorem}[Martingale Property] \label{thm:martingale}
    Let $G$ be a graph with vertex set $V$. Suppose we have a (pull) voter model with agnostic states (either synchronous or asynchronous) with associated matrix $H$ on $G$, and $\mu$ such that $\mu H = \mu$. Suppose, furthermore, that the voter model is a reversible Markov chain, or in other words that, $H(v,w)\mu(v)=H(w,v)\mu(w)$. Let $S_t$ be the state of each vertex at time $t$. Moreover, define $R(v)$ as the event where $v$ is coloured red once it is gnostic. Then the following sequence is a Martingale with respect to $S_t$:
    \begin{equation}
        X_t = \sum_{v\in V}\mu(v)\mathbb{P}(R(v)\mid S_t)
    \end{equation}

    As a result, if $G$ is such that the process always converges~\footnote{For some $G$, such as bipartite graphs, the process may never converge. For a general result on convergence, see \cite{marzag2021influence}.} on $G$, then
    \begin{equation}\label{eq:consensus}
        \mathbb{P}(\text{red wins}) = X_0.
    \end{equation}
\end{theorem}
\begin{proof}
First note that if $v$ is already red in $S_t$, then $\mathbb{P}(R(v)\mid S_t) = 1$ and if already blue, $\mathbb{P}(R(v)\mid S_t) = 0$. Let $S_t(v)=\{0,1,2\}$ denote the state of each vertex $v$ at time $t$, with $0$ representing color white, $1$ representing red, and $2$ representing blue. We want to show that \begin{align}\label{eq:main}\mathbb{E}(X_{t+1}|S_t)= X_t.\end{align} 
Observe that $X_{t+1}=\sum_{v\in V}\mu(v)\mathbb{P}(R(v)| S_{t+1})$, and we have, by linearity of expectation: 
$$\mathbb{E}(X_{t+1}\mid S_t) = \sum_{v\in V}\mu(v)\mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t).$$ 



We distinguish two cases: 1) $S_t(v)$ gnostic (that is equal $1$ or $2$) and 2) $S_t(v)$ agnostic (equal $0$). For the second case observe that 
$$\mathbb{P}(R(v)\mid S_t) = \sum_{S_{t+1}}\mathbb{P}(R(v)\mid S_{t+1})\mathbb{P}(S_{t+1}\mid S_t)$$
by law of total probability. The second expression is the definition of the conditional expectation so we have 
$$\mathbb{P}(R(v)\mid S_t) = \mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t)$$
for the case where we assume $S_t(v)=0$.
We have
$$\mathbb{E}(X_{t+1}\mid S_t) = \sum_{v\in V}\mu(v)\mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t).$$

Since $X_t = \sum_{v\in V}\mu(v)\mathbb{P}(R(v)\mid S_t)$, the goal is then to show that: 
$$\sum_{v\in V}\mu(v)\mathbb{P}(R(v)\mid S_t) = \sum_{v\in V}\mu(v)\mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t)$$

For that we split both left and right side in two sums:
$$\sum_{S_t(v) = 0}\mu(v)\mathbb{P}(R(v)\mid S_t) = \sum_{S_t(v)=0}\mu(v)\mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t)$$

and:
$$\sum_{S_t(v)\in\{1,2\}}\mu(v)\mathbb{P}(R(v)\mid S_t) = \sum_{S_t(v)\in\{1,2\}}\mu(v)\mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t)$$

that is, we look at the graph at time $t$ and split the vertices between agnostic and gnostic and we will show that the sum is equal in each part. The first case of only agnostic vertices follows from $\mathbb{P}(R(v)\mid S_t) = \mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t)$ which is why have shown this equality. The second case is done in the rest of the text and requires reversibility.






The equation $\mathbb{E}(X_{t+1}|S_t)= X_t$ then becomes equivalent to showing that:
$$\sum_{S_{t}(v)=1}\mu(v) = \sum_{S_{t}(v)\in\{1,2\}} \mu(v)\mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t)$$

because we have already shown that the terms on the left and right side where $S_t(v)=0$ are equal.

We now turn to the case where $S_t(v)\neq 0$. Observe that 
$$\mathbb{E}(\mathbb{P}(R(v)\mid S_{t+1})\mid S_t) = \mathbb{P}(S_{t+1}(v)=1\mid S_t),$$ 
as if $S_t(v)$ is gnostic, then $S_{t+1}(v)$ is also gnostic. Now, in terms of $H$, we can write $\mathbb{P}(S_{t+1}(v)=1|S_t)$ equal to the sum $\sum_{S_{t}(w)\in\{0,1\}} H(v,w)$ if $S_t(v)=1$ and equal to $\sum_{S_{t}(w)=1} H(v,w)$ if $S_t(v)=2$.\footnote{These specific formulas assume that we are dealing with a synchronous case. The asynchronous case would have the probability be $\frac{n-1}{n}+\frac{1}{n}\sum_{S_{t}(w)\in\{0,1\}} H(v,w)$ when $S_t(v)=1$ and $\frac{1}{n}\sum_{S_{t}(w)=1} H(v,w)$ when $S_t(v)=2$. The equation that we need to verify stays unchanged after simple manipulation.} Thus, we just need to verify the equality: 
$$\sum_{\scriptscriptstyle S_{t}(v)=1}\mu(v) = \sum_{\scriptscriptstyle S_{t}(v)=1} \mu(v)\sum_{\scriptscriptstyle S_t(w)\in\{0,1\}} H(v,w) +  \sum_{\scriptscriptstyle S_{t}(v)=2} \mu(v)\sum_{\scriptscriptstyle S_{t}(w)=1} H(v,w).$$ 
Using the fact that $\sum_{w} H(v,w) = 1$ we can rewrite the last equality as being equivalent to:

$$\sum_{S_{t}(v)=1} \mu(v)\sum_{S_t(w)=2} H(v,w) = \sum_{S_{t}(v)=2} \mu(v)\sum_{S_t(w)=1} H(v,w)$$

This last equality is equivalent\footnote{Note that the expression has to hold for the case where there is a single blue and red nodes which shows one side. The other side is similarly simple} to $H(v,w)\mu(v) = H(w,v)\mu(w)$ for every pair of neighbours $w,v$. This holds for example when the graphs are undirected and the choice of neighbour is uniform. Thus, \eqref{eq:main} follows. 

Now we assume $G$ is such that every process converges and prove Equation~\ref{eq:consensus}. It is a known technique to use Doob's Stopping Theorem to go from a martingale to the probability of consensus, and that is what we use to show Equation \ref{eq:consensus}. Doob's Stopping Theorem guarantees that $\mathbb{E}(X_\tau) = \mathbb{E}(X_0) = X_0$, where $\tau$ is the time where consensus is reached. Together with the fact that $\mathbb{E}(X_\tau) = \mathbb{P}(\text{red wins}) \mathbb{E}(X_\tau \mid \text{red wins})  + \mathbb{P}(\text{blue wins}) E(X_\tau \mid \text{blue wins})$ (due to the process converging to either a complete red or blue state), and noting that $\mathbb{E}(X_\tau \mid \text{red wins})=1$ and $\mathbb{E}(X_\tau \mid \text{blue wins})=0$, we have finally that $\mathbb{P}(\text{red wins}) = X_0$.
% \end{remark}
\end{proof}


\subsubsection{Counterexample indicating that Reversibility is Required}

One may wonder whether the property $H(v,w)\mu(v)=H(w,v)\mu(w)$ of reversibility is required for the martingale to hold, as this is not a requirement in the generalised voter model (Proposition \ref{prop:nicola's-result}). Here, we give a simple (counter-)example of a graph with 3 nodes such that the voter model on it is not a reversible Markov chain. 

\begin{example}[Counterexample for non-reversible chains]
    Consider the initial configuration $S_0 = s_0$ depicted in Figure \ref{fig:counterexample} in a graph with matrix $H$ given by:
    \begin{equation}
        H = \begin{bmatrix}
        \frac{1}{2} & \frac{1}{2} & 0 \\
        0 & \frac{1}{2} &\frac{1}{2} \\
        \frac{1}{2} & 0 & \frac{1}{2}
        \end{bmatrix}
    \end{equation}
    Solving $\mu H = \mu$, we get $\mu = (\frac{1}{3},\frac{1}{3},\frac{1}{3})$. Note that the chain is not reversible because, e.g., $H(v_1, v_2)\mu(v_1) = \frac{1}{6} \neq 0 = H(v_2, v_1)\mu(v_2)$. We can determine $\mathbb{P}(R(v_3)\mid S_0)$ by solving the equation putting together $S_0$ (Figure \ref{fig:counterexample}) and the four possible states for $S_1$ (Figures \ref{fig:counterexample1}, \ref{fig:counterexample2}, \ref{fig:counterexample3}, \ref{fig:counterexample4}) also using that $\mathbb{P}(R(v_3)\mid S_0 = s_0) = \mathbb{P}(R(v_3)\mid S_1= s_{14})$, i.e, 
    \begin{equation}\label{eq:matingale}
        \mathbb{P}(R(v_3)\mid S_0) = s_0) = \frac{1}{4}\left(1 + 1 + 0 + \mathbb{P}(R(v_3)\mid S_0) \right)
    \end{equation}
    We get that $\mathbb{P}(R(v_3)\mid S_0 = s_0) = \frac{2}{3}$. From that, we get that $X_0 = \frac{5}{9}$ and that, on average, $X_1$ is given by $\mathbb{E}(X_1 \mid S_0) = \frac{1}{4}\left(\frac{1}{3} + \frac{2}{3} + 0 + \frac{5}{9} \right)= \frac{7}{18} \neq X_0$. Another way to see the martingale property does not hold is to evaluate the probability of red winning using a similar technique as in Equation \ref{eq:matingale} to get $\mathbb{P}(\text{red wins} \mid S_0) = \frac{1}{3} \neq X_0$.
\end{example}


\tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=0.5}}
\tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=0.5}}
\tikzset{white/.style={draw=black, circle, fill=white, inner sep=0.5}}


\begin{figure}[t]
\centering
\vspace{-0.5cm}
\begin{subfigure}[b]{1\linewidth}
\centering
\begin{tikzpicture}
    \tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=1.5}}
    \tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=1.5}}
    \tikzset{white/.style={draw=black, circle, fill=white, inner sep=1.5}}
    % Nodes
    \node[red] (A) at (-0.8, 0) { $v_1$};
    \node[blue] (B) at (0, 1) { $v_2$};
    \node[white] (C) at (0.8, 0) {$v_3$};
    
    % Edges - forming a complete triangle
    \path[->] (A) edge node[above left] {$\frac{1}{2}$} (B);  % v1 to v2
    \path[->] (B) edge node[above right] {$\frac{1}{2}$} (C);  % v2 to v3
    \path[->] (C) edge node[below] {$\frac{1}{2}$} (A);  % v3 to v1

    % Loops
    \path[->] (A) edge[loop left, min distance=3mm, in=160, out=200, looseness=1] node[left] {$\frac{1}{2}$} (A);  % Loop at v1
    \path[->] (B) edge[loop above, min distance=3mm, in=70, out=110, looseness=1] node[above] {$\frac{1}{2}$} (B);  % Loop at v2
    \path[->] (C) edge[loop right, min distance=3mm, in=340, out=380,] node[right] {$\frac{1}{2}$} (C);  % Loop at v3
\end{tikzpicture}
\caption{Initial Config. $S_0 = s_0$. We have $X_0 = \frac{5}{9}$, but $\mathbb{P}(\text{red wins} \mid S_0) = \frac{1}{3}$.}
\label{fig:counterexample}
\end{subfigure}

\vspace{-0.1cm}

\tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=0.5}}
\tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=0.5}}
\tikzset{white/.style={draw=black, circle, fill=white, inner sep=0.5}}

\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
   \tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=0.5}}
\tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=0.5}}
\tikzset{white/.style={draw=black, circle, fill=white, inner sep=0.5}}
% Nodes
    \node[blue] (A) at (-0.35, 0) {\tiny $v_1$};
    \node[blue] (B) at (0, 0.5) {\tiny $v_2$};
    \node[red] (C) at (0.35, 0) {\tiny $v_3$};
    
    % Edges - forming a complete triangle
    \path[->] (A) edge node[above left] {} (B);  % v1 to v2
    \path[->] (B) edge node[above right] {} (C);  % v2 to v3
    \path[->] (C) edge node[below] {} (A);  % v3 to v1

    % Loops
    % \path[->] (A) edge[loop left] node[left] {$\frac{1}{2}$} (A);  % Loop at v1
    \path[->] (A) edge[loop left, min distance=3mm, in=160, out=200, looseness=1] node[left] {} (A);  % Loop at v1
    \path[->] (B) edge[loop above, min distance=3mm, in=70, out=110, looseness=1] node[above] {} (B);  % Loop at v2
    \path[->] (C) edge[loop right, min distance=3mm, in=340, out=380,] node[right] {} (C);  % Loop at v3
\end{tikzpicture}
\caption{Config. $s_{11}$ with $X_1 = \frac{1}{3}$.}
\label{fig:counterexample1}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
   \tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=0.5}}
\tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=0.5}}
\tikzset{white/.style={draw=black, circle, fill=white, inner sep=0.5}}
% Nodes
    \node[red] (A) at (-0.35, 0) {\tiny $v_1$};
    \node[blue] (B) at (0, 0.5) {\tiny $v_2$};
    \node[red] (C) at (0.35, 0) {\tiny $v_3$};
    
    % Edges - forming a complete triangle
    \path[->] (A) edge node[above left] {} (B);  % v1 to v2
    \path[->] (B) edge node[above right] {} (C);  % v2 to v3
    \path[->] (C) edge node[below] {} (A);  % v3 to v1

    % Loops
    \path[->] (A) edge[loop left, min distance=3mm, in=160, out=200, looseness=1] node[left] {} (A);  % Loop at v1
    \path[->] (B) edge[loop above, min distance=3mm, in=70, out=110, looseness=1] node[above] {} (B);  % Loop at v2
    \path[->] (C) edge[loop right, min distance=3mm, in=340, out=380,] node[right] {} (C);  % Loop at v3
\end{tikzpicture}
\caption{Config. $s_{12}$ with  $X_1 = \frac{2}{3}$.}
\label{fig:counterexample2}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
   \tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=0.5}}
\tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=0.5}}
\tikzset{white/.style={draw=black, circle, fill=white, inner sep=0.5}}
% Nodes
    \node[blue] (A) at (-0.35, 0) {\tiny $v_1$};
    \node[blue] (B) at (0, 0.5) {\tiny $v_2$};
    \node[white] (C) at (0.35, 0) {\tiny $v_3$};
    
    % Edges - forming a complete triangle
    \path[->] (A) edge node[above left] {} (B);  % v1 to v2
    \path[->] (B) edge node[above right] {} (C);  % v2 to v3
    \path[->] (C) edge node[below] {} (A);  % v3 to v1

    % Loops
    \path[->] (A) edge[loop left, min distance=3mm, in=160, out=200, looseness=1] node[left] {} (A);  % Loop at v1
    \path[->] (B) edge[loop above, min distance=3mm, in=70, out=110, looseness=1] node[above] {} (B);  % Loop at v2
    \path[->] (C) edge[loop right, min distance=3mm, in=340, out=380,] node[right] {} (C);  % Loop at v3
\end{tikzpicture}
\caption{Config. $s_{13}$ with $X_1 = 0$.}
\label{fig:counterexample3}
\end{subfigure}
\begin{subfigure}[b]{.23\linewidth}
\centering
\begin{tikzpicture}
   \tikzset{red/.style={draw=red, circle, fill=red!30, inner sep=0.5}}
\tikzset{blue/.style={draw=blue, circle, fill=blue!30, inner sep=0.5}}
\tikzset{white/.style={draw=black, circle, fill=white, inner sep=0.5}}
% Nodes
    \node[red] (A) at (-0.35, 0) {\tiny $v_1$};
    \node[blue] (B) at (0, 0.5) {\tiny $v_2$};
    \node[white] (C) at (0.35, 0) {\tiny $v_3$};
    
    % Edges - forming a complete triangle
    \path[->] (A) edge node[above left] {} (B);  % v1 to v2
    \path[->] (B) edge node[above right] {} (C);  % v2 to v3
    \path[->] (C) edge node[below] {} (A);  % v3 to v1

    % Loops
    \path[->] (A) edge[loop left, min distance=3mm, in=160, out=200, looseness=1] node[left] {} (A);  % Loop at v1
    \path[->] (B) edge[loop above, min distance=3mm, in=70, out=110, looseness=1] node[above] {} (B);  % Loop at v2
    \path[->] (C) edge[loop right, min distance=3mm, in=340, out=380,] node[right] {} (C);  % Loop at v3
\end{tikzpicture}
\caption{Config. $s_{14}$ with $X_1 = \frac{5}{9}$.}
\label{fig:counterexample4}
\end{subfigure}
\caption{Counterexample for the conjecture that the Martingale property (Theorem \ref{thm:martingale}) is valid for non-reversible chains. Note that edge weights were omitted from Figures \ref{fig:counterexample1},\ref{fig:counterexample2}, \ref{fig:counterexample3}, and \ref{fig:counterexample4} for readability.}
\label{fig:counterexample_general}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Theorem~\ref{thm:martingale} is not easy to use for computing the probabilities in the general case. One simple use case is when there are no edges connecting agnostic vertices. It is clear in this case that $\mathbb{P}(R(v)\mid S_0)$ equals the proportion of red nodes among the neighbours (as we have seen in Remark \ref{rmk:one_round_probabilities}). There is an additional case where one can say something about the consensus probability as presented in the following corollary. 

\begin{corollary}[Solution for Complete Graph]
\label{corollary:complete_graph}
Let $G$ be a complete graph and consider the asynchronous (pull) voter model with agnostic states, with initial state $S_0$. Let $\gamma$ denote the proportion of red nodes among the gnostic ones. Then:
\begin{equation}\label{eq:1}
    \mathbb{P}(R(u)\mid S_0) = \gamma \text{ for all } u\in I
\end{equation}
As a consequence, 
\begin{equation}
    \mathbb{P}(\text{red wins}\mid S_0) = \gamma 
\end{equation}
\end{corollary}

\begin{proof}
    We want to compute $\mathbb{P}(R(u)\mid S_0)$ for every vertex $u\in I$. Let $T(u)$ denote the first time $u$ gets a colour. From the law of total probability, we can write 
    $$\mathbb{P}(R(u)\mid S_0) = \sum_{i=1}^{\infty} \mathbb{P}(T(u)=i)\mathbb{P}(R(u)\mid S_0, T(u)=i).$$ 
    It is easy to see that for nodes on $I$, irrespective of $S_0$, we have that $\mathbb{P}(R(u)\mid S_0, T(u)=1)$ equals the proportion of red nodes among the gnostic ones (which is $\gamma$). Thus, we would like to show that $\mathbb{P}(R(u)\mid S_0, T(u)=i)$ equals to  $\gamma$ and we will have concluded the result. For that, we will use induction.
    We need a strong induction hypothesis. Consider an alternative initial configuration $\tilde{S}_0$ with gnostic set that contains (or is equal to) the gnostic set in the original configuration ${S}_0$. We will show by induction that $\mathbb{P}(R(u)\mid \tilde{S}_0, T(u)=i)$ equals the proportion of red nodes in $\tilde{A}_0$. The base case, $i=1$ is easy as observed before. Suppose that for every $\tilde{S}_0$ we have the desired result for a certain $i$. We want to show it for $i+1$. We can now write the above by conditioning on all possible values $\tilde{S}_1$ can take: 
    \begin{multline}
        \mathbb{P}(R(u)\mid \tilde{S}_0, T(u)=i+1) = \\ \sum_{\tilde{S}_1} \mathbb{P}(\tilde{S}_1 \mid \tilde{S}_0, T(u)=i+1)\mathbb{P}(R(u) \mid\tilde{S}_1, T(u) = i+1).
    \end{multline}
    For convenience, we will omit the conditionals on $\tilde{S}_0$ and $T(u)$ from the term $\mathbb{P}(\tilde{S}_1 \mid \tilde{S}_0, T(u)=i+1)$ writing simply $\mathbb{P}(\tilde{S}_1)$. We can write $\mathbb{P}(R(u)|\tilde{S}_1, T(u) = i+1) = \mathbb{P}(R(u)\mid \bar{S}_0=\tilde{S}_1, \tilde{T}(u) = i)$, where we use $\bar{S}_0$ to denote a Markov chain at time $0$ that starts at state $\tilde{S}_1$. By the induction hypothesis, we have that $\mathbb{P}(R(u)|\tilde{S}_1, T(u) = i+1)$ equals the proportion of red nodes in the gnostic set of $\tilde{S}_1$. %Until now we have not used the fact that the subgraph generated by $A$ is $d$-regular.

    Let $r$ be the initial number of red nodes in $\tilde{S}_0$ and $b$ be the initial number of blue nodes in $\tilde{S}_0$. We would like to get 
    \begin{equation*}
        \sum_{\tilde{S}_1} \mathbb{P}(\tilde{S}_1)\mathbb{P}(R(u) \mid \tilde{S}_1, T(u) = i+1) = \frac{r}{b+r}.
    \end{equation*}
    Note that the term $\mathbb{P}(R(u) \mid\tilde{S}_1, T(u) = i+1)$ equals $\frac{|\tilde{S}_1=1|}{|\tilde{S}_1\in\{1,2\}|}$ by the induction hypothesis as argued above.  We are left to compute the term $\mathbb{P}(\tilde{S}_1)$ (which is conditioned on $\tilde{S}_0$ and $T(u)=i+1$). Observe that conditioning on $T(u)=i+1>1$ is equivalent to conditioning on vertex $u$ not becoming gnostic in round $1$. Thus, the term $\mathbb{P}(\tilde{S}_1)$ is the transition probability from $\tilde{S}_0$ to $\tilde{S}_1$ divided by the probability that vertex $u$ does not become gnostic in round $1$. First, the probability that vertex $u$ does not become gnostic in round $1$ is $1-\frac{b+r}{n(n-1)}$. Furthermore, note that we can combine multiple states $\tilde{S}_1$ based on the number of red and blue nodes they contain (as the term $\frac{|\tilde{S}_1=1|}{|\tilde{S}_1\in\{1,2\}|}$ depends only on that). It is thus, enough to compute the probability of transitioning into a state with a given number of red and blue nodes as that sums up the probabilities of all the relevant states $\tilde{S}_1$. 
    Using this fact, the states $\tilde{S}_1$ where the number of red and blue vertices does not change are not interesting as they will have exactly the desired target fraction. We are interested in the states where the ratio of red and blue vertices changes. Let $p_{r_1,b_1}$ be the probability that we have exactly $r_1$ red nodes and $b_1$ blue nodes in state $\tilde{S}_1$. 
    As in the asynchronous case, at most one vertex can change state, we only need to care about the terms: 
    $$p_{r+1,b}=\frac{(n-r-b-1)r}{n(n-1)\left(1-\frac{b+r}{n(n-1)}\right)}, p_{r,b+1}=\frac{(n-r-b-1)b}{n(n-1)\left(1-\frac{b+r}{n(n-1)}\right)}$$ 
    for the terms where one agnostic vertex different from $u$ becomes gnostic, and
    $$p_{r+1,b-1}=p_{r-1,b+1}=\frac{br}{n(n-1)\left(1-\frac{b+r}{n(n-1)}\right)}$$
    for the terms where one of the gnostic vertices changes color. 
    The remaining probability mass is on $p_{r,b}$. Thus, we would like to verify that 
    % the equation we would like to verify becomes:
    \begin{multline*}
        p_{r+1,b}\frac{r+1}{r+b+1}+p_{r,b+1}\frac{r}{r+b+1}+p_{r+1,b-1}\frac{r+1}{r+b}+p_{r-1,b+1}\frac{r-1}{r+b} 
        % \\= (1-p_{r,b})\frac{r}{r+b}
    \end{multline*}
    is equal to $(1-p_{r,b})\frac{r}{r+b}$. 
    By plugging in the computed values for the $p_{r_1,b_1}$ terms we get:
    $$\frac{(n-r-b-1)r(r+b+1)}{(r+b+1)(n(n-1)-b-r)}+\frac{2br^2}{(r+b)(n(n-1)-b-r)}$$
    which is the same as:
    $$\frac{(n-r-b-1)r(r+b)+2br^2}{(r+b)(n(n-1)-b-r)} = \frac{(n-r-b-1)(r+b)+2br}{(n(n-1)-b-r)}\frac{r}{r+b}$$
    and it can be easily seen that the term multiplying $\frac{r}{r+b}$ equals $p_{r+1,b}+p_{r,b+1}+p_{r+1,b-1}+p_{r-1,b+1}$ which is exactly $(1-p_{r,b})$. Thus, by induction we have $\mathbb{P}(R(u)\mid S_0, T(u)=i)=\frac{r}{r+b}= \gamma$ for every positive integer $i$ (and any starting state $S_0$). As a result, $\mathbb{P}(R(u)\mid S_0) = \sum_{i=1}^{\infty} \mathbb{P}(T(u)=i)\mathbb{P}(R(u)\mid S_0, T(u)=i) = \gamma$.
    Plugging the values in the martingale, it is easy to see that the probability of red consensus is also $\gamma$.


\end{proof}


\subsection{Estimating probabilities with Markov chain Monte Carlo}
\label{subsec:MCMC}
While Theorem~\ref{thm:martingale} gives us a potential means of computing the exact probability of achieving consensus with a certain colour, a generalization of Proposition~\ref{prop:nicola's-result}, i.e., in general solution for  $\mathbb{P}(R(v)\mid S_t)$ may not be efficient to compute. 
With that in mind, we propose evaluating the probabilities of consensus of the voter model with agnostic nodes on general graphs by:


\begin{algorithm}
\caption{Estimating the Probability of Red Consensus}
\begin{algorithmic}[1]
    \State \textbf{Simulate} the process until all nodes are gnostic.
    \State \textbf{Apply} Proposition~\ref{prop:nicola's-result} once all nodes are gnostic to obtain a probability of red consensus $p$.
    \State \textbf{Repeat} the simulation multiple times and compute the average of the probabilities of red consensus $p$ from each run as an unbiased estimate of the true probability.
\end{algorithmic}
\end{algorithm}

By using known results from rumour spreading, Section~\ref{sec:time_bounds} shows that the simulation time required until all nodes are gnostic is at most $O(n^2\log(n))$ for general graphs and only $O(n\log(n))$ for Erd\"os R\'enyi random graphs with high probability.
As usual, 'with high probability' means with probability going to $1$ as the graph size $n$ goes to infinity. The probability space is the product space of the random graph and the consensus process.
In particular, this implies that running the simulation once is not substantially slower than computing the terms $\mu(v)$ (at least on the worst case). In Section~\ref{sec:experiments}, we present experiments showing that only a few runs (typically less than a $200$) are required to obtain a good estimate (standard error below $0.01$) of the consensus probability. In particular, not only running the process until all nodes are gnostic is much faster than waiting until consensus is reached, the error of the estimate obtained from multiple runs is also much lower when we end the process at the point where all vertices are gnostic.

\section{Convergence Time Bounds for Consensus}
\label{sec:time_bounds}

In addition to the question of what are the probabilities of achieving consensus with a particular colour, the community has also focused on determining the time it takes for consensus to be achieved on general graphs. Lemma~\ref{lemma:time_bounds} shows that, when agnostic vertices are present, expected consensus times are bounded by the expected time it takes for agnostic vertices to disappear plus whatever bounds one can prove for the classical voter model.

\begin{lemma}[Time bounds for consensus]
\label{lemma:time_bounds}
    Given a graph $G$ and a (pull) voter model on $G$ with agnostic states (either synchronous or asynchronous). Let $T_c$ denote the time it takes for consensus to be achieved and let $T_a$ denote the time it takes for the agnostic vertices to disappear. Let $S_0$ be the starting configuration. Let $f(n)$ be a bound on the expected time of consensus being reached on $G$ in the classical voter model (without any agnostic vertices) and any initial configuration. Then $\mathbb{E}(T_c|S_0)\leq \mathbb{E}(T_a|S_0)+f(n)$.
\end{lemma}

\begin{proof}
    In order for the process to reach consensus, it is necessary that all agnostic vertices become gnostic first. Thus, $T_c = T_a+T$, where $T$ is the time it takes for consensus to be reached once all agnostic vertices became gnostic. Taking expectations on both sides, and using linearity of expectation, we have that $\mathbb{E}(T_c|S_0) = \mathbb{E}(T_a|S_0)+\mathbb{E}(T|S_0)$. By using law of total expectation we have that $\mathbb{E}(T|S_0) = \sum_{s\in\{r,b\}^V} \mathbb{P}(S_{T_a} = s|S_0)\mathbb{E}(T|S_{T_a}=s, S_0)$ where $V$ is the vertex set of the graph, and $\{r,b\}$ represent the colours set (say, red and blue). Now, observe that once all agnostic vertices become gnostic, the process becomes equivalent to a standard consensus process with whatever configuration $S_{T_a}=s$ was reached by that point. That is, conditioned on $s$ we have by our assumption $\mathbb{E}(T|S_{T_a}=s)\leq f(n)$. Thus $\mathbb{E}(T|S_0) \leq \sum_{s\in\{r,b\}^V} \mathbb{P}(S_{T_a}=s|S_0)f(n) = f(n)$. Adding $\mathbb{E}(T_a|S_0)$ and using linearity of expectation, we get $\mathbb{E}(T_c|S_0) \leq \mathbb{E}(T_a|S_0)+f(n)$
\end{proof}

Note that Lemma~\ref{lemma:time_bounds} is only useful for graphs $G$ in which consensus is guaranteed. Otherwise, $f(n)$ may be infinite, in which case, although true, the result does not give us any useful information.  As a result of Lemma~\ref{lemma:time_bounds}, all bounds which hold for the standard voter model will also hold for the case where agnostic vertices are present with an extra time for the agnostic vertices to disappear. In practice, the time for agnostic vertices to become gnostic is typically much shorter than the time it takes to reach consensus as a simple consequence of rumour spreading results. Indeed, it is easy to see that the process of agnostic vertices disappearing is analogous to that of standard rumour spreading. However, the typical results in the literature for the rumour spreading process deal only with the push model, or sometimes the push-pull model. We, on the other hand, want results for the pull model. This is only a minor inconvenience though, as the proof ideas from previous works can also be used for the pull model. Proposition~\ref{prop:rumour_general} shows that for general graphs, the expected time it takes in the pull model for the agnostic vertices to disappear is $O(n^2\log(n))$. Observe that a star graph, with a gnostic node not on the center, as well as a path, would take an expected time of order $n^2$ for the agnostic vertices to become gnostic. Thus, Proposition~\ref{prop:rumour_general} is almost tight. We defer the proof of Proposition~\ref{prop:rumour_general} to the Appendix as it is just reusing known ideas from the rumour spreading model for the push model in the pull case.

\begin{proposition}[rumour spreading bounds for general graphs]
    Given a regular graph $G$ (potentially with loops) and a (pull) voter model with agnostic states (either synchronous or asynchronous), where the vertices choose their neighbours (including potentially itself) with equal probability. Let $T_a$ denote the number of rounds it takes for the agnostic vertices to disappear from the graph. Then $\mathbb{E}[T_a] = O(f(n))$ where $f(n) = n\log(n)$ for the synchronous case and $f(n) = n^2\log(n)$ for the asynchronous case.
    \label{prop:rumour_general}
\end{proposition}

While Proposition~\ref{prop:rumour_general} gives a bound for general graphs, it is likely that for a typical graph, the rumour spreading process is significantly faster. Indeed, it has been shown by~\cite{fountoulakis2010reliable, panagiotou2017asynchronous} that in the case of random graphs, the time it takes for agnostic vertices to disappear is of order $n\log(n)$. As their results are for the push model, we state Proposition~\ref{prop:rumour_random} which is for the pull model. We again defer the proof to the appendix as we are just reusing previously known ideas for the push model of the rumour spreading process in the pull case.

\begin{proposition}[rumour spreading bounds for random graphs]
    Let $p>> \log(n)/n$ and $G\sim G(n,p)$ be an Erd\"os R\'enyi random graph. Consider a (pull) voter model on $G$ with agnostic states (either synchronous or asynchronous). Let $T_a$ denote the number of rounds it takes for the agnostic vertices to disappear. Then, with high probability we have that $\mathbb{E}[T_a] = O(f(n)\log(n))$ where $f(n) = 1$ in the synchronous case and $f(n)=n$ in the asynchronous case.
    \label{prop:rumour_random}
\end{proposition}

Lastly, as a consequence of the above results, one can easily see that a single run of our Markov Chain Monte Carlo algorithm is relatively fast, as it is the time to simulate the process until no agnostic vertex remains plus the computation of the influences $\mu(v)$ (which only needs to be done once for the graph). Apart from the number of runs necessary to obtain a good estimate (which is analyzed in Section~\ref{sec:experiments}), both propositions show that a single run of the MCMC algorithm proposed takes time $O(n^2\log(n))$ in the worst case and $O(n\log(n))$ in the typical case which means MCMC is an efficient way of estimating the probabilities of consensus.

    

\section{Experimental Analysis with MCMC}
\label{sec:experiments}


Our key approach is to estimate probabilities by using MCMC, performing the simulations only until the point where all nodes are gnostic and using Proposition~\ref{prop:nicola's-result} to get probability values that can later be averaged over many runs to obtain an unbiased estimator for the consensus probability of a certain colour. Results from the rumour spreading theory ensure that each single run is fast. In this section, we aim to argue that not many runs are necessary to obtain good probability estimates by making a few experiments for certain graph families. 

Before we describe the experiments in detail, note that an upper bound on the number of experiments required can be trivially obtained by considering the case where we do not stop the algorithm when all nodes become gnostic but instead go all the way until consensus is reached. This will give us a series of zeros and ones (corresponding to the target colour being the consensus one or not), which can then be averaged to get a probability. It is easy to see that each run is distributed as a Bernoulli and therefore their sum is distributed as a Binomial with the number of iterations $it$ and the target consensus probability $p$ as parameters. The average will then have variance equal to $\frac{p(1-p)}{it}$ which implies that the standard error (which is a proxy of the true error in the estimate) will scale as the inverse of the square root of the number of iterations. It is also easy to see that this approach of running the algorithm until consensus is reached gives worse estimates than the one where we stop the algorithm when agnostic nodes disappear.

Our first experiment will show that using our approach is substantially better than the error estimate from the previous paragraph. We run our algorithm for cliques and cycles with $1001$ nodes for a varying number of runs and plot the standard error $\sigma_{\bar{x}}$ of the estimate for it.\footnote{Note that having an odd number of nodes on cycles guarantees the process always converges} We contrast those results with the standard error of the algorithm, where each run goes all the way until consensus is reached (described in the previous paragraph). Observe that Figure~\ref{fig:exp1} shows that the error rates are substantially lower for the case where we use our algorithm versus the case where one runs until consensus is reached~\footnote{Note that, additionally, each individual run takes substantially longer to complete if we wait until consensus is reached}. In fact, even as little as $40$ runs are enough to be below $0.01$ error in both cases. 

Figure~\ref{fig:exp1} also contains an additional line showing the standard error of our algorithm using a connected subgraph with $1001$ nodes and $1925$ edges of the graph representing a social network in Slovakia (Pokec from~\cite{takac2012data}). This subgraph was generated by selecting an initial random vertex $v$ and performing a random walk (depth first search) until $1001$ vertices were selected. Naturally, the line corresponding to the algorithm where we wait for consensus is very close to the result for cliques and cycles as should be expected (since it approximates the standard deviation of a Binomial distribution with number of iterations and target probability $p$ as parameters divided by the number of iterations). Moreover, note that our algorithm is again substantially better than waiting until consensus is reached, just like for cliques and cycles, suggesting that our approach is also very effective for a typical social network. 

\begin{figure}
  \includegraphics{exp1.pdf}
  \caption{A comparison of the cumulative standard error of the probability of red consensus after all nodes are gnostic and the actual consensus until the simulation finishes. Each simulation ran 400 times on cliques, cycles and a connected subgraph of the Pokec social network with 1001 nodes (5\% red, 5\% blue, 90\% agnostic). For cycles, the initial configuration has all red nodes side by side follow by all blue nodes side by side. For the Pokec subgraph, red and blue nodes were assigned at random, with the rest being agnostic.}
  \label{fig:exp1}
\end{figure}

As an additional experiment, we wonder whether there is a positive effect of increasing the graph sizes on $\sigma_{\bar{x}}$. It turns out that the answer is yes as Figure~\ref{fig:exp2} shows. There, we run our algorithm for graph sizes varying from $300$ to $3000$, with a fixed number of iterations ($400$).\footnote{Using less than $400$ iterations yields a graph with more variance as expected, but the same type of decay.} We perform the experiment on cliques and cycles again and vary the proportion of gnostic nodes as well.

\begin{figure}
  \includegraphics{exp2.pdf}
  \caption{A comparison of the standard error of the probability of red consensus running the simulation 400 times for cliques and cycles of different sizes starting with different proportions of gnostic nodes (5\%, 50\%). Again, the initial configuration for cycles has all red nodes side by side follow by all blue nodes side by side.}
  \label{fig:exp2}
\end{figure}

Lastly, we add that we also performed both experiments on Erd\H{o}s-R\'enyi random graphs with the same values of $n$ and $p=0.05$ and found that the $\sigma_{\bar{x}}$ line for the random graphs essentially stays superposed with the complete graph so we did not find it necessary to include it in the figure to avoid pollution.

For the code repository for these experiments, see \cite{gauy2025vmmrs} or access \url{https://github.com/tmadeira/vmmrs}. 


\section{Discussion}
\label{sec:discussion}

For simplicity, in this work, we focused on the pull model as that allows us to analyse both synchronous and asynchronous protocols. However, it is natural to wonder what happens when other strategies are used to transmit information. While synchronous push does not make sense for the voter model, asynchronous push and asynchronous push-and-pull could be used and Proposition~\ref{prop:nicola's-result} shows that the martingale for the voter model holds for asynchronous push. It is likely possible to use our martingale from Theorem~\ref{thm:martingale} for those strategies as well, requiring only a little extra work, much like the distinction between the asynchronous and synchronous pull protocols.

Much more importantly though, the Markov chain Monte Carlo method of estimating the probabilities of consensus would function in exactly the same way. We could make use of the many known results from the rumour spreading literature to give us guarantees of fast runtime of a single run of the MCMC algorithm on many different types of graphs for those protocols, such as: bounds for the case of general graphs~\cite{feige1990randomized, acan2015push}, random graphs~\cite{fountoulakis2010reliable, panagiotou2017asynchronous, acan2015push}, preferential attachment graphs~\cite{doerr2012asynchronous}, graphs with good conductance~\cite{chierichetti2010rumour} and social networks~\cite{chierichetti2011rumor}. In general, our work implies that one can efficiently estimate probabilities of consensus for a given colour even in the case where agnostic nodes are present as long as a protocol that allows for fast rumour spreading is used.

\section{Conclusion and Future Work}

Here, we introduce a variant of the voter model in which nodes can be agnostic, i.e., have no opinion or colour. Once gnostic, nodes cannot return to being agnostic. This can therefore been seen as a merge between two well-studied processes: the classical voter model and the rumour spreading process.  Our approach allows for efficient estimation of the consensus probabilities for many different information transmission protocols (such as, synchronous pull, asynchronous pull, asynchronous push and asynchronous push-and-pull). We also provide a martingale akin to the one from the classical voter model, and use it to compute exact probabilities of consensus for complete graphs, and initial configurations in general graphs but where there are no edges between agnostic nodes. 

In future work, we consider attempting exact computation of the consensus probabilities for other graph families, like $d$-regular graphs. Moreover, as observed in Section~\ref{sec:related_work} there is an information transmission protocol that involves nodes transitioning into an undecided state. That protocol has the advantage of guaranteeing (with probability converging to $1$) that a majority opinion achieves consensus. We find it an interesting question whether the results with that protocol also hold in the case where agnostic nodes are present and how the agnostic nodes influence the consensus times. Additionally, it may be of interest to study continuous consensus protocols~\cite{mizrahi2008continuous} in the presence of initially agnostic processes.

\section{Related Work}
\label{sec:related_work}

The notion of reversibility in the context of the voter model has previously been used by Hassin \& Peleg~\cite{hassin2001distributed} to provide winning probabilities on a class of dynamic networks called `stabilising dynamic graphs'. 
In these networks, until a given round, edges may disconnect and nodes attempting to copy a disconnected neighbour keep their own colour, similarly to gnostic nodes choosing agnostic ones in our protocol. 
The authors showed that their previous results hold for networks with reversible Markov chains, i.e. the total influence of the nodes of a given colour remains a martingale in the new process.

Several works studied agents that can be `undecided' as an intermediate state, with nodes transitioning to this state when they select a differently coloured neighbour (\cite{angluin,perron, clementi_et_al:LIPIcs.MFCS.2018.28, petra1}). For a complete graph with binary opinions, the synchronous variant of this protocol has been shown to converge to the most common (plurality) colour in $O(\log n)$ rounds with high probability, assuming there is an initial difference of $\Omega(\sqrt{n \log n})$ in the numbers of agents with each colour~\cite{clementi2018tight}. Similar results have been obtained for the asynchronous protocol in the context of chemical reaction networks~\cite{condon2020approximate}. Additionally, for the consensus problem with $k>2$ opinions, Becchetti et al. defined a `monochromatic distance' function which measures the distance between any colour configuration and consensus, and used this to bound the convergence time of the synchronous process by $O(k \log n)$~\cite{becchetti2015plurality}. 

On the other hand, Demers et al. proposed rumour-spreading protocols to aid the maintenance of distributed databases; these include push, pull, and push-pull transmissions~\cite{demers1987epidemic}. For the synchronous push model, it is known that the number of rounds required to broadcast the rumour to all nodes is at most $O(n \log n)$, which is tight for the star graph~\cite{feige1990randomized}. This process has also been analysed for several other topologies, including complete graphs, hypercubes, bounded-degree graphs, and random graphs~\cite{feige1990randomized}.

Our model also has similarities to the biased voter model proposed in \cite{george}, where one colour (corresponding to the agnostic state) has a bias of $0$. However, their results do not apply in our setting since they assume that one colour has a strictly higher preference than all other colours. See also \cite{Lanchier_Neuhauser_2007} for the biased voter model with $2$ opinions in the continuous-time model. Our work is also related to~\cite{zehmakan2024viral}. In it, the authors study the evolution of a process with agnostic nodes, where the key difference is that gnostic nodes can never change colour. 

Previous works on opinion diffusion have also studied related concepts to agnostic nodes, such as stubbornness. Those are mostly in the context of the majority model~\cite{auletta2017information,out2021majority} and the related Friedkin-Johnson model~\cite{xu2022effects, shirzadi2024stubborn}. The main difference between these models and ours is that the process dynamics is deterministic in the majority and Friedkin-Johnson models, whereas in the voter model, the process dynamics are randomised.

\section{Code Repository}
For the code repository for these experiments, see \cite{gauy2025vmmrs} or access \url{https://github.com/tmadeira/vmmrs}. 

\begin{acks}
This work was supported by FAPESP grant number 2022/16374-6 (MMG) and EPSRC grant EP/W005573/1 (FMT).
\end{acks}




\bibliographystyle{ACM-Reference-Format} 
\bibliography{sample}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage

\appendix 



\section{Additional Experiments}
\begin{figure}[b]
  \includegraphics{exp3.pdf}
  \caption{A comparison of the cumulative standard error of the probability of red consensus after all nodes are gnostic. Each simulation ran 200 times on cliques and the full Pokec social network, both with 1,632,803 nodes (5\% red, 5\% blue, 90\% agnostic). For the Pokec graph, red and blue nodes were assigned at random, with the rest being agnostic.}
  \label{fig:exp3}
\end{figure}

As the experiments in Section \ref{sec:experiments} were only done for smaller graphs, here we include additional experiments to show our algorithm at work on larger, real-world graphs. 

The first  additional experiment (Figure~\ref{fig:exp3}) shows the cumulative standard error of the probability of red consensus at the point in which all nodes become gnostic. The graphs compared are the full Pokec social network and a clique of the same size. We perform $200$ repetitions of the MCMC algorithm and plot the standard error curve over the number of runs. Observe that, for large graph sizes the initial error is much smaller than for smaller graph sizes. This is already suggested by Figure~\ref{fig:exp2} but is further confirmed by the experiment on large graphs.





\begin{figure}[t]
    \includegraphics{exp4.pdf}
    \caption{A comparison of the standard error of the probability of red consensus running the simulation 400 times for stochastic block models of different sizes with 10\% of the nodes gnostic (half red, half blue).}
    \label{fig:exp4}
\end{figure}


%%%

We also do a second additional experiment (shown in Figure~\ref{fig:exp4}) where the graphs considered are stochastic block models with different community sizes. These are generalizations of Erd\H{o}s-Rényi random graphs where the graph is split into communities and there are different connection probabilities inside each community and between communities. For simplicity, we set the connection probability inside a community to be $1$ and the probability between communities to be $0.05$ and vary the community size $k$. Observe that the standard error $\sigma_{\bar{x}}$ is quite small after $400$ runs, that $\sigma_{\bar{x}}$ decreases as the graph size increases and the community size does not make a significant difference on $\sigma_{\bar{x}}$.


\section{Proof of Propositions~\ref{prop:rumour_general} and~\ref{prop:rumour_random}}

The literature on the rumour spreading process typically dealt with the push model. As we are dealing with a pull model we cannot directly refer to those results. For completeness, we include the proofs here for the pull model. In general, they are mere adaptations of the known approaches for the push model.

Proposition~\ref{prop:rumour_general} is a direct consequence of the following slightly stronger statement.
\begin{proposition}[rumour spreading bounds for agnostic]
    Given a regular graph $G$ (potentially with loops) and a (pull) voter model with agnostic states (either synchronous or asynchronous), where the vertices choose their neighbours (including potentially itself) with equal probability, with starting configuration $S_0\in \{0,1,2\}^n$. Let $T_a$ denote the number of rounds it takes for the agnostic vertices to disappear from the graph. The probability $P[T_a\geq Cf(n)]\leq 2n^{-C/3+2}$, where $f(n) = n\log(n)$ in the synchronous case and $f(n)=n^2\log(n)$ in the asynchronous case. In particular, this implies that $\mathbb{E}[T_a] \leq 6f(n)+4$ for all $n$.
\end{proposition}

\begin{proof}
    Let $D_0$ denote the initial set of gnostic vertices. For every $i>0$, let $D_i$ be the set of vertices of distance exactly $i$ to the set $D_0$. Assuming the vertices from $\cup_{j=0}^{i}D_j$ have already become gnostic, given a constant $C$, we bound the probability that it takes longer than $Cd_{i+1}g(n)$ for all vertices in $D_{i+1}$ to become gnostic, where $d_{i+1}$ is defined as one plus the max degree of a vertex in $D_{i+1}$ in the subgraph $\cup_{j=i+1}^{\infty}D_j$ and $g(n) = f(n)/n$ (ie, $g(n) = \log(n)$ in the synchronous case and $n\log(n)$ in the asynchronous case). If we let $T_v$ denote the time it takes for a vertex $v\in D_{i+1}$ to become gnostic, the probability that it takes longer than $Cd_{i+1}g(n)$ for all vertices in $D_{i+1}$ to become gnostic can be written as $\mathbb{P}(\exists v\in D_{i+1},T_v>Cd_{i+1}g(n))$.
    By union bound, this is at most $\sum_{v\in D_{i+1}}\mathbb{P}(T_v\geq Cd_{i+1}g(n))$. Now, we bound $\mathbb{P}(T_v\geq Cd_{i+1}g(n))$. Note that vertex $v$ has at least one gnostic neighbour as we assume all vertices from $D_{i}$ have turned gnostic already. Moreover, it has at most $d_{i+1}-1$ agnostic neighbours. In the asynchronous case, it also has a probability of $\frac{1}{n}$ of being picked. Thus, the probability that $v$ turns gnostic in a single step is at least $\frac{1}{d_{i+1}h(n)}$, where $h(n) = 1$ in the synchronous case and $h(n)=n$ in the asynchronous case. Therefore, the probability that $v$ remains agnostic for $Cd_{i+1}g(n)-1$ steps is at most $(1-\frac{1}{d_{i+1}h(n)})^{Cd_{i+1}g(n)-1} \leq 2n^{-C}$, where we used that $(1-\frac{1}{x})^x\leq e$, that $g(n) = h(n)\log(n)$ and $(1-\frac{1}{d_{i+1}h(n)})\geq \frac{1}{2}$. Thus, we have shown that, for all constants $C$, $\mathbb{P}(T_v>Cd_{i+1}g(n)) \leq 2n^{-C}$. As a result, if $T_{i+1}$ denotes the time it takes for all vertices in $D_{i+1}$ to become gnostic, we have 
    \begin{equation*}
    \mathbb{P}(T_{i+1}>Cd_{i+1}g(n))\leq \sum_{v\in D_{i+1}}\mathbb{P}(T_v>Cd_{i+1}g(n))\leq 2n^{-C+1}.
    \end{equation*}

    Now, we observe that we can bound $T_a$ by the time it takes for the agnostic vertices to disappear in the following auxiliary process: only when all the vertices in $D_{j}$ for $j\leq i$ have turned gnostic, can a vertex in $D_{i+1}$ turn gnostic. Let $T_{i}$ be the time it takes for all vertices in $D_{i}$ to turn gnostic after all vertices in $D_{i-1}$ have turned gnostic in this auxiliary process. We have that $T_a\leq \sum_{i=1}^d T_{i}$, where $d$ is the max distance between a vertex and the initially gnostic set $D_{0}$. The probability that there is a $T_i$ which takes longer than $\frac{C}{3}d_ig(n)$ is bounded by $dn^{-C/3+1} \leq n^{-C/3+2}$ by union bound. Now, observe that 
    $d_i\leq 1+|D_{i}|+|D_{i+1}|$ and thus, 
    $$\sum_{i=1}^{d} \frac{C}{3}d_ig(n) \leq \sum_{i=1}^d \frac{C}{3}(1+|D_i|+|D_{i+1}|)g(n)\leq Cng(n)$$
    where we use that $d\leq n$ and $\sum_{i=1}^d |D_i|\leq n$. Thus, we have 
    $$P\Bigg(\sum_{i=1}^d T_i > Cng(n)\Bigg) \leq \mathbb{P}(\exists i, T_i > \frac{C}{3}d_ig(n))\leq 2n^{-C/3+2}.$$ As a consequence, we have that 
    $$P\Bigg(\sum_{i=1}^d T_i \leq Cng(n)\Bigg) \geq 1-2n^{-C/3+2},$$ which implies $$\mathbb{P}(T_a\leq Cng(n))\geq 1-2n^{-C/3+2}$$ as $T_a\leq \sum_{i=1}^d T_i$. Finally, we get 
    $$\mathbb{P}(T_a\geq Cf(n))\leq 2n^{-C/3+2}$$ where we used that $f(n) = ng(n)$.

    Now we set $C=6$ and observe that: $\mathbb{E}[T_a] = \sum_{t}t\mathbb{P}(T_a=t) = \sum_{t\leq Cf(n)}t\mathbb{P}(T_a=t)+\sum_{t>Cf(n)}t\mathbb{P}(T_a=t)$. We have that 
    $$\sum_{t\leq Cf(n)}t\mathbb{P}(T_a=t)\leq Cf(n)$$ and  
    $$\sum_{t>Cf(n)}t\mathbb{P}(T_a=t)\leq \sum_{t>Cf(n)}t\mathbb{P}(T_a>=t).$$ As 
    $\mathbb{P}(T_a\geq t) \leq \frac{n^{-\frac{t}{3f(n)}+2}}{2}$. If we set $t = 3cf(n)$ for some real $c>2$ we have $\mathbb{P}(T_a\geq t)\leq 2n^{-c+2}$. Thus, we have 
    $$\sum_{t>Cf(n)}2n^{-c+2}\leq \int_{2}^{\infty} 2n^{-c+2}dc < 4.$$
\end{proof}

The rumour spreading literature also dealt with the case for Erd\H{o}s-R\'enyi random graphs in the push model. We use the same proof ideas from~\cite{panagiotou2017asynchronous} and~\cite{fountoulakis2010reliable} to show Proposition~\ref{prop:rumour_random} which is the pull model for random graphs.

\begin{proposition}[Rumour spreading bounds for random graphs - pull version]
    Let $p>> \log(n)/n$ and $G\sim G(n,p)$ be an Erd\H{o}s-R\'enyi random graph. Consider a (pull) voter model on $G$ with agnostic states (either synchronous or asynchronous). Let $T_a$ denote the number of rounds it takes for the agnostic vertices to disappear. Then, with high probability we have that $\mathbb{E}[T_a] = O(f(n))$ where $f(n) = \log(n)$ in the synchronous case and $f(n)=n\log(n)$ in the asynchronous case.
\end{proposition}

\begin{proof}
   \textbf{Asynchronous case:} For the asynchronous case, we will show that $\mathbb{E}(T_a) \geq (2+o(1))n\log(n)$. The idea is simple and borrows from~\cite{panagiotou2017asynchronous}. The main difference in the proof is that we do it for the pull model, instead of the push model. Assuming that $i$ vertices are gnostic, let $t_i$ denote the number of rounds it takes for the vertex of number $i+1$ to become gnostic. We can assume, wlog, that we start with a single gnostic vertex. Then $T_a = t_1+\ldots+t_{n-1}$. By linearity of expectation, we have that $\mathbb{E}(T_a) = \sum_{i=1}^{n-1}\mathbb{E}(t_i)$. Thus, we just need to bound $\mathbb{E}(t_i)$ for each $i$. The crucial step is to notice that conditioned on the current gnostic set $S$ of size $i$, we have that $t_i$ is geometrically distributed with probability of a newly gnostic vertex at any given round given by $\frac{1}{n}\sum_{v\in [n]\setminus S} \frac{d_{S}(v)}{d(v)}$, where $d_{S}(v)$ denotes the degree of vertex v in the set $S$ and $d(v)$ is the degree of vertex $v$. We can assume that $d(v) = (1+o(1))np$ for all $v$ as that happens with high probability. Moreover 
   \begin{equation*}
   \sum_{v\in [n]\setminus S} d_{S}(v) = e(S,[n]\setminus S) = (1\pm \sqrt{\frac{8}{\alpha(n)}})(n-|S|)|S|p
   \end{equation*}
   by Lemma~\ref{lemma:kosta} with high probability. The expected time of a geometrically distributed random variable with probability $q$ is given by $1/q$. Thus, by combining all of the above, for all graphs $G$ satisfying the conditions of Lemma~\ref{lemma:kosta}, we have by law of total expectation that $\mathbb{E}(t_i) = \sum_{S}\mathbb{P}(S)\mathbb{E}(t_i|S)$ and $\mathbb{E}(t_i|S) = \frac{n^2p}{(1\pm \sqrt{\frac{8}{\alpha(n)}})(n-i)ip}$ (where we use that $|S| = i$). Thus $\mathbb{E}(t_i) = (1+o(1))\frac{n^2}{i(n-i)} = (1+o(1))(\frac{n}{i}+\frac{n}{n-i})$. By summing over all $i$ we get $\mathbb{E}(T_a) = \sum_{i=1}^{n-1}(2+o(1))(\frac{n}{i}) \leq (2+o(1))n\log(n)$.
    
    \textbf{Synchronous case:} We follow similar steps to the proof of the rumour spreading process with a push protocol for random graphs presented in~\cite{fountoulakis2010reliable}. Let $S_t$ denote the gnostic set at round $t$. We split the process in 2 stages: given a small $\varepsilon>0$, the first stage, which takes time $T_1$, is for the range where $|S_t|\leq \varepsilon n$; the second stage, which takes time $T_2$, is for the range $\varepsilon n\leq |S_t|$.

    For the first stage, we split it in two parts: the time $T_1'$ it takes to activate the first $\log^{1/2}(n)$ vertices and the time ($T_1-T_1'$) it takes to activate $\varepsilon n$ vertices after the first $\log^{1/2}(n)$ were activated. To compute $T_1'$, observe that there are at least $(1+o(1))np$ agnostic vertices with at least one gnostic neighbour (as a single gnostic vertex has $(1+o(1))np$ neighbours and as $np>>\log^{1/2}(n)$ so at least $(1+o(1))np$ of those are agnostic). For every round, the probability that no new agnostic vertex becomes gnostic is at most $(1-\frac{1}{(1+o(1))np})^{(1+o(1)np)}\leq ce^{-1}$ (where $c$ is a constant very close to $1$ for large enough $n$, for simplicity set $n$ large enough so that $c<2$) as there are at least $(1+o(1))np$ agnostic vertices with at least one gnostic neighbour and each of them has degree $(1+o(1))np$. As a result, the probability that $T_1' > C\log^{1/2}n$, for some constant $C>1$, is at most $(2/e)^{(C-1)\log^{1/2}(n)} = o(1)$ as there are at least $(C-1)\log^{1/2}(n)$ rounds where no new agnostic vertex became gnostic. 
    
    Thus, $T_1' = o(\log(n))$ with high probability. Let us now compute how long it takes to go from $\log^{1/2}(n)$ gnostic vertices to $\varepsilon n$ gnostic vertices for some small $\varepsilon$ constant. Let $v$ be an agnostic vertex at some round $t$. The probability that $v$ becomes gnostic is $\frac{d_{S_t}(v)}{d(v)}$, where $d_{S_t}(v)$ is the size of the neighbourhood of $v$ in the set $S_t$ and $d(v)$ is the degree of vertex $v$. Let $X_t(v)$ be an indicator random variable for the event that $v$ turns gnostic at round $t$. By the discussion above, $\mathbb{E}[X_t(v)] = \frac{d_{S_t}(v)}{d(v)}$. If $X_t = \sum_{v\notin S_t}X_{t}(v)$ denotes the number of newly acquired vertices at round $t$, then we have that $\mathbb{E}[X_t] = \sum_{v\notin S_t}\mathbb{E}[X_{t}(v)]$, which by the above discussion is equal to $\mathbb{E}[X_t] = \sum_{v\notin S_t}\frac{d_{S_t}(v)}{d(v)}$. By using Lemma~\ref{lemma:kosta}, we can assume that, with high probability, every vertex v has degree $d(v) = (1+o(1))np$ and moreover we can assume that 
    \begin{equation}
        \sum_{v\notin S_t}d_{S_t}(v) = e(S_t, [n]\setminus S_t) = (1\pm \sqrt{\frac{8}{\alpha(n)}})(n-|S|)|S|p.
    \end{equation}
    As a result, we have that $\mathbb{E}[X_t] = (1\pm \sqrt{\frac{8}{\alpha(n)}})\frac{(n-|S_t|)|S_t|}{n}$. For the first stage, where $|S_t|\leq \varepsilon n$ we cannot use Azuma-Hoeffding's inequality~\footnote{The random variable considered is the number of agnostic vertices that turn gnostic in one round. Note that this is a function of random variables denoting the choice of neighbour to pull from made by each agnostic vertex and that these choices are independent.} 
    % \fnote{not super clear what teh r.v. is. Is it jsut the change in one step, then thats' fine. Are they independent?}
    (Theorem~\ref{thm:azuma}), as there are at least $(1-\varepsilon)n$ agnostic vertices and the expectation $\mathbb{E}[X_t] < |S_t|$ so Azuma-Hoeffding won't give meaningful bounds. Talagrand's inequality (Theorem~\ref{thm:talagrand}), however, can give us the bounds we need: changing $X_t(v)$, will change $X_t$ by at most one so the bounded differences condition is satisfied (necessary for both Talagrand and Azuma). For the second condition, observe that $X_t=r$ implies that there are $r$ agnostic vertices which pulled their opinion from gnostic vertices. We can take those agnostic vertices as the set $J$ and the condition will be satisfied with $\psi(r) = \lceil{r}\rceil$. Thus, by Talagrand, if $m$ is a median of $X_t$, we have that $\mathbb{P}(|X_t-m|\geq x)\leq 4\exp\left(\frac{x^2}{\lceil{m+x}\rceil}\right)$. As $\mathbb{E}[X_t]\geq m\mathbb{P}(X_t>m)\geq m/2$, we can rewrite the last inequality in term of $\mathbb{E}[X_t]$:

    $$\mathbb{P}(|X_t-m|\geq x)\leq 4\exp\left(\frac{x^2}{\lceil{2\mathbb{E}[X_t)+x}\rceil}\right]$$

    Moreover, note that by triangle inequality, 
    $$|X_t-\mathbb{E}[X_t]| \leq |X_t-m|+|m-\mathbb{E}[X_t]| = |X_t-m|+O(\sqrt{\mathbb{E}[X_t]}.$$ Since we are assuming that $\log^{1/2}(n)\leq|S_t|\leq \varepsilon n$, we have that $|S_t|\geq \mathbb{E}[X_t]\geq (1-2\varepsilon)|S_t|$ for large enough $n$. Thus, we have that $\mathbb{P}(|X_t-\mathbb{E}[X_t]| \geq \varepsilon |S_t|) \leq \mathbb{P}(|X_t-m|\geq \varepsilon|S_t|/2)$ and by Talagrand $\mathbb{P}(|X_t-\mathbb{E}[X_t]| \geq \varepsilon |S_t|) \leq 4\exp\left(-\frac{\varepsilon^2|S_t|^2}{(2+\varepsilon)|S_t|}\right) \leq 4\exp\left(-\frac{\varepsilon|S_t|}{3}\right)$. 
    
    We have shown that $\mathbb{P}(X_t < (1-3\varepsilon)|S_t|)\leq 4\exp\left(-\frac{\varepsilon|S_t|}{3}\right)$. Now, for computing the time $T_1-T_1'$, we can observe that with probability at most $\sum_{t=T_1'}^{T_1}4\exp\left(-\frac{\varepsilon|S_t|}{3}\right)$, we will have at least one of $X_t < (1-3\varepsilon)|S_t|$ for some $T_1'\leq t\leq T_1$. As $|S_{t+1}|\geq 3/2 |S_t|$ if $X_t\geq (1-3\varepsilon)|S_t|$, the probability that $T_1-T_1'\geq \log_{3/2}(\varepsilon n)$ is at most $\sum_{|S_t|=\log^{1/2}(n)}^{|S_t|=\varepsilon n}4\exp\left(-\frac{\varepsilon|S_t|}{3}\right)$ which is upper bounded by the integral of $4\exp\left(-\frac{\varepsilon|S_t|}{3}{}\right)$ with the same intervals as the sum, which is at most $\frac{12}{\varepsilon}e^{-\varepsilon \log^{1/2}(n)/3} = o(1)$. We conclude the first stage having shown that $T_1\leq \log(n)$ with high probability.

    Now, we move on to the last stage. The idea is again to show that the number of new vertices per round is concentrated around its expectation and said expectation leads to exponential growth. The main advantage is that this time we can just use Azuma-Hoeffding's inequality instead of Talagrand. As before, we will again have $\mathbb{E}[X_t] = (1\pm \sqrt{\frac{8}{\alpha(n)}})\frac{(n-|S_t|)|S_t|}{n}$. This time, however, we can just use Azuma-Hoeffding's inequality as $\mathbb{E}[X_t] = O(n-|S_t|)$. The bounded differences condition again holds with $c_k=1$. As a result, we can apply Azuma's inequality with $c_k=1$ to conclude that $X_t$ is sharply concentrated around $\mathbb{E}[X_t]$. That is, we have 
    $$\mathbb{P}(|X_t-\mathbb{E}[X_t]| \geq \varepsilon \mathbb{E}[X_t])\leq 2\exp\left(-\frac{\varepsilon^2\mathbb{E}[X_t]^2}{2(n-|S_t|)}\right).$$ 
    Replacing the value of $\mathbb{E}[X_t]$, we have 
    $$\mathbb{P}(|X_t-\mathbb{E}[X_t]| \geq \varepsilon \mathbb{E}[X_t])\leq 2\exp\left(-\frac{\varepsilon^2(n-|S_t|)^2|S_t|^2}{4n^2(n-|S_t|)}\right),$$
    where we use that $1\pm \sqrt{\frac{8}{\alpha(n)}}\geq 1/2$. We then get 
    $$\mathbb{P}(|X_t-\mathbb{E}[X_t]| \geq \varepsilon \mathbb{E}[X_t])\leq 2\exp\left(-\frac{\varepsilon^4(n-|S_t|)}{4}\right)$$ 
    where we use that $|S_t|\geq \varepsilon n$.
    Thus,
    $$\mathbb{P}(X_t < (1-\varepsilon)\mathbb{E}[X_t]) \leq 2\exp\left(-\frac{\varepsilon^4(n-|S_t|)}{4}\right).$$
    Observe that $(1-\varepsilon)\mathbb{E}[X_t] \geq (1-2\varepsilon)|S_t|(1-|S_t|/n)$ where we use that $1-2\varepsilon \geq (1-\varepsilon)(1\pm \sqrt{\frac{8}{\alpha(n)}})$ for sufficiently large $n$. We have $|S_{t+1}| = |S_t|+X_t$. If $X_t>(1-\varepsilon)\mathbb{E}[X_t]$, we have 
    $$|S_{t+1}|\geq |S_t|+(1-2\varepsilon)|S_t|(1-|S_t|/n) \geq |S_t|(1+(1-2\varepsilon)(1-|S_t|/n)).$$
    We can then write $n-|S_{t+1}|\leq (n-|S_t|)-(1-2\varepsilon)|S_t|(1-|S_t|/n)$, which can be rewritten as $n-|S_{t+1}|\leq (n-|S_t|)(1-(1-2\varepsilon)|S_t|/n)$ and the term multiplying $(n-|S_t|)$ is seen to be at most $(1-\varepsilon(1-2\varepsilon))\leq 1/(1+\varepsilon/2)$. We can then let $T_2'$ be the time it takes to activate $n-\log(n)$ vertices after we have already activated $\varepsilon n$ vertices. From the previous discussion, we either have $T_2' < \log_{1+\varepsilon/2}(n)$ which is $O(\log(n))$ or there is at least one $t$ for which $X_t<(1-\varepsilon)\mathbb{E}[X_t]$, which happens with probability at most $\sum_{|S_t|=\varepsilon n}^{|S_t|=n-\log(n)}2\exp\left(-\frac{\varepsilon^4(n-|S_t|)}{4}\right)$. This is upper bounded by the integral of $2\exp\left(-\frac{\varepsilon^4(n-|S_t|)}{4}\right)$ with the same intervals as the sum, which is at most $\frac{8}{\varepsilon^4}e^{-\varepsilon^4 \log(n)} = o(1)$. Thus, $T_2' = O(\log(n))$ with high probability.
    Now, we have to bound the time it takes to activate the remaining $\log(n)$ vertices. Note that the still agnostic vertices have degree $(1+o(1))np >> \log(n)$, due to the assumption on $p$. This means that every round, each of the still agnostic vertices has probability close to $1$ (easily larger than $1/2$) of turning gnostic (as it has at most $\log(n)$ edges to agnostic vertices). Thus, after $C\log(n)$ rounds, for some $C>0$ the probability that one vertex is still agnostic is at most $\log(n)2^{-C\log(n)}<n^{-C+1} = o(1)$.

    Combining all of the above steps we have that $T_a$ is $O(\log(n))$ with high probability as both stages happen in $O(\log(n))$ with high probability.
    
\end{proof}

\begin{lemma}[Lemma $1$ from~\cite{panagiotou2017asynchronous}]
    Let $p=\alpha(n)\log(n)/n$, where $\alpha(n) = \omega(1)$. Then, with high probability, $G_{n,p}$ is such that for all $S\subseteq [n]$:
    $$e(S,[n]\setminus S) = \left(1\pm \sqrt{\frac{8}{\alpha(n)}}\right)(n-|S|)|S|p$$
    \label{lemma:kosta}
\end{lemma}

\begin{theorem}[Azuma-Hoeffding's inequality]
\label{thm:azuma}
Let $Z_1,\ldots, Z_n$ be independent random variables taking values in the sets $\Lambda_1,\ldots, \Lambda_n$. Let $\Lambda=\Lambda_1\times\ldots\times\Lambda_n$. Let $f:\Lambda\to \mathbb{R}$ be a function and set $X = f(Z_1,\ldots, Z_n)$. Assume there are quantities $c_1,\ldots, c_n$ satisfying the following condition: if $z,z'\in \Lambda$ differ only in the $k$-th coordinate, then $|f(z)-f(z')| \leq c_k$. Then, for every $x\geq 0$ we have that 
$$\mathbb{P}(|X-\mathbb{E}[X]|\geq x) \leq 2\exp\left(-\frac{x^2}{2\sum_{k=1}^n c_k^2}\right).$$
\end{theorem}

\begin{theorem}[Talagrand's Inequality]
\label{thm:talagrand}
Suppose we are in the conditions of Azuma-Hoeffding's inequality. Additionally, assume there is an increasing function $\psi$ satisfying the following: if $z\in\Lambda$ and $r\in \mathbb{R}$ is such that $f(z)\geq r$, then there exists a set $J\subseteq\{1,\ldots, n\}$ with $\sum_{j\in J} c_j^2\leq \psi(r)$, such that for all $y\in \Lambda$ with $y_i=z_i$ for $i\in J$, we have $f(y)\geq r$. 
Then, if $m$ is a median of $X$, we have that, for every $x\geq 0$:
$$\mathbb{P}(|X-m|\geq x) \leq 4 \exp\left(\frac{x^2}{\psi(m+x)}\right).$$
Moreover, when $\psi(r)\leq \lceil{r}\rceil$, we have that $|m-\mathbb{E}[X]|\leq O(\sqrt{\mathbb{E}[X]})$.
\end{theorem}




\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

