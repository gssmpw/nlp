\subsubsection{Main Idea Examples}
\label{subsubsec:example_main_idea}

\autoref{subsubsec:example_main_idea} presents examples of the top main ideas extracted from the paper "Attention is All You Need"~\cite{attention_is_all_you_need} in \name (step \circled{3}{customgreen} in \autoref{fig:savaal-workflow}). These main ideas capture some of the key concepts of the paper.

\begin{figure}[t]
\centering
\begin{tcolorbox}[width=1\linewidth, colframe=white, colback=gray!10, sharp corners=southwest, title=]

\begin{tcolorbox}[colframe=black!50, colback=gray!10, title=Transformer model]
The Transformer model is a groundbreaking sequence transduction model that relies entirely on attention mechanisms, eliminating the need for recurrence, and is composed of an encoder-decoder architecture with self-attention and point-wise, fully connected layers, allowing for greater parallelization and efficiency in training.
\end{tcolorbox}


\begin{tcolorbox}[colframe=black!50, colback=gray!10, title=Self-attention mechanism]
Self-attention is a mechanism that relates different positions of a single sequence to compute a representation, and is used in tasks such as reading comprehension, abstractive summarization, and learning task-independent sentence representations.
\end{tcolorbox}

\begin{tcolorbox}
[colframe=black!50, colback=gray!10, title=Positional encoding]
Positional encoding provides information about the order of tokens in a sequence, using fixed sinusoidal functions or learned embeddings, which is crucial for models like the Transformer that lack inherent sequence order awareness.
\end{tcolorbox}

\end{tcolorbox}
\centering
\caption{Main idea examples generated for ``Attention is All You Need''~\cite{attention_is_all_you_need}.}
\label{fig:main_idea_example}
\end{figure}

\subsubsection{Baseline Quiz Example}
\label{subsubsec:example_baseline}
\autoref{fig:baseline_repeat_example} enumerates the questions outputted when prompting an LLM (in this case \gpt) for 20 questions at once. Occasionally, duplicate questions will be output in the same turn. Each pair of duplicated question statements is highlighted in a different color.

\begin{figure*}
\centering
\begin{tcolorbox}[width=\textwidth,fontupper=\footnotesize,title=Repeated Baseline Questions]

\begin{enumerate}
    \item What is the primary advantage of the Transformer model over traditional recurrent neural networks in sequence modeling tasks?
    \begin{itemize}
        \item A. It uses fewer parameters, reducing the model size.
        \item B. It eliminates the need for positional encodings.
        \item C. It allows for more parallelization, improving training efficiency.
        \item D. It requires less data for training, reducing overfitting.
    \end{itemize}
    \textbf{Correct Answer: C. It allows for more parallelization, improving training efficiency.}

    \item How does the Transformer model handle the sequential nature of input data without using recurrence?
    \begin{itemize}
        \item A. By using convolutional layers to capture sequence information.
        \item B. By employing self-attention mechanisms to model dependencies.
        \item C. By using a single attention head to focus on sequence order.
        \item D. By relying on recurrent layers for sequence alignment.
    \end{itemize}
    \textbf{Correct Answer: B. By employing self-attention mechanisms to model dependencies.}

    \item In the context of the Transformer model, what is the purpose of multi-head attention?
    \begin{itemize}
        \item A. To reduce the computational cost of attention mechanisms.
        \item B. To allow the model to attend to different parts of the sequence simultaneously.
        \item C. To increase the dimensionality of the input data.
        \item D. To simplify the model architecture by reducing the number of layers.
    \end{itemize}
    \textbf{Correct Answer: B. To allow the model to attend to different parts of the sequence simultaneously.}

    \item 
        \colorbox{green}{Why is the dot product in the Scaled Dot-Product Attention scaled by \( \frac{1}{\sqrt{d_k}} \)?}
    \begin{itemize}
        \item A. To increase the magnitude of the dot products for better attention.
        \item B. To prevent the softmax function from having extremely small gradients.
        \item C. To ensure the keys and values have the same dimensionality.
        \item D. To make the computation more efficient by reducing the number of operations.
    \end{itemize}
    \textbf{Correct Answer: B. To prevent the softmax function from having extremely small gradients.}

    \item \colorbox{pink}{What role do positional encodings play in the Transformer model?}
    \begin{itemize}
        \item A. They provide a mechanism for the model to learn the order of the sequence.
        \item B. They reduce the dimensionality of the input data.
        \item C. They replace the need for attention mechanisms.
        \item D. They enhance the model's ability to generalize to unseen data.
    \end{itemize}
    \textbf{Correct Answer: A. They provide a mechanism for the model to learn the order of the sequence.}

    \item \colorbox{cyan}{\parbox{0.93\textwidth}{How does the Transformer model ensure that the decoder only attends to previous positions during training?}}
    \begin{itemize}
        \item A. By using a separate attention mechanism for each position.
        \item B. By applying a mask to prevent attention to future positions.
        \item C. By using a recurrent layer to control the flow of information.
        \item D. By training the model on reversed sequences.
    \end{itemize}
    \textbf{Correct Answer: B. By applying a mask to prevent attention to future positions.}

    \item What is the main reason for using residual connections in the Transformer model?
    \begin{itemize}
        \item A. To increase the depth of the model without increasing complexity.
        \item B. To prevent overfitting by adding noise to the model.
        \item C. To facilitate the training of deep networks by mitigating vanishing gradients.
        \item D. To reduce the number of parameters in the model.
    \end{itemize}
    \textbf{Correct Answer: C. To facilitate the training of deep networks by mitigating vanishing gradients.}
\end{enumerate}
\end{tcolorbox}
\end{figure*}

\begin{figure*}\ContinuedFloat
\centering
\begin{tcolorbox}[width=\textwidth,fontupper=\footnotesize,title=Repeated Baseline Questions - Continued]
\begin{enumerate}
    \setcounter{enumi}{7}
    \item Why might self-attention layers be more interpretable than recurrent layers?
    \begin{itemize}
        \item A. They use fewer parameters, making the model simpler to understand.
        \item B. They provide explicit attention distributions that can be analyzed.
        \item C. They rely on fixed positional encodings, reducing variability.
        \item D. They are less sensitive to the order of input data.
    \end{itemize}
    \textbf{Correct Answer: B. They provide explicit attention distributions that can be analyzed.}

    \item In the Transformer model, what is the purpose of using layer normalization?
    \begin{itemize}
        \item A. To increase the model's capacity by adding more layers.
        \item B. To stabilize the training process by normalizing the inputs of each layer.
        \item C. To reduce the model's dependency on positional encodings.
        \item D. To enhance the model's ability to learn long-range dependencies.
    \end{itemize}
    \textbf{Correct Answer: B. To stabilize the training process by normalizing the inputs of each layer.}

    \item \colorbox{yellow}{\parbox{0.93\textwidth}{How does the Transformer model achieve a constant number of operations for relating signals from two arbitrary positions?}}
    \begin{itemize}
        \item A. By using a single attention head for all positions.
        \item B. By employing self-attention mechanisms that connect all positions directly.
        \item C. By using convolutional layers with a large kernel size.
        \item D. By reducing the dimensionality of the input data.
    \end{itemize}
    \textbf{Correct Answer: B. By employing self-attention mechanisms that connect all positions directly.}

    \item What is the primary benefit of using the Adam optimizer in training the Transformer model?
    \begin{itemize}
        \item A. It reduces the need for dropout regularization.
        \item B. It adapts the learning rate based on the training step, improving convergence.
        \item C. It eliminates the need for positional encodings.
        \item D. It simplifies the model architecture by reducing the number of layers.
    \end{itemize}
    \textbf{Correct Answer: B. It adapts the learning rate based on the training step, improving convergence.}

    \item Why is label smoothing used during the training of the Transformer model?
    \begin{itemize}
        \item A. To increase the model's confidence in its predictions.
        \item B. To improve the model's ability to generalize to unseen data.
        \item C. To reduce the model's dependency on positional encodings.
        \item D. To enhance the model's ability to learn long-range dependencies.
    \end{itemize}
    \textbf{Correct Answer: B. To improve the model's ability to generalize to unseen data.}

    \item How does the Transformer model handle the challenge of learning long-range dependencies?
    \begin{itemize}
        \item A. By using a large number of recurrent layers.
        \item B. By employing self-attention mechanisms that connect all positions directly.
        \item C. By using convolutional layers with a large kernel size.
        \item D. By reducing the dimensionality of the input data.
    \end{itemize}
    \textbf{Correct Answer: B. By employing self-attention mechanisms that connect all positions directly.}

    \item What is the main reason for using multiple attention heads in the Transformer model?
    \begin{itemize}
        \item A. To increase the model's capacity by adding more layers.
        \item B. To allow the model to attend to different parts of the sequence simultaneously.
        \item C. To reduce the model's dependency on positional encodings.
        \item D. To enhance the model's ability to learn long-range dependencies.
    \end{itemize}
    \textbf{Correct Answer: B. To allow the model to attend to different parts of the sequence simultaneously.}
\end{enumerate}

\end{tcolorbox}
\end{figure*}

\begin{figure*}\ContinuedFloat
\centering
\begin{tcolorbox}[width=\textwidth,fontupper=\footnotesize,title=Repeated Baseline Questions - Continued]
\begin{enumerate}
    \setcounter{enumi}{14}
    \item Why is the Transformer model considered more efficient than recurrent models for sequence transduction tasks?
    \begin{itemize}
        \item A. It uses fewer parameters, reducing the model size.
        \item B. It eliminates the need for positional encodings.
        \item C. It allows for more parallelization, improving training efficiency.
        \item D. It requires less data for training, reducing overfitting.
    \end{itemize}
    \textbf{Correct Answer: C. It allows for more parallelization, improving training efficiency.}
    
    \item \colorbox{cyan}{\parbox{0.93\textwidth}{How does the Transformer model ensure that the decoder only attends to previous positions during training?}}
    \begin{itemize}
        \item A. By using a separate attention mechanism for each position.
        \item B. By applying a mask to prevent attention to future positions.
        \item C. By using a recurrent layer to control the flow of information.
        \item D. By training the model on reversed sequences.
    \end{itemize}
    \textbf{Correct Answer: B. By applying a mask to prevent attention to future positions.}

    \item What is the primary advantage of using self-attention over convolutional layers in the Transformer model?
    \begin{itemize}
        \item A. Self-attention requires fewer parameters, reducing model size.
        \item B. Self-attention allows for more parallelization, improving training efficiency.
        \item C. Self-attention captures long-range dependencies more effectively.
        \item D. Self-attention eliminates the need for positional encodings.
    \end{itemize}
    \textbf{Correct Answer: C. Self-attention captures long-range dependencies more effectively.}

    \item \colorbox{green}{Why is the dot product in the Scaled Dot-Product Attention scaled by \( \frac{1}{\sqrt{d_k}} \)?}
    \begin{itemize}
        \item A. To increase the magnitude of the dot products for better attention.
        \item B. To prevent the softmax function from having extremely small gradients.
        \item C. To ensure the keys and values have the same dimensionality.
        \item D. To make the computation more efficient by reducing the number of operations.
    \end{itemize}
    \textbf{Correct Answer: B. To prevent the softmax function from having extremely small gradients.}

    \item \colorbox{pink}{What role do positional encodings play in the Transformer model?}
    \begin{itemize}
        \item A. They provide a mechanism for the model to learn the order of the sequence.
        \item B. They reduce the dimensionality of the input data.
        \item C. They replace the need for attention mechanisms.
        \item D. They enhance the model's ability to generalize to unseen data.
    \end{itemize}
    \textbf{Correct Answer: A. They provide a mechanism for the model to learn the order of the sequence.}

    \item \colorbox{yellow}{\parbox{0.93\textwidth}{How does the Transformer model achieve a constant number of operations for relating signals from two arbitrary positions?}}
    \begin{itemize}
        \item A. By using a single attention head for all positions.
        \item B. By employing self-attention mechanisms that connect all positions directly.
        \item C. By using convolutional layers with a large kernel size.
        \item D. By reducing the dimensionality of the input data.
    \end{itemize}
    \textbf{Correct Answer: B. By employing self-attention mechanisms that connect all positions directly.}
\end{enumerate}
\end{tcolorbox}
\caption{An example of repeated questions using the baseline method. Duplicated questions are highlighted in the same color.}
\label{fig:baseline_repeat_example}
\end{figure*}