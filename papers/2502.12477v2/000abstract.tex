Assessing and enhancing human learning through question-answering is vital, yet automating this process remains challenging. While large language models (LLMs) excel at summarization and query responses, their ability to generate meaningful questions for learners is underexplored. 


We propose \name,\footnote{\name means ``question'' in Hindi, and has a similar root in Persian and Arabic.} a scalable question-generation system with three objectives: (i) {\em scalability}, enabling question-generation from hundreds of pages of text (ii) {\em depth of understanding}, producing questions beyond factual recall to test conceptual reasoning, and (iii) {\em domain-independence}, automatically generating questions across diverse knowledge areas. Instead of providing an LLM with large documents as context, \name improves results with a three-stage processing pipeline. 
Our evaluation with \totalevaluators human experts on \totaldocuments papers and PhD dissertations shows that \name generates questions that better test depth of understanding by 6.5$\times$ for dissertations and 1.5$\times$ for papers compared to a direct-prompting LLM baseline. Notably, as document length increases, \name's advantages in higher question quality and lower cost become more pronounced.
