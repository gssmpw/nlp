\section{Related Work}
\label{sec:related-work}

\textbf{Automated question-generation} has evolved from early Seq2Seq models \cite{du-etal-2017-learning, Neural_QG} to transformer-based approaches \cite{attention_is_all_you_need}. Models like BERT \cite{devlin-etal-2019-bert}, T5 \cite{t5}, BART \cite{lewis-etal-2020-bart}, and GPT-3 \cite{gpt_3} have significantly improved question generation \cite{chan-fan-2019-bert, li-etal-2021-addressing-semantic}. However, reliance on labeled datasets such as SQuAD \cite{rajpurkar-etal-2016-squad} and HotpotQA \cite{yang-etal-2018-hotpotqa} limits generalizability to other domains.

Researchers have explored LLMs for question generation~\cite{knowledge_base_prompting, reading_comprehension_language_llm, code_QG, mcq_computing, MIT_law}. However, these efforts have focused on generating questions from short, domain-specific context. Our work mitigates this limitation and generates high-quality questions from long documents.


Prior methods for \textbf{automated evaluation using LLMs} use metrics like ROUGE \cite{lin-2004-rouge} and BLEU \cite{papineni-etal-2002-bleu}, but these often misalign with humans~\cite{guo2024survey-neural-question-gen}. Some papers fine-tune small models for specific metrics \cite{zhu2023judgelm, wang2024pandalm}, but they face scalability issues, annotation reliance, or poor generalizability \cite{zhu2023judgelm}. Recent work uses  LLMs like GPT-4o as evaluators \cite{zheng2023judging, lin2023llmevalunifiedmultidimensionalautomatic}. While they achieve good human alignment, they focus on multi-turn conversations, a different context from ours.

For multiple-choice question generation, small models like BART and T5 assess relevance and usability \cite{moon-etal-2024-generative, raina2022multiplechoicequestiongenerationautomated} but require ground-truth data, limiting scalability. Others use LLM judges to rate relevance, coverage, and fluency on a 1-5 Likert scale \cite{microsoft_agriculture}. 
We adopt a similar approach with GPT-4o on a 1-4 scale. 
%for better human alignment. 
LLM judges can introduce positional \cite{zheng2024large, wang-etal-2024-large-language-models-fair}, egocentric \cite{koo-etal-2024-benchmarking}, and misinformation biases \cite{chen-etal-2024-humans, koo-etal-2024-benchmarking}.
%, which require mitigation.


\textbf{Retrieval-Augmented Generation (RAG)} enhances language model accuracy by retrieving relevant information to ground responses and reduce hallucinations~\cite{lewis2020retrieval, shuster2021retrieval, colbertv2, gottumukkala2022investigating}. Advances like dense passage retrieval~\cite{karpukhin2020dense} and late interaction models~\cite{colbert} improve efficiency. \name's pipeline uses recent advances in information retrieval models to fetch the most relevant context for question generation.