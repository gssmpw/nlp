\section{Related Work}
\label{sec:related-work}

\textbf{Automated question-generation} has evolved from early Seq2Seq models ____ to transformer-based approaches ____. Models like BERT ____, T5 ____, BART ____, and GPT-3 ____ have significantly improved question generation ____. However, reliance on labeled datasets such as SQuAD ____ and HotpotQA ____ limits generalizability to other domains.

Researchers have explored LLMs for question generation____. However, these efforts have focused on generating questions from short, domain-specific context. Our work mitigates this limitation and generates high-quality questions from long documents.


Prior methods for \textbf{automated evaluation using LLMs} use metrics like ROUGE ____ and BLEU ____, but these often misalign with humans____. Some papers fine-tune small models for specific metrics ____, but they face scalability issues, annotation reliance, or poor generalizability ____. Recent work uses  LLMs like GPT-4o as evaluators ____. While they achieve good human alignment, they focus on multi-turn conversations, a different context from ours.

For multiple-choice question generation, small models like BART and T5 assess relevance and usability ____ but require ground-truth data, limiting scalability. Others use LLM judges to rate relevance, coverage, and fluency on a 1-5 Likert scale ____. 
We adopt a similar approach with GPT-4o on a 1-4 scale. 
%for better human alignment. 
LLM judges can introduce positional ____, egocentric ____, and misinformation biases ____.
%, which require mitigation.


\textbf{Retrieval-Augmented Generation (RAG)} enhances language model accuracy by retrieving relevant information to ground responses and reduce hallucinations____. Advances like dense passage retrieval____ and late interaction models____ improve efficiency. \name's pipeline uses recent advances in information retrieval models to fetch the most relevant context for question generation.