\section{Evaluation}
\label{sec:eval}

We evaluated \name on \totaldocuments documents using both human experts and an AI judge. We used \texttt{GPT-4o} via the OpenAI API as our primary LLM. We also evaluated \texttt{Meta-Llama-3.3-70B-Instruct} (\autoref{subsec:ablation-model}). All models are set to temperature 0.0 for all experiments, with default settings for all other parameters. \name is model-agnostic and is compatible with current LLMs. We implemented our pipeline using LangChain~\cite{langchain} and traced our experiments in Weights \& Biases~\cite{wandb}.
%any commercial or open-source LLM.

\subsection{Datasets}
\label{sec:evaluation-data}
%We gathered three datasets for evaluation:
\begin{itemize}[topsep=0pt, itemsep=0pt, leftmargin=*]
    \item \textbf{PhD dissertations}: \numphd long documents in Aerospace, Machine Learning, Networks, Systems, and Databases (\autoref{tab:human-eval-dataset-stats}).
    \item \textbf{Conference papers}: \numpapers papers from conferences in CS and Aeronautics in 2023 and 2024.
    \item \textbf{Diverse \arxiv papers}: \numloogle papers from CS, Physics, Mathematics, Economics, and Biology (\autoref{tab:benchmark-stats}). 
\end{itemize}

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1} % Increase row height for better readability
\setlength{\tabcolsep}{1pt} % Adjust column spacing for better fit
% \begin{small}
\begin{tabular}{|l|c|c|}
\hline
\small  \textbf{Statistic} & \small  \textbf{Conference Papers} & \small \textbf{Dissertations} \\
\hline
\small \textbf{No. Documents} & \small \numpapers & \small \numphd \\ 
\hline
\small \textbf{Avg. Words} & \small 10,354 & \small 26,511 \\ 
\hline
\small \textbf{Avg. Pages} & \small \avgpaperpages & \small \avgphdpages \\
\hline
\end{tabular}
\caption{Statistics for the number of words in the conference papers and PhD dissertations.}
\label{tab:human-eval-dataset-stats}
% \end{small}
\end{table}



\subsection{Methods Compared}
\label{sec:evaluation-baselines}
We compare \name to \Baseline, a direct-prompting baseline (\autoref{sec:insights-whole-context}) that provides the entire document to the LLM with a detailed prompt to generate $N$ multiple-choice questions (\autoref{fig:baseline_question_generation_prompt}). We found that when $N$ exceeds $\approx$ 20, \Baseline fails to produce $N$ distinct questions. Since broad concept coverage requires generating a large pool of questions and sampling for shorter quizzes, we generate $N > 20$ questions in batches of $b=20$ using an additional prompt (\autoref{fig:baseline_large_question_generation_prompt}). We use this {\em multi-turn method} for \Baseline on longer documents. 


We evaluate other methods using the AI judge: Summary (\autoref{sec:insights-summary}) and Single-Prompt Savaal, which condenses Savaal's idea extraction, retrieval, and question generation into a single prompt (\autoref{subsec:ablation-methods}).


\subsection{Evaluation Criteria}
\label{sec:evaluation-metrics}

Evaluating the quality of questions is challenging because it involves subjective human judgment~\cite{fu2024qgeval}. We primarily rely on human evaluations but also use \texttt{GPT-4o} as an AI judge~\cite{naismith2023automated} to expand the scope of our evaluation to more approaches, documents, and criteria. 

% Exempt ID: E-6417

\paragraph{Human experts:} We generated 10 multiple-choice questions from Savaal and 10 from \Baseline for each of the \numphd PhD dissertations and \numpapers conference papers. We contacted the primary authors to evaluate the quality of questions via a secure web-based feedback form.\footnote{\emph{MIT} Institutional Review Board exempted this study (Exemption Number: E-6417). All the personnel were certified, and participants were over 18 years of age and provided informed consent before participating.} We asked each expert to rate their questions on clarity, depth of understanding\footnote{Used interchangeably with understanding.}, and quality of choices using a four-point scale: \emph{Disagree}, \emph{Somewhat Disagree}, \emph{Somewhat Agree}, and \emph{Agree}. They also assessed usability by answering: ``Would I use this question on a graduate-level quiz?'' with options: {\em Yes}, {\em Yes with small changes}, and {\em No}. The questions were randomly mixed and the evaluators were blind to their source. In all, \totalevaluators experts participated (\autoref{subsec:appendix_human_eval_conduct}).
 

\label{sec:metrics-auto}
\paragraph{AI judge:} We prompted \texttt{GPT-4o} at temperature 0.0 to score each question on a 1â€“4 scale (\autoref{subsubsec:eval-prompts}) on Depth of Understanding, Quality of Choices, Clarity, Usability, Difficulty, Cognitive Level, and Engagement (\autoref{subsec:ablation-metrics}). Our evaluation prompts provide detailed guidelines than those given to humans, including explicit criteria for each rating (\autoref{subsubsec:eval-prompts}).



\subsection{Results with Human Experts}
\label{sec:evaluation-results}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.9\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{FIG/thesis_only_disagree_all_metrics_bar_charts.pdf}
        \caption{PhD dissertations: \numphdquestions questions, \numphdevaluators experts.}
        \label{fig:human-eval-disagree-phd}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.9\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{FIG/no_dedup_no_refine_final_only_disagree_all_metrics_bar_charts.pdf}
        \caption{Conference papers: \numpaperquestions questions, \numpaperevaluators experts.}
        \label{fig:human-eval-disagree-paper}
    \end{subfigure}
    \vspace{-10 pt}
    \caption{ Summary of human evaluation: The charts show the percentage and standard error of respondents who {\em Disagree} or {\em Somewhat Disagree} with questions on understanding, choice quality, and usability. {\bf Lower values indicate better performance.}}
    \label{fig:human-eval-disagree}
\vspace{-20 pt}
\end{figure}

\begin{figure*}[!t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/thesis_understanding_num_AGREE_scatter.pdf}
        \caption{Depth of understanding: 61.9\% prefer \name, 9.5\% \Baseline.}
        \label{fig:thesis-scatter-understanding}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/thesis_quality_of_choices_num_AGREE_scatter.pdf}
        \caption{Quality of choices: 57.1\% prefer \name, 19\% \Baseline.}
        \label{fig:thesis-scatter-choices}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/thesis_overall_quality_num_AGREE_scatter.pdf}
        \caption{Usability: 47.6\% prefer \name, 23.8\% \Baseline.}
        \label{fig:thesis-scatter-overall}
    \end{subfigure}
    
    \caption{Expert preferences for \numphdevaluators PhD dissertations. Each point shows the number of \emph{Agree}s or \emph{Somewhat Agree}s in a 10-question quiz for each of \name and \Baseline. The majority of experts prefer \name to \Baseline on depth of understanding, quality of choices, and usability on long documents (experts above $y=x$ prefer \name).}
    \label{fig:human-eval-scatter}
    \vspace{-10 pt}
\end{figure*}


\label{sec:evaluation-human}

\autoref{fig:human-eval-disagree} summarizes the results of our expert human evaluation on PhD dissertations and papers. We show here the negative sentiment of the experts, i.e., the percentage of questions that experts responded with \emph{Disagree} or \emph{Somewhat Disagree} for each criterion (see \autoref{fig:human-phd-breakdown} and \autoref{fig:human_paper_breakdown} for the full breakdown). 

For the \numphdquestions questions from \numphd PhD dissertations (\autoref{fig:human-eval-disagree-phd}), the experts responded that \baselinephdunderstanding of \Baseline's questions {\em did not test understanding}; by contrast, only \savaalphdunderstanding of \name's questions did not, a  \phdunderstandingreduction reduction in negative sentiment. They also rated \baselinephdusability of \Baseline's questions as {\em unusable in a quiz}, versus \savaalphdusability for \name, a \phdusabilityreduction reduction.

For  conference papers (\autoref{fig:human-eval-disagree-paper}), on \numpaperquestions questions, \numpaperevaluators experts\footnote{Some papers had multiple expert respondents.} found that \savaalpaperunderstanding of \name's questions {\em did not} test understanding, versus \baselinepaperunderstanding for \Baseline, a \paperunderstandingreduction improvement. They also rated \baselinepaperusability of \Baseline's questions as {\em unusable}, versus \savaalpaperusability for \name.

The experts agreed or somewhat agreed that over 90\% of the questions in both \Baseline and \name had clarity (not shown in the figure). This result is unsurprising because LLMs can be prompted to generate coherent and unambiguous text. 

For PhD dissertations, \autoref{fig:human-eval-scatter} shows how each of the \numphdevaluators experts scored \name vs. \Baseline on the metrics for the PhD dissertations. The $x$ and $y$ axes show number of \emph{Agree} or \emph{Somewhat Agree} for \Baseline and \name, respectively. Each point represents one expert evaluator. 

We observe that \savaalunderstandingpreferrationpercent favor \name over \Baseline for understanding (\autoref{fig:thesis-scatter-understanding}), whereas only \baselineunderstandingpreferrationpercent (\understandingpreferration fewer) prefer \Baseline over \name (\sameunderstandingpreferrationpercent rate the two systems the same). For choice quality, \savaalchoicespreferrationpercent prefer \name compared to \baselinechoicespreferrationpercent for \Baseline (\choicespreferration more, see \autoref{fig:thesis-scatter-choices}), while for usability \savaalusabilitypreferrationpercent prefer \name compared to \baselineusabilitypreferrationpercent for \Baseline (\usabilitypreferration more, see \autoref{fig:thesis-scatter-overall}). 

The data in \autoref{fig:human-eval-scatter} also shows that, on average, expert evaluators rated \emph{Agree} or \emph{Somewhat Agree} for more questions in \name quizzes than \Baseline: \phdunderstandingWAD more for understanding, \phdchoicesWAD more for quality of choices, and \phdusabilityWAD more for usability.

\autoref{fig:human_paper_breakdown} shows the breakdown of expert responses for \numpaperquestions questions from the conference papers. On these shorter documents, experts slightly prefer \name over \Baseline in terms of depth of understanding. They reported that 16.7\% of \TheSystem's questions {\em did not} test understanding, compared to 10.9\% for \Baseline. Experts rated the two methods similarly for choice quality and usability. As in the results for Ph.D. dissertations (\autoref{fig:human-auto-correlation}), the \texttt{GPT-4o} scores (\autoref{fig:AI_paper_breakdown}) correlated poorly with expert evaluations.


\autoref{fig:paper-human-eval-scatter} shows how each of the \numpaperevaluators experts scored \name vs. \Baseline. The $x$-axis shows the number of \emph{Agree} or \emph{Somewhat Agree} for \Baseline, and the $y$-axis shows the same for \name. Each point represents one expert evaluator. Among evaluators with a preference, 1.5$\times$ more experts favor \TheSystem over \Baseline in understanding (34.5\% for \name vs 21.8\% for \Baseline, \autoref{fig:paper-scatter-understanding}). Experts do not exhibit a strong preference between \name and \Baseline for choice quality (\autoref{fig:paper-scatter-choices}) or usability (\autoref{fig:paper-scatter-overall}). The average relative increase in the Agree score for \TheSystem compared to \Baseline is 5.8\% for understanding, 4\% for quality of choices, and 1.5\% for usability.
% , meaning that on average, experts like at least one more question in \name's quizzes compared to \Baseline.


\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/no_dedup_no_refine_final_understanding_num_AGREE_scatter.pdf}
        \caption{Depth of understanding: 34.5\% prefer \name, 21.8\% prefer \Baseline.}
        \label{fig:paper-scatter-understanding}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/no_dedup_no_refine_final_quality_of_choices_num_AGREE_scatter.pdf}
        \caption{Quality of choices: no specific preference exhibited.}
        \label{fig:paper-scatter-choices}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/no_dedup_no_refine_final_overall_quality_num_AGREE_scatter.pdf}
        \caption{Usability: no specific preference exhibited.}
        \label{fig:paper-scatter-overall}
    \end{subfigure}
    
    \caption{Human expert preferences for \numpaperevaluators experts on short conference papers. Each point shows the number of \emph{Agree}s in a 10-question quiz for \name and \Baseline respectively. More experts prefer \name to \Baseline on the depth of understanding. Experts don't exhibit any preference between the quality of choices and usability on short documents (experts above $y=x$ prefer \name).}
    \label{fig:paper-human-eval-scatter}
\end{figure*}


\subsection{Results with an AI Judge}
\label{sec:evaluation-auto}


We used an AI judge to scale evaluations across more documents and criteria. We first examined its alignment with human experts by having \texttt{GPT-4o} evaluate the same \numphdquestions questions from the expert-reviewed dissertations dataset. 

\autoref{fig:human-auto-correlation} compares the AI judge with human experts. The AI judge rarely assigns \emph{Disagree} or \emph{Somewhat Disagree} for understanding and usability and slightly favors \name, giving it 28.6\% Agree rating in comparison to 14.3\% Agree ratings for \Baseline for understanding. However, for quality of choices, it rates both schemes poorly, with only 9.6\% \emph{Agree} or \emph{Somewhat Agree} for \name and 19\% for \Baseline.

We observed similar trends in the \numpaperquestions questions from the conference-paper dataset (\autoref{fig:paper_breakdown}), where the AI judge again slightly preferred \name but remained misaligned with human expert evaluations. For completeness, we also present AI judge results on the Diverse \arxiv dataset in \autoref{subsec:ablation}.

%\paragraph{Limitations of the AI judge.} 
Our takeaway is that our \texttt{GPT-4o} AI judge was unaligned with human expert judgments (see \autoref{fig:auto-correlation-ai} vs. \autoref{fig:human-phd-breakdown}). Despite our extensive efforts in prompt engineering to maximize alignment---including using the prompt optimizer program in DSPy~\citep{khattab2024dspy}---AI-human correlation did not improve. Our experience calls into question the wisdom of using only AI judges in research studies. 




\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{FIG/thesis_no_combine_all_metrics_bar_charts.pdf}
        \caption{Breakdown of human expert scores on PhD dissertations.}
        \label{fig:human-phd-breakdown}
    \end{subfigure}  
    \hfill
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{FIG/FINAL_PLOTS_AUTO/thesis_human_auto_all_metrics_bar_charts.pdf}
        \caption{Breakdown of GPT-4o AI judge scores on PhD dissertations.}
        \label{fig:auto-correlation-ai}
    \end{subfigure}
    \caption{Score distribution for  \protect\numphdquestions questions from dissertations: GPT-4o as a judge does not align with humans for assessing the metrics.}
    \label{fig:human-auto-correlation}
    \vspace{-20 pt}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/no_dedup_no_refine_final_no_combine_all_metrics_bar_charts.pdf}
        \caption{Breakdown of human expert scores on conference papers.}
        \label{fig:human_paper_breakdown}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/FINAL_PLOTS_AUTO/papers_human_auto_all_metrics_bar_charts.pdf}
        \caption{Breakdown of GPT-4o scores on conference papers.}
        \label{fig:AI_paper_breakdown}
    \end{subfigure}
    \caption{Score distribution for  \protect \numpaperquestions questions from conference papers.}
    \label{fig:paper_breakdown}
    \vspace{-10 pt}
\end{figure}





\subsection{Cost Scalability}
\label{sec:scalability-case}

\autoref{fig:cost-scalability} compares the costs of \name and \BaselineMT on the dissertations. While \name incurs a higher one-time cost to generate the concepts, it becomes less expensive when generating more questions. At $N = 60$ questions, \name has the same cost as \BaselineMT; when $N$ grows to 100 questions, \BaselineMT is \directcostinflation more expensive. 

% Details are in \autoref{appendix:costs}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{FIG/phd_cost_comparison.pdf}
    \caption{Average cost comparison of \BaselineMT and \name when generating questions from \numphd PhD dissertations. \name becomes less expensive as $N$ grows. We calculated costs by tracing prompt and completion tokens with OpenAI's February 2025 API pricing.}
    \label{fig:cost-scalability}
    \vspace{-20 pt}
\end{figure}

\input{cost_detail}