\section{Why is Generating Good Questions Hard?}
\label{sec:insights}

Our goal is to enhance human learning from large documents spanning dozens to hundreds of pages by generating multiple-choice questions. Multiple-choice questions are widely used in assessments, are easy to use by learners, and are easy to grade. The task involves generating a set of clear questions, each with four  choices and a correct answer.

High-quality questions assess {\em depth of understanding}, requiring conceptual reasoning and plausible choices (distractors) that challenge the learner. Beyond clarity and precision, our notion of a good question is one that could appear in an advanced quiz on the material as judged by a human expert. While this paper focuses on generating individual high-quality questions, effective quiz sessions should ensure {\em concept coverage} and {\em adapting the difficulty} to prior answers in the session, both avenues for future work.


The main challenge in scalable question generation using LLMs is selecting an appropriate context to use with LLM prompts. We examine four potential strategies: (i) using the full document corpus, (ii) dividing the corpus into sections, (iii) summarizing the corpus, and (iv) using content selection classifiers~\citep{context_Steuer, Context_diverse_hadifar}. Although each strategy has merits, we show that each strategy fails on at least one of our key objectives: {\em scalability}, {\em depth of understanding}, or {\em domain-independence}.

\newcommand{\questionbox}[1]{%
    \colorbox{customblue}{\parbox{0.97\linewidth}{\vspace{1pt}\textbf{#1}\vspace{1pt}}}
}

\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.0} % Adjust row height for better readability
\begin{tabular}{|p{0.2\textwidth}|m{0.5\textwidth}|m{0.2\textwidth}|} 
\hline
\rowcolor{gray!20} % Light gray background for header row
\textbf{Context} & \textbf{Generated Question} & \textbf{Issue} \\ \hline
\footnotesize
\sfboxed{1} \textbf{Entire Document} & 
\footnotesize
\questionbox{What is the primary benefit of using the Adam optimizer in training the Transformer model?}
A. It reduces the need for dropout regularization. \newline
\textbf{B.} It adapts the learning rate based on the training step, improving convergence. \newline
C. It eliminates the need for positional encodings. \newline
D. It simplifies the model architecture by reducing the number of layers. 
&
\footnotesize
\textbf{Too general}: The question is about a basic property of the Adam optimizer rather than the key ideas of the paper.

$\Rightarrow$ Does not test depth of understanding
\\ \hline
\footnotesize
\sfboxed{2} \textbf{Document Section}& 
\footnotesize
\questionbox{In evaluating the performance and efficiency of the Transformer (big) model on the WMT 2014 English-to-French translation task, which of the following factors most significantly contributes to its ability to outperform previous models at a reduced training cost?}
A. The use of a dropout rate of 0.1 instead of 0.3, which enhances model regularization and reduces overfitting.\newline
B. The implementation of beam search with a beam size of 4 and a length penalty $\alpha$ = 0.6, which optimizes the translation output quality.\newline
\textbf{C.} The averaging of the last 20 checkpoints, which stabilizes the model's performance and improves translation accuracy.\newline
D. The reduction in training time to less than 1/4 of the previous state-of-the-art model, which directly correlates with improved BLEU scores.&
\footnotesize
\textbf{Irrelevant detail:} Because the method looks at one section at a time, it fixates on minutiae and irrelevant details (e.g., “averaging the last 20 checkpoints”) that may seem important in isolation, but are not.

$\Rightarrow$ Does not test depth of understanding
\\ \hline
\footnotesize
\sfboxed{3} \textbf{Document Summary} &
\footnotesize
\questionbox{How does the Transformer model address the challenge of learning dependencies between distant positions in sequences compared to models like ConvS2S and ByteNet?}
A. By using convolutional layers to capture long-range dependencies\newline
B. By increasing the number of layers in the encoder and decoder stacks\newline
C. By employing a recurrent neural network to process sequences\newline
\textbf{D.} By reducing the number of operations to a constant using self-attention mechanisms"
& 
\footnotesize
\textbf{Missing context:} The summary mentions ``...The Transformer model addresses this by reducing the number of operations to a constant, using self-attention mechanisms.'' which led the LLM design this incomplete question.


$\Rightarrow$ Leads to inaccurate questions
\\ \hline
\end{tabular}
\caption{Examples from the ``Attention Is All You Need'' paper \citep{attention_is_all_you_need} using three different context selection methods. The questions are drawn from three separate 20-question quizzes, each generated using a different method via OpenAI's API \citep{openai_api} with the \texttt{gpt-4o} model.}
\label{tab:bad-examples-attention}
\vspace{-10 pt}
\end{table*}


\subsection{Using the Entire Document Corpus}
\label{sec:insights-whole-context}

One approach is to provide the entire document as context to an LLM for quiz generation. However, this method has two major drawbacks.
First, as prior research shows~\citep{lost-in-the-middle}, LLMs allocate attention unevenly across long documents, focusing more on the beginning and end while largely neglecting the middle. 

Second, LLMs struggle to capture dependencies between different sections of a long document~\citep{loogle}, leading to superficial questions and missing key concepts. When we prompted OpenAI's \texttt{gpt-4o} model with the full text of the ``Attention Is All You Need'' paper~\citep{attention_is_all_you_need}, many of the 20 generated questions overlooked key ideas. See Example \sfboxed{1} in \autoref{tab:bad-examples-attention} for a question, which is not relevant to the paper's key ideas.

We found that LLMs struggle to follow instructions when the context length is large~\cite{gao2024insights}. For example, we instruct the LLM not to repeat questions. While it avoids repetition when generating a few questions, larger batches (e.g., 20 questions) often contain duplicates. 


\subsection{Using Document Sections}
\label{sec:insights-section-context}

An alternative is to split the document into sections, generate a limited number of questions per section, and later combine them into a quiz. While this method mitigates long-context issues, it introduces {\em context fragmentation}: the LLM cannot connect concepts spanning multiple sections. It often misses deeper connections that can assess stronger conceptual understanding. For example, key insights in a paper’s Algorithm or Methods section may be essential for understanding its Results, but treating these sections independently leads to disjointed, narrow questions.

Another issue is {\em uneven importance weighting}. Not all sections contribute equally to the document’s  ideas, yet a naïve section-based approach may overemphasize minor details while missing key concepts. Example \sfboxed{2} in \autoref{tab:bad-examples-attention} shows how this can generate irrelevant memorization questions.


\subsection{Summarization}
\label{sec:insights-summary}

Providing a {\em document summary} as context offers another way to streamline question generation. While LLMs are effective at summarization, summaries often lack critical details, leading to vague or incomplete questions. More concerning, summaries can introduce hallucinations~\citep{llm_hallucination}, distorting or misrepresenting causal relationships and fabricating details, further degrading question quality.

Example \sfboxed{3} in \autoref{tab:bad-examples-attention} illustrates how summarization can result in misleading or imprecise questions. Here, the summary includes a statement about using self-attention to ``reduce the number of operations to a constant'', but omits that this refers to {\em sequential} operations and maximum path length (Sec. 4 of \citep{attention_is_all_you_need}), leading to an inaccurate question. 



\subsection{Content Selection Classifiers}

Some methods attempt to select relevant content for question generation, often using trained models to identify key passages~\citep{context_Steuer, Context_diverse_hadifar}. However, these approaches typically require domain-specific training data (e.g., pre-existing question-answer pairs), making them {\em domain-dependent}. Such approaches are frequently limited in scope, making them neither reliable nor generalizable to diverse domains. 
