.
\clearpage


\subsection{Additional Methods and Quality Criteria}
\label{subsec:ablation}


We extend the evaluation to compare \name against other methods and metrics using the AI judge on the arXiv dataset. For these experiments, we generate 20 questions per method for each paper. 

\autoref{tab:benchmark-stats} provides further information  about the arXiv dataset. It consists of 48 scientific papers across five topic categories: Computer Science, Physics, Mathematics, Economics, and Quantitative Biology. These papers are divided into two sub-categories: \emph{old} and \emph{new}.

\begin{itemize}[topsep=0pt, itemsep=0pt, leftmargin=*]
    \item \emph{new} Papers: papers published on arXiv after October 2023, which is after the knowledge cutoff date for the LLMs used in this paper. We randomly selected five papers per category from arXiv.
    \item \emph{old} Papers: papers published on arXiv prior to October 2023. We randomly selected five papers per category from the LooGLE dataset~\cite{loogle}, except for Quantitative Biology, where only three papers were available on LooGLE.
\end{itemize}

We split the dataset into ``old'' and ``new'' papers to evaluate whether the performance is different on documents that were not included in the LLM's training data. We did not observe any significant differences for old and new papers, with any of the question generation methods. Thus, we aggregate results for old and new papers for the analysis below.  




\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.2} % Increases row height for better readability
\setlength{\tabcolsep}{8pt} % Adjusts column spacing
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|cc|cc|cc|cc|cc|}
\hline
\multirow{2}{*}{\textbf{Category}} & \multicolumn{2}{c|}{\textbf{Computer Science}} & \multicolumn{2}{c|}{\textbf{Physics}} & \multicolumn{2}{c|}{\textbf{Mathematics}} & \multicolumn{2}{c|}{\textbf{Economics}} & \multicolumn{2}{c|}{\textbf{Quantitative Biology}} \\ 
\cline{2-11} 
 & \multicolumn{1}{c|}{\textbf{Old}} & \textbf{New} & \multicolumn{1}{c|}{\textbf{Old}} & \textbf{New} & \multicolumn{1}{c|}{\textbf{Old}} & \textbf{New} & \multicolumn{1}{c|}{\textbf{Old}} & \textbf{New} & \multicolumn{1}{c|}{\textbf{Old}} & \textbf{New} \\ 
\hline
\textbf{No. Papers} & \multicolumn{1}{c|}{5} & 5 & \multicolumn{1}{c|}{5} & 5 & \multicolumn{1}{c|}{5} & 5 & \multicolumn{1}{c|}{5} & 5 & \multicolumn{1}{c|}{3} & 5 \\ 
\hline
\multirow{2}{*}{\textbf{Avg. Words}} & \multicolumn{1}{c|}{12,498} & 7,307 & \multicolumn{1}{c|}{14,298} & 21,088 & \multicolumn{1}{c|}{12,049} & 16,596 & \multicolumn{1}{c|}{14,010} & 16,112 & \multicolumn{1}{c|}{19,390} & 6,613 \\ 
\cline{2-11} 
 & \multicolumn{2}{c|}{9,903} & \multicolumn{2}{c|}{17,693} & \multicolumn{2}{c|}{14,323} & \multicolumn{2}{c|}{15,061} & \multicolumn{2}{c|}{11,404} \\ 
\hline
\end{tabular}%
}
\caption{Statistics for the number of words for the random papers selected for Diverse \arxiv dataset.}
\label{tab:benchmark-stats}
\end{table*}



\paragraph{Additional Comparison Methods:}
\label{subsec:ablation-methods}
In addition to \Baseline (\autoref{sec:evaluation-baselines}), we consider two other strategies:
\begin{itemize}[topsep=1pt, itemsep=0pt, leftmargin=*]

    
    \item \textbf{Summary}: Uses the summary of the document as the context for question generation (\autoref{sec:insights-summary}). The summary is generated using a map-reduce approach. The prompt used to generate questions from the summary is identical to the \Baseline prompt (\autoref{fig:baseline_question_generation_prompt}). 
    


    \item \textbf{Single-Prompt \TheSystem}: Concatenate all of the prompts used in the stages of \name's pipeline (\autoref{sec:pipeline}) into a single prompt, using the entire document as context. We described each step of \name's pipeline (see \autoref{fig:savaal-workflow}) in detail, and asked the LLM to ``think step by step'' and follow the steps (prompt not shown due to its long length).
    
\end{itemize}

\paragraph{Additional Metrics:}
\label{subsec:ablation-metrics}
In addition to Understanding, Quality of Choices, and Usability, we consider additional criteria for the AI judge to evaluate the questions. These metrics include difficulty, cognitive level, and engagement. The prompts used for these criteria are shown in \autoref{subsubsec:eval-prompts}.

\paragraph{Results:} \autoref{fig:auto_gpt_metrics} summarizes the AI judge scores on all metrics (Understanding, Quality of Choices, Usability, Difficulty, Cognitive Level, Engagement) across all methods (\autoref{subsec:ablation-methods}). The judge rates most of the questions with any method as usable, with the highest amount of usability for \name's questions. It also does not rate any method highly in terms of quality of choices, but gives \name the highest percentage of \emph{Agree}s and the lowest percentage of \emph{Disagree}s among all the methods. On the other criteria, Savaal performs better according to the AI judge.


\begin{figure*}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/FINAL_PLOTS_AUTO/auto_eval_benchmark_gpt-4o_all_metrics_bar_charts.pdf}
        % \caption{Evaluation of GPT-4o on primary metrics.}
        \label{fig:auto_gpt_primary}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/FINAL_PLOTS_AUTO/auto_eval_benchmark_gpt-4o_additional_all_metrics_bar_charts.pdf}
        \label{fig:auto_gpt_additional}
    \end{subfigure}
    \caption{Results of AI evaluation on the quizzes generated with \texttt{GPT-4o} on the \arxiv dataset, evaluated by the AI Judge (\texttt{GPT-4o}).}
    \label{fig:auto_gpt_metrics}
\end{figure*}

\begin{figure}[t]
        \centering
            \includegraphics[width=\linewidth]{FIG/FINAL_PLOTS_AUTO/auto_eval_benchmark_llama_all_metrics_bar_charts.pdf}
        \caption{Results of AI evaluation on the quizzes generated with Llama-3.3-70B on the \arxiv dataset evaluated by the AI Judge (\texttt{GPT-4o}).}
        \label{fig:auto_llama_metrics}
    \end{figure}


\subsection{Evaluating \name with other LLMs}
\label{subsec:ablation-model}
To understand the sensitivity of our results to the underlying LLM, we replace \texttt{GPT-4o} with \texttt{Meta-Llama-3.3-70B-Instruct} and generate questions using the different methods. We used model \texttt{Meta-Llama-3.3-70B-Instruct} hosted at Together.ai~\citep{together_ai_api} for these experiments. We use \texttt{GPT-4o} as the AI judge for these experiments. 


\autoref{fig:auto_llama_metrics} shows the scores that the AI judge gives to the questions generated using the Llama-3.3-70B-Instruct model with \Baseline and \name. The trends are similar for Llama-generated and GPT-4o questions: the AI judge rates \name higher in terms of depth of understanding and usability. It rates both \Baseline and \name poorly on choice quality overall, but prefers \Baseline for Llama-generated questions. 

