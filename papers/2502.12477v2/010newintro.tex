\section{Introduction}
\label{sec:intro}

Many people learn new material effectively by taking quizzes. Answering questions not only assesses knowledge, but also  reinforces learning by strengthening correct responses and revealing gaps in understanding. A major challenge in the 21st century is the rapid expansion of knowledge across fields like science, technology, medicine, law, finance, and more. AI tools are accelerating this growth, making it increasingly difficult for students, researchers, and professionals---from engineers to salespeople---to stay current. The need to learn efficiently and at scale has never been greater.


One response is to rely on AI for answers, effectively outsourcing expertise. While sometimes necessary, this does little to improve human understanding. Instead, we advocate using AI to enhance {\em our} ability to learn and master new material. %, and our work aims to advance this goal.


Programs like ChatGPT, Gemini, Claude, NotebookLM, Perplexity, and DeepSeek built atop large language models (LLMs) have made remarkable strides in summarization and question-answering. However, less attention has been given to {\em question generation}, specifically, creating high-quality questions that test human understanding and mastery of knowledge. That is the focus of this paper.

Anyone who has made an exam knows how difficult and time-consuming it is to make a good set of questions. Our goal is to produce questions automatically with three objectives:
\begin{CompactEnumerate}
\item {\em Scalability}: Generating questions across vast document corpora, such as rapidly evolving research fields or enterprise knowledge bases.
\item {\em Depth of understanding}: Producing questions beyond memorization and the superficial, requiring conceptual reasoning, synthesis, and analysis.
\item {\em Domain-independence}: Creating high-quality questions across diverse fields, including new material absent in an LLMâ€™s pre-training data.
\end{CompactEnumerate}


Prior work on question generation has produced a small number of questions from short passages, but has not demonstrated scalability~\citep{du-etal-2017-learning, Neural_QG, chan-fan-2019-bert, li-etal-2021-addressing-semantic, knowledge_base_prompting, reading_comprehension_language_llm, code_QG, mcq_mult_sentence}. Our results (\autoref{sec:eval}) show that even well-engineered prompts to an LLM produce poor, repetitive questions on large text contexts (tens to hundreds of pages), highlighting the scalability challenge.
 

We present \textbf{\name}, a scalable question generation system for large documents. Savaal uses a three-stage pipeline. The first stage extracts and ranks the key concepts in a corpus of documents\footnote{We use ``document'' to also refer to the corpus of documents used to generate a quiz.} using a map-reduce computation. The second stage retrieves relevant passages corresponding to each concept with an efficient vector embedding retrieval model such as ColBERT~\cite{colbert}. Finally, the third stage prompts an LLM to generate questions for each ranked concept using the retrieved passages as context.

This approach scales well because each LLM computation is confined to a distinct, self-contained task while operating within a manageable context size. By first identifying core concepts and later synthesizing questions from all relevant passages, \name ensures that the generated questions are both targeted and conceptually rich, requiring deeper understanding by linking a given concept across different sections of a document.

We compare \name to a direct-prompting baseline (\Baseline) using \totalevaluators human expert evaluators (the primary authors of \numpapers recent conference papers and \numphd PhD dissertations in subfields of computer science and aeronautics) on \numtotalhumanquestion questions. We also evaluate each paper, as well as 48 arXiv papers, using an LLM as an AI judge.
%and also using an LLM judge on \numloogle papers from the Loogle \cite{loogle} dataset and \arxiv. 
%These evaluations span \numfields distinct fields. 
We find that: 
\begin{CompactEnumerate}
    \item On \numphdquestions questions from \numphdevaluators large documents (dissertations with average \avgphdpages pages), experts reported that \baselinephdunderstanding of \Baseline's questions {\em did not} test understanding, compared to \savaalphdunderstanding of \name, a \phdunderstandingreduction improvement. 
    They reported that \baselinephdchoices of \Baseline's questions lacked good choice quality, compared to \name's \savaalphdchoices, improving by \phdchoicesreduction. They found \baselinephdusability of \Baseline's questions {\em unusable} in a quiz, compared to \savaalphdusability of \name's questions, a \phdusabilityreduction reduction. Moreover, among experts with a preference, \understandingpreferration more favored \name over baseline in understanding, \choicespreferration in choice quality, and \usabilitypreferration in usability.


    \item Even on shorter documents, experts rated \name better in terms of depth of understanding and usability. On \numpaperquestions questions from \numpapers conference papers, \numpaperevaluators experts reported that \baselinepaperunderstanding of baseline's questions {\em did not} test understanding, compared to \savaalpaperunderstanding of \name, a \paperunderstandingreduction improvement. 
    

    
    \item \name is less expensive than \Baseline as the number of questions grows: \Baseline's cost for 100 questions generated from the dissertations is \directcostinflation higher than \name (\$0.47 vs. \$0.77 on average per document).

    
    \item There is a large gap between AI judgments and human evaluations. Despite several attempts to align the AI judge to human responses, scores remained misaligned.% with human expert evaluations.

\end{CompactEnumerate}


