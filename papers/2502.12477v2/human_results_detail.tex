\subsection{Observations from Expert Evaluations}
\label{subsec:human_eval_observations}
We discuss some additional findings from our expert evaluations. 
% \autoref{tab:human-eval-dataset-stats} provides statistics on the length of the documents in the PhD dissertation and conference paper datasets.

% \begin{table}[h]
% \centering
% \renewcommand{\arraystretch}{1} % Increase row height for better readability
% \setlength{\tabcolsep}{1pt} % Adjust column spacing for better fit
% % \begin{small}
% \begin{tabular}{|l|c|c|}
% \hline
% \small  \textbf{Statistic} & \small  \textbf{Conference Papers} & \small \textbf{Dissertations} \\
% \hline
% \small \textbf{No. Documents} & \small \numpapers & \small \numphd \\ 
% \hline
% \small \textbf{Avg. Words} & \small 10,354 & \small 26,511 \\ 
% \hline
% \small \textbf{Avg. Pages} & \small \avgpaperpages & \small \avgphdpages \\
% \hline
% \end{tabular}
% \caption{Statistics for the number of words in the conference papers and PhD dissertations.}
% \label{tab:human-eval-dataset-stats}
% % \end{small}
% \end{table}

% \subsubsection{Ratings for Conference Paper Questions}
% \label{subsubsec:human_eval_observations_breakdown}
% \autoref{fig:human_paper_breakdown} shows the breakdown of expert responses for \numpaperquestions questions from the conference papers. On these shorter documents, experts slightly prefer \name over \Baseline in terms of depth of understanding. They reported that 16.7\% of \TheSystem's questions {\em did not} test understanding, compared to 10.9\% for \Baseline. Experts rated the two methods similarly for choice quality and usability. As in the results for Ph.D. dissertations (\autoref{fig:human-auto-correlation}), the \texttt{GPT-4o} scores (\autoref{fig:AI_paper_breakdown}) correlated poorly with expert evaluations.


% \autoref{fig:paper-human-eval-scatter} shows how each of the \numpaperevaluators experts scored \name vs. \Baseline. The $x$-axis shows the number of \emph{Agree} or \emph{Somewhat Agree} for \Baseline, and the $y$-axis shows the same for \name. Each point represents one expert evaluator. Among evaluators with a preference, 1.5$\times$ more experts favor \TheSystem over \Baseline in understanding (34.5\% for \name vs 21.8\% for \Baseline, \autoref{fig:paper-scatter-understanding}). Experts do not exhibit a strong preference between \name and \Baseline for choice quality (\autoref{fig:paper-scatter-choices}) or usability (\autoref{fig:paper-scatter-overall}). The average relative increase in the Agree score for \TheSystem compared to \Baseline is 5.8\% for understanding, 4\% for quality of choices, and 1.5\% for usability.
% % , meaning that on average, experts like at least one more question in \name's quizzes compared to \Baseline.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}{\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{FIG/no_dedup_no_refine_final_no_combine_all_metrics_bar_charts.pdf}
%         \caption{Breakdown of human expert scores.}
%         \label{fig:human_paper_breakdown}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{FIG/FINAL_PLOTS_AUTO/papers_human_auto_all_metrics_bar_charts.pdf}
%         \caption{Breakdown of GPT-4o scores.}
%         \label{fig:AI_paper_breakdown}
%     \end{subfigure}
%     \caption{Score distribution for  \protect \numpaperquestions questions from conference papers.}
%     \label{fig:paper_breakdown}
% \end{figure}

% \begin{figure*}[h]
%     \centering
%     \begin{subfigure}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{FIG/no_dedup_no_refine_final_understanding_num_AGREE_scatter.pdf}
%         \caption{Depth of understanding: 34.5\% prefer \name, 21.8\% prefer \Baseline.}
%         \label{fig:paper-scatter-understanding}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{FIG/no_dedup_no_refine_final_quality_of_choices_num_AGREE_scatter.pdf}
%         \caption{Quality of choices: no specific preference exhibited.}
%         \label{fig:paper-scatter-choices}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{FIG/no_dedup_no_refine_final_overall_quality_num_AGREE_scatter.pdf}
%         \caption{Usability: no specific preference exhibited.}
%         \label{fig:paper-scatter-overall}
%     \end{subfigure}
    
%     \caption{Human expert preferences for \numpaperevaluators experts on short conference papers. Each point shows the number of \emph{Agree}s in a 10-question quiz for \name and \Baseline respectively. More experts prefer \name to \Baseline on the depth of understanding. Experts don't exhibit any preference between the quality of choices and usability on short documents (experts above $y=x$ prefer \name).}
%     \label{fig:paper-human-eval-scatter}
% \end{figure*}


\subsubsection{Bias When Responding Incorrectly}
\label{subsubsec:human_eval_observations_bias}

Prior to rating a question, evaluators select a response and see the ``correct'' answer (more accurately, the choice that the question generation system thinks is correct).  Experts rate questions that they answer ``correctly'' differently from those that they answer incorrectly. \autoref{fig:correct-circle} shows the distribution of responses across 1411 correctly answered questions (695 \name and 716 \Baseline), while \autoref{fig:incorrect-circle} shows the same for 109 questions answered incorrectly (65 \name and 44 \Baseline). When experts select the wrong answer, they penalize the quality of choices, usability, and clarity. However, their rating for depth of understanding is relatively unaffected. 

\begin{figure}[h]
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/all_correct_circle_metrics.pdf}
        \caption{Ratings for \textbf{correct} responses (1411 questions).}
        \label{fig:correct-circle}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/all_incorrect_circle_metrics.pdf}
        \caption{Ratings for \textbf{incorrect} responses (109 questions).}
        \label{fig:incorrect-circle}
    \end{subfigure}
    \caption{Comparison of expert ratings on different metrics for correct and incorrectly answered questions.}
    \label{fig:savaal-circles}
\end{figure}


% \begin{figure*}[h]
%     \centering
%     \begin{subfigure}{0.44\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{FIG/all_correct_circle_metrics.pdf}
%         \caption{Ratings for \textbf{correct} responses (1411 questions).}
%         \label{fig:correct-circle}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.44\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{FIG/all_incorrect_circle_metrics.pdf}
%         \caption{Ratings for \textbf{incorrect} responses (109 questions).}
%         \label{fig:incorrect-circle}
%     \end{subfigure}
%     \caption{Comparison of expert ratings on different metrics for correct and incorrectly answered questions.}
%     \label{fig:savaal-circles}
% \end{figure*}

\subsubsection{Inter-Human Correlation}

On the conference paper dataset, there were 5 papers with two evaluators each. We examine the correlation of their scores in \autoref{fig:inter-human-correlation}. Each point represents Evaluator 1's average score compared to Evaluator 2's average score across each metric. We plot against the perfect-agreement $y = x$ line. To quantify their differences, we also compute the Mean Absolute Error (MAE) for each method across all pairs of evaluators and the average Spearman coefficient, which is measured on the pairwise ordinal observations on each question per document, averaged across the methods.

We find that the evaluators had poor correlation between themselves when visualizing their aggregate scores for each method (\autoref{fig:inter-human-correlation}). Binarizing their scores, however, increased their correlations, particularly for depth of understanding ($\rho = 0.76$) (\autoref{fig:binary-inter-human-correlation}). We expect that with more samples of evaluations drawn from the same set of questions, we can find stronger correlation trends.

\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/evaluator_correlation_understanding.pdf}
        \caption{Human correlation on depth of understanding. Both \name and \Baseline exhibit weak correlation ($\rho = 0.32$ and $\rho = 0.04$ respectively).}
        \label{fig:correlation-scatter-understanding}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/evaluator_correlation_quality_of_choices.pdf}
        \caption{Human correlation on quality of choices. Both \name and \Baseline exhibit weak correlation ($\rho = 0.22$ and $\rho = 0.13$ respectively).}
        \label{fig:correlation-scatter-choices}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/evaluator_correlation_usability.pdf}
        \caption{Human correlation on usability. Both \name and \Baseline exhibit weak correlation ($\rho = 0.13$ and $\rho = 0.08$ respectively).}
        \label{fig:correlation-scatter-usability}
    \end{subfigure}
    \caption{Correlation between human evaluators on the same document across metrics. Each point is the score of Evaluator 1 vs. Evaluator 2 on a particular document. $y = x$ is where human evaluators perfectly align with each other. We also compute the Mean Average Error (MAE), as well as the average Spearman correlation coefficient $\rho$.}
    \label{fig:inter-human-correlation}
\end{figure*}


\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/binary_evaluator_correlation_understanding.pdf}
        \caption{Human correlation on binarized depth of understanding. \name shows strong correlation ($\rho = 0.76$) while the \Baseline shows weak correlation ($\rho = 0.24$).}
        \label{fig:bin-correlation-scatter-understanding}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/binary_evaluator_correlation_quality_of_choices.pdf}
        \caption{Human correlation on binarized quality of choices. \Baseline showed weak correlation ($\rho = 0.10$) while \name showed negative weak correlation ($\rho = -0.18$).}
        \label{fig:bin-correlation-scatter-choices}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{FIG/binary_evaluator_correlation_usability.pdf}
        \caption{Human correlation on binarized usability. Both \name and \Baseline exhibit weak correlation ($\rho = 0.22$ and $\rho = 0.12$ respectively).}
        \label{fig:bin-correlation-scatter-usability}
    \end{subfigure}

    \caption{Correlation between human evaluators on the same document across metrics. Each point is the score of Evaluator 1 vs. Evaluator 2 on a particular document. $y = x$ is where human evaluators perfectly align with each other. We also compute the Mean Average Error (MAE), as well as the average Spearman correlation coefficient $\rho$.}
    \label{fig:binary-inter-human-correlation}
\end{figure*}
