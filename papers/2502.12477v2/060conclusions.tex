\section{Conclusion and Future Work}
\label{sec:concl}

\name uses LLMs and RAG in a concept-driven, three-stage framework to generate multiple-choice quizzes that assess deep understanding of large documents. Evaluations with \totalevaluators experts on \totaldocuments papers and dissertations show that, among those with a preference, \name outperforms a direct-prompting LLM baseline by 6.5$\times$ for dissertations and 1.5$\times$ for papers. Additionally, as document length increases, \name's advantages in question quality and cost efficiency become more pronounced.


We now discuss several avenues for future work.
While \name generates conceptual questions that test depth of understanding, few of them require mathematical analysis, logical reasoning, or creative thinking. \name produces quiz sessions, but we have not yet evaluated session quality. Currently, \name has not utilized human feedback to improve, which could be done using direct-preference optimization (DPO)~\cite{dpo}, Kahneman-Twersky Optimization (KTO) \cite{kto}, or reinforcement learning with human feedback (RLHF) \cite{rlhf}. To help learners, \name should adapt the difficulty of questions to the learner's answering accuracy and the time to answer questions. 


Our attempts to align AI-generated evaluations with human expert judgments have been unsuccessful. Further research is necessary to improve AI judges in educational contexts.
Finally, validating \name's domain-independence requires testing across a broader spectrum of fields. 