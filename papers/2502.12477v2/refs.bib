@inproceedings{colbert,
author = {Khattab, Omar and Zaharia, Matei},
title = "{ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT}",
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401075},
doi = {10.1145/3397271.3401075},
abstract = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {39–48},
numpages = {10},
keywords = {bert, deep language models, efficiency, neural ir},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{llm_hallucination,
author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
title = "{A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}",
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3703155},
doi = {10.1145/3703155},
abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {42},
numpages = {55},
keywords = {Large Language Models, Hallucination, Factuality, Faithfulness}
}

@inproceedings{colbertv2,
    title = "{{C}ol{BERT}v2: Effective and Efficient Retrieval via Lightweight Late Interaction}",
    author = "Santhanam, Keshav  and
      Khattab, Omar  and
      Saad-Falcon, Jon  and
      Potts, Christopher  and
      Zaharia, Matei",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.272/",
    doi = "10.18653/v1/2022.naacl-main.272",
    pages = "3715--3734",
    abstract = "Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6{--}10x."
}

@article{lost-in-the-middle,
    title = "Lost in the Middle: How Language Models Use Long Contexts",
    author = "Liu, Nelson F.  and
      Lin, Kevin  and
      Hewitt, John  and
      Paranjape, Ashwin  and
      Bevilacqua, Michele  and
      Petroni, Fabio  and
      Liang, Percy",
    journal = "{Trans. Association for Computational Linguistics}",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.9/",
    doi = "10.1162/tacl_a_00638",
    pages = "157--173",
    abstract = "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models."
}

@misc{nakano2022webgptbrowserassistedquestionansweringhuman,
      title="{WebGPT: Browser-assisted question-answering with human feedback}", 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.09332}, 
}

@inproceedings{yang-etal-2024-matplotagent,
    title = "{M}at{P}lot{A}gent: Method and Evaluation for {LLM}-Based Agentic Scientific Data Visualization",
    author = "Yang, Zhiyu  and
      Zhou, Zihan  and
      Wang, Shuo  and
      Cong, Xin  and
      Han, Xu  and
      Yan, Yukun  and
      Liu, Zhenghao  and
      Tan, Zhixing  and
      Liu, Pengyuan  and
      Yu, Dong  and
      Liu, Zhiyuan  and
      Shi, Xiaodong  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.701/",
    doi = "10.18653/v1/2024.findings-acl.701",
    pages = "11789--11804",
    abstract = "Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores."
}

@article{context_Steuer,
  title="{I Do Not Understand What I Cannot Define: Automatic Question Generation With Pedagogically-Driven Content Selection}",
  author={Tim Steuer and Anna Filighera and Tobias Meuser and Christoph Rensing},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.04123},
  url={https://api.semanticscholar.org/CorpusID:238531395}
}

@inproceedings{Context_diverse_hadifar,
    title = "Diverse Content Selection for Educational Question Generation",
    author = "Hadifar, Amir  and
      Bitew, Semere Kiros  and
      Deleu, Johannes  and
      Hoste, Veronique  and
      Develder, Chris  and
      Demeester, Thomas",
    editor = "Bassignana, Elisa  and
      Lindemann, Matthias  and
      Petit, Alban",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-srw.13/",
    doi = "10.18653/v1/2023.eacl-srw.13",
    pages = "123--133",
    abstract = "Question Generation (QG) systems have shown promising results in reducing the time and effort required to create questions for students. Typically, a first step in QG is to select the content to design a question for. In an educational setting, it is crucial that the resulting questions cover the most relevant/important pieces of knowledge the student should have acquired. Yet, current QG systems either consider just a single sentence or paragraph (thus do not include a selection step), or do not consider this educational viewpoint of content selection. Aiming to fill this research gap with a solution for educational document level QG, we thus propose to select contents for QG based on relevance and topic diversity. We demonstrate the effectiveness of our proposed content selection strategy for QG on 2 educational datasets. In our performance assessment, we also highlight limitations of existing QG evaluation metrics in light of the content selection problem."
}

@inproceedings{MIT_law,
    title = "Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling",
    author = "Jiang, Hang  and
      Zhang, Xiajie  and
      Mahari, Robert  and
      Kessler, Daniel  and
      Ma, Eric  and
      August, Tal  and
      Li, Irene  and
      Pentland, Alex  and
      Kim, Yoon  and
      Roy, Deb  and
      Kabbara, Jad",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.388/",
    doi = "10.18653/v1/2024.acl-long.388",
    pages = "7194--7219",
    abstract = "Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond."
}

@inproceedings{mathprompter,
    title = "{M}ath{P}rompter: Mathematical Reasoning using Large Language Models",
    author = "Imani, Shima  and
      Du, Liang  and
      Shrivastava, Harsh",
    editor = "Sitaram, Sunayana  and
      Beigman Klebanov, Beata  and
      Williams, Jason D",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-industry.4/",
    doi = "10.18653/v1/2023.acl-industry.4",
    pages = "37--42",
    abstract = "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose {\textquoteleft}MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the {\textquoteleft}MultiArith' dataset (78.7{\%} - 92.5{\%}) evaluated using 175B parameter GPT-based LLM."
}

@inproceedings{
metamath,
title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
author={Longhui Yu and Weisen Jiang and Han Shi and Jincheng YU and Zhengying Liu and Yu Zhang and James Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=N8N0hgNDRt}
}

@inproceedings{
llemma,
title={Llemma: An Open Language Model for Mathematics},
author={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen Marcus McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=4WnqRR915j}
}

@misc{codellamaopenfoundation,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
luo2024wizardcoder,
title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
author={Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=UnUwSIgK5W}
}

@article{alpha_geometry,
  title={Solving olympiad geometry without human demonstrations},
  author={Trieu H. Trinh and Yuhuai Wu and Quoc V. Le and He He and Thang Luong},
  journal={Nature},
  year={2024},
  volume={625},
  pages={476 - 482},
  url={https://api.semanticscholar.org/CorpusID:267032902}
}

@inproceedings{mcq_mult_sentence,
    title = "Generating Questions and Multiple-Choice Answers using Semantic Analysis of Texts",
    author = "Araki, Jun  and
      Rajagopal, Dheeraj  and
      Sankaranarayanan, Sreecharan  and
      Holm, Susan  and
      Yamakawa, Yukari  and
      Mitamura, Teruko",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "{Proc. 26th International Conference on Computational Linguistics}",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    url = "https://aclanthology.org/C16-1107/",
    pages = "1125--1136",
    abstract = "We present a novel approach to automated question generation that improves upon prior work both from a technology perspective and from an assessment perspective. Our system is aimed at engaging language learners by generating multiple-choice questions which utilize specific inference steps over multiple sentences, namely coreference resolution and paraphrase detection. The system also generates correct answers and semantically-motivated phrase-level distractors as answer choices. Evaluation by human annotators indicates that our approach requires a larger number of inference steps, which necessitate deeper semantic understanding of texts than a traditional single-sentence approach."
}

@inproceedings{
khattab2024dspy,
title={{DSP}y: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines},
author={Omar Khattab and Arnav Singhvi and Paridhi Maheshwari and Zhiyuan Zhang and Keshav Santhanam and Sri Vardhamanan A and Saiful Haq and Ashutosh Sharma and Thomas T. Joshi and Hanna Moazam and Heather Miller and Matei Zaharia and Christopher Potts},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=sY5N0zY5Od}
}

@InProceedings{Quiz_Maker,
author="Gabajiwala, Ebrahim
and Mehta, Priyav
and Singh, Ritik
and Koshy, Reeta",
editor="Singh, Pradeep Kumar
and Wierzcho{\'{n}}, S{\l}awomir T.
and Chhabra, Jitender Kumar
and Tanwar, Sudeep",
title="Quiz Maker: Automatic Quiz Generation from Text Using NLP",
booktitle="Futuristic Trends in Networks and Computing Technologies ",
year="2022",
publisher="Springer Nature Singapore",
address="Singapore",
pages="523--533",
abstract="In the past few years, there has been great technological advancement in the field of deep learning and natural language processing. One of the applications is automatic generation of quizzes from text. The recent advancement in NLP techniques has shown a lot of promise. The proposed solution uses an NLP pipeline involving Bert and T5 transformers to extract keywords and gain insights from the text input. From the extracted keywords, different types of questions are generated such as fill in the blanks, true or false, Wh-type and multiple choice questions. Latest state-of-the-art models proved to perform better in all stages of our pipeline. The results from these models have shown a lot of promise. Through a survey created for evaluating the model, around 60{\%} questions generated by the model were incorrectly identified as human generated or could not be determined by the survey participants.",
isbn="978-981-19-5037-7"
}

@inproceedings{Multi-Question,
    title = "{Educational Multi-Question Generation for Reading Comprehension}",
    author = "Rathod, Manav  and
      Tu, Tony  and
      Stasaski, Katherine",
    editor = {Kochmar, Ekaterina  and
      Burstein, Jill  and
      Horbach, Andrea  and
      Laarmann-Quante, Ronja  and
      Madnani, Nitin  and
      Tack, Ana{\"i}s  and
      Yaneva, Victoria  and
      Yuan, Zheng  and
      Zesch, Torsten},
    booktitle = "Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bea-1.26/",
    doi = "10.18653/v1/2022.bea-1.26",
    pages = "216--223",
    abstract = "Automated question generation has made great advances with the help of large NLP generation models. However, typically only one question is generated for each intended answer. We propose a new task, Multi-Question Generation, aimed at generating multiple semantically similar but lexically diverse questions assessing the same concept. We develop an evaluation framework based on desirable qualities of the resulting questions. Results comparing multiple question generation approaches in the two-question generation condition show a trade-off between question answerability and lexical diversity between the two questions. We also report preliminary results from sampling multiple questions from our model, to explore generating more than two questions. Our task can be used to further explore the educational impact of showing multiple distinct question wordings to students."
}

@inproceedings{seqQG,
    title = "{Machine Comprehension by Text-to-Text Neural Question Generation}",
    author = "Yuan, Xingdi  and
      Wang, Tong  and
      Gulcehre, Caglar  and
      Sordoni, Alessandro  and
      Bachman, Philip  and
      Zhang, Saizheng  and
      Subramanian, Sandeep  and
      Trischler, Adam",
    editor = "Blunsom, Phil  and
      Bordes, Antoine  and
      Cho, Kyunghyun  and
      Cohen, Shay  and
      Dyer, Chris  and
      Grefenstette, Edward  and
      Hermann, Karl Moritz  and
      Rimell, Laura  and
      Weston, Jason  and
      Yih, Scott",
    booktitle = "Proceedings of the 2nd Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-2603/",
    doi = "10.18653/v1/W17-2603",
    pages = "15--25",
    abstract = "We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD."
}

@misc{narayan2020qurious,
    title={QURIOUS: Question Generation Pretraining for Text Generation},
    author={Shashi Narayan and Gonçalo Simoes and Ji Ma and Hannah Craighead and Ryan Mcdonald},
    year={2020},
    eprint={2004.11026},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{KASNECI2023102274,
title = {ChatGPT for good? On opportunities and challenges of large language models for education},
journal = {Learning and Individual Differences},
volume = {103},
pages = {102274},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2023.102274},
url = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
author = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
keywords = {Large language models, Artificial intelligence, Education, Educational technologies},
abstract = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.}
}

@misc{microsoft_agriculture,
    title={{RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture}},
    author={Angels Balaguer and Vinamra Benara and Renato Luiz de Freitas Cunha and Roberto de M. Estevão Filho and Todd Hendry and Daniel Holstein and Jennifer Marsman and Nick Mecklenburg and Sara Malvar and Leonardo O. Nunes and Rafael Padilha and Morris Sharp and Bruno Silva and Swati Sharma and Vijay Aski and Ranveer Chandra},
    year={2024},
    eprint={2401.08406},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{knowledge_base_prompting,
    title = "{Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation}",
    author = "Liang, Yuanyuan  and
      Wang, Jianing  and
      Zhu, Hanlun  and
      Wang, Lei  and
      Qian, Weining  and
      Lan, Yunshi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.263/",
    doi = "10.18653/v1/2023.emnlp-main.263",
    pages = "4329--4343",
    abstract = "The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively."
}

@inproceedings{reading_comprehension_language_llm,
    title = "{Evaluating Reading Comprehension Exercises Generated by {LLM}s: A Showcase of {C}hat{GPT} in Education Applications}",
    author = "Xiao, Changrong  and
      Xu, Sean Xin  and
      Zhang, Kunpeng  and
      Wang, Yufang  and
      Xia, Lei",
    editor = {Kochmar, Ekaterina  and
      Burstein, Jill  and
      Horbach, Andrea  and
      Laarmann-Quante, Ronja  and
      Madnani, Nitin  and
      Tack, Ana{\"i}s  and
      Yaneva, Victoria  and
      Yuan, Zheng  and
      Zesch, Torsten},
    booktitle = "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.bea-1.52/",
    doi = "10.18653/v1/2023.bea-1.52",
    pages = "610--625",
    abstract = "The recent advancement of pre-trained Large Language Models (LLMs), such as OpenAI`s ChatGPT, has led to transformative changes across fields. For example, developing intelligent systems in the educational sector that leverage the linguistic capabilities of LLMs demonstrates a visible potential. Though researchers have recently explored how ChatGPT could possibly assist in student learning, few studies have applied these techniques to real-world classroom settings involving teachers and students. In this study, we implement a reading comprehension exercise generation system that provides high-quality and personalized reading materials for middle school English learners in China. Extensive evaluations of the generated reading passages and corresponding exercise questions, conducted both automatically and manually, demonstrate that the system-generated materials are suitable for students and even surpass the quality of existing human-written ones. By incorporating first-hand feedback and suggestions from experienced educators, this study serves as a meaningful pioneering application of ChatGPT, shedding light on the future design and implementation of LLM-based systems in the educational context."
}

@inproceedings{code_QG,
author = {Sarsa, Sami and Denny, Paul and Hellas, Arto and Leinonen, Juho},
title = {{Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models}},
year = {2022},
isbn = {9781450391948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501385.3543957},
doi = {10.1145/3501385.3543957},
abstract = {This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.},
booktitle = {ACM Conf. on International Computing Education Research - Volume 1},
pages = {27–43},
numpages = {17},
keywords = {Automated feedback, CS1, Code explanations, Exercise generation, GPT-3, Large language models, Natural language generation, OpenAI Codex, Programming exercises, Resource generation, Robosourcing},
location = {Lugano and Virtual Event, Switzerland},
series = {ICER '22}
}

@inproceedings{du-etal-2017-learning,
    title = "{Learning to Ask: Neural Question Generation for Reading Comprehension}",
    author = "Du, Xinya  and
      Shao, Junru  and
      Cardie, Claire",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1123/",
    doi = "10.18653/v1/P17-1123",
    pages = "1342--1352",
    abstract = "We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (\textit{i.e.,}, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer)."
}

@InProceedings{Neural_QG,
author="Zhou, Qingyu
and Yang, Nan
and Wei, Furu
and Tan, Chuanqi
and Bao, Hangbo
and Zhou, Ming",
editor="Huang, Xuanjing
and Jiang, Jing
and Zhao, Dongyan
and Feng, Yansong
and Hong, Yu",
title="{Neural Question Generation from Text: A Preliminary Study}",
booktitle="Natural Language Processing and Chinese Computing",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="662--671",
abstract="Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub-spans of the given passage. Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions. In this work, we propose to apply the neural encoder-decoder model to generate meaningful and diverse questions from natural language sentences. The encoder reads the input text and the answer position, to produce an answer-aware input representation, which is fed to the decoder to generate an answer focused question. We conduct a preliminary study on neural question generation from text with the SQuAD dataset, and the experiment results show that our method can produce fluent and diverse questions.",
isbn="978-3-319-73618-1"
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text}",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Conf. on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264/",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392"
}

@inproceedings{attention_is_all_you_need,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 title = {{Attention Is All You Need}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{chan-fan-2019-bert,
    title = "{{BERT} for Question Generation}",
    author = "Chan, Ying-Hong  and
      Fan, Yao-Chung",
    editor = "van Deemter, Kees  and
      Lin, Chenghua  and
      Takamura, Hiroya",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "–" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-8624/",
    doi = "10.18653/v1/W19-8624",
    pages = "173--177",
    abstract = "In this study, we investigate the employment of the pre-trained BERT language model to tackle question generation tasks. We introduce two neural architectures built on top of BERT for question generation tasks. The first one is a straightforward BERT employment, which reveals the defects of directly using BERT for text generation. And, the second one remedies the first one by restructuring the BERT employment into a sequential manner for taking information from previous decoded results. Our models are trained and evaluated on the question-answering dataset SQuAD. Experiment results show that our best model yields state-of-the-art performance which advances the BLEU4 score of existing best models from 16.85 to 18.91."
}

@inproceedings{devlin-etal-2019-bert,
    title = "{{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding}",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@misc{t5,
      title={{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@inproceedings{lewis-etal-2020-bart,
    title = "{{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703/",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
}

@inproceedings{gpt_3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {{Language Models are Few-Shot Learners}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{yang-etal-2018-hotpotqa,
    title = "{{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering}",
    author = "Yang, Zhilin  and
      Qi, Peng  and
      Zhang, Saizheng  and
      Bengio, Yoshua  and
      Cohen, William  and
      Salakhutdinov, Ruslan  and
      Manning, Christopher D.",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1259/",
    doi = "10.18653/v1/D18-1259",
    pages = "2369--2380",
    abstract = "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions."
}

@inproceedings{li-etal-2021-addressing-semantic,
    title = "{Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction}",
    author = "Li, Chenliang  and
      Bi, Bin  and
      Yan, Ming  and
      Wang, Wei  and
      Huang, Songfang",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.118/",
    doi = "10.18653/v1/2021.acl-short.118",
    pages = "942--947",
    abstract = "Recently, question answering (QA) based on machine reading comprehension has become popular. This work focuses on generative QA which aims to generate an abstractive answer to a given question instead of extracting an answer span from a provided passage. Generative QA often suffers from two critical problems: (1) summarizing content irrelevant to a given question, (2) drifting away from a correct answer during generation. In this paper, we address these problems by a novel Rationale-Enriched Answer Generator (REAG), which incorporates an extractive mechanism into a generative model. Specifically, we add an extraction task on the encoder to obtain the rationale for an answer, which is the most relevant piece of text in an input document to a given question. Based on the extracted rationale and original input, the decoder is expected to generate an answer with high confidence. We jointly train REAG on the MS MARCO QA+NLG task and the experimental results show that REAG improves the quality and semantic accuracy of answers over baseline models."
}

@INPROCEEDINGS{mcq_computing,
  author={Tran, Andrew and Angelikas, Kenneth and Rama, Egi and Okechukwu, Chiku and Smith, David H. and MacNeil, Stephen},
  booktitle={{IEEE Frontiers in Education Conf. (FIE)}}, 
  title={{Generating Multiple Choice Questions for Computing Courses Using Large Language Models}}, 
  year={2023},
  pages={1-8},
  keywords={Codes;Correlation;Computational modeling;Education;Real-time systems;Natural language processing;Recycling;large language models;generative AI;multiple-choice questions;computing education},
  doi={10.1109/FIE58773.2023.10342898}}


@misc{sahoo2024systematic,
    title={{A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications}},
    author={Pranab Sahoo and Ayush Kumar Singh and Sriparna Saha and Vinija Jain and Samrat Mondal and Aman Chadha},
    year={2024},
    eprint={2402.07927},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{
Zhang*2020BERTScore:,
title={{BERTScore: Evaluating Text Generation with BERT}},
author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@misc{openai2024gpt4o,
    title={{GPT-4o System Card}},
    author={OpenAI},
    year={2024},
    eprint={2410.21276},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{chatgpt,
  author       = {{OpenAI}},
  title        = {{ChatGPT: Large Language Model}},
  year         = {2023},
  howpublished = {\url{https://openai.com/chatgpt}},
  note         = {Jan 2024 version},
}

@misc{GROBID,
    author = "Grobid",
    title = {{GROBID}},
    howpublished = {\url{https://github.com/kermitt2/grobid}},
    publisher = {GitHub},
    year = {2008--2025},
    archivePrefix = {swh},
}

@article{loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2311.04939},
  year={2023}
}

@misc{openai_api,
  author       = {{OpenAI}},
  title        = {{OpenAI API}},
  year         = {2025},
  howpublished = {\url{https://platform.openai.com}},
}

@misc{together_ai_api,
  author       = {{Together AI}},
  title        = {{Together AI API}},
  year         = {2025},
  howpublished = {\url{https://docs.together.ai}},
}


@misc{zheng2023judging,
      title={{Judging LLM-as-a-judge with MT-Bench and Chatbot Arena}},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lin2023llmevalunifiedmultidimensionalautomatic,
      title="{LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models}", 
      author={Yen-Ting Lin and Yun-Nung Chen},
      year={2023},
      eprint={2305.13711},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13711}, 
}

@inproceedings{moon-etal-2024-generative,
    title = "Generative Interpretation: Toward Human-Like Evaluation for Educational Question-Answer Pair Generation",
    author = "Moon, Hyeonseok  and
      Lee, Jaewook  and
      Eo, Sugyeong  and
      Park, Chanjun  and
      Seo, Jaehyung  and
      Lim, Heuiseok",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.145/",
    pages = "2185--2196",
    abstract = "Educational question-answer generation has been extensively researched owing to its practical applicability. However, we have identified a persistent challenge concerning the evaluation of such systems. Existing evaluation methods often fail to produce objective results and instead exhibit a bias towards favoring high similarity to the ground-truth question-answer pairs. In this study, we demonstrate that these evaluation methods yield low human alignment and propose an alternative approach called Generative Interpretation (GI) to achieve more objective evaluations. Through experimental analysis, we reveal that GI outperforms existing evaluation methods in terms of human alignment, and even shows comparable performance with GPT3.5, only with BART-large."
}

@misc{raina2022multiplechoicequestiongenerationautomated,
      title={{Multiple-Choice Question Generation: Towards an Automated Assessment Framework}}, 
      author={Vatsal Raina and Mark Gales},
      year={2022},
      eprint={2209.11830},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.11830}, 
}

@inproceedings{dong-etal-2024-llm,
    title = "{Can {LLM} be a Personalized Judge?}",
    author = "Dong, Yijiang River  and
      Hu, Tiancheng  and
      Collier, Nigel",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.592/",
    doi = "10.18653/v1/2024.findings-emnlp.592",
    pages = "10126--10141",
    abstract = "As large language models (LLMs) gain widespread adoption, ensuring they cater to diverse user needs has become increasingly important. While many researchers have studied LLM personalization and role-playing, they primarily use LLM-as-a-Judge for evaluation without thoroughly examining its validity. This paper investigates the reliability of LLM-as-a-Personalized-Judge{---}asking LLMs to judge user preferences based on persona. Our results suggest that LLM-as-a-Personalized-Judge is less reliable for personalization than previously believed, showing low agreement with human ground truth. We observed that the personas provided to the LLM often have limited predictive power for the tasks, leading us to introduce verbal uncertainty estimation. We find that powerful LLMs are aware of the certainty of their prediction and can achieve high agreement with ground truth on high-certainty samples, indicating a promising approach for building reliable and scalable proxies for evaluating LLM personalization. Our human annotation reveals that third-person crowd worker evaluations of personalized preferences are even worse than LLM predictions, highlighting the challenges of evaluating LLM personalization."
}

@misc{zhu2023judgelm,
      title={{JudgeLM: Fine-tuned Large Language Models are Scalable Judges}}, 
      author={Lianghui Zhu and Xinggang Wang and Xinlong Wang},
      year={2023},
      eprint={2310.17631},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
wang2024pandalm,
title={Panda{LM}: An Automatic Evaluation Benchmark for {LLM} Instruction Tuning Optimization},
author={Yidong Wang and Zhuohao Yu and Wenjin Yao and Zhengran Zeng and Linyi Yang and Cunxiang Wang and Hao Chen and Chaoya Jiang and Rui Xie and Jindong Wang and Xing Xie and Wei Ye and Shikun Zhang and Yue Zhang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=5Nn2BLV7SB}
}

@misc{huang2024empiricalstudyllmasajudgellm,
      title={{An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4}}, 
      author={Hui Huang and Yingqi Qu and Xingyuan Bu and Hongli Zhou and Jing Liu and Muyun Yang and Bing Xu and Tiejun Zhao},
      year={2024},
      eprint={2403.02839},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.02839}, 
}

@inproceedings{
zheng2024large,
title={{Large Language Models Are Not Robust Multiple Choice Selectors}},
author={Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},
booktitle={{The Twelfth International Conference on Learning Representations}},
year={2024},
url={https://openreview.net/forum?id=shr9PXz7T0}
}

@inproceedings{wang-etal-2024-large-language-models-fair,
    title = "Large Language Models are not Fair Evaluators",
    author = "Wang, Peiyi  and
      Li, Lei  and
      Chen, Liang  and
      Cai, Zefan  and
      Zhu, Dawei  and
      Lin, Binghuai  and
      Cao, Yunbo  and
      Kong, Lingpeng  and
      Liu, Qi  and
      Liu, Tianyu  and
      Sui, Zhifang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.511/",
    doi = "10.18653/v1/2024.acl-long.511",
    pages = "9440--9450",
    abstract = "In this paper, we uncover a positional bias in the evaluation paradigm of adopting large language models (LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. We propose a simple yet effective calibration framework to address our discovered positional bias.To evaluate the effectiveness of our framework, we manually annotate the {\textquotedblleft}win/tie/lose{\textquotedblright} outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark`s question prompt. Extensive experiments demonstrate that our approach successfully alleviates evaluation bias, resulting in closer alignment with human judgments."
}

@inproceedings{koo-etal-2024-benchmarking,
    title = "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
    author = "Koo, Ryan  and
      Lee, Minhwa  and
      Raheja, Vipul  and
      Park, Jong Inn  and
      Kim, Zae Myung  and
      Kang, Dongyeop",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.29/",
    doi = "10.18653/v1/2024.findings-acl.29",
    pages = "517--545",
    abstract = "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 16 LLMs encompassing four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLer), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (40{\%} of comparisons made by all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 44{\%}, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences."
}

@inproceedings{chen-etal-2024-humans,
    title = "{Humans or {LLM}s as the Judge? A Study on Judgement Bias}",
    author = "Chen, Guiming Hardy  and
      Chen, Shunian  and
      Liu, Ziche  and
      Jiang, Feng  and
      Wang, Benyou",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proc. Conf. on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.474/",
    doi = "10.18653/v1/2024.emnlp-main.474",
    pages = "8301--8327",
    abstract = "Adopting human and large language models (LLM) as judges (*a.k.a* human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating **Misinformation Oversight Bias**, **Gender Bias**, **Authority Bias** and **Beauty Bias** on LLM and human judges. We curate a dataset referring to the revised Bloom`s Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems."
}

@inproceedings{papineni-etal-2002-bleu,
    title = {{BLEU: A Method for Automatic Evaluation of Machine Translation}},
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040/",
    doi = "10.3115/1073083.1073135",
    pages = "311--318"
}


@inproceedings{guo2024survey-neural-question-gen,
author = {Guo, Shash and Liao, Lizi and Li, Cuiping and Chua, Tat-Seng},
title = "{A survey on neural question generation: Methods, applications, and prospects}",
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/889},
doi = {10.24963/ijcai.2024/889},
abstract = {In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking perspective on the trajectory of NQG, identifying emergent research trends and prospective developmental paths. Accompanying this survey is a curated collection of related research papers, datasets, and codes, all of which are available on GitHub. This provides an extensive reference for those delving into NQG.},
booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
articleno = {889},
numpages = {10},
location = {Jeju, Korea},
series = {IJCAI '24}
}


@article{lewis2020retrieval,
  title={{Retrieval-augmented generation for knowledge-intensive nlp tasks}},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{shuster2021retrieval,
  title={{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}},
  author={Shuster, Kurt and Humeau, Samuel and Bordes, Antoine and Weston, Jason},
  booktitle={arXiv preprint arXiv:2102.05638},
  year={2021}
}

@inproceedings{karpukhin2020dense,
  title="{Dense Passage Retrieval for Open-Domain Question Answering}",
  author={Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Fan, Angela and Grave, Edouard and Joulin, Armand},
  booktitle={Conf. on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6769--6781},
  year={2020}
}

@article{izacard2021leveraging,
  title={{Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering}},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2104.07972},
  year={2021}
}

@inproceedings{gottumukkala2022investigating,
  title={{Investigating the Utility of Retrieval-Augmented Generation for Code Summarization}},
  author={Gottumukkala, Aditya and Sas, Mihai and McMillan, Collin},
  booktitle={Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1215--1226},
  year={2022}
}

@article{pezeshkpour2023large,
  title={{Large language models sensitivity to the order of options in multiple-choice questions}},
  author={Pezeshkpour, Pouya and Hruschka, Estevam},
  journal={arXiv preprint arXiv:2308.11483},
  year={2023}
}

@article{wei2022chain,
  title={{Chain-of-thought prompting elicits reasoning in large language models}},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{naismith2023automated,
  title={{Automated evaluation of written discourse coherence using GPT-4}},
  author={Naismith, Ben and Mulcaire, Phoebe and Burstein, Jill},
  booktitle={Proc. 18th Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={394--403},
  year={2023}
}

@book{anderson2001taxonomy,
  author    = {Lorin W. Anderson and David R. Krathwohl},
  title     = {{A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives}},
  publisher = {Addison Wesley Longman},
  year      = {2001},
  address   = {New York}
}

@article{fu2024qgeval,
  title={{QGEval: A Benchmark for Question Generation Evaluation}},
  author={Fu, Weiping and Wei, Bifan and Hu, Jianxiang and Cai, Zhongmin and Liu, Jun},
  journal={arXiv preprint arXiv:2406.05707},
  year={2024}
}

@misc{langchain_mapreduce,
  author    = {LangChain Team},
  title     = {{Map Reduce -- LangChain Documentation}},
  year      = {2023},
  url       = {https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/},
  note      = {Accessed: 2025-02-15}
}

@article{huang2024empirical,
  title={{An empirical study of LLM-as-a-judge for LLM evaluation: Fine-tuned judge models are task-specific classifiers}},
  author={Huang, Hui and Qu, Yingqi and Liu, Jing and Yang, Muyun and Zhao, Tiejun},
  journal={arXiv preprint arXiv:2403.02839},
  year={2024}
}

@inproceedings{gao2024insights,
  title={{Insights into LLM long-context failures: When transformers know but don’t tell}},
  author={Gao, Muhan and Lu, TaiMing and Yu, Kuai and Byerly, Adam and Khashabi, Daniel},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={7611--7625},
  year={2024}
}

@article{dpo,
  title={{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kto,
  title={{KTO: Model Alignment as Prospect Theoretic Optimization}},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{rlhf,
  title="{Deep Reinforcement Learning from Human Preferences}",
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{bender2021dangers,
  title={{On the dangers of stochastic parrots: Can language models be too big?}},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@misc{langchain,
  title = {{LangChain}},
  author = {Harrison Chase et al.},
  year = {2022},
  url = {https://www.langchain.com/}
}

@misc{wandb,
  title = {{Weights \& Biases}},
  author = {Biewald, Lukas},
  year = {2020},
  url = {https://wandb.ai/}
}

@misc{ACLCodeOfEthics,
  author       = {{Association for Computational Linguistics}},
  title        = {ACL Code of Ethics},
  howpublished = {Online},
  url          = {https://www.aclweb.org/portal/content/acl-code-ethics},
  note         = {Accessed: 2025-02-21}
}
