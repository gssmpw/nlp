\begin{figure*}[!t]
\centering
    \includegraphics[width=1\linewidth]{FIG/savaal.drawio.pdf}
    \caption{\name's Pipeline. \bluecircle\ \name extracts main ideas from sections of the document in parallel, \redcircle\ combines them into a succinct list, and \greencircle\ ranks them in order of importance. Next, \purplecircle\ \name fetches relevant passages from the document using a vector-based retrieval model. Finally, \orangecircle\ given a main idea and fetched passages, \name generates questions.}
    \label{fig:savaal-workflow}
\vspace{-10 pt}
\end{figure*}

\section{\name's Question-Generation Pipeline}
\label{sec:pipeline}


To address challenges of \autoref{sec:insights}, we propose a novel three-stage pipeline: \emph{main idea extraction}, \emph{relevant passage retrieval}, and \emph{question generation}. \autoref{fig:savaal-workflow} shows Savaal's workflow. The idea is to generate questions targeted at key explicitly determined concepts and to retrieve passages relevant to the concept from the source document.
% to generate questions.


\subsection{Extracting Main Ideas}
\label{sec:pipeline-main-idea}
This stage extracts succinct main ideas from different document chapters. This is done in a map-combine-reduce fashion~\cite{langchain_mapreduce}. First, we use GROBID~\citep{GROBID} to parse and segment documents into distinct sections.


In the map stage, \circled{1}{customblue}, we use an LLM to extract the main ideas for each section individually. These extracted main ideas are aggregated and deduplicated in the combine stage, \circled{2}{customred}, into a single, cohesive list of the paperâ€™s main ideas. If the combined output exceeds a predefined length threshold (set to the maximum token window of the LLM), the reduce stage collapses the list further for brevity and clarity. The result is a curated list of main ideas, including main idea titles and their short descriptions (see \autoref{subsubsec:example_main_idea} for examples). The same (or a different) LLM then ranks the main ideas based on their importance in the ranking stage in \circled{3}{customgreen} (see \autoref{subsec:appendix_prompts} for the prompts).

Initially, we attempted to extract the main ideas for the entire document in one shot. However, as noted in \autoref{sec:insights-whole-context}, as the context length grew, this became less effective. We found that using map-reduce extracted main ideas that were more detailed and useful for question generation, particularly on large documents.



\subsection{Retrieving Relevant Passages}
\label{sec:pipeline-retrieval}

Because the main ideas in \autoref{sec:pipeline-main-idea} are concise, they lack sufficient content to generate a question. As discussed in \autoref{sec:insights-summary}, asking an LLM to generate questions based on a concept alone (a main idea or even a summary) has shortcomings. To overcome this problem, \name retrieves relevant text segments directly from the original document to provide granular content for generating a question and to ensure that the questions are grounded in truth.


\name's retriever uses ColBERT, a late-interaction retrieval method~\citep{colbert, colbertv2}, to find the most relevant passages for each main idea (stage \circled{4}{custompurple}).
% integrated in the RAGatouille \footnote{\url{https://github.com/AnswerDotAI/RAGatouille}} library.
For each ranked main idea in \circled{3}{customgreen}, we retrieve the top $k$ passages as added context for the next stage ($k=3$ in our experiments).

We chose ColBERT for its state-of-the-art performance and wide adoption, but any high-performing retrieval method could be used. We also tried using the LLM to identify passages related to a main idea, but as in \autoref{sec:insights-whole-context} and \autoref{sec:pipeline-main-idea}, it struggled with large context sizes.


\subsection{Generating Questions and Choices}
\label{sec:pipeline-QG}


After retrieving the passages for each main idea, stage \circled{5}{customorange} instructs an LLM to generate questions. To create $N$ questions from $M$ ideas, we generate $N/M$ questions per idea.\footnote{We use only the top $N$ ranked main ideas if $N < M$.}  The prompt (\autoref{fig:question_generation}) includes the main idea and its retrieved passages.


Although LLMs often produce good questions, generating good {\em choices} is more challenging. Poorly designed choices can make the correct answer too obvious or, worse, introduce ambiguity or multiple correct options. We experimented with many prompt variations to improve choice quality, yielding mixed results. We also tested a separate ``choice refinement'' stage, where an LLM was specifically instructed to improve the answer choices for a given question. This prompt included detailed constraints, such as ensuring alignment with the question's intent (e.g., a question about benefits should not include limitations as choices; see \autoref{appendix:choice-refine}).
Although this additional step produced more challenging choices, we found that it caused excessive ambiguity and was less preferred by human expert evaluators. Therefore, \name does not include a choice refinement stage. Instead, its question-generation prompt explicitly emphasizes that the choices should be ``plausible distractors''.

Finally, we observed {\em positional biases} in the placement of the correct choice, corroborating prior findings~\cite{pezeshkpour2023large}. For example, in a set of 1000 questions from 50 papers (20 per paper) generated by \texttt{GPT-4o}, choice B was correct 73.3\% of the time! Thus, we randomize the choices to eliminate this bias.
