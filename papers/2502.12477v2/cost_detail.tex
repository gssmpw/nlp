% \subsection{Discussion of Cost Scalability}
% \label{appendix:costs}

\name is also more cost-effective as the size of the document, $D$, grows. \BaselineMT costs $\approx \frac{N}{b} \cdot (A \cdot D + 100b \cdot B)$, where $A$ is cost per input token, $B$ is cost per output token, $N$ is the number of questions, $b$ is the batch size of \BaselineMT, and $100b$ is the approximate number of output tokens when generating $b$ questions. By contrast, \name costs $\approx f(D) + 100NB$ where $f(D)$ is the cost of main idea extraction, and $N$ is the number of questions. Thus, \name incurs a fixed cost that depends on the size of the document, but the marginal cost of generating additional questions is then independent of document size. By contrast, \Baseline incurs additional input token cost of $AD$ for each  batch of generated questions. 

In our experiments, for a PhD dissertation, $f(D) \approx 1.48A \cdot D$ on average.  Therefore, \name has lower cost when $\frac{N}{b} > 1.48$. For $N = 100$, \Baseline requires $b \approx 67$ to incur the same cost as \name, which is impractical with current LLMs. Both \texttt{GPT-4o} and \texttt{Meta-Llama-3.3-70B-Instruct} do not reliably generate more than $\approx$ 20 questions in a batch. 



In \autoref{fig:cost-scalability}, we also notate \BaselineMT with caching. Prompt caching is a feature made available from various LLM providers. It works by matching a prompt prefix, like a long system prompt or other long context from previous multi-turn conversations, to reduce computation time and API costs. As of writing in February 2025, the OpenAI API charged 50\% less for cached prompt tokens, resulting in up-to 80\% latency improvements. The \BaselineMT method benefits from this caching scheme, as it repeatedly sends the entire document as a cache prefix to the API. As shown in \autoref{fig:cost-scalability}, \BaselineMT is more cost-effective than \name up until $N \approx 80$ with prompt caching, as opposed to $N \approx 60$ without prompt caching.

However, prompt caching has several limitations. First, many providers evict cache entries after a short period of time, around 5-10 minutes. Thus, all $N$ questions must be generated within a set time frame to benefit. Moreover, many open-source model providers do not include prompt caching as a feature (as of the time of writing). Therefore, while we present the benefits that prompt caching may provide \BaselineMT, we still demonstrate that \name is more cost effective at large scale.
